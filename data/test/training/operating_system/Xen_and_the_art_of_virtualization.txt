Xen and the Art of VirtualizationPaul Barham, Boris Dragovic, Keir Fraser, Steven Hand, Tim Harris,Alex Ho, Rolf Neugebauer, Ian Pratt, Andrew WarfieldUniversity of Cambridge Computer Laboratory15 JJ Thomson Avenue, Cambridge, UK, CB3 0FDfirstname.lastnamecl.cam.ac.ukABSTRACTNumerous systems have been designed which use virtualization tosubdivide the ample resources of a modern computer. Some requirespecialized hardware, or cannot support commodity operating systems. Some target 100 binary compatibility at the expense ofperformance. Others sacrifice security or functionality for speed.Few offer resource isolation or performance guarantees most provide only besteffort provisioning, risking denial of service.This paper presents Xen, an x86 virtual machine monitor whichallows multiple commodity operating systems to share conventionalhardware in a safe and resource managed fashion, but without sacrificing either performance or functionality. This is achieved byproviding an idealized virtual machine abstraction to which operating systems such as Linux, BSD and Windows XP, can be portedwith minimal effort.Our design is targeted at hosting up to 100 virtual machine instances simultaneously on a modern server. The virtualization approach taken by Xen is extremely efficient we allow operating systems such as Linux and Windows XP to be hosted simultaneouslyfor a negligible performance overhead  at most a few percentcompared with the unvirtualized case. We considerably outperformcompeting commercial and freely available solutions in a range ofmicrobenchmarks and systemwide tests.Categories and Subject DescriptorsD.4.1 Operating Systems Process Management D.4.2 Operating Systems Storage Management D.4.8 Operating SystemsPerformanceGeneral TermsDesign, Measurement, PerformanceKeywordsVirtual Machine Monitors, Hypervisors, ParavirtualizationMicrosoft Research Cambridge, UKIntel Research Cambridge, UKPermission to make digital or hard copies of all or part of this work forpersonal or classroom use is granted without fee provided that copies arenot made or distributed for profit or commercial advantage and that copiesbear this notice and the full citation on the first page. To copy otherwise, torepublish, to post on servers or to redistribute to lists, requires prior specificpermission andor a fee.SOSP03, October 1922, 2003, Bolton Landing, New York, USA.Copyright 2003 ACM 1581137575030010 ...5.00.1. INTRODUCTIONModern computers are sufficiently powerful to use virtualizationto present the illusion of many smaller virtual machines VMs,each running a separate operating system instance. This has led toa resurgence of interest in VM technology. In this paper we presentXen, a high performance resourcemanaged virtual machine monitor VMM which enables applications such as server consolidation 42, 8, colocated hosting facilities 14, distributed web services 43, secure computing platforms 12, 16 and applicationmobility 26, 37.Successful partitioning of a machine to support the concurrentexecution of multiple operating systems poses several challenges.Firstly, virtual machines must be isolated from one another it is notacceptable for the execution of one to adversely affect the performance of another. This is particularly true when virtual machinesare owned by mutually untrusting users. Secondly, it is necessaryto support a variety of different operating systems to accommodatethe heterogeneity of popular applications. Thirdly, the performanceoverhead introduced by virtualization should be small.Xen hosts commodity operating systems, albeit with some sourcemodifications. The prototype described and evaluated in this papercan support multiple concurrent instances of our XenoLinux guestoperating system each instance exports an application binary interface identical to a nonvirtualized Linux 2.4. Our port of WindowsXP to Xen is not yet complete but is capable of running simpleuserspace processes. Work is also progressing in porting NetBSD.Xen enables users to dynamically instantiate an operating system to execute whatever they desire. In the XenoServer project 15,35 we are deploying Xen on standard server hardware at economically strategic locations within ISPs or at Internet exchanges. Weperform admission control when starting new virtual machines andexpect each VM to pay in some fashion for the resources it requires.We discuss our ideas and approach in this direction elsewhere 21this paper focuses on the VMM.There are a number of ways to build a system to host multipleapplications and servers on a shared machine. Perhaps the simplestis to deploy one or more hosts running a standard operating system such as Linux or Windows, and then to allow users to installfiles and start processes  protection between applications beingprovided by conventional OS techniques. Experience shows thatsystem administration can quickly become a timeconsuming taskdue to complex configuration interactions between supposedly disjoint applications.More importantly, such systems do not adequately support performance isolation the scheduling priority, memory demand, network traffic and disk accesses of one process impact the performance of others. This may be acceptable when there is adequateprovisioning and a closed user group such as in the case of computational grids, or the experimental PlanetLab platform 33, butnot when resources are oversubscribed, or users uncooperative.One way to address this problem is to retrofit support for performance isolation to the operating system. This has been demonstrated to a greater or lesser degree with resource containers 3,LinuxRK 32, QLinux 40 and SILK 4. One difficulty withsuch approaches is ensuring that all resource usage is accounted tothe correct process  consider, for example, the complex interactions between applications due to buffer cache or page replacementalgorithms. This is effectively the problem of QoS crosstalk 41within the operating system. Performing multiplexing at a low levelcan mitigate this problem, as demonstrated by the Exokernel 23and Nemesis 27 operating systems. Unintentional or undesiredinteractions between tasks are minimized.We use this same basic approach to build Xen, which multiplexesphysical resources at the granularity of an entire operating systemand is able to provide performance isolation between them. In contrast to processlevel multiplexing this also allows a range of guestoperating systems to gracefully coexist rather than mandating aspecific application binary interface. There is a price to pay for thisflexibility  running a full OS is more heavyweight than runninga process, both in terms of initialization e.g. booting or resumingversus fork and exec, and in terms of resource consumption.For our target of up to 100 hosted OS instances, we believe thisprice is worth paying it allows individual users to run unmodifiedbinaries, or collections of binaries, in a resource controlled fashionfor instance an Apache server along with a PostgreSQL backend.Furthermore it provides an extremely high level of flexibility sincethe user can dynamically create the precise execution environmenttheir software requires. Unfortunate configuration interactions between various services and applications are avoided for example,each Windows instance maintains its own registry.The remainder of this paper is structured as follows in Section 2we explain our approach towards virtualization and outline howXen works. Section 3 describes key aspects of our design and implementation. Section 4 uses industry standard benchmarks to evaluate the performance of XenoLinux running above Xen in comparison with standalone Linux, VMware Workstation and UsermodeLinux UML. Section 5 reviews related work, and finally Section 6discusses future work and concludes.2. XEN APPROACH  OVERVIEWIn a traditional VMM the virtual hardware exposed is functionally identical to the underlying machine 38. Although full virtualization has the obvious benefit of allowing unmodified operatingsystems to be hosted, it also has a number of drawbacks. This isparticularly true for the prevalent IA32, or x86, architecture.Support for full virtualization was never part of the x86 architectural design. Certain supervisor instructions must be handled bythe VMM for correct virtualization, but executing these with insufficient privilege fails silently rather than causing a convenienttrap 36. Efficiently virtualizing the x86 MMU is also difficult.These problems can be solved, but only at the cost of increasedcomplexity and reduced performance. VMwares ESX Server 10dynamically rewrites portions of the hosted machine code to inserttraps wherever VMM intervention might be required. This translation is applied to the entire guest OS kernel with associated translation, execution, and caching costs since all nontrapping privileged instructions must be caught and handled. ESX Server implements shadow versions of system structures such as page tables andmaintains consistency with the virtual tables by trapping every update attempt  this approach has a high cost for updateintensiveoperations such as creating a new application process.Notwithstanding the intricacies of the x86, there are other arguments against full virtualization. In particular, there are situationsin which it is desirable for the hosted operating systems to see realas well as virtual resources providing both real and virtual timeallows a guest OS to better support timesensitive tasks, and to correctly handle TCP timeouts and RTT estimates, while exposing realmachine addresses allows a guest OS to improve performance byusing superpages 30 or page coloring 24.We avoid the drawbacks of full virtualization by presenting a virtual machine abstraction that is similar but not identical to the underlying hardware  an approach which has been dubbed paravirtualization 43. This promises improved performance, althoughit does require modifications to the guest operating system. It isimportant to note, however, that we do not require changes to theapplication binary interface ABI, and hence no modifications arerequired to guest applications.We distill the discussion so far into a set of design principles1. Support for unmodified application binaries is essential, orusers will not transition to Xen. Hence we must virtualize allarchitectural features required by existing standard ABIs.2. Supporting full multiapplication operating systems is important, as this allows complex server configurations to bevirtualized within a single guest OS instance.3. Paravirtualization is necessary to obtain high performanceand strong resource isolation on uncooperative machine architectures such as x86.4. Even on cooperative machine architectures, completely hiding the effects of resource virtualization from guest OSesrisks both correctness and performance.Note that our paravirtualized x86 abstraction is quite differentfrom that proposed by the recent Denali project 44. Denali is designed to support thousands of virtual machines running networkservices, the vast majority of which are smallscale and unpopular. In contrast, Xen is intended to scale to approximately 100 virtual machines running industry standard applications and services.Given these very different goals, it is instructive to contrast Denalisdesign choices with our own principles.Firstly, Denali does not target existing ABIs, and so can elidecertain architectural features from their VM interface. For example, Denali does not fully support x86 segmentation although it isexported and widely used1 in the ABIs of NetBSD, Linux, andWindows XP.Secondly, the Denali implementation does not address the problem of supporting application multiplexing, nor multiple addressspaces, within a single guest OS. Rather, applications are linkedexplicitly against an instance of the Ilwaco guest OS in a mannerrather reminiscent of a libOS in the Exokernel 23. Hence each virtual machine essentially hosts a singleuser singleapplication unprotected operating system. In Xen, by contrast, a single virtualmachine hosts a real operating system which may itself securelymultiplex thousands of unmodified userlevel processes. Althougha prototype virtual MMU has been developed which may help Denali in this area 44, we are unaware of any published technicaldetails or evaluation.Thirdly, in the Denali architecture the VMM performs all pagingto and from disk. This is perhaps related to the lack of memorymanagement support at the virtualization layer. Paging within the1For example, segments are frequently used by thread libraries to addressthreadlocal data.Memory ManagementSegmentation Cannot install fullyprivileged segment descriptors and cannot overlap with the top end of the linearaddress space.Paging Guest OS has direct read access to hardware page tables, but updates are batched and validated bythe hypervisor. A domain may be allocated discontiguous machine pages.CPUProtection Guest OS must run at a lower privilege level than Xen.Exceptions Guest OS must register a descriptor table for exception handlers with Xen. Aside from page faults,the handlers remain the same.System Calls Guest OS may install a fast handler for system calls, allowing direct calls from an application intoits guest OS and avoiding indirecting through Xen on every call.Interrupts Hardware interrupts are replaced with a lightweight event system.Time Each guest OS has a timer interface and is aware of both real and virtual time.Device IONetwork, Disk, etc. Virtual devices are elegant and simple to access. Data is transferred using asynchronous IO rings.An event mechanism replaces hardware interrupts for notifications.Table 1 The paravirtualized x86 interface.VMM is contrary to our goal of performance isolation maliciousvirtual machines can encourage thrashing behaviour, unfairly depriving others of CPU time and disk bandwidth. In Xen we expecteach guest OS to perform its own paging using its own guaranteed memory reservation and disk allocation an idea previouslyexploited by selfpaging 20.Finally, Denali virtualizes the namespaces of all machine resources, taking the view that no VM can access the resource allocations of another VM if it cannot name them for example, VMs haveno knowledge of hardware addresses, only the virtual addressescreated for them by Denali. In contrast, we believe that secure access control within the hypervisor is sufficient to ensure protectionfurthermore, as discussed previously, there are strong correctnessand performance arguments for making physical resources directlyvisible to guest OSes.In the following section we describe the virtual machine abstraction exported by Xen and discuss how a guest OS must be modifiedto conform to this. Note that in this paper we reserve the term guestoperating system to refer to one of the OSes that Xen can host andwe use the term domain to refer to a running virtual machine withinwhich a guest OS executes the distinction is analogous to that between a program and a process in a conventional system. We callXen itself the hypervisor since it operates at a higher privilege levelthan the supervisor code of the guest operating systems that it hosts.2.1 The Virtual Machine InterfaceTable 1 presents an overview of the paravirtualized x86 interface,factored into three broad aspects of the system memory management, the CPU, and device IO. In the following we address eachmachine subsystem in turn, and discuss how each is presented inour paravirtualized architecture. Note that although certain partsof our implementation, such as memory management, are specificto the x86, many aspects such as our virtual CPU and IO devicescan be readily applied to other machine architectures. Furthermore,x86 represents a worst case in the areas where it differs significantlyfrom RISCstyle processors  for example, efficiently virtualizinghardware page tables is more difficult than virtualizing a softwaremanaged TLB.2.1.1 Memory managementVirtualizing memory is undoubtedly the most difficult part ofparavirtualizing an architecture, both in terms of the mechanismsrequired in the hypervisor and modifications required to port eachguest OS. The task is easier if the architecture provides a softwaremanaged TLB as these can be efficiently virtualized in a simplemanner 13. A tagged TLB is another useful feature supportedby most serverclass RISC architectures, including Alpha, MIPSand SPARC. Associating an addressspace identifier tag with eachTLB entry allows the hypervisor and each guest OS to efficientlycoexist in separate address spaces because there is no need to flushthe entire TLB when transferring execution.Unfortunately, x86 does not have a softwaremanaged TLB instead TLB misses are serviced automatically by the processor bywalking the page table structure in hardware. Thus to achieve thebest possible performance, all valid page translations for the currentaddress space should be present in the hardwareaccessible pagetable. Moreover, because the TLB is not tagged, address spaceswitches typically require a complete TLB flush. Given these limitations, we made two decisions i guest OSes are responsible forallocating and managing the hardware page tables, with minimalinvolvement from Xen to ensure safety and isolation and ii Xenexists in a 64MB section at the top of every address space, thusavoiding a TLB flush when entering and leaving the hypervisor.Each time a guest OS requires a new page table, perhaps because a new process is being created, it allocates and initializes apage from its own memory reservation and registers it with Xen.At this point the OS must relinquish direct write privileges to thepagetable memory all subsequent updates must be validated byXen. This restricts updates in a number of ways, including onlyallowing an OS to map pages that it owns, and disallowing writablemappings of page tables. Guest OSes may batch update requests toamortize the overhead of entering the hypervisor. The top 64MBregion of each address space, which is reserved for Xen, is not accessible or remappable by guest OSes. This address region is notused by any of the common x86 ABIs however, so this restrictiondoes not break application compatibility.Segmentation is virtualized in a similar way, by validating updates to hardware segment descriptor tables. The only restrictionson x86 segment descriptors are i they must have lower privilege than Xen, and ii they may not allow any access to the Xenreserved portion of the address space.2.1.2 CPUVirtualizing the CPU has several implications for guest OSes.Principally, the insertion of a hypervisor below the operating system violates the usual assumption that the OS is the most privilegedentity in the system. In order to protect the hypervisor from OSmisbehavior and domains from one another guest OSes must bemodified to run at a lower privilege level.Many processor architectures only provide two privilege levels.In these cases the guest OS would share the lower privilege levelwith applications. The guest OS would then protect itself by running in a separate address space from its applications, and indirectlypass control to and from applications via the hypervisor to set thevirtual privilege level and change the current address space. Again,if the processors TLB supports addressspace tags then expensiveTLB flushes can be avoided.Efficient virtualizion of privilege levels is possible on x86 because it supports four distinct privilege levels in hardware. The x86privilege levels are generally described as rings, and are numberedfrom zero most privileged to three least privileged. OS codetypically executes in ring 0 because no other ring can execute privileged instructions, while ring 3 is generally used for applicationcode. To our knowledge, rings 1 and 2 have not been used by anywellknown x86 OS since OS2. Any OS which follows this common arrangement can be ported to Xen by modifying it to executein ring 1. This prevents the guest OS from directly executing privileged instructions, yet it remains safely isolated from applicationsrunning in ring 3.Privileged instructions are paravirtualized by requiring them tobe validated and executed within Xen this applies to operationssuch as installing a new page table, or yielding the processor whenidle rather than attempting to hlt it. Any guest OS attempt todirectly execute a privileged instruction is failed by the processor,either silently or by taking a fault, since only Xen executes at asufficiently privileged level.Exceptions, including memory faults and software traps, are virtualized on x86 very straightforwardly. A table describing the handler for each type of exception is registered with Xen for validation. The handlers specified in this table are generally identicalto those for real x86 hardware this is possible because the exception stack frames are unmodified in our paravirtualized architecture. The sole modification is to the page fault handler, whichwould normally read the faulting address from a privileged processor register CR2 since this is not possible, we write it into anextended stack frame2. When an exception occurs while executingoutside ring 0, Xens handler creates a copy of the exception stackframe on the guest OS stack and returns control to the appropriateregistered handler.Typically only two types of exception occur frequently enough toaffect system performance system calls which are usually implemented via a software exception, and page faults. We improve theperformance of system calls by allowing each guest OS to registera fast exception handler which is accessed directly by the processor without indirecting via ring 0 this handler is validated beforeinstalling it in the hardware exception table. Unfortunately it is notpossible to apply the same technique to the page fault handler because only code executing in ring 0 can read the faulting addressfrom register CR2 page faults must therefore always be deliveredvia Xen so that this register value can be saved for access in ring 1.Safety is ensured by validating exception handlers when they arepresented to Xen. The only required check is that the handlers codesegment does not specify execution in ring 0. Since no guest OScan create such a segment, it suffices to compare the specified segment selector to a small number of static values which are reservedby Xen. Apart from this, any other handler problems are fixed upduring exception propagation  for example, if the handlers code2In hindsight, writing the value into a preagreed shared memory locationrather than modifying the stack frame would have simplified the XP port.OS subsection  linesLinux XPArchitectureindependent 78 1299Virtual network driver 484 Virtual blockdevice driver 1070 Xenspecific nondriver 1363 3321Total 2995 4620Portion of total x86 code base 1.36 0.04Table 2 The simplicity of porting commodity OSes to Xen. Thecost metric is the number of lines of reasonably commented andformatted code which are modified or added compared with theoriginal x86 code base excluding device drivers.segment is not present or if the handler is not paged into memory then an appropriate fault will be taken when Xen executes theiret instruction which returns to the handler. Xen detects thesedouble faults by checking the faulting program counter value ifthe address resides within the exceptionvirtualizing code then theoffending guest OS is terminated.Note that this lazy checking is safe even for the direct systemcall handler access faults will occur when the CPU attempts todirectly jump to the guest OS handler. In this case the faultingaddress will be outside Xen since Xen will never execute a guestOS system call and so the fault is virtualized in the normal way.If propagation of the fault causes a further double fault then theguest OS is terminated as described above.2.1.3 Device IORather than emulating existing hardware devices, as is typicallydone in fullyvirtualized environments, Xen exposes a set of cleanand simple device abstractions. This allows us to design an interface that is both efficient and satisfies our requirements for protection and isolation. To this end, IO data is transferred to and fromeach domain via Xen, using sharedmemory, asynchronous bufferdescriptor rings. These provide a highperformance communication mechanism for passing buffer information vertically throughthe system, while allowing Xen to efficiently perform validationchecks for example, checking that buffers are contained within adomains memory reservation.Similar to hardware interrupts, Xen supports a lightweight eventdelivery mechanism which is used for sending asynchronous notifications to a domain. These notifications are made by updating abitmap of pending event types and, optionally, by calling an eventhandler specified by the guest OS. These callbacks can be held offat the discretion of the guest OS  to avoid extra costs incurred byfrequent wakeup notifications, for example.2.2 The Cost of Porting an OS to XenTable 2 demonstrates the cost, in lines of code, of porting commodity operating systems to Xens paravirtualized x86 environment. Note that our NetBSD port is at a very early stage, and hencewe report no figures here. The XP port is more advanced, but still inprogress it can execute a number of userspace applications froma RAM disk, but it currently lacks any virtual IO drivers. For thisreason, figures for XPs virtual device drivers are not presented.However, as with Linux, we expect these drivers to be small andsimple due to the idealized hardware abstraction presented by Xen.Windows XP required a surprising number of modifications toits architecture independent OS code because it uses a variety ofstructures and unions for accessing pagetable entries PTEs. Eachpagetable access had to be separately modified, although some ofXENHW SMP x86, phy mem, enet, SCSIIDEvirtual networkvirtual blockdevvirtual x86 CPUvirtual phy memControlPlaneSoftwareGuestOSXenoLinuxGuestOSXenoBSDGuestOSXenoXPUserSoftwareUserSoftwareUserSoftwareGuestOSXenoLinuxXenoAwareDevice DriversXenoAwareDevice DriversXenoAwareDevice DriversXenoAwareDevice DriversDomain0controlinterfaceFigure 1 The structure of a machine running the Xen hypervisor, hosting a number of different guest operating systems,including Domain0 running control software in a XenoLinuxenvironment.this process was automated with scripts. In contrast, Linux neededfar fewer modifications to its generic memory system as it uses preprocessor macros to access PTEs  the macro definitions providea convenient place to add the translation and hypervisor calls required by paravirtualization.In both OSes, the architecturespecific sections are effectivelya port of the x86 code to our paravirtualized architecture. Thisinvolved rewriting routines which used privileged instructions, andremoving a large amount of lowlevel system initialization code.Again, more changes were required in Windows XP, mainly dueto the presence of legacy 16bit emulation code and the need fora somewhat different bootloading mechanism. Note that the x86specific code base in XP is substantially larger than in Linux andhence a larger porting effort should be expected.2.3 Control and ManagementThroughout the design and implementation of Xen, a goal hasbeen to separate policy from mechanism wherever possible. Although the hypervisor must be involved in datapath aspects forexample, scheduling the CPU between domains, filtering networkpackets before transmission, or enforcing access control when reading data blocks, there is no need for it to be involved in, or evenaware of, higher level issues such as how the CPU is to be shared,or which kinds of packet each domain may transmit.The resulting architecture is one in which the hypervisor itselfprovides only basic control operations. These are exported throughan interface accessible from authorized domains potentially complex policy decisions, such as admission control, are best performedby management software running over a guest OS rather than inprivileged hypervisor code.The overall system structure is illustrated in Figure 1. Note thata domain is created at boot time which is permitted to use the control interface. This initial domain, termed Domain0, is responsiblefor hosting the applicationlevel management software. The control interface provides the ability to create and terminate other domains and to control their associated scheduling parameters, physical memory allocations and the access they are given to the machines physical disks and network devices.In addition to processor and memory resources, the control interface supports the creation and deletion of virtual network interfacesVIFs and block devices VBDs. These virtual IO devices haveassociated accesscontrol information which determines which domains can access them, and with what restrictions for example, areadonly VBD may be created, or a VIF may filter IP packets toprevent sourceaddress spoofing.This control interface, together with profiling statistics on thecurrent state of the system, is exported to a suite of applicationlevel management software running in Domain0. This complementof administrative tools allows convenient management of the entireserver current tools can create and destroy domains, set networkfilters and routing rules, monitor perdomain network activity atpacket and flow granularity, and create and delete virtual networkinterfaces and virtual block devices. We anticipate the developmentof higherlevel tools to further automate the application of administrative policy.3. DETAILED DESIGNIn this section we introduce the design of the major subsystemsthat make up a Xenbased server. In each case we present bothXen and guest OS functionality for clarity of exposition. The current discussion of guest OSes focuses on XenoLinux as this is themost mature nonetheless our ongoing porting of Windows XP andNetBSD gives us confidence that Xen is guest OS agnostic.3.1 Control Transfer Hypercalls and EventsTwo mechanisms exist for control interactions between Xen andan overlying domain synchronous calls from a domain to Xen maybe made using a hypercall, while notifications are delivered to domains from Xen using an asynchronous event mechanism.The hypercall interface allows domains to perform a synchronoussoftware trap into the hypervisor to perform a privileged operation,analogous to the use of system calls in conventional operating systems. An example use of a hypercall is to request a set of pagetable updates, in which Xen validates and applies a list of updates,returning control to the calling domain when this is completed.Communication from Xen to a domain is provided through anasynchronous event mechanism, which replaces the usual deliverymechanisms for device interrupts and allows lightweight notification of important events such as domaintermination requests. Akinto traditional Unix signals, there are only a small number of events,each acting to flag a particular type of occurrence. For instance,events are used to indicate that new data has been received over thenetwork, or that a virtual disk request has completed.Pending events are stored in a perdomain bitmask which is updated by Xen before invoking an eventcallback handler specifiedby the guest OS. The callback handler is responsible for resettingthe set of pending events, and responding to the notifications in anappropriate manner. A domain may explicitly defer event handlingby setting a Xenreadable software flag this is analogous to disabling interrupts on a real processor.3.2 Data Transfer IO RingsThe presence of a hypervisor means there is an additional protection domain between guest OSes and IO devices, so it is crucialthat a data transfer mechanism be provided that allows data to movevertically through the system with as little overhead as possible.Two main factors have shaped the design of our IOtransfermechanism resource management and event notification. For resource accountability, we attempt to minimize the work required todemultiplex data to a specific domain when an interrupt is receivedfrom a device  the overhead of managing buffers is carried outlater where computation may be accounted to the appropriate domain. Similarly, memory committed to device IO is provided bythe relevant domains wherever possible to prevent the crosstalk inherent in shared buffer pools IO buffers are protected during datatransfer by pinning the underlying page frames within Xen.Request ConsumerPrivate pointerin XenRequest ProducerShared pointerupdated by guest OSResponse ConsumerPrivate pointerin guest OSResponse ProducerShared pointerupdated byXenRequest queue  Descriptors queued by the VM but not yet accepted by XenOutstanding descriptors  Descriptor slots awaiting a response from XenResponse queue  Descriptors returned by Xen in response to serviced requestsUnused descriptorsFigure 2 The structure of asynchronous IO rings, which areused for data transfer between Xen and guest OSes.Figure 2 shows the structure of our IO descriptor rings. A ringis a circular queue of descriptors allocated by a domain but accessible from within Xen. Descriptors do not directly contain IO datainstead, IO data buffers are allocated outofband by the guest OSand indirectly referenced by IO descriptors. Access to each ringis based around two pairs of producerconsumer pointers domainsplace requests on a ring, advancing a request producer pointer, andXen removes these requests for handling, advancing an associatedrequest consumer pointer. Responses are placed back on the ringsimilarly, save with Xen as the producer and the guest OS as theconsumer. There is no requirement that requests be processed inorder the guest OS associates a unique identifier with each requestwhich is reproduced in the associated response. This allows Xen tounambiguously reorder IO operations due to scheduling or priorityconsiderations.This structure is sufficiently generic to support a number of different device paradigms. For example, a set of requests can provide buffers for network packet reception subsequent responsesthen signal the arrival of packets into these buffers. Reorderingis useful when dealing with disk requests as it allows them to bescheduled within Xen for efficiency, and the use of descriptors withoutofband buffers makes implementing zerocopy transfer easy.We decouple the production of requests or responses from thenotification of the other party in the case of requests, a domainmay enqueue multiple entries before invoking a hypercall to alertXen in the case of responses, a domain can defer delivery of anotification event by specifying a threshold number of responses.This allows each domain to tradeoff latency and throughput requirements, similarly to the flowaware interrupt dispatch in theArseNIC Gigabit Ethernet interface 34.3.3 Subsystem VirtualizationThe control and data transfer mechanisms described are used inour virtualization of the various subsystems. In the following, wediscuss how this virtualization is achieved for CPU, timers, memory, network and disk.3.3.1 CPU schedulingXen currently schedules domains according to the Borrowed Virtual Time BVT scheduling algorithm 11. We chose this particular algorithms since it is both workconserving and has a special mechanism for lowlatency wakeup or dispatch of a domainwhen it receives an event. Fast dispatch is particularly importantto minimize the effect of virtualization on OS subsystems that aredesigned to run in a timely fashion for example, TCP relies onthe timely delivery of acknowledgments to correctly estimate network roundtrip times. BVT provides lowlatency dispatch by using virtualtime warping, a mechanism which temporarily violatesideal fair sharing to favor recentlywoken domains. However,other scheduling algorithms could be trivially implemented overour generic scheduler abstraction. Perdomain scheduling parameters can be adjusted by management software running in Domain0.3.3.2 Time and timersXen provides guest OSes with notions of real time, virtual timeand wallclock time. Real time is expressed in nanoseconds passedsince machine boot and is maintained to the accuracy of the processors cycle counter and can be frequencylocked to an external timesource for example, via NTP. A domains virtual time only advances while it is executing this is typically used by the guest OSscheduler to ensure correct sharing of its timeslice between application processes. Finally, wallclock time is specified as an offsetto be added to the current real time. This allows the wallclock timeto be adjusted without affecting the forward progress of real time.Each guest OS can program a pair of alarm timers, one for realtime and the other for virtual time. Guest OSes are expected tomaintain internal timer queues and use the Xenprovided alarmtimers to trigger the earliest timeout. Timeouts are delivered using Xens event mechanism.3.3.3 Virtual address translationAs with other subsystems, Xen attempts to virtualize memoryaccess with as little overhead as possible. As discussed in Section 2.1.1, this goal is made somewhat more difficult by the x86architectures use of hardware page tables. The approach taken byVMware is to provide each guest OS with a virtual page table, notvisible to the memorymanagement unit MMU 10. The hypervisor is then responsible for trapping accesses to the virtual pagetable, validating updates, and propagating changes back and forthbetween it and the MMUvisible shadow page table. This greatlyincreases the cost of certain guest OS operations, such as creating new virtual address spaces, and requires explicit propagation ofhardware updates to accessed and dirty bits.Although full virtualization forces the use of shadow page tables,to give the illusion of contiguous physical memory, Xen is not soconstrained. Indeed, Xen need only be involved in page table updates, to prevent guest OSes from making unacceptable changes.Thus we avoid the overhead and additional complexity associatedwith the use of shadow page tables  the approach in Xen is toregister guest OS page tables directly with the MMU, and restrictguest OSes to readonly access. Page table updates are passed toXen via a hypercall to ensure safety, requests are validated beforebeing applied.To aid validation, we associate a type and reference count witheach machine page frame. A frame may have any one of the following mutuallyexclusive types at any point in time page directory PD, page table PT, local descriptor table LDT, global descriptor table GDT, or writable RW. Note that a guest OS mayalways create readable mappings to its own page frames, regardlessof their current types. A frame may only safely be retasked whenits reference count is zero. This mechanism is used to maintain theinvariants required for safety for example, a domain cannot have awritable mapping to any part of a page table as this would requirethe frame concerned to simultaneously be of types PT and RW.The type system is also used to track which frames have alreadybeen validated for use in page tables. To this end, guest OSes indicate when a frame is allocated for pagetable use  this requires aoneoff validation of every entry in the frame by Xen, after whichits type is pinned to PD or PT as appropriate, until a subsequentunpin request from the guest OS. This is particularly useful whenchanging the page table base pointer, as it obviates the need to validate the new page table on every context switch. Note that a framecannot be retasked until it is both unpinned and its reference counthas reduced to zero  this prevents guest OSes from using unpinrequests to circumvent the referencecounting mechanism.To minimize the number of hypercalls required, guest OSes canlocally queue updates before applying an entire batch with a singlehypercall  this is particularly beneficial when creating new address spaces. However we must ensure that updates are committedearly enough to guarantee correctness. Fortunately, a guest OS willtypically execute a TLB flush before the first use of a new mappingthis ensures that any cached translation is invalidated. Hence, committing pending updates immediately before a TLB flush usuallysuffices for correctness. However, some guest OSes elide the flushwhen it is certain that no stale entry exists in the TLB. In this caseit is possible that the first attempted use of the new mapping willcause a pagenotpresent fault. Hence the guest OS fault handlermust check for outstanding updates if any are found then they areflushed and the faulting instruction is retried.3.3.4 Physical memoryThe initial memory allocation, or reservation, for each domain isspecified at the time of its creation memory is thus statically partitioned between domains, providing strong isolation. A maximumallowable reservation may also be specified if memory pressurewithin a domain increases, it may then attempt to claim additionalmemory pages from Xen, up to this reservation limit. Conversely,if a domain wishes to save resources, perhaps to avoid incurring unnecessary costs, it can reduce its memory reservation by releasingmemory pages back to Xen.XenoLinux implements a balloon driver 42, which adjusts adomains memory usage by passing memory pages back and forthbetween Xen and XenoLinuxs page allocator. Although we couldmodify Linuxs memorymanagement routines directly, the balloondriver makes adjustments by using existing OS functions, thus simplifying the Linux porting effort. However, paravirtualization canbe used to extend the capabilities of the balloon driver for example, the outofmemory handling mechanism in the guest OS can bemodified to automatically alleviate memory pressure by requestingmore memory from Xen.Most operating systems assume that memory comprises at mosta few large contiguous extents. Because Xen does not guarantee toallocate contiguous regions of memory, guest OSes will typicallycreate for themselves the illusion of contiguous physical memory,even though their underlying allocation of hardware memory issparse. Mapping from physical to hardware addresses is entirelythe responsibility of the guest OS, which can simply maintain anarray indexed by physical page frame number. Xen supports efficient hardwaretophysical mapping by providing a shared translation array that is directly readable by all domains  updates to thisarray are validated by Xen to ensure that the OS concerned ownsthe relevant hardware page frames.Note that even if a guest OS chooses to ignore hardware addresses in most cases, it must use the translation tables when accessing its page tables which necessarily use hardware addresses.Hardware addresses may also be exposed to limited parts of theOSs memorymanagement system to optimize memory access. Forexample, a guest OS might allocate particular hardware pages soas to optimize placement within a physically indexed cache 24,or map naturally aligned contiguous portions of hardware memoryusing superpages 30.3.3.5 NetworkXen provides the abstraction of a virtual firewallrouter VFR,where each domain has one or more network interfaces VIFs logically attached to the VFR. A VIF looks somewhat like a modernnetwork interface card there are two IO rings of buffer descriptors, one for transmit and one for receive. Each direction also hasa list of associated rules of the form pattern, action  ifthe pattern matches then the associated action is applied.Domain0 is responsible for inserting and removing rules. In typical cases, rules will be installed to prevent IP source address spoofing, and to ensure correct demultiplexing based on destination IPaddress and port. Rules may also be associated with hardware interfaces on the VFR. In particular, we may install rules to performtraditional firewalling functions such as preventing incoming connection attempts on insecure ports.To transmit a packet, the guest OS simply enqueues a bufferdescriptor onto the transmit ring. Xen copies the descriptor and,to ensure safety, then copies the packet header and executes anymatching filter rules. The packet payload is not copied since we usescattergather DMA however note that the relevant page framesmust be pinned until transmission is complete. To ensure fairness,Xen implements a simple roundrobin packet scheduler.To efficiently implement packet reception, we require the guestOS to exchange an unused page frame for each packet it receivesthis avoids the need to copy the packet between Xen and the guestOS, although it requires that pagealigned receive buffers be queuedat the network interface. When a packet is received, Xen immediately checks the set of receive rules to determine the destinationVIF, and exchanges the packet buffer for a page frame on the relevant receive ring. If no frame is available, the packet is dropped.3.3.6 DiskOnly Domain0 has direct unchecked access to physical IDE andSCSI disks. All other domains access persistent storage throughthe abstraction of virtual block devices VBDs, which are createdand configured by management software running within Domain0.Allowing Domain0 to manage the VBDs keeps the mechanismswithin Xen very simple and avoids more intricate solutions such asthe UDFs used by the Exokernel 23.A VBD comprises a list of extents with associated ownershipand access control information, and is accessed via the IO ringmechanism. A typical guest OS disk scheduling algorithm will reorder requests prior to enqueuing them on the ring in an attempt toreduce response time, and to apply differentiated service for example, it may choose to aggressively schedule synchronous metadatarequests at the expense of speculative readahead requests. However, because Xen has more complete knowledge of the actual disklayout, we also support reordering within Xen, and so responsesmay be returned out of order. A VBD thus appears to the guest OSsomewhat like a SCSI disk.A translation table is maintained within the hypervisor for eachVBD the entries within this table are installed and managed byDomain0 via a privileged control interface. On receiving a diskrequest, Xen inspects the VBD identifier and offset and producesthe corresponding sector address and physical device. Permissionchecks also take place at this time. Zerocopy data transfer takesplace using DMA between the disk and pinned memory pages inthe requesting domain.Xen services batches of requests from competing domains in asimple roundrobin fashion these are then passed to a standard elevator scheduler before reaching the disk hardware. Domains mayexplicitly pass down reorder barriers to prevent reordering whenthis is necessary to maintain higher level semantics e.g. when using a writeahead log. The lowlevel scheduling gives us goodthroughput, while the batching of requests provides reasonably fairaccess. Future work will investigate providing more predictableisolation and differentiated service, perhaps using existing techniques and schedulers 39.3.4 Building a New DomainThe task of building the initial guest OS structures for a newdomain is mostly delegated to Domain0 which uses its privilegedcontrol interfaces Section 2.3 to access the new domains memoryand inform Xen of initial register state. This approach has a number of advantages compared with building a domain entirely withinXen, including reduced hypervisor complexity and improved robustness accesses to the privileged interface are sanity checkedwhich allowed us to catch many bugs during initial development.Most important, however, is the ease with which the buildingprocess can be extended and specialized to cope with new guestOSes. For example, the boottime address space assumed by theLinux kernel is considerably simpler than that expected by Windows XP. It would be possible to specify a fixed initial memorylayout for all guest OSes, but this would require additional bootstrap code within every guest OS to lay things out as required bythe rest of the OS. Unfortunately this type of code is tricky to implement correctly for simplicity and robustness it is therefore betterto implement it within Domain0 which can provide much richerdiagnostics and debugging support than a bootstrap environment.4. EVALUATIONIn this section we present a thorough performance evaluationof Xen. We begin by benchmarking Xen against a number of alternative virtualization techniques, then compare the total systemthroughput executing multiple applications concurrently on a single native operating system against running each application in itsown virtual machine. We then evaluate the performance isolationXen provides between guest OSes, and assess the total overhead ofrunning large numbers of operating systems on the same hardware.For these measurements, we have used our XenoLinux port basedon Linux 2.4.21 as this is our most mature guest OS. We expectthe relative overheads for our Windows XP and NetBSD ports tobe similar but have yet to conduct a full evaluation.There are a number of preexisting solutions for running multiple copies of Linux on the same machine. VMware offers severalcommercial products that provide virtual x86 machines on whichunmodified copies of Linux may be booted. The most commonlyused version is VMware Workstation, which consists of a set ofprivileged kernel extensions to a host operating system. BothWindows and Linux hosts are supported. VMware also offer anenhanced product called ESX Server which replaces the host OSwith a dedicated kernel. By doing so, it gains some performancebenefit over the workstation product. ESX Server also supports aparavirtualized interface to the network that can be accessed by installing a special device driver vmxnet into the guest OS, wheredeployment circumstances permit.We have subjected ESX Server to the benchmark suites describedbelow, but sadly are prevented from reporting quantitative resultsdue to the terms of the products End User License Agreement. Weare authorized simply to say that Xen significantly outperformsESX Server on all benchmarks in the suite. We present resultsfor VMware Workstation 3.2, the most recent VMware product released without the benchmarking restriction, running over a Linuxhost OS.We also present results for Usermode Linux UML, an increasingly popular platform for virtual hosting. UML is a port of Linuxto run as a userspace process on a Linux host. Like XenoLinux, thechanges required are restricted to the architecture dependent codebase. However, the UML code bears little similarity to the nativex86 port due to the very different nature of the execution environments. Although UML can run on an unmodified Linux host, wepresent results for the Single Kernel Address Space skas3 variant that exploits patches to the host OS to improve performance.We also investigated three other virtualization techniques for running ported versions of Linux on the same x86 machine. Connectixs Virtual PC and forthcoming Virtual Server products now acquired by Microsoft are similar in design to VMwares, providingfull x86 virtualization. Since all versions of Virtual PC have benchmarking restrictions in their license agreements we did not subjectthem to closer analysis. UMLinux is similar in concept to UMLbut is a different code base and has yet to achieve the same level ofperformance, so we omit the results. Work to improve the performance of UMLinux through host OS modifications is ongoing 25.Although Plex86 was originally a general purpose x86 VMM, it hasnow been retargeted to support just Linux guest OSes. The guestOS must be specially compiled to run on Plex86, but the sourcechanges from native x86 are trivial. The performance of Plex86 iscurrently well below the other techniques.All the experiments were performed on a Dell 2650 dual processor 2.4GHz Xeon server with 2GB RAM, a Broadcom Tigon 3 Gigabit Ethernet NIC, and a single Hitachi DK32EJ 146GB 10k RPMSCSI disk. Linux version 2.4.21 was used throughout, compiledfor architecture i686 for the native and VMware guest OS experiments, for xenoi686 when running on Xen, and architecture umwhen running on UML. The Xeon processors in the machine support SMT hyperthreading, but this was disabled because noneof the kernels currently have SMTaware schedulers. We ensuredthat the total amount of memory available to all guest OSes plustheir VMM was equal to the total amount available to native Linux.The RedHat 7.2 distribution was used throughout, installed onext3 file systems. The VMs were configured to use the same diskpartitions in persistent raw mode, which yielded the best performance. Using the same file system image also eliminated potentialdifferences in disk seek times and transfer rates.4.1 Relative PerformanceWe have performed a battery of experiments in order to evaluatethe overhead of the various virtualization techniques relative to running on the bare metal. Complex applicationlevel benchmarksthat exercise the whole system have been employed to characterizeperformance under a range of servertype workloads. Since neither Xen nor any of the VMware products currently support multiprocessor guest OSes although they are themselves both SMPcapable, the test machine was configured with one CPU for theseexperiments we examine performance with concurrent guest OSeslater. The results presented are the median of seven trials.The first cluster of bars in Figure 3 represents a relatively easyscenario for the VMMs. The SPEC CPU suite contains a seriesof longrunning computationallyintensive applications intended tomeasure the performance of a systems processor, memory system,and compiler quality. The suite performs little IO and has littleinteraction with the OS. With almost all CPU time spent executingin userspace code, all three VMMs exhibit low overhead.The next set of bars show the total elapsed time taken to builda default configuration of the Linux 2.4.21 kernel on a local ext3file system with gcc 2.96. Native Linux spends about 7 of theCPU time in the OS, mainly performing file IO, scheduling andmemory management. In the case of the VMMs, this system timeis expanded to a greater or lesser degree whereas Xen incurs aL567X567V554U550SPEC INT2000 scoreL263X271V334U535Linux build time sL172X158V80U65OSDBIR tupsL1714X1633V199U306OSDBOLTP tupsL418X400V310U111dbench scoreL518X514V150U172SPEC WEB99 score0.00.10.20.30.40.50.60.70.80.91.01.1Relative score to LinuxFigure 3 Relative performance of native Linux L, XenoLinux X, VMware workstation 3.2 V and UserMode Linux U.mere 3 overhead, the other VMMs experience a more significantslowdown.Two experiments were performed using the PostgreSQL 7.1.3database, exercised by the Open Source Database Benchmark suiteOSDB in its default configuration. We present results for themultiuser Information Retrieval IR and OnLine Transaction Processing OLTP workloads, both measured in tuples per second. Asmall modification to the suites test harness was required to produce correct results, due to a UML bug which loses virtualtimerinterrupts under high load. The benchmark drives the databasevia PostgreSQLs native API callable SQL over a Unix domainsocket. PostgreSQL places considerable load on the operating system, and this is reflected in the substantial virtualization overheadsexperienced by VMware and UML. In particular, the OLTP benchmark requires many synchronous disk operations, resulting in manyprotection domain transitions.The dbench program is a file system benchmark derived fromthe industrystandard NetBench. It emulates the load placed on afile server by Windows 95 clients. Here, we examine the throughput experienced by a single client performing around 90,000 filesystem operations.SPEC WEB99 is a complex applicationlevel benchmark for evaluating web servers and the systems that host them. The workload isa complex mix of page requests 30 require dynamic content generation, 16 are HTTP POST operations and 0.5 execute a CGIscript. As the server runs it generates access and POST logs, sothe disk workload is not solely readonly. Measurements thereforereflect general OS performance, including file system and network,in addition to the web server itself.A number of client machines are used to generate load for theserver under test, with each machine simulating a collection ofusers concurrently accessing the web site. The benchmark is runrepeatedly with different numbers of simulated users to determinethe maximum number that can be supported. SPEC WEB99 definesa minimum Quality of Service that simulated users must receive inorder to be conformant and hence count toward the score usersmust receive an aggregate bandwidth in excess of 320Kbs over aseries of requests. A warmup phase is allowed in which the number of simultaneous clients is slowly increased, allowing servers topreload their buffer caches.For our experimental setup we used the Apache HTTP serverversion 1.3.27, installing the modspecweb99 plugin to performmost but not all of the dynamic content generation  SPEC rulesrequire 0.5 of requests to use full CGI, forking a separate process. Better absolute performance numbers can be achieved withthe assistance of TUX, the Linux inkernel static content webserver, but we chose not to use this as we felt it was less likely to berepresentative of our realworld target applications. Furthermore,although Xens performance improves when using TUX, VMwaresuffers badly due to the increased proportion of time spent emulating ring 0 while executing the guest OS kernel.SPEC WEB99 exercises the whole system. During the measurement period there is up to 180Mbs of TCP network traffic andconsiderable disk readwrite activity on a 2GB dataset. The benchmark is CPUbound, and a significant proportion of the time isspent within the guest OS kernel, performing network stack processing, file system operations, and scheduling between the manyhttpd processes that Apache needs to handle the offered load.XenoLinux fares well, achieving within 1 of native Linux performance. VMware and UML both struggle, supporting less than athird of the number of clients of the native Linux system.4.2 Operating System BenchmarksTo more precisely measure the areas of overhead within Xen andthe other VMMs, we performed a number of smaller experimentstargeting particular subsystems. We examined the overhead of virtualization as measured by McVoys lmbench program 29. Weused version 3.0a3 as this addresses many of the issues regarding the fidelity of the tool raised by Seltzers hbench 6. The OSperformance subset of the lmbench suite consist of 37 microbenchmarks. In the native Linux case, we present figures for both uniprocessor LUP and SMP LSMP kernels as we were somewhatsurprised by the performance overhead incurred by the extra locking in the SMP system in many cases.In 24 of the 37 microbenchmarks, XenoLinux performs similarly to native Linux, tracking the uniprocessor Linux kernel perConfignullcallnullIO statopencloseslctTCPsiginstsighndlforkprocexecprocshprocLSMP 0.53 0.81 2.10 3.51 23.2 0.83 2.94 143 601 4k2LUP 0.45 0.50 1.28 1.92 5.70 0.68 2.49 110 530 4k0Xen 0.46 0.50 1.22 1.88 5.69 0.69 1.75 198 768 4k8VMW 0.73 0.83 1.88 2.99 11.1 1.02 4.63 874 2k3 10kUML 24.7 25.1 36.1 62.8 39.9 26.0 46.0 21k 33k 58kTable 3 lmbench Processes  times in sConfig2p0K2p16K2p64K8p16K8p64K16p16K16p64KLSMP 1.69 1.88 2.03 2.36 26.8 4.79 38.4LUP 0.77 0.91 1.06 1.03 24.3 3.61 37.6Xen 1.97 2.22 2.67 3.07 28.7 7.08 39.4VMW 18.1 17.6 21.3 22.4 51.6 41.7 72.2UML 15.5 14.6 14.4 16.3 36.8 23.6 52.0Table 4 lmbench Context switching times in sConfig 0K File 10K File Mmap Prot Pagecreate delete create delete lat fault faultLSMP 44.9 24.2 123 45.2 99.0 1.33 1.88LUP 32.1 6.08 66.0 12.5 68.0 1.06 1.42Xen 32.5 5.86 68.2 13.6 139 1.40 2.73VMW 35.3 9.3 85.6 21.4 620 7.53 12.4UML 130 65.7 250 113 1k4 21.8 26.3Table 5 lmbench File  VM system latencies in sformance closely and outperforming the SMP kernel. In Tables 3to 5 we show results which exhibit interesting performance variations among the test systems particularly large penalties for Xenare shown in bold face.In the process microbenchmarks Table 3, Xen exhibits slowerfork, exec and sh performance than native Linux. This is expected,since these operations require large numbers of page table updateswhich must all be verified by Xen. However, the paravirtualizationapproach allows XenoLinux to batch update requests. Creating newpage tables presents an ideal case because there is no reason tocommit pending updates sooner, XenoLinux can amortize each hypercall across 2048 updates the maximum size of its batch buffer.Hence each update hypercall constructs 8MB of address space.Table 4 shows context switch times between different numbersof processes with different working set sizes. Xen incurs an extra overhead between 1s and 3s, as it executes a hypercall tochange the page table base. However, context switch results forlarger working set sizes perhaps more representative of real applications show that the overhead is small compared with cache effects. Unusually, VMware Workstation is inferior to UML on thesemicrobenchmarks however, this is one area where enhancementsin ESX Server are able to reduce the overhead.The mmap latency and page fault latency results shown in Table 5 are interesting since they require two transitions into Xen perpage one to take the hardware fault and pass the details to the guestOS, and a second to install the updated page table entry on the guestOSs behalf. Despite this, the overhead is relatively modest.One small anomaly in Table 3 is that XenoLinux has lower signalhandling latency than native Linux. This benchmark does not require any calls into Xen at all, and the 0.75s 30 speedup is presumably due to a fortuitous cache alignment in XenoLinux, henceunderlining the dangers of taking microbenchmarks too seriously.TCP MTU 1500 TCP MTU 500TX RX TX RXLinux 897 897 602 544Xen 897 0 897 0 516 14 467 14VMW 291 68 615 31 101 83 137 75UML 165 82 203 77 61.190 91.483Table 6 ttcp Bandwidth in Mbs4.2.1 Network performanceIn order to evaluate the overhead of virtualizing the network, weexamine TCP performance over a Gigabit Ethernet LAN. In all experiments we use a similarlyconfigured SMP box running nativeLinux as one of the endpoints. This enables us to measure receiveand transmit performance independently. The ttcp benchmark wasused to perform these measurements. Both sender and receiver applications were configured with a socket buffer size of 128kB, aswe found this gave best performance for all tested systems. The results presented are a median of 9 experiments transferring 400MB.Table 6 presents two sets of results, one using the default Ethernet MTU of 1500 bytes, the other using a 500byte MTU chosenas it is commonly used by dialup PPP clients. The results demonstrate that the pageflipping technique employed by the XenoLinuxvirtual network driver avoids the overhead of data copying andhence achieves a very low perbyte overhead. With an MTU of 500bytes, the perpacket overheads dominate. The extra complexity oftransmit firewalling and receive demultiplexing adversely impactthe throughput, but only by 14.VMware emulate a pcnet32 network card for communicatingwith the guest OS which provides a relatively clean DMAbasedinterface. ESX Server also supports a special vmxnet driver forcompatible guest OSes, which provides significant networking performance improvements.4.3 Concurrent Virtual MachinesIn this section, we compare the performance of running multiple applications in their own guest OS against running them onthe same native operating system. Our focus is on the results using Xen, but we comment on the performance of the other VMMswhere applicable.Figure 4 shows the results of running 1, 2, 4, 8 and 16 copiesof the SPEC WEB99 benchmark in parallel on a two CPU machine. The native Linux was configured for SMP on it we ranmultiple copies of Apache as concurrent processes. In Xens case,each instance of SPEC WEB99 was run in its own uniprocessorLinux guest OS along with an sshd and other management processes. Different TCP port numbers were used for each web serverto enable the copies to be run in parallel. Note that the size of theSPEC data set required for c simultaneous connections is 25 c 0.66 4.88 MBytes or approximately 3.3GB for 1000 connections. This is sufficiently large to thoroughly exercise the diskand buffer cache subsystems.Achieving good SPEC WEB99 scores requires both high throughput and bounded latency for example, if a client request gets stalleddue to a badly delayed disk read, then the connection will be classedas non conforming and wont contribute to the score. Hence, it isimportant that the VMM schedules domains in a timely fashion. Bydefault, Xen uses a 5ms time slice.In the case of a single Apache instance, the addition of a second CPU enables native Linux to improve on the score reportedin section 4.1 by 28, to 662 conformant clients. However, theL662X16.3 nonSMP guest11001L924X2887L896X4842L906X8880L874X1602004006008001000Aggregate number of conforming clientsSimultaneous SPEC WEB99 Instances on Linux L and XenXFigure 4 SPEC WEB99 for 1, 2, 4, 8 and 16 concurrent Apacheservers higher values are better.best aggregate throughput is achieved when running two Apacheinstances, suggesting that Apache 1.3.27 may have some SMP scalability issues.When running a single domain, Xen is hampered by a lack ofsupport for SMP guest OSes. However, Xens interrupt load balancer identifies the idle CPU and diverts all interrupt processingto it, improving on the single CPU score by 9. As the numberof domains increases, Xens performance improves to within a fewpercent of the native case.Next we performed a series of experiments running multiple instances of PostgreSQL exercised by the OSDB suite. Running multiple PostgreSQL instances on a single Linux OS proved difficult,as it is typical to run a single PostgreSQL instance supporting multiple databases. However, this would prevent different users having separate database configurations. We resorted to a combinationof chroot and software patches to avoid SysV IPC namespaceclashes between different PostgreSQL instances. In contrast, Xenallows each instance to be started in its own domain allowing easyconfiguration.In Figure 5 we show the aggregate throughput Xen achieveswhen running 1, 2, 4 and 8 instances of OSDBIR and OSDBOLTP. When a second domain is added, full utilization of the second CPU almost doubles the total throughput. Increasing the number of domains further causes some reduction in aggregate throughput which can be attributed to increased context switching and diskhead movement. Aggregate scores running multiple PostgreSQLinstances on a single Linux OS are 2535 lower than the equivalent scores using Xen. The cause is not fully understood, but itappears that PostgreSQL has SMP scalability issues combined withpoor utilization of Linuxs block cache.Figure 5 also demonstrates performance differentiation between8 domains. Xens schedulers were configured to give each domainan integer weight between 1 and 8. The resulting throughput scoresfor each domain are reflected in the different banding on the bar.In the IR benchmark, the weighting has a precise influence overthroughput and each segment is within 4 of its expected size.However, in the OLTP case, domains given a larger share of resources to not achieve proportionally higher scores The high level1 2 4 8 8diffOSDBIR1 2 4 8 8diffOSDBOLTP158318289282 290166132892833268521040.00.51.01.52.0Aggregate score relative to single instanceSimultaneous OSDBIR and OSDBOLTP Instances on XenFigure 5 Performance of multiple instances of PostgreSQLrunning OSDB in separate Xen domains. 8diff bars show performance variation with different scheduler weights.of synchronous disk activity highlights a weakness in our currentdisk scheduling algorithm causing them to underperform.4.4 Performance IsolationIn order to demonstrate the performance isolation provided byXen, we hoped to perform a bakeoff between Xen and other OSbased implementations of performance isolation techniques suchas resource containers. However, at the current time there appearto be no implementations based on Linux 2.4 available for download. QLinux 2.4 has yet to be released and is targeted at providingQoS for multimedia applications rather than providing full defensive isolation in a server environment. Ensims Linuxbased PrivateVirtual Server product appears to be the most complete implementation, reportedly encompassing control of CPU, disk, network andphysical memory resources 14. We are in discussions with Ensimand hope to be able to report results of a comparative evaluation ata later date.In the absence of a sidebyside comparison, we present resultsshowing that Xens performance isolation works as expected, evenin the presence of a malicious workload. We ran 4 domains configured with equal resource allocations, with two domains runningpreviouslymeasured workloads PostgreSQLOSDBIR and SPECWEB99, and two domains each running a pair of extremely antisocial processes. The third domain concurrently ran a disk bandwidthhog sustained dd together with a file system intensive workloadtargeting huge numbers of small file creations within large directories. The fourth domain ran a fork bomb at the same time asa virtual memory intensive application which attempted to allocateand touch 3GB of virtual memory and, on failure, freed every pageand then restarted.We found that both the OSDBIR and SPEC WEB99 results wereonly marginally affected by the behaviour of the two domains running disruptive processes  respectively achieving 4 and 2below the results reported earlier. We attribute this to the overhead of extra context switches and cache effects. We regard this assomewhat fortuitous in light of our current relatively simple diskscheduling algorithm, but under this scenario it appeared to pro1.01.21.41.61.82.0 0  10  20  30  40  50  60  70  80  90  100  110  120  130Normalised ThroughputConcurrent ProcessesDomainsLinuxXenoLinux 50ms time sliceXenoLinux 5ms time sliceFigure 6 Normalized aggregate performance of a subset ofSPEC CINT2000 running concurrently on 1128 domainsvide sufficient isolation from the pageswapping and diskintensiveactivities of the other domains for the benchmarks to make goodprogress. VMware Workstation achieves similar levels of isolation,but at reduced levels of absolute performance.We repeated the same experiment under native Linux. Unsurprisingly, the disruptive processes rendered the machine completelyunusable for the two benchmark processes, causing almost all theCPU time to be spent in the OS.4.5 ScalabilityIn this section, we examine Xens ability to scale to its targetof 100 domains. We discuss the memory requirements of runningmany instances of a guest OS and associated applications, and measure the CPU performance overhead of their execution.We evaluated the minimum physical memory requirements ofa domain booted with XenoLinux and running the default set ofRH7.2 daemons, along with an sshd and Apache web server. Thedomain was given a reservation of 64MB on boot, limiting the maximum size to which it could grow. The guest OS was instructed tominimize its memory footprint by returning all pages possible toXen. Without any swap space configured, the domain was able toreduce its memory footprint to 6.2MB allowing the use of a swapdevice reduced this further to 4.2MB. A quiescent domain is able tostay in this reduced state until an incoming HTTP request or periodic service causes more memory to be required. In this event, theguest OS will request pages back from Xen, growing its footprintas required up to its configured ceiling.This demonstrates that memory usage overhead is unlikely tobe a problem for running 100 domains on a modern server classmachine  far more memory will typically be committed to application data and buffer cache usage than to OS or application textpages. Xen itself maintains only a fixed 20kB of state per domain,unlike other VMMs that must maintain shadow page tables etc.Finally, we examine the overhead of context switching betweenlarge numbers of domains rather than simply between processes.Figure 6 shows the normalized aggregate throughput obtained whenrunning a small subset of the SPEC CINT2000 suite concurrentlyon between 1 and 128 domains or processes on our dual CPU server.The line representing native Linux is almost flat, indicating thatfor this benchmark there is no loss of aggregate performance whenscheduling between so many processes Linux identifies them all ascompute bound, and schedules them with long time slices of 50msor more. In contrast, the lower line indicates Xens throughputwhen configured with its default 5ms maximum scheduling slice.Although operating 128 simultaneously compute bound processeson a single server is unlikely to be commonplace in our target application area, Xen copes relatively well running 128 domains welose just 7.5 of total throughput relative to Linux.Under this extreme load, we measured usertouser UDP latencyto one of the domains running the SPEC CINT2000 subset. Wemeasured mean response times of 147ms standard deviation 97ms.Repeating the experiment against a 129th domain that was otherwise idle, we recorded a mean response time of 5.4ms s.d. 16ms.These figures are very encouraging  despite the substantial background load, interactive domains remain responsive.To determine the cause of the 7.5 performance reduction, weset Xens scheduling slice to 50ms the default value used byESX Server. The result was a throughput curve that tracked nativeLinuxs closely, almost eliminating the performance gap. However, as might be expected, interactive performance at high load isadversely impacted by these settings.5. RELATED WORKVirtualization has been applied to operating systems both commercially and in research for nearly thirty years. IBM VM370 19,38 first made use of virtualization to allow binary support for legacycode. VMware 10 and Connectix 8 both virtualize commodityPC hardware, allowing multiple operating systems to run on a single host. All of these examples implement a full virtualization ofat least a subset of the underlying hardware, rather than paravirtualizing and presenting a modified interface to the guest OS. Asshown in our evaluation, the decision to present a full virtualization, although able to more easily support offtheshelf operatingsystems, has detremental consequences for performance.The virtual machine monitor approach has also been used byDisco to allow commodity operating systems to run efficiently onccNUMA machines 7, 18. A small number of changes had to bemade to the hosted operating systems to enable virtualized execution on the MIPS architecture. In addition, certain other changeswere made for performance reasons.At present, we are aware of two other systems which take theparavirtualization approach IBM presently supports a paravirtualized version of Linux for their zSeries mainframes, allowing largenumbers of Linux instances to run simultaneously. Denali 44,discussed previously, is a contemporary isolation kernel which attempts to provide a system capable of hosting vast numbers of virtualized OS instances.In addition to Denali, we are aware of two other efforts to uselowlevel virtualization to build an infrastructure for distributedsystems. The vMatrix 1 project is based on VMware and aimsto build a platform for moving code between different machines.As vMatrix is developed above VMware, they are more concernedwith higherlevel issues of distribution that those of virtualizationitself. In addition, IBM provides a Managed Hosting service, inwhich virtual Linux instances may be rented on IBM mainframes.The PlanetLab 33 project has constructed a distributed infrastructure which is intended to serve as a testbed for the research anddevelopment of geographically distributed network services. Theplatform is targeted at researchers and attempts to divide individualphysical hosts into slivers, providing simultaneous lowlevel accessto users. The current deployment uses VServers 17 and SILK 4to manage sharing within the operating system.We share some motivation with the operating system extensibility and active networks communities. However, when runningover Xen there is no need to check for safe code, or for guaranteed termination  the only person hurt in either case is the clientin question. Consequently, Xen provides a more general solutionthere is no need for hosted code to be digitally signed by a trustedcompiler as in SPIN 5, to be accompanied by a safety proof aswith PCC 31, to be written in a particular language as in SafetyNet 22 or any Javabased system, or to rely on a particular middleware as with mobileagent systems. These other techniquescan, of course, continue to be used within guest OSes running overXen. This may be particularly useful for workloads with more transient tasks which would not provide an opportunity to amortize thecost of starting a new domain.A similar argument can be made with regard to languagelevelvirtual machine approaches while a resourcemanaged JVM 9should certainly be able to host untrusted applications, these applications must necessarily be compiled to Java bytecode and followthat particular systems security model. Meanwhile, Xen can readily support languagelevel virtual machines as applications runningover a guest OS.6. DISCUSSION AND CONCLUSIONWe have presented the Xen hypervisor which partitions the resources of a computer between domains running guest operatingsystems. Our paravirtualizing design places a particular emphasison performance and resource management. We have also describedand evaluated XenoLinux, a fullyfeatured port of a Linux 2.4 kernel that runs over Xen.6.1 Future WorkWe believe that Xen and XenoLinux are sufficiently complete tobe useful to a wider audience, and so intend to make a public releaseof our software in the very near future. A beta version is alreadyunder evaluation by selected parties once this phase is complete, ageneral 1.0 release will be announced on our project page3.After the initial release we plan a number of extensions and improvements to Xen. To increase the efficiency of virtual block devices, we intend to implement a shared universal buffer cache indexed on block contents. This will add controlled data sharing toour design without sacrificing isolation. Adding copyonwrite semantics to virtual block devices will allow them to be safely sharedamong domains, while still allowing divergent file systems.To provide better physical memory performance, we plan to implement a lastchance page cache LPC  effectively a systemwide list of free pages, of nonzero length only when machinememory is undersubscribed. The LPC is used when the guest OSvirtual memory system chooses to evict a clean page rather thandiscarding this completely, it may be added to the tail of the freelist. A fault occurring for that page before it has been reallocatedby Xen can therefore satisfied without a disk access.An important role for Xen is as the basis of the XenoServerproject which looks beyond individual machines and is buildingthe control systems necessary to support an Internetscale computing infrastructure. Key to our design is the idea that resource usagebe accounted precisely and paid for by the sponsor of that job if payments are made in real cash, we can use a congestion pricingstrategy 28 to handle excess demand, and use excess revenues topay for additional machines. This necessitates accurate and timelyIO scheduling with greater resilience to hostile workloads. We alsoplan to incorporate accounting into our block storage architectureby creating leases for virtual block devices.In order to provide better support for the management and administration of XenoServers, we are incorporating more thoroughsupport for auditing and forensic logging. We are also developingadditional VFR rules which we hope will allow us to detect andprevent a wide range of antisocial network behaviour. Finally, we3httpwww.cl.cam.ac.uknetosxenare continuing our work on XenoXP, focusing in the first instanceon writing network and block device drivers, with the aim of fullysupporting enterprise servers such as IIS.6.2 ConclusionXen provides an excellent platform for deploying a wide variety of networkcentric services, such as local mirroring of dynamicweb content, media stream transcoding and distribution, multiplayergame and virtual reality servers, and smart proxies 2 to provide aless ephemeral network presence for transientlyconnected devices.Xen directly addresses the single largest barrier to the deployment of such services the present inability to host transient serversfor short periods of time and with low instantiation costs. By allowing 100 operating systems to run on a single server, we reduce theassociated costs by two orders of magnitude. Furthermore, by turning the setup and configuration of each OS into a software concern,we facilitate much smallergranularity timescales of hosting.As our experimental results show in Section 4, the performanceof XenoLinux over Xen is practically equivalent to the performanceof the baseline Linux system. This fact, which comes from the careful design of the interface between the two components, means thatthere is no appreciable cost in having the resource management facilities available. Our ongoing work to port the BSD and WindowsXP kernels to operate over Xen is confirming the generality of theinterface that Xen exposes.AcknowledgmentsThis work is supported by ESPRC Grant GRS0189401 and byMicrosoft. We would like to thank Evangelos Kotsovinos, AnilMadhavapeddy, Russ Ross and James Scott for their contributions.7. REFERENCES1 A. Awadallah and M. Rosenblum. The vMatrix A network of virtualmachine monitors for dynamic content distribution. In Proceedingsof the 7th International Workshop on Web Content Caching andDistribution WCW 2002, Aug. 2002.2 A. Bakre and B. R. Badrinath. ITCP indirect TCP for mobile hosts.In Proceedings of the 15th International Conference on DistributedComputing Systems ICDCS 1995, pages 136143, June 1995.3 G. Banga, P. Druschel, and J. C. Mogul. Resource containers A newfacility for resource management in server systems. In Proceedingsof the 3rd Symposium on Operating Systems Design andImplementation OSDI 1999, pages 4558, Feb. 1999.4 A. Bavier, T. Voigt, M. Wawrzoniak, L. Peterson, andP. Gunningberg. SILK Scout paths in the Linux kernel. TechnicalReport 2002009, Uppsala University, Department of InformationTechnology, Feb. 2002.5 B. N. Bershad, S. Savage, P. Pardyak, E. G. Sirer, M. Fiuczynski,D. Becker, S. Eggers, and C. Chambers. Extensibility, safety andperformance in the SPIN operating system. In Proceedings of the15th ACM SIGOPS Symposium on Operating Systems Principles,volume 295 of ACM Operating Systems Review, pages 267284,Dec. 1995.6 A. Brown and M. Seltzer. Operating System Benchmarking in theWake of Lmbench A Case Study of the Performance of NetBSD onthe Intel x86 Architecture. In Proceedings of the 1997 ACMSIGMETRICS Conference on Measurement and Modeling ofComputer Systems, June 1997.7 E. Bugnion, S. Devine, K. Govil, and M. Rosenblum. DiscoRunning commodity operating systems on scalable multiprocessors.In Proceedings of the 16th ACM SIGOPS Symposium on OperatingSystems Principles, volume 315 of ACM Operating SystemsReview, pages 143156, Oct. 1997.8 Connectix. Product Overview Connectix Virtual Server, 2003.httpwww.connectix.comproductsvs.html.9 G. Czajkowski and L. Daynes. Multitasking without compromise avirtual machine evolution. ACM SIGPLAN Notices, 3611125138,Nov. 2001. Proceedings of the 2001 ACM SIGPLAN Conference onObject Oriented Programming, Systems, Languages andApplications OOPSLA 2001.10 S. Devine, E. Bugnion, and M. Rosenblum. Virtualization systemincluding a virtual machine monitor for a computer with a segmentedarchitecture. US Patent, 6397242, Oct. 1998.11 K. J. Duda and D. R. Cheriton. BorrowedVirtualTime BVTscheduling supporting latencysensitive threads in a generalpurposescheduler. In Proceedings of the 17th ACM SIGOPS Symposium onOperating Systems Principles, volume 335 of ACM OperatingSystems Review, pages 261276, Kiawah Island Resort, SC, USA,Dec. 1999.12 G. W. Dunlap, S. T. King, S. Cinar, M. Basrai, and P. M. Chen.ReVirt Enabling Intrusion Analysis through VirtualMachineLogging and Replay. In Proceedings of the 5th Symposium onOperating Systems Design and Implementation OSDI 2002, ACMOperating Systems Review, Winter 2002 Special Issue, pages211224, Boston, MA, USA, Dec. 2002.13 D. Engler, S. K. Gupta, and F. Kaashoek. AVM Applicationlevelvirtual memory. In Proceedings of the 5th Workshop on Hot Topics inOperating Systems, pages 7277, May 1995.14 Ensim. Ensim Virtual Private Servers, 2003.httpwww.ensim.comproductsmaterialsdatasheetvps051003.pdf.15 K. A. Fraser, S. M. Hand, T. L. Harris, I. M. Leslie, and I. A. Pratt.The Xenoserver computing infrastructure. Technical ReportUCAMCLTR552, University of Cambridge, ComputerLaboratory, Jan. 2003.16 T. Garfinkel, M. Rosenblum, and D. Boneh. Flexible OS Support andApplications for Trusted Computing. In Proceedings of the 9thWorkshop on Hot Topics in Operating Systems, Kauai, Hawaii, May2003.17 J. Gelinas. Virtual Private Servers and Security Contexts, 2003.httpwww.solucorp.qc.camiscprjscontext.hc.18 K. Govil, D. Teodosiu, Y. Huang, and M. Rosenblum. Cellular DiscoResource management using virtual clusters on sharedmemorymultiprocessors. In Proceedings of the 17th ACM SIGOPSSymposium on Operating Systems Principles, volume 335 of ACMOperating Systems Review, pages 154169, Dec. 1999.19 P. H. Gum. System370 extended architecture facilities for virtualmachines. IBM Journal of Research and Development,276530544, Nov. 1983.20 S. Hand. Selfpaging in the Nemesis operating system. InProceedings of the 3rd Symposium on Operating Systems Design andImplementation OSDI 1999, pages 7386, Oct. 1999.21 S. Hand, T. L. Harris, E. Kotsovinos, and I. Pratt. Controlling theXenoServer Open Platform, April 2003.22 A. Jeffrey and I. Wakeman. A Survey of Semantic Techniques forActive Networks, Nov. 1997. httpwww.cogs.susx.ac.ukprojectssafetynet.23 M. F. Kaashoek, D. R. Engler, G. R. Granger, H. M. Briceno,R. Hunt, D. Mazieres, T. Pinckney, R. Grimm, J. Jannotti, andK. Mackenzie. Application performance and flexibility on Exokernelsystems. In Proceedings of the 16th ACM SIGOPS Symposium onOperating Systems Principles, volume 315 of ACM OperatingSystems Review, pages 5265, Oct. 1997.24 R. Kessler and M. Hill. Page placement algorithms for largerealindexed caches. ACM Transaction on Computer Systems,104338359, Nov. 1992.25 S. T. King, G. W. Dunlap, and P. M. Chen. Operating System Supportfor Virtual Machines. In Proceedings of the 2003 Annual USENIXTechnical Conference, Jun 2003.26 M. Kozuch and M. Satyanarayanan. Internet SuspendResume. InProceedings of the 4th IEEE Workshop on Mobile ComputingSystems and Applications, Calicoon, NY, Jun 2002.27 I. M. Leslie, D. McAuley, R. Black, T. Roscoe, P. Barham, D. Evers,R. Fairbairns, and E. Hyden. The design and implementation of anoperating system to support distributed multimedia applications.IEEE Journal on Selected Areas In Communications,14712801297, Sept. 1996.28 J. MacKieMason and H. Varian. Pricing congestible networkresources. IEEE Journal on Selected Areas In Communications,13711411149, Sept. 1995.29 L. McVoy and C. Staelin. lmbench Portable tools for performanceanalysis. In Proceedings of the USENIX Annual TechnicalConference, pages 279294, Berkeley, Jan. 1996. UsenixAssociation.30 J. Navarro, S. Iyer, P. Druschel, and A. Cox. Practical, transparentoperating system support for superpages. In Proceedings of the 5thSymposium on Operating Systems Design and Implementation OSDI2002, ACM Operating Systems Review, Winter 2002 Special Issue,pages 89104, Boston, MA, USA, Dec. 2002.31 G. C. Necula. Proofcarrying code. In Conference Record ofPOPL 1997 The 24th ACM SIGPLANSIGACT Symposium onPrinciples of Programming Languages, pages 106119, Jan. 1997.32 S. Oikawa and R. Rajkumar. Portable RK A portable resource kernelfor guaranteed and enforced timing behavior. In Proceedings of theIEEE Real Time Technology and Applications Symposium, pages111120, June 1999.33 L. Peterson, D. Culler, T. Anderson, and T. Roscoe. A blueprint forintroducing disruptive technology into the internet. In Proceedings ofthe 1st Workshop on Hot Topics in Networks HotNetsI, Princeton,NJ, USA, Oct. 2002.34 I. Pratt and K. Fraser. Arsenic A useraccessible gigabit ethernetinterface. In Proceedings of the Twentieth Annual Joint Conference ofthe IEEE Computer and Communications Societies INFOCOM01,pages 6776, Los Alamitos, CA, USA, Apr. 2226 2001. IEEEComputer Society.35 D. Reed, I. Pratt, P. Menage, S. Early, and N. Stratford. Xenoserversaccounted execution of untrusted code. In Proceedings of the 7thWorkshop on Hot Topics in Operating Systems, 1999.36 J. S. Robin and C. E. Irvine. Analysis of the Intel Pentiums ability tosupport a secure virtual machine monitor. In Proceedings of the 9thUSENIX Security Symposium, Denver, CO, USA, pages 129144,Aug. 2000.37 C. P. Sapuntzakis, R. Chandra, B. Pfaff, J. Chow, M. S. Lam, andM. Rosenblum. Optimizing the Migration of Virtual Computers. InProceedings of the 5th Symposium on Operating Systems Design andImplementation OSDI 2002, ACM Operating Systems Review,Winter 2002 Special Issue, pages 377390, Boston, MA, USA, Dec.2002.38 L. Seawright and R. MacKinnon. VM370  a study of multiplicityand usefulness. IBM Systems Journal, pages 417, 1979.39 P. Shenoy and H. Vin. Cello A Disk Scheduling Framework forNextgeneration Operating Systems. In Proceedings of ACMSIGMETRICS98, the International Conference on Measurement andModeling of Computer Systems, pages 4455, June 1998.40 V. Sundaram, A. Chandra, P. Goyal, P. Shenoy, J. Sahni, andH.M.Vin. Application Performance in the QLinux MultimediaOperating System. In Proceedings of the 8th ACM Conference onMultimedia, Nov. 2000.41 D. Tennenhouse. Layered Multiplexing Considered Harmful. InRudin and Williamson, editors, Protocols for HighSpeed Networks,pages 143148. North Holland, 1989.42 C. A. Waldspurger. Memory resource management in VMware ESXserver. In Proceedings of the 5th Symposium on Operating SystemsDesign and Implementation OSDI 2002, ACM Operating SystemsReview, Winter 2002 Special Issue, pages 181194, Boston, MA,USA, Dec. 2002.43 A. Whitaker, M. Shaw, and S. D. Gribble. Denali LightweightVirtual Machines for Distributed and Networked Applications.Technical Report 020201, University of Washington, 2002.44 A. Whitaker, M. Shaw, and S. D. Gribble. Scale and performance inthe Denali isolation kernel. In Proceedings of the 5th Symposium onOperating Systems Design and Implementation OSDI 2002, ACMOperating Systems Review, Winter 2002 Special Issue, pages195210, Boston, MA, USA, Dec. 2002.
