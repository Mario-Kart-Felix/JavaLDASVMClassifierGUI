Smartphone Security Trends and PredictionsDan Wallach, Rice UniversityFebruary 17, 20111 IntroductionIts no exaggeration to say that smartphones, whether RIMs Blackberry, Apples iPhone, Googles Android,or other models coming out every day, have become ubiquitous in government and in the population atlarge, and its no wonder. For the worker on the move, everything from email to calendering, as well asnews and entertainment, is now available in a convenient pocketable device. These devices allow users toremain productive, even when walking between destinations, stuck on public transit, or during downtime inmeetings. Clearly, theyre here to stay. This think piece considers the various security risks and opportunitiesthat arise with the ubiquity of modern smartphones.Trend 1 smartphones are real computers. Currentgeneration smartphones generally use ARM microprocessors with clock rates over 1GHz, with new models coming soon that have dual or quadcore CPUsand even higher clock rates. They have gigabytes of storage and reasonably fast networks. In these respects, current smartphones are faster and betterprovisioned than desktop computers from a decade ago.This raises an interesting opportunity because smartphones have more than adequate resources to leveragedecades of research into secure operating systems. Smartphones from most vendors can or will soon runfullblown Unixstyle operating system kernels. This means that security features ranging from virtualization and secure booting to information flow control and multilevel security can potentially be applied tocreating highassurance software within our phones.Trend 2 many government, military, and private sector IT departments are locking personnel out ofsocial network sites, pushing those users to do these activities on personal smartphones. The justification for these actions vary, but a recent order from the Marines is fairly typicalInternet SNS are defined as webbased services that allow communities of people to share common interests andor experiences existing outside of DoD networks or for those who want toexplore interests and background different from their own. These Internet sites in general are aproven haven for malicious actors and content and are particularly high risk due to informationexposure, user generated content and targeting by adversaries. The very nature of SNS creates alarger attack and exploitation window, exposes unnecessary information to adversaries and provides an easy conduit for information leakage that puts OPSEC, COMSEC, personnel and theMCEN at an elevated risk of compromise. Examples of Internet SNS sites include Facebook,MySpace, and Twitter.1. . . access is hereby prohibited to Internet SNS from the MCEN NIPRNET, including over virtual private network VPN connections.Marine Corps order 6, emphasis mineThis Marine Corps order may protect the MCEN internal network, but it fails to protect Marine personnel, who will continue visiting social network sites, albeit now on their smartphones or home computers.For personnel who work in environments where smartphones are banned, they may well go outside to usetheir phones on breaks. As such, all that these policies accomplish is pushing dangerous behavior frominternal systems, where administrators may be able to monitor them, to external systems, where attacks maywell go unnoticed.Trend 3 Safe web sites can and will be compromised. In environments such as the MCEN NIPRNET,where social networking is banned, a typical firewall policy is to ban sites known to be undesirable andallow everything else sometimes called a blacklist policy. More aggressive IT managers may defaultto banning everything, by default, and only allowing a handful of knowngood sites sometimes called awhitelist policy. The problem with blacklists is that theyre always missing something bad. The problemwith whitelists is that any arbitrary web site may fall victim to a securityrelated attack.To pick a notable example, visitors to the the New York Timess web site were attacked in September2009 by a malicious advertisement that tried to trick the user into installing fake antivirus software 12.The creator of the malicious ads posed as Vonage, the Internet telephone company, and persuaded NYTimes.com to run ads that initially appeared as real ads for Vonage. At some point,possibly late Friday, the campaign switched to displaying the virus warnings.Because The Times thought the campaign came straight from Vonage, which has advertised onthe site before, it allowed the advertiser to use an outside vendor that it had not vetted to actuallydeliver the ads, Ms. McNulty said. That allowed the switch to take place. In the future, wewill not allow any advertiser to use unfamiliar thirdparty vendors, she said.Mr. Frons said it was unclear how many people saw the ads.The advertisement displayed what appeared to be a virus scanner, really just a graphical simulation of one,running inside the browser. It then claimed to have found viruses on the computer and offered to allow theuser to install software to clean up the infection.Security experts say that people who followed the ads instructions and installed the fake antivirus software will likely receive periodic offers to buy more types of software. Most legitimate antivirus programs are able to clean up the mess left behind.Once theyve fooled you with one thing, they try and fool you with something else, said KevinHaley, a director in the security software maker Symantecs response team. Its extremelyprofitable for them.This attack had no particular target population in mind, but an adversary interested in attacking the Marinesor any other branch of the military could mount a similar attack, using similar means, only selecting a website whose audience is more likely to be military personnel. At the point where an attacker has convincedmilitary personnel to install thirdparty software on their smartphone, the risks are quite substantial. Modernsmartphones have GPS hardware, to measure their precise location. These phones also have microphones2and antennas, making them excellent remotelyoperated listening devices. Commercial software companies, such as FlexiSpy, already offer phonemodification products that do exactly this. Apple even offers aservice to allow users to find lost phones, which could clearly be used in a variety of other ways. Smartphones also have within them the necessary user names, passwords, or other credentials to connect to remotemail servers and otherwise allow an attacker to go quite far into attacking an enterprise.Thesis The modern smartphone is part of the military and civilian attack surface and needs to betreated as such. The remainder of this paper considers how we might be able to better manage usersneeds to visit arbitrary web sites social networking and otherwise, how we can engineer smartphones tobetter work with the security infrastructure we already have in place for desktop computers, and what openresearch topics remain in managing and mitigating against these sorts of threats.2 Threat modelsBefore we delve into particular technologies for smartphones, we should consider the threats that their userswill face. For the purposes of this document, we will segment users into two distinct populations lowvalue targets LVT and highvalue targets HVT, with the core difference being that HVTs are sufficientlyattractive that adversaries will consider attacks that require physical access to an HVTs phone, acquiredtemporarily via espionage of one sort or another. LVTs, on the other hand, may still be worth attacking, butnot enough that an attacker would go after them in person. Speculation as to what classes of government,military, or corporate personnel may be classified better as LVT vs. HVT is beyond the scope of thisdocument, although its certainly safe to say that most government and military personnel are in the LVTcategory.With these LVT and HVT definitions, several things become clear. HVT users must worry about theirphones being surreptitiously modified or even being stolen and replaced with replicas. An HVT phone mustthen resist physical tampering, including logic to zeroize itself when an attack is in progress. This thinkpiece will discuss the SMEPED program in Section 3, which is intended to be robust against HVT threats.Of course, a persons LVT vs. HVT status may change as they travel. A visit to an overseas countryknown to have the capability and motive to attack smartphones can convert an LVT target into an HVT target,if only for the duration of the trip. Luckily, mitigations such conditions can be very simple to implementleave your laptop and cellular phone at home, perhaps giving out loaner gear for the duration of the trip.The LVT threat model still covers a lot of ground. People will still be subject to broadspectrum attacksfrom hackers and the like, mostly interested in their credit card numbers, banking passwords, and otherways they can make money. In addition to such broad attacks, various personnel may well be individuallyand personally targeted, albeit not in the flesh. A foreign intelligence agency, for example, may well havea strong interest in turning White House staffers phones into bugging devices, but they may not have themeans to physically acquire and tamper with those staffers phones. Even if they do have the means, theymay not wish to risk a physical operation that could endanger their agents. Purely electronic attacks, becausethey can potentially be conducted with some measure of anonymity and from afar, may be judged to offerless risk to the attacker, and thus would be more attractive to pursue.Its also important to note that any user might be tricked into attacking themselves, which could be assimple as being given a series of instructions for how to install the latest cool game. Also, consider userswho connect their phones to their personal computers. An iPhone must be regularly connected to its hostcomputer to synchronize with iTunes. Even for Android phones, which synchronize over the air, users maywell just to charge their phones from a computers USB port. In these cases, a security vulnerability in the3computer could well be exploited, giving the attacker access to whatever control is available via the USBport. For many phones, this is sufficient to mount an HVT attack.The key distinction of the LVT threat model is that it allows us to build a trusted computing baseTCB into the phone. If we trust the phone hardware, then we can build a secure operating system kernel.We can lock down the mechanisms for upgrading that kernel to require physical access to the phone e.g.,using the docking connector. With this TCB in place, we can introduce a variety of mechanisms to separatepotentially hostile apps from one another within the phone, among other benefits. We will discuss thisfurther in Section 4.3 SMEPEDThe Secure Mobile EnvironmentPortable Electronic Device SMEPED, pronounced smeeepehd program aims to give U.S. military personnel a device analogous to civilian smartphones, yet also able to maketopsecret phone calls and to interact at the secret level with the U.S. militarys classified network SIPRNET resources. The core design of the current SMEPED smartphones involves four distinct electronicsboards a trusted crypto module, the semitrusted black and red compute modules, and the untrustedRF module, which can be physically removed and replaced e.g., to convert the SMEPED from GSM toCDMA, one would only need to replace the RF module, which clips onto the outside of the phone.The crypto board does more than just cryptography. It acts in an analogous fashion to a KVM keyboard,video, mouse switch, typically used to share a display across multiple separate computers. Here, the cryptomodule selects whether or not either compute module can see the keys and the microphone, the screen, andso forth. The crypto module has dedicated mode selector buttons, with which the user selects one side of thesystem or the other. Most importantly, the crypto module has a trusted path to tell the user whats going on.On the General Dynamics implementation, this is implemented as a secondary screen, below the keypad.On the L3 implementation, this is implemented with a segment on the main display that can only be writtento by the crypto module.SMEPED phones have a variety of other features that might be attractive to military personnel, including support for CAC cards personal identification cards, used throughout the government, which can act ascrypto plugin providers for unclassifiedbutsensitive email systems. They also meet MILSPEC ratings forheat, water, and other environmental factors that would destroy a normal phone.One curious property of the SMEPED design is that operating system security mechanisms are largelyunused. Rather than trusting the operating system, in this case Windows Mobile, to provide separation between black and red materials, the designers required distinct hardware boards. This certainly simplifiesthe security analysis, and it also completely contains attacks on the black side, such as might originatefrom a hacked public web server, ensuring that the compromise cannot cross the air gap1 to the red sidewhere it might compromise more important secrets.To deal with the HVT threat model, SMEPED phones require the user to enter a PIN and will zeroizethe internal storage if the PIN entry fails enough times. Furthermore, administrative policies allow forzeroization under various other conditions. All data is encrypted as its stored, making it more difficult foran attacker to learn valuable information by forcibly extracting the Flash memory chips.Remote zeroization and encrypted storage are claimed features of RIM Blackberry products. Comparable features either already exist or are planned for other smartphones.1There isnt very much actual air in this particular air gap. A broader analysis of the extent to which the military maintains theair gap to its secret and higher materials is beyond the scope of this whitepaper.4Unfortunately, despite all of this engineering, SMEPED phones are fantastically expensive almost4000 each. SMEPEDs are heavier and more cumbersome than their civilian counterparts, their batterylife isnt very good, and theyre largely incompatible with the rapidly growing world of thirdparty apps,such as have become a major selling point for Apples iPhone and Googles Android.Furthermore, even with the SMEPEDs sophisticated design, a number of important security concernsremain, at least with regard to the lesstrusted black side of the phone. If an attacker can compromisethe system software, the attacker can then capture audio from the microphone and key presses from thekeyboard. When a CAC card is inserted, an attacker could also potentially use it from the black side togenerate digital signatures. Of course, switching the phone to the red mode would disable all of thesefeatures, but a SMEPED may well spend a lot of its time in the black mode where such vulnerabilitieswould be relevant.An important challenge is to learn lessons from the SMEPED program that can be applied towardcivilian phones.4 Better SmartphonesUnlike the SMEPED, civilian smartphones absolutely must be cheap to manufacture. Given the volumesin which these phones ship, their manufacturers have huge incentives to save money on the margins, so theredundant hardware approach, used in SMEPED systems, would never be taken seriously.Regardless, civilian smartphones still need appropriate machinery, most likely in software, to separatedifferent apps from interfering with one another. They still need a notion of a trusted path, so users can distinguish whether the app they see is the app they want. And they also need the kinds of remote managementand administration that we normally only associate with desktop computers or servers.4.1 Trusted pathSMEPED handsets achieve trusted path with a dedicated screen or dedicated area of the screen thatidentifies critical security information to the user, such as whether the black or red side of the phone isengaged, and for secure calls, the identity of the remote party. Even if the software on the computer boardis compromised, the trusted path display will hopefully still say whats going on.This screen is controlled by the SMEPEDs crypto module and cannot be overridden by either computemodule. This means that personnel can be trained to pay attention to it. If malicious software were tocompromise the black untrusted side of the phone and were to pretend that it was showing the userssecure email, the user would hopefully be able to recognize the forgery because the trusted display wouldclearly indicate which compute module was in charge of the phone.In addition to the trusted display, the SMEPED also has trusted keys, such as the redblack selectorbuttons. The wires from these keys presumably go directly to the crypto module, eliminating the blackCPUs ability to interfere. If the user hits the red button, the user will then be guaranteed to be interactingwith the red CPU.One of our challenges is to get trusted path semantics without having a dedicated screen. The best hookwe have is the use of hardwired buttons, such as the iPhones central home key. On current iPhones,the home key is trapped by the operating system, which will go so far as to kill off an unresponsive appto return the user to the home screen. Microsoft similarly leveraged the ControlAltDelete key sequenceon PCs as a form of trusted path in its earlier Windows NT versions. To log in, you had to type ControlAltDelete first, and no application besides the kernel would ever receive that key. Microsoft removed this5requirement in Windows XP, since users didnt like it. It also offers less benefit when remote attackers careless about learning the system login password and would much rather learn passwords for online servicessuch as banking.4.2 Userfacing security policiesFigure 1 iPhone security dialog forlocationbased services, shown here withthe Shazam music recognition service.For better or for worse, computer users are fantastically unableto respond to security dialogs. To offer an anecdote, a friendwas trying to play a DVD on her Windowsbased laptop and itwasnt working. I suggested she download VLC, a popularopensource application that has worked well for me in the past.Her computers aftermarket security software then proceededto generate an impressive storm of messages, including at onepoint blacking out the screen to present a strident warning. Sheclicked through all of these without ever bothering to read them.After all, they were getting in the way of what she knew shewanted. A recent study quantified these effects with SSL warnings 11, finding that poorly engineered web browser warningswould be ignored by virtually all users around 90 while eventhe bestdesigned warnings would only be heeded by half of theusers. These studies were conducted with university students,who presumably have more experience with computers than thegeneral population.The only obvious solution available, unfortunately, is to takethese choices out of the hands of users. Every possibly dangerous policy question could ostensibly be answered in advance, inthe negative, on behalf of users. This will certainly yield a saferexperience, but will also anger users, for whom one of the largeattractions of getting a smartphone is to have access to the world of thirdparty applications. All the benefitof having centrally administered smartphones will be lost if they are sufficiently locked down that usersconsider them unusable.Apples solution is a curious hybrid of enforcing a wide variety of rules in advance, prior to allowingapps into the iTunes Store, along with requiring apps to request permission for more invasive permissionsat runtime. Figure 1 shows an interaction with the Shazam app, asking for permission to use the GPSlocation service. Shazams normal purpose is to listen to ambient music, via the microphone, ship this offto a remote computer for processing, and then tell the user what song it may have recognized. Of course,Shazam will then helpfully offer the ability to buy the music. So why does Shazam need to know wherethe phone is, physically Maybe they can offer some social networking feature. Users can tell the worldwhere they were when they heard favorite songs. Alternatively, users can decline to reveal their location toShazam, as the app works just fine without this information.Because dialogs such as this are infrequent, the appearance of one will hopefully give some pause to theuser, but we should not expect anything close to all users to say no in circumstances when that would bethe preferred answer.64.3 Systemenforced policiesApples iPhone security mechanism has two essential flaws. First, it assumes that its app store policingfunction can prevent dangerous apps from ever reaching users. Second, it assumes that the apps that arepresent on the phone cannot be exploited. In fact, the latter issue has given rise to the phenomenon of iPhonejailbreaking, where vulnerabilities are methodically discovered, either in apps or libraries or wherever else,which are then exploited to disable the iPhones requirement for code to be digitally signed by Apple. Atthat point, users are free to install apps from anywhere. Jailbreaking methods have changed over time manynow require the iPhone to be tethered to a computer, leveraging the additional power available to the attackerby using the docking connector. In our LVT vs. HVT taxonomy, this would be an HVT attack.An alternate approach is taken by Googles Android. Android phones, under the hood, are running astrippeddown Linux system, much like Apple iPhones run a strippeddown version of the Macintosh OS Xsystem. Android leverages Linuxs own protection system, giving each app its own Linux userid 1, andmarking the protection bits in the filesystem such that users can neither see nor edit each others files. Bytreating each app as a distinct user, a security compromise in any one app is insufficient to take over theentire Android phone. All of this happens, by design, without requiring any user input. Android apps maystill request access to nondefault permissions, as with the iPhone, ultimately having similar weaknesses.For both the iPhone and Android platforms, its easy to imagine getting the systems administrator in theloop on these permission checks. When a user installs an app, the system could enforce the standard defaultpermissions, yet also send a note to the administrator. The administrator could then make policy exceptionson an appbyapp basis and publish those to every managed phone. This naturally requires having a systemsadministrator, which will not happen when users are using personal equipment, particularly if they acquiredpersonal smartphones specifically to work around onerous administrative policies elsewhere in their workenvironment see the Trends listed in the Introduction.Consider the issue of what permissions should be allowed by default. Should apps have the privilegeto interrogate the address book One iPhone game developer, Storm8, is being sued over allegedly copyingusers address books 3. Clearly, in an Androidlike system, the privilege to access the phone book neednot be enabled by default, which would clearly defeat such an exfiltration attack. On the other hand, if toomany such privileges were denied, then more exceptions would be necessary to the default security policyin order to enable useful apps which have legitimate reasons to access resources like the address book.While there will probably never be a clear answer on default, onesizefitsall security policies, we canimagine building a novel security policy from a rule that Apple requires for the purposes of saving power.Recall that an iPhone is a generalpurpose Unix computer. It can certainly allow applications to run in thebackground, with no visible indication that they are running. Of course, this will slow down the phoneand consume more power, so Apple bans this practice as a requirement to get into its online store. Othersmartphones, notably Android and HPPalm phones, allow and encourage background applications.Apples powersaving policy could be enforced by the operating system kernel, turning it into a contextsensitive security policy. When an app is in the foreground, because the user selected it, we can allow theapp to access the microphone, speakers, and perhaps also the GPS device. When its not in the foreground,the OS kernel can forcibly take away these privileges or kill the process altogether. Combined with a trustedpath mechanism to select the foreground app see Section 4.1, this gives users assurance that an app cannotmisbehave when its not visibly running. This, in turn, will give security administrators a greater comfortlevel in allowing users to use locationaware apps.74.4 Virtualization and forensicsThe concept of virtual machines first appeared on mainframe computers, decades ago, and is now gainingprominence both on desktop machines and in server farms. The idea is that an operating system, such asWindows, does not need to run directly on the hardware. Instead, it can run on a hardware emulator, whichthen runs on the real hardware. For consumers, this technology, from companies such as VMware andParallels, makes it possible to run Windows applications seamlessly on a Macintosh computer. For the datacenter, it allows old software to continue running on new computers, even when the software is incompatiblewith newer operating systems. Virtualization also creates separation between mutually distrusting entitieswho share resources. Amazon uses this, to great effect, in its Elastic Compute Cloud2 service, allowingcustomers to install any operating system they wish without requiring separate computers for each customer.Modern smartphones clearly have the necessary resources to support virtualization. How can we leverage this for improved smartphone security Virtualization allows the smartphone to save and restore itsentire state to remote, trusted network services. Think of this as a backup service. If a user loses a phone,the entire contents of the phone, including all of its thirdparty apps and their data, could potentially berestored in minutes. VMware is already working on such a technology 5. In addition to backups, wecould also consider replicated execution on the phone and on a remote trusted server. See, for example,Microsofts Ripley system 13. Replicated execution doesnt solve security attacks, but it does bring theexecution closer to where it can be monitored and controlled in real time.At the point where we have offphone backups, we now have the opportunity for offphone attack forensics. A forensic analyst, in such cases, has access to the full state of the phone, so even if the phone appearedto be unmodified, stealthy changes could still be discovered. Furthermore, a forensic analyst would haveaccess to large numbers of phone backups, creating interesting opportunities for bulk analysis. If any onephone stands out from the crowd, it may well be under attack and would warrant bringing the phone in fora closer look. Of course, once an attack is detected, a variety of responses can be devised.With virtualization, we could even imagine implementing software security mechanisms that parallelthe hardware security separation techniques used in the SMEPED. This may never be acceptable for secretlevel data or voice processing, but it could be used to create a separation between users work uses of thephone and their home uses. Making this usable would be quite tricky, particularly dealing with thornyissues like the address book. Should it be shared across the two virtual systems How should we indicateto the user which VM is in use and how can we guarantee that a compromised VM cannot defeat the VMswitching mechanism These issues could possibly be addressed by borrowing some of the trusted pathmechanisms in the SMEPED see Section 4.1 for a regular consumer smartphone, although it may well besimpler and cheaper for users to simply carry two phones.Virtualization also is one of the key components of secure bootstrapping see, e.g., Arbaugh et al. 2,where the hardware verifies integrity of the boot ROM by comparison to a cryptographic checksum, whichthen verifies the integrity of the virtualization microkernel, which can then verify the operating systemimage, and so forth. If at any point in this chain, the verification fails, then access to lowerlevel resources,such as cryptographic key material, can be denied, thus protecting the confidentiality of data storage, remoteauthentication, and so forth. These technologies have never really taken off in the commercial world forexample, dealing with the staggering diversity of operating system configurations, but they could play avaluable role in smartphones.Its important to note that this virtualization approach is actually an excellent solution to Trends 2 and3 see the Introduction to secure the desktop environment. If government and military personnel wish to2httpaws.amazon.comec28access Facebook, Twitter, or the New York Times, these can be separated from internal data by runningvirtual machines. While a variety of different implementation tactics can be taken, broadly speaking we cancreate the equivalent of having two separate computers on everybodys desk, one configured for using theinternal network, and the other configured for using the external network. Similar approaches are alreadytaken for access to SIPRNET, so the necessary tools and systems administration skills are already availableand understood in government and military networks.4.5 Attack surface analysisA typical way to analyze the security of a system is to consider the size of its attack surface, which is tosay, to consider all the possible ways that an attacker might try to interact with the system. Every softwareservice that accepts network packets is part of the attack surface.The attack surface of the VM layer is really no different than the security of the regular operating systemkernel, assuming the operating system is designed to limit the privileges of regular applications such asAndroid but unlike the iPhone. In the HVT threat model, an attacker with physical access to a phone willultimately be able to replace the code running on it. In the LVT threat model, however, either a VM layer ora traditional operating system kernel running above it, need only be designed to resist external attacks, viahostile data transmitted over the network.Ultimately, an attacker may try to attack an existing, trusted app whether builtin or thirdparty, anattacker may manage to get a malicious app installed, or an attacker may choose to go after the kernel orVM if present. The separation that comes from Androids model see Section 4.3 would seem like a goodway to contain attacks against apps. Defeating malicious apps, particularly if they appear to be legitimateones, is tricky. This requires limiting the ability of illegitimate apps to pretend to be legitimate ones. Applesolves this by requiring all apps to be digitally signed by its online store and by enforcing that it doesntendorse apps that appear to be the same as others. Absent centralized control, such as where Android allowsapps to come from anywhere, it becomes a UI problem to ensure that a malicious app cannot be confusedfor something else combined with the risk of social engineering attacks e.g., spear phishing which try toconvince users to download and install things which they shouldnt.The attack surface also includes whatever mechanism is in place to do software upgrades to the phoneoperating system. This includes official upgrades, which may be pushed by phone carriers over the air.But should phone carriers be trusted In July 2009, Etisalat, a phone provider in the United Arab Emirates,sent out a notice to Blackberry customers inviting them to download an update which turned out to bespyware 7, 9. If the phone carrier cannot be trusted, then who can This particular update was reallyjust a regular app that wired itself deeply into the Blackberry system, to spy on the user, and send thatdata back to a central server 4, so its really more in the above category of a malicious app than in thecategory of an attack on the operating system itself. Nonetheless, the concern remains. Should we insistthat OS upgrades be digitally signed by the original vendor That would limit the ability of a maliciousphone carrier to monkey with a phone, but the crypto would need to be reasonably strong, as it would be anattractive target for concerted nationstate cryptanalytic attacks. For contrast, the digital signature schemeused by Texas Instruments for its calculators was broken 14, enabled by virtue of the 512bit signaturescheme used being far too weak 10.If we want to protect LVT phone users against attack, then we can consider security policies that requireall installations, whether to replace the operating system or its apps, to go through the docking connector.On the one hand, this eliminates the Etisalat attack and others like it. On the other hand, the modern personalcomputer isnt exactly a trustworthy place, either, and PCbased malware could well be engineered to attackthe smartphone when its plugged in. This suggests that a combination of strong digital signatures plus the9use of the physical docking connector may be required in combination.5 RecommendationsThis think piece opened by considering several trends in government, military, and corporate computersecurity policies which are pushing users away from doing personal activities on their work equipment.Of course, this just pushes such activities to personal computers and smartphones without the scrutiny andadministrative controls available on professionally managed computers.A better solution is to harden the users equipment, whether using virtualization technologies to separate users personal and work computing spaces, while still running in a single computer. Furthermore,replacing old and insecure web browsers such as Internet Explorer 6 with newer, more robust web browsers,such as Googles Chrome 8, would significantly improve the security posture for web surfing. With suchmitigations in place, it becomes safer and preferable to have users visit potentially hostile web sites fromthe inside, rather than using personal equipment to visit such sites from home.Smartphones, meanwhile, have all the computational power of fullblown desktop computers, yet theyfit in our pockets and go with us everywhere, opening up a variety of new security concerns. If a phone iscompromised, it becomes a powerful electronic eavesdropping device. While banning such phones wouldappear to be the answer, and many government and military sites forbid any phones from being broughtinside, they nonetheless serve as attractive targets for remote exploitation. With improved software architectures on these phones, they could be centrally managed, allowing professional systems administrators to doforensic analyses, detect attacks when they happen, and respond in a timely fashion. None of that is possiblewhen users are using personal equipment.Many of the technologies discussed here are fairly close to technologies that are already shipping in commercial products, but there a world of difference between close and secure. By virtue being such a largepurchaser, the government can have a significant influence on the design and engineering of smartphones.If commercial smartphone engineers were brought in and briefed on how close they are and how they coulddo better for all of their users without costly hardware modifications, this could have a real impact.While much of this think piece has focused on fairly practical and shortterm issues, some longtermresearch challenges are also worth considering. Imagine a database with millions of smartphone backups,all in one place. How should this be analyzed How, exactly, does this enable the administrator to detectattacks Tools need to be developed that can distinguish between normal variances from one phone to thenext and identify attacks without generating too many false alarms.Furthermore, if we require that apps be digitally signed before they are installed, then the signing authority can apply significant static analysis or binary rewriting for dynamic analyses to the apps. Can weprove that apps are well behaved Can we even formalize what good behavior should be The IARPASTONESOUP research program3 is pursuing exactly this problem set.Lastly, when humans are being required to take actions with security ramifications, the next immediatequestion should be to measure how often they can succeed. Human subject experiments can measure this,can determine what sort of training can have the best bang for the buck, and can consider design variationsto determine how best to structure the interface. Trusted path mechanisms are essential to the security ofsmartphones. Identifying whether users respond properly when the trusted path indicators show theres aproblem is essential to understanding whether the system, at the end of the day, is actually going to besecure.3httpwww.iarpa.govsolicitationsstonesoup.html10References1 Android Developers. Security and Permissions, Oct. 2009. httpdeveloper.android.comguidetopicssecuritysecurity.html, downloaded November 2, 2009.2 W. A. Arbaugh, D. J. Farber, and J. M. Smith. A secure and reliable bootstrap architecture. In Proceedings of the 1997 IEEE Symposium on Security and Privacy, pages 6571, Oakland, CA, May1997.3 R. Beschizza. iPhone game dev accused of stealing players phone numbers. Boing Boing, Nov. 2009.httpwww.boingboing.net20091105iphonegamedevaccu.html.4 C. Eng. BlackBerry Spyware Dissected. Veracode, July 2009. httpwww.veracode.comblog200907blackberryspywaredissected.5 A. R. Hickey. Interop VMware to bring virtualization to smartphones. ChannelWeb, May 2009.httpwww.crn.comstorage217600096.6 Marine Corps. Immediate ban of Internet social networking sites SNS on Marine Corps enterprisenetwork MCEN NIPRNET, Aug. 2009. MARADMIN Active Number 045809, httpwww.marines.milnewsmessagesPagesMARADMIN045809.aspx.7 B. Ray. BlackBerry update bursting with spyware. The Register, July 2009. httpwww.theregister.co.uk20090714blackberrysnooping.8 C. Reis, A. Barth, and C. Pizano. Browser security Lessons from Google Chrome. ACM Queue, June2009.9 Research in Motion. App Remover for removing Etisalats Registration application on BlackBerrysmartphones, July 2009. httpna.blackberry.comengataglancesecurityregappremover.jsp.10 B. Schneier. Texas Instruments signing keys broken. Schneier on Security, Sept. 2009. httpwww.schneier.comblogarchives200909texasinstrumen.html.11 J. Sunshine, S. Egelman, H. Almuhimedi, N. Atri, and L. F. Cranor. Crying wolf An empirical studyof SSL warning effectiveness. In 18th USENIX Security Symposium, Montreal, Canada, Aug. 2009.12 A. Vance. Times web ads show security breach. New York Times, Sept. 2009. httpwww.nytimes.com20090915technologyinternet15adco.html.13 K. Vikram, A. Prateek, and B. Livshits. Ripley Automatically securing Web 2.0 applications throughreplicated execution. In Proceedings of the 16th ACM Conference on Computer and CommunicationsSecurity CCS09, Chicago, IL, Nov. 2009.14 M. Vincent. TI83 Plus OS signing key cracked. ticalc.org, July 2009. httpwww.ticalc.orgarchivesnewsarticles14145145154.html.11
