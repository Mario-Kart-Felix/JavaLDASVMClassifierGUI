The following paper was originally published in theProceedings of the 3rd Symposium on Operating Systems Design and ImplementationNew Orleans, Louisiana, February, 1999For more information about USENIX Association contact1. Phone 1.510.528.86492. FAX 1.510.548.57383. Email officeusenix.org4. WWW URL httpwww.usenix.orgResource Containers A New Facility for Resource Management in Server Systems Gaurav Banga, Peter DruschelRice UniversityJeffrey C. MogulWestern Research Laboratory, Compaq Computer Corp.Resource containers A new facility for resource managementin server systemsGaurav Banga Peter Druschel Jeffrey C. MogulDept. of Computer Science Western Research LaboratoryRice University Compaq Computer CorporationHouston, TX 77005 Palo Alto, CA 94301fgaurav, druschelgcs.rice.edu mogulpa.dec.comAbstractGeneralpurpose operating systems provide inadequate support for resource management in largescaleservers. Applications lack sufficient control overscheduling and management of machine resources,which makes it difficult to enforce priority policies, andto provide robust and controlled service. There is a fundamental mismatch between the original design assumptions underlying the resource management mechanismsof current generalpurpose operating systems, and thebehavior of modern server applications. In particular, theoperating systems notions of protection domain and resource principal coincide in the process abstraction. Thiscoincidence prevents a process that manages large numbers of network connections, for example, from properlyallocating system resources among those connections.We propose and evaluate a new operating system abstraction called a resource container, which separates thenotion of a protection domain from that of a resourceprincipal. Resource containers enable finegrained resource management in server systems and allow the development of robust servers, with simple and firm controlover priority policies.1 IntroductionNetworked servers have become one of the most important applications of large computer systems. For manyusers, the perceived speed of computing is governed byserver performance. We are especially interested in theperformance of Web servers, since these must often scaleto thousands or millions of users.Operating systems researchers and system vendorshave devoted much attention to improving the performance of Web servers. Improvements in operating system performance have come from reducing data movement costs 2, 35, 43, developing better kernel algorithms for protocol control block PCB lookup 26 andfile descriptor allocation 6, improving stability underoverload 15, 30, and improving server control mechanisms 5, 21. Application designers have also attacked performance problems by making more efficientuse of existing operating systems. For example, whileearly Web servers used a process per connection, recentservers 41, 49 use a singleprocess model, which reduces contextswitching costs.While the work cited above has been fruitful, it hasgenerally treated the operating systems application programming interface API, and therefore its core abstractions, as a constant. This has frustrated efforts to solvethornier problems of server scaling and effective control over resource consumption. In particular, serversmay still be vulnerable to denial of service attacks, inwhich a malicious client manages to consume all of theservers resources. Also, service providers want to exertexplicit control over resource consumption policies, inorder to provide differentiated quality of service QoS toclients 1 or to control resource usage by guest serversin a RentAServer host 45. Existing APIs do not allow applications to directly control resource consumption throughout the host system.The root of this problem is the model for resourcemanagement in current generalpurpose operating systems. In these systems, scheduling and resource management primitives do not extend to the execution of significant parts of kernel code. An application has no control over the consumption of many system resources thatthe kernel consumes on behalf of the application. Theexplicit resource management mechanisms that do existare tied to the assumption that a process is what constitutes an independent activity1. Processes are the resourceprincipals those entities between which the resources ofthe system are to be shared.Modern highperformance servers, however, often usea single process to perform many independent activities.For example, a Web server may manage hundreds oreven thousands of simultaneous network connections, allwithin the same process. Much of the resource consumption associated with these connections occurs in kernel1We use the term independent activity to denote a unit of computation for which the application wishes to perform separate resourceallocation and accounting for example, the processing associated witha single HTTP request.mode, making it impossible for the application to controlwhich connections are given priority2.In this paper, we address resource management inmonolithic kernels. While microkernels and other novelsystems offer interesting alternative approaches to thisproblem, monolithic kernels are still commercially significant, especially for Internet server applications.We describe a new model for finegrained resourcemanagement in monolithic kernels. This model is basedon a new operating system abstraction called a resourcecontainer. A resource container encompasses all systemresources that the server uses to perform a particular independent activity, such as servicing a particular clientconnection. All user and kernel level processing for anactivity is charged to the appropriate resource container,and scheduled at the priority of the container. This modelallows fairly arbitrary interrelationships between protection domains, threads and resource containers, and cantherefore support a wide range of resource managementscenarios.We evaluate a prototype implementation of this model,as a modification of Digital UNIX, and show that it is effective in solving the problems we described.2 Typical models for highperformanceserversThis section describes typical execution models forhighperformance Internet server applications, and provides the background for the discussion in following sections. To be concrete, we focus on HTTP servers andproxy servers, but most of the issues also apply to otherservers, such as mail, file, and directory servers. We assume the use of a UNIXlike API however, most of thisdiscussion is valid for servers based on Windows NT.An HTTP server receives requests from its clients viaTCP connections. In HTTP1.1, several requests may besent serially over one connection. The server listens ona wellknown port for new connection requests. When anew connection request arrives, the system delivers theconnection to the server application via the acceptsystem call. The server then waits for the client to senda request for data on this connection, parses the request,and then returns the response on the same connection.Web servers typically obtain the response from the localfile system, while proxies obtain responses from otherservers however, both kinds of server may use a cacheto speed retrieval. Stevens 42 describes the basic operation of HTTP servers in more detail.The architecture of HTTP servers has undergone radical changes. Early servers forked a new process to handle each HTTP connection, following the classical UNIX2In this paper, we use the term priority loosely to mean the current scheduling precedence of a resource principal, as defined by thescheduling policy based on the principals scheduling parameters. Thescheduling policy in use may not be priority based.Kernel    HTTP ConnectionsUser levelHTTP Slave ProcessesListen SocketHTTP MasterProcessTCPIPPending HTTP ConnectionsFig. 1 A processper connection HTTP server with amaster process.model. The forking overhead quickly became a problem,and subsequent servers such as the NCSA httpd 32,used a set of preforked processes. In this model, shownin Figure 1, a master process accepts new connectionsand passes them to the preforked worker processes.Kernel    HTTP ConnectionsUser levelListen SocketTCPIPPending HTTP ConnectionsHTTP Server  Processselect  HTTP ThreadFig. 2 A singleprocess eventdriven server.Multiprocess servers can suffer from contextswitching and interprocess communication IPC overheads 11, 38, so many recent servers use a singleprocess architecture. In the eventdriven model Figure 2, the server uses a single thread to manage all connections at the server. Eventdriven servers designedfor multiprocessors use one thread per processor. Theserver uses the select or poll system callto simultaneously wait for events on all connections itis handling. When select delivers one or moreevents, the servers main loop invokes handlers for eachready connection. Squid 41 and Zeus 49 are examplesof eventdriven servers.Alternatively, in the singleprocess multithreadedmodel Figure 3, each connection is assigned to a uniquethread. These can either be userlevel threads or kernelthreads. The thread scheduler is responsible for timesharing the CPU between the various server threads.Kernel    HTTP ConnectionsUser levelListen SocketTCPIPPending HTTP ConnectionsHTTP Server  Process  HTTP ThreadsFig. 3 A singleprocess multithreaded server.Idle threads accept new connections from the listeningsocket. The AltaVista frontend uses this model 8.So far, we have assumed the use of static documentsor resources, in HTTP terms. HTTP also supportsrequests for dynamic resources, for which responses arecreated on demand, perhaps based on clientprovided arguments. For example, a query to a Web search enginesuch as AltaVista resolves to a dynamic resource.Dynamic responses are typically created by auxiliarythirdparty programs, which run as separate processes toprovide fault isolation and modularity. To simplify theconstruction of such auxiliary programs, standard interfaces such as CGI 10 and FastCGI 16 support communication between Web servers and these programs.The earliest interface, CGI, creates a new process foreach request to a dynamic resource the newer FastCGIallows persistent CGI processes. Microsoft and Netscapehave defined librarybased interfaces 29, 34 to allowthe construction of thirdparty dynamic resource modules that reside in the main server process, if fault isolation is not required this minimizes overhead.In summary, modern highperformance HTTP serversare implemented as a small set of processes. One mainserver process services requests for static documents dynamic responses are created either by library code withinthe main server process, or, if fault isolation is desired,by auxiliary processes communicating via a standard interface. This is ideal, in theory, because the overheadof switching context between protection domains is incurred only if absolutely necessary. However, structuring a server as a small set of processes poses numerousimportant problems, as we show in the next section.3 Shortcomings of current resource management modelsAn operating systems scheduling and memory allocation policies attempt to provide fairness among resourceprincipals, as well as graceful behavior of the system under various load conditions. Most operating systems treata process, or a thread within a process, as the schedulableentity. The process is also the chargeable entity for theallocation of resources, such as CPU time and memory.A basic design premise of such processcentric systems is that a process is the unit that constitutes an independent activity. This give the process abstraction adual function it serves both as a protection domain andas a resource principal. As protection domains, processesprovide isolation between applications. As resource principals, processes provide the operating systems resourcemanagement subsystem with accountable entities, between which the systems resources are shared.We argue that this equivalence between protection domains and resource principals, however, is not always appropriate. We will examine several scenarios in whichthe natural boundaries of resource principals do not coincide with either processes or threads.3.1 The distinction between scheduling entities andactivitiesApplication ProcessApplication ThreadsKernelSingle Independent          ActivityProtection Domain  Resource PrincipalUser levelFig. 4 A classical application.A classical application uses a single process to perform an independent activity. For such applications, thedesired units of isolation and resource consumption areidentical, and the process abstraction suffices. Figure 4shows a mostly usermode application, using one processto perform a single independent activity.In a networkintensive application, however, much ofthe processing is done in the kernel. The process is thecorrect unit for protection isolation, but it does not encompass all of the associated resource consumption inmost operating systems, the kernel generally does notcontrol or properly account for resources consumed during the processing of network traffic. Most systems doprotocol processing in the context of software interrupts,whose execution is either charged to the unlucky processrunning at the time of the interrupt, or to no process atall. Figure 5 shows the relationship between the application, process, resource principal and independent activityentities for a networkintensive application.Some applications are split into multiple protectionApplication domainreally extends into the kernel, but this activity is uncontrolled.Application ProcessApplication ThreadsKernelSingle Independent         ActivityProtection Domain  Resource PrincipalUser levelFig. 5 A classical networkintensive application.Application ProcessApplication ThreadsKernelApplication Process     Single Independent      ActivityProtection Domain  Resource Principal Protection Domain  Resource PrincipalUser levelFig. 6 A multiprocess application.domains for example, to provide fault isolation betweendifferent components of the application. Such applications may still perform a single independent activity, sothe desired unit of protection the process is differentfrom the desired unit of resource management all theprocesses of the application. A mostly usermode multiprocess application trying to perform a single independent activity is shown in Figure 6.KernelApplication ProcessHTTP ConnectionsApplication  ThreadsApplication domainextends into the kernel, but is uncontrolled.  Independent      Activities        Protection Domain Resource PrincipalUser levelFig. 7 A singleprocess multithreaded server.In yet another scenario, an application consists of asingle process performing multiple independent activities. Such applications use a single protection domain, toreduce contextswitching and IPC overheads. For theseapplications, the correct unit of resource management issmaller than a process it is the set of all resources beingused by the application to accomplish a single independent activity. Figure 7 shows, as an example, a singleprocess multithreaded Internet server.Realworld singleprocess Internet servers typicallycombine the last two scenarios a single process usuallymanages all of servers connections, but additional processes are employed when modularity or fault isolationis necessary see section 2. In this case, the desired unitof resource management includes part of the activity ofthe main server process, and also the entire activity of,for example, a CGI process.In some operating systems, e.g., Solaris, threads assume some of the role of a resource principal. In thesesystems, CPU usage is charged to individual threadsrather than to their parent processes. This allows threadsto be scheduled either independently, or based on thecombined CPU usage of the parent processs threads.The process is still the resource principal for the allocation of memory and other kernel resources, such as sockets and protocol buffers.We stress that it is not sufficient to simply treat threadsas the resource principals. For example, the processingfor a particular connection activity may involve multiple threads, not always in the same protection domainprocess. Or, a single thread may be multiplexed between several connections.3.2 Integrating network processing with resourcemanagementAs described above, traditional systems provide littlecontrol over the kernel resources consumed by networkintensive applications. This can lead to inaccurate accounting, and therefore inaccurate scheduling. Also,much of the network processing is done as the result ofinterrupt arrivals, and interrupts have strictly higher priority than any userlevel code this can lead to starvationor livelock 15, 30. These issues are particularly important for largescale Internet servers.Lazy Receiver Processing LRP 15 partially solvesthis problem, by more closely following the processcentric model. In LRP, network processing is integratedinto the systems global resource management. Resources spent in processing network traffic are associatedwith and charged to the application process that causedthe traffic. Incoming network traffic is processed at thescheduling priority of the process that received the traffic, and excess traffic is discarded early. LRP systemsexhibit increased fairness and stable overload behavior.LRP extends a processcentered resource principalinto the kernel, leading to the situation shown in FigApplication ProcessApplication ThreadsKernelProtection DomainResourcePrincipalSingle Independent         ActivityApplications ResourcePrincipal extends into      the kernel. User levelFig. 8 A networkintensive application in a LRP system.ure 8. However, LRP maintains the equivalence betweenresource principal and process it simply makes it moreaccurate. LRP, by itself, does not solve all of the problems that arise when the process is not the correct unit ofresource management.3.3 Consequences of misidentified resource principalsOur fundamental concern is to allow an application toexplicitly allocate resource consumption among the independent activities that it manages. This is infeasible if theoperating systems view of activity differs from that ofthe application, or if the system fails to account for largechunks of consumption. Yet it is crucial for a server tosupport accurately differentiated QoS among its clients,or to prevent overload from denialofservice attacks, orto give its existing connections priority over new ones.With a singleprocess server, for example, traditionaloperating systems see only one resource principal  theprocess. This prevents the application from controllingconsumption of kernel CPU time and other kernel resources by various network connections within this resource principal. The application cannot control the order in which the kernel delivers its network events nor,in most systems, can it control whether it receives network events before other processes do.It is this lack of a carefully defined concept of resource principal, independent from other abstractionssuch as process or thread, that precludes the applicationcontrol we desire.4 A new model for resource managementTo address the problems of inadequate control overresource consumption, we propose a new model for finegrained resource management in monolithic kernels. Weintroduce a new abstraction, called a resource container,for the operating systems resource principal.Sections 4.1 through 4.7 describe the resource container model in detail. Section 4.8 then discusses its usein Internet servers.4.1 Resource containersA resource container is an abstract operating systementity that logically contains all the system resources being used by an application to achieve a particular independent activity. For a given HTTP connection managedby a Web server, for example, these resources includeCPU time devoted to the connection, and kernel objectssuch as sockets, protocol control blocks, and networkbuffers used by the connection.Containers have attributes these are used to providescheduling parameters, resource limits, and network QoSvalues. A practical implementation would require an access control model for containers and their attributesspace does not permit a discussion of this issue.The kernel carefully accounts for the system resources, such as CPU time and memory, consumed bya resource container. The system scheduler can accessthis usage information and use it to control how it schedules threads associated with the container we discussscheduling in detail in Section 4.3. The application process can also access this usage information, and mightuse it, for example, to adjust the containers numeric priority.Current operating systems, as discussed in Section 3,implicitly treat processes as the resource principals,while ignoring many of the kernel resources they consume. By introducing an explicit abstraction for resourcecontainers, we make a clear distinction between protection domains and resource principals, and we provide forfuller accounting of kernel resource consumption. Thisprovides the flexibility necessary for servers to handlecomplex resource management problems.4.2 Containers, processes, and threadsIn classical systems, there is a fixed association between threads and resource principals which are eitherthe threads themselves, or the processes containing thethreads. The resource consumption of a thread is chargedto the associated resource principal, and this informationis used by the system when scheduling threads.With resource containers, the binding between athread and a resource principal is dynamic, and under the explicit control of the application we call thisthe threads resource binding. The kernel charges thethreads resource consumption to this container. Multiple threads, perhaps from multiple processes, may simultaneously have their resource bindings set to a givencontainer.A thread starts with a default resource container binding inherited from its creator. The application can rebind the thread to another container as the need arises.For example, a thread timemultiplexed between severalconnections changes its resource binding as it switchesfrom handling one connection to another, to ensure correct accounting of resource consumption.4.3 Resource containers and CPU schedulingCPU schedulers make their decisions using information about both the desired allocation of CPU time, andthe recent history of actual usage. For example, the traditional UNIX scheduler uses numeric process priorities which indicate desired behavior modified by timedecayed measures of recent CPU usage lottery scheduling 48 uses lottery tickets to represent the allocations.In systems that support threads, the allocation for athread may be with respect only to the other threads ofthe same process process contention scope, or it maybe with respect to all of the threads in the system system contention scope.Resource containers allow an application to associatescheduling information with an activity, rather than witha thread or process. This allows the systems scheduler toprovide resources directly to an activity, no matter how itmight be mapped onto threads.The container mechanism supports a large variety ofscheduling models, including numeric priorities, guaranteed CPU shares, or CPU usage limits. The allocationattributes appropriate to the scheduling model are associated with each resource container in the system. In ourprototype, we implemented a multilevel scheduling policy that supports both fixedshare scheduling and regulartimeshared scheduling.A thread is normally scheduled according to thescheduling attributes of the container to which it isbound. However, if a thread is multiplexed betweenseveral containers, it may cost too much to rescheduleit recompute its numeric priority and decide whetherto preempt it every time its resource binding changes.Also, with a feedbackbased scheduler, using only thecurrent containers resource usage to calculate a multiplexed threads numeric priority may not accurately reflect its recent usage. Instead, the thread should be scheduled based on the combined resource allocations and usage of all the containers it is currently handling.To support this, our model defines a binding, calleda scheduler binding, between each thread and the setof containers over which it is currently multiplexed. Aprioritybased scheduler, for example, would construct athreads scheduling priority from the combined numericpriorities of the resource containers in its scheduler binding, possibly taking into account the recent resource consumption of this set of containers.A threads scheduler binding is set implicitly by theoperating system, based on the systems observation ofthe threads resource bindings. A thread that servicesonly one container will therefore have a scheduler binding that includes just this container. The kernel prunesthe scheduler binding set of a container, periodically removing resource containers that the thread has not recently had a resource binding to. In addition, an application can explicitly reset a threads scheduler bindingto include only the container to which it currently has aresource binding.4.4 Other resourcesLike CPU cycles, the use of other system resourcessuch as physical memory, disk bandwidth and socketbuffers can be conveniently controlled by resource containers. Resource usage is charged to the correct activity,and the various resource allocation algorithms can balance consumption between principals depending on specific policy goals.We stress here that resource containers are just amechanism, and can be used in conjunction with a largevariety of resource management policies. The containermechanism causes resource consumption to be chargedto the correct principal, but does not change what thesecharges are. Unfortunately, policies currently deployedin most generalpurpose systems are able to controlconsumption of resources other than CPU cycles onlyin a very coarse manner, which is typically based onstatic limits on total consumption. The development ofmore powerful policies to control the consumption ofsuch resources has been the focus of complimentary research in applicationspecific paging 27, 20, 24 and filecaching 9, disk bandwidth allocation 46, 47, and TCPbuffer management 39.4.5 The resource container hierarchyResource containers form a hierarchy. The resourceusage of a child container is constrained by the scheduling parameters of its parent container. For example, ifa parent container is guaranteed at least 70 of the systems resources, then it and its child containers are collectively guaranteed 70 of the systems resources.Hierarchical resource containers make it possible tocontrol the resource consumption of an entire subsystem without constraining or even understanding howthe subsystem allocates and schedules resources amongits various independent activities. For example, a systemadministrator may wish to restrict the total resource usage of a Web server by creating a parent container for allthe servers resource containers. The Web server can create an arbitary number of child containers to manage anddistribute the resources allocated to its parent containeramong its various independent activities, e.g. differentclient requests.The hierarchical structure of resource containersmakes it easy to implement fixedshare schedulingclasses, and to enforce a rich set of priority policies.Our prototype implementation supports a hierarchy ofresource principals, but only supports resource bindingsbetween threads and leaf containers.4.6 Operations on resource containersThe resource container mechanism includes these operations on containersCreating a new container A process can create a newresource container at any time and may have multiple containers available for its use. A defaultresource container is created for a new process aspart of a fork, and the first thread of the newprocess is bound to this container. Containers arevisible to the application as file descriptors and soare inherited by a new process after a fork.Set a containers parent A process can change a containers parent container or set it to no parent.Container release Processes release their references tocontainers usingclose once there are no suchdescriptors, and no threads with resource bindings,to the container, it is destroyed. If the parent P ofa container C is destroyed, Cs parent is set to noparent.Sharing containers between processes Resource containers can be passed between processes, analogous to the transfer of descriptors between UNIXprocesses the sending process retains access to thecontainer. When a process receives a reference toa resource container, it can use this container as aresource context for its own threads. This allowsan application to move or share a computation between multiple protection domains, regardless ofthe container inheritance sequence.Container attributes An application can set and readthe attributes of a container. Attributes includescheduling parameters, memory allocation limits,and network QoS values.Container usage information An application can obtain the resource usage information charged to aparticular container. This allows a thread thatserves multiple containers to timeshare its execution between these containers based on its particular scheduling policy.These operations control the relationship between containers, threads, sockets, and filesBinding a thread to a container A process can set theresource binding of a thread to a container at anytime. Subsequent resource usage by the threadis charged to this resource container. A processcan also obtain the current resource binding of athread.Reset the scheduler binding An application can reseta threads scheduler binding to include only its current resource binding.Binding a socket or file to a container A process canbind the descriptor for a socket or file to a container subsequent kernel resource consumption onbehalf of this descriptor is charged to the container.A descriptor may be bound to at most one container, but many descriptors may be bound to onecontainer. Our prototype currently supports binding only sockets, not disk files.4.7 Kernel execution modelResource containers are effective only if kernel processing on behalf of a process is performed in the resource context of the appropriate container. As discussedin Section 3, most current systems do protocol processing in the context of a software interrupt, and may fail tocharge the costs to the proper resource principal.LRP, as discussed in Section 3.2, addresses this problem by associating arriving packets with the receivingprocess as early as possible, which allows the kernel tocharge the cost of receivedpacket processing to the correct process. We extend the LRP approach, by associating a received packet with the correct resource container,instead of with a process. If the kernel uses threads fornetwork processing, the thread handling a network eventcan set its resource binding to the resource container anonthreaded kernel might use a more adhoc mechanismto perform this accounting.When there is pending protocol processing for multiple containers, the priority or other scheduling parameters of these containers determines the order in whichthey are serviced by the kernels network implementation.4.8 The use of resource containersWe now describe how a server application can use resource containers to provide robust and controlled behavior. We consider several example server designs.First, consider a singleprocess multithreaded Webserver, that uses a dedicated kernel thread to handle eachHTTP connection. The server creates a new resourcecontainer for each new connection, and assigns one of apool of free threads to service the connection. The application sets the threads resource binding to the container.Any subsequent kernel processing for this connection ischarged to the connections resource container. This situation is shown in Figure 9.If a particular connection for example, a long filetransfer consumes a lot of system resources, this consumption is charged to the resource container. As a result, the scheduling priority of the associated thread willdecay, leading to the preferential scheduling of threadshandling other connections.Next, consider an eventdriven server, on a uniprocessor, using a single kernel thread to handle all of its connections. Again, the server creates a new resource container for each new connection. When the server doesprocessing for a given connection, it sets the threads resource binding to that container. The operating systemadds each such container to the threads scheduler bindKernelHTTP ConnectionsApplication ProcessResource ContainersProtection DomainApplication ThreadsIndependent    ActivitiesUser levelFig. 9 Containers in a multithreaded server.KernelHTTP ConnectionsApplication ProcessResource ContainersProtection DomainApplication  ThreadIndependent   Activities User levelFig. 10 Containers in an eventdriven server.ing. Figure 10 depicts this situation.If a connection consumes a lot of resources, this usageis charged to the corresponding container. The serverapplication can obtain this usage information, and useit both to adjust the containers numeric priority, and tocontrol how it subsequently expends its resources for theconnection.Both kinds of servers, when handling a request fora dynamic CGI document, pass the connections container to the CGI process. This may either be done byinheritance, for traditional CGI using a child process, orexplicitly, when persistent CGI server processes are used.If the dynamic processing is done in a module withinthe server process itself, the application simply binds itsthread to the appropriate container.A server may wish to assign different priorities to requests from different sources, even for processing thatoccurs in the kernel before the application sees the connection. This could be used to defend against somedenialofservice attacks, and could also be used by anISP to provide an enhanced class of service to users whohave paid a premium.To support this prioritization, we define a newsockaddr namespace that includes a filter specifying a set of foreign addresses, in addition to the usualInternet address and port number. Filters are specifiedas tuples consisting of a template address and a CIDRnetwork mask 36. The application uses the bindsystem call to bind multiple server sockets, each with thesame localaddress, localport tuple but with a different templateaddress, CIDRmask filter. The system uses these filters to assign requests from a particularclient, or set of clients, to the socket with a matching filter. By associating a different resource container witheach socket, the server application can assign differentpriorities to different sets of clients, prior to listening forand accepting new connections on these sockets. Onemight also want to be able to specify complement filters,to accept connections except from certain clients.The server can use the resource container associatedwith a listening socket to set the priority of acceptingnew connections relative to servicing the existing ones.In particular, to defend against a denialofservice attack from a specific set of clients, the server can create a socket whose filter matches this set, and then bindit to a resource container with a numeric priority ofzero. This requires the network infrastructure to rejectspoofed source addresses, a problem currently being addressed 33.A server administrator may wish to restrict the totalCPU consumption of certain classes of requests, such asCGI requests, requests from certain hosts, or requests forcertain resources. The application can do this by creating a container for each such class, setting its attributesappropriately e.g., limiting the total CPU usage of theclass, and then creating the resource container for eachindividual request as the child of the corresponding classspecific container.Because resource containers enable precise accounting for the costs of an activity, they may be useful toadministrators simply for sending accurate bills to customers, and for use in capacity planning.Resource containers are in some ways similar to manyresource management mechanisms that have been developed in the context of multimedia and realtime operating systems 17, 19, 22, 28, 31. Resource containers aredistinguished from these other mechanism by their generality, and their direct applicability to existing generalpurpose operating systems. See Section 6 for more discussion of this related work.5 PerformanceWe performed several experiments to evaluatewhether resource containers are an effective way for aWeb server to control resource consumption, and to provide robust and controlled service.5.1 Prototype implementationOur prototype was implemented as modifications tothe Digital UNIX 4.0D kernel. We changed the CPUscheduler, the resource management subsystem, and thenetwork subsystem to understand resource containers.We modified Digital UNIXs CPU scheduler scheduler to treat resource containers as its resource principals. A resource container can obtain a fixedshare guarantee from the scheduler within the CPU usage restrictions of its parent container, or can choose to timesharethe CPU resources granted to its parent container with itssibling containers. Fixedshare guarantees are ensuredfor timescales that are in the order of tens of seconds orlarger. Containers with fixedshare guarantees can havechild containers timeshare containers cannot have children. In our prototype, threads can only be bound to leaflevel containers.We changed the TCPIP subsystem to implement LRPstyle processing, treating resource containers as resourceprincipals. A perprocess kernel thread is used to perform processing of network packets in priority order oftheir containers. To ensure correct accounting, this threadsets its resource binding appropriately while processingeach packet.Implementing the container abstraction added 820lines of new code to the Digital UNIX kernel. About1730 lines of kernel code were changed and 4820 linesof code were added to integrate containers as the systems resource principals, and to implement LRPstylenetwork processing. Of these 6550 lines 1730  4820of integration code, 2342 lines 142 changed, 2200 newconcerned the CPU scheduler, 2136 lines 205 changed,1931 new were in the network subsystem, and the remainder were spread across the rest of the kernel.Code changes were small for all the server applications that we considered, though they were sometimesfairly pervasive throughout the application.5.2 Experimental environmentIn all experiments, the server was a Digital PersonalWorkstation 500au 500Mhz 21164, 8KB Icache, 8KBDcache, 96KB level 2 unified cache, 2MB level 3 unified cache, SPECint95  12.3, 128MB of RAM, runningour modified version of Digital UNIX 4.0D. The clientmachines were 166MHz Pentium Pro PCs, with 64MBof memory, and running FreeBSD 2.2.5. All experimentsran over a private 100Mbps switched Fast Ethernet.Our server software was a singleprocess eventdrivenprogram derived from thttpd 44. We started from amodified version of thttpd with numerous performanceimprovements, and changed it to optionally use resourcecontainers. Our clients used the SClient software 4.5.3 Baseline throughputWe measured the throughput of our HTTP server running on the unmodified kernel. When handling requestsfor small files 1 KByte that were in the filesystem cache,our server achieved a rate of 2954 requestssec. usingconnectionperrequest HTTP, and 9487 requestssec. using persistentconnection HTTP. These rates saturated theCPU, corresponding to perrequest CPU costs of 338sand 105s, respectively.5.4 Costs of new primitivesWe measured the costs of primitive operations on resource containers. For each new primitive, a userlevelprogram invoked the system call 10,000 times, measuredthe total elapsed time, and divided to obtain a meanwarmcache cost. The results, in Table 1, show thatall such operations have costs much smaller than that ofa single HTTP transaction. This implies that the use ofresource containers should add negligible overhead.Operation Cost screate resource container 2.36destroy resource container 2.10change threads resource binding 1.04obtain container resource usage 2.04setget container attributes 2.10move container between processes 3.15obtain handle for existing container 1.90Table 1 Cost of resource container primitives.We verified this by measuring the throughput of ourserver running on the modified kernel. In this test, theWeb server process created a new resource container foreach HTTP request. The throughput of the system remained effectively unchanged.5.5 Prioritized handling of clientsOur next experiment tested the effectiveness of resource containers in enabling prioritized handling ofclients by a Web server. We consider a scenario where aservers administrator wants to differentiate between twoclasses of clients for example, based on payment tariffs.Our experiment used an increasing number of lowpriority clients to saturate a server, while a single highpriority client made requests of the server. All requestswere for the same static 1KB file, with one request perconnection. We measured the response time perceivedby the highpriority client.Figure 11 shows the results. The yaxis shows the response time seen by the highpriority client Thigh as afunction of the number of concurrent lowpriorityclients.The dotted curve shows how Thigh varies when usingthe unmodified kernel. The application attempted to givepreference to requests from the highpriority client byhandling events on its socket, returned by select,before events on other sockets. The figures shows that,despite this preferential treatment, Thigh increases sharplywhen there are enough lowpriority clients to saturate theserver. This happens because most of request processingoccurs inside the kernel, and so is uncontrolled.RC System 2RC System 1LRP SystemUnmodified SystemNumber of concurrent CGI requestsHTTPThroughputrequestssec543210300025002000150010005000Fig. 12 Throughput with competing CGI requests.RC System 2RC System 1LRP SystemUnmodified SystemNumber of concurrent CGI requestsCPUshareofCGIprocessing543210100806040200Fig. 13 CPU share of CGI requests.Without containersWith containersselectWith containersnew event APINumber of concurrent lowpriority clientsResponsetimems353025201510509876543210Fig. 11 How Thigh varies with load.The dashed and the solid curve in Figure 11 showsthe effect of using resource containers. Here, the serveruses two containers, with different numeric priorities, assigning the highpriority requests to one container, andthe lowpriority requests to another. The dashed curve,labeled With containersselect, shows the effectof resource containers with the application still usingselect to wait for events. Thigh increases muchless than in the original system. Resource containers allow the application to control resource consumption atalmost all levels of the system. For example, TCPIPprocessing, which is performed in FIFO order in classical systems, is now performed in priority order.The remaining increase in response time is due to someknown scalability problems of the select systemcall 5, 6. These problems can be alleviated by a smartimplementation described in 6, but some inefficiencyis inherent to the semantics of the select API. Theproblem is that each call to selectmust specify, viaa bitmap, the complete set of descriptors that the application is interested in. The kernel must check the statusof each descriptor in this set. This causes overhead linearin the number of descriptors handled by the application.The solid curve, labeled With containersnew eventAPI, shows the variation in Thigh when the server usesa new scalable event API, described in 5. In thiscase, Thigh increases very slightly as the number of lowpriority clients increases. The remaining slight increasein Thigh reflects the cost of packetarrival interrupts fromlowpriority connections. The kernel must handle theseinterrupts and invoke a packet filter to determine the priority of the packet.5.6 Controlling resource usage of CGI processingSection 2 described how requests for dynamic resources are typically handled by processes other thanthe main Web server process. In a system that timeshares the CPU equally between processes, these backend CGI processes may gain an excessive share of theCPU, which reduces the throughput for static documents.We constructed an experiment to show how a server canuse resource containers to explicitly control the CPUcosts of CGI processes.We measured the throughput of our Web server forcached, 1 KB static documents while increasing the number of concurrent requests for a dynamic CGI resource.Each CGI request process consumed about 2 seconds ofCPU time. These results are shown in the curve labeledUnmodified System in Figure 12.As the number of concurrent CGI requests increases,the CPU is shared among a larger set of processes, andthe main Web servers share decreases this sharply reduces the throughput for static documents. For example, with only 4 concurrent CGI requests, the Web serveritself gets only 40 of the CPU, and the staticrequestthroughput drops to 44 of its maximum.The main server process actually gets slightly more ofthe CPU than does each CGI process, because of misaccounting for network processing. This is shown in Figure 13, which plots the total CPU time used by all CGIprocesses.In Figures 12 and 13, the curves labeled LRP System show the performance of an LRP version of DigitalUNIX. LRP fixes the misaccounting, so the main serverprocess shares the CPU equally with other processes.This further reduces the throughput for static documents.To measure how well resource containers allow finegrained control over CGI processes, we modified ourserver so that each container created for a CGI requestwas the child of a specific CGIparent container. ThisCGIparent container was restricted to a maximum fraction of the CPU recall that this restriction includes itschildren. In Figures 12 and 13, the curves labeled RCSystem 1 show the performance when the CGIparentcontainer was limited to 30 of the CPU the curves labeled RC System 2 correspond to a limit of 10.Figure 13 shows that the CPU limits are enforced almost exactly. Figure 12 shows that this effectively formsa resource sandbox around the CGI processes, and sothe throughput of static requests remains almost constantas the number of concurrent CGI requests increases from1 to 5.Note that the Web server could additionally imposerelative priorities among the CGI requests, by adjustingthe resource limits on each corresponding container.5.7 Immunity against SYNfloodingWe constructed an experiment to determine if resourcecontainers, combined with the filtering mechanism described in Section 4.7, allow a server to protect againstdenialofservice attacks using SYNflooding. In thisexperiment, a set of malicious clients sent bogus SYNpackets to the servers HTTP port, at a high rate. We thenmeasured the servers throughput for requests from wellbehaved clients for a cached, 1 KB static document.Figure 14 shows that the throughput of the unmodifiedsystem falls drastically as the SYNflood rate increases,and is effectively zero at about 10,000 SYNssec. Wemodified the kernel to notify the application when itdrops a SYN due to queue overflow. We also modified our server to isolate the misbehaving clients to alowpriority listensocket, using the filter mechanism described in Section 4.8. With these modifications, evenat 70,000 SYNssec., the useful throughput remains atabout 73 of maximum. This slight degradation resultsfrom the interrupt overhead of the SYN flood. Note thatLRP, in contrast to our system, cannot protect againstsuch SYN floods it cannot filter traffic to a given portbased on the source address.Unmodified SystemWith Resource ContainersSYNFlood Rate 1000s of SYNssecHTTPThroughputrequestssec706050403020100300025002000150010005000Fig. 14 Server behavior under SYNflooding attack.5.8 Isolation of virtual serversSection 5.6 shows how resource containers allow resource sandboxes to be put around CGI processes. Thisapproach can be used in other applications, such as controlling the total resource usage of guest servers in a RentAServer 45 environment.In current operating systems, each guest server, whichmight consist of many processes, can appear to the system as numerous resource principals. The number mayvary dynamically, and has little relation to how muchCPU time the servers administrator wishes to allow eachguest server.We performed an informal experiment to show howresource containers solve this problem. We created 3toplevel containers and restricted their CPU consumption to fixed CPU shares. Each container was then usedas the root container for a guest server. Subsequently,three sets of clients placed varying request loads on theseservers the requests included CGI resources. We observed that the total CPU time consumed by each guestserver exactly matched its allocation. Moreover, becausethe resource container hierarchy is recursive, each guestserver can itself control how its allocated resources areredivided among competing connections.6 Related WorkMany mechanisms have been developed to supportfinegrained research management. Here, we contrastthese with our resource container abstraction.The Scout operating system 31 is based on the pathabstraction, representing an IO channel such as a TCPconnection through a multilayered system. A path encapsulates the specific attributes of an IO channel, andallows access to these attributes across layers. Paths havebeen used to implement finegrained resource management in network appliances, including Web server appliances 40. Resource containers, in contrast to paths,allow the application to treat the resources consumed byseveral IO channels as being part of the same activity.Moreover, the composition of a path is limited by therouter graph specified at kernelbuild time resource containers encompass arbitrary sets of resources at runtime.Mercer et al. 28 introduced the reserve abstraction inthe context of RealTime Mach. Reserves insulate programs from the timing and execution characteristics ofother programs. An application can reserve system resources, and the system ensures that these resources willbe available, when needed, to threads associated with thereserve. Like a resource container, a reserve provides athread with a resource context, may be passed betweenprotection domains, and may be bound to one thread ormultiple threads. Thus, reserves can be used to chargeto one resource principal the resources consumed by anactivity distributed across protection domains. Unlike resource containers, reserves neither account for, nor control, kernelmode processing on behalf of an activity RTMach is a microkernel system, so network processing isdone in user mode 25. Moreover, resources containerscan be structured hierarchically and can manage systemresources other than CPU.The activity abstraction in Rialto 22 is similar to resource containers. Like a resource container, an activitycan account for resource consumption both across protection domains and at a granularity smaller than a protection domain. However, Rialto is an experimental realtime objectoriented operating system and was designedfrom scratch for resource accountability. In contrast toScout, RT Mach and Rialto, our work aimed at developing a resource accounting mechanism for traditionalUNIX systems with minimal disruption to existing APIsand implementations.The migrating threads of Mach 17 and AlphaOS 13, and the shuttles of Spring 19 allow the resource consumption of a thread or a shuttle performinga particular independent activity to be charged to the correct resource management entity, even when the threador shuttle moves across protection domains. However,these systems do not separate the concepts of thread andresource principal, and so cannot correctly handle applications in which a single thread is associated with multiple independent activities, such as an eventdriven Webserver. Mach and Spring are also microkernel systems,and so do not raise the issue of accounting for kernelmode network processing.The reservation domains 7 of Eclipse and the Software Performance Units of Verghese et al. 46 allow theresource consumption of a group of processes to be considered together for the purpose of scheduling. Theseabstractions allow a resource principal to encompass anumber of protection domains unlike resource containers, neither abstraction addresses scenarios, such a singleprocess Web server, where the natural extent of a resource principal is more complicated.A number of mainframe operating systems 14, 37,12 provide resource management at a granularity otherthan a process. These systems allow a group of processese.g. all processes owned by a given user to be treated asa single resource principal in this regard, they are similarto resource containers. Unlike our work, however, thereare no provisions for resource accounting at a granularity smaller than a process. These systems account andlimit the resources consumed by a process group overlong periods of time on the order of hundreds of minutes or longer. Resource containers, on the other hand,can support policies for finegrained, shortterm resourcescheduling, including realtime policies.The resource container hierarchy is similar to otherhierarchical structures described in the scheduling literature 18, 48. These hierarchical scheduling algorithmsare complementary to resource containers, and could beused to schedule threads according to the resource container hierarchy.The exokernel approach 23 gives application software as much control as possible over raw system resources. Functions implemented by traditional operatingsystems are instead provided in usermode libraries. Ina network server built using an exokernel, the application controls essentially all of the protocol stack, including the device drivers the storage system is similarly exposed. The application can therefore directly control theresource consumption for all of its network and file IO.It seems feasible to implement the resource containerabstraction as a feature of an exokernel library operating system, since the exokernel delegates most resourcemanagement to user code.Almeida et al. 1 attempted to implement QoS support in a modified Apache 3 Web server, running ona generalpurpose monolithic operating system. Apacheuses a process for each connection, and so they mappedQoS requirements onto numeric process priorities, experimenting both with a fully userlevel implementation,and with a slightly modified Linux kernel scheduler. Theywere able to provide differentiated HTTP service to different QoS classes. However, the effectiveness of thistechnique was limited by their inability to control kernelmode resource consumption, or to differentiate betweenexisting connections and new connection requests. Also,this approach does not extend to eventdriven servers.Several researchers have studied the problem of controllingkernelmode network processing. Mogul and Ramakrishnan 30 improved the overload behavior of abusy system by converting interruptdriven processinginto explicitlyscheduled processing. Lazy Receiver Processing LRP 15 extended this by associating receivedpackets as early as possible with the receiving process,and then performed their subsequent processing basedon that processs scheduling priority. Resource containers generalize this idea, by separating the concept of aresource principal from that of a protection domain.7 ConclusionWe introduced the resource container, an operatingsystem abstraction to explicitly identify a resource principal. Resource containers allow explicit and finegrainedcontrol over resource consumption at all levels in the system. Performance evaluations demonstrate that resourcecontainers allow a Web server to closely control the relative priority of connections and the combined CPU usage of various classes of requests. Together with a newsockaddr namespace, resource containers provide immunity against certain types of denial of service attacks.Our experience suggests that containers can be used toaddress a large variety of resource management scenarios beyond servers for instance, we expect that containerhierarchies are effective in controlling resource usage inmultiuser systems and workstation farms.AcknowledgmentsWe are grateful to Deborah Wallach, Carl Waldspurger, Willy Zwaenepoel, our OSDI shepherd MikeJones, and the anonymous reviewers, whose commentshave helped to improve this paper. This work wassupported in part by NSF Grants CCR9803673, CCR9503098, and by Texas TATP Grant 003604.References1 J. Almeida, M. Dabu, A. Manikutty, and P. Cao.Providing Differentiated QualityofService in WebHosting Services. In Proc. Workshop on InternetServer Performance, June 1998.2 E. W. Anderson and J. Pasquale. The Performanceof the Container Shipping IO System. In Proc. Fifteenth ACM Symposium on Operating System Principles, Dec. 1995.3 Apache. httpwww.apache.org.4 G. Banga and P. Druschel. Measuring the Capacityof a Web Server. In Proc. 1997 USENIX Symp. onInternet Technologies and Systems, Dec. 1997.5 G. Banga, P. Druschel, and J. C. Mogul. Better operating system features for faster networkservers. In Proc. Workshop on Internet Server Performance, June 1998. Condensed version appearsin ACM SIGMETRICS Performance EvaluationReview 2632330, Dec. 1998.6 G. Banga and J. C. Mogul. Scalable kernel performance for Internet servers under realistic loads.In Proc. 1998 USENIX Technical Conference, June1998.7 J. Bruno, E. Gabber, B. Ozden, and A. Silberschatz.The Eclipse Operating System Providing Qualityof Service via Reservation Domains. In Proc. 1998USENIX Technical Conference, June 1998.8 M. Burrows. Personal communication, Mar. 1998.9 P. Cao. Application Controlled File Caching andPrefetching. PhD thesis, Princeton University, Jan.1996.10 The Common Gateway Interface. httphoohoo.ncsa.uiuc.educgi.11 A. Chankhunthod, P. B. Danzig, C. Neerdaels,M. F. Schwartz, and K. J. Worrell. A Hierarchical Internet Object Cache. In Proc. 1996 USENIXTechnical Conference, Jan. 1996.12 D. Chess and G. Waldbaum. The VM370 resource limiter. IBM Systems Journal, 204424437, 1981.13 R. K. Clark, E. D. Jensen, and F. D. Reynolds. AnArchitectural Overview of The Alpha RealTimeDistributed Kernel. In Workshop on MicroKernelsand Other Kernel Architectures, Apr. 1992.14 P. Denning. Third generation computer systems. ACM Computing Surveys, 34175216,Dec. 1971.15 P. Druschel and G. Banga. Lazy Receiver Processing LRP A Network Subsystem Architecture forServer Systems. In Proc. 2nd Symp. on OperatingSystems Design and Implementation, Oct. 1996.16 Open Market. FastCGI Specification.httpwww.fastcgi.com.17 B. Ford and J. Lepreau. Evolving Mach 3.0 toa migrating thread model. In Proc. 1994 WinterUSENIX Conference, Jan. 1994.18 P. Goyal, X. Guo, and H. M. Vin. A HierarchicalCPU Scheduler for Multimedia Operating Systems.In Proc. 2nd Symp. on Operating Systems Designand Implementation, Oct. 1996.19 G. Hamilton and P. Kougiouris. The Spring nucleus A microkernel for objects. In Proc. 1993Summer USENIX Conference, June 1993.20 K. Harty and D. R. Cheriton. ApplicationControlled Physical Memory using External PageCache Replacement. In Proc. of the 5th Intl Conf.on Architectural Support for Programming Languages and Operating Systems, Oct. 1992.21 J. C. Hu, I. Pyrali, and D. C. Schmidt. Measuringthe impact of event dispatching and concurrencymodels on web server performance over highspeednetworks. In Proc. 2nd Global Internet Conf., Nov.1997.22 M. B. Jones, P. J. Leach, R. P. Draves, and J. S. Barrera. Modular realtime resource management inthe Rialto operating system. In Proc. 5th Workshopon Hot Topics in Operating Systems, May 1995.23 M. F. Kaashoek, D. R. Engler, G. R. Ganger,H. Briceno, R. Hunt, D. Mazieres, T. Pinckney,R. Grimm, J. Janotti, and K. Mackenzie. Application performance and flexibility on Exokernel systems. In Proc. 16th Symp. on Operating SystemPrinciples, Oct. 1997.24 K. Krueger, D. Loftesness, A. Vahdat, and T. Anderson. Tools for the Development of ApplicationSpecific Virtual Memory Management. In Proc.8th Annual Conf. on ObjectOriented ProgrammingSystems, Languages, and Applications, Oct. 1993.25 C. Lee, K. Yoshida, C. Mercer, and R. Rajkumar.Predictable communication protocol processing inrealtime Mach. In In Proc. IEEE Realtime Technology and Applications Symp., June 1996.26 P. E. McKenney and K. F. Dove. Efficient Demultiplexing of Incoming TCP Packets. In Proceedingsof the SIGCOMM 92 Conference, Aug. 1993.27 D. McNamee and K. Armstrong. Extending theMach External Pager Interface to AccomodateUserLevel Page Replacement Policies. In Proc.USENIX Mach Symp., Oct. 1990.28 C. W. Mercer, S. Savage, and H. Tokuda. ProcessorCapacity Reserves for Multimedia Operating Systems. In Proc. of the IEEE Intl Conf. on Multimedia Computing and Systems, May 1994.29 Microsoft Corporation ISAPI Overview.httpwww.microsoft.commsdnsdkplatformsdocsdkinternetsrcisapimrg.htm.30 J. C. Mogul and K. K. Ramakrishnan. Eliminating Receive Livelock in an Interruptdriven Kernel.ACM Trans. on Computer Systems, 153217252,Aug. 1997.31 D. Mosberger and L. L. Peterson. Making pathsexplicit in the scout operating system. In Proc.2nd Symp. on Operating Systems Design and Implementation, Oct. 1996.32 NCSA httpd. httphoohoo.ncsa.uiuc.edu.33 North American Network Operators GroupNANOG. Mailing List Archives, Thread 01974.httpwww.merit.edumail.archiveshtmlnanogthreads.html01974, Apr. 1998.34 Netscape Server API. httpwww.netscape.comnewsrefstdserver api.html.35 V. S. Pai, P. Druschel, and W. Zwaenepoel. IOLite A unified IO buffering and caching system.In Proc. 3rd Symp. on Operating Systems Designand Implementation, Feb. 1999.36 Y. Rekhter and T. Li. An Architecture for IP Address Allocation with CIDR. RFC 1518, Sept.1993.37 R. Schardt. An MVS tuning approach OS problem solving. IBM Systems Journal, 191102119, 1980.38 S. E. Schechte and J. Sutaria. A Study of the Effectsof Context Switching and Caching on HTTP ServerPerformance. httpwww.eecs.harvard.edustuartTarantulaFirstPaper.html.39 J. Semke and J. M. M. Mathis. Automatic TCPBuffer Tuning. In Proc. SIGCOMM 98 Conference, Sept. 1998.40 O. Spatscheck and L. L. Petersen. DefendingAgainst Denial of Service Attacks in Scout. InProc. 3rd Symp. on Operating Systems Design andImplementation, Feb. 1999.41 Squid. httpsquid.nlanr.netSquid.42 W. Stevens. TCPIP Illustrated Volume 3. AddisonWesley, Reading, MA, 1996.43 M. N. Thadani and Y. A. Khalidi. An efficient zerocopy IO framework for UNIX. Technical ReportSMLI TR9539, Sun Microsystems Laboratories,Inc., May 1995.44 thttpd. httpwww.acme.comsoftwarethttpd.45 A. Vahdat, E. Belani, P. Eastham, C. Yoshikawa,T. Anderson, D. Culler, and M. Dahlin. WebOSOperating System Services For Wide Area Applications. In Proc. Seventh Symp. on High PerformanceDistributed Computing, July 1998.46 B. Verghese, A. Gupta, and M. Rosenblum. Performance Isolation Sharing and Isolation in SharedMemory Multiprocessors. In Proc. 8th Intl. Conf.on Architectural Support for Programming Languages and Operating Systems, Oct. 1998.47 C. A. Waldspurger. Lottery and Stride Scheduling Flexible ProportionalShare Resource Managament. PhD thesis, Massachusetts Institute ofTechnology, Sept. 1995.48 C. A. Waldspurger and W. E. Weihl. LotteryScheduling Flexible ProportionalShare ResourceManagement. In Proc. Symp. on Operating SystemsDesign and Implementation, Nov. 1994.49 Zeus. httpwww.zeus.co.uk.
