A Fast File System for UNIXMarshall Kirk McKusick, William N. Joy,Samuel J. Leffler, Robert S. FabryComputer Systems Research GroupComputer Science DivisionDepartment of Electrical Engineering and Computer ScienceUniversity of California, BerkeleyBerkeley, CA 94720ABSTRACTA reimplementation of the UNIX file system is described. The reimplementationprovides substantially higher throughput rates by using more flexible allocation policiesthat allow better locality of reference and can be adapted to a wide range of peripheraland processor characteristics. The new file system clusters data that is sequentiallyaccessed and provides two block sizes to allow fast access to large files while not wastinglarge amounts of space for small files. File access rates of up to ten times faster than thetraditional UNIX file system are experienced. Long needed enhancements to the programmers interface are discussed. These include a mechanism to place advisory lockson files, extensions of the name space across file systems, the ability to use long filenames, and provisions for administrative control of resource usage.Revised February 18, 1984CR Categories and Subject Descriptors D.4.3 Operating Systems File Systems Management  fileorganization, directory structures, access methods D.4.2 Operating Systems Storage Management allocationdeallocation strategies, secondary storage devices D.4.8 Operating Systems Performance measurements, operational analysis H.3.2 Information Systems Information Storage  file organizationAdditional Keywords and Phrases UNIX, file system organization, file system performance, file systemdesign, application program interface.General Terms file system, measurement, performance. UNIX is a trademark of Bell Laboratories. William N. Joy is currently employed by Sun Microsystems, Inc, 2550 Garcia Avenue, Mountain View, CA94043 Samuel J. Leffler is currently employed by Lucasfilm Ltd., PO Box 2009, San Rafael, CA 94912This work was done under grants from the National Science Foundation under grant MCS8005144, and theDefense Advance Research Projects Agency DoD under ARPA Order No. 4031 monitored by Naval Electronic System Command under Contract No. N0003982C0235.SMM052 A Fast File System for UNIXTABLE OF CONTENTS1. Introduction2. Old file system3. New file system organization3.1. Optimizing storage utilization3.2. File system parameterization3.3. Layout policies4. Performance5. File system functional enhancements5.1. Long file names5.2. File locking5.3. Symbolic links5.4. Rename5.5. QuotasAcknowledgementsReferences1. IntroductionThis paper describes the changes from the original 512 byte UNIX file system to the new onereleased with the 4.2 Berkeley Software Distribution. It presents the motivations for the changes, the methods used to effect these changes, the rationale behind the design decisions, and a description of the newimplementation. This discussion is followed by a summary of the results that have been obtained, directions for future work, and the additions and changes that have been made to the facilities that are availableto programmers.The original UNIX system that runs on the PDP11 has simple and elegant file system facilities.File system inputoutput is buffered by the kernel there are no alignment constraints on data transfers andall operations are made to appear synchronous. All transfers to the disk are in 512 byte blocks, which canbe placed arbitrarily within the data area of the file system. Virtually no constraints other than availabledisk space are placed on file growth Ritchie74, Thompson78.When used on the VAX11 together with other UNIX enhancements, the original 512 byte UNIX filesystem is incapable of providing the data throughput rates that many applications require. For example,applications such as VLSI design and image processing do a small amount of processing on a large quantities of data and need to have a high throughput from the file system. High throughput rates are also neededby programs that map files from the file system into large virtual address spaces. Paging data in and out ofthe file system is likely to occur frequently Ferrin82b. This requires a file system providing higher bandwidth than the original 512 byte UNIX one that provides only about two percent of the maximum diskbandwidth or about 20 kilobytes per second per arm White80, Smith81b.Modifications have been made to the UNIX file system to improve its performance. Since the UNIXfile system interface is well understood and not inherently slow, this development retained the abstractionand simply changed the underlying implementation to increase its throughput. Consequently, users of thesystem have not been faced with massive software conversion.Problems with file system performance have been dealt with extensively in the literature seeSmith81a for a survey. Previous work to improve the UNIX file system performance has been done byFerrin82a. The UNIX operating system drew many of its ideas from Multics, a large, high performance DEC, PDP, VAX, MASSBUS, and UNIBUS are trademarks of Digital Equipment Corporation. In practice, a files size is constrained to be less than about one gigabyte.A Fast File System for UNIX SMM053operating system Feiertag71. Other work includes Hydra Almes78, Spice Thompson80, and a file system for a LISP environment Symbolics81. A good introduction to the physical latencies of disks isdescribed in Pechura83.2. Old File SystemIn the file system developed at Bell Laboratories the traditional file system, each disk drive isdivided into one or more partitions. Each of these disk partitions may contain one file system. A file system never spans multiple partitions. A file system is described by its superblock, which contains thebasic parameters of the file system. These include the number of data blocks in the file system, a count ofthe maximum number of files, and a pointer to the free list, a linked list of all the free blocks in the file system.Within the file system are files. Certain files are distinguished as directories and contain pointers tofiles that may themselves be directories. Every file has a descriptor associated with it called an inode. Aninode contains information describing ownership of the file, time stamps marking last modification andaccess times for the file, and an array of indices that point to the data blocks for the file. For the purposesof this section, we assume that the first 8 blocks of the file are directly referenced by values stored in aninode itself. An inode may also contain references to indirect blocks containing further data block indices.In a file system with a 512 byte block size, a singly indirect block contains 128 further block addresses, adoubly indirect block contains 128 addresses of further singly indirect blocks, and a triply indirect blockcontains 128 addresses of further doubly indirect blocks.A 150 megabyte traditional UNIX file system consists of 4 megabytes of inodes followed by 146megabytes of data. This organization segregates the inode information from the data thus accessing a filenormally incurs a long seek from the files inode to its data. Files in a single directory are not typicallyallocated consecutive slots in the 4 megabytes of inodes, causing many nonconsecutive blocks of inodes tobe accessed when executing operations on the inodes of several files in a directory.The allocation of data blocks to files is also suboptimum. The traditional file system never transfersmore than 512 bytes per disk transaction and often finds that the next sequential data block is not on thesame cylinder, forcing seeks between 512 byte transfers. The combination of the small block size, limitedreadahead in the system, and many seeks severely limits file system throughput.The first work at Berkeley on the UNIX file system attempted to improve both reliability andthroughput. The reliability was improved by staging modifications to critical file system information sothat they could either be completed or repaired cleanly by a program after a crash Kow alski78. The filesystem performance was improved by a factor of more than two by changing the basic block size from 512to 1024 bytes. The increase was because of two factors each disk transfer accessed twice as much data,and most files could be described without need to access indirect blocks since the direct blocks containedtwice as much data. The file system with these changes will henceforth be referred to as the old file system.This performance improvement gav e a strong indication that increasing the block size was a goodmethod for improving throughput. Although the throughput had doubled, the old file system was still usingonly about four percent of the disk bandwidth. The main problem was that although the free list was initially ordered for optimal access, it quickly became scrambled as files were created and removed. Eventually the free list became entirely random, causing files to have their blocks allocated randomly over thedisk. This forced a seek before every block access. Although old file systems provided transfer rates of upto 175 kilobytes per second when they were first created, this rate deteriorated to 30 kilobytes per secondafter a few weeks of moderate use because of this randomization of data block placement. There was noway of restoring the performance of an old file system except to dump, rebuild, and restore the file system.Another possibility, as suggested by Maruyama76, would be to have a process that periodically By partition here we refer to the subdivision of physical space on a disk drive. In the traditional file system, as in the new file system, file systems are really located in logical disk partitions that may overlap. Thisoverlapping is made available, for example, to allow programs to copy entire disk drives containing multiple filesystems. The actual number may vary from system to system, but is usually in the range 513.SMM054 A Fast File System for UNIXreorganized the data on the disk to restore locality.3. New file system organizationIn the new file system organization as in the old file system organization, each disk drive containsone or more file systems. A file system is described by its superblock, located at the beginning of the filesystems disk partition. Because the superblock contains critical data, it is replicated to protect againstcatastrophic loss. This is done when the file system is created since the superblock data does not change,the copies need not be referenced unless a head crash or other hard disk error causes the default superblockto be unusable.To insure that it is possible to create files as large as 232 bytes with only two lev els of indirection, theminimum size of a file system block is 4096 bytes. The size of file system blocks can be any power of twogreater than or equal to 4096. The block size of a file system is recorded in the file systems superblock soit is possible for file systems with different block sizes to be simultaneously accessible on the same system.The block size must be decided at the time that the file system is created it cannot be subsequently changedwithout rebuilding the file system.The new file system organization divides a disk partition into one or more areas called cylindergroups. A cylinder group is comprised of one or more consecutive cylinders on a disk. Associated witheach cylinder group is some bookkeeping information that includes a redundant copy of the superblock,space for inodes, a bit map describing available blocks in the cylinder group, and summary informationdescribing the usage of data blocks within the cylinder group. The bit map of available blocks in the cylinder group replaces the traditional file systems free list. For each cylinder group a static number of inodesis allocated at file system creation time. The default policy is to allocate one inode for each 2048 bytes ofspace in the cylinder group, expecting this to be far more than will ever be needed.All the cylinder group bookkeeping information could be placed at the beginning of each cylindergroup. However if this approach were used, all the redundant information would be on the top platter. Asingle hardware failure that destroyed the top platter could cause the loss of all redundant copies of thesuperblock. Thus the cylinder group bookkeeping information begins at a varying offset from the beginning of the cylinder group. The offset for each successive cylinder group is calculated to be about one trackfurther from the beginning of the cylinder group than the preceding cylinder group. In this way the redundant information spirals down into the pack so that any single track, cylinder, or platter can be lost withoutlosing all copies of the superblock. Except for the first cylinder group, the space between the beginning ofthe cylinder group and the beginning of the cylinder group information is used for data blocks.3.1. Optimizing storage utilizationData is laid out so that larger blocks can be transferred in a single disk transaction, greatly increasingfile system throughput. As an example, consider a file in the new file system composed of 4096 byte datablocks. In the old file system this file would be composed of 1024 byte blocks. By increasing the blocksize, disk accesses in the new file system may transfer up to four times as much information per disk transaction. In large files, several 4096 byte blocks may be allocated from the same cylinder so that even largerdata transfers are possible before requiring a seek.The main problem with larger blocks is that most UNIX file systems are composed of many smallfiles. A uniformly large block size wastes space. Table 1 shows the effect of file system block size on theamount of wasted space in the file system. The files measured to obtain these figures reside on one of our While it appears that the first cylinder group could be laid out with its superblock at the known location,this would not work for file systems with blocks sizes of 16 kilobytes or greater. This is because of a requirement that the first 8 kilobytes of the disk be reserved for a bootstrap program and a separate requirement that thecylinder group information begin on a file system block boundary. To start the cylinder group on a file systemblock boundary, file systems with block sizes larger than 8 kilobytes would have to leave an empty spacebetween the end of the boot block and the beginning of the cylinder group. Without knowing the size of the filesystem blocks, the system would not know what roundup function to use to find the beginning of the first cylinder group.A Fast File System for UNIX SMM055time sharing systems that has roughly 1.2 gigabytes of online storage. The measurements are based on theactive user file systems containing about 920 megabytes of formatted space.Space used  waste Organization775.2 Mb 0.0 Data only, no separation between files807.8 Mb 4.2 Data only, each file starts on 512 byte boundary828.7 Mb 6.9 Data  inodes, 512 byte block UNIX file system866.5 Mb 11.8 Data  inodes, 1024 byte block UNIX file system948.5 Mb 22.4 Data  inodes, 2048 byte block UNIX file system1128.3 Mb 45.6 Data  inodes, 4096 byte block UNIX file systemTable 1  Amount of wasted space as a function of block size.The space wasted is calculated to be the percentage of space on the disk not containing user data. As theblock size on the disk increases, the waste rises quickly, to an intolerable 45.6 waste with 4096 byte filesystem blocks.To be able to use large blocks without undue waste, small files must be stored in a more efficient way.The new file system accomplishes this goal by allowing the division of a single file system block into oneor more fragments. The file system fragment size is specified at the time that the file system is createdeach file system block can optionally be broken into 2, 4, or 8 fragments, each of which is addressable. Thelower bound on the size of these fragments is constrained by the disk sector size, typically 512 bytes. Theblock map associated with each cylinder group records the space available in a cylinder group at the fragment level to determine if a block is available, aligned fragments are examined. Figure 1 shows a piece ofa map from a 40961024 file system.Bits in map XXXX XXOO OOXX OOOOFragment numbers 03 47 811 1215Block numbers 0 1 2 3Figure 1  Example layout of blocks and fragments in a 40961024 file system.Each bit in the map records the status of a fragment an X shows that the fragment is in use, while a Oshows that the fragment is available for allocation. In this example, fragments 05, 10, and 11 are in use,while fragments 69, and 1215 are free. Fragments of adjoining blocks cannot be used as a full block,ev en if they are large enough. In this example, fragments 69 cannot be allocated as a full block only fragments 1215 can be coalesced into a full block.On a file system with a block size of 4096 bytes and a fragment size of 1024 bytes, a file is represented by zero or more 4096 byte blocks of data, and possibly a single fragmented block. If a file systemblock must be fragmented to obtain space for a small amount of data, the remaining fragments of the blockare made available for allocation to other files. As an example consider an 11000 byte file stored on a40961024 byte file system. This file would uses two full size blocks and one three fragment portion ofanother block. If no block with three aligned fragments is available at the time the file is created, a full sizeblock is split yielding the necessary fragments and a single unused fragment. This remaining fragment canbe allocated to another file as needed.Space is allocated to a file when a program does a write system call. Each time data is written to afile, the system checks to see if the size of the file has increased. If the file needs to be expanded to holdthe new data, one of three conditions exists1 There is enough space left in an already allocated block or fragment to hold the new data. The newdata is written into the available space.2 The file contains no fragmented blocks and the last block in the file contains insufficient space tohold the new data. If space exists in a block already allocated, the space is filled with new data. Ifthe remainder of the new data contains more than a full block of data, a full block is allocated and the A program may be overwriting data in the middle of an existing file in which case space would already havebeen allocated.SMM056 A Fast File System for UNIXfirst full block of new data is written there. This process is repeated until less than a full block ofnew data remains. If the remaining new data to be written will fit in less than a full block, a blockwith the necessary fragments is located, otherwise a full block is located. The remaining new data iswritten into the located space.3 The file contains one or more fragments and the fragments contain insufficient space to hold the newdata. If the size of the new data plus the size of the data already in the fragments exceeds the size ofa full block, a new block is allocated. The contents of the fragments are copied to the beginning ofthe block and the remainder of the block is filled with new data. The process then continues as in 2above. Otherwise, if the new data to be written will fit in less than a full block, a block with the necessary fragments is located, otherwise a full block is located. The contents of the existing fragmentsappended with the new data are written into the allocated space.The problem with expanding a file one fragment at a a time is that data may be copied many times asa fragmented block expands to a full block. Fragment reallocation can be minimized if the user programwrites a full block at a time, except for a partial block at the end of the file. Since file systems with different block sizes may reside on the same system, the file system interface has been extended to provide application programs the optimal size for a read or write. For files the optimal size is the block size of the filesystem on which the file is being accessed. For other objects, such as pipes and sockets, the optimal size isthe underlying buffer size. This feature is used by the Standard InputOutput Library, a package used bymost user programs. This feature is also used by certain system utilities such as archivers and loaders thatdo their own input and output management and need the highest possible file system bandwidth.The amount of wasted space in the 40961024 byte new file system organization is empiricallyobserved to be about the same as in the 1024 byte old file system organization. A file system with 4096byte blocks and 512 byte fragments has about the same amount of wasted space as the 512 byte blockUNIX file system. The new file system uses less space than the 512 byte or 1024 byte file systems forindexing information for large files and the same amount of space for small files. These savings are offsetby the need to use more space for keeping track of available free blocks. The net result is about the samedisk utilization when a new file systems fragment size equals an old file systems block size.In order for the layout policies to be effective, a file system cannot be kept completely full. For eachfile system there is a parameter, termed the free space reserve, that gives the minimum acceptable percentage of file system blocks that should be free. If the number of free blocks drops below this level only thesystem administrator can continue to allocate blocks. The value of this parameter may be changed at anytime, even when the file system is mounted and active. The transfer rates that appear in section 4 weremeasured on file systems kept less than 90 full a reserve of 10. If the number of free blocks falls tozero, the file system throughput tends to be cut in half, because of the inability of the file system to localizeblocks in a file. If a file systems performance degrades because of overfilling, it may be restored byremoving files until the amount of free space once again reaches the minimum acceptable level. Accessrates for files created during periods of little free space may be restored by moving their data once enoughspace is available. The free space reserve must be added to the percentage of waste when comparing theorganizations given in Table 1. Thus, the percentage of waste in an old 1024 byte UNIX file system isroughly comparable to a new 4096512 byte file system with the free space reserve set at 5. Compare11.8 wasted with the old file system to 6.9 waste  5 reserved space in the new file system.3.2. File system parameterizationExcept for the initial creation of the free list, the old file system ignores the parameters of the underlying hardware. It has no information about either the physical characteristics of the mass storage device,or the hardware that interacts with it. A goal of the new file system is to parameterize the processor capabilities and mass storage characteristics so that blocks can be allocated in an optimum configurationdependent way. Parameters used include the speed of the processor, the hardware support for mass storage transfers, and the characteristics of the mass storage devices. Disk technology is constantly improving and agiven installation can have sev eral different disk technologies running on a single processor. Each file system is parameterized so that it can be adapted to the characteristics of the disk on which it is placed.For mass storage devices such as disks, the new file system tries to allocate new blocks on the samecylinder as the previous block in the same file. Optimally, these new blocks will also be rotationally wellA Fast File System for UNIX SMM057positioned. The distance between rotationally optimal blocks varies greatly it can be a consecutiveblock or a rotationally delayed block depending on system characteristics. On a processor with aninputoutput channel that does not require any processor intervention between mass storage transferrequests, two consecutive disk blocks can often be accessed without suffering lost time because of an intervening disk revolution. For processors without inputoutput channels, the main processor must field aninterrupt and prepare for a new disk transfer. The expected time to service this interrupt and schedule anew disk transfer depends on the speed of the main processor.The physical characteristics of each disk include the number of blocks per track and the rate at whichthe disk spins. The allocation routines use this information to calculate the number of millisecondsrequired to skip over a block. The characteristics of the processor include the expected time to service aninterrupt and schedule a new disk transfer. Giv en a block allocated to a file, the allocation routines calculate the number of blocks to skip over so that the next block in the file will come into position under thedisk head in the expected amount of time that it takes to start a new disk transfer operation. For programsthat sequentially access large amounts of data, this strategy minimizes the amount of time spent waiting forthe disk to position itself.To ease the calculation of finding rotationally optimal blocks, the cylinder group summary information includes a count of the available blocks in a cylinder group at different rotational positions. Eight rotational positions are distinguished, so the resolution of the summary information is 2 milliseconds for a typical 3600 revolution per minute drive. The superblock contains a vector of lists called rotational layouttables. The vector is indexed by rotational position. Each component of the vector lists the index into theblock map for every data block contained in its rotational position. When looking for an allocatable block,the system first looks through the summary counts for a rotational position with a nonzero block count. Itthen uses the index of the rotational position to find the appropriate list to use to index through only the relevant parts of the block map to find a free block.The parameter that defines the minimum number of milliseconds between the completion of a datatransfer and the initiation of another data transfer on the same cylinder can be changed at any time, evenwhen the file system is mounted and active. If a file system is parameterized to lay out blocks with a rotational separation of 2 milliseconds, and the disk pack is then moved to a system that has a processor requiring 4 milliseconds to schedule a disk operation, the throughput will drop precipitously because of lost diskrevolutions on nearly every block. If the eventual target machine is known, the file system can be parameterized for it even though it is initially created on a different processor. Even if the move is not known inadvance, the rotational layout delay can be reconfigured after the disk is moved so that all further allocationis done based on the characteristics of the new host.3.3. Layout policiesThe file system layout policies are divided into two distinct parts. At the top level are global policiesthat use file system wide summary information to make decisions regarding the placement of new inodesand data blocks. These routines are responsible for deciding the placement of new directories and files.They also calculate rotationally optimal block layouts, and decide when to force a long seek to a new cylinder group because there are insufficient blocks left in the current cylinder group to do reasonable layouts.Below the global policy routines are the local allocation routines that use a locally optimal scheme to layout data blocks.Tw o methods for improving file system performance are to increase the locality of reference to minimize seek latency as described by Trivedi80, and to improve the layout of data to make larger transferspossible as described by Nevalainen77. The global layout policies try to improve performance by clustering related information. They cannot attempt to localize all data references, but must also try to spreadunrelated data among different cylinder groups. If too much localization is attempted, the local cylindergroup may run out of space forcing the data to be scattered to nonlocal cylinder groups. Taken to anextreme, total localization can result in a single huge cluster of data resembling the old file system. Theglobal policies try to balance the two conflicting goals of localizing data that is concurrently accessed whilespreading out unrelated data.One allocatable resource is inodes. Inodes are used to describe both files and directories. Inodes offiles in the same directory are frequently accessed together. For example, the list directory commandSMM058 A Fast File System for UNIXoften accesses the inode for each file in a directory. The layout policy tries to place all the inodes of files ina directory in the same cylinder group. To ensure that files are distributed throughout the disk, a differentpolicy is used for directory allocation. A new directory is placed in a cylinder group that has a greater thanav erage number of free inodes, and the smallest number of directories already in it. The intent of this policy is to allow the inode clustering policy to succeed most of the time. The allocation of inodes within acylinder group is done using a next free strategy. Although this allocates the inodes randomly within acylinder group, all the inodes for a particular cylinder group can be read with 8 to 16 disk transfers. Atmost 16 disk transfers are required because a cylinder group may have no more than 2048 inodes. Thisputs a small and constant upper bound on the number of disk transfers required to access the inodes for allthe files in a directory. In contrast, the old file system typically requires one disk transfer to fetch the inodefor each file in a directory.The other major resource is data blocks. Since data blocks for a file are typically accessed together,the policy routines try to place all data blocks for a file in the same cylinder group, preferably at rotationallyoptimal positions in the same cylinder. The problem with allocating all the data blocks in the same cylindergroup is that large files will quickly use up available space in the cylinder group, forcing a spill over toother areas. Further, using all the space in a cylinder group causes future allocations for any file in thecylinder group to also spill to other areas. Ideally none of the cylinder groups should ever become completely full. The heuristic solution chosen is to redirect block allocation to a different cylinder group whena file exceeds 48 kilobytes, and at every megabyte thereafter. The newly chosen cylinder group is selectedfrom those cylinder groups that have a greater than average number of free blocks left. Although big filestend to be spread out over the disk, a megabyte of data is typically accessible before a long seek must beperformed, and the cost of one long seek per megabyte is small.The global policy routines call local allocation routines with requests for specific blocks. The localallocation routines will always allocate the requested block if it is free, otherwise it allocates a free block ofthe requested size that is rotationally closest to the requested block. If the global layout policies had complete information, they could always request unused blocks and the allocation routines would be reduced tosimple bookkeeping. However, maintaining complete information is costly thus the implementation of theglobal layout policy uses heuristics that employ only partial information.If a requested block is not available, the local allocator uses a four level allocation strategy1 Use the next available block rotationally closest to the requested block on the same cylinder. It isassumed here that head switching time is zero. On disk controllers where this is not the case, it maybe possible to incorporate the time required to switch between disk platters when constructing therotational layout tables. This, however, has not yet been tried.2 If there are no blocks available on the same cylinder, use a block within the same cylinder group.3 If that cylinder group is entirely full, quadratically hash the cylinder group number to choose anothercylinder group to look for a free block.4 Finally if the hash fails, apply an exhaustive search to all cylinder groups.Quadratic hash is used because of its speed in finding unused slots in nearly full hash tablesKnuth75. File systems that are parameterized to maintain at least 10 free space rarely use this strategy.File systems that are run without maintaining any free space typically have so few free blocks that almostany allocation is random the most important characteristic of the strategy used under such conditions isthat the strategy be fast. The first spill over point at 48 kilobytes is the point at which a file on a 4096 byte block file system firstrequires a single indirect block. This appears to be a natural first point at which to redirect block allocation.The other spillover points are chosen with the intent of forcing block allocation to be redirected when a file hasused about 25 of the data blocks in a cylinder group. In observing the new file system in day to day use, theheuristics appear to work well in minimizing the number of completely filled cylinder groups.A Fast File System for UNIX SMM0594. PerformanceUltimately, the proof of the effectiveness of the algorithms described in the previous section is thelong term performance of the new file system.Our empirical studies have shown that the inode layout policy has been effective. When running thelist directory command on a large directory that itself contains many directories to force the system toaccess inodes in multiple cylinder groups, the number of disk accesses for inodes is cut by a factor of two.The improvements are even more dramatic for large directories containing only files, disk accesses forinodes being cut by a factor of eight. This is most encouraging for programs such as spooling daemons thataccess many small files, since these programs tend to flood the disk request queue on the old file system.Table 2 summarizes the measured throughput of the new file system. Several comments need to bemade about the conditions under which these tests were run. The test programs measure the rate at whichuser programs can transfer data to or from a file without performing any processing on it. These programsmust read and write enough data to insure that buffering in the operating system does not affect the results.They are also run at least three times in succession the first to get the system into a known state and thesecond two to insure that the experiment has stabilized and is repeatable. The tests used and their resultsare discussed in detail in Kridle83. The systems were running multiuser but were otherwise quiescent.There was no contention for either the CPU or the disk arm. The only difference between the UNIBUS andMASSBUS tests was the controller. All tests used an AMPEX Capricorn 330 megabyte Winchester disk.As Table 2 shows, all file system test runs were on a VAX 11750. All file systems had been in productionuse for at least a month before being measured. The same number of system calls were performed in alltests the basic system call overhead was a negligible portion of the total running time of the tests.Type of Processor and ReadFile System Bus Measured Speed Bandwidth  CPUold 1024 750UNIBUS 29 Kbytessec 29983 3 11new 40961024 750UNIBUS 221 Kbytessec 221983 22 43new 81921024 750UNIBUS 233 Kbytessec 233983 24 29new 40961024 750MASSBUS 466 Kbytessec 466983 47 73new 81921024 750MASSBUS 466 Kbytessec 466983 47 54Table 2a  Reading rates of the old and new UNIX file systems.Type of Processor and WriteFile System Bus Measured Speed Bandwidth  CPUold 1024 750UNIBUS 48 Kbytessec 48983 5 29new 40961024 750UNIBUS 142 Kbytessec 142983 14 43new 81921024 750UNIBUS 215 Kbytessec 215983 22 46new 40961024 750MASSBUS 323 Kbytessec 323983 33 94new 81921024 750MASSBUS 466 Kbytessec 466983 47 95Table 2b  Writing rates of the old and new UNIX file systems.Unlike the old file system, the transfer rates for the new file system do not appear to change overtime. The throughput rate is tied much more strongly to the amount of free space that is maintained. Themeasurements in Table 2 were based on a file system with a 10 free space reserve. Synthetic work loadssuggest that throughput deteriorates to about half the rates given in Table 2 when the file systems are full.The percentage of bandwidth given in Table 2 is a measure of the effective utilization of the disk bythe file system. An upper bound on the transfer rate from the disk is calculated by multiplying the numberof bytes on a track by the number of revolutions of the disk per second. The bandwidth is calculated bycomparing the data rates the file system is able to achieve as a percentage of this rate. Using this metric,the old file system is only able to use about 35 of the disk bandwidth, while the new file system uses upto 47 of the bandwidth. A UNIX command that is similar to the reading test that we used is cp file devnull, where file is eightmegabytes long.SMM0510 A Fast File System for UNIXBoth reads and writes are faster in the new system than in the old system. The biggest factor in thisspeedup is because of the larger block size used by the new file system. The overhead of allocating blocksin the new system is greater than the overhead of allocating blocks in the old system, however fewer blocksneed to be allocated in the new system because they are bigger. The net effect is that the cost per byte allocated is about the same for both systems.In the new file system, the reading rate is always at least as fast as the writing rate. This is to beexpected since the kernel must do more work when allocating blocks than when simply reading them. Notethat the write rates are about the same as the read rates in the 8192 byte block file system the write ratesare slower than the read rates in the 4096 byte block file system. The slower write rates occur because thekernel has to do twice as many disk allocations per second, making the processor unable to keep up withthe disk transfer rate.In contrast the old file system is about 50 faster at writing files than reading them. This is becausethe write system call is asynchronous and the kernel can generate disk transfer requests much faster thanthey can be serviced, hence disk transfers queue up in the disk buffer cache. Because the disk buffer cacheis sorted by minimum seek distance, the average seek between the scheduled disk writes is much less thanit would be if the data blocks were written out in the random disk order in which they are generated. Howev er when the file is read, the read system call is processed synchronously so the disk blocks must beretrieved from the disk in the nonoptimal seek order in which they are requested. This forces the diskscheduler to do long seeks resulting in a lower throughput rate.In the new system the blocks of a file are more optimally ordered on the disk. Even though reads arestill synchronous, the requests are presented to the disk in a much better order. Even though the writes arestill asynchronous, they are already presented to the disk in minimum seek order so there is no gain to behad by reordering them. Hence the disk seek latencies that limited the old file system have little effect inthe new file system. The cost of allocation is the factor in the new system that causes writes to be slowerthan reads.The performance of the new file system is currently limited by memory to memory copy operationsrequired to move data from disk buffers in the systems address space to data buffers in the users addressspace. These copy operations account for about 40 of the time spent performing an inputoutput operation. If the buffers in both address spaces were properly aligned, this transfer could be performed withoutcopying by using the VAX virtual memory management hardware. This would be especially desirablewhen transferring large amounts of data. We did not implement this because it would change the user interface to the file system in two major ways user programs would be required to allocate buffers on pageboundaries, and data would disappear from buffers after being written.Greater disk throughput could be achieved by rewriting the disk drivers to chain together kernelbuffers. This would allow contiguous disk blocks to be read in a single disk transaction. Many disks usedwith UNIX systems contain either 32 or 48 512 byte sectors per track. Each track holds exactly two orthree 8192 byte file system blocks, or four or six 4096 byte file system blocks. The inability to use contiguous disk blocks effectively limits the performance on these disks to less than 50 of the available bandwidth. If the next block for a file cannot be laid out contiguously, then the minimum spacing to the nextallocatable block on any platter is between a sixth and a half a revolution. The implication of this is that thebest possible layout without contiguous blocks uses only half of the bandwidth of any giv en track. If eachtrack contains an odd number of sectors, then it is possible to resolve the rotational delay to any number ofsectors by finding a block that begins at the desired rotational position on another track. The reason thatblock chaining has not been implemented is because it would require rewriting all the disk drivers in thesystem, and the current throughput rates are already limited by the speed of the available processors.Currently only one block is allocated to a file at a time. A technique used by the DEMOS file systemwhen it finds that a file is growing rapidly, is to preallocate several blocks at once, releasing them when thefile is closed if they remain unused. By batching up allocations, the system can reduce the overhead of allocating at each write, and it can cut down on the number of disk writes needed to keep the block pointers onthe disk synchronized with the block allocation Powell79. This technique was not included because blockallocation currently accounts for less than 10 of the time spent in a write system call and, once again, thecurrent throughput rates are already limited by the speed of the available processors.A Fast File System for UNIX SMM05115. File system functional enhancementsThe performance enhancements to the UNIX file system did not require any changes to the semanticsor data structures visible to application programs. However, sev eral changes had been generally desired forsome time but had not been introduced because they would require users to dump and restore all their filesystems. Since the new file system already required all existing file systems to be dumped and restored,these functional enhancements were introduced at this time.5.1. Long file namesFile names can now be of nearly arbitrary length. Only programs that read directories are affected bythis change. To promote portability to UNIX systems that are not running the new file system, a set ofdirectory access routines have been introduced to provide a consistent interface to directories on both oldand new systems.Directories are allocated in 512 byte units called chunks. This size is chosen so that each allocationcan be transferred to disk in a single operation. Chunks are broken up into variable length records termeddirectory entries. A directory entry contains the information necessary to map the name of a file to its associated inode. No directory entry is allowed to span multiple chunks. The first three fields of a directoryentry are fixed length and contain an inode number, the size of the entry, and the length of the file namecontained in the entry. The remainder of an entry is variable length and contains a null terminated filename, padded to a 4 byte boundary. The maximum length of a file name in a directory is currently 255characters.Av ailable space in a directory is recorded by having one or more entries accumulate the free space intheir entry size fields. This results in directory entries that are larger than required to hold the entry nameplus fixed length fields. Space allocated to a directory should always be completely accounted for by totaling up the sizes of its entries. When an entry is deleted from a directory, its space is returned to a previousentry in the same directory chunk by increasing the size of the previous entry by the size of the deletedentry. If the first entry of a directory chunk is free, then the entrys inode number is set to zero to indicatethat it is unallocated.5.2. File lockingThe old file system had no provision for locking files. Processes that needed to synchronize theupdates of a file had to use a separate lock file. A process would try to create a lock file. If the creation succeeded, then the process could proceed with its update if the creation failed, then the processwould wait and try again. This mechanism had three drawbacks. Processes consumed CPU time by looping over attempts to create locks. Locks left lying around because of system crashes had to be manuallyremoved normally in a system startup command script. Finally, processes running as system administrator are always permitted to create files, so were forced to use a different mechanism. While it is possible toget around all these problems, the solutions are not straight forward, so a mechanism for locking files hasbeen added.The most general schemes allow multiple processes to concurrently update a file. Several of thesetechniques are discussed in Peterson83. A simpler technique is to serialize access to a file with locks. Toattain reasonable efficiency, certain applications require the ability to lock pieces of a file. Locking down tothe byte level has been implemented in the Onyx file system by Bass81. However, for the standard system applications, a mechanism that locks at the granularity of a file is sufficient.Locking schemes fall into two classes, those using hard locks and those using advisory locks. Theprimary difference between advisory locks and hard locks is the extent of enforcement. A hard lock isalways enforced when a program tries to access a file an advisory lock is only applied when it is requestedby a program. Thus advisory locks are only effective when all programs accessing a file use the lockingscheme. With hard locks there must be some override policy implemented in the kernel. With advisorylocks the policy is left to the user programs. In the UNIX system, programs with system administrator privilege are allowed override any protection scheme. Because many of the programs that need to use locksmust also run as the system administrator, we chose to implement advisory locks rather than create an additional protection scheme that was inconsistent with the UNIX philosophy or could not be used by systemSMM0512 A Fast File System for UNIXadministration programs.The file locking facilities allow cooperating programs to apply advisory shared or exclusive locks onfiles. Only one process may have an exclusive lock on a file while multiple shared locks may be present.Both shared and exclusive locks cannot be present on a file at the same time. If any lock is requested whenanother process holds an exclusive lock, or an exclusive lock is requested when another process holds anylock, the lock request will block until the lock can be obtained. Because shared and exclusive locks areadvisory only, even if a process has obtained a lock on a file, another process may access the file.Locks are applied or removed only on open files. This means that locks can be manipulated withoutneeding to close and reopen a file. This is useful, for example, when a process wishes to apply a sharedlock, read some information and determine whether an update is required, then apply an exclusive lock andupdate the file.A request for a lock will cause a process to block if the lock can not be immediately obtained. In certain instances this is unsatisfactory. For example, a process that wants only to check if a lock is presentwould require a separate mechanism to find out this information. Consequently, a process may specify thatits locking request should return with an error if a lock can not be immediately obtained. Being able toconditionally request a lock is useful to daemon processes that wish to service a spooling area. If thefirst instance of the daemon locks the directory where spooling takes place, later daemon processes can easily check to see if an active daemon exists. Since locks exist only while the locking processes exist, lockfiles can never be left active after the processes exit or if the system crashes.Almost no deadlock detection is attempted. The only deadlock detection done by the system is thatthe file to which a lock is applied must not already have a lock of the same type i.e. the second of two successive calls to apply a lock of the same type will fail.5.3. Symbolic linksThe traditional UNIX file system allows multiple directory entries in the same file system to reference a single file. Each directory entry links a files name to an inode and its contents. The link conceptis fundamental inodes do not reside in directories, but exist separately and are referenced by links. Whenall the links to an inode are removed, the inode is deallocated. This style of referencing an inode does notallow references across physical file systems, nor does it support intermachine linkage. To avoid theselimitations symbolic links similar to the scheme used by Multics Feiertag71 have been added.A symbolic link is implemented as a file that contains a pathname. When the system encounters asymbolic link while interpreting a component of a pathname, the contents of the symbolic link is prependedto the rest of the pathname, and this name is interpreted to yield the resulting pathname. In UNIX, pathnames are specified relative to the root of the file system hierarchy, or relative to a processs current working directory. Pathnames specified relative to the root are called absolute pathnames. Pathnames specifiedrelative to the current working directory are termed relative pathnames. If a symbolic link contains anabsolute pathname, the absolute pathname is used, otherwise the contents of the symbolic link is evaluatedrelative to the location of the link in the file hierarchy.Normally programs do not want to be aware that there is a symbolic link in a pathname that they areusing. However certain system utilities must be able to detect and manipulate symbolic links. Three newsystem calls provide the ability to detect, read, and write symbolic links seven system utilities requiredchanges to use these calls.In future Berkeley software distributions it may be possible to reference file systems located onremote machines using pathnames. When this occurs, it will be possible to create symbolic links that spanmachines.5.4. RenamePrograms that create a new version of an existing file typically create the new version as a temporaryfile and then rename the temporary file with the name of the target file. In the old UNIX file system renaming required three calls to the system. If a program were interrupted or the system crashed between thesecalls, the target file could be left with only its temporary name. To eliminate this possibility the renamesystem call has been added. The rename call does the rename operation in a fashion that guarantees theA Fast File System for UNIX SMM0513existence of the target name.Rename works both on data files and directories. When renaming directories, the system must dospecial validation checks to insure that the directory tree structure is not corrupted by the creation of loopsor inaccessible directories. Such corruption would occur if a parent directory were moved into one of itsdescendants. The validation check requires tracing the descendents of the target directory to insure that itdoes not include the directory being moved.5.5. QuotasThe UNIX system has traditionally attempted to share all available resources to the greatest extentpossible. Thus any single user can allocate all the available space in the file system. In certain environments this is unacceptable. Consequently, a quota mechanism has been added for restricting the amount offile system resources that a user can obtain. The quota mechanism sets limits on both the number of inodesand the number of disk blocks that a user may allocate. A separate quota can be set for each user on eachfile system. Resources are given both a hard and a soft limit. When a program exceeds a soft limit, a warning is printed on the users terminal the offending program is not terminated unless it exceeds its hard limit.The idea is that users should stay below their soft limit between login sessions, but they may use moreresources while they are actively working. To encourage this behavior, users are warned when logging in ifthey are over any of their soft limits. If users fails to correct the problem for too many login sessions, theyare eventually reprimanded by having their soft limit enforced as their hard limit.AcknowledgementsWe thank Robert Elz for his ongoing interest in the new file system, and for adding disk quotas in arational and efficient manner. We also acknowledge Dennis Ritchie for his suggestions on the appropriatemodifications to the user interface. We appreciate Michael Powells explanations on how the DEMOS filesystem worked many of his ideas were used in this implementation. Special commendation goes to PeterKessler and Robert Henry for acting like real users during the early debugging stage when file systems wereless stable than they should have been. The criticisms and suggestions by the reviews contributed significantly to the coherence of the paper. Finally we thank our sponsors, the National Science Foundation undergrant MCS8005144, and the Defense Advance Research Projects Agency DoD under ARPA Order No.4031 monitored by Naval Electronic System Command under Contract No. N0003982C0235.ReferencesAlmes78 Almes, G., and Robertson, G. An Extensible File System for Hydra Proceedingsof the Third International Conference on Software Engineering, IEEE, May 1978.Bass81 Bass, J. Implementation Description for File Locking, Onyx Systems Inc, 73 E.Trimble Rd, San Jose, CA 95131 Jan 1981.Feiertag71 Feiertag, R. J. and Organick, E. I., The Multics InputOutput System, Proceedings of the Third Symposium on Operating Systems Principles, ACM, Oct 1971.pp 3541Ferrin82a Ferrin, T.E., Performance and Robustness Improvements in Version 7 UNIX,Computer Graphics Laboratory Technical Report 2, School of Pharmacy, University of California, San Francisco, January 1982. Presented at the 1982 WinterUsenix Conference, Santa Monica, California.Ferrin82b Ferrin, T.E., Performance Issuses of VMUNIX Revisited, login The UsenixAssociation Newsletter, Vol 7, 5, November 1982. pp 36Kridle83 Kridle, R., and McKusick, M., Performance Effects of Disk Subsystem Choicesfor VAX Systems Running 4.2BSD UNIX, Computer Systems Research Group,SMM0514 A Fast File System for UNIXDept of EECS, Berkeley, CA 94720, Technical Report 8.Kow alski78 Kow alski, T. FSCK  The UNIX System Check Program, Bell Laboratory, Murray Hill, NJ 07974. March 1978Knuth75 Kunth, D. The Art of Computer Programming, Volume 3  Sorting and Searching, AddisonWesley Publishing Company Inc, Reading, Mass, 1975. pp 506549Maruyama76 Maruyama, K., and Smith, S. Optimal reorganization of Distributed Space DiskFiles, CACM, 19, 11. Nov 1976. pp 634642Nevalainen77 Nevalainen, O., Vesterinen, M. Determining Blocking Factors for SequentialFiles by Heuristic Methods, The Computer Journal, 20, 3. Aug 1977. pp 245247Pechura83 Pechura, M., and Schoeffler, J. Estimating File Access Time of Floppy Disks,CACM, 26, 10. Oct 1983. pp 754763Peterson83 Peterson, G. Concurrent Reading While Writing, ACM Transactions on Programming Languages and Systems, ACM, 5, 1. Jan 1983. pp 4655Powell79 Powell, M. The DEMOS File System, Proceedings of the Sixth Symposium onOperating Systems Principles, ACM, Nov 1977. pp 3342Ritchie74 Ritchie, D. M. and Thompson, K., The UNIX TimeSharing System, CACM 17,7. July 1974. pp 365375Smith81a Smith, A. InputOutput Optimization and Disk Architectures A Survey, Performance and Evaluation 1. Jan 1981. pp 104117Smith81b Smith, A. Bibliography on File and IO System Optimization and Related Topics, Operating Systems Review, 15, 4. Oct 1981. pp 3954Symbolics81 Symbolics File System, Symbolics Inc, 9600 DeSoto Ave, Chatsworth, CA91311 Aug 1981.Thompson78 Thompson, K. UNIX Implementation, Bell System Technical Journal, 57, 6,part 2. pp 19311946 JulyAugust 1978.Thompson80 Thompson, M. Spice File System, CarnegieMellon University, Department ofComputer Science, Pittsburg, PA 15213 CMUCS80, Sept 1980.Trivedi80 Trivedi, K. Optimal Selection of CPU Speed, Device Capabilities, and FileAssignments, Journal of the ACM, 27, 3. July 1980. pp 457473White80 White, R. M. Disk Storage Technology, Scientific American, 2432, August1980.
