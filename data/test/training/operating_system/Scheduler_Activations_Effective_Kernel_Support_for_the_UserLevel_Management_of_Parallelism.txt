ACM Transactions on Computer Systems, Vol. 10, No. 1, February 1992, Pages 5379.Scheduler Activations Effective Kernel Support for the UserLevel Management of ParallelismTHOMAS E. ANDERSON, BRIAN N. BERSHAD, EDWARD D. LAZOWSKA, and HENRY M. LEVYUniversity of WashingtonThreads are the vehicle for concurrency in many approaches to parallel programming.Threads can be supported either by the operating system kernel or by userlevel librarycode in the application address space, but neither approach has been fully satisfactory.  This paper addresses this dilemma. First, we argue that the performance of kernel threadsis inherently worse than that of userlevel threads, rather than this being an artifact ofexisting implementations managing parallelism at the user level is essential to highperformance parallel computing. Next, we argue that the problems encountered inintegrating userlevel threads with other system services is a consequence of the lack ofkernel support for userlevel threads provided by contemporary multiprocessor operatingsystems kernel threads are the wrong abstraction on which to support userlevelmanagement of parallelism. Finally, we describe the design, implementation, andperformance of a new kernel interface and userlevel thread package that together providethe same functionality as kernel threads without compromising the performance andflexibility advantages of userlevel management of parallelism.Categories and Subject Descriptors D.4.1 Operating Systems Process ManagementD.4.4 Operating Systems Communications Managementinputoutput D.4.7Operating Systems Organization and Design D.4.8 Operating Systems PerformanceGeneral Terms Design, Measurement, PerformanceAdditional Key Words and Phrases Multiprocessor, thread1. INTRODUCTIONThe effectiveness of parallel computing depends to a great extent on theperformance of the primitives that are used to express and control theparallelism within programs. Even a coarsegrained parallel program canThis work was supported in part by the National Science Foundation grants CCR5619663, CCR5703049, and CCR.8907666, the Washington Technology Center, andDigital Equipment Corporation the Systems Research Center and the External ResearchProgram. Anderson was supported by an IBM Graduate Fellowship and Bershad by anATT Ph.D. Scholarship. Anderson is now with the Computer Science Division,University of California at Berkeley Bershad is now with the School of Computer Science,Carnegie Mellon University.Authors address Department of Computer Science and Engineering, University ofWashington, Seattle, WA 98195.Permission to copy without fee all or part of this material is granted provided that thecopies are not made or distributed for direct commercial advantage, the ACM copyrightnotice and the title of the publication and its date appear, and notice is given that copying isby permission of the Association for Computing Machinery. To copy otherwise, or torepublish, requires a fee andor specific permission. 1992 ACM 073420719202000053 01.50Recreated electronic version by Eric A. Brewer brewercs.berkeley.edu, Oct 20, 199854      l      T. E. Anderson et al.ACM Transactions on Computer Systems, Vol. 10, No. 1, February 1992.exhibit poor performance if the cost of creating and managing parallelismis high. Even a finegrained program can achieve good performance if thecost of creating and managing parallelism is low.One way to construct a parallel program is to share memory between acollection of traditional UNIXlike processes, each consisting of a singleaddress space and a single sequential execution stream within that addressspace. Unfortunately, because such processes were designed formultiprogramming in a uniprocessor environment, they are simply tooinefficient for generalpurpose parallel programming they handle onlycoarsegrained parallelism well.The shortcomings of traditional processes for generalpurpose parallelprogramming have led to the use of threads. Threads separate the notionof a sequential execution stream from the other aspects of traditionalprocesses such as address spaces and IO descriptors. This separation ofconcerns yields a significant performance advantage relative to traditionalprocesses.1.1 The ProblemThreads can be supported either at user level or in the kernel. Neitherapproach has been fully satisfactory.Userlevel threads are managed by runtime library routines linked intoeach application so that thread management operations require no kernelintervention. The result can be excellent performance in systems such asPCR 25 and FastThreads 2, the cost of userlevel thread operations iswithin an order of magnitude of the cost of a procedure call. Userlevelthreads are also flexible they can be customized to the needs of thelanguage or user without kernel modification.Userlevel threads execute within the context of traditional processesindeed, userlevel thread systems are typically built without anymodifications to the underlying operating system kernel. The threadpackage views each process as a virtual processor, and treats it as aphysical processor executing under its control each virtual processor runsuserlevel code that pulls threads off the ready list and runs them. Inreality, though, these virtual processors are being multiplexed across real,physical processors by the underlying kernel. Real world operatingsystem activity, such as multiprogramming, IO, and page faults, distortsthe equivalence between virtual and physical processors in the presence ofthese factors, userlevel threads built on top of traditional processes canexhibit poor performance or even incorrect behavior.Multiprocessor operating systems such as Mach 2, Topaz 22, and V7 provide direct kernel support for multiple threads per address space.Programming with kernel threads avoids the system integration problemsexhibited by userlevel threads, because the kernel directly schedules eachapplications threads onto physical processors. Unfortunately, kernelthreads, just like traditional UNIX processes, are too heavyweight for usein many parallel programs. The performance of kernel threads, althoughtypically an order of magnitude better than that of traditional processes,has been typically an order of magnitude worse than the bestcaseperformance of userlevel threads e.g., in the absence ofScheduler Activations Effective Kernel Support     l      55ACM Transactions on Computer Systems, Vol. 10, No. 1, February 1992.multiprogramming and IO. As a result, userlevel threads have ultimatelybeen implemented on top of the kernel threads of both Mach C Threads8 and Topaz WorkCrews 24. Userlevel threads are built on top ofkernel threads exactly as they are built on top of traditional processes theyhave exactly the same performance, and they suffer exactly the sameproblems.The parallel programmer, then, has been faced with a difficult dilemmaemploy userlevel threads, which have good performance and correctbehavior provided the application is uniprogrammed and does no IO, oremploy kernel threads, which have worse performance but are not asrestricted.1.2 The Goals of this WorkIn this paper we address this dilemma. We describe a kernel interface anda userlevel thread package that together combine the functionality ofkernel threads with the performance and flexibility of userlevel threads.Specifically,In the common case when thread operations do not need kernelintervention, our performance is essentially the same as that achieved bythe best existing userlevel thread management systems which sufferfrom poor system integration.In the infrequent case when the kernel must be involved, such as onprocessor reallocation or IO, our system can mimic the behavior of akernel thread management systemNo processor idles in the presence of ready threads.No highpriority thread waits for a processor while a lowprioritythread runs.When a thread traps to the kernel to block for example, because of apage fault, the processor on which the thread was running can beused to run another thread from the same or from a different addressspace.The userlevel part of our system is structured to simplify applicationspecific customization. It is easy to change the policy for scheduling anapplications threads, or even to provide a different concurrency modelsuch as workers 16, Actors 1, or Futures 10. The difficulty in achieving these goals in a multiprogrammedmultiprocessor is that the necessary control and scheduling information isdistributed between the kernel and each applications address space. To beable to allocate processors among applications, the kernel needs access touserlevel scheduling information e.g., how much parallelism there is ineach address space. To be able to manage the applications parallelism,the userlevel support software needs to be aware of kernel events e.g.,processor reallocations and IO requestcompletions that are normallyhidden from the application.1.3 The ApproachOur approach provides each application with a virtual multiprocessor,56      l      T. E. Anderson et al.ACM Transactions on Computer Systems, Vol. 10, No. 1, February 1992.an abstraction of a dedicated physical machine. Each application knowsexactly how many and which processors have been allocated to it andhas complete control over which of its threads are running on thoseprocessors. The operating system kernel has complete control over theallocation of processors among address spaces including the ability tochange the number of processors assigned to an application during itsexecution.To achieve this, the kernel notifies the address space thread scheduler ofevery kernel event affecting the address space, allowing the application tohave complete knowledge of its scheduling state. The thread system ineach address space notifies the kernel of the subset of userlevel threadoperations that can affect processor allocation decisions, preserving goodperformance for the majority of operations that do not need to be reflectedto the kernel.The kernel mechanism that we use to realize these ideas is calledscheduler activations. A scheduler activation vectors control from thekernel to the address space thread scheduler on a kernel event the threadscheduler can use the activation to modify userlevel thread datastructures, to execute userlevel threads, and to make requests of the kernel. We have implemented a prototype of our design on the DEC SRCFirefly multiprocessor workstation 22. While the differences betweenscheduler activations and kernel threads are crucial, the similarities aregreat enough that the kernel portion of our implementation required onlyrelatively straightforward modifications to the kernel threads of Topaz, thenative operating system on the Firefly. Similarly, the userlevel portion ofour implementation involved relatively straightforward modifications toFastThreads, a userlevel thread system originally designed to run on topof Topaz kernel threads.Since our goal is to demonstrate that the exact functionality of kernelthreads can be provided at the user level, the presentation in this paperassumes that userlevel threads are the concurrency model used by theprogrammer or compiler. We emphasize, however, that other concurrencymodels, when implemented at user level on top of kernel threads orprocesses, suffer from the same problems as userlevel threadsproblemsthat are solved by implementing them on top of scheduler activations.2. USERLEVEL THREADS PERFORMANCE ADVANTAGES AND FUNCTIONALITY LiMITATIONSIn this section we motivate our work by describing the advantages thatuserlevel threads offer relative to kernel threads, and the difficulties thatarise when userlevel threads are built on top of the interface provided bykernel threads or processes. We argue that the performance of userlevelthreads is inherently better than that of kernel threads, rather than thisbeing an artifact of existing implementations. Userlevel threads have anadditional advantage of flexibility with respect to programming modelsand environments. Further, we argue that the lack of system integrationexhibited by userlevel threads is not inherent in userlevel threadsthemselves, but is a consequence of inadequate kernel support.Scheduler Activations Effective Kernel Support     l      57ACM Transactions on Computer Systems, Vol. 10, No. 1, February 1992.2.1 The Case for UserLevel Thread ManagementIt is natural to believe that the performance optimizations found in userlevel thread systems could be applied within the kernel, yielding kernelthreads that are as efficient as userlevel threads without the compromisesin functionality. Unfortunately, there are significant inherent costs tomanaging threads in the kernelThe cost of accessing thread management operations With kernelthreads, the program must cross an extra protection boundary on everythread operation, even when the processor is being switched betweenthreads in the same address space. This involves not only an extrakernel trap, but the kernel must also copy and check parameters in orderto protect itself against buggy or malicious programs. By contrast,invoking userlevel thread operations can be quite inexpensive,particularly when compiler techniques are used to expand code inlineand perform sophisticated register allocation. Further, safety is notcompromised address space boundaries isolate misuse of a userlevelthread system to the program in which it occurs.The cost of generality With kernel thread management, a singleunderlying implementation is used by all applications. To be generalpurpose, a kernel thread system must provide any feature needed by anyreasonable application this imposes overhead on those applications thatdo not use a particular feature. In contrast, the facilities provided by auserlevel thread system can be closely matched to the specific needs ofthe applications that use it, since different applications can be linkedwith different userlevel thread libraries. As an example, most kernelthread systems implement preemptive priority scheduling, even thoughmany parallel applications can use a simpler policy such as firstinfirstout 24.These factors would not be important if thread management operationswere inherently expensive. Kernel trap overhead and priority scheduling,for instance, are not major contributors to the high cost of UNIXlikeprocesses. However, the cost of thread operations can be within an orderof magnitude of a procedure call. This implies that any overhead added bya kernel implementation, however small, will be significant, and a wellwritten userlevel thread system will have significantly better performancethan a wellwritten kernellevel thread system.To illustrate this quantitatively, Table I shows the performance ofexample implementations of userlevel threads, kernel threads, and UNIXlike processes, all running on similar hardware, a CVAX processor.FastThreads and Topaz kernel threads were measured on a CVAX FireflyUltrix DECs derivative of UNIX was measured on a CVAXuniprocessor workstation. Each of these implementations, while good, isnot optimal. Thus, our measurements are illustrative and not definitive. The two benchmarks are Null Fork, the time to create, schedule, executeand complete a processthread that invokes the null procedure in otherwords, the overhead of forking a thread, and SignalWait, the time for a58      l      T. E. Anderson et al.ACM Transactions on Computer Systems, Vol. 10, No. 1, February 1992.processthread to signal a waiting processthread, and then wait on acondition the overhead of synchronizing two threads together. Eachbenchmark was executed on a single processor, and the results wereaveraged across multiple repetitions. For comparison, a procedure calltakes about 7 sec. on the Firefly, while a kernel trap takes about 19 sec. Table I shows that while there is an order of magnitude difference incost between Ultrix process management and Topaz kernel threadmanagement, there is yet another order of magnitude difference betweenTopaz threads and FastThreads. This is despite the fact that the Topazthread code is highly tuned with much of the critical path written inassembler.Commonly, a tradeoff arises between performance and flexibility inchoosing where to implement system services 26. Userlevel threads,however, avoid this tradeoff they simultaneously improve bothperformance and flexibility. Flexibility is particularly important in threadsystems since there are many parallel programming models, each of whichmay require specialized support within the thread system. With kernelthreads, supporting multiple parallel programming models may requiremodifying the kernel, which increases complexity, overhead, and thelikelihood of errors in the kernel.2.2 Sources of Poor Integration in UserLevel Threads Built on the Traditional Kernel InterfaceUnfortunately, it has proven difficult to implement userlevel threads thathave the same level of integration with system services as is available withkernel threads. This is not inherent in managing parallelism at the userlevel, but rather is a consequence of the lack of kernel support in existingsystems. Kernel threads are the wrong abstraction for supporting userlevel thread management. There are two related characteristics of kernelthreads that cause difficultyKernel threads block, resume, and are preempted without notification tothe user level.Kernel threads are scheduled obliviously with respect to the userlevelthread state.These can cause problems even on a uniprogrammed system. A userlevel thread system will often create as many kernel threads to serve asvirtual processors as there are physical processors in the system eachwill be used to run userlevel threads. When a userlevel thread makes aTable I  Thread Operation Latencies sec.Operation FastThreadsTopaz threadsUltrix processesNull Fork 34 948 11300SignalWait 37 441 1840Scheduler Activations Effective Kernel Support     l      59ACM Transactions on Computer Systems, Vol. 10, No. 1, February 1992.blocking IO request or takes a page fault, though, the kernel threadserving as its virtual processor also blocks. As a result, the physicalprocessor is lost to the address space while the IO is pending, becausethere is no kernel thread to run other userlevel threads on the justidledprocessor.A plausible solution to this might be to create more kernel threads thanphysical processors when one kernel thread blocks because its userlevelthread blocks in the kernel, another kernel thread is available to run userlevel threads on that processor. However, a difficulty occurs when the IOcompletes or the page fault returns there will be more runnable kernelthreads than processors, each kernel thread in the middle of running a userlevel thread. In deciding which kernel threads are to be assignedprocessors, the operating system will implicitly choose which userlevelthreads are assigned processors.In a traditional system, when there are more runnable threads thanprocessors, the operating system could employ some kind of timeslicingto ensure each thread makes progress. When userlevel threads are runningon top of kernel threads, however, timeslicing can lead to problems. Forexample, a kernel thread could be preempted while its userlevel thread isholding a spinlock any userlevel threads accessing the lock will thenspinwait until the lock holder is rescheduled. Zahorjan et al. 28 haveshown that timeslicing in the presence of spinlocks can result in poorperformance. As another example, a kernel thread running a userlevelthread could be preempted to allow another kernel thread to run thathappens to be idling in its userlevel scheduler. Or a kernel thread runninga highpriority userlevel thread could be descheduled in favor of a kernelthread that happens to be running a lowpriority userlevel thread.Exactly the same problems occur with multiprogramming as with IOand page faults. If there is only one job in the system, it can receive all ofthe machines processors if another job enters the system, the operatingsystem should preempt some of the first jobs processors to give to the newjob 23. The kernel then is forced to choose which of the first jobs kernelthreads, and thus implicitly which userlevel threads, to run on theremaining processors. The need to preempt processors from an addressspace also occurs due to variations in parallelism within jobs Zahorjanand McCann 27 show that the dynamic reallocation of processors amongaddress spaces in response to variations in parallelism is important toachieving high performance.While a kernel interface can be designed to allow the user level toinfluence which kernel threads are scheduled when the kernel has a choice5, this choice is intimately tied to the userlevel thread state thecommunication of this information between the kernel and the userlevelnegates many of the performance and flexibility advantages of using userlevel threads in the first place.Finally, ensuring the logical correctness of a userlevel thread systembuilt on kernel threads can be difficult. Many applications, particularlythose that require coordination among multiple address spaces, are freefrom deadlock based on the assumption that all runnable threadseventually receive processor time. When kernel threads are used directlyby applications, the kernel satisfies this assumption by timeslicing the60      l      T. E. Anderson et al.ACM Transactions on Computer Systems, Vol. 10, No. 1, February 1992.processors among all of the runnable threads. But when userlevel threadsare multiplexed across a fixed number of kernel threads, the assumptionmay no longer hold because a kernel thread blocks when its userlevelthread blocks, an application can run out of kernel threads to serve asexecution contexts, even when there are runnable userlevel threads andavailable processors.3. EFFECTIVE KERNEL SUPPORT FOR THE USERLEVEL MANAGEMENT OF PARALLELISMSection 2 described the problems that arise when kernel threads are usedby the programmer to express parallelism poor performance and poorflexibility and when userlevel threads are built on top of kernel threadspoor behavior in the presence of multiprogramming and IO. To addressthese problems, we have designed a new kernel interface and userlevelthread system that together combine the functionality of kernel threadswith the performance and flexibility of userlevel threads.The operating system kernel provides each userlevel thread system withits own virtual multiprocessor, the abstraction of a dedicated physicalmachine except that the kernel may change the number of processors inthat machine during the execution of the program. There are severalaspects to this abstractionThe kernel allocates processors to address spaces the kernel hascomplete control over how many processors to give each address spacesvirtual multiprocessor.Each address spaces userlevel thread system has complete control overwhich threads to run on its allocated processors, as it would if theapplication were running on the bare physical machine.The kernel notifies the userlevel thread system whenever the kernelchanges the number of processors assigned to it the kernel also notifiesthe thread system whenever a userlevel thread blocks or wakes up inthe kernel e.g., on IO or on a page fault. The kernels role is to vectorevents to the appropriate thread scheduler, rather than to interpret theseevents on its own.The userlevel thread system notifies the kernel when the applicationneeds more or fewer processors. The kernel uses this information toallocate processors among address spaces. However, the user levelnotifies the kernel only on those subset of userlevel thread operationsthat might affect processor allocation decisions. As a result,performance is not compromised the majority of thread operations donot suffer the overhead of communication with the kernel.The application programmer sees no difference, except forperformance, from programming directly with kernel threads. Our userlevel thread system manages its virtual multiprocessor transparently tothe programmer, providing programmers a normal Topaz threadinterface 4. The userlevel runtime system could easily be adapted,though, to provide a different parallel programming model. In the remainder of this section we describe how kernel events areScheduler Activations Effective Kernel Support     l      61ACM Transactions on Computer Systems, Vol. 10, No. 1, February 1992.vectored to the userlevel thread system, what information is provided bythe application to allow the kernel to allocate processors among jobs, andhow we handle userlevel spinlocks.3.1  Explicit Vectoring of Kernel Events to the UserLevel Thread SchedulerThe communication between the kernel processor allocator and the userlevel thread system is structured in terms of scheduler activations. Theterm scheduler activation was selected because each vectored eventcauses the userlevel thread system to reconsider its scheduling decision ofwhich threads to run on which processors.A scheduler activation serves three rolesIt serves as a vessel, or execution context, for running userlevelthreads, in exactly the same way that a kernel thread does.It notifies the userlevel thread system of a kernel event.It provides space in the kernel for saving the processor context of theactivations current userlevel thread, when the thread is stopped by thekernel e.g., because the thread blocks in the kernel on IO or the kernelpreempts its processor.A scheduler activations data structures are quite similar to those of atraditional kernel thread. Each scheduler activation has two executionstacks   one mapped into the kernel and one mapped into the applicationaddress space. Each userlevel thread is allocated its own userlevel stackwhen it starts running 2 when a userlevel thread calls into the kernel, ituses its activations kernel stack. The userlevel thread scheduler runs onthe activations userlevel stack. In addition, the kernel maintains anactivation control block akin to a thread control block to record the stateof the scheduler activations thread when it blocks in the kernel or ispreempted the userlevel thread scheduler maintains a record of whichuserlevel thread is running in each scheduler activation.When a program is started, the kernel creates a scheduler activation,assigns it to a processor, and upcalls into the application address space at afixed entry point. The userlevel thread management system receives theupcall and uses that activation as the context in which to initialize itselfand run the main application thread. As the first thread executes, it maycreate more user threads and request additional processors. In this case, thekernel will create an additional scheduler activation for each processor anduse it to upcall into the user level to tell it that the new processor isavailable. The user level then selects and executes a thread in the contextof that activation.Similarly, when the kernel needs to notify the user level of an event, thekernel creates a scheduler activation, assigns it to a processor, and upcallsinto the application address space. Once the upcall is started, the activationis similar to a traditional kernel threadit can be used to process theevent, run userlevel threads, and trap into and block within the kernel.62      l      T. E. Anderson et al.ACM Transactions on Computer Systems, Vol. 10, No. 1, February 1992. The crucial distinction between scheduler activations and kernel threadsis that once an activations userlevel thread is stopped by the kernel, thethread is never directly resumed by the kernel. Instead, a new scheduleractivation is created to notify the userlevel thread system that the threadhas been stopped. The userlevel thread system then removes the state ofthe thread from the old activation, tells the kernel that the old activationcan be reused, and finally decides which thread to run on the processor. Bycontrast, in a traditional system, when the kernel stops a kernel thread,even one running a userlevel thread in its context, the kernel nevernotifies the user level of the event. Later, the kernel directly resumes thekernel thread and by implication, its userlevel thread, again withoutnotification. By using scheduler activations, the kernel is able to maintainthe invariant that there are always exactly as many running scheduleractivations vessels for running userlevel threads as there are processorsassigned to the address space.Table II lists the events that the kernel vectors to the user level usingscheduler activations the parameters to each upcall are in parentheses, andthe action taken by the userlevel thread system is italicized. Note thatevents are vectored at exactly the points where the kernel would otherwisebe forced to make a scheduling decision. In practice, these events occur incombinations when this occurs, a single upcall is made that passes all ofthe events that need to be handled.As one example of the use of scheduler activations, Figure 1 illustrateswhat happens on an IO requestcompletion. Note that this is theuncommon case in normal operation, threads can be created, run, andcompleted, all without kernel intervention. Each pane in Figure 1 reflects adifferent time step. Straight arrows represent scheduler activations, sshaped arrows represent userlevel threads, and the cluster of userlevelthreads to the right of each pane represents the ready list.At time T1, the kernel allocates the application two processors. On eachprocessor, the kernel upcalls to userlevel code that removes a thread fromthe ready list and starts running it. At time T2, one of the userlevelthreads thread 1 blocks in the kernel. To notify the user level of thisTable II Scheduler Activation Upcall PointsAdd this processor processor Execute a runnable userlevel thread.Processor has been preempted preempted activation  and its machine stateReturn to the ready list the userlevel thread that was executing in the contextof the preempted scheduler activation.Scheduler activation has blocked blocked activation The blocked scheduler activation is no longer using its processor.Scheduler activation has unblocked unblocked activation  and its machine stateReturn to the ready list the userlevel thread that was executing in the contextof the blocked scheduler activation.Scheduler Activations Effective Kernel Support     l      63ACM Transactions on Computer Systems, Vol. 10, No. 1, February 1992.event, the kernel takes the processor that had been running thread 1 andperforms an upcall in the context of a fresh scheduler activation. The userlevel thread scheduler can then use the processor to take another thread offthe ready list and start running it.At time T3, the IO completes. Again, the kernel must notify the userlevel thread system of the event, but this notification requires a processor.The kernel preempts one of the processors running in the address spaceand uses it to do the upcall. If there are no processors assigned to theaddress space when the IO completes, the upcall must wait until thekernel allocates one. This upcall notifies the user level of two things the IO completion and the preemption. The upcall invokes code in the userlevel thread system that 1 puts the thread that had been blocked on theready list and 2 puts the thread that was preempted on the ready list. Atthis point, scheduler activations A and B can be discarded. Finally, at timeT4, the upcall takes a thread off the ready list and starts running it.When a user level thread blocks in the kernel or is preempted, most ofthe state needed to resume it is already at the user levelnamely, thethreads stack and control block. The threads register state, however, issaved by lowlevel kernel routines, such as the interrupt and page faulthandlers the kernel passes this state to the user level as part of the upcallnotifying the address space of the preemption andor IO completion.We use exactly the same mechanism to reallocate a processor from one64      l      T. E. Anderson et al.ACM Transactions on Computer Systems, Vol. 10, No. 1, February 1992.address space to another due to multiprogramming. For example, supposethe kernel decides to take a processor away from one address space andgive it to another. The kernel does this by sending the processor aninterrupt, stopping the old activation, and then using the processor to do anupcall into the new address space with a fresh activation. The kernel neednot obtain permission in advance from the old address space to steal itsprocessor to do so would violate the semantics of address space prioritiese.g., the new address space could have higher priority than the old addressspace. However, the old address space must still be notified that thepreemption occurred. The kernel does this by doing another preemptionon a different processor still running in the old address space. The secondprocessor is used to make an upcall into the old address space using a freshscheduler activation, notifying the address space that two userlevelthreads have been stopped. The userlevel thread scheduler then has fullcontrol over which of these threads should be run on its remainingprocessors. When the last processor is preempted from an address space,we could simply skip notifying the address space of the preemption, butinstead, we delay the notification until the kernel eventually reallocates itas a processor. Notification allows the user level to know which processorsit has been assigned, in case it is explicitly managing cache locality.The above description is oversimplified in several minor respects. First,if threads have priorities, an additional preemption may have to take placebeyond the ones described above. In the example in Figure 1, supposethread 3 is lower priority than both threads 1 and 2. In that case, the userlevel thread system can ask the kernel to preempt thread 3s processor.The kernel will then use that processor to do an upcall, allowing the userlevel thread system to put thread 3 on the ready list and run thread 2instead. The user level can know to ask for the additional preemptionbecause it knows exactly which thread is running on each of its processors.Second, while we described the kernel as stopping and saving thecontext of userlevel threads, the kernels interaction with the application isentirely in terms of scheduler activations. The application is free to buildany other concurrency model on top of scheduler activations the kernelsbehavior is exactly the same in every case. In particular, the kernel needsno knowledge of the data structures used to represent parallelism at theuser level.Third, scheduler activations work properly even when a preemption or apage fault occurs in the userlevel thread manager when no userlevelthread is running. In this case, it is the thread manager whose state is savedby the kernel. The subsequent upcall, in a new activation with its ownstack, allows the reentrant thread manager to recover in one way if a userlevel thread is running, and in a different way if not. For example, if apreempted processor was in the idle loop, no action is necessary if it washandling an event during an upcall, a userlevel context switch can bemade to continue processing the event. The only added complication forthe kernel is that an upcall to notify the program of a page fault may inturn page fault on the same location the kernel must check for this, andwhen it occurs, delay the subsequent upcall until the page fault completes. Scheduler Activations Effective Kernel Support     l      65ACM Transactions on Computer Systems, Vol. 10, No. 1, February 1992.Finally, a userlevel thread that has blocked in the kernel may still needto execute further in kernel mode when the IO completes. If so, the kernelresumes the thread temporarily, until it either blocks again or reaches thepoint where it would leave the kernel. It is when the latter occurs that thekernel notifies the user level, passing the userlevel threads register stateas part of the upcall.3.2 Notifying the Kernel of UserLevel Events Affecting Processor AllocationThe mechanism described in the last subsection is independent of thepolicy used by the kernel for allocating processors among address spaces.Reasonable allocation policies, however, must be based on the availableparallelism in each address space. In this subsection, we show that thisinformation can be efficiently communicated for policies that both respectpriorities and guarantee that processors do not idle if runnable threadsexist. These constraints are met by most kernel thread systems as far aswe know, they are not met by any userlevel thread system built on top ofkernel threads.The key observation is that the user level thread system need not tell thekernel about every thread operation, but only about the small subset thatcan affect the kernels processor allocation decision. By contrast, whenkernel threads are used directly for parallelism, a processor traps to thekernel even when the best thread for it to run nexta thread that respectspriorities while minimizing overhead and preserving cache contextiswithin the same address space.In our system, an address space notifies the kernel whenever it makes atransition to a state where it has more runnable threads than processors, ormore processors than runnable threads. Provided an application has extrathreads to run and the processor allocator has not reassigned it additionalprocessors, then all processors in the system must be busy. Creating moreparallelism cannot violate the constraints. Similarly, if an application hasnotified the kernel that it has idle processors and the kernel has not takenthem away, then there must be no other work in the system. The kernelneed not be notified of additional idle processors. An extension to thisapproach handles the situation where threads, rather than address spaces,have globally meaningful priorities.Table III lists the kernel calls made by an address space on these statetransitions. For example, when an address space notifies the kernel that itneeds more processors, the kernel searches for an address space that hasregistered that has idle processors. If none are found, nothing happens, butthe address space may eventually get a processor if one becomes idle inthe future. These notifications are only hints if the kernel gives an addressspace a processor that is no longer needed by the time it gets there, theaddress space simply returns the processor to the kernel with the updatedinformation. Of course, the userlevel thread system must serialize itsnotifications to the kernel, since ordering matters.An apparent drawback to this approach is that applications may not behonest in reporting their parallelism to the operating system. This problemis not unique to multiprocessors a dishonest or misbehaving program can66      l      T. E. Anderson et al.ACM Transactions on Computer Systems, Vol. 10, No. 1, February 1992.consume an unfair proportion of resources on a multiprogrammeduniprocessor as well. In either kernellevel or userlevel thread systems,multilevel feedback can be used to encourage applications to providehonest information for processor allocation decisions. The processorallocator can favor address spaces that use fewer processors and penalizethose that use more. This encourages address spaces to give up processorswhen they are needed elsewhere, since the priorities imply that it is likelythat the processors will be returned when they are needed. On the otherhand, if overall the system has fewer threads than processors, the idleprocessors should be left in the address spaces most likely to create workin the near future, to avoid the overhead of processor reallocation when thework is created. Many production uniprocessor operating systems do something similar.Average response time, and especially interactive performance, isimproved by favoring jobs with the least remaining service, oftenapproximated by reducing the priority of jobs as they accumulate servicetime. We expect a similar policy to be used in multiprogrammedmultiprocessors to achieve the same goal this policy could easily beadapted to encourage honest reporting of idle processors.3.3  Critical SectionsOne issue we have not yet addressed is that a userlevel thread could beexecuting in a critical section at the instant when it is blocked orpreempted.1 There are two possible ill effects poor performance e.g.,because other threads continue to test an applicationlevel spinlock heldby the preempted thread 28, and deadlock e.g., the preempted threadcould be holding the ready list lock if so, deadlock would occur if theupcall attempted to place the preempted thread onto the ready list.Problems can occur even when critical sections are not protected by alock. For example, FastThreads uses unlocked perprocessor really, peractivation free lists of thread control blocks to improve latency 2accesses to these free lists also must be done atomically. Prevention andrecovery are two approaches to dealing with the problem of inopportunepreemption. With prevention, inopportune preemptions are avoidedthrough the use of a scheduling and locking protocol between the kernel1  The need for critical sections would be avoided if we were to use waitfree synchronization11. Many commercial architectures, however, do not provide the required hardware support we assume only an atomic testandset instruction in addition, the overhead of waitfree synchronization can be prohibitive for protecting anything but very small data structures.Table III Scheduler Activation Upcall PointsAdd more processors additional  of processors neededAllocate more processors to this address space and start them runningscheduler activations.This processor is idle Preempt this processor if another address space needs it.Scheduler Activations Effective Kernel Support     l      67ACM Transactions on Computer Systems, Vol. 10, No. 1, February 1992.and the user level. Prevention has a number of serious drawbacks,particularly in a multiprogrammed environment. Prevention requires thekernel to yield control over processor allocation at least temporarily tothe userlevel, violating the semantics of address space priorities.Prevention is inconsistent with the efficient implementation of criticalsections that we will describe in Section 4.3. Finally, in the presence ofpage faults, prevention requires pinning to physical memory all virtualpages that might be touched while in a critical section identifying thesepages can be cumbersome.Instead, we adopt a solution based on recovery. When an upcall informsthe userlevel thread system that a thread has been preempted orunblocked, the thread system checks if the thread was executing in acritical section. Of course, this check must be made before acquiring anylocks. if so, the thread is continued temporarily via a userlevel contextswitch. When the continued thread exits the critical section, it relinquishescontrol back to the original upcall, again via a userlevel context switch.At this point, it is safe to place the userlevel thread back on the ready list.We use the same mechanism to continue an activation if it was preemptedin the middle of processing a kernel event.This technique is free from deadlock. By continuing the lock holder, weensure that once a lock is acquired, it is always eventually released, evenin the presence of processor preemptions or page faults. Further, thistechnique supports arbitrary userlevel spinlocks, since the userlevelthread system is always notified when a preemption occurs, allowing it tocontinue the spinlock holder. Although correctness is not affected,processor time may be wasted spinwaiting when a spinlock holder takesa page fault a solution to this is to relinquish the processor after spinningfor a while 4.4. IMPLEMENTATIONWe have implemented the design described in Section 3 by modifyingTopaz, the native operating system for the DEC SRC Fireflymultiprocessor workstation, and FastThreads, a userlevel thread package.We modified the Topaz kernel thread management routines toimplement scheduler activations. Where Topaz formerly blocked,resumed, or preempted a thread, it now performs upcalls to allow the userlevel to take these actions see Table II. In addition, we modified Topazto do explicit allocation of processors to address spaces formerly, Topazscheduled threads obliviously to the address spaces to which theybelonged. We also maintained object code compatibility existing Topazand therefore UNIX applications still run as before.FastThreads was modified to process upcalls, to resume interruptedcritical sections, and to provide Topaz with the information needed for itsprocessor allocation decisions see Table III. In all, we added a few hundred lines of code to FastThreads and about1200 lines to Topaz. For comparison, the original Topaz implementationof kernel threads was over 4000 lines of code. The majority of the newTopaz code was concerned with implementing the processor allocationpolicy discussed below, and not with scheduler activations per se.68      l      T. E. Anderson et al.ACM Transactions on Computer Systems, Vol. 10, No. 1, February 1992.Our design is neutral on the choice of policies for allocatingprocessors to address spaces and for scheduling threads onto processors.Of course, some pair of policies had to be selected for our prototypeimplementation we briefly describe these, as well as some performanceenhancements and debugging considerations, in the subsections thatfollow.4.1 Processor Allocation PolicyThe processor allocation policy we chose is similar to the dynamic policyof Zahorjan and McCann 27. The policy spaceshares processors whilerespecting priorities and guaranteeing that no processor idles if there iswork to do. Processors are divided evenly among the highest priorityaddress spaces if some address spaces do not need all of the processors intheir share, those processors are divided evenly among the remainder.Spacesharing reduces the number of processor reallocations processorsare timesliced only if the number of available processors is not an integermultiple of the number of address spaces at the same priority that wantthem.Our implementation makes it possible for an address space to use kernelthreads, rather than requiring that every address space use scheduleractivations. Continuing to support Topaz kernel threads was necessary topreserve binary compatibility with existing possibly sequential Topazapplications. In our implementation, address spaces that use kernel threadscompete for processors in the same way as applications that use scheduleractivations. The kernel processor allocator only needs to know whethereach address space could use more processors or has some processors thatare idle. An application can be in neither state for instance, if it has askedfor a processor, received it, and has not asked for another processor yet.The interface described in Section 3.2 provides this information foraddress spaces that use scheduler activations internal kernel datastructures provide it for address spaces that use kernel threads directly.Processors assigned to address spaces using scheduler activations arehanded to the userlevel thread scheduler via upcalls processors assignedto address spaces using kernel threads are handed to the original Topazthread scheduler. As a result, there is no need for static partitioning ofprocessors.4.2 Thread Scheduling PolicyAn important aspect of our design is that the kernel has no knowledge ofan applications concurrency model or scheduling policy, or of the datastructures used to manage parallelism at the user level. Each application iscompletely free to choose these as appropriate they can be tuned to fit theapplications needs. The default policy in FastThreads uses perprocessorready lists accessed by each processor in lastinfirstout order to improvecache locality a processor scans for work if its own ready list is empty.This is essentially the policy used by Multilisp 10.In addition, our implementation includes hysteresis to avoid unnecessaryprocessor reallocations an idle processor spins for a short period beforeScheduler Activations Effective Kernel Support     l      69ACM Transactions on Computer Systems, Vol. 10, No. 1, February 1992.notifying the kernel that it is available for reallocation. 4.3  Performance EnhancementsWhile the design as just described is sufficient to provide userlevelfunctionality equivalent to that of kernel threads, there are some additionalconsiderations that are important for performance.The most significant of these relates to critical sections, described inSection 3.3 In order to provide temporary continuation of critical sectionswhen a userlevel thread is preempted or when it blocks in the kernel andcan be resumed, the userlevel thread system must be able to checkwhether the thread was holding a lock. One way to do this is for the threadto set a flag when it enters a critical section, clear the flag when it leaves,and then check to see if it is being continued. The check is needed so thatthe thread being temporarily continued will relinquish the processor to theoriginal upcall when it reaches a safe place. Unfortunately, this imposesoverhead on lock acquisition and release whether or not a preemption orpage fault occurs, even though these events are infrequent. Latency isparticularly important since we use these continuable critical sections inbuilding our userlevel thread system.We adopt a different solution that imposes no overhead in the commoncase a related technique was used on a uniprocessor in the TrellisOwlgarbage collector 17. We make an exact copy of every lowlevel criticalsection. We do this by delimiting, with special assembler labels, eachcritical section in the C source code for the userlevel thread package wethen postprocess the compilergenerated assembly code to make the copy.This would also be straightforward to do given language and compilersupport. At the end of the copy, but not the original version of the criticalsection, we place code to yield the processor back to the resumer. Normalexecution uses the original code. When a preemption occurs, the kernelstarts a new scheduler activation to notify the userlevel thread systemthis activation checks the preempted threads program counter to see if itwas in one of these critical sections, and if so, continues the thread at thecorresponding place in the copy of the critical section. The copyrelinquishes control back to the original upcall at the end of the criticalsection. Because normal execution uses the original code, and this code isexactly the same as it would be if we were not concerned aboutpreemptions, there is no impact on lock latency in the common case. Inour implementation, occasionally a procedure call must be made fromwithin a critical section. In this case, we bracket the call, but not thestraight line path, with the setting and clearing of an explicit flag.A second significant performance enhancement relates to themanagement of scheduler activations. Logically, a new scheduleractivation is created for each upcall. Creating a new scheduler activation isnot free, however, because it requires data structures to be allocated andinitialized. Instead, discarded scheduler activations can be cached foreventual reuse. The userlevel thread system can recycle an old scheduleractivation by returning it to the kernel as soon as the userlevel thread ithad been running is removed from its context in the case of preemption,after processing the upcall that notifies the user level of the preemption in70      l      T. E. Anderson et al.ACM Transactions on Computer Systems, Vol. 10, No. 1, February 1992.the case of blocking in the kernel, after processing the upcall that notifiesthe user level that resumption is possible. A similar optimization is used inmany kernel thread implementations kernel threads, once created, can becached when destroyed to speed future thread creations 13.Further, discarded scheduler activations can be collected and returned tothe kernel in bulk, instead of being returned one at a time. Ignoring theoccasional bulk deposit of discards, our system makes the same number ofapplicationkernel boundary crossings on IO or processor preemption as atraditional kernel thread system. In a kernel thread system, one crossing isneeded to start an IO, and another is needed when the IO completes. Thesame kernel boundary crossings occur in our system.4.4  Debugging ConsiderationsWe have integrated scheduler activations with the Firefly Topaz debugger.There are two separate environments, each with their own needsdebugging the userlevel thread system and debugging application coderunning on top of the thread system.Transparency is crucial to debuggingthe debugger should have aslittle effect as possible on the sequence of instructions being debugged.The kernel support we have described informs the userlevel thread systemof the state of each of its physical processors, but this is inappropriatewhen the thread system itself is being debugged. Instead, the kernelassigns each scheduler activation being debugged a logical processorwhen the debugger stops or singlesteps a scheduler activation, theseevents do not cause upcalls into the userlevel thread system.Assuming the userlevel thread system is working correctly, thedebugger can use the facilities of the thread system to stop and examinethe state of application code running in the context of a userlevel thread18.5. PERFORMANCEThe goal of our research is to combine the functionality of kernel threadswith the performance and flexibility advantages of managing parallelismat the user level within each application address space. The functionalityand flexibility issues have been addressed in previous sections. In terms ofperformance, we consider three questions. First, what is the cost of userlevel thread operations e.g., fork, block and yield in our system Second,what is the cost of communication between the kernel and the user levelspecifically, of upcalls Third, what is the overall effect on theperformance of applications5.1 Thread PerformanceThe cost of userlevel thread operations in our system is essentially thesame as those of the FastThreads package running on the Firefly prior toour workthat is, running on top of Topaz kernel threads, with theassociated poor system integration. Table IV adds the performance of oursystem to the data for original FastThreads, Topaz kernel threads, andUltrix processes contained in Table I. Our system preserves the order ofScheduler Activations Effective Kernel Support     l      71ACM Transactions on Computer Systems, Vol. 10, No. 1, February 1992.magnitude advantage that userlevel threads offer over kernel threads.There is a 3 sec. degradation in Null Fork relative to originalFastThreads, which is due to incrementing and decrementing the numberof busy threads and determining whether the kernel must be notified. Thiscould be eliminated for a program running on a uniprogrammed machineor running with sufficient parallelism that it can inform the kernel that italways wants as many processors as are available. There is a 5 sec.degradation in SignalWait, which is due to this factor plus the cost ofchecking whether a preempted thread is being resumed in which caseextra work must be done to restore the condition codes. Although still anorder of magnitude better than kernel threads, our performance would besignificantly worse without a zerooverhead way of marking when a lockis held see Section 4.3. Removing this optimization from FastThreadsyielded a Null Fork time of 49 sec. and a SignalWait time of 48 sec.The Null Fork benchmark has more critical sections in its execution paththan does SignalWait.5.2  UpcalI PerformanceThread performance Section 5.1 characterizes the frequent case whenkernel involvement is not necessary. Upcall performancethe infrequentcaseis important, though, for several reasons. First, it helps determinethe breakeven point, the ratio of thread operations that can be done atuser level to those that require kernel intervention, needed for userlevelthreads to begin to outperform kernel threads. if the cost of blocking orpreempting a userlevel thread in the kernel using scheduler activations issimilar to the cost of blocking or preempting a kernel thread, thenscheduler activations could be practical even on a uniprocessor. Further,the latency between when a thread is preempted and when the upcallreschedules it determines how long other threads running in theapplication may have to wait for a critical resource held by the preemptedthread.When we began our implementation, we expected our upcallperformance to be commensurate with the overhead of Topaz kernelthread operations. Our implementation is considerably slower than that.One measure of upcall performance is the time for two userlevel threadsto signal and wait through the kernel this is analogous to the SignalWaittest in Table IV, except that the synchronization is forced to be in thekernel. This approximates the overhead added by the scheduler activationmachinery of making and completing an IO request or a page fault. TheSignalWait time is 2.4 milliseconds, a factor of five worse than Topazthreads.We see nothing inherent in scheduler activations that is responsible forTable IV.  Thread Operation Latencies sec.OperationFastThreads on Topaz ThreadsFastThreads on Scheduler Activations Topaz threads Ultrix processesNull Fork 34 37 948 11300SignalWait 37 42 441 184072      l      T. E. Anderson et al.ACM Transactions on Computer Systems, Vol. 10, No. 1, February 1992.this difference, which we attribute to two implementation issues. First,because we built scheduler activations as a quick modification to theexisting implementation of the Topaz kernel thread system, we mustmaintain more state, and thus have more overhead, than if we haddesigned that portion of the kernel from scratch. As importantly, much ofthe Topaz thread system is written in carefully tuned assembler our kernelimplementation is entirely in Modula2. For comparison, Schroeder andBurrows 19 reduced SRC RPC processing costs by over a factor of fourby recoding Modula2 in assembler. Thus, we expect that, if tuned, ourupcall performance would be commensurate with Topaz kernel threadperformance. As a result, the application performance measurements in thenext section are somewhat worse than what might be achieved in aproduction scheduler activation implementation.5.3  Application PerformanceTo illustrate the effect of our system on application performance, wemeasured the same parallel application using Topaz kernel threads,original FastThreads built on top of Topaz threads, and modifiedFastThreads running on scheduler activations. The application wemeasured was an ON log N solution to the Nbody problem 3. Thealgorithm constructs a tree representing the center of mass of each portionof space and then traverses portions of the tree to compute the force oneach body. The force exerted by a cluster of distant masses can beapproximated by the force that they would exert if they were all at thecenter of mass of the cluster.Depending on the relative ratio of processor speed to available memory,this application can be either compute or IO bound. We modified theapplication to manage a part of its memory explicitly as a buffer cache forthe applications data. This allowed us to control the amount of memoryused by the application a small enough problem size was chosen so thatthe buffer cache always fit in our Fireflys physical memory. As a furthersimplification, threads that miss in the cache simply block in the kernel for50 msec. cache misses would normally cause a disk access. Ourmeasurements were qualitatively similar when we took contention for thedisk into account because the Fireflys floating point performance andphysical memory size are orders of magnitude less than current generationsystems, our measurements are intended to be only illustrative. All testswere run on a six processor CVAX Firefly.First, we demonstrate that when the application makes minimal use ofkernel services, it runs as quickly on our system as on original FastThreadsand much faster than if Topaz threads were used. Figure 2 graphs theapplications speedup versus the number of processors for each of the threesystems when the system has enough memory so that there is negligible IO and there are no other applications running. Speedup is relative to asequential implementation of the algorithm.With one processor, all three systems perform worse than the sequentialimplementation, because of the added overhead of creating andsynchronizing threads to parallelize the application. This overhead isgreater for Topaz kernel threads than for either userlevel thread system. Scheduler Activations Effective Kernel Support     l      73ACM Transactions on Computer Systems, Vol. 10, No. 1, February 1992.As processors are added, the performance with Topaz kernel threadsinitially improves and then flattens out. In Topaz, a thread can acquire andrelease an application lock on a critical section without trapping to thekernel, provided there is no contention for the lock. If a thread tries toacquire a busy lock, however, the thread will block in the kernel and berescheduled only when the lock is released. Thus, Topaz lock overhead ismuch greater in the presence of contention. The good speedup attained byboth userlevel thread systems shows that the application has enoughparallelism it is the overhead of kernel threads that prevents goodperformance. We might be able to improve the performance of theapplication when using kernel threads by restructuring it so that its criticalsections are less of a bottleneck or perhaps by spinning for a short time atuser level if the lock is busy before trapping to the kernel 12 theseoptimizations are less crucial if the application is built with userlevelthreads. The performance of original FastThreads and our system divergesslightly with four or five processors. Even though no other applicationswere running during our tests, the Topaz operating system has severaldaemon threads which wake up periodically, execute for a short time, andthen go back to sleep. Because our system explicitly allocates processorsto address spaces, these daemon threads cause preemptions only whenthere are no idle processors available this is not true with the native Topazscheduler, which controls the kernel threads used as virtual processors byoriginal FastThreads. When the application tries to use all of theprocessors of the machine in this case, six processors, the number ofpreemptions for both userlevel thread systems is similar. Thepreemptions have only a small impact on the performance of originalFastThreads because of their short duration.Next, we show that when the application requires kernel involvementbecause it does IO, our system performs better than either original FastFig. 2.  Speedup of NBody application versus number of processors, 100 of memoryavailable.74      l      T. E. Anderson et al.ACM Transactions on Computer Systems, Vol. 10, No. 1, February 1992.Threads or Topaz threads. Figure 3 graphs the applications execution timeon six processors as a function of the amount of available memory. For all three systems, performance degrades slowly at first, and thenmore sharply once the applications working set does not fit in memory.However, application performance with original FastThreads degradesmore quickly than with the other two systems. This is because when a userlevel thread blocks in the kernel, the kernel thread serving as its virtualprocessor also blocks, and thus the application loses that physicalprocessor for the duration of the IO. The curves for modified FastThreadsand for Topaz threads parallel each other because both systems are able toexploit the parallelism of the application to overlap some of the IOlatency with useful computation. As in Figure 2, though, applicationperformance is better with modified FastThreads than with Topaz becausemost thread operations can be implemented without kernel involvement.Finally, while Figure 3 shows the effect on performance of applicationinduced kernel events, multiprogramming causes systeminduced kernelevents that result in our system having better performance than eitheroriginal FastThreads or Topaz threads. To test this, we ran two copies ofthe Nbody application at the same time on a six processor Firefly andthen averaged their execution times. Table V lists the resulting speedupsfor each system note that a speedup of three would be the maximumpossible.Table V shows that application performance with modified FastThreadsis good even in a multiprogrammed environment the speedup is within5 of that obtained when the application ran uniprogrammed on threeprocessors. This small degradation is about what we would expect frombus contention and the need to donate a processor periodically to run akernel daemon thread. In contrast multiprogrammed performance is muchworse with either original FastThreads or Topaz threads, although fordifferent reasons. When applications using original FastThreads areFig. 3.  Execution time of NBody application versus amount of available memory, 6processors.Scheduler Activations Effective Kernel Support     l      75ACM Transactions on Computer Systems, Vol. 10, No. 1, February 1992.multiprogrammed, the operating system timeslices the kernel threadsserving as virtual processors this can result in physical processors idlingwaiting for a lock to be released while the lock holder is descheduled.Performance is worse with Topaz threads than with our system becausecommon thread operations are more expensive. In addition, because Topazdoes not do explicit processor allocation, it may end up scheduling morekernel threads from one address space than from the other Figure 2shows, however, that performance flattens out for Topaz threads whenmore than three processors are assigned to the application.While the Firefly is an excellent vehicle for constructing proofofconcept prototypes, its limited number of processors makes it less thanideal for experimenting with significantly parallel applications or withmultiple, multiprogrammed parallel applications. For this reason, we areimplementing scheduler activations in C Threads and Mach we are alsoporting Amber 6, a programming system for a network ofmultiprocessors, onto our Firefly implementation.6. RELATED iDEASThe two systems with goals most closely related to our ownachievingproperly integrated userlevel threads through improved kernel supportare Psyche 20 and Symunix 9. Both have support for NUMAmultiprocessors as a primary goal Symunix in a highperformance parallelUNIX implementation, and Psyche in the context of a new operatingsystem.Psyche and Symunix provide virtual processors as described inSections 1 and 2, and augment these virtual processors by definingsoftware interrupts that notify the user level of some kernel events.Software interrupts are like upcalls, except that all interrupts on the sameprocessor use the same stack and thus are not reentrant. Psyche has alsoexplored the notion of multimodel parallel programming in which userdefined threads of various kinds, in different address spaces, cansynchronize while sharing code and data.While Psyche, Symunix, and our own work share similar goals, theapproaches taken to achieve these goals differ in several important ways.Unlike our work, neither Psyche nor Symunix provides the exactfunctionality of kernel threads with respect to IO, page faults andmultiprogramming further, the performance of their userlevel threadoperations can be compromised. We discussed some of the reasons for thisin Section 2 these systems notify the user level of some but not all of thekernel events that affect the address space. For example, neither PsycheTable V.  Speedup of NBody application, Multiprogramming Level  2, 6 Processors,100 of Memory AvailableTopaz ThreadsOriginal FastThreadsNewFastThreads1.29 1.26 2.4576      l      T. E. Anderson et al.ACM Transactions on Computer Systems, Vol. 10, No. 1, February 1992.nor Symunix notify the user level when a preempted virtual processor isrescheduled. As a result, the userlevel thread system does not know howmany processors it has or what user threads are running on thoseprocessors.Both Psyche and Symunix provide shared writable memory between thekernel and each application, but neither system provides an efficientmechanism for the userlevel thread system to notify the kernel when itsprocessor allocation needs to be reconsidered. The number of processorsneeded by each application could be written into this shared memory, butthat would give no efficient way for an application that needs moreprocessors to know that some other application has idle processors. Applications in both Psyche and Symunix share synchronization statewith the kernel in order to avoid preemption at inopportune moments e.g.,while spinlocks are being held. In Symunix, the application sets and laterclears a variable shared with the kernel to indicate that it is in a criticalsection in Psyche, the application checks for an imminent preemptionbefore starting a critical section. The setting, clearing, and checking ofthese bits adds to lock latency, which constitutes a large portion of theoverhead when doing highperformance userlevel thread management 2.By contrast, our system has no effect on lock latency unless a preemptionactually occurs. Furthermore, in these other systems the kernel notifies theapplication of its intention to preempt a processor before the preemptionactually occurs based on this notification, the application can choose toplace a thread in a safe state and voluntarily relinquish a processor. Thismechanism violates the constraint that higher priority threads are alwaysrun in place of lower priority threads.Gupta et al. 9a share our goal of maintaining a onetoonecorrespondence between physical processors and execution contexts forrunning userlevel threads. When a processor preemption or IOcompletion results in there being more contexts than processors, Gupta etal.s kernel timeslices contexts until the application reaches a point whereit is safe to suspend a context. Our kernel eliminates the need for timeslicing by notifying the application thread system of the event whilekeeping the number of contexts constant. Some systems provide asynchronous kernel IO as a mechanism to solvesome of the problems with userlevel thread management onmultiprocessors 9, 25. Indeed, our work has the flavor of anasynchronous IO system when an IO request is made, the processor isreturned to the application, and later, when the IO completes, theapplication is notified. There are two major differences between our workand traditional asynchronous IO systems, though. First, and mostimportant, scheduler activations provide a single uniform mechanism toaddress the problems of processor preemption, IO, and page faults.Relative to asynchronous IO, our approach derives conceptual simplicityfrom the fact that all interaction with the kernel is synchronous from theperspective of a single scheduler activation. A scheduler activation thatblocks in the kernel is replaced with a new scheduler activation when theawaited event occurs. Second, while asynchronous IO schemes mayrequire significant changes to both application and kernel code, ourscheme leaves the structure of both the userlevel thread system and theScheduler Activations Effective Kernel Support     l      77ACM Transactions on Computer Systems, Vol. 10, No. 1, February 1992.kernel largely unchanged.Finally, parts of our scheme are related in some ways to Hydra 26, oneof the earliest multiprocessor operating systems, in which schedulingpolicy was moved out of the kernel. However, in Hydra, this separationcame at a performance cost because policy decisions requiredcommunication through the kernel to a scheduling policy server, and thenback to the kernel to implement a context switch. In our system, anapplication can set its own policy for scheduling its threads onto itsprocessors, and can implement this policy without trapping to the kernel.Longerterm processor allocation decisions in our system are the kernelsresponsibility, although as in Hydra, this could be delegated to adistinguished applicationlevel server.7. SUMMARYManaging parallelism at the user level is essential to highperformanceparallel computing, but kernel threads or processes, as provided in manyoperating systems, are a poor abstraction on which to support this. Wehave described the design, implementation and performance of a kernelinterface and a userlevel thread package that together combine theperformance of userlevel threads in the common case of threadoperations that can be implemented entirely at user level with thefunctionality of kernel threads correct behavior in the infrequent casewhen the kernel must be involved. Our approach is based on providingeach application address space with a virtual multiprocessor in which theapplication knows exactly how many processors it has and exactly whichof its threads are running on those processors. Responsibilities are dividedbetween the kernel and each application address spaceProcessor allocation the allocation of processors to address spaces isdone by the kernel.Thread scheduling the assignment of an address spaces threads to itsprocessors is done by each address space.The kernel notifies the address space thread scheduler of every eventaffecting the address space.The address space notifies the kernel of the subset of userlevel eventsthat can affect processor allocation decisions.The kernel mechanism that we use to implement these ideas is calledscheduler activations. A scheduler activation is the execution context forvectoring control from the kernel to the address space on a kernel event.The address space thread scheduler uses this context to handle the event,e.g., to modify userlevel thread data structures, to execute userlevelthreads, and to make requests of the kernel. While our prototypeimplements threads as the concurrency abstraction supported at the userlevel, scheduler activations are not linked to any particular modelscheduler activations can support any userlevel concurrency modelbecause the kernel has no knowledge of userlevel data structures.78      l      T. E. Anderson et al.ACM Transactions on Computer Systems, Vol. 10, No. 1, February 1992.ACKNOWLEDGMENTSWe would like to thank Andrew Black, Mike Burrows, Jan Edler, MikeJones, Butler Lampson, Tom LeBlanc, Kai Li, Brian Marsh, SapeMullender, Dave Redell, Michael Scott, Garret Swart, and John Zahorjanfor their helpful comments. We would also like to thank the DEC SystemsResearch Center for providing us with their Firefly hardware and software.REFERENCES1. AGHA, G. Actors A Model of Concurrent Computation in Distributed Systems. MITPress, Cambridge, Mass. 1986.2. ANDERSON, T., LAZOWSKA, E., AND LEVY, H. The performance implications of threadmanagement alternatives for shared memory multiprocessors. IEEE Trans. Comput.38, 12 Dec. 1989, 16311644. Also appeared in Proceedings of the 1989 ACMSIGMETRICS and Performance 59 Conference on Measurement and Modeling ofComputer Systems Oakland, Calif., May 1989, pp. 4960.3. BARNES, J., AND HUT, P. A hierarchical ON log N forcecalculation algorithm.Nature 324  1986, 446449.4. BIRRELL, A., GUTTAG, J., HORNING, J., AND LEVIN, R. Synchronization primitives fora multiprocessor A formal specification. In Proceedings of the 11th ACM Symposiumon Operating Systems Principles Austin, Tex., Nov.1987, pp. 94102.5. BLACK, D. Scheduling support for concurrency and parallelism in the Mach operatingsystem. IEEE Comput. Mag. 23, 5 May 1990, 3543.6. CHASE, J., AMADOR, F., LAZOWSKA, E., LEVY, H., AND LITTLEFIELD, R. The Ambersystem Parallel programming on a network of multiprocessors. In Proceedings of the12th ACM Symposium on Operating Systems Principles Litchfield Park, Ariz.,Dec.1989, pp. 147158.7. CHERITON, D. The V distributed system. Commun. ACM. 31, 3 Mar. 1988, 314333. 8. DRAVES, R., AND COOPER, E. C Threads. Tech. Rep. CMUCS88154, School ofComputer Science, Carnegie Mellon Univ., June 1988.9. EDLER, J., LIPKIS, J., AND SCHONBERG, E. Process management for highly parallelUNIX systems. In Proceedings of the USENIX workshop on UNIX andSupercomputers Sept. 1988, pp. 117 9A. GUPTA, A., TUCKER, A., AND STEVENS, L. Making effective use of sharedmemorymultiprocessors The process control approach. Tech. Rep. CSLTR91475A,Computer Systems Laboratory, Stanford Univ., July 1991. 10. HALSTEAD, R. Multilisp A language for concurrent symbolic computation. ACMTrans. Program. Lang. Syst. 7, 4 Oct. 1985, 501538.11. HERLIHY, M. A methodology for implementing highly concurrent data structures. InProceedings of the 2nd ACM SIGPLAN Symposium on Principles and Practice ofParallel Programming  Seattle, Wash., Mar. 1990, pp. 197206.12. KARLIN, A., LI, K., MANASSE, M., AND OWICKI, S. Empirical studies of competitivespinning for a sharedmemory multiprocessor. In Proceedings of the 13th ACMSymposium on Operating Systems Principles Pacific Grove, Calif., Oct. 1991, pp.4155.13. LAMPSON, B., AND REDELL, D. Experiences with processes and monitors in Mesa.Commun. ACM. 23, 2 Feb. 1980, 104117. 14. LO, S.P., AND GLIGOR, V. A comparative analysis of multiprocessor schedulingalgorithms. In Proceedings of the 7th International Conference on DistributedComputing  Systems Sept. 1987, pp. 356363.15. MARSH, B., SCOTT, M., LEBLANC, T., AND MARKATOS, E. Firstclass userlevelthreads. In Proceedings of the 13th ACM Symposium on Operating Systems PrinciplesPacific Grove, Calif., Oct. 1991, pp. 110121.16. MOELLERNIELSEN, P., AND STAUNSTRUP, J. Problemheap A paradigm formultiprocessor algorithms. Parallel  Comput. 4, 1 Feb. 1987, 6374. 17. MOSS, J., AND KOHLER, W. Concurrency features for the TrellisOwl language. InProceedings of European Conference on ObjectOriented Programming 1987Scheduler Activations Effective Kernel Support     l      79ACM Transactions on Computer Systems, Vol. 10, No. 1, February 1992.ECOOP 87 June 1987, pp. 171180. 18. REDELL, D. Experience with Topaz teledebugging. In Proceedings of the ACMSIGPLANSIGOPS Workshop on Parallel and Distributed Debugging Madison,Wisc., May 1988, pp. 3544. 19. SCHROEDER, M., AND BURROWS, M. Performance of Firefly RPC. ACM Trans.Comput. Syst. 8, 1 Feb. 1990, 117.20. SCOTT, M., LEBLANC, T., AND MARSH, B. Multimodel parallel programming inPsyche. In Proceedings of the 2nd ACM SIGPLAN Symposium on Principles andPractice of Parallel Programming Mar. 1990, pp. 7078.21. TEVANIAN, A., RASHID, R., GOLUB, D., BLACK, D., COOPER, E., AND YOUNG, M.Mach Threads and the Unix Kernel The battle for control. In Proceedings of the 1987USENIX Summer Conference 1987, pp. 185197. 22. THACKER, C., STEWART, L., AND SATTERTHWAITE, JR., E. Firefly A multiprocessorworkstation. IEEE Trans. Comput. 37, 8 Aug. 1988, 909920.23. TUCKER, A., AND GUPTA, A. Process control and scheduling issues formultiprogrammed shared memory multiprocessors. In Proceedings of the 12th ACMSymposium on Operating Systems Principles Litchfield Park, Ariz., Dec. 1989, pp.159166. 24. VANDEVOORDE, M., AND ROBERTS, E. WorkCrews An abstraction for controllingparallelism. Int. J. Parallel Program. 17, 4 Aug. 1988, 347366. 25. WEISER, M., DEMERS, A., AND HAUSER, C. The portable common runtime approach tointeroperability. In Proceedings of the 12th ACM Symposium on Operating SystemsPrinciples Litchfield Park, Ariz., Dec. 1989, pp. 114122.26. WULF, W., LEVIN, R., AND HARBISON, S. HydraC.mmp An Experimental ComputerSystem. McGrawHill, New York, 1981. 27. ZAHORJAN, J., AND MCCANN, C. Processor scheduling in shared memorymultiprocessors. In Proceedings of the 1990 ACM SIGMETRICS Conference onMeasurement and Modeling of Computer Systems Boulder, Colo., May 1990, pp.214225. 28. ZAHORJAN, J., LAZOWSKA, E., AND EAGER, D. The effect of scheduling discipline onspin overhead in shared memory multiprocessors. IEEE Trans. Parallel Distrib. Syst.2, 2 Apr. 1991, 180198.Received June 1991 revised August 1991 accepted September 1991
