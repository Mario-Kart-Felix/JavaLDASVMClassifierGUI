PlanetLab An Overlay Testbed forBroadCoverage ServicesBrent Chun, David Culler, Timothy RoscoeIntel Research  BerkeleyAndy Bavier, Larry Peterson, Mike WawrzoniakPrinceton UniversityMic BowmanIntel CorporationABSTRACTPlanetLab is a global overlay network for developing and accessing broadcoverage network services. Our goal is to growto 1000 geographically distributed nodes, connected by a diverse collection of links. PlanetLab allows multiple servicesto run concurrently and continuously, each in its own slice ofPlanetLab. This paper describes our initial implementationof PlanetLab, including the mechanisms used to implementvirtualization, and the collection of core services used tomanage PlanetLab.1. INTRODUCTIONThe last few years have seen the emergence of a new class ofnetwork services, including networkembedded storage 12,peertopeer file sharing 19, 5, content distribution networks 25, robust routing overlays 20, 2, scalable objectlocation 3, 18, 23, 17, and scalable event propagation 6.Researchers are also beginning to deploy network measurement and monitoring tools 21, 22.What all these applications have in common is that theybenefit from being widely distributed over the Internet. Tosupport the design and evaluation of such applications, weare building a global overlay network called PlanetLab. Ourgoal is to grow PlanetLab to 1000 geographically distributednodes connected by a diverse collection of links, includingedge sites, colocation and routing centers, and homes i.e.,at the end of DSL lines and cable modems. This paper describes an early version of PlanetLab Version 0.5 in January2003, originally deployed on 100 nodes distributed across 42sites.To appreciate the design decisions we have made, it is important to understand that while we have a shortterm goal ofsupporting experimentation with the types of services mentioned above, our mediumterm goal is to support continuously running services that potentially serve a client community. In other words, PlanetLab is designed to support theseamless migration of an application from early prototype,through multiple design iterations, to a popular service thatcontinues to evolve. In the longterm, we envision PlanetLab serving as a microcosm for the next generation Internet16.2. ARCHITECTURAL OVERVIEWThe centerpiece of the PlanetLab architecture is a slice ahorizontal cut of global PlanetLab resources. Each servicea set of distributed and cooperating programs deliveringsome higherlevel functionality runs in a slice of PlanetLab. A slice encompasses some amount of processing, memory, storage, and network resources across a set of individual PlanetLab nodes distributed over the network. A sliceis more than just the sum of the distributed resources, however. It is more accurate to view a slice as a network ofvirtual machines, with a set of local resources bound to eachvirtual machine.A virtual machine is the environment where the programthat implements some aspect of a service runs. Each virtualmachine runs on a single node and is allowed to consumesome fraction of that nodes resources. In addition to being bound to a set of resources, a virtual machine also defines the programming interface execution environment towhich programs are written. Multiple virtual machines runon each PlanetLab node, where a virtual machine monitorVMM arbitrates the nodes resources among them.This section gives a highlevel overview of the PlanetLabarchitecture, first from the local pernode perspective, andthen from the global networkwide perspective. This overview is still evolving, but provides the framework in whichthe specific mechanisms described in the next two sectionsare defined.2.1 Node PerspectivePlanetLab slices are collections of virtual machines, eachrunning on a physical PlanetLab node. Each node, therefore, has to provide a virtual machine abstraction. Thisvirtual machine must be useful it should be no harder toprogram than a conventional server machine and protectedfrom other virtual machines on the same node. In addition,node resources CPU, local disk space, network bandwidth,etc. must be shared fairly so that one slice cannot starveanother. Virtual machines must also be restricted in someways, particularly with regard to the volume and nature ofnetwork traffic they can generate. This is because unlikemany testbeds, PlanetLab is implemented over the realInternet, and experimental PlanetLab services must coexistpeaceably with existing Internet traffic.The kind of virtual machine available to a slice, and theoperating system VMM that provides it, is one of the central design questions in PlanetLab. The solutions adoptedare also expected to change over time as PlanetLab evolvesACM SIGCOMM Computer Communications Review Volume 33, Number 3 July 20033planned obsolescence of building blocks is an important aspect of the design.PlanetLabs design philosophy draws an important distinction between the Application Programming Interface usedby typical services and the Protection Interface implementedby the VMM. The latter is the level at which both interVM protection and resource allocation to VMs is performed.Once separated, these two interfaces can evolve relatively independently. To some extent, solutions in the design spacefor PlanetLab node virtualization mechanisms are characterized by where these two interfaces are drawn.For example, software runtimes such as the Java Virtual Machine and the Microsoft Common Language Runtime placethe API at a very high level of abstraction, and rely onan underlying operating system to provide protection andresource control between applications. This option was rejected for PlanetLab because of the large amount of designpolicy mandated by these systemsmore lightweight solutions can always support such language VMs inside theirown VM, but allow much greater flexibility and can oftenavoid the overhead of the entire runtime.At the other end of the scale, complete virtual machine monitors like VMware provide a protection interface at a verylow level of abstraction that of conventional hardware. Thisoffers complete flexibility in choice of API any operatingsystem can be run in a VM, even without recompilation,and excellent protection properties slices are completely encapsulated along with their associated operating systems,but comes at a high price in CPU and memory resourceseven highend commercial server VMMs typically support10s of VMs per machine, which is insufficient for PlanetLab.Mainstream operating systems such as Unix provide protection and resource control at the same level as the APIe.g. at the Unix system callprocess interface. For example, a PlanetLab node implementation using such an OSmight give each slice a process group on the machine. Thishas efficiency advantages since the API and protection interfaces are relatively highlevel, considerable sharing of resources e.g., physical memory for code and data is possible,and thus, many slices can be supported on a machine. Thedownside is that protection is problematic, and the numberof resources that must be allocated is large, with a highlevelprotection interface introducing much complexity to the system e.g., policies may now need to include file descriptors,sockets, network port numbers, loopback interfaces, and soon.A middle ground between the complete virtualization andmainstream operating systems are recent modifications toUnix systems to provide virtual kernels or virtual servers.Examples of this approach include Linux VServers 10, TheBSD Jail 11, UserMode Linux 24, and commercial offerings such as Ensims Private Server technology 7. Suchsystems provide the sharing efficiencies of the Unix option,but with better possibilities for protection and resource control. On the downside, supporting alternative, future APIsin an efficient manner remains problematic. Version 0.5 ofPlanetLab, as described in this paper, uses Linux VServerswith additional resource control and protection facilities derived from the Scout 15 operating system.Recently, the notion of an isolation kernel has been proposed by at least two groups in the research community 26,8. Isolation kernels claim to occupy an attractive point inthe design space they provide a protection interface close toreal hardware suitable for running and consequently evolving operating systems and their APIs, and a lowlevel multiplexing point for system resources. Unlike pure VMs, however, the hardware presented to a guest operating systemis considerably more efficient for the operating system tohandle, and more cooperation between VMs and the VMMitself is possible.2.2 Network PerspectiveAt the global level, the central challenge is discovering whatresources are available, dynamically creating slices that spanthese resources, and launching services within these slices.This process is rooted in the individual nodes, which indirectly issue tickets that specify resource amounts e.g.,cycle and link bandwidths and a time frame for which theresources can be acquired. The bearer of a ticket can redeemit at the node for a lease on the resources over the given timeframe, subject to the nodes admission control policy. In theabstract, this process works as follows Section 4 describedthe implementation in Version 0.5.First, a node manager, implemented as part of the VMM,runs on each node. It takes a set of tickets as input, andconsulting the local admission control policy, determines ifthe tickets can be redeemed. The ticket set is signed by anauthority that the node trusts to enforce the global allocation policy. If the request can be satisfied, the node managerreserves the specified resources, creates a virtual machine,binds it to those resources, and returns a lease. This leaseis later used by the service manager to launch a programwithin the virtual machine.Second, a resource monitor is an service running on eachnode. It monitors resource availability on the node and periodically reports the results to one or more agents. Resourcemonitors depend on an interface exported by the virtualmachine that allows them to record certain facts about thestate of the node e.g., the current CPU load and the available link bandwidth.Third, an agent collects resource availability informationfrom a set of resource monitors or other agents, and issuestickets that can be used to acquire resources on the monitored nodes. An agent can respond to two kinds of queries1 it can advertise the tickets it is holding that meet certain criteria, and 2 it can grant tickets themselves to arequester. The agent may also overbook the resources available on the nodes it is responsible for.Fourth, a resource broker responds to queries from servicemanagers trying to discover a slice to run in. Each querydescribes the resources needed by the service, and specifiesthe principal on whose behalf the request is being made.Given a query, the broker first contacts one or more agents todiscover what tickets it is possible to obtain. It then matchesthis information with the services resource requirements toACM SIGCOMM Computer Communications Review Volume 33, Number 3 July 20034produce a slice specification, contacts the relevant agents toobtain the tickets, and returns them to the service manager.Finally, a service manager is associated with each service. Itcontacts a resource broker to discover a slice and obtain thetickets needed to instantiate it. It then submits the ticketsto the admission control mechanism on each node to create a network of virtual machines. Should virtual machinecreation succeed on each node, the service manager thenlaunches the service, that is, loads and starts a program ineach virtual machine. Admission control returns a lease onthe slice. The manager must periodically renew this lease.Broker Service24153AgentNodeNodeAdsRequestRequestTicketsTicketsLeaseMonitoringMonitoringFigure 1 Acquiring a SliceFigure 1 shows one picture of how these components mightinteract. Step 1 shows resource monitoring information flowing from a set of nodes to their trusted agent. At step 2,a resource broker running within the service manager requests a description of tickets held by the agent, and theagent responds with a set of advertisements. At step 3, thebroker combines the advertisements with the known servicerequirements to produce a slice specification. The brokerrequests the tickets for the slice from the agent at step 4and the agent replies with the tickets. Finally, at step 5 theservice presents the tickets to the node on which they aregood the node instantiates a virtual machine bound to theresources and returns a lease to this VM to the service.3. VIRTUAL MACHINESThis section describes how PlanetLab implements virtualmachines on each node. The plan is to evolve the PlanetLab VMM, by initially supporting a single, wellknown APIthe Linux system call interface, and then changing theunderlying implementation over time. As described in thissection, Version 0.5 adopts the virtualized kernel strategy.We are exploring the use of isolation kernels as the basis fornode virtualization in future versions of PlanetLab.3.1 VserversVservers is a patch to the Linux 2.4 kernel that provides theillusion of multiple, independently managed virtual serversvservers running on a single machine 10. It implementsa level of virtualization at the system call interface by extending the nonreversible isolation provided by chroot forfilesystems to other operating system resources such as processes and SysV IPC. Processes within a vserver are givenfull access to files, processes, SysV IPC, network interfaces,and accounts which can be named in their containing vserverand are denied access to all other operating system resourcesotherwise. Each vserver is also given a weaker form of rootalong with its own UIDGID namespace which allows eachvserver to have its own superuser while at the same timenot compromising the security of the underlying machine.3.1.1 VirtualizationVirtualization is implemented at the system call interfaceand isolation is enforced based on the idea of a security context. Each vserver on a machine is assigned a unique securitycontext, and each process running on that machine is associated with a specific vserver through its security context. Aprocesss security context is assigned via a new system calland inherited by all of the processs descendants. Isolationbetween different vservers is enforced through the systemcall interface by using a combination of a processs securitycontext and UIDGID when checking access control privileges and deciding what information should be exposed to agiven process. A special security context context 1 is givena complete view of the entire machine and, as described inSection 4.3, is used to create administrative slices.By virtualizing above a standard Linux kernel, vservers achievescalability through large amounts of resource sharing and noactive state for idle vservers. Sharing of physical memoryand disk space is substantial. For physical memory, savingsare accrued by having a single copy of the kernel, a singlecopy of all kernel and userlevel daemons, and, perhaps mostimportantly, sharing of readonly and copyonwrite memory segments across unrelated vservers. Disk space sharing is also significant due to the introduction of the filesystem immutable invert bit which allows for a primitive formof filesystem copyonwrite COW. By using COW on chrooted vserver root filesystems, vserver disk footprints arereduced to just 5.7 of what would be requiring with copying Section 3.1.3. Achieving comparable amounts of sharing in a virtual machine monitor or isolation kernel approachis strictly harder, albeit the isolation guarantees are different.Virtualizing above the kernel, however, comes at a costweaker guarantees on isolation and additional challenges foreliminating QoS crosstalk. Unlike virtual machine monitorsand isolation kernels that provide isolation at a lowlevel,vservers implement isolation at the system call interface.Hence, a malicious vserver that exploits some obscure bugin the Linux operating system could potentially gain controlof the underlying operating system, and hence compromisesecurity of the machine. In practice, we have yet to observesuch an incident. However, in principle, such an attack isstill possible. Such an attack would not be possible in aVMM or isolation kernel. Another cost incurred by virtualizing above the kernel is QoS crosstalk. Eliminating allforms of QoS crosstalk e.g., interactions through the Linuxbuffer cache is strictly harder in a vserverbased approach,even in the presence of proportionalshare schedulers.3.1.2 Vserver RootA weaker version of root allows each vserver to have its ownsuperuser while at the same time not compromising the security of the underlying machine. Superuser privileges aregranted safely to vservers by having each vserver root relinquish a subset of the true superusers capabilities andACM SIGCOMM Computer Communications Review Volume 33, Number 3 July 20035by leveraging the isolation already provided by vservers tolimit the scope of a vserver roots activities to the containing vserver. In Linux, access control for privileged operations is based on a capabilities system. Capabilities determine whether privileged operations such as pinning physicalmemory or rebooting the machine are allowed or disallowed.Vserver root is denied all capabilities that could underminethe security of the machine e.g., accessing raw devices andgranted all other capabilities.Despite having only a subset of the true superusers capabilities, vserver root is still useful in practice. It allows formodification of the vservers root filesystem which, for example, allows users to customize what software packages areinstalled in a particular vserver. Combined with pervserverUIDGID namespaces, it allows vservers to implement theirown internal account management schemes e.g., by maintaining a vserverspecific etcpasswd and running an sshddaemon a different TCP port, which provides the basis forpotential integration with other widearea testbeds such asNetBed 28 and RON 1. Finally, as we gain additional experience on what privileges services actually require, addingadditional extensions to the existing set of Linux capabilitiesprovides a natural path towards exposing privileged operations in a controlled manner.3.1.3 ScalabilityScalability in the current implementation is determined primarily by disk space for vserver root filesystems and servicespecific storage. On PlanetLab, each vserver is created witha root filesystem that points back to a trimmeddown reference root filesystem which comprises 1408 directories and28003 files covering 508 MB of disk. Using vservers primitive COW on all files, excluding those in etc and var, eachvserver root filesystem mirrors the reference root filesystemwhile only requiring 29 MB of disk space, 5.7 of the original root filesystem size. This 29 MB consists of 17.5 MBfor a copy of var, 5.6 MB for a copy of etc, and 5.9MB to create 1408 directories 4 KB per directory. Giventhe reduction in vserver disk footprints afforded by COW,hundreds of vservers can easily coexist on a given PlanetLab node. In the future, we would like to push disk spacesharing even further by using a true filesystem COW and applying techniques from systems such as the Windows SingleInstance Store 4.Operating system resource limits are a secondary factor whichultimately determine the scalability of vservers. While eachvserver is provided with the illusion of its virtual executionenvironment, there still remains a single copy of the underlying operating system and associated kernel resources. Underheavy degrees of concurrent vserver activity, it is possiblethat limits on kernel resources may become exposed andconsequently limit system scalability. We have already observed this with file descriptors. The nature of such limits,however, are no different from that of large degrees of concurrency or resource usage within a single vserver or evenon an unmodified Linux kernel. In both cases, one solutionis to simply extend kernel resource limits by recompiling thekernel. Of course, simple scaling up of kernel resources maybe insufficient if inefficient algorithms are employed withinthe kernel e.g., On searches on linked lists. Thus far, wehave yet to run into these types of algorithmic bottlenecks.3.2 Protected Raw SocketsOne key decision made very early in design process was thatusers of PlanetLab should not have root access to the machines. This is because we expect PlanetLab to support alarge number of users researchers that cannot all be trustedto not misuse root priviledge. On the other hand, we recognize that many users will need access to services that normally require root priviledge. Access to raw sockets is onesuch example.Our resolution of this dilemma is to provide a protectedversion of the priviledged service. For example, in the caseof raw sockets, rather than allow a service to gain accessto all incoming packets and write arbitrary packets to thenetwork, services are forced create sockets that are bound tospecific UDP or TCP ports incoming packets are classifiedand delivered only the to the service that created the socket,and outgoing packets are filtered to ensure that they areproperly formed e.g., the process does not spoof the sourceIP address or UDPTCP port numbers. The protected rawsocket facility consults a policy database that indicates whatport numbers are allocated to each service.3.2.1 UDP and TCP SocketsProtected raw sockets use the standard Linux socket APIwith minor semantic differences. Just as in standard Linux,first the socket must be created with thesocketint domain, int type, int protocolsystem call. To create a protected raw socket, the domainargument must be set to PF INET, type to SOCK RAW andprotocol can be either IPPROTO TCP or IPPROTOT UDP forTCP or UDP sockets, respectively. Return values are thesame as in Linux.Once the socket is created, it is necessary to bind it to aparticular local port of the specified protocol. The standardLinux bind system call is used. For example, the followingcode fragmentbzerochar  sin, sizeofsinsin.sinport  htons9090ifbindsock, struct sockaddr  sin, sizeofsin 0 perrorbindexit1binds the previously created socket to local TCP port 9090.After the socket is created and bound to a local port, it isready to be used to send and receive data. The usual send,sendto, sendmsg, recv, recvfrom, recvmsg and selectcalls can be used. The data received includes the IP andTCPUDP headers, but not the link layer header. The datasent, by default, does not need to include the IP header aservice that wants to include the IP header sets the IP HDRINCLsocket option on the socket. The IP and UDPTCP checksum are performed by the kernel.ACM SIGCOMM Computer Communications Review Volume 33, Number 3 July 200363.2.2 ICMP SocketsICMP packets can be sent and received through protectedraw ICMP sockets. To protect users from interfering witheach other, each ICMP socket is allowed to send and receive only packets of the registered type bound to the socket.Similar to standard sockets, the bind system call is used tospecify the packets that are to be received and sent througha socket. To receive and send packets associated with a specific local TCPUDP port e.g., Destination Unreachable,Source Quench, Redirect, Time Exceeded, Parameter Problem, the ICMP socket needs to be bound to the specificport. For example, the following code fratmentifsock  socketPFINET, SOCKRAW, IPPROTOICMPUDP 0 perrorsocketexit1bzerochar  sin, sizeofsinsin.sinport  htons9090ifbindsock, struct sockaddr  sin, sizeofsin 0 perrorbindexit1creates and binds the ICMP socket to local UDP port 9090.Only ICMP messages associated with the local UDP port9090 can be received and sent through this socket.To exchange ICMP messages that are not associated witha specific TCPUDP port numbere.g., Echo, Echo Reply, Timestamp, Timestamp Reply, Information Request,and Information Replythe socket has to be bound to aspecific ICMP identifier. The ICMP identifier is a 16bitfield present in the ICMP header that is used to demultiplexmatch packets. Only messages containing the rightidentifier are received and sent through a protected rawICMP socket. For example, the following code fragmentifsock  socketPFINET, SOCKRAW, IPPROTOICMP 0 perrorsocketexit1bzerochar  sin, sizeofsinsin.sinport  htons23456ifbindsock, struct sockaddr  sin, sizeofsin 0 perrorbindexit1creates and binds the ICMP socket to identifier 23456.3.3 Resource Limits and IsolationPlanetLab provides resource limits per vserver as well as resource isolation between the vservers running on a node.More specifically, resource limits on outgoing traffic protect the rest of the world from PlanetLab, while resourceisolation between vservers protect PlanetLab services fromeach other. This section describes the resource managementmechanisms now in place on PlanetLab and outlines futurework in this area.PlanetLab currently enforces a cap on the total outgoingbandwidth of a node while providing fair service betweenvservers. This is done using the hierarchical token buckethtb queueing discipline of the Linux Traffic Control facilitytc 13 as follows. First, the node administrator configuresthe root token bucket with the maximum rate at which heis willing to allow traffic to leave the node. Next, for eachvserver, a token bucket is automatically created that is achild of the root token bucket. The htb queueing disciplinethen provides each child token bucket with its configuredrate, and fairly distributes the excess capacity from the rootto the children that can use it in proportion to their rates.For example, if the node administrator sets the root tokenbucket rate to 5Mbps, and the rate of each vserver tokenbucket is set to 1Kbps, the 5Mbps will be fairly dividedamong all vservers in this case we expect vservers to usemore than 1Kbps, but we assign each the same small rateso that they fairly share the total.In addition to this general ratelimiting facility, PlanetLabalso limits the outgoing rate for certain classes of packetsthat may raise alarms within the network. For instance, wemay choose to limit the rate of outgoing pings, or of packetscontaining IP options to a small number per second thissimply involves creating additional child token buckets usinghtb and classifying outgoing packets so that they end up inthe correct bucket. Figuring out reasonable output rates forpotentially troublesome packets is ongoing work.Isolation between vservers is desirable to minimize crosstalk among slices as they contend for resources. Two possible approaches to providing resource isolation are fairnessand guarantees. Fairness ensures that each of the N slicesrunning on a node receives no less than 1N of the availableresources during periods of contention, while guarantees provide a slice with a reserved amount of the resource. In thelatter case, the scheduling mechanism that provides guarantees as a consequence provides isolation for instance, ifa slice is truly guaranteed 10Mcps on the CPU, then thisguarantee protects it from excessive CPU usage by otherslices. PlanetLab will ultimately provide CPU and bandwidth guarantees for slices that request them, and fair besteffort service for the rest.The hierarchical token bucket rate limiter described abovefairly distributes bandwidth between outgoing packet flows,and can also serve as a first step towards granting bandwidthguarantees for slices. For instance, in order to provide avserver with 1Mbps of bandwidth on a node, the rate of thevservers token bucket can be set to this amount. The tokenbucket can be configured so that the vserver can share theexcess capacity over 1Mbps, or its usage can be capped to1Mbps. In either case, the admission controller must ensurethat the sum of all vserver token bucket rates is no greaterACM SIGCOMM Computer Communications Review Volume 33, Number 3 July 20037than the root bucket rate in order to provide a reasonableguarantee of service. We require more experience to determine how well the htb mechanism is able to provide serviceswith true bandwidth guarantees.The Linux CPU scheduler provides approximate fairness between processes, but this raises two problems for PlanetLab. First, a vserver with many processes could use upmore than its fair share of the CPU. We want to enforceisolation primarily on the level of vservers rather than processes. Second, the standard Linux scheduler cannot provideCPU guarantees in the form of reservations. We are currently planning on leveraging Scouts 15 CPU schedulinginfrastructure to provide vservers with fairness and resourceguarantees, but this feature has not yet been deployed inPlanetLab.4. MANAGEMENT SERVICESRather than view PlanetLab management as a single, fixedservice, our approach is to unbundled management of theoverlay into a set of largely independent services, each running in their own slice of PlanetLab. Some aspects of management are servicespecific e.g., monitoring the health ofa running service, while others are part of the shared infrastructure e.g., discovering the set of available nodes.PlanetLab provides an initial version of the latter services,as described in this section, although our expectation is thatthey will be replaced by better alternatives over time.4.1 System Installation and UpdatePlanetLab poses challenges for the maintenance of infrastructure software on PlanetLab nodes, in particular becausethe system software is expected to evolve over time, whilesupporting continuously running services. It is a requirement to be able to upgrade pretty much all of PlanetLabscore software, including the operating system kernel, underremote control. Fortunately, the nature of the applicationsrunning over PlanetLab reduces the burden of maintainingcontinuous operation it is safe to take a single node downfor a software upgrade since applications should assume thata small number of nodes will always be going down anyway.Our original approach to installing software on machines,adopted for reasons of rapid initial deployment, was to usethe U.C. Berkeley RootStock 14 system. This had the advantage that it included an update daemon on each node toallow software to be updated after installation, but suffereda number of drawbacks in the PlanetLab case, the most serious being that a human operator is required to insert andremove a floppy disk when the machine is installed.The solution currently adopted consists of three components a powerful boot monitor a Linux kernel booted froma CD that allows almost arbitrary remote manipulation ofthe machine, a boot server that securely downloads instructions to a newlybooted machine including complete installsof operating system software, and a process for runtimeupdate of nonkernel packages which we retain from RootStock. The system is centralized, but still appropriate fora federated model where multiple PlanetLabs exist run bydifferent organisations.4.1.1 Boot CDThe boot environment for PlanetLab was developped fromthe University of Cambridge XenoBoot disk 29 and sharesmany features with the BSDbased NetBed CD 27 indeedthe two projects have exchanged several ideas over the design. The CD supplied for PlanetLab machines is a minimal,though complete, Linux system that boots from the CD andruns from the CD and dynamic RAMdisk. The PlanetLabnode always boots from the CD, but is capable of then booting another kernel from a harddisk if required.At boot time, the node brings up the main network interface using DHCP, or using static address information on afloppy disk if present. It then connects to a web server theboot server using SSL, verifying the servers identity witha certificate burned onto the CD. The node posts to theboot server a variety of information about itself, includinghardware specification, current network configuration, andethernet MAC address. It recieves in return a file encryptedand signed with an offline private key held at Intel Research.The node decrypts this file and verifies its authenticity using a public key also burned on the CD, and finally executesthe file as a script. What happens next is entirely up tothe script, and consequently is under the control of the bootserver.The motivation for both authenticating the server and thescript is to prevent replay attacks from a fake web serverand also protect PlanetLab in the event that the real webserver is compromised adversaries still cannot execute arbitrary scripts on booting PlanetLab nodes since they stillcannot sign a new script. During a normal install processdescribed below, all files downloaded from the web serverare signed in this way.4.1.2 Boot Server and Software MaintenanceThe task of the boot server is to respond to this request froma booting machine with an appropriate script. Typically,one of three scripts is sent to the machine one to performa reinstall of the current PlanetLab software distribution onthe node, one to simply boot a second kernel from the nodeshard disk, and a third to bring up a heavily firewalled sshserver to allow remote diagnostics and login by a PlanetLabadministrator. A database on the boot server holds the bootstate for each node, including a state machine that keepstrack of whether the machine needs a reformat and reinstall,or should simply boot from the hard disk as normal.This arrangement confers great flexibility in both bringingup nodes, and installing software. In principle, nearly anyfiling system and kernel can be installed on the bare hardware of the node using this technique, provided that codecan be written for the Linux distribution on the CD to do thejob. Most importantly, this can all be written or modifiedafter the CD has been distributed and the node installed.In combination with the ability the remotely powercycle amachine, the result is a powerful remote management facility.Since the current PlanetLab system software environmentis a derivative of RedHat Linux, we can leverage RedHatspackage management and installation tools to maintain PlanACM SIGCOMM Computer Communications Review Volume 33, Number 3 July 20038etLab nodes. The installation boot script mimics the conventional RedHat network install behavior, and we retainthe use of RootStock to regularly update software packagesinstalled on the nodes while they are running. The onlychanges we have made to this process is to ensure that allfiles downloaded from the boot server are signed by the offline key, for the reasons detailed above.4.2 Dynamic Slice CreationDynamic slice creation is currently implemented as a set ofdaemons and a commandline program that communicatevia secure RPC protocols. Each node runs a node manager daemon that implements the nodes admission controlpolicy and handles lease and virtual machine managementrequests. Each node manager delegates authority to issuetickets for its resources to an agent daemon that runs ona wellknown machine www.planetlab.org. The agentkeeps track of which node managers are available for dynamic slice creation and issues tickets in response to requestsfrom brokers. The service manager is currently a commandline program and uses an integrated broker that speaks toa single agent to obtain tickets to create slices. Both nodemanagers and agents also run a tiny embedded web serverto allow for remote inspection of their state.4.2.1 Resource MonitoringEach node runs a resource monitor that periodically reportsthe status of its resources to an agent. The resource monitorcurrently uses the Ganglia cluster monitoring toolkit 9, andreports to a centralized agent running at www.planetlab.org.The monitoring facility currently reports overall CPU andmemory utilization, as well as perslice network usage. Thisinformation is aggregated at the agent and made availableas an XML document.4.2.2 AgentsWe currently implement a single, centralized agent, runningas a daemon process on www.planetlab.org. It uses usesecure XMLRPC protocols to communicate with brokers,issues tickets for all nodes, and discovers such nodes by periodically polling a Ganglia resource monitor. The agenthandles two main types of XMLRPCs from brokers ticketadvertisement requests getads and new ticket requestsgettickets. Ticket advertisement requests are handledby returning the current set of available nodes as a set ofadvertisements, each of which includes a nodes IP address.New ticket requests are handled by performing a twowayauthentication via SSL, verifying the broker is authorizedto request tickets, creating a set of signed tickets, storingthe tickets in a local DB file for crash recovery, and finallyreturning them to the broker. Requests for tickets are specified using an XML slice description document. In the current implementation, slice description files consist of a slicename e.g., oceanstore, the number of nodes requested,and a desired lease length in seconds.4.2.3 Node ManagersA node manager is that part of each nodes VMM that implements admission control and handles all the mechanics ofcreating and deleting vservers. Node managers are implemented as daemon processes that use secure XMLRPC protocols to communicate with service managers. They makethemselves available for dynamic slice creation through thecentral agent by periodically sending an existence packetincluding a heartbeat for debugging to a local Ganglia resource monitor, and by accepting signed tickets from thetrusted agent.Using XMLRPCs over SSL, node managers handle a number of different types of requests from service managers related to lease management and access control to the virtual machines underlying the leases. The most importantrequests are the following new lease requests newlease,lease cancellations deletelease, lease renewals rewewlease,and adding addkey and removing delkey SSH keys froma leases virtual machine. In addition, node managers alsosupport a number of passive requests to allow probing of agiven virtual machines state e.g., the getsshkeys RPC returns a list of SSH public keys in the virtual machines SSH.authorized keys file.The most important request handled by a node manager isthe lease creation request. Lease creation requests from service managers are handled first through an authenticationphase a twoway authentication via SSL, ascertaining theservice managers identity through the SSL handshake, verifying the ticket presented as part of the request is signed bya trusted agent and has not expired, and finally by verifyingthat the service managers identity matches the identity ofthe principal named in ticket. Assuming service managerauthentication succeeds, the node manager creates a newlease by creating a new virtual machine, creating a signedlease that specifies the term of the lease, storing the lease ina local DB file for crash recovery, and returning the leaseto the service manager. Creating a new virtual machine entails creating a new vserver and creating a pair of accounts,one in the main vserver and one in the vserver just created,to allow for transparent redirection using SSHSCP into thevserver created for the virtual machine.Vserver creation is done by first choosing a unique securitycontext and creating a mirror of a reference root filesystemfor the vserver using hard links and the immutable and immutable invert filesystem bits. Two Linux accounts are thencreated, one in the nodes primary vserver and one in thevserver just created. Both accounts use a login name identical to that of the slice. The account in the main vserveris specified to use a special shell, binvsh. This shell is essentially a modified bash shell which performs the followingfour actions upon login a switch to the slices vserver security context, a chroot to the vservers root filesystem, relinquishing of a subset of the true superusers capabilities, andredirection into an account in the vserver with an identicallogin name. The end result of this two account arrangementis that users accessing their virtual machines remotely viaSSHSCP are transparently redirected into the appropriatevserver and need not modify any of their existing servicemanagement scripts.4.2.4 Service Managers and BrokersService managers are implemented as executions of as acommandline program that implements the secure XMLRPC protocols required to communicate with both agentsand node managers. They use a simple integrated brokerthat is capable of probing and requesting tickets from aACM SIGCOMM Computer Communications Review Volume 33, Number 3 July 20039single agent in the creation of a slice. They authenticatethemselves to agents and node managers using an RSA keypair and an X.509 certificate signed by a trusted certificate authority, both of which are stored locally in the users.planetlab directory. Service managers store informationabout the slices they are currently managing, including theslices XML slice description file, unredeemed tickets, andvalid leases. This information is stored on disk in the users.planetlab directory and used to recover from partial completion of slice management operations. For example, if anode was unreachable when the service manager attemptedto redeem a ticket for a lease, it can retry the request later byreading the unredeemed ticket from disk. Service managersperform all node manager RPCs in parallel to all nodes toreduce execution time and return either 0 or a positive errorcode to allow service managers to be called programmatically from other programs.4.2.5 Trust RelationshipsTrust relationships and delegations of trust are expressedusing a combination of X.509 certificates and signed XMLfiles. Agents accept ticket requests from brokers that presentX.509 certificates signed by a trusted certificate authorityand prove they possess the private keys associated with thepublic keys contained in the X.509 certificates. Node managers accept lease creation requests from service managersthat present tickets signed by a trusted agent and provethey are the principle named in the tickets. The latter isdone by authenticating the service manager as part of theSSL handshake protocol and comparing the SHA1 hash ofservice managers public key to the principle named in theticket. Both tickets and leases are expressed as XML fileswhich include a principle, an IP address, a slice name, andan interval of UTC time where the ticket or lease is valid.Also included is the authorizing principles RSA signatureon the SHA1 hash of either the ticket or leases XML Figure 2.4.3 Administrative SlicesAdministrative slices provide management services with acomplete view of node state along with additional capabilities to perform privileged operations. Unlike conventionalvirtual machines, which are confined to particular vservers,virtual machines in administrative slices run in a special security context context 1 that exposes the entire state of theunderlying physical machine. In addition, they also carry aset of Linux capabilities that specify the set of privileges thatroot should have within a specific administrative slice. Theformer allows management services to perform tasks such asconventional virtual machine management and distributedprocess monitoring, tasks which would be impossible withina conventional slice due to the isolation enforced by vservers.The latter allows management services to perform tasks suchas passive monitoring of all outgoing network packets, a taskthat requires elevated privileges, in this case the CAP NET RAWLinux capability.Administrative slices are created in a similar manner as thatof conventional slices but require additional details to bespecified and require additional user privileges in order fortickets to be obtained from an agent. To request ticketsfor an administrative slice, users augment their XML slicesigrsasha1base64 mP0ttuTL2NadGyWhKZKdSC0ul01R6jhZ4J4C7zmlhscKdBUnxVHNX0ma9RE2xS3LNs4nYatunqXdihkhwYGp4PVKWPsHlxd0y2gugbdC3sTps6v3NDqIaJz1Aoxx6fIKrEMR6SZcY5l3OujnoLqGMTWz6tIc6IXSVRnDpwxml version1.0 leaseprinciplesha1ec6c223a8a2a8be1caf90f8b51e8c121f805c4a4principlesha1ip12.155.161.149ipsliceoceanstoreslicestarttime20021207 015226starttimeendtime20030607 015226endtimeleaseFigure 2 An example lease.description to specify a request for an administrative sliceand include a set of Linux capabilities for root within thatslice. Unlike conventional slices, which may be created byany PlanetLab principle investigator, administrative slicesmay only be created by PlanetLab administrators. Restrictions on which users may create which types of slices areenforced by the agent running on www.planetlab.org bycontrolling the issuing of tickets for specific slice types basedon user privileges as stored in a PostgreSQL database.5. CONCLUDING REMARKSAt the time of this writing, PlanetLab is being used by over140 researchers around the world, with as many as a dozenservices on the verge of running continuously. We are currently testing a beta release of the Boot CD, which we expectto facilitate substantial growth over the next several months.This paper describes the key elements of Version 0.5 of thePlanetLab software, including both support for virtual machines running on each node, and the global managementfacilities needed to keep the software uptodate and allocateresources to services. By no means, however, do we believethat Version 0.5 represents the final system. As outlinedin Section 2, for example, the design space for both virtualmachines and global resource allocation are quite rich, andwe expect both to be a focus of continuing research in thenext few years. In addition, once PlanetLab begins to hostcontinuously running overlay services, we expect the issue ofservice composition to come to the forefront. Recognizingand codifying the common subservices is the key to evolvingthe next generation Internet.6. REFERENCES1 D. Andersen, H. Balakrishnan, F. Kaashoek, andR. Morris. Resilient overlay networks. In ProceedingsACM SIGCOMM Computer Communications Review Volume 33, Number 3 July 200310of the 18th ACM Symposium on Operating SystemsPrinciples, October 2001.2 D. Andersen, H. Balakrishnan, M. F. Kaashoek, andR. Morris. Resilient Overlay Networks. In Proceedingsof the 18th ACM Symposium on Operating SystemsPrinciples SOSP, pages 131145, Chateau LakeLouise, Banff, Alberta, Canada, October 2001.3 M. Balazinska, H. Balakrishnan, and D. Karger.INSTwine A Scalable PeertoPeer Architecture forIntentional Resource Discovery. In Proceedings ofPervasive 2002  International Conference onPervasive Computing, Zurich, Switzerland, August2002.4 W. J. Bolosky, S. Corbin, D. Goebel, and J. R.Douceur. Single instance storage in windows 2000. InProceedings of the 4th USENIX Windows SystemsSymposium, August 2000.5 F. Dabek, M. F. Kaashoek, D. Karger, R. Morris, andI. Stoica. Widearea cooperative storage with CFS. InProceedings of the 18th ACM Symposium on OperatingSystems Principles SOSP, Chateau Lake Louise,Banff, Alberta, Canada, October 2001.6 P. Druschel, M. Castro, A.M. Kermarrec, andA. Rowstron. Scribe A largescale and decentralizedapplicationlevel multicast infrastructure. IEEEJournal on Selected Areas in Communications, 20,2002.7 Ensim Corp. Ensim Virtual Private Server.httpwww.ensim.comproductsmaterialsdatasheet vps 051003.pdf,2000.8 K. Fraser, S. Hand, T. Harris, I. Leslie, and I. Pratt.The Xenoserver Computing Infrastructure, 2002.httpwww.cl.cam.ac.ukResearchSRGnetosxenoxenogeneral.pdf.9 Ganglia. httpganglia.sourceforge.net.10 J. Gelinas. Virtual private servers and securitycontexts.httpwww.solucorp.qc.camiscprjs context.hc.11 P.H. Kamp and R. N. M. Watson. Jails Confiningthe Omnipotent Root. In Proceedings of the 2ndInternational SANE Conference, Maastricht, TheNetherlands, May 2000.12 J. Kubiatowicz, D. Bindel, Y. Chen, S. Czerwinski,P. Eaton, D. Geels, R. Gummadi, S. Rhea,H. Weatherspoon, W. Weimer, C. Wells, and B. Zhao.OceanStore An Architecture for GlobalScalePersistent Storage. In Proceedings of the Ninthinternational Conference on Architectural Support forProgramming Languages and Operating SystemsASPLOS 2000, Nov. 2000.13 Linux Advanced Routing and Traffic Control.httplartc.org.14 Millennium Research Group. Rootstock.httpwww.millennium.berkeley.edu.15 D. Mosberger and L. L. Peterson. Making pathsexplicit in the scout operating system. In USENIX,editor, 2nd Symposium on Operating Systems Designand Implementation OSDI 96, October 2831,1996. Seattle, WA, pages 153167, Berkeley, CA,USA, Oct. 1996. USENIX.16 L. Peterson, T. Anderson, D. Culler, and T. Roscoe. ABlueprint for Introducing Disruptive Technology intothe Internet. In Proceedings of the 1st ACM Workshopon Hot Topics in Networks HotNetsI, October 2002.17 S. Ratnasamy, M. Handley, R. Karp, and S. Shenker.TopologicallyAware Overlay Construction and ServerSelection. In Proceedings of the IEEE INFOCOMConference, New York, NY, June 2002.18 A. Rowstron and P. Druschel. Pastry Scalable,distributed object location and routing for largescalepeertopeer systems. In Proceedings of the 18thIFIPACM International Conference on DistributedSystems Platforms Middleware 2001, Heidelberg,Germany, November 2001.19 A. Rowstron and P. Druschel. Storage Managementand Caching in PAST, A LargeScale PersistentPeertoPeer Storage Utility. In Proceedings of the18th ACM Symposium on Operating SystemsPrinciples SOSP, pages 188201, Chateau LakeLouise, Banff, Alberta, Canada, October 2001.20 S. Savage, A. Collins, E. Hoffman, J. Snell, andT. Anderson. The Endtoend Effects of Internet PathSelection. In Proceedings of the ACM SIGCOMMConference, Cambridge, MA, September 1999.21 N. Spring, R. Mahajan, and D. Wetherall. MeasuringISP Topologies with Rocketfuel. In Proceedings of theACM SIGCOMM Conference, pages 133146,Pittsburgh, PA, August 2002.22 N. Spring, D. Wetherall, and T. Anderson.Scriptroute A facility for distributed internetmeasurement. In Proceedings of the 4th USITSSymposium, Seattle, WA, March 2003.23 I. Stoica, R. Morris, D. Karger, F. Kaashoek, andH. Balakrishnan. Chord A PeertoPeer LookupService for Internet Applications. In Proceedings of theACM SIGCOMM Conference, San Diego, CA,September 2001.24 UserMode Linux. httpwww.usermodelinux.org.25 L. Wang, V. Pai, and L. Peterson. The Effectiveness ofRequest Redirection on CDN Robustness. InProceedings of the 5th Symposium on OperatingSystem Design and Implementatio OSDI, Boston,MA, December 2002.26 A. Whitaker, M. Shaw, and S. Gribble. Scale andPerformance in the Denali Isolation Kernel. InProceedings of the 5th Symposium on OperatingSystem Design and Implementatio OSDI, Boston,MA, December 2002.ACM SIGCOMM Computer Communications Review Volume 33, Number 3 July 20031127 B. White, J. Lepreau, L. Stoller, R. Ricci,S. Guruprasad, M. Newbold, M. Hibler, C. Barb, andA. Joglekar. An Integrated Experimental Environmentfor Distributed Systems and Networks. In Proceedingsof the 5th Symposium on Operating System Design andImplementatio OSDI, Boston, MA, December 2002.28 B. White, J. Lepreau, L. Stoller, R. Ricci,S. Guruprasad, M. Newbold, M. Hibler, C. Barb, andA. Joglekar. An integrated experimental environmentfor distributed systems and networks. In Proceedingsof the 5th USENIX Symposium on Operating SystemsDesign and Implementation, December 2002.29 Xenoboot.httpwww.cl.cam.ac.ukResearchSRGnetosxenoboot.ACM SIGCOMM Computer Communications Review Volume 33, Number 3 July 200312
