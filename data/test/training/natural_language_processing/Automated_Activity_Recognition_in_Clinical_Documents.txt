International Joint Conference on Natural Language Processing, pages
Nagoya, Japan, 14-18 October
Automated Activity Recognition in Clinical Documents
C. Thorne M. Montali D. Calvanese
Free University of BozenBolzano
Bolzano, Italy
surnameinfunibzit
E. Cardillo C. Eccher
Fondazione Bruno Kessler
Trento, Italy
surnamefbkedu
Abstract
We describe a first experiment on the
identification and extraction of computer
interpretable guideline (CIG) components
(activities, actors and consumed artifacts
from clinical documents, based on clinical
entity recognition techniques. We rely on
MetaMap and the UMLS Metathesaurus
to provide lexical information, and study
the impact of clinical document syntax and
semantics on activity recognition
Keywords. Clinical entity recognition
computer interpretable guideline, UMLS
Metathesaurus
Introduction. Clinical practice guidelines are sys
tematically developed documents specifying the
activities, resources and personnel required to cure
or treat an specific illness or medical condition
(Field and Lohr (1990)). The need to instan
tiate them into clinical protocols and workflows
has given rise to computer-interpretable guide
lines (CIGs) (De Clercq et al. (2008)), i.e., formal
representations of the care process or plan, and to
several natural language processing (NLP) tech
niques aimed at automating the costly manual CIG
generation process (Kaiser et al. (2007), Serban et
al. (2007)). All NLP approaches leverage on an
notated biomedical resources (e.g., the CLEF cor
pus from Roberts et al. (2007) and Mykowiecka
and Marciniak (2011)), or on frameworks such
as cTAKES (Savova et al. (2010)). The key
lexical-semantic resource in this domain is the US
National Library of Medicine’s Unified Medical
Language System (UMLS) Metathesaurus Bo
denreider (2004)), complemented by its frontend
MetaMap (Aronson and Lang
In this paper we conduct a first experiment on
how to apply entity recognition techniques in
spired by Abacha and Zweigenbaum (2011), to
recognize CIG components in medical documents
The process dimension of CIGs consists of four
pillars: (1) activities to be executed; (2) the re
sources they use or consume; (3) the actors that
execute them; (4) control flows and gates that tem
porally constrain activities. We focus in this pa
per on activities, the main building block of CIGs
and to a lesser extent on resources and actors. All
these components are denoted by content words
and can be used to build CIG fragments. We rely
on MetaMap annotations and evaluate our tech
niques over an UMLS-annotated clinical corpus
CIGs and Activities. Activities are entities diffi
cult to identify with current resources: within clin
ical documents, in fact, not only verbs (VBs) but
also proper nouns (PNs), common nouns NNs
and, more in general, noun phrases (NPs)1 can
refer to them. Figure 1 shows an example from
the type-2 diabetes guideline of the National In
stitute for Health and Clinical Excellence NICE
(NICE - NHS (2009)) expressing a conditional
CIG/process fragment, annotated automatically
with MetaMap. To correctly extract the deep
intended representations it is necessary to recog
nize that the two entities “blood glucose control
and “oral glucose-lowering medication” are activ
ity tokens. MetaMap annotations provide a clue
but we still need to “filter out” the “clinical at
tribute” UMLS annotation. We want to under
stand how this information can be used for this
task within an entity recognition framework
Clinical Entity Recognition. Let ~c denote a vec
tor of clinical entity type labels, and ~α a vec
tor of input noun phrases (NPs) or entities. The
goal of clinical entity recognition, see Abacha and
Zweigenbaum (2011), can be formulated as the
task of finding the best scoring vector of clinical
1In this paper we refer to the Penn Treebank partof
speech (POS) notation as described by Marcus et al.

clinical attribute

Continue with metformin if blood glucose control remains
⇓ ⇓
reg. activity pharm. substance laboratory procedure
clinical attribute

inadequate and another oral glucose-lowering medication is added
⇓ ⇓
ql. concept therapeutic procedure fc. concept
continue
adequate
deep
continue
blood adequate
and glucose
medication added
shallow
added
control
met
morphin
glu
cose
glu
cose
medi
cation
met
morphin
administer
...
Figure 1: Top: MetaMap UMLS (automated) annotations of the NICE diabetes guideline fragment
boxes surround entities, annotations are MetaMap’s. Bottom: Two candidate CIG fragments repre
sented in Business Process Modeling Notation (BPMN), see Ko et al. (2009)): to the left, the intended
“deep” CIG, to the right a “shallow” CIG. Control flows (diamonds) specify the acceptable orderings of
the activities (rounded rectangles); activities consume resources (folded-corner rectangles
entity type labels: ~c∗ = arg max{~c | c
where µ(·) denotes a recognizer built using a clas
sification model (e.g., a logistic regression algo
rithm), and ρ(·, ·) is a feature extraction func
tion. In the following paragraphs we study this
task w.r.t. the set {activity, resource, actor, other
of entity types
The SemRep corpus. Since no UMLS anno
tated clinical guideline corpora are available for
research purposes, we ran our experiments over
the SemRep corpus by Kilicoglu et al.
a small annotated clinical corpus whose domain
largely overlaps with that of guidelines. It con
sists of 500 clinical excerpts MedLinePubMed
and contains 13, 948 word tokens manually anno
tated by clinicians and domain experts, covering
the whole clinical domain. UMLS concept types
annotate a total of 827 NPs
Features. The focus of our experiments is to un
derstand the predictive power of syntax and se
mantics for CIG entity recognition, and in par
ticular for activity recognition. Intuitively, both
syntax and semantics can contribute to the predic
tion of clinical entity types, but it is not a priori
clear which one contributes more. Similarly to
Zhou and He (2011) we used the Stanford parser
(see Klein and Manning (2003)) to extract syn
tactic features, and MetaMap to extract seman
tic features. We harvested clinical types by map
ping UMLS concept types returned by MetaMap
to their subsuming clinical types. In the top of Ta
ble 1 we show a sample of UMLS concept types
subsumed by “activity”, “resource”, “actor” and
“other”, whereas in its bottom we summarize the
extracted features, described in detail below
By mining the NPs sentence parse trees, we ex
tracted the following syntactic features: depth of
nesting (nest); position in the phrase (pos); occur
rence in a subordinated phrase (sub). The intuition
behind these features is that certain types may cor
relate strongly with syntax (e.g., one would expect
“resource” to annotate an object NP
The semantic features were extracted by com
puting several measures of label overlap and fre
quency. The rationale of these features is that
while MetaMap outputs many possible clinical
meanings of the constituent NNs of an NP entity
giving rise to multiple “activity”, “resource”, ac
tor” and “other” annotations per NN and NP, it
tends to output meanings that are semantically re
lated (within the UMLS Metathesaurus hierarchy
to the NP’s intended type
We measured the raw frequency freq of the NP
entity type c in the SemRep corpus, the degree of
annotation overlap hd between the bag of possi

activity actor resource other
laboratory professional manufactured qualitative
procedure society object concept
feature F description value f
nest nesting level in tree integer ∈ N
pos position w.r.t. verb subject, predicate
sub occurs in clause? yes, no
freq freq. of label in corpus integer ∈ N
lf rel. freq. of label in NP real ∈ [0,
hd head/NP overlap real ∈ [0,
ls label/NP overlap real ∈ [0,
class NP entity type act., actor, res., other
Table 1: Top: CIG entity labels and sample
UMLS concept types they subsume. Bottom: NP
features considered; the class label is the depen
dent feature we want to predict
bly repeated labels labs collected using MetaMap
from all the NNs in an NP, and the bag of possibly
repeated labels of its head noun labsh. In addition
we computed the relative frequency lf of the NP
entity type c w.r.t. labs
hd
||labs e labsh
labslabsh
lf
||labs e c
labs

where || · || and e denote resp. bag cardinality and
intersection. The intuition behind these two fea
tures is that the intended type will tend to prevail
within the annotations of an NP, and in particular
among its head NN and its modifiers. Finally, we
took into account the taxonomical structure of the
UMLS Metathesaurus and defined the following
label/NP overlap ls
ls
||labs e subc
labssubc

where sub(c) is the bag of all the UMLS concept
types that are subsumed by the entity type label
c. The ls feature measures how similar are the
MetaMap NP annotations to the UMLS hierarchy
subsumed by c. In all cases, a simple Laplace
smoothing was applied
Evaluation Framework. In our experiments the
main goal was to evaluate activity recognition fea
tures rather than classifier design and evaluation
We thus relied on standard classification models
from the known Weka2 data mining framework
We trained and evaluated the following classi
fiers: (i) logistic classifier (Logit), (ii) support vec
tor machine (SVM), (iii) naive Bayes classifier
wwwcswaikatoacnzmlweka
(Bayes), (iv) neural network (Neural), and (v) de
cision tree (Tree). To measure the significance of
each single feature, we removed each time a fea
ture Fi from the space {F1, . . . , F7} of syntactic
and semantic independent features from Table
and retrained and reevaluated the classifiers wrt
the feature space {F1, . . . , Fi−1, Fi+1, . . . , F
In parallel to this, we studied the impact of
context over activity recognition, and its interplay
with our features. To this end we considered a
baseline scenario, in which context is restricted to
NPs, and a scenario in which we take into consid
eration all the annotated NPs of a SemRep sen
tence. This distinction is important since Sem
Rep is a small and sparsely annotated corpus, for
which enhanced feature spaces may not prove in
formative. These two scenarios were modeled as
follows. (1) A set of NP observations: for each
NP α in SemRep, we extracted the feature vector
(fα1 , . . . , f

7 , c
α)T . (2) A set of sentence observa
tions: for each vector (α1, . . . , αk)T of annotated
NPs in a SemRep sentence, we extracted feature
vectors (fα11 ,. . f

7 c
α1,. . .,fαk1 ,. . f
k
7 c
αk)T
For each combination of classifier feature and
scenario, we performed a 10-fold crossvalidation
to measure precision (Pr), recall (Re), Fmeasure
and the overall accuracy (Ac) of the classifiers for
the activity recognition task
Results and Discussion. The baseline scenario
(see Figure 2, left) shows a drop in average pre
cision, recall, F-measure and accuracy when hd
and freq are disregarded, and a minor drop when ls
is disregarded. The removal of syntactic features
on the other hand has a smaller effect. Consider
ing sentence context (see Figure 2, center), we can
observe a greater impact for sub, and a minor drop
when ls is disregarded. But sentence context gives
rise also to a clear decrease in average classifier
performance. Thus sub, while significant, is less
useful than the semantic features
This last observation is substantiated by corpus
evidence. One way to see how, is to focus on the
distribution of syntax relatively to corpus domain
Syntactic structures can be approximated by func
tion words4 (e.g., subordinators (INs) such as if
3For reasons of space, we present here a summary of the
results obtained; for a more detailed description, please re
fer to wwwinfunibzitcathornevericlig
ijcnlpexppdf
4For the POS tagging we relied on a Natural Language
Toolkit (NLTK) 3-gram tagger by Bird et al. (2009), trained
over the (POS annotated) Brown corpus

no
ne lf su
b
ne
st po
s ls
fre
q hd






N
o
u
n
P
h
ra
se
s
a
v
g

Pr Re F1 Ac
no
ne lf su
b
ne
st po
s ls
fre
q hd






S
e
n
te
n
ce
s
a
v
g

Pr Re F1 Ac
Pr Re F1 Ac
Logit (NP) 0.66 0.69 0.68
(sen.) 0.66 0.62 0.64
SVM (NP) 0.64 0.73 0.68
(sen.) 0.62 0.71 0.66
Bayes (NP) 0.65 0.66 0.66
(sen.) 0.61 0.59 0.60
Neural (NP) 0.66 0.79 0.72
(sen.) 0.63 0.67 0.65
(NP) 0.74 0.73 0.73
Tree (sen.) 0.66 0.70 0.68
Figure 2: Left, Center: Results of 10-fold cross-validation by scenario. On the y-axis, activity recog
nition precision, recall, F1-measure and classifier accuracy (classifier averages). On the x-axis, the
feature(s) removed. The tag “none” means that no feature was removed. Right: Results for the original
(complete) feature space, by classifier and label context (noun phrase NP or sentence sen
corpus size (words) domain rel. freq
Brown 1,391,708 news
Friederich 3,824 processes
SemRep 13,948 clinical
diabetes2 7,109 clinical
eating dis. 5,078 clinical
schizophrenia 5,367 clinical
χ2 p df. t-score p df
43.13 0.00 2 1.03 0.36
Table 2: Top: Function word relative frequency
across corpora and domains. Bottom: Statistical
tests (χ2-test of independence and ttest
or “then”, coordinators (CCs) such as or
We compared to SemRep: (i) a subset of
the Brown corpus (Francis and Kucera
(ii) a corpus of business process specifications
(Friederich et al. (2011)), (iii) a subset of the
NICE diabetes-2 guideline (NICE - NHS
(iv) a subset of the NICE eating disorders guide
line (NICE - NHS (2004)), and (v) a subset of
the NICE schizophrenia guideline (NICE - NHS
(2010)). We run the following statistical tests
(see Gries (2010)) at p = 0.01 significance
(1) a t-test (null hypothesis: cross-corpora func
tion word mean relative frequency is 0.20); (2) a
χ2-test of independence (null hypothesis: function
word distribution is correlated to corpus domain
The test results (see Table 2) show that syntax is
uniform across domains, and thus has a more lim
ited impact relatively to semantics
Syntax, however, can be leveraged to optimize
prediction results when exploited by classifiers
sensitive to categorical data. The classifier that
performed better overall was the decision tree see
Figure 2, right), which seems to exploit better the
more limited impact of sub, pos, and nest
Conclusions and Further Work. We have con
ducted preliminary experiments on automatic clin
ical activity recognition using MetaMap and en
tity recognition techniques. We experimented our
techniques on the SemRep gold standard UMLS
annotated corpus. Our experiments suggest that
the semantic environment of an entity is more use
ful for this task. Corpus analysis on SemRep and
other corpora seems to confirm this observation
In the future, we plan to consider more powerful
classification models for NLP, such as conditional
random fields (CRFs), able to exploit possible de
pendencies among features. We plan to focus on
document semantics, by considering more com
plex semantic features (based on, e.g., thesaurus
based similarity metrics). Finally, to better cope
with data sparseness we intend to consider a big
ger corpus by integrating SemRep with, e.g., the
i2b2 clinical corpus as suggested by Abacha and
Zweigenbaum
Acknowledgments. The present work has
been done within the context of the VERICLIG
project5, supported by a grant from the Free Uni
versity of Bozen-Bolzano Foundation
wwwunibzitcathornevericlig

References
Asma Ben Abacha and Pierre Zweigenbaum.
Medical entity recognition: A comparison of seman
tic and statistical methods. In Proceedings of the
BioNLP 2011 Workshop
Alan R. Aronson and François-Michel Lang.
And overview of MetaMap: Historical perspective
and recent advances. Journal of the American Med
ical Informatics Association,
Steven Bird, Ewan Klein, and Edward Loper
2009. Natural Language Processing with Python
OReilly
Olivier Bodenreider. 2004. The unified medical lan
guage system (UMLS): Integrating biomedical ter
minology. Nucleic Acids Research, DD
Paul De Clercq, Katharina Kaiser, and Arie Hasman
2008. Computer interpretable medical guidelines
In A. Ten Teije et al., editor, Computer-based Medi
cal Guidelines and Protocols: A Primer and Current
Trends, chapter 2, pages 22–43. IOS Press
Marilyn J. Field and Kathleen N. Lohr, editors.
Clinical Practice Guidelines. Directions for a New
Program. National Academy Press
Nelson Francis and Henry Kucera. 1964. A standard
corpus of present-day edited american english, for
use with digital computers. Technical report, De
partment of Linguistics, Brown University, Provi
dence, Rhode Island, USA
Fabian Friederich, Jan Mendling, and Frank Puhlmann
2011. Process model generation from natural lan
guage text. In Proceedings of the 23rd International
Conference on Advanced Information Systems Engi
neering (CAiSE
Stefan Th. Gries. 2010. Useful statistics for corpus lin
guistics. In Aquilino Sánchez and Moisés Almela
editors, A mosaic of corpus linguistics: selected ap
proaches, pages 269–291. Peter Lang
Katharina Kaiser, Cem Akaya, and Silvia Miksch
2007. How can information extraction ease formal
izing treatment processes in clinical practice guide
lines? A method and its evaluation. Artificial Intel
ligence in Medicine,
Halil Kilicoglu, Graciela Rosenblat, Marcelo Fisz
man, and Thomas C. Rindfleisch. 2011. Con
structing a semantic predication gold standard from
the biomedical literature. BMC Bioinformatics

Dan Klein and Christopher D. Manning. 2003. Ac
curate unlexicalized parsing. In Proceedings of the
41st Meeting of the Association for Computational
Linguistics ACL
Ryan K.L. Ko, Stephen S.G. Lee, and Eng Wah Lee
2009. Business process mangament (BPM) stan
dards: A survey. Business Process Management
Journal,
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn treebank. Computa
tional Linguistics,
Agnieszka Mykowiecka and Malgorzata Marciniak
2011. Some remarks on automatic semantic annota
tion of a medical corpus. In Proceedings of the rd
International Workshop on Health Document Text
Mining and Information Systems
NICE - NHS. 2004. Eating dissorders. Available from
httpwwwniceorguknicemedia
livepdf
NICE - NHS. 2009. Type 2 diabetes. Available from
httpwwwniceorguknicemedia
pdfCGNICEGuidelinepdf
NICE - NHS. 2010. Schizophrenia. Available from
httpwwwniceorguknicemedia
pdfCGNICEGuidelinepdf
Angus Roberts, Robert Gaizaskas, Mark Hepple, Neil
Davis, George Demetriou, Yikun Guo, Jay Kola, Ian
Roberts, Andrea Setzer, Archana Tapuria, and Bill
Wheeldin. 2007. The CLEF corpus: Semantic an
notation of a clinical text. In Proceedings of the
AMIA 2007 Annual Symposium
Guergana K. Savova, James J. Masanz, Philip V
Ogren, Jiaping Zheng, Sunghwan Sohn, Karin C
Kipper-Schuler, and Christopher G. Chute.
Mayo clinical text analysis and knowledge extrac
tion system (cTAKES): Architecture, component
evaluation and applications. Journal of the Amer
ican Medical Informatics Association,

Radu Serban, Anette ten Teije, Frank van Harmelen
Mar Marcos, and Cristina Polo-Conde. 2007. Ex
traction and use of linguistics patterns for mod
elling medical guidelines. Artificial Intelligence in
Medicine,
Deyu Zhou and Yulan He. 2011. Semantic parsing
for biomedical event extraction. In Proceedings of
the 9th International Conference on Computational
Semantics IWCS


