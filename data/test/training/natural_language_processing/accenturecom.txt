Portfolio Finding Relevant Functions And Their UsagesCollin McMillanCollege of William MaryWilliamsburg, VA 23185cmccs.wm.eduMark GrechanikAccenture Technology LabChicago, IL 60601mark.grechanikaccenture.comDenys PoshyvanykCollege of William MaryWilliamsburg, VA 23185denyscs.wm.eduQing Xie, Chen FuAccenture Technology LabChicago, IL 60601qing.xie,chen.fuaccenture.comABSTRACTDifferent studies show that programmers are more interested infinding definitions of functions and their uses than variables, statements, or arbitrary code fragments 30, 29, 31. Therefore, programmers require support in finding relevant functions and determining how those functions are used. Unfortunately, existing codesearch engines do not provide enough of this support to developers,thus reducing the effectiveness of code reuse.We provide this support to programmers in a code search system called Portfolio that retrieves and visualizes relevant functionsand their usages. We have built Portfolio using a combination ofmodels that address surfing behavior of programmer and sharingrelated concepts among functions. We conducted an experimentwith 49 professional programmers to compare Portfolio to GoogleCode Search and Koders using a standard methodology. The results show with strong statistical significance that users find morerelevant functions with higher precision with Portfolio than withGoogle Code Search and Koders.Categories and Subject DescriptorsD.2.9 Software Engineering,Management Productivity D.2.mSoftware Engineering, Miscellaneous Reusable softwareGeneral TermsAlgorithms, ExperimentationKeywordsCode search, portfolio, pagerank, function call graph, ranking.1. INTRODUCTIONDifferent studies show that programmers are more interested infinding definitions of functions and their uses than variables, statements, or arbitrary fragments of source code 31. More specifically, programmers use different tools including code search engines to answer three types of questions 30, 29. First, programmers want to find initial focus points such as relevant functions thatPermission to make digital or hard copies of all or part of this work forpersonal or classroom use is granted without fee provided that copies arenot made or distributed for profit or commercial advantage and that copiesbear this notice and the full citation on the first page. To copy otherwise, torepublish, to post on servers or to redistribute to lists, requires prior specificpermission andor a fee.ICSE 11, May 2128, 2011, Honolulu, Hawaii, USACopyright 2011 ACM 97814503044501105 ...10.00.implement highlevel requirements. Second, programmers mustunderstand how a function is used in order to use it themselves.Third, programmers must see the chain of function invocations inorder to understand how concepts are implemented in these functions. It is important that source code search engines support programmers in finding answers to these questions.In general, understanding code and determining how to use it, isa manual and laborious process that takes anywhere from 50 to80 of programmers time 5, 8. Short code fragments that arereturned as results to user queries do not give enough backgroundor context to help programmers determine how to reuse these codefragments, and programmers typically invest a significant intellectual effort i.e., they need to overcome a high cognitive distance17 to understand how to reuse these code fragments. On theother hand, if code fragments are retrieved as functions, it makes iteasier for developers to understand how to reuse these functions.A majority of code search engines treat code as plain text whereall words have unknown semantics. However, applications contain functional abstractions that provide a basic level of code reuse,since programmer define functions once and call them from different places in source code. The idea of using functional abstractionsto improve code search was proposed and implemented elsewhere3, 10, 23, 32 however, these code search engines do not automatically analyze how functions are used in the context of otherfunctions, despite the fact that understanding the chains of functioninvocations is a key question that programmers ask. Unfortunately,existing code search engines do little to ensure that they retrievecode fragments in a broader context of relevant functions that invoke one another to accomplish certain tasks.Our idea is that since programmers frequently ask various questions about functions, a code search engine should incorporate information about these functions that is used to answer the programmers questions. Browsing retrieved functions that are relevant toqueries means that programmers follow function calls and reviewdeclarations, definitions, and uses of these functions to combinethem in a solution to a given task. That is, programmers want toaccomplish the whole task quickly, rather than obtain multiple examples for different components of the task.For example, consider the query mip map ditheringtexture image graphics, which we use as an examplequery throughout this paper. Programmers dont want to just seeexamples that implement mip map techniques, and others that render texture, and others that manipulate graphic images. A programmer wants to accomplish the complete task of dithering mip mapimages that accompany a texture. However, among relevant results there are functions that implement mipmapping, functions thatmanipulate texture, and there are multiple functions that deal withgraphic images. Typically, programmers investigate these functionsto determine which of them are relevant and determine how to compose these functions to achieve the goal that is expressed with thequery. That is, a programmer wants to see code for the whole taskof how to mip map images that accompany a texture in computergraphics. A search engine can support programmers efficiently ifit incorporates in its ranking how these functions call one another,and displays that information to the user.We created a code search system called Portfolio that supportsprogrammers in finding relevant functions that implement highlevel requirements reflected in query terms i.e., finding initial focus points, determining how these functions are used in a waythat is highly relevant to the query i.e., building on found focuspoints, and visualizing dependencies of the retrieved functions toshow their usages. Portfolio finds highly relevant functions in closeto 270 Millions LOC in projects from FreeBSD Ports1 by combining various natural language processing NLP and indexing techniques with PageRank and spreading activation network SAN algorithms. With NLP and indexing techniques, initial focus pointsare found that match key words from queries with PageRank, wemodel the surfing behavior of programmers, and with SAN we elevate highly relevant chains of function calls to the top of searchresults. We have built Portfolio and conducted an experiment with49 professional C programmers to evaluate Portfolio and compare it with the wellknown and successful engines Google CodeSearch and Koders. The results show with strong statistical significance that users find more relevant code with higher precisionwith Portfolio than those with Google Code Search and Koders.To the best of our knowledge, we are not aware of any existingcode search engines that have been evaluated against and shownto be more accurate than widely used commercial code search engines, with strong statistical significance and over a large codebaseand using a standard information retrieval methodology 22, pages151153. Portfolio is free and available for public use2.2. THE MODELThe search model of Portfolio uses a key abstraction in whichthe search space is represented as a directed graph with nodes asfunctions and directed edges between nodes that specify usages ofthese functions i.e., a call graph. For example, if the function g isinvoked in the function f, then a directed edge exists from the nodethat represents the function f to the node that represents the function g. Since the main goal of Portfolio is to enable programmersto find relevant functions and their usages, we need models that effectively represent the behavior of programmers when navigating alarge graph of functional dependencies. These are navigation andassociation models that address surfing behavior of programmersand associations of terms in functions in the search graph.2.1 Navigation ModelWhen using text search engines, users navigate among pages byfollowing links contained in those pages. Similarly, in Portfolio,programmers can navigate between functions by following edgesin the directed graph of functional dependencies using Portfoliosvisual interface. To model the navigation behavior of programmers,we adopt the model of the random surfer that is used in popularsearch engines such as Google. Following functional dependencieshelps programmers to understand how to use found functions. Thesurfer model is called random because the surfer can jump to anew URL, or in case of source code, to a new function. Theserandom jumps are called teleportations, and this navigation model1httpwww.freebsd.orgports2httpwww.searchportfolio.netFigure 1 Example of associations between different functions.is the basis for the popular ranking algorithm PageRank 2, 19.In the random surfer model, the content of functions and queriesdoes not matter, navigations are guided only by edges in the graphthat specifies functional dependencies. Accordingly, PageRank reflects only the surfing behavior of users, and this rank is based onthe popularity of a function that is determined by how many functions call it. However, the surfing model is query independent sinceit ignores terms that are used in search queries. Taking into consideration query terms may improve the precision of code searching. That is, if different functions share concepts that are related toquery terms and these functions are connected using functional dependencies, then these functions should be ranked higher. We needa search model that should automatically make embedded conceptsexplicit by using associations between functions that share relatedconcepts, and then we combine this model with the surfing modelin Portfolio.2.2 Association ModelThe main idea of an association model is to establish relevanceamong facts whose content does not contain terms that match userqueries directly. Consider the query mipmap ditheringtexture image graphics. Among relevant results thereare functions that implement mip map techniques, and others thatrender texture, and there are multiple functions that manipulategraphic images. This situation is schematically shown in Figure 1,where the function F contains the term mip map, the function Gcontains the term dithering, the function P contains the termsgraphics and image, and the term texture is contained inthe function Q. Function F calls the function G, which in turn callsthe function H, which is also called from the function Q, which is inturn called from the function P. The functions F, P, and Q will bereturned by a search engine that is based on matching query termsto those that are contained in documents. Meanwhile, the function H may be highly relevant to the query but it is not retrievedsince it has no words that match the search terms. In addition, thefunction G can be called from many other functions since its dithering functionality is generic however, its usage is most valuable forprogrammers in the context of the function that is related to queryterms. A problem is how to ensure that the functions H and G endup on the list of highly relevant functions.To remedy this situation we use an association model that isbased on a Spreading Activation Network SAN 4, 6. In SANs,nodes represent documents, while edges specify properties thatconnect these documents. The edges direction and weight reflectthe meaning and strength of associations among documents. Forexample, an article about clean energy and a different article aboutthe melting polar ice cap are connected with an edge that is labeledwith the common property climate change. Once applied to SAN,spreading activation computes new weights for nodes i.e., ranksthat reflect implicit associations in the networks of these nodes.In Portfolio, we view function call graphs as SANs where nodesrepresent functions, edges represent functional dependencies, andweights represent a strength of associations, which includes thenumber of shared terms. After the user enters a query, a list ofFigure 2 Portfolio architecture.functions is retrieved and sorted based on the score that reflects thematch between query terms and terms in functions. Once Portfolioidentifies top matching functions, it computes SAN to propagateconcepts from these functions to others. The result is that everyfunction will have a new score that reflects the associations betweenconcepts in these functions and user queries.2.3 The Combined ModelThe ranking vectors for PageRank PR and spreading activation SAN are computed separately and later are linearly combined in a single ranking vector C  f PR,SAN. PageRank is query independent and is precomputed automatically for afunction call graph, while SAN is computed automatically in response to user queries. Assigning different weights in the linearcombination of these rankings enables finetuning of Portfolio byspecifying how each model contributes to the resulting score.3. OUR APPROACHIn this section we describe the architecture of Portfolio and showhow to use Portfolio.3.1 Portfolio ArchitectureThe architecture for Portfolio is shown in Figure 2. The mainelements of the Portfolio architecture are the database holding software applications i.e., the Projects Archive, the Metadata Builder,the Function Graph Builder, the SAN and PageRank algorithms,the Visualizer and the key word search engine. Applications metadata describes functions that are declared, defined and invoked inthe applications and words that are contained in the source codeof these functions and comments. Portfolio is built on an internal,extensible database of 18,203 CC projects that contain close to2.3Mil files with close to 8.6Mil functions that contain 2,496,172indexed words. Portfolio indexes and searches close to 270MilLOC in these CC projects that are extracted from FreeBSDssource code repository called ports3. It is easy to extend Portfolioby adding new projects to the Projects Archive. The user input toPortfolio is shown in Figure 2 with the arrow labeled 7. Theoutput is shown with the arrow labeled 18.Portfolio works as follows. The input to the system is the set ofapplications from the Projects Archive that contain various functions 1. The Function Graph Builder analyzes the source codeof these applications statically and it outputs 2 the function callgraph FCG that contains functional dependencies. This opera3httpwww.freebsd.orgports  last checked August 17,2010.tion is imprecise since resolving dynamic dispatch calls and function pointers statically is an undecidable problem 18. Since thisis done offline, precise program analysis can be accommodated inthis framework to achieve better results in obtaining correct functional dependencies. We conduct the sensitivity analysis of Portfolio and its constituent algorithms in Section 5.7.1. Next, the algorithm PageRank is run 3 on the FCG, and it computes 4 therank vector, PR, in which every element is a ranking score foreach function in the FCG.The Metadata Builder reads in 5 the source code of applications, applies NLP techniques such as stemming and identifier splitting, and indexes the source code as text resulting 6 in ProjectsMetadata. When the user enters a query 7, it is passed to thekey word search component along with the Projects Metadata 8.The key word search engine searches the metadata using the wordsin the query as keys and outputs 9 the set of Relevant Functionswhose source code and comments contain words that match thewords from the query. These relevant functions 10 along withthe FCG 11 serve as an input to the algorithm SAN. The algorithm SAN computes 12 spreading activation vector of scoresSAN for functions that are associated with the relevant functions 10. Ranking vectors PR 14 and SAN 13 arecombined into the resulting vector  15 that contains rankingscores for all relevant functions. The Visualizer takes 16 the listof relevant functions that are sorted in descending order using theirranking scores and 17 the metadata, in order to present 18the resulting visual map to the user as it is shown in Figure 3.3.2 Portfolio Visual InterfaceAfter the user submits a search query, the Portfolio search engine presents functions relevant to the query in a browser windowas it is shown in Figure 3. The left side contains the ranked list ofretrieved functions and project names, while the right side containsa static call graph that contains these and other functions. Edgesof this graph indicate the directions of function invocations. Hovering a cursor over a function on the list shows a label over thecorresponding function on the call graph. Font sizes reflect thecombined ranking the higher the ranking of the function, the bigger the font size used to show it on the graph. Clicking on the labelof a function loads its source code in a separate browser window.4. RANKINGIn this section we discuss our ranking algorithm.4.1 Components of RankingThere are three components that compute different scores in thePortfolio ranking mechanism a component that computes a scorebased on word occurrences WOS, a component that computes ascore based on the random surfer navigation model PageRankdescribed in Section 2.1, and a component that computes a scorebased on SAN connections between these calls based on the association model described in Section 2.2. WOS ranking is used tobootstrap SAN by providing rankings to functions based on queryterms. The total ranking score is the weighted sum of the PageRank and SAN ranking scores. Each component produces resultsfrom different perspectives i.e., word matches, navigation, associations. Our goal is to produce a unified ranking by putting theseorthogonal, yet complementary rankings together in a single score.4.2 WOS RankingThe purpose of WOS is to enable Portfolio to retrieve functionsbased on matches between words in queries and words in the sourceFigure 3 A visual interface of Portfolio. The left side contains a list of ranked retrieved functions for the motivating example query and theright side contains a call graph that contains these functions edges of this graph indicate the directions of function invocations. Hovering a cursorover a function on the list shows a label over the corresponding function on the call graph. Font sizes reflect the score the higher the score of thefunction, the bigger the font size used to show it on the graph. Clicking on the label of a function loads its source code in a separate browser window.code of applications. This is a bootstraping ranking procedure thatserves as the input to the SAN algorithm.TheWOS component uses the Vector Space Model VSM, whichis a ranking function typically used by search engines to rank matching documents according to their relevance to a given search query.This function is implemented in the Lucene Java Framework whichis used in Portfolio. VSM is a standard bagofwords retrieval function that ranks a set of documents based on the relative proximityof query terms e.g., without dependencies appearing in each document. Each document is modeled as a vector of terms containedin that document. The weights of those terms in each document arecalculated using the Term FrequencyInverse Document FrequencyTFIDF formula. Using TFIDF, the weight for a term is calculated as t f  nk nkwhere n is the number of occurrences of the termin the document, and k nk is the sum of the number of occurencesof the term in all documents. Then the similarities among the documents are calculated using the cosine distance between each pair ofdocuments cos  d1d2d1d2where d1 and d2 are document vectors.4.3 PageRankPageRank is widely described in literature, so here we give itsconcise mathematical explanation as it is related to Portfolio 2,19. The original formula for PageRank of a function Fi, denotedrFi, is the sum of the PageRanks of all functions that invoke FirFi  FjBFirFjFj , where BFi is the set of functions that invokeFi and Fj is the number of functions that the function Fj invokes.This formula is applied iteratively starting with r0Fi  1n, wheren is the number of functions. The process is repeated until PageRank converges to some stable values or it is terminated after somenumber of steps. Functions that are called from many other functions have a significantly higher score than those that are used infrequently or not at all.4.4 Spreading ActivationSpreading activation computes weights for nodes in two stepspulses and termination checks. Initially, a set of starting nodes isselected using a number of top ranked functions using the WOSranking. During pulses, new weights for different nodes are transitively computed from the starting nodes using the formula N j i f Niwi j, where the weight of the node N j is equal to the sumof all nodes Ni that are incident to the node N j with edges whoseweights are wi j. This edge weight serves to give a reduced valueto nodes further away from the initial nodes. Therefore, the weightis a value between 0 and 1. The function f is typically called thethreshold function that returns nonzero value only if the value ofthe argument is greater than some chosen threshold, which acts asa termination check preventing flooding of the SAN.4.5 Example of SAN ComputationConsider an example of SAN computation that is shown in Figure 4. This example is closely related to the motivating examplequery mip map dithering texture image graphics.The first ranking component, WOS, assigned the weights 0.65 and0.52 to the two functions TiledTexture and ImageTexturecorrespondingly. We label these functions with 1. All weightsare to the right rounded off to the second digit. Their subscriptsindicate the order in which weights are computed from the firstfunction weights. For example, the weight is computed for thefunction CreateTextureFromImageby multiplying theWOSweight for the function TiledTexture by the SAN edge weight0.8. Several functions e.g., load, initRendered get differentweights by following different propagation paths from the initialfunction nodes. In these cases, we use the highest value for eachnode the final value assigned to initRenderer is 0.27.4.6 Combined RankingThe combined rank is S  PRPR SANSAN , where  isthe interpolation weight for each type of the score. These weightsFigure 4 Example of SAN weight computation, wi j  0.8.are determined independently of queries unlike the scores WOSand SAN, which are querydependent. Adjusting these weights enables experimentation with how underlying structural and textualinformation in application affects resulting ranking scores. Experimentation with PageRank involves changing the teleportation parameter that we briefly discussed in Section 2.1.5. EXPERIMENTAL DESIGNTypically, search engines are evaluated using manual relevancejudgments by experts 22, pages 151153. To determine how effective Portfolio is, we conducted an experiment with 49 participants who are CC programmers. Our goal was to evaluate howwell these participants could find code fragments or functions thatmatched given tasks using three different search engines GoogleCode Search or simply, Google4, Koders5 and Portfolio6. Wechose to compare Portfolio with Google and Koders because theyare popular search engines with the large open source code repositories, and these engines are used by tens of thousands of programmers every day.5.1 MethodologyWe used a cross validation experimental design in a cohort of 49participants who were randomly divided into three groups. The experiment was sectioned in three experiments in which each groupwas given a different search engine i.e., Google, Koders, or Portfolio to find code fragments or functions for given tasks. Eachgroup used a different task in each experiment. The same task wasperformed by different participants on different engines in each experiment. Before the experiment we gave a onehour tutorial onusing these search engines.In the course of each experiment, participants translated tasksinto a sequence of keywords that described key concepts they neededto find. Once participants obtained lists of code fragments or functions that were ranked in descending order, they examined thesefunctions to determine if they matched the tasks. Each participantaccomplished this step individually, assigning a confidence level,4httpwww.google.comcodesearch5httpwww.koders.com6httpwww.searchportfolio.netC, to the examined code fragments or functions using a fourlevelLikert scale. We asked participants to examine only the top tencode fragments that resulted from their searches since the time foreach experiment was limited to two hours.The guidelines for assigning confidence levels are the following.1. Completely irrelevant  there is absolutely nothing that theparticipant can use from this retrieved code fragments, nothing in it is related to keywords that the participant chosebased on the descriptions of the tasks.2. Mostly irrelevant  a retrieved code fragment is only remotelyrelevant to a given task it is unclear how to reuse it.3. Mostly relevant  a retrieved code fragment is relevant to agiven task and participant can understand with some modesteffort how to reuse it to solve a given task.4. Highly relevant  the participant is highly confident that codefragment can be reused and she clearly see how to use it.Forty four participants are Accenture employees who work onconsulting engagements as professional programmers for different client companies. Five participants are graduate students fromthe University of Illinois at Chicago who have at least six monthsof CC experience. Accenture participants have different backgrounds, experience, and belong to different groups of the total Accenture workforce of approximately 211,000 employees. Out of 49participants, 16 had programming experience with CC rangingfrom six months to two years, and 18 participants reported morethan three years of experience writing programs in C. Ten participants reported prior experience with Google Code Search andthree participants with Koders which are used in this experimentthus introducing a bias toward these code search engines, nine participants reported frequent use of code search engines, and 16 saidthat they never used code search engines. All participants havebachelor degrees and 28 have master degrees in different technicaldisciplines.5.2 PrecisionTwo main measures for evaluating the effectiveness of retrievalare precision and recall 36, page 188191. The precision is calculated as Pr  of retrieved functions that are relevanttotal  of retrieved functions,i.e., the precision of a ranking method is the fraction of the top rranked documents that are relevant to the query, where r  10 inthis experiment. Relevant code fragments or functions are countedonly if they are ranked with the confidence levels 4 or 3. The precision metrics reflects the accuracy of the search. Since we limitthe investigation of the retrieved code fragments or functions to topten, the recall is not measured in this experiment.We created the variable precision, P as a categorization of the response variable confidence, C. We did it for two reasons improvediscrimination of subjects in the resulting data and additionally validate statistical evaluation of results. Precision, P imposes a stricterboundary on what is considered reusable code. For example, consider a situation where one participant assigns the level two to allreturned functions, and another participant assigns level three tohalf of these functions and level one to the other half. Even thoughthe average of C  2 in both cases, the second participant reportsmuch higher precision, P  0.5 while the precision that is reportedby the first participant is zero. Achieving statistical significancewith a stricter discriminative response variable will give assurancethat the result is not accidental.a Confidence level,C. b Precision, P.Figure 5 Statistical summary of the results of the experiment for C and P.The central box represents the values from the lower to upperquartile 25 to 75 percentile. The middle line represents the median. The thicker vertical line extends from the minimum to the maximum value.The filledout box represents the values from the minimum to the mean, and the thinner vertical line extends from the quarter below the mean tothe quarter above the mean. An outside value is defined as a value that is smaller than the lower quartile minus 1.5 times the interquartile range, orlarger than the upper quartile plus 1.5 times the interquartile range inner fences. A far out value is defined as a value that is smaller than the lowerquartile minus three times the interquartile range, or larger than the upper quartile plus three times the interquartile range outer fences.5.3 VariablesThe main independent variable is the search engine Portfolio,Google Code Search, and Koders that participants use to find relevant CC code fragments and functions. The other independentvariable is participants C experience. Dependent variables arethe values of confidence level,C, and precision, P. We report thesevariables in this section. The effects of other variables task description length, prior knowledge are minimized by the design ofthis experiment.5.4 HypothesesWe introduce the following null and alternative hypotheses toevaluate how close the means are for theCs and Ps for control andtreatment groups. Unless we specify otherwise, participants of thetreatment group use Portfolio, and participants of the control groupuse either Google or Koders. We seek to evaluate the followinghypotheses at a 0.05 level of significance.H0 The primary null hypothesis is that there is no difference inthe values of confidence level and precision per task betweenparticipants who use Portfolio, Google, and Koders.H1 An alternative hypothesis to H0 is that there is statistically significant difference in the values of confidence level and precision between participants who use Portfolio, Google, andKoders.Once we test the null hypothesis H0, we are interested in thedirectionality of means, , of the results of control and treatmentgroups. We are interested to compare the effectiveness of Portfolioversus Google Code Search and Koders with respect to the valuesof confidence level,C, and precision, P.H1 C of Portfolio versus Google The effective null hypothesisis that PortC  GC , while the true null hypothesis is that PortC PC . Conversely, the alternative hypothesis is PortC  GC .H2P of Portfolio versus Google The effective null hypothesis isthat PortP  GP , while the true null hypothesis is that PortP GP . Conversely, the alternative hypothesis is PortP  GP .H3 C of Portfolio versus Koders The effective null hypothesisis that PortC  KC , while the true null hypothesis is that PortC KC . Conversely, the alternative is PortC  KC .H4P of Portfolio versus Koders The effective null hypothesis isthat PortP  KP , while the true null hypothesis is that PortP KP . Conversely, the alternative is PortP  KP .The rationale behind the alternative hypotheses to H1H4 is thatPortfolio allows users to quickly understand how queries are relatedto retrieved functions. These alternative hypotheses are motivatedby our belief that if users see visualization of functional dependencies in addition to functions whose ranks are computed higherusing our ranking algorithm, they can make better decisions abouthow closely retrieved functions match given tasks.5.5 Task DesignWe designed 15 tasks for participants to work on during experiments in a way that these tasks belong to domains that are easy tounderstand, and they have similar complexity. The authors of thispaper visited various programming forums and internet groups toextract descriptions of tasks from the questions that programmersasked. In addition, we interviewed several programmers at Accenture who explained what tasks they worked on in the past year.Additional criteria for these tasks is that they should represent realworld programming tasks and should not be biased towards any ofthe search engines that are used in this experiment. These tasks andH Var Approach Samples Min Max Median  StdDev 2 DF PCC p T TcritH1 CPortfolio 1276 1 4 3 2.86 1.07 1.151372 0.04 4.2 10108 24 1.96Google 1373 1 4 2 1.97 1.11 1.23H2 PPortfolio 184 0 1 0.7 0.65 0.28 0.08197 0.12 3 1022 10.9 1.97Google 198 0 1 0.25 0.35 0.33 0.11H3 CPortfolio 1276 1 4 3 2.86 1.07 1.151485 0.06 1.1 1026 10.9 1.96Koders 1486 1 4 2 2.45 1.12 1.25H4 PPortfolio 184 0 1 0.7 0.65 0.28 0.8207 0.041 3 108 5.76 1.97Koders 208 0 1 0.5 0.49 0.3 0.09Table 1 Results of ttests of hypotheses, H, for paired two sample for means for twotail distribution, for dependent variable specified in thecolumn Var either C or P whose measurements are reported in the following columns. Extremal values, Median, Means, , standard deviation,StdDev, variance, 2, degrees of freedom, DF, and the pearson correlation coefficient, PCC, are reported along with the results of the evaluation ofthe hypotheses, i.e., statistical significance, p, and the T statistics.the results of the experiment are available for download7 .5.6 TasksThe following three tasks are examples from the set of 15 taskswe used in our experiment. Implement a module for reading and playing midi files8. Implement a module that adjusts different parameters of apicture, including brightness, contrast and white balance9. Build a program for managing USB devices. The programshould implement routines such as opening, closing, writingand reading from an USB device10.5.7 Threats to ValidityIn this section, we discuss threats to the validity of this experiment and how we address these threats.5.7.1 Internal ValidityInternal validity refers to the degree of validity of statementsabout causeeffect inferences. In the context of our experiment,threats to internal validity come from confounding the effects ofdifferences among participants, tasks, and time pressure.Participants. Since evaluating hypotheses is based on the datacollected from participants, we identify two threats to internal validity C proficiency and motivation of participants.Even though we selected participants who have working knowledge of C as it was documented by human resources, we did notconduct an independent assessment of how proficient these participants are in C. This threat is mitigated by the fact that out of 44participants from Accenture, 31 have worked on successful commercial projects as C programmers for more than two years.The other threat to validity is that not all participants could bemotivated sufficiently to evaluate retrieved code fragments or functions. We addressed this threat by asking participants to explain ina couple of sentences why they chose to assign certain confidencelevel to retrieved, and we discarded 27 results for all search enginesthat were not properly explained.Time pressure. Each experiment lasted for two hours. For someparticipants, this was not enough time to explore all 50 retrievedcode fragments for five tasks ten results for each of five tasks.7httpwww.searchportfolio.net, follow the Experiment link.8httpwww.codeproject.comMessages1427393HowCanIReadMidiFile.aspx9httpwww.codeguru.comforumshowthread.phpt43233910httpwww.cplusplus.comforumgeneral25172Therefore, one threat to validity is that some participants could tryto accomplish more tasks by shallowly evaluating retrieved codefragments and functions. To counter this threat we notified participants that their results would be discarded if we did not seesufficient reported evidence of why they evaluated retrieved codefragments and functions with certain confidence levels.Sensitivity of Portfolio. Recovering functional dependenciesautomatically introduces imprecision, since it is an undecidableproblem to recover precise functional dependencies in the presenceof dynamic dispatch and functional pointers 18. Since the precision of Portfolio depends on the quality of recovered functional dependencies, we conducted an evaluation of these recovered dependencies with twelve graduate computer science students at DePauluniversity. We randomly selected a representative sample of 25 different projects in Portfolio and we asked these students to manuallyinspect source code of these projects to determine the precision ofFCG computed in Portfolio.The results of this evaluation show that the precision of recovered functional dependencies is approximately 76. While the precision appears to be somewhat lower than desired, it is known thatPagerank is resilient to incorrect links. Link farms, for example,are web spam where people create fake web sites that link to oneanother in an attempt to skew the PageRank vector. It is estimatedthat close to 20 of all links on the Internet are spam 11, 28, 1.However, it is shown that the PageRank vector is not affected significantly by these spam links since its sensitivity is controlled bydifferent factors, one of which is teleportation parameter 9. Toevaluate the effect of incorrect links on Pagerank vector we conducted experiments where we randomly modified 25 and 50 oflinks between functions. Our results show that the metric lengthof the Pagerank vector computed as the square root of the sum ofsquares of its components changes only by approximately 7 for50 of perturbed functional dependencies. A brief explanation isthat by adding or removing a couple of links to functions that areeither wellconnected or not connected at all, their Pagerank scoreis not strongly affected. Investigating the sensitivity of Portfolio aswell as improving recovery of functional dependencies is the subject of future work.5.7.2 External ValidityTo make the results of this experiment generalizable, we mustaddress threats to external validity, which refer to the generalizability of a casual relationship beyond the circumstances of our experiment. The fact that supports the validity of this experimentaldesign is that the participants are highly representative of professional CC programmers. However, a threat to external validityCC Cs  Level 1 Cs  Level 2 Cs  Level 3 Cs  Level 4TotalExperts Google Koders Portf Google Koders Portf Google Koders Portf Google Koders PortfYes 450 269 130 178 252 185 189 272 229 139 247 339 2,879No 222 131 56 79 101 92 65 108 106 49 98 135 1,242Total 672 400 186 257 353 277 254 380 335 188 345 474 4,121Table 2 The numbers of the different levels of confidence,C for participants with and without expert CC experience.concerns the usage of search tools in the industrial settings, whererequirements are updated on a regular basis. Programmers usethese updated requirements to refine their queries and locate relevant code fragments or functions using multiple iterations of working with search engines. We addressed this threat only partially, byallowing programmers to refine their queries multiple times.In addition, participants performed multiple searches using different combinations of keywords, and they select certain retrievedcode fragments or functions from each of the search results. Webelieve that the results produced by asking participants to decideon keywords and then perform a single search and rank code fragments and functions do not deviate significantly from the situationwhere searches using multiple refined queries are performed.Another threat to external validity comes from different sizes ofsoftware repositories. Koders.com claims to search more than 3Billion LOC, which is also close to the number of LOC reportedby Google Code Search. Even though we populated Portfoliosrepository with close to 270 Mil LOC, it still remains a threat toexternal validity.6. RESULTSIn this section, we report the results of the experiment and evaluate the hypotheses. We use oneway ANOVA, ttests for paired twosample for means, and 2 to evaluate the hypotheses that we statedin Section 5.4.6.1 Testing the Null HypothesisWe used ANOVA to evaluate the null hypothesis H0 that thevariation in an experiment is no greater than that due to normalvariation of individuals characteristics and error in their measurement. The results of ANOVA confirm that there are large differences between the groups for C with F  261.3  Fcrit  3 withp 5  10108 which is strongly statistically significant. The meanC for the Google Code Search is 1.97 with the variance 1.14, whichis smaller than the meanC for Koders, 2.45 with the variance 1.26,and it is smaller than the mean C for Portfolio, 2.86 with the variance 0.99. Also, the results of ANOVA confirm that there are largedifferences between the groups for P with F  52.5  Fcrit  3.01with p 8.6  1022 which is strongly statistically significant. Themean P for the Google Code Search is 0.35 with the variance 0.1,which is smaller than the mean P for Koders, 0.49 with the variance0.09, and it is smaller than the mean P for Portfolio, 0.65 with thevariance 0.07. Based on these results we reject the null hypothesisand we accept the alternative hypothesis H1.A statistical summary of the results of the experiment forC and Tmedian, quartiles, range and extreme values is shown as boxandwhisker plots in Figure 5a and Figure 5b correspondingly with95 confidence interval for the mean. Even though the numbersof sample sizes are slightly different since some users missed oneexperiment, we replaced missing values with their averages. Eventhough replacing missing data introduces an error, given extremelylow values of p, this error is highly unlikely to affect our results.6.2 Comparing Portfolio with GoogleTo test the null hypothesis H1 and H2 we applied two ttests fortwo paired sample means, in this caseC and P for participants whoused Google Code Search and Portfolio. The results of this test forC and for P are shown in Table 1. The column Samples showsdifferent values that indicate that not all 49 participants participatedin all experiments three different participants missed two differentexperiments. Based on these results we reject the null hypothesesH1 and H2 and we accept the alternative hypotheses that states thatparticipants who use Portfolio report higher relevance and precision on finding relevant functions than those who use GoogleCode Search.6.3 Comparing Portfolio with KodersTo test the null hypotheses H3 and H4, we applied two ttests fortwo paired sample means, in this caseC and P for participants whoused Portfolio and Koders. The results of this test for C and forP are shown in Table 1. Based on these results we reject the nullhypotheses H3 and H4 that say that participants who use Portfolio report higher relevance and precision on finding relevantfunctions than those who use Koders.6.4 Experience RelationshipsWe construct contingency tables to establish a relationship between C for participants with 2 years and without less than 2years expert C experience who use different search engines.These tables are retrieved from the table that is shown in Table 2that shows the numbers of the different levels of confidence, C forparticipants with and without expert CC experience. To test thenull hypotheses that the categorical variable C is independent fromthe categorical variable Java experience, we apply three 2tests,2G, 2K , and 2P for the search engines Google, Koders, and Portfolio respectively. We obtain 2G  6.7 for p  0.09, 2K  2.6 forp  0.47, and 2P  2.09 for p  0.56. The insignificant values of2 and large values of p  0.05 allow us to accept these null hypotheses suggesting that there is no statistically strong relationship between expert C programming experiences of participants and the values of reported Cs for the code search enginesGoogle Code Search, Koders, and Portfolio.6.5 Usefulness of VisualizationThirty three participants reported that the visualization of functional dependencies in Portfolio is useful and helped them to evaluate potential reuse of retrieved functions, while 12 respondents didnot find this visualization useful. Out these 33 participants whofound it useful, 27 had more than one year of C experience,while out of these 12 participants who did not find this visualizationuseful, only two had more than one year of C experience.7. RELATED WORKDifferent code mining techniques and tools have been proposedto find relevant software components as it is shown in Table 3.CodeFinder iteratively refines code repositories in order to improvethe precision of returned software components 12. Unlike Portfolio, CodeFinder heavily depends on the descriptions often incomplete of software components to use word matching, while Portfolio uses Pagerank and SANs to help programmers navigate andunderstand usages of retrieved functions.Codebroker system uses source code and comments written byprogrammers to query code repositories to find relevant artifacts37. Unlike Portfolio, Codebroker is dependent upon the descriptions of documents and meaningful names of program variablesand types, and this dependency often leads to lower precision ofreturned projects.Even though it returns code snippets rather than functions, Micais similar to Portfolio since it uses API calls from Java Development Kit to guide code search 32. However, Mica uses help documentation to refine the results of the search, while Portfolio automatically retrieves functions from arbitrary code repositories and ituses more sophisticated models to help programmers evaluate thepotential of code reuse faster and a with higher precision.Exemplar, SNIFF, and Mica use documentation for API calls forquery expansion 10, 32, 3. SNIFF then performs the intersection of types in these code chunks to retain the most relevant andcommon part of the code chunks. SNIFF also ranks these prunedchunks using the frequency of their occurrence in the indexed codebase. In contrast to SNIFF, Portfolio uses navigation and association models that reflect behavior of programmers and improve theprecision of the search engine. In addition, Portfolio offers a visualization of usages of functions that it retrieves automatically fromexisting source code, thus avoiding the need for thirdparty documentation for API calls.Webmining techniques have been applied to graphs derived fromprogram artifacts before. Notably, Inoue et al. proposed Component Rank16 as a method to highlight the mostfrequently usedclasses by applying a variant of PageRank to a graph composed ofJava classes and an assortment of relations among them. Qualityof match QOM ranking measures the overall goodness of matchbetween two given components 33, which is different from Portfolio in many respects, one of which is to retrieve functions basedon surfing behavior of programmers and associations between concepts in these functions.Gridle24 also applies PageRank to a graph of Java classes. InPortfolio, we apply PageRank to a graph with nodes as functionsand edges as call relationships among the functions. In addition,we use spreading activation on the call graph to retrieve chains ofrelevant function invocations, rather than single fragments of code.Programming taskoriented tools like Prospector, Hipikat, Strathcona, and xSnippet assist programmers in writing complicated code21, 7, 14, 27. However, their utilities are not applicable whensearching for relevant functions given a query containing highlevelconcepts with no source code.Robillard proposed an algorithm for calculating program elements of likely interest to a developer 26. Portfolio is similarto this algorithm in that it uses relations between functions in theretrieved projects to compute the level of interest ranking of theproject, however, Robillard does not use models that reflect thesurfing behavior of programmers and association models that improve the precision of search. We think there is a potential in exploring connections between Robillards approach and Portfolio.S6 is a code search engine that uses a set of userguided programtransformations to map highlevel queries into a subset of relevantcode fragments 25, not necessarily functions. Like Portfolio, S6uses query expansion, however, it requires additional lowlevel details from the user, such as data types of test cases.Approach Granularity Search ResultUnit Usage MethodAMC 13 U N W TCodeBroker 37 P,U Y W,Q TCodeFinder 12 F,U Y W,Q TCodeGenie 20 P N W TExemplar 10 A Y W,Q TGoogle Code Search U N W TGridle 24 U N W THipikat 7 P Y W,Q TKoders U N W TKrugle U N W TMAPO 38 F N W,Q TMica 32 U,F Y W,Q TParseWeb 34 U,F N W,Q TPortfolio F,P Y P,S,W GProspector 21 F N T TS6 25 F,P,U Y W,Q TSNIFF 3 F,U Y T,W TSourceforge A N W TSourcerer 23 F,P,U Y P,W TSPARSJ 1516 F Y P TSpotWeb 35 U N W TStrathcona 14 F Y W TxSnippet 27 F Y T,W TTable 3 Comparison of Portfolio with other related approaches. Column Granularity specifies how search results arereturned by each approach Projects, Functions, or Unstructured text,and if the usage of these resulting code units is shown Yes or No. Thecolumn Search Method specifies the search algorithms or techniquesthat are used in the code search engine, i.e., Pagerank, Spreading activation, simple Word matching, parameter Type matching, or Queryexpansion techniques. Finally, the last column tells if the search engineshows a list of code fragments as Text or it uses a Graphical representation of search results to illustrate code usage for programmers.8. CONCLUSIONWe created an approach called Portfolio for finding highly relevant functions and projects from a large archive of CC sourcecode. In Portfolio, we combined various natural language processing NLP and indexing techniques with a variation of PageRankand spreading activation network SAN algorithms to address theneed of programmers to reuse retrieved code as functional abstractions. We evaluated Portfolio with 49 professional CC programmers and found with strong statistical significance that it performedbetter than Google Code Search and Koders in terms of reportinghigher confidence levels and precisions for retrieved CC codefragments and functions. In addition, participants expressed strongsatisfaction with using Portfolios visualization technique since itenabled them to assess how retrieved functions are used in contextsof other functions.AcknowledgmentsWewarmly thank nine graduate students, Luca DiMinervino, Arunrajkumar Dharumar, Rohan Dhond, Sekhar Gopisetty, HariharanSubramanian, Ameya Barve, Naresh Regunta, Ashim Shivhare,Denzil Rodrigues, from the University of Illinois at Chicago whocontributioned to Portfolio as part of their work towards the completion of thier Master of Science in Computer Science degrees.We also thank Bogdan Dit from the College of William and Maryfor his help in building parts of Portfolio. We are grateful to theanonymous ICSE11 reviewers for their relevant and useful comments and suggestions, which helped us to significantly improvean earlier version of this paper. This work is supported by NSFCCF0916139, CCF0916260, and Accenture.9. REFERENCES1 L. Becchetti, C. Castillo, D. Donato, R. BaezaYATES, andS. Leonardi. Link analysis for web spam detection. ACMTrans. Web, 21142, 2008.2 S. Brin and L. Page. The anatomy of a largescalehypertextual web search engine. Computer Networks,3017107117, 1998.3 S. Chatterjee, S. Juvekar, and K. Sen. Sniff A search enginefor java using freeform queries. In FASE, pages 385400,2009.4 A. M. Collins and E. F. Loftus. A spreadingactivation theoryof semantic processing. Psychological Review, 826407 428, 1975.5 T. A. Corbi. Program understanding Challenge for the1990s. IBM Systems Journal, 282294306, 1989.6 F. Crestani. Application of spreading activation techniques ininformation retrieval. Artificial Intelligence Review,116453482, 1997.7 D. Cubranic, G. C. Murphy, J. Singer, and K. S. Booth.Hipikat A project memory for software development. IEEETrans. Software Eng., 316446465, 2005.8 J. W. Davison, D. Mancl, and W. F. Opdyke. Understandingand addressing the essential costs of evolving systems. BellLabs Technical Journal, 524454, 2000.9 D. F. Gleich, P. G. Constantine, A. D. Flaxman, andA. Gunawardana. Tracking the random surfer empiricallymeasured teleportation parameters in pagerank. InWWW,pages 381390, 2010.10 M. Grechanik, C. Fu, Q. Xie, C. McMillan, D. Poshyvanyk,and C. M. Cumby. A search engine for finding highlyrelevant applications. In ICSE 1, pages 475484, 2010.11 Z. Gyngyi and H. GarciaMolina. Link spam alliances. InVLDB 05, pages 517528. VLDB Endowment, 2005.12 S. Henninger. Supporting the construction and evolution ofcomponent repositories. In ICSE, pages 279288, 1996.13 R. Hill and J. Rideout. Automatic method completion. InASE, pages 228235, 2004.14 R. Holmes and G. C. Murphy. Using structural context torecommend source code examples. In ICSE, pages 117125,2005.15 K. Inoue, R. Yokomori, H. Fujiwara, T. Yamamoto,M. Matsushita, and S. Kusumoto. Component rank Relativesignificance rank for software component search. In ICSE,pages 1424, 2003.16 K. Inoue, R. Yokomori, T. Yamamoto, M. Matsushita, andS. Kusumoto. Ranking significance of software componentsbased on use relations. IEEE Trans. Softw. Eng.,313213225, 2005.17 C. W. Krueger. Software reuse. ACM Comput. Surv.,242131183, 1992.18 W. Landi. Undecidability of static analysis. LOPLAS,14323337, 1992.19 A. N. Langville and C. D. Meyer. Googles PageRank andBeyond The Science of Search Engine Rankings. PrincetonUniversity Press, Princeton, NJ, USA, 2006.20 O. A. L. Lemos, S. K. Bajracharya, J. Ossher, R. S. Morla,P. C. Masiero, P. Baldi, and C. V. Lopes. Codegenie usingtestcases to search and reuse source code. In ASE 07, pages525526, New York, NY, USA, 2007. ACM.21 D. Mandelin, L. Xu, R. Bodk, and D. Kimelman. Jungloidmining helping to navigate the API jungle. In PLDI, pages4861, 2005.22 C. D. Manning, P. Raghavan, and H. Schtze. Introduction toInformation Retrieval. Cambridge University Press, NewYork, NY, USA, 2008.23 J. Ossher, S. Bajracharya, E. Linstead, P. Baldi, andC. Lopes. Sourcererdb An aggregated repository ofstatically analyzed and crosslinked open source javaprojects. MSR, 0183186, 2009.24 D. Puppin and F. Silvestri. The social network of javaclasses. In SAC 06, pages 14091413, New York, NY, USA,2006. ACM.25 S. P. Reiss. Semanticsbased code search. In ICSE, pages243253, 2009.26 M. P. Robillard. Automatic generation of suggestions forprogram investigation. In ESECFSE, pages 1120, 2005.27 N. Sahavechaphan and K. T. Claypool. XSnippet mining forsample code. In OOPSLA, pages 413430, 2006.28 H. Saito, M. Toyoda, M. Kitsuregawa, and K. Aihara. Alargescale study of link spam detection by graph algorithms.In AIRWeb 07, pages 4548, New York, NY, USA, 2007.ACM.29 J. Sillito, G. C. Murphy, and K. De Volder. Asking andanswering questions during a programming change task.IEEE Trans. Softw. Eng., 344434451, 2008.30 J. Sillito, G. C. Murphy, and K. D. Volder. Questionsprogrammers ask during software evolution tasks. InSIGSOFT FSE, pages 2334, 2006.31 S. Sim, C. Clarke, and R. Holt. Archetypal source codesearches A survey of software developers and maintainers.ICPC, 0180, 1998.32 J. Stylos and B. A. Myers. A websearch tool for findingAPI components and examples. In IEEE Symposium on VLand HCC, pages 195202, 2006.33 N. Tansalarak and K. T. Claypool. Finding a needle in thehaystack A technique for ranking matches betweencomponents. In CBSE, pages 171186, 2005.34 S. Thummalapenta and T. Xie. Parseweb a programmerassistant for reusing open source code on the web. In ASE07, pages 204213, New York, NY, USA, 2007. ACM.35 S. Thummalapenta and T. Xie. Spotweb Detectingframework hotspots and coldspots via mining open sourcecode on the web. In ASE 08, pages 327336, Washington,DC, USA, 2008. IEEE Computer Society.36 I. H. Witten, A. Moffat, and T. C. Bell.Managing GigabytesCompressing and Indexing Documents and Images, SecondEdition. Morgan Kaufmann, 1999.37 Y. Ye and G. Fischer. Supporting reuse by deliveringtaskrelevant and personalized information. In ICSE, pages513523, 2002.38 H. Zhong, T. Xie, L. Zhang, J. Pei, and H. Mei. MAPOMining and recommending API usage patterns. In ECOOP2009, July 2009.
