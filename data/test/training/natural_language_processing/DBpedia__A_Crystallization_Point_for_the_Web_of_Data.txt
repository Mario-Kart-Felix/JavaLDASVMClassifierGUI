DBpedia  A Crystallization Point for theWeb of DataChristian Bizer a,, Jens Lehmann b,, Georgi Kobilarov a,Soren Auer b, Christian Becker a, Richard Cyganiak cSebastian Hellmann baFreie Universitat Berlin, Webbased Systems Group,Garystr. 21, D14195 Berlin, GermanybUniversitat Leipzig, Department of Computer Science,Johannisgasse 26, D04103 Leipzig, GermanycDigital Enterprise Research Institute, National University of Ireland,Lower Dangan, Galway, IrelandAbstractThe DBpedia project is a community effort to extract structured informationfrom Wikipedia and to make this information accessible on the Web. The resultingDBpedia knowledge base currently describes over 2.6 million entities. For each ofthese entities, DBpedia defines a globally unique identifier that can be dereferencedover the Web into a rich RDF description of the entity, including humanreadabledefinitions in 30 languages, relationships to other resources, classifications in fourconcept hierarchies, various facts as well as datalevel links to other Web datasources describing the entity. Over the last year, an increasing number of datapublishers have begun to set datalevel links to DBpedia resources, making DBpediaa central interlinking hub for the emerging Web of data. Currently, the Web ofinterlinked data sources around DBpedia provides approximately 4.7 billion pieces ofinformation and covers domains such as geographic information, people, companies,films, music, genes, drugs, books, and scientific publications. This article describesthe extraction of the DBpedia knowledge base, the current status of interlinkingDBpedia with other data sources on the Web, and gives an overview of applicationsthat facilitate the Web of Data around DBpedia.Key words Web of Data, Linked Data, Knowledge Extraction, Wikipedia, RDF Corresponding authors.Email addresses chrisbizer.de Christian Bizer,lehmanninformatik.unileipzig.de Jens Lehmann.Preprint submitted to Elsevier May 25, 20091 IntroductionKnowledge bases play an increasingly important role in enhancing the intelligence of Web and enterprise search, as well as in supporting informationintegration. Today, most knowledge bases cover only specific domains, arecreated by relatively small groups of knowledge engineers, and are very costintensive to keep uptodate as domains change. At the same time, Wikipediahas grown into one of the central knowledge sources of mankind, maintainedby thousands of contributors.The DBpedia project leverages this gigantic source of knowledge by extractingstructured information from Wikipedia and making this information accessibleon the Web. The resulting DBpedia knowledge base currently describes morethan 2.6 million entities, including 198,000 persons, 328,000 places, 101,000musical works, 34,000 films, and 20,000 companies. The knowledge base contains 3.1 million links to external web pages and 4.9 million RDF links intoother Web data sources. The DBpedia knowledge base has several advantagesover existing knowledge bases It covers many domains, it represents real community agreement, it automatically evolves as Wikipedia changes, it is trulymultilingual, and it is accessible on the Web.For each entity, DBpedia defines a globally unique identifier that can be dereferenced according to the Linked Data principles 4,5. As DBpedia covers awide range of domains and has a high degree of conceptual overlap with various openlicense datasets that are already available on the Web, an increasingnumber of data publishers have started to set RDF links from their datasources to DBpedia, making DBpedia one of the central interlinking hubs ofthe emerging Web of Data. The resulting Web of interlinked data sourcesaround DBpedia contains approximately 4.7 billion RDF triples 1 and covers domains such as geographic information, people, companies, films, music,genes, drugs, books, and scientific publications.The DBpedia project makes the following contributions to the developmentof the Web of Data We develop an information extraction framework that converts Wikipediacontent into a rich multidomain knowledge base. By accessing the Wikipedialive article update feed, the DBpedia knowledge base timely reflects the actual state of Wikipedia. A mapping from Wikipedia infobox templates toan ontology increases the data quality. We define a Webdereferenceable identifier for each DBpedia entity. Thishelps to overcome the problem of missing entity identifiers that has hindered1 httpesw.w3.orgtopicTaskForcesCommunityProjectsLinkingOpenDataDataSetsStatistics2the development of the Web of Data so far and lays the foundation forinterlinking data sources on the Web. We publish RDF links pointing from DBpedia into other Web data sourcesand support data publishers in setting links from their data sources to DBpedia. This has resulted in the emergence of a Web of Data around DBpedia.This article documents the recent progress of the DBpedia effort. It buildsupon two earlier publications about the project 1,2. The article is structuredas follows We give an overview of the DBpedia architecture and knowledgeextraction techniques in Section 2. The resulting DBpedia knowledge base isdescribed in Section 3. Section 4 discusses the different access mechanisms thatare used to serve DBpedia on the Web. In Section 5, we give an overview of theWeb of Data that has developed around DBpedia. We showcase applicationsthat facilitate DBpedia in Section 6 and review related work in Section 7.Section 8 concludes and outlines future work.2 The DBpedia Knowledge Extraction FrameworkWikipedia articles consist mostly of free text, but also contain various types ofstructured information in the form of wiki markup. Such information includesinfobox templates, categorisation information, images, geocoordinates, linksto external Web pages, disambiguation pages, redirects between pages, andlinks across different language editions of Wikipedia. The DBpedia projectextracts this structured information from Wikipedia and turns it into a richknowledge base. In this chapter, we give an overview of the DBpedia knowledgeextraction framework, and discuss DBpedias infobox extraction approach inmore detail.2.1 Architecture of the Extraction FrameworkFigure 1 gives an overview of the DBpedia knowledge extraction framework.The main components of the framework are PageCollections which are anabstraction of local or remote sources of Wikipedia articles, Destinations thatstore or serialize extracted RDF triples, Extractors which turn a specific typeof wiki markup into triples, Parsers which support the extractors by determining datatypes, converting values between different units and splitting markupinto lists. ExtractionJobs group a page collection, extractors and a destinationinto a workflow. The core of the framework is the Extraction Manager whichmanages the process of passing Wikipedia articles to the extractors and delivers their output to the destination. The Extraction Manager also handlesURI management and resolves redirects between articles.3Figure 1. Overview of DBpedia components.The framework currently consists of 11 extractors which process the followingtypes of Wikipedia content Labels. All Wikipedia articles have a title, which is used as an rdfslabelfor the corresponding DBpedia resource. Abstracts. We extract a short abstract first paragraph, represented usingrdfscomment and a long abstract text before a table of contents, at most500 words, using the property dbpediaabstract from each article. Interlanguage links. We extract links that connect articles about the sametopic in different language editions of Wikipedia and use them for assigninglabels and abstracts in different languages to DBpedia resources. Images. Links pointing at Wikimedia Commons images depicting a resourceare extracted and represented using the foafdepiction property. Redirects. In order to identify synonymous terms, Wikipedia articles canredirect to other articles. We extract these redirects and use them to resolvereferences between DBpedia resources. Disambiguation. Wikipedia disambiguation pages explain the different meanings of homonyms. We extract and represent disambiguation links using thepredicate dbpediadisambiguates. External links. Articles contain references to external Web resources whichwe represent using the DBpedia property dbpediareference. Pagelinks. We extract all links between Wikipedia articles and represent4them using the dbpediawikilink property. Homepages. This extractor obtains links to the homepages of entities such ascompanies and organisations by looking for the terms homepage or websitewithin article links represented using foafhomepage. Categories. Wikipedia articles are arranged in categories, which we representusing the SKOS vocabulary 2 . Categories become skosconcepts categoryrelations are represented using skosbroader. Geocoordinates. The geoextractor expresses coordinates using the BasicGeo WGS84 latlong Vocabulary 3 and the GeoRSS Simple encoding ofthe W3C Geospatial Vocabulary 4 . The former expresses latitude and longitude components as separate facts, which allows for simple areal filteringin SPARQL queries.The DBpedia extraction framework is currently set up to realize two workflows A regular, dumpbased extraction and the live extraction.Dumpbased extraction. The Wikimedia Foundation publishes SQL dumpsof all Wikipedia editions on a monthly basis. We regularly update the DBpediaknowledge base with the dumps of 30 Wikipedia editions. The dumpbasedworkflow uses the DatabaseWikipedia page collection as the source of articletexts and the NTriples serializer as the output destination. The resultingknowledge base is made available as Linked Data, for download, and via DBpedias main SPARQL endpoint cf. Section 4.Live Extraction. The Wikimedia Foundation has given the DBpedia projectaccess to the Wikipedia OAIPMH live feed that instantly reports all Wikipediachanges. The live extraction workflow uses this update stream to extract newRDF whenever a Wikipedia article is changed. The text of these articles is accessed via the LiveWikipedia page collection, which obtains the current articleversion encoded according to the OAIPMH protocol. The SPARQLUpdateDestination deletes existing and inserts new triples into a separate triple store.According to our measurements, about 1.4 article pages are updated each second on Wikipedia. The framework can handle up to 8.8 pages per second ona 2.4 GHz dualcore machine this includes consumption from the stream, extraction, diffing and loading the triples into a Virtuoso triple store. The timelag for DBpedia to reflect Wikipedia changes lies between one or two minutes.The bottleneck here is the update stream, since changes normally need morethan one minute to arrive from Wikipedia. More information about the liveextraction is found at httpen.wikipedia.orgwikiUserDBpedia.2 httpwww.w3.org200402skos3 httpwww.w3.org200301geo4 httpwww.w3.org2005IncubatorgeoXGRgeo5Figure 2. Infobox Tom Hanks. Figure 3. Infobox Andre Agassi.2.2 Generic versus Mappingbased Infobox ExtractionThe type of wiki contents that is most valuable for the DBpedia extractionare Wikipedia infoboxes. Infoboxes display an articles most relevant facts asa table of attributevalue pairs on the top righthand side of the Wikipediapage. Figure 2 and Figure 3 show excerpts of the wiki markup behind theinfoboxes describing Tom Hanks and Andre Agassi. Wikipedias infobox template system has evolved over time without central coordination. Differentcommunities use different templates to describe the same type of things e.g.infoboxcityjapan, infoboxswisstown and infoboxtownde. Different templates use different names for the same attribute e.g. birthplace andplaceofbirth. As many Wikipedia editors do not strictly follow the recommendations given on the page that describes a template, attribute values areexpressed using a wide range of different formats and units of measurement.The DBpedia project has decided to deal with this situation by using twodifferent extraction approaches in parallel A generic approach which aims atwide coverage and a mappingbased approach which aims at high data quality.Generic Infobox Extraction. The generic infobox extraction algorithm,which is described in detail in 2, processes all infoboxes within a Wikipediaarticle. It creates triples from the infobox data in the following mannerThe corresponding DBpedia URI of the Wikipedia article is used as subject. The predicate URI is created by concatenating the namespace fragmenthttpdbpedia.orgproperty and the name of the infobox attribute. Objects are created from the attribute value. Property values are postprocessedin order to generate suitable URI references or literal values. This includesrecognizing MediaWiki links, detecting lists, and using units as datatypes.MediaWiki templates may be nested, which we handle through a blanknodecreation algorithm. The advantage of the generic extraction is its complete coverage of all infoboxes and infobox attributes. The main disadvantage is thatsynonymous attribute names are not resolved, which makes writing queriesagainst generic infobox data rather cumbersome. As Wikipedia attributes donot have explicitly defined datatypes, a further problem is the relatively higherror rate of the heuristics that are used to determine the datatypes of attribute values.Mappingbased Infobox Extraction. In order to overcome the problems of6synonymous attribute names and multiple templates being used for the sametype of things, we mapped Wikipedia templates to an ontology. This ontology was created by manually arranging the 350 most commonly used infoboxtemplates within the English edition of Wikipedia into a subsumption hierarchy consisting of 170 classes and then mapping 2350 attributes from withinthese templates to 720 ontology properties. The property mappings definefinegrained rules on how to parse infobox values and define target datatypes,which help the parsers to process attribute values. For instance, if a mappingdefines the target datatype to be a list of links, the parser will ignore additional text that might be present in the attribute value. The ontology currentlyuses 55 different datatypes. Deviant units of measurement are normalized toone of these datatypes. Instance data within the infobox ontology is thereforecleaner and better structured than data that is generated using the generic extraction algorithm. The disadvantage of the mappingbased approach is thatit currently covers only 350 Wikipedia templates therefore it only providesdata about 843,000 entities compared to 1,462,000 entities that are coveredby the generic approach. While the ontology is currently relatively simple,we plan to extend it further, e.g. with class disjointness axioms and inverseproperties. The main purpose of such extensions will be to allow consistencychecks in DBpedia and use inferences when answering SPARQL queries. Themembers of the DBpedia team will not be able to extend the ontology, themappings and the parser rules to cover all Wikipedia infoboxes, due to the sizeof the task and the knowledge required to map templates from exotic domains.We are therefore working on methods to crowdsource this task. We are currently developing an approach to integrate the DBpedia ontology itself backinto Wikipedia. Ontology definitions related to a certain infobox templateswill be represented themselves as infoboxes on the corresponding templatedefinition page. Combined with the live extraction, the wider Wikipedia community would thus have a powerful tool for extending and refining of both the ontology and the infobox mappings.3 The DBpedia Knowledge BaseThe DBpedia knowledge base currently consists of around 274 million RDFtriples, which have been extracted from the English, German, French, Spanish, Italian, Portuguese, Polish, Swedish, Dutch, Japanese, Chinese, Russian,Finnish, Norwegian, Catalan, Ukranian, Turkish, Czech, Hungarian, Romanian, Volapuk, Esperanto, Danish, Slovak, Indonesian, Arabic, Korean, Hebrew, Lithuanian, Vietnamese, Slovenian, Serbian, Bulgarian, Estonian, andWelsh versions of Wikipedia. The knowledge base describes more than 2.6million entities. It features labels and short abstracts in 30 different languages 609,000 links to images 3,150,000 links to external web pages 415,0007Wikipedia categories, and 286,000 YAGO categories.Ontology Class Instances Example PropertiesPerson 198,056 name, birthdate, birthplace, employer, spouseArtist 54,262 activeyears, awards, occupation, genreActor 26,009 academyaward, goldenglobeaward, activeyearsMusicalArtist 19,535 genre, instrument, label, voiceTypeAthlete 74,832 currentTeam, currentPosition, currentNumberPolitician 12,874 predecessor, successor, partyPlace 247,507 lat, longBuilding 23,304 architect, location, openingdate, styleAirport 7,971 location, owner, IATA, lat, longBridge 1,420 crosses, mainspan, openingdate, lengthSkyscraper 2,028 developer, engineer, height, architect, costPopulatedPlace 181,847 foundingdate, language, area, populationRiver 10,797 sourceMountain, length, mouth, maxDepthOrganisation 91,275 location, foundationdate, keypersonBand 14,952 currentMembers, foundation, homeTown, labelCompany 20,173 industry, products, netincome, revenueEduc.Institution 21,052 dean, director, graduates, staff, studentsWork 189,620 author, genre, languageBook 15,677 isbn, publisher, pages, author, mediatypeFilm 34,680 director, producer, starring, budget, releasedMusicalWork 101,985 runtime, artist, label, producerAlbum 74,055 artist, label, genre, runtime, producer, coverSingle 24,597 album, format, releaseDate, band, runtimeSoftware 5,652 developer, language, platform, licenseTelevisionShow 10,169 network, producer, episodenumber, themeTable 1Common DBpedia classes with the number of their instances and example properties.Table 1 gives an overview of common DBpedia classes, and shows the numberof instances and some example properties for each class. In the following, wedescribe the structure of the DBpedia knowledge base, explain how identifiersare built and compare the four classification schemata that are offered byDBpedia.3.1 Identifying EntitiesDBpedia uses English article names for creating identifiers. Information fromother language versions of Wikipedia is mapped to these identifiers by bidirectionally evaluating the interlanguage links between Wikipedia articles.Resources are assigned a URI according to the pattern httpdbpedia.orgresourceName , where Name is taken from the URL of the source Wikipediaarticle, which has the form httpen.wikipedia.orgwikiName . This yields8certain beneficial properties DBpedia URIs cover a wide range of encyclopedic topics. They are defined by community consensus. There are clear policies in place for their management. A extensive textual definition of the entity is available at a wellknown Weblocation the Wikipedia page.3.2 Classifying EntitiesDBpedia entities are classified within four classification schemata in order tofulfill different application requirements. We compare these schemata belowWikipedia Categories. DBpedia contains a SKOS representation of theWikipedia category system. The category system consists of 415,000 categories. The main advantage of the category system is that it is collaboratively extended and kept uptodate by thousands of Wikipedia editors. Adisadvantage is that categories do not form a proper topical hierarchy, asthere are cycles in the category system and as categories often only representa rather loose relatedness between articles.YAGO. The YAGO classification schema consists of 286,000 classes whichform a deep subsumption hierarchy. The schema was created by mappingWikipedia leaf categories, i.e. those not having subcategories, to WordNetsynsets. Details of the mapping algorithm are described in 19. Characteristics of the YAGO hierarchy are its deepness and the encoding of muchinformation in one class e.g. the class MultinationalCompaniesHeadquarteredInTheNetherlands. While YAGO achieves a high accuracy in general,there are a few errors and omissions e.g. the mentioned class is not a subclass of MultinationalCompanies due to its automatic generation. Wejointly developed a script that assigns YAGO classes to DBpedia entities.The script is available at the YAGO download page 5 .UMBEL. The Upper Mapping and Binding Exchange Layer UMBEL isa lightweight ontology that has been created for interlinking Web contentand data. UMBEL was derived from OpenCyc and consists of 20,000 classes.OpenCyc classes in turn are partially derived from Cyc collections, whichare based on WordNet synsets. Since YAGO also uses WordNet synsets andis based on Wikipedia, a mapping from OpenCyc classes to DBpedia canbe derived via UMBEL 6 . The classification is maintained by the UMBELproject itself and details about its generation process can be found at the5 httpwww.mpiinf.mpg.deyagonagayagodownloads.html6 httpfgiasson.comblogindex.php20080904explodingdbpediasdomainusingumbel9UMBEL website 7 .DBpedia Ontology. The DBpedia ontology consists of 170 classes that forma shallow subsumption hierarchy. It includes 720 properties with domain andrange definitions. The ontology was manually created from the most commonly used infobox templates within the English edition of Wikipedia. Theontology is used as target schema by the mappingbased infobox extractiondescribed in Section 2.2. The left column in Table 1 displays a part of theclass hierarchy of the DBpedia ontology.3.3 Describing EntitiesEvery DBpedia entity is described by a set of general properties and a setof infoboxspecific properties, if the corresponding English Wikipedia articlecontains an infobox. The general properties include a label, a short and a longEnglish abstract, a link to the corresponding Wikipedia article, if availablegeocoordinates, a link to an image depicting the entity, links to external Webpages, and links to related DBpedia entities. If an entity exists in multiplelanguage versions of Wikipedia, then short and long abstracts within theselanguages and links to the different language Wikipedia articles are added tothe description.Described Mio. Unique Triples TriplesEntities Triples Properties Property EntityGenericExtract. 1,462,108 26.0 38,659 673.7 17.81Mappingbased 843,169 7.0 720 9722.2 8.34Pagelinks 2,853,315 70.2 1 70.2mio 24.61Table 2Comparison of the generic infobox, mappingbased infobox and pagelinks datasets.Infoboxspecific properties which result from the generic extraction are defined in the httpdbpedia.orgproperty namespace. Properties resulting from the mappingbased infobox extraction are defined in the namespacehttpdbpedia.orgontology.Table 2 compares the datasets that result from generic infobox extraction, themappingbased infobox extraction and from the extraction of links betweenWikipedia pages all numbers are for DBpedia release 3.2, English version.DBpedia contains generic infobox data for 1,462,000 resources compared to843,000 resources that are covered by the mappingbased approach. Thereare links between 2,853,315 Wikipedia pages. This number of pages is higherthan the number of entities in DBpedia 2.6 million as there are additional7 httpwww.umbel.org10pages for lists and templates in Wikipedia. The mappingbased dataset contains 720 different properties compared to 38,659 different properties that areused within the generic dataset including many synonymous properties. Thepagelinks dataset uses a single property to represent the untyped links betweenWikipedia pages.Connected Mio. Unique Indegree ClusterEntities Triples Properties Max Avg CoefficientGenericExtract. 1,029,712 5.6 9911 105,840 8.76 0.1336Mappingbased 627,941 2.9 340 65,387 11.03 0.1037Pagelinks 2,796,401 46.2 1 190,995 19.15 0.1696Table 3Comparison of the graph structure of the generic infobox, mappingbased infoboxand pagelinks datasets.In a next step, we measured characteristics of the RDF graph that connectsDBpedia entities. For doing this, we removed all triples from the datasets thatdid not point at a DBpedia entity, including all literal triples, all external linksand all dead links. The size of and number of link properties within thesereduced datasets is listed in Table 3. Removing the triples showed, that thepercentage of properties pointing to other DBpedia entities is much higher inthe mappingbased dataset 53 compared to generic dataset 25.6. Wecalculated the average node indegree as the sum of all inbound edges divided bythe number of objects, which had at least one inbound edge from the dataset.This allows to analyse the indegree separately from the coverage or the sizeof the dataset. The entity with the highest indegree within all three datasetsis United States. As shown in Figure 4, the node indegrees follow a powerlaw distribution in all datasets which is a typical characteristic of small worldnetworks 18. The clustering coefficient given in the last column of Table 3 wascalculated as the number of existing connections between neighbors of a node,divided by possible connections in a directed graph k  k  1, k  numberof node neighbors and averaged over all nodes. The mappingbased approachhas a slightly lower clustering coefficient because of its lower coverage.4 Accessing the DBpedia Knowledge Base over the WebDBpedia is served on the Web under the terms of the GNU Free Documentation License. In order to fulfill the requirements of different client applications,we provide the DBpedia knowledge base through four access mechanismsLinked Data is a method of publishing RDF data on the Web that relieson HTTP URIs as resource identifiers and the HTTP protocol to retrieve11Figure 4. Comparison of the generic infobox, mappingbased infobox and pagelinksdatasets in terms of node indegree versus rank.resource descriptions 4,5. DBpedia resource identifiers such as httpdbpedia.orgresourceBerlin are set up to return a RDF descriptionswhen accessed by Semantic Web agents such as data browsers or crawlersof Semantic Web search engines, and b a simple HTML view of the sameinformation to traditional Web browsers. HTTP content negotiation is usedto deliver the appropriate format.SPARQL Endpoint. We provide a SPARQL endpoint for querying the DBpedia knowledge base. Client applications can send queries over the SPARQLprotocol to the endpoint at httpdbpedia.orgsparql. In addition tostandard SPARQL, the endpoint supports several extensions of the querylanguage that have proved useful for developing client applications, suchas full text search over selected RDF predicates, and aggregate functions,notably COUNT. To protect the service from overload, limits on query complexity and result size are in place. The endpoint is hosted using VirtuosoUniversal Server 8 .RDF Dumps. We have sliced the DBpedia knowledge base by triple predicate into several parts and offer NTriple serialisations of these parts fordownload on the DBpedia website 9 . In addition to the knowledge basethat is served as Linked Data and via the SPARQL endpoint, the downloadpage also offers infobox datasets that have been extracted from Wikipediaeditions in 29 languages other than English. These datasets can be usedas foundation for fusing knowledge between Wikipedia editions or to build8 httpvirtuoso.openlinksw.com9 httpwiki.dbpedia.orgDownloads3212applications that rely on localized Wikipedia knowledge.Lookup Index. In order to make it easy for Linked Data publishers to findDBpedia resource URIs to link to, we provide a lookup service that proposesDBpedia URIs for a given label. The Web service is based on a Lucene indexproviding a weighted label lookup, which combines string similarity with arelevance ranking similar to PageRank in order to find the most likelymatches for a given term. DBpedia lookup is available as a Web service athttplookup.dbpedia.orgapisearch.asmx.The DBpedia Web interfaces are described using the Semantic Web CrawlingSitemap Extension format 10 . Client applications can use this description tochoose the most efficient access mechanism for the task they perform.5 Interlinked Web ContentIn order to enable DBpedia users to discover further information, the DBpedia knowledge base is interlinked with various other data sources on the Webaccording to the Linked Data principles 4,5. The knowledge base currentlycontains 4.9 million outgoing RDF links 5 that point at complementary information about DBpedia entities, as well as metainformation about mediaitems depicting an entity. Over the last year, an increasing number of datapublishers have started to set RDF links to DBpedia entities. These incoming links, together with the outgoing links published by the DBpedia project,make DBpedia one of the central interlinking hubs of the emerging Web ofData. These RDF links lay the foundation forWeb of Data Browsing and Crawling. RDF links enable information consumers to navigate from data within one data source to related data withinother sources using a Linked Data browser 20,3. RDF links are also followed by the crawlers of Semantic Web search engines, which provide searchand query capabilities over crawled data 7,9,21.Web Data Fusion and Mashups. As RDF links connect data about anentity within different data sources, they can be used as a basis for fusingdata from these sources in order to generate integrated views over multiplesources 16.Web Content Annotation. DBpedia entity URIs are also used to annotateclassic Web content like blog posts or news with topical subjects as well asreferences to places, companies and people. As the number of sites that useDBpedia URIs for annotation increases, the DBpedia knowledge base coulddevelop into a valuable resource for discovering classic Web content that isrelated to an entity.10 httpsw.deri.org200707sitemapextension13Figure 5 shows RDF data links that illustrate these use cases. The first fourlinks connect the DBpedia entity Spain with complementary data about thecountry from EuroStat, the CIA World Factbook, Freebase and OpenCyc.Agents can follow these links to retrieve additional information about Spain,which again might contain further deeper links into the data sources. Thefifth link illustrates how the DBpedia identifier Data Integration is used toannotate the topical subject of a research paper from the European SemanticWeb Conference. After this and similar annotations from other sites have beencrawled by a search engine, such links enable the discovery of Web contentthat is related to a topic.httpdbpedia.orgresourceSpain owlsameAshttprdf.freebase.comnsguid.9202a8c04000641f8000000000034e30http...fuberlin.defactbookresourceSpainhttp...fuberlin.deeurostatresourcecountriesEspaC3B1ahttpsw.opencyc.org20080610conceptMx4rvVjowpwpEbGdrcN5Y29ycA.httpdata.semanticweb.orgconferenceeswc2008paper356swchasTopic httpdbpedia.orgresourceDataintegration .Figure 5. Example RDF links connecting the DBpedia entity Spain with additionalinformation from other data sources, and showing how the DBpedia identifier DataIntegration is used to annotate the topic of a conference paper.Figure 6. Data sources that are interlinked with DBpedia.14Figure 6 gives an overview of the data sources that are currently interlinkedwith DBpedia. Altogether this Web of Data amounts to approximately 4.7billion RDF triples. Two billion of these triples are served by data sourcesparticipating in the W3C Linking Open Data community project 11 , an effortto make openlicense datasets interoperable on the Web of Data by convertingthem into RDF and by interlinking them. DBpedia, with its broad topic coverage, intersects with practically all of these datasets and therefore is a usefulinterlinking hub for such efforts. A second massive source of Linked Data isthe Bio2RDF project 12 which publishes bioinformatics datasets as LinkedData on the Web in order to simplify data integration in the genomic field.Altogether, the datasets published by Bio2RDF sum up to approximately2.5 billion triples. A related effort is the Linking Open Drug Data project 13within the W3C Health Care and Life Sciences interest group which publishesdata about drugs and clinical trials and interlinks published data with theBio2RDF data cloud as well as with DBpedia.Table 4 lists the data sources that are reachable from DBpedia by outgoingRDF links 14 . The second column shows the distribution of the 4.9 millionoutgoing links over the data sources. Using these links, one can, for instance,navigate from a computer scientist in DBpedia to her publications in theDBLP database, from a DBpedia book to reviews and sales offers for thisbook provided by the RDF Book Mashup, or from a band in DBpedia to alist of their songs provided by MusicBrainz. Outgoing links to ontologies likeOpenCyc or UMBEL allow agents to retrieve additional conceptual knowledgewhich can then be used for reasoning over DBpedia and interlinked data.Data Source No. of LinksFreebase 2,400,000flickr wrappr 1,950,000WordNet 330,000GeoNames 85,000OpenCyc 60,000UMBEL 20,000Bio2RDF 25,000Data Source No. of LinksWikiCompany 25,000MusicBrainz 23,000Book Mashup 7,000Project Gutenberg 2,500DBLP Bibliography 200CIA World Factbook 200EuroStat 200Table 4Distribution of outgoing RDF links pointing from DBpedia to other datasets.Many of the outgoing links were generated based on common identificationschemata that are used within Wikipedia and within the external data sources.For instance, Wikipedia articles about books often contain ISBN numbers11 httpesw.w3.orgtopicSweoIGTaskForcesCommunityProjectsLinkingOpenData12 httpbio2rdf.wiki.sourceforge.net13 httpesw.w3.orgtopicHCLSIGLODD14 For more information about the datasets please refer to httpwiki.dbpedia.orgInterlinking15Wikipedia articles about chemical compounds are likely to contain gene, protein and molecule identifiers which are also used by other bioinformatics datasources. For generating the links to GeoNames and MusicBrainz, rulebasedapproaches that rely on a combination of different properties to match locations similar name, geocoordinates, country, administrative division, population 15 and bands similar name, similar albums, similar members 17 areused. In order to maintain the links to other data sources, we plan to employthe Silk  Link Discovery Framework 23.Data Source ClassesBBC Music musicians,bandsBio2RDF genes,proteins,moleculesCrunchBase companiesDiseasome diseasesFaviki various classesflickr wrappr various classesFOAF various classesGeoNames placesGeoSpecies speciesJohn Peel musicians,worksData Source ClassesLIBRIS authorsLinkedCT intervention, conditionsLinked DrugBank drugs, diseasesLinkedMDB filmsLingvoj languagesOpenCyc various classesOpenCalais locations, peopleSurge Radio musicians, bandsUMBEL various classesRDFohloh programming languagesRevyu various classesLODD SIDER drug side effectsSemantic WebCorpusvarious classesTable 5Data sources publishing RDF links pointing at DBpedia entities.In order to get an overview of the external data sources that currently publishRDF links pointing at DBpedia entities, we analyzed 8 million RDF documentsthat have been crawled from the Web by the Sindice Semantic Web Searchengine 21. The analysis revealed that there are currently 23 external datasources setting RDF links to DBpedia. Table 5 lists these data sources togetherwith the classes of DBpedia entities that are the targets of the incoming links.6 Applications facilitated by DBpediaThe DBpedia knowledge base and the Web of Data around DBpedia lay thefoundation for a broad range of applications. Section 6.1 describes applicationsthat rely on DBpedia as an interlinking hub to browse and explore the Webof Data. Section 6.2 focuses on applications that use the DBpedia knowledgebase to answer complex queries. Section 6.3 gives an overview of applicationsthat use DBpedia entity identifiers for the annotation of Web content.15 httplists.w3.orgArchivesPublicsemanticweb2006Dec0027.html166.1 Browsing and ExplorationAs DBpedia is interlinked with various other data sources, DBpedia URIsmake good starting points to explore or crawl the Web of Data. Data browsersthat can be used to explore the Web of Data include Tabulator 20, Marbles 16 ,Disco 17 , and the OpenLink Data Explorer 18 .DBpedia Mobile. In the following, we describe DBpedia Mobile 3, a locationaware client for the Semantic Web that uses DBpedia locations as navigationstarting points. DBpedia Mobile 19 allows users to discover, search and publish Linked Data pertaining to their current physical environment using aniPhone and other mobile devices as well as standard web browsers. Based onthe current GPS position of a mobile device, DBpedia Mobile renders an interactive map indicating nearby locations from the DBpedia dataset, as shownin Figure 7. Locations may be labeled in any the 30 languages supportedby DBpedia, and are depicted with adequate icons based on a mapping ofselected YAGO categories 19. Starting from this map, the user can explorebackground information about his surroundings by navigating along data linksinto other Web data sources Clicking on a resource retrieves web data aboutthe resource, from where RDF links may be followed into other datasets.Figure 7. DBpedia Mobile running on an iPhone 3G and showing a map view ofresources in the users proximity.DBpedia Mobile is not limited to a fixed set of data sources but may be usedto access all data sources that are or will in the future be interlinked withDBpedia or with other data sources that are reachable from DBpedia. Thisallows interesting navigation paths From a location, the user may navigateto a person within the DBpedia dataset that was born, died or worked at the16 httpbeckr.orgmarbles17 httpsites.wiwiss.fuberlin.desuhlbizerng4jdisco18 httpode.openlinksw.comexample.html19 httpbeckr.orgDBpediaMobile17location. If the person is an author, he may then follow datalevel links intothe RDF Book Mashup or the Project Gutenberg data sources and exploreinformation about the authors books. If the user is interested in local bands,he may navigate from DBpedia into MusicBrainz and find out more aboutalbums of the bands.Besides accessing Web data, DBpedia Mobile offers flexible means of filteringusing SPARQL Filters and enables users to publish their current location,pictures and reviews to the Web of Data so that they can be used by otherapplications. Instead of simply being tagged with geographical coordinates,published content is interlinked with a nearby DBpedia resource and thuscontributes to the overall richness of the geospatial Semantic Web.DBpedia Mobile is based on Marbles, a serverside application that generatesentitycentric XHTML views over Web data from several data sources usingFresnel 6 lenses and formats. Prior to rendering a view for a resource, Marblesperforms data augmentation, whereby it retrieves interlinked data from theWeb and caches retrieved data in an RDF store. This involves dereferencingthe resource URI and querying the Sindice 21 and Falcons 7 Semantic Websearch engines for related information, as well as Revyu 20 for reviews. Specificpredicates found in retrieved data such as owlsameAs and rdfsseeAlso arethen followed for up to two levels in order to gain more information aboutthe resource, and to obtain humanfriendly resource labels. Marbles employsan owlsameAs inferencer to connect URI Aliases 5 between distinct datasources, allowing it to generate unified views of resources.6.2 Querying and SearchThe DBpedia knowledge base contains a large amount of generalpurposeknowledge and can thus be used to answer quite surprising queries abouta wide range of topics.DBpedia Query Builder. A tool that has been developed to demonstratethese capabilities is the DBpedia Query Builder 21 . Figure 8 shows how thequery builder is used to answer a query about soccer players that play forspecific clubs and are born in countries with more than 10 million inhabitants.Other example queries are listed in the box on the righthand side of thescreenshot.Queries are expressed by means of a graph pattern consisting of multiple triplepatterns. For each triple pattern three form fields capture variables, identifiers20 httprevyu.com21 httpquerybuilder.dbpedia.org18or filters for the subject, predicate and object of a triple. Due to the widecoverage of DBpedia, users can hardly know which properties and identifiersare used in the knowledge base and hence can be used for querying. Consequently, users have to be guided when building queries and reasonable alternatives should be suggested. Therefore, while users type identifier names intoone of the form fields, a lookahead search proposes suitable options. Theseare obtained not just by looking for matching identifiers but by executing thecurrently built query using a variable for the currently edited identifier andfiltering the results returned for this variable for matches starting with thesearch string the user supplied. This method ensures that the identifier proposed is really used in conjunction with the graph pattern under construction,and that the query actually returns results.Figure 8. Formbased DBpedia query builder.Relationship Finder. A user interface that can be used to explore the DBpedia knowledge base is the DBpedia Relationship Finder 22 . The RelationshipFinder allows users to find connections between two different entities in DBpedia. The Relationship Finder user interface initially contains a simple formto enter two entities, as well as a small number of options, and a list of previously saved queries. While typing, the user is offered suggestions for the objecthe wants to enter. After submitting the query, the user is instantly informedwhether a connection between the objects exists. If such a connection exists,the user can furthermore preview a connection between the objects, whichis not necessarily the shortest see Figure 9. After that, several queries areposed to compute the shortest paths between the two objects, which are thendisplayed. Details of the used procedure can be found in 14.22 httprelfinder.dbpedia.org19Figure 9. The DBpedia Relationship Finder, displaying a connection between twoobjects.6.3 Content AnnotationSeveral applications have recently become publicly available that allow theannotation of Web content with DBpedia identifiers in an automated or semiautomated fashion. These annotations allow the discovery of Web content thatis related to a DBpedia entity and enable third parties to enrich their contentwith data provided by DBpedia and interlinked data sources.Muddy Boots 23 is a project commissioned by the BBC that aims to enhance the BBC news stories with external data. The Muddyboots APIsallow to identify the main actors people and companies in a BBC newsstory in an unambiguous way by means of DBpedia identifiers. In this way,the story is linked to DBpedia data, which is then used by a BBC prototypeto populate a sidebar with background information on identified actors 12.Open Calais 24 is a project by Thomson Reuters that provides a web servicefor named entity recognition from freetext as well as related tools. With therecent release 4 of the web service, entity descriptors are published as LinkedData with outgoing owlsameAs links to DBpedia, Freebase and GeoNames.With this foundation, Thomson Reuters intends to publish several commercial datasets as Linked Data.Faviki 25 is a social bookmarking tool that allows tagging of bookmarks withWikipediabased identifiers to prevent ambiguities. Identifiers are automatically suggested using the Zemanta API see below. DBpedia is leveragedto view tags by topics and to provide tag descriptions in different languages.Zemanta 26 provides tools for the semiautomated enrichment of blogs. Thecompany offers its annotation engine to third parties via an API. Zemantarecently extended its API to generate RDF links pointing at DBpedia, Freebase, MusicBrainz and Semantic CrunchBase.LODr 27 allows users to tag content that they contributed to popular Web 2.0services Flickr, del.icio.us, slideshare using Linked Data identifiers, suchas those provided by DBpedia. Tags may be translated semiautomaticallyinto identifiers using Sindice.Topbraid Composer 28 is a Semantic Web modeling environment that includes a builtin capability to resolve a label to a Wikipedia article, fromwhich it derives a DBpedia resource URI. This functionality is also exposed20to scripts using the SPARQLMotion scripting language 29 .7 Related WorkThere is a vast body of works related to the semantification of Wikipedia.Comprehensive listings are provided by Michael Bergman 30 and by Wikipediaitself 31 . We will discuss some of the important approaches in the sequel.Extraction of structured Wikipedia content. A second Wikipedia knowledge extraction effort is the Freebase Wikipedia Extraction WEX 15. Freebase 32 is a commercial company that builds a huge online database whichusers can edit in a similar fashion as they edit Wikipedia articles today. Freebase employs Wikipedia knowledge as initial content for their database thatwill afterwards be edited by Freebase users. By synchronizing the DBpediaknowledge base with Wikipedia, DBpedia in contrast relies on the existingWikipedia community to update content. Since November 2008, Freebase publishes its database as Linked Data and DBpedia as well as Freebase have setRDF links to same entities in the other data source.A third project that extracts structured knowledge from Wikipedia is theYAGO project 19. YAGO extracts 14 relationship types, such as subClassOf,type, familyNameOf, locatedIn etc. from different sources of information inWikipedia. One source is the Wikipedia category system for subClassOf,locatedIn, diedInYear, bornInYear, and another one are Wikipedia redirects. YAGO does not perform an infobox extraction like our approach. Inorder to improve the quality of its classification hierarchy, YAGO links leafcategories of the Wikipedia category hierarchy into the WordNet hierarchy.The YAGO and DBpedia projects cooperate and we serve the resulting YAGOclassification together with the DBpedia knowledge base.In 24 the KOG system is presented, which refines existing Wikipedia infoboxes based on machine learning techniques using both SVMs and a morepowerful jointinference approach expressed in Markov Logic Networks. In conjunction with DBpedia, KOG could give Wikipedia authors valuable insightsabout inconsistencies and possible improvements of infobox data.NLPbased knowledge extraction. There is a vast number of approachesemploying natural language processing techniques to obtain semantics from29 httpwww.topquadrant.comsparqlmotionsmf.htmlsmfdbpedia30 httpwww.mkbergman.comp41731 httpen.wikipedia.orgwikiWikipediaWikipediainacademicstudies32 httpwww.freebase.com21Wikipedia. Yahoo Research Barcelona, for example, published a semanticallyannotated snapshot of Wikipedia 33 , which is used by Yahoo for entity ranking 25. A commercial venture in this context is the Powerset search engine 34 ,which uses NLP for both understanding queries in natural language as wellretrieving relevant information from Wikipedia. Further potential for the DBpedia extraction as well as for the NLPfield in general lies in the idea touse huge bodies of background knowledge  like DBpedia  to improve theresults of NLPalgorithms 11,8.Stability of Wikipedia identifiers. Hepp et al. show in 10 that Wikipediapage IDs are reliable identifiers for conceptual entities and that they are stableenough to be used within knowledge management applications. Their findingconfirms the approach of using DBpedia URIs for interlinking data sourcesacross the Web of Data.Advancing Wikipedia itself. The Semantic MediaWiki project 13,22 alsoaims at enabling the reuse of information within wikis as well as at enhancingsearch and browse facilities. Semantic MediaWiki is an extension of the MediaWiki software, which allows to add structured data into wikis using a specificsyntax. Ultimately, the DBpedia and Semantic MediaWiki have similar goals.Both want to deliver the benefits of structured information in Wikipedia to theusers, but use different approaches to achieve this aim. Semantic MediaWikirequires authors to deal with a new syntax and covering all structured information within Wikipedia would require converting all information into thissyntax. DBpedia exploits the structure that already exists within Wikipedia.Therefore DBpedias approach does not require changes from Wikipedia authors and can be employed against the complete content of Wikipedia. Bothapproaches could be combined synergetically by using DBpedias extractionalgorithms for existing Wikipedia content, while SMWs typed link functionality could be used to encode and represent additional semantics in wiki texts.8 Conclusions and Future WorkThe DBpedia project showed that a rich corpus of diverse knowledge can beobtained from the large scale collaboration of endusers, who are not evenaware that they contribute to a structured knowledge base. The resultingDBpedia knowledge base covers a wide range of different domains and connectsentities across these domains. The knowledge base represents the conceptualagreement of thousands of Wikipedia editors and evolves as conceptualizations33 httpwww.yrbcn.esdokuwikidoku.phpidsemanticallyannotatedsnapshotofwikipedia34 httpwww.powerset.com22change.By allowing complex queries to be asked against Wikipedia content, the DBpedia knowledge base has the potential to revolutionize the access to Wikipedia.In the context of classic Web search engines, the knowledge base can be usedto relate search terms to entities and to improve search results based on DBpedias conceptual structure. The utility of the knowledge base as interlinkinghub for the Web of Data is demonstrated by the increasing number of datasources that decide to set RDF links to DBpedia and the growing numberof annotation tools that use DBpedia identifiers. Already today, the resultingWeb of Data around DBpedia forms an exciting testbed to develop, compare, and evaluate data integration, reasoning, and uncertainty managementtechniques, and to deploy operational Semantic Web applications.As future work, the DBpedia project currently aims in the following directionsCrosslanguage infobox knowledge fusion. Infoboxes within differentWikipedia editions cover different aspects of an entity at varying degreesof completeness. For instance, the Italian Wikipedia contains more knowledge about Italian cities and villages than the English one, while the German Wikipedia contains more structured information about persons than theEnglish edition. By fusing infobox knowledge across editions and by applying different conflict resolution and consistency checking strategies within thisprocess, it should be possible to derive an astonishingly detailed multidomainknowledge base and to significantly increase the quality of this knowledge basecompared to knowledge bases that are derived from single Wikipedia editions.Wikipedia article augmentation. Interlinking DBpedia with other datasources makes it possible to develop a MediaWiki extension that augmentsWikipedia articles with additional information as well as media items pictures, audio from these sources. For instance, a Wikipedia page about a geographic location like a city or monument can could augmented with additionalpictures from Web data sources such as Flickr or with additional facts fromstatistical data sources such as Eurostat or the CIA Factbook.Wikipedia consistency checking. The extraction of different Wikipediaeditions and interlinking DBpedia with external Web knowledge sources laysthe foundation for checking the consistency of Wikipedia content. For instance,whenever an Wikipedia author edits an infobox within a Wikipedia article, thecontent of the infobox could be checked against external data sources and thecontent of infoboxes within different language editions. Inconsistencies couldbe pointed out along with proposals on how to solve these inconsistencies. Thisway, DBpedia and the Web of Data could contribute back to the Wikipediacommunity and help to improve the overall quality of Wikipedia.23References1 S. Auer, C. Bizer, G. Kobilarov, J. Lehmann, R. Cyganiak, Z.Ives, DBpediaA nucleus for a web of open data, in Proceedings of the 6th InternationalSemantic Web Conference, 2007.2 S. Auer, J. Lehmann, What have innsbruck and leipzig in common extractingsemantics from wiki content, in Proceedings of the 4th European SemanticWeb Conference, 2007.3 C. Becker, C. Bizer, DBpedia Mobile  A LocationAware Semantic Web Client,in Proceedings of the Semantic Web Challenge, 2008.4 T. BernersLee, Linked Data  Design Issues, httpwww.w3.orgDesignIssuesLinkedData.html 2006.5 C. Bizer, R. Cyganiak, T. Heath, How to publish Linked Data on the Web,httpsites.wiwiss.fuberlin.desuhlbizerpubLinkedDataTutorial 2007.6 C. Bizer, E. Pietriga, R. Lee, D. Karger, Fresnel A browserindependentpresentation vocabulary for rdf, in Proceedings of the 5th InternationalSemantic Web Conference, 2006.7 G. Cheng, W. Ge, H. Wu, Y. Qu, Searching Semantic Web Objects Based onClass Hierarchies, in Proceedings of the 1st Linked Data on the Web Workshop,2008.8 S. Cucerzan, LargeScale Named Entity Disambiguation Based on WikipediaData, in Joint Conference on Empirical Methods in Natural LanguageProcessing and Computational Natural Language Learning, 2007.9 A. Harth, A. Hogan, J. Umbrich, S. Decker, Swse Objects before documents,in Proceedings of the Semantic Web Challenge, 2008.10 M. Hepp, K. Siorpaes, D. Bachlechner, Harvesting wiki consensus Usingwikipedia entries as vocabulary for knowledge management, IEEE InternetComputing 11 5 2007 5465.11 J. Kazama, K. Torisawa, Exploiting Wikipedia as External Knowledge forNamed Entity Recognition, in Joint Conference on Empirical Methods inNatural Language Processing and Computational Natural Language Learning,2007.12 G. Kobilarov, T. Scott, Y. Raimond, S. Oliver, C. Sizemore, M. Smethurst,R. Lee, C. Bizer, Media meets semantic web  how the bbc uses dbpedia andlinked data to make connections, in Proceedings of the 6th European SemanticWeb Conference, 2009.13 M. Krotzsch, D. Vrandecic, M. Volkel, Wikipedia and the Semantic Web  TheMissing Links, in Proceedings of Wikimania, 2005.2414 J. Lehmann, J. Schuppel, S. Auer, Discovering unknown connections  thedbpedia relationship finder, in Proceedings of the 1st SABRE Conference onSocial Semantic Web, 2007.15 Metaweb Technologies, Freebase wikipedia extraction wex, httpdownload.freebase.comwex 2009.16 F. Naumann, A. Bilke, J. Bleiholder, M. Weis, Data fusion in three stepsResolving schema, tuple, and value inconsistencies, IEEE Data EngineeringBulletin 29 2 2006 2131.17 Y. Raimond, C. Sutton, M. Sandler, Automatic Interlinking of Music Datasetson the Semantic Web , in Proceedings of the 1st Linked Data on the WebWorkshop, 2008.18 A. Reka, B. AlbertLaszlo, Statistical mechanics of complex networks, Rev.Mod. Phys. 74 2002 4797.19 F. M. Suchanek, G. Kasneci, G. Weikum, Yago A large ontology from wikipediaand wordnet, Journal of Web Semantics 6 3 2008 203217.20 T. BernersLee et al., Tabulator Exploring and analyzing linked data on thesemantic web, in Proceedings of the 3rd International Semantic Web UserInteraction Workshop, 2006.21 G. Tummarello, R. Delbru, E. Oren, Sindice.com Weaving the Open LinkedData, in Proceedings of the 6th International Semantic Web Conference, 2007.22 M. Volkel, M. Krotzsch, D. Vrandecic, H. Haller, R. Studer, Semantic wikipedia,in 15th World Wide Web Conference, 2006.23 J. Volz, C. Bizer, M. Gaedke, G. Kobilarov, Silk  A Link Discovery Frameworkfor the Web of Data, in Proceedings of the 2nd Linked Data on the WebWorkshop, 2009.24 F. Wu, D. Weld, Automatically Refining the Wikipedia Infobox Ontology, inProceedings of the 17th World Wide Web Conference, 2008.25 H. Zaragoza, H. Rode, P. Mika, J. Atserias, M. Ciaramita, G. Attardi, Rankingvery many typed entities on wikipedia., in 16th Conference on Conference onInformation and Knowledge Management, 2007.25
