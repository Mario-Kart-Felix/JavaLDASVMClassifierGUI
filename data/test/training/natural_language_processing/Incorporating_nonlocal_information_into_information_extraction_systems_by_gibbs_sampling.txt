Proceedings of the 43rd Annual Meeting of the ACL, pages 363370,Ann Arbor, June 2005. c2005 Association for Computational LinguisticsIncorporating Nonlocal Information into InformationExtraction Systems by Gibbs SamplingJenny Rose Finkel, Trond Grenager, and Christopher ManningComputer Science DepartmentStanford UniversityStanford, CA 94305jrfinkel, grenager, mannningcs.stanford.eduAbstractMost current statistical natural language processing models use only local features so as to permitdynamic programming in inference, but this makesthem unable to fully account for the long distancestructure that is prevalent in language use. Weshow how to solve this dilemma with Gibbs sampling, a simple Monte Carlo method used to perform approximate inference in factored probabilistic models. By using simulated annealing in placeof Viterbi decoding in sequence models such asHMMs, CMMs, and CRFs, it is possible to incorporate nonlocal structure while preserving tractableinference. We use this technique to augment anexisting CRFbased information extraction systemwith longdistance dependency models, enforcinglabel consistency and extraction template consistency constraints. This technique results in an errorreduction of up to 9 over stateoftheart systemson two established information extraction tasks.1 IntroductionMost statistical models currently used in natural language processing represent only local structure. Although this constraint is critical in enabling tractablemodel inference, it is a key limitation in many tasks,since natural language contains a great deal of nonlocal structure. A general method for solving thisproblem is to relax the requirement of exact inference, substituting approximate inference algorithmsinstead, thereby permitting tractable inference inmodels with nonlocal structure. One such algorithm is Gibbs sampling, a simple Monte Carlo algorithm that is appropriate for inference in any factoredprobabilistic model, including sequence models andprobabilistic context free grammars Geman and Geman, 1984. Although Gibbs sampling is widelyused elsewhere, there has been extremely little useof it in natural language processing.1 Here, we useit to add nonlocal dependencies to sequence modelsfor information extraction.Statistical hidden state sequence models, suchas Hidden Markov Models HMMs Leek, 1997Freitag and McCallum, 1999, Conditional MarkovModels CMMs Borthwick, 1999, and Conditional Random Fields CRFs Lafferty et al., 2001are a prominent recent approach to information extraction tasks. These models all encode the Markovproperty decisions about the state at a particular position in the sequence can depend only on a small local window. It is this property which allows tractablecomputation the Viterbi, Forward Backward, andClique Calibration algorithms all become intractablewithout it.However, information extraction tasks can benefitfrom modeling nonlocal structure. As an example,several authors see Section 8 mention the value ofenforcing label consistency in named entity recognition NER tasks. In the example given in Figure 1,the second occurrence of the token Tanjug is mislabeled by our CRFbased statistical NER system,because by looking only at local evidence it is unclear whether it is a person or organization. The firstoccurrence of Tanjug provides ample evidence thatit is an organization, however, and by enforcing label consistency the system should be able to get itright. We show how to incorporate constraints ofthis form into a CRF model by using Gibbs sampling instead of the Viterbi algorithm as our inference procedure, and demonstrate that this techniqueyields significant improvements on two establishedIE tasks.1Prior uses in NLP of which we are aware include Kim etal. 1995, Della Pietra et al. 1997 and Abney 1997.363the news agency Tanjug reported . . . airport , Tanjug said .Figure 1 An example of the label consistency problem excerpted from a document in the CoNLL 2003 English dataset.2 Gibbs Sampling for Inference inSequence ModelsIn hidden state sequence models such as HMMs,CMMs, and CRFs, it is standard to use the Viterbialgorithm, a dynamic programming algorithm, to infer the most likely hidden state sequence given theinput and the model see, e.g., Rabiner 1989. Although this is the only tractable method for exactcomputation, there are other methods for computing an approximate solution. Monte Carlo methodsare a simple and effective class of methods for approximate inference based on sampling. Imaginewe have a hidden state sequence model which defines a probability distribution over state sequencesconditioned on any given input. With such a modelM we should be able to compute the conditionalprobability PM so of any state sequence s s0, . . . , sN given some observed input sequenceo  o0, . . . , oN. One can then sample sequences from the conditional distribution defined bythe model. These samples are likely to be in highprobability areas, increasing our chances of findingthe maximum. The challenge is how to sample sequences efficiently from the conditional distributiondefined by the model.Gibbs sampling provides a clever solution Geman and Geman, 1984. Gibbs sampling defines aMarkov chain in the space of possible variable assignments in this case, hidden state sequences suchthat the stationary distribution of the Markov chainis the joint distribution over the variables. Thus itis called a Markov Chain Monte Carlo MCMCmethod see Andrieu et al. 2003 for a good MCMCtutorial. In practical terms, this means that wecan walk the Markov chain, occasionally outputtingsamples, and that these samples are guaranteed tobe drawn from the target distribution. Furthermore,the chain is defined in very simple terms from eachstate sequence we can only transition to a state sequence obtained by changing the state at any oneposition i, and the distribution over these possibletransitions is justPGstst1  PM sti st1i ,o. 1where si is all states except si. In other words, thetransition probability of the Markov chain is the conditional distribution of the label at the position giventhe rest of the sequence. This quantity is easy tocompute in any Markov sequence model, includingHMMs, CMMs, and CRFs. One easy way to walkthe Markov chain is to loop through the positions ifrom 1 to N , and for each one, to resample the hidden state at that position from the distribution givenin Equation 1. By outputting complete sequencesat regular intervals such as after resampling all Npositions, we can sample sequences from the conditional distribution defined by the model.This is still a gravely inefficient process, however. Random sampling may be a good way to estimate the shape of a probability distribution, but itis not an efficient way to do what we want findthe maximum. However, we cannot just transition greedily to higher probability sequences at eachstep, because the space is extremely nonconvex. Wecan, however, borrow a technique from the studyof nonconvex optimization and use simulated annealing Kirkpatrick et al., 1983. Geman and Geman 1984 show that it is easy to modify a GibbsMarkov chain to do annealing at time t we replacethe distribution in 1 withPAstst1 PM sti st1i ,o1ctj PM stj st1j ,o1ct2where c  c0, . . . , cT  defines a cooling schedule.At each step, we raise each value in the conditionaldistribution to an exponent and renormalize beforesampling from it. Note that when c  1 the distribution is unchanged, and as c  0 the distribution364Inference CoNLL SeminarsViterbi 85.51 91.85Gibbs 85.54 91.85Sampling 85.51 91.8585.49 91.8585.51 91.8585.51 91.8585.51 91.8585.51 91.8585.51 91.8585.51 91.86Mean 85.51 91.85Std. Dev. 0.01 0.004Table 1 An illustration of the effectiveness of Gibbs sampling,compared to Viterbi inference, for the two tasks addressed inthis paper the CoNLL named entity recognition task, and theCMU Seminar Announcements information extraction task. Weshow 10 runs of Gibbs sampling in the same CRF model thatwas used for Viterbi. For each run the sampler was initializedto a random sequence, and used a linear annealing schedule thatsampled the complete sequence 1000 times. CoNLL performance is measured as perentity F1, and CMU Seminar Announcements performance is measured as pertoken F1.becomes sharper, and when c  0 the distributionplaces all of its mass on the maximal outcome, having the effect that the Markov chain always climbsuphill. Thus if we gradually decrease c from 1 to0, the Markov chain increasingly tends to go uphill. This annealing technique has been shown tobe an effective technique for stochastic optimizationLaarhoven and Arts, 1987.To verify the effectiveness of Gibbs sampling andsimulated annealing as an inference technique forhidden state sequence models, we compare Gibbsand Viterbi inference methods for a basic CRF, without the addition of any nonlocal model. The results,given in Table 1, show that if the Gibbs sampler isrun long enough, its accuracy is the same as a Viterbidecoder.3 A Conditional Random Field ModelOur basic CRF model follows that of Lafferty et al.2001. We choose a CRF because it represents thestate of the art in sequence modeling, allowing bothdiscriminative training and the bidirectional flow ofprobabilistic information across the sequence. ACRF is a conditional sequence model which represents the probability of a hidden state sequencegiven some observations. In order to facilitate obtaining the conditional probabilities we need forGibbs sampling, we generalize the CRF model in aFeature NER TFCurrent Word X XPrevious Word X XNext Word X XCurrent Word Character ngram all length  6Current POS Tag XSurrounding POS Tag Sequence XCurrent Word Shape X XSurrounding Word Shape Sequence X XPresence of Word in Left Window size 4 size 9Presence of Word in Right Window size 4 size 9Table 2 Features used by the CRF for the two tasks namedentity recognition NER and template filling TF.way that is consistent with the Markov Network literature see Cowell et al. 1999 we create a linearchain of cliques, where each clique, c, represents theprobabilistic relationship between an adjacent pairof states2 using a clique potential c, which is justa table containing a value for each possible state assignment. The table is not a true probability distribution, as it only accounts for local interactions withinthe clique. The clique potentials themselves are defined in terms of exponential models conditioned onfeatures of the observation sequence, and must beinstantiated for each new observation sequence. Thesequence of potentials in the clique chain then defines the probability of a state sequence given theobservation sequence asPCRFso Ni1isi1, si 3where isi1, si is the element of the clique potential at position i corresponding to states si1 andsi.3Although a full treatment of CRF training is beyond the scope of this paper our technique assumesthe model is already trained, we list the featuresused by our CRF for the two tasks we address inTable 2. During training, we regularized our exponential models with a quadratic prior and used thequasiNewton method for parameter optimization.As is customary, we used the Viterbi algorithm toinfer the most likely state sequence in a CRF.2CRFs with larger cliques are also possible, in which casethe potentials represent the relationship between a subsequenceof k adjacent states, and contain Sk elements.3To handle the start condition properly, imagine also that wedefine a distinguished start state s0.365The clique potentials of the CRF, instantiated forsome observation sequence, can be used to easilycompute the conditional distribution over states ata position given in Equation 1. Recall that at position i we want to condition on the states in the restof the sequence. The state at this position can beinfluenced by any other state that it shares a cliquewith in particular, when the clique size is 2, thereare 2 such cliques. In this case the Markov blanketof the state the minimal set of states that rendersa state conditionally independent of all other statesconsists of the two neighboring states and the observation sequence, all of which are observed. The conditional distribution at position i can then be computed simply asPCRFsisi,o  isi1, sii1si, si1 4where the factor tables F in the clique chain are already conditioned on the observation sequence.4 Datasets and EvaluationWe test the effectiveness of our technique on two established datasets the CoNLL 2003 English namedentity recognition dataset, and the CMU SeminarAnnouncements information extraction dataset.4.1 The CoNLL NER TaskThis dataset was created for the shared task of theSeventh Conference on Computational Natural Language Learning CoNLL,4 which concerned namedentity recognition. The English data is a collectionof Reuters newswire articles annotated with four entity types person PER, location LOC, organization ORG, and miscellaneous MISC. The datais separated into a training set, a development settesta, and a test set testb. The training set contains 945 documents, and approximately 203,000 tokens. The development set has 216 documents andapproximately 51,000 tokens, and the test set has231 documents and approximately 46,000 tokens.We evaluate performance on this task in the manner dictated by the competition so that results can beproperly compared. Precision and recall are evaluated on a perentity basis and combined into an F1score. There is no partial credit an incorrect entity4Available at httpcnts.uia.ac.beconll2003ner.boundary is penalized as both a false positive and asa false negative.4.2 The CMU Seminar Announcements TaskThis dataset was developed as part of Dayne Freitags dissertation research Freitag 1998.5 It consists of 485 emails containing seminar announcements at Carnegie Mellon University. It is annotatedfor four fields speaker, location, start time, and endtime. Sutton and McCallum 2004 used 5fold crossvalidation when evaluating on this dataset, so we obtained and used their data splits, so that results canbe properly compared. Because the entire dataset isused for testing, there is no development set. Wealso used their evaluation metric, which is slightlydifferent from the method for CoNLL data. Insteadof evaluating precision and recall on a perentity basis, they are evaluated on a pertoken basis. Then, tocalculate the overall F1 score, the F1 scores for eachclass are averaged.5 Models of Nonlocal StructureOur models of nonlocal structure are themselvesjust sequence models, defining a probability distribution over all possible state sequences. It is possible to flexibly model various forms of constraintsin a way that is sensitive to the linguistic structureof the data e.g., one can go beyond imposing justexact identity conditions. One could imagine manyways of defining such models for simplicity we usethe formPM so ,s,o 5where the product is over a set of violation types ,and for each violation type  we specify a penaltyparameter . The exponent , s,o is the countof the number of times that the violation  occursin the state sequence s with respect to the observation sequence o. This has the effect of assigningsequences with more violations a lower probability. The particular violation types are defined specifically for each task, and are described in the following two sections.This model, as defined above, is not normalized,and clearly it would be expensive to do so. This5Available at httpnlp.shef.ac.ukdot.komresources.html.366PER LOC ORG MISCPER 3141 4 5 0LOC 6436 188 3ORG 2975 0MISC 2030Table 3 Counts of the number of times multiple occurrences ofa token sequence is labeled as different entity types in the samedocument. Taken from the CoNLL training set.PER LOC ORG MISCPER 1941 5 2 3LOC 0 167 6 63ORG 22 328 819 191MISC 14 224 7 365Table 4 Counts of the number of times an entity sequence islabeled differently from an occurrence of a subsequence of itelsewhere in the document. Rows correspond to sequences, andcolumns to subsequences. Taken from the CoNLL training set.doesnt matter, however, because we only use themodel for Gibbs sampling, and so only need to compute the conditional distribution at a single positioni as defined in Equation 1. One inefficient wayto compute this quantity is to enumerate all possible sequences differing only at position i, computethe score assigned to each by the model, and renormalize. Although it seems expensive, this computation can be made very efficient with a straightforward memoization technique at all times we maintain data structures representing the relationship between entity labels and token sequences, from whichwe can quickly compute counts of different types ofviolations.5.1 CoNLL Consistency ModelLabel consistency structure derives from the fact thatwithin a particular document, different occurrencesof a particular token sequence are unlikely to be labeled as different entity types. Although any oneoccurrence may be ambiguous, it is unlikely that allinstances are unclear when taken together.The CoNLL training data empirically supports thestrength of the label consistency constraint. Table 3shows the counts of entity labels for each pair ofidentical token sequences within a document, whereboth are labeled as an entity. Note that inconsistent labelings are very rare.6 In addition, we also6A notable exception is the labeling of the same text as bothorganization and location within the same document. This is aconsequence of the large portion of sports news in the CoNLLwant to model subsequence constraints having seenGeoff Woods earlier in a document as a person isa good indicator that a subsequent occurrence ofWoods should also be labeled as a person. However, if we examine all cases of the labelings ofother occurrences of subsequences of a labeled entity, we find that the consistency constraint does nothold nearly so strictly in this case. As an example, one document contains references to both TheChina Daily, a newspaper, and China, the country.Counts of subsequence labelings within a documentare listed in Table 4. Note that there are many offdiagonal entries the China Daily case is the mostcommon, occurring 328 times in the dataset.The penalties used in the long distance constraintmodel for CoNLL are the Empirical Bayes estimatestaken directly from the data Tables 3 and 4, exceptthat we change counts of 0 to be 1, so that the distribution remains positive. So the estimate of a PERalso being an ORG is 53151  there were 5 instance ofan entity being labeled as both, PER appeared 3150times in the data, and we add 1 to this for smoothing,because PERMISC never occured. However, whenwe have a phrase labeled differently in two different places, continuing with the PERORG example,it is unclear if we should penalize it as PER that isalso an ORG or an ORG that is also a PER. To dealwith this, we multiply the square roots of each estimate together to form the penalty term. The penaltyterm is then multiplied in a number of times equalto the length of the offending entity this is meant toencourage the entity to shrink.7 For example, saywe have a document with three entities, Rotor Volgograd twice, once labeled as PER and once as ORG,and Rotor, labeled as an ORG. The likelihood of aPER also being an ORG is 53151 , and of an ORG alsobeing a PER is 53169 , so the penalty for this violationis 53151 53151 2. The likelihood of a ORG being a subphrase of a PER is 2842 . So the total penaltywould be 53151 53169 2842 .dataset, so that city names are often also team names.7While there is no theoretical justification for this, we foundit to work well in practice.3675.2 CMU Seminar AnnouncementsConsistency ModelDue to the lack of a development set, our consistency model for the CMU Seminar Announcementsis much simpler than the CoNLL model, the numbers where selected due to our intuitions, and we didnot spend much time hand optimizing the model.Specifically, we had three constraints. The first isthat all entities labeled as start time are normalized, and are penalized if they are inconsistent. Thesecond is a corresponding constraint for end times.The last constraint attempts to consistently label thespeakers. If a phrase is labeled as a speaker, we assume that the last word is the speakers last name,and we penalize for each occurrance of that wordwhich is not also labeled speaker. For the start andend times the penalty is multiplied in based on howmany words are in the entity. For the speaker, thepenalty is only multiplied in once. We used a handselected penalty of exp4.0.6 Combining Sequence ModelsIn the previous section we defined two models ofnonlocal structure. Now we would like to incorporate them into the local model in our case, thetrained CRF, and use Gibbs sampling to find themost likely state sequence. Because both the trainedCRF and the nonlocal models are themselves sequence models, we simply combine the two models into a factored sequence model of the followingformPF so  PM soPLso 6where M is the local CRF model, L is the new nonlocal model, and F is the factored model.8 In thisform, the probability again looks difficult to compute because of the normalizing factor, a sum overall hidden state sequences of length N . However,since we are only using the model for Gibbs sampling, we never need to compute the distribution explicitly. Instead, we need only the conditional probability of each position in the sequence, which canbe computed asPF sisi,o  PM sisi,oPLsisi,o. 78This model doublegenerates the state sequence conditioned on the observations. In practice we dont find this tobe a problem.CoNLLApproach LOC ORG MISC PER ALLBM LTRMN     80.09BM GLTRMN     82.30LocalViterbi 88.16 80.83 78.51 90.36 85.51NonLocGibbs 88.51 81.72 80.43 92.29 86.86Table 5 F1 scores of the local CRF and nonlocal models on theCoNLL 2003 named entity recognition dataset. We also providethe results from Bunescu and Mooney 2004 for comparison.CMU Seminar AnnouncementsApproach STIME ETIME SPEAK LOC ALLSM CRF 97.5 97.5 88.3 77.3 90.2SM SkipCRF 96.7 97.2 88.1 80.4 90.6LocalViterbi 96.67 97.36 83.39 89.98 91.85NonLocGibbs 97.11 97.89 84.16 90.00 92.29Table 6 F1 scores of the local CRF and nonlocal models onthe CMU Seminar Announcements dataset. We also providethe results from Sutton and McCallum 2004 for comparison.At inference time, we then sample from the Markovchain defined by this transition probability.7 Results and DiscussionIn our experiments we compare the impact of addingthe nonlocal models with Gibbs sampling to ourbaseline CRF implementation. In the CoNLL namedentity recognition task, the nonlocal models increase the F1 accuracy by about 1.3. Althoughsuch gains may appear modest, note that they areachieved relative to a near stateoftheart NER system the winner of the CoNLL English task reportedan F1 score of 88.76. In contrast, the increases published by Bunescu and Mooney 2004 are relativeto a baseline system which scores only 80.9 onthe same task. Our performance is similar on theCMU Seminar Announcements dataset. We showthe perfield F1 results that were reported by Suttonand McCallum 2004 for comparison, and note thatwe are again achieving gains against a more competitive baseline system.For all experiments involving Gibbs sampling, weused a linear cooling schedule. For the CoNLLdataset we collected 200 samples per trial, and forthe CMU Seminar Announcements we collected 100samples. We report the average of all trials, and in allcases we outperform the baseline with greater than95 confidence, using the standard ttest. The trialshad low standard deviations  0.083 and 0.007 and high minimun Fscores  86.72, and 92.28368 for the CoNLL and CMU Seminar Announcements respectively, demonstrating the stability ofour method.The biggest drawback to our model is the computational cost. Taking 100 samples dramaticallyincreases test time. Averaged over 3 runs on bothViterbi and Gibbs, CoNLL testing time increasedfrom 55 to 1738 seconds, and CMU Seminar Announcements testing time increases from 189 to6436 seconds.8 Related WorkSeveral authors have successfully incorporated alabel consistency constraint into probabilistic sequence model named entity recognition systems.Mikheev et al. 1999 and Finkel et al. 2004 incorporate label consistency information by using adhoc multistage labeling procedures that are effective but specialpurpose. Malouf 2002 and Curranand Clark 2003 condition the label of a token ata particular position on the label of the most recentprevious instance of that same token in a prior sentence of the same document. Note that this violatesthe Markov property, but is achieved by slightly relaxing the requirement of exact inference. Insteadof finding the maximum likelihood sequence overthe entire document, they classify one sentence at atime, allowing them to condition on the maximumlikelihood sequence of previous sentences. This approach is quite effective for enforcing label consistency in many NLP tasks, however, it permits a forward flow of information only, which is not sufficient for all cases of interest. Chieu and Ng 2002propose a solution to this problem for each token, they define additional features taken from otheroccurrences of the same token in the document.This approach has the added advantage of allowingthe training procedure to automatically learn goodweightings for these global features relative to thelocal ones. However, this approach cannot easilybe extended to incorporate other types of nonlocalstructure.The most relevant prior works are Bunescu andMooney 2004, who use a Relational Markov Network RMN Taskar et al., 2002 to explicitly models longdistance dependencies, and Sutton and McCallum 2004, who introduce skipchain CRFs,which maintain the underlying CRF sequence modelwhich Bunescu and Mooney, 2004 lack whileadding skip edges between distant nodes. Unfortunately, in the RMN model, the dependencies mustbe defined in the model structure before doing anyinference, and so the authors use crude heuristicpartofspeech patterns, and then add dependenciesbetween these text spans using clique templates.This generates a extremely large number of overlapping candidate entities, which then necessitatesadditional templates to enforce the constraint thattext subsequences cannot both be different entities,something that is more naturally modeled by a CRF.Another disadvantage of this approach is that it usesloopy belief propagation and a voted perceptron forapproximate learning and inference  illfoundedand inherently unstable algorithms which are notedby the authors to have caused convergence problems. In the skipchain CRFs model, the decisionof which nodes to connect is also made heuristically, and because the authors focus on named entityrecognition, they chose to connect all pairs of identical capitalized words. They also utilize loopy beliefpropagation for approximate learning and inference.While the technique we propose is similar mathematically and in spirit to the above approaches, itdiffers in some important ways. Our model is implemented by adding additional constraints into themodel at inference time, and does not require thepreprocessing step necessary in the two previouslymentioned works. This allows for a broader class oflongdistance dependencies, because we do not needto make any initial assumptions about which nodesshould be connected, and is helpful when you wishto model relationships between nodes which are thesame class, but may not be similar in any other way.For instance, in the CMU Seminar Announcementsdataset, we can normalize all entities labeled as astart time and penalize the model if multiple, nonconsistent times are labeled. This type of constraintcannot be modeled in an RMN or a skipCRF, because it requires the knowledge that both entities aregiven the same class label.We also allow dependencies between multiwordphrases, and not just single words. Additionally,our model can be applied on top of a preexistingtrained sequence model. As such, our method doesnot require complex training procedures, and can369instead leverage all of the established methods fortraining high accuracy sequence models. It can indeed be used in conjunction with any statistical hidden state sequence model HMMs, CMMs, CRFs, oreven heuristic models. Third, our technique employsGibbs sampling for approximate inference, a simpleand probabilistically wellfounded algorithm. As aconsequence of these differences, our approach iseasier to understand, implement, and adapt to newapplications.9 ConclusionsWe have shown that a constraint model can be effectively combined with an existing sequence model ina factored architecture to successfully impose various sorts of long distance constraints. Our modelgeneralizes naturally to other statistical models andother tasks. In particular, it could in the futurebe applied to statistical parsing. Statistical contextfree grammars provide another example of statisticalmodels which are restricted to limiting local structure, and which could benefit from modeling nonlocal structure.AcknowledgementsThis work was supported in part by the AdvancedResearchand Development Activity ARDAsAdvanced Question Answeringfor IntelligenceAQUAINT Program. Additionally, we would liketo that our reviewers for their helpful comments.ReferencesS. Abney. 1997. Stochastic attributevalue grammars. Computational Linguistics, 23597618.C. Andrieu, N. de Freitas, A. Doucet, and M. I. Jordan. 2003.An introduction to MCMC for machine learning. MachineLearning, 50543.A. Borthwick. 1999. A Maximum Entropy Approach to NamedEntity Recognition. Ph.D. thesis, New York University.R. Bunescu and R. J. Mooney. 2004. Collective informationextraction with relational Markov networks. In Proceedingsof the 42nd ACL, pages 439446.H. L. Chieu and H. T. Ng. 2002. Named entity recognitiona maximum entropy approach using global information. InProceedings of the 19th Coling, pages 190196.R. G. Cowell, A. Philip Dawid, S. L. Lauritzen, and D. J.Spiegelhalter. 1999. Probabilistic Networks and Expert Systems. SpringerVerlag, New York.J. R. Curran and S. Clark. 2003. Language independent NERusing a maximum entropy tagger. In Proceedings of the 7thCoNLL, pages 164167.S. Della Pietra, V. Della Pietra, and J. Lafferty. 1997. Inducing features of random fields. IEEE Transactions on PatternAnalysis and Machine Intelligence, 19380393.J. Finkel, S. Dingare, H. Nguyen, M. Nissim, and C. D. Manning. 2004. Exploiting context for biomedical entity recognition from syntax to the web. In Joint Workshop on NaturalLanguage Processing in Biomedicine and Its Applications atColing 2004.D. Freitag and A. McCallum. 1999. Information extractionwith HMMs and shrinkage. In Proceedings of the AAAI99Workshop on Machine Learning for Information Extraction.D. Freitag. 1998. Machine learning for information extractionin informal domains. Ph.D. thesis, Carnegie Mellon University.S. Geman and D. Geman. 1984. Stochastic relaxation, Gibbsdistributions, and the Bayesian restoration of images. IEEETransitions on Pattern Analysis and Machine Intelligence,6721741.M. Kim, Y. S. Han, and K. Choi. 1995. Collocation mapfor overcoming data sparseness. In Proceedings of the 7thEACL, pages 5359.S. Kirkpatrick, C. D. Gelatt, and M. P. Vecchi. 1983. Optimization by simulated annealing. Science, 220671680.P. J. Van Laarhoven and E. H. L. Arts. 1987. Simulated Annealing Theory and Applications. Reidel Publishers.J. Lafferty, A. McCallum, and F. Pereira. 2001. ConditionalRandom Fields Probabilistic models for segmenting andlabeling sequence data. In Proceedings of the 18th ICML,pages 282289. Morgan Kaufmann, San Francisco, CA.T. R. Leek. 1997. Information extraction using hidden Markovmodels. Masters thesis, U.C. San Diego.R. Malouf. 2002. Markov models for languageindependentnamed entity recognition. In Proceedings of the 6th CoNLL,pages 187190.A. Mikheev, M. Moens, and C. Grover. 1999. Named entityrecognition without gazetteers. In Proceedings of the 9thEACL, pages 18.L. R. Rabiner. 1989. A tutorial on Hidden Markov Models andselected applications in speech recognition. Proceedings ofthe IEEE, 772257286.C. Sutton and A. McCallum. 2004. Collective segmentationand labeling of distant entities in information extraction. InICML Workshop on Statistical Relational Learning and Itsconnections to Other Fields.B. Taskar, P. Abbeel, and D. Koller. 2002. Discriminativeprobabilistic models for relational data. In Proceedings ofthe 18th Conference on Uncertianty in Artificial IntelligenceUAI02, pages 485494, Edmonton, Canada.370
