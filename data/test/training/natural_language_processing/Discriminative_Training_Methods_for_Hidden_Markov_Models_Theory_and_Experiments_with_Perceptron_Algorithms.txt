Discriminative Training Methods for Hidden Markov ModelsTheory and Experiments with Perceptron AlgorithmsMichael CollinsATT LabsResearch, Florham Park, New Jersey.mcollinsresearch.att.comAbstractWe describe new algorithms for training tagging models, as an alternativeto maximumentropy models or conditional random elds CRFs. The algorithms rely on Viterbi decoding oftraining examples, combined with simple additive updates. We describe theory justifying the algorithms througha modication of the proof of convergence of the perceptron algorithm forclassication problems. We give experimental results on partofspeech tagging and base noun phrase chunking, inboth cases showing improvements overresults for a maximumentropy tagger.1 IntroductionMaximumentropy ME models are justiablya very popular choice for tagging problems inNatural Language Processing for example seeRatnaparkhi 96 for their use on partofspeechtagging, and McCallum et al. 2000 for theiruse on a FAQ segmentation task. ME modelshave the advantage of being quite exible in thefeatures that can be incorporated in the model.However, recent theoretical and experimental results in Laerty et al. 2001 have highlightedproblems with the parameter estimation methodfor ME models. In response to these problems,they describe alternative parameter estimationmethods based on Conditional Markov RandomFields CRFs. Laerty et al. 2001 give experimental results suggesting that CRFs can perform signicantly better than ME models.In this paper we describe parameter estimation algorithms which are natural alternatives toCRFs. The algorithms are based on the perceptron algorithm Rosenblatt 58, and the votedor averaged versions of the perceptron describedin Freund  Schapire 99. These algorithmshave been shown by Freund  Schapire 99 tobe competitive with modern learning algorithmssuch as support vector machines however, theyhave previously been applied mainly to classication tasks, and it is not entirely clear how thealgorithms can be carried across to NLP taskssuch as tagging or parsing.This paper describes variants of the perceptron algorithm for tagging problems. The algorithms rely on Viterbi decoding of trainingexamples, combined with simple additive updates. We describe theory justifying the algorithm through a modication of the proof of convergence of the perceptron algorithm for classication problems. We give experimental resultson partofspeech tagging and base noun phrasechunking, in both cases showing improvementsover results for a maximumentropy tagger a11.9 relative reduction in error for POS tagging, a 5.1 relative reduction in error for NPchunking. Although we concentrate on taggingproblems in this paper, the theoretical framework and algorithm described in section 3 ofthis paper should be applicable to a wide variety of models where Viterbistyle algorithmscan be used for decoding examples are Probabilistic ContextFree Grammars, or ME modelsfor parsing. See Collins and Duy 2001 Collinsand Duy 2002 Collins 2002 for other applications of the voted perceptron to NLP problems.12 Parameter Estimation2.1 HMM TaggersIn this section, as a motivating example, we describe a special case of the algorithm in thispaper the algorithm applied to a trigram tagger. In a trigram HMM tagger, each trigram1The theorems in section 3, and the proofs in section 5, apply directly to the work in these other papers.                                            Association for Computational Linguistics.                        Language Processing EMNLP, Philadelphia, July 2002, pp. 18.                         Proceedings of the Conference on Empirical Methods in Naturalof tags and each tagword pair have associatedparameters. We write the parameter associatedwith a trigram hx y zi as xyz, and the parameter associated with a tagword pair t w astw. A common approach is to take the parameters to be estimates of conditional probabilitiesxyz  logP z j x y, tw  logP w j t.For convenience we will use w1n as shorthand for a sequence of words w1 w2    wn,and t1n as shorthand for a taq sequencet1 t2    tn. In a trigram tagger the score fora tagged sequence t1n paired with a word sequence w1n is2 Pni1 ti2ti1ti Pni1 tiwi .When the parameters are conditional probabilities as above this score is an estimate of thelog of the joint probability P w1n t1n. TheViterbi algorithm can be used to nd the highestscoring tagged sequence under this score.As an alternative to maximumlikelihood parameter estimates, this paper will propose thefollowing estimation algorithm. Say the training set consists of n tagged sentences, the ithsentence being of length ni. We will write theseexamples as wi1ni ti1ni for i  1    n. Thenthe training algorithm is as follows Choose a parameter T dening the numberof iterations over the training set.3 Initially set all parameters xyz and twto be zero. For t  1    T i  1    n Use the Viterbialgorithm to nd the best tagged sequence forsentence wi1ni under the current parametersettings we call this tagged sequence z1ni.For every tag trigram hx y zi seen c1 times inti1ni and c2 times in z1ni where c1 6 c2 setxyz  xyz  c1  c2. For every tagwordpair ht wi seen c1 times in wi1ni ti1ni andc2 times in wi1ni z1ni where c1 6 c2 settw  tw  c1  c2.As an example, say the ith tagged sentencewi1ni ti1ni in training data istheD manN sawV theD dogNand under the current parameter settings thehighest scoring tag sequence wi1ni z1ni is2We take t1 and t2 to be special NULL tag symbols.3T is usually chosen by tuning on a development set.theD manN sawN theD dogNThen the parameter update will add 1 to theparameters DNV , NVD, VDN , Vsaw andsubtract 1 from the parameters DNN , NND,NDN , Nsaw. Intuitively this has the effect of increasing the parameter values for features which were missing from the proposedsequence z1ni, and downweighting parametervalues for incorrect features in the sequencez1ni. Note that if z1ni  ti1ni i.e., theproposed tag sequence is correct  no changesare made to the parameter values.2.2 Local and Global Feature VectorsWe now describe how to generalize the algorithmto more general representations of tagged sequences. In this section we describe the featurevector representations which are commonly usedin maximumentropy models for tagging, andwhich are also used in this paper.In maximumentropy taggers Ratnaparkhi96 McCallum et al. 2000, the tagging problem is decomposed into sequence of decisions intagging the problem in lefttoright fashion. Ateach point there is a history  the context inwhich a tagging decision is made  and the taskis to predict the tag given the history. Formally,a history is a 4tuple ht1 t2 w1n ii wheret1 t2 are the previous two tags, w1n is an array specifying the n words in the input sentence,and i is the index of the word being tagged. Weuse H to denote the set of all possible histories.Maximumentropy models represent the tagging task through a featurevector representationof historytag pairs. A feature vector representation   HT  Rd is a function  that maps ahistorytag pair to a ddimensional feature vector. Each component sh t for s  1    dcould be an arbitrary function of h t. It iscommon e.g., see Ratnaparkhi 96 for eachfeature s to be an indicator function. For example, one such feature might be1000h t 81 if current word wi is theand t  DT0 otherwiseSimilar features might be dened for everywordtag pair seen in training data. Anotherfeature type might track trigrams of tags, for example 1001h t  1 if ht2 t1 ti  hD, N, Viand 0 otherwise. Similar features would be dened for all trigrams of tags seen in training. Areal advantage of these models comes from thefreedom in dening these features for example,Ratnaparkhi 96 McCallum et al. 2000 bothdescribe feature sets which would be dicult toincorporate in a generative model.In addition to feature vector representationsof historytag pairs, we will nd it convenientto dene feature vectors of w1n t1n pairswhere w1n is a sequence of n words, and t1nis an entire tag sequence. We use  to denote a function from w1n t1n pairs to ddimensional feature vectors. We will often referto  as a global representation, in contrastto  as a local representation. The particularglobal representations considered in this paperare simple functions of local representationssw1n t1n nXi1shi ti 1where hi  hti1 ti2 w1n ii. Each globalfeature sw1n t1n is simply the value forthe local representation s summed over all historytag pairs in w1n t1n. If the local features are indicator functions, then the global features will typically be counts. For example,with 1000 dened as above, 1000w1n t1nis the number of times the is seen tagged as DTin the pair of sequences w1n t1n.2.3 MaximumEntropy TaggersIn maximumentropy taggers the feature vectors together with a parameter vector  2 Rd areused to dene a conditional probability distribution over tags given a history asP t j h  ePssshtZh where Zh  Pl2T ePssshl. The log ofthis probability has the form log pt j h  Pds1 ssh t logZh , and hence the logprobability for a w1n t1n pair will beXidXs1sshi tiXilogZhi  2where hi  hti1 ti2 w1n ii. Given parameter values , and an input sentence w1n, thehighest probability tagged sequence under theformula in Eq. 2 can be found eciently usingthe Viterbi algorithm.The parameter vector  is estimated from atraining set of sentencetaggedsequence pairs.Maximumlikelihood parameter values can beestimated using Generalized Iterative ScalingRatnaparkhi 96, or gradient descent methods.In some cases it may be preferable to apply abayesian approach which includes a prior overparameter values.2.4 A New Estimation MethodWe now describe an alternative method for estimating parameters of the model. Given a sequence of words w1n and a sequence of part ofspeech tags, t1n, we will take the score of atagged sequence to benXi1dXs1sshi ti dXs1ssw1n t1n where hi is again hti1 ti2 w1n ii. Note thatthis is almost identical to Eq. 2, but without thelocal normalization terms logZhi . Underthis method for assigning scores to tagged sequences, the highest scoring sequence of tags foran input sentence can be found using the Viterbialgorithm. We can use an almost identical decoding algorithm to that for maximumentropytaggers, the dierence being that local normalization terms do not need to be calculated.We then propose the training algorithm in gure 1. The algorithm takes T passes over thetraining sample. All parameters are initially setto be zero. Each sentence in turn is decoded using the current parameter settings. If the highest scoring sequence under the current model isnot correct, the parameters s are updated in asimple additive fashion.Note that if the local features s are indicator functions, then the global features s will becounts. In this case the update will add cs  dsto each parameter s, where cs is the numberof times the sth feature occurred in the correct tag sequence, and ds is the number of timesInputs A training set of tagged sentences,wi1ni ti1ni for i  1    n. A parameter Tspecifying number of iterations over the training set. Alocal representation  which is a function that mapshistorytag pairs to ddimensional feature vectors. Theglobal representation  is dened through  as in Eq. 1.Initialization Set parameter vector   0.AlgorithmFor t  1    T i  1    n Use the Viterbi algorithm to nd the output of themodel on the ith training sentence with the current parameter settings, i.e.,z1ni  argmaxu1ni2TniPssswi1ni u1niwhere T ni is the set of all tag sequences of length ni. If z1ni 6 ti1nithen update the parameterss  s swi1ni ti1ni swi1ni z1niOutput Parameter vector .Figure 1 The training algorithm for tagging.it occurs in highest scoring sequence under thecurrent model. For example, if the features sare indicator functions tracking all trigrams andwordtag pairs, then the training algorithm isidentical to that given in section 2.1.2.5 Averaging ParametersThere is a simple renement to the algorithmin gure 1, called the averaged parametersmethod. Dene tis to be the value for the sthparameter after the ith training example hasbeen processed in pass t over the training data.Then the averaged parameters are dened ass Pt1Ti1n tis nT for all s  1    d.It is simple to modify the algorithm to storethis additional set of parameters. Experimentsin section 4 show that the averaged parametersperform signicantly better than the nal parameters Tns . The theory in the next sectiongives justication for the averaging method.3 Theory Justifying the AlgorithmIn this section we give a general algorithm forproblems such as tagging and parsing, and givetheorems justifying the algorithm. We also showhow the tagging algorithm in gure 1 is a special case of this algorithm. Convergence theorems for the perceptron applied to classicationproblems appear in Freund  Schapire 99 the results in this section, and the proofs in section 5, show how the classication results can beInputs Training examples xi yiInitialization Set   0AlgorithmFor t  1    T , i  1    nCalculate zi  argmaxz2GENxi xi z  Ifzi 6 yi then   xi yi xi ziOutput Parameters Figure 2 A variant of the perceptron algorithm.carried over to problems such as tagging.The task is to learn a mapping from inputsx 2 X to outputs y 2 Y. For example, X mightbe a set of sentences, with Y being a set of possible tag sequences. We assume Training examples xi yi for i  1    n. A functionGEN which enumerates a set ofcandidates GENx for an input x. A representation mapping each x y 2X  Y to a feature vector x y 2 Rd. A parameter vector  2 Rd.The componentsGEN and  dene a mapping from an input x to an output F x throughF x  arg maxy2GENxx y  where x y   is the inner productPs ssx y. The learning task is to set theparameter values  using the training examplesas evidence.The tagging problem in section 2 can bemapped to this setting as follows The training examples are sentencetaggedsequence pairs xi  wi1niand yi  ti1nifor i  1    n. Given a set of possible tags T , we deneGENw1n  Tn, i.e., the function GENmaps an input sentence w1n to the set ofall tag sequences of length n. The representation x y w1n t1n is dened through localfeature vectors h t where h t is ahistorytag pair. See Eq. 1.Figure 2 shows an algorithm for setting theweights . It can be veried that the trainingalgorithm for taggers in gure 1 is a special caseof this algorithm, if we dene xi yiGEN and as just described.We will now give a rst theorem regardingthe convergence of this algorithm. This theoremtherefore also describes conditions under whichthe algorithm in gure 1 converges. First, weneed the following denitionDenition 1 Let GENxi  GENxi  fyig. Inother words GENxi is the set of incorrect candidatesfor an example xi. We will say that a training sequencexi yi for i  1    n is separable with margin   0if there exists some vector U with jjUjj  1 such that8i 8z 2 GENxi U xi yiU xi z   3jjUjj is the 2norm of U, i.e., jjUjj pPsU2s.We can then state the following theorem seesection 5 for a proofTheorem 1 For any training sequence xi yi which isseparable with margin , then for the perceptron algorithmin gure 2Number of mistakes R22where R is a constant such that 8i 8z 2GENxi jjxi yi xi zjj  R.This theorem implies that if there is a parameter vector U which makes zero errors on thetraining set, then after a nite number of iterations the training algorithm will have convergedto parameter values with zero training error. Acrucial point is that the number of mistakes is independent of the number of candidates for eachexample i.e. the size of GENxi for each i,depending only on the separation of the trainingdata, where separation is dened above. Thisis important because in many NLP problemsGENx can be exponential in the size of theinputs. All of the convergence and generalization results in this paper depend on notions ofseparability rather than the size of GEN.Two questions come to mind. First, are thereguarantees for the algorithm if the training datais not separable Second, performance on atraining sample is all very well, but what doesthis guarantee about how well the algorithmgeneralizes to newly drawn test examples Freund  Schapire 99 discuss how the theory canbe extended to deal with both of these questions.The next sections describe how these results canbe applied to the algorithms in this paper.3.1 Theory for inseparable dataIn this section we give bounds which apply whenthe data is not separable. First, we need thefollowing denitionDenition 2 Given a sequence xi yi, for a U,  pairdene mi  U xi yimaxz2GENxiU xi z andi  maxf0  mig. Finally, dene DU pPni12i .The value DU is a measure of how close Uis to separating the training data with margin .DU is 0 if the vector U separates the data withat least margin . If U separates almost all ofthe examples with margin , but a few examplesare incorrectly tagged or have margin less than, then DU will take a relatively small value.The following theorem then applies see section 5 for a proofTheorem 2 For any training sequence xi yi, for therst pass over the training set of the perceptron algorithmin gure 2,Number of mistakes  minURDU22where R is a constant such that 8i 8z 2GENxi jjxi yi xi zjj  R, and themin is taken over   0, jjUjj  1.This theorem implies that if the training datais close to being separable with margin  i.e., there exists some U such that DU is relatively small  then the algorithm will again makea small number of mistakes. Thus theorem 2shows that the perceptron algorithm can be robust to some training data examples being difcult or impossible to tag correctly.3.2 Generalization resultsTheorems 1 and 2 give results bounding thenumber of errors on training samples, but thequestion we are really interested in concernsguarantees of how well the method generalizesto new test examples. Fortunately, there areseveral theoretical results suggesting that if theperceptron algorithm makes a relatively smallnumber of mistakes on a training sample then itis likely to generalize well to new examples. Thissection describes some of these results, whichoriginally appeared in Freund  Schapire 99,and are derived directly from results in Helmbold and Warmuth 95.First we dene a modication of the perceptron algorithm, the voted perceptron. We canconsider the rst pass of the perceptron algorithm to build a sequence of parameter settings 1i for i  1    n. For a given test example x, each of these will dene an outputvi  argmaxz2GENx 1i  x z. The votedperceptron takes the most frequently occurringoutput in the set fv1    vng. Thus the votedperceptron is a method where each of the parameter settings 1i for i  1    n get a single vote for the output, and the majority wins.The averaged algorithm in section 2.5 can beconsidered to be an approximation of the votedmethod, with the advantage that a single decoding with the averaged parameters can be performed, rather than n decodings with each ofthe n parameter settings.In analyzing the voted perceptron the one assumption we will make is that there is someunknown distribution P x y over the set X Y, and that both training and test examplesare drawn independently, identically distributedi.i.d. from this distribution. Corollary 1 ofFreund  Schapire 99 then statesTheorem 3 Freund  Schapire 99 Assume all examples are generated i.i.d. at random. Lethx1 y1i    xn yni be a sequence of training examplesand let xn1 yn1 be a test example. Then the probability over the choice of all n  1 examples that thevotedperceptron algorithm does not predict yn1 on input xn1 is at most2n 1En1minURDU22where En1 is an expected value taken over n  1 examples, R and DU are as dened above, and the min istaken over   0, jjUjj  1.4 Experiments4.1 Data SetsWe ran experiments on two data sets partofspeech tagging on the Penn Wall Street Journaltreebank Marcus et al. 93, and base nounphrase recognition on the data sets originally introduced by Ramshaw and Marcus 95. In eachcase we had a training, development and test set.For partofspeech tagging the training set wassections 018 of the treebank, the developmentset was sections 1921 and the nal test set wassections 2224. In NP chunking the training setCurrent word wi  tiPrevious word wi1  tiWord two back wi2  tiNext word wi1  tiWord two ahead wi2  tiBigram features wi2 wi1  tiwi1 wi  tiwi wi1  tiwi1 wi2  tiCurrent tag pi  tiPrevious tag pi1  tiTag two back pi2  tiNext tag pi1  tiTag two ahead pi2  tiBigram tag features pi2 pi1  tipi1 pi  tipi pi1  tipi1 pi2  tiTrigram tag features pi2 pi1 pi  tipi1 pi pi1  tipi pi1 pi2  tiFigure 3 Feature templates used in the NP chunkingexperiments. wi is the current word, and w1    wn is theentire sentence. pi is POS tag for the current word, andp1    pn is the POS sequence for the sentence. ti is thechunking tag assigned to the ith word.was taken from section 1518, the developmentset was section 21, and the test set was section20. For POS tagging we report the percentageof correct tags on a test set. For chunking wereport Fmeasure in recovering bracketings corresponding to base NP chunks.4.2 FeaturesFor POS tagging we used identical features tothose of Ratnaparkhi 96, the only dierencebeing that we did not make the rare word distinction in table 1 of Ratnaparkhi 96 i.e.,spelling features were included for all words intraining data, and the word itself was used as afeature regardless of whether the word was rare.The feature set takes into account the previoustag and previous pairs of tags in the history, aswell as the word being tagged, spelling featuresof the words being tagged, and various featuresof the words surrounding the word being tagged.In the chunking experiments the input sentences included words as well as partsofspeechfor those words from the tagger in Brill 95. Table 3 shows the features used in the experiments.The chunking problem is represented as a threetag task, where the tags are B, I, O for wordsbeginning a chunk, continuing a chunk, and being outside a chunk respectively. All chunks begin with a B symbol, regardless of whether theprevious word is tagged O or I.NP Chunking ResultsMethod FMeasure NumitsPerc, avg, cc0 93.53 13Perc, noavg, cc0 93.04 35Perc, avg, cc5 93.33 9Perc, noavg, cc5 91.88 39ME, cc0 92.34 900ME, cc5 92.65 200POS Tagging ResultsMethod Error rate NumitsPerc, avg, cc0 2.93 10Perc, noavg, cc0 3.68 20Perc, avg, cc5 3.03 6Perc, noavg, cc5 4.04 17ME, cc0 3.4 100ME, cc5 3.28 200Figure 4 Results for various methods on the partofspeech tagging and chunking tasks on development data.All scores are error percentages. Numits is the numberof training iterations at which the best score is achieved.Perc is the perceptron algorithm, ME is the maximumentropy method. Avgnoavg is the perceptron with orwithout averaged parameter vectors. cc5 means onlyfeatures occurring 5 times or more in training are included, cc0 means all features in training are included.4.3 ResultsWe applied both maximumentropy models andthe perceptron algorithm to the two taggingproblems. We tested several variants for eachalgorithm on the development set, to gain someunderstanding of how the algorithms performance varied with various parameter settings,and to allow optimization of free parameters sothat the comparison on the nal test set is a fairone. For both methods, we tried the algorithmswith feature count cutos set at 0 and 5 i.e.,we ran experiments with all features in trainingdata included, or with all features occurring 5times or more included  Ratnaparkhi 96 usesa count cuto of 5. In the perceptron algorithm, the number of iterations T over the training set was varied, and the method was testedwith both averaged and unaveraged parametervectors i.e., with Tns and Tns , as dened insection 2.5, for a variety of values for T . Inthe maximum entropy model the number of iterations of training using Generalized IterativeScaling was varied.Figure 4 shows results on development dataon the two tasks. The trends are fairly clearaveraging improves results signicantly for theperceptron method, as does including all features rather than imposing a count cuto of 5.In contrast, the ME models performance suerswhen all features are included. The best perceptron conguration gives improvements over themaximumentropy models in both cases an improvement in Fmeasure from 9265 to 9353in chunking, and a reduction from 328 to293 error rate in POS tagging. In lookingat the results for dierent numbers of iterationson development data we found that averagingnot only improves the best result, but also givesmuch greater stability of the tagger the nonaveraged variant has much greater variance inits scores.As a nal test, the perceptron and ME taggers were applied to the test sets, with the optimal parameter settings on development data.On POS tagging the perceptron algorithm gave2.89 error compared to 3.28 error for themaximumentropy model a 11.9 relative reduction in error. In NP chunking the perceptron algorithm achieves an Fmeasure of 93.63,in contrast to an Fmeasure of 93.29 for theME model a 5.1 relative reduction in error.5 Proofs of the TheoremsThis section gives proofs of theorems 1 and 2.The proofs are adapted from proofs for the classication case in Freund  Schapire 99.Proof of Theorem 1 Let k be the weightsbefore the kth mistake is made. It follows that1  0. Suppose the kth mistake is made atthe ith example. Take z to the output proposedat this example, z  argmaxy2GENxi xi y k. It follows from the algorithm updates thatk1  k xi yixi z. We take innerproducts of both sides with the vector UU  k1  U  k U  xi yiU  xi z U  k  where the inequality follows because of the property of U assumed in Eq. 3. Because 1  0,and therefore U  1  0, it follows by induction on k that for all k, U  k1  k. Because U  k1  jjUjj jjk1jj, it follows thatjjk1jj  k.We also derive an upper bound for jjk1jj2jjk1jj2  jjkjj2  jjxi yi xi zjj22k  xi yi xi z jjkjj2 R2where the inequality follows becausejjxi yi xi zjj2  R2 by assumption, and k  xi yi xi z  0 becausez is the highest scoring candidate for xi underthe parameters k. It follows by induction thatjjk1jj2  kR2.Combining the bounds jjk1jj  k andjjk1jj2  kR2 gives the result for all k thatk22  jjk1jj2  kR2  k  R22Proof of Theorem 2 We transform the representation x y 2 Rd to a new representationx y 2 Rdn as follows. For i  1    d dene ix y  ix y. For j  1    n denedjx y   if x y  xj  yj, 0 otherwise,where  is a parameter which is greater than 0.Similary, say we are given a U  pair, and corresponding values for i as dened above. Wedene a modied parameter vector U 2 Rdnwith Ui  Ui for i  1    d and Udj  jfor j  1    n. Under these denitions it can beveried that8i 8z 2 GENxi U  xi yi U  xi z  8i 8z 2 GENxi jjxi yi xi zjj2  R2 2jjUjj2  jjUjj2 Pi2i 2  1 D2U2It can be seen that the vector UjjUjj separatesthe data with margin q1 D2U2. By theorem 1, this means that the rst pass of the perceptron algorithm with representation  makesat most kmax 12R2 21 D2U2  mistakes. But the rst pass of the original algorithm with representation  is identical to therst pass of the algorithm with representation, because the parameter weights for the additional features dj for j  1    n each aect asingle example of training data, and do not aectthe classication of test data examples. Thusthe original perceptron algorithm also makes atmost kmax mistakes on its rst pass over thetraining data. Finally, we can minimize kmaxwith respect to , giving  pRDU, andkmaxpRDU  R2D2U2, implying thebound in the theorem.6 ConclusionsWe have described new algorithms for tagging,whose performance guarantees depend on a notion of separability of training data examples. The generic algorithm in gure 2, andthe theorems describing its convergence properties, could be applied to several other modelsin the NLP literature. For example, a weightedcontextfree grammar can also be conceptualized as a way of dening GEN,  and , so theweights for generative models such as PCFGscould be trained using this method.AcknowledgementsThanks to Nigel Duy, Rob Schapire and YoramSinger for many useful discussions regardingthe algorithms in this paper, and to FernandoPereira for pointers to the NP chunking dataset, and for suggestions regarding the featuresused in the experiments.ReferencesBrill, E. 1995. TransformationBased ErrorDrivenLearning and Natural Language Processing A CaseStudy in Part of Speech Tagging. Computational Linguistics.Collins, M., and Duy, N. 2001. Convolution Kernelsfor Natural Language. In Proceedings of Neural Information Processing Systems NIPS 14.Collins, M., and Duy, N. 2002. New Ranking Algorithms for Parsing and Tagging Kernels over DiscreteStructures, and the Voted Perceptron. In Proceedingsof ACL 2002.Collins, M. 2002. Ranking Algorithms for NamedEntity Extraction Boosting and the Voted Perceptron. In Proceedings of ACL 2002.Freund, Y.  Schapire, R. 1999. Large Margin Classication using the Perceptron Algorithm. In MachineLearning, 373277296.Helmbold, D., and Warmuth, M. On weak learning. Journal of Computer and System Sciences, 503551573,June 1995.Laerty, J., McCallum, A., and Pereira, F. 2001. Conditional random elds Probabilistic models for segmenting and labeling sequence data. In Proceedings ofICML 2001.McCallum, A., Freitag, D., and Pereira, F. 2000 Maximum entropy markov models for information extraction and segmentation. In Proceedings of ICML 2000.Marcus, M., Santorini, B.,  Marcinkiewicz, M. 1993.Building a large annotated corpus of english ThePenn treebank. Computational Linguistics, 19.Ramshaw, L., and Marcus, M. P. 1995. Text ChunkingUsing TransformationBased Learning. In Proceedingsof the Third ACL Workshop on Very Large Corpora,Association for Computational Linguistics, 1995.Ratnaparkhi, A. 1996. A maximum entropy partofspeech tagger. In Proceedings of the empirical methodsin natural language processing conference.Rosenblatt, F. 1958. The Perceptron A ProbabilisticModel for Information Storage and Organization in theBrain. Psychological Review, 65, 386408. Reprintedin Neurocomputing MIT Press, 1998.
