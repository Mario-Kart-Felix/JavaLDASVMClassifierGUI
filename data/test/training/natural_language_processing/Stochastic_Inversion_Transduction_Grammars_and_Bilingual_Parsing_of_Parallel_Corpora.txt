Stochastic Inversion Transduction Grammars and Bilingual Parsing of Parallel Corpora D e k a i  Wu Hong Kong University of Science and Technology We introduce 1 a novel stochastic inversion transduction grammar formalism for bilingual language modeling of sentencepairs, and 2 the concept of bilingual parsing with a variety of parallel corpus analysis applications. Aside from the bilingual orientation, three major features distinguish the formalism from the finitestate transducers more traditionally found in compu tational linguistics it skips directly to a contextfree rather than finitestate base, it permits a minimal extra degree of ordering flexibility, and its probabilistic formulation admits an efficient maximumlikelihood bilingual parsing algorithm. A convenient normal form is shown to exist. Analysis of the formalisms expressiveness suggests that it is particularly well suited to modeling ordering shifts between languages, balancing needed flexibility against complexity constraints. We discuss a number of examples of how stochastic inversion transduction grammars bring bilin gual constraints to bear upon problematic corpus analysis tasks such as segmentation, bracketing, phrasal alignment, and parsing. 1. Introduction We introduce a general formalism for modeling of bilingual sentence pairs, known as an inversion transduction grammar, with potential application in a variety of corpus analysis areas. Transduction grammar models, especially of the finitestate family, have long been known. However, the imposition of identical ordering constraints upon both streams severely restricts their applicability, and thus transduction grammars have re ceived relatively little attention in languagemodeling research. The inversion trans duction grammar formalism skips directly to a contextfree, rather than finitestate, base and permits one extra degree of ordering flexibility, while retaining properties necessary for efficient computation, thereby sidestepping the limitations of traditional transduction grammars. In tandem with the concept of bilingual languagemodeling, we propose the con cept of bilingual parsing, where the input is a sentencepair rather than a sentence. Though inversion transduction grammars remain inadequate as fullfledged transla tion models, bilingual parsing with simple inversion transduction grammars turns out to be very useful for parallel corpus analysis when the true grammar is not fully known. Parallel bilingual corpora have been shown to provide a rich source of con straints for statistical analysis Brown et al. 1990 Gale and Church 1991 Gale, Church, and Yarowsky 1992 Church 1993 Brown et al. 1993 Dagan, Church, and Gale 1993 Department of Computer Science, University of Science and Technology, Clear Water Bay, Hong Kong. Email dekaics.ust.hk  1997 Association for Computational Linguistics Computational Linguistics Volume 23, Number 3 Fung and Church 1994 Wu and Xia 1994 Fung and McKeown 1994. The primary purpose of bilingual parsing with inversion transduction grammars is not to flag un grammatical inputs rather, the aim is to extract structure from the input data, which is assumed to be grammatical, in keeping with the spirit of robust parsing. The for malisms uniform integration of various types of bracketing and alignment constraints is one of its chief strengths. The paper is divided into two main parts. We begin in the first part below by laying out the basic formalism, then show that reduction to a normal form is possible. We then raise several desiderata for the expressiveness of any bilingual language modeling formalism in terms of its constituentmatching flexibility and discuss how the characteristics of the inversion transduction formalism are particularly suited to address these criteria. Afterwards we introduce a stochastic version and give an al gorithm for finding the optimal bilingual parse of a sentencepair. The formalism is independent of the languages we give examples and applications using Chinese and English because languages from different families provide a more rigorous testing ground. In the second part, we survey a number of sample applications and exten sions of bilingual parsing for segmentation, bracketing, phrasal alignment, and other parsing tasks. 2. Inversion Transduction Grammars A transduction grammar describes a structurally correlated pair of languages. For our purposes, the generative view is most convenient the grammar generates trans ductions, so that two output streams are simultaneously generated, one for each lan guage. This contrasts with the common inputoutput view popularized by both syntax directed transduction grammars and finitestate transducers. The generative view is more appropriate for our applications because the roles of the two languages are sym metrical, in contrast to the usual applications of syntaxdirected transduction gram mars. Moreover, the inputoutput view works better when a machine for accepting one of the languages the input language has a high degree of determinism, which is not the case here. Our transduction model is contextfree, rather than finitestate. Finitestate trans ducers, or FSTs, are well known to be useful for specific tasks such as analysis of inflectional morphology Koskenniemi 1983, texttospeech conversion Kaplan and Kay 1994, and nominal, number, and temporal phrase normalization Gazdar and Mellish 1989. FSTs may also be used to parse restricted classes of contextfree gram mars Pereira 1991 Roche 1994 Laporte 1996. However, the bilingual corpus analysis tasks we consider in this paper are quite different from the tasks for which FSTs are apparently well suited. Our domain is broader, and the model possesses very little a priori specific structural knowledge of the language. As a stepping stone to inversion transduction grammars, we first consider what a contextfree model known as a simple transduction grammar Lewis and Stearns 1968 would look like. Simple transduction grammars as well as inversion transduc tion grammars are restricted cases of the general class of contextfree syntaxdirected transduction grammars Aho and Ullman 1969a, 1969b, 1972 however, we will avoid the term syntaxdirected here, so as to deemphasize the inputoutput connotation as discussed above. A simple transduction grammar can be written by marking every terminal symbol for a particular output stream. Thus, each rewrite rule emits not one but two streams. For example, a rewrite rule of the form A  Bxly2Czl  means that the terminal symbols x and z are symbols of. the language L1 emitted on stream 1, while y is a symbol of 378 Wu Bilingual Parsing a S SP PP NP NN VP VV Det Prep Pro N A Conj Aux Cop Stop SP Stop NP VP I NP VV I NP V  Prep NP  Det NN I Det N Pro I NP Conj NP A N I INN PP Aux VP I Aux VV I VV PP . V NP I Cop A the  t o     I    l you  a u t h o r i t y    I secretary a c c o u n t a b l e    I f i n a n c i a l     ,  andl  w i l l    bec   O b VP  VV PP Figure 1 A simple transduction grammar a and an invertedorientation production b. the language L2 emitted on stream 2. It follows that every nonterminal stands for a class of derivable substring pairs. We can use a simple transduction grammar to model the generation of bilingual sentence pairs. As a mnemonic convention, we usually use the alternative notation A . B xy C zc to associate matching output tokens. Though this additional informa tion has no formal generative effect, it reminds us that xy must be a valid entry in the translation lexicon. We call a matched terminal symbol pair such as xy a couple. The null symbol  means that no output token is generated. We call x an Llsingleton, and y an L2singleton. Consider the simple transduction grammar fragment shown in Figure la. It will become apparent below why we explicitly include brackets around righthand sides containing nonterminals, which are usually omitted with standard CFGs. The simple transduction grammar can generate, for instance, the following pair of English and Chinese sentences in translation 1 a. The Financial SecretaryNN NP and INp NP will be accountablew vP sP .s b.        NN NP  NP NP       V V  lVP lSP o S Notice that each nonterminal derives two substrings, one in each language. The two substrings are counterparts of each other. In fact, it is natural to write the parse trees together 2 Thec FinanciallqCJ SecretaryJlNN NP andl IJNp NP wil l  bec accountabletvv vP IsP .o s Of course, in general, simple transduction grammars are not very useful, precisely 379 Computational Linguistics Volume 23, Number 3 because they require the two languages to share exactly the same grammatical structure modulo those distinctions that can be handled with lexical singletons. For example, the following sentence pair from our corpus cannot be generated 3 a. The Authority will be accountable to the Financial Secretary. b. t a   ,q   .  o Authority will to Financial Secretary accountable. To make transduction grammars truly useful for bilingual tasks, we must escape the rigid parallel ordering constraint of simple transduction grammars. At the same time, any relaxation of constraints must be traded off against increases in the com putational complexity of parsing, which may easily become exponential. The key is to make the relaxation relatively modest but still handle a wide range of ordering variations. The inversion transduction grammar ITG formalism only minimally extends the generative power of a simple transduction grammar, yet turns out to be surprisingly effective. 1 Like simple transduction grammars, ITGs remain a subset of contextfree syntaxdirected transduction grammars Lewis and Steams 1968 but this view is too general to be of much help. 2 The productions of an inversion transduction grammar are interpreted just as in a simple transduction grammar, except that two possible orientations are allowed. Pure simple transduction grammars have the implicit char acteristic that for both output streams, the symbols generated by the righthandside constituents of a production are concatenated in the same lefttoright order. Inversion transduction grammars also allow such productions, which are said to have straight orientation. In addition, however, inversion transduction grammars allow productions with inverted orientation, which generate output for stream 2 by emitting the con stituents on a productions righthand side in righttoleft order. We indicate a produc tions orientation with explicit notation for the two varieties of concatenation operators on stringpairs. The operator  performs the usual pairwise concatenation so that lAB yields the stringpair C1, C2 where C1  AtB1 and C2  A2B2. But the operator 0 concatenates constituents on output stream I while reversing them on stream 2, so that Ct  A1B1 but C2  B2A2. Since inversion is permitted at any level of rule expansion, a derivation may intermix productions of either orientation within the parse tree. For example, if the invertedorientation production of Figure lb is added to the earlier simple transduction grammar, sentencepair 3 can then be generated as follows 4 a. The AuthorityNp will be accountablevv to the Financial SecretarylNN NNN NP PP VP VP SP S b.      N P           JNN NNN NP PP    V V  VP VP sp o ls We can show the common structure of the two sentences more clearly and com pactly with the aid of the notation 1 The expressiveness of simple transduction grammars is equivalent to nondeterministic pushdown transducers Savitch 1982. 2 Also keep in mind that ITGs turn out to be especially suited for bilingual parsing applications, whereas pushdown transducers and syntaxdirected transduction grammars are designed for monolingual parsing in tandem with generation. 380 Wu Bilingual Parsing S . o  w i l l    The   p  A u t h o r i t y      P bee  accountable NN thec Financialll Secretary Figure 2 Inversion transduction grammar parse tree. 5 The A u t h o r i t y    NP wi l l  bec a c c o u n t a b l e    v v  toFhJ the   F i n a n c i a l    SecretarylNN NNN NP PP VP vP lsp   o  Is Alternatively, a graphical parse tree notation is shown in Figure 2, where the  level of bracketing is indicated by a horizontal line. The English is read in the usual depth first lefttoright order, but for the Chinese, a horizontal line means the right subtree is traversed before the left. Parsing, in the case of an ITG, means building matched constituents for input sentencepairs rather than sentences. This means that the adjacency constraints given by the nested levels must be obeyed in the bracketings of both languages. The result of the parse yields labeled bracketings for both sentences, as well as a bracket alignment indicating the parallel constituents between the sentences. The constituent alignment includes a word alignment as a byproduct. The nonterminals may not always look like those of an ordinary CFG. Clearly, the nonterminals of an ITG must be chosen in a somewhat different manner than for a monolingual grammar, since they must simultaneously account for syntactic patterns of both languages. One might even decide to choose nonterminals for an ITG that do not match linguistic categories, sacrificing this to the goal of ensuring that all corresponding substrings can be aligned. An ITG can accommodate a wider range of ordering variation between the lan 381 Computational Linguistics Volume 23, Number 3 Where is the Secretary of Finance when needed  II       J     Figure 3 An extremely distorted alignment that can be accommodated by an ITG. guages than might appear at first blush, through appropriate decomposition of pro ductions and thus constituents, in conjuction with introduction of new auxiliary non terminals where needed. For instance, even messy alignments such as that in Figure 3 can be handled by interleaving orientations 6 WhereJJ i s T   theE Secretary of Financellq whenl n e e d e d          This bracketing is of course linguistically implausible, so whether such parses are ac ceptable depends on ones objective. Moreover, it may even remain possible to align constituents for phenomena whose underlying structure is not contextfreesay, ellip sis or coordinationas long as the surface structures of the two languages fortuitously parallel each other though again the bracketing would be linguistically implausible. We will return to the subject of ITGs ordering flexibility in Section 4. We stress again that the primary purpose of ITGs is to maximize robustness for parallel corpus analysis rather than to verify grammaticality, and therefore writing grammars is made much easier since the grammars can be minimal and very leaky. We consider elsewhere an extreme special case of leaky ITGs, inversion invariant  transduction grammars, in which all productions occur with both orientations Wu 1995. As the applications below demonstrate, the bilingual lexical constraints carry greater importance than the tightness of the grammar. Formally, an inversion transduction grammar, or ITG, is denoted by G  N, W1,W2,T,S, where dV is a finite set of nonterminals, W1 is a finite set of words terminals of language 1, 42 is a finite set of words terminals of language 2, T is a finite set of rewrite rules productions, and S E A is the start symbol. The space of wordpairs terminalpairs X  W1 U c x W2 U c contains lexical transla tions denoted xy and singletons denoted x or y, where x E W1 and y E W2. Each production is either of straight orientation written A  ala2 . . .  ar, or of inverted ori entation written A  ala2..  a r  ,  where ai E A U X and r is the rank of the production. The set of transductions generated by G is denoted TG. The sets of monolingual strings generated by G for the first and second output languages are denoted LffG and L2G, respectively. 3. A Normal  Form for Inversion Transduction Grammars We now show that every ITG can be expressed as an equivalent ITG in a 2normal form that simplifies algorithms and analyses on ITGs. In particular, the parsing algorithm of the next section operates on ITGs in normal form. The availability of a 2normal 382 Wu Bilingual Parsing form is a no tewor thy  characteristic of ITGs no such normal  form is available for unrestr icted contextfree syntaxdirected t ransduct ion g r a m m a r s  Aho and Ul lman 1969b. The proof  closely follows that  for s tandard  CFGs, and  the proofs  of the l emmas  are omitted.  L e m m a  For any  duct ion 1 inversion t ransduct ion g r a m m a r  G, there exists an equivalent  inversion trans g r a m m a r  G where  TG  TG,  such that . . If  E LIG and  C L2G, then G contains a single product ion  of the fo rm S   cc, where  S  is the start  symbol  of G  and  does not  appea r  on the r ighthand side of any  product ion  of G o therwise  G contains no product ions  of the fo rm A  cc. n e m m a  For any  duct ion duct ion 2 inversion t ransduct ion g r a m m a r  G, there exists an equivalent  inversion trans g r a m m a r  G where  TG  TG,  such that  the r ighthand side of any  pro  of G t contains either a single terminalpair  or a list of nonterminals .  L e m m a  For any  duct ion tions of 3 inversion t ransduct ion g r a m m a r  G, there exists an equivalent  inversion trans g r a m m a r  G t where  TG  TG,  such that  G does not contain any  produc  the fo rm A  B. Theorem 1 For any  inversion t ransduct ion g r a m m a r  G, there exists an equivalent  inversion trans duction g r a m m a r  G t in which every product ion  takes one of the fol lowing forms s cc A xc A IBC A xy A y A BC Proof By Lemmas  1, 2, and 3, we  m a y  assume  G contains only product ions  of the fo rm S  cc, A  x y ,  A  x G A  y, A  BIB2, A  BIB2, A  B1... Bn, and A  B1 . . .  B, where  n  3 and  A  S. Include in G  all product ions  of the first six types. The remaining two types  are t rans formed as follows For each product ion  of the form A  B1...  Bn we introduce new nonterminals  X1 . . .  X,2 in order to replace the product ion  wi th  the set of rules A  B1X1,X1   B 2 X 2   . . . . .  X n  3    B n  2 X n  a  , X n  2   BnIB,. Let e,c be any  str ingpair  deriv able f rom A   B 1 .   Bn, where  e is ou tpu t  on s t ream 1 and  c on s t ream 2. Define e i a s  the substr ing of e der ived f rom Bi ,  and similarly define c i. Then Xi generates   e  i  1 . .  . e n ,  c i1 . . . C  n for all 1  i  n  1, so the new product ion  A   B I X 1   also generates  e, c. No addit ional  str ingpairs  are genera ted  due to the new product ions  since each Xi is only reachable f rom Xi1 and X1 is only reachable f rom A. For each product ion  of the form A  B1 . . .  Bn we replace the product ion  wi th  the set of rules A   B1Y1 , Y1   B2 Y 2   , . . . , Y n   3   B n   R Y n   2 ,  Y n   2   B n   I Bn .  Let e, c be any  str ingpair  derivable f rom A  B1   .  B n  ,  where  e is ou tpu t  on s t ream 1 and c on s t ream 2. Again define e i and  c i as the substr ings der ived f rom Bi, but  in this case e, c  e 1    e , c    c 1 . Then Y i  generates   e  i1     e n, c n    c i1   for all 383 Computational Linguistics Volume 23, Number 3 1  i  n  1, so the new production A  B1Y1 also generates e,c. Again, no additional stringpairs are generated due to the new productions.  Henceforth all transduction grammars will be assumed to be in normal form. 4. Expressiveness Characteristics We now turn to the expressiveness desiderata for a matching formalism. It is of course difficult to make precise claims as to what characteristics are necessary andor  suffi cient for such a model, since no cognitive studies that are directly pertinent to bilingual constituent alignment are available. Nonetheless, most related previous parallel cor pus analysis models share certain conceptual approaches with ours, loosely based on crosslinguistic theories related to constituency, case frames, or thematic roles, as well as computational feasibility needs. Below we survey the most common constraints and discuss their relation to ITGs. Crossing Constraints. Arrangements where the matchings between subtrees cross each another are prohibited by crossing constraints, unless the subtrees immediate parent constituents are also matched to each other. For example, given the constituent matchings depicted as solid lines in Figure 4, the dottedline matchings corresponding to potential lexical translations would be ruled illegal. Crossing constraints are im plicit in many phrasal matching approaches, both constituencyoriented Kaji, Kida, and Morimoto 1992 Cranias, Papageorgiou, and Peperidis 1994 Grishman 1994 and dependencyoriented Sadler and Vendelmans 1990 Matsumoto, Ishimoto, and Ut suro 1993. The theoretical crosslinguistic hypothesis here is that the core arguments of frames tend to stay together over different languages. The constraint is also useful for computational reasons, since it helps avoid exponential bilingual matching times. ITGs inherently implement a crossing constraint in fact, the version enforced by ITGs is even stronger. This is because even within a single constituent, immediate subtrees are only permitted to cross in exact inverted order. As we shall argue below, this restriction reduces matching flexibility in a desirable fashion. Rank Constraints. The second expressiveness desideratum for a matching formal ism is to somehow limit the rank of constituents the number of children or right handside symbols, which dictates the span over which matchings may cross. As the number of subtrees of an Llconstituent grows, the number of possible matchings to subtrees of the corresponding L2constituent grows combinatorially, with correspond ing time complexity growth on the matching process. Moreover, if constituents can immediately dominate too many tokens of the sentences, the crossing constraint loses effectivenessin the extreme, if a single constituent immediately dominates the en tire sentencepair, then any permutation is permissible without violating the crossing constraint. Thus, we would like to constrain the rank as much as possible, while still permitting some reasonable degree of permutation flexibility. Recasting this issue in terms of the general class of contextfree syntaxdirected transduction grammars, the number of possible subtree matchings for a single con stituent grows combinatorially with the number of symbols on a productions right hand side. However, it turns out that the ITG restriction of allowing only matchings with straight or inverted orientation effectively cuts the combinatorial growth, while still maintaining flexibility where needed. To see how ITGs maintain needed flexibility, consider Figure 5, which shows all 24 possible complete matchings between two constituents of length four each. Nearly all of these22 out of 24can be generated by an ITG, as shown by the parse trees whose 384 Wu Bilingual Parsing The Security Bureau grante  authori ty  t o   t h e  polic station Figure 4 The crossing constraint. nonterminal  labels are omitted. 3 The 22 permit ted matchings are representative of real transpositions in word  order be tween the EnglishChinese sentences in our  data. The only two matchings that cannot be generated are very  distorted transposit ions that we might  call  insideout matchings. We have been unable to find real examples in our  data of consti tuent arguments  undergoing insideout transposition. Note that this hypothesis  is for f ixedwordorder languages that are lightly in flected, such as English and Chinese. It would  not  be expected to hold for socalled scrambling or freewordorder languages, or heavily inflected languages. However ,  inflections provide alternative surface cues for determining consti tuent roles and 3 As discussed later, in many cases more than one parse tree can generate the same subconstituent matching. The trees shown are the canonical parses, as generated by the grammar of Figure 10. 385 Wu Bilingual Parsing r ITG all matchings ratio 0 1 1 1.000 1 1 1 1.000 2 2 2 1.000 3 6 6 1.000 4 22 24 0.917 5 90 120 0.750 6 394 720 0.547 7 1,806 5,040 0.358 8 8,558 40,320 0.212 9 41,586 362,880 0.115 10 206,098 3,628,800 0.057 11 1,037,718 39,916,800 0.026 12 5,293,446 479,001,600 0.011 13 27,297,738 6,227,020,800 0.004 14 142,078,746 87,178,291,200 0.002 15 745,387,038 1,307,674,368,000 0.001 16 3,937,603,038 20,922,789,888,000 0.000 Figure 6 Growth in number of legal complete subconstituent matchings for contextfree syntaxdirected transduction grammars with rank r, versus ITGs on a pair of subconstituent sequences of length r each. 5. Stochastic Inversion Transduction Grammars In a stochastic ITG SITG, a probabili ty is associated wi th  each rewrite rule. Following the s tandard convention, we use a and b to denote  probabilities for syntactic and lexical rules, respectively. For example, the probabili ty of the rule N N  0 A N is aNN,A N  0.4. The probabili ty of a lexical rule A 0.0001 xy is bAX,y  0.001. Let W1, W2 be the vocabulary sizes of the two languages, and X  A1 . . . . .  AN be the set of nonterminals  wi th  indices 1 , . . . , N .  For conciseness, we sometimes abuse the notat ion by  writing an index when  we mean  the corresponding nonterminal  symbol,  as long as this introduces no confusion. Then for every 1  i  N, the product ion probabilities are subject to the constraint that Y aiqjk aijk  y  bix,y  1 1Kj,kKN lxwl IyW2 We now introduce an algori thm for parsing with stochastic ITGs that computes  an optimal parse given a sentencepair using dynamic  programming.  In bilingual parsing, just as with ordinary monolinguat  parsing, probabilizing the grammar  permits  ambiguities to be resolved by  choosing the maximuml ikel ihood parse. Our  algori thm is similar in spirit to the recognition algori thm for HMMs Viterbi 1967 and to CYK parsing Kasami 1965 Younger 1967. Let the input  English sentence be el . . . . .  eT and the corresponding input  Chinese sentence be cl . . . . .  cv. As an abbreviation we write es.t for the sequence of words  1, es2 . . . . .  et, and similarly for cu v also, es s  c is the empty  string. It is convenient  to use a 4tuple of the form q  s, t, u, v to identify each node of the parse tree, where 387 Computational Linguistics Volume 23 Number 3 r ITG all matchings ratio 0 1 1 1.000 1 2 2 1.000 2 7 7 1.000 3 34 34 1.000 4 207 209 0.990 5 1,466 1,546 0.948 6 11,471 13,327 0.861 7 96,034 130,922 0.734 8 843,527 1,441,729 0.585 9 7,678,546 17,572,114 0.437 10 71,852,559 234,662,231 0.306 11 687,310,394 3,405,357,682 0.202 12 6,693,544,171 53,334,454,417 0.126 13 66,167,433,658 896,324,308,634 0.074 14 662,393,189,919 16,083,557,845,279 0.041 15 6,703,261,197,506 306,827,170,866,106 0.022 16 68,474,445,473,303 6,199,668,952,527,617 0.011 Figure 7 Growth in number of all legal subconstituent matchings complete or partial, meaning that some subconstituents are permitted to remain unmatched as singletons for contextflee syntaxdirected transduction grammars with rank r, versus ITGs on a pair of subconstituent sequences of length r each. the substrings es..t and u..v both  derive from the node q. Denote  the nonterminal  label on q by  fq. Then for any node q  s, t, u, v, define 6qi  6stuvi  max Psubtree of q,eq  i , i   es..tCu..v subtrees of q as the max imum probabili ty of any derivation from i that successfully parses both  es .t and cu..v. Then the best parse of the sentence pair  has probabili ty 60,T,0,vS. The algori thm computes  60,T,0,vS using the following recurrences. Note  that we generalize argmax to the case where maximizat ion ranges over mult iple indices, by making it vectorvalued. Also note  that  and 0 are s imply constants, wri t ten mnemonically. The condit ion S  s   t   S     U   u   v   U  0 is a w ay  to specify that the substring in one, but  not  both, languages m ay  be split into an empty  string c and the substring itself this ensures that the recursion terminates,  but  permits  words  that have no match in the other  language to map  to an  instead. 1. Init ial izat ion l  t  T  t l , t ,v l ,v i    bi e t Cv ,  1  v  V 1 1  t  T  6tu ,v ,v i    bi e t  ,  0  v  V 2 0  t  T  t,t,vl,vi  biCv,  1  V  V 3 388 Wu Bil ingual  Pars ing . . R e c u r s i o n  l  i  N  For all i ,s,t ,u,v such that os,r ouvv tsvu2 Gt.vi Gtuvi ma   0   xG.d0, s, v0  if 6,vi   6  i    0 otherwise 4 5 where 6,vi s lur   nuvi oi Vuvi 6Juvi l,vi cr  D stuvk  v 0 ri stuv   max IjN 1SkSN Ssst uUv  s s   t  s u .  uo  argmax 1SjSN 1SkSN st uUKv SstSUuvUO max l jN lkN sSSSt uUv SstSUuvUO a r g m a x  IjN lkN sst .uv StsUuvuao ai.jk 6sSuUj 5Stuvk 6 aik 6S,Uj 6stuvk 7 aijk 6sSUvj 6Stuuk 8 aik 6sSUvj 5Stuuk 9 R e c o n s t r u c t i o n  Initialize by setting the root of the parse tree to ql  0, T, 0, V and its nonterminal label to tql  S. The remaining descendants in the optimal parse tree are then given recursively for any q  s, t, u, v by NIL if t  s  v  u  2  LEFTq  S,Ofq,U,v.q if Oqfq   and tsvu3 10 s, cr  fq, v  fq,v if Oqt.q  0 and tsvu2 NIL if t  s  v  u  2  RmHTq  ot.q,t,vfq,V if qfq   and tsvu2 11 cr fq, t, u, v  fq if Oqfq  0 and ,svu2 LEFTq  ffqq 12 RIGHTq   qqfq 13 The time complexity of this algorithm in the general case is ON3T3V3, where N is the number  of distinct nonterminals and T and V are the lengths of the two sentences. This is a factor of V 3 more than monolingual  chart parsing, but has turned out to remain quite practical for corpus analysis, where parsing need not be realtime. 389 Computational Linguistics Volume 23, Number 3 6. Translationdriven Segmentation Segmentation of the input sentences is an important step in preparing bilingual cor pora for various learning procedures. Different languages realize the same concept using varying numbers of words for example, a single English word may surface as a compound in French. This complicates the problem of matching the words between a sentencepair, since it means that compounds or collocations must sometimes be treated as lexical units. The translation lexicon is assumed to contain collocation trans lations to facilitate such multiword matchings. However, the input sentences do not come broken into appropriately matching chunks, so it is up to the parser to decide when to break up potential collocations into individual words. The problem is particularly acute for English and Chinese because word bound aries are not orthographically marked in Chinese text, so not even a default chunking exists upon which word matchings could be postulated. Sentences 2 and 5 demon strate why the obvious trick of taking single characters as words is not a workable strategy. The usual Chinese NLP architecture first preprocesses input text through a word segmentation module Chiang et al. 1992 Lin, Chiang, and Su 1992, 1993 Chang and Chen 1993 Wu and Tseng 1993 Sproat et al. 1994 Wu and Fung 1994, but, clearly, bilingual parsing will be hampered by any errors arising from segmentation ambigui ties that could not be resolved in the isolated monolingual context because even if the Chinese segmentation is acceptable monolingually, it may not agree with the words present in the English sentence. Matters are made still worse by unpredictable omis sions in the translation lexicon, even for valid compounds. We therefore extend the algorithm to optimize the Chinese sentence segmentation in conjunction with the bracketing process. Note that the notion of a Chinese word is a longstanding linguistic question, that our present notion of segmentation does not address. We adhere here to a purely taskdriven definition of what a correct seg mentation is, namely that longer segments are desirable only when no compositional translation is possible. The algorithm is modified to include the following computa tions, and remains the same otherwise 1. Initialization 0  stuvl  bie t cu. .v ,  O  s  t  T  0  u  v  V  14 2. Recursion Sstuvi maxuvi, 0     stuvO, 6stuvO 15  if 6uvi   6uvi  and   0  stuvz  6stuvZ Gtuvi   if 6,vi   6,vi  and vstuv i,  6st,vZ  16 0 otherwise 3. Reconstruction LEFTq z NIL s,  eq , u,,,  eq   s,,, eq, ,1 eq,v NIL if tsvu2 if Oqfq   and t  s  v  u  2  if Oqeq  0 and t  s  v  u  2  otherwise 17 390 Wu Bilingual Parsing NIL if tsvu2 rrfq,t,vfq,v if Oqfq   and tsvu2 18 RIGHTq  fq,t,u,v fq if Oqfq  0 and tsvu2 NIL otherwise In our experience, this method has proven extremely effective for avoiding misseg mentation pitfalls, essentially erring only in pathological cases involving coordination constructions or lexicon coverage inadequacies. The method is also straightforward to employ in tandem with other applications, such as those below. 7. Bracketing Bracketing is another intermediate corpus annotation, useful especially when a full coverage grammar with which to parse a corpus is unavailable for Chinese, an even more common situation than with English. Aside from purely linguistic interest, bracket structure has been empirically shown to be highly effective at constraining sub sequent training of, for example, stochastic contextfree grammars Pereira and Schabes 1992 Black, Garside, and Leech 1993. Previous algorithms for automatic bracketing operate on monolingual texts and hence require more grammatical constraints for ex ample, tactics employing mutual information have been applied to tagged text Mager man and Marcus 1990. Our method based on SITGs operates on the novel principle that lexical correspon dences between parallel sentences yields information from which partial bracketings for both sentences can be extracted. The assumption that no grammar is available means that constituent categories are not differentiated. Instead, a generic bracket ing transduction grammar is employed, containing only one nonterminal symbol, A, which rewrites either recursively as a pair of As or as a single terminalpair A a A A A a A A A  UiV j A  ui bj A ,   v j  for all i,j EnglishChinese lexical translations for all i English vocabulary for all j Chinese vocabulary Longer productions with rank  2 are not needed we show in the subsections below that this minimal transduction grammar in normal form is generatively equivalent to any reasonable bracketing transduction grammar. Moreover, we also show how postprocessing using rotation and flattening operations restores the rank flexibility so that an output bracketing can hold more than two immediate constituents, as shown in Figure 11. The bq distribution actually encodes the EnglishChinese translation lexicon with degrees of probability on each potential word translation. We have been using a lexicon that was automatically learned from the HKUST EnglishChinese Parallel Bilingual Corpus via statistical sentence alignment Wu 1994 and statistical Chinese word and collocation extraction Fung and Wu 1994 Wu and Fung 1994, followed by an EM wordtranslationlearning procedure Wu and Xia 1994. The latter stage gives us the bij probabilities directly. For the two singleton productions, which permit any word in either sentence to be unmatched, a small cconstant can be chosen for the probabilities bit and bq, so that the optimal bracketing resorts to these productions only when it is 391 Computational Linguistics Volume 23, Number 3 otherwise impossible to match the singletons. The parameter a here is of no practical effect, and is chosen to be very small relative to the bq probabilities of lexical translation pairs. The result is that the maximumlikelihood parser selects the parse tree that best meets the combined lexical translation preferences, as expressed by the bij probabilities. Prepostpositional biases. Many bracketing errors are caused by singletons. With singletons, there is no crosslingual discrimination to increase the certainty between alternative bracketings. A heuristic to deal with this is to specify for each of the two languages whether prepositions or postpositions are more common, where preposi tion here is meant not in the usual partofspeech sense, but rather in a broad sense of the tendency of function words to attach left or right. This simple strategem is effective because the majority of unmatched singletons are function words that lack counterparts in the other language. This observation holds assuming that the transla tion lexicons coverage is reasonably good. For both English and Chinese, we specify a prepositional bias, which means that singletons are attached to the right whenever possible. A SingletonRebalancing Algorithm. We give here an algorithm for further improv ing the bracketing accuracy in cases of singletons. Consider the following bracketing produced by the algorithm of the previous section 7 Thec Au thor i ty Yj  wi l l  be accountablet  to thec f6J  F i n a n c i a l    Secretary  .o  The prepositional bias has already correctly restricted the singleton The to attach to the right, but of course The does not belong outside the rest of the sentence, but rather with Authority. The problem is that singletons have no discriminative power between alternative bracket matchingsthey only contribute to the ambiguity. We can minimize the impact by moving singletons as deep as possible, closer to the individual word they precede or succeed or in other words, we can widen the scope of the brackets immediately following the singleton. In general this improves precision since wide scope brackets are less constraining. The algorithm employs a rebalancing strategy reminiscent of balanced tree struc tures using left and right rotations. A left rotation changes a ABC structure to a ABC structure, and vice versa for a right rotation. The task is complicated by the presence of both  and 0 brackets with both L1 and L2singletons, since each com bination presents different interactions. To be legal, a rotation must preserve symbol order on both output streams. However, the following lemma shows that any subtree can always be rebalanced at its root if either of its children is a singleton of either language. L e m m a  4 Let x be an Llsingleton, y be an L2singleton, and A, B, C be arbitrary terminal or nonterminal symbols. Then the following properties hold for the  and  operators, where the  relation means that the same two output strings are generated, and the matching of the symbols is preserved Associativity ABC  lABC ABC  ABC 392 Wu Bilingual Parsing SINKSINGLETONnode 1 if node is not a leaf 2 if a rotation property applies at node 3 apply the rotation to node 4 child  the child into which the singleton was rotated 5 SINKSINGLETONchild REBALANCE TREEnode 1 if node is not a leaf 2 REBALANCETREEleftchildnode 3 REBALANCETREErightchildnode 4 SINKSINGLETONnode Figure 8 The singleton rebalancing schema. Llsingleton bidirectionality lax  Ax xA  xA L2singleton flipping commutativity lAy  yA yA  Ay Llsingleton rotation properties xAB  xAB  xAB  xAB xAB  xAB  xAB  xAB aBx  aBx  ABx  ABx ABx  lABx  ABx  ABx L2singleton rotation properties yAB  ABy  ABy  AyB yAB  lABy  ABy  AyB ABy  yAB  yAB  AyB ABly  yAB  yAB  AyB The method of Figure 8 modifies the input tree to attach singletons as closely as possible to couples, but remaining consistent with the input tree in the following sense singletons cannot escape their immediately surrounding brackets. The key is that for any given subtree, if the outermost bracket involves a singleton that should be rotated into a subtree, then exactly one of the singleton rotation properties will apply. The method proceeds depthfirst, sinking each singleton as deeply as possible. 393 Computational Linguistics Volume 23, Number 3 1 2 3 4 1 2 3 4 1 2 3 4 1 2 3 4 a b Figure 9 Alternative ITG parse trees for the same matching. 1 2 3 4 1 2 3 4 c For example, after rebalancing, sentence 7 is bracketed as follows 8 The A u t h o r i t y     wil l J be a c c o u n t a b l e     to the 6J  F i n a n c i a l    Secretary  .o  Flattening the Bracketing. In the worst case, both sentences might have perfectly aligned words, lending no discriminative leverage whatsoever to the bracketer. This leaves a very large number of choices if both sentences are of length l, then there 2t i are  l   possible bracketings with rank 2, none of which is better justified than any other. Thus to improve accuracy, we should reduce the specificity of the bracketings commitment in such cases. An inconvenient problem with ambiguity arises in the simple bracketing grammar above, illustrated by Figure 9 there is no justification for preferring either a or b over the other. In general the problem is that both the straight and inverted concatenation operations are associative. That is, AAA and AAA generate the same two output strings, which are also generated by AAA and similarly with AAA and AAA, which can also be generated by AAA. Thus the parse shown in c is preferable to either a or b since it does not make an unjustifiable commitment either way. Productions in the form of c, however, are not permitted by the normal form we use, in which each bracket can only hold two constituents. Parsing must overcommit, since the algorithm is always forced to choose between ABC and ABC structures even when no choice is clearly better. We could relax the normal form constraint, but longer productions clutter the grammar unnecessarily and, in the case of generic bracketing grammars, reduce parsing efficiency considerably. Instead, we employ a more complicated but betterconstrained grammar as shown in Figure 10, designed to produce only canonical tailrecursive parses. We differenti ate type A and B constituents, representing subtrees whose roots have straight and inverted orientation, respectively. Under this grammar, a series of nested constituents with the same orientation will always have a leftheavy derivation. The guarantee that parsing will produce a tailrecursive tree facilitates easily identification of those nesting levels that are associative and therefore arbitrary, so that those levels can be flattened by a postprocessing stage after parsing into nonnormal form trees like the one in Figure 9c. The algorithm proceeds bottomup, eliminating as many brack ets as possible, by making use of the associativity equivalences lABC  ABC and ABIC  ABC. The singleton bidirectionality and flipping commutativity equiv alences see Lemma 4 can also be applied whenever they render the associativity equivalences applicable. 3 9 4  Wu Bilingual Parsing A a A B A  B B A a C B A a A C A a B C B Z A A B a B A B  C A B a AC B  B C C  uivj C  ui C bi vj for all i,j EnglishChinese lexical translations for all i English vocabulary for all j Chinese vocabulary Figure 10 A stochastic constituentmatching ITG. The final result after flattening sentence 8 is as follows 9  Thee Author i ty wi l l    be accountable   to the Financial   Secretary  .   Experiment. Approximately 2,000 sentencepairs with both English and Chinese lengths of 30 words or less were extracted from our corpus and bracketed using the algorithm described. Several additional criteria were used to filter out unsuitable sentencepairs. If the lengths of the pair of sentences differed by more than a 21 ratio, the pair was rejected such a difference usually arises as the result of an earlier error in automatic sentence alignment. Sentences containing more than one word absent from the translation lexicon were also rejected the bracketing method is not intended to be robust against lexicon inadequacies. We also rejected sentencepairs with fewer than two matching words, since this gives the bracketing algorithm no discriminative leverage such pairs accounted for less than 2 of the input data. A random sample of the bracketed sentencepairs was then drawn, and the bracket precision was computed under each criterion for correctness. Examples are shown in Figure 11. The bracket precision was 80 for the English sentences, and 78 for the Chinese sentences, as judged against manual bracketings. Inspection showed the errors to be due largely to imperfections of our translation lexicon, which contains approximately 6,500 English words and 5,500 Chinese words with about 86 translation accuracy Wu and Xia 1994, so a better lexicon should yield substantial performance improvement. Moreover, if the resources for a good monolingual partofspeech or grammarbased bracketer such as that of Magerman and Marcus 1990 are available, its output can readily be incorporated in complementary fashion as discussed in Section 9. 395 Computational Linguistics Volume 23, Number 3 These a arrangements J willc cJ enhanceJIl o u r     abilityll   toe e  E t   m a i n t a i n  , .   m o n e t a r y    s t a b i l i t y  l   in the years to comeel .o  The A u t h o r i t y    w i l l    bee a c c o u n t a b l e     to the     F i n a n c i a l    Secretary  .o  Theyd  aree rightiEtf e nL toe  d o    eJ soe  .o   Evene morel impor t an t  l   , howeverEI  ,c eB, i s    to make the very best of oure  e JE ownT eIYJ talent,kq 1o   I    hopec  e  e m p l o y e r s   E  w i l l    make fulle e      u s e    ofe thoselaZ a fJr w h o  X   have acquirede e newJ skillsl   th rough i  t h i s  L   programmeilll .o 1 IJ have,   ate length,lt  one  how,a, w e    el,, canJJ booste e   j   our2i e    prosperityli .o I Figure 11 Bracketing output examples.   unrecognized input token. 8. Alignment 8.1 Phrasal Alignment Phrasal translation examples at the subsentential level are an essential resource for many MT and MAT architectures. This requirement is becoming increasingly direct for the examplebased machine translation paradigm Nagao 1984, whose translation flexibility is strongly restricted if the examples are only at the sentential level. It can now be assumed that a parallel bilingual corpus may be aligned to the sentence level with reasonable accuracy Kay and Ri3cheisen 1988 Catizone, Russel, and Warwick 1989 Gale and Church 1991 Brown, Lai, and Mercer 1991 Chen 1993, even for lan guages as disparate as Chinese and English Wu 1994. Algorithms for subsentential alignment have been developed as well as granularities of the character Church 1993, word Dagan, Church, and Gale 1993 Fung and Church 1994 Fung and McKeown 1994, collocation Smadja 1992, and specially segmented Kupiec 1993 levels. How ever, the identification of subsentential, nested, phrasal translations within the parallel texts remains a nontrivial problem, due to the added complexity of dealing with con stituent structure. Manual phrasal matching is feasible only for small corpora, either for toyprototype testing or for narrowly restricted applications. Automatic approaches to identification of subsentential translation units have largely followed what  we might call a parseparsematch procedure. Each half of the parallel corpus is first parsed individually using a monolingual grammar. Subse quently, the constituents of each sentencepair are matched according to some heuristic procedure. A number  of recent proposals can be cast in this framework Sadler and Vendelmans 1990 Kaji, Kida, and Morimoto 1992 Matsumoto, Ishimoto, and Utsuro 1993 Cranias, Papageorgiou, and Peperidis 1994 Grishman 1994. The parseparsematch procedure is susceptible to three weaknesses Appropriate, robust, monolingual grammars may not be available. This condition is particularly relevant for many nonWestern European languages such as Chinese. A grammar for this purpose must  be robust since it must  still identify constituents for the subsequent matching process even for unanticipated or illformed input sentences. 396 Wu Bilingual Parsing The grammars may be incompatible across languages. The bestmatching constituent types between the two languages may not include the same core arguments. While grammatical differences can make this problem unavoidable, there is often a degree of arbitrariness in a grammars chosen set of syntactic categories, particularly if the grammar is designed to be robust. The mismatch can be exacerbated when the monolingual grammars are designed independently, or under different theoretical considerations. Selection between multiple possible arrangements may be arbitrary. By an arrangement between any given pair of sentences from the parallel corpus, we mean a set of matchings between the constituents of the sentences. The problem is that in some cases, a constituent in one sentence may have several potential matches in the other, and the matching heuristic may be unable to discriminate between the options. In the sentence pair of Figure 4, for example, both Security Bureau and police station are potential lexical matches to  j .  To choose the best set of matchings, an optimization over some measure of overlap between the structural analysis of the two sentences is needed. Previous approaches to phrasal matching employ arbitrary heuristic functions on, say, the number of matched subconstituents. Our method attacks the weaknesses of the parseparsematch procedure by us ing 1 only a translation lexicon with no languagespecific grammar, 2 a bilingual rather than monolingual formalism, and 3 a probabilistic formulation for resolving the choice between candidate arrangements. The approach differs in its singlestage operation that simultaneously chooses the constituents of each sentence and the match ings between them. The raw phrasal translations suggested by the parse output were then filtered to remove those pairs containing more than 50 singletons, since such pairs are likely to be poor translation examples. Examples that occurred more than once in the corpus were also filtered out, since repetitive sequences in our corpus tend to be nongram matical markup. This yielded approximately 2,800 filtered phrasal translations, some examples of which are shown in Figure 12. A random sample of the phrasal translation pairs was then drawn, giving a precision estimate of 81.5. Although this already represents a useful level of accuracy, it does not in our opin ion reflect the full potential of the formalism. Inspection revealed that performance was greatly hampered by our noisy translation lexicon, which was automatically learned it could be manually postedited to reduce errors. Commercial online translation lex icons could also be employed if available. Higher precision could be also achieved without great effort by engineering a small number of broad nonterminal categories. This would reduce errors for known idiosyncratic patterns, at the cost of manual rule building. The automatically extracted phrasal translation examples are especially useful where the phrases in the two languages are not compositionally derivable solely from obvious word translations. An example is have acquired       newJ sk i l l s   j in Figure 11. The same principle applies to nested structures also, such as      I who  ,    have acquired      newJ s k i l l s    , on up to the sentence level. 397 Computational Linguistics Volume 23, Number 3 1 in real 1      Would you      an acceptable starting point for this new policy   I J    are about 3 . 5  million p k   3 5 0   born in Hong    for Hong  have the right to decide our  J  m  J   in what way the Government would increase   J    t   J l l       their job opportunities  and last month L J never to say  never         reserves and surpluses     I  ,  starting point for this new policy      there will be many practical difficulties in terms       I  ,  t  of implementation year ended 3 1 March 1 9 9 1    P h  J   n u   Figure 12 Examples of extracted phrasal translations. 8.2 Word Alignment Under the ITG model, word alignment becomes simply the special case of phrasal alignment at the parse tree leaves. This gives us an interesting alternative perspective, from the standpoint of algorithms that match the words between parallel sentences. By themselves, word alignments are of little use, but they provide potential anchor points for other applications, or for subsequent learning stages to acquire more interesting structures. Word alignment is difficult because correct matchings are not usually linearly ordered, i.e., there are crossings. Without some additional constraints, any word po sition in the source sentence can be matched to any position in the target sentence, an assumption that leads to high error rates. More sophisticated word alignment al gorithms therefore attempt to model the intuition that proximate constituents in close relationships in one language remain proximate in the other. The later IBM models are formulated to prefer collocations Brown et al. 1993. In the case of wordalign Dagan, Church, and Gale 1993 Dagan and Church 1994, a penalty is imposed according to the deviation from an ideal matching, as constructed by linear interpolation From this point of view, the proposed technique is a word alignment method that imposes a more realistic distortion penalty. The tree structure reflects the assumption that crossings should not be penalized as long as they are consistent with constituent structure. Figure 7 gives theoretical upper bounds on the matching flexibility as the lengths of the sequences increase, where the constituent structure constraints are re flected by high flexibility up to length4 sequences and a rapid dropoff thereafter. In other words, ITGs appeal to a language universals hypothesis, that the core arguments of frames, which exhibit great ordering variation between languages, are relatively few and surface in syntactic proximity. Of course, this assumption oversimplistically 4 Direct comparison with wordalign should be avoided, however, since it is intended to work on corpora whose sentences are not aligned. 398 Wu Bilingual Parsing blends syntactic and semantic notions. That semantic frames for different languages share common core arguments is more plausible than that syntactic frames do. In ef fect we are relying on the tendency of syntactic arguments to correlate closely with semantics. If in particular cases this assumption does not hold, however, the damage is not too greatthe model will simply drop the offending word matchings dropping as few as possible. In experiments with the minimal bracketing transduction grammar, the large ma jority of errors in word alignment were caused by two outside factors. First, word matchings can be overlooked simply due to deficiencies in our translation lexicon. This accounted for approximately 42 of the errors. Second, sentences containing nonliteral translations obviously cannot be aligned down to the word level. This accounted for another approximate 50 of the errors. Excluding these two types of errors, accuracy on word alignment was 96.3. In other words, the tree structure constraint is strong enough to prevent most false matches, but almost never inhibits correct word matches when they exist. 9. Bi l ingual  Constraint  Transfer 9.1 M o n o l i n g u a l  Parse Trees A parse may be available for one of the languages, especially for wellstudied lan guages such as English. Since this eliminates all degrees of freedom in the English sentence structure, the parse of the Chinese sentence must conform with that given for the English. Knowledge of English bracketing is thus used to help parse the Chi nese sentence this method facilitates a kind of transfer of grammatical expertise in one language toward bootstrapping grammar acquisition in another. A parsing algorithm for this case can be implemented very efficiently. Note that the English parse tree already determines the split point S for breaking e0. T into two constituent subtrees deriving e0..s and eS..T respectively, as well as the nonterminal labels j and k for each subtree. The same then applies recursively to each subtree. We indicate this by turning S, j, and k into deterministic functions on the English constituents, writing Sst, jst, and kst to denote the split point and the subtree labels for any constituent es..t. The following simplifications can then be made to the parsing algorithm . Recurs ion  For all English constituents es, t and all i, u, v such that  KiN 0V  6uvi     m a x  air, k., 6s st,, ujst 6s,,cu,vkst 19  u  U  v  ust stj , . , , Vuvi   argmax6s,t,u,ujst 6s,,t,U,vkst 20 u  U K v  6uvi   m a x  aijtka  6s,S,,,U,vjst  6s,t,t,u,Ukst 21 u  U  v  v  i s t u v ,  ,  argmax 6s,  S , t , U , v  j s t   6 S , t , t , u , U   k s t   22 u  U K v  3. Reconstruct ion  s,St,u,vfq if Oqq   LFTq  s, Gt, veq,v if Oqeq   23 399 Computational Linguistics Volume 23, Number 3 Sst, t,vgq,v if Oqgq   RIGHTq 24 Sst, t,u,v eq if Oqfq  0 gLEFTq  jst 25 RIGHTq  kst 26 The time complexity for this constrained version of the algorithm drops from ONBT3V 3 to OTV3. 9.2 Partial Parse Trees A more realistic inbetween scenario occurs when partial parse information is available for one or both of the languages. Special cases of particular interest include applications where bracketing or word alignment constraints may be derived from external sources beforehand. For example, a broadcoverage English bracketer may be available. If such constraints are reliable, it would be wasteful to ignore them. A straightforward extension to the original algorithm inhibits hypotheses that are inconsistent with given constraints. Any entries in the dynamic programming ta ble corresponding to illegal subhypothesesi.e., those that would violate the given bracketnesting or word alignment conditionsare preassigned negative infinity val ues during initialization indicating impossibility. During the recursion phase, computa tion of these entries is skipped. Since their probabilities remain impossible throughout, the illegal subhypotheses will never participate in any ML bibracketing. The running time reduction in this case depends heavily on the domain constraints. We have found this strategy to be useful for incorporating punctuation constraints. Certain punctuation characters give constituency indications with high reliability per fect separators include colons and Chinese full stops, while perfect delimiters in clude parentheses and quotation marks. 10. Unrestr ictedForm Grammars  It is possible to construct a parser that accepts unrestrictedform, rather than normal form, grammars. In this case an Earleystyle scheme Earley 1970, employing an active chart, can be used. The time complexity remains the same as the normalform case. We have found this to be useful in practice. For bracketing grammars of the type considered in this paper, there is no advantage. However, for more complex, linguisti cally structured grammars, the more flexible parser does not require the unreasonable numbers of productions that can easily arise from normalform requirements. For most grammars, we have found performance to be comparable or faster than the normal form parser. 11. C o n c l u s i o n  The twin concepts of bilingual language modeling and bilingual parsing have been proposed. We have introduced a new formalism, the inversion transduction grammar, and surveyed a variety of its applications to extracting linguistic information from parallel corpora. Its amenability to stochastic formulation, useful flexibility with leaky and minimal grammars, and tractability for practical applications are desirable proper ties. Various tasks such as segmentation, word alignment, and bracket annotation are naturally incorporated as subproblems, and a high degree of compatibility with con ventional monolingual methods is retained. In conjunction with automatic procedures for learning word translation lexicons, SITGs bring relatively underexploited bilingual 400 Wu Bilingual Parsing correlations to bear on the task of extracting linguistic information for languages less studied than English. We are currently pursuing several directions. We are developing an iterative train ing me thod  based on expectat ionmaximizat ion for estimating the probabilities from parallel training corpora. Also, in contrast to the applications discussed here, which deal with analysis and annotat ion of parallel corpora, we are working on incorporating the SITG model  directly into our  runt ime translation architecture. The initial results indicate excellent performance gains. Acknowledgments I would like to thank Xuanyin Xia, Eva WaiMan Fong, Pascale Fung, and Derick Wood, as well as an anonymous reviewer whose comments were of great value. References Aho, Alfred V. and Jeffrey D. Ullman. 1969a. Properties of syntax directed translations. Journal of Computer and System Sciences, 33319334. Aho, Alfred V. and Jeffrey D. Ullman. 1969b. Syntax directed translations and the pushdown assembler. Journal of Computer and System Sciences, 313756. Aho, Alfred V. and Jeffrey D. Ullman. 1972. The Theory of Parsing, Translation, and Compiling. Prentice Hall, Englewood Cliffs, NJ. Black, Ezra, Roger Garside, and Geoffrey Leech, editors. 1993. StatisticallyDriven Computer Grammars of English The IBMLancaster Approach. Editions Rodopi, Amsterdam. Brown, Peter F., John Cocke, Stephen A. DellaPietra, Vincent J. DellaPietra, Frederick Jelinek, John D. Lafferty, Robert L. Mercer, and Paul S. Roossin. 1990. A statistical approach to machine translation. Computational Linguistics, 1622985. Brown, Peter F., Stephen A. DellaPietra, Vincent J. DellaPietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation Parameter estimation. Computational Linguistics, 192263311. Brown, Peter F., Jennifer C. Lai, and Robert L. Mercer. 1991. Aligning sentences in parallel corpora. In Proceedings of the 29th Annual Meeting, pages 169176, Berkeley, CA. Association for Computational Linguistics. Catizone, Roberta, Graham Russell, and Susan Warwick. 1989. Deriving translation data from bilingual texts. In Proceedings of the First Lexical Acquisition Workshop, Detroit, MI. Chang, ChaoHuang and ChengDer Chen. 1993. HMMbased partorspeech tagging for Chinese corpora. In Proceedings of the Workshop on Very Large Corpora, pages 4047, Columbus, OH, June. Chen, Stanley F. 1993. Aligning sentences in bilingual corpora using lexical information. In Proceedings of the 31st Annual Meeting, pages 916, Columbus, OH. Association for Computational Linguistics. Chiang, TungHui, JingShin Chang, MingYu Lin, and KehYih Su. 1992. Statistical models for word segmentation and unknown resolution. In Proceedings of ROCLING92, pages 121146. Church, Kenneth W. 1993. Charalign A program for aligning parallel texts at the character level. In Proceedings of the 31st Annual Meeting, pages 18, Columbus, OH. Association for Computational Linguistics. Cranias, Lambros, Harris Papageorgiou, and Stelios Peperidis. 1994. A matching technique in examplebased machine translation. In Proceedings of the Fifteen th International Conference on Computational Linguistics, pages 100104, Kyoto. Dagan, Ido and Kenneth W. Church. 1994. Termight Identifying and translating technical terminology. In Proceedings of the Fourth Conference on Applied Natural Language Processing, pages 3440, Stuttgart, October. Dagan, Ido, Kenneth W. Church, and William A. Gale. 1993. Robust bilingual word alignment for machine aided translation. In Proceedings of the Workshop on Very Large Corpora, pages 18, Columbus, OH, June. Earley, Jay. 1970. An efficient contextfree parsing algorithm. Communications of the Association for Computing Machinery, 13294102. Fung, Pascale and Kenneth W. Church. 1994. Kvec A new approach for aligning parallel texts. In Proceedings of the Fifteenth International conference on Computational Linguistics, pages 10961102, Kyoto. Fung, Pascale and Kathleen McKeown. 1994. Aligning noisy parallel corpora 401 Computational Linguistics Volume 23, Number 3 across language groups Word pair feature matching by dynamic time warping. In AMTA94, Association for Machine Translation in the Americas, pages 8188, Columbia, MD, October. Fung, Pascale and Dekai Wu. 1994. Statistical augmentation of a Chinese machinereadable dictionary. In Proceedings of the Second Annual Workshop on Very Large Corpora, pages 6985, Kyoto, August. Gale, William A. and Kenneth W. Church. 1991. A program for aligning sentences in bilingual corpora. In Proceedings of the 29th Annual Meeting, pages 177184, Berkeley, CA. Association for Computational Linguistics. Gale, William A., Kenneth W. Church, and David Yarowsky. 1992. Using bilingual materials to develop word sense disambiguation methods. In TMI92, Proceedings of the Fourth International Conference on Theoretical and Methodological Issues in Machine Translation, pages 101112, Montreal. Gazdar, Gerald and Christopher S. Mellish. 1989. Natural Language Processing in LISP An Introduction to Computational Linguistics. AddisonWesley, Reading, MA. Grishman, Ralph. 1994. Iterative alignment of syntactic structures for a bilingual corpus. In Proceedings of the Second Annual Workshop on Very Large Corpora, pages 5768, Kyoto, August. Kaji, Hiroyuki, Yuuko Kida, and Yasutsugu Morimoto. 1992. Learning translation templates from bilingual text. In Proceedings of the Fourteenth International Conference on Computational Linguistics, pages 672678, Nantes. Kaplan, Ronald M. and Martin Kay. 1994. Regular models of phonological rule systems. Computational Linguistics, 203331378. Kasami, T. 1965. An efficient recognition and syntax analysis algorithm for contextfree languages. Technical Report AFCRL65758, Air Force Cambridge Research Laboratory, Bedford, MA. Kay, Martin and M. ROscheisen. 1988. Texttranslation alignment. Technical Report P9000143, Xerox Palo Alto Research Center. Koskenniemi, Kimmo. 1983. Twolevel morphology A general computational model for wordform recognition and production. Technical Report 11, Department of General Linguistics, University of Helsinki. Kupiec, Julian. 1993. An algorithm for finding noun phrase correspondences in bilingual corpora. In Proceedings of the 31st Annual Meeting, pages 1722, Columbus, OH. Association for Computational Linguistics. Laporte, Eric. 1996. Contextfree parsing with finitestate transducers. In String Processing Colloquium, Recife, Brazil. Lewis, P. M. and R. E. Stearns. 1968. Syntaxdirected transduction. Journal of the Association for Computing Machinery, 15465488. Lin, YiChung, TungHui Chiang, and KehYih Su. 1992. discrimination oriented probabilistic tagging. In Proceedings of ROCLING92, pages 8596. Lin, MingYu, TungHui Chiang, and KehYih Su. 1993. A preliminary study on unknown word problem in chinese word segmentation. In Proceedings of ROCLING93, pages 119141. Magerman, David M. and Mitchell P. Marcus. 1990. Parsing a natural language using mutual information statistics. In Proceedings of AAAI90, Eighth National Conference on Artificial Intelligence, pages 984989. Matsumoto, Yuji, Hiroyuki Ishimoto, and Takehito Utsuro. 1993. Structural matching of parallel texts. In Proceedings of the 31st Annual Meeting, pages 2330, Columbus, OH. Association for Computational Linguistics. Nagao, Makoto. 1984. A framework of a mechanical translation between Japanese and English by analogy principle. In Alick Elithorn and Ranan Banerji, editors, Artifiical and Human Intelligence Edited Review Papers Presented at the International NATO Symposium on Artificial and Human Intelligence. NorthHolland, Amsterdam, pages 173180. Pereira, Fernando. 1991. Finitestate approximation of phrase structure grammars. In Proceedings of the 29th Annual Meeting, Berkeley, CA. Association for Computational Linguistics. Pereira, Fernando and Yves Schabes. 1992. Insideoutside reestimation from partially bracketed corpora. In Proceedings of the 30th Annual Meeting, pages 128135, Newark, DE. Association for Computational Linguistics. Roche, Emmanuel. 1994. Two parsing algorithms by means of finitestate transducers. In Proceedings of the Fifteenth International Conference on Computational Linguistics, Kyoto. Sadler, Victor and Ronald Vendelmans. 1990. Pilot implementation of a bilingual knowledge bank. In Proceedings of the Thirteenth International Conference on 402 Wu Bilingual Parsing Computational Linguistics, pages 449451, Helsinki. Savitch, Walter J. 1982. Abstract Machines and Grammars. Little, Brown, Boston, MA. Smadja, Frank A. 1992. How to compile a bilingual collocational lexicon automatically. In AAAI92 Workshop on StatisticallyBased NLP Techniques, pages 6571, San Jose, CA, July. Sproat, Richard, Chilin Shih, William Gale, and Nancy Chang. 1994. A stochastic word segmentation algorithm for a Mandarin texttospeech system. In Proceedings of the 32nd Annual Meeting, pages 6672, Las Cruces, NM, June. Association for Computational Linguistics. Viterbi, Andrew J. 1967. Error bounds for convolutional codes and an asymptotically optimal decoding algorithm. IEEE Transactions on Information Theory, 13260269. Wu, Dekai. 1994. Aligning a parallel EnglishChinese corpus statistically with lexical criteria. In Proceedings of the 32nd Annual Meeting, pages 8087, Las Cruces, NM, June. Association for Computational Linguistics. Wu, Dekai. 1995. An algorithm for simultaneously bracketing parallel texts by aligning words. In Proceedings of the 33rd Annual Meeting, pages 244251, Cambridge, MA, June. Association for Computational Linguistics. Wu, Dekai and Pascale Fung. 1994. Improving Chinese tokenization with linguistic filters on statistical lexical acquisition. In Proceedings of the Fourth Conference on Applied Natural Language Processing, pages 180181, Stuttgart, October. Wu, Dekai and Xuanyin Xia. 1994. Learning an EnglishChinese lexicon from a parallel corpus. In AMTA94, Association for Machine Translation in the Americas, pages 206213, Columbia, MD, October. Wu, Zimin and Gwyneth Tseng. 1993. Chinese text segmentation for text retrieval Achievements and problems. Journal of The American Society for Information Sciences, 449532542. Younger, David H. 1967. Recognition and parsing of contextfree languages in time n 3. Information and Control, 102189208. 403
