In Proceedings of International Conference Research on Computational Linguistics ROCLING X, 1997, Taiwan.Semantic Similarity Based on Corpus Statistics and LexicalTaxonomy        Jay J. Jiang David W. ConrathDepartment of Management Sciences        MGD School of BusinessUniversity of Waterloo           McMaster University  Waterloo, Ontario, Canada N2L 3G1 Hamilton, Ontario, Canada L8S 4M4 jjianguwaterloo.ca         conrathdmcmaster.caAbstractThis paper presents a new approach for measuring semantic similaritydistance betweenwords and concepts.  It combines a lexical taxonomy structure with corpus statisticalinformation so that the semantic distance between nodes in the semantic space constructedby the taxonomy can be better quantified with the computational evidence derived from adistributional analysis of corpus data.  Specifically, the proposed measure is a combinedapproach that inherits the edgebased approach of the edge counting scheme, which is thenenhanced by the nodebased approach of the information content calculation.  When testedon a common data set of word pair similarity ratings, the proposed approach outperformsother computational models.  It gives the highest correlation value r  0.828 with abenchmark based on human similarity judgements, whereas an upper bound r  0.885 isobserved when human subjects replicate the same task.1.  IntroductionThe characteristics of polysemy and synonymy that exist in words of natural language have alwaysbeen a challenge in the fields of Natural Language Processing NLP and Information RetrievalIR.  In many cases, humans have little difficulty in determining the intended meaning of anambiguous word, while it is extremely difficult to replicate this process computationally.  Formany tasks in psycholinguistics and NLP, a job is often decomposed to the requirement ofresolving the semantic relation between words or concepts.  One needs to come up with aconsistent computational model to assess this type of relation.  When a word level semanticrelation requires exploration, there are many potential types of relations that can be consideredhierarchical e.g. ISA or hypernymhyponym, partwhole, etc., associative e.g. causeeffect,equivalence synonymy, etc.  Among these, the hierarchical relation represents the major andmost important type, and has been widely studied and applied as it maps well to the humancognitive view of classification i.e. taxonomy.  The ISA relation, in particular, is a typicalrepresentative of the hierarchical relation.  It has been suggested and employed to study a specialcase of semantic relations  semantic similarity or semantic distance Rada et al. 1989.  In thisstudy of semantic similarity, we will take this view, although it excludes some potential usefulinformation that could be derived from other relations.The study of wordsterms relationships can be viewed in terms of the information sources used.The least information used are knowledgefree approaches that rely exclusively on the corpus data2themselves.  Under the corpusbased approach, word relationships are often derived from theircooccurrence distribution in a corpus Church and Hanks 1989, Hindle 1990, Grefenstette 1992.With the introduction of machine readable dictionaries, lexicons, thesauri, and taxonomies, thesemanually built pseudoknowledge bases provide a natural framework for organising words orconcepts into a semantic space.  Kozima and Furugori 1993 measured word distance byadaptive scaling of a vector space generated from LDOCE Longman Dictionary ofContemporary English.  Morris and Hirst 1991 used Rogets thesaurus to detect wordsemantic relationships.  With the recently developed lexical taxonomy WordNet Miller 1990,Miller et al. 1990, many researches have taken the advantage of this broadcoverage taxonomy tostudy wordconcept relationships Resnik 1995, Richardson and Smeaton 1995.In this paper, we will discuss the use of the corpusbased method in conjunction with lexicaltaxonomies to calculate semantic similarity between wordsconcepts.  In the next section we willdescribe the thread and major methods in modelling semantic similarity.  Based on the discussion,we will present a new similarity measure, which is a combined approach of previous methods.  Insection 3, experiments are conducted to evaluate various computational models compared againsthuman similarity judgements.  Finally, we discuss the related work and future direction of thisstudy.2.  Semantic Similarity in a TaxonomyThere are certain advantages in the work of semantic association discovery by combining ataxonomy structure with corpus statistics.  The incorporation of a manually built pseudoknowledge base e.g. thesaurus or taxonomy may complement the statistical approach wheretrue understanding of the text is unobtainable.  By doing this, the statistics model can takeadvantage of a conceptual space structured by a handcrafted taxonomy, while providingcomputational evidence from manoeuvring in the conceptual space via distributional analysis ofcorpora data.  In other words, calculating the semantic association can be transformed to theestimation of the conceptual similarity or distance between nodes words or concepts in theconceptual space generated by the taxonomy.  Ideally, this kind of knowledge base should bereasonably broadcoverage, well structured, and easily manipulated in order to derive desiredassociative or similarity information.Since a taxonomy is often represented as a hierarchical structure, which can be seen as a specialcase of network structure, evaluating semantic similarity between nodes in the network can makeuse of the structural information embedded in the network.  There are several ways to determinethe conceptual similarity of two words in a hierarchical semantic network.  Topographically, thiscan be categorised as node based and edge based approaches, which correspond to theinformation content approach and the conceptual distance approach, respectively.2.1.  Nodebased Information Content ApproachOne node based approach to determine the conceptual similarity is called the information contentapproach Resnik 1992, 1995.  Given a multidimensional space upon which a node represents a3unique concept consisting of a certain amount of information, and an edge represents a directassociation between two concepts, the similarity between two concepts is the extent to which theyshare information in common.  Considering this in a hierarchical conceptclass space, this commoninformation carrier can be identified as a specific concept node that subsumes both of the two inthe hierarchy.  More precisely, this superclass should be the first class upward in this hierarchythat subsumes both classes.  The similarity value is defined as the information content value of thisspecific superordinate class.  The value of the information content of a class is then obtained byestimating the probability of occurrence of this class in a large text corpus.Following the notation in information theory, the information content IC of a conceptclass ccan be quantified as followsIC c P c  log   1 , 1where Pc is the probability of encountering an instance of concept c.  In the case of thehierarchical structure, where a concept in the hierarchy subsumes those lower in the hierarchy,this implies that Pc is monotonic as one moves up the hierarchy.  As the nodes probabilityincreases, its information content or its informativeness decreases.  If there is a unique top node inthe hierarchy, then its probability is 1, hence its information content is 0.Given the monotonic feature of the information content value, the similarity of two concepts canbe formally defined assim c cc Sup c cIC cc Sup c cp c ,  max ,    max ,  log  ,1 21 2 1 2 2where Sup c c , 1 2 is the set of concepts that subsume both c1  and c2 .  To maximize therepresentativeness, the similarity value is the information content value of the node whose ICvalue is the largest among those super classes.  In another word, this node is the lowest upperbound among those that subsume both c1  and c2 .In the case of multiple inheritances, where words can have more than one sense and hencemultiple direct super classes, word similarity can be determined by the best similarity value amongall the class pairs which their various senses belong tosim w wc sen w c sen wsim c c ,  max     , ,1 21 1 2 21 2   3where senw denotes the set of possible senses for word w.For the implementation of the information content model, there are some slightly differentapproaches toward calculating the conceptclass probabilities in a corpus.  Before giving thedetailed calculation, we need to define two concept sets wordsc and classesw.  Wordsc is theset of words subsumed directly or indirectly by the class c.  This can be seen as a subtree in the4whole hierarchy, including the subtree root c.  Classesw is defined as the classes in which theword w is contained in another word, it is the set of possible senses that the word w hasclasses w c w words c     .  4Resnik 1995 defined a simple classconcept frequency formulafreq c freq ww words c     . 5Richardson and Smeaton 1995 proposed a slightly different calculation by considering thenumber of word senses factorfreq cfreq wclasses ww words c      6Finally, the classconcept probability can be computed using maximum likelihood estimationMLEP Cfreq cN   7This methodology can be best illustrated by examples.  Assume that we want to determine thesimilarities between the following classes car, bicycle and car, fork.  Figure 1 depicts thefragment of the WordNet Version 1.5 noun hierarchy that contains these classes.  The number inthe bracket of a node indicates the corresponding information content value.  From the figure wefind that the similarity between car and bicycle is the information content value of the classvehicle, which has the maximum value among all the classes that subsume both of the two classes,i.e. simcar, bicycle  8.30.  In contrast, simcar, fork  3.53.  These results conform to ourperception that cars and forks are less similar than cars and bicycles.Artifact 3.53Instrumentality 4.91Conveyance 8.14Vehicle 8.30Motor VehicleCarWheeled VehicleCycleBicycleArticleWareTable WareCutleryForkObject 2.79Figure 1. Fragments of the WordNet noun taxonomy52.2.  Edgebased Distance ApproachThe edge based approach is a more natural and direct way of evaluating semantic similarity in ataxonomy.  It estimates the distance e.g. edge length between nodes which correspond to theconceptsclasses being compared.  Given the multidimensional concept space, the conceptualdistance can conveniently be measured by the geometric distance between the nodes representingthe concepts.  Obviously, the shorter the path from one node to the other, the more similar theyare.For a hierarchical taxonomy, Rada et al. 1989 pointed out that the distance should satisfy theproperties of a metric, namely zero property, symmetric property, positive property, andtriangular inequality.  Furthermore, in an ISA semantic network, the simplest form of determiningthe distance between two elemental concept nodes, A and B, is the shortest path that links A andB, i.e. the minimum number of edges that separate A and B Rada et al. 1989.In a more realistic scenario, the distances between any two adjacent nodes are not necessarilyequal.  It is therefore necessary to consider that the edge connecting the two nodes should beweighted.  To determine the edge weight automatically, certain aspects should be considered inthe implementation.  Most of these are typically related to the structural characteristics of ahierarchical network.  Some conceivable features are local network density the number of childlinks that span out from a parent node, depth of a node in the hierarchy, type of link, and finally,perhaps the most important of all, the strength of an edge link.  We will briefly discuss theconcept for each feature With regard to network density, it can be observed that the densities in different parts of thehierarchy are higher than others.  For example, in the plantflora section of WordNet thehierarchy is very dense.  One parent node can have up to several hundred child nodes.  Sincethe overall semantic mass is of a certain amount for a given node and its subordinates, thelocal density effect Richardson and Smeaton 1995 would suggest that the greater the density,the closer the distance between the nodes i.e. parent child nodes or sibling nodes.  As for node depth, it can be argued that the distance shrinks as one descends the hierarchy,since differentiation is based on finer and finer details.  Type of link can be viewed as the relation type between nodes.  In many thesaurus networksthe hyponymhypernym ISA link is the most common concern.  Many edgebased modelsconsider only the ISA link hierarchy Rada et al. 1989, Lee et al. 1993.  In fact, other linktypesrelations, such as MeronymHolonym Partof, Substanceof, should also be consideredas they would have different effects in calculating the edge weight, provided that the dataabout the type of relation are available.  To differentiate the weights of edges connecting a node and all its child nodes, one needs toconsider the link strength of each specific child link.  This could be measured by the closeness6between a specific child node and its parent node, against those of its siblings.  Obviously,various methods could be applied here.  In particular, this is the place where corpus statisticscould contribute.  Ideally the method chosen should be both theoretical sound andcomputational efficient. Two studies have been conducted in edgebased similarity determination by responding to theabove concerns.  Richardson and Smeaton 1995 considered the first two and the last factors intheir edge weight calculation for each link type.  Network density is simply counting the numberof edges of that type.  The link strength is a function of a nodes information content value, andthose of its siblings and parent nodes.  The result of these two operations is then normalised bydividing them by the link depth.  Notice that the precise formula of their implementation was notgiven in the paper.Sussna 1993 considered the first three factors in the edge weight determination scheme.  Theweight between two nodes c1  and c2  is calculated as followswt c cwt c c wt c cdr r ,    1 21 2 2 12    8givenwt x yn xr rr rr  maxmax min   9where  r  is a relation of type r,  r  is its reverse, d is the depth of the deeper one of the two,max and min are the maximum and minimum weights possible for a specific relation type rrespectively, and n xr    is the number of relations of type r leaving node x.Applying this distance formula to a word sense disambiguation task, Sussna 1993 showed animprovement where multiple sense words have been disambiguated by finding the combination ofsenses from a set of contiguous terms which minimizes total pairwise distance between senses.He found that the performance is robust under a number of perturbations however, depth factorscaling and restricting the type of link to a strictly hierarchical relation do noticeably impairperformance.In determining the overall edge based similarity, most methods just simply sum up all the edgeweights along the shortest path.  To convert the distance measure to a similarity measure, onemay simply subtract the path length from the maximum possible path length Resnik 1995sim w w dc sen w c sen wlen c c ,   min    , ,max1 21 1 2 21 22    10where dmax  is the maximum depth of the taxonomy, and the len function is the simple calculationof the shortest path length i.e. weight  1 for each edge.72.3.  Comparison of the Two ApproachesThe two approaches target semantic similarity from quite different angles.  The edgebaseddistance method is more intuitive, while the nodebased information content approach is moretheoretically sound.  Both have inherent strength and weakness.Rada et al. 1989 applied the distance method to a medical domain, and found that the distancefunction simulated well human assessments of conceptual distance.  However, Richardson andSmeaton 1995 had concerns that the measure was less accurate than expected when applied to acomparatively broad domain e.g. WordNet taxonomy.  They found that irregular densities oflinks between concepts result in unexpected conceptual distance outcomes.  Also, without causingserious side effects elsewhere, the depth scaling factor does not adjust the overall measure welldue to the general structure of the taxonomy e.g. higher sections tend to be too similar to eachother.In addition, we feel that the distance measure is highly depended upon the subjectively predefinednetwork hierarchy.  Since the original purpose of the design of the WordNet was not for similaritycomputation purpose, some local network layer constructions may not be suitable for the directdistance manipulation.The information content method requires less information on the detailed structure of a taxonomy.It is not sensitive to the problem of varying link types Resnik 1995.  However, it is stilldependent on the skeleton structure of the taxonomy.  Just because it ignores information on thestructure it has its weaknesses.  It normally generates a coarse result for the comparison ofconcepts.  In particular, it does not differentiate the similarity values of any pair of concepts in asubhierarchy as long as their smallest common denominator i.e. the lowest superordinateclass is the same.  For example, given the concepts in Figure 1, the results of the similarityevaluation between bicycle, table ware and bicycle, fork would be the same.  Also, other typeof link relations information is overlooked here.  Additionally, in the calculation of informationcontent, polysemous words will have an exaggerated content value if only word not its sensefrequency data are used Richardson and Smeaton 1995.2.4.  A Combined ApproachWe propose a combined model that is derived from the edgebased notion by adding theinformation content as a decision factor.  We will consider various concerns of the edge weightingschemes discussed in the previous section.  In particular, attention is given to the determination ofthe link strength of an edge that links a parent node to a child node.We first consider the link strength factor.  We argue that the strength of a child link isproportional to the conditional probability of encountering an instance of the child concept cigiven an instance of its parent concept p Pci  p.8P c pP c pP pP cP pii i       11Notice that the definition and determination of the information content see equations 1 and 5indicate that ci is a subset of p when a concepts informativeness is concerned.  Following thestandard argument of information theory, we define the link strength LS by taking the negativelogarithm of the above probability.  We obtain the following formulaLS c p P c p IC c IC pi i i ,  log           . 12This states that the link strength LS is simply the difference of the information content valuesbetween a child concept and its parent concept.Considering other factors, such as local density, node depth, and link type, the overall edgeweight wt for a child node c and its parent node p can be determined as followswt c pEE pd pd pIC c IC p T c p ,            ,    11 , 13where dp denotes the depth of the node p in the hierarchy, Ep the number of edges in the childlinks i.e. local density, E  the average density in the whole hierarchy, and Tc,p the linkrelationtype factor.  The parameters     0  and    0 1   control the degree of howmuch the node depth and density factors contribute to the edge weighting computation.  Forinstance, these contributions become less significant when  approaches 0 and  approaches 1.The overall distance between two nodes would thus be the summation of edge weights along theshortest path linking two nodes.Dist w w wt c parent cc path c c LSuper c c ,   ,    ,   , 1 21 2 1 2  14where c1senw1, c2senw2, and path c1, c2 is the set that contains all the nodes in theshortest path from c1 to c2.  One of the elements of the set is LSuperc1,c2, which denotes thelowest superordinate of c1 and c2.  In the special case when only link strength is considered in theweighting scheme of equation 13, i.e.   0,   1, and Tc,p  1, the distance function can besimplified as followsDist w w IC c IC c IC LSuper c c ,        , 1 2 1 2 1 22    15Imagine a special multidimensional semantic space where every node concept in the space lieson a specific axis and has a mass based on its information content or informativeness.  Thesemantic distance between any such two nodes is the difference of their semantic mass if they areon the same axis, or the addition of the two distances calculated from each node to a common9node where two axes meet if the two original nodes are on different axes.  It is easy to prove thatthe proposed distance measure also satisfies the properties of a metric.3.  Evaluation3.1.  Task DescriptionIt would be reasonable to evaluate the performance of machine measurements of semanticsimilarity between concepts by comparing them with human ratings on the same setting.  Thesimplest way to implement this is to set up an experiment to rate the similarity of a set of wordpairs, and examine the correlation between human judgement and machine calculations.  To makeour experimental results comparable with other previous experiments, we decided to use the samesample of 30 noun pairs that were selected in an experiment when only human subjects wereinvolved Miller and Charles 1991, and in another more recent experiment when somecomputational models were constructed and compared as well Resnik 1995.  In fact, in theResnik 1995 experiment, he replicated the human judgements on the same set of word pairs thatMiller and Charles did.  When the correlation between his replication and the one done by Millerand Charles 1991 was calculated, a baseline from human ratings was obtained for evaluation,which represents an upper bound that one could expect from a machine computation on the sametask.  In our experiment, we compare the proposed model with the nodebased InformationContent model developed by Resnik 1995 and the basic edgebased edge counting model, in thecontext of how well these perform against human ratings i.e. the upper bound.For consistency in comparison, we will use semantic similarity measures rather than the semanticdistance measures.  Hence our proposed distance measure needs to be converted to a similaritymeasure.  Like the edge counting measure in equation 10, the conversion can be made bysubtracting the total edge weights from the maximum possible total edge weights.  Note that thisconversion does not affect the result of the evaluation, since a linear transformation of each datumwill not change the magnitude of the resulting correlation coefficient, although its sign maychange from positive to negative.3.2.  ImplementationThe noun portion of the latest version 1.5 of WordNet was selected as the taxonomy tocompute the similarity between concepts.  It contains about 60,000 nodes synsets.  Thefrequencies of concepts were estimated using noun frequencies from a universal semanticconcordance SemCor Miller et al. 1993, a semantically tagged text consisting of 100 passagesfrom the Brown Corpus.  Since the tagging scheme was based on the WordNet word sensedefinition, this enables us to obtain a precise frequency distribution for each node synset in thetaxonomy.  Therefore it avoids potentially spurious results in occasions when only word notword sense frequencies are used Resnik 1995.  The downside of using the SemCor data is therelatively small size of the corpus due to the need to manually tag the sense for each word in thecorpus.  Slightly over 25 of the WordNet noun senses actually appeared in the corpus.Nevertheless, this is the only publicly available sense tagged corpus.  The MLE method would10seem unsuitable for probability estimation from the SemCor corpus.  To circumvent the problemof data sparseness, we use the GoodTuring estimation with linear interpolation.3.3.  ResultsTable 1 lists the complete results of each similarity rating measure for each word pair.  The dataon human ratings are from the publication of previous results Miller and Charles 1991, Resnik1995.  Notice that two values in Resniks replication are not available, as he dropped two nounpairs in his experiment since the word woodland was not yet in the WordNet taxonomy at thattime.  The correlation values between the similarity ratings and the mean ratings reported byMillers and Charles are listed in Table 2.  The optimal parameter settings for the proposedsimilarity approach are 0.5, 0.3.  Table 3 lists the results of the correlation values for theproposed approach given a combination of a range of parameter settings.Word Pair MCmeansReplicationmeansSimedge Simnode Simdistcar automobile 3.92 3.9 30 10.358 30gem jewel 3.84 3.5 30 17.034 30journey voyage 3.84 3.5 29 10.374 27.497boy lad 3.76 3.5 29 9.494 25.839coast shore 3.7 3.5 29 12.223 28.702asylum madhouse 3.61 3.6 29 15.492 28.138magician wizard 3.5 3.5 30 14.186 30midday noon 3.42 3.6 30 13.558 30furnace stove 3.11 2.6 23 3.527 17.792food fruit 3.08 2.1 24 2.795 23.775bird cock 3.05 2.2 29 9.122 26.303bird crane 2.97 2.1 27 9.122 24.452tool implement 2.95 3.4 29 8.84 29.311brother monk 2.82 2.4 25 2.781 19.969crane implement 1.68 0.3 26 4.911 19.579lad brother 1.66 1.2 26 2.781 20.326journey car 1.16 0.7 0 0 17.649monk oracle 1.1 0.8 23 2.781 18.611cemetery woodland 0.95 NA 0 0 10.672food rooster 0.89 1.1 18 1.03 17.657coast hill 0.87 0.7 26 8.917 25.461forest graveyard 0.84 0.6 0 0 14.52shore woodland 0.63 NA 25 2.795 16.836monk slave 0.55 0.7 26 2.781 20.887coast forest 0.42 0.6 24 2.795 15.538lad wizard 0.42 0.7 26 2.781 20.717chord smile 0.13 0.1 20 4.452 17.535glass magician 0.11 0.1 22 1.03 17.098noon string 0.08 0 0 0 12.987rooster voyage 0.08 0 0 0 12.506Table 1.  Word Pair Semantic Similarity Measurement11Similarity Method Correlation rHuman Judgement replication 0.8848Node Based Information Content 0.7941Edge Based Edge Counting 0.6004Combined Distance Model 0.8282Table 2.  Summary of Experimental Results 30 noun pairs3.4.  DiscussionThe results of the experiment confirm that the information content approach proposed by Resnik1995 provides a significant improvement over the traditional edge counting method.  It alsoshows that our proposed combined approach outperforms the information content approach.  Oneshould recognize that even a small percentage improvement over the existing approaches is ofsignificance since we are nearing the observed upper bound.The results from Table 3 conform to our projection that the density factor and the depth factor inthe hierarchy do affect although not significantly the semantic distance metric.  A properselection of these two factors will enhance the distance estimation.  Setting the density factorparameter at 0.3 seems optimal as most of the resultant values outperform others under arange of depth factor settings.  The optimal depth scaling factor   ranges from 0 to 0.5, whichindicates it is less influential than the density factor.  This would support the Richardson andSmeaton 1995 argument about the difficulty of the adjustment of the depth scaling factor.Another explanation would be that this factor is already absorbed in the proposed link strengthconsideration.  Overall, there is a small performance improvement 2.1 over the result whenonly the link strength factor is considered.  Since the results are not very sensitive to the variationin parameter settings,  we can conclude that they are not the major determinants of the overalledge weight.Depth Factor Density Factor  1.0 0.5 0.3 0.22 0.79844 0.81104 0.81153 0.806581 0.80503 0.82255 0.82625 0.822660.5 0.80874 0.82397 0.82817 0.825090 0.81127 0.82284 0.82737 0.824111 0.81435 0.81598 0.81818 0.813492 0.81315 0.80228 0.80118 0.79492Table 3.  Correlation coefficient values of various parameter settings for the proposed approachFurther examinations of the individual results in Table 1 may provide a deeper understanding ofthe models performance.  The ratings in the table are sorted in descending order based on Millerand Charles 1991 findings.  This trend can be observed more or less consistently in four otherratings.  However, there are some abnormalities that exist in the results.  For example, the pairfurnacestove was given high similarity values in human ratings, whereas a very low ratingsecond to the lowest was found in the proposed distance measure.  A further look at their12classification in the WordNet hierarchy seems to provide an explanation.  Figure 2 depicts aportion of WordNet hierarchy that includes all the senses of these two words.  We can observethat furnace and stove are classified under very distinct substructures.  Their closest superordinate class is artifact, which is a very high level abstraction.  It would be more reasonable if thesubstructure containing furnace were placed under the class of device or appliance.  If so thedistance between furnace and stove would have been shorter and closer to humans judgements.This observation reenforces our earlier thought that the structure of a taxonomy may generate abias towards a certain distance calculation due to the nature of its classification scheme.    entity    objectartifact commodityenclosure   instrumentation consumer goods chamber          device   durable goodsfurnace            heater      appliance           stove home appliancekitchen appliance         stoveFigure 2.  A fragment of WordNet taxonomyTable 4 shows calculations of the correlation coefficients based on removing the furnacestovepair due to a questionable classification of the concept furnace in the taxonomy.  The result showsan immediate improvement of all the computational models.  In particular, our proposed modelindicates a large marginal lead.Similarity Method Correlation rNode Based Information Content 0.8191Edge Based Edge Counting 0.6042Combined Distance Model 0.8654Table 4.  Summary of Experimental Results29 noun pairs, removing the furnace  stove pair4.  Related WorkClosely related works to this study are those that were aligned with the thread of our discussion.In the line of the edgebased approach, Rada et al. 1989 and Lee et al. 1993 derived semanticdistance formulas using the edge counting principle, which were then used to support higher levelresult ranking in document retrieval.  Sussna 1993 defined a similarity measure that takes intoaccount taxonomy structure information.  Resniks 1995 information content measure is atypical representative of the nodebased approach.  Most recently, Richardson and Smeaton131995 and Smeaton and Quigley 1996 worked on a combined approach that is very similar toours.One of the many applications of semantic similarity models is for word sense disambiguationWSD.  Agirre and Rigau 1995 proposed an interesting conceptual density concept for WSD.Given the WordNet as the structured hierarchical network, the conceptual density for a sense of aword is proportional to the number of contextual words that appear on a subhierarchy of theWordNet where that particular sense exists.  The correct sense can be identified as the one thathas the highest density value.Using an online dictionary, Niwa and Nitta 1994 built a reference network of words where aword as a node in the network is connected to other words that are its definitional words.  Thenetwork is used to measure the conceptual distance between words.  A word vector is defined asthe list of distances from a word to a certain set of selected words.  These selected words are notnecessarily its definitional words, but rather certain types of representational words called origins.Word similarity can then be computed by means of their distance vectors.  They compared thisproposed dictionarybased distance vector method with a corpusbased cooccurrence vectormethod for WSD and found the latter has a higher precision performance.  However, in a test ofleaning positive or negative meanings from example words, the former gave remarkable higherprecision than the latter.  Kozima and Furugori 1993 also proposed a word similarity measureby spreading activation on a semantic net composed by the online dictionary LDOCE.In the area of IR using NLP, approaches have be pursued to take advantage of the statistical termassociation results Strzalkowski and Vauthey 1992, Grefenstette 1992.  Typically, the text isfirst parsed to generated syntactic constructs.  Then the headmodifier pairs are identified forvarious syntactical structure.  Finally, a specific term association algorithm similar to the mutualinformation principle is applied to the comparison process on a single termconcept basis.Although only modest improvement has been shown, the significance of this approach is that itdoes not require any domainspecific knowledge or the sophisticated NLP techniques.  In essence,our proposed combination model is similar to this approach, except that we also resort to extraknowledge sourcesmachine readable lexical taxonomies.5.  ConclusionIn this paper, we have presented a new approach for measuring semantic similarity between wordsand concepts.  It combines the lexical taxonomy structure with corpus statistical information sothat the semantic distance between nodes in the semantic space constructed by the taxonomy canbe better quantified with the computational evidence derived from distributional analysis of corpusdata.  Specifically, the proposed measure is a combined approach that inherits the edgebasedapproach of the edge counting scheme, which is enhanced by the nodebased approach ofinformation content calculation.  When tested on a common data set of word pair similarityratings, the proposed approach outperforms other computational models.  It gives the highestcorrelation value r0.828, with a benchmark resulting from human similarity judgements,whereas an upper bound r0.885 is observed when human subjects are replicating the same task.14One obvious application of this approach is for word sense disambiguation.  In fact, this is part ofthe ongoing work.  Further applications would be in the field of information retrieval.  With thelesson learned from Richardson and Smeaton 1995, when they applied their similarity measureto free text document retrieval, it seems that the IR task would benefit most from the semanticsimilarity measures when both document and query are relatively short in length Smeaton andQuigley 1996.ReferencesAgirre, E. and G. Rigau, 1995, A proposal for Word Sense Disambiguation Using ConceptualDistance, Proceedings of the First International Conference on Recent Advanced in NLP,Bulgaria.Church, K.W. and P. Hanks, 1989, Word Association Norms, Mutual Information, andLexicography, Proceedings of the 27th Annual Meeting of the Association forComputational Linguistics, ACL2789, 7683.Grefenstette, G., 1992, Use of Syntactic Context to Produce Term Association Lists for TextRetrieval, Proceedings of the 15th Annual International Conference on Research andDevelopment in Information Retrieval, SIGIR92.Hindle, D., 1990, Noun Classification from PredicateArgument Structures, Proceedings of the28th Annual Meeting of the Association for Computational Linguistics, ACL2890, 268275.Kozima, H. and T. Furugori, 1993, Similarity Between Words Computed by SpreadingActivations on an English Dictionary, Proceedings of the 5th Conference of the EuropeanChapter of the Association for Computational Linguistics,  EACL93, 232239.Lee, J.H., M.H. Kim, and Y.J. Lee, 1993, Information Retrieval Based on Conceptual Distancein ISA Hierarchies,  Journal of Documentation, Vol. 49, No. 2, 188207.Miller, G., 1990, Nouns in WordNet A Lexical Inheritance System, International Journal ofLexicography, Vol. 3, No. 4, 245264.Miller, G., R. Beckwith, C. Fellbaum, D. Gross, and K. Miller, 1990, Introduction to WordNetAn Online Lexical Database, International Journal of Lexicography, Vol. 3, No. 4, 235244.Miller, G. and W.G. Charles, 1991, Contextual Correlates of Semantic Similarity, Languageand Cognitive Processes, Vol. 6, No. 1, 128.15Miller, G., C. Leacock, R. Tengi, and R.T. Bunker, 1993, A Semantic Concordance,Proceedings of ARPA Workshop on Human Language Technology, 303308, March 1993.Morris, J. and G. Hirst, 1991, Lexical Cohesion Computed by Thesaural Relations as anIndicator of the Structure of Text, Computational Linguistics, Vol. 17, 2148.Niwa, Y. and Y. Nitta. 1994, Cooccurrence Vectors from Corpora vs. Distance Vectors fromDictionaries, Proceedings of the 17th International Conference on computationalLinguistics, COLING94, 304309.Rada, R., H. Mili, E. Bicknell, and M. Bletner, 1989, Development and Application of a Metricon Semantic Nets,  IEEE Transactions on Systems, Man, and Cybernetics, Vol. 19, No. 1,1730.Resnik, P., 1992, WordNet and Distributional Analysis A Classbased Approach to LexicalDiscovery, Proceedings of the AAAI Symposium on Probabilistic Approaches to NaturalLanguage, San Joe, CA.Resnik, P., 1995, Using Information Content to Evaluate Semantic Similarity in a Taxonomy,Proceedings of the 14th International Joint Conference on Artificial Intelligence, Vol. 1,448453,  Montreal, August 1995.Richardson, R. and A.F. Smeaton, 1995, Using WordNet in a KnowledgeBased Approach toInformation Retrieval, Working Paper, CA0395, School of Computer Applications,Dublin City University, Ireland.Smeaton, A.F. and I. Quigley, 1996, Experiments on Using Semantic Distance Between Wordsin Image Caption Retrieval, Working Paper, CA0196, School of Computer Applications,Dublin City University, Ireland.Strzalkowski, T. and B. Vauthey, 1992, Information Retrieval Using Robust Natural LanguageProcessing, Proceedings of the 30th Annual Meeting of the Association for ComputationalLinguistics, ACL92, 104111.Sussna, M., 1993, Word Sense Disambiguation for Freetext Indexing Using a Massive SemanticNetwork, Proceedings of the Second International Conference on Information andKnowledge Management, CIKM93, 6774.
