Inductive Learning Algorithms and Representations forText CategorizationSusan DumaisMicrosoft ResearchOne Microsoft WayRedmond, WA 98052sdumaismicrosoft.comJohn PlattMicrosoft ResearchOne Microsoft WayRedmond, WA 98052jplattmicrosoft.comMehran SahamiComputer Science DepartmentStanford UniversityStanford, CA 943059010sahamics.stanford.eduDavid HeckermanMicrosoft ResearchOne Microsoft WayRedmond, WA 98052heckermamicrosoft.com1. ABSTRACTText categorization  the assignment of naturallanguage texts to one or more predefinedcategories based on their content  is animportant component in many informationorganization and management tasks.  Wecompare the effectiveness of five differentautomatic learning algorithms for textcategorization in terms of learning speed, realtime classification speed, and classificationaccuracy.  We also examine training set size,and alternative document representations.Very accurate text classifiers can be learnedautomatically from training examples.  LinearSupport Vector Machines SVMs areparticularly promising because they are veryaccurate, quick to train, and quick to evaluate.1.1 KeywordsText categorization, classification, support vector machines,machine learning, information management.2. INTRODUCTIONAs the volume of information available on the Internet andcorporate intranets continues to increase, there is growinginterest in helping people better find, filter, and managethese resources.  Text categorization  the assignment ofnatural language texts to one or more predefined categoriesbased on their content  is an important component in manyinformation organization and management tasks.  Its mostwidespread application to date has been for assigningsubject categories to documents to support text retrieval,routing and filtering.Automatic text categorization can play an important role ina wide variety of more flexible, dynamic and personalizedinformation management tasks as well realtime sorting ofemail or files into folder hierarchies topic identification tosupport topicspecific processing operations structuredsearch andor browsing or finding documents that matchlongterm standing interests or more dynamic taskbasedinterests.  Classification technologies should be able tosupport category structures that are very general, consistentacross individuals, and relatively static e.g., DeweyDecimal or Library of Congress classification systems,Medical Subject Headings MeSH, or Yahoos topichierarchy, as well as those that are more dynamic andcustomized to individual interests or tasks e.g., email aboutthe CIKM conference.In many contexts Dewey, MeSH, Yahoo, CyberPatrol,trained professionals are employed to categorize new items.This process is very timeconsuming and costly, thuslimiting its applicability.  Consequently there is increasedinterest in developing technologies for automatic textcategorization.  Rulebased approaches similar to thoseused in expert systems are common e.g., Hayes andWeinsteins CONSTRUE system for classifying Reutersnews stories, 1990, but they generally require manualconstruction of the rules, make rigid binary decisions aboutcategory membership, and are typically difficult to modify.Another strategy is to use inductive learning techniques toautomatically construct classifiers using labeled trainingdata.  Text classification poses many challenges forinductive learning methods since there can be millions ofword features.  The resulting classifiers, however, havemany advantages they are easy to construct and update,they depend only on information that is easy for people toprovide i.e., examples of items that are in or out ofcategories, they can be customized to specific categories ofinterest to individuals, and they allow users to smoothlytradeoff precision and recall depending on their task.A growing number of statistical classification and machinelearning techniques have been applied to textcategorization, including multivariate regression modelsFuhr et al., 1991 Yang and Chute, 1994 Schtze et al.,1995, nearest neighbor classifiers Yang, 1994,probabilistic Bayesian models Lewis and Ringuette, 1994,decision trees Lewis and Ringuette, 1994, neural networksWiener et al., 1995 Schtze et al., 1995, and symbolicrule learning Apte et al., 1994 Cohen and Singer, 1996.More recently, Joachims 1998 has explored the use ofSupport Vector Machines SVMs for text classificationwith promising results.In this paper we describe results from experiments using acollection of handtagged financial newswire stories fromReuters.  We use supervised learning methods to build ourclassifiers, and evaluate the resulting models on new testcases.  The focus of our work has been on comparing theeffectiveness of different inductive learning algorithmsFind Similar, Nave Bayes, Bayesian Networks, DecisionTrees, and Support Vector Machines in terms of learningspeed, realtime classification speed, and classificationaccuracy.  We also explored alternative documentrepresentations words vs. syntactic phrases, and binary vs.nonbinary features, and training set size.3. INDUCTIVE LEARNING METHODS3.1 ClassifiersA classifier is a function that maps an input attribute vector, n321 ,...x,x,xxx r , to a confidence that the input belongsto a class  that is,  xf r confidenceclass,.  In the caseof text classification, the attributes are words in thedocument and the classes correspond to text categoriese.g., typical Reuters categories include acquisitions,earnings, interest.Examples of classifiers for the Reuters category interestinclude if interest AND rate OR quarterly, thenconfidenceinterest category  0.9 confidenceinterest category  0.3interest 0.4rate  0.7quarterlySome of the classifiers that we consider decision trees,naveBayes classifier, and Bayes nets are probabilistic inthe sense that confidenceclass is a probability distribution.3.2 Inductive Learning of ClassifiersOur goal is to learn classifiers like these using inductivelearning methods.  In this paper we compared five learningmethods Find Similar a variant of Rocchios method forrelevance feedback Decision Trees Nave Bayes Bayes Nets Support Vector Machines SVMWe describe these different models in detail in section 2.4.All methods require only on a small amount of labeledtraining data i.e., examples of items in each category asinput.  This training data is used to learn parameters ofthe classification model.  In the testing or evaluation phase,the effectiveness of the model is tested on previouslyunseen instances.Learned classifiers are easy to construct and update.  Theyrequire only subject knowledge I know it when I see itand not programming or rulewriting skills.  Inductivelylearned classifiers make it easy for users to customizecategory definitions, which is important for someapplications.  In addition, all the learning methods welooked at provide graded estimates of category membershipallowing for tradeoffs between precision and recall,depending on the task.3.3 Text Representation and FeatureSelectionEach document is represented as a vector of words, as istypically done in the popular vector representation forinformation retrieval Salton  McGill, 1983.  For theFind Similar algorithm, tfidf term weights are computedand all features are used.  For the other learning algorithms,the feature space is reduced substantially as describedbelow and only binary feature values are used  a wordeither occurs or does not occur in a document.For reasons of both efficiency and efficacy, featureselection is widely used when applying machine learningmethods to text categorization.  To reduce the number offeatures, we first remove features based on overallfrequency counts, and then select a small number offeatures based on their fit to categories.  Yang and Pedersen1997 compare a number of methods for feature selection.We used the mutual information measure.  The mutualinformation MIxi, c between a feature, xi, and a category,c is defined as    1,0 1,0 ,log,,ix iiici cPxPcxPcxPcxMIWe select the k features for which mutual information islargest for each category.  These features are used as inputto the various inductive learning algorithms.  For the SVMand decisiontree methods we used k300, and for theremaining methods we used k50.  We did not rigorouslyexplore the optimum number of features for this problem,but these numbers provided good results on a trainingvalidation set so they were used for testing.3.4 Inductive Learning of Classifiers3.4.1 Find SimilarOur Find Similar method is a variant of Rocchios methodfor relevance feedback Rocchio,. 1971 which is a popularmethod for expanding user queries on the basis of relevancejudgements.  In Rocchios formulation, the weight assignedto a term is a combination of its weight in an original query,and judged relevant and irrelevant documents.rrelnonijirrelijijqj nNxnxxx,,, The parameters , , and  control the relative importanceof the original query vector, the positive examples and thenegative examples.  In the context of text classification,there is no initial query, so 0.  We also set 0 so wecould easily use available code.  Thus, for our Find Similarmethod the weight of each term is simply the average orcentroid of its weights in positive instances of the category.There is no explicit error minimization involved incomputing the Find Similar weights.  Thus, there is nolearning time so to speak, except for taking the sum ofweights from positive examples of each category.  Testinstances are classified by comparing them to the categorycentroids using the Jaccard similarity measure.  If the scoreexceeds a threshold, the item is classified as belonging tothe category.3.4.2 Decision TreesA decision tree was constructed for each category using theapproach described by Chickering et al. 1997.  Thedecision trees were grown by recursive greedy splitting, andsplits were chosen using the Bayesian posterior probabilityof model structure.  We used a structure prior thatpenalized each additional parameter with probability 0.1,and derived parameter priors from a prior network asdescribed in Chickering et al. 1997 with an equivalentsample size of 10.  A class probability rather than a binarydecision is retained at each node.3.4.3 Nave BayesA naveBayes classifier is constructed by using the trainingdata to estimate the probability of each category given thedocument feature values of a new instance.  We use Bayestheorem to estimate the probabilitiesxPcCPcCxPxcCP kkk rrr The quantity  kcCxP ris often impractical tocompute without simplifying assumptions.  For the NaveBayes classifier Good, 1965, we assume that the featuresX1,Xn are conditionally independent , given the categoryvariable C.  This simplifies the computations yielding ikik cCxPcCxP rDespite the fact the assumption of conditionalindependence is generally not true for word appearance indocuments, the Nave Bayes classifier is surprisinglyeffective.3.4.4 Bayes NetsMore recently, there has been interest in learning moreexpressive Bayesian networks Heckerman et al., 1995 aswell as methods for learning networks specifically forclassification Sahami, 1996.  Sahami, for example, allowsfor a limited form of dependence between feature variables,thus relaxing the very restrictive assumptions of the NaveBayes classifier.  We used a 2dependence Bayesianclassifier that allows the probability of each feature xi  to bedirectly influenced by the appearancenonappearance of atmost two other features.3.4.5 Support Vector Machines SVMsVapnik proposed Support Vector Machines SVMs in1979 Vapnik, 1995, but they have only recently beengaining popularity in the learning community.  In itssimplest linear form, an SVM is a hyperplane that separatesa set of positive examples from a set of negative exampleswith maximum margin  see Figure 1.Figure 1  Linear Support Vector MachineThe formula for the output of a linear SVM is,bxwu  rr where wr is the normal vector to thehyperplane, and xris the input vector.In the linear case, the margin is defined by the distance ofthe hyperplane to the nearest of the positive and negativeVXSSRUWYHFWRUVwrexamples.  Maximizing the margin can be expressed as anoptimization problem  minimize221wr   subject toibxwy ii  ,1rr where xi is the ith training exampleand yi is the correct output of the SVM for the ith trainingexample.  Of course, not all problems are linearlyseparable.  Cortes and Vapnik 1995 proposed amodification to the optimization formulation that allows,but penalizes, examples that fall on the wrong side of thedecision boundary.  Additional extensions to nonlinearclassifiers were described by Boser et al. in 1992. SVMshave been shown to yield good generalization performanceon a wide variety of classification problems, includinghandwritten character recognition LeCun et al., 1995, facedetection Osuna et al., 1997 and most recently textcategorization Joachims, 1998.  We used the simplestlinear version of the SVM because it provided goodclassification accuracy, is fast to learn and fast forclassifying new instances.Training an SVM requires the solution of a QP problemAny quadratic programming QP optimization method canbe used to learn the weights, wr, on the basis of trainingexamples.  However, many QP methods can be very slowfor large problems such as text categorization.  We used anew and very fast method developed by Platt 1998 whichbreaks the large QP problem down into a series of small QPproblems that can be solved analytically.  Additionalimprovements can be realized because the training sets usedfor text classification are sparse and binary.  Once theweights are learned, new items are classified by computingxwrr   where wr is the vector of learned weights, and xr  isthe binary vector representing the new document to classify.After training the SVM, we fit a sigmoid to the output ofthe SVM using regularized maximum likelihood fitting,  sothat the SVM can produce posterior probabilities that aredirectly comparable between categories.4. REUTERS DATA SET4.1 Reuters21578 ModApte splitWe used the new version of Reuters, the socalled Reuters21578 collection.  This collection is publicly available athttpwww.research.att.comlewisreuters21578.html.We used the 12,902 stories that had been classified into 118categories e.g., corporate acquisitions, earnings, moneymarket, grain, and interest.  The stories average about 200words in length.We followed the ModApte split in which 75 of the stories9603 stories are used to build classifiers and theremaining 25 3299 stories to test the accuracy of theresulting models in reproducing the manual categoryassignments.  The stories are split temporally, so thetraining items all occur before the test items.  The meannumber of categories assigned to a story is 1.2, but manystories are not assigned to any of the 118 categories, andsome stories are assigned to 12 categories.  The number ofstories in each category varied widely as well, ranging fromearnings which contains 3964 documents to castoroilwhich contains only one test document.  Table 1 shows theten most frequent categories along with the number oftraining and test examples in each.  These 10 categoriesaccount for 75 of the training instances, with theremainder distributed among the other 108 categories.DWHJRU1DPH 1XP7UDLQ 1XP7HVWDUQ  FTXLVLWLRQV  0RQHI  UDLQ  UXGH  7UDGH  ,QWHUHVW  6KLS  KHDW  RUQ  Table 1  Number of TrainingTest Items4.2 Summary of Inductive Learning Processfor ReutersFigure 2 summarizes the process we use for testing thevarious learning algorithms.  Text files are processed usingMicrosofts Index Server.  All features are saved along withtheir tfidf weights.  We distinguished between wordsoccurring in the Title and Body of the stories.  For the FindSimilar method, similarity is computed between testexamples and category centroids using all these features.For all other methods, we reduce the feature space byeliminating words that appear in only a single documenthapax legomena, then selecting the k words with highestmutual information with each category.  These kelementbinary feature vectors are used as input to four differentlearning algorithms. For SVMs and decision trees k300,and for the other methods, k50.Figure 2  Schematic of Learning Processtext filesword counts per filedata setDecision treeIndex ServerFeature selection Nave BayesFind similarBayes nets Support vectormachineLearning Methodstest classifierA separate classifier is learned for each category.  Newinstances are classified by computing a score andcomparing the score with a learned threshold.  Newinstances exceeding the threshold are said to belong to thecategory.  As already mentioned, all classifiers output agraded measure of category membership, so differentthresholds can be set to favor precision or recall dependingon the application  for Reuters we optimized the averageof precision and recall details below.All model parameters and thresholds are set to optimizeperformance on a validation set and are not modified duringtesting.  For Reuters, the training set contains 9603 storiesand the test set 3299 stories.  In order to decide whichmodels to use we performed initial experiments on a subsetof the training data, which we subdivided into 7147 trainingstories and 2456 validation stories for this purpose.  Weused this to set the number of features k, decisionthresholds and document representations to use for the finalruns.  We estimated parameters for these chosen modelsusing the full 9603 training stories and evaluatedperformance on the 3299 test items.  We did not furtheroptimize performance by tuning parameters to achieveoptimal performance in the test set.5. RESULTS5.1 Training TimeTraining times for the 9603 training examples varysubstantially across methods.  We tested these algorithmson a 266MHz Pentium II running Windows NT.  Unlessotherwise noted times are for the 10 largest categories,because they take longest to learn.  Find Similar is thefastest learning method 1 CPU seccategory becausethere is no explicit error minimization.  The linear SVM isthe next fastest 2 CPU secscategory.  These are bothsubstantially faster than Nave Bayes 8 CPUsecscategory, Bayes Nets 145 CPU secscategory orDecision Trees 70 CPU secscategory.  In general,performing the mutualinformation featureextraction steptakes much more time than any of the inductive learningalgorithms. The linear SVM with SMO, for example, takesan average of 0.26 CPU seconds to train a category whenaveraged over all 118 Reuters categories.The training speeds for the SVM are particularlyimpressive, since training speed has been a barrier to itswide spread applicability for large problems.  Platts SMOalgorithm is roughly 30 times faster than the popularchunking algorithm on the Reuters data set Vapnik, 1995.5.2 Classification Speed for New InstancesIn many applications, it is important to quickly classify newinstances.  All of the classifiers we explored are very fast inthis regard  all require less than 2 msec to determine if anew document should be assigned to a particular category.Far more time is spent in preprocessing the text to extracteven simple words than is spent in categorization.  With theSVM model, for example, we need only compute xwrr  ,where wris the vector of learned weights, and xr is featurevector for the new instance.  Since features are binary, thisis just the sum of up to 300 numbers.5.3 Classification AccuracyMany evaluation criteria for classification have beenproposed.  The most popular measures are based onprecision and recall.  Precision is the proportion of itemsplaced in the category that are really in the category, andRecall is the proportion of items in the category that areactually placed in the category.  We report the average ofprecision and recall the socalled breakeven point forcomparability to earlier results in text classification.  Inaddition, we plot precision as a function of recall in order tounderstand the relationship among methods at differentpoints along this curve.  Table 2 summarizes microaveraged break even performance for the 5 differentlearning algorithms for the 10 most frequent categories aswell as the overall score for all 118 categories.Support Vector Machines were the most accurate method,averaging 92 for the 10 most frequent categories and 87over all 118 categories.  Accuracy for Decision Trees was3.6 lower, averaging 88.4 for the 10 most frequentcategories.  Bayes Nets provided some performanceimprovement over Nave Bayes as expected, but theadvantages were rather small.  As has previously beenreported, all the more advanced learning algorithmsincrease performance by 1520 compared with RocchioFin dsim N B a yes B ayesN ets Trees Lin earSVMearn 92.9 95.9 95.8 97.8 98.0acq 64.7 87.8 88.3 89.7 93.6m oneyfx 46.7 56.6 58.8 66.2 74.5g rain 67.5 78.8 81.4 85.0 94.6crude 70.1 79.5 79.6 85.0 88.9trade 65.1 63.9 69.0 72.5 75.9in terest 63.4 64.9 71.3 67.1 77.7ship 49.2 85.4 84.4 74.2 85.6w heat 68.9 69.7 82.7 92.5 91.8corn 48.2 65.3 76.4 91.8 90.3Avg Top 10 64.6 81.5 85.0 88.4 92.0Avg All C at 61.7 75.2 80.0 N A 87.0Table 2  Breakeven Performance for 10 Largest Categories, and over all 118 Categories.style query expansion Find Similar.Both SVMs and Decision Trees produce very high overallclassification accuracy, and are among the best knownresults for this test collection.  Most previous results haveused the older Reuters collection, so it is difficult tocompare precisely, but 85 is the best microaveragedbreakeven point previously reported Yang, 1997.Joachims 1998 used the new collection, and our SVMresults are more accurate 87 for our linear SVM vs.84.2 for Joachims linear SVM and 86.5 for his radialbasis function network with gamma equals 0.8 and farmore efficient for both initial model learning and for realtime classification of new instances.  It is also worth notingthat Joachims chose optimal parameters based on the testdata and used only the 90 categories that have at least onetraining and test item, and our results would improve someif we did the same.  Apte, et al. 1998 have recentlyreported accuracies slightly better than ours 87.8 for asystem with 100 decision trees.  Their approach involveslearning many decision trees using an adaptive resamplingapproach boosting and is much more complex to learnthan our one simple linear classifier.The 92 breakeven point for the top 10 categoriescorresponds roughly to 92 precision at 92 recall.  Note,however, that the decision threshold can be varied toproduce higher precision at the cost of lower recall, orhigher recall at the cost of lower precision, as appropriatefor different applications.  A user would be quite happywith 92 precision for information discovery tasks, butmight want additional human confirmation before deletingimportant email messages with this level of accuracy.Figure 3 shows a representative ROC curve for the categorygrain.  The advantages of SVM can be seen over theentire recallprecision space.Figure 3  PrecisionRecall Curve for Category grainAlthough we have not conducted any formal tests, thelearned classifiers appear to be intuitively reasonable.  Forexample, the SVM representation for the category interestincludes the words prime .70, rate .67, interest .63,rates .60, and discount .46 with large positive weights,and the words group .24, year .25, sees .33 world .35, and dlrs .71 with large negative weights.5.4 Other Experiments5.4.1 Sample SizeFor an application like Reuters, it is easy to imaginedeveloping a large training corpus of the sort we workedwith e.g., a few categories had more than 1000 positivetraining instances.  For other applications, training datamay be much harder to come by.  For this reason weexamined how many positive training examples werenecessary to provide good generalization performance.  Welooked at performance for the 10 most frequent categories,varying the number of positive instances but keeping thenegative data the same.  For the linear SVM, using 100 ofthe training data 7147 stories, the microaveragedbreakeven point is 92.  For smaller training sets we tookmultiple random samples and report the average score.Using only 10 of the training sets data performance is89.6, with a 5 sample 86.2, and with a 1 sample72.6.  When we get down to a training set with only 1of the positive examples, most of the categories have fewerthan 5 training instances resulting in somewhat unstableperformance for some categories.  In general, having 20 ormore training instances provides stable generalizationperformance.While the number of examples needed per category willvary across application, we find these results encouraging.In addition, it is important to note that in mostcategorization scenarios, the distribution of instances variestremendously across categories  some categories will havehundreds or thousands of instances, and others only a few akind of Zipfs law for category size.  In such cases, themost popular categories will quickly receive the necessarynumber of training examples in the normal course ofoperation.5.4.2 Simple words vs. NLPderived phrasesFor all the results reported so far, we simply used thedefault preprocessing provided by Microsofts IndexServer, resulting in single words as index terms.  Wewanted to explore how NLP analyses might improveclassification accuracy.  For example, the phrase interestrate is more predictive of the Reuters category interestthan is either the word interest or rate.  We used NLPanalyses in a very simply fashion to aid in the extraction ofricher phases for indexing accuracy see Lewis and SparckJones, 1996 for an overview of related NLP issues.  Weconsidered factoids e.g., SalomonBrothersInternational,April8 multiword dictionary entries e.g., NewYork,interestrate noun phrases e.g., firstquarter, modestgrowth00.20.40.60.810 0.2 0.4 0.6 0.8 1Re callPrecisionLSV MDecision TreeNave BayesFind SimilarAs before, we used tfidf weights for Find Similar and themutual information criterion for selecting features for NaveBayes and SVM.  Unfortunately, the NLPderived phrasesdid not improve classification accuracy.  For the SVM, theNLP features actually reduced performance on the 118categories by 0.2  Because of these initial results, we didnot try the NLPderived phrases for Decision Trees or themore complex 2dependence Bayesian network, or use NLPfeatures in any of the final evaluations.5.4.3 Binary vs. 012 featuresWe also looked at whether moving to a richerrepresentation than binary features would improvecategorization accuracy.  To this end, we considered arepresentation that encoded words as appearing 0,1, or 2times in each document.  Initial results using thisrepresentation with Decision Tree classifiers did not yieldimproved performance, so we did not pursue this further.6. SUMMARYVery accurate text classifiers can be learned automaticallyfrom training examples, as others have shown.  Theaccuracy of our simple linear SVM is among the bestreported for the Reuters21578 collection.  In addition, themodel is very simple 300 binary features per category,and Platts SMO training method for SVMs provides a veryefficient method for learning the classifier  at least 30times faster than the chunking method for QP, and 35 timesfaster than the next most accurate classifier DecisionTrees we examined.  Classification of new items is fast aswell since we need only compute the sum of the learnedweights for features in the test items.We found that the simplest document representation usingindividual words delimited by white spaces with nostemming was at least as good as representations involvingmore complicated syntactic and morphological analysis.And, representing documents as binary vectors of words,chosen using a mutual information criterion for eachcategory, was as good as finergrained coding at least forDecision Trees.Joachims 1998 work is similar to ours in its use of SVMsfor the purpose of text categorization.  Our results aresomewhat more accurate than his, but, more importantly,based on a much simpler and more efficient model.Joachims best results are obtained using a nonlinear radialbasis function of 9962 realvalued input features based onthe popular tfidf term weights.  In contrast, we use asingle linear function of 300 binary features per category.SVMs work well because they create a classifier whichmaximizes the margin between positive and negativeexamples.  Other algorithms, such as boosting Schapire, etal., 1998, have been shown to maximize margin and arealso very effective at text categorization.We have also used SVMs for categorizing email messagesand Web pages with results comparable to those reportedhere  SVMs are the most accurate classifier and the fastestto train.  We hope to extend the text representation modelsto include additional structural information aboutdocuments, as well as knowledgebased features whichhave been shown to provide substantial improvements inclassification accuracy Sahami et al., 1998.  Finally, wewill look at extending this work to automatically classifyitems into hierarchical category structures.We believe that inductive learning methods like the ones wehave described can be used to support flexible, dynamic,and personalized information access and management in awide variety of tasks.  Linear SVMs are particularlypromising since they are both very accurate and fast.7. REFERENCES1 Apte, C., Damerau, F. and Weiss, S.  Automatedlearning of decision rules for text categorization. ACMTransactions on Information Systems, 123, 233251,1994.2 Apte, C., Damerau, F. and Weiss, S..  Text Miningwith decision rules and decision trees.  Proceedings ofthe Conference on Automated Learning and Discovery,CMU, June, 1998.3 Boser, B. E., Guyon, I. M., and Vapnik, V., A TrainingAlgorithm for Optimal Margin Classifiers. FifthAnnual Workshop on Computational Learning Theory,ACM, 1992.4 Chickering D., Heckerman D., and Meek, C.  ABayesian approach for learning Bayesian networkswith local structure.  In Proceedings of ThirteenthConference on Uncertainty in Artificial Intelligence,1997.5 Cohen, W.W. and Singer, Y.  Contextsensitivelearning methods for text categorization  In SIGIR 96Proceedings of the 19th Annual International ACMSIGIR Conference on Research and Development inInformation Retrieval, 307315, 1996.6 Cortes, C., and Vapnik, V., Support vector networks.Machine Learning, 20, 273297, 1995.7 Fuhr, N., Hartmanna, S., Lustig, G., Schwantner, M.,and Tzeras, K.  AirX  A rulebased multistageindexing system for lage subject fields.  In Proceedingsof RIAO91, 606623, 1991.8 Good, I.J.  The Estimation of Probabilities An Essayon Modern Bayesian Methods.  MIT Press, 1965.9 Hayes, P.J. and Weinstein. S.P.  CONSTRUETIS Asystem for contentbased indexing of a database ofnews stories.  In Second Annual Conference onInnovative Applications of Artificial Intelligence, 1990.10 Heckerman, D. Geiger, D. and Chickering, D.M.Learning Bayesian networks the combination ofknowledge and statistical data.  Machine Learning, 20,131163, 1995.11 Joachims, T. Text categorization with support vectormachines Learning with many relevant features.  InProceedings 10th European Conference on MachineLearning ECML, Springer Verlag, 1998.  httpwwwai.cs.unidortmund.deDOKIMENTEJoachims97a.ps.gz12 LeCun, Y., Jackel, L. D., Bottou, L., Cortes, C.,Denker, J. S., Drucker, H., Guyon, I., Muller, U. A.,Sackinger, E., Simard, P. and Vapnik, V. Learningalgorithms for classification A comparison onhandwritten digit recognition.  Neural Networks TheStatistical Mechanics Perspective, 261276, 1995.13 Lewis, D.D.. An evaluation of phrasal and clusteredrepresentations on a text categorization task. InSIGIR92 Proceedings of the 15th AnnualInternational ACM SIGIR Conference on Researchand Development in Information Retrieval, 3750,1992.14 Lewis, D.D. and Hayes, P.J. Eds. ACM Transactionson Information Systems  Special Issue on TextCategorization, 123, 1994.15 Lewis, D.D. and Ringuette, M.. A comparison of twolearning algorithms for text categorization. In ThirdAnnual Symposium on Document Analysis andInformation Retrieval, 8193, 1994.16 Lewis. D.D. and Sparck Jones. K. Natural languageprocessing for information retrieval. Communicationsof the ACM, 391, 92101, January 1996.17 Lewis, D.D., Schapire, R., Callan, J.P., and Papka, R.Training algorithms for linear text classifiers. In SIGIR96 Proceedings of the 19th Annual InternationalACM SIGIR Conference on Research andDevelopment in Information Retrieval, 298306, 1996.18 Osuna, E., Freund, R., and Girosi, F. Training supportvector machines An application to face detection.  InProceedings of Computer Vision and PatternRecognition 97, 130136, 1997.19 Platt, J. Fast training of SVMs using sequentialminimal optimization.  To appear in B. Scholkopf, C.Burges, and A. Smola Eds. Advances in KernelMethods  Support Vector Learning, MIT Press, 1998.20 Rocchio, J.J. Jr.  Relevance feedback in informationretrieval.  In G.Salton Ed., The SMART RetrievalSystem Experiments in Automatic DocumentProcessing, 313323.  Prentice Hall, 1971.21 Sahami, M. Learning Limited Dependence BayesianClassifiers. In KDD96 Proceedings of the SecondInternational Conference on Knowledge Discoveryand Data Mining, 335338, AAAI Press, 1996.httprobotics.stanford.eduuserssahamipapersdirkdd96learnbn.ps22 Sahami, M., Dumais, S., Heckerman, D., Horvitz, E.  ABayesian approach to filtering junk email.  AAAI 98Workshop on Text Categorization, July 1998.httprobotics.stanford.eduuserssahamipapersdirspam.ps23 Salton, G. and McGill, M. Introduction to ModernInformation Retrieval.  McGraw Hill, 1983.24 Schapire, R., Freund, Y., Bartlett, P. and Lee, W. S.Boosting the margin A new explanation for theeffectiveness of voting methods. Annals of Statistics, toappear, 1998.25 Schtze, H., Hull, D. and Pedersen, J.O.  A comparisonof classifiers and document representations for therouting problem.  In SIGIR 95 Proceedings of the18th Annual International ACM SIGIR Conference onResearch and Development in Information Retrieval,229237, 1995.26 Vapnik, V., The Nature of Statistical Learning Theory,SpringerVerlag, 1995.27 Wiener E., Pedersen, J.O. and Weigend, A.S. A neuralnetwork approach to topic spotting.  In Proceedings ofthe Fourth Annual Symposium on Document Analysisand Information Retrieval SDAIR95, 1995.28 Yang, Y. Expert network Effective and efficientlearning from human decisions in text categorizationand retrieval.  SIGIR 94 Proceedings of the 17thAnnual International ACM SIGIR Conference onResearch and Development in Information Retrieval,1322, 1994.29 Yang. Y. and Chute, C.G.  An examplebased mappingmethod for text categorization and retrieval.  ACMTransactions on Information Systems, 123, 252277,1994.30 Yang, Y. and Pedersen, J.O.  A comparative study onfeature selection in text categorization.  In MachineLearning Proceedings of the Fourteenth InternationalConference ICML97, 412420, 1997.31 Yang, Y.  An evaluation of statistical approaches totext categorization.  CMU Technical Report, CMUCS97127, April 1997.32 The Reuters21578 collection is available athttpwww.research.att.comlewisreuters21578.html
