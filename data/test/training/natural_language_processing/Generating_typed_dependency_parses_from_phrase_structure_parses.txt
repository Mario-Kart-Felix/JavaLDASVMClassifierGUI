Generating Typed Dependency Parses from Phrase Structure ParsesMarieCatherine de Marneffe, Bill MacCartney, and Christopher D. Manning Department of Computing Science, Universite catholique de LouvainB1340 LouvainlaNeuve, Belgium Computer Science Department, Stanford UniversityStanford, CA 94305, USAmcdm,wcmac,manningstanford.eduAbstractThis paper describes a system for extracting typed dependency parses of English sentences from phrase structure parses. In order tocapture inherent relations occurring in corpus texts that can be critical in realworld applications, many NP relations are included in theset of grammatical relations used. We provide a comparison of our system with Minipar and the Link parser. The typed dependencyextraction facility described here is integrated in the Stanford Parser, available for download.1. IntroductionWe describe a system for automatically extracting typed dependency parses of English sentences from phrase structure parses. Typed dependencies and phrase structuresare different ways of representing the structure of sentences while a phrase structure parse represents nesting of multiword constituents, a dependency parse represents dependencies between individual words. A typeddependency parse additionally labels dependencies withgrammatical relations, such as subject or indirect object.There has been much linguistic discussion of the two formalisms. There are formal isomorphisms between certainstructures, such as between dependency grammars and onebarlevel, headed phrase structure grammars Miller, 2000.In more complex theories there is significant debate dominant Chomskyan theories Chomsky, 1981 have definedgrammatical relations as configurations at phrase structure,while other theories such as LexicalFunctional Grammarhas rejected the adequacy of such an approach Bresnan,2001. Our goals here are more practical, though in essencewe are following an approach where structural configurations are used to define grammatical roles.Recent years have seen the introduction of a number oftreebanktrained statistical parsers Collins Collins, 1999,Charniak Charniak, 2000, Stanford Klein and Manning,2003 capable of generating parses with high accuracy.The original treebanks, in particular the Penn Treebank,were for English, and provided only phrase structure trees,and hence this is the native output format of these parsers.At the same time, there has been increasing interest in usingdependency parses for a range of NLP tasks, from machinetranslation to question answering. Such applications benefit particularly from having access to dependencies betweenwords typed with grammatical relations, since these provide information about predicateargument structure whichare not readily available from phrase structure parses. Perhaps partly as a consequence of this, several more recenttreebanks have adopted dependency representation as theirprimary annotation format, even if a conversion to a phrasestructure tree form is also provided e.g., the Dutch Alpinocorpus van der Beek et al., 2002 and the Danish Dependency Treebank Kromann, 2003. However, existing dependency parsers for English such as Minipar Lin, 1998and the Link Parser Sleator and Temperley, 1993 are notas robust and accurate as phrasestructure parsers trainedon very large corpora. The present work remedies this resource gap by facilitating the rapid extraction of grammatical relations from phrase structure parses. The extractionuses rules defined on the phrase structure parses.2. Grammatical relationsThis section presents the grammatical relations output byour system.The selection of grammatical relations to include in ourschema was motivated by practical rather than theoreticalconcerns. We used as a starting point the set of grammatical relations defined in Carroll et al., 1999 and King etal., 2003. The grammatical relations are arranged in a hierarchy, rooted with the most generic relation, dependent.When the relation between a head and its dependent can beidentified more precisely, relations further down in the hierarchy can be used. For example, the dependent relation canbe specialized to aux auxiliary, arg argument, or modmodifier. The arg relation is further divided into the subjsubject relation and the comp complement relation, andso on. The whole hierarchy of our grammatical relations isgiven in Figure 2.Altogether, the hierarchy contains 48 grammatical relations. While the backbone of the hierarchy is quite similar to that in Carroll et al., 1999, over time we have introduced a number of extensions and refinements to facilitate use in applications. Many NPinternal relations playa very minor role in theoretically motivated frameworks,but are an inherent part of corpus texts and can be critical in realworld applications. Therefore, besides the commonest grammatical relations for NPs amod  adjectivemodifier, rcmod  relative clause modifier, det  determiner,partmod  participial modifier, infmod  infinitival modifier,prep  prepositional modifier, our hierarchy includes thefollowing grammatical relations appos appositive modifier, nn noun compound, num numeric modifier, number element of compound number and abbrev abbreviation. The example sentence Bills on ports and immigration were submitted by Senator Brownback, Republican449Billsonprepportspobjandccimmigrationconjweresubmittednsubjpass auxpassbyprepBrownbackpobjSenatornnRepublicanapposofprepKansaspobjFigure 1 An example of a typed dependency parse for thesentence Bills on ports and immigration were submittedby Senator Brownback, Republican of Kansas.of Kansas in Figure 1 illustrates the appos relation between Brownback and Republican and the nn relationbetween Brownback and Senator. The num relationqualifies a number that serves to modify the meaning of aNP numsheep, 3 in Sam ate 3 sheep, whereas the number relation captures the internal structure of multiwordnumbers like number5, million in I lost 5 million dollars. The abbrev relation indicates that MIT is the abbreviation for Massachusetts Institute of Technology in thefollowing sentence The Massachusetts Institute of Technology MIT is located in Boston. Such information canbe useful in the context of a textual inference application,as explained below.3. Extraction methodOur technique for producing typed dependencies is essentially based on rules  or patterns  applied on phrase structure trees. The method is general, but requires appropriaterules for each language and treebank representation. Herewe present details only for Penn Treebank English, but wehave also developed a similar process for Penn TreebankChinese. The method for generating typed dependencieshas two phases dependency extraction and dependencytyping. The dependency extraction phase is quite simple.First, a sentence is parsed with a phrase structure grammar parser. Any Penn Treebank parser could be used forthe process described here, but in practice we are usingthe Stanford parser Klein and Manning, 2003, a highaccuracy statistical phrase structure parser trained on thePenn Wall Street Journal Treebank. The head of each constituent of the sentence is then identified, using rules akin tothe Collins head rules, but modified to retrieve the semantic head of the constituent rather than the syntactic head.While heads chosen for phrase structure parsing do not really matter, retrieving sensible heads is crucial for extracting semantically appropriate dependencies. For example,in relative clauses, the Collins rule will choose as head thedep  dependentaux  auxiliaryauxpass  passive auxiliarycop  copulaconj  conjunctcc  coordinationarg  argumentsubj  subjectnsubj  nominal subjectnsubjpass  passive nominal subjectcsubj  clausal subjectcomp  complementobj  objectdobj  direct objectiobj  indirect objectpobj  object of prepositionattr  attributiveccomp  clausal complement with internal subjectxcomp  clausal complement with external subjectcompl  complementizermark  marker word introducing an advclrel  relative word introducing a rcmodacomp  adjectival complementagent  agentref  referentexpl  expletive expletive theremod  modifieradvcl  adverbial clause modifierpurpcl  purpose clause modifiertmod  temporal modifierrcmod  relative clause modifieramod  adjectival modifierinfmod  infinitival modifierpartmod  participial modifiernum  numeric modifiernumber  element of compound numberappos  appositional modifiernn  noun compound modifierabbrev  abbreviation modifieradvmod  adverbial modifierneg  negation modifierposs  possession modifierpossessive  possessive modifier sprt  phrasal verb particledet  determinerprep  prepositional modifiersdep  semantic dependentxsubj  controlling subjectFigure 2 The grammatical relation hierarchy.pronoun introducing the relative clause. As all the otherwords in the relative clause will depend on the head, itmakes more sense to choose the verb as head when determining dependencies. In general, we prefer content wordsas heads, and have auxiliaries, complementizers, etc. be dependents of them. Another example concerns NPs withambiguous structure or multiple heads which are annotated450with a flat structure in the Penn TreebankNP the new phone book and tour guideUsing the Collins rule, the head for this example is the wordguide, and all the words in the NP depend on it. In order to find semantically relevant dependencies, we need toidentify two heads, book and guide. We will then getthe right dependencies the noun book still has primacyas a governing verb will link to it, but this seems reasonablennbook, phonennguide, tourCC andbook, guideamodbook, newdetbook, theIt is essential in such cases to determine heads that will enable us to find the correct dependencies.In the second phase, we label each of the dependencies extracted with a grammatical relation which is as specific aspossible. For each grammatical relation, we define one ormore patterns over the phrase structure parse tree using thetreeexpression syntax defined by tregex Levy and Andrew, 2006. Conceptually, each pattern is matched againstevery tree node, and the matching pattern with the most specific grammatical relation is taken as the type of the dependency in practice, some optimizations are used to prunethe search.Up until this point, if one assumes an extra root for thesentence, then each word token is the dependent of onething, and the number of typed dependencies in the representation is the same as the number of words in the sentence. The dependency graph is a tree a singly rooted directed acyclic graph with no reentrancies. However, forsome applications, it can be useful to regard some words,such as prepositions and conjunctions, as themselves expressing a grammatical relation. This is achieved by collapsing a pair of typed dependencies into a single typeddependency, which is then labeled with a name based onthe word between the two dependencies the word itself being excised from the dependency graph. This facility isprovided by our system, primarily targeted at prepositions,conjunctions, and possessive clitics. As already mentioned,Figure 1 shows the typed dependency parse obtained forthe sentence Bills on ports and immigration were submitted by Senator Brownback, Republican of Kansas. Figure5 gives the typed dependency parse for the same sentenceafter the collapsing process, where the dependencies related to the prepositions on and of have been collapsed,as well as the conjunct dependencies for ports and immigration. Our system optionally provides another layer ofprocessing of conjunct dependencies which aims to produce a representation closer to the semantics of the sentence. In our example, this processing will add a PREP ondependency between Bills and immigration as shownin Figure 6. An additional example of dependency structure modification is in a relative clause such as I saw theman who loves you, the dependencies ref man, who andnsubjloves, who will be extracted, as shown in Figure 3.However it might be more useful to get nsubjloves, manIsawnsubjmandobjthedetwhoref lovesrcmodrel nsubjyoudobjFigure 3 An example of a typed dependency parse for thesentence I saw the man who loves you.IsawnsubjmandobjthedetlovesrcmodwhonsubjrelyoudobjFigure 4 An example of a typed dependency parse for thesentence I saw the man who loves you, with collapsingturned on.where the relative pronoun is replaced by its actual referent. In such case the output will be the one in Figure 4.Note that as a result of this structure modification, a dependency graph may actually become cyclic, as shown inFigure 4. The usefulness of such structures depends ondownstream software being able to correctly handle cyclicdirected graphs.4. ComparisonDirect comparison between our system and other dependency parsers like Minipar and the Link Parser is compliBillsportsPREPonimmigrationCCandweresubmittednsubjpass auxpassBrownbackPREPbySenatornnRepublicanapposKansasPREPofFigure 5 A dependency parse for the sentence Bills onports and immigration were submitted by Senator Brownback, Republican of Kansas, with collapsing turned on.451BillsportsPREPonimmigrationPREPonCCandweresubmittednsubjpass auxpassBrownbackagentSenatornnRepublicanapposKansasPREPofFigure 6 A dependency parse for the sentence Bills onports and immigration were submitted by Senator Brownback, Republican of Kansas, with collapsing turned onand processing of the conjunct dependencies.cated by differences between the annotation schemes targeted by each system, presumably reflecting variation intheoretical and practical motivations. The differences fallinto two main categories dependency structure whichpairs of words are in a dependency relation and dependency typing what the grammatical relation for a particulardependency is.First, the systems do not always agree about which wordsshould be counted as the dependents of a particular governor. For example, the Link Parser has a dependency type Cwhich is described as follows C links conjunctions to subjects of subordinate clauses He left WHEN HE saw me.It also links certain verbs to subjects of embedded clausesHe SAID HE was sorry.1 This leads the Link Parser tolink that with irregularities and said with investigation in sentence 1 of table 2. In contrast, our system linkssubordinating conjunctions with the verb of the clause andmain verbs to the verb of an embedded clause in sentence1, that is linked with took place compltook place,that and said with produced ccompsaid, produced.Another example regards the word below in sentence 6the Link parser connects it with he, whereas our systemlinks it with see advmodsee, Below.Moreover, there are differences among the systems with regard to the collapsing of prepositions and coordinationas discussed above in section 3, we have tried to handlethese in a way that facilitates semantic analysis.Even where the systems agree about whether two words arein a dependency relation, they may diverge about the typeof the dependency. Each system assigns dependency typesfrom a different set of grammatical relations, and it is notstraightforward to establish mappings between these sets.Of course, the names used for relations vary considerably,and the distinctions between different relations may vary aswell. But the most salient difference between the schemesis the level of granularity. As indicated in table 1, theset of relations defined by Carroll is comparatively coarsegrained. Carrolls scheme makes a distinction between verb1A complete summary of the grammatical relations used by the Link parser can be found athttpbobo.link.cs.cmu.edulinkdictsummarizelinks.html.or noun arguments, but doesnt further distinguish amongthese. A mapping of our grammatical relations into Carrolls scheme in order to evaluate our system using CarrollsGreval test suite2 would not reflect the finer distinctions wemake. But often these finer distinctions drive success in applications. For example, our PASCAL Recognizing TextualEntailment see Section 5 derives considerable value fromrelations such as appos and abbrev.In contrast, the Link Parser uses a very finegrained setof relations, which often makes distinctions of a structuralrather than a semantic nature, as for example the MX relation which connects modifying phrases with commasto preceding nouns The DOG, a POODLE, was blackJOHN, IN a black suit, looked great. The Link Parserhas specific relations for idiomatic expressions. It also hasthree different relations for an adverb modifying anotheradverb, or an adjective, or a comparative adjective. TheLink Parser uses a different set of dependency types for dependencies appearing in questions and relative clauses. Wesuggest that many of these distinctions are too fine to beof practical value, and in our system we have aimed for anintermediate level of granularity, motivated by the needs ofpractical applications.Such differences make it difficult to directly compare thequality of the three systems. Lin Lin, 1998 proposestwo ways to evaluate the correctness of a dependency parseagainst a gold standard. In the first method, one simplyexamines whether each output dependency also occurs inthe gold standard, while ignoring the grammatical type ofthe dependency this method is therefore sensitive only tothe structure of the dependency tree. The second methodalso considers whether the type of each output dependencymatches the gold standard. But because the correctness ofa dependency parser must be evaluated according to the annotation scheme it targets, and because each parser targetsa different scheme, quantitative comparison is difficult.However, a qualitative comparison may be of value. Figures 6, 7, and 8, show a comparison of the outputs ofthe Stanford parser, MiniPar and the Link Parser respectively on the sentence Bills on ports and immigration weresubmitted by Senator Brownback, Republican of Kansas.We chose this sentence as an illustrative example because it is short but shows typical structures like prepositional phrases, coordination, and noun componding. Thegraph representing Minipar output collapses directed pathsthrough preposition nodes. It also adds antecedent linksto clone nodes between brackets. The graph for theLink Parser presents the same collapsing of directed pathsthrough preposition nodes.To provide a qualitative comparison, we parsed, with thethree parsers, ten sentences randomly chosen from theBrown Corpus. The sentences we examined are given intable 2. Globally, the Stanford parser and the Link parserlead to more accurate structure trees than Minipar. However all parsers are misled by sentence 10 where ride isanalyzed as a noun.The Stanford parser trained on the Penn Wall Street Journal2Carrolls evaluation software is available athttpwww.informatics.susx.ac.ukresearchnlpcarrollgreval.html452Scheme  GRCarroll 23MiniPar 59Link 106Stanford 47Table 1 Number of grammatical relations of four differentannotation schemes.BillsportsonBillsandpuncimmigrationconjweresubmitteds obj beSenatorbyBrownbackperson,puncRepublicanappoKansasofFigure 7 Minipar dependency parse for the sentenceBills on ports and immigration were submitted by Senator Brownback, Republican of Kansas.Treebank does a poor job at parsing questions sentences 7and 9 and the dependencies outputted are therefore wrongor not specific enough. This is easily explained by the factthat the parser is trained on the Wall Street Journal sectionof the Penn Treebank in which not many questions occur.For use in other projects, we have augmented the trainingdata with a modest number of additional questions. In sentence 8, we got depchair, out while out should be connected to sat. This link is correctly identified by bothMinipar and the Link parser.Minipar is confused by punctuation this fact has alreadybe mentioned in Lin, 1998 e.g., in sentence 5 no subject of the verb had suggested is found, and the parseroutputs only chunks of the sentence not related to one another. Minipar is also confused by conjunction in sentence3, awarding is connected with administrators, while itshould be related to appointment. An advantage of Minipar is its capacity to identify collocations as comment onin sentence 3 or how many in sentence 7.As already mentioned, the MX relation of the Link parserleads to weird dependencies in sentence 9, smoking andwaiting are dependents of tree. They should howeverbe related to Rector. The Link parser has trouble withconjunction the parse of sentence 3 is wrong. Question 9is also wrongly parsed.We evaluated our system on this sample of 10 sentences,with the collapsing option turned on. A dependencytagged as dep is considered to be wrong if a more specificdependency type should have been used. We obtained aperdependency accuracy of 80.3. However it can be onlyconsidered as a rough estimate because the sample size isvery small.BillsimmigrationonportsandwereplsubjsubmittedpasvpartbyprepafterpartBrownbackcomplofprepSenatorcnounmodpnounRepublicanmodaftercomma,punctleftKansasof.punctrightFigure 8 Link Parser dependency parse for the sentenceBills on ports and immigration were submitted by SenatorBrownback, Republican of Kansas.5. ApplicationThe typed dependency trees generated by this system havebeen used as the foundation for systems Raina et al., 2005de Marneffe et al., 2006 which were Stanfords entry inthe PASCAL Recognizing Textual Entailment RTE challenges. Here the task is to determine whether one sentencecan reasonably be inferred from another sentence. TheStanford system exploits the information about predicateargument structure encoded in the generated typed dependency trees in three ways in generating a quasilogicalrepresentation of the event structure represented by eachsentence following the work of Moldovan and Harabagiuin question answering Moldovan et al., 2003, in findinga good alignment between the structures of the two sentences, and in generating features used as input to a learning module. The Stanford system, which used the information supplied by our typed dependency extractor, attainedthe highest confidenceweighted score of all entrants in the2005 competition by a significant margin.The typed dependency generation facility describedin this paper has been integrated into the Stanfordparser, which is available for download at httpwwwnlp.stanford.edusoftwarelexparser.shtml.6. ReferencesJoan Bresnan. 2001. LexicalFunctional Syntax. Blackwell, Oxford.John Carroll, Guido Minnen, and Ted Briscoe. 1999. Corpus annotation for parser evaluation. In Proceedings of the EACLworkshop on Linguistically Interpreted Corpora LINC.Eugene Charniak. 2000. A maximumentropyinspired parser. InProceedings of NAACL2000.Noam Chomsky. 1981. Lectures on Government and Binding.Foris, Dordrecht.Michael Collins. 1999. HeadDriven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania.4531 The Fulton County Grand Jury said Friday an investigation of Atlanta s recent primary election produced noevidence that any irregularities took place.2 However, the jury said it believes these two offices should be combined to achieve greater efficiency and reducethe cost of administration.3 The jury also commented on the Fulton ordinarys court which has been under fire for its practices in the appointment of appraisers, guardians and administrators and the awarding of fees and compensation.4 When the larvae hatch, they feed on the beebread, although they also receive extra honey meals from their mother.5 In her letter to John Brown, E. B., the Quakeress from Newport, had suggested that the American people owedmore honor to John Brown for seeking to free the slaves than they did to George Washington.6 Below he could see the bright torches lighting the riverbank.7 How many pamphlets do we have in stock, Rector said.8 Then Rector, attired in his best blue serge suit, sat in a chair out on the lawn, in the shade of a tree, smoking acigarette and waiting.9 Have you any objection to the following plan10 She was watching a tree ride wildly down that roiling current.Table 2 10 sentences from the Brown Corpus, to compare outputs of Minipar, the Link Parser and the Stanford parser.MarieCatherine de Marneffe, Bill MacCartney, Trond Grenager,Daniel Cer, Anna Rafferty, and Christopher D. Manning. 2006.Learning to distinguish valid textual entailments. To appear inPASCAL RTE2 Challenge workshop.Tracy H. King, Richard Crouch, Stefan Riezler, Mary Dalrymple, and Ronald Kaplan. 2003. The PARC 700 dependencybank. In 4th International Workshop on Linguistically Interpreted Corpora LINC03.Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Meeting of the Association for Computational Linguistics.Matthias T. Kromann. 2003. The Danish Dependency Treebankand the underlying linguistic theory. In Joakim Nivre and Erhard Hinrichs, editors, Proceedings of the Second Workshop onTreebanks and Linguistic Theories TLT 2003. Vaxjo University Press.Roger Levy and Galen Andrew. 2006. Tregex andTsurgeon tools for querying and manipulatingtree data structures. In LREC 2006. httpwwwnlp.stanford.edusoftwaretregex.shtml.Dekang Lin. 1998. Dependencybased evaluation of MINIPAR.In Workshop on the Evaluation of Parsing Systems, Granada,Spain.Philip H. Miller. 2000. Strong Generative Capacity The Semantics of Linguistic Formalism. Number 46 in Lecture Notes.CSLI Publications, Stanford, CA.Dan Moldovan, Christine Clark, Sanda Harabagiu, and SteveMaiorano. 2003. Cogex A logic prover for question answering. In HLTNAACL.Rajat Raina, Andrew Y. Ng, and Christopher D. Manning. 2005.Robust textual inference via learning and abductive reasoning.In Proceedings of AAAI 2005. AAAI Press.Daniel D. Sleator and Davy Temperley. 1993. Parsing Englishwith a link grammar. In Third International Workshop on Parsing Technologies.Leonoor van der Beek, Gosse Bouma, and Robert MaloufandGertjan van Noord. 2002. The Alpino Dependency Treebank. In Computational Linguistics in the Netherlands CLIN2001.454
