International Journal of Artificial Intelligence in Education 2000 11,477847Animated Pedagogical Agents FacetoFace Interaction inInteractive Learning EnvironmentsW. Lewis Johnson and Jeff W. Rickel, Center for Advanced Research in Technology forEducation CARTE, USC Information Sciences Institute, 4676 Admiralty Way, Marina del Rey,CA 90292 USAemail johnsonisi.edu, rickelisi.eduJames C. Lester, Department of Computer Science, North Carolina State University, Raleigh,NC 276957534 USAemail lestercsc.ncsu.eduAbstract. Recent years have witnessed the birth of a new paradigm for learning environmentsanimated pedagogical agents. These lifelike autonomous characters cohabit learningenvironments with students to create rich, facetoface learning interactions. This opens upexciting new possibilities for example, agents can demonstrate complex tasks, employlocomotion and gesture to focus students attention on the most salient aspect of the task at hand,and convey emotional responses to the tutorial situation. Animated pedagogical agents offergreat promise for broadening the bandwidth of tutorial communication and increasing learningenvironments ability to engage and motivate students. This article sets forth the motivationsbehind animated pedagogical agents, describes the key capabilities they offer, and discusses thetechnical issues they raise. The discussion is illustrated with descriptions of a number ofanimated agents that represent the current state of the art.INTRODUCTION AND BACKGROUNDThis paper explores a new paradigm for education and training facetoface interaction withintelligent, animated agents in interactive learning environments. The paradigm joins twopreviously distinct research areas. The first area, animated interface agents Andr  Rist 1996,Andr 1997, Ball et al. 1997, HayesRoth  Doyle 1998, Laurel 1990, Maes 1994, Nagao Takeuchi 1994, Thorisson 1996, provides a new metaphor for humancomputer interactionbased on facetoface dialogue. The second area, knowledgebased learning environmentsCarbonell 1970, Sleeman  Brown 1982, Wenger 1987, seeks instructional software that canadapt to individual learners through the use of artificial intelligence. By combining these twoideas, we arrive at a new breed of software agent an animated pedagogical agent Lester et al.1999a, Lester, Stone,  Stelling 1999, Rickel  Johnson 1999a, Shaw, Johnson,  Ganeshan1999.Animated pedagogical agents share deep intellectual roots with previous work onknowledgebased learning environments, but they open up exciting new possibilities. As inprevious work, students can learn and practice skills in a virtual world, and the computer caninteract with students through mixedinitiative, tutorial dialogue Carbonell 1970 in the role ofa coach Goldstein 1976, Burton  Brown 1982 or learning companion Chan 1996. However,the vast majority of work on tutorial and taskoriented dialogues has focused on verbalinteractions, even though the earliest studies clearly showed the ubiquity of nonverbalcommunication in similar human dialogues Deutsch 1974. An animated agent that cohabits thelearning environment with students allows us to exploit such nonverbal communication. Theagent can demonstrate how to perform actions Rickel  Johnson 1997a. It can uselocomotion, gaze, and gestures to focus the students attention Lester et al. 1999a, Noma Badler 1997, Rickel  Johnson 1997a. It can use gaze to regulate turntaking in a mixedinitiative dialogue Cassell et al. 1994a. Head nods and facial expressions can provideJohnson, Rickel and Lester48unobtrusive feedback on the students utterances and actions without unnecessarily disruptingthe students train of thought. All of these nonverbal devices are a natural component of humandialogues. Moreover, the mere presence of a lifelike agent may increase the students arousaland motivation to perform the task well Lester et al. 1997a, Walker, Sproull,  Subramani1994. Thus, animated pedagogical agents present two key advantages over earlier work theyincrease the bandwidth of communication between students and computers, and they increasethe computers ability to engage and motivate students.Animated pedagogical agents share aspects in common with synthetic agents developed forentertainment applications Elliott  Brzezinski 1998 they need to give the user an impressionof being lifelike and believable, producing behavior that appears to the user as natural andappropriate. There are two important reasons for making pedagogical agents lifelike andbelievable. First, lifelike agents are likely to be more engaging, making the learning experiencemore enjoyable. Second, unnatural behaviors typically call attention to themselves and distractusers. As Bates et al. Bates, Loyall,  Reilly 1992 have argued, it is not always necessary foran agent to have deep knowledge of a domain in order for it to generate behavior that isbelievable. To some extent the same is true for pedagogical agents. We frequently find it usefulto give our agents behaviors that make them appear knowledgeable, attentive, helpful,concerned, etc. These behaviors may or may not reflect actual knowledge representations andmental states and attitudes in the agents. However, the need to support pedagogical interactionsgenerally imposes a closer correspondence between appearance and internal state than is typicalin agents for entertainment applications. We can create animations that give the impression thatthe agent is knowledgeable, but if the agent is unable to answer student questions and giveexplanations, the impression of knowledge will be quickly destroyed.Animated pedagogical agents also share issues with work on autonomous agents, i.e.,systems that are capable of performing tasks and achieving goals in complex, dynamicenvironments. Architectures such as RAP Firby 1994 and Soar Laird, Newell,  Rosenbloom1987 have been used to create agents that can seamlessly integrate planning and execution,adapting to changes in their environments. They are able to interact with other agents andcollaborate with them to achieve common goals Mller 1996, Tambe 1997. Pedagogicalagents must likewise exhibit robust behavior in rich, unpredictable environments they mustcoordinate their behavior with that of other agents and they must manage their own behavior ina coherent fashion, arbitrating between alternative actions and responding to a multitude ofenvironmental stimuli. Their environment includes both students and the learning environmentin which the agents are situated. Student behavior is by nature unpredictable, since students mayexhibit a variety of aptitudes, levels of proficiency, and learning styles. However, the need tosupport instruction imposes additional requirements that other types of agents do not alwayssatisfy in order to support instructional interactions, a pedagogical agent requires a deeperunderstanding of the rationales and relationships between actions than would be needed simplyto perform the task Clancey 1983.This paper lays out the motivations behind animated pedagogical agents, the keycapabilities they offer, and the technical issues they raise. Full technical accounts of individualmethods and systems can be found in the cited references.EXAMPLE PEDAGOGICAL AGENTSThis paper will make frequent reference to several implemented animated pedagogical agents.These agents will be used to illustrate the range of behaviors that such agents are capable ofproducing and the design requirements that they must satisfy. Some of these behaviors aresimilar to those found in intelligent tutoring systems, while others are quite different andunique.The USC Information Sciences Institutes Center for Advanced Research in Technology forEducation CARTE has developed two animated pedagogical agents Steve Soar TrainingExpert for Virtual Environments and Adele Agent for Distance Learning Light Edition.Steve Figure 1 is designed to interact with students in networked immersive virtualAnimated pedagogical agents facetoface interaction in interactive learning environments49environments, and has been applied to naval training tasks such as operating the engines aboardUS Navy surface ships Johnson et al. 1998, Johnson  Rickel 1998, Rickel  Johnson 1999a,Rickel  Johnson 1997b. Immersive virtual environments permit rich interactions betweenhumans and agents the students can see the agents in stereoscopic 3D and hear them speak, andthe agents rely on the virtual environments tracking hardware to monitor the students positionand orientation in the environment. Steve is combined with 3D display and interaction softwareby Lockheed Martin Stiles, McCarthy,  Pontecorvo 1995, simulation authoring software byUSC Behavioral Technologies Laboratory Munro et al. 1993, and speech recognition andgeneration software by Entropic Research to produce a rich virtual environment in whichstudents and agents can interact in instructional settings.Figure 1. SteveAdele Figure 2, in contrast, was designed to run on desktop platforms with conventionalinterfaces, in order to broaden the applicability of pedagogical agent technology. Adele runs in astudents Web browser and is designed to integrate into Webbased electronic learning materialsShaw, Johnson,  Ganeshan 1999, Shaw et al. 1999. Adelebased courses are currently beingdeveloped for continuing medical education in family medicine and graduate level geriatricdentistry, and further courses are planned for development both at the University of SouthernCalifornia and at the University of Oregon.Johnson, Rickel and Lester50Figure 2. AdeleNorth Carolina State Universitys IntelliMedia Initiative has developed three animatedpedagogical agents Herman the Bug Lester, Stone,  Stelling 1999, Cosmo Lester et al.1999a, and WhizLow Lester et al. 1999b. Herman the Bug inhabits DesignAPlant, alearning environment for the domain of botanical anatomy and physiology Figure 3. Given aset of environmental conditions, children interact with DesignAPlant by graphicallyassembling customized plants that can thrive in those conditions. Herman is a talkative, quirkyinsect that dives into plant structures as he provides problemsolving advice to students. Asstudents build plants, Herman observes their actions and provides explanations and hints. In theprocess of explaining concepts, he performs a broad range of actions, including walking, flying,shrinking, expanding, swimming, fishing, bungee jumping, teleporting, and acrobatics.Cosmo provides problemsolving advice in the Internet Protocol Advisor Figure 4.Students interact with Cosmo as they learn about network routing mechanisms by navigatingthrough a series of subnets. Given a packet to escort through the Internet, they direct it throughnetworks of connected routers. At each subnet, they may send their packet to a specified routerand view adjacent routers. By making decisions about factors such as address resolution andtraffic congestion, they learn the fundamentals of network topology and routing mechanisms.Helpful, encouraging, and with a bit of an attitude, Cosmo explains how computers areconnected, how routing is performed, and how traffic considerations come into play. Cosmowas designed to study spatial deixis in pedagogical agents, i.e., the ability of agents toAnimated pedagogical agents facetoface interaction in interactive learning environments51dynamically combine gesture, locomotion, and speech to refer to objects in the environmentwhile they deliver problemsolving advice.Figure 3. Herman the BugFigure 4. CosmoThe WhizLow agent inhabits the CPU City 3D learning environment Figure 5. CPUCitys 3D world represents a motherboard housing three principal components the RAM, theCPU, and the hard drive. It focuses on architecture including the control unit which is reducedto a simple decoder and an ALU, system algorithms such as the fetch cycle, page faults, andvirtual memory, and the basics of compilation and assembly. WhizLow can carry out studentsJohnson, Rickel and Lester52tasks by picking up data and instruction packets, dropping them off in specified locations suchas registers, and interacting with devices that cause arithmetic and comparison operations to beperformed. He manipulates address and data packets, which can contain integervaluedvariables. As soon as task specification is complete, he begins performing the students task inless than one second.Figure 5. WhizLowFigure 6. PPP PersonaAnimated pedagogical agents facetoface interaction in interactive learning environments53Andr, Rist, and Mller at DFKI the German Research Center for Artificial Intelligencehave developed an animated agent for giving online help instructions, called the PPP PersonaAndr, Rist,  Mller 1999. The agent guides the learner through Webbased materials, usingpointing gestures to draw the students attention to elements of Web pages, and providingcommentary via synthesized speech Figure 6. The underlying PPP system generatesmultimedia presentation plans for the agent to present the agent then executes the planadaptively, modifying it in real time based on user actions such as repositioning the agent on thescreen or asking followon questions.ENHANCING LEARNING ENVIRONMENTS WITH ANIMATED AGENTSThis section lists the key benefits provided by animated pedagogical agents by describing thenovel types of humancomputer interaction they support. No current agent supports all of thesetypes of interaction. Each type can significantly enhance a learning environment without theothers, and different combinations will be useful for different kinds of learning environments.To provide a summary of achievements to date, we use existing agents to illustrate each type ofinteraction. At the end of the section, we discuss some early empirical results on theeffectiveness of animated pedagogical agents.Interactive DemonstrationsA simulated mockup of a students real work environment, coupled with an animated agent thatinhabits the virtual world, provides new opportunities for teaching the student how to performtasks in that environment. Perhaps the most compelling advantage is that the agent candemonstrate physical tasks, such as operation and repair of equipment. For example, Figures 1and 7 depict Steve showing a student how to operate a High Pressure Air Compressor HPACaboard a US Navy ship. Steve integrates his demonstrations with spoken commentarydescribing objectives and actions. Figure 1 shows him providing such commentaryI will now perform a functional check of the temperature monitor to make sure that allof the alarm lights are functional. First, press the function test button. This will trip allof the alarm switches, so all of the alarm lights should illuminate.Steve then proceeds with the demonstration, as shown in Figure 7. As the demonstrationproceeds, Steve points out important features of the objects in the environment that relate to thetask. For example, when the alarm lights illuminate, Steve points to the lights and says All ofthe alarm lights are illuminated, so they are all working properly.Demonstrating a task may be far more effective than trying to describe how to perform it,especially when the task involves spatial motor skills, and the experience of seeing a taskperformed is likely to lead to better retention. Moreover, an interactive demonstration given byan agent offers a number of advantages over showing students a videotape. Students are free tomove around in the environment and view the demonstration from different perspectives. Theycan interrupt with questions, or even ask to finish the task themselves, in which case Steve willmonitor the students performance and provide assistance. Also, Steve is able to construct andrevise plans for completing a task, so he can adapt the demonstration to unexpected events. Thisallows him to demonstrate the task under different initial states and failure modes, as well ashelp the student recover from errors.The utility of agent demonstrations is not restricted to teaching physical tasks that thestudent must perform. Agents can also demonstrate procedures performed by complex devicesby taking on the role of an actor in a virtual process. For example, WhizLow, the agent in theCPU City learning environment, demonstrates computational procedures to teach novices thefundamentals of computer architecture. As he transports data packets and addresses packets tothe CPU, RAM, and hard drive, WhizLow teaches students how fetchexecute cycle algorithmswork. In contrast to Steamerstyle interactions Hollan, Hutchins,  Weitzman 1984, Stevens,Johnson, Rickel and Lester54Roberts,  Stead 1983 in which knowledgebased simulations guide the actions in a simulatedworld, learning environments in which the instructions are provided by lifelike charactersprovide a visual focus and an engaging presence that are sometimes absent from their agentlesscounterparts.Figure 7. Steve pressing a button on the HPAC consoleNavigational GuidanceWhen a students work environment is large and complex, such as a ship, one of the primaryadvantages of a virtual mockup is to teach the student where things are and how to get around.In this context, animated agents are valuable as navigational guides, leading students around andpreventing them from becoming lost. For example, Steve inhabits a complex shipboardenvironment, including multiple rooms. The engine room alone is quite complex, with the largeturbine engines that propel the ship, several platforms and pathways around and into theengines, a console, and a variety of different parts of the engines that must be manipulated, suchas valves. As Steve demonstrates tasks, he leads students around this environment, showingthem where relevant objects are and how to get to them. Because Steve has an internalrepresentation of the spatial layout of the ship see Interface to the Environment section, he isalways able to plan the shortest path from his current location to the next relevant object.Leading someone down a hallway, up a flight of stairs, around a corner, and through some pipesto the valve they must turn is likely to be more effective than trying to tell them where the valveis located. Our experience in training people using immersive virtual reality has shown thatstudents can easily become disoriented and lost in complex environments, so animated agentsthat can serve as guides are an important instructional aid.By enabling students to participate in immersive experiences, 3D learning environmentswith navigational guides can help students develop spatial models of the subject matter, even ifthese environments present worlds that the student will never occupy. For example, the CPUCity environment depicts a virtual computer that the student can travel through and interact withto acquire a mental model of the workings of a computer. Similar experiences could be providedby learning environments that offer students tours of civilizations long past, e.g., the wonders ofancient Greece, or of virtual museums housing the worlds masterpieces. Accompanied byAnimated pedagogical agents facetoface interaction in interactive learning environments55knowledgeable guides, students can travel through these virtual worlds to learn about a varietyof domains that lend themselves to spatial exploratory metaphors.Although Steve and WhizLow both inhabit 3D worlds, an animated navigational guide mayeven be useful in 2D environments. For example, the CAETI Center Associate Murray1997serves as a Webbased guide to a large collection of intelligent tutoring system projects. Avirtual building houses these projects in individual rooms. When a user first enters the world,the CAETI guide interviews her about her interests to construct a customized itinerary. It thenescorts her from room to room project to project based on her interests. While the guidesdescribed above help students navigate 3D worlds, the CAETI Associate demonstrates that 2Dworlds may also benefit from the presence of animated agents.Gaze and Gesture as Attentional GuidesBecause of significant advances in the capabilities of graphics technologies in the past decade,tutoring systems increasingly incorporate visual aids. These range from simple maps or chartsthat are automatically generated Mittal et al. 1995 to 3D simulations of physical phenomenasuch as electromagnetic interactions in physics Towns, Callaway,  Lester 1998 and fullscale3D simulated worlds such as the ship that Steve inhabits. To draw students attention to aspecific aspect of a chart, graphic or animation, tutoring systems make use of many devices,such as arrows and highlighting by color. An animated agent, however, can guide a studentsattention with the most common and natural methods gaze and deictic gesture.Steve uses gaze and deictic gestures in a variety of ways. He points at objects whendiscussing them. He looks at an object immediately before manipulating or pointing at it. Helooks at objects when they are manipulated by students or other agents. He looks at an objectwhen checking its state e.g., to see whether a light is on or a reservoir is full. He looks at astudent or another agent when waiting for them, listening to them, or speaking to them. Steve iseven capable of tracking moving objects for example, if something e.g., the student is movingcounterclockwise around Steve, he will track it over his left shoulder until it moves directlybehind him, at which point he will track it over his right shoulder.Figure 8. Adele looking at the students mouse selectionAgents can employ deictic behaviors to create contextspecific references to physicalobjects in virtual worlds. In the same manner that humans refer to objects in their environmentthrough judicious combinations of speech, locomotion, and gesture, animated agents can movethrough their environment, point to objects, and refer to them appropriately as they provideproblemsolving advice. An agent might include some or all of these capabilities. For example,to produce deictic references to particular objects under discussion, the Edward systemJohnson, Rickel and Lester56Claassen 1992 employs a stationary persona that grows a pointer to a particular object in theinterface. Similarly, the PPP Persona is able to dynamically indicate various onscreen objectswith an adjustable pointer Figure 6. Adele is able to point toward objects on the screen, andcan also direct her gaze toward them Figure 8 shows her looking at the students mouseselection. The Cosmo agent employs a deictic behavior planner that exploits a simple spatialmodel to select and coordinate locomotive, gestural, and speech behaviors. The planner enablesCosmo to walk to, point at, and linguistically refer to particular computers in its virtual world asit provides students with problemsolving advice.Noma and Badlers Presenter Jack Noma  Badler 1997, shown in Figure 9, exhibits avariety of different deictic gestures. Like Steve and Cosmo, Presenter Jack can use his indexfinger to point at individual elements on his visual aid. He can also point with his palm facingtowards the visual aid to indicate a larger area, and he can move his hand to indicate a flow on amap or chart. He also smoothly integrates these gestures into his presentation, moving over tothe target object before his speech reaches the need for the deictic gesture, and dynamicallychoosing the best hand for the gesture based on a heuristic that minimizes both visual aidocclusion and the distance from the current body position to the next one in the presentation.Figure 9. Presenter Jack pointing at a weather patternNonverbal FeedbackOne primary role of a tutor is to provide feedback on a students actions. In addition toproviding verbal feedback, an animated agent can also use nonverbal communication toinfluence the student. For example, Steve uses a nod of approval to show agreement with astudents actions and shakes his head to indicate disapproval. Adele nods or smiles to indicateagreement with the students actions, presents a look of puzzlement when the student makes anerror, and shows pleasant surprise when the student finishes their task. Moreover, bodylanguage can help indicate to students that they have just committed or are on the verge ofcommitting a very serious error. This can make a strong impression on them.The ability to use nonverbal feedback in addition to verbal comments allows an animatedagent to provide more varied degrees of feedback than earlier tutoring systems. Nonverbalfeedback through facial expressions may often be preferable because it is less obtrusive than aAnimated pedagogical agents facetoface interaction in interactive learning environments57verbal comment. For example, a simple nod of approval can reassure a student withoutinterrupting them. Similarly, human tutors often display a look of concern or puzzlement tomake a student think twice about their actions in cases where either they are unsure that thestudent has actually made a mistake or they do not want to interrupt with a verbal correction yet.While some occasions call for these types of unobtrusive feedback, other occasions may call formore exaggerated feedback than a verbal comment can offer. For example, when studentssuccessfully complete design problems in the DesignAPlant learning environment, theanimated agent Herman sometimes congratulates them by cartwheeling across the screen. Inthe Internet Advisor, Cosmo employs stylized animations Culhane 1988 in contrast to lifequality animations for nonverbal feedback. For example, when a student solves a problem,Cosmo smiles broadly and uses his entire body to applaud her success.Conversational SignalsWhen people carry on facetoface dialogues, they employ a wide variety of nonverbal signalsto help regulate the conversation and complement their verbal utterances. While tutorialdialogue in most previous tutoring systems resembles Internet chat or a phone conversation,animated pedagogical agents allow us to more closely model the facetoface interactions towhich people are most accustomed. Some nonverbal signals are closely tied to spokenutterances, and could be used by any animated agent that produces speech output. For example,intonational pitch accents indicate the degree and type of salience of words and phrases in anutterance, including rhematic i.e., new elements of utterances and contrastive elements Pierrehumbert  Hirschberg 1990 to further highlight such utterance elements, a pitch accentis often accompanied by a short movement of the eyebrows or head, a blink of the eyes, andor abeat gesture i.e., a short batonlike movement of the hands Cassell et al. 1994a. As anotherexample, facial displays can provide the speakers personal judgement of the accompanyingutterance e.g., a scrunched nose to indicate distaste for the subject Cassell et al. 1994a.Other nonverbal signals help regulate the flow of conversation, and would be most valuablein tutoring systems that support speech recognition as well as speech output, such as Steve orthe Circuit FixIt Shop Smith  Hipp 1994. This includes backchannel feedback, such ashead nods to acknowledge understanding of a spoken utterance. It also includes the use of eyecontact to regulate turn taking in mixedinitiative dialogue. For example, during a pause, aspeaker will either break eye contact to retain the floor or make eye contact to request feedbackor give up the floor Cassell et al. 1994a. Although people can clearly communicate in theabsence of these nonverbal signals e.g., by telephone, communication and collaborationproceed most smoothly when they are available.Several projects have made serious attempts to draw on the extensive psychological andsociological literature on human nonverbal conversational behavior. Pelachaud et al.Pelachaud, Badler,  Steedman 1996 developed a computational model of facial expressionsand head movements of a speaker. Cassell et al. Cassell et al. 1994a, Cassell et al. 1994bdeveloped perhaps the most comprehensive computational model of nonverbal communicativebehavior. Their agents coordinate speech, intonation, gaze, facial expressions, and a variety ofgestures in the context of a simple dialogue. However, their agents do not converse withhumans their algorithm simply generates an animation file for a facetoface conversationbetween two computer characters, Gilbert and George Figure 10, using the Jack human figuresoftware Badler, Phillips,  Webber 1993. In contrast, the Gandalf agent Figure 11 supportsfull multimodal conversation between human and computer Thorisson1996, Cassell Thorisson 1999. Like other systems, Gandalf combines speech, intonation, gaze, facialexpressions, and a few gestures. Unlike most other systems, Gandalf also perceives thesecommunicative signals in humans people talking with Gandalf wear a suit that tracks theirupper body movement, an eye tracker that tracks their gaze, and a microphone that allowsGandalf to hear their words and intonation. Although none of these projects has specificallyaddressed tutorial dialogues, they contribute significantly to our understanding ofcommunication with animated agents.Johnson, Rickel and Lester58Figure 10. Animated ConversationFigure 11. Gandalf speaking with a userConveying and Eliciting EmotionMotivation is a key ingredient in learning, and emotions play an important role in motivation.By employing a computational model of emotion, animated agents can improve studentslearning experiences in several ways Elliott, Rickel,  Lester 1999. First, an agent thatappears to care about a students progress may encourage the student to care more about herown progress. Second, an emotive pedagogical agent may convey enthusiasm for the subjectAnimated pedagogical agents facetoface interaction in interactive learning environments59matter and thereby foster similar levels of enthusiasm in the learner. Finally, a pedagogicalagent with a rich and interesting personality may simply make learning more fun. A learner thatenjoys interacting with a pedagogical agent may have a more positive perception of the overalllearning experience and may consequently opt to spend more time in the learning environment.Perhaps as a result of the inherent psychosocial nature of studentagent interactions and ofhumans tendency to anthropomorphize software Reeves  Nass 1998, recent evidencesuggests that tutoring systems with lifelike characters can be pedagogically effective Lester etal. 1997b while at the same time having a strong motivating effect on students Lester et al.1997a. It is even becoming apparent that particular features e.g., personal characteristics oflifelike agents can have an important impact on learners acceptance of them Hietala Niemirepo 1998. As master animators have discovered repeatedly over the past century, thequality, overall clarity, and dramatic impact of communication can be increased through thecreation of emotive movement that underscores the affective content of the message to becommunicated Noake 1988, Jones 1989, Lenburg 1993, Thomas  Johnston 1981. Bycarefully orchestrating facial expression, body placement, arm movements, and hand gestures,animated pedagogical agents could visually augment verbal problemsolving advice, giveencouragement, convey empathy, and perhaps increase motivation. For example, the Cosmoagent employs a repertoire of fullbody emotive behaviors to advise, encourage, and appearto empathize with students. When a student makes a suboptimal problemsolving decision,Cosmo informs the student of the illeffect of her decision as he takes on a sad facial expressionand slumping body language while dropping his hands. As computational models of emotionbecome more sophisticated, e.g., Elliott 1992, animated agents will be well positioned toimprove students motivation.Virtual TeammatesComplex tasks often require the coordinated actions of multiple team members. Team tasks areubiquitous in todays society for example, teamwork is critical in manufacturing, in anemergency room, and on a battlefield. To perform effectively in a team, each member mustmaster their individual role and learn to coordinate their actions with their teammates.Distributed virtual reality provides a promising vehicle for training teams students, possibly atdifferent locations, cohabit a virtual mockup of their work environment, where they canpractice together in realistic situations. In such training, animated agents can play two valuableroles they can serve as instructors for individual students, and they can substitute for missingteam members, allowing students to practice team tasks when some or all human instructors andteammates are unavailable.Steve supports this type of training Rickel  Johnson 1999b. The team can consist of anycombination of Steve agents and human students, each assigned a particular role in the teame.g., officer of the watch or propulsion operator. Each student is accompanied by an instructorhuman or agent that coaches them on their role. Each person sees each other person in thevirtual world as a head and two hands the head is simply a graphical model, so each person canhave a distinct appearance, possibly with their own face texturemapped onto the graphicalhead. To distinguish different agents, each agent can be configured with its own shirt, hair, eye,and skin color, and its voice can be made distinct by setting its speech rate, baseline pitch, andvocal tract size. Thus, students can easily track the activities of their teammates. Team memberscommunicate through spoken dialogue, and Steve agents also incorporate valuable nonverbalcommunication they look at a teammate when waiting for them or speaking to them, they reactto their teammates actions, and they nod in acknowledgment when they understand something ateammate says to them. Each Steve agents behavior is guided by a task representation thatspecifies the overall steps in the task as well as how various team members interact and dependupon each other.In addition to serving as teammates, animated pedagogical agents could serve as othertypes of companions for students. Chan and Baskin Chan  Baskin 1990 developed asimulated learning companion, which acts as a peer instead of a teacher. DillenbourgDillenbourg 1996 investigated the interaction between real students and computersimulatedJohnson, Rickel and Lester60students as a collaborative social process. Chan Chan 1996 has investigated other types ofinteractions between students and computer systems, such as competitors or reciprocal tutors.Frasson et al. Frasson et al. 1996 have explored the use of an automated troublemaker, alearning companion that sometimes provides incorrect information in order to check, andimprove, the students selfconfidence. None of these automated companions appears as ananimated character, although recent work by Ameur et al. Ameur et al. 1997 has explored theuse of a 2D face with facial expressions for the troublemaker. However, since all these effortsshare the perspective of learning as a social process, this seems like a natural direction for futureresearch.Adaptive Pedagogical InteractionsIn addition to the types of interactions described above, animated pedagogical agents need to becapable of many of the same pedagogical abilities as other intelligent tutoring systems. Forinstance, it is useful for them to be able to answer questions, generate explanations, ask probingquestions, and track the learners skill levels. An animated pedagogical agent must be able toperform these functions while at the same time responding to the learners actions. Thus thecontext of facetoface interaction has a pervasive influence on the pedagogical functionsincorporated in an animated pedagogical agent pedagogy must be dynamic and adaptive, asopposed to deliberate, sequential, or preplanned. For example, Steve adapts his demonstrationsin midstream if the student performs actions that interact with the demonstration he alsoresponds to student interruptions. Similarly, the PPP Persona seamlessly integrates reactivebehaviors responding to user inputs with planned presentations.The ability to deliver opportunistic instruction, based on the current situation, is a commontrait of animated pedagogical agents. Herman the Bug, for example, makes extensive use ofproblem solving contexts as opportunities for instruction. When the student is working onselecting a leaf to include in a plant, Herman uses this as an opportunity to provide instructionabout leaf morphology. Adele constantly assesses the current situation, using the situation spacemodel of Marsella and Johnson Marsella  Johnson 1998, and dynamically generates adviceappropriate to the current situation. Another type of opportunistic instruction provided by Adeleis suggesting pointers to online medical resources that are relevant to the current stage of thecase workup. For example, when the student selects a diagnostic procedure to perform on thesimulated patient, Adele may point the student to video clips showing how the procedure isperformed.Preliminary Empirical ResultsBecause animated pedagogical agent technologies are still very much in their infancy, little isknown empirically about their effectiveness in learning environments. As discussed in the nextsection, nearly every major facet of their communicative abilities needs considerable research.For this reason, it is much too early in their development to conduct comprehensive, definitiveempirical studies that demonstrate their effectiveness in learning environments. Because theircommunicative abilities are still very limited compared to what we expect they will be in thenear future, the results of such studies will be skewed by the immaturity of the technology.Despite this caveat, it is essential to make an initial foray into assessing their impact onlearning, and several studies have been undertaken with this objective in mind. Below wesummarize the results of several representative studies1.The largest formal empirical study of an animated pedagogical agent to date was conductedwith Herman the Bug in the DesignAPlant learning environment Lester et al. 1997b.Researchers wanted to obtain a baseline reading on the potential effectiveness of animatedpedagogical agents and examine the impact of various forms of agents advice. They conducteda study with one hundred middle school students in which each student interacted with one ofseveral versions of the Herman agent. The different versions varied along two dimensions. First,                                                     1 Complete descriptions of the experimental methods and analyses are contained in the cited papers.Animated pedagogical agents facetoface interaction in interactive learning environments61different versions of Herman employed different modalities some provided only visual advice,some only verbal advice, and some provided combinations of the two. Second, differentversions provided different levels of advice some agents provided only highlevel principlebased advice, others provided lowlevel taskspecific advice, and some were completelymute. During the interactions, the learning environment logged all problemsolving activities,and the students were given rigorous pretests and posttests. The results of the study werethreefoldBaseline ResultStudents interacting with learning environments with an animated pedagogical agent showstatistically significant increases from pretests to posttests. Some critics have suggestedthat animated agents could distract students and hence prevent learning. This findingestablishes that a welldesigned agent in a welldesigned learning environment can createsuccessful learning experiences.MultiLevel, MultiModality EffectsAnimated pedagogical agents that provide multiple levels of advice combining multiplemodalities yield greater improvements in problem solving than less expressive agents. Thisfinding indicates that there may be important learning benefits from introducing animatedagents that employ both visual animated and auditory verbal modalities to give bothpractical and theoretical advice.Complexity BenefitsThe benefits of animated pedagogical agents increase with problemsolving complexity. Asstudents are faced with more complex problems, the positive effects of animatedpedagogical agents on problem solving are more pronounced. This finding suggests thatagents may be particularly effective in helping students solve complex technical problemsas opposed to simple toy problems.The DesignAPlant study also revealed the persona effect Lester et al. 1997a the verypresence of a lifelike character in an interactive learning environment can have a strong positiveeffect on learners perception of their learning experience. The study also demonstrated animportant synergistic effect of multiple types of explanatory behaviors on students perceptionof agents agents that are more expressive both in modes of communication and in levels ofadvice are perceived as having greater utility and communicating with greater clarity.In a separate study, the PPP research team conducted an experiment to evaluate the degreeto which their PPP agent contributes to learning Andr, Rist,  Mller 1999. To this end, theycreated two versions of their learning environment software, one with the PPP Persona and onewithout. The latter uses identical narration and uses an arrow for deictic reference. Each subjectall of them adults viewed several presentations some presentations provided technicalinformation descriptions of pulley systems while others provided nontechnical informationdescriptions of office employees. Unlike the DesignAPlant study, the subjects in this studydid not perform any problem solving under the guidance of the agent. The results indicate thatthe presence of the animated agent made no difference to subjects comprehension of thepresentations. This finding neither supports nor contradicts the DesignAPlant study, which didnot involve an agent vs. noagent comparison, and which involved a very different learningenvironment. However, 29 out of 30 subjects in the PPP study preferred the presentations withthe agent. Moreover, subjects found the technical presentations but not the nontechnicalpresentations significantly less difficult and more entertaining with the agent. This result isconsistent with the persona effect found in the DesignAPlant study.Johnson, Rickel and Lester62It is important to emphasize that both of these studies were conducted with agents thatemployed first generation animated pedagogical agent technologies. All of theircommunicative capabilities were very limited compared to the level of functionality that isexpected to emerge over the next few years, and Herman and the PPP Persona only employ afew of the types of interaction that have been discussed in this paper. As animated pedagogicalagents become more sophisticated, it will be critical to repeat these experiments en route to acomprehensive, empiricallybased theory of animated pedagogical agents and learningeffectiveness.TECHNICAL ISSUESAnimated pedagogical agents share many technical issues with previous work in intelligenttutoring systems and interactive learning environments, including representing and reasoningabout domain knowledge, modeling and adapting to the students knowledge, choosingappropriate pedagogical strategies, and maintaining a coherent tutorial dialogue. However, justas animated agents raise new instructional opportunities, as described in the last section, theyalso pose new technical challenges. This section outlines the key challenges and some of therelevant work to date in addressing them.Interface to the EnvironmentViewed as an autonomous agent, an animated pedagogical agents environment includes thelearning environment e.g., anything from a 3D virtual world to a simple 2D Web interface, thestudents, and any other agents in the learning environment. Before discussing the innerworkings of such an agent, it is helpful to discuss the interface between the agent and thisenvironment. The interface can be divided into two parts the agents awareness of theenvironment its perception, and its ability to affect the environment its motor actions.One of the primary motivations for animated pedagogical agents is to broaden the bandwidth ofhumancomputer interaction, so their perception and motor actions are typically more diversethan previous computer tutors and learning companions.Animated pedagogical agents share some types of perception with earlier tutoring systems.Most track the state of the problem the student is addressing. For example, Steve tracks the stateof the simulated ship, Adele tracks the state of the simulated patient, and Herman maintains arepresentation of the environment for which the student is designing a plant. Most track thestudents problemsolving actions. For example, Steve knows when the student manipulatesobjects e.g., pushes buttons or turns knobs, Adele knows when the student questions orexamines the patient e.g., inspects a lesion or listens to the heart, and Herman knows when thestudent extends the plant design e.g., chooses the type of leaves. Finally, most allow thestudent to ask them questions. For example, students can ask Steve and Adele what they shoulddo next and why, they can ask Herman and Cosmo for problemsolving assistance, and they canask WhizLow to perform a task that they have designed for him.In addition, some agents track other, more unusual events in their environment. Some trackadditional speech events. When an external speech synthesizer is used to generate the agentsvoice, the agent must receive a message indicating when speech is complete, and the agent mayreceive interim messages during speech output specifying information such as the appropriateviseme for the current phoneme for lip synchronization or the timing of a pitch accent forcoordinated use of a beat gesture, a head movement, or raised eyebrows. To maintainawareness of when others are speaking, the agent may receive messages when the studentbegins and finishes speaking e.g., from a speech recognition program and when other agentsbegin or finish speaking from their speech synthesizers, as well as a representation of whatwas said. Some agents, such as Steve, track the students location in the virtual world, andagents for team training may track the locations of other agents. Some track the students visualattention. For example, Steve gets messages from the virtual reality software indicating whichobjects are within the students field of view, and he pauses his demonstrations when the studentAnimated pedagogical agents facetoface interaction in interactive learning environments63is not looking in the right place. Gandalf tracks the students gaze as a guide to conversationalturn taking, and he also tracks their gestures. It is very likely that future pedagogical agents willtrack still other features, such as students facial expressions Cohn et al. 1998 and emotionsPicard 1997.Interactions between an agents body and its environment require spatial knowledge of thatenvironment. As described in the section Enhancing Learning Environments with AnimatedAgents, such interactions are a key motivation for animated pedagogical agents, including theability to look at objects, point at them, demonstrate how to manipulate them, and navigatearound them. Relatively simple representations of spatial knowledge have sufficed to supportthe needs of animated pedagogical agents to date. For example, Herman maintains a simplerepresentation of the students task bar location in the DesignAPlant environment so he canconduct his activities e.g., standing, sitting, walking appropriately on the screen. Agents suchas the PPP Persona that point at elements of bitmapped images need the screen location of thereferenced elements. Cosmo maintains a similar representation of the locations of objects on thescreen so he can perform his deictic locomotion and gestural behaviors he also uses thisknowledge for selecting appropriate referring expressions.Agents that inhabit 3D worlds require still richer representations. Steve relies on the virtualreality software to provide bounding spheres for objects, thus giving him knowledge of anobjects position and a coarse approximation of its spatial extent for purposes of gaze and deicticgesture. Steve also requires a vector pointing at the front of each object from which hedetermines where to stand and, to support object manipulation, vectors specifying the directionto press or grasp each object. WhizLow maintains knowledge about the physical properties ofvarious objects and devices. For example, the representation encodes knowledge that datapackets can be picked up, carried, and deposited in particular types of receptacles and that leverscan be pulled.Agents in 3D environments may need additional knowledge to support collisionfreelocomotion. Steve represents the world as an adjacency graph each node in the graph representsa location, and there is an edge between two nodes if there is a collisionfree path directlybetween them. To move to a new location, he uses Dijkstras shortest path algorithm Cormen,Leiserson,  Rivest 1989 to identify a collisionfree path. In contrast, WhizLows navigationplanner first invokes the A algorithm to determine an approximate collisionfree path on a 2Drepresentation of the 3D worlds terrain. However, this only represents an approximate pathbecause it is found by searching through a discretized representation of the terrain. It is criticalthat control points, i.e., the coordinates determining the actual path to be navigated, beinterpolated in a manner that 1 enables the agents movement to appear smooth and continuousand 2 guarantees retaining the collisionfree property. To achieve this natural behavior, thenavigation planner generates a Bezier spline that interpolates the discretized path from theavatars current location, through each successive control point, to the target destination.To affect their environment, pedagogical agents need a repertoire of motor actions. Thesegenerally fall into three categories speech, control of the agents body, and control of thelearning environment. Speech is typically generated as a text string to speak to a student oranother agent. This string might be displayed as is or sent to a speech synthesizer. Control of theagents body may involve playing existing animation clips for the whole body or may bedecomposed into separate motor commands to control gaze, facial expression, gestures, objectmanipulations, and locomotion. This issue is discussed in further detail in the BehavioralBuilding Blocks section. Finally, the agent may need to control the learning environment. Forexample, to manipulate an object, Steve sends a message to the virtual reality software togenerate the appropriate motions of his body and then sends a separate message to the simulatorto cause the desired change e.g., to push a button. Actions in the environment are not restrictedto physical behaviors directly performed by the agent. For example, Herman changes thebackground music to reflect the students progress. To contextualize the score, he tracks thestate of the task model and sequences the elements of the music so that, as progress is madetoward successful completion of subtasks, the number of musical voices added increases.For modularity, it is useful to insulate an agents cognitive capabilities from the details ofits motor capabilities. For example, Steves cognitive module, which controls his behavior,Johnson, Rickel and Lester64outputs abstract motor commands such as look at an object, move to an object, point at anobject, manipulate an object in various ways, and speak to someone. A separate motor controlmodule decomposes these into detailed messages sent to the simulator, the virtual realitysoftware, and the speech synthesizer. This layered approach means that Steves cognition isindependent of the details of these other pieces of software, and even of the details of Stevesbody. Because this architecture makes it easy to plug in different bodies, we can evaluate thetradeoffs among them. Steve uses a similarly layered approach on the perception side, toinsulate the cognitive module from the particular types of input devices used.Behavioral Building BlocksDesigning the behavior of an agent requires addressing two issues. This section addresses thefirst issue designing the building blocks from which the agents behavior will be generated. Thenext section discusses the second issue developing the code that will select and combine theright building blocks to respond appropriately to the dynamically unfolding tutorial situation.Behavior SpacesThe behavior space approach is the most common method for generating the behavior of apedagogical agent. A behavior space is a library of behavior fragments. To generate thebehavior of the agent, a behavior sequencing engine dynamically strings these fragmentstogether at runtime. When this is done well, the agents behavior appears seamless to the studentas it provides visually contextualized problemsolving advice.Figure 12 illustrates the basic idea. It shows a behavior space with three types of behaviorfragments visual segments serving as the agents repertoire of movements depicted in thefigure as a drawing of the character, audio clips serving as the agents repertoire of utterancesdepicted as an audio wave, and segments of background music depicted as a musical note.The arrows in the behavior space represent the behavior fragments selected by the behaviorsequencing engine for a particular interaction with the student, and the lower section of thefigure shows how the engine combines them to generate the agents behavior and accompanyingmusic.Figure 12. Behavior SpaceAnimated pedagogical agents facetoface interaction in interactive learning environments65Creating the behavior fragments for a behavior space can range from simple to quitecomplex depending on the desired quality of animation. Musical segments are simply audioclips of different varieties of music to create different moods, and utterance segments aretypically just voice recordings. A visual segment of the agent could be a simple bitmap image ofthe agent in a particular pose, a graphical animation sequence of the agent moving from onepose to another, or even an image or video clip of a real person. All three approaches have beenused in existing pedagogical agents.To allow the behavior sequencing engine to select appropriate behavior fragments atruntime, each fragment must be associated with additional information describing its content.For example, behavior fragments in the behavior space for Herman the Bug are indexedontologically, intentionally, and rhetorically. An ontological index is imposed on explanatorybehaviors. Each behavior is labeled with the structure and function of the aspects of the primarypedagogical object that the agent discusses in that segment. For example, explanatory segmentsin Hermans behavior space are labeled by 1 the type of botanical structures discussed, e.g.,anatomical structures such as roots, stems, and leaves, and by 2 the physiological functionsthey perform, e.g., photosynthesis. An intentional index is imposed on advisory behaviors.Given a problemsolving goal, intentional indices enable the sequencing engine to identify theadvisory behaviors that help the student achieve the goal. For example, one of Hermansbehaviors indicates that it should be exhibited when a student is experiencing difficulty with alow water table environment. Finally, a rhetorical index is imposed on audio segments. Thisindicates the rhetorical role played by each clip, e.g., an introductory remark or interjection.The following example of behavior sequencing in Herman the Bug illustrates this process.If Herman intervenes in a lesson, say because the student is unable to decide on a leaf type, thebehavior sequencing engine first selects a topic to provide advice about, some component of theplant being constructed. The engine then chooses how direct a hint to provide an indirect hintmay talk about the functional constraints that a choice must satisfy, whereas a direct hintproposes a specific choice. The level of directness then helps to determine the types of media tobe used in the presentation indirect hints tend to be realized as animated depictions of therelationships between environmental factors and the plant components, while direct hints areusually rendered as speech. Finally, a suitable coherent set of media elements with the selectedmedia characteristics are chosen and sequenced.One of the biggest challenges in designing a behavior space and a sequencing engine isensuring visual coherence of the agents behavior at runtime. When done poorly, the agentsbehavior will appear discontinuous at the seams of the behavior fragments. For somepedagogical purposes, this may not be serious, but it will certainly detract from the believabilityof the agent, and it may be distracting to the student. Thus, to assist the sequencing engine inassembling behaviors that exhibit visual coherence, it is critical that the specifications for theanimated segments take into account continuity. One simple technique employed by somebehavior sequencing engines is the use of visual bookending. Visually bookended animationsbegin and end with frames that are identical. Just as walk cycles and looped backgrounds can beseamlessly composed, visually bookended animated behaviors can be joined in any order andthe global behavior will always be flawlessly continuous. Although it is impractical for allvisual segments to begin and end with the same frame, judicious use of this technique cangreatly simplify the sequencing engines job.More generally, the design of behavior spaces can exploit lessons and methods from thefilm industry. Because the birth and maturation of the film medium over the past century hasprecipitated the development of a visual language with its own syntax and semantics Monaco1981, the grammar of this language can be employed in all aspects of the agents behaviors.Careful selection of the agents behaviors, its accouterments e.g., props such as microscopes,jetpacks, etc., and visual expressions of its emotive state Bates 1994 can emphasize the mostsalient aspects of the domain for the current problemsolving context.Many animated agents employ variants of the behavior space approach. Vincent Paiva Machado 1998, an animated pedagogical agent for onthejob training, uses a very simplebehavior space, consisting of 4 animation sequences happy, friendly, sad, and impatient and80 utterances. Adeles animation is produced from a set of bitmap images of her in differentJohnson, Rickel and Lester66poses, which were created from an artists drawings. Hermans behavior sequencing engineorchestrates his actions by selecting and assembling behaviors from a behavior space of 30animations and 160 audio clips. The animations were rendered by a team of graphic artists andanimators. Hermans engine also employs a large library of runtimemixable soundtrackelements to dynamically compose a score that complements the agents activities.The PPP Persona and Cosmo also use the behavior space approach. However, to achievemore flexibility in their behavior, they use independent behavior fragments for different visualcomponents of the agent, and the behavior sequencing engine must combine these at runtime.Like Adele, the PPP Personas behavior is generated from bitmaps of the agent in differentposes. However, the PPP Persona can also use a dynamically generated pointer to refer tospecific entities in the world as it provides advice the sequencing engine must combine animage of the agent in a pointing pose with a pointer drawn from the agents hand to thereferenced entity.Cosmo takes this approach much farther. Depending on the physical and pedagogicalcontexts in which Cosmo will deliver advice, at runtime each frame at a rate ofapproximately 15second is assembled from independent components for torsos, heads, andarms. Dynamic head assembly provides flexibility in gaze direction, while dynamic armassembly provides flexibility in performing deictic and emotive gestures. Finally, Cosmoexhibits vocal flexibility by dynamically splicing in referring expression phrases to voice clipsequences. For example, this technique enables Cosmo to take into account the physical anddialogue contexts to alternatively refer to an object or group of objects with a proximaldemonstrative this, a nonproximal demonstrative those, or perhaps withpronominalization it. Although it is more difficult to dynamically combine body fragmentsat runtime, the different possible combinations allow for a wider repertoire of behaviors. Cosmostill follows the behavior space approach, since he relies on behavior fragments created ahead oftime by designers, but the granularity of his fragments is clearly smaller than an agent likeHerman.The behavior space approach to behavior generation offers an important advantage over thealternate techniques described below it provides very high quality animations. The granularityof the building block is relatively high, and skilled animators have significant control over theprocess before runtime, so the overall visual impact can at times be quite striking. However, thebehavior space suffers from several disadvantages. It is labor intensive requiring muchdevelopment time by the animation staff, and because it involves 2D graphics, the studentsviewpoint is fixed. Perhaps most lacking, however, is the degree of flexibility that can beexhibited by these agents. Because it is not a fundamentally generative approach, designersmust anticipate all of the behavior fragments and develop robust rules for assembling themtogether.Generating Behavior DynamicallyTo achieve more flexibility, the alternative approach is to completely generate behavior as it isneeded, without reusing any canned animation segments or even individual frames. Thisapproach has been used for several systems described in the Enhancing Learning Environmentswith Animated Agents section, including Jack Badler, Phillips,  Webber 1993, Steve, andWhizLow. These characters each include a 3D graphical model of the agent, segmented into itsmovable parts. In addition, each includes algorithms that can take a specification of a desiredposture and generate the appropriate body motions to transition from the agents current postureto the desired one. For example, given an object that Steve should look at, an algorithmgenerates an animation path for his head to follow. The difficulty lies in the fact that typically anumber of body parts must move in concert. For instance, even in the simple gaze example,Steve may have to turn his eyes, head, neck, shoulders, and torso, all subject to constraints ontheir flexibility, and these must move at differential speeds to look natural.This generative approach works for speech as well as animation. While the behavior spaceapproach pieces together prerecorded voice clips, the texttospeech synthesizers used bySteve, Adele, and WhizLow generate speech from individual phonemes. These synthesizers canAnimated pedagogical agents facetoface interaction in interactive learning environments67also apply a wide variety of prosodic transformations. For example, the synthesizer could beinstructed to speak an utterance in an angry tone of voice or a more polite tone depending on thecontext. A wide variety of commercial and public domain speech synthesizers with suchcapabilities are currently available.The flexibility of this generative approach to animation and speech comes at a price it isdifficult to achieve the same level of quality that is possible within a handcrafted animation orspeech fragment. For now, the designer of a new application must weigh the tradeoff betweenflexibility and quality. Further research on computer animation and speech synthesis is likely todecrease the difference in quality between the two approaches, making the generative approachincreasingly attractive.Tools for Creating Behavioral Building BlocksMost of the projects described in this paper have involved people with graphics and animationexpertise to create the behavioral building blocks. For projects without this luxury, tools arerapidly becoming available to allow designers to add animated characters to their learningenvironment even in the absence of such expertise. For example, Microsoft Agent2 and Adelesanimated persona3 are both available free for download over the World Wide Web. Bothprovide animated characters with some existing behaviors as well as the ability to create newcharacters and add new behaviors. Both employ the behavior space approach for animationwhile using speech synthesizers for voice. In contrast, Jack4, available as a commercial product,supports dynamically generated behavior. The increasing availability of tools for creatinganimated characters will greatly simplify the development of animated pedagogical agents.However, creating the behavioral building blocks for an animated character is only the firstchallenge in developing an animated pedagogical agent. The next challenge is developing thecode that will select and combine the right building blocks to respond appropriately to thedynamically unfolding tutorial situation. We now turn to that issue.Behavior ControlControlling the behavior of an animated pedagogical agent requires attention to many issues.Like any other autonomous agent, the agent must be able to react to a dynamic environment.Additionally, like any intelligent tutoring system or learning companion, the agent must carryon a coherent dialogue with the student, and it must make pedagogical decisions, such as whento intervene and what types of information to provide. On top of these considerations, ananimated agent must additionally provide appropriate control of its body, complementing itsverbal utterances with appropriate nonverbal behavior. The presence of a body marks asignificant shift in the problem of behavior control while a typical tutoring systems behavior isrelatively discrete, providing occasional, atomic interventions, nonverbal behavior necessitatesmore continuous control. In this section we focus on these additional control problems raised byanimated agents, and we survey current approaches.The key to maintaining coherent behavior in the face of a dynamic environment is tomaintain a rich representation of context. The ability to react to unexpected events and handleinterruptions is crucial for pedagogical agents, yet it threatens the overall coherence of theagents behavior. A good representation of context allows the agent to be responsive whilemaintaining its overall focus. Animated pedagogical agents must maintain at least the followingthree types of context.Pedagogical contextThe pedagogical context includes the instructional goals and a model of the studentsknowledge. This area has been studied extensively by past researchers work in animated                                                     2 httpmsdn.microsoft.comworkshopimediaagentdefault.asp3 httpwww.isi.eduisdcartecartedemos.htm4 httpwww.transom.comJohnson, Rickel and Lester68pedagogical agents to date has contributed relatively little on this issue.Task contextThe task context represents the state of the students and agents problem solving. Thisincludes the goals of the task, the current state of the learning environment, and the actionsthat will be needed to complete the task. For example, Steve and Adele model tasks using ahierarchical partialorder plan representation, which they generate automatically using taskdecomposition planning Sacerdoti 1977. As the task proceeds, they continually monitorthe state of the virtual world, and they use the task model to maintain a plan for how tocomplete the task, using a variant of partialorder planning techniques Weld 1994.Because Herman the Bug provides problemsolving advice for designcentered problemsolving, he maintains a task model that includes knowledge about the active designconstraints, the subtask the student is currently addressing, and a history of her designdecisions. Cosmo maintains analogous knowledge about each of the factors bearing onstudents problemsolving decisions in the Internet Protocol Advisor learning environment.Dialogue contextThe dialogue context represents the state of the collaborative interaction between thestudent and the agent. This may include many types of information a focus stack Grosz Sidner 1986 representing the hierarchy of tasks, subtasks, and actions in which the agentand student are currently engaged the state of their interaction on the current task step forinstance, the state might be that the agent has explained what must be done next but neitherhe nor the student has done it a record of whether the agent or student is currentlyresponsible for completing the task this task initiative can change during a mixedinitiativeinteraction the last answer the agent gave, in case the student asks a followup questionand the actions that the agent and student have already taken. While this list is notexhaustive, it captures the most important items used in current animated pedagogicalagents.Given a rich representation of context, much of an agents nonverbal behavior can begenerated dynamically in response to the current situation. In Steve, nonverbal behavior isgenerated in several different layers. Some elements are generated as deliberate acts in Stevescognitive module. This includes such things as looking at someone when waiting for them orlistening to them or releasing the conversational turn, nodding the head when Steve is informedof something or when the student takes an appropriate action, and shaking the head when thestudent makes a mistake. Other actions are generated in his motor control module to accompanymotor commands from the cognitive module. For example, Steve looks where he is going, looksat an object immediately before manipulating or pointing at it, looks at someone immediatelybefore speaking to them, and changes facial expression to a speaking face i.e., mouth openand eyebrows slightly raised when speaking. Finally, lowlevel behavior that requires a framebyframe update is implemented in the virtual reality software. This includes the animation ofSteves locomotion and arm movements, periodic blinking, slight periodic movement of the lipswhen speaking, and tracking abilities of Steves gaze.The approach to behavior generation discussed so far can be viewed as a mapping from arepresentation of context to the next appropriate behavioral action. The resulting behavior iscoherent to the extent that the regularities of human conversation are built into the mapping.This approach is similar to the schemata approach to explanation generation pioneered byMcKeown McKeown 1985. The other common approach to explanation generation is to plana coherent sequence of utterances by searching through alternative sequences until one is foundthat satisfies all coherence constraints Hovy 1993, Moore 1995. This approach has beenadapted to the problem of generating the behavior of an animated agent by Andr et al. Andr Rist 1996, Andr, Rist,  Mller 1999 and implemented in their PPP Persona.Animated pedagogical agents facetoface interaction in interactive learning environments69Figure 13. A Presentation PlanTheir approach is illustrated in Figure 13. The planning process starts with an abstractcommunicative goal e.g., provideinformation in the figure. The planners presentationknowledge is in the form of goal decomposition methods called presentation strategies. In thefigure, each nonterminal node in the tree represents a communicative goal, and its childrenrepresent one possible presentation strategy for achieving it. For example, the goal provideinformation can be achieved by introduce followed by a sequence of elaborate acts.Each presentation strategy captures a rhetorical structure found in human discourse, basedlargely on Rhetorical Structure Theory Mann  Thompson 1987, and each has applicabilityconditions that specify when the strategy may be used and constrain the variables to beinstantiated. Given the toplevel communicative goal, the presentation planner tries to find amatching presentation strategy, and it posts the inferior acts of this strategy as new subgoals. Ifa subgoal cannot be achieved, the presentation planner backtracks and tries another strategy.The process is repeated until all leaves of the tree are elementary presentation acts. A variant ofthe PPP Persona called WebPersona allows some other types of leaves as well. Thus, the leavesof the tree in Figure 13 represent the planned presentation, and the tree represents its rhetoricalstructure.This presentation script is forwarded to a Persona Engine, which executes it bydynamically merging it with lowlevel navigation acts when the agent has to move to a newposition on the screen, idletime acts to give the agent lifelike behavior when idle, andreactive behaviors so that the agent can react to user interactions. The Persona Enginedecomposes the persona behaviors at the leaves of the presentation plan into more primitiveanimation sequences and combines these with unplanned behaviors such as idletime actionsbreathing or tapping a foot and reactive behaviors such as hanging suspended when the userpicks up and moves the persona with the mouse. When behavior execution begins, the personafollows the schedule in the presentation plan. However, since the Persona Engine may executeadditional actions, this in turn may require the schedule to be updated, subject to the constraintsof the presentation plan. The result is behavior that is adaptive and interruptible, whilemaintaining coherence to the extent possible.Johnson, Rickel and Lester70One of the most difficult yet important issues in controlling the behavior of an animatedagent is the timing of its nonverbal actions and their synchronization with verbal utterances.Relatively small changes in timing or synchronization can significantly change peoplesinterpretation or their judgement of the agent. Andr et al. Andr, Rist,  Mller 1999 addresstiming issues through explicit temporal reasoning. Each presentation strategy includes a set oftemporal constraints over its inferior acts. Constraints may include Allens qualitative temporalrelations Allen 1983 relating pairs of acts, as well as quantitative inequality constraints on thestart and end times of the acts. Any presentation whose temporal constraints becomeinconsistent during planning is eliminated from further consideration.One important area for further research is the synchronization of nonverbal acts withspeech at the level of individual words or syllables. This capability is needed to support manyfeatures of human conversation, such as the use of gestures, head nods, and eyebrowmovements to highlight emphasized words. Most current animated agents are incapable of suchprecise timing. One exception is the work of Cassell and her colleagues Cassell et al. 1994a.However, they achieve their synchronization through a multipass algorithm that generates ananimation file for two synthetic, conversational agents. Achieving a similar degree ofsynchronization during a realtime dialogue with a human student is a more challengingproblem that will require further research.BelievabilityBecause of the immediate and deep affinity that people seem to develop for these interactivelifelike characters, the direct pedagogical benefits that pedagogical agents provide are perhapsexceeded by their motivational benefits. By creating the illusion of life, dynamically animatedagents have the potential to significantly increase the time that people seek to spend witheducational software, and recent advances in affordable graphics hardware are beginning tomake the widespread distribution of realtime animation technology a reality. Endowinganimated agents with believable, lifelike qualities has been the subject of much recent researchBates 1994, Tu  Terzopoulos 1994, Granieri et al. 1995, Blumberg  Galyean 1995,Kurlander  Ling 1995, Maes et al. 1995.Believability is a product of two forces 1 the visual qualities of the agent and 2 thecomputational properties of the behavior control system that creates its behaviors in response toevolving interactions with the user. The behavior canon of the animated film Noake 1988,Jones 1989, Lenburg 1993 has much to say about aesthetics, movement, and characterdevelopment, and the pedagogical goals of learning environments impose additionalrequirements on character behaviors. In particular, techniques for increasing the believability ofanimated pedagogical agents should satisfy the following criteriaSituated LivenessThroughout problemsolving sessions, agents should remain alive by continuing to exhibitbehaviors that indicate their awareness of events playing out in the learning environment,e.g., they can visually track students activities and provide anticipatory cues Thomas Johnston 1981 to signal their upcoming actions.Controlled Visual ImpactSome behaviors such as moving from one location to another have high visual impact,while others, such as small head movements, have low visual impact. In general, the higherthe visual impact, the more interesting a behavior will be, but agents must control the visualimpact of their behaviors in such a manner that they do not divert students attention atcritical junctures.Animated pedagogical agents facetoface interaction in interactive learning environments71Complex Behavior PatternsBecause students will interact with animated pedagogical agents over extended periods oftime, it is critical that agents behavior patterns be sufficiently complex that they cannot bequickly induced. Easily recognized behavior patterns significantly reduce believability.Natural Unobtrusive BehaviorIt is critical that students attention not be drawn to agents because they behave unnaturally.For example, a common problem in early implementations of any pedagogical agent is thatthe designer has neglected to have them assume a reasonable stance or blink. Omissionssuch as these typically result in surprisingly odd behaviors.Achieving believability in animated pedagogical agents poses three major challenges. First,the primary goal of pedagogical agents is to promote learning, and any agent behaviors thatwould interfere with students problemsolvingno matter how much these behaviors mightcontribute to believabilitywould be inappropriate. For example, if the agent were to cartwheelacross the screen when the student was grappling with a difficult problem, the studentsconcentration would be immediately broken. Second, believabilityenhancing behaviors mustcomplement and somehow be dynamically interleaved with the advisory and explanatorybehaviors that pedagogical agents perform. Third, if observers see that an agent is acting like asimple automaton, believability is either substantially diminished or eliminated altogether.To achieve believability, agents typically exhibit a variety of believabilityenhancingbehaviors that are in addition to advisory and attending behaviors. For example, the PPPPersona exhibits idletime behaviors such as breathing and foottapping to achievebelievability. To deal with the concerns of controlled visual impact for sensitive pedagogicalsituations in which the student must focus his attention on problemsolving, a competitionbasedbelievabilityenhancing technique is used by one version of the Herman agent. At each moment,the strongest eligible behavior is heuristically selected as the winner and is exhibited. Thealgorithm takes into account the probable visual impact of candidate behaviors so that behaviorsinhabiting upper strata of the impact spectrum are rewarded when the student is addressingless critical subproblems.Throughout learning sessions, the agent attends to students problemsolving activities.Believabilityenhancing behaviors compete with one another for the right to be exhibited. Whenthe agent is not giving advice, he is kept alive by a sequencing engine that enables it toperform a large repertoire of contextually appropriate, believabilityenhancing behaviors such asvisual focusing e.g., motionattracted head movements, reorientation e.g., standing up, lyingdown, locomotion e.g., walking across the scene, body movements e.g., back scratching,head scratching, restlessness e.g., toe tapping, body shifting, and propbased movementse.g., glasses cleaning. When a student is solving an unimportant subproblem, Herman is morelikely to perform an interesting propbased behavior such as cleaning his glasses or alocomotive behavior such as jumping across the screen. The net result of the ongoingcompetition is that the agent behaves in a manner that significantly increases its believabilitywithout sacrificing pedagogical effectiveness.EmotionEngaging, lifelike pedagogical agents that are visually expressive could clearly communicateproblemsolving advice and simultaneously have a strong motivating effect on learners. If theycould draw on a rich repertoire of emotive behaviors to exhibit contextually appropriate facialexpressions and expressive gestures, they could exploit the visual channel to advise, encourage,and empathize with learners. However, enabling lifelike pedagogical agents to communicate theaffective content of problemsolving advice poses serious challenges. Agents fullbody emotivebehaviors must support expressive movements and visually complement the problemsolvingadvice they deliver. Moreover, these behaviors must be planned and coordinated in real time inresponse to learners progress. In short, to create the illusion of life typified by well craftedJohnson, Rickel and Lester72animated characters, animated pedagogical agents must be able to communicate through bothvisual and aural channels.To be maximally entertaining, animated characters must be able to express many differentkinds of emotion. As different social situations arise, they must be able to convey emotions suchas happiness, elation, sadness, fear, envy, shame, and gloating. In a similar fashion, becauselifelike pedagogical agents should be able to communicate with a broad range of speech acts,they should be able to visually support these speech acts with an equally broad range of emotivebehaviors. However, because their role is primarily to facilitate positive learning experiences,only a critical subset of the full range of emotive expression is essential for pedagogical agents.For example, they should be able to exhibit body language that expresses joy and excitementwhen learners do well, inquisitiveness for uncertain situations such as when rhetoricalquestions are posed, and disappointment when problemsolving progress is less than optimal.The Cosmo agent, for instance, can scratch his head in wonderment when he poses a rhetoricalquestion.Cosmo illustrates how an animated pedagogical agent using the behavior space approachcan employ contextually appropriate emotive behaviors. Cosmo employs an emotivekinestheticbehavior sequencing framework for dynamically sequencing his fullbody emotive expressions.Creating an animated pedagogical agent with this framework consists of three phases, each ofwhich is a special case of the phases in the general behavior space approach described above.First, designers add behavior fragments representing emotive behavior to the behavior space.For example, Cosmo includes emotive behavior fragments for his facial expressions with eyes,eyebrows, and mouth and gestures with arms and hands. Second, these behavior fragmentsmust be indexed by their emotional intent i.e., which emotion is exhibited and their kinestheticexpression i.e., how it is exhibited. Third, the behavior sequencing engine must integrate theemotive behavior fragments into the agents behavior in appropriate situations. For example,Cosmos emotivekinesthetic behavior sequencing engine dynamically plans fullbody emotivebehaviors in real time by selecting relevant pedagogical speech acts and then assemblingappropriate visual behaviors. By associating appropriate emotive behaviors with differentpedagogical speech act categories e.g., empathy when providing negative feedback, it canweave small expressive behaviors into larger visually continuous ones that are then exhibited bythe agent in response to learners problemsolving activities.Both emotive behavior sequencing and its counterpart, affective student modeling, inwhich users emotive state is tracked Picard 1997, will play important roles in futurepedagogical agent research. There is currently considerable research activity on computationalmodels of emotion, and a variety of useful frameworks are now available. Research on applyingsuch models to interactive learning environments, on the other hand, has only begun Elliott,Rickel,  Lester 1999.Platform and Networking IssuesAll successful animated pedagogical agent designs must take into account the capabilities of theplatform and network that are intended to be used. At the present time, highfidelity interactiveagents with dynamically generated behavior can only run on configurations with high processorspeed, powerful graphics acceleration, and low latency. For applications where such power isnot guaranteed to be available, compromises must be made. The behavior space approach can beused in place of the dynamic behavior generation approach in order to reduce realtimerendering requirements. Reducing the repertoire of gestures can also reduce processingrequirements. For example, the Verbots created by Virtual Personalities, Inc.5 have limitedgestures other than lip movement these agents can run on most Pentium personal computerswithout graphics acceleration.The problem of integrating pedagogical agents into Webbased learning materials is aninteresting case in point. The Web has become the delivery mechanism of choice for onlinecourses. At the same time, Webbased instruction can be very impersonal, with limited ability to                                                     5 httpwww.vperson.comAnimated pedagogical agents facetoface interaction in interactive learning environments73adapt and respond to the user. An agent that can integrate with Webbased materials is desirableboth because it can be applied to a range of course materials and because it can improve theinteractivity and responsiveness of such materials.The most difficult technical problem associated with Webbased agents is reconciling thehighly interactive nature of facetoface interaction with the slow response times of the Web. Intypical Webbased courseware delivery systems, the student must choose a response, submit itto a remote server, and wait for the server to send back a new page. Animated pedagogicalagents, on the other hand, need to be able to respond to a continuous stream of student actions,watching what the student is doing, nodding in agreement, interrupting if the student isperforming an inappropriate action, and responding to student interruptions. It is difficult toachieve such interactivity if every action must be routed through a central HTTP server.Two Webbased architectures for animated pedagogical agents, the PPP Persona andAdele, both address this problem by moving reactive agent behavior from the server to theclient. The PPP Persona compiles the agent behavior into an efficient state machine that is thendownloaded to the client for execution. The presentation planning capability, on the other hand,resides on the central server. In the case of Adele, a solution plan for the given case or problemis downloaded, and is executed by a lightweight student monitoring engine. This approachrequires a more sophisticated engine to run on the client side, capable of a range of differenttypes of pedagogical interactions. Nevertheless, the engine remains simple enough to execute ona client computer with a reasonable amount of memory and processor speed. Focusing on onecase or problem at a time ensures that the knowledge base employed by the agent at any onetime remains small.The latencies involved in Webbased interaction also become significant when oneattempts to coordinate the activities of multiple students on different computers. Adele mustaddress this problem when students work together on the same case at the same time. Separatecopies of Adele run on each client machine. Student events are shared between Adele enginesusing Javas RMI protocol. Each Adele persona then reacts to student events as soon as theyarrive at each client machine. This gives the impression at each station of rapid response, even ifevents are not occurring simultaneously at all client computers.In summary, integration of animated pedagogical agents into Webbased learning materialsinevitably entails developing ways of working around the latencies associated with the HTTPand CGI protocols to some extent. Nevertheless, such agents do take advantage of Web browserenvironment as appropriate. They point students to relevant Web sites and can respond tobrowsing actions. Thus, they can be easily integrated into a Webbased curriculum, providing avaluable enhancement.CONCLUSIONAnimated pedagogical agents offer enormous promise for interactive learning environments.Though still in the early stages of development, it is becoming apparent that this new generationof learning technologies will have a significant impact on education and training. By broadeningthe bandwidth of communication to include many of the modalities of humanhuman tutoring,pedagogical agents are slowly but surely becoming something akin to what ITS foundersenvisioned at the inception of the field. Now, rather than being restricted to textual dialogue ona terminal, pedagogical agents are beginning to perform a variety of tasks in surprisingly lifelikeways. What began as complex but nevertheless small prototype systems have quickly becomepractical. Some of the systems described here will soon be used in online courses others havebeen and continue to be subject to largescale empirical studies.Despite the great strides made in honing the communication skills of animated pedagogicalagents, much remains to be done. In many ways, the current state of the art represents the earlydevelopmental stages of what promises to be a fundamentally new and interesting species oflearning technology. This article has set forth the key functionalities that lifelike agents willneed to succeed at facetoface communication. While the ITS community benefits from theconfluence of multidisciplinary research in cognition, learning, pedagogy, and AI, animatedJohnson, Rickel and Lester74pedagogical agents will further require the collaboration of communication theorists, linguists,graphics specialists, and animators. These efforts could well establish a new paradigm incomputerassisted learning, glimpses of which we can already catch on the horizon.AcknowledgmentsSupport for this work was provided by the Office of Naval Research under grant N0001495C0179 and AASERT grant N000149710598 the Air Force Research Laboratory under contractF4162497C5018 a gift from Mitsubishi Electric Research Laboratory an internal researchand development grant from the USC Information Sciences Institute the National ScienceFoundation under grants CDA9720395 Learning and Intelligent Systems Initiative and IRI9701503 CAREER Award Program the William S. Kenan Institute for Engineering,Technology and Science the North Carolina State University IntelliMedia Initiative anindustrial gift from Novell and equipment donations from Apple and IBM. We are grateful toBrad Mott and the members of CARTE for their valuable comments on an earlier draft of thisarticle, and to Charles Callaway for his assistance with the preparation of the online version ofthe article.ReferencesAmeur, E., Dufort, H., Leibu, D., and Frasson, C. 1997. Some justifications for the learningby disturbing strategy. In Proceedings of the Eighth World Conference on ArtificialIntelligence in Education, 119126. IOS Press.Allen, J. 1983. Maintaining knowledge about temporal intervals. Communications of the ACM2611832843.Andr, E., and Rist, T. 1996. Coping with temporal constraints in multimedia presentationplanning. In Proceedings of the Thirteenth National Conference on Artificial IntelligenceAAAI96, 142147. Menlo Park, CA AAAI PressMIT Press.Andr, E., Rist, T., and Mller, J. 1999. Employing AI methods to control the behavior ofanimated interface agents. Applied Artificial Intelligence 13415448.Andr, E., ed. 1997. Proceedings of the IJCAI Workshop on Animated Interface AgentsMaking Them Intelligent.Badler, N. I., Phillips, C. B., and Webber, B. L. 1993. Simulating Humans. New York OxfordUniversity Press.Ball, G., Ling, D., Kurlander, D., Miller, J., Pugh, D., Skelly, T., Stankosky, A., Thiel, D., vanDantzich, M., and Wax, T. 1997. Lifelike computer characters the persona project atmicrosoft. In Bradshaw, J., ed., Software Agents. Menlo Park, CA AAAIMIT Press.Bates, J., Loyall, A., and Reilly, W. 1992. Integrating reactivity, goals, and emotion in a broadagent. In Proceedings of the Fourteenth Annual Conference of the Cognitive ScienceSociety, 696701.Bates, J. 1994. The role of emotion in believable agents. Communications of the ACM 377.Blumberg, B., and Galyean, T. 1995. Multilevel direction of autonomous creatures for realtime virtual environments. In Computer Graphics Proceedings, 4754.Burton, R. R., and Brown, J. S. 1982. An investigation of computer coaching for informallearning activities. In Sleeman, D., and Brown, J., eds., Intelligent Tutoring Systems.Academic Press. 7998.Carbonell, J. R. 1970. AI in CAI An artificialintelligence approach to computerassistedinstruction. IEEE Transactions on ManMachine Systems 114190202.Cassell, J., and Thorisson, K. R. 1999. The power of a nod and a glance Envelope vs.emotional feedback in animated conversational agents. Applied Artificial Intelligence13519538.Cassell, J., Pelachaud, C., Badler, N., Steedman, M., Achorn, B., Becket, T., Douville, B.,Prevost, S., and Stone, M. 1994a. Animated conversation Rulebased generation of facialexpression, gesture and spoken intonation for multiple conversational agents. InProceedings of ACM SIGGRAPH 94.Animated pedagogical agents facetoface interaction in interactive learning environments75Cassell, J., Steedman, M., Badler, N., Pelachaud, C., Stone, M., Douville, B., Prevost, S., andAchorn, B. 1994b. Modeling the interaction between speech and gesture. In Proceedingsof the Sixteenth Annual Conference of the Cognitive Science Society. Hillsdale, NJLawrence Erlbaum Associates.Chan, T.W., and Baskin, A. 1990. Learning companion systems. In Frasson, C., and Gauthier,G., eds., Intelligent Tutoring Systems At the Crossroads of Artificial Intelligence andEducation. Ablex.Chan, T.W. 1996. Learning companion systems, social learning systems, and the globalsocial learning club. Journal of Artificial Intelligence in Education 72125159.Claassen, W. 1992. Generating referring expressions in a multimodal environment. In Dale,R., Hovy, E., Rosner, D., and Stock, O., eds., Aspects of Automated Natural LanguageGeneration. Berlin SpringerVerlag. 24762.Clancey, W. 1983. The epistemology of a rulebased expert system A framework forexplanation. Artificial Intelligence 33215251.Cohn, J. F., Lien, J. J.J., Kanade, T., Hua, W., and Zlochower, A. 1998. Beyond prototypicexpressions Discriminating subtle changes in the face. In Proceedings of the IEEEWorkshop on Robot and Human Communication ROMAN 98.Cormen, T. H., Leiserson, C. E., and Rivest, R. L. 1989. Introduction to Algorithms. NewYork McGrawHill.Culhane, S. 1988. Animation from Script to Screen. New York St. Martins Press.Deutsch, B. G. 1974. The structure of task oriented dialogs. In Proceedings of the IEEESpeech Symposium. Pittsburgh, PA CarnegieMellon University. Also available asStanford Research Institute Technical Note 90.Dillenbourg, P. 1996. Some technical implications of distributed cognition on the design ofinteractive learning environments. Journal of Artificial Intelligence in Education 72161179.Elliott, C., and Brzezinski, J. 1998. Autonomous agents as synthetic characters. AI Magazine1921330.Elliott, C., Rickel, J., and Lester, J. 1999. Lifelike pedagogical agents and affectivecomputing An exploratory synthesis. In Wooldridge, M., and Veloso, M., eds., ArtificialIntelligence Today, volume 1600 of Lecture Notes in Computer Science. SpringerVerlag.195212.Elliott, C. 1992. The Affective Reasoner A Process Model of Emotions in a MultiagentSystem. Ph.D. Dissertation, Northwestern University.Firby, R. J. 1994. Task networks for controlling continuous processes. In Proceedings of theSecond International Conference on AI Planning Systems.Frasson, C., Mengelle, T., Ameur, E., and Gouarderes, G. 1996. An actorbased architecturefor intelligent tutoring systems. In Proceedings of the Third International Conference onIntelligent Tutoring Systems ITS 96, number 1086 in Lecture Notes in Computer Science,5765. Springer.Goldstein, I. P. 1976. The computer as coach An athletic paradigm for intellectual education.Artificial Intelligence Laboratory Memo 389, Massachusetts Institute of Technology,Cambridge, MA.Granieri, J. P., Becket, W., Reich, B. D., Crabtree, J., and Badler, N. I. 1995. Behavioralcontrol for realtime simulated human agents. In Proceedings of the 1995 Symposium onInteractive 3D Graphics, 173180.Grosz, B. J., and Sidner, C. L. 1986. Attention, intentions, and the structure of discourse.Computational Linguistics 123175204.HayesRoth, B., and Doyle, P. 1998. Animate characters. Autonomous Agents and MultiAgentSystems 12195230.Hietala, P., and Niemirepo, T. 1998. The competence of learning companion agents.International Journal of Artificial Intelligence in Education 9178192.Hollan, J. D., Hutchins, E. L., and Weitzman, L. 1984. Steamer An interactive inspectablesimulationbased training system. AI Magazine 521527.Johnson, Rickel and Lester76Hovy, E. 1993. Automated discourse generation using discourse structure relations. ArtificialIntelligence 63341385.Johnson, W. L., and Rickel, J. 1998. Steve An animated pedagogical agent for proceduraltraining in virtual environments. SIGART Bulletin 81621.Johnson, W. L., Rickel, J., Stiles, R., and Munro, A. 1998. Integrating pedagogical agents intovirtual environments. Presence Teleoperators and Virtual Environments 76523546.Jones, C. 1989. Chuck Amuck The Life and Times of an Animated Cartoonist. New YorkAvon.Kurlander, D., and Ling, D. T. 1995. Planningbased control of interface animation. InProceedings of CHI 95, 472479.Laird, J. E., Newell, A., and Rosenbloom, P. S. 1987. Soar An architecture for generalintelligence. Artificial Intelligence 331164.Laurel, B. 1990. Interface agents Metaphors with character. In Laurel, B., ed., The Art ofHumanComputer Interface Design. New York AddisonWesley.Lenburg, J. 1993. The Great Cartoon Directors. New York Da Capo Press.Lester, J. C., Converse, S. A., Kahler, S. E., Barlow, S. T., Stone, B. A., and Bhogal, R. S.1997a. The persona effect Affective impact of animated pedagogical agents. InProceedings of CHI 97, 359366.Lester, J. C., Converse, S. A., Stone, B. A., Kahler, S. E., and Barlow, S. T. 1997b. Animatedpedagogical agents and problemsolving effectiveness A largescale empirical evaluation.In Proceedings of the Eighth World Conference on Artificial Intelligence in Education, 2330. IOS Press.Lester, J. C., Voerman, J. L., Towns, S. G., and Callaway, C. B. 1999a. Deictic believabilityCoordinating gesture, locomotion, and speech in lifelike pedagogical agents. AppliedArtificial Intelligence 13383414.Lester, J. C., Zettlemoyer, L. S., Gregoire, J., and Bares, W. H. 1999b. Explanatory lifelikeavatars Performing userdesigned tasks in 3d learning environments. In Proceedings of theThird International Conference on Autonomous Agents.Lester, J. C., Stone, B. A., and Stelling, G. D. 1999. Lifelike pedagogical agents for mixedinitiative problem solving in constructivist learning environments. User Modeling andUserAdapted Interaction 9144.Maes, P., Darrell, T., Blumberg, B., and Pentland, A. 1995. The ALIVE system Fullbodyinteraction with autonomous agents. In Proceedings of Computer Animation 95, 1118.Geneva, Switzerland IEEE Press.Maes, P. 1994. Agents that reduce work and information overload. Communications of theACM 377.Mann, W. C., and Thompson, S. A. 1987. Rhetorical structure theory A theory of textorganization. In Polanyi, L., ed., The Structure of Discourse. Norwood, NJ AblexPublishing Corporation. Also available as USCInformation Sciences Institute RS87190.Marsella, S. C., and Johnson, W. L. 1998. An instructors assistant for teamtraining indynamic multiagent virtual worlds. In Proceedings of the Fourth International Conferenceon Intelligent Tutoring Systems ITS 98, number 1452 in Lecture Notes in ComputerScience, 464473. Springer.McKeown, K. R. 1985. Text Generation. Cambridge University Press.Mittal, V., Roth, S., Moore, J. D., Mattis, J., and Carenini, G. 1995. Generating explanatorycaptions for information graphics. In Proceedings of the International Joint Conference onArtificial Intelligence, 12761283.Monaco, J. 1981. How To Read a Film. New York Oxford University Press.Moore, J. D. 1995. Participating in Explanatory Dialogues. Cambridge, MA MIT Press.Mller, J. P. 1996. The Design of Intelligent Agents A Layered Approach. Number 1177 inLecture Notes in Artificial Intelligence. Springer.Munro, A., Johnson, M., Surmon, D., and Wogulis, J. 1993. Attributecentered simulationauthoring for instruction. In Proceedings of the World Conference on Artificial Intelligencein Education AIED 93, 8289. Association for the Advancement of Computing inEducation.Animated pedagogical agents facetoface interaction in interactive learning environments77Murray, W. R. 1997. Knowledgebased guidance in the CAETI center associate. InProceedings of the Eighth World Conference on Artificial Intelligence in Education, 331339. IOS Press.Nagao, K., and Takeuchi, A. 1994. Social interaction Multimodal conversation with socialagents. In Proceedings of the Twelfth National Conference on Artificial Intelligence AAAI94, 2228. Menlo Park, CA AAAI Press.Noake, R. 1988. Animation Techniques. London Chartwell.Noma, T., and Badler, N. I. 1997. A virtual human presenter. In Proceedings of the IJCAIWorkshop on Animated Interface Agents Making Them Intelligent, 4551.Paiva, A., and Machado, I. 1998. Vincent, an autonomous pedagogical agent for onthejobtraining. In Proceedings of the Fourth International Conference on Intelligent TutoringSystems ITS 98, 584593. Springer.Pelachaud, C., Badler, N. I., and Steedman, M. 1996. Generating facial expressions forspeech. Cognitive Science 201.Picard, R. W. 1997. Affective Computing. MIT Press.Pierrehumbert, J., and Hirschberg, J. 1990. The meaning of intonational contours in theinterpretation of discourse. In Cohen, P. Morgan, J. and Pollack, M., eds., Intentions inCommunication. MIT Press. chapter 14, 271311.Reeves, B., and Nass, C. 1998. The Media Equation How People Treat Computers,Television and New Media Like Real People and Places. New York CSLI.Rickel, J., and Johnson, W. L. 1997a. Integrating pedagogical capabilities in a virtualenvironment agent. In Proceedings of the First International Conference on AutonomousAgents. ACM Press.Rickel, J., and Johnson, W. L. 1997b. Intelligent tutoring in virtual reality A preliminaryreport. In Proceedings of the Eighth World Conference on Artificial Intelligence inEducation, 294301. IOS Press.Rickel, J., and Johnson, W. L. 1999a. Animated agents for procedural training in virtualreality Perception, cognition, and motor control. Applied Artificial Intelligence 13343382.Rickel, J., and Johnson, W. L. 1999b. Virtual humans for team training in virtual reality. InProceedings of the Ninth International Conference on Artificial Intelligence in Education.IOS Press.Sacerdoti, E. 1977. A Structure for Plans and Behavior. New York Elsevier NorthHolland.Shaw, E., Ganeshan, R., Johnson, W. L., and Millar, D. 1999. Building a case for agentassisted learning as a catalyst for curriculum reform in medical education. In Proceedingsof the Ninth International Conference on Artificial Intelligence in Education. IOS Press.Shaw, E., Johnson, W. L., and Ganeshan, R. 1999. Pedagogical agents on the web. InProceedings of the Third International Conference on Autonomous Agents.Sleeman, D., and Brown, J., eds. 1982. Intelligent Tutoring Systems. Academic Press.Smith, R. W., and Hipp, D. R. 1994. Spoken Natural Language Dialog Systems. Cambridge,Massachusetts Oxford University Press.Stevens, A., Roberts, B., and Stead, L. 1983. The use of a sophisticated graphics interface incomputerassisted instruction. IEEE Computer Graphics and Applications 32531.Stiles, R., McCarthy, L., and Pontecorvo, M. 1995. Training studio A virtual environment fortraining. In Workshop on Simulation and Interaction in Virtual Environments SIVE95.Iowa City, IW ACM Press.Tambe, M. 1997. Towards flexible teamwork. Journal of Artificial Intelligence Research783124.Thomas, F., and Johnston, O. 1981. The Illusion of Life Disney Animation. New York WaltDisney Productions.Thorisson, K. R. 1996. Communicative Humanoids A Computational Model of PsychosocialDialogue Skills. Ph.D. Dissertation, Massachusetts Institute of Technology.Towns, S. G., Callaway, C. B., and Lester, J. C. 1998. Generating coordinated naturallanguage and 3D animations for complex spatial explanations. In Proceedings of theFifteenth National Conference on Artificial Intelligence.Johnson, Rickel and Lester78Tu, X., and Terzopoulos, D. 1994. Artificial fishes Physics, locomotion, perception, andbehavior. In Computer Graphics Proceedings, 4350.Walker, J. H., Sproull, L., and Subramani, R. 1994. Using a human face in an interface. InProceedings of CHI94, 8591.Weld, D. S. 1994. An introduction to least commitment planning. AI Magazine 1542761.Wenger, E. 1987. Artificial Intelligence and Tutoring Systems. Los Altos, CA MorganKaufmann.
