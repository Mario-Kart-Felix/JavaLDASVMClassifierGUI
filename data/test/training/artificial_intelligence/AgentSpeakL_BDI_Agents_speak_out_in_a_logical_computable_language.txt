AgentSpeakL BDI Agents speak out in a logicalcomputable languageAnand S. RaoAustralian Artificial Intelligence InstituteLevel 6, 171 La Trobe Street, MelbourneVictoria 3000, AustraliaEmail anandaaii.oz.auAbstract. BeliefDesireIntention BDI agents have been investigated by manyresearchers from both a theoretical specification perspectiveand a practical designperspective. However, there still remains a large gap between theory and practice.The main reason for this has been the complexity of theoremproving or modelchecking in these expressive specification logics. Hence, the implemented BDIsystems have tended to use the three major attitudes as data structures, rather thanas modal operators. In this paper, we provide an alternative formalization of BDIagents by providing an operational and prooftheoretic semantics of a languageAgentSpeakL. This language can be viewed as an abstraction of one of the implemented BDI systems i.e., PRS and allows agent programs to be written andinterpreted in a manner similar to that of hornclause logic programs. We showhow to perform derivations in this logic using a simple example. These derivations can then be used to prove the properties satisfied by BDI agents.1 IntroductionThe specification, design, verification, and applications of a particular type of agents,called BDI agents, have received a great deal of attention in recent years. BDI agentsare systems that are situated in a changing environment, receive continuous perceptualinput, and take actions to affect their environment, all based on their internal mentalstate. Beliefs, desires, and intentions are the three primary mental attitudes and they capture the informational, motivational, and decision components of an agent, respectively.In addition to these attitudes, other notions such as commitments, capabilities, knowhow, etc. have been investigated. Sophisticated, multimodal, temporal, action, and dynamic logics have been used to formalize some of these notions 2, 6, 8, 13, 18, 20, 21.The complexity of theoremproving and the completeness of these logics have not beenclear 12, 23.On the other hand, there are a number of implementations of BDI agents 1, 3, 10,17 that are being used successfully in critical application domains. These implementations have made a number of simplifying assumptions and modelled the attitudes ofbeliefs, desires, and intentions as data structures. Also, user written plans or programsspeed up the computation in these systems. The complexity of the code written for thesesystems and the simplifyingassumptions made by them have meant that the implementedsystems have lacked a strong theoretical underpinning. The specification logics haveshed very little light on the practical problems. As a result the two streams of work seemto be diverging.Our earlier attempt to bridge this gap between theory and practice has concentratedon providing an abstract BDI architecture 14, that serves both as an idealization of animplemented system and also as a vehicle for investigating certain theoretical properties. Due to its abstraction this work was unable to show a onetoone correspondencebetween the model theory, proof theory, and the abstract interpreter. The holy grail ofBDI agent research is to show such a onetoone correspondence with a reasonably useful and expressive language.This paper makes another attempt at specifying such a logical language. Unlike someof the previous attempts, it takes as its starting point one of the implemented systems andformalizes its operational semantics. The implemented system being considered is theProcedural Reasoning System PRS 5 and its more recent incarnation, the DistributedMultiAgent Reasoning System dMARS. The language AgentSpeakL can be viewedas a simplified, textual language of PRS or dMARS. The language and its operationalsemantics are similar to the implemented system in their essential details. The implemented system has more language constructs to make the task of agent programmingeasier.AgentSpeakL is a programming language based on a restricted firstorder languagewith events and actions. The behaviour of the agent i.e., its interaction with the environment is dictated by the programs written in AgentSpeakL. The beliefs, desires, andintentions of the agent are not explicitly represented as modal formulas. Instead, we asdesigners can ascribe these notions to agents written in AgentSpeakL. The current stateof the agent, which is a model of itself, its environment, and other agents, can be viewedas its current belief state states which the agent wants to bring about based on its external or internal stimuli can be viewed as desires and the adoption of programs to satisfysuch stimuli can be viewed as intentions. This shift in perspective of taking a simplespecification language as the execution model of an agent and then ascribing the mentalattitudes of beliefs, desires, and intentions, from an external viewpoint is likely to havea better chance of unifying theory and practice.In Section 2 we discuss the agent language AgentSpeakL. The specification language consists of a set of base beliefs or facts in the logic programming sense and aset of plans. Plans are contextsensitive, eventinvoked recipes that allow hierarchicaldecomposition of goals as well as the execution of actions. Although syntactically planslook similar to the definite clauses of logic programming languages, they are quite different in their behaviour.Section 3 formalizes the operational semantics of AgentSpeakL. At runtime anagent can be viewed as consisting of a set of beliefs, a set of plans, a set of intentions,a set of events, a set of actions, and a set of selection functions. The selection of plans,their adoption as intentions, and the execution of these intentions are described formallyin this section. An interpreter for AgentSpeakL is given and a simple example is usedto illustrate some of the definitions and the operational semantics of the language.In Section 4, we provide the proof theory of the language. The proof theory is givenas a labeled transition system. Proof rules define the transition of the agent from one configuration to the next. These transitions have a direct relationship to the operational semantics of the language and hence help to establish the strong correspondence betweenthe AgentSpeakL interpreter and its proof theory.The primary contributionof this work is in opening up an alternative, restricted, firstorder characterization of BDI agents. We hope that the operational and prooftheoreticsemantics of AgentSpeakL will stimulate research in both the pragmatic and theoretical aspects of BDI agents.2 Agent ProgramsIn this section, we introduce the language for writing agent programs. The alphabet ofthe formal language consists of variables, constants, function symbols, predicate symbols, action symbols, connectives, quantifiers, and punctuationsymbols. Apart from firstorder connectives, we also use  for achievement,  for test,  for sequencing, and for implication1. Standard firstorder definitions of terms, firstorder formulas, closedformulas, and free and bound occurrences of variables are used.Definition 1. If b is a predicate symbol, and t1,...,tn are terms then bt1,...,tn or bt isa belief atom. If bt and cs are belief atoms, bt  cs, andbt are beliefs. A beliefatom or its negation will be referred to as a belief literal. A ground belief atom will becalled a base belief.For example, let us consider a trafficworld simulation, where there are four adjacentlanes and cars can appear in any lane and move in the same lane from north to south.Waste paper can appear on any of the lanes and a robot has to pick up the waste paperand place it in the bin. While doing this the robot must not be in the same lane as thecar, as it runs the risk of getting run over by the car. Consider that we are writing agentprograms for such a robot.The beliefs of such an agent represent the configuration of the lanes and the locationsof the robot, cars, waste, and the bin i.e., adjacentX,Y, locationrobot,X,locationcar, X, etc.. The base beliefs of such an agent are ground instancesof belief atoms i.e., adjacenta,b, locationrobot, a, etc..A goal2 is a state of the system which the agent wants to bring about. We considertwo types of goals an achievement goal and a test goal. An achievement goal, written asgt states that the agent wants to achieve a state where gt is a true belief. A test goal,written as gt states that the agent wants to test if the formula gt is a true belief or not.In our example, clearing the waste on a particular lane can be stated as an achievementgoal, i.e., clearedb, and seeing if the car is in a particular lane can be stated as atest goal, i.e., locationcar, b.Definition 2. If g is a predicate symbol, and t1,...,tn are terms then gt1,...,tn or gtand gt1,...,tn or gt are goals.1 In the agent programs we use  for , not for ,  for . Also, like PROLOG, we requirethat all negations be ground when evaluated. We use the convention that variables are writtenin uppercase and constants in lowercase.2 In this paper, we discuss only goals, and not desires. Goals can be viewed as adopted desires.When an agent acquires a new goal or notices a change in its environment, it maytrigger additions or deletions to its goals or beliefs. We refer to these events as triggeringevents. We consider the additiondeletion of beliefsgoals as the four triggering events.Addition is denoted by the operator  and deletion is denoted by the operator. In ourexample, noticing the waste in a certain lane X, written as locationwaste, Xor acquiring the goal to clear the lane X, written as clearedX are example of twotriggering events.Definition 3. If bt is a belief atom, gt and gt are goals, thenbt,bt gt,gt, gt, gt are triggering events.The purpose of an agent is to observe the environment, and based on its observationand its goals, execute certain actions. These actions may change the state of the environment. For example, if move is an action symbol, the robot moving from lane X to laneY, written as moveX,Y, is an action. This action results in an environmental statewhere the robot is in lane Y and is no longer in lane X.Definition 4. If a is an action symbol and t1,...,tn are firstorder terms, then at1,...,tnor at is an action.An agent has plans which specify the means by which an agent should satisfy an end.A plan consists of a head and a body. The head of a plan consists of a triggeringevent anda context, separated by a . The triggering event specifies why the plan was triggered,i.e., the addition or deletion of a belief or goal. The context of a plan specifies thosebeliefs that should hold in the agents set of base beliefs, when the plan is triggered. Thebody of a plan is a sequence of goals or actions. It specifies the goals the agent shouldachieve or test, and the actions the agent should execute. For example, we want to writea plan that gets triggered when some waste appears on a particular lane. If the robot is inthe same lane as the waste, it will perform the action of picking up the waste, followedby achieving the goal of reaching the bin location, followed by performing the primitiveaction of putting it in the bin. This plan can be written aslocationwaste,Xlocationrobot,X locationbin,Y pickwastelocationrobot,Ydropwaste. P1Consider the plan for the robot to change locations. If it has acquired the goal to moveto a location X and it is already in location X, it does not have to do anything and hencethe body is true. If the context is such that it is not at the desired location then it needsto find an adjacent lane with no cars in it, and then move to that lane.locationrobot,Xlocationrobot,X  true. P2locationrobot,Xlocationrobot,Y not X  Y adjacentY,Z not locationcar, Z moveY,Zlocationrobot,X. P3More formally, we have the following definition of plans.Definition 5. If e is a triggering event, b1,...,bm are belief literals, and h1,...,hn are goalsor actions then eb1      bm  h1...hn is a plan. The expression to the left of thearrow is referred to as the head of the plan and the expression to the right of the arrowis referred to as the body of the plan. The expression to the right of the colon in the headof a plan is referred to as the context. For convenience, we shall rewrite an empty bodywith the expression true.With this we complete the specification of an agent. In summary, a designer specifiesan agent by writing a set of base beliefs and a set of plans. This is similar to a logicprogramming specification of facts and rules. However, some of the major differencesbetween a logic program and an agent program are as follows In a pure logic program there is no difference between a goal in the body of a ruleand the head of a rule. In an agent program the head consists of a triggering event,rather than a goal. This allows for a more expressive invocation of plans by allowing both datadirected using additiondeletion of beliefs and goaldirected usingadditiondeletion of goals invocations. Rules in a pure logic program are not contextsensitive as plans. Rules execute successfully returning a binding for unbound variables however, execution of plans generates a sequence of ground actions that affect the environment. While a goal is being queried the execution of that query cannot be interrupted in alogic program. However, the plans in an agent program can be interrupted.3 Operational SemanticsInformally, an agent consists of a set of base beliefs, B, a set of plans, P, a set of events,E, a set of actions, A, a set of intentions, I, and three selection functions,SE , SO, and SI .When the agent notices a change in the environment or an external user has asked thesystem to adopt a goal, an appropriate triggering event is generated. These events correspond to external events. An agent can also generate internal events. Events, internalor external, are asynchronously added to the set of events E. The selection function SEselects an event to process from the set of events E. This event is removed from E and isused to unify with the triggering events of the plans in the set P. The plans whose triggering events so unify are called relevant plans and the unifier is called the relevant unifier.Next, the relevant unifier is applied to the context condition and a correct answer substitution is obtained for the context, such that the context is a logical consequence of theset of base beliefs, B. Such plans are called applicable plans or options and the composition of the relevant unifier with the correct answer substitution is called the applicableunifier.For each event there may be many applicable plans or options. The selection function SO chooses one of these plans. Applying the applicable unifier to the chosen optionyields the intended means of responding to the triggering event. Each intention is a stackof partially instantiated plans or intention frames. In the case of an external event the intended means is used to create a new intention, which is added to the set of intentions I.In the case of an internal event to add a goal the intended means is pushed on top of anexisting intention that triggered the internal event.Next, the selection function SI selects an intention to execute. When the agent executes an intention, it executes the first goal or action of the body of the top of the intention. Executing an achievement goal is equivalent to generating an internal event toadd the goal to the current intention. Executing a test goal is equivalent to finding a substitution for the goal which makes it a logical consequence of the base beliefs. If such asubstitution is found the test goal is removed from the body of the top of the intentionand the substitution is applied to the rest of the body of the top of the intention. Executing an action results in the action being added to the set of actions, A, and it beingremoved from the body of the top of the intention.The agent now goes to the set of events, E, and the whole cycle continues until thereare no events in E or there is no runnable intention.Now we formalize the above process3.The state of an agent at any instant of time can be formally defined as followsDefinition 6. An agent is given by a tuple E,B,P,I,A,SE ,SO ,SI, where E is a set ofevents, B is a set of base beliefs, P is a set of plans, I is a set of intentions, and A is a set ofactions. The selection functionSE selects an event from the set E the selection functionSO selects an option or an applicable plan see Definition 10 from a set of applicableplans and SI selects an intention from the set I.The sets B, P, and A are as defined before and are relatively straightforward. Herewe describe the sets E and I.Definition 7. The set I is a set of intentions. Each intention is a stack of partially instantiated plans, i.e., plans where some of the variables have been instantiated. An intentionis denoted by p1z  zpz, where p1 is the bottom of the stack and pz is the top of thestack. The elements of the stack are delimited by z. For convenience, we shall refer tothe intention truetrue  true as the true intention and denote it by T.Definition 8. The set E consists of events. Each event is a tuple e, i, where e is atriggering event and i is an intention. If the intention i is the true intention, the event iscalled an external event otherwise it is an internal event.Now we can formally define the notion of relevant and applicable plans and unifiers.As we saw earlier, a triggering event d from the set of events, E, is to be unified with thetriggering event of all the plans in the set P. The most general unifier mgu that unifiesthese two events is called the relevant unifier. The intention i could be wither the trueintention or an existing intention which triggered this event. More formally,3 The reader can refer to the Appendix for some basic definitions from firstorder logic and hornclause logic.Definition 9. Let SE E     d i  and let p be e  b1   bm  h1    hn. Theplan p is a relevant plan with respect to an event  iff there exists a most general unifier such that d  e.  is called the relevant unifier for .For example, assume that the triggering event of the event selected from E islocationrobot,b.The two plansP2 andP3 are relevant for this event with the relevant unifier beingfXbg.A relevant plan is also applicable if there exists a substitutionwhich, when composedwith the relevant unifier and applied to the context, is a logical consequence of the setof base beliefs B. In other words, the context condition of a relevant plan needs to be alogical consequence of B, for it to be an applicable plan. More formally,Definition 10. A plan p, denoted by e  b1      bm  h1    hn is an applicableplan with respect to an event  iff there exists a relevant unifier  for  and there exists asubstitution such that 8b1...bm is a logical consequence of B. The composition is referred to as the applicable unifier for  and  is referred to as the correct answersubstitution.Continuing with the same example, consider that the set of base beliefs is given byadjacenta,b.adjacentb,c.adjacentc,d.locationrobot,a.locationwaste,b.locationbin,d.The applicable unifier is fXb, Ya, Zbg and only plan P3 is applicable.Depending on the type of the event i.e., internal or external, the intention will bedifferent. In the case of external events, the intended means is obtained by first selectingan applicable plan for that event and then applying the applicable unifier to the body ofthe plan. This intended means is used to create a new intention which is added to the setof intentions I.Definition 11. Let SOO  p, where O is the set of all applicable plans or options forthe event  d i  and p is e  b1  bm  h1    hn. The plan p is intended withrespect to an event , where i is the true intention iff there exists an applicable unifier such that true  true trueze  b1     bm  h1    hn 2 I.In our example, the only applicable plan P3 will be intended with the intention I nowbeinglocationrobot,b locationrobot,a notb  a adjacenta, b notlocationcar,b movea,blocationrobot,b.In the case of internal events the intended means for the achievement goal is pushedon top of the existing intention that triggered the internal event.Definition 12. Let SOO  p, where O is the set of all applicable plans or options forthe event    d p1z  zf  c1   cy  gth2    hn , and p is gs b1   bm  k1     kj. The plan p is intended with respect to an event  iff there existsan applicable unifier  such that p1z  zf  c1   cy  gth2    hnzgs b1     bm  k1     kj h2    hn 2 I.The above definition is very similar to SLDresolution of logic programming languages.However, the primary difference between the two is that the goal g is called indirectlyby generating an event. This gives the agent better realtime control as it can change itsfocus of attention, if needed, by adopting and executing a different intention. Thus, onecan view agent programs as multithreaded interruptible logic programming clauses.When an intention is selected and executed, the first formula in the body of the top ofthe intention can be a an achievement goal b a test goal or c an action or d true.In the case of an achievement goal the system executes it by generating an event in thecase of a test goal it looks for a mgu that will unify the goal with the set of base beliefsof the agent, and if such an mgu exists it applies it to the rest of the means in the caseof an action the system adds it to the set of actions A and in the last case the top of theintention and the achievement goal that was satisfied are removed and the substitutionis applied to the rest of the body of that intention.Definition 13. Let SII  i, where i is p1z  zf  c1     cy  gth2    hn.The intention i is said to have been executed iff  gt i  2 E.Definition 14. Let SII  i, where i is p1z  zf  c1     cy  gth2    hn.The intention i is said to have been executed iff there exists a substitution  such that8gt is a logical consequence of B and i is replaced by p1z  zf  c1    cy  h2    hn.Definition 15. Let SII  i, where i is p1z  zf  c1     cy  ath2    hn.The intention i is said to have been executed iff at2A, and i is replaced by p1z  zf c1     cy  h2    hn.Definition 16. Let SII  i, where i is p1z  zpz1zgt  c1      cy  true,where pz1 is e  b1      bx  gsh2    hn. The intention i is said to havebeen executed iff there exists a substitution  such that gt  gs and i is replacedby p1z  zpz1ze  b1     bx  h2    hn.Continuing our example, we would execute I and by Definition 15 we would addfmovea,bg to A and change I to be as followslocationrobot,b locationrobot,a notb  a adjacenta, b notlocationcar,b locationrobot,b.In the next iteration, after the robot moves from a to b the environment will send theagent a belief update event to change the location of the robot to b. This will result inthe belieflocationrobot,b being added to the set B and the event locationrobot,b being added to the set of events, E. As there are no relevant plans for thisthe system will choose the above intention to execute. Executing this will result in anintention add event being generated and added to the set of events, E in other words Eis flocation robot,b,ig, where i is the same intention as before. ByDefinition 12 the relevant plan in this case is P1 with the relevant unifier fXbg. Thisplan is also applicable and the applicable unifier is the same. As the body of this planis true, the intention is satisfied and the set of events is empty. This terminates theexecution until the next event is added into the set E.From the above definitions and description of the operational semantics of the language AgentSpeakL we can write an interpreter for AgentSpeakL. Figure 1 describessuch an interpreter. We use the function top to return the top of an intention stack thefunctionhead to return the head of an intended plan the function body to return the bodyof an intended plan. In addition, the functions first and rest are used to return the firstelement of a sequence, and all but the first element of a sequence. The function pushtakes an intention frame and an intention i.e., stack of intention frames and pushes theintention frame on to the top of the intention. The function pop takes an intention as anargument and returns the top of the intention.4 Proof TheorySo far we have presented the operational semantics of AgentSpeakL. Now we brieflydiscuss its proof theory based on labeled transition systems.Definition 17. A BDI transition system is a pair h i consisting of A set  of BDI configurations and A binary transition relation      .We define a BDI configuration as followsDefinition 18. A BDI configuration is a tuple of hEi Bi Ii Ai ii, where Ei  E, Bi B, Ii  I, Ai  A, and i is the label of the transition.Note that we have not taken the set of plans, P, in the configuration as we have assumed itto be constant. Also, we do not explicitly keep track of goals as they appear as intentionswhen adopted by the agent. Now we can write transition rules that take an agent fromone configuration to its subsequent configuration.The following proof rule IntendEnd gives the transition for intending a plan at thetop level. It states how the agents set of intentions I changes in response to an externalevent that has been chosen by the SE function to be processed.IntendEnd f     gt T    g Bi Ii Ai i  f  g Bi Ii  fpg Ai i 1 Algorithm Interpreterwhile E 6  do   d i   SE EE  EO  fp j  is an applicable unifier for event  and plan pgif externalevent then I  I  SO Oelse pushSOO, i, where  is an applicable unifier for case firstbodytopSI I  truex  popSIIpushheadtopSI I  restbodytopSI I, SI I,where  is an mgu such that x  headtopSI Icase firstbodytopSI I  gtE  E  gt,SI Icase firstbodytopSI I  gtpopSIIpushheadtopSI I  restbodytopSI I, SI I,where  is the correct answer substitutioncase firstbodytopSI I  atpopSIIpushheadtopSI I restbodytopSI I, SI IA  A  fatgendwhile.Fig. 1. Algorithm for the BDI Interpreterwhere p  gs  b1   bm  h1    hn 2P , SEE   gt T , gt gs and 8 b1...bm is a logical consequence of Bi.The proof rule IntendMeans is similar to the previous proof rule, except that the applicable plan is pushed at the top of the intention given as the second argument of thechosen event. More formally we have,IntendMeans f     gt j    g Bi f    p1z   zpz   g Ai i  f  g Bi f    p1z   zpzzp   g Ai i 1 where pz  f  c1      cy  gth2    hn, p  gs  b1      bm  k1     kx, SE E  gt j , j is p1z  zpn , gt  gs and 8 c1...cyis a logical consequence of Bi.Next, we have four proof rules for execution. The four proof rules are based on thetype of the goal or action that appears as the first literal of the body of the top of anintention chosen to be executed by the function SI . We give the execution proof rulefor achieve ExecAch, the other proof rules can be written analogously.ExecAch Ei Bi f    p1z    zf  c1     cy  gth2    hn   g Ai i  Ei  f gt j g Bi f    p1z   zpz   g Ai i 1 where SIIi  j  p1z    zpz and pz  f  c1     cy  gth2    hn.Although we have given the proof rules only for additions of goals, similar proofrules apply for deletion of goals, and addition and deletion of beliefs.With these proof rules one can formally define derivations and refutations. The definition of derivations is straightforward and is a sequence of transitions using the aboveproof rules.Definition 19. A BDI derivation is a finite or infinite sequence of BDI configurations,i.e., 0,  ,i,  .The notion of refutation in AgentSpeakL is with respect to a particular intention. Inother words, the refutation for an intention starts when an intention is adopted and endswhen the intention stack is empty. Thus, using the above proof rules we can formallyprove certain behavioural properties, such as safety and liveness of agent systems, aswas done elsewhere 15. Furthermore, there is a onetoone correspondence betweenthe proof rules discussed in this section and the operational semantics discussed in theprevious section. Such a correspondence has not been possible before, because the prooftheory usually based on multimodal logics has been far removed from the realities ofthe operational semantics.In addition to the internal events considered in this paper i.e., addition of intentions,one can extend the operational semantics and proof rules with respect to other internalevents, such as deletion of intentions, and success and failure events for actions, plans,goals, and intentions.The body of the plans considered in this paper includes only sequences of goals oractions. Other dynamic logic operators, such as nondeterministic or, parallel, and iteration, operators can be allowed in the body of plans. In addition, assertion and deletion of beliefs in plan bodies can also be included. Another useful feature of the implemented system dMARS is different postconditions for successful and failure executionsof plans. The operational semantics and proof rules can once again be modified to account for the above constructs.5 Comparisons and ConclusionA number of agentoriented languages such as AGENT0 17, PLACA PLAnning CommunicatingAgents 19, AgentSpeak 22, SLP 16, 4, and CONGOLOG 9 have beenproposed in the literature.AGENT0 and its successor PLACA can model beliefs, commitments, capabilities,and communications between agents. These attitudes are treated as data structures ofan agent program. An interpreter that can execute such agent programs are described.However, the authors do not provide a formal proof theory or justify how the data structures capture the modeltheoretic semantics of beliefs, commitments, and capabilities.In contrast, the work described here discusses the connections between the interpreterand a proof theory based on labeled transition systems.SLP or Stream Logic Programming is based on reactive, guarded, horn clauses. Aclause in SLP consists of a guard and a behaviour. The guard is further decomposed intoan head and a boolean constraint. The boolean constraint is similar to our context. Thehead in SLP is an object and the body is a network of concurrent objects connected bycommunication message slots. Behaviour is specified by object replacement. The execution model of SLP and AgentSpeakL are fundamentally different. The behaviour of anagent to a particular external stimuli is captured in a single intention, as a stack of committed subbehaviours. This provides a global coherence absent in SLP. For example,consider an agent that wants to drop its intention because it no longer needs to achieve agiven toplevel goal. Killing such an intention would be much easier in AgentSpeakLthan in SLP.The semantics of CONGOLOG is based on situationcalculus. Although it provides aricher set of actions than what has been discussed here, it is essentially a single intentionor singlethreaded system, unlike AgentSpeakL. The language AgentSpeak 22 isan objectoriented analogue of AgentSpeakL.AgentSpeakL is a textual and simplified version of the language used to programthe Procedural Reasoning System 3 and its successor dMARS. These implementationshave been in use since the mid1980s. Other agentoriented systems, such as COSY 1,INTERRAP 10, and GRATE 7, have been built based on the BDI architecture. Theformal operational semantics given here could apply to some of these systems as well.However, a more thoroughanalysis of these systems and their relation to AgentSpeakLis beyond the scope of this paper.Bridging the gap between theory and practice in the field of agents, and in particular the area of BDI agents, has proved elusive. In this paper, we provide an alternativeapproach by providing the operational semantics of AgentSpeakL which abstracts animplemented BDI system. The primary contribution of this work is in opening up an alternative, restricted, firstorder characterization of BDI agents and showing a onetoonecorrespondence between the operational and prooftheoretic semantics of such a characterization. We are confident that this approach is likely to be more fruitful than theprevious approaches in bridging the gap between theory and practice in this area andwill stimulate research in both the pragmatic and theoretical aspects of BDI agents.Acknowledgements The research reported in this paper was funded partly by theGeneric Industry Research and Development Grant on Distributed RealTime ArtificialIntelligence and partly by the Cooperative Research Centre for IntelligentDecision Systems. The author wishes to thank Michael Georgeff and Lawrence Cavedon for theirvaluable input and comments on this paper.AppendixDefinition 20. An atom of the form s  t, where s and t are terms is called an equation.Definition 21. A substitution is a finite set fx1t1,...,xntng, where x1,...,xn are distinctvariables, and t1,...,tn are terms such that xi 6 ti for any i from 1..n.Definition 22. The application of a substitution   fx1t1,...,xntng to a variable xi,written as xi, yields ti iff xiti 2  and xi otherwise. The application of  to a term orformula is the term or formula obtained by simultaneously replacing every occurrenceof xi by ti for all i from 1 to n.Definition 23. Let   fx1t1,...,xntng and   fy1s1,...,ym smg. The composition of  and  is the substitution obtained from the set fx1t1,...,xntng   by removing all xiti for which xi  ti 1  i  nand removing those yjtj for which yj 2fx1,...,xng 1  j m 11.Definition 24. A substitution is a solution or unifier of a set of equations fs1  t1, ...,sn  tng iff si  ti for all i  1,...,n. A substitution is more general than  iff thereis a substitution such that   . A most general unifier mgu of two terms atomsis a maximally general unifier of the terms.References1. B. Burmeister and K. Sundermeyer. Cooperative problemsolving guided by intentions andperception. In E. Werner and Y. Demazeau, editors, Decentralized A.I. 3, Amsterdam, TheNetherlands, 1992. North Holland.2. P. R. Cohen and H. J. Levesque. Intention is choice with commitment. Artificial Intelligence,423, 1990.3. M. P. Georgeff and A. L. Lansky. Procedural knowledge. In Proceedingsof the IEEE SpecialIssue on Knowledge Representation, volume 74, pages 13831398, 1986.4. M. M. Huntbach, N. R. Jennings, and G. A. Ringwood. How agents do it in stream logic programming. In Proceedingsof the International Conferenceon MultiAgent Systems ICMAS95, San Francisco, USA, June, 1995.5. F. F. Ingrand, M. P. Georgeff, and A. S. Rao. An architecture for realtime reasoning andsystem control. IEEE Expert, 76, 1992.6. N. R. Jennings. On being responsible. In Y. Demazeau and E. Werner, editors, Decentralized A.I. 3. North Holland, Amsterdam, The Netherlands, 1992.7. N. R. Jennings. Specification and implementation of belief, desire, jointintention architecture for collaborative problem solving. Journal of Intelligent and Cooperative InformationSystems, 23289318, 1993.8. D. Kinny, M. Ljungberg, A. S. Rao, E. A. Sonenberg, G. Tidhar, and E. Werner. Plannedteam activity. In Artificial Social Systems, LectureNotes in Artificial Intelligence LNAI830,Amsterdam, Netherlands, 1994. Springer Verlag.9. Y. Lesperance, H. J. Levesque, F. Lin, D. Marcu, R. Reiter, and R. B. Scherl. Foundationsof a logical approach to agent programming. In Working notes of the IJCAI95 Workshop onAgent Theories, Architectures, and Languages, Montreal, Canada, 1995.10. J. P. Muller, M. Pischel, and M. Thiel. Modelling reactive behaviour in vertically layeredagent architectures. In Intelligent Agents Theories, Architectures, and Languages. LectureNotes in Artificial Intelligence LNAI 890, Heidelberg, Germany, 1995. Springer Verlag.11. U Nilsson. Abstract interpretations and abstract machines. Technical Report Dissertation No265, Department of Computer and Information Science, Linkoping University, Linkoping,Sweden, 1992.12. A. S. Rao. Decision procedures for propositional lineartime beliefdesireintention logics. In Working notes of the IJCAI95 Workshop on Agent Theories, Architectures, and Languages, Montreal, Canada, 1995.13. A. S. Rao and M. P. Georgeff. Modeling rational agents within a BDIarchitecture. InJ. Allen, R. Fikes, and E. Sandewall, editors, Proceedings of the Second International Conference on Principles of Knowledge Representation and Reasoning. Morgan Kaufmann Publishers, San Mateo, CA, 1991.14. A. S. Rao and M. P. Georgeff. An abstract architecture for rational agents. In C. Rich,W. Swartout, and B. Nebel, editors, Proceedings of the Third International Conference onPrinciples of Knowledge Representation and Reasoning. Morgan Kaufmann Publishers, SanMateo, CA, 1992.15. A. S. Rao and M. P. Georgeff. A modeltheoretic approach to the verification of situated reasoning systems. In Proceedingsof the Thirteenth International Joint Conferenceon ArtificialIntelligence IJCAI93, Chamberey, France, 1993.16. G. A. Ringwood. A brief history of stream parallel logic programming. Logic ProgrammingNewsletter, 7224, 1994.17. Y. Shoham. Agentoriented programming. Artificial Intelligence, 6015192, 1993.18. M. Singh and N. Asher. Towards a formal theory of intentions. In J. van Eijck, editor, Logicsin AI, volume LNAI478, pages 472486. Springer Verlag, Amsterdam, Netherlands, 1990.19. S. R. Thomas. The PLACA agent programming language. In Intelligent Agents Theories,Architectures, and Languages.Lecture Notes in Artificial Intelligence LNAI 890, Amsterdam,Netherlands, 1995. Springer Verlag.20. W. van der Hoek, B. van Linder, and J.J. Ch. Meyer. A logic of capabilities. In Proceedings of the Third International Symposium on the Logical Foundations of Computer ScienceLFCS94, Lecture Notes in Computer Science LNCS 813. Springer Verlag, Heidelberg, Germany, 1994.21. B. van Linder, W. van der Hoek, and J. J. Ch. Meyer. How to motivate your agents InWorking notes of the IJCAI95 Workshop on Agent Theories, Architectures, and Languages,Montreal, Canada, 1995.22. D. Weerasooriya, A. S. Rao, and K. Ramamohanarao. Design of a concurrent agentorientedlanguage. In Intelligent Agents Theories, Architectures, and Languages. Lecture Notes inArtificial Intelligence LNAI 890, Amsterdam, Netherlands, 1995. Springer Verlag.23. M. Wooldridge and M. Fisher. A decision procedure for a temporal belief logic. In Proceedings of the First International Conference on Temporal Logic, Bonn, Germany, 1994.This article was processed using the LATEX macro package with LLNCS style
