In Proceedings of the Seventeenth International Conference on Machine Learning ICML00,pages 287295, Stanford, California, June 1998Support Vector Machine Active Learningwith Applications to Text ClassificationSimon Tong SIMON.TONGCS.STANFORD.EDUDaphne Koller KOLLERCS.STANFORD.EDUComputer Science Department, Stanford University, Stanford, CA 94305, USAAbstractSupport vector machines have met with significant success in numerous realworld learningtasks. However, like most machine learning algorithms, they are generally applied using a randomly selected training set classified in advance.In many settings, we also have the option of using poolbased active learning. Instead of usinga randomly selected training set, the learner hasaccess to a pool of unlabeled instances and canrequest the labels for some number of them. Weintroduce an new algorithm for performing active learning with support vector machines, i.e.,an algorithm for choosing which instances to request next. We provide a theoretical motivationfor the algorithm. We present experimental results showing that employing our active learningmethod can significantly reduce the need for labeled training instances in both the standard inductive and transductive settings.1. IntroductionIn many supervised learning tasks, labeling instances tocreate a training set is timeconsuming and costly thus,finding ways to minimize the number of labeled instancesis beneficial. Usually, the training set is chosen to be arandom sampling of instances. However, in many casesactive learning can be employed. Here, the learner can actively choose the training data. It is hoped that allowing thelearner this extra flexibility will reduce the learners needfor large quantities of labeled data.Poolbased active learning has been recently introduced Lewis  Gale, 1994 McCallum  Nigam, 1998.The learner has access to a pool of unlabeled data and canrequest the true class label for a certain number of instancesin the pool. In many domains this is reasonable since alarge quantity of unlabeled data is readily available. Themain issue with active learning is finding a way to choosegood requests or queries from the pool.We present a new algorithm that performs poolbased active learning with support vector machines SVMs. Weprovide theoretical motivations for our approach to choosing the queries, together with experimental results showingthat active learning with SVMs can significantly reduce theneed for labeled training instances.Clearly poolbased active learning can be applied to numerous domains  creating automated email filters, interactive searching of an image database, identifying good oildrilling sites. We shall use text classification as a runningexample throughout this paper. This is the task of determining to which predefined topic a given text document belongs. It has an important role to play  especially with therecent explosion of readily available text data. There havebeen many approaches to achieve this goal Rocchio, 1971Dumais et al., 1998. Furthermore, it is also a domain inwhich SVMs have shown notable success Joachims, 1998Dumais et al., 1998 and it is of interest to see whetheractive learning can offer further improvement over this already highly effective method.2. Support Vector MachinesSupport vector machines Vapnik, 1982 have strong theoretical foundations and excellent empirical successes. Theyhave been applied to tasks such as handwritten digit recognition, object recognition, as well as text classification.We shall consider SVMs in the binary classification setting. We are given training datathat are vectors in some space   . We are also given their labelswhere  . In their simplest form,SVMs are hyperplanes that separate the training data by amaximal margin. All vectors lying on one side of the hyperplane are labeled as , and all vectors lying on the otherside are labeled as 1. The training instances that lie closestto the hyperplane are called support vectors. More generally, SVMs allow one to project the original training datain space  to a higher dimensional feature space  via aMercer kernel operator  . In other words, we consider theset of classifiers of the form ,.0.When  satisfies Mercers condition Burges, 1998 wecan write    where  and denotes an inner product. We can then rewrite  as  where    1Thus, by using  we are implicitly projecting the training data into a different often higher dimensional featurespace  . The SVM then computes thes that correspondto the maximal margin hyperplane in  . By choosing different kernel functions we can implicitly project the training data from  into spaces  for which hyperplanes in correspond to more complex decision boundaries in theoriginal space  . One commonly used kernel is the polynomial kernel      which induces polynomial boundaries of degree  in the original space  . Forthe majority of this paper we will assume that the featurevectors are normalized, i.e.,   . It is possibleto relax this constraint and we will discuss this later in thepaper.In addition to regular induction, SVMs can also be used fortransduction. Here we are first given a set of both labeledand unlabeled data. The learning task is to assign labelsto the unlabeled data as accurately as possible. SVMs canperform transduction by finding the hyperplane that maximizes the margin relative to both the labeled and unlabeled data. Recently, transductive SVMs TSVMs havebeen used for text classification Joachims, 1999, attaining some improvements in precisionrecall breakeven performance over regular inductive SVMs.3. Version SpaceGiven a set of labeled training data and a Mercer kernel , there is a set of hyperplanes that separate the data inthe induced feature space  . We call this set of consistenthypotheses the version space Mitchell, 1982. In otherwords, hypothesis  is in version space if for every traininginstancewith labelwe have that if  and if . More formallyDefinition 3.1 Our set of possible hypotheses is given as   where  where our parameter spaceis simply equal to  . TheVersion space,  is then defined as,    . Notice that sinceis a set of hyperplanes, there is a bijection between unit vectorsand hypotheses  in. Thuswe will redefine  as  0 123  Note that a version space only exists if the training data arelinearly separable in feature space. Thus, in this paper, wedo require linear separability of the training data in featurespace. This restriction is less harsh than it at first may seemsince the feature space often has a very high dimension.Nevertheless, requiring linear separability in feature spaceis a condition we wish to relax in future work.There exists a duality between the feature space  and theparameter spaceVapnik, 1998 Herbrich et al., 1999which we shall take advantage of in the next section pointsin  correspond to hyperplanes inand vice versa.Clearly, by definition points incorrespond to hyperplanes in  . The intuition behind the converse is that observing a training instance in feature space restricts theset of separating hyperplanes to ones that classifycorrectly. In fact, we can show that the set of allowable pointsinis restricted to lie on one side of a hyperplane in. More formally, to show that points in  correspondto hyperplanes in, suppose we are given a new traininginstancewith label. Then any separating hyperplanemust satisfy40 5. Now, instead of viewingas the normal vector of a hyperplane in  , think ofas being the normal vector of a hyperplane in.Thus637 3defines a half spacein. Furthermore8  9defines a hyperplaneinthat acts as one of the boundaries to version space  .Notice that version space is a connected region on the surface of a hypersphere in parameter space. See Figure 1afor an example.SVMs find the hyperplane that maximizes the margin infeature space  . One way to pose this is as followsmaximize . BA subject to    By having the conditions   and Cwe cause the solution to lie in version space. Now, wecan view the above problem as finding the pointin version space that maximizes the distance DA 7 E  .From the duality between feature and parameter space, andsince   , eachis a unit normal vectorof a hyperplane in parameter space and each of these hyperplanes delimit the version space. Thus we want to findthe point in version space that maximizes the minimum distance to any of the delineating hyperplanes. That is, SVMsfind the center of the largest radius hypersphere whose center can be placed in version space and whose surface doesnot intersect with the hyperplanes corresponding to the labeled instances, as in Figure 1b. It can be easily shownthat the hyperplanes that are touched by the maximal radiushypersphere correspond to the support vectors and that theradius of the hypersphere is the margin of the SVM.a b cFigure 1. a Version space duality. The surface of the hypersphere represents unit weight vectors. Each of the two hyperplanes corresponds to a labeled training instance. Each hyperplane restricts the area on the hypersphere in which consistent hypotheses can lie. Hereversion space is the surface segment of the hypersphere closest to the camera. b An SVM classifier in version space. The dark embedded sphere is the largest radius sphere whose center lies in version space and whose surface does not intersect with the hyperplanes. Thecenter of the embedded sphere corresponds to the SVM, its radius is the margin of the SVM in  and the training points correspondingto the hyperplanes that it touches are the support vectors. c   Margin Method.4. Active LearningIn poolbased active learning we have a pool of unlabeledinstances. It is assumed that the instancesare independently and identically distributed according to some underlying distribution  and the labels are distributed according to some conditional distribution  .Given an unlabeled pool  , an active learner  has threecomponents    . The first component is a classifier,8  , trained on the current set of labeled dataand possibly unlabeled instances in  too. The secondcomponent is the querying function that, given a current labeled set, decides which instance in  to querynext. The active learner can return a classifier  after eachquery online learning or after a set number of queries.The main difference between an active learner and a passivelearner is the querying component. This brings us to theissue of how to choose the next unlabeled instance to query.As in Seung et al. 1992, we use an approach that queriespoints so as to attempt to reduce the size of the versionspace as much as possible. We need two more definitionsbefore we can proceedDefinition 4.1 Area 2is the surface area that the versionspace  occupies on the hypersphere   .Definition 4.2 Given an active learner  , let denote theversion space of  after  queries have been made. Now,given the 2  th query, define   .3    .3 So  and denote the resulting version spaces whenthe next queryis labeled as and respectively.We can now appeal to the following lemma to motivatewhich instances to queryLemma 4.3 Suppose we have an input space  , finite dimensional feature space  induced via a kernel  , andparameter space. Suppose active learner  alwaysqueries instances whose corresponding hyperplanes in parameter spacehalves the area of the current versionspace. Let  be any other active learner. Denote the version spaces of   and  after  queries as  and respectively. Let  denote the set of all conditional distributionsofgiven. Then,   Area 2  Area    with strict inequality whenever there exists a query  by  that does not halve version space ,.The proof is straightforward and appears in the longer version of this paper. This lemma says that, for any givennumber of queries,  minimizes the maximum expectedsize of the version space, where the maximum is taken overall conditional distributions ofgiven. Suppose .  is the unit parameter vector corresponding to the SVM thatwe would have obtained had we known the actual labels ofall of the data in the pool. We know that . must lie ineach of the version spaces 1032034, where denotes the version space after  queries. Thus, by shrinking the size of the version space as much as possible witheach query we are reducing as fast as possible the space inwhich .  can lie. Hence, the SVM that we learn from ourlimited number of queries will lie close to . .If one is willing to assume that there is a hypothesis lying withinthat generates the data and that the generatinghypothesis is deterministic and that the data are noise free,then strong generalization performance properties of an algorithm that halves version space can also be shown Freund et al., 1997.Hence, we would like to query instances that split the current version space into two equal parts as much as possia b cFigure 2. a    Margin will query  . b Margin will query  . The two SVMs with margins  and  for  are shown.c  Margin will query  . The two SVMs with margins  and  for  are shown.ble. Given an unlabeled instancefrom the pool, it is notpractical to explicitly compute the sizes of the new versionspaces and i.e., the version spaces obtained whenis labeled as and  respectively. We next presentthree ways of approximating this procedure. Simple Margin. Recall from section 3 that, given somedataand labels, the SVM unit vectorobtained from this data is the center of the largesthypersphere that can fit inside the current version space .The position ofin the version space clearly dependson the shape of the region , however it is often approximately in the center of the version space. Now, we cantest each of the unlabeled instancesin the pool to seehow close their corresponding hyperplanes income tothe centrally placed . The closer a hyperplane inisto the point , the more centrally it is placed in versionspace, and the more it bisects version space. Thus we canpick the unlabeled instance in the pool whose hyperplaneincomes closest to the vector. For each unlabeledinstance, the shortest distance between its hyperplane inand the vectoris simply the distance between thefeature vectorand the hyperplanein   whichis easily computed by   . This results in the naturalrule learn an SVM on the existing labeled data and chooseas the next instance to query the instance that comes closestto the hyperplane in  .Figure 1c presents an illustration. In this stylized picturewe have flattened out the surface of the unit weight vectorhypersphere that appears in 1a. The white area is versionspace which is bounded by solid lines corresponding tolabeled instances. The five dotted lines represent unlabeledinstances in the pool. The circle represents the largest radius hypersphere that can fit in version space. Note thatthe edges of the circle do not touch the solid lines  justas the dark sphere in 1b does not meet the hyperplanes onthe surface of the larger hypersphere they meet somewhereunder the surface. Instance  is the closest to the SVMand so we will choose to query  . MaxMin Margin. The    Margin method can bea rather rough approximation. It relies on version spacebeing fairly symmetric and being centrally placed. Ithas been demonstrated, both in theory and practice, thatthese assumptions can fail significantly Herbrich et al.,1999. Indeed, if we are not careful we may actually queryan instance whose hyperplane does not even intersect theversion space. The  approximation is designedto somewhat overcome these problems. Given some dataand labels,the SVM unit vectoris the center of the largest hypersphere that can fit insidethe current version space and the radius of the hypersphere is size of the margin of . We can use the radiusas an indication of the size of the version space Vapnik,1998. Suppose we have a candidate unlabeled instancein the pool. We can estimate the relative size of the resulting version space by labelingas , finding theSVM obtained from addingto our labeled training dataand looking at the size of its margin . We can performa similar calculation for by relabelingas class andfinding the resulting SVM to obtain margin .Since we want and equal split of the version space, wewish Area 2and Area  to be similar. Now, consider the quantity BA Area   Area 2. It will besmall if Area 2and Area 2are very different. Thuswe will consider BA,as an approximation andwe will choose to query thefor which this quantity islargest. Hence, the  query algorithm is as followsfor each unlabeled instancecompute the margins andof the SVMs obtained when we labelas and respectively then choose to query the unlabeled instance forwhich the quantity DA.is greatest.Figures 2a and 2b show an example comparing the   Margin and 0 Margin methods. MaxRatio Margin. This method is similar in spirit tothe  Margin method. We use and as indications of the sizes of and . However, we shall tryto take into account the fact that the current version spaceRandomSimpleMaxMinRatio0 20 40 60 80 100Labeled Training Set Size70.080.090.0100.0Test Set AccuracytioMaxMinSimplendom0 20 40 60 80 100Labeled Training Set Size85.087.590.092.595.097.5100.0Test Set AccuracyActive 1000 PoolActive 500 PoolRandomInductive PassiveTransductive PassiveInductive ActiveTransductive Active20 40 60 80 100Labeled Training Set Size0.010.020.030.040.050.060.070.080.090.0100.0PrecisionRecall Breakeven PointTransductive ActiveInductive ActiveTransductive PassiveInductive PassiveFigure 3. a Average test set accuracy over the ten most frequently occurring topics when using a pool size of 1000. b Average test setaccuracy over the ten most frequently occurring topics when using a pool sizes of 500 and 1000. c Average pool set precisionrecallbreakeven point over the ten most frequently occurring topics when using a pool size of 1000.may be quite elongated and for somein the pool bothand may be small simply because of the shape ofversion space. Thus we will instead look at the relativesizes of and and choose to query thefor whichBAis largest see Figure 2c.The above three methods are approximations to the querying component that always halves version space. After performing some number of queries we then return a classifierby learning a SVM with the labeled instances.The margin can be used as an indication of the versionspace size irrespective of whether the feature vectors arenormalized. Thus the explanation for the 0 and    methods still holds even with unnormalized feature vectors. The    method can still be used when thefeature vector are unnormalized, however the motivatingexplanation no longer holds since the SVM can no longerbe viewed as the center of the largest allowable sphere.5. Experiments5.1 Reuters Data Collection ExperimentsThe   21578 data set1 is a commonly used collectionof newswire stories categorized into hand labeled topics,some of which overlap. We used the 12902 articles fromthe ModApte split of the data and we considered the topten most frequently occurring topics. We learned ten different binary classifiers, one to distinguish each topic. Eachdocument was represented as a stemmed, TFIDF weightedword frequency unit vector.2 A stop list of common wordswas used and words occurring in less than three documentswere also ignored. Using this representation, the documentvectors had around 10000 dimensions. Our test set consisted of 3299 documents.1Obtained from www.research.att.comlewis.2We used Rainbow www.cs.cmu.edumccallumbow fortext processing.For each of the ten topics we performed the following. Wecreated a pool of unlabeled data by sampling 1000 documents from the remaining data and removing their labels.We then randomly selected two documents in the pool togive as the initial labeled training set. One document wasabout the desired topic, and the other document was notabout the topic. Thus we gave each learner 998 unlabeleddocuments and 2 labeled documents. After a fixed numberof queries we asked each learner to return a classifier anSVM with a polynomial kernel of degree one3 learned onthe labeled training documents. We then tested the classifier on the independent test set.The above procedure was repeated thirty times for eachtopic and the results were averaged. We considered the   Margin,  Margin and     Marginquerying methods as well as a   Sample method.The   Sample method simply randomly chooses thenext query point from the unlabeled pool. This last methodreflects what happens in the regular passive learning setting the training set is a random sampling of the data.To measure performance we used two metrics test setclassification error and, to stay compatible with previous Reuters corpus results, the precisionrecall breakevenpoint Joachims, 1998.Figure 3a present the average test set accuracy over theten topics as we vary the number of queries permitted. Theprecisionrecall breakeven point graph is virtually identical and is omitted. The dashed horizontal line is the performance level achieved when the SVM is trained on all1000 labeled documents comprising the pool. Over the  corpus the three active learning methods performvirtually identically with no notable difference to distinguish between them. Each method also appreciably out3For SVM and transductive SVM learning we used T.Joachims SVMlightwwwai.informatik.unidortmund.dethorstensvm light.html.Table 1. Average test set precisionrecall breakeven point over thetop ten most frequently occurring topics most frequent topic firstwhen trained with ten labeled documents.Topic    Equivalent   sizeEarn       Acq      Moneyfx       Grain      Crude       Trade       Interest       Ship       Wheat      Corn       performs random sampling. Table 1 shows the breakevenperformance of the active methods after they have asked forjust eight labeled instances so, together with the initial tworandom instances, they have seen ten labeled instances.The last column shows approximately how many instanceswould be needed if we were to use   to achieve thesame level of performance as the     active learningmethod. In this instance, passive learning on average requires over six times as much data to achieve comparablelevels of performance as the active learning methods. Thetable for test set accuracy is very similar and is omitted.Table 1 indicates that active learning provides more benefit with the infrequent classes. This last observation hasalso been noted before in previous studies McCallum Nigam, 1998.Figure 3b shows the average accuracy of the    method with two different pool sizes. The breakeven graphis similar. Clearly the   sampling methods performance will not be affected by the pool size. However, thegraphs indicate that increasing the pool of unlabeled datawill improve both the accuracy and breakeven performanceof active learning. This is quite intuitive since a good activemethod should be able to take advantage of a larger pool ofpotential queries and ask more targeted questions.We also investigated active learning in a transductive setting. Here we queried the points as usual except now eachmethod     and    returned a transductive SVMtrained on both the labeled and remaining unlabeled datain the pool. Since this was transduction the performanceof each classifier was measured on the pool of data ratherthan a separate test set. Figure 3c shows that using aTSVM provides a slight advantage over a regular SVM inboth querying methods when comparing breakeven points.However, the graph also shows that active learning provides notably more benefit than transduction  indeed using a TSVM with a   querying method needs over100 queries to achieve the same performance as a regular SVM with a    method that has only seen 20 labeled instances. Interestingly, we found that the TSVMmisclassification rates were notably worse than those ofregular SVMs. The breakeven point is a one number summary of the precisionrecall PR curve. Typically, eachTSVM PR curve was very erratic, only peaking aroundthe breakeven point . So, although we happened to get better breakeven performance, most of a typical TSVMs PRcurve was actually worse than that of the correspondingSVM. It could also be because the TSVM package we usedwas an approximation to the infeasible global solution.5.2 Newsgroups Data Collection ExperimentsOur second data collection was Ken Langs   collection4. We used the five  groups, discardingthe Usenet headers and subject lines. We processed the textdocuments exactly as before resulting in vectors of around10000 dimensions.We placed half of the 5000 documents aside to use asan independent test set, and repeatedly, randomly chosea pool of 500 documents from the remaining instances.We performed twenty runs for each of the five topics and averaged the results. We used test set accuracy to measure performance. Figure 4a contains thelearning curve averaged over all of the results for thefive  topics for the three active learning methods and   sampling. Again, the dashed horizontalline indicates the performance of an SVM that has beentrained on the entire pool. There is no appreciable difference between the 0 and     methods but, intwo of the five newsgroups      ,.1021.13and    24502  6  the    active learning method performs notably worse than the 0 and    methods. Figure 4b shows the average learningcurve for the  7 8  ,.902.13topic. In aroundten to fifteen per cent of the runs for both of the two newsgroups the    method was misled and performed extremely poorly for instance, achieving only 25 accuracyeven with fifty training instances, which is worse than random guessing. This indicates that the    queryingmethod may be more unstable than the other two methods.One reason for this could be that the    method tendsnot to explore the feature space as aggressively as the otheractive methods, and can end up ignoring entire clusters ofunlabeled instances. In Figure 5 the    method takesseveral queries before it even considers an instance in theunlabeled cluster while both the  and    query a point in the unlabeled cluster immediately.While 0 and     appear more stable they aremuch more computationally intensive. With a large poolof  instances, they require around  SVMs to be learned4Obtained from www.cs.cmu.edutextlearning.RandomSimpleMaxMinRatio0 20 40 60 80 100Labeled Training Set Size40.050.060.070.080.090.0100.0Test Set AccuracytioMaxMinSimplendomRatioMaxMinSimpleRandom0 20 40 60 80 100Labeled Training Set Size40.050.060.070.080.090.0100.0Test Set AccuracyRatioHybridSimple0 20 40 60 80 100Labeled Training Set Size40.050.060.070.080.090.0100.0Test Set AccuracyFigure 4. a Average test set accuracy over the five 9  topics when using a pool size of 500. b Average test set accuracy for 9     with a 500 pool size. c Macro average test set accuracy for 6      and9     where    uses the    method for the first ten queries and   for the rest.Figure 5. Simple example of querying unlabeled clusters.for each query. Most of the computational cost is incurredwhen the number of queries that have already been asked islarge. This is because that now training each SVM is costlytaking around two minutes to generate the 50th query on aSun workstation with a pool of 1000 documents. However,when the number of alreadyasked queries is small, evenwith a large pool size,  and     are fairlyfast taking a few seconds per query since now trainingeach SVM is fairly cheap. Interestingly, is it in the first fewqueries that the    seems to suffer the most throughits lack of aggressive exploration. This motivates a  , method. We can use  or     for the first fewqueries and then use the    method for the rest. Preliminary experiments with the   .  method show that itmaintains the stability of the  and     methods while allowing the scalability of the    method.See Figure 4c.6. Related WorkThere have been several studies of active learning for classification. The Query by Committee algorithm Seunget al., 1992 Freund et al., 1997 uses a prior distributionover hypotheses. This general algorithm has been appliedin domains and with classifiers for which specifying andsampling from a prior distribution is natural. They havebeen used with probabilistic models Dagan  Engelson,1995 and specifically with the Naive Bayes model for textclassification in a Bayesian learning setting McCallum Nigam, 1998. The Naive Bayes classifier provides an interpretable model and principled ways to incorporate priorknowledge and data with missing values. However, it typically does not perform as well as discriminative methodssuch as SVMs, particularly in the text classification domain Joachims, 1998 Dumais et al., 1998. Although adirect comparison has not been explicitly made here, the results of the active SVMs presented in this paper are generally significantly better than those of the active Naive Bayesapproach investigated by McCallum and Nigam 1998.Lewis and Gale 1994 introduced uncertainty samplingand applied it to a text domain using logistic regressionand, in a companion paper, using decision trees Lewis Catlett, 1994. The    querying method for SVMactive learning is essentially the same as their uncertaintysampling method choose the instance that our current classifier is most uncertain about, however they provided substantially less justification as to why the algorithm shouldbe effective. They also noted that the performance of theuncertainty sampling method can be variable, performingquite poorly on occasions.7. Conclusions and Future WorkWe have introduced a new algorithm for performing activelearning with SVMs. By taking advantage of the dualitybetween parameter space and feature space we arrived atthree algorithms that attempt to reduce version space asmuch as possible at each query. We have shown empirically that these techniques can provide considerable gainsin both the inductive and transductive settings  in somecases shrinking the need for labeled instances by over an order of magnitude, and in almost all cases reaching the performance achievable on the entire pool having only seen afraction of the data. Furthermore, larger pools of unlabeleddata improve the quality of the resulting classifier.Of the three methods presented the    method is computationally the fastest. However, the    methodwould seem to be a rougher and more unstable approximation as we witnessed when it performed poorly on two ofthe five Newsgroup topics. If asking each query is expensive relative to computing time such as in the oil drillingexample then using either the 0 or     maybe preferable. However, if the cost of asking each queryis relatively cheap and more emphasis is placed upon fastfeedback then the    method may be more suitable.In either case, we have shown that the use of these methods for learning can substantially outperform standard passive learning. Furthermore, experiments with the   , method indicate that it is possible to combine the benefitsof the     and    methods.Several studies have noted that gains in computationalspeed can be obtained at the expense of generalization performance by querying multiple instances at a time Lewis Gale, 1994 McCallum  Nigam, 1998. Viewing SVMsin terms of the version space gives an insight as to wherethe approximations are being made, and this may providea guide as to which multiple instances are better to query.For instance, it is suboptimal to query two instances whoseversion space hyperplanes are fairly parallel to each other.So, with the    method, instead of blindly choosing toquery the two instances that are the closest to the currentSVM, it may be better to query two instances that are closeto the current SVM and whose hyperplanes in version spaceare fairly perpendicular. Similar tradeoffs can be made forthe     and  methods.Clearly the three active learning methods described in thispaper can be applied to other domains. However, text classification is a particularly suitable domain for our algorithms as they stand at the moment. Sets of documentsare almost always linearly separable which is an importantconsideration for the time being. As we mentioned before,the notion of a version space only holds when the training data are linearly separable in feature space. If the dataare not separable then there is no version space. However,SVMs can still learn with linearly nonseparable data viathe use of a soft margin Cortes  Vapnik, 1995. One canenvisage using the soft margin value or a similar quantityto approximate the size of the version space when dealing with an hypothesis class of Soft Margin SVMs. Indeed,preliminary experiments for algorithms that work alongthese lines have produced promising results. We would liketo extend our analysis for this scenario as well as apply ouractive learning algorithms to a wider class of domains.AcknowledgementsThis work was supported by DARPAs Information Assurance program under subcontract to SRI International, andby ARO grant DAAH049610341 under the MURI program Integrated Approach to Intelligent Systems.ReferencesBurges, C. J. 1998. A tutorial on support vector machines forpattern recognition. Data Mining and Knowledge Discovery,2, 121167.Cortes, C.,  Vapnik, V. 1995. Support vector networks. Machine Learning, 20, 125.Dagan, I.,  Engelson, S. 1995. Committeebased samplingfor training probabilistic classifiers. Proceedings of the TwelfthInternational Conference on Machine Learning pp. 150157.Morgan Kaufmann.Dumais, S., Platt, J., Heckerman, D.,  Sahami, M. 1998. Inductive learning algorithms and representations for text categorization. Proceedings of the Seventh International Conferenceon Information and Knowledge Management. ACM Press.Freund, Y., Seung, H., Shamir, E.,  Tishby, N. 1997. Selectivesampling using the Query by Committee algorithm. MachineLearning, 28, 133168.Herbrich, R., Graepel, T.,  Campbell, C. 1999. Bayes pointmachines Estimating the bayes point in kernel space. International Joint Conference on Artificial Intelligence Workshop onSupport Vector Machines pp. 2327.Joachims, T. 1998. Text categorization with support vector machines. Proceedings of the European Conference on MachineLearning. SpringerVerlag.Joachims, T. 1999. Transductive inference for text classificationusing support vector machines. Proceedings of the SixteenthInternational Conference on Machine Learning pp. 200209.Morgan Kaufmann.Lewis, D.,  Catlett, J. 1994. Heterogeneous uncertainty sampling for supervised learning. Proceedings of the Eleventh International Conference on Machine Learning pp. 148156.Morgan Kaufmann.Lewis, D.,  Gale, W. 1994. A sequential algorithm for trainingtext classifiers. Proceedings of the Seventeenth Annual International ACMSIGIR Conference on Research and Developmentin Information Retrieval pp. 312. SpringerVerlag.McCallum, A.,  Nigam, K. 1998. Employing EM in poolbased active learning for text classification. Proceedings ofthe Fifteenth International Conference on Machine Learning.Morgan Kaufmann.Mitchell, T. 1982. Generalization as search. Artificial Intelligence, 28, 203226.Rocchio, J. 1971. Relevance feedback in information retrieval.The SMART retrival system Experiments in automatic document processing. PrenticeHall.Seung, H., Opper, M.,  Sompolinsky, H. 1992. Query by committee. Proceedings of the Fifth Workshop on ComputationalLearning Theory pp. 287294. Morgan Kaufmann.Vapnik, V. 1982. Estimation of dependences based on empiricaldata. Springer Verlag.Vapnik, V. 1998. Statistical learning theory. Wiley.
