What is the realitionship between deep learning methods and reservoir computing (if any)? - ResearchGate For full functionality of ResearchGate it is necessary to enable JavaScript. Here are the instructions how to enable JavaScript in your web browser . Aureli Soria-Frisch Starlab Barcelona SL What is the realitionship between deep learning methods and reservoir computing (if any)? I have the intuition that these two types of methodologies are related but I could not find any references nor any clear explanation of this relationship besides the fact that they are 2 types of modern, novel and evolved artificial neural networks. Topics Cognitive Science and Artificial Thinking × 187 Questions 20,908 Followers Follow Artificial Neural Networks × 745 Questions 60,913 Followers Follow Artificial Intelligence × 1,608 Questions 190,909 Followers Follow Computer Vision × 832 Questions 22,238 Followers Follow Machine Learning × 1,723 Questions 30,905 Followers Follow Computational Intelligence × 323 Questions 22,228 Followers Follow Time Series Forecasting × 113 Questions 1,025 Followers Follow Classification Algorithms × 292 Questions 1,425 Followers Follow Soft Computing × 221 Questions 7,211 Followers Follow Pattern Recognition × 853 Questions 36,946 Followers Follow Advanced Machine Learning × 614 Questions 26,026 Followers Follow Jul 18, 2014 Recommend 1 Recommendation Popular Answers Antonio Valerio Miceli Barone · The University of Edinburgh Reservoir computing generally refers to some kinds of recurrent neural networks where only the parameters of the final, non-recurrent output layer (known as the readout layer) are trained, while all the other parameters are randomly initialized subject to some condition that essentially prevents chaotic behavior and then they are left untrained. There is also a non-recurrent analogue of reservoir computing, which undergoes various names including "extreme neural networks", that consists of plain feed-forward neural networks where only the readout layer is trained. All these methods can be considered to belong to the larger class of "random projection" techniques. You can "unfold" a recurrent neural network into a feed-forward, generally deep, neural network where the internal layers are time-shifted replicas of each others. This is the intuition behind the backpropagation-through-time training algorithm. In fact, if you train a deep neural network with vanilla backpropagation, or a recurrent neural network with vanilla backpropagation-through-time, you often observe that the parameters in the hidden/recurrent layers don't change much from the random values they got at initialization, due to an issue known as the "vanishing gradient problem" (there is also an "exploding gradient problem" that can cause chaotic behavior and numerical instability in some cases). This is where reservoir computing and deep learning part ways: "Extreme"/Reservoir computing argues that since backpropagation/backpropagation-through-time is computationally very expensive but typically doesn't affect much the internal layers and it can run into chaotic behavior and numerical instability, we can often avoid it altogether and only train the readout layer for a small fraction of the computational cost (since it is a generalized linear classification/regression problem), while avoiding any instability by enforcing a simple constraint on the random parameters of the internal layers. This works very well for some problems. Deep learning, on the other hand, argues that there are very hard problems that  really do benefit from the training of the internal layers, and develops training algorithms, such as staged autoencoder pre-training, designed to overcome the limitations of vanilla backpropagation. ... · Recommend 5 Recommendations All Answers (6) Larry M. Manevitz · University of Haifa Dear Aureli, 1) rReservoir Computing essentially is using the path of iterative updating on recurrent networks as an indication of the input.  Since the input can occur at different times, it is in principle a method for spatial-temporal pattern recognition.  See the articles for Hananel Hazan for recent work on this.   The original articles are by Maass (Liquid State Machines) and Jaeger (Echo computing). ( The iterations, on the one hand, cause distinct patterns to diverge making potential classification in principle easier.) On the other hand, deep learning methodologies are (at least in the basic formulation) a way to allow feed forward networks with many levels to make use of their potential power. Thus, they are two very different ideas; one is essentially about static pattern recognition, while the other is about dynamic patterns. However, having said that,  they can potentially be connected in many ways.  For example, in reservoir computing, typically a "detector" that looks at the patterns can be any good classifier, and in particular, it might be very useful to use the power of deep learning classifiers for this part. In addition, one might consider investigating the deep learning paradigm for training the interconnections in the reservoir level; however this is still a research stretch. I hope this helps you. Regards, manevitz@cs.haifa.ac.il ... · Recommend 4 Recommendations Antonio Valerio Miceli Barone · The University of Edinburgh Reservoir computing generally refers to some kinds of recurrent neural networks where only the parameters of the final, non-recurrent output layer (known as the readout layer) are trained, while all the other parameters are randomly initialized subject to some condition that essentially prevents chaotic behavior and then they are left untrained. There is also a non-recurrent analogue of reservoir computing, which undergoes various names including "extreme neural networks", that consists of plain feed-forward neural networks where only the readout layer is trained. All these methods can be considered to belong to the larger class of "random projection" techniques. You can "unfold" a recurrent neural network into a feed-forward, generally deep, neural network where the internal layers are time-shifted replicas of each others. This is the intuition behind the backpropagation-through-time training algorithm. In fact, if you train a deep neural network with vanilla backpropagation, or a recurrent neural network with vanilla backpropagation-through-time, you often observe that the parameters in the hidden/recurrent layers don't change much from the random values they got at initialization, due to an issue known as the "vanishing gradient problem" (there is also an "exploding gradient problem" that can cause chaotic behavior and numerical instability in some cases). This is where reservoir computing and deep learning part ways: "Extreme"/Reservoir computing argues that since backpropagation/backpropagation-through-time is computationally very expensive but typically doesn't affect much the internal layers and it can run into chaotic behavior and numerical instability, we can often avoid it altogether and only train the readout layer for a small fraction of the computational cost (since it is a generalized linear classification/regression problem), while avoiding any instability by enforcing a simple constraint on the random parameters of the internal layers. This works very well for some problems. Deep learning, on the other hand, argues that there are very hard problems that  really do benefit from the training of the internal layers, and develops training algorithms, such as staged autoencoder pre-training, designed to overcome the limitations of vanilla backpropagation. ... · Recommend 5 Recommendations Aureli Soria-Frisch · Starlab Barcelona SL Thanks for both answers. So summarizing and as far as I understand: bot methods are recurrent multi-layer neural networks. They differ in the learning algorithms. While RC use algorithms that only train the read-out layer (and this seems to be sufficient for some problems), deep learning techniques claim the need to train all layers (although the changes in other layers than the read-out one are minimal). If this is true, deep learning seem to me a generalization of reservoir computing. ... · Recommend Antonio Valerio Miceli Barone · The University of Edinburgh Deep learning doesn't necessarily involve recurrent neural networks. In fact, most research in deep learning is done on feed-forward neural networks. A feed-forward neural network is generally considered to be deep if it has more than one hidden layer. A recurrent neural network, when unfolded over time for an example of duration T, essentially becomes a feed-forward neural netowork with k*T hidden layers (where k is a constant usually equal to one). Since training many hidden layers using standard backpropagation techniques is difficult, extreme/reservoir computing gives up and just trains the output layer, while deep learning trains all the layers using techniques that extend standard backpropagation. ... · Recommend 1 Recommendation Daniel Brunner · Institut FEMTO-ST Dear Aureli, we have implemented Reservoir Computing in electronic, optoelectronic and all-optical (lasers) hardware, reaching information injections rates above 1 GSample/s and good performance values. For us, one of the most important features of Reservoir Computing is its conceptual simplicity, strongly fostering such implementations. In case you want to know more I would be happy to send you some publications regarding hardware implementations of Reservoir Computing. ... · Recommend Can you help by adding an answer? Add your answer Question followers (20) See all Graeme Smith   Gabriele Scheler   Carl Correns Foundation for Mathematical Biology Oswaldo Ludwig   Zalando Komala Anamalamudi   Madanapalle Institute of Technology & Science Devis Pantano   University of Padova Ali Naderi   Semnan University Blessing Ojeme   University of Cape Town Martin Dinov   Imperial College London Daniel Brunner   Institut FEMTO-ST Vishal A. Bhalla   Technische Universität München Similar Questions Does anybody know how we can add the missing citations to our profile in Google Scholar? 81 answers added Which are the new trends in the development of ANNs? 12 answers added Why do liquid state machines work despite no training in the reservoir? 10 answers added Is STDP a complete learning algorithm? 5 answers added Why Extreme Learning machine is not so popular as Deep Learning? 12 answers added Views 3597 Followers 20 Answers 6 © 2008- 2017 researchgate.net. All rights reserved. About us  ·  Help Center  ·  Careers  ·  Developers  ·  News  ·  Contact us  ·  Privacy  ·  Terms  ·  Copyright  |  Advertising  ·  Recruiting
