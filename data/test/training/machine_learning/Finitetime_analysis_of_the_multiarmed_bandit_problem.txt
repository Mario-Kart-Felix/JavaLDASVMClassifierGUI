Machine Learning, 47, 235256, 2002c 2002 Kluwer Academic Publishers. Manufactured in The Netherlands.Finitetime Analysis of the Multiarmed BanditProblemPETER AUER pauerigi.tugraz.ac.atUniversity of Technology Graz, A8010 Graz, AustriaNICOLO CESABIANCHI cesabianchidti.unimi.itDTI, University of Milan, via Bramante 65, I26013 Crema, ItalyPAUL FISCHER fischerls2.informatik.unidortmund.deLehrstuhl Informatik II, Universitat Dortmund, D44221 Dortmund, GermanyEditor Jyrki KivinenAbstract. Reinforcement learning policies face the exploration versus exploitation dilemma, i.e. the search fora balance between exploring the environment to find profitable actions while taking the empirically best action asoften as possible. A popular measure of a policys success in addressing this dilemma is the regret, that is the lossdue to the fact that the globally optimal policy is not followed all the times. One of the simplest examples of theexplorationexploitation dilemma is the multiarmed bandit problem. Lai and Robbins were the first ones to showthat the regret for this problem has to grow at least logarithmically in the number of plays. Since then, policieswhich asymptotically achieve this regret have been devised by Lai and Robbins and many others. In this work weshow that the optimal logarithmic regret is also achievable uniformly over time, with simple and efficient policies,and for all reward distributions with bounded support.Keywords bandit problems, adaptive allocation rules, finite horizon regret1. IntroductionThe exploration versus exploitation dilemma can be described as the search for a balancebetween exploring the environment to find profitable actions while taking the empiricallybest action as often as possible. The simplest instance of this dilemma is perhaps themultiarmed bandit, a problem extensively studied in statistics Berry  Fristedt, 1985 thathas also turned out to be fundamental in different areas of artificial intelligence, such asreinforcement learning Sutton  Barto, 1998 and evolutionary programming Holland,1992.In its most basic formulation, a K armed bandit problem is defined by random variablesXi,n for 1  i  K and n  1, where each i is the index of a gambling machine i.e., thearm of a bandit. Successive plays of machine i yield rewards Xi,1, Xi,2, . . . which areA preliminary version appeared in Proc. of 15th International Conference on Machine Learning, pages 100108.Morgan Kaufmann, 1998236 P. AUER, N. CESABIANCHI AND P. FISCHERindependent and identically distributed according to an unknown law with unknown expectation i . Independence also holds for rewards across machines i.e., Xi,s and X j,t areindependent and usually not identically distributed for each 1  i  j  K and eachs, t  1.A policy, or allocation strategy, A is an algorithm that chooses the next machine to playbased on the sequence of past plays and obtained rewards. Let Ti n be the number of timesmachine i has been played by A during the first n plays. Then the regret of A after n playsis defined byn   jKj1IETj n where  def max1iKiand IE denotes expectation. Thus the regret is the expected loss due to the fact that thepolicy does not always play the best machine.In their classical paper, Lai and Robbins 1985 found, for specific families of rewarddistributions indexed by a single real parameter, policies satisfyingIETj n 1Dp jp  o1ln n 1where o1  0 as n   andDp jp defp j lnp jpis the KullbackLeibler divergence between the reward density p j of any suboptimal machine j and the reward density p of the machine with highest reward expectation .Hence, under these policies the optimal machine is played exponentially more often thanany other machine, at least asymptotically. Lai and Robbins also proved that this regret isthe best possible. Namely, for any allocation strategy and for any suboptimal machine j ,IETj n  ln nDp jp asymptotically, provided that the reward distributions satisfysome mild assumptions.These policies work by associating a quantity called upper confidence index to each machine. The computation of this index is generally hard. In fact, it relies on the entire sequenceof rewards obtained so far from a given machine. Once the index for each machine is computed, the policy uses it as an estimate for the corresponding reward expectation, pickingfor the next play the machine with the current highest index. More recently, Agrawal 1995introduced a family of policies where the index can be expressed as simple function ofthe total reward obtained so far from the machine. These policies are thus much easier tocompute than Lai and Robbins, yet their regret retains the optimal logarithmic behaviorthough with a larger leading constant in some cases.1In this paper we strengthen previous results by showing policies that achieve logarithmicregret uniformly over time, rather than only asymptotically. Our policies are also simple toimplement and computationally efficient. In Theorem 1 we show that a simple variant ofAgrawals indexbased policy has finitetime regret logarithmically bounded for arbitrarysets of reward distributions with bounded support a regret with better constants is provenFINITETIME ANALYSIS 237in Theorem 2 for a more complicated version of this policy. A similar result is shownin Theorem 3 for a variant of the wellknown randomized greedy heuristic. Finally, inTheorem 4 we show another indexbased policy with logarithmically bounded finitetimeregret for the natural case when the reward distributions are normally distributed withunknown means and variances.Throughout the paper, and whenever the distributions of rewards for each machine areunderstood from the context, we defineidef   iwhere, we recall, i is the reward expectation for machine i and  is any maximal elementin the set 1, . . . , K .2. Main resultsOur first result shows that there exists an allocation strategy, UCB1, achieving logarithmicregret uniformly over n and without any preliminary knowledge about the reward distributions apart from the fact that their support is in 0, 1. The policy UCB1 sketched infigure 1 is derived from the indexbased policy of Agrawal 1995. The index of this policyis the sum of two terms. The first term is simply the current average reward. The second termis related to the size according to ChernoffHoeffding bounds, see Fact 1 of the onesidedconfidence interval for the average reward within which the true expected reward falls withoverwhelming probability.Theorem 1. For all K  1, if policy UCB1 is run on K machines having arbitrary rewarddistributions P1, . . . , PK with support in 0, 1, then its expected regret after any numbern of plays is at most8i i ln ni1  23 Kj1 jwhere 1, . . . , K are the expected values of P1, . . . , PK .Figure 1. Sketch of the deterministic policy UCB1 see Theorem 1.238 P. AUER, N. CESABIANCHI AND P. FISCHERFigure 2. Sketch of the deterministic policy UCB2 see Theorem 2.To prove Theorem 1 we show that, for any suboptimal machine j ,IETj n  82jln n 2plus a small constant. The leading constant 82i is worse than the corresponding constant1Dp j  p in Lai and Robbins result 1. In fact, one can show that Dp j  p  22jwhere the constant 2 is the best possible.Using a slightly more complicated policy, which we call UCB2 see figure 2, we can bringthe main constant of 2 arbitrarily close to 122j . The policy UCB2 works as follows.The plays are divided in epochs. In each new epoch a machine i is picked and thenplayed ri  1  ri  times, where  is an exponential function and ri is the number ofepochs played by that machine so far. The machine picked in each new epoch is the onemaximizing xi  an,ri , where n is the current number of plays, xi is the current averagereward for machine i , andan,r 1   lnenr2r3wherer  1  r.In the next result we state a bound on the regret of UCB2. The constant c , here left unspecified, is defined in 18 in the appendix, where the theorem is also proven.Theorem 2. For all K  1, if policy UCB2 is run with input 0    1 on K machineshaving arbitrary reward distributions P1, . . . , PK with support in 0, 1, then its expectedregret after any numbern  maxi i 122iFINITETIME ANALYSIS 239of plays is at mosti  i 1  1  4 ln 2e2i n2i ci4where 1, . . . , K are the expected values of P1, . . . , PK .Remark. By choosing  small, the constant of the leading term in the sum 4 gets arbitrarily close to 122i  however, c   as   0. The two terms in the sum can betradedoff by letting   n be slowly decreasing with the number n of plays.A simple and wellknown policy for the bandit problem is the socalled greedy rulesee Sutton,  Barto, 1998. This policy prescribes to play with probability 1 the machinewith the highest average reward, and with probability  a randomly chosen machine. Clearly,the constant exploration probability  causes a linear rather than logarithmic growth inthe regret. The obvious fix is to let  go to zero with a certain rate, so that the explorationprobability decreases as our estimates for the reward expectations become more accurate.It turns out that a rate of 1n, where n is, as usual, the index of the current play, allowsto prove a logarithmic bound on the regret. The resulting policy, nGREEDY, is shown infigure 3.Theorem 3. For all K  1 and for all reward distributions P1, . . . , PK with support in0, 1, if policy nGREEDY is run with input parameter0  d  mini i i ,Figure 3. Sketch of the randomized policy nGREEDY see Theorem 3.240 P. AUER, N. CESABIANCHI AND P. FISCHERthen the probability that after any number n  cKd of plays nGREEDY chooses a suboptimal machine j is at mostcd2n 2cd2lnn  1d2e12cKcKn  1d2e12c5d2 4ed2cKn  1d2e12c2.Remark. For c large enough e.g. c  5 the above bound is of order cd2n  o1n forn  , as the second and third terms in the bound are O1n1 for some   0 recallthat 0  d  1. Note also that this is a result stronger than those of Theorems 1 and 2, asit establishes a bound on the instantaneous regret. However, unlike Theorems 1 and 2, herewe need to know a lower bound d on the difference between the reward expectations of thebest and the second best machine.Our last result concerns a special case, i.e. the bandit problem with normally distributedrewards. Surprisingly, we could not find in the literature regret bounds not even asymptotical for the case when both the mean and the variance of the reward distributions areunknown. Here, we show that an indexbased policy called UCB1NORMAL, see figure 4,achieves logarithmic regret uniformly over n without knowing means and variances of thereward distributions. However, our proof is based on certain bounds on the tails of the 2and the Student distribution that we could only verify numerically. These bounds are statedas Conjecture 1 and Conjecture 2 in the Appendix.The choice of the index in UCB1NORMAL is based, as for UCB1, on the size of the onesided confidence interval for the average reward within which the true expected reward fallswith overwhelming probability. In the case of UCB1, the reward distribution was unknown,and we used ChernoffHoeffding bounds to compute the index. In this case we know thatFigure 4. Sketch of the deterministic policy UCB1NORMAL see Theorem 4.FINITETIME ANALYSIS 241the distribution is normal, and for computing the index we use the sample variance as anestimate of the unknown variance.Theorem 4. For all K  1, if policy UCB1NORMAL is run on K machines having normalreward distributions P1, . . . , PK , then its expected regret after any number n of plays is atmost256log n i i  2ii1  22 8 log nKj1 jwhere 1, . . . , K and  21 , . . . , 2K are the means and variances of the distributionsP1, . . . , PK .As a final remark for this section, note that Theorems 13 also hold for rewards that are notindependent across machines, i.e. Xi,s and X j,t might be dependent for any s, t , and i  j .Furthermore, we also do not need that the rewards of a single arm are i.i.d., but only theweaker assumption that IE Xi,t  Xi,1, . . . , Xi,t1  i for all 1  t  n.3. ProofsRecall that, for each 1  i  K , IEXi,n  i for all n  1 and   max1iK i . Also,for any fixed policy A, Ti n is the number of times machine i has been played by A in thefirst n plays. Of course, we always haveKi1 Ti n  n. We also define the r.v.s I1, I2, . . .,where It denotes the machine played at time t .For each 1  i  K and n  1 defineXi,n  1nnt1Xi,t .Given 1, . . . , K , we call optimal the machine with the least index i such that i  .In what follows, we will always put a superscript    to any quantity which refers to theoptimal machine. For example we write T n and Xn instead of Ti n and Xi,n , where i isthe index of the optimal machine.Some further notation For any predicate  we define x to be the indicator fuctionof the event x i.e., x  1 if x is true and x  0 otherwise. Finally,VarX  denotes the variance of the random variable X .Note that the regret after n plays can be written asj  j  j IETj n 5So we can bound the regret by simply bounding each IETj n.We will make use of the following standard exponential inequalities for bounded randomvariables see, e.g., the appendix of Pollard, 1984.242 P. AUER, N. CESABIANCHI AND P. FISCHERFact 1 ChernoffHoeffding bound. Let X1, . . . , Xn be random variables with commonrange 0, 1 and such that IEXt X1, . . . , Xt1  . Let Sn  X1      Xn. Then forall a  0IPSn  n  a  e2a2n and IPSn  n  a  e2a2nFact 2 Bernstein inequality. Let X1, . . . , Xn be random variables with range 0, 1 andnt1VarXt  Xt1, . . . , X1   2.Let Sn  X1      Xn. Then for all a  0IPSn  IESn  a  exp a22 2  a2.Proof of Theorem 1 Let ct,s 2 ln ts. For any machine i , we upper bound Ti non any sequence of plays. More precisely, for each t  1 we bound the indicator functionof It  i as follows. Let  be an arbitrary positive integer.Ti n  1 ntK1It  i  ntK1It  i, Ti t  1    ntK1XT t1  ct1,T t1  Xi,Ti t1 ct1,Ti t1, Ti t  1    ntK1min0stXs  ct1,s  maxsi tX i,si  ct1,si  t1t1s1t1si Xs  ct,s  Xi,si  ct,si. 6Now observe that Xs  ct,s  Xi,si  ct,si implies that at least one of the following mustholdXs    ct,s 7Xi,si  i  ct,si 8  i  2ct,si . 9We bound the probability of events 7 and 8 using Fact 1 ChernoffHoeffding boundIPXs    ct,s  e4 ln t  t4FINITETIME ANALYSIS 243IPXi,si  i  ct,si  e4 ln t  t4.For   8 ln n2i , 9 is false. In fact  i  2ct,si    i  22ln tsi    i  i  0for si  8 ln n2i . So we getIETi n 8 ln n2it1t1s1t1si 8 ln n2i  IPXs    ct,s  IPXi,si  i  ct,si 8 ln n2it1ts1tsi 12t4 8 ln n2i 1  23which concludes the proof. Proof of Theorem 3 Recall that, for n  cKd2, n  cKd2n. Letx0  12Knt1t .The probability that machine j is chosen at time n isIPIn  j  nK1  nKIPX j,Tj n1  XT n1andIPX j,Tj n  XT n IPX j,Tj n   j  j2 IPXT n    j2. 10Now the analysis for both terms on the righthand side is the same. Let T Rj n be the numberof plays in which machine j was chosen at random in the first n plays. Then we haveIPX j,Tj n   j  j2nt1IPTj n  t  X j,t   j   j211244 P. AUER, N. CESABIANCHI AND P. FISCHERnt1IPTj n  t  X j,t   j   j2 IPX j,t   j   j2nt1IPTj n  t  X j,t   j   j2 e2j t2by Fact 1 ChernoffHoeffding boundx0t1IPTj n  t  X j,t   j   j2 22je2j x02sincetx1 et  1exx0t1IPT Rj n  t  X j,t   j  j2 22je2j x02 x0  IPT Rj n  x0  22je2j x02 12where in the last line we dropped the conditioning because each machine is played at randomindependently of the previous choices of the policy. SinceIET Rj n  1Knt1tandVarT Rj n  nt1tK1  tK 1Knt1t ,by Bernsteins inequality 2 we getIPT Rj n  x0  ex05. 13Finally it remains to lower bound x0. For n  n  cKd2, n  cKd2n and we havex0  12Knt1t 12Knt1t  12Kntn1t n2K cd2lnnn cd2lnnd2e12cK.FINITETIME ANALYSIS 245Thus, using 1013 and the above lower bound on x0 we obtainIPIn  j  nK 2x0ex05  42je2j x02 cd2n 2cd2lnn  1d2e12cKcKn  1d2e12c5d2 4ed2cKn  1d2e12c2.This concludes the proof. 4. ExperimentsFor practical purposes, the bound of Theorem 1 can be tuned more finely. We useVj sdef1ss1X2j, X2j,s 2 ln tsas un upper confidence bound for the variance of machine j . As before, this means thatmachine j , which has been played s times during the first t plays, has a variance that isat most the sample variance plus2 ln ts. We then replace the upper confidence bound2 lnnn j of policy UCB1 withln nn jmin14, Vj n j the factor 14 is an upper bound on the variance of a Bernoulli random variable. Thisvariant, which we call UCB1TUNED, performs substantially better than UCB1 in essentiallyall of our experiments. However, we are not able to prove a regret bound.We compared the empirical behaviour policies UCB1TUNED, UCB2, and nGREEDY onBernoulli reward distributions with different parameters shown in the table below.1 2 3 4 5 6 7 8 9 101 0.9 0.62 0.9 0.83 0.55 0.4511 0.9 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.612 0.9 0.8 0.8 0.8 0.7 0.7 0.7 0.6 0.6 0.613 0.9 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.814 0.55 0.45 0.45 0.45 0.45 0.45 0.45 0.45 0.45 0.45246 P. AUER, N. CESABIANCHI AND P. FISCHERRows 13 define reward distributions for a 2armed bandit problem, whereas rows 1114 define reward distributions for a 10armed bandit problem. The entries in each rowdenote the reward expectations i.e. the probabilities of getting a reward 1, as we work withBernoulli distributions for the machines indexed by the columns. Note that distributions 1and 11 are easy the reward of the optimal machine has low variance and the differences  i are all large, whereas distributions 3 and 14 are hard the reward of the optimalmachine has high variance and some of the differences   i are small.We made experiments to test the different policies or the same policy with differentinput parameters on the seven distributions listed above. In each experiment we trackedtwo performance measures 1 the percentage of plays of the optimal machine 2 theactual regret, that is the difference between the reward of the optimal machine and thereward of the machine played. The plot for each experiment shows, on a semilogarithmicscale, the behaviour of these quantities during 100,000 plays averaged over 100 differentruns. We ran a first round of experiments on distribution 2 to find out good values for theparameters of the policies. If a parameter is chosen too small, then the regret grows linearlyexponentially in the semilogarithmic plot if a parameter is chosen too large then theregret grows logarithmically, but with a large leading constant corresponding to a steepline in the semilogarithmic plot.Policy UCB2 is relatively insensitive to the choice of its parameter , as long as it iskept relatively small see figure 5. A fixed value 0.001 has been used for all the remainingexperiments. On other hand, the choice of c in policy nGREEDY is difficult as there is novalue that works reasonably well for all the distributions that we considered. Therefore, wehave roughly searched for the best value for each distribution. In the plots, we will alsoshow the performance of nGREEDY for values of c around this empirically best value. Thisshows that the performance degrades rapidly if this parameter is not appropriately tuned.Finally, in each experiment the parameter d of nGREEDY was set to    maxi i i .Figure 5. Search for the best value of parameter  of policy UCB2.FINITETIME ANALYSIS 2474.1. Comparison between policiesWe can summarize the comparison of all the policies on the seven distributions as followssee Figs. 612. An optimally tuned nGREEDY performs almost always best. Significant exceptions aredistributions 12 and 14 this is because nGREEDY explores uniformly over all machines,thus the policy is hurt if there are several nonoptimal machines, especially when theirreward expectations differ a lot. Furthermore, if nGREEDY is not well tuned its performance degrades rapidly except for distribution 13, on which nGREEDY performs wella wide range of values of its parameter. In most cases, UCB1TUNED performs comparably to a welltuned nGREEDY. Furthermore, UCB1TUNED is not very sensitive to the variance of the machines, that is why itperforms similarly on distributions 2 and 3, and on distributions 13 and 14. Policy UCB2 performs similarly to UCB1TUNED, but always slightly worse.Figure 6. Comparison on distribution 1 2 machines with parameters 0.9, 0.6.Figure 7. Comparison on distribution 2 2 machines with parameters 0.9, 0.8.248 P. AUER, N. CESABIANCHI AND P. FISCHERFigure 8. Comparison on distribution 3 2 machines with parameters 0.55, 0.45.Figure 9. Comparison on distribution 11 10 machines with parameters 0.9, 0.6, . . . , 0.6.Figure 10. Comparison on distribution 12 10 machines with parameters 0.9, 0.8, 0.8, 0.8, 0.7, 0.7, 0.7, 0.6,0.6, 0.6.FINITETIME ANALYSIS 249Figure 11. Comparison on distribution 13 10 machines with parameters 0.9, 0.8, . . . , 0.8.Figure 12. Comparison on distribution 14 10 machines with parameters 0.55, 0.45, . . . , 0.45.5. ConclusionsWe have shown simple and efficient policies for the bandit problem that, on any set of rewarddistributions with known bounded support, exhibit uniform logarithmic regret. Our policiesare deterministic and based on upper confidence bounds, with the exception of nGREEDY,a randomized allocation rule that is a dynamic variant of the greedy heuristic. Moreover,our policies are robust with respect to the introduction of moderate dependencies in thereward processes.This work can be extended in many ways. A more general version of the bandit problemis obtained by removing the stationarity assumption on reward expectations see Berry Fristedt, 1985 Gittins, 1989 for extensions of the basic bandit problem. For example,suppose that a stochastic reward process Xi,s  s  1, 2, . . . is associated to each machinei  1, . . . , K . Here, playing machine i at time t yields a reward Xi,s and causes the current250 P. AUER, N. CESABIANCHI AND P. FISCHERstate s of i to change to s  1, whereas the states of other machines remain frozen. A wellstudied problem in this setup is the maximization of the total expected reward in a sequenceof n plays. There are methods, like the Gittins allocation indices, that allow to find theoptimal machine to play at each time n by considering each reward process independentlyfrom the others even though the globally optimal solution depends on all the processes.However, computation of the Gittins indices for the average undiscounted reward criterionused here requires preliminary knowledge about the reward processes see, e.g., Ishikida Varaiya, 1994. To overcome this requirement, one can learn the Gittins indices, as proposedin Duff 1995 for the case of finitestate Markovian reward processes. However, there are nofinitetime regret bounds shown for this solution. At the moment, we do not know whetherour techniques could be extended to these more general bandit problems.Appendix A Proof of Theorem 2Note thatr  1  r  1  r  11    1 14for r  1. Assume that n  122j  for all j and let r j be the largest integer such thatr j  1 1  4 ln 2en2j22j.Note that r j  1. We haveTj n  1 r1 r  r  1 machine j finishes its r th epoch r j  rr j r  r  1 machine j finishes its r th epochNow consider the following chain of implicationsmachine j finishes its r th epoch i  0, t  r  1  i such thatX j, r1  at,r1  Xi  at,i t  r  1 such that X j, r1  at,r1     j2or i  0, t   r  1  i such thatXi  at ,i     j2 X j, r1  an,r1     j2or i  0 such that Xi  ar1i,i     j2where the last implication hold because at,r is increasing in t . HenceIETj n   r j  rr j r  r  1IPX j, r1  an,r1     j2FINITETIME ANALYSIS 251rr ji0 r  r  1 IPXi  ar1i,i     j2. 15The assumption n  122j  implies ln2en2j   1. Therefore, for r  r j , we haver  1  1  4 ln2en2j22j16andan,r1 1   lnenr  12r  1  j1   lnenr  11  4 ln2en2j using 16 above  j 1   ln2en2j1  4 ln2en2jusing r  1  122j derived from 16  j1  1  4 . 17We start by bounding the first sum in 15. Using 17 and Fact 1 ChernoffHoeffdingbound we getIPX j, r1  an,r1     j2 IPX j, r1  an,r1   j   j   j2 exp  2r  12j1  2  1  1  42 exp2r  12j 1  2  1  2 exp r  12j22for   110. Now let gx  x  11  . By 14 we get gx  r  1 forr  1  x  r and r  1. Hencerr j r  r  1IPX j, r1  an,r1     j2rr j r  r  1 exp  r  12j2 0ecgxdx252 P. AUER, N. CESABIANCHI AND P. FISCHERwhere c   j2  1. Further manipulation yields 0exp c1   x  1dx  ec1 1  c 1  e j2.We continue by bounding the second sum in 15. Using once more Fact 1, we getrr ji0 r  r  1IPXi  ar1i,i     j2i0rr j r  r  1 exp i  j 22 1   lner  1  i ii0expi j 22 rr j r  r  1 exp 1   ln1  r  1 ii0expi j 22 rr j r  r  11  r  1 i1i0expi j 22  01  x  11  i1dxi0i expi j 221  1  11  ii0i expi j 221  1  as i  11  1 i0i expi j 22.Now, as 1  x1  i  1  x  1 for i  x  i  1, we can bound the series inthe last formula above with an integrali0i expi j 22 1  11  x  1 exp1  x1 j 22 dxFINITETIME ANALYSIS 253 1  1z  1z ln1   exp z j 221  dzby change of variable z  1  x 1  1ln1  e exxdxwhere we set   j 221   .As 0  ,  j  1, we have 0    14. To upper bound the bracketed formula above,consider the functionF  e   exxdxwith derivativesF   exxdx  2e F   2e  exxdx .In the interval 0, 14, F  is seen to have a zero at   0.0108 . . .. As F   0 in thesame interval, this is the unique maximum of F , and we find F0.0108 . . .  1110. Sowe havee exxdx 1110 111  5 j 2Piecing everything together, and using 14 to upper bound  r j , we find thatIETj n   r j   1  e j21  11  111  5 j 2 ln1   1  1  4 ln2en2j22j c2jwherec  1  1  e21  11  111  52 ln1  . 18This concludes the proof. 254 P. AUER, N. CESABIANCHI AND P. FISCHERAppendix B Proof of Theorem 4The proof goes very much along the same lines as the proof of Theorem 1. It is based onthe two following conjectures which we only verified numerically.Conjecture 1. Let X be a Student random variable with s degrees of freedom. Then, forall 0  a  2s  1,IPX  a  ea24.Conjecture 2. Let X be a 2 random variable with s degrees of freedom. ThenIPX  4s  es12.We now proceed with the proof of Theorem 4. LetQi,n nt1X2i,t .Fix a machine i and, for any s and t , setct,s 16  Qi,s  s X2i,ss  1 ln tsLet ct,s be the corresponding quantity for the optimal machine. To upper bound Ti n, weproceed exactly as in the first part of the proof of Theorem 1 obtaining, for any positiveinteger ,Ti n   t1t1s1t1si Xs    ct,s  Xi,si  i  ct,si    i  2ct,si .The random variable Xi,si i Qi,si  si X2i,si si si  1 has a Student distributionwith si  1 degrees of freedom see, e.g., Wilks, 1962, 8.4.3 page 211. Therefore, usingConjecture 1 with s  si  1 and a  4ln t , we getIPXi,si  i  ct,si  IP Xi,si  iQi,si  si X2i,si si si  1 4ln t  t4for all si  8 ln t . The probability of Xs    ct,s is bounded analogously. Finally, sinceQi,si  si X2i,si  2i is 2distributed with si  1 degrees of freedom see, e.g., Wilks, 1962,FINITETIME ANALYSIS 2558.4.1 page 208. Therefore, using Conjecture 2 with s  si  1 and a  4s, we getIP  i  2ct,si  IP Qi,si  si X2i,si  2i  si  12i 2isi64 ln t IPQi,si  si X2i,si  2i  4si  1 esi 2  t4forsi  max256 2i2i, 8ln t.Setting max256 2i2i, 8ln tcompletes the proof of the theorem. AcknowledgmentsThe support from ESPRIT Working Group EP 27150, Neural and Computational LearningII NeuroCOLT II, is gratefully acknowledged.Note1. Similar extensions of Lai and Robbins results were also obtained by Yakowitz and Lowe 1991, and byBurnetas and Katehakis 1996.ReferencesAgrawal, R. 1995. Sample mean based index policies with Olog n regret for the multiarmed bandit problem.Advances in Applied Probability, 27, 10541078.Berry, D.,  Fristedt, B. 1985. Bandit problems. London Chapman and Hall.Burnetas, A.,  Katehakis, M. 1996. Optimal adaptive policies for sequential allocation problems. Advances inApplied Mathematics, 172, 122142.Duff, M. 1995. Qlearning for bandit problems. In Proceedings of the 12th International Conference on MachineLearning pp. 209217.Gittins, J. 1989. Multiarmed bandit allocation indices, WileyInterscience series in Systems and Optimization.New York John Wiley and Sons.Holland, J. 1992. Adaptation in natural and artificial systems. Cambridge MIT PressBradford Books.Ishikida, T.,  Varaiya, P. 1994. Multiarmed bandit problem revisited. Journal of Optimization Theory andApplications, 831, 113154.Lai, T.,  Robbins, H. 1985. Asymptotically efficient adaptive allocation rules. Advances in Applied Mathematics,6, 422.Pollard, D. 1984. Convergence of stochastic processes. Berlin Springer.256 P. AUER, N. CESABIANCHI AND P. FISCHERSutton, R.,  Barto, A. 1998. Reinforcement learning, an introduction. Cambridge MIT PressBradfordBooks.Wilks, S. 1962. Matematical statistics. New York John Wiley and Sons.Yakowitz, S.,  Lowe, W. 1991. Nonparametric bandit methods. Annals of Operations Research, 28, 297312.Received September 29, 2000Revised May 21, 2001Accepted June 20, 2001Final manuscript June 20, 2001
