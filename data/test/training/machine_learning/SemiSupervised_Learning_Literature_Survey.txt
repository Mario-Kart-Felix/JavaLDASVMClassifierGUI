SemiSupervised Learning Literature SurveyXiaojin ZhuComputer Sciences TR 1530University of Wisconsin  MadisonLast modified on December 9, 20061Contents1 FAQ 32 Generative Models 72.1 Identifiability . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72.2 Model Correctness . . . . . . . . . . . . . . . . . . . . . . . . . 72.3 EM Local Maxima . . . . . . . . . . . . . . . . . . . . . . . . . 102.4 ClusterandLabel . . . . . . . . . . . . . . . . . . . . . . . . . . 102.5 Fisher kernel for discriminative learning . . . . . . . . . . . . . . 103 SelfTraining 104 CoTraining 115 Avoiding Changes in Dense Regions 135.1 Transductive SVMs S3VMs . . . . . . . . . . . . . . . . . . . . 135.2 Gaussian Processes . . . . . . . . . . . . . . . . . . . . . . . . . 155.3 Information Regularization . . . . . . . . . . . . . . . . . . . . . 165.4 Entropy Minimization . . . . . . . . . . . . . . . . . . . . . . . . 165.5 A Connection to Graphbased Methods . . . . . . . . . . . . . . 166 GraphBased Methods 176.1 Regularization by Graph . . . . . . . . . . . . . . . . . . . . . . 176.1.1 Mincut . . . . . . . . . . . . . . . . . . . . . . . . . . . 176.1.2 Discrete Markov Random Fields Boltzmann Machines . . 186.1.3 Gaussian Random Fields and Harmonic Functions . . . . 186.1.4 Local and Global Consistency . . . . . . . . . . . . . . . 196.1.5 Tikhonov Regularization . . . . . . . . . . . . . . . . . . 196.1.6 Manifold Regularization . . . . . . . . . . . . . . . . . . 206.1.7 Graph Kernels from the Spectrum of Laplacian . . . . . . 206.1.8 Spectral Graph Transducer . . . . . . . . . . . . . . . . . 216.1.9 TreeBased Bayes . . . . . . . . . . . . . . . . . . . . . 216.1.10 Some Other Methods . . . . . . . . . . . . . . . . . . . . 226.2 Graph Construction . . . . . . . . . . . . . . . . . . . . . . . . . 226.3 Fast Computation . . . . . . . . . . . . . . . . . . . . . . . . . . 236.4 Induction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 246.5 Consistency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 256.6 Directed Graphs and Hypergraphs . . . . . . . . . . . . . . . . . 266.7 Connection to Standard Graphical Models . . . . . . . . . . . . . 2627 Computational Learning Theory 278 Semisupervised Learning in Structured Output Spaces 288.1 Generative Models . . . . . . . . . . . . . . . . . . . . . . . . . 288.2 Graphbased Kernels . . . . . . . . . . . . . . . . . . . . . . . . 289 Related Areas 299.1 Spectral Clustering . . . . . . . . . . . . . . . . . . . . . . . . . 299.2 Learning with Positive and Unlabeled Data . . . . . . . . . . . . 309.3 Semisupervised Clustering . . . . . . . . . . . . . . . . . . . . . 309.4 Semisupervised Regression . . . . . . . . . . . . . . . . . . . . 319.5 Active Learning and Semisupervised Learning . . . . . . . . . . 319.6 Nonlinear Dimensionality Reduction . . . . . . . . . . . . . . . . 329.7 Learning a Distance Metric . . . . . . . . . . . . . . . . . . . . . 329.8 Inferring Label Sampling Mechanisms . . . . . . . . . . . . . . . 359.9 MetricBased Model Selection . . . . . . . . . . . . . . . . . . . 3510 Scalability Issues of SemiSupervised Learning Methods 3611 Do Humans do SemiSupervised Learning 3611.1 Visual Object Recognition with Temporal Association . . . . . . . 3611.2 Infant WordMeaning Mapping . . . . . . . . . . . . . . . . . . . 381 FAQQ Whats in this DocumentA We review the literature on semisupervised learning, which is an area in machine learning and more generally, artificial intelligence. There has been a wholespectrum of interesting ideas on how to learn from both labeled and unlabeled data,i.e. semisupervised learning. This document is a chapter excerpt from the authorsdoctoral thesis Zhu, 2005. However the author plans to update the online versionfrequently to incorporate the latest development in the field. Please obtain the latestversion athttpwww.cs.wisc.edujerryzhupubssl survey.pdfPlease cite the survey using the following bibtex entrytechreportzhu05survey,author  Xiaojin Zhu,title  SemiSupervised Learning Literature Survey,institution  Computer Sciences, University of WisconsinMadison,number  1530,3year  2005,note  httpwww.cs.wisc.edusimjerryzhupubsslsurvey.pdfThe review is by no means comprehensive as the field of semisupervised learning is evolving rapidly. It is difficult for one person to summarize the field. Theauthor apologizes in advance for any missed papers and inaccuracies in descriptions. Corrections and comments are highly welcome. Please send them to jerryzhucs.wisc.edu.Q What is semisupervised learningA In this survey we focus on semisupervised classification. It is a special form ofclassification. Traditional classifiers use only labeled data feature  label pairs totrain. Labeled instances however are often difficult, expensive, or time consumingto obtain, as they require the efforts of experienced human annotators. Meanwhileunlabeled data may be relatively easy to collect, but there has been few ways to usethem. Semisupervised learning addresses this problem by using large amount ofunlabeled data, together with the labeled data, to build better classifiers. Becausesemisupervised learning requires less human effort and gives higher accuracy, itis of great interest both in theory and in practice.Semisupervised classifications cousins, semisupervised clustering and regression, are briefly discussed in section 9.3 and 9.4.Q Can we really learn anything from unlabeled data It sounds like magic.A Yes we can  under certain assumptions. Its not magic, but good matching ofproblem structure with model assumption.Many semisupervised learning papers, including this one, start with an introduction like labels are hard to obtain while unlabeled data are abundant, thereforesemisupervised learning is a good idea to reduce human labor and improve accuracy. Do not take it for granted. Even though you or your domain expert donot spend as much time in labeling the training data, you need to spend reasonableamount of effort to design good models  features  kernels  similarity functionsfor semisupervised learning. In my opinion such effort is more critical than forsupervised learning to make up for the lack of labeled training data.Q Does unlabeled data always helpA No, theres no free lunch. Bad matching of problem structure with model assumption can lead to degradation in classifier performance. For example, quite afew semisupervised learning methods assume that the decision boundary shouldavoid regions with high px. These methods include transductive support vector4machines TSVMs, information regularization, Gaussian processes with null category noise model, graphbased methods if the graph weights is determined by pairwise distance. Nonetheless if the data is generated from two heavily overlappingGaussian, the decision boundary would go right through the densest region, andthese methods would perform badly. On the other hand EM with generative mixture models, another semisupervised learning method, would have easily solvedthe problem. Detecting bad match in advance however is hard and remains an openquestion.Anecdotally, the fact that unlabeled data do not always help semisupervisedlearning has been observed by multiple researchers. For example people have longrealized that training Hidden Markov Model with unlabeled data the BaumWelshalgorithm, which by the way qualifies as semisupervised learning on sequencescan reduce accuracy under certain initial conditions Elworthy, 1994. See Cozman et al., 2003 for a more recent argument. Not much is in the literature though,presumably because of the publication bias.Q How many semisupervised learning methods are thereA Many. Some oftenused methods include EM with generative mixture models,selftraining, cotraining, transductive support vector machines, and graphbasedmethods. See the following sections for more methods.Q Which method should I use  is the bestA There is no direct answer to this question. Because labeled data is scarce, semisupervised learning methods make strong model assumptions. Ideally one shoulduse a method whose assumptions fit the problem structure. This may be difficultin reality. Nonetheless we can try the following checklist Do the classes producewell clustered data If yes, EM with generative mixture models may be a goodchoice Do the features naturally split into two sets If yes, cotraining may beappropriate Is it true that two points with similar features tend to be in the sameclass If yes, graphbased methods can be used Already using SVM TransductiveSVM is a natural extension Is the existing supervised classifier complicated andhard to modify Selftraining is a practical wrapper method.Q How do semisupervised learning methods use unlabeled dataA Semisupervised learning methods use unlabeled data to either modify or reprioritize hypotheses obtained from labeled data alone. Although not all methodsare probabilistic, it is easier to look at methods that represent hypotheses by pyx,and unlabeled data by px. Generative models have common parameters for thejoint distribution px, y. It is easy to see that px influences pyx. Mixturemodels with EM is in this category, and to some extent selftraining. Many other5methods are discriminative, including transductive SVM, Gaussian processes, information regularization, and graphbased methods. Original discriminative training cannot be used for semisupervised learning, since pyx is estimated ignoringpx. To solve the problem, px dependent terms are often brought into the objective function, which amounts to assuming pyx and px share parameters.Q What is the difference between transductive learning and semisupervisedlearningA Different authors use slightly different names. In this survey we will use thefollowing convention Semisupervised learning refers to the use of both labeled and unlabeleddata for training. It contrasts supervised learning data all labeled or unsupervised learning data all unlabeled. Other names are learning from labeled and unlabeled data or learning from partially labeledclassified data.Notice semisupervised learning can be either transductive or inductive. Transductive learning will be used to contrast inductive learning. A learneris transductive if it only works on the labeled and unlabeled training data,and cannot handle unseen data. The early graphbased methods are oftentransductive. Inductive learners can naturally handle unseen data. Noticeunder this convention transductive support vector machines TSVMs arein fact inductive learners, because the resulting classifiers are defined overthe whole space. The name TSVM originates from the intention to workonly on the observed data though people use them for induction anyway,which according to Vapnik, 1998 is solving a simpler problem. Peoplesometimes use the analogy that transductive learning is takehome exam,while inductive learning is inclass exam. In this survey semisupervised learning refers to semisupervised classification, where one has additional unlabeled data and the goal is classification.Its cousin semisupervised clustering, where one has unlabeled data withsome pairwise constraints and the goal is clustering, is only briefly discussedlater in the survey.We will follow the above convention in the survey.Q Where can I learn moreA An existing survey can be found in Seeger, 2001. A book on semisupervisedlearning is Chapelle et al., 2006c.62 Generative ModelsGenerative models are perhaps the oldest semisupervised learning method. It assumes a model px, y  pypxy where pxy is an identifiable mixture distribution, for example Gaussian mixture models. With large amount of unlabeleddata, the mixture components can be identified then ideally we only need onelabeled example per component to fully determine the mixture distribution, seeFigure 1. One can think of the mixture components as soft clusters.Nigam et al. 2000 apply the EM algorithm on mixture of multinomial forthe task of text classification. They showed the resulting classifiers perform betterthan those trained only from L. Baluja 1998 uses the same algorithm on a faceorientation discrimination task. Fujino et al. 2005 extend generative mixturemodels by including a bias correction term and discriminative training using themaximum entropy principle.One has to pay attention to a few things2.1 IdentifiabilityThe mixture model ideally should be identifiable. In general let p be a family ofdistributions indexed by a parameter vector .  is identifiable if 1 6 2  p1 6p2 , up to a permutation of mixture components. If the model family is identifiable,in theory with infinite U one can learn  up to a permutation of component indices.Here is an example showing the problem with unidentifiable models. Themodel pxy is uniform for y  1,1. Assuming with large amount of unlabeled data U we know px is uniform in 0, 1. We also have 2 labeled datapoints 0.1,1, 0.9,1. Can we determine the label for x  0.5 No. Withour assumptions we cannot distinguish the following two modelspy  1  0.2, pxy  1  unif0, 0.2, pxy  1  unif0.2, 1 1py  1  0.6, pxy  1  unif0, 0.6, pxy  1  unif0.6, 1 2which give opposite labels at x  0.5, see Figure 2. It is known that a mixture ofGaussian is identifiable. Mixture of multivariate Bernoulli McCallum  Nigam,1998a is not identifiable. More discussions on identifiability and semisupervisedlearning can be found in e.g. Ratsaby  Venkatesh, 1995 and Corduneanu Jaakkola, 2001.2.2 Model CorrectnessIf the mixture model assumption is correct, unlabeled data is guaranteed to improveaccuracy Castelli  Cover, 1995 Castelli  Cover, 1996 Ratsaby  Venkatesh,75 4 3 2 1 0 1 2 3 4 5543210123455 4 3 2 1 0 1 2 3 4 554321012345a labeled data b labeled and unlabeled data small dots5 4 3 2 1 0 1 2 3 4 5543210123455 4 3 2 1 0 1 2 3 4 554321012345c model learned from labeled data d model learned from labeled and unlabeled dataFigure 1 In a binary classification problem, if we assume each class has a Gaussiandistribution, then we can use unlabeled data to help parameter estimation.80 1px10 0.2 10 0.2 1 0.6 pxy11.670 10.6pxy12.5 0.4 0 0.6 1pxy15pxy11.25 0.8  0.2 Figure 2 An example of unidentifiable models. Even if we known px topis a mixture of two uniform distributions, we cannot uniquely identify the twocomponents. For instance, the mixtures on the second and third line give the samepx, but they classify x  0.5 differently.6 4 2 0 2 4 66420246Class 1Class 26 4 2 0 2 4 664202466 4 2 0 2 4 66420246a Horizontal class separation b High probability c Low probabilityFigure 3 If the model is wrong, higher likelihood may lead to lower classificationaccuracy. For example, a is clearly not generated from two Gaussian. If we insistthat each class is a single Gaussian, b will have higher probability than c. Butb has around 50 accuracy, while cs is much better.1995. However if the model is wrong, unlabeled data may actually hurt accuracy.Figure 3 shows an example. This has been observed by multiple researchers. Cozman et al. 2003 give a formal derivation on how this might happen.It is thus important to carefully construct the mixture model to reflect reality.For example in text categorization a topic may contain several subtopics, and willbe better modeled by multiple multinomial instead of a single one Nigam et al.,2000. Some other examples are Shahshahani  Landgrebe, 1994 Miller Uyar, 1997. Another solution is to downweighing unlabeled data Corduneanu Jaakkola, 2001, which is also used by Nigam et al. 2000, and by CallisonBurchet al. 2004 who estimate word alignment for machine translation.92.3 EM Local MaximaEven if the mixture model assumption is correct, in practice mixture componentsare identified by the ExpectationMaximization EM algorithm Dempster et al.,1977. EM is prone to local maxima. If a local maximum is far from the globalmaximum, unlabeled data may again hurt learning. Remedies include smart choiceof starting point by active learning Nigam, 2001.2.4 ClusterandLabelWe shall also mention that instead of using an probabilistic generative mixturemodel, some approaches employ various clustering algorithms to cluster the wholedataset, then label each cluster with labeled data, e.g. Demiriz et al., 1999 Daraet al., 2002. Although they can perform well if the particular clustering algorithmsmatch the true data distribution, these approaches are hard to analyze due to theiralgorithmic nature.2.5 Fisher kernel for discriminative learningAnother approach for semisupervised learning with generative models is to convert data into a feature representation determined by the generative model. The newfeature representation is then fed into a standard discriminative classifier. Holubet al. 2005 used this approach for image categorization. First a generative mixture model is trained, one component per class. At this stage the unlabeled data canbe incorporated via EM, which is the same as in previous subsections. Howeverinstead of directly using the generative model for classification, each labeled example is converted into a fixedlength Fisher score vector, i.e. the derivatives of loglikelihood w.r.t. model parameters, for all component models Jaakkola  Haussler, 1998. These Fisher score vectors are then used in a discriminative classifierlike an SVM, which empirically has high accuracy.3 SelfTrainingSelftraining is a commonly used technique for semisupervised learning. In selftraining a classifier is first trained with the small amount of labeled data. Theclassifier is then used to classify the unlabeled data. Typically the most confidentunlabeled points, together with their predicted labels, are added to the trainingset. The classifier is retrained and the procedure repeated. Note the classifieruses its own predictions to teach itself. The procedure is also called selfteachingor bootstrapping not to be confused with the statistical procedure with the same10name. The generative model and EM approach of section 2 can be viewed as aspecial case of soft selftraining. One can imagine that a classification mistakecan reinforce itself. Some algorithms try to avoid this by unlearn unlabeled pointsif the prediction confidence drops below a threshold.Selftraining has been applied to several natural language processing tasks.Yarowsky 1995 uses selftraining for word sense disambiguation, e.g. decidingwhether the word plant means a living organism or a factory in a give context.Riloff et al. 2003 uses it to identify subjective nouns. Maeireizo et al. 2004classify dialogues as emotional or nonemotional with a procedure involvingtwo classifiers.Selftraining has also been applied to parsing and machine translation. Rosenberg et al. 2005 apply selftraining to object detection systems fromimages, and show the semisupervised technique compares favorably with a stateoftheart detector.4 CoTrainingCotraining Blum  Mitchell, 1998 Mitchell, 1999 assumes that features canbe split into two sets Each subfeature set is sufficient to train a good classifierThe two sets are conditionally independent given the class. Initially two separateclassifiers are trained with the labeled data, on the two subfeature sets respectively.Each classifier then classifies the unlabeled data, and teaches the other classifierwith the few unlabeled examples and the predicted labels they feel most confident. Each classifier is retrained with the additional training examples given by theother classifier, and the process repeats.In cotraining, unlabeled data helps by reducing the version space size. In otherwords, the two classifiers or hypotheses must agree on the much larger unlabeleddata as well as the labeled data.We need the assumption that subfeatures are sufficiently good, so that we cantrust the labels by each learner on U . We need the subfeatures to be conditionallyindependent so that one classifiers high confident data points are iid samples forthe other classifier. Figure 4 visualizes the assumption.Nigam and Ghani 2000 perform extensive empirical experiments to comparecotraining with generative mixture models and EM. Their result shows cotrainingperforms well if the conditional independence assumption indeed holds. In addition, it is better to probabilistically label the entire U , instead of a few most confident data points. They name this paradigm coEM. Finally, if there is no naturalfeature split, the authors create artificial split by randomly break the feature set intotwo subsets. They show cotraining with artificial feature split still helps, thoughnot as much as before. Jones 2005 used cotraining, coEM and other related11    a x1 view b x2 viewFigure 4 CoTraining Conditional independent assumption on feature split. Withthis assumption the high confident data points in x1 view, represented by circledlabels, will be randomly scattered in x2 view. This is advantageous if they are tobe used to teach the classifier in x2 view.methods for information extraction from text.Cotraining makes strong assumptions on the splitting of features. One mightwonder if these conditions can be relaxed. Goldman and Zhou 2000 use twolearners of different type but both takes the whole feature set, and essentially useone learners high confidence data points, identified with a set of statistical tests, inU to teach the other learning and vice versa. Later Zhou and Goldman 2004 propose a singleview multiplelearner Democratic Colearning algorithm. An ensemble of learners with different inductive bias are trained separately on the completefeature of the labeled data. They then make predictions on the unlabeled data. Ifa majority of learners confidently agree on the class of an unlabeled point xu, thatclassification is used as the label of xu. xu and its label is added to the trainingdata. All learners are retrained on the updated training set. The final prediction ismade with a variant of a weighted majority vote among all the learners. SimilarlyZhou and Li 2005b propose tritraining which uses three learners. If two ofthem agree on the classification of an unlabeled point, the classification is used toteach the third classifier. This approach thus avoids the need of explicitly measuring label confidence of any learner. It can be applied to datasets without differentviews, or different types of classifiers.Balcan et al. 2005b relax the conditional independence assumption with amuch weaker expansion condition, and justify the iterative cotraining procedure.More generally, we can define learning paradigms that utilize the agreementamong different learners. Cotraining can be viewed as a special case with twolearners and a specific algorithm to enforce agreement. For instance, the work ofLeskes 2005 is discussed in Section 7.125 Avoiding Changes in Dense Regions5.1 Transductive SVMs S3VMsDiscriminative methods work on pyx directly. This brings up the danger ofleaving px outside of the parameter estimation loop, if px and pyx do notshare parameters. Notice px is usually all we can get from unlabeled data. It isbelieved that if px and pyx do not share parameters, semisupervised learningcannot help. This point is emphasized in Seeger, 2001.Transductive support vector machines TSVMs1 builds the connection between px and the discriminative decision boundary by not putting the boundaryin high density regions. TSVM is an extension of standard support vector machineswith unlabeled data. In a standard SVM only the labeled data is used, and the goalis to find a maximum margin linear boundary in the Reproducing Kernel HilbertSpace. In a TSVM the unlabeled data is also used. The goal is to find a labeling ofthe unlabeled data, so that a linear boundary has the maximum margin on both theoriginal labeled data and the now labeled unlabeled data. The decision boundary has the smallest generalization error bound on unlabeled data Vapnik, 1998.Intuitively, unlabeled data guides the linear boundary away from dense regions.Figure 5 In TSVM, U helps to put the decision boundary in sparse regions. Withlabeled data only, the maximum margin boundary is plotted with dotted lines. Withunlabeled data black dots, the maximum margin boundary would be the one withsolid lines.However finding the exact transductive SVM solution is NPhard. Major efforthas focused on efficient approximation algorithms. Early algorithms Bennett Demiriz, 1999 Demirez  Bennett, 2000 Fung  Mangasarian, 1999 eithercannot handle more than a few hundred unlabeled examples, or did not do so inexperiments. The SVMlight TSVM implementation Joachims, 1999 is the firstwidely used software.1In recent papers, TSVMs are also called SemiSupervised Support Vector Machines S3VM,because the learned classifiers can in fact be used inductively to predict on unseen data.13Xu and Schuurmans 2005 present a training method based on semidefiniteprogramming SDP, which applies to the completely unsupervised SVMs as well.In the simple binary classification case, the goal of finding a good labeling for unlabeled data is formulated as finding a positive semidefinite matrix M . M is meantto be the continuous relaxation of the label outer product matrix yy, and the SVMobjective is expressed as semidefinite programming on M . There are effective although still expensive SDP solvers. Importantly, the authors propose multiclassversion of the SDP, which results in multiclass SVM for semisupervised learning.The computational cost of SDP is still high though.TSVM can be viewed as SVM with an additional regularization term on unlabeled data. Let fx  hx  b where h  HK . The optimization problemisminfli11 yifxi  1h2HK  2nil11 fxi 3where z  maxz, 0. The last term arises from assigning label signfx tounlabeled point x. The margin on unlabeled point is thus signfxfx  fx.The loss function 1 fxi has a nonconvex hat shape as shown in Figure 6,which is the root of the optimization difficulty.2 1.5 1 0.5 0 0.5 1 1.5 200.511.522.53Figure 6 The TSVM loss function 1 fxiChapelle and Zien 2005 propose SVM, which approximates the hat loss1fxi with a Gaussian function, and perform gradient search in the primalspace. Sindhwani et al. 2006 use a deterministic annealing approach, whichstarts from an easy problem, and gradually deforms it to the TSVM objective. Ina similar spirit, Chapelle et al. 2006a use a continuation approach, which alsostarts by minimizing an easy convex objective function, and gradually deforms itto the TSVM objective with Gaussian instead of hat loss, using the solution ofprevious iterations to initialize the next ones. Collobert et al. 2006 optimizethe hard TSVM directly, using an approximate optimization procedure known asconcaveconvex procedure CCCP. The key is to notice that the hat loss is a sum of14a convex function and a concave function. By replacing the concave function witha linear upper bound, one can perform convex minimization to produce an upperbound of the loss function. This is repeated until a local minimum is reached. Theauthors report significant speed up of TSVM training with CCCP. Sindhwani andKeerthi 2006 proposed a fast algorithm for linear S3VMs, suitable for large scaletext applications. Their implementation can be found at httppeople.cs.uchicago.eduvikasssvmlin.html.With all the approximation solutions to TSVMs, it is interesting to understandjust how good a global optimum TSVM can be. With the Branch and Bound searchtechnique, Chapelle et al. 2006b finds the global optimal solution for smalldatasets. The results indicate excellent accuracy. Although Branch and Boundwill probably never be useful for large datasets, the results provide some groundtruth, and points to the potentials of TSVMs with better approximation methods.Weston et al. 2006 learn with a universum, which is a set of unlabeled datathat is known to come from neither of the two classes. The decision boundary isencouraged to pass through the universum. One interpretation is similar to the maximum entropy principle the classifier should be confident on labeled examples, yetmaximally ignorant on unrelated examples.Zhang and Oles 2000 argued against TSVMs.The maximum entropy discrimination approach Jaakkola et al., 1999 alsomaximizes the margin, and is able to take into account unlabeled data, with SVMas a special case.5.2 Gaussian ProcessesLawrence and Jordan 2005 proposed a Gaussian process approach, which can beviewed as the Gaussian process parallel of TSVM. The key difference to a standardGaussian process is in the noise model. A null category noise model maps thehidden continuous variable f to three instead of two labels, specifically to the neverused label 0 when f is around zero. On top of that, it is restricted that unlabeleddata points cannot take the label 0. This pushes the posterior of f away from zerofor the unlabeled points. It achieves the similar effect of TSVM where the marginavoids dense unlabeled data region. However nothing special is done on the processmodel. Therefore all the benefit of unlabeled data comes from the noise model. Avery similar noise model is proposed in Chu  Ghahramani, 2004 for ordinalregression.Chu et al. 2006 develop Guassian process models that incorporate pairwiselabel relations e.g. two points tends to have similar or different labels. Notesuch similarlabel information is equivalent to those used in graphbased semisupervised learning. Such models, using only similarity information, are applied15to semisupervised learning successfully. However dissimilarity is only briefly discussed, with many questions remain open.There is a finite form of a Gaussian process in Zhu et al., 2003c, in fact ajoint Gaussian distribution on the labeled and unlabeled points with the covariancematrix derived from the graph Laplacian. Semisupervised learning happens in theprocess model, not the noise model.5.3 Information RegularizationSzummer and Jaakkola 2002 propose the information regularization frameworkto control the label conditionals pyx by px, where px may be estimated fromunlabeled data. The idea is that labels shouldnt change too much in regions wherepx is high. The authors use the mutual information Ix y between x and y asa measure of label complexity. Ix y is small when the labels are homogeneous,and large when labels vary. This motives the minimization of the product of pxmass in a region with Ix y normalized by a variance term. The minimizationis carried out on multiple overlapping regions covering the data space.The theory is developed further in Corduneanu  Jaakkola, 2003. Corduneanu and Jaakkola 2005 extend the work by formulating semisupervisedlearning as a communication problem. Regularization is expressed as the rate ofinformation, which again discourages complex conditionals pyx in regions withhigh px. The problem becomes finding the unique pyx that minimizes a regularized loss on labeled data. The authors give a local propagation algorithm.5.4 Entropy MinimizationThe hyperparameter learning method in section 7.2 of Zhu, 2005 uses entropyminimization. Grandvalet and Bengio 2005 used the label entropy on unlabeleddata as a regularizer. By minimizing the entropy, the method assumes a prior whichprefers minimal class overlap.Lee et al. 2006 apply the principle of entropy minimization for semisupervisedlearning on 2D conditional random fields for image pixel classification. In particular, the training objective is to maximize the standard conditional loglikelihood,and at the same time minimize the conditional entropy of label predictions on unlabeled image pixels.5.5 A Connection to Graphbased MethodsLet px be a probability distribution from which labeled and unlabeled data aredrawn. Narayanan et al. 2006 prove that the weighted boundary volume, i.e.16the surface integralS psds along a decision boundary S, is approximated byNtfLf when the number of iid data points N tends to infinity. Here L is thenormalized graph Laplacian and f is an indicator function of the cut, and t is thebandwidth of the edge weight Gaussian function, which must tend to zero at acertain rate. This result suggests that S3VMs and related methods which seek adecision boundary that passes through low density regions, and graphbased semisupervised learning methods which approximately compute the graph cut, mightbe more strongly connected that previously thought.6 GraphBased MethodsGraphbased semisupervised methods define a graph where the nodes are labeledand unlabeled examples in the dataset, and edges may be weighted reflect thesimilarity of examples. These methods usually assume label smoothness over thegraph. Graph methods are nonparametric, discriminative, and transductive in nature.6.1 Regularization by GraphMany graphbased methods can be viewed as estimating a function f on the graph.One wants f to satisfy two things at the same time 1 it should be close to thegiven labels yL on the labeled nodes, and 2 it should be smooth on the wholegraph. This can be expressed in a regularization framework where the first term isa loss function, and the second term is a regularizer.Several graphbased methods listed here are similar to each other. They differ in the particular choice of the loss function and the regularizer. We believe itis more important to construct a good graph than to choose among the methods.However graph construction, as we will see later, is not a well studied area.6.1.1 MincutBlum and Chawla 2001 pose semisupervised learning as a graph mincut alsoknown as stcut problem. In the binary case, positive labels act as sources andnegative labels act as sinks. The objective is to find a minimum set of edges whoseremoval blocks all flow from the sources to the sinks. The nodes connecting to thesources are then labeled positive, and those to the sinks are labeled negative. Equivalently mincut is the mode of a Markov random field with binary labels Boltzmannmachine. The loss function can be viewed as a quadratic loss with infinity weightiLyi  yiL2, so that the values on labeled data are in fact fixed at their17given labels. The regularizer is12i,jwij yi  yj  12i,jwijyi  yj2 4The equality holds because the ys take binary 0 and 1 labels. Putting the twotogether, mincut can be viewed to minimize the functioniLyi  yiL2 12i,jwijyi  yj2 5subject to the constraint yi  0, 1,i.One problem with mincut is that it only gives hard classification without confidence i.e. it computes the mode, not the marginal probabilities. Blum et al.2004 perturb the graph by adding random noise to the edge weights. Mincut isapplied to multiple perturbed graphs, and the labels are determined by a majorityvote. The procedure is similar to bagging, and creates a soft mincut.Pang and Lee 2004 use mincut to improve the classification of a sentence intoeither objective or subjective, with the assumption that sentences close to eachother tend to have the same class.6.1.2 Discrete Markov Random Fields Boltzmann MachinesThe proper but hard way is to compute the marginal probabilities of the discreteMarkov random fields. This is inherently a difficult inference problem. Zhu andGhahramani 2002 attempted exactly this, but were limited by the MCMC sampling techniques they used global Metropolis and SwendsenWang sampling.Getz et al. 2005 computes the marginal probabilities of the discrete Markovrandom field at any temperature with the Multicanonical MonteCarlo method,which seems to be able to overcome the energy trap faced by the standard Metropolis or SwendsenWang method. The authors discuss the relationship between temperatures and phases in such systems. They also propose a heuristic procedure toidentify possible new classes.6.1.3 Gaussian Random Fields and Harmonic FunctionsThe Gaussian random fields and harmonic function methods in Zhu et al., 2003ais a continuous relaxation to the difficulty discrete Markov random fields or Boltzmann machines. It can be viewed as having a quadratic loss function with infinityweight, so that the labeled data are clamped fixed at given label values, and a18regularizer based on the graph combinatorial Laplacian iLfi  yi2  12i,jwijfi  fj2 6 iLfi  yi2  ff 7Notice fi  R, which is the key relaxation to Mincut. This allows for a simpleclosedform solution for the node marginal probabilities. The mean is known as aharmonic function, which has many interesting properties Zhu, 2005.Recently Grady and FunkaLea 2004 applied the harmonic function methodto medical image segmentation tasks, where a user labels classes e.g. differentorgans with a few strokes. Levin et al. 2004 use the equivalent of harmonicfunctions for colorization of grayscale images. Again the user specifies the desired color with only a few strokes on the image. The rest of the image is used asunlabeled data, and the labels propagation through the image. Niu et al. 2005 applied the label propagation algorithm which is equivalent to harmonic functionsto word sense disambiguation. Goldberg and Zhu 2006 applied the algorithm tosentiment analysis for movie rating prediction.6.1.4 Local and Global ConsistencyThe local and global consistency method Zhou et al., 2004a uses the loss functionni1fiyi2, and the normalized Laplacian D12D12  ID12WD12in the regularizer,12i,jwijfiDii  fjDjj2  fD12D12f 86.1.5 Tikhonov RegularizationThe Tikhonov regularization algorithm in Belkin et al., 2004a uses the loss function and regularizer1kifi  yi2  fSf 9where S   or p for some integer p.196.1.6 Manifold RegularizationThe manifold regularization framework Belkin et al., 2004b Belkin et al., 2005employs two regularization terms1lli1V xi, yi, f  Af 2K  I f 2I 10where V is an arbitrary loss function, K is a base kernel, e.g. a linear or RBFkernel. I is a regularization term induced by the labeled and unlabeled data. Forexample, one can usef 2I 1l  u2ff 11where f is the vector of f evaluations on L  U .Sindhwani et al. 2005a give a semisupervised kernel that is not limited tothe unlabeled points, but defined over all input space. The kernel thus supportsinduction. Essentially the kernel is a new interpretation of the manifold regularization framework above. Starting from a base kernel K defined over the whole inputspace e.g. linear kernels, RBF kernels, the authors modify the RKHS by keepingthe same function space but changing the norm. Specifically a pointcloud normdefined by LU is added to the original norm. The pointcloud norm correspondsto f 2I . Importantly this results in a new RKHS space, with a correspondingnew kernel that deforms the original one along a finitedimensional subspace givenby the data. The new kernel is defined over the whole space, yet it follows themanifold. Standard supervised kernel machines with the new kernel, trained onL only, are able to perform inductive semisupervised learning. In fact they areequivalent to LapSVM and LapRLS Belkin et al., 2005 with a certain parameter.Nonetheless finding the new kernel involves inverting a n  n matrix. Like manyother methods it can be costly. Also notice the new kernel depends on the observedL  U data, thus it is a random kernel.6.1.7 Graph Kernels from the Spectrum of LaplacianFor kernel methods, the regularizer is a typically monotonically increasing function of the RKHS norm f K  fK1f with kernel K. Such kernels are derivedfrom the graph, e.g. the Laplacian.Chapelle et al. 2002 and Smola and Kondor 2003 both show the spectraltransformation of a Laplacian results in kernels suitable for semisupervised learning. The diffusion kernel Kondor  Lafferty, 2002 corresponds to a spectrum20transform of the Laplacian withr  exp22 12The regularized Gaussian process kernel   I2 in Zhu et al., 2003c corresponds tor 1 13Similarly the order constrained graph kernels in Zhu et al., 2005 are constructed from the spectrum of the Laplacian, with nonparametric convex optimization. Learning the optimal eigenvalues for a graph kernel is in fact a way toat least partially improve an imperfect graph. In this sense it is related to graphconstruction.Kapoor et al. 2005 learn both the graph weight hyperparameter, the hyperparameter for Laplacian spectrum transformation r    , and the noisemodel hyperparameter with evidence maximization. Expectation Propagation EPis used for approximation. The authors also propose a way to classify unseenpoints. This spectrum transformation is relatively simple.6.1.8 Spectral Graph TransducerThe spectral graph transducer Joachims, 2003 can be viewed with a loss functionand regularizermin cf  Cf    fLf 14s.t.f1  0andff  n 15where i ll for positive labeled data, ll for negative data, lbeing the number of negative data and so on. L can be the combinatorial or normalized graph Laplacian, with a transformed spectrum. c is a weighting factor, andC is a diagonal matrix for misclassification costs.Pham et al. 2005 perform empirical experiments on word sense disambiguation, comparing variants of cotraining and spectral graph transducer. The authors notice spectral graph transducer with carefully constructed graphs SGTCotraining produces good results.6.1.9 TreeBased BayesKemp et al. 2003 define a probabilistic distribution P Y T  on discrete e.g. 0and 1 labellings Y over an evolutionary tree T . The tree T is constructed with21the labeled and unlabeled data being the leaf nodes. The labeled data is clamped.The authors assume a mutation process, where a label at the root propagates downto the leaves. The label mutates with a constant rate as it moves down along theedges. As a result the tree T its structure and edge lengths uniquely defines thelabel prior P Y T . Under the prior if two leaf nodes are closer in the tree, theyhave a higher probability of sharing the same label. One can also integrate over alltree structures.The treebased Bayes approach can be viewed as an interesting way to incorporate structure of the domain. Notice the leaf nodes of the tree are the labeled andunlabeled data, while the internal nodes do not correspond to physical data. This isin contrast with other graphbased methods where labeled and unlabeled data areall the nodes.6.1.10 Some Other MethodsSzummer and Jaakkola 2001 perform a tstep Markov random walk on the graph.The influence of one example to another example is proportional to how easy therandom walk goes from one to the other. It has certain resemblance to the diffusionkernel. The parameter t is important.Chapelle and Zien 2005 use a densitysensitive connectivity distance betweennodes i, j a given path between i, j consists of several segments, one of themis the longest now consider all paths between i, j and find the shortest longestsegment. Exponentiating the negative distance gives a graph kernel.Bousquet et al. 2004 propose measurebased regularization, the continuous counterpart of graphbased regularization. The intuition is that two points aresimilar if they are connected by high density regions. They define regularizationbased on a known density px and provide interesting theoretical analysis. However it seems difficult in practice to apply the theoretical results to higher D  2dimensional tasks.6.2 Graph ConstructionAlthough the graph is at the heart of graphbased semisupervised learning methods, its construction has not been studied extensively. The issue has been discussedin Zhu, 2005 Chapter 3 and Chapter 7. Balcan et al. 2005a build graphs forvideo surveillance using strong domain knowledge, where the graph of webcamimages consists of time edges, color edges and face edges. Such graphs reflect adeep understanding of the problem structure and how unlabeled data is expected tohelp. CarreiraPerpinan and Zemel 2005 build robust graphs from multiple minimum spanning trees by perturbation and edge removal. Wang and Zhang 200622perform an operation very similar to locally linear embedding LLE on the datapoints first, but constraining the LLE weights to be nonnegative. These weightsare then used as graph weights.Hein and Maier 2006 propose an algorithm to denoise points sampled from amanifold. That is, data points are assumed to be noisy samples of some unknownunderlying manifold. They used the denoising algorithm as a preprocessing step forgraphbased semisupervised learning, so that the graph can be constructed frombetter separated data points. Such preprocessing results in better semisupervisedclassification accuracy.When using a Gaussian function as edge weights, the bandwidth of the Gaussian needs to be carefully chosen. Zhang and Lee 2006 derive a cross validation approach to tune the bandwidth for each feature dimension, by minimizingthe leaveoneout mean squared error of predictions and given labels on labeledpoints. By invoking the matrix inversion lemma and careful precomputation, thetime complexity of LOO tuning is moderately reduced but still at Ou3.6.3 Fast ComputationMany semisupervised learning methods scale as badly as On3 as they were originally proposed. Because semisupervised learning is interesting when the size ofunlabeled data is large, this is clearly a problem. Many methods are also transductive section 6.4. In 2005 several papers start to address these problems.Fast computation of the harmonic function with conjugate gradient methodsis discussed in Argyriou, 2004. A comparison of three iterative methods labelpropagation, conjugate gradient and loopy belief propagation is presented in Zhu,2005 Appendix F. Recently numerical methods for fast Nbody problems havebeen applied to dense graphs in semisupervised learning, reducing the computational cost from On3 to On Mahdaviani et al., 2005. This is achieved withKrylov subspace methods and the fast Gauss transform.The harmonic mixture models Zhu  Lafferty, 2005 convert the originalgraph into a much smaller backbone graph, by using a mixture model to carveup the original L  U dataset. Learning on the smaller graph is much faster. Similar ideas have been used for e.g. dimensionality reduction Teh  Roweis, 2002.The heuristics in Delalleau et al., 2005 similarly create a small graph with a subset of the unlabeled data. They enables fast approximate computation by reducingthe problem size.Garcke and Griebel 2005 propose the use of sparse grids for semisupervisedlearning. The main advantages are On computation complexity for sparse graphs,and the ability of induction. The authors start from the same regularization problem of Belkin et al., 2005. The key idea is to approximate the function space23with a finite basis, with sparse grids. The minimizer f in this finite dimensionalsubspace can be efficiently computed. As the authors point out, this method isdifferent from the general kernel methods which rely on the representer theoremfor finite representation. In practice the method is limited by data dimensionalityaround 20. A potential drawback is that the method employs a regular grid, andcannot zoom in to small interesting data regions with higher resolution.Yu et al. 2005 solve the large scale semisupervised learning problem byusing a bipartite graph. The labeled and unlabeled points form one side of thebipartite split, while a much smaller number of blocklevel nodes form the otherside. The authors show that the harmonic function can be computed using theblocklevel nodes. The computation involves inverting a much smaller matrix onblocklevel nodes. It is thus cheaper and more scalable than working directly on theLU matrix. The authors propose two methods to construct the bipartite graph, sothat it approximates the given weight matrix W on L  U . One uses NonnegativeMatrix Factorization, the other uses mixture models. The latter method has theadditional benefit of induction, and is similar to the harmonic mixtures Zhu Lafferty, 2005. However in the latter method the mixture model is derived basedon the given weight matrix W . But in harmonic mixtures W and the mixture modelare independent, and the mixture model serves as a second knowledge source inaddition to W .The original manifold regularization framework Belkin et al., 2004b needs toinvert a lu lu matrix, and is not scalable. To speed up things, Sindhwaniet al. 2005c consider linear manifold regularization. Effectively this is a specialcase when the base kernel is taken to be the linear kernel. The authors show thatit is advantageous to work with the primal variables. The resulting optimizationproblem can be much smaller if the data dimensionality is small, or sparse.Tsang and Kwok 2006 scale manifold regularization up by adding in an insensitive loss into the energy function, i.e. replacingwij fxi fxj2 bywij fxi fxj2, where z  maxz  , 0. The intuition is thatmost pairwise differences fxi fxj are very small. By tolerating differencessmaller than , the solution becomes sparse. They were able to handle one millionunlabeled points in manifold regularization with this method.6.4 InductionMost graphbased semisupervised learning algorithms are transductive, i.e. theycannot easily extend to new test points outside of L  U . Recently induction hasreceived increasing attention. One common practice is to freeze the graph onL  U . New points do not although they should alter the graph structure. Thisavoids expensive graph computation every time one encounters new points.24Zhu et al. 2003c propose that new test point be classified by its nearest neighbor in LU . This is sensible when U is sufficiently large. In Chapelle et al., 2002the authors approximate a new point by a linear combination of labeled and unlabeled points. Similarly in Delalleau et al., 2005 the authors proposes an inductionscheme to classify a new point x byfx iLU wxifxiiLU wxi16This can be viewed as an application of the Nystrom method Fowlkes et al., 2004.Yu et al. 2004 report an early attempt on semisupervised induction usingRBF basis functions in a regularization framework. In Belkin et al., 2004b, thefunction f does not have to be restricted to the graph. The graph is merely used toregularize f which can have a much larger support. It is necessarily a combinationof an inductive algorithm and graph regularization. The authors give the graphregularized version of least squares and SVM. Note such an SVM is different fromthe graph kernels in standard SVM in Zhu et al., 2005. The former is inductivewith both a graph regularizer and an inductive kernel. The latter is transductivewith only the graph regularizer. Following the work, Krishnapuram et al. 2005use graph regularization on logistic regression. Sindhwani et al. 2005a give asemisupervised kernel that is defined over the whole space, not just on the trainingdata points. These methods create inductive learners that naturally handle new testpoints.The harmonic mixture model Zhu  Lafferty, 2005 naturally handles newpoints as well. The idea is to model the labeled and unlabeled data with a mixturemodel, e.g. mixture of Gaussian. In standard mixture models, the class probability pyi for each mixture component i is optimized to maximize label likelihood. However in harmonic mixture models, pyi is optimized differently tominimize an underlying graphbased cost function. Under certain conditions, theharmonic mixture model converts the original graph on unlabeled data into a backbone graph, with the components being super nodes. Harmonic mixture modelsnaturally handle induction just like standard mixture models.Several other inductive methods have been discussed in section 6.3 togetherwith fast computation.6.5 ConsistencyThe consistency of graphbased semisupervised learning algorithms is an openresearch area. By consistency we mean whether classification converges to theright solution as the number of labeled and unlabeled data grows to infinity. Recently von Luxburg et al. 2005 von Luxburg et al., 2004 study the consistency25of spectral clustering methods. The authors find that the normalized Laplacian isbetter than the unnormalized Laplacian for spectral clustering. The convergence ofthe eigenvectors of the unnormalized Laplacian is not clear, while the normalizedLaplacian always converges under general conditions. There are examples wherethe top eigenvectors of the unnormalized Laplacian do not yield a sensible clustering. The corresponding problem in semisupervised classification needs furtherstudy. One reason is that in semisupervised learning the whole Laplacian normalized or not is often used for regularization, not only the top eigenvectors.Zhang and Ando 2006 prove that semisupervised learning based on graphkernels is wellbehaved in that the solution converges as the size of unlabeled dataapproaches infinity. They also derived a generalization bound, which leads to away to optimizing kernel eigentransformations.6.6 Directed Graphs and HypergraphsFor semisupervised learning on directed graphs, Zhou et al. 2005b take a hub authority approach and essentially convert a directed graph into an undirectedone. Two hub nodes are connected by an undirected edge with appropriate weightif they colink to authority nodes, and vice versa. Semisupervised learning thenproceeds on the undirected graph.Zhou et al. 2005a generalize the work further. The algorithm takes a transition matrix with a unique stationary distribution as input, and gives a closed formsolution on unlabeled data. The solution parallels and generalizes the normalizedLaplacian solution for undirected graphs Zhou et al., 2004a. The previous workZhou et al., 2005b is a special case with the 2step random walk transition matrix.In the absence of labels, the algorithm is the generalization of the normalized cutShi  Malik, 2000 on directed graphs.Lu and Getoor 2003 convert the link structure in a directed graph into pernode features, and combines them with pernode object features in logistic regression. They also use an EMlike iterative algorithm.Zhou et al. 2006 propose to formulate relational objects using hypergraphs,where an edge can connect more than two vertices, and extend spectral clustering,classification and embedding to such hypergraphs.6.7 Connection to Standard Graphical ModelsThe Gaussian random field formulation Zhu et al., 2003a is a standard undirected graphical model, with continuous random variables. Given labeled nodesobserved variables, the inference is used to obtain the mean equivalently themode hi of the remaining variables, which is the harmonic function. However the26interpretation of the harmonic function as parameters for Bernoulli distributions atthe nodes i.e. each unlabeled node has label 1 with probability hi, 0 otherwise isnonstandard.Burges and Platt 2005 propose a directed graphical model, called ConditionalHarmonic Mixing, that is somewhat between graphbased semisupervised learning and standard Bayes nets. In standard Bayes nets there is one conditional probability table on each node, which looks at the values of all its parents and determinesthe distribution of the node. However in Conditional Harmonic Mixing there is onetable on each directed edge. On one hand it is simpler because each table dealswith only one parent node. On the other hand at the child node the estimated distributions from the parents may not be consistent, and the child takes the averagedistribution in KL divergence. Importantly the directed graph can contain loops,and there is always a unique global solution. It can be shown that the harmonicfunction can be interpreted as a special case of Conditional Harmonic Mixing.7 Computational Learning TheoryIn this survey we have primarily focused on various semisupervised learning algorithms. The theory of semisupervised learning has been touched upon occasionally in the literature. However it was not until recently that the computationallearning theory community began to pay more attention to this interesting problem.Leskes 2005 presents a generalization error bound for semisupervised learning with multiple learners, an extension to cotraining. The author shows thatif multiple learning algorithms are forced to produce similar hypotheses i.e. toagree given the same training set, and such hypotheses still have low training error, then the generalization error bound is tighter. The unlabeled data is used toassess the agreement among hypotheses. The author proposes a new AgreementBoost algorithm to implement the procedure.Kaariainen 2005 presents another generalization error bound for semisupervisedlearning. The idea is that the target function is in the version space. If a hypothesisis in the version space revealed by labeled data, and is close to all other hypotheses in the version space revealed by unlabeled data, then it has to be close tothe target function. Closeness is defined as classification agreement, and can beapproximated using unlabeled data. This idea builds on metricbased model selection Section 9.9.Balcan and Blum 2005 propose a PACstyle model for semisupervised learning. This is the first PAC model that explains when unlabeled data might helpnotice the classic PAC model cannot incorporate unlabeled data at all. Therehas been previous particular analysis for explaining when unlabeled data helps,27but they were all based on specific settings and assumptions. In contrast this PACmodel is a general, unifying model. The authors define an interesting quantitythe compatibility of a hypothesis w.r.t. the unlabeled data distribution. For example in SVM a hyperplane that cuts through high density regions would have lowcompatibility, while one that goes along gaps would have high compatibility. Wenote that the compatibility function can be defined much more generally. The intuition of the results is the following. Assuming apriori that the target functionhas high compatibility with unlabeled data. Then if a hypothesis has zero trainingerror standard PAC style and high compatibility, the theory gives the number oflabeled and unlabeled data to guarantee the hypothesis is good. The number oflabeled data needed can be quite small.8 Semisupervised Learning in Structured Output SpacesIn most of this paper we consider classification on individual instances. In thissection we discuss semisupervised learning in structured output spaces, e.g. forsequences and trees.8.1 Generative ModelsOne example of generative models for semisupervised sequence learning is theHidden Markov Model HMM, in particular the BaumWelsh HMM training algorithm Rabiner, 1989. It is essentially the sequence version of the EM algorithmon mixture models as mentioned in section 2. BaumWelsh algorithm has a longhistory, well before the recent emergence of interest on semisupervised learning.It has been successfully applied to many areas including speech recognition. It isusually not presented as a semisupervised learning algorithm, but certainly qualifies as one. Some cautionary notes can be found in Elworthy, 1994.8.2 Graphbased KernelsMany existing structured learning algorithms e.g. conditional random fields, maximum margin Markov networks can be endowed with a semisupervised kernel.Take the example of learning on sequences. One first creates a graph kernel on theunion of all elements in the sequences i.e. ignoring the sequence structure, treating the elements of a sequence as if they were individual instances. The graphkernel can be constructed with any of the above methods. Next one applies thegraph kernel to a standard structured learning kernel machine. Such kernel machines include the kernelized conditional random fields Lafferty et al., 2004 and28maximum margin Markov networks Taskar et al., 2003, which differ primarilyby the loss function they use.With a graph kernel the kernel machine thus perform semisupervised learning on structured data. Lafferty et al. 2004 hinted this idea and tested it on abioinformatics dataset. The graph kernel matrix they used is transductive in nature, which is defined only on elements in the training data. Altun et al. 2005defines a graph kernel over the whole space by linearly combining the norms ofa standard kernel and a graph regularization term, resulting in a nonlinear graphkernel similar to Sindhwani et al. 2005a. They use the kernel with a margin loss.Brefeld and Scheffer 2006 extend structured SVM with a multiview regularizer,which penalizes disagreements between classifications on unlabeled data, wherethe classifiers operate on different feature subsets.9 Related AreasThe focus of the survey is on classification with semisupervised methods. Thereare some closely related areas with a rich literature.9.1 Spectral ClusteringSpectral clustering is unsupervised. As such there is no labeled data to guide theprocess. Instead the clustering depends solely on the graph weights W . On theother hand semisupervised learning for classification has to maintain a balancebetween how good the clustering is, and how well the labeled data can be explained by it. Such balance is expressed explicitly in the regularization framework.As we have seen in section 8.1 of Zhu, 2005 and section 6.5 here, the topeigenvectors of the graph Laplacian can unfold the data manifold to form meaningful clusters. This is the intuition behind spectral clustering. There are severalcriteria on what constitutes a good clustering Weiss, 1999.The normalized cut Shi  Malik, 2000 seeks to minimizeNcutA,B cutA,BassocA, V cutA,BassocB, V 17The continuous relaxation of the cluster indicator vector can be derived from thenormalized Laplacian. In fact it is derived from the second smallest eigenvector ofthe normalized Laplacian. The continuous vector is then discretized to obtain theclusters.The data points are mapped into a new space spanned by the first k eigenvectors of the normalized Laplacian in Ng et al., 2001, with special normalization.29Clustering is then performed with traditional methods like kmeans in this newspace. This is very similar to kernel PCA.Fowlkes et al. 2004 use the Nystrom method to reduce the computation costfor large spectral clustering problems. This is related to the method in Zhu, 2005Chapter 10.Chung 1997 presents the mathematical details of spectral graph theory.9.2 Learning with Positive and Unlabeled DataIn many real world applications, labeled data may be available from only one ofthe two classes. Then there is the unlabeled data, known to contain both classes.There are two ways to formulate the problem classification or ranking.Classification Here one builds a classifier even though there is no negativeexample. It is important to note that with the positive training data one can estimatethe positive class conditional probability px, and with the unlabeled data onecan estimate px. If the prior p is known or estimated from other sources, onecan derive the negative class conditional aspx px ppx1 p18With px one can then perform classification with Bayes rule. Denis et al.2002 use this fact for text classification with Naive Bayes models.Another set of methods heuristically identify some reliable negative examplesin the unlabeled set, and use EM on generative Naive Bayes models Liu et al.,2002 or logistic regression Lee  Liu, 2003.Ranking Given a large collection of items, and a few query items, rankingorders the items according to their similarity to the queries. Information retrievalis the standard technique under this setting, and we will not attempt to include theextensive literatures on this mature field. It is worth pointing out that graphbasedsemisupervised learning can be modified for such settings. Zhou et al. 2004btreat it as semisupervised learning with positive data on a graph, where the graphinduces a similarity measure, and the queries are positive examples. Data pointsare ranked according to their graph similarity to the positive training set.9.3 Semisupervised ClusteringAlso known as clustering with side information, this is the cousin of semisupervisedclassification. The goal is clustering but there are some labeled data in the formof mustlinks two points must in the same cluster and cannotlinks two pointscannot in the same cluster. There is a tension between satisfying these constraints30and optimizing the original clustering criterion e.g. minimizing the sum of squareddistances within clusters. Procedurally one can modify the distance metric to tryto accommodate the constraints, or one can bias the search. We refer readers to arecent short survey Grira et al., 2004 for the literatures.9.4 Semisupervised RegressionIn principle all graphbased semisupervised classification methods in section 6are indeed function estimators. That is, they estimate soft labels before makinga classification. The function tries to be close to the targets y in the labeled set,and at the same time be smooth on the graph. Therefore these graphbased semisupervised methods can also naturally perform regression. Some of the methodscan be thought of as Gaussian processes with a special kernel that is constructedfrom unlabeled data.Zhou and Li 2005a proposed using cotraining for semisupervised regression. The paper used two kNN regressors, each with a different pnorm as distancemeasure. Like in cotraining, each regressor makes prediction on unlabeled data,and the most confident predictions are used to train the other regressor. The confidence of a prediction on unlabeled point is measured by the MSE on labeledset before and after adding this prediction as training data to the current regressor. Similarly Sindhwani et al. 2005b Brefeld et al. 2006 perform multiviewregression, where a regularization term depends on the disagreement among regressors on different views.Cortes and Mohri 2006 propose a simple yet efficient transductive regressionmodel. On top of a standard ridge regression model, an addition term is applied toeach unlabeled point xu. This additional regularization term makes the predictionfxu close to a heuristic prediction yu, which is computed by a weighted averageof the labels of labeled points in a neighborhood of xu. A generalization errorbound is also given.9.5 Active Learning and Semisupervised LearningActive learning and semisupervised learning face the same issue, i.e. that labeleddata is scarce and hard to obtain. It is quite natural to combine active learning andsemisupervised learning to address this issue from both ends.McCallum and Nigam 1998b use EM with unlabeled data integrated into theactive learning algorithm. Muslea et al. 2002 propose COEMT which combinesmultiview e.g. cotraining learning with active learning. Zhou et al. 2004c apply semisupervised learning together with active learning to contentbased imageretrieval.31Many active learning algorithms naively select as query the point with maximum label ambiguity entropy, or least confidence, or maximum disagreementbetween multiple learners. Zhu et al. 2003b show that these are not necessarilythe right things to do, if one is interested in classification error. They show thatone can select active learning queries that minimize the estimated generalizationerror, in a graphbased semisupervised learning framework.9.6 Nonlinear Dimensionality ReductionThe goal of nonlinear dimensionality reduction is to find a faithful low dimensionalmapping of the high dimensional data. As such it belongs to unsupervised learning.However the way it discovers low dimensional manifold within a high dimensionalspace is closely related to spectral graph semisupervised learning. Representativemethods include Isomap Tenenbaum et al., 2000, locally linear embedding LLERoweis  Saul, 2000 Saul  Roweis, 2003, Hessian LLE Donoho  Grimes,2003, Laplacian eigenmaps Belkin  Niyogi, 2003, and semidefinite embeddingSDE Weinberger  Saul, 2004 Weinberger et al., 2004 Weinberger et al.,2005.9.7 Learning a Distance MetricMany learning algorithms depend, either explicitly or implicitly, on a distance metric on X . We use the term metric here loosely to mean a measure of distance ordissimilarity between two data points. The default distance in the feature spacemay not be optimal, especially when the data forms a lower dimensional manifoldin the feature vector space. With a large amount of U , it is possible to detect suchmanifold structure and its associated metric. The graphbased methods above arebased on this principle. We review some other methods next.The simplest example in text classification might be Latent Semantic IndexingLSI, a.k.a. Latent Semantic Analysis LSA, Principal Component Analysis PCA,or sometimes Singular Value Decomposition SVD. This technique defines a linear subspace, such that the variance of the data, when projected to the subspace,is maximumly preserved. LSI is widely used in text classification, where the original space for X is usually tens of thousands dimensional, while people believemeaningful text documents reside in a much lower dimensional space. Zelikovitzand Hirsh 2001 and Cristianini et al. 2001 both use U , in this case unlabeleddocuments, to augment the termbydocument matrix of L. LSI is performed onthe augmented matrix. This representation induces a new distance metric. By theproperty of LSI, words that cooccur very often in the same documents are mergedinto a single dimension of the new space. In the extreme this allows two docu32ments with no common words to be close to each other, via chains of cooccurword pairs in other documents.Oliveira et al. 2005 propose a simple procedure for semisupervised learningFirst one runs PCA on L  U ignoring the labels. The result is a linear subspacethat is constructed with more data points if one uses only L in PCA. In the nextstep, only L is mapped onto the subspace, and an SVM is learned. The method isuseful when class separation is linear and along the principal component directions,and unlabeled helps by reducing the variance in estimating such directions.Probabilistic Latent Semantic Analysis PLSA Hofmann, 1999 is an important improvement over LSI. Each word in a document is generated by a topic amultinomial, i.e. unigram. Different words in the document may be generated bydifferent topics. Each document in turn has a fixed topic proportion a multinomial on a higher level. However there is no link between the topic proportions indifferent documents.Latent Dirichlet Allocation LDA Blei et al., 2003 is one step further. Itassumes the topic proportion of each document is drawn from a Dirichlet distribution. With variational approximation, each document is represented by a posteriorDirichlet over the topics. This is a much lower dimensional representation. Griffiths et al. 2005 extend LDA model to HMMLDA which uses both shorttermsyntactic and longterm topical dependencies, as an effort to integrate semanticsand syntax. Li and McCallum 2005 apply the HMMLDA model to obtain wordclusters, as a rudimentary way for semisupervised learning on sequences.Some algorithms derive a metric entirely from the density of U . These are motivated by unsupervised clustering and based on the intuition that data points in thesame high density clump should be close in the new metric. For instance, if Uis generated from a single Gaussian, then the Mahalanobis distance induced by thecovariance matrix is such a metric. Tipping 1999 generalizes the Mahalanobisdistance by fitting U with a mixture of Gaussian, and define a Riemannian manifold with metric at x being the weighted average of individual component inversecovariance. The distance between x1 and x2 is computed along the straight line inEuclidean space between the two points. Rattray 2000 further generalizes themetric so that it only depends on the change in log probabilities of the density, noton a particular Gaussian mixture assumption. And the distance is computed alonga curve that minimizes the distance. The new metric is invariant to linear transformation of the features, and connected regions of relatively homogeneous densityin U will be close to each other. Such metric is attractive, yet it depends on thehomogeneity of the initial Euclidean space. Their application in semisupervisedlearning needs further investigation. Sajama and Orlitsky 2005 analyze the lowerand upper bounds on estimating datadensitybased distance. There are two sourcesof error one stems from the fact that the true density px is not known, the second33is that for practical reasons one typically build a grid on the data points, instead ofa regular grid in Rd. The authors separate these two kinds of errors computationaland estimation, and analyze them independently. It sheds light on the complexity of densitybased distance, independent of the specific method one uses. It alsosheds some light on approximation errors when using neighborhood graphs ondata points, which is used widely in semisupervised learning and nonlinear dimensionality reduction, etc. Understanding this dichotomy is helpful when tryingto improve methods for semisupervised learning.We caution the reader that the metrics proposed above are based on unsupervised techniques. They all identify a lower dimensional manifold within which thedata reside. However the data manifold may or may not correlate with a particularclassification task. For example, in LSI the new metric emphasizes words withprominent count variances, but ignores words with small variances. If the classification task is subtle and depends on a few words with small counts, LSI mightwipe out the salient words all together. Therefore the success of these methodsis hard to guarantee without putting some restrictions on the kind of classificationtasks. It would be interesting to include L into the metric learning process.In a separate line of work, Baxter 1997 proves that there is a unique optimalmetric for classification if we use 1nearestneighbor. The metric, named Canonical Distortion Measure CDM, defines a distance dx1, x2 as the expected loss ifwe classify x1 with x2s label. The distance measure proposed in Yianilos, 1995can be viewed as a special case. Yianilos assume a Gaussian mixture model hasbeen learned from U , such that a class correspond to a component, but the correspondence is unknown. In this case CDM dx1, x2  px1, x2from same componentand can be computed analytically. Now that a metric has been learned from U , wecan find within L the 1nearestneighbor of a new data point x, and classify x withthe nearest neighbors label. It will be interesting to compare this scheme with EMbased semisupervised learning, where L is used to label mixture components.Weston et al. 2004 propose the neighborhood mismatch kernel and the baggedmismatch kernel. More precisely both are kernel transformation that modifies aninput kernel. In the neighborhood method, one defines the neighborhood of a pointas points close enough according to certain similarity measure note this is notthe measure induced by the input kernel. The output kernel between point i, j isthe average of pairwise kernel entries between is neighbors and js neighbors. Inbagged method, if a clustering algorithm thinks they tend to be in the same clusternote again this is a different measure than the input kernel, the correspondingentry in the input kernel is boosted.349.8 Inferring Label Sampling MechanismsMost semisupervised learning methods assume L and U are both i.i.d. from theunderlying distribution. However as Rosset et al., 2005 points out that is notalways the case. For example y can be the binary label whether a customer issatisfied, obtained through a survey. It is conceivable survey participation andthus labeled data depends on the satisfaction y.Let si be the binary missing indicator for yi. The authors model psx, ywith a parametric family. The goal is to estimate psx, y which is the labelsampling mechanism. This is done by computing the expectation of an arbitrary function gx in two ways on L  U as 1nni1 gxi, and on L only as1niL gxipsi  1xi, yi. By equating the two psx, y can be estimated.The intuition is that the expectation on L requires weighting the labeled samplesinversely proportional to the labeling probability, to compensate for ignoring theunlabeled data.9.9 MetricBased Model SelectionMetricbased model selection Schuurmans  Southey, 2001 is a method to detecthypotheses inconsistency with unlabeled data. We may have two hypotheses whichare consistent on L, for example they all have zero training set error. However theymay be inconsistent on the much larger U . If so we should reject at least one ofthem, e.g. the more complex one if we employ Occams razor.The key observation is that a distance metric is defined in the hypothesis spaceH . One such metric is the number of different classifications two hypotheses makeunder the data distribution px dph1, h2  Eph1x 6 h2x. It is easy toverify that the metric satisfies the three metric properties. Now consider the trueclassification function h and two hypotheses h1, h2. Since the metric satisfies thetriangle inequality the third property, we havedph1, h2  dph1, h  dph, h2Under the premise that labels in L is noiseless, lets assume we can approximatedph1, h and dph, h2 by h1 and h2s training set error rates dLh1, h anddLh2, h, and approximate dph1, h2 by the difference h1 and h2 make on alarge amount of unlabeled data U  dU h1, h2. We getdU h1, h2  dLh1, h  dLh, h2which can be verified directly. If the inequality does not hold, at least one of theassumptions is wrong. If U  is large enough and Uiid px, dU h1, h2 will be35a good estimate of dph1, h2. This leaves us with the conclusion that at least oneof the training errors does not reflect its true error. If both training errors are closeto zero, we would know that at least one model is overfitting. An Occams razortype of argument then can be used to select the model with less complexity. Suchuse of unlabeled data is very general and can be applied to almost any learningalgorithms. However it only selects among hypotheses it does not generate newhypothesis based on unlabeled data.The covalidation method Madani et al., 2005 also uses unlabeled data formodel selection and active learning. Kaariainen 2005 uses the metric to derive ageneralization error bound, see Section 7.10 Scalability Issues of SemiSupervised Learning MethodsCurrent semisupervised learning methods have not yet handled large amount ofdata. The complexity of many elegant graphbased methods is close to On3.Speedup improvements have been proposed Mahdaviani et al. 2005 Delalleau etal. 2005 Zhu and Lafferty 2005 Yu et al. 2005 Garcke and Griebel 2005 andmore, but their effectiveness has yet to be proven on real large problems. Figure 7compares the experimental dataset sizes in many representative semisupervisedlearning papers. The unlabeled dataset size in these papers are evidently not large.Ironically huge amount of unlabeled data should have been the optimal operationenvironment for semisupervised learning. More research efforts are needed toaddress the scalability issue.11 Do Humans do SemiSupervised LearningNow let us turn our attention from machine learning to human learning. It is possible that understanding of the human cognitive model will lead to novel machinelearning approaches Langley, 2006 Mitchell, 2006. We ask the question Dohumans do semisupervised learning My hypothesis is yes. We humans accumulate unlabeled input data, which we use often unconsciously to help buildingthe connection between labels and input once labeled data is provided. I presentsome evidence below.11.1 Visual Object Recognition with Temporal AssociationThe appearance of an object usually changes greatly when viewed from differentangles. In the case of faces, the difference between the same face from two view361001021041021041061081010world populationinternet users in the USpeople in full stadiumlabeled data sizeunlabeled data sizeFigure 7 As recently as 2005, semisupervised learning methods have not addressed largescale problems. Shown above are the largest dataset size labeledand unlabeled portion respectively used in representative semisupervised learning papers. Each dot is a paper, with darkness indicating publication year darkest2005, lightest 1998. Most papers only used hundreds of labeled points and tensof thousands of unlabeled points. Also shown are some interesting large numbersfor comparison. Note the loglog scale.37right classleft classlarge withinclass distancesmallbetweenclassdistanceFigure 8 Classify teapot images by its spout orientation. Some images within thesame class are quite different, while some images from different classes are similar.points can be much larger than the difference between two faces from the sameangle. Human observers nonetheless can connect the correct faces. It has beensuggested that temporal correlation serves as the glue, as summarized by Sinhaet al., 2006 Result 14. It seems when we observe an object with changing angles,we link the images as containing the same object by the virtue that the images areclose in time. Wallis and Bulthoff 2001 created artificial image sequences wherea frontal face is morphed into the profile face of a different person. When observersare shown such sequences during training, their ability to match frontal and profilefaces was impaired during test, due to the wrong links. The authors further arguethat the object has to have similar location in the images to establish the link.The idea of spatiotemporal link is directly related to graphbased semisupervisedlearning. Consider the Teapot dataset used in Zhu  Lafferty, 2005 originallyfrom Weinberger et al., 2004, with images of a teapot viewed from differentangles. Now suppose we want to classify an image by whether its spout pointsto the left or right. As Figure 8 shows there are large withinclass distances andsmall betweenclass distances. However the similarity between adjacent imageswhich comes from temporal relation allow a graph to be constructed for semisupervised learning. In another work, Balcan et al. 2005a construct a graph onwebcam images using temporal links as well as color, face similarity links forsemisupervised learning.11.2 Infant WordMeaning Mapping17month old infants were shown to be able to associate a word with a visual objectbetter if they have heard the word many times before Graf Estes et al., 2006. Ifthe word was not heard before, the infants ability to associate it with the objectwas weaker. If we view the sound of the word as unlabeled data, and the object asthe label, we can propose a model where an infant builds up clusters of familiar38sounding words, which are easily labeled as a whole. This is similar to semisupervised learning with mixture models Nigam et al., 2000 or clusters Daraet al., 2002 Demiriz et al., 1999.AcknowledgmentI thank John Lafferty, Zoubin Ghahramani, Tommi Jaakkola, Ronald Rosenfeld,Maria Florina Balcan, Kai Yu, Sajama, Matthias Seeger, Yunpeng Xu, OlivierChapelle, ZhiHua Zhou, and all other colleagues who discussed the literature withme.ReferencesAltun, Y., McAllester, D.,  Belkin, M. 2005. Maximum margin semisupervisedlearning for structured variables. Advances in Neural Information ProcessingSystems NIPS 18.Argyriou, A. 2004. Efficient approximation methods for harmonic semisupervised learning. Masters thesis, University College London.Balcan, M.F.,  Blum, A. 2005. A PACstyle model for learning from labeledand unlabeled data. COLT 2005.Balcan, M.F., Blum, A., Choi, P. P., Lafferty, J., Pantano, B., Rwebangira, M. R., Zhu, X. 2005a. Person identification in webcam images An applicationof semisupervised learning. ICML2005 Workshop on Learning with PartiallyClassified Training Data.Balcan, M.F., Blum, A.,  Yang, K. 2005b. Cotraining and expansion Towardsbridging theory and practice. In L. K. Saul, Y. Weiss and L. Bottou Eds.,Advances in neural information processing systems 17. Cambridge, MA MITPress.Baluja, S. 1998. Probabilistic modeling for face orientation discriminationLearning from labeled and unlabeled data. Neural Information Processing Systems.Baxter, J. 1997. The canonical distortion measure for vector quantization andfunction approximation. Proc. 14th International Conference on Machine Learning pp. 3947. Morgan Kaufmann.39Belkin, M., Matveeva, I.,  Niyogi, P. 2004a. Regularization and semisupervised learning on large graphs. COLT.Belkin, M.,  Niyogi, P. 2003. Laplacian eigenmaps for dimensionality reductionand data representation. Neural Computation, 15, 13731396.Belkin, M., Niyogi, P.,  Sindhwani, V. 2004b. Manifold regularization Ageometric framework for learning from examples Technical Report TR200406. University of Chicago.Belkin, M., Niyogi, P.,  Sindhwani, V. 2005. On manifold regularization.Proceedings of the Tenth International Workshop on Artificial Intelligence andStatistics AISTAT 2005.Bennett, K.,  Demiriz, A. 1999. Semisupervised support vector machines.Advances in Neural Information Processing Systems, 11, 368374.Blei, D. M., Ng, A. Y.,  Jordan, M. I. 2003. Latent dirichlet allocation. Journalof Machine Learning Research, 3, 9931022.Blum, A.,  Chawla, S. 2001. Learning from labeled and unlabeled data usinggraph mincuts. Proc. 18th International Conf. on Machine Learning.Blum, A., Lafferty, J., Rwebangira, M.,  Reddy, R. 2004. Semisupervisedlearning using randomized mincuts. ICML04, 21st International Conference onMachine Learning.Blum, A.,  Mitchell, T. 1998. Combining labeled and unlabeled data withcotraining. COLT Proceedings of the Workshop on Computational LearningTheory.Bousquet, O., Chapelle, O.,  Hein, M. 2004. Measure based regularization.Advances in Neural Information Processing Systems 16..Brefeld, U., Gaertner, T., Scheffer, T.,  Wrobel, S. 2006. Efficient coregularized least squares regression. ICML06, 23rd International Conferenceon Machine Learning. Pittsburgh, USA.Brefeld, U.,  Scheffer, T. 2006. Semisupervised learning for structured output variables. ICML06, 23rd International Conference on Machine Learning.Pittsburgh, USA.Burges, C. J.,  Platt, J. C. 2005. Semisupervised learning with conditional harmonic mixing. In O. Chapelle, B. Scholkopf and A. Zien Eds., Semisupervisedlearning. Cambridge, MA MIT Press.40CallisonBurch, C., Talbot, D.,  Osborne, M. 2004. Statistical machine translation with word and sentencealigned parallel corpora. Proceedings of the ACL.CarreiraPerpinan, M. A.,  Zemel, R. S. 2005. Proximity graphs for clusteringand manifold learning. In L. K. Saul, Y. Weiss and L. Bottou Eds., Advancesin neural information processing systems 17. Cambridge, MA MIT Press.Castelli, V.,  Cover, T. 1995. The exponential value of labeled samples. PatternRecognition Letters, 16, 105111.Castelli, V.,  Cover, T. 1996. The relative value of labeled and unlabeled samples in pattern recognition with an unknown mixing parameter. IEEE Transactions on Information Theory, 42, 21012117.Chapelle, O., Chi, M.,  Zien, A. 2006a. A continuation method for semisupervised SVMs. ICML06, 23rd International Conference on Machine Learning. Pittsburgh, USA.Chapelle, O., Sindhwani, V.,  Keerthi, S. S. 2006b. Branch and bound for semisupervised support vector machines. Advances in Neural Information ProcessingSystems NIPS.Chapelle, O., Weston, J.,  Scholkopf, B. 2002. Cluster kernels for semisupervised learning. Advances in Neural Information Processing Systems, 15.Chapelle, O.,  Zien, A. 2005. Semisupervised classification by low densityseparation. Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics AISTAT 2005.Chapelle, O., Zien, A.,  Scholkopf, B. Eds.. 2006c. Semisupervised learning.MIT Press.Chu, W.,  Ghahramani, Z. 2004. Gaussian processes for ordinal regressionTechnical Report. University College London.Chu, W., Sindhwani, V., Ghahramani, Z.,  Keerthi, S. S. 2006. Relationallearning with gaussian processes. Advances in NIPS.Chung, F. R. K. 1997. Spectral graph theory, regional conference series in mathematics, no. 92. American Mathematical Society.Collobert, R., Weston, J.,  Bottou, L. 2006. Trading convexity for scalability.ICML06, 23rd International Conference on Machine Learning. Pittsburgh, USA.41Corduneanu, A.,  Jaakkola, T. 2001. Stable mixing of complete and incompleteinformation Technical Report AIM2001030. MIT AI Memo.Corduneanu, A.,  Jaakkola, T. 2003. On information regularization. NineteenthConference on Uncertainty in Artificial Intelligence UAI03.Corduneanu, A.,  Jaakkola, T. S. 2005. Distributed information regularizationon graphs. In L. K. Saul, Y. Weiss and L. Bottou Eds., Advances in neuralinformation processing systems 17. Cambridge, MA MIT Press.Cortes, C.,  Mohri, M. 2006. On transductive regression. Advances in NeuralInformation Processing Systems NIPS 19.Cozman, F., Cohen, I.,  Cirelo, M. 2003. Semisupervised learning of mixturemodels. ICML03, 20th International Conference on Machine Learning.Cristianini, N., ShaweTaylor, J.,  Lodhi, H. 2001. Latent semantic kernels.Proc. 18th International Conf. on Machine Learning.Dara, R., Kremer, S.,  Stacey, D. 2002. Clsutering unlabeled data with SOMsimproves classification of labeled realworld data. Proceedings of the WorldCongress on Computational Intelligence WCCI.Delalleau, O., Bengio, Y.,  Roux, N. L. 2005. Efficient nonparametric functioninduction in semisupervised learning. Proceedings of the Tenth InternationalWorkshop on Artificial Intelligence and Statistics AISTAT 2005.Demirez, A.,  Bennett, K. 2000. Optimization approaches to semisupervisedlearning. In M. Ferris, O. Mangasarian and J. Pang Eds., Applications andalgorithms of complementarity. Boston Kluwer Academic Publishers.Demiriz, A., Bennett, K.,  Embrechts, M. 1999. Semisupervised clusteringusing genetic algorithms. Proceedings of Artificial Neural Networks in Engineering.Dempster, A., Laird, N.,  Rubin, D. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, SeriesB.Denis, F., Gilleron, R.,  Tommasi, M. 2002. Text classification from positiveand unlabeled examples. The 9th International Conference on Information Processing and Management of Uncertainty in KnowledgeBased SystemsIPMU.42Donoho, D. L.,  Grimes, C. E. 2003. Hessian eigenmaps locally linear embedding techniques for highdimensional data. Proceedings of the NationalAcademy of Arts and Sciences, 100, 55915596.Elworthy, D. 1994. Does BaumWelch reestimation help taggers Proceedingsof the 4th Conference on Applied Natural Language Processing.Fowlkes, C., Belongie, S., Chung, F.,  Malik, J. 2004. Spectral grouping using the Nystrom method. IEEE Transactions on Pattern Analysis and MachineIntelligence, 26, 214225.Fujino, A., Ueda, N.,  Saito, K. 2005. A hybrid generativediscriminative approach to semisupervised classifier design. AAAI05, The Twentieth NationalConference on Artificial Intelligence.Fung, G.,  Mangasarian, O. 1999. Semisupervised support vector machines forunlabeled data classification Technical Report 9905. Data Mining Institute,University of Wisconsin Madison.Garcke, J.,  Griebel, M. 2005. Semisupervised learning with sparse grids.Proc. of the 22nd ICML Workshop on Learning with Partially Classified TrainingData. Bonn, Germany.Getz, G., Shental, N.,  Domany, E. 2005. Semisupervised learning  a statistical physics approach. Proc. of the 22nd ICML Workshop on Learning withPartially Classified Training Data. Bonn, Germany.Goldberg, A. B.,  Zhu, X. 2006. Seeing stars when there arent manystars Graphbased semisupervised learning for sentiment categorization. HLTNAACL 2006 Workshop on Textgraphs Graphbased Algorithms for NaturalLanguage Processing. New York, NY.Goldman, S.,  Zhou, Y. 2000. Enhancing supervised learning with unlabeleddata. Proc. 17th International Conf. on Machine Learning pp. 327334. Morgan Kaufmann, San Francisco, CA.Grady, L.,  FunkaLea, G. 2004. Multilabel image segmentation for medicalapplications based on graphtheoretic electrical potentials. ECCV 2004 workshop.Graf Estes, K., Evans, J. L., Alibali, M. W.,  Saffran, J. R. 2006. Can infantsmap meaning to newly segmented words statistical segmentation and wordlearning. Psychological Science. to appear.43Grandvalet, Y.,  Bengio, Y. 2005. Semisupervised learning by entropy minimization. In L. K. Saul, Y. Weiss and L. Bottou Eds., Advances in neuralinformation processing systems 17. Cambridge, MA MIT Press.Griffiths, T. L., Steyvers, M., Blei, D. M.,  Tenenbaum, J. B. 2005. Integratingtopics and syntax. NIPS 17.Grira, N., Crucianu, M.,  Boujemaa, N. 2004. Unsupervised and semisupervised clustering a brief survey. in A Review of Machine Learning Techniques for Processing Multimedia Content, Report of the MUSCLE EuropeanNetwork of Excellence FP6.Hein, M.,  Maier, M. 2006. Manifold denoising. Advances in Neural Information Processing Systems NIPS 19.Hofmann, T. 1999. Probabilistic latent semantic analysis. Proc. of Uncertaintyin Artificial Intelligence, UAI99. Stockholm.Holub, A., Welling, M.,  Perona, P. 2005. Exploiting unlabelled data for hybridobject classification. NIPS 2005 Workshop in InterClass Transfer.Jaakkola, T.,  Haussler, D. 1998. Exploiting generative models in discriminativeclassifiers. Advances in Neural Information Processing Systems 11.Jaakkola, T., Meila, M.,  Jebara, T. 1999. Maximum entropy discrimination.Neural Information Processing Systems, 12, 12.Joachims, T. 1999. Transductive inference for text classification using supportvector machines. Proc. 16th International Conf. on Machine Learning pp. 200209. Morgan Kaufmann, San Francisco, CA.Joachims, T. 2003. Transductive learning via spectral graph partitioning. Proceedings of ICML03, 20th International Conference on Machine Learning.Jones, R. 2005. Learning to extract entities from labeled and unlabeled textTechnical Report CMULTI05191. Carnegie Mellon University. DoctoralDissertation.Kaariainen, M. 2005. Generalization error bounds using unlabeled data. COLT2005.Kapoor, A., Qi, Y., Ahn, H.,  Picard, R. 2005. Hyperparameter and kernellearning for graph based semisupervised classification. Advances in NIPS.44Kemp, C., Griffiths, T., Stromsten, S.,  Tenenbaum, J. 2003. Semisupervisedlearning with trees. Advances in Neural Information Processing System 16.Kondor, R. I.,  Lafferty, J. 2002. Diffusion kernels on graphs and other discreteinput spaces. Proc. 19th International Conf. on Machine Learning.Krishnapuram, B., Williams, D., Xue, Y., Hartemink, A., Carin, L.,  Figueiredo,M. 2005. On semisupervised classification. In L. K. Saul, Y. Weiss and L. Bottou Eds., Advances in neural information processing systems 17. Cambridge,MA MIT Press.Lafferty, J., Zhu, X.,  Liu, Y. 2004. Kernel conditional random fields Representation and clique selection. Proceedings of ICML04, 21st InternationalConference on Machine Learning.Langley, P. 2006. Intelligent behavior in humans and machines Technical Report. Computational Learning Laboratory, CSLI, Stanford University.Lawrence, N. D.,  Jordan, M. I. 2005. Semisupervised learning via Gaussianprocesses. In L. K. Saul, Y. Weiss and L. Bottou Eds., Advances in neuralinformation processing systems 17. Cambridge, MA MIT Press.Lee, C.H., Wang, S., Jiao, F., Schuurmans, D.,  Greiner, R. 2006. Learning tomodel spatial dependency Semisupervised discriminative random fields. Advances in Neural Information Processing Systems NIPS 19.Lee, W. S.,  Liu, B. 2003. Learning with positive and unlabeled examplesusing weighted logistic regression. Proceedings of the Twentieth InternationalConference on Machine Learning ICML.Leskes, B. 2005. The value of agreement, a new boosting algorithm. COLT 2005.Levin, A., Lischinski, D.,  Weiss, Y. 2004. Colorization using optimization.ACM Transactions on Graphics.Li, W.,  McCallum, A. 2005. Semisupervised sequence modeling with syntactic topic models. AAAI05, The Twentieth National Conference on ArtificialIntelligence.Liu, B., Lee, W. S., Yu, P. S.,  Li, X. 2002. Partially supervised classificationof text documents. Proceedings of the Nineteenth International Conference onMachine Learning ICML.45Lu, Q.,  Getoor, L. 2003. Linkbased classification using labeled and unlabeleddata. ICML 2003 workshop on The Continuum from Labeled to Unlabeled Datain Machine Learning and Data Mining.Madani, O., Pennock, D. M.,  Flake, G. W. 2005. Covalidation Using modeldisagreement to validate classification algorithms. In L. K. Saul, Y. Weiss andL. Bottou Eds., Advances in neural information processing systems 17. Cambridge, MA MIT Press.Maeireizo, B., Litman, D.,  Hwa, R. 2004. Cotraining for predicting emotionswith spoken dialogue data. The Companion Proceedings of the 42nd AnnualMeeting of the Association for Computational Linguistics ACL.Mahdaviani, M., de Freitas, N., Fraser, B.,  Hamze, F. 2005. Fast computational methods for visually guided robots. The 2005 International Conferenceon Robotics and Automation ICRA.McCallum, A.,  Nigam, K. 1998a. A comparison of event models for naivebayes text classification. AAAI98 Workshop on Learning for Text Categorization.McCallum, A. K.,  Nigam, K. 1998b. Employing EM in poolbased activelearning for text classification. Proceedings of ICML98, 15th International Conference on Machine Learning pp. 350358. Madison, US Morgan KaufmannPublishers, San Francisco, US.Miller, D.,  Uyar, H. 1997. A mixture of experts classifier with learning basedon both labelled and unlabelled data. Advances in NIPS 9 pp. 571577.Mitchell, T. 1999. The role of unlabeled data in supervised learning. Proceedings of the Sixth International Colloquium on Cognitive Science. San Sebastian,Spain.Mitchell, T. 2006. The discipline of machine learning Technical Report CMUML06108. Carnegie Mellon University.Muslea, I., Minton, S.,  Knoblock, C. 2002. Active  semisupervised learning  robust multiview learning. Proceedings of ICML02, 19th InternationalConference on Machine Learning pp. 435442.Narayanan, H., Belkin, M.,  Niyogi, P. 2006. On the relation between lowdensity separation, spectral clustering and graph cuts. Advances in Neural Information Processing Systems NIPS 19.46Ng, A., Jordan, M.,  Weiss, Y. 2001. On spectral clustering Analysis and analgorithm. Advances in Neural Information Processing Systems, 14.Nigam, K. 2001. Using unlabeled data to improve text classification TechnicalReport CMUCS01126. Carnegie Mellon University. Doctoral Dissertation.Nigam, K.,  Ghani, R. 2000. Analyzing the effectiveness and applicabilityof cotraining. Ninth International Conference on Information and KnowledgeManagement pp. 8693.Nigam, K., McCallum, A. K., Thrun, S.,  Mitchell, T. 2000. Text classificationfrom labeled and unlabeled documents using EM. Machine Learning, 39, 103134.Niu, Z.Y., Ji, D.H.,  Tan, C.L. 2005. Word sense disambiguation using labelpropagation based semisupervised learning. Proceedings of the ACL.Oliveira, C. S., Cozman, F. G.,  Cohen, I. 2005. Splitting the unsupervised andsupervised components of semisupervised learning. Proc. of the 22nd ICMLWorkshop on Learning with Partially Classified Training Data. Bonn, Germany.Pang, B.,  Lee, L. 2004. A sentimental education Sentiment analysis usingsubjectivity summarization based on minimum cuts. Proceedings of the Association for Computational Linguistics pp. 271278.Pham, T. P., Ng, H. T.,  Lee, W. S. 2005. Word sense disambiguation with semisupervised learning. AAAI05, The Twentieth National Conference on ArtificialIntelligence.Rabiner, L. 1989. A tutorial on Hidden Markov Models and selected applicationsin speech recognition. Proceedings of the IEEE, 77, 257285.Ratsaby, J.,  Venkatesh, S. 1995. Learning from a mixture of labeled and unlabeled examples with parametric side information. Proceedings of the EighthAnnual Conference on Computational Learning Theory, 412417.Rattray, M. 2000. A modelbased distance for clustering. Proc. of InternationalJoint Conference on Neural Networks.Riloff, E., Wiebe, J.,  Wilson, T. 2003. Learning subjective nouns using extraction pattern bootstrapping. Proceedings of the Seventh Conference on NaturalLanguage Learning CoNLL2003.47Rosenberg, C., Hebert, M.,  Schneiderman, H. 2005. Semisupervised selftraining of object detection models. Seventh IEEE Workshop on Applications ofComputer Vision.Rosset, S., Zhu, J., Zou, H.,  Hastie, T. 2005. A method for inferring labelsampling mechanisms in semisupervised learning. In L. K. Saul, Y. Weiss andL. Bottou Eds., Advances in neural information processing systems 17. Cambridge, MA MIT Press.Roweis, S. T.,  Saul, L. K. 2000. Nonlinear dimensionality reduction by locallylinear embedding. Science, 290, 23232326.Sajama,  Orlitsky, A. 2005. Estimating and computing density based distancemetrics. ICML05, 22nd International Conference on Machine Learning. Bonn,Germany.Saul, L. K.,  Roweis, S. T. 2003. Think globally, fit locally unsupervisedlearning of low dimensional manifolds. Journal of Machine Learning Research,4, 119155.Schuurmans, D.,  Southey, F. 2001. Metricbased methods for adaptive modelselection and regularization. Machine Learning, Special Issue on New Methodsfor Model Selection and Model Combination, 48, 5184.Seeger, M. 2001. Learning with labeled and unlabeled data Technical Report.University of Edinburgh.Shahshahani, B.,  Landgrebe, D. 1994. The effect of unlabeled samples inreducing the small sample size problem and mitigating the Hughes phenomenon.IEEE Trans. On Geoscience and Remote Sensing, 32, 10871095.Shi, J.,  Malik, J. 2000. Normalized cuts and image segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 22, 888905.Sindhwani, V., Keerthi, S.,  Chapelle, O. 2006. Deterministic annealing forsemisupervised kernel machines. ICML06, 23rd International Conference onMachine Learning. Pittsburgh, USA.Sindhwani, V.,  Keerthi, S. S. 2006. Large scale semisupervised linear SVMs.SIGIR 2006.Sindhwani, V., Niyogi, P.,  Belkin, M. 2005a. Beyond the point cloud fromtransductive to semisupervised learning. ICML05, 22nd International Conference on Machine Learning.48Sindhwani, V., Niyogi, P.,  Belkin, M. 2005b. A coregularized approach tosemisupervised learning with multiple views. Proc. of the 22nd ICML Workshopon Learning with Multiple Views.Sindhwani, V., Niyogi, P., Belkin, M.,  Keerthi, S. 2005c. Linear manifoldregularization for large scale semisupervised learning. Proc. of the 22nd ICMLWorkshop on Learning with Partially Classified Training Data.Sinha, P., Balas, B., Ostrovsky, Y.,  Russell, R. 2006. Face recognition byhumans 20 results all computer vision researchers should know about. underreview.Smola, A.,  Kondor, R. 2003. Kernels and regularization on graphs. Conferenceon Learning Theory, COLTKW.Szummer, M.,  Jaakkola, T. 2001. Partially labeled classification with Markovrandom walks. Advances in Neural Information Processing Systems, 14.Szummer, M.,  Jaakkola, T. 2002. Information regularization with partiallylabeled data. Advances in Neural Information Processing Systems, 15.Taskar, B., Guestrin, C.,  Koller, D. 2003. Maxmargin Markov networks.NIPS03.Teh, Y. W.,  Roweis, S. 2002. Automatic alignment of local representations.Advances in NIPS.Tenenbaum, J. B., de Silva, V., ,  Langford, J. C. 2000. A global geometricframework for nonlinear dimensionality reduction. Science, 290, 23192323.Tipping, M. 1999. Deriving cluster analytic distance functions from Gaussianmixture models.Tsang, I.,  Kwok, J. 2006. Largescale sparsified manifold regularization. Advances in Neural Information Processing Systems NIPS 19.Vapnik, V. 1998. Statistical learning theory. Springer.von Luxburg, U., Belkin, M.,  Bousquet, O. 2004. Consistency of spectralclustering Technical Report TR134. Max Planck Institute for Biological Cybernetics.von Luxburg, U., Bousquet, O.,  Belkin, M. 2005. Limits of spectral clustering.In L. K. Saul, Y. Weiss and L. Bottou Eds., Advances in neural informationprocessing systems 17. Cambridge, MA MIT Press.49Wallis, G.,  Bulthoff, H. 2001. Effects of temporal association on recognitionmemory. Proceedings of the National Academy of Sciences, 98, 48004804.Wang, F.,  Zhang, C. 2006. Label propagation through linear neighborhoods.ICML06, 23rd International Conference on Machine Learning. Pittsburgh, USA.Weinberger, K. Q., Packer, B. D.,  Saul, L. K. 2005. Nonlinear dimensionality reduction by semidefinite programming and kernel matrix factorization.Proceedings of the Tenth International Workshop on Artificial Intelligence andStatistics AISTAT 2005.Weinberger, K. Q.,  Saul, L. K. 2004. Unsupervised learning of image manifolds by semidefinite programming. IEEE Conference on Computer Vision andPattern Recognition CVPR pp. 988995.Weinberger, K. Q., Sha, F.,  Saul, L. K. 2004. Learning a kernel matrix fornonlinear dimensionality reduction. Proceedings of ICML04 pp. 839846.Weiss, Y. 1999. Segmentation using eigenvectors A unifying view. ICCV 2pp. 975982.Weston, J., Collobert, R., Sinz, F., Bottou, L.,  Vapnik, V. 2006. Inference withthe universum. ICML06, 23rd International Conference on Machine Learning.Pittsburgh, USA.Weston, J., Leslie, C., Zhou, D., Elisseeff, A.,  Noble, W. S. 2004. Semisupervised protein classification using cluster kernels. In S. Thrun, L. Sauland B. Scholkopf Eds., Advances in neural information processing systems16. Cambridge, MA MIT Press.Xu, L.,  Schuurmans, D. 2005. Unsupervised and semisupervised multiclasssupport vector machines. AAAI05, The Twentieth National Conference on Artificial Intelligence.Yarowsky, D. 1995. Unsupervised word sense disambiguation rivaling supervised methods. Proceedings of the 33rd Annual Meeting of the Association forComputational Linguistics pp. 189196.Yianilos, P. 1995. Metric learning via normal mixtures Technical Report. NECResearch Institute.Yu, K., Tresp, V.,  Zhou, D. 2004. Semisupervised induction with basis functions Technical Report 141. Max Planck Institute for Biological Cybernetics,Tubingen, Germany.50Yu, K., Yu, S.,  Tresp, V. 2005. Blockwise supervised inference on large graphs.Proc. of the 22nd ICML Workshop on Learning with Partially Classified TrainingData. Bonn, Germany.Zelikovitz, S.,  Hirsh, H. 2001. Improving text classification with LSI usingbackground knowledge. IJCAI01 Workshop Notes on Text Learning BeyondSupervision.Zhang, T.,  Ando, R. 2006. Analysis of spectral kernel design based semisupervised learning. In Y. Weiss, B. Scholkopf and J. Platt Eds., Advances inneural information processing systems 18. Cambridge, MA MIT Press.Zhang, T.,  Oles, F. J. 2000. A probability analysis on the value of unlabeleddata for classification problems. Proc. 17th International Conf. on MachineLearning pp. 11911198. Morgan Kaufmann, San Francisco, CA.Zhang, X.,  Lee, W. S. 2006. Hyperparameter learning for graph based semisupervised learning algorithms. Advances in Neural Information Processing Systems NIPS 19.Zhou, D., Bousquet, O., Lal, T., Weston, J.,  Schlkopf, B. 2004a. Learningwith local and global consistency. Advances in Neural Information ProcessingSystem 16.Zhou, D., Huang, J.,  Schoelkopf, B. 2006. Learning with hypergraphs Clustering, classification, and embedding. Advances in Neural Information ProcessingSystems NIPS 19.Zhou, D., Huang, J.,  Scholkopf, B. 2005a. Learning from labeled and unlabeled data on a directed graph. ICML05, 22nd International Conference onMachine Learning. Bonn, Germany.Zhou, D., Scholkopf, B.,  Hofmann, T. 2005b. Semisupervised learning ondirected graphs. In L. K. Saul, Y. Weiss and L. Bottou Eds., Advances inneural information processing systems 17. Cambridge, MA MIT Press.Zhou, D., Weston, J., Gretton, A., Bousquet, O.,  Schlkopf, B. 2004b. Rankingon data manifolds. Advances in Neural Information Processing System 16.Zhou, Y.,  Goldman, S. 2004. Democratic colearing. Proceedings of the16th IEEE International Conference on Tools with Artificial Intelligence ICTAI2004.51Zhou, Z.H., Chen, K.J.,  Jiang, Y. 2004c. Exploiting unlabeled data incontentbased image retrieval. Proceedings of ECML04, 15th European Conference on Machine Learning. Italy.Zhou, Z.H.,  Li, M. 2005a. Semisupervised regression with cotraining. International Joint Conference on Artificial Intelligence IJCAI.Zhou, Z.H.,  Li, M. 2005b. Tritraining exploiting unlabeled data using threeclassifiers. IEEE Transactions on Knowledge and Data Engineering, 17, 15291541.Zhu, X. 2005. Semisupervised learning with graphs. Doctoral dissertation,Carnegie Mellon University. CMULTI05192.Zhu, X.,  Ghahramani, Z. 2002. Towards semisupervised classification withMarkov random fields Technical Report CMUCALD02106. Carnegie Mellon University.Zhu, X., Ghahramani, Z.,  Lafferty, J. 2003a. Semisupervised learning usingGaussian fields and harmonic functions. ICML03, 20th International Conference on Machine Learning.Zhu, X., Kandola, J., Ghahramani, Z.,  Lafferty, J. 2005. Nonparametric transforms of graph kernels for semisupervised learning. In L. K. Saul, Y. Weissand L. Bottou Eds., Advances in neural information processing systems 17.Cambridge, MA MIT Press.Zhu, X.,  Lafferty, J. 2005. Harmonic mixtures combining mixture modelsand graphbased methods for inductive and scalable semisupervised learning.ICML05, 22nd International Conference on Machine Learning.Zhu, X., Lafferty, J.,  Ghahramani, Z. 2003b. Combining active learning andsemisupervised learning using Gaussian fields and harmonic functions. ICML2003 workshop on The Continuum from Labeled to Unlabeled Data in MachineLearning and Data Mining.Zhu, X., Lafferty, J.,  Ghahramani, Z. 2003c. Semisupervised learning FromGaussian fields to Gaussian processes Technical Report CMUCS03175.Carnegie Mellon University.52
