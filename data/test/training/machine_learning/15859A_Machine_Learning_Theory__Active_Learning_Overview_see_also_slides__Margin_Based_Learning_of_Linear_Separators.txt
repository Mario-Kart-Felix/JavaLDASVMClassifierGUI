15859A Machine Learning Theory Active Learning Overview see also slides Margin Based Learning of Linear SeparatorsThere has recently been substantial interest in using unlabeled data together with labeled datafor machine learning. The motivation is that unlabeled data can often be much cheaper and moreplentiful than labeled data, and so if useful information can be extracted from it that reducesdependence on labeled examples, this can be a significant benefit.There are currently two settings that have been considered to incorporate unlabeled data in thelearning process. The first one is the socalled Semisupervised Learning 2, 4, where, in additionto a set of labeled examples drawn at random from the underlying data distribution, the learningalgorithm can also use a usually larger set of unlabeled examples from the same distribution. Inthis setting, unlabeled data becomes informative under additional assumptions and beliefs about thelearning problem. Examples of such assumptions are the one used by Transductive SVM namely,that the target function should cut through low density regions of the space, or by Cotrainingnamely, that the target should be selfconsistent in some way. Unlabeled data is then potentiallyuseful in this setting because it allows one to reduce search space from the whole set of hypotheses,down to the set of apriori reasonable with respect to the underlying distribution.The second setting, an increasingly popular one for the past few years, is Active Learning 1, 5, 7.Here, the learning algorithm has both the capability of drawing random unlabeled examples fromthe underlying distribution and that of asking for the labels of any of these examples, and thehope is that a good classifier can be learned with significantly fewer labels by actively directing thequeries to informative examples. As opposed to the Semisupervised learning setting, and similarlyto the classical supervised learning settings PAC and Statistical Learning Theory settings theonly prior belief about the learning problem in the Active Learning setting is that the targetfunction or a good approximation of it belongs to a given concept class. Luckily, it turns outthat for simple concept classes such as linear separators on the line one can achieve an exponentialimprovement over the usual supervised learning setting in the labeled data sample complexity,under no additional assumptions about the learning problem 1, 5.1 In general, however, for morecomplicated concept classes, the speedups achievable in the active learning setting depend on thematch between the distribution over examplelabel pairs and the hypothesis class, and therefore onthe target hypothesis in the class. Furthermore, there are simple examples where active learningdoes not help at all, even if there in the realizable case see, for example, 7. Recent work ofDasgupta 7 gives a generic characterization of the sample complexity aspect of active learning inthe realizable case.1 Margin Based Learning of Linear SeparatorsWe analyze here the Marginbased algorithm in Figure 1. A general version of the type of algorithmwe analyze here with extensions to certain types of noise appears in 3.1For this simple concept class one can achieve a pure exponential improvement 5 in the realizable case, while inthe agnostic case the improvement depends upon the noise rate 1.1We denote by df, g the probability that the two classifiers f and g predict differently on anexample coming at random from P . Furthermore, for   0, 1 we denote by B f,  the setg  df, g  .1.1 The Realizable Case and the Uniform DistributionsWe consider here a commonly studied setting in the active learning literature 6, 7, 8. Specifically,we assume that the data instances are drawn uniformly from the the unit ball in Rd, and that thelabels are consistent with a linear separator w going through the origin that is P w xy  0  0.We assume that w2  1. It is worth noting that even in this seemingly simple looking scenario,there exists an 1d log 1lower bound on the PAC learning sample complexity 9.Note that given our assumption about the data distribution the error rate of any given separatorw is errw  w,w , where w,w  arccosw  w.Input allowed error rate , sampling oracle for D, labeling oracle Oa sequence of sample sizes mk  0, k  Za sequence of cutoff values bk  0, k  Za sequence of hypothesis space radii rk  0, k  ZOutputclassifier ws of error at most First use Od examples to find an hypothesis w1 of error at most 18 .iterate k  1, . . . , sRejection sample mk samples x from D satisfying wTk  x  bkAsk for labels and find a separator wk1  B wk, rk consistent with all these examples.end iterateNote In at each iteration k, we can apply our favorite algorithm for finding a consistentlinear separator SVM for the realizable case, linear programming, etc.Figure 1 Marginbased Active LearningTheorem 1.1 For any ,   0, using Procedure 1 with bk  Olog 2k2kd, rk  12k and mk Oln 1d ln ln 1  d lnln 1, after s  log 1 iterations, we find a separator of error at most  withprobability 1 .Proof We prove by induction on k that at the kth iteration we have errwk  2k withprobability 1 k . For k  1, by standard VCbounds, we only need m1  Od ln1 examplesto obtain the desired result.Assume that wk has error at most . We are done if we can show that for our choice of bk theseparator wk1 we find in round k1 has error at most 2 with probability 1k , and we only needOln 1d ln ln 1  d lnln 1examples to transition between wk and wk1.Let us denote by errbk, wk,  the quantity supwBwk, Prw errs on x  wk  x  k.2Figure 2 If the angle between u and wk is at most , then at most a8 fraction of the region ofdisagreement between u and wk is outside a band of size  C sin log1dalong the hyperplanespecified by wk.The key point is that for the uniform distribution for bk as small asO log 1dwe have errbk, wk,  4  for a proof see Lemma 1.2. This then implies that in order to find wk1 that has error2 weonly need to sample and label about Oln 1d ln ln 1  d lnln 1examples from D satisfyingwTk1  x  bk. This follows from the fact that we can decomposeerrwk1 Prwk1 errs on x  wk  x  bk Prwk  x  bkPrwk1 errs on x  wk  x  bk Prwk  x  bk 1By our choice of bk we made sure that the first term Equation 1 is at most 4 , and we also knowfrom Lemma A.1 that Prwk x  bk  C log 1 . So, its enough to find wk1 with the propertythat Pr wk1 errs on x  wk  x  k  C2log 1. By standard VCbounds, we can do so by usingonly Oln 1d ln ln 1  d lnln 1examples.Note This algorithm is more aggressive than the version space based algorithms of 5 and 1.Indeed, we do not necessarily sample from the entire region of uncertainty  but we sample justfrom a subregion carefully chosen.Lemma 1.1 There exists C s.t.   C1, if  C sin log1dwe have wk, u  Bwk, Prxu  xwk  x  0, wk  x   8.Proof For simplicity, we present here a somewhat informal argument. For a formal proof see 3.3Let C be such that   C1 we havePrX  C log1  8,where X is a standard normal random variable. Let us fix   C1, and let  C sin log1d. Letsalso fix wk, and let u  Bwk, . So, du,wk   and therefore u,wk  arccosuwk    .Assume now without loosing generality that u  1, 0, 0, ..., 0 and wk cos, sin, 0, 0, ..., 0.So, u wk  cos and for x  x1, x2, ..., xd we have u  x  x1 and wk  x  cosx1  sinx2.It is enough to upper bound PrxSd wk  x    u  x  0 since the desired probability isPrxSdu  xwk  x  0, wk  x   2 PrxSdu  x  0, wk  x   PrxSdwk  x  u  x  0ButPrxSdwk  x  u  x  0  PrxSdcosx1  sinx2  x1  0 PrxSdsinx2  x1  0 PrxSdsinx2   PrxSdsinx2   .This impliesPrxSdwk  x  u  x  0  PrxSdsinx2  C sin log1d,which by our choice of C is at most 8 , as desired.2 For an illustration of this lemma see alsoFigure 1.1.Lemma 1.2 There exists C s.t.   C1, if  C sin log1dwe have wk, u  Bwk,  andwk1  Bwk, Prxu  xwk1  x  0, wk  x   4.Proof Let us fix   C1 and let  C sin2 log12d, where C is the constant specified inLemma 1.2. Lets also fix wk, and let u  Bwk,  and wk1  Bwk, .We have from Lemma 1.2bothPrxu  xwk  x  0, wk  x   8and2We have used the fact that for large d, x2 looks almost like a Gaussian with mean 0 and variance1d.4Prxu  xwk1  x  0, wk  x   8,and a simple union bound impliesPrxu  xwk1  x  0, wk  x   4.Finally note thatC sin2 log12d2C sin log1d, which clearly implies that the probabilityPrxu  xwk1  x  0, wk  x  C sin2 log12dis at least as large as the probabilityPrxu  xwk1  x  0, wk  x  2C sin log1d .This implies that its enough to choose C  2C.References1 M.F. Balcan, A. Beygelzimer, and J. Langford. Agnostic active learning. In ICML, 2006.2 M.F. Balcan and A. Blum. A PACstyle model for learning from labeled and unlabeled data.In Proceedings of the Annual Conference on Computational Learning Theory, 2005.3 M.F. Balcan, A. Broder, and T. Zhang. Margin based active learning. In Proceedings of theAnnual Conference on Computational Learning Theory, 2007.4 O. Chapelle, B. Scholkopf, and A. Zien, editors. SemiSupervised Learning. MIT Press, Cambridge, MA, 2006.5 D. Cohen, L. Atlas, and R. Ladner. Improving generalzation with active learning. 152201221,1994.6 S. Dasgupta. Analysis of a greedy active learning strategy. In Advances in Neural InformationProcessing Systems, 2004.7 S. Dasgupta. Coarse sample complexity bounds for active learning. In Advances in NeuralInformation Processing Systems, 2005.8 S. Dasgupta, A. Kalai, and C. Monteleoni. Analysis of perceptronbased active learning. InProceedings of the Annual Conference on Computational Learning Theory, 2005.9 P. M. Long. On the sample complexity of PAC learning halfspaces against the uniform distribution. IEEE Transactions on Neural Networks, 6615561559, 1995.5A Useful FactsA well known result concerning the behavior of the uniform distribution on the sphere is thefollowingLemma A.1 For any fixed unit vector w and any 0    1,4 Prxw  x  d ,where x is drawn uniformly from the unit sphere.We state now a useful property of the normal distribution.Lemma A.2 Let X be a standard normal random variable. Let t  PrX  t for t  R. Thent  12tet22 .Notice Lemma A.2 implies the followingLemma A.3 Let X be a standard normal random variable and let C1  116 . There exists C suchthat for all   C1 we havePrX  C log1  8.6
