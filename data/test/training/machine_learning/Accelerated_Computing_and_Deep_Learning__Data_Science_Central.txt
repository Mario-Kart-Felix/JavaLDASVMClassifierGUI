Accelerated Computing and Deep Learning - Data Science Central Search Sign Up Sign In Analytics Big Data Hadoop Data Plumbing DataViz Jobs Webinars Membership Previous Digests Search Classifieds Contact Subscribe to DSC Newsletter All Blog Posts My Blog Add Accelerated Computing and Deep Learning Posted by Vincent Granville on October 25, 2016 at 5:00pm View Blog Guest blog post by  Jen-Hsun Hunag , Founder, President and CEO at NVIDIA, Originally entitled "The Intelligent Industrial Revolution". A New Era of Computing Intelligent machines powered by AI computers that can learn, reason and interact with people are no longer science fiction. Today, a self-driving car powered by AI can meander through a country road at night and find its way. An AI-powered robot can learn motor skills through trial and error. This is truly an extraordinary time. In my three decades in the computer industry, none has held more potential, or been more fun. The era of AI has begun. Our industry drives large-scale industrial and societal change. As computing evolves, new companies form, new products are built, our lives change. Looking back at the past couple of waves of computing, each was underpinned by a revolutionary computing model, a new architecture that expanded both the capabilities and reach of computing. In 1995, the PC-Internet era was sparked by the convergence of low-cost microprocessors (CPUs), a standard operating system (Windows 95), and a new portal to a world of information (Yahoo!). The PC-Internet era brought the power of computing to about a billion people and realized Microsoft’s vision to put “a computer on every desk and in every home.” A decade later, the iPhone put “an Internet communications” device in our pockets. Coupled with the launch of Amazon’s AWS, the Mobile-Cloud era was born. A world of apps entered our daily lives and some 3 billion people enjoyed the freedom that mobile computing afforded. Today, we stand at the beginning of the next era, the AI computing era, ignited by a new computing model, GPU deep learning. This new model — where deep neural networks are trained to recognize patterns from massive amounts of data — has proven to be “unreasonably” effective at solving some of the most complex problems in computer science. In this era, software writes itself and machines learn. Soon, hundreds of billions of devices will be infused with intelligence. AI will revolutionize every industry. GPU Deep Learning “Big Bang” Why now? As I wrote in an earlier post (“ Accelerating AI with GPUs: A New Computing Model ”), 2012 was a landmark year for AI. Alex Krizhevsky of the University of Toronto created a deep neural network that automatically learned to recognize images from 1 million examples. With just several days of training on two NVIDIA GTX 580 GPUs, “AlexNet” won that year’s ImageNet competition, beating all the human expert algorithms that had been honed for decades. That same year, recognizing that the larger the network, or the bigger the brain, the more it can learn, Stanford’s Andrew Ng and NVIDIA Research teamed up to develop a method for training networks using large-scale GPU-computing systems. The world took notice. AI researchers everywhere turned to GPU deep learning. Baidu, Google, Facebook and Microsoft were the first companies to adopt it for pattern recognition. By 2015, they started to achieve “superhuman” results — a computer can now recognize images better than we can. In the area of speech recognition, Microsoft Research used GPU deep learning to achieve a  historic milestone  by reaching “human parity” in conversational speech. Image recognition and speech recognition — GPU deep learning has provided the foundation for machines to learn, perceive, reason and solve problems. The GPU started out as the engine for simulating human imagination, conjuring up the amazing virtual worlds of video games and Hollywood films. Now, NVIDIA’s GPU runs deep learning algorithms, simulating human intelligence, and acts as the brain of computers, robots and self-driving cars that can perceive and understand the world. Just as human imagination and intelligence are linked, computer graphics and artificial intelligence come together in our architecture. Two modes of the human brain, two modes of the GPU. This may explain why NVIDIA GPUs are used broadly for deep learning, and NVIDIA is increasingly known as “the AI computing company.” An End-to-End Platform for a New Computing Model As a new computing model, GPU deep learning is changing how software is developed and how it runs. In the past, software engineers crafted programs and meticulously coded algorithms. Now, algorithms learn from tons of real-world examples — software writes itself. Programming is about coding instruction. Deep learning is about creating and training neural networks. The network can then be deployed in a data center to infer, predict and classify from new data presented to it. Networks can also be deployed into intelligent devices like cameras, cars and robots to understand the world. With new experiences, new data is collected to further train and refine the neural network. Learnings from billions of devices make all the devices on the network more intelligent. Neural networks will reap the benefits of both the exponential advance of GPU processing and large network effects — that is, they will get smarter at a pace way faster than Moore’s Law. Whereas the old computing model is “instruction processing” intensive, this new computing model requires massive “data processing.” To advance every aspect of AI, we’re building an end-to-end AI computing platform — one architecture that spans training, inference and the billions of intelligent devices that are coming our way. Let’s start with training. Our new Pascal GPU is a $2 billion investment and the work of several thousand engineers over three years. It is the first GPU optimized for deep learning. Pascal can train networks that are 65 times larger or faster than the Kepler GPU that Alex Krizhevsky used in his paper. (1)  A single computer of eight Pascal GPUs connected by NVIDIA NVLink the highest throughput interconnect ever created, can train a network faster than 250 traditional servers. Soon, the tens of billions of internet queries made each day will require AI, which means that each query will require billions more math operations. The total load on cloud services will be enormous to ensure real-time responsiveness.  Read more  here .  About the Author Jen-Hsun Huang founded NVIDIA in 1993 and has served since its inception as president, chief executive officer and a member of the board of directors. Huang is a recipient of the Dr. Morris Chang Exemplary Leadership Award from the Global Semiconductor Association in recognition of his exceptional contributions to driving the development, innovation, growth and long-term opportunities of the fabless semiconductor industry. He has received the Daniel J. Epstein Engineering Management Award from the University of Southern California, and the EB Lemon Distinguished Alumni Award and an honorary doctorate from Oregon State University. He was named to the U.S. Immigrant Entrepreneur Hall of Fame when it was established in 2012. In 2016, Harvard Business Review ranked him No. 6 on its list of the world’s 100 best-performing CEOs, and No. 1 in the U.S., over the lifetime of their tenure. Prior to founding NVIDIA, Huang worked at LSI Logic and Advanced Micro Devices. He holds a BSEE degree from Oregon State University and an MSEE degree from Stanford University. Views: 2919 Tags: Like 2 members like this Share Tweet Facebook < Previous Post Next Post > Comment You need to be a member of Data Science Central to add comments! Join Data Science Central RSS Welcome to Data Science Central Sign Up or Sign In Or sign in with: Follow Us @DataScienceCtrl  |   RSS Feeds Top Content   Edit 1 Data Science Summarized in One Picture 2 Data Science, Machine Learning, BI Explained in a Amazing Few Pictures 3 The Mathematics of Machine Learning 4 How to Become a Data Scientist - On your own 5 Executive Guide to Artificial Intelligence 6 Weekly Digest, February 27 RSS View All Announcements The mecca of all things data Faster and Scalable Analytical Processing Gartner Magic Quadrant Report - Qlik a BI Leader for 7th year in a row Check Out TDWI’s April Data and Analytics Event and Save 30% Doctoral Program in Decision Sciences Data Science: Cloud Trends for 2017 Get a Guaranteed Data Science Job 10 reasons to choose Villanova's online MSA Improve Your Regression with CART and Gradient Boosting Top 10 Big Data Trends for 2017 Videos How to Keep Your R Code Simple While Tackling Big Datasets Added by Tim Matteson 1 Comment 2 Likes A Natural Language Processing (NLP) Approach to Data Exploration Added by Tim Matteson 1 Comment 1 Like Add Videos View All Resources Top Categories Machine Learning   R Programming     Python for Data Science     Visualization, Dashboards   NoSQL and NewSQL   Big Data   Cheat Sheets Internet of Things   Excel   © 2017   Data Science Central   Powered by Badges  |  Report an Issue  |  Privacy Policy  |  Terms of Service Hello, you need to enable JavaScript to use Data Science Central. Please check your browser settings or contact your system administrator.
