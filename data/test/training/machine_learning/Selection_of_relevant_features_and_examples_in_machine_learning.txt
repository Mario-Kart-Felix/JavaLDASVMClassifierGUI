Selection of Relevant Features andExamples in Machine LearningAvrim L Blum AvrimcscmueduSchool of Computer Science Carnegie Mellon UniversityPittsburgh Pennsylvania Pat Langley LangleyisleorgInstitute for the Study of Learning and Expertise Staunton Court Palo Alto California AbstractIn this survey we review work in machine learning on methods for handling data sets containing largeamounts of irrelevant information We focus on two key issues the problem of selecting relevant featuresand the problem of selecting relevant examples We describe the advances that have been made on thesetopics in both empirical and theoretical work in machine learning and we present a general framework thatwe use to compare dierent methods We close with some challenges for future work in this areaTo appear in the special issue of Articial Intelligence on Relevance R Greiner and D Subramanian Eds Also aliated with the Intelligent Systems Laboratory DaimlerBenz Research and Technology Center Page Mill Road Palo Alto CA Selecting Relevant Features and Examples Page  IntroductionAs Machine Learning aims to address larger more complex tasks the problem of focusing on the mostrelevant information in a potentially overwhelming quantity of data has become increasingly important Forinstance data mining of corporate or scientic records often involves dealing with both many features andmany examples and the internet and World Wide Web have put a huge volume of lowquality informationat the easy access of a learning system Similar issues arise in the personalization of ltering systems forinformation retrieval electronic mail netnews and the likeIn this paper we address two specic aspects of this focusing task that have received signicant attentionin the AI literature the problem of focusing on the most relevant features for use in representing the dataand the problem of selecting the most relevant examples to drive the learning process We review recent workon these topics presenting general frameworks that we use to compare and contrast dierent approachesWe begin with the problem of focusing on relevant features In Section  we present and relate severalimportant notions of relevance for this task and describe some general goals of feature selection algorithmsWe report on methods that have been developed for this problem characterizing them as embedded lteror wrapper approaches and we compare explicit feature selection techniques to those based on weightingschemes We then turn in Section  to the problem of focusing on relevant examples describing methodsfor ltering both labeled and unlabeled data We conclude in Section  with open problems and challengesfor future work on both the empirical and theoretical frontsBefore proceeding we should clarify the scope of our survey which focuses on methods and results fromcomputational learning theory and experimental machine learning There has been substantial work onfeature selection in other elds such as pattern recognition and statistics and on data selection in elds suchas statistics information theory and the philosophy of science Although we do not have the space to coverthe work in these areas readers should be aware that there are many similarities to the approaches that wewill discuss The Problem of Irrelevant FeaturesAt a conceptual level one can divide the task of concept learning into two subtasks deciding which featuresto use in describing the concept and deciding how to combine those features In this view the selection ofrelevant features and the elimination of irrelevant ones is one of the central problems in machine learningand many induction algorithms incorporate some approach to addressing itAt a practical level we would like induction algorithms that scale well to domains with many irrelevantfeatures More specically as one goal we would like the number of training examples needed to reach adesired level of accuracy often called the sample complexity to grow slowly with the number of featurespresent if indeed not all these are needed to achieve good performance For instance it is not uncommon ina text classication task to represent examples using  to  attributes with the expectation that only asmall fraction of these are crucial Lewis a Lewis b In recent years a growing amount of workin machine learning  both experimental and theoretical in nature  has focused on developing algorithmswith such desirable propertiesInduction algorithms dier considerably in their emphasis on focusing on relevant features At one extremelies the simple nearest neighbor method which classies test instances by retrieving the nearest storedtraining example using all available attributes in its distance computations Although Cover and Hart showed that this approach has excellent asymptotic accuracy a little thought reveals that the presence ofirrelevant attributes should considerably slow the rate of learning In fact Langley and Ibas  averagecase analysis of simple nearest neighbor indicates that number of training examples needed to reach aPage  Selecting Relevant Features and Examplesgiven accuracy similar to the PAC notion of sample complexity grows exponentially with the number ofirrelevant attributes even for conjunctive target concepts Experimental studies of nearest neighbor Aha Langley  Sage  are consistent with this discouraging conclusionAt the other extreme lie induction methods that explicitly attempt to select relevant features and rejectirrelevant ones Techniques for learning logical descriptions constitute the simplest example of this approachand there are more sophisticated methods for identifying relevant attributes that can augment and improveany induction method including nearest neighbor Theoretical and experimental results for these methodsare much more encouraging For instance theoretical results show that if by focusing on only a smallsubset of features an algorithm can signicantly reduce the number of hypotheses under consideration thenthere is a corresponding reduction in the sample size sucient to guarantee good generalization Blumeret al  Somewhat in the middle of the above two extremes are featureweighting methods that do notexplicitly select subsets of features but still aim to achieve good scaling behaviorWe structure the remainder of this section as follows We begin by describing several important formalnotions of relevance in the context of supervised learning In addition to introducing terminology thesedenitions help to illustrate some of the general goals of feature selection algorithms We then turn todiscussing some of the methods that have been developed for this problem characterizing them as eitherembedded lter or wrapper approaches based on the relation between the selection scheme and the basicinduction algorithm This decomposition in part reects historical trends but it also helps for comparingapproaches that may seem to be very dierent but can be seen to belong to the same category and thereforein certain ways have similar motivations We also compare explicit feature selection techniques to thosebased on weighting schemes which tackle the same problem from a somewhat dierent perspective Denitions of RelevanceThere are a number of dierent denitions in the machine learning literature for what it means for featuresto be relevant The reason for this variety is that it generally depends on the question relevant to whatMore to the point dierent denitions may be more appropriate depending on ones goals Here we describeseveral important denitions of relevance and discuss their signicance In doing so we hope to illustratesome of the issues involved and some of the variety of motivations and approaches taken in the literatureFor concreteness let us consider a setting in which there are n features or attributes used to describeexamples and each feature i has some domain Fi For instance a feature may be Boolean is reddiscrete with multiple values what color or continuous what wavelength An example is a point inthe instance space F  F     Fn The learning algorithm is given a set S of training data where eachdata point is an example paired with an associated label or classication which might also be Booleanmultiple valued or continuousAlthough the learning algorithm sees only the xed sample S it is often helpful to postulate two additionalquantities as is done in the PAC learning model eg see Kearns  Vazirani  a probability distributionD over the instance space and a target function c from examples to labels We then model the sample Sas having been produced by repeatedly selecting examples from D and then labeling them according to thefunction c The target function c may be deterministic or probabilistic in the latter case for some exampleA cA would be a probability distribution over labels rather than just a single label Note that we can usethe distribution D to model integrity constraints in the data For instance suppose we are representing adecimal digit by nine boolean features such that feature i is  if the digit is greater than or equal to i Wecan model this by having D assign examples such as  the probability zero even though the targetfunction c is still dened on such examplesSelecting Relevant Features and Examples Page Given this setup perhaps the simplest notion of relevance is a notion of being relevant to the targetconceptDenition  Relevant to the target A feature xi is relevant to a target concept c if there exists a pairof examples A and B in the instance space such that A and B dier only in their assignment to xi andcA  cBAnother way of stating this denition is that feature xi is relevant if there exists some example in the instancespace for which twiddling the value of xi aects the classication given by the target conceptNotice that this notion has the drawback that the learning algorithm given access to only the sampleS cannot necessarily determine whether or not some feature xi is relevant Even worse if the encoding offeatures is redundant say every feature is repeated twice it may not even be possible to see two examplesthat dier in only one feature since at least one of those examples would have probability zero under D Onthe other hand this is often the denition of choice for theoretical analyses of learning algorithms wherethe notion of relevance is used to prove some convergence properties of an algorithm rather than in thealgorithm itself The denition also is useful in situations where the target function c is a real object thatthe learning algorithm can actively query at inputs of its own choosing eg if the learning algorithm istrying to reverse engineer some piece of hardware rather than just a convenient ctionTo remedy some of the drawbacks of the above denition John Kohavi and Peger  dene two notions of what might be termed relevance with respect to a distribution which also has a nice interpretationas a notion of relevance with respect to a sampleDenition  Strongly Relevant to the sampledistribution A feature xi is strongly relevant to sample S if there exist examples A and B in S that dier only in their assignment to xi and have dierent labelsor have dierent distributions of labels if they appear in S multiple times Similarly xi is strongly relevantto target c and distribution D if there exist examples A and B having nonzero probability over D that dieronly in their assignment to xi and satisfy cA  cBIn other words this is just like Denition  except A and B are now required to be in S or have nonzeroprobabilityDenition  Weakly Relevant to the sampledistribution A feature xi is weakly relevant to sample S or to target c and distribution D if it is possible to remove a subset of the features so that xi becomesstrongly relevantThese notions of relevance are useful from the viewpoint of a learning algorithm attempting to decide whichfeatures to keep and which to ignore Features that are strongly relevant are generally important to keep nomatter what at least in the sense that removing a strongly relevant feature adds ambiguity to the sampleFeatures that are weakly relevant may or may not be important to keep depending on which other featuresare ignored In practice one may wish to adjust these denitions to account for statistical variations Forinstance a special case of Denition  is that feature xi is weakly relevant if it is correlated with the targetfunction ie xi is strongly relevant when all other features are removed so given a nite sample one wouldwant to account for variance and statistical signicanceIn a somewhat dierent vein than the above denitions in many cases rather than caring about exactlywhich features are relevant we simply want to use relevance as a measure of complexity That is we want touse relevance to say how complicated a function is and rather than requiring our algorithm to explicitlyselect a subset of features we just want it to perform well when this quantity is low For this purposeanother notion of relevance as a complexity measure with respect to a sample of data S and a set of conceptsC is usefulPage  Selecting Relevant Features and ExamplesDenition  Relevance as a complexity measure Given a sample of data S and a set of conceptsC let rSC be the number of features relevant using Denition  to a concept in C that out of all thosewhose error over S is least has the fewest relevant featuresIn other words we are asking for the smallest number of features needed to achieve optimal performance overS via a concept in C The reason for specifying the concept class C is that there may be a feature such as apersons socialsecurity number that is highly relevant from the point of view of the information containedbut that is useless with respect to the sorts of concepts under consideration For additional robustness thisdenition is sometimes modied to allow concepts in C with nearly minimal error over S if this producesa smaller relevant setThe above notions of relevance are independent of the specic learning algorithm being used There isno guarantee that just because a feature is relevant it will necessarily be useful to an algorithm or viceversa Caruana and Freitag b make this explicit with a notion of what we might term incrementalusefulness and which they simply call usefulnessDenition  Incremental usefulness Given a sample of data S a learning algorithm L and a featureset A feature xi is incrementally useful to L with respect to A if the accuracy of the hypothesis that Lproduces using the feature set fxig  A is better than the accuracy achieved using just the feature set AThis notion is especially natural for featureselection algorithms that search the space of feature subsets byincrementally adding or removing features to their current set  for instance many that follow the generalframework described in Section  belowTo make these denitions more clear consider concepts that can be expressed as disjunctions of featureseg x  x  x and suppose that the learning algorithm sees these ve examples     The relevant features using Denition  would depend on the true target concept though any consistenttarget disjunction c must include the rst feature Using Denitions  and  we would say that x isstrongly relevant and the rest are weakly relevant note that x is weakly relevant because it can be madestrongly relevant by removing x and x     x Using Denition  we would say simply that there arethree relevant features rSC   since this is the number of features relevant to the smallest consistentdisjunction The notion of incremental usefulness in Denition  depends on the learning algorithm butpresumably given the feature set f g the third feature would not be useful but any of features x to xwould be We will revisit the question of how Denition  is related to the others at the end of Section when we discuss a simple specic algorithmThere are a variety of natural extensions one can make to the above denitions For instance one canconsider relevant linear combinations of features rather than just relevant individual features In thiscase in analogy to Denition  above one could ask What is the lowestdimensional space such thatprojecting all the examples in S onto that space preserves the existence of a good function in the classC This notion of relevance is often most natural for statistical approaches to learning Indeed methodssuch as principal component analysis Jollie  are commonly used as heuristics for nding these lowdimensional subspacesSelecting Relevant Features and Examples Page Figure  Each state in the space of feature subsets species the attributes to use during induction Note that thestates in the space in this case involving four features are partially ordered with each of a states childrento the right including one more attribute dark circles than its parents Feature Selection as Heuristic SearchWe now turn to discussing feature selection algorithms and more generally algorithms for dealing withdata sets that contain large numbers of irrelevant attributes A convenient paradigm for viewing many ofthese approaches especially those that perform explicit feature selection is that of heuristic search witheach state in the search space specifying a subset of the possible features According to this view we cancharacterize any feature selection method in terms of its stance on four basic issues that determine the natureof the heuristic search processFirst one must determine the starting point or points in the space which in turn inuences the directionof search and the operators used to generate successor states As Figure  depicts there is a natural partialordering on this space with each child having exactly one more feature than its parents This suggests thatone might start with nothing and successively add attributes or one might start with all attributes andsuccessively remove them The former approach is sometimes called forward selection whereas the latter isknown as backward elimination One can also use variations on this partial ordering Devijver and Kittler report an operator that adds k features and takes one away and genetic operators like crossoverproduce somewhat dierent types of connectivityA second decision involves the organization of the search Clearly an exhaustive search of the space isimpractical as there exist a possible subsets of a attributes A more realistic approach relies on a greedymethod to traverse the space At each point in the search one considers local changes to the current setof attributes selects one and then iterates For instance the hillclimbing approach known as stepwiseselection or elimination considers both adding and removing features at each decision point which lets oneretract an earlier decision without keeping explicit track of the search path Within these options one canconsider all states generated by the operators and then select the best or one can simply choose the rst statethat improves accuracy over the current set One can also replace the greedy scheme with more sophisticatedmethods such as bestrst search which are more expensive but still tractable in some domainsA third issue concerns the strategy used to evaluate alternative subsets of attributes One commonlyused metric involves an attributes ability to discriminate among classes that occur in the training dataPage  Selecting Relevant Features and ExamplesMany induction algorithms incorporate a criterion based on information theory but others directly measureaccuracy on the training set or on a separate evaluation set A broader issue concerns how the featureselection strategy interacts with the basic induction algorithm as we discuss shortly in more detailFinally one must decide on some criterion for halting the search For example one might stop adding orremoving attributes when none of the alternatives improves the estimate of classication accuracy one mightcontinue to revise the feature set as long as accuracy does not degrade or one might continue generatingcandidate sets until reaching the other end of the search space and then select the best One simple haltingcriterion is to stop when each combination of values for the selected attributes maps onto a single class valuebut this assumes noisefree training data A more robust alternative simply orders the features according tosome relevancy score then uses a system parameter to determine the break pointNote that the above design decisions must be made for any induction algorithm that carries out featureselection Thus they provide useful dimensions for describing the techniques developed to address thisproblem and we will refer to them repeatedlyTo make this more concrete let us revisit the scenario given at the end of Section  we are consideringconcepts expressible as a disjunction of Boolean features with a simple strategy known as the greedy setcoveralgorithmBegin with a disjunction of zero features which by convention outputs negative on every exampleThen out of those features not present in any negative example and thus are safe to add into thehypothesis choose the one whose inclusion into the current hypothesis most increases the number ofcorrectly classied positive examples breaking ties arbitrarily Repeat until there are no more safefeatures that would increase the number of correctly classied positives and then haltWith respect to our framework this algorithm begins at the leftmost point in Figure  incrementallymoves rightward only evaluates subsets based on performance on the training set with an innite penaltyfor misclassifying negative examples and halts when it can take no further step that strictly improves itsevaluated performanceGiven the ve data points listed at the end of Section  this algorithm would rst put in x thenperhaps x then perhaps x and then would halt It is not hard to see that if there exists a disjunctionconsistent with the training set then this method will nd one In fact the number of features selectedby this method is at most Olog jSj times larger than the number of relevant features using Denition Johnson  Haussler We can also use this algorithm to illustrate relationships between some of the denitions in the previoussection For instance the incrementally useful features for this algorithm Denition  will also be weaklyrelevant Denition  but the converse is not necessarily true In fact if the data is not consistent with anydisjunction then even strongly relevant features Denition  may be ignored by the algorithm due to thealgorithms conservative nature it ignores any feature that may cause it to misclassify a negative exampleOn the other hand if the data is consistent with some disjunction then all strongly relevant features areincrementally useful and all will eventually be placed in the algorithms hypothesis though the algorithmmay prefer a weakly relevant feature to a strongly relevant one due to its evaluation criterionWe now review some specic feature selection methods which we have grouped into three classes thosethat embed the selection within the basic induction algorithm those that use feature selection to lter featurespassed to induction and those that treat feature selection as a wrapper around the induction process This is not too hard to see and follows from the fact that there must always exist some feature to add that captures atleast a rSC fraction of the stillmisclassied positive examples In the other direction nding the smallest disjunctionconsistent with a given set of data is NPhard Garey  Johnson  a polynomialtime algorithm to nd disjunctionsonly c log n times larger than the smallest for c   would place NP into quasipolynomial time Lund  YannakakisSelecting Relevant Features and Examples Page  Embedded Approaches to Feature SelectionMethods for inducing logical descriptions provide the clearest example of feature selection methods embeddedwithin a basic induction algorithm In fact many algorithms for inducing logical conjunctions eg Mitchell Vere  Winston  and the greedy setcover algorithm given above do little more than addor remove features from the concept description in response to prediction errors on new instances For thesemethods the partial ordering in Figure  also describes the space of hypotheses and the algorithms typicallyuse this ordering to organize their search for concept descriptionsTheoretical results for learning pure conjunctive or pure disjunctive concepts are encouraging As mentioned above the greedy setcover approach nds a hypothesis at most a logarithmic factor larger than thesmallest possible In fact Warmuth personal communication notes that one can achieve slightly betterbounds in the PAC setting by halting earlier so that some training examples are misclassied Because theresulting hypothesis is guaranteed to be fairly small the sample complexity grows only logarithmically withthe number of irrelevant features These results apply directly to other settings in which the target conceptcan be characterized as a conjunction or disjunction of a list of functions produced by the induction algorithm Situations of this form include learning intersections of halfspaces in constantdimensional spacesBlumer et al  and algorithms for learning DNF formulas in nOlogn time under the uniform distribution Verbeurgt  The above results for the greedy setcover method are distribution free and worstcase but Pazzani and Sarrett  report an averagecase analysis of even simpler methods for conjunctivelearning that imply logarithmic growth for certain product distributionsSimilar operations for adding and removing features form the core of methods for inducing more complexlogical concepts but these methods also involve routines for combining features into richer descriptionsFor example recursive partitioning methods for induction such as Quinlans ID  and C and CART Breiman et al  carry out a greedy search through the space of decision trees at eachstage using an evaluation function to select the attribute that has the best ability to discriminate amongthe classes They partition the training data based on this attribute and repeat the process on each subsetextending the tree downward until no further discrimination is possibleDhagat and Hellerstein  have also extended techniques for greedy set cover in a recursive fashionto apply to more complex functions such as kterm DNF formulas and kalternation decision lists Blum describes methods that can be used even when the set of all attributes is unbounded so long as eachindividual example satises a reasonably small number of them this is often a good model when dealingwith text documents for instance that may each contain only a small number of the possible words inthe dictionary For all these cases the featureselection process is clearly embedded within another morecomplex algorithmSeparateandconquer methods for learning decision lists Michalski  Clark  Niblett  Pagallo Haussler  embed feature selection in a similar manner These techniques use an evaluation functionto select a feature that helps distinguish a class C from others then add the resulting test to a singleconjunctive rule for C They repeat this process until the rule excludes all members of other classes thenremove the members of C that the rule covers and repeat the process on the remaining training casesClearly both partitioning and separateandconquer methods explicitly select features for inclusion in abranch or rule in preference to other features that appear less relevant or irrelevant For this reason onemight expect them to scale well to domains that involve many irrelevant features Although few theoreticalresults exist for these methods experimental studies by Langley and Sage  suggest that decisiontreemethods scale linearly with the number of irrelevant features for certain target concepts such as logicalconjunctions However the same studies also show that for other targets concepts they exhibit the samePage  Selecting Relevant Features and Examplesexponential growth as does nearest neighbor Experiments by Almuallim and Dietterich  and by Kiraand Rendell  also show substantial decreases in accuracy for a given sample size when irrelevantfeatures are introduced into selected Boolean target conceptsThe standard explanation of this eect involves the reliance of such algorithms on greedy selection ofattributes to discriminate among classes This approach works well in domains where there is little interactionamong the relevant attributes as in conjunctive concepts However the presence of attribute interactionswhich can lead a relevant feature in isolation to look no more discriminating than an irrelevant one cancause signicant problems for this scheme Parity concepts constitute the most extreme example of thissituation but it also arises with other target conceptsSome researchers have attempted to remedy these problems by replacing greedy search with lookaheadtechniques eg Norton  with some success Of course more extensive search carries with it asignicant increase in computational cost Others have responded by selectively dening new features ascombinations of existing ones so as to make greedy search more powerful by letting it take larger steps egMatheus  Rendell  Pagallo  Haussler  However neither approach has been directly evaluatedin terms of its ability to handle large numbers of irrelevant features either experimentally or theoretically Filter Approaches to Feature SelectionA second general approach to feature selection introduces a separate process for this purpose that occursbefore the basic induction step For this reason John Kohavi and Peger  have termed them ltermethods because they lter out irrelevant attributes before induction occurs The preprocessing step usesgeneral characteristics of the training set to select some features and exclude others Thus ltering methodsare independent of the induction algorithm that will use their output and they can be combined with anysuch methodPerhaps the simplest ltering scheme is to evaluate each feature individually based on its correlation withthe target function eg using a mutual information measure and then to select the k features with thehighest value The best choice of k can then be determined by testing on a holdout set This method iscommonly used in text categorization tasks Lewis a Lewis b often in combination with eithera naive Bayes or a nearest neighbor classication scheme and has achieved good empirical successKira and Rendells Relief algorithm follows this general paradigm but incorporates a more complexfeatureevaluation function Their system then uses ID to induce a decision tree from the training datausing only the selected features Kononenko  reports two extensions to this method that handle moregeneral types of featuresAlmuallim and Dietterich  describe a ltering approach to feature selection that involves a greaterdegree of search through the feature space Their Focus algorithm looks for minimal combinations ofattributes that perfectly discriminate among the classes This method begins by looking at each featurein isolation then turns to pairs of features triples and so forth halting only when it nds a combinationthat generates pure partitions of the training set ie in which no instances have dierent classes Focusthen passes on the original training examples described using only the selected features to an algorithm fordecisiontree inductionComparative studies with a regular decisiontree method showed that for a given number of training caseson random Boolean target concepts Focuswas almost unaected by the introduction of irrelevant attributeswhereas decisiontree accuracy degraded signicantly Schlimmer  describes a related method that Note that this problem does not disappear with increasing sample size Embedded selection methods that rely on greedysearch cannot distinguish between relevant and irrelevant features early in the search process even when the entire instancespace is availableSelecting Relevant Features and Examples Page Table  Characterization of recent work on lter approaches to feature selection in terms of heuristic search throughthe space of feature setsAuthors System Starting Search Halting InductionPoint Control Criterion AlgorithmAlmuallim Focus None Breadth First Consistency Dec TreeCardie None Greedy Consistency Near NeighKollerSahami All Greedy Threshold TreeBayesKiraRendell Relief  Ordering Threshold Dec TreeKubat et al None Greedy Consistency Naive BayesSchlimmer None Systematic Consistency NoneSinghProvan None Greedy No info gain Bayes Netcarries out systematic search to avoid revisiting states through the space of feature sets again startingwith the empty set and adding features until it nds a combination consistent with the training dataAlthough Focus and Relief follow feature selection with decisiontree construction one can of course useother induction methods For instance Cardie  uses ltering as a preprocessor for nearest neighbor retrieval and Kubat Flotzinger and Pfurtscheller  lter features for use with a naive Bayesian classierInterestingly both used a decisiontree method that relies on an embedded selection scheme as the lter toproduce a reduced set of attributes More recently Singh and Provan  have used informationtheoreticmetrics to lter features for inclusion in a Bayesian network while Koller and Sahami  have employeda crossentropy measure designed to nd Markov blankets of features for use in both naive Bayes anddecisiontree induction In a somewhat dierent vein Greiner Grove and Kogan in this issue considersettings where a helpful tutor lters out conditionally irrelevent attributesTable  characterizes the recent work on lter methods in terms of the dimensions described earlierin the section along with the induction algorithm that takes advantage of the reduced feature set Thetypical results show some improvement over embedded selection methods Most experiments have focusedon natural domains that contain an unknown number of irrelevant features but a few researchers Almuallim Dietterich  Kira  Rendell  have studied experimentally the eect of introducing such featuresAnother class of lter methods actually constructs higherorder features from the original ones ordersthem in terms of the variance they explain and selects the best such features The statistical techniqueof principal components analysis Jollie  the bestknown example of this approach generates linearcombinations of features whose vectors are orthogonal in the original space Empirically principal components has successfully reduced dimensionality on a variety of learning tasks Blum and Kannan describe theoretical guarantees for methods of this form when the target function is an intersection ofhalfspaces and the examples are chosen from a suciently benign distribution The related method of independent component analysis Comon  incorporates similar ideas but insists only that the new featuresbe independent rather than orthogonal Wrapper Approaches to Feature SelectionA third generic approach for feature selection also occurs outside the basic induction method but uses thatmethod as a subroutine rather than as a postprocessor For this reason John et al  refer to these aswrapper approaches see also the paper by Kohavi and John in this issue The typical wrapper algorithmsearches the same space of feature subsets see Figure  as embedded and lter methods but it evaluatesPage  Selecting Relevant Features and Examplesalternative sets by running some induction algorithm on the training data and using the estimated accuracyof the resulting classier as its metric Actually the wrapper scheme has a long history within the literatureon statistics and pattern recognition eg Devijver  Kittler  where the problem of feature selectionhas long been an active research topic but its use within machine learning is relatively recentThe general argument for wrapper approaches is that the induction method that will use the featuresubset should provide a better estimate of accuracy than a separate measure that may have an entirelydierent inductive bias For example both Doak  and John et al  argue in favor of using awrapper method to improve the behavior of decisiontree induction Doak reports experimental comparisonsof forward selection and backward elimination as well as the impact of dierent searchcontrol techniquesJohn et al present similar comparative studies including the eect of using wrappers versus lters Caruanaand Freitag a report a third set of empirical studies also focusing on decision trees that explorevariations on wrapper methodsThe major disadvantage of wrapper methods over lter methods is the formers computational cost whichresults from calling the induction algorithm for each feature set considered This cost has led some researchersto invent ingenious techniques for speeding the evaluation process In particular Caruana and Freitagdescribe a scheme for caching decision trees that lets their algorithms search larger spaces in reasonabletime Moore and Lee  describe an alternative scheme that instead speeds feature selection by reducingthe percentage of training cases used during evaluationCertainly not all work within the wrapper framework has focused on decisiontree induction Indeed onemight expect methods like nearestneighbor which by default take into account all attributes would benetmore from featureselection wrappers than algorithms that themselves incorporate embedded schemes Thisexpectation has led to a substantial body of work on wrapper methods for nearestneighbor and casebasedlearningLet us consider one such approach and its behavior in some detail Langley and Sages a Oblivion algorithm combines the wrapper idea with the simple nearest neighbor method which assigns to newinstances the class of the nearest case stored in memory during learning The featureselection process effectively alters the distance metric used in these decisions taking into account the features judged relevantand ignoring the othersOblivion carries out a backward elimination search through the space of feature sets starting with allfeatures and iteratively removing the one that leads to the greatest improvement in estimated accuracy Thesystem continues this process until the estimated accuracy actually declines We characterize Oblivion asusing a wrapper method because its evaluation metric involves running nearest neighbor itself on the trainingdata to measure the accuracy with alternative feature sets In particular the system uses leaveoneout crossvalidation to estimate the accuracy of each feature set on novel test casesAlthough this approach may seem computationally expensive Oblivion uses an insight from Moore andLee  to make it tractable The leaveoneout technique estimates accuracy on N training cases byholding out each case in turn constructing a classier based on the remaining N   cases seeing whetherthe classier correctly predicts the case and averaging the results over all N cases Because nearest neighborsimply stores the training cases in memory one can implement leave one out by successively removing eachcase and using the remaining ones to classify it This scheme is no more expensive than estimating accuracyon the training set itself One natural metric involves running the induction algorithm over the entire training data using a given set of featuresthen measuring the accuracy of the learned structure on the training data However John et al argue convincingly that acrossvalidation method provides a better measure of expected accuracy on novel test cases Kohavi  has incorporated the same idea into his technique for inducing decision tables which has many similaritiesto OblivionSelecting Relevant Features and Examples Page Table  Characterization of recent work on wrapper approaches to feature selection in terms of heuristic searchthrough the space of feature setsAuthors System Starting Search Halting InductionPoint Control Criterion AlgorithmAhaBankert Beam Random Comparison No Better Near NeighCaruanaFreitag CAP Comparison Greedy All Used Dec TreeDoak Comparison Comparison Not Enough Better TreeBayesJohnKohaviPfleger Comparison Greedy No Better Dec TreeLangleySage Oblivion All Greedy Worse Near NeighLangleySage Sel Bayes None Greedy Worse Naive BayesMooreLee Race Comparison Greedy No Better Near NeighSinghProvan KAS None Greedy Worse Bayes NetSkalak Random Mutation Enough Times Near NeighTownsendWeberKibler All Comparison No Better Near NeighLangley and Sage designed a number of experiments to evaluate their system Results with syntheticdomains suggest that when some features are irrelevant Oblivion learns highaccuracy classiers frommany fewer instances than simple nearest neighbor However they also found that this eect was absentfrom many of the UCI data sets suggesting that Holtes  nding about the accuracy of oneleveldecision trees was due to highly correlated features which cause no diculty for nearest neighbor ratherthan completely irrelevant ones Oblivion did fare signicantly better on classifying chess end games andpredicting a words semantic class giving evidence that these domains do contain irrelevant featuresOther researchers have also developed wrapper methods for use with nearest neighbor For instance Ahaand Bankert  report an a technique much like Oblivion but their system starts with a randomlyselected subset of features and includes an option for beam search rather than greedy decisions They reportimpressive improvements on a cloud classication task that involves over  numeric features Skalaks work on feature selection for nearest neighbor also starts with a random feature set but replacesgreedy search with random hill climbing that continues for a specied number of cyclesMost research on wrapper methods has focused on classication but both Moore and Lee  andTownsendWeber and Kibler  combine this idea with knearest neighbor for numeric prediction Alsomost work has emphasized the advantages of feature selection for induction methods that are highly sensitiveto irrelevant features However Langley and Sage b have shown that the naive Bayesian classierwhich is sensitive to redundant features can benet from the same basic approach as did Doaks earlierwork Singh and Provan  have extended this idea to learning more complex Bayesian networks Thissuggests that techniques for feature selection can improve the behavior of induction algorithms in a varietyof situations not only in the presence of irrelevant attributes As Caruana and Freitag b argue mostmethods for feature selection focus on nding attributes that are useful for performance in the sense ofDenition  rather than necessarily nding the relevant onesTable  characterizes the recent eorts on wrapper methods in terms of the dimensions discussed earlieras well as the induction method used in each case to direct the search process The table shows the diversityof techniques that researchers have developed and the heavy reliance on the experimental comparison ofvariant methods Unfortunately few of these experiments directly study the algorithms ability to deal withincreasing numbers of irrelevant features and few theoretical results are available for themPage  Selecting Relevant Features and Examples Feature Weighting MethodsSo far we have discussed algorithms that explicitly attempt to select a most relevant subset of featuresHowever another approach especially for embedded algorithms is to apply a weighting function to featuresin eect assigning them degrees of perceived relevance We have separated this from the explicit featureselection approach because the motivations and uses for these two methods tend to be dierent Explicitfeature selection is generally most natural when the result is intended to be understood by humans or fedinto another algorithm Weighting schemes tend to be easier to implement in online incremental settingsand are generally more purely motivated by eciency considerationsWeighting schemes can be characterized in terms of heuristic search as we viewed explicit featureselectionmethods However because the weight space lacks the partial ordering of feature sets most approaches tofeature weighting rely on quite dierent forms of search For instance the most common techniques involvesome form of gradient descent in which successive passes through the training instances lead to iterativechanges in all weightsPerhaps the bestknown attributeweighting method is the perceptron updating rule Minsky  Papert which adds or subtracts weights on a linear threshold unit in response to errors on training instancesThe leastmean squares algorithm Widrow  Ho  for linear units and backpropagation RumelhartHinton  Williams  its generalization for multilayer neural networks also make additive changes toa set of weights in order to reduce error on the training set Baluja and Pomerleau in this issue discussusing a neuralnetwork approach in domains where the degree of feature relevance can vary over timePerceptron weighting techniques can have diculty in settings dominated by truly irrelevant featuresfor instance see the paper by Kivinen Warmuth and Auer in this issue In response Littlestone developedWinnow an algorithm that updates weights in a multiplicative manner rather than additively asin the perceptron rule Littlestone showed that on any online stream of data consistent with a disjunctionof r features Winnow makes at most Or logn mistakes This eectively uses the notion of relevancegiven in Denition  Thus its behavior degrades only logarithmically with the number of features that areirrelevant to the target concept More generallyWinnow achieves this logarithmic degradation for conceptclasses such as conjunctions kDNF formulas and linear threshold functions with good separation betweenpositive and negative examplesFor concreteness we present a version of the Winnow algorithm for the disjunctionlearning scenariodiscussed in Sections  and  along with a proof of Littlestones theoremThe Winnow algorithm a simple version Initialize the weights w     wn of the features to  Given an example x     xn output  if wx     wnxn  n and output  otherwise If the algorithm makes a mistakea If the algorithm predicts negative on a positive example then for each xi equal to  double thevalue of wib If the algorithm predicts positive on a negative example then for each xi equal to  cut the valueof wi in half Go to  While most work on embedded weighting schemes has a neuralnetwork avor Aha  reports an errordriven methodembedded within a nearestneighbor learner that modies its distance metric by altering weightsSelecting Relevant Features and Examples Page Theorem  Winnow makes at most  r lgn mistakes on any sequence of examples consistent witha disjunction of r featuresProof Let us rst bound the number of mistakes that will be made on positive examples Any mistake madeon a positive example must double at least one of the weights in the target function the relevant weightsand a mistake made on a negative example will not halve any of these weights by denition of a disjunctionFurthermore each of relevant weights can be doubled at most   lgn times since only weights less than ncan ever be doubled Therefore Winnow makes at most r  lg n mistakes on positive examplesNow we bound the number of mistakes made on negative examples The total weight summed over allfeatures is initially n Each mistake made on a positive example increases the total weight by at most nsince before doubling we must have had wx     wnxn  n On the other hand each mistake madeon a negative example decreases the total weight by at least n since before halving we must have hadwx     wnxn  n The total weight never drops below zero Therefore the number of mistakes madeon negative examples is at most twice the number made on positive examples plus  that is  r lgnAdding this to the bound on the number of mistakes on positive examples yields the theoremThe same general approach ofWinnow has been used in algorithms developed by Littlestone andWarmuth Vovk  Littlestone Long and Warmuth  and CesaBianchi et al  Kivinen andWarmuth  describe relations between these approaches and additive updating methods such as theleast mean squares algorithm In fact these multiplicative updating schemes are very similar to the kind ofmultiplicative probability updates that occur in Bayesian methods and several of the results provide boundson the performance of Bayesian updating even when the probabilistic assumptions of that approach are notmet Experimental tests of Winnow and related multiplicative methods on natural domains have revealedgood behavior Armstrong et al  Blum  and studies with synthetic data show that they scalevery well to domains with even thousands of irrelevant features Littlestone  Mesterharm More generally weighting methods are often cast as ways of merging advice from dierent knowledgesources that may themselves be generated through learning In this light the weighting process plays aninteresting dual role with respect to the lter methods discussed earlier Filter approaches pass their outputa set of selected features to a blackbox learning algorithm whereas weighting approaches can take asinput the classiers generated by blackbox learning algorithms and determine the best way to combine theirpredictionsOn the other hand direct analogs to the lter and wrapper approaches do exist for determining weightsStanll  and Ting  describe lterlike methods that use conditional probability distributions toweight attributes for nearest neighbor Daelemans et al  present a dierent weighting scheme thatnormalizes features based on an informationtheoretic metric and one could use the scores produced byRelief Kira  Rendell  to the same end Finally Kohavi Langley and Yun  have adaptedthe wrapper method to search through a discretized weight space that can be explored in much the sameway as feature sets Each of these approaches shows improvement over use of all features but only the latterreports comparisons with a simple selection of attributes The Problem of Irrelevant ExamplesJust as some attributes are more useful than others so may some examples better aid the learning processthan others This suggests a second broad type of relevance that concerns the examples themselves and herewe briey consider techniques for their selection Some work has assumed the presence of a benevolent tutorwho gives informative instances such as near misses or provides ideal training sequences Winston However a more robust approach involves letting the learner select or focus on training cases by itselfPage  Selecting Relevant Features and ExamplesResearchers have proposed at least three reasons for selecting examples used during learning One is if thelearning algorithm is computationally intensive in this case if sucient training data is available it makessense to learn only from some examples for purposes of computational eciency Another reason is if thecost of labeling is high eg when labels must be obtained from experts but many unlabeled examples areavailable or are easy to generate Yet a third reason for example selection is to increase the rate of learningby focusing attention on informative examples thus aiding search through the space of hypotheses Here weshould distinguish between examples that are relevant from the viewpoint of information and ones that arerelevant from the viewpoint of ones algorithm Most work emphasizes the latter though informationbasedmeasures are sometimes used for this purposeAs with featureselection schemes we can separate exampleselection methods into those that embed theselection process within the learning algorithm those that lter examples before passing them to the induction process and those that wrap example selection around successive calls to the learning techniqueAlthough we will refer to this dimension below we will instead organize the section around another distinction between methods that select relevant examples from labeled training instances and ones that selectfrom unlabeled instances Selecting Labeled DataThe rst generic approach assumes that a set of labeled training data is available for use by the learningsystem but that not all of these examples are equally useful As we noted above one can embed the process ofexample selection within the basic learning algorithm and many simple induction schemes take this approachFor instance the perceptron algorithm edited nearest neighbor methods and some incremental conjunctivemethods only learn from an example when their current hypothesis misclassies it Such embedded methodssometimes called conservative algorithms ignore all examples on which their hypothesis is correctIf one assumes that training data and test data are both taken from a single xed distribution then onecan guarantee that with high probability the data used for training will overall be relevant to the successcriteria used for testing Blumer et al  As learning progresses however the learners knowledgeabout certain parts of the input space increases and examples in the wellunderstood portion of the spacebecome less useful For instance when a conservative algorithm has a  error rate it will ignore  ofthe training cases and when it achieves  error it will ignore  of the dataIn the PAC model learning algorithms need to roughly double the number of examples seen in orderto halve their error rate Schapire  Freund  Blumer et al  However for conservativealgorithms since the number of examples actually used for learning is proportional to the error rate thenumber of new examples used by the algorithm each time it wishes to halve its error rate remains roughlyconstant Thus the number of examples actually used to achieve some error rate  is really just logarithmicin  rather than linearAlthough this result holds only for conservative algorithms that embed the example selection processwithin learning one can use explicit example selection to achieve similar eects for other induction methodsIn particular Schapire  describes a wrapper method called boosting that takes a generic learningalgorithm and adjusts the distribution given to it by removing some training data based on the algorithmsbehavior The basic idea is that as learning progresses the booster samples the input distribution to keep theaccuracy of the learners current hypothesis near to that of random guessing As a result the learning processfocuses on the currently hard data Schapire has shown that boosting lets one achieve the logarithmic useof examples described above under quite general conditions and Freund   has further improved Littlestone and Mesterharm  have shown that a variant of naive Bayes that learns only from errors can deal betterwith irrelevant features than the standard version which updates its statistics on each example This shows there existinteractions between the problems of feature selection and example selectionSelecting Relevant Features and Examples Page on this technique On the experimental front Drucker et al   have shown that boosting canimprove the accuracy of neural network methods on tasks involving optical character recognition Thisapproach seems especially appropriate for techniques like backpropagation for which training is much moreexpensive than predictionAnother class of wrapper methods for example selection originated in the experimental study of decisiontree induction Quinlan  reports a windowing technique designed to reduce the time needed to construct decision tress from very large training sets Windowing selects a random sample of the training datato induce an initial decision tree then uses that tree to classify all the remaining examples From the misclassied cases the method selects another random set to augment the original sample constructs a newdecision tree and so forth repeating the process until it has a tree that correctly classies all of the trainingdata Quinlan reports that windowing led to substantial reduction in processing time on a large collection ofchess endgames and Catlett  describes another wrapper method called peepholing designed for evenlarger training sets John and Langley  report a much simpler use of wrappers to determine the propersize of a randomly selected training sampleLewis and Catlett  describe a lter approach to selection of labeled data but such techniques areless commmon in the machine learning literature than embedded or wrapper methods One can imaginesimple techniques for cleaning training data say by removing inconsistent examples that are identical exceptfor their class but such methods are not widely used Onepass sampling of the training data would alsoconstitute ltering but again research has leaned towards iterative versions of sampling like those in boostingand windowing Selecting Unlabeled DataThe learner can also select data even before it has been labeled This can be useful in scenarios whereunlabeled data is plentiful but where the labeling process is expensive One generic approach to this problemwhich can be embedded within an induction algorithm that maintains a set of hypotheses consistent with thetraining data is called query by committee Seung et al  Given an unlabeled instance the methodselects two hypotheses at random from the consistent set and if they make dierent predictions requeststhe label for the instance The basic idea is that informative or relevant examples are more likely to passthe test than those that most hypotheses classify the same way Unfortunately to obtain theoretical resultsfor query by committee requires much stronger constraints on the space of hypotheses than does boostingSpecically this method requires an ability to sample random consistent hypotheses which can be quitedicult although it is also a major topic of algorithmic research eg Sinclair  Jerrum  Dyer Frieze Kannan  and Lovasz  Simonovits There has been a larger body of work on algorithms that generate examples of their own choosing underthe heading of membership query algorithms within the theoretical community and experimentation withinthe empirical community A common technique used by algorithms of this sort is to take a known exampleand slightly alter its feature values to determine the eect on its classication For instance one might taketwo examples with dierent labels and then walk them towards each other to determine at what point thedesired classication changes this in turn is often used to determine relevant features tying in with ourearlier discussion Another class of methods eectively designs critical experiments to distinguish amongcompeting hypotheses letting them eliminate competitors and thus reduce the complexity of the learningtask Mitchell  suggested an informationtheoretic approach to example selection whereas Sammut Although boosting has clear empirical uses it was originally developed for the theoretical goal of showing that weaklearning implies strong learning in the PAC model In other words if one has an algorithm that will perform somewhatbetter than guessing over every distribution then there cannot be a hard core to the function being learned and one canboost performance to produce highquality predictionsPage  Selecting Relevant Features and Examplesand Banerji  and Gross  used less formal methods but demonstrated their advantage empiricallyMore recently work on active learning has continued this tradition for instance Cohn Ghahramani andJordan  report successful results with a system that selects examples designed to reduce the learnersvariance In parallel theoretical researchers Angluin  Angluin et al  Bshouty  Rivest Schapire  Jackson  have shown that the ability to generate queries greatly enlarges the types ofconcept classes for which one can guarantee polynomialtime learningAlthough much work on queries and experimentation has emphasized simple classication learning othereorts have addressed more complex learning tasks For example Knobe and Knobe  let theirgrammarinduction system query an oracle about the legality of candidate strings to distinguish amongcompeting hypotheses and Kulkarni and Simons  Kekada and Rajamoneys  Coast designcritical experiments to distinguish among competing hypotheses in scientic domains Finally both Shenand Simon  and Gil  have explored the uses of experimentation in learning action models forplanning tasksOther learning systems incorporate strategies for exploring portions of the instance space that have notyet been encountered to obtain more representative information about the domain For example Scott andMarkovitch  adapt this idea to unsupervised learning situations and many methods for reinforcementlearning include a bias toward exploring unfamiliar parts of the state space eg Lin  Both approachescan considerably increase learning rates over random presentationsMost work on selecting and querying unlabeled data has used embedded methods but Angluin et al and Blum et al  describe theoretical results for a wrapper query method that can be appliedto any algorithm Specically they show that when membership queries are available any algorithm with apolynomial mistake bound for learning a reasonable concept class can be converted in an automated wayinto one in which the number of mistakes plus queries has only a logarithmic dependence on the number ofirrelevant features present The basic idea is to gradually grow a set of features known to be relevant andwhenever the algorithm makes a mistake to use queries to determine if the mistake results from a missingrelevant feature and if so to place a new relevant feature into the set Challenges for Future Relevance ResearchDespite the recent activity and the associated progress in methods for selecting relevant features andexamples there remain many directions in which machine learning can improve its study of these importantproblems Here we outline some research challenges for the theoretical and empirical learning communities Theoretical ChallengesWe claim that in a sense many of the central open theoretical problems in machine learning revolve aroundquestions of nding relevant features For instance consider the wellknown question of whether there arepolynomialtime algorithms that can guarantee learning of polynomialsize DNF formulas in the PAC oruniform distribution models Or consider the similar question of whether polynomialsize decision trees arelearnable in either model These questions both include the following open problem as a special caseDoes there exist a polynomial time algorithm for learning the class of Boolean functions over f gnthat have logn relevant features in the PAC or uniform distribution modelsThis is a special case because any function that has only log n relevant features can by denition be writtenas a truth table having only n entries and therefore it must have a small decision tree and a small DNFrepresentation note that the learning problem would be trivial if we knew a priori which log n variablesSelecting Relevant Features and Examples Page were relevant On the other hand this problem appears to be a quite dicult special case For instanceany algorithm to solve this problem would need to be unusual in the sense that the class has been provenimpossible to learn in the statistical query model of Kearns Blum et al  Thus issues of ndingrelevant features seem to be at the core of what makes those classes hardAs a practical matter it is unclear how to experimentally test a proposed algorithm for this problem sinceno distribution on the target functions is given In fact functions with random truth tables in this classare generally easy To allow for easier experimental testing of algorithms for this problem the following isa specic distribution on the target functions that seems quite hard even for uniform random examples forconvenience the number of relevant features is  log nSelect at random two disjoint sets S T  f     ng each of size log n On input x compute theparity of the bits indexed by S that is does S contain an odd number of ones and the majorityfunction of the bits indexed by T that is does T contain more ones than zeroes and output theexclusiveor of the two resultsA second theoretical challenge is to develop algorithms with the focusing ability of Winnow that applyto more complex target classes such as decision lists parity functions or general linear threshold functionsThis would greatly extend the class of problems for which there exist positive results in online settingsIn the framework of example selection one important direction is to connect the work on membershipquery models which have the advantage of generally being algorithmic but assume that arbitrary points inthe input space may be probed with the work on ltering unlabeled instances which apply when only a xeddata stream is available but often require solving a computationally hard subproblem Another challenge isto further theoretically analyze the ways in which example selection can aid the feature selection process Empirical ChallengesConsiderable work also remains on the empirical front with one of the most urgent needs being studieson more challenging data sets For instance few of the domains used to date have involved more than features Two exceptions are Aha and Bankerts study of cloud classication  attributes and Kollerand Sahamis work on information retrieval  attributes but typical experiments have dealt with farfewer features Moreover Langley and Sages  results with the nearest neighbor method suggest thatmany of the widelyused UCI data sets have few completely irrelevant attributes In hindsight this seemsnatural for diagnostic domains in which experts tend to ask about relevant features and ignore other onesHowever we believe that many realworld domains do not have this character and that we must nd datasets with a substantial fraction of irrelevant attributes if we want to test adequately our ideas on featureselectionExperiments with synthetic data also have important roles to play in the study of featureselection methodsSuch data sets can let one systematically vary factors of interest such as the number of relevant and irrelevantattributes while holding other factors constant In this way one can directly measure the sample complexityof algorithms as a function of these factors showing their ability to scale to domains with many irrelevantfeatures However we distinguish between the use of synthetic data for such systematic experiments andreliance on isolated articial data sets such as the Monks problems which seem much less useful In fact this class is easy to learn when the algorithm can make active membership queries about examples of its ownchoosing Indeed the algorithm of Bshouty  learns the larger class of decision trees with membership queries in theexact leaning model and a recent algorithm of Jackson  learns the even larger class of general DNF formulas usingmembership queries with respect to the uniform distribution For instance if S  f  g and T  f  g then the classication of the example  would be positive sincethe rst three bits have an even number of ones making their parity  and the next three bits have more ones than zerosso the majority function is  and the XOR of those two quantities is Page  Selecting Relevant Features and ExamplesMore challenging domains with more features and a higher proportion of irrelevant ones will requiremore sophisticated methods for feature selection Although further increases in eciency would increasethe number of states examined such constantfactor improvements cannot eliminate problems caused byexponential growth in the number of feature sets However viewing these problems in terms of heuristicsearch suggests some places to look for solutions In general we must invent better techniques for selectingan initial feature set from which to start the search formulate searchcontrol methods that take advantage ofstructure in the space of feature sets devise improved frameworks for evaluating the usefulness of alternativefeature sets and design better halting criteria that will improve eciency without sacricing accuracyFuture research in the area should also compare more carefully the behavior of feature selection and attributeweighting schemes Presumably each approach has some advantages leaving an open question that is bestanswered by experiment but preferably by informed experiments designed to test specic hypotheses aboutthese two approaches to relevanceMore generally feature selection and example selection are tasks that seem to be intimately related andwe need more studies designed to help understand and quantify this relationship Much of the empiricalwork on example selection eg Gross  Cohn et al  has dealt with lowdimensional spaces yetthis approach clearly holds even greater potential for domains involving many irrelevant features Resolvingbasic issues of this sort promises to keep the eld of machine learning occupied for many years to comeAcknowledgementsThis research was supported in part by Grant No CCR from the National Science Foundationby a Sloan Foundation Research Fellowship and by Grant No N from the Oce of NavalResearch Many of the researchers active in the area of feature and example selection contributed directlyor indirectly to the ideas presented in this paper We would also like to thank the referees and the editorsof this issue for their helpful comments and suggestionsReferencesAha D  A study of instancebased algorithms for supervised learning tasks Mathematical empiricaland psychological evaluations  Doctoral dissertation Department of Information  Computer ScienceUniversity of California IrvineAha D W  Bankert R L  A comparative evaluation of sequential feature selection algorithmsIn D Fisher  JH Lenz Eds Articial intelligence and statistics V New York SpringerVerlagAlmuallim H  Dietterich T G  Learning with many irrelevant features Proceedings of the NinthNational Conference on Articial Intelligence pp  San Jose CA AAAI PressAngluin D  Learning regular sets from queries and counterexamples Information and Computation  Angluin D Hellerstein L  Karpinski M  Learning readonce formulas with queries Journal ofthe ACM   Armstrong R Freitag D Joachims T  Mitchell T  Webwatcher A learning apprentice for theworld wide web AAAI Spring Symposium on Information Gathering from Heterogeneous DistributedEnvironmentsBlum A  Learning Boolean functions in an innite attribute space Machine Learning   Blum A  Empirical support for Winnow and weightedmajority based algorithms Results on acalendar scheduling domain Proceedings of the Twelfth International Conference on Machine Learningpp  Lake Tahoe CA Morgan KaufmannSelecting Relevant Features and Examples Page Blum A Furst M Jackson J Kearns M Mansour Y  Rudich S  Weakly learning DNF andCharacterizing Statistical Query learning using Fourier analysis Proceedings of the th Annual ACMSymposium on Theory of Computing pp Blum A Hellerstein L  Littlestone N  Learning in the presence of nitely or innitely manyirrelevant attributes Journal of Computer and System Sciences    Blum A  Kannan R  Learning an intersection of k halfspaces over a uniform distributionProceedings of the th Annual IEEE Symposium on Foundations of Computer Science pp IEEEBlumer A Ehrenfeucht A Haussler D  Warmuth M K  Occams razor InformationProcessing Letters   Blumer A Ehrenfeucht A Haussler D  Warmuth M K  Learnability and the VapnikChervonenkis dimension Journal of the ACM   Breiman L Friedman J H Olshen R A  Stone C J  Classication and regression trees Belmont CA WadsworthBshouty N H  Exact learning via the monotone theory Proceedings of the IEEE Symposium onFoundations of Computer Science pp  IEEE Palo Alto CACardie C  Using decision trees to improve casebased learning Proceedings of the Tenth InternationalConference on Machine Learning pp  Amherst MA Morgan KaufmannCaruana R A  Freitag D a Greedy attribute selection Proceedings of the Eleventh InternationalConference on Machine Learning pp  New Brunswick NJ Morgan KaufmannCaruana R A  Freitag D b How useful is relevance Working Notes of the AAAI Fall Symposiumon Relevance pp  New Orleans LA AAAI PressCatlett J  Peepholing Choosing attributes eciently for megainduction Proceedings of the NinthInternational Conference on Machine Learning pp  Aberdeen Scotland Morgan KaufmannCesaBianchi N Freund Y Helmbold D P Haussler D Schapire R E  Warmuth M K How to use expert advice Proceedings of the Annual ACM Symposium on the Theory of Computingpp Clark P  Niblett T  The CN induction algorithm Machine Learning    Cohn D A Ghahramani Z  Jordan M I  Active learning with statistical models Journal ofArticial Intelligence Research   Comon P  Independent component analysis A new concept Signal Processing    Cover T M  Hart P E  Nearest neighbor pattern classication IEEE Transactions on Information Theory    Daelemans W Gillis S  Durieux G  The acquisition of stress A dataoriented approachComputational Linguistics   Devijver P A  Kittler J  Pattern recognition A statistical approach New York PrenticeHallDhagat A  Hellerstein L  PAC learning with irrelevant attributes Proceedings of the IEEESymposium on Foundations of Computer Science pp  IEEEDoak J  An evaluation of featureselection methods and their application to computer securityTechnical Report CSE Davis University of California Department of Computer SciencePage  Selecting Relevant Features and ExamplesDrucker H Schapire R  Simard P  Improving performance in neural networks using a boostingalgorithm In J E Moody S J Hanson  R P Lippmann Eds Advances in neural informationprocessing systems Vol  San Francisco Morgan KaufmannDrucker H Cortes C Jackel L D LeCun Y  Vapnik V  Boosting and other machine learningalgorithms Proceedings of the Eleventh International Conference on Machine Learning pp New Brunswick NJ Morgan KaufmannDyer M Frieze A  Kannan R  A random polynomial time algorithm for approximating thevolume of convex bodies Proceedings of the Annual ACM Symposium on the Theory of Computingpp Freund Y  Boosting a weak learning algorithm by majority Proceedings of the Third AnnualWorkshop on Computational Learning Theory pp  San Francisco Morgan KaufmannFreund Y  An improved boosting algorithm and its implications on learning complexity Proceedingsof the Fifth Annual ACM Workshop on Computational Learning Theory pp  ACM PressGarey M  Johnson D  Computers and intractability A guide to the theory of NPcompletenessSan Francisco W H FreemanGil Y  Ecient domainindependent experimentation Proceedings of the Tenth International Conference on Machine Learning pp  Amherst MA Morgan KaufmannGross K P  Concept acquisition through attribute evolution and experiment selection Doctoraldissertation School of Computer Science Carnegie Mellon University Pittsburgh PAHaussler D  Quantifying the inductive bias in concept learning Proceedings of the Fifth NationalConference on Articial Intelligence pp  Philadelphia AAAI PressHolte R  Very simple classication rules perform well on most commonly used domains MachineLearning    Jackson J  An ecient membershipquery algorithm for learning DNF with respect to the uniformdistribution Proceedings of the IEEE Symposium on Foundations of Computer Science IEEEJohn G H Kohavi R  Peger K  Irrelevant features and the subset selection problem Proceedings of the Eleventh International Conference on Machine Learning pp  New BrunswickNJ Morgan KaufmannJohn G H  Langley P  Static vs dynamic sampling for data mining Proceedings of the SecondInternational Conference of Knowledge Discovery and Data Mining pp  Portland AAAIPressJollie I T  Principal component analysis New York SpringerVerlagJohnson D S  Approximation algorithms for combinatorial problems Journal of Computer andSystem Sciences   Kearns M J  Vazirani U V  An introduction to computational learning theory  CambridgeMA MIT PressKira K  Rendell L  A practical approach to feature selection Proceedings of the Ninth International Conference on Machine Learning pp  Aberdeen Scotland Morgan KaufmannKivinen J  Warmuth M K  Additive versus exponentiated gradient updates for linear predictionProceedings of the th Annual ACM Symposium on Theory of Computing pp  New YorkACM PressSelecting Relevant Features and Examples Page Kohavi R  The power of decision tables Proceedings of the Eighth European Conference on MachineLearning Kohavi R Langley P  Yun Y  The utility of feature weighting in nearestneighbor algorithmsProceedings of the Ninth European Conference on Machine Learning  Prague SpringerVerlagKnobe B  Knobe K  A method for inferring contextfree grammars Information and Control   Koller D  Sahami M  Toward optimal feature selection Proceedings of the Thirteenth International Conference on Machine Learning  Bari Italy Morgan KaufmannKononenko I  Estimating attributes Analysis and extensions of Relief Proceedings of the SeventhEuropean Conference on Machine Learning Kubat M Flotzinger D  Pfurtscheller G  Discovering patterns in EEG signals Comparativestudy of a few methods Proceedings of the  European Conference on Machine Learning pp  Vienna SpringerVerlagKulkarni D  Simon H A  Experimentation in machine discovery In J Shrager  P LangleyEds Computational models of scientic discovery and theory formation San Francisco MorganKaufmannLangley P  Iba W  Averagecase analysis of a nearest neighbor algorithm Proceedings of theThirteenth International Joint Conference on Articial Intelligence pp  Chambery FranceLangley P  Sage S a Oblivious decision trees and abstract cases Working Notes of the AAAIWorkshop on CaseBased Reasoning pp  Seattle WA AAAI PressLangley P  Sage S b Induction of selective Bayesian classiers Proceedings of the Tenth Conference on Uncertainty in Articial Intelligence pp  Seattle WA Morgan KaufmannLangley P  Sage S  Scaling to domains with many irrelevant features In R Greiner EdComputational learning theory and natural learning systems Vol  Cambridge MA MIT PressLewis D D a Representation and learning in information retrieval  Doctoral dissertation Department of Computer Science University of Massachusetts Amherst Also available as Technical ReportUMCSLewis D D b Feature selection and feature extraction for text categorization Proceedings of Speechand Natural Language Workshop pp  San Francsico Morgan KaufmannLewis D D  Catlett J  Heterogeneous uncertainty sampling Proceedings of the Eleventh International Conference on Machine Learning pp  New Brunswick NJ Morgan KaufmannLin L J  Selfimproving reactive agents based on reinforcement learning planning and teachingMachine Learning    Littlestone N  Learning quickly when irrelevant attributes abound A new linear threshold algorithmMachine Learning    Littlestone N Long P M  Warmuth M K  Online learning of linear functions In Proceedingsof the Twentythird Annual ACM Symposium on Theory of Computing pp  New OrleansACM PressLittlestone N  Mesterharm C  An apobayesian relative of Winnow In M C Mozer M I Jordan  T Petsche Eds Advances in neural information processing systems Vol  Denver COMIT PressPage  Selecting Relevant Features and ExamplesLittlestone N Warmuth M K  The weighted majority algorithm Information and Computation  Lovasz L  Simonovits M  On the randomized complexity of volume and diameter Proceedingsof the IEEE Symposium on Foundations of Computer Science pp  IEEELund C  Yannakakis M  On the hardness of approximating minimization problems Proceedingsof the Annual ACM Symposium on the Theory of Computing pp Matheus C J  Rendell L A  Constructive induction on decision trees Proceedings of theEleventh International Joint Conference on Articial Intelligence pp  Detroit MI MorganKaufmannMichalski R S  Pattern recognition as ruleguided inductive inference IEEE Transactions onPattern Analysis and Machine Intelligence   Minsky M  Papert S  Perceptrons An introduction to computational geometry The MIT PressMitchell T M  Generalization as search Articial Intelligence    Reprinted inJ W Shavlik  T G Dietterich Eds  Readings in machine learning  San Francisco CAMorgan KaufmannMoore A W  Lee M S  Ecient algorithms for minimizing cross validation error Proceedingsof the Eleventh International Conference on Machine Learning pp  New Brunswick NJMorgan KaufmannNorton S W  Generating better decision trees Proceedings of the Eleventh International Conferenceon Articial Intelligence pp  Detroit MI Morgan KaufmannPazzani M J  Sarrett W  A framework for the average case analysis of conjunctive learningalgorithms Machine Learning    Pagallo G  Haussler D Boolean feature discovery in empirical learning Machine Learning    Quinlan J R  Learning ecient classication procedures and their application to chess end gamesIn R S Michalski J G Carbonell  T M Mitchell Eds Machine learning An articial intelligenceapproach San Francisco CA Morgan KaufmannQuinlan J R  C Programs for machine learning  San Francisco Morgan KaufmannRajamoney S  A computational approach to theory revision In J Shrager  P Langley EdsComputational models of scientic discovery and theory formation San Francisco CA Morgan KaufmannRivest R L  Schapire R E  Inference of nite automata using homing sequences Informationand Computation   Rumelhart D E Hinton G  Williams R J  Learning internal representations by error propagation In D E Rumelhart  J L McClelland Eds Parallel distributed processing Explorations inthe microstructure of cognition Vol  Cambridge MA MIT PressSammut C  Banerji R B  Learning concepts by asking questions In R S Michalski J G Carbonell  T M Mitchell Eds Machine learning An articial intelligence approach Vol  SanFrancisco CA Morgan KaufmannSchapire R E  The strength of weak learnability Machine Learning   Schlimmer J C  Eciently inducing determinations A complete and ecient search algorithm thatuses optimal pruning Proceedings of the Tenth International Conference on Machine Learning pp Amherst MA Morgan KaufmannSelecting Relevant Features and Examples Page Scott P D  Markovitz S  Representation generation in an exploratory learning system InD H Fisher M J Pazzani  P Langley Eds Concept formation Knowledge and experience inunsupervised learning  San Francisco CA Morgan KaufmannSeung H S Opper M  Sompolinsky H  Query by committee Proceedings of the Fifth AnnualWorkshop on Computational Learning Theory pp  New York ACM PressShen W M  Simon H A  Rule creation and rule learning through environmental explorationProceedings of the Eleventh International Joint Conference on Articial Intelligence pp Detroit MI Morgan KaufmannSinclair A  Jerrum M  Approximate counting uniform generation and rapidly mixing Markovchains Information and Computation   Singh M  Provan G M  A comparison of induction algorithms for selective and nonselectiveBayesian classiers Proceedings of the Twelfth International Conference on Machine Learning pp  Lake Tahoe CA Morgan KaufmannSingh M  Provan G M  Ecient learning of selective Bayesian network classiers Proceedingsof the Thirteenth International Conference on Machine Learning  Bari Italy Morgan KaufmannSkalak D B  Prototype and feature selection by sampling and random mutation hillclimbingalgorithms Proceedings of the Eleventh International Conference on Machine Learning pp New Brunswick NJ Morgan KaufmannStanll C W  Memorybased reasoning applied to English pronunciation Proceedings of the SixthNational Conference on Articial Intelligence pp  Seattle WA AAAI PressTing K M  Discretization of continuousvalued attributes and instancebased learning TechnicalReport No  Sydney University of Sydney Basser Department of Computer ScienceTownsendWeber T  Kibler D  Instancebased prediction of continuous values Working Notesof the AAAI Workshop on CaseBased Reasoning pp  Seattle WA AAAI PressVerbeurgt K  Learning DNF under the uniform distribution in polynomial time Proceedings of theThird Annual Workshop on Computational Learning Theory pp  Morgan KaufmannVere S A  Induction of concepts in the predicate calculus Proceedings of the Fourth InternationalJoint Conference on Articial Intelligence pp  Tbilisi USSR Morgan KaufmannVovk V  Aggregating strategies Proceedings of the Third Annual Workshop on ComputationalLearning Theory pp  Morgan KaufmannWidrow B  Ho M E  Adaptive switching circuits IRE WESCON Convention Record pp Winston P H  Learning structural descriptions from examples In P H Winston Ed Thepsychology of computer vision New York McGrawHill
