Published in  William W Cohen  Haym Hirsh eds Machine Learning Proceedings of theEleventh International Conference  Morgan Kaufmann Publishers San Francisco CAIrrelevant Features and the Subset Selection ProblemGeorge H JohnComputer Science DeptStanford UniversityStanford CA gjohnCSStanfordEDURon KohaviComputer Science DeptStanford UniversityStanford CA ronnykCSStanfordEDUKarl PegerComputer Science DeptStanford UniversityStanford CA kpflegerCSStanfordEDUAbstractWe address the problem of nding a subsetof features that allows a supervised induction algorithm to induce small highaccuracyconcepts We examine notions of relevanceand irrelevance and show that the denitionsused in the machine learning literature do notadequately partition the features into usefulcategories of relevance We present denitions for irrelevance and for two degrees ofrelevance These denitions improve our understanding of the behavior of previous subset selection algorithms and help dene thesubset of features that should be sought Thefeatures selected should depend not only onthe features and the target concept but alsoon the induction algorithm We describea method for feature subset selection usingcrossvalidation that is applicable to any induction algorithm and discuss experimentsconducted with ID and C on articial andreal datasets INTRODUCTIONIn supervised learning one is given a training set containing labelled instances The instances are typicallyspecied by assigning values to a set of features andthe task is to induce a hypothesis that accurately predicts the label of novel instances Following Occamsrazor Blumer et al  minimum descriptionlength Rissanen  and minimummessage lengthWallace  Freeman  one usually attempts tond structures that correctly classify a large subset ofthe training set and yet are not so complex that theybegin to overt the data Ideally the induction algorithm should use only the subset of features that leadsto the best performanceSince induction of minimal structures is NPhard inmany cases Hancock  Blum  Rivest algorithms usually conduct a heuristic search in theCorrelatedA10Irrelevant1B00A01A001100B11B1011001100B01001100A11B00110011Figure  An example where ID picks a bad relevantfeature correlated for the root and an irrelevant feature irrelevantspace of possible hypotheses This heuristic searchmay lead to induced concepts which depend on irrelevant features or in some cases even relevant featuresthat hurt the overall accuracy Figure  shows sucha choice of a nonoptimal split at the root made byID Quinlan  The Boolean target concept isA  A  B  B The feature named irrelevant is uniformly random and the feature correlated matches the class label  of the time The leftsubtree is the correct decision tree which is correctlyinduced if the correlated feature is removed fromthe data C Quinlan  and CART Breimanet al  induce similar trees with the correlatedfeature at the root Such a split causes all these induction algorithms to generate trees that are less accuratethan if this feature is completely removedThe problem of feature subset selection involves ndinga good set of features under some objective functionCommon objective functions are prediction accuracystructure size and minimal use of input features egwhen features are tests that have an associated costIn this paper we chose to investigate the possibility ofimproving prediction accuracy or decreasing the sizeof the structure without signicantly decreasing prediction accuracy This specic problem has been thoroughly investigated in the statistics literature but under assumptions that do not apply to most learningalgorithmssee Section We begin by describing the notions of relevance andirrelevance that have been previously dened by researchers We show that the denitions are toocoarsegrained and that better understanding can beachieved by looking at two degrees of relevance Section  looks at two models for feature subset selectionthe lter model and the wrapper model We claim thatthe wrapper model is more appropriate than the ltermodel which has received more attention in machinelearning Section  presents our experimental resultsSection  describes related work and Section  provides a summary and discussion of future work DEFINING RELEVANCEIn this section we present denitions of relevance thathave been suggested in the literature We then show asingle example where the denitions give unexpectedanswers and we suggest that two degrees of relevanceare needed weak and strongThe input to a supervised learning algorithm is a setof n training instances Each instance X is an elementof the set FF  Fm where Fi is the domain ofthe ith feature Training instances are tuples hX Y iwhere Y is the label or output Given an instancewe denote the value of feature Xi by xi The task ofthe induction algorithm is to induce a structure ega decision tree or a neural net such that given a newinstance it is possible to accurately predict the labelY  We assume a probability measure p on the spaceFF  FmY  Our general discussion does notmake any assumptions on the features or on the labelthey can be discrete continuous linear or structuredand the label may be singlevalued or a multivaluedvector of arbitrary dimension EXISTING DEFINITIONSAlmuallim and Dietterich  p  dene relevance under the assumption that all features and thelabel are Boolean and that there is no noiseDenition  A feature Xi is said to be relevant to aconcept C if Xi appears in every Boolean formula thatrepresents C and irrelevant otherwiseGennari et al  Section  dene relevance asThe denition given is a formalization of their stateDenition Relevant IrrelevantDenition  X X X X XDenition  None AllDenition  All NoneDenition  X X X X XTable  Feature relevance for the Correlated XORproblem under the four denitionsDenition  Xi is relevant i there exists some xiand y for which pXi  xi   such thatpY  y j Xi  xi  pY  y Under this denition Xi is relevant if knowing itsvalue can change the estimates for Y  or in otherwords if Y is conditionally dependent of Xi Notethat this denition fails to capture the relevance offeatures in the parity concept and may be changed asfollowsLet Si be the set of all features except Xi ie Si fX     Xi Xi     Xmg Denote by si a valueassignment to all features in SiDenition  Xi is relevant i there exists some xiy and si for which pXi  xi   such thatpY  y Si  si j Xi  xi  pY  y Si  si Under the following denition Xi is relevant if theprobability of the label given all features can changewhen we eliminate knowledge about the value of XiDenition  Xi is relevant i there exists some xiy and si for which pXi  xi Si  si   such thatpY  y j Xi  xi Si  si  pY  y j Si  si The following example shows that all the denitionsabove give unexpected resultsExample  Correlated XORLet features X     X be Boolean The instancespace is such that X and X are negatively correlatedwith X and X respectively ie X  X X  XThere are only eight possible instances and we assumethey are equiprobable The deterministic target concept isY  X  X  denotes XOR Note that the target concept has an equivalent Booleanexpression namely Y  X  X The features Xand X are irrelevant in the strongest possible senseX is indispensable and one ofX X can be disposedment Features are relevant if their values vary systematically with category membershipThe feature subset that Relief and RelieveD approximate. The feature subset that FOCUS approximates.Weakly relevant featuresIrrelevant features Strongly relevant featuresFigure  A view of feature relevanceof but we must have one of them Table  shows foreach denition which features are relevant and whichare notAccording to Denition  X and X are clearly irrelevant both X and X are irrelevant because each canbe replaced by the negation of the other By Denition  all features are irrelevant since for any outputvalue y and feature value x there are two instancesthat agree with the values By Denition  every feature is relevant because knowing its value changes theprobability of four of the eight possible instances from to zero By Denition  X and X are clearly irrelevant and bothX andX are irrelevant since theydo not add any information to S and S respectivelyAlthough such simple negative correlations are unlikely to occur domain constraints create a similareect When a nominal attribute such as color is encoded as input to a neural network it is customary touse a local encoding where each value is representedby an indicator variable For example the local encoding of a fourvalued nominal fa b c dg would bef  g Under such an encoding anysingle indicator variable is redundant and can be determined by the rest Thus most denitions of relevancywill declare all indicator variables to be irrelevant STRONG AND WEAK RELEVANCEWe now claim that two degrees of relevance are required Denition  denes strong relevance Strongrelevance implies that the feature is indispensable inthe sense that it cannot be removed without loss ofprediction accuracyDenition  Weak relevanceA feature Xi is weakly relevant i it is not stronglyrelevant and there exists a subset of features Si of Sifor which there exists some xi y and si with pXi xi Si  si   such thatpY  y j Xi  xi Si  si  pY  y j Si  siWeak relevance implies that the feature can sometimescontribute to prediction accuracy Features are relevant if they are either strongly or weakly relevant andare irrelevant otherwise Irrelevant features can nevercontribute to prediction accuracy by denitionIn Example  feature X is strongly relevant featuresX and X are weakly relevant and X and X areirrelevant Figure  shows our view of relevanceAlgorithms such as FOCUS Almuallim  Dietterich see Section  nd a minimal set of features that are sucient to determine the conceptGiven enough data these algorithms will select allstrongly relevant features none of the irrelevant onesand a smallest subset of the weakly relevant featuresthat are sucient to determine the concept Algorithms such as Relief Kira  Rendell a bKononenko  see Section  attempt to eciently approximate the set of relevant features FEATURE SUBSET SELECTIONThere are a number of dierent approaches to subset selection In this section we claim that the ltermodel the basic methodology used by algorithms likeFOCUS and Relief should be replaced with the wrapper model that utilizes the induction algorithm itself THE FILTER MODELWe review three instances of the lter model FOCUSRelief and the method used by Cardie  The FOCUS algorithm Almuallim Dietterich originally dened for noisefree Boolean domains exhaustively examines all subsets of features selectingthe minimal subset of features that is sucient to determine the label This is referred to as the MINFEATURES biasThis bias has severe implications when applied blindlywithout regard for the resulting induced concept Forsubset selectionFeatureInputfeatures AlgorithmInductionFigure  The feature lter model in which the features are ltered independent of the induction algorithmexample in a medical diagnosis task a set of featuresdescribing a patient might include the patients social security number SSN We assume that featuresother than SSN are sucient to determine the correctdiagnosis When FOCUS searches for the minimumset of features it will pick the SSN as the only featureneeded to uniquely determine the label Given onlythe SSN any induction algorithm will generalize verypoorlyThe Relief algorithm Kira  Rendell a bassigns a relevance weight to each feature which ismeant to denote the relevance of the feature to thetarget concept Relief is a randomized algorithm Itsamples instances randomly from the training set andupdates the relevance values based on the dierencebetween the selected instance and the two nearest instances of the same and opposite class the nearhitand nearmissThe Relief algorithm does not attempt to determineuseful subsets of the weakly relevant featuresRelief does not help with redundant featuresIf most of the given features are relevant tothe concept it would select most of themeven though only a fraction are necessary forconcept description Kira  Rendell apage In real domains many features have high correlationsand thus many are weakly relevant and will not beremoved by ReliefCardie  uses subset selection to remove irrelevant features from a dataset to be used with thenearestneighbor algorithm As a metric of an attributes usefulness C was used to induce a decision tree from a training set and those features thatdid not appear in the resulting tree were removed Theresulting performance of the nearestneighbor classierwas higher than with the entire set of featuresFigure  describes the feature lter model which characterizes these algorithms In this model the featureThis is true even if SSN is encoded in  binary features as long as more than  other features are required touniquely determine the diagnosisIn the simple parity example used in Kira  Rendella b there were only strongly relevant and irrelevant features so Relief found the strongly relevant featuresmost of the timeFeature subset evaluationFeature subset searchInduction AlgorithmInputfeaturesInductionAlgorithmFigure  The wrapper model The induction algorithm is used as a black box by the subset selectionalgorithmsubset selection is done as a preprocessing step Thedisadvantage of the lter approach is that it totallyignores the eects of the selected feature subset on theperformance of the induction algorithmWe claim that to determine a useful subset of featuresthe subset selection algorithm must take into accountthe biases of the induction algorithm in order to selecta subset that will ultimately result in an induced structure with high predictive accuracy on unseen dataThis motivated us to consider the the following approach which does employ such information THE WRAPPER MODELIn the wrapper model that we propose the featuresubset selection algorithm exists as a wrapper aroundthe induction algorithm see Figure  The featuresubset selection algorithm conducts a search for a goodsubset using the induction algorithm itself as part ofthe evaluation function Subset EvaluationGiven a subset of features we want to estimate theaccuracy of the induced structure using only the givenfeatures We propose evaluating the subset using nfold cross validation Breiman et al  Weiss Kulikowski  The training data is split into napproximately equally sized partitions The inductionalgorithm is then run n times each time using n  partitions as the training set and the other partitionas the test set The accuracy results from each of then runs are then averaged to produce the estimatedaccuracyNote that no knowledge of the induction algorithmis necessary except the ability to test the resultingstructure on the validation sets Searching the space of subsetsFinding a good subset of features under some measure requires searching the space of feature subsetsMany common AI search algorithms may be employedfor this task and some have been suggested in thestatistics literature under various assumptions aboutthe induction algorithm see Section  These assumptions do not hold for most machine learning algorithms hence heuristic search is usedOne simple greedy algorithm called backward elimination starts with the full set of features and greedilyremoves the one that most improves performance ordegrades performance slightly A similar algorithmcalled forward selection starts with the empty set offeatures and greedily adds featuresThe algorithms can be improved by considering bothaddition of a feature and deletion of a feature at eachstep For example during backward elimination consider adding one of the deleted features if it improvesperformance Thus at each step the algorithm greedily either adds or deletes The only dierence betweenthe backward and forward versions is that the backward version starts with all features and the forwardversion starts with no features The algorithms arestraightforward and are described in many statisticsbooks Draper  Smith  Neter Wasserman Kutner  under the names backward stepwise elimination and forward stepwise selection One only hasto be careful to set the degradation and improvementmargins so that cycles will not occurThe above heuristic increases the overall running timeof the blackbox induction algorithm by a multiplicative factor of Om in the worst case where m isthe number of features While this may be impractical in some situations it does not depend on n thenumber of instances As noted in Cohen   divide and conquer systems need much more time forpruning than for growing the structure by a factorof On for random data By pruning after featuresubset selection pruning may be much faster EXPERIMENTAL RESULTSIn order to evaluate the feature subset selection usingthe wrapper model we propose we ran experiments onnine datasets The C program is the program thatcomes with Quinlans book Quinlan  the IDresults were obtained by running C and using theunpruned trees On the articial datasets we used thes m C ags which indicate that subset splitsmay be used and that splitting should continue untilpurity To estimate the accuracy for feature subsetswe used fold cross validation Thus our feature subsets were evaluated solely on the basis of the trainingdata without using data from the test set Only afterthe best feature subset was chosen by our algorithmdid we use the test set to give the results appearing inthis sectionIn our experiments we found signicant variance in therelevance rankings given by Relief Since Relief randomly samples instances and their neighbors from thetraining set the answers it gives are unreliable withouta very high number of samples We were worried bythis variance and implemented a deterministic versionof Relief that uses all instances and all nearhits andnearmisses of each instance This gives the results onewould expect from Relief if run for an innite amountof time but requires only as much time as the standardRelief algorithm with the number of samples equal tothe size of the training set Since we are no longer worried by high variance we call this deterministic variantRelieveD In our experiments features with relevancyrankings below  were removedThe realworld datasets were taken from the UCIrvinerepository Murphy  Aha  and from Quinlan  Figures  and  summarize our results Wegive details for those datasets that had the largest differences either in accuracy or tree sizeArticial datasetsCorrAL This is the same dataset and conceptdescribed in the Introduction Figure  whichhas a high Correlation between one Attribute andthe Label hence CorrALMonkMonk These datasets were takenfrom Thrun et al  The datasets have sixfeatures and both target concepts are disjunctiveWe created  random training sets of the samesize as was given in Thrun et al  and testedon the full spaceParity  The target concept is the parityof ve bits The dataset contains  features uniformly random irrelevant The training setcontained  instances while all  instanceswere used in the test setRealworld datasetsVote This dataset includes votes for US Houseof Representatives Congresspersons on the  keyvotes identied by the Congressional QuarterlyAlmanac Volume XL The data set consists of features  training instances and  test instancesCredit or CRX The dataset contains instances for credit card applications There are features and a Boolean label The dataset was divided by Quinlan into  training instances and test instancesLabor The dataset contains instances for acceptable and unacceptable contracts It is a smalldataset with  features a training set of  instances and a test set of  instancesOur results show that the main advantage of doingsubset selection is that smaller structures are createdID ForwardID BackwardIDParityErr SizeAttsLaborErr SizeAttsVoteErr SizeAttsCreditErr SizeAttsFigure  Results for subset selection using the ID Algorithm For each dataset and algorithm we show theerror on the test set the relative size of the induced tree as compared with the largest of the three whoseabsolute size is given and the relative number of features in the training setC ForwardC BackwardC RelieveDCCorrALErr SizeAttsMonkErr SizeAttsVoteErr SizeAttsCreditErr SizeAttsFigure  Results for the C AlgorithmSmaller trees allow better understanding of the domain and are thus preferable if the error rate doesnot increase signicantly In the Credit database thesize of the resulting tree after forward stepwise selection with C decreased from  nodes to  nodesaccompanied by a slight improvement in accuracyFeature subset selection using the wrapper model didnot signicantly change generalization performanceThe only signicant dierence in performance was onparity and CorrAL using stepwise backward elimination which reduced the error to  from  and respectively Experiments were also run on theIris Thyroid and Monk datasets The results onthese datasets were similar to those reported in thispaperWe observed high variance in the fold crossvalidation estimates of the error Since our algorithmsdepend on crossvalidation to choose which feature toadd or remove a single optimistic CV estimatecaused premature stopping in many cases Such anestimate of low error could not be improved and thealgorithm stoppedRelieveD performed well in practice and reduced thenumber of features The number of features deletedhowever is low compared to our forward stepwise selectionAlthough in most cases the subset selection algorithmshave found small subsets this need not always be thecase For example if the data has redundant featuresbut also has many missing values a learning algorithmshould induce a hypothesis which makes use of theseredundant features Thus the best feature subset isnot always the minimal one RELATED WORKResearchers in statistics Boyce Farhi  Weischedel Narendra  Fukunaga  Draper  Smith Miller  Neter Wasserman  Kutner and pattern recognition Devijver  Kittler BenBassat  have investigated the feature subset selection problem for decades but most work hasconcentrated on subset selection using linear regressionSequential backward elimination sometimes called sequential backward selection was introduced in Marill  Green  Kittler generalized the dierentvariants including forward methods stepwise methods and plus take away r Branch and boundalgorithms were introduced by Narendra  Fukunaga Finally more recent papers attempt to useAI techniques such as beam search and bidirectionalsearch Siedlecki  Sklansky  best rst searchXu Yan  Chang  and genetic algorithmsVafai  De Jong Many measures have been suggested to evaluate thesubset selection as opposed to cross validation suchas adjusted mean squared error adjusted multiplecorrelation coecient and the Cp statistic Mallows In Mucciardi  Gose  seven dierenttechniques for subset selection were empirically compared for a nineclass electrocardiographic problemThe search for the best subset can be improved bymaking assumptions on the evaluation function Themost common assumption is monotonicity that increasing the subset can only increase the performance Under such assumptions the search space canbe pruned by the use of dynamic programming andbranchandbound techniques The monotonicity assumption is not valid for many induction algorithmsused in machine learning see for example Figure The terms weak and strong relevance are used in Levy to denote formulas that appear in one minimalderivation or in all minimal derivations We foundthe analog for feature subset selection helpful Moret denes redundant features and indispensablefeatures for the discrete case The denitions are similar to our notions of irrelevance and strong relevancebut do not coincide on some boundary cases Determinations were introduced by Russel   undera probabilistic setting and used in a deterministicnonnoisy setting in Schlimmer  and may helpanalyze redundanciesIn the machine learning literature the most closelyrelated work is FOCUS and Relief which we have described The PRESET algorithm described in Modrzejewski  is another lter algorithm that usesthe theory of Rough Sets to heuristically rank the features assuming a noiseless Boolean domain Littlestone  introduced the WINNOW family of algorithms that eciently learns linear threshold functionswith many irrelevant features in the mistake boundmodel and in Valiants PAC modelRecently the machine learning community has shownincreasing interest in this topic Moore and Lee present a set of ecient algorithms to race competing subsets until one outperforms all others thusavoiding the computation involved in fully evaluatingeach subset Their method is an example of the wrapper model using a memorybased instancebased algorithm as the induction engine and leaveoneoutcross validation LOOCV as the subset evaluationfunction Searching for feature subsets is done usingbackward and forward hillclimbing techniques similar to ours but they also present a new methodschemata searchthat seems to provide a fourfoldspeedup in some cases Langley and Sage  havealso recently used LOOCV in a nearestneighbor algorithmCaruana and Freitag  test the forward and backward stepwise methods on the calendar apprentice domain using the wrapper model and a variant of IDas the induction engine They introduce a cachingscheme to save evaluations of subsets which speedsup the search quite a bit but it seems to be specicto IDSkalak  uses the wrapper model for feature subset selection and for decreasing the number of prototypes stored in instancebased methods He shows thatthis can sometimes increase the prediction accuracy insome cases DISCUSSION AND FUTUREWORKWe dened three categories of feature relevance in order to clarify our understanding of existing algorithmsand to help dene our goal nd all strongly relevantfeatures no irrelevant features and a useful subset ofthe weakly relevant features that yields good performance We advocated the wrapper model as a meansof identifying useful feature subsets and tested twogreedy search heuristicsforward stepwise selectionand backward stepwise eliminationusing cross validation to evaluate performanceOur results show that while accuracy did not improvesignicantly except for the parity and CorrALdatasets the generated trees induced by ID and Cwere generally smaller using the wrapper model Wealso tested C on several datasets using RelieveD asa feature lter and observed that while it removessome features it does not remove as many features asdid our forward selection methodWe included the results for forward and backwardsearch methods separately to illustrate the dierentbiases of the two greedy strategies but one can easily imagine combining the two methods to achieve thebest behavior of both In the simplest approach wecould run both methods separately and select the bestof the two results based on our evaluation methodThis should yield the same positive results as forwardsearch in most cases while retaining the reasonable behavior of backward search for problems with high feature interaction such as parity In all but one experiment the smaller of the two trees produced by forward stepwise selection and backward stepwise elimination was smaller than the tree induced by ID orC and it was not larger in the last caseNote that even the better of the backward and forwardresults should not be taken as the best performancepossible from the wrapper model More comprehensivesearch strategies could search a larger portion of thesearch space and might yield improved performanceThe feature relevance rankings produced by RelieveDcould be used to create a set of initial states in thespace from which to searchOne possible reason for the lack of signicant improvement of prediction accuracy over C is that C doesquite well on most of the datasets tested here leavinglittle room for improvement This seems to be in linewith with Holtes claims Holte  Harder datasetsmight show more signicant improvement Indeed thewrapper model produced the most signicant improvement for the two datasets parity and CorrAL onwhich C performed the worstFuture work should address better search strategiesbetter evaluation estimates and should test the wrapper model with other classes of learning algorithmsResearch aimed at improving the evaluation estimatesfor subsets should attempt to nd a method of reducing the problem of high variance in the cross validation estimates We believe this may be possible byaveraging a number of separate cross validation runsshu ing data between runs and by using stratiedcross validationFeature subset selection is an important problem thathas many ramications Our introductory exampleFigure  shows that common algorithms such as IDC and CART fail to ignore features which if ignored would improve accuracy Feature subset selection is also useful for constructive induction Pagallo Haussler  where features can be constructedand tested using the wrapper model to determine ifthey improve performance Finally in real world applications features may have an associated cost iewhen the value of a feature is determined by an expensive test The feature selection algorithms can bemodied to prefer removal of highcost testsAcknowledgementsWe have benetted from the comments and advice ofWray Buntine Tom Dietterich Jerry Friedman IgorKononenko Pat Langley Scott Roy and the anonymous reviewers Richard Olshen kindly gave us accessto the CART software Thanks to Nils Nilsson andYoav Shoham for supporting theMLC project andeveryone working onMLC especially Brian Frascaand Richard Long George John was supported by aNational Science Foundation Graduate Research Fellowship The MLC project is partly funded byONR grant NReferencesAlmuallim H and Dietterich T G  Learning with many irrelevant features In Ninth NationalConference on Articial Intelligence  MITPressBenBassat M  Use of distance measuresinformation measures and error bounds in featureevaluation In Krishnaiah P R and Kanal L Neds Handbook of Statistics volume  NorthHollandPublishing Company Blum A L and Rivest R L  Training anode neural network is NPcomplete Neural Networks Blumer A Ehrenfeucht A Haussler D and Warmuth M K  Occams razor Information Processing Letters Boyce D Farhi A and Weischedel R  Optimal Subset Selection SpringerVerlagBreiman L Friedman J H Olshen R A andStone C J  Classication and RegressionTrees Wadsworth International GroupCardie C  Using decision trees to improvecasebased learning In Proceedings of the Tenth International Conference on Machine Learning Morgan KaufmannCaruana R and Freitag D  Greedy attributeselection In Cohen W W and Hirsh H eds Machine Learning Proceedings of the Eleventh International Conference Morgan KaufmannCohen W W  Ecient pruning methods forseparateandconquer rule learning systems In thInternational Joint Conference on Articial Intelligence  Morgan KaufmannDevijver P A and Kittler J  Pattern Recognition A Statistical Approach PrenticeHall InternationalDraper N R and Smith H  Applied Regression Analysis John Wiley  Sons nd editionGennari J H Langley P and Fisher D Models of incremental concept formation ArticialIntelligence Hancock T R  On the diculty of nding small consistent decision trees UnpublishedManuscript Harvard UniversityHolte R C  Very simple classication rulesperform well on most commonly used datasets Machine Learning Kira K and Rendell L A a The featureselection problem Traditional methods and a newalgorithm In Tenth National Conference on ArticialIntelligence  MIT PressKira K and Rendell L A b A practicalapproach to feature selection In Proceedings of theNinth International Conference on Machine Learning Morgan KaufmannKononenko I  Estimating attributes Analysis and extensions of Relief In Proceedings of theEuropean Conference on Machine LearningLangley P and Sage S  Oblivious decision trees and abstract cases In Working Notes ofthe AAAI Workshop on CaseBased Reasoning InpressLevy A Y  Irrelevance Reasoning in Knowledge Based Systems PhD Dissertation StanfordUniversityLittlestone N  Learning quickly when irrelevant attributes abound A new linearthreshold algorithm Machine Learning Mallows C L  Some comments on cp Technometrics Marill T and Green D M  On the eectiveness of receptors in recognition systems IEEETransactions on Information Theory Miller A J  Subset Selection in RegressionChapman and HallModrzejewski M  Feature selection using roughsets theory In Brazdil P B ed Proceedings of theEuropean Conference on Machine Learning Moore A W and Lee M S  Ecient algorithms for minimizing cross validation error In Cohen W W and Hirsh H eds Machine LearningProceedings of the Eleventh International ConferenceMorgan KaufmannMoret B M E  Decision trees and diagramsACM Computing Surveys Mucciardi A N and Gose E E  A comparison of seven techniques for choosing subsets ofpattern recognition properties IEEE Transactionson Computers CMurphy P M and Aha D W  UCI repository of machine learning databases For informationcontact mlrepositoryicsucieduNarendra M P and Fukunaga K  A branchand bound algorithm for feature subset selectionIEEE Transactions on Computers CNeter J Wasserman W and Kutner M H Applied Linear Statistical Models Irwin HomewoodIL rd editionPagallo G and Haussler D  Boolean featurediscovery in empirical learning Machine LearningQuinlan J R  Induction of decision treesMachine Learning  Reprinted in Shavlik andDietterich eds Readings in Machine LearningQuinlan J R  C Programs for MachineLearning Los Altos California Morgan KaufmannRissanen J  Stochastic complexity and modeling Ann Statist Russel S J  Preliminary steps toward the automation of induction In Proceedings of the NationalConference on Articial Intelligence Russel S J  The Use of Knowledge in Analogyand Induction Morgan KaufmannSchlimmer J C  Eciently inducing determinations A complete and systematic search algorithm that uses optimalpruning In Proceedings of theTenth International Conference on Machine Learning  Morgan KaufmannSiedlecki W and Sklansky J  On automaticfeature selection International Journal of PatternRecognition and Articial Intelligence Skalak D B  Prototype and feature selection by sampling and random mutation hill climbingalgorithms In Cohen W W and Hirsh H edsMachine Learning Proceedings of the Eleventh International Conference Morgan KaufmannThrun S B et al  The monks problemsA performance comparison of dierent learning algorithms Technical Report CMUCS CarnegieMellon UniversityVafai H and De Jong K  Genetic algorithmsas a tool for feature selection in machine learning InFourth International Conference on Tools with Articial Intelligence  IEEE Computer SocietyPressWallace C and Freeman P  Estimation andinference by compact coding Journal of the RoyalStatistical Society B Weiss S M and Kulikowski C A  Computer Systems that Learn San Mateo CA MorganKaufmannXu L Yan P and Chang T  Best rststrategy for feature selection In Ninth InternationalConference on Pattern Recognition  IEEEComputer Society Press
