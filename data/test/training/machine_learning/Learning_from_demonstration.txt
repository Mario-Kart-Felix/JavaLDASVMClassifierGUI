Schaal, S. 1997. Learning from demonstration. In M.C. Mozer, M. Jordan,  T. Petsche eds.,Advances in Neural Information Processing Systems 9, pp.10401046. Cambridge, MA MIT PressLearning From DemonstrationStefan Schaalsschaalcc.gatech.edu httpwww.cc.gatech.edufacStefan.SchaalCollege of Computing, Georgia Tech, 801 Atlantic Drive, Atlanta, GA 303320280ATR Human Information Processing, 22 Hikaridai, Seikocho, Sorakugun, 61902 KyotoAbstractBy now it is widely accepted that learning a task from scratch, i.e., withoutany prior knowledge, is a daunting undertaking. Humans, however, rarely attempt to learn from scratch. They extract initial biases as well as strategieshow to approach a learning problem from instructions andor demonstrationsof other humans. For learning control, this paper investigates how learningfrom demonstration can be applied in the context of reinforcement learning.We consider priming the Qfunction, the value function, the policy, and themodel of the task dynamics as possible areas where demonstrations can speedup learning. In general nonlinear learning problems, only modelbased reinforcement learning shows significant speedup after a demonstration, while inthe special case of linear quadratic regulator LQR problems, all methodsprofit from the demonstration. In an implementation of pole balancing on acomplex anthropomorphic robot arm, we demonstrate that, when facing thecomplexities of real signal processing, modelbased reinforcement learningoffers the most robustness for LQR problems. Using the suggested methods,the robot learns pole balancing in just a single trial after a 30 second longdemonstration of the human instructor.1. INTRODUCTIONInductive supervised learning methods have reached a high level of sophistication. Givena data set and some prior information about its nature, a host of algorithms exist that canextract structure from this data by minimizing an error criterion. In learning control, however, the learning task is often less well defined. Here, the goal is to learn a policy, i.e., theappropriate actions in response to a perceived state, in order to steer a dynamical system toaccomplish a task. As the task is usually described in terms of optimizing an arbitrary performance index, no direct training data exist which could be used to learn a controller in asupervised way. Even worse, the performance index may be defined over the long termbehavior of the task, and a problem of temporal credit assignment arises in how to creditor blame actions in the past for the current performance. In such a setting, typical for reinforcement learning, learning a task from scratch can require a prohibitively timeconsuming amount of exploration of the stateaction space in order to find a good policy.On the other hand, learning without prior knowledge seems to be an approach that is rarelytaken in human and animal learning. Knowledge how to approach a new task can be transferred from previously learned tasks, andor it can be extracted from the performance of ateacher. This opens the questions of how learning control can profit from these kinds of information in order to accomplish a new task more quickly. In this paper we will focus onlearning from demonstration.Learning from demonstration, also known as programming by demonstration, imitationlearning, and teaching by showing received significant attention in automatic robot assembly over the last 20 years. The goal was to replace the timeconsuming manual pro2gramming of a robot by an automatic programming process, solely driven by showing the robot the assembly taskby an expert. In concert with the main stream of ArtificialIntelligence at the time, research was driven by symbolicapproaches the experts demonstration was segmentedinto primitive assembly actions and spatial relationshipsbetween manipulator and environment, and subsequentlysubmitted to symbolic reasoning processes e.g., LozanoPerez, 1982 Dufay  Latombe, 1983 Segre  DeJong,1985. More recent approaches to programming by demonstration started to include more inductive learningcomponents e.g., Ikeuchi, 1993 Dillmann, Kaiser, Ude, 1995. In the context of human skill learning,teaching by showing was investigated by Kawato, Gandolfo, Gomi,  Wada 1994 and Miyamoto et al. 1996for a complex manipulation task to be learned by an anthropomorphic robot arm. An overview of several otherprojects can be found in Bakker  Kuniyoshi 1996.In this paper, the focus lies on reinforcement learning andhow learning from demonstration can be beneficial in thiscontext. We divide reinforcement learning into two categories reinforcement learning for nonlinear tasksSection 2 and for approximately linear tasks Section3, and investigate how methods like Qlearning, valuefunction learning, and modelbased reinforcement learning can profit from data from a demonstration. In Section2.3, one example task, pole balancing, is placed in thecontext of using an actual, anthropomorphic robot to learnit, and we reconsider the applicability of learning fromdemonstration in this more complex situation.2. REINFORCEMENT LEARNING FROM DEMONSTRATIONTwo example tasks will be the basis of our investigation of learning from demonstration.The nonlinear task is the pendulum swingup with limited torque Atkeson, 1994 Doya,1996, as shown in Figure 1a. The goal is to balance the pendulum in an upright positionstarting from hanging downward. As the maximal torque available is restricted such thatthe pendulum cannot be supported against gravity in all states, a pumping trajectory isnecessary, similar as in the mountain car example of Moore 1991, but more delicately inits timing since building up too much momentum during pumping will overshoot the upright position. The approximately linear example, Figure 1b, is the wellknown cartpolebalancing problem Widrow  Smith, 1964 Barto, Sutton,  Anderson, 1983. For bothtasks, the learner is given information about the onestep reward r Figure 1 , and bothtasks are formulated as continuous state and continuous action problems. The goal of eachtask is to find a policy which minimizes the infinite horizon discounted rewardV t e r s s ds V t r i is tti ti tx x u x x u                    , ,    or   where the left hand equation is the continuous time formulation, while the right handequation is the corresponding discrete time version, and where x  and u  denote a ndimensional state vector and a mdimensional command vector, respectively. For theSwingUp, we assume that a teacher provided us with 5 successful trials starting from dif a,   mgl ml mglr cm l g NmT22 2221 9 81 0 05 5  sin , ,, log cos, . , . ,,  ,maxmax                              define    x ub ,   mglmc x,  x Fmlx ml mglm m x ml ml Fx x Frl m m kg m kgdiagcTT Tc cos  sin  cos  sin, , ,  ,,. , . , .. , , , .                    2200 75 0 15 1 01 25 1 12 0 25define    x ux u x Q x u RuQ   , .R 0 01Figure 1 a pendulum swing up,b cart pole balancing 13ferent initial conditions. Each trial consists of a time series of data vectors  , ,     sampled at 60Hz. For the CartPole, we have a 30 second demonstration of successful balancing, represented as a 60Hz time series of data vectors  , , , , x x F  . How can these demonstrations be used to speed up reinforcement learning2.1 THE NONLINEAR TASK SWINGUPWe applied reinforcement learning based on learning a value function Vfunction Dyer McReynolds, 1970 for the SwingUp task, as the alternative method, QlearningWatkins, 1989, has yet received very limited research for continuous stateaction spaces.The Vfunction assigns a scalar reward value Vxt to each state x such that the entire Vfunction fulfills the consistency equationV t r t t V ttx x u xu  arg min  ,              1For clarity, this equation is given for a discrete stateaction system the continuous formulation can be found, e.g., in Doya 1996. The optimal policy, u  x, chooses the actionu in state x such that 2 is fulfilled. Note that this computation involves an optimizationstep that includes knowledge of the subsequent state xt1. Hence, it requires a model ofthe dynamics of the controlled system, xt1xt,ut. From the viewpoint of learningfrom demonstration, Vfunction learning offers three candidates which can be primed froma demonstration the value function Vx, the policy x, and the model x, u.2.1.1 VLearningIn order to assess the benefits of a demonstration for the SwingUp, we implemented Vlearning as suggested in Doyas1996 continuous TD CTD learning algorithm. The Vfunction and the dynamics model were incrementally learned by anonlinear function approximator, Receptive Field Weighted Regression RFWRSchaal  Atkeson 1996. Differingfrom Doyas 1996 implementation, weused the optimal action suggested by CTDto learn a model of the policy  anactor as in Barto et al. 1983, again represented by RFWR. The following learning conditions were tested empiricallya Scratch Trial by trial learning ofvalue function V, model , and actor  from scratch.b Primed Actor Initial training of  from the demonstration, then trial by trial learning.c Primed Model Initial training of  from the demonstration, then trial by trial learning.d Primed ActorModel Priming of  and  as in b and c, then trial by trial learning.Figure 2 shows the results of learning the SwingUp. Each trial lasted 60 seconds. Thetime Tup the pole spent in the interval      , 2 2  during each trial was taken as theperformance measure Doya, 1996. Comparing conditions a and c, the results demonstrate that learning the pole model from the demonstration did not speed up learning. Thisis not surprising since learning the Vfunction is significantly more complicated thanlearning the model, such that the learning process is dominated by Vfunction learning.Interestingly, priming the actor from the demonstration had a significant effect on the initial performance condition a vs. b. The system knew right away how to pump up thependulum, but, in order to learn how to balance the pendulum in the upright position, it finally took the same amount of time as learning from scratch. This behavior is due to the 201020304050601 10 100TupTriala scratchb primed actorc primed modeld primed actormodelFigure 2 Smoothed learning curves of the averageof 10 learning trials for the learning conditions ato d see text. Good performance is characterizedby Tup 45s below this value the system is usually able to swing up properly but it does not knowhow to stop in the upright position.4fact that, theoretically, the Vfunction can only be approximated correctly if the entirestateaction space is explored densely. Only if the demonstration covered a large fractionof the entire state space one would expect that Vlearning can profit from it. We also investigated using the demonstration to prime the Vfunction by itself or in combinationwith the other functions. The results were qualitatively the same as in shown in Figure 2if the policy was included in the priming, the learning traces were like b and d, otherwiselike a and c. Again, this is not totally surprising. Approximating a Vfunction is not justsupervised learning as for  and  , it requires an iterative procedure to ensure the validityof 2 and amounts to a complicated nonstationary function approximation process. Giventhe limited amount of data from the demonstration, it is generally very unlikely to approximate a good value function.2.1.2 ModelBased VLearningIf learning a model  is required, one canmake more powerful use of it. Accordingto the certainty equivalence principle, can substitute the real world, and planningcan be run in mental simulations insteadof interaction with the real world. In reinforcement learning, this idea was originally pursued by Suttons 1990 DYNAalgorithms for discrete stateaction spaces.Here we will explore in how far a continuous version of DYNA, DYNACTD,can help in learning from demonstration.The only difference compared to CTD inSection 2.1.1 is that after every real trial,DYNACTD performs five mental trialsin which the model of the dynamics acquired so far replaces the actual pole dynamics. Two learning conditions we be exploreda Scratch Trial by trial learning of V, model , and policy  from scratch.b Primed Model Initial training of  from the demonstration, then trial by trial learning.Figure 3  demonstrates that in contrast to Vlearning in the previous section, learning fromdemonstration can make a significant difference now after the demonstration, it onlytakes about 23 trials to accomplish a good swingup with stable balancing, indicated byTup 45s. Note that also learning from scratch is significantly faster than in Figure 2.2.2 THE LINEAR TASK CARTPOLE BALANCINGOne might argue that applying reinforcement learning from demonstration to the SwingUp task is premature, since reinforcement learning with nonlinear function approximatorshas yet to obtain appropriate scientific understanding. Thus, in this section we turn to aneasier task the cartpole balancer. The task is approximately linear if the pole is started ina close to upright position, and the problem has been well studied in the dynamic programming literature in the context of linear quadratic regulation LQR Dyer  McReynolds, 1970.2.2.1 QLearningIn contrast to Vlearning, Qlearning Watkins, 1989 Singh  Sutton, 1996 learns a morecomplicated value function, Qx,u, which depends both on the state and the command.The analogue of the consistency equation 2 for Qlearning isQ t t r t t Q t ttx u x u x uu ,    ,   arg min  ,            11 101020304050601 10 100TupTriala scratch b primed modelFigure 3 Smoothed learning curves of the averageof 10 learning trials for the learning conditions aand b see text of the SwingUp problem usingmental simulations. See Figure 2 for explanations how to interpret the graph.35At every state x, picking the action u which minimizes Q is the optimal action under thereward function  1. As an advantage, evaluating the Qfunction to find the optimal policy does not require a model the dynamical system  that is to be controlled only thevalue of the onestep reward r is needed. For learning from demonstration, priming the Qfunction andor the policy are the two candidates to speed up learning.For LQR problems, Bradtke 1993 suggested a Qlearning method that is ideally suitedfor learning from demonstration, based on extracting a policy. He observed that for LQRthe Qfunction is quadratic in the states and commandsQ n n m m n mT T T TT Tx u x uH HH Hx u H H H H, , , , , ,            11 1221 2211 22 12 21   and that the linear policy, represented as again matrix K, can be extracted from 4 asuopt    K x H H x22121Conversely, given a stabilizing initial policyKdemo, the current Qfunction can be approximated by a recursive least squares procedure,and it can be optimized by a policy iterationprocess with guaranteed convergence Bradkte,1993. As a demonstration allows one to extractan initial policy Kdemo by linearly regressingthe observed command u against the corresponding observed states x, oneshot learningof pole balancing is achievable. As shown inFigure 4, after about 120 seconds 12 policy iteration steps, the policy is basically indistinguishable from the optimal policy. A caveat ofthis Qlearning, however, is that it cannot not learn without a stabilizing initial policy.2.2.2 Modelbased VLearningLearning an LQR task by learning the Vfunction is one of the classic forms of dynamicprogramming Dyer  McReynolds, 1970. Using a stabilizing initial policy Kdemo, thecurrent Vfunction can be approximated by recursive least squares in analogy withBradtke 1993. Similarly as for Kdemo, a linear model demo of the cartpole dynamicscan be extracted from a demonstration by linear regression of the cartpole state xt vs.the previous state and command vector xt1, ut1, and the model can be refined withevery new data point experienced during learning. The policy update becomesK B H B B H A x x H x A B A B             R V f n n n mT T T demo1 ,  where , , ,Thus, a similar process as in Bradtke 1993 can be used to find the optimal policy K, andthe system accomplishes one shot learning, qualitatively indistinguishable from Figure 4.Again, as pointed out in Section 2.1.2, one can make more efficient use of the learnedmodel by performing mental simulations. Given the model demo, the policy K can be calculated by offline policy iteration from an initial estimate of H, e.g., taken to be the identity matrix Dyer  McReynolds, 1970. Thus, no initial stabilizing policy is required,but rather an estimate of the task dynamics. Also this method achieves one shot learning.2.3 POLE BALANCING WITH AN ACTUAL ROBOTAs a result of the previous section, it seems that there are no real performance differencesbetween Vlearning, Qlearning, and modelbased Vlearning for LQR problems. To explore the usefulness of these methods in a more realistic framework, we implemented400.0050.010.0150.020.0250.030.0350.040.0450 20 40 60 80 100 120OneStep RewardTime sKdemo 0.59,   1.81, 18.71,   6.67Kfinal   5.76, 11.37, 83.05, 21.92Figure 4 Typical learning curve of a noisysimulation of the cartpole balancer using Qlearning. The graph shows the value of theonestep reward over time for the firstlearning trial. The pole is never dropped.566learning from demonstration of pole balancing on an anthropomorphic robot arm. The robot is equipped with a 60 Hz videobased stereo vision. The pole is marked by two colorblobs which can be tracked in realtime. A 30 second long demonstration of pole balancing was is provided by a human standing in front of the two robot cameras.There are a few crucial differences in comparison with the simulations. First, as the demonstration is visionbased, only kinematic variables can be extracted from the demonstration. Second, visual signal processing has about 120ms time delay. Third, a commandgiven to the robot is not executed with very high accuracy due to unknown nonlinearitiesof the robot. And lastly, humans use internal state for pole balancing, i.e., their policy ispartially based on nonobservable variables. These issues have the following impactKinematic Variables In this implementation, the robot armreplaces the cart of the CartPole problem. Since we have anestimate of the inverse dynamics and inverse kinematics ofthe arm, we can use the acceleration of the finger in Cartesian space as command input to the task. The arm is alsomuch heavier than the pole which allows us to neglect theinteraction forces the pole exerts on the arm. Thus, the polebalancing dynamics of Figure 1b can be reformulated asuml ml mgl x ucos  sin ,      2 0All variables in this equation can be extracted from a demonstration. We omit the 3D extension of these equations.Delayed Visual Information There are two possibilities of dealing with delayed variables.Either the state of the system is augmented by delayed commands corresponding to7160s 120s delay time, xT t t tx x u u u    , , , , , , ,   1 2 7 , or a state predictive controlleris employed. The former method increases the complexity of a policy significantly, whilethe latter method requires a model .Inaccuracies of Command Execution Given an acceleration command u, the robot willexecute something close to u, but not u exactly. Thus, learning a function which includesu, e.g., the dynamics model 7, can be dangerous since the mapping  , , , ,    ,  x x u x  is contaminated by the nonlinear dynamics of the robot arm. Indeed, it turned out that wecould not learn such a model reliably. This could be remedied by observing the command u, i.e., by extracting u x   from visual feedback.Internal State in Demonstrated Policy Investigations with human subjects have shownthat humans use internal state in pole balancing. Thus, a policy cannot be observed thateasily anymore as claimed in Section 2.2 a regression analysis for extracting the policy ofa teacher must find the appropriate timealignment of observed current state and commands in the past. This can become a numerically involved process as regressing a policy based on delayed commands is endangered by singular regression matrices. Consequently, it easily happens that one extracts a nonstabilizing policy from the demonstration,which prevents the application of Qlearning and Vlearning as described in Section 2.2.As a result of these considerations, the most trustworthy item to extract from a demonstration is the model of the pole dynamics. In our implementation it was used in two ways, forcalculating the policy as in 6, and in statepredictive control with a Kalman filter toovercome the delays in visual information processing. The model was learned incrementally in realtime by an implementation of RFWR Schaal  Atkeson 1996. Figure 6shows the results of learning from scratch and learning from demonstration of the actualrobot. Without a demonstration, it took about 1020 trials before learning succeeded in reliable performance longer than one minute. With a 30 second long demonstration, learningwas reliably accomplished in one single trial, using a large variety of physically differentpoles and using demonstrations from arbitrary people in the laboratory.Figure 5 Sketch of SARCOSanthropomorphic robot arm773. CONCLUSIONWe discussed learning from demonstration in thecontext of reinforcement learning, focusing on Qlearning, value function learning, and modelbased reinforcement learning. Qlearning andvalue function learning can theoretically profitfrom a demonstration by extracting a policy, byusing the demonstration data to prime the Qvaluefunction, or, in the case of value function learning, by extracting a predictive model of theworld. Only in the special case of LQR problems,however, could we find a significant benefit ofpriming the learner from the demonstration. Incontrast, modelbased reinforcement learning wasable to greatly profit from the demonstration byusing the predictive model of the world formental simulations. In an implementation withan anthropomorphic robot arm, we illustrated that even in LQR problems, modelbasedreinforcement learning offers larger robustness towards the complexity in real learningsystems than Qlearning and value function learning. Using a modelbased strategy, ourrobot learned polebalancing from a demonstration in a single trial with great reliability.The important message of this work is that not every learning approach is equally suited toallow knowledge transfer andor the incorporation of biases. This issue may serve as acritical additional constraint to evaluate artificial and biological models of learning.AcknowledgmentsSupport was provided by the ATR Human Information Processing Labs, the German ResearchAssociation, the Alexander v. Humboldt Foundation, and the German Scholarship Foundation.ReferencesAtkeson, C. G. 1994. Using local trajectory optimizers to speed up global optimization in dynamic programming. In Moody, Hanson,  Lippmann Ed.,Adv. in Neural Inf. Proc. Sys. 6. Morgan Kaufmann.Bakker, P.,  Kuniyoshi, Y. 1996. Robot see, robotdo An overview of robot imitation., AutonomousSystems Section, Electrotechnical Laboratory, TsukubaScience City, Japan.Barto, A. G., Sutton, R. S.,  Anderson, C. W. 1983.Neuronlike adaptive elements that can solve difficultlearning control problems. IEEE Transactions on Systems, Man, and Cybernetics, SMC13, 5.Bradtke, S. J. 1993. Reinforcement learning appliedto linear quadratic regulation. In Hanson, J. S., Cowan,J. D.,  Giles, C. L. Eds., Advances in Neural Inf.Processing Systems 5, pp.295302. Morgan Kaufmann.Dillmann, R., Kaiser, M.,  Ude, A. 1995.Acquisition of elementary robot skills from humandemonstration. In International Symposium on Intelligent Robotic Systems SIRS95, Pisa, Italy.Doya, K. 1996. Temporal difference learning in continuous time and space. In Touretzky, D. S., Mozer,M. C.,  Hasselmo, M. E. Eds., Advances in NeuralInformation Processing Systems 8. MIT Press.Dufay, B.,  Latombe, J.C. 1984. An approach toautomatic robot programming based on inductivelearning. In Brady, M.,  Paul, R. Eds., Robotics Research, pp.97115. Cambridge, MA MIT Press.Dyer, P.,  McReynolds, S. R. 1970. The computationand theory of opitmal control. NY Academic Press.Ikeuchi, K. 1993b. Assembly plan from observation., School of Computer Science, Carnegie MellonUniversity, Pittsburgh, PA.Kawato, M., Gandolfo, F., Gomi, H.,  Wada, Y.1994b. Teaching by showing in kendama based onoptimization principle. In Proceedings of the International Conference on Artificial Neural NetworksICANN94, 1, pp.601606.LozanoPerez, T. 1982. TaskPlanning. In Brady,M., Hollerbach, J. M., Johnson, T. L., LozanoPrez, T., Mason, M. T. Eds., , pp.473498. MIT Press.Miyamoto, H., Schaal, S., Gandolfo, F., Koike, Y., Osu,R., Nakano, E., Wada, Y.,  Kawato, M. in press. AKendama learning robot based on bidirectional theory.Neural Networks.Moore, A. 1991a. Fast, robust adaptive control bylearning only forward models. In Moody, J. E., Hanson, S. J.,  and Lippmann, R. P. Eds., Advances inNeural Inf. Proc. Systems 4. Morgan Kaufmann.Schaal, S.,  Atkeson, C. G. 1996. From isolation tocooperation An alternative of a system of experts. InTouretzky, D. S., Mozer, M. C.,  Hasselmo, M. E.Eds., Advances in Neural Information ProcessingSystems 8. Cambridge, MA MIT Press.Segre, A. B.,  DeJong, G. 1985. Explanationbasedmanipulator learning Acquisition of planning abilitythrough observation. In Conference on Robotics andAutomation, pp.555560.Singh, S. P.,  Sutton, R. S. 1996. Reinforcementlearning with eligibility traces. Machine Learning.Sutton, R. S. 1990. Integrated architectures forlearning, planning, and reacting based on approximatingdynamic programming. In Proceedings of the International Machine Learning Conference.Watkins, C. J. C. H. 1989. Learning with delayed rewards. Ph.D. thesis, Cambridge University UK, .Widrow, B.,  Smith, F. W. 1964. Pattern recognizing control systems. In 1963 Comp. and Inf. SciencesCOINS Symp. Proc., 288317, Washington Spartan.010203040506070801 10 100TupTriala scratchb primed modelFigure 6 Smoothed average of 10 learning curves of the robot for pole balancing.The trials were aborted after successfulbalancing of 60 seconds. We also testedlong term performance of the learningsystem by running pole balancing for overan hourthe pole was never dropped.
