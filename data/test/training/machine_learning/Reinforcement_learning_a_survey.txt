Journal of Articial Intelligence Research
Submitted published
Reinforcement Learning A Survey
Leslie Pack Kaelbling lpkcsbrownedu
Michael L Littman mlittmancsbrownedu
Computer Science Department Box Brown University
Providence RI USA
Andrew W Moore awmcscmuedu
Smith Hall Carnegie Mellon University Forbes Avenue
Pittsburgh PA USA
Abstract
This paper surveys the of reinforcement learning from a computerscience per
spective It is written to be accessible to researchers familiar with machine learning Both
the historical basis of the and a broad selection of current work are summarized
Reinforcement learning is the problem faced by an agent that learns behavior through
trialanderror interactions with a dynamic environment The work described here has a
resemblance to work in psychology but diers considerably in the details and in the use
of the word The paper discusses central issues of reinforcement learning
including trading o exploration and exploitation establishing the foundations of the
via Markov decision theory learning from delayed reinforcement constructing empirical
models to accelerate learning making use of generalization and hierarchy and coping with
hidden state It concludes with a survey of some implemented systems and an assessment
of the practical utility of current methods for reinforcement learning
Introduction
Reinforcement learning dates back to the early days of cybernetics and work in statistics
psychology neuroscience and computer science In the last to ten years it has attracted
rapidly increasing interest in the machine learning and articial intelligence communities
Its promise is beguilinga way of programming agents by reward and punishment without
needing to specify how the task is to be achieved But there are formidable computational
obstacles to fullling the promise
This paper surveys the historical basis of reinforcement learning and some of the current
work from a computer science perspective We give a highlevel overview of the and a
taste of some specic approaches It is of course impossible to mention all of the important
work in the this should not be taken to be an exhaustive account
Reinforcement learning is the problem faced by an agent that must learn behavior
through trialanderror interactions with a dynamic environment The work described here
has a strong family resemblance to eponymous work in psychology but diers considerably
in the details and in the use of the word It is appropriately thought of as
a class of problems rather than as a set of techniques
There are two main strategies for solving reinforcementlearning problems The is to
search in the space of behaviors in order to one that performs well in the environment
This approach has been taken by work in genetic algorithms and genetic programming
c AI Access Foundation and Morgan Kaufmann Publishers All rights reserved
Kaelbling Littman Moore
a
T
s
i
r
B
I
R
Figure
The standard reinforcementlearning model
as well as some more novel search techniques  The second is to use
statistical techniques and dynamic programming methods to estimate the utility of taking
actions in states of the world This paper is devoted almost entirely to the second set of
techniques because they take advantage of the special structure of reinforcementlearning
problems that is not available in optimization problems in general It is not yet clear which
set of approaches is best in which circumstances
The rest of this section is devoted to establishing notation and describing the basic
reinforcementlearning model Section explains the tradeo between exploration and
exploitation and presents some solutions to the most basic case of reinforcementlearning
problems in which we want to maximize the immediate reward Section considers the more
general problem in which rewards can be delayed in time from the actions that were crucial
to gaining them Section considers some classic modelfree algorithms for reinforcement
learning from delayed reward
adaptive heuristic critic TD and Qlearning Section
demonstrates a continuum of algorithms that are sensitive to the amount of computation an
agent can perform between actual steps of action in the environment Generalizationthe
cornerstone of mainstream machine learning researchhas the potential of considerably
aiding reinforcement learning as described in Section Section considers the problems
that arise when the agent does not have complete perceptual access to the state of the
environment Section catalogs some of reinforcement learnings successful applications
Finally Section concludes with some speculations about important open problems and
the future of reinforcement learning
ReinforcementLearning Model
In the standard reinforcementlearning model an agent is connected to its environment
via perception and action as depicted in Figure  On each step of interaction the agent
receives as input i some indication of the current state s of the environment the agent
then chooses an action a to generate as output The action changes the state of the
environment and the value of this state transition is communicated to the agent through
a scalar reinforcement signal r The agents behavior B should choose actions that tend
to increase the longrun sum of values of the reinforcement signal It can learn to do this
over time by systematic trial and error guided by a wide variety of algorithms that are the
subject of later sections of this paper

Reinforcement Learning A Survey
Formally the model consists of
a discrete set of environment states S
a discrete set of agent actions A and
a set of scalar reinforcement signals typically f g or the real numbers
The also includes an input function I which determines how the agent views the
environment state we will assume that it is the identity function is the agent perceives
the exact state of the environment until we consider partial observability in Section
An intuitive way to understand the relation between the agent and its environment is
with the following example dialogue
Environment You are in state You have possible actions
Agent Ill take action
Environment You received a reinforcement of units You are now in state
You have possible actions
Agent Ill take action
Environment You received a reinforcement of units You are now in state
You have possible actions
Agent Ill take action
Environment You received a reinforcement of units You are now in state
You have possible actions


The agents job is to a policy mapping states to actions that maximizes some
longrun measure of reinforcement We expect in general that the environment will be
nondeterministic that is that taking the same action in the same state on two dierent
occasions may result in dierent next states andor dierent reinforcement values This
happens in our example above
from state applying action produces diering rein
forcements and diering states on two occasions However we assume the environment is
stationary that is that the probabilities of making state transitions or receiving specic
reinforcement signals do not change over time
Reinforcement learning diers from the more widely studied problem of supervised learn
ing in several ways The most important dierence is that there is no presentation of in
putoutput pairs Instead after choosing an action the agent is told the immediate reward
and the subsequent state but is not told which action would have been in its best longterm
interests It is necessary for the agent to gather useful experience about the possible system
states actions transitions and rewards actively to act optimally Another dierence from
supervised learning is that online performance is important
the evaluation of the system
is often concurrent with learning
This assumption may be disappointing	 after all
operation in nonstationary environments is one of the
motivations for building learning systems In fact
many of the algorithms described in later sections
are eective in slowlyvarying nonstationary environments
but there is very little theoretical analysis
in this area


Kaelbling Littman Moore
Some aspects of reinforcement learning are closely related to search and planning issues
in articial intelligence AI search algorithms generate a satisfactory trajectory through a
graph of states Planning operates in a similar manner but typically within a construct
with more complexity than a graph in which states are represented by compositions of
logical expressions instead of atomic symbols These AI algorithms are less general than the
reinforcementlearning methods in that they require a predened model of state transitions
and with a few exceptions assume determinism On the other hand reinforcement learning
at least in the kind of discrete cases for which theory has been developed assumes that
the entire state space can be enumerated and stored in memoryan assumption to which
conventional search algorithms are not tied
Models of Optimal Behavior
Before we can start thinking about algorithms for learning to behave optimally we have
to decide what our model of optimality will be In particular we have to specify how the
agent should take the future into account in the decisions it makes about how to behave
now There are three models that have been the subject of the majority of work in this
area
The model is the easiest to think about at a given moment in time the
agent should optimize its expected reward for the next h steps

E
hX
t
rt
it need not worry about what will happen after that In this and subsequent expressions
rt represents the scalar reward received t steps into the future This model can be used in
two ways In the the agent will have a nonstationary policy that is one that changes
over time On its step it will take what is termed a hstep optimal action This is
dened to be the best action available given that it has h steps remaining in which to act
and gain reinforcement On the next step it will take a step optimal action and so
on until it takes a step optimal action and terminates In the second the agent
does recedinghorizon control in which it always takes the hstep optimal action The agent
always acts according to the same policy but the value of h limits how far ahead it looks
in choosing its actions The model is not always appropriate In many cases
we may not know the precise length of the agents life in advance
The innitehorizon discounted model takes the longrun reward of the agent into ac
count but rewards that are received in the future are geometrically discounted according
to discount factor

E

t

We can interpret in several ways It can be seen as an interest rate a probability of living
another step or as a mathematical trick to bound the innite sum The model is conceptu
ally similar to recedinghorizon control but the discounted model is more mathematically
tractable than the model This is a dominant reason for the wide attention
this model has received

Reinforcement Learning A Survey
Another optimality criterion is the averagereward model in which the agent is supposed
to take actions that optimize its longrun average reward

lim
h
E

h
hX
t
rt
Such a policy is referred to as a gain optimal policy it can be seen as the limiting case of
the innitehorizon discounted model as the discount factor approaches
One problem with this criterion is that there is no way to distinguish between two policies
one of which gains a large amount of reward in the initial phases and the other of which
does not Reward gained on any initial prex of the agents life is overshadowed by the
longrun average performance It is possible to generalize this model so that it takes into
account both the long run average and the amount of initial reward than can be gained
In the generalized bias optimal model a policy is preferred if it maximizes the longrun
average and ties are broken by the initial extra reward
Figure contrasts these models of optimality by providing an environment in which
changing the model of optimality changes the optimal policy In this example circles
represent the states of the environment and arrows are state transitions There is only
a single action choice from every state except the start state which is in the upper left
and marked with an incoming arrow All rewards are zero except where marked Under a
model with h the three actions yield rewards of and so
the action should be chosen under an innitehorizon discounted model with
the three choices yield and so the second action should be chosen
and under the average reward model the third action should be chosen since it leads to
an average reward of If we change h to  and to then the second action is
optimal for the model and the for the innitehorizon discounted model
however the average reward model will always prefer the best longterm average Since the
choice of optimality model and parameters matters so much it is important to choose it
carefully in any application
The model is appropriate when the agents lifetime is known one im
portant aspect of this model is that as the length of the remaining lifetime decreases the
agents policy may change A system with a hard deadline would be appropriately modeled
this way The relative usefulness of innitehorizon discounted and biasoptimal models is
still under debate Biasoptimality has the advantage of not requiring a discount parameter
however algorithms for biasoptimal policies are not yet as wellunderstood as those
for optimal innitehorizon discounted policies
Measuring Learning Performance
The criteria given in the previous section can be used to assess the policies learned by a
given algorithm We would also like to be able to evaluate the quality of learning itself
There are several incompatible measures in use
Eventual convergence to optimal Many algorithms come with a provable guar
antee of asymptotic convergence to optimal behavior Dayan  This
is reassuring but useless in practical terms An agent that quickly reaches a plateau

Kaelbling Littman Moore
Finite horizon, h
Infinite horizon,
Average reward



Figure
Comparing models of optimality All unlabeled arrows produce a reward of zero
at of optimality may in many applications be preferable to an agent that has a
guarantee of eventual optimality but a sluggish early learning rate
Speed of convergence to optimality Optimality is usually an asymptotic result
and so convergence speed is an illdened measure More practical is the speed of
convergence to nearoptimality This measure begs the denition of how near to
optimality is sucient A related measure is level of performance after a given time
which similarly requires that someone dene the given time
It should be noted that here we have another dierence between reinforcement learning
and conventional supervised learning In the latter expected future predictive accu
racy or statistical eciency are the prime concerns For example in the wellknown
PAC framework  there is a learning period during which mistakes do
not count then a performance period during which they do The framework provides
bounds on the necessary length of the learning period in order to have a probabilistic
guarantee on the subsequent performance That is usually an inappropriate view for
an agent with a long existence in a complex environment
In spite of the mismatch between embedded reinforcement learning and the traintest
perspective Fiechter provides a PAC analysis for Qlearning in
Section that sheds some light on the connection between the two views
Measures related to speed of learning have an additional weakness An algorithm
that merely tries to achieve optimality as fast as possible may incur unnecessarily
large penalties during the learning period A less aggressive strategy taking longer to
achieve optimality but gaining greater total reinforcement during its learning might
be preferable
Regret A more appropriate measure then is the expected decrease in reward gained
due to executing the learning algorithm instead of behaving optimally from the very
beginning This measure is known as regret Fristedt  It penalizes
mistakes wherever they occur during the run Unfortunately results concerning the
regret of algorithms are quite hard to obtain

Reinforcement Learning A Survey
Reinforcement Learning and Adaptive Control
Adaptive control Graham  Stengel  is also concerned with algo
rithms for improving a sequence of decisions from experience Adaptive control is a much
more mature discipline that concerns itself with dynamic systems in which states and ac
tions are vectors and system dynamics are smooth
linear or locally linearizable around a
desired trajectory A very common formulation of cost functions in adaptive control are
quadratic penalties on deviation from desired state and action vectors Most importantly
although the dynamic model of the system is not known in advance and must be esti
mated from data the structure of the dynamic model is leaving model estimation
as a parameter estimation problem These assumptions permit deep elegant and powerful
mathematical analysis which in turn lead to robust practical and widely deployed adaptive
control algorithms
Exploitation versus Exploration The SingleState Case
One major dierence between reinforcement learning and supervised learning is that a
reinforcementlearner must explicitly explore its environment In order to highlight the
problems of exploration we treat a very simple case in this section The fundamental issues
and approaches described here will in many cases transfer to the more complex instances
of reinforcement learning discussed later in the paper
The simplest possible reinforcementlearning problem is known as the karmed bandit
problem which has been the subject of a great deal of study in the statistics and applied
mathematics literature Fristedt  The agent is in a room with a collection of
k gambling machines called a bandit in colloquial English The agent is
permitted a number of pulls h Any arm may be pulled on each turn The machines
do not require a deposit to play the only cost is in wasting a pull playing a suboptimal
machine When arm i is pulled machine i pays o 	 or according to some underlying
probability parameter pi where payos are independent events and the pis are unknown
What should the agents strategy be
This problem illustrates the fundamental tradeo between exploitation and exploration
The agent might believe that a particular arm has a fairly high payo probability should
it choose that arm all the time or should it choose another one that it has less information
about but seems to be worse Answers to these questions depend on how long the agent
is expected to play the game the longer the game lasts the worse the consequences of
prematurely converging on a suboptimal arm and the more the agent should explore
There is a wide variety of solutions to this problem We will consider a representative
selection of them but for a deeper discussion and a number of important theoretical results
see the book by Berry and Fristedt We use the term to indicate the
agents choice of arm to pull This eases the transition into delayed reinforcement models
in Section It is very important to note that bandit problems our denition of a
reinforcementlearning environment with a single state with only self transitions
Section discusses three solutions to the basic onestate bandit problem that have
formal correctness results Although they can be extended to problems with realvalued
rewards they do not apply directly to the general multistate delayedreinforcement case

Kaelbling Littman Moore
Section presents three techniques that are not formally justied but that have had wide
use in practice and can be applied similar lack of guarantee to the general case
Formally Justied Techniques
There is a fairly welldeveloped formal theory of exploration for very simple problems
Although it is instructive the methods it provides do not scale well to more complex
problems
DynamicProgramming Approach
If the agent is going to be acting for a total of h steps it can use basic Bayesian reasoning
to solve for an optimal strategy Fristedt  This requires an assumed prior
joint distribution for the parameters fpig the most natural of which is that each pi is
independently uniformly distributed between and  We compute a mapping from belief
states of the agents experiences during this run to actions Here a belief state
can be represented as a tabulation of action choices and payos
fn w n w nk wkg
denotes a state of play in which each arm i has been pulled ni times with wi payos We
write V w nk wk as the expected payo remaining given that a total of h pulls
are available and we use the remaining pulls optimally
If
P
i ni h then there are no remaining pulls and V
w nk wk This is
the basis of a recursive denition If we know the V value for all belief states with t pulls
remaining we can compute the V value of any belief state with t 	 pulls remaining

V w nk wk maxiE

Future payo if agent takes action i
then acts optimally for remaining pulls

maxi


wi ni  wi  nk wk
wi ni  wi nk wk

where is the posterior subjective probability of action i paying o given ni wi and
our prior probability For the uniform priors which result in a beta distribution
ni
The expense of in the table of V values in this way for all attainable belief states
is linear in the number of belief states times actions and thus exponential in the horizon
Gittins Allocation Indices
Gittins gives an index method for the optimal choice of action at each
step in karmed bandit problems  The technique only applies under the
discounted expected reward criterion For each action consider the number of times it has
been chosen n versus the number of times it has paid o w For certain discount factors
there are published tables of values In w for each pair of n and w Look up
the index value for each action i Ini wi It represents a comparative measure of the
combined value of the expected payo of action i its history of payos and the value
of the information that we would get by choosing it Gittins has shown that choosing the
action with the largest index value guarantees the optimal balance between exploration and
exploitation

Reinforcement Learning A Survey
1 2 3 N-1 N 2N 2N-1 N+3 N+2 N
a = 0 a =
r =
r =
1 2 3 N-1 N 2N 2N-1 N+3 N+2 N
a = 0 a =
Figure
A Tsetlin automaton with states The top row shows the state transitions
that are made when the previous action resulted in a reward of  the bottom
row shows transitions after a reward of In states in the left half of the
action is taken in those on the right action 	 is taken
Because of the guarantee of optimal exploration and the simplicity of the technique
the table of index values this approach holds a great deal of promise for use in more
complex applications This method proved useful in an application to robotic manipulation
with immediate reward Ungar  Unfortunately no one has yet been
able to an analog of index values for delayed reinforcement problems
Learning Automata
A branch of the theory of adaptive control is devoted to learning automata surveyed by
Narendra and Thathachar which were originally described explicitly as state
automata The Tsetlin automaton shown in Figure provides an example that solves a
bandit arbitrarily near optimally as N approaches innity
It is inconvenient to describe algorithms as automata so a move was made
to describe the internal state of the agent as a probability distribution according to which
actions would be chosen The probabilities of taking dierent actions would be adjusted
according to their previous successes and failures
An example which stands among a set of algorithms independently developed in the
mathematical psychology literature Bower  is the linear rewardinaction
algorithm Let pi be the agents probability of taking action i
When action ai succeeds
pi
pi pi
pj
pj for j i
When action ai fails pj remains unchanged all j
This algorithm converges with probability 	 to a vector containing a single 	 and the
rest a particular action with probability  Unfortunately it does not always
converge to the correct action but the probability that it converges to the wrong one can
be made arbitrarily small by making small Thathachar  There is no
literature on the regret of this algorithm

Kaelbling Littman Moore
AdHoc Techniques
In reinforcementlearning practice some simple ad hoc strategies have been popular They
are rarely if ever the best choice for the models of optimality we have used but they may
be viewed as reasonable computationally tractable heuristics Thrun has surveyed
a variety of these techniques
Greedy Strategies
The strategy that comes to mind is to always choose the action with the highest esti
mated payo The is that early unlucky sampling might indicate that the best actions
reward is less than the reward obtained from a suboptimal action The suboptimal action
will always be picked leaving the true optimal action starved of data and its superiority
never discovered An agent must explore to ameliorate this outcome
A useful heuristic is optimism in the face of uncertainty in which actions are selected
greedily but strongly optimistic prior beliefs are put on their payos so that strong negative
evidence is needed to eliminate an action from consideration This still has a measurable
danger of starving an optimal but unlucky action but the risk of this can be made arbitrar
ily small Techniques like this have been used in several reinforcement learning algorithms
including the interval exploration method b shortly the ex
ploration bonus in Dyna  curiositydriven exploration a
and the exploration mechanism in prioritized sweeping Atkeson
Randomized Strategies
Another simple exploration strategy is to take the action with the best estimated expected
reward by default but with probability p choose an action at random Some versions of
this strategy start with a large value of p to encourage initial exploration which is slowly
decreased
An objection to the simple strategy is that when it experiments with a nongreedy action
it is no more likely to try a promising alternative than a clearly hopeless alternative A
slightly more sophisticated strategy is Boltzmann exploration In this case the expected
reward for taking action a ERa is used to choose an action probabilistically according to
the distribution
P
eERaTP
aA e
ERaT

The temperature parameter T can be decreased over time to decrease exploration This
method works well if the best action is well separated from the others but suers somewhat
when the values of the actions are close It may also converge unnecessarily slowly unless
the temperature schedule is manually tuned with great care
Intervalbased Techniques
Exploration is often more ecient when it is based on secondorder information about the
certainty or variance of the estimated values of actions Kaelblings interval estimation
algorithm stores statistics for each action ai
wi is the number of successes and ni
the number of trials An action is chosen by computing the upper bound of a

Reinforcement Learning A Survey
condence interval on the success probability of each action and choosing the action with
the highest upper bound Smaller values of the parameter encourage greater exploration
When payos are boolean the normal approximation to the binomial distribution can be
used to construct the condence interval the binomial should be used for small
n Other payo distributions can be handled using their associated statistics or with
nonparametric methods The method works very well in empirical trials It is also related
to a certain class of statistical techniques known as experiment design methods
Draper  which are used for comparing multiple treatments example fertilizers
or drugs to determine which treatment any is best in as small a set of experiments as
possible
More General Problems
When there are multiple states but reinforcement is still immediate then any of the above
solutions can be replicated once for each state However when generalization is required
these solutions must be integrated with generalization methods section this is
straightforward for the simple adhoc methods but it is not understood how to maintain
theoretical guarantees
Many of these techniques focus on converging to some regime in which exploratory
actions are taken rarely or never this is appropriate when the environment is stationary
However when the environment is nonstationary exploration must continue to take place
in order to notice changes in the world Again the more adhoc techniques can be modied
to deal with this in a plausible manner temperature parameters from going to decay
the statistics in interval estimation but none of the theoretically guaranteed methods can
be applied
Delayed Reward
In the general case of the reinforcement learning problem the agents actions determine
not only its immediate reward but also least probabilistically the next state of the
environment Such environments can be thought of as networks of bandit problems but
the agent must take into account the next state as well as the immediate reward when it
decides which action to take The model of longrun optimality the agent is using determines
exactly how it should take the value of the future into account The agent will have to be
able to learn from delayed reinforcement
it may take a long sequence of actions receiving
insignicant reinforcement then arrive at a state with high reinforcement The agent
must be able to learn which of its actions are desirable based on reward that can take place
arbitrarily far in the future
Markov Decision Processes
Problems with delayed reinforcement are well modeled asMarkov decision processes
An MDP consists of
a set of states S
a set of actions A

Kaelbling Littman Moore
a reward function R
S and
a state transition function T
SA where a member of is a probability
distribution over the set S it maps states to probabilities We write T a s
for the probability of making a transition from state s to state s using action a
The state transition function probabilistically species the next state of the environment as
a function of its current state and the agents action The reward function species expected
instantaneous reward as a function of the current state and action The model is Markov if
the state transitions are independent of any previous environment states or agent actions
There are many good references toMDP models  Bertsekas  Howard
Puterman
Although general MDPs may have innite uncountable state and action spaces
we will only discuss methods for solving and problems In section
we discuss methods for solving problems with continuous input and output spaces
Finding a Policy Given a Model
Before we consider algorithms for learning to behave in MDP environments we will ex
plore techniques for determining the optimal policy given a correct model These dynamic
programming techniques will serve as the foundation and inspiration for the learning al
gorithms to follow We restrict our attention mainly to optimal policies for the
innitehorizon discounted model but most of these algorithms have analogs for the
horizon and averagecase models as well We rely on the result that for the innitehorizon
discounted model there exists an optimal deterministic stationary policy
We will speak of the optimal value of a stateit is the expected innite discounted sum
of reward that the agent will gain if it starts in that state and executes the optimal policy
Using as a complete decision policy it is written
V max

E


t



This optimal value function is unique and can be dened as the solution to the simultaneous
equations
V max
a

a X
sS
T a sV

A 	 S
which assert that the value of a state s is the expected instantaneous reward plus the
expected discounted value of the next state using the best available action Given the
optimal value function we can specify the optimal policy as
argmax
a

a X
sS
T a sV

A
Value Iteration
One way then to an optimal policy is to the optimal value function It can
be determined by a simple iterative algorithm called value iteration that can be shown to
converge to the correct V values  Bertsekas

Reinforcement Learning A Survey
initialize V arbitrarily
loop until policy good enough
loop for s 	 S
loop for a 	 A
Qs a
Rs a
P
sS T a s

V
maxaQs a
end loop
end loop
It is not obvious when to stop the value iteration algorithm One important result
bounds the performance of the current greedy policy as a function of the Bellman residual of
the current value function Baird b It says that if the maximum dierence
between two successive value functions is less than  then the value of the greedy policy
policy obtained by choosing in every state the action that maximizes the estimated
discounted reward using the current estimate of the value function diers from the value
function of the optimal policy by no more than at any state This provides an
eective stopping criterion for the algorithm Puterman discusses another stopping
criterion based on the span seminorm which may result in earlier termination Another
important result is that the greedy policy is guaranteed to be optimal in some number
of steps even though the value function may not have converged  And in
practice the greedy policy is often optimal long before the value function has converged
Value iteration is very The assignments to V need not be done in strict order
as shown above but instead can occur asynchronously in parallel provided that the value
of every state gets updated innitely often on an innite run These issues are treated
extensively by Bertsekas who also proves convergence results
Updates based on Equation 	 are known as full backups since they make use of infor
mation from all possible successor states It can be shown that updates of the form
Qs a
Qs a
a
Qs a Qs a
can also be used as long as each pairing of a and s is updated innitely often s is sampled
from the distribution T a s r is sampled with mean Rs a and bounded variance and
the learning rate is decreased slowly This type of sample backup  is critical
to the operation of the modelfree methods discussed in the next section
The computational complexity of the valueiteration algorithm with full backups per
iteration is quadratic in the number of states and linear in the number of actions Com
monly the transition probabilities T a s are sparse If there are on average a constant
number of next states with nonzero probability then the cost per iteration is linear in the
number of states and linear in the number of actions The number of iterations required to
reach the optimal value function is polynomial in the number of states and the magnitude
of the largest reward if the discount factor is held constant However in the worst case
the number of iterations grows polynomially in  so the convergence rate slows
considerably as the discount factor approaches 	 Dean Kaelbling b


Kaelbling Littman Moore
Policy Iteration
The policy iteration algorithm manipulates the policy directly rather than it indi
rectly via the optimal value function It operates as follows

choose an arbitrary policy
loop


compute the value function of policy

solve the linear equations
Vs Rs
P
sS T s

improve the policy at each state

argmaxa a
P
sS T a s


until
The value function of a policy is just the expected innite discounted reward that will
be gained at each state by executing that policy It can be determined by solving a set
of linear equations Once we know the value of each state under the current policy we
consider whether the value could be improved by changing the action taken If it can
we change the policy to take the new action whenever it is in that situation This step is
guaranteed to strictly improve the performance of the policy When no improvements are
possible then the policy is guaranteed to be optimal
Since there are at most jAjjSj distinct policies and the sequence of policies improves at
each step this algorithm terminates in at most an exponential number of iterations
man  However it is an important open question how many iterations policy iteration
takes in the worst case It is known that the running time is pseudopolynomial and that for
any discount factor there is a polynomial bound in the total size of theMDP
et al b
Enhancement to Value Iteration and Policy Iteration
In practice value iteration is much faster per iteration but policy iteration takes fewer
iterations Arguments have been put forth to the eect that each approach is better for
large problems Putermans modied policy iteration algorithm Shin
provides a method for trading iteration time for iteration improvement in a smoother way
The basic idea is that the expensive part of policy iteration is solving for the exact value
of V Instead of an exact value for V we can perform a few steps of a modied
valueiteration step where the policy is held over successive iterations This can be
shown to produce an approximation to V that converges linearly in In practice this can
result in substantial speedups
Several standard numericalanalysis techniques that speed the convergence of dynamic
programming can be used to accelerate value and policy iteration Multigrid methods can
be used to quickly seed a good initial approximation to a high resolution value function
by initially performing value iteration at a coarser resolution ude  State aggre
gation works by collapsing groups of states to a single metastate solving the abstracted
problem Castanon

Reinforcement Learning A Survey
Computational Complexity
Value iteration works by producing successive approximations of the optimal value function
Each iteration can be performed in OjAjjSj steps or faster if there is sparsity in the
transition function However the number of iterations required can grow exponentially in
the discount factor  as the discount factor approaches  the decisions must
be based on results that happen farther and farther into the future In practice policy
iteration converges in fewer iterations than value iteration although the periteration costs
of OjAjjSj jSj can be prohibitive There is no known tight worstcase bound available
for policy iteration et al b Modied policy iteration Shin
seeks a tradeo between cheap and eective iterations and is preferred by some
practictioners
Linear programming  is an extremely general problem and MDPs can
be solved by generalpurpose linearprogramming packages  DEpenoux
Homan Karp  An advantage of this approach is that commercialquality
linearprogramming packages are available although the time and space requirements can
still be quite high From a theoretic perspective linear programming is the only known
algorithm that can solve MDPs in polynomial time although the theoretically ecient
algorithms have not been shown to be ecient in practice
Learning an Optimal Policy Modelfree Methods
In the previous section we reviewed methods for obtaining an optimal policy for an MDP
assuming that we already had a model The model consists of knowledge of the state tran
sition probability function T a s and the reinforcement function Rs a Reinforcement
learning is primarily concerned with how to obtain the optimal policy when such a model
is not known in advance The agent must interact with its environment directly to obtain
information which by means of an appropriate algorithm can be processed to produce an
optimal policy
At this point there are two ways to proceed
Modelfree Learn a controller without learning a model
Modelbased Learn a model and use it to derive a controller
Which approach is better This is a matter of some debate in the reinforcementlearning
community A number of algorithms have been proposed on both sides This question also
appears in other such as adaptive control where the dichotomy is between direct and
indirect adaptive control
This section examines modelfree learning and Section examines modelbased meth
ods
The biggest problem facing a reinforcementlearning agent is temporal credit assignment
How do we know whether the action just taken is a good one when it might have far
reaching eects One strategy is to wait until the and reward the actions taken if
the result was good and punish them if the result was bad In ongoing tasks it is dicult
to know what the is and this might require a great deal of memory Instead we
will use insights from value iteration to adjust the estimated value of a state based on

Kaelbling Littman Moore
AHC
RL
vs
r
a
Figure
Architecture for the adaptive heuristic critic
the immediate reward and the estimated value of the next state This class of algorithms
is known as temporal dierence methods  We will consider two dierent
temporaldierence learning strategies for the discounted innitehorizon model
Adaptive Heuristic Critic and TD
The adaptive heuristic critic algorithm is an adaptive version of policy iteration
Sutton Anderson  in which the valuefunction computation is no longer imple
mented by solving a set of linear equations but is instead computed by an algorithm called
TD A block diagram for this approach is given in Figure It consists of two compo
nents
a critic AHC and a reinforcementlearning component RL The
reinforcementlearning component can be an instance of any of the karmed bandit algo
rithms modied to deal with multiple states and nonstationary rewards But instead of
acting to maximize instantaneous reward it will be acting to maximize the heuristic value
v that is computed by the critic The critic uses the real external reinforcement signal to
learn to map states to their expected discounted values given that the policy being executed
is the one currently instantiated in the RL component
We can see the analogy with modied policy iteration if we imagine these components
working in alternation The policy implemented by RL is and the critic learns the
value function V for that policy Now we the critic and let the RL component learn a
new policy that maximizes the new value function and so on In most implementations
however both components operate simultaneously Only the alternating implementation
can be guaranteed to converge to the optimal policy under appropriate conditions Williams
and Baird explored the convergence properties of a class of AHCrelated algorithms they
call variants of policy iteration Baird a
It remains to explain how the critic can learn the value of a policy We dene hs a r si
to be an experience tuple summarizing a single transition in the environment Here s is the
agents state before the transition a is its choice of action r the instantaneous reward it
receives and s its resulting state The value of a policy is learned using Suttons TD
algorithm  which uses the update rule
V
V V
Whenever a state s is visited its estimated value is updated to be closer to r
since r is the instantaneous reward received and V is the estimated value of the actually
occurring next state This is analogous to the samplebackup rule from value iterationthe
only dierence is that the sample is drawn from the real world rather than by simulating
a known model The key idea is that r is a sample of the value of V and it is

Reinforcement Learning A Survey
more likely to be correct because it incorporates the real r If the learning rate is adjusted
properly must be slowly decreased and the policy is held TD is guaranteed to
converge to the optimal value function
The TD rule as presented above is really an instance of a more general class of
algorithms called TD with TD looks only one step ahead when adjusting
value estimates although it will eventually arrive at the correct answer it can take quite a
while to do so The general TD rule is similar to the TD rule given above
V
V V
but it is applied to every state according to its eligibility eu rather than just to the
immediately previous state s One version of the eligibility trace is dened to be
es
tX
k

ssk where
ssk

if s sk
otherwise

The eligibility of a state s is the degree to which it has been visited in the recent past
when a reinforcement is received it is used to update all the states that have been recently
visited according to their eligibility When this is equivalent to TD When
it is roughly equivalent to updating all the states according to the number of times they
were visited by the end of a run Note that we can update the eligibility online as follows

es


if s current state
otherwise

It is computationally more expensive to execute the general TD though it often
converges considerably faster for large  Dayan Sejnowski  There
has been some recent work on making the updates more ecient Mulawka
and on changing the denition to make TD more consistent with the certaintyequivalent
method Sutton  which is discussed in Section
Qlearning
The work of the two components of AHC can be accomplished in a unied manner by
Watkins Qlearning algorithm  Watkins Dayan  Qlearning is
typically easier to implement In order to understand Qlearning we have to develop some
additional notation Let Qs a be the expected discounted reinforcement of taking action
a in state s then continuing by choosing actions optimally Note that V is the value
of s assuming the best action is taken initially and so V maxaQ
a Qs a can
hence be written recursively as
Qs a Rs a
X
sS
T a smax
a
Qs a
Note also that since V maxaQs a we have argmaxaQs a as an
optimal policy
Because the Q function makes the action explicit we can estimate the Q values on
line using a method essentially the same as TD but also use them to dene the policy

Kaelbling Littman Moore
because an action can be chosen just by taking the one with the maximum Q value for the
current state
The Qlearning rule is
Qs a
Qs a
a
Qs a Qs a
where hs a r si is an experience tuple as described earlier If each action is executed in
each state an innite number of times on an innite run and is decayed appropriately the
Q values will converge with probability 	 to Q  Tsitsiklis  Jaakkola
Jordan Singh  Qlearning can also be extended to update states that occurred
more than one step previously as in TD Williams
When the Q values are nearly converged to their optimal values it is appropriate for
the agent to act greedily taking in each situation the action with the highest Q value
During learning however there is a dicult exploitation versus exploration tradeo to be
made There are no good formally justied approaches to this problem in the general case
standard practice is to adopt one of the ad hoc methods discussed in section
AHC architectures seem to be more dicult to work with than Qlearning on a practical
level It can be hard to get the relative learning rates right in AHC so that the two
components converge together In addition Qlearning is exploration insensitive
that
is that the Q values will converge to the optimal values independent of how the agent
behaves while the data is being collected long as all stateaction pairs are tried often
enough This means that although the explorationexploitation issue must be addressed
in Qlearning the details of the exploration strategy will not aect the convergence of the
learning algorithm For these reasons Qlearning is the most popular and seems to be the
most eective modelfree algorithm for learning from delayed reinforcement It does not
however address any of the issues involved in generalizing over large state andor action
spaces In addition it may converge quite slowly to a good policy
Modelfree Learning With Average Reward
As described Qlearning can be applied to discounted innitehorizon MDPs It can also
be applied to undiscounted problems as long as the optimal policy is guaranteed to reach a
rewardfree absorbing state and the state is periodically reset
Schwartz examined the problem of adapting Qlearning to an averagereward
framework Although his Rlearning algorithm seems to exhibit convergence problems for
some MDPs several researchers have found the averagereward criterion closer to the true
problem they wish to solve than a discounted criterion and therefore prefer Rlearning to
Qlearning
With that in mind researchers have studied the problem of learning optimal average
reward policies Mahadevan surveyed modelbased averagereward algorithms from
a reinforcementlearning perspective and found several diculties with existing algorithms
In particular he showed that existing reinforcementlearning algorithms for average reward
some dynamic programming algorithms do not always produce biasoptimal poli
cies Jaakkola Jordan and Singh described an averagereward learning algorithm
with guaranteed convergence properties It uses a MonteCarlo component to estimate the
expected future reward for each state as the agent moves through the environment In

Reinforcement Learning A Survey
addition Bertsekas presents a Qlearninglike algorithm for averagecase reward in his new
textbook Although this recent work provides a much needed theoretical foundation
to this area of reinforcement learning many important problems remain unsolved
Computing Optimal Policies by Learning Models
The previous section showed how it is possible to learn an optimal policy without knowing
the models T a s or Rs a and without even learning those models en route Although
many of these methods are guaranteed to optimal policies eventually and use very
little computation time per experience they make extremely inecient use of the data they
gather and therefore often require a great deal of experience to achieve good performance
In this section we still begin by assuming that we dont know the models in advance but
we examine algorithms that do operate by learning these models These algorithms are
especially important in applications in which computation is considered to be cheap and
realworld experience costly
Certainty Equivalent Methods
We begin with the most conceptually straightforward method
learn the T and R
functions by exploring the environment and keeping statistics about the results of each
action next compute an optimal policy using one of the methods of Section This
method is known as certainty equivlance Varaiya
There are some serious objections to this method

It makes an arbitrary division between the learning phase and the acting phase
How should it gather data about the environment initially Random exploration
might be dangerous and in some environments is an immensely inecient method of
gathering data requiring exponentially more data  than a system
that interleaves experience gathering with policybuilding more tightly
Simmons  See Figure for an example
The possibility of changes in the environment is also problematic Breaking up an
agents life into a pure learning and a pure acting phase has a considerable risk that
the optimal controller based on early life becomes without detection a suboptimal
controller if the environment changes
A variation on this idea is certainty equivalence in which the model is learned continually
through the agents lifetime and at each step the current model is used to compute an
optimal policy and value function This method makes very eective use of available data
but still ignores the question of exploration and is extremely computationally demanding
even for fairly small state spaces Fortunately there are a number of other modelbased
algorithms that are more practical
Dyna
Suttons Dyna architecture  exploits a middle ground yielding strategies that
are both more eective than modelfree learning and more computationally ecient than

Kaelbling Littman Moore
. . . . . . . Goal1 2 3 n
Figure
In this environment due to Whitehead random exploration would take
take On steps to reach the goal even once whereas a more intelligent explo
ration strategy any untried action leads directly to goal would
require only On steps
the certaintyequivalence approach It simultaneously uses experience to build a model T
and R uses experience to adjust the policy and uses the model to adjust the policy
Dyna operates in a loop of interaction with the environment Given an experience tuple
hs a s ri it behaves as follows

Update the model incrementing statistics for the transition from s to s on action a
and for receiving reward r for taking action a in state s The updated models are T
and R
Update the policy at state s based on the newly updated model using the rule
Qs a
Rs a
X
s
"T a smax
a
Qs a
which is a version of the valueiteration update for Q values
Perform k additional updates
choose k stateaction pairs at random and update them
according to the same rule as before

Qsk ak
Rsk ak
X
s
"T ak s

a
Qs a
Choose an action a to perform in state s based on the Q values but perhaps modied
by an exploration strategy
The Dyna algorithm requires about k times the computation of Qlearning per instance
but this is typically vastly less than for the naive modelbased method A reasonable value
of k can be determined based on the relative speeds of computation and of taking action
Figure shows a grid world in which in each cell the agent has four actions S E
W and transitions are made deterministically to an adjacent cell unless there is a block
in which case no movement occurs As we will see in Table  Dyna requires an order of
magnitude fewer steps of experience than does Qlearning to arrive at an optimal policy
Dyna requires about six times more computational eort however

Reinforcement Learning A Survey
Figure
A grid world This was formulated as a shortestpath reinforcement
learning problem which yields the same result as if a reward of 	 is given at the
goal a reward of zero elsewhere and a discount factor is used
Steps before Backups before
convergence convergence
Qlearning
Dyna
prioritized sweeping
Table
The performance of three algorithms described in the text All methods used
the exploration heuristic of in the face of uncertainty
any state not
previously visited was assumed by default to be a goal state Qlearning used
its optimal learning rate parameter for a deterministic maze
Dyna and
prioritized sweeping were permitted to take k backups per transition For
prioritized sweeping the priority queue often emptied before all backups were
used

Kaelbling Littman Moore
Prioritized Sweeping 	 QueueDyna
Although Dyna is a great improvement on previous methods it suers from being relatively
undirected It is particularly unhelpful when the goal has just been reached or when the
agent is stuck in a dead end it continues to update random stateaction pairs rather than
concentrating on the parts of the state space These problems are addressed
by prioritized sweeping Atkeson  and QueueDyna Williams
which are two independentlydeveloped but very similar techniques We will describe
prioritized sweeping in some detail
The algorithm is similar to Dyna except that updates are no longer chosen at random
and values are now associated with states in value iteration instead of stateaction pairs
in Qlearning To make appropriate choices we must store additional information in
the model Each state remembers its predecessors
the states that have a nonzero transition
probability to it under some action In addition each state has a priority initially set to
zero
Instead of updating k random stateaction pairs prioritized sweeping updates k states
with the highest priority For each highpriority state s it works as follows

Remember the current value of the state
Vold V
Update the states value
V
max
a

Rs a
X
s
"T a sV


Set the states priority back to
Compute the value change # jVold V
Use # to modify the priorities of the predecessors of s
If we have updated the V value for state s and it has changed by amount  then the
immediate predecessors of s are informed of this event Any state s for which there exists
an action a such that "T a s has its priority promoted to # "T a s unless its
priority already exceeded that value
The global behavior of this algorithm is that when a realworld transition is
agent happens upon a goal state for instance then lots of computation is directed
to propagate this new information back to relevant predecessor states When the real
world transition is actual result is very similar to the predicted result then
computation continues in the most deserving part of the space
Running prioritized sweeping on the problem in Figure we see a large improvement
over Dyna The optimal policy is reached in about half the number of steps of experience
and onethird the computation as Dyna required therefore about times fewer steps
and twice the computational eort of Qlearning

Reinforcement Learning A Survey
Other ModelBased Methods
Methods proposed for solving MDPs given a model can be used in the context of model
based methods as well
RTDP dynamic programming Bradtke Singh  is another
modelbased method that uses Qlearning to concentrate computational eort on the areas
of the statespace that the agent is most likely to occupy It is specic to problems in which
the agent is trying to achieve a particular goal state and the reward everywhere else is
By taking into account the start state it can a short path from the start to the goal
without necessarily visiting the rest of the state space
The Plexus planning system Kaelbling Kirman Nicholson  Kirman
exploits a similar intuition It starts by making an approximate version of the MDP
which is much smaller than the original one The approximateMDP contains a set of states
called the envelope that includes the agents current state and the goal state if there is one
States that are not in the envelope are summarized by a single state The planning
process is an alternation between an optimal policy on the approximate MDP and
adding useful states to the envelope Action may take place in parallel with planning in
which case irrelevant states are also pruned out of the envelope
Generalization
All of the previous discussion has tacitly assumed that it is possible to enumerate the state
and action spaces and store tables of values over them Except in very small environments
this means impractical memory requirements It also makes inecient use of experience In
a large smooth state space we generally expect similar states to have similar values and sim
ilar optimal actions Surely therefore there should be some more compact representation
than a table Most problems will have continuous or large discrete state spaces some will
have large or continuous action spaces The problem of learning in large spaces is addressed
through generalization techniques which allow compact storage of learned information and
transfer of knowledge between states and actions
The large literature of generalization techniques from inductive concept learning can be
applied to reinforcement learning However techniques often need to be tailored to specic
details of the problem In the following sections we explore the application of standard
functionapproximation techniques adaptive resolution models and hierarchical methods
to the problem of reinforcement learning
The reinforcementlearning architectures and algorithms discussed above have included
the storage of a variety of mappings including S A S functions
S A functions and rewards S A S transitions and S
A S  	% probabilities Some of these mappings such as transitions and
immediate rewards can be learned using straightforward supervised learning and can be
handled using any of the wide variety of functionapproximation techniques for supervised
learning that support noisy training examples Popular techniques include various neural
network methods McClelland  fuzzy logic  Lee
CMAC  and local memorybased methods Atkeson Schaal
such as generalizations of nearest neighbor methods Other mappings especially the policy


Kaelbling Littman Moore
mapping typically need specialized algorithms because training sets of inputoutput pairs
are not available
Generalization over Input
A reinforcementlearning agents current state plays a central role in its selection of reward
maximizing actions Viewing the agent as a statefree black box a description of the
current state is its input Depending on the agent architecture its output is either an
action selection or an evaluation of the current state that can be used to select an action
The problem of deciding how the dierent aspects of an input aect the value of the output
is sometimes called the creditassignment problem This section examines
approaches to generating actions or evaluations as a function of a description of the agents
current state
The group of techniques covered here is specialized to the case when reward is not
delayed the second group is more generally applicable
Immediate Reward
When the agents actions do not inuence state transitions the resulting problem becomes
one of choosing actions to maximize immediate reward as a function of the agents current
state These problems bear a resemblance to the bandit problems discussed in Section
except that the agent should condition its action selection on the current state For this
reason this class of problems has been described as associative reinforcement learning
The algorithms in this section address the problem of learning from immediate boolean
reinforcement where the state is vector valued and the action is a boolean vector Such
algorithms can and have been used in the context of a delayed reinforcement for instance
as the RL component in the AHC architecture described in Section They can also be
generalized to realvalued reward through reward comparison methods
CRBP The complementary reinforcement backpropagation algorithm Littman
consists of a feedforward network mapping an encoding of the state to an
encoding of the action The action is determined probabilistically from the activation of
the output units
if output unit i has activation yi then bit i of the action vector has value
with probability yi and otherwise Any neuralnetwork supervised training procedure
can be used to adapt the network as follows If the result of generating action a is r
then the network is trained with inputoutput pair hs ai If the result is r then the
network is trained with inputoutput pair hs ampai where &a a  an
The idea behind this training rule is that whenever an action fails to generate reward
crbp will try to generate an action that is dierent from the current choice Although it
seems like the algorithm might oscillate between an action and its complement that does
not happen One step of training a network will only change the action slightly and since
the output probabilities will tend to move toward this makes action selection more
random and increases search The hope is that the random distribution will generate an
action that works better and then that action will be reinforced
ARC The associative reinforcement comparison algorithm  is an
instance of the ahc architecture for the case of boolean actions consisting of two feed

Reinforcement Learning A Survey
forward networks One learns the value of situations the other learns a policy These can
be simple linear networks or can have hidden units
In the simplest case the entire system learns only to optimize immediate reward First
let us consider the behavior of the network that learns the policy a mapping from a vector
describing s to a or  If the output unit has activation yi then a the action generated
will be 	 if y where is normal noise and otherwise
The adjustment for the output unit is in the simplest case
e ra
where the factor is the reward received for taking the most recent action and the second
encodes which action was taken The actions are encoded as and  so a  always has
the same magnitude if the reward and the action have the same sign then action 	 will be
made more likely otherwise action will be
As described the network will tend to seek actions that given positive reward To extend
this approach to maximize reward we can compare the reward to some baseline b This
changes the adjustment to
e ba
where b is the output of the second network The second network is trained in a standard
supervised mode to estimate r as a function of the input state s
Variations of this approach have been used in a variety of applications
Barto et al  Lin b Sutton
REINFORCE Algorithms Williams  studied the problem of choosing ac
tions to maximize immedate reward He identied a broad class of update rules that per
form gradient descent on the expected reward and showed how to integrate these rules with
backpropagation This class called reinforce algorithms includes linear rewardinaction
as a special case
The generic reinforce update for a parameter wij can be written
#wij bij

lngj
where is a nonnegative factor r the current reinforcement bij a reinforcement baseline
and gi is the probability density function used to randomly generate actions based on unit
activations Both and bij can take on dierent values for each wij however when
is constant throughout the system the expected update is exactly in the direction of the
expected reward gradient Otherwise the update is in the same half space as the gradient
but not necessarily in the direction of steepest increase
Williams points out that the choice of baseline bij can have a profound eect on the
convergence speed of the algorithm
LogicBased Methods Another strategy for generalization in reinforcement learning is
to reduce the learning problem to an associative problem of learning boolean functions
A boolean function has a vector of boolean inputs and a single boolean output Taking
inspiration from mainstream machine learning work Kaelbling developed two algorithms
for learning boolean functions from reinforcement
one uses the bias of kDNF to drive

Kaelbling Littman Moore
the generalization process b the other searches the space of syntactic
descriptions of functions using a simple generateandtest method a
The restriction to a single boolean output makes these techniques dicult to apply In
very benign learning situations it is possible to extend this approach to use a collection
of learners to independently learn the individual bits that make up a complex output In
general however that approach suers from the problem of very unreliable reinforcement

if a single learner generates an inappropriate output bit all of the learners receive a low
reinforcement value The cascademethod b allows a collection of learners
to be trained collectively to generate appropriate joint outputs it is considerably more
reliable but can require additional computational eort
Delayed Reward
Another method to allow reinforcementlearning techniques to be applied in large state
spaces is modeled on value iteration and Qlearning Here a function approximator is used
to represent the value function by mapping a state description to a value
Many reseachers have experimented with this approach
Boyan and Moore used
local memorybased methods in conjunction with value iteration Lin used backprop
agation networks for Qlearning Watkins used CMAC for Qlearning Tesauro
used backpropagation for learning the value function in backgammon in
Section Zhang and Dietterich used backpropagation and TD to learn good
strategies for jobshop scheduling
Although there have been some positive examples in general there are unfortunate in
teractions between function approximation and the learning rules In discrete environments
there is a guarantee that any operation that updates the value function to the
Bellman equations can only reduce the error between the current value function and the
optimal value function This guarantee no longer holds when generalization is used These
issues are discussed by Boyan and Moore who give some simple examples of value
function errors growing arbitrarily large when generalization is used with value iteration
Their solution to this applicable only to certain classes of problems discourages such diver
gence by only permitting updates whose estimated values can be shown to be nearoptimal
via a battery of MonteCarlo experiments
Thrun and Schwartz theorize that function approximation of value functions
is also dangerous because the errors in value functions due to generalization can become
compounded by the operator in the denition of the value function
Several recent results  Tsitsiklis Van Roy  show how the appro
priate choice of function approximator can guarantee convergence though not necessarily to
the optimal values Bairds residual gradient technique  provides guaranteed
convergence to locally optimal solutions
Perhaps the gloominess of these counterexamples is misplaced Boyan and Moore
report that their counterexamples can be made to work with problemspecic handtuning
despite the unreliability of untuned algorithms that provably converge in discrete domains
Sutton shows how modied versions of Boyan and Moores examples can converge
successfully An open question is whether general principles ideally supported by theory
can help us understand when value function approximation will succeed In Suttons com

Reinforcement Learning A Survey
parative experiments with Boyan and Moores counterexamples he changes four aspects
of the experiments

Small changes to the task specications
A very dierent kind of function approximator  that has weak
generalization
A dierent learning algorithm
SARSA Niranjan  instead of value
iteration
A dierent training regime Boyan and Moore sampled states uniformly in state space
whereas Suttons method sampled along empirical trajectories
There are intuitive reasons to believe that the fourth factor is particularly important but
more careful research is needed
Adaptive Resolution Models In many cases what we would like to do is partition
the environment into regions of states that can be considered the same for the purposes of
learning and generating actions Without detailed prior knowledge of the environment it
is very dicult to know what granularity or placement of partitions is appropriate This
problem is overcome in methods that use adaptive resolution during the course of learning
a partition is constructed that is appropriate to the environment
Decision Trees In environments that are characterized by a set of boolean or discrete
valued variables it is possible to learn compact decision trees for representing Q values The
Glearning algorithm Kaelbling  works as follows It starts by assuming
that no partitioning is necessary and tries to learn Q values for the entire environment as
if it were one state In parallel with this process it gathers statistics based on individual
input bits it asks the question whether there is some bit b in the state description such
that the Q values for states in which b 	 are signicantly dierent from Q values for
states in which b If such a bit is found it is used to split the decision tree Then
the process is repeated in each of the leaves This method was able to learn very small
representations of the Q function in the presence of an overwhelming number of irrelevant
noisy state attributes It outperformed Qlearning with backpropagation in a simple video
game environment and was used by McCallum conjunction with other techniques
for dealing with partial observability to learn behaviors in a complex drivingsimulator It
cannot however acquire partitions in which attributes are only signicant in combination
as those needed to solve parity problems
Variable Resolution Dynamic Programming The VRDP algorithm
enables conventional dynamic programming to be performed in realvalued multivariate
statespaces where straightforward discretization would fall prey to the curse of dimension
ality A kdtree to a decision tree is used to partition state space into coarse
regions The coarse regions are rened into detailed regions but only in parts of the state
space which are predicted to be important This notion of importance is obtained by run
ning trajectories through state space This algorithm proved eective on a number
of problems for which full highresolution arrays would have been impractical It has the
disadvantage of requiring a guess at an initially valid trajectory through statespace

Kaelbling Littman Moore
G
Start
Goal
a
G
b
G
c
Figure
A twodimensional maze problem The point robot must a path from
start to goal without crossing any of the barrier lines The path taken by
PartiGame during the entire trial It begins with intense exploration to a
route out of the almost entirely enclosed start region Having eventually reached
a suciently high resolution it discovers the gap and proceeds greedily towards
the goal only to be temporarily blocked by the goals barrier region The
second trial
PartiGame Algorithm Moores PartiGame algorithm  is another solution
to the problem of learning to achieve goal congurations in deterministic highdimensional
continuous spaces by learning an adaptiveresolution model It also divides the environment
into cells but in each cell the actions available consist of aiming at the neighboring cells
aiming is accomplished by a local controller which must be provided as part of the
problem statement The graph of cell transitions is solved for shortest paths in an online
incremental manner but a minimax criterion is used to detect when a group of cells is
too coarse to prevent movement between obstacles or to avoid limit cycles The oending
cells are split to higher resolution Eventually the environment is divided up just enough to
choose appropriate actions for achieving the goal but no unnecessary distinctions are made
An important feature is that as well as reducing memory and computational requirements
it also structures exploration of state space in a multiresolution manner Given a failure
the agent will initially try something very dierent to rectify the failure and only resort to
small local changes when all the qualitatively dierent strategies have been exhausted
Figure shows a twodimensional continuous maze Figure shows the performance
of a robot using the PartiGame algorithm during the very trial Figure shows the
second trial started from a slightly dierent position
This is a very fast algorithm learning policies in spaces of up to nine dimensions in less
than a minute The restriction of the current implementation to deterministic environments
limits its applicability however McCallum suggests some related treestructured
methods

Reinforcement Learning A Survey
Generalization over Actions
The networks described in Section generalize over state descriptions presented as
inputs They also produce outputs in a discrete factored representation and thus could be
seen as generalizing over actions as well
In cases such as this when actions are described combinatorially it is important to
generalize over actions to avoid keeping separate statistics for the huge number of actions
that can be chosen In continuous action spaces the need for generalization is even more
pronounced
When estimating Q values using a neural network it is possible to use either a distinct
network for each action or a network with a distinct output for each action When the
action space is continuous neither approach is possible An alternative strategy is to use a
single network with both the state and action as input and Q value as the output Training
such a network is not conceptually dicult but using the network to the optimal action
can be a challenge One method is to do a local gradientascent search on the action in
order to one with high value Klopf
Gullapalli  has developed a reinforcementlearning unit for use in
continuous action spaces The unit generates actions with a normal distribution it adjusts
the mean and variance based on previous experience When the chosen actions are not
performing well the variance is high resulting in exploration of the range of choices When
an action performs well the mean is moved in that direction and the variance decreased
resulting in a tendency to generate more action values near the successful one This method
was successfully employed to learn to control a robot arm with many continuous degrees of
freedom
Hierarchical Methods
Another strategy for dealing with large state spaces is to treat them as a hierarchy of
learning problems In many cases hierarchical solutions introduce slight suboptimality in
performance but potentially gain a good deal of eciency in execution time learning time
and space
Hierarchical learners are commonly structured as gated behaviors as shown in Figure
There is a collection of behaviors that map environment states into lowlevel actions and
a gating function that decides based on the state of the environment which behaviors
actions should be switched through and actually executed Maes and Brooks used
a version of this architecture in which the individual behaviors were a priori and the
gating function was learned from reinforcement Mahadevan and Connell used the
dual approach
they the gating function and supplied reinforcement functions for the
individual behaviors which were learned Lin and Dorigo and Colombetti
both used this approach training the behaviors and then training the gating
function Many of the other hierarchical learning methods can be cast in this framework
Feudal Qlearning
Feudal Qlearning Hinton  Watkins  involves a hierarchy of learning
modules In the simplest case there is a highlevel master and a lowlevel slave The master
receives reinforcement from the external environment Its actions consist of commands that

Kaelbling Littman Moore
s b
b
b
g a
Figure
A structure of gated behaviors
it can give to the lowlevel learner When the master generates a particular command to
the slave it must reward the slave for taking actions that satisfy the command even if they
do not result in external reinforcement The master then learns a mapping from states to
commands The slave learns a mapping from commands and states to external actions The
set of and their associated reinforcement functions are established in advance
of the learning
This is really an instance of the general behaviors approach in which the slave
can execute any of the behaviors depending on its command The reinforcement functions
for the individual behaviors are given but learning takes place simultaneously
at both the high and low levels
Compositional Qlearning
Singhs compositional Qlearning a consists of a hierarchy based on
the temporal sequencing of subgoals The elemental tasks are behaviors that achieve some
recognizable condition The highlevel goal of the system is to achieve some set of condi
tions in sequential order The achievement of the conditions provides reinforcement for the
elemental tasks which are trained to achieve individual subgoals Then the gating
function learns to switch the elemental tasks in order to achieve the appropriate highlevel
sequential goal This method was used by Tham and Prager to learn to control a
simulated multilink robot arm
Hierarchical Distance to Goal
Especially if we consider reinforcement learning modules to be part of larger agent archi
tectures it is important to consider problems in which goals are dynamically input to the
learner Kaelblings HDG algorithm uses a hierarchical approach to solving prob
lems when goals of achievement agent should get to a particular state as quickly as
possible are given to an agent dynamically
The HDG algorithm works by analogy with navigation in a harbor The environment
is partitioned priori but more recent work  addresses the case of learning
the partition into a set of regions whose centers are known as If the agent is

Reinforcement Learning A Survey
2/5

printer
office

hallhall
Figure
An example of a partially observable environment
currently in the same region as the goal then it uses lowlevel actions to move to the goal
If not then highlevel information is used to determine the next landmark on the shortest
path from the agents closest landmark to the goals closest landmark Then the agent uses
lowlevel information to aim toward that next landmark If errors in action cause deviations
in the path there is no problem the best aiming point is recomputed on every step
Partially Observable Environments
In many realworld environments it will not be possible for the agent to have perfect and
complete perception of the state of the environment Unfortunately complete observability
is necessary for learning methods based on MDPs In this section we consider the case in
which the agent makes observations of the state of the environment but these observations
may be noisy and provide incomplete information In the case of a robot for instance
it might observe whether it is in a corridor an open room a Tjunction etc and those
observations might be errorprone This problem is also referred to as the problem of
perception aliasing or state
In this section we will consider extensions to the basic MDP framework for solving
partially observable problems The resulting formal model is called a partially observable
Markov decision process or POMDP
StateFree Deterministic Policies
The most naive strategy for dealing with partial observability is to ignore it That is to
treat the observations as if they were the states of the environment and try to learn to
behave Figure shows a simple environment in which the agent is attempting to get to
the printer from an oce If it moves from the oce there is a good chance that the agent
will end up in one of two places that look like but that require dierent actions for
getting to the printer If we consider these states to be the same then the agent cannot
possibly behave optimally But how well can it do
The resulting problem is not Markovian and Qlearning cannot be guaranteed to con
verge Small breaches of the Markov requirement are well handled by Qlearning but it is
possible to construct simple environments that cause Qlearning to oscillate

Kaelbling Littman Moore
Littman  It is possible to use a modelbased approach however act according to
some policy and gather statistics about the transitions between observations then solve for
the optimal policy based on those observations Unfortunately when the environment is not
Markovian the transition probabilities depend on the policy being executed so this new
policy will induce a new set of transition probabilities This approach may yield plausible
results in some cases but again there are no guarantees
It is reasonable though to ask what the optimal policy from observations to
actions in this case is It is NPhard b to this mapping and even the
best mapping can have very poor performance In the case of our agent trying to get to the
printer for instance any deterministic statefree policy takes an innite number of steps to
reach the goal on average
StateFree Stochastic Policies
Some improvement can be gained by considering stochastic policies these are mappings
from observations to probability distributions over actions If there is randomness in the
agents actions it will not get stuck in the hall forever Jaakkola Singh and Jordan
have developed an algorithm for locallyoptimal stochastic policies but a
globally optimal policy is still NP hard
In our example it turns out that the optimal stochastic policy is for the agent when
in a state that looks like a hall to go east with probability p
and west with
probability
p

This policy can be found by solving a simple this case
quadratic program The fact that such a simple example can produce irrational numbers
gives some indication that it is a dicult problem to solve exactly
Policies with Internal State
The only way to behave truly eectively in a widerange of environments is to use memory
of previous actions and observations to disambiguate the current state There are a variety
of approaches to learning policies with internal state
Recurrent Qlearning One intuitively simple approach is to use a recurrent neural net
work to learn Q values The network can be trained using backpropagation through time
some other suitable technique and learns to retain features to predict value This
approach has been used by a number of researchers McGraw Blank  Lin
Mitchell  Schmidhuber b It seems to work eectively on simple problems
but can suer from convergence to local optima on more complex problems
Classier Systems Classier systems  Goldberg  were explicitly
developed to solve problems with delayed reward including those requiring shortterm
memory The internal mechanism typically used to pass reward back through chains of
decisions called the bucket brigade algorithm bears a close resemblance to Qlearning In
spite of some early successes the original design does not appear to handle partially ob
served environments robustly
Recently this approach has been reexamined using insights from the reinforcement
learning literature with some success Dorigo did a comparative study of Qlearning and
classier systems Bersini  Cli and Ross start with Wilsons zeroth

Reinforcement Learning A Survey
i
b a
SE
Figure
Structure of a POMDP agent
level classier system  and add one and twobit memory registers They
that although their system can learn to use shortterm memory registers eectively the
approach is unlikely to scale to more complex environments
Dorigo and Colombetti applied classier systems to a moderately complex problem of
learning robot behavior from immediate reinforcement  Dorigo Colombetti

Finitehistorywindow Approach One way to restore the Markov property is to allow
decisions to be based on the history of recent observations and perhaps actions Lin and
Mitchell used a history window to learn a pole balancing task
McCallum describes the sux memory which learns a variablewidth window
that serves simultaneously as a model of the environment and a policy This
system has had excellent results in a very complex drivingsimulation domain
Ring has a neuralnetwork approach that uses a variable history window
adding history when necessary to disambiguate situations
POMDP Approach Another strategy consists of using hidden Markov model
techniques to learn a model of the environment including the hidden state then to use that
model to construct a perfect memory controller Kaelbling Littman
Lovejoy  Monahan
Chrisman showed how the forwardbackward algorithm for learning HMMs could
be adapted to learning POMDPs He and later McCallum also gave heuristic state
splitting rules to attempt to learn the smallest possible model for a given environment The
resulting model can then be used to integrate information from the agents observations in
order to make decisions
Figure  illustrates the basic structure for a perfectmemory controller The component
on the left is the state estimator which computes the agents belief state b as a function of
the old belief state the last action a and the current observation i In this context a belief
state is a probability distribution over states of the environment indicating the likelihood
given the agents past experience that the environment is actually in each of those states
The state estimator can be constructed straightforwardly using the estimated world model
and Bayes rule
Now we are left with the problem of a policy mapping belief states into action
This problem can be formulated as anMDP but it is dicult to solve using the techniques
described earlier because the input space is continuous Chrismans approach does
not take into account future uncertainty but yields a policy after a small amount of com
putation A standard approach from the operationsresearch literature is to solve for the


Kaelbling Littman Moore
optimal policy a close approximation thereof based on its representation as a piecewise
linear and convex function over the belief space This method is computationally intractable
but may serve as inspiration for methods that make further approximations
et al  Littman Cassandra Kaelbling a
Reinforcement Learning Applications
One reason that reinforcement learning is popular is that is serves as a theoretical tool for
studying the principles of agents learning to act But it is unsurprising that it has also
been used by a number of researchers as a practical computational tool for constructing
autonomous systems that improve themselves with experience These applications have
ranged from robotics to industrial manufacturing to combinatorial search problems such
as computer game playing
Practical applications provide a test of the ecacy and usefulness of learning algorithms
They are also an inspiration for deciding which components of the reinforcement learning
framework are of practical importance For example a researcher with a real robotic task
can provide a data point to questions such as

How important is optimal exploration Can we break the learning period into explo
ration phases and exploitation phases
What is the most useful model of longterm reward
Finite horizon Discounted
Innite horizon
How much computation is available between agent decisions and how should it be
used
What prior knowledge can we build into the system and which algorithms are capable
of using that knowledge
Let us examine a set of practical applications of reinforcement learning while bearing these
questions in mind
Game Playing
Game playing has dominated the Articial Intelligence world as a problem domain ever since
the was born Twoplayer games do not into the established reinforcementlearning
framework since the optimality criterion for games is not one of maximizing reward in the
face of a environment but one of maximizing reward against an optimal adversary
Nonetheless reinforcementlearning algorithms can be adapted to work for a
very general class of games a and many researchers have used reinforcement
learning in these environments One application spectacularly far ahead of its time was
Samuels checkers playing system  This learned a value function represented
by a linear function approximator and employed a training scheme similar to the updates
used in value iteration temporal dierences and Qlearning
More recently Tesauro   applied the temporal dierence algorithm
to backgammon Backgammon has approximately  states making tablebased rein
forcement learning impossible Instead Tesauro used a backpropagationbased threelayer

Reinforcement Learning A Survey
Training
Games
Hidden
Units
Results
Basic Poor
TD  Lost by  points in
games
TD Lost by points in
games
TD  Lost by 	 point in
games
Table
TDGammons performance in games against the top human professional players
A backgammon tournament involves playing a series of games for points until one
player reaches a set target TDGammon won none of these tournaments but came
suciently close that it is now considered one of the best few players in the world
neural network as a function approximator for the value function
Board Position Probability of victory for current player
Two versions of the learning algorithm were used The which we will call Basic TD
Gammon used very little predened knowledge of the game and the representation of a
board position was virtually a raw encoding suciently powerful only to permit the neural
network to distinguish between conceptually dierent positions The second TDGammon
was provided with the same raw state information supplemented by a number of hand
crafted features of backgammon board positions Providing handcrafted features in this
manner is a good example of how inductive biases from human knowledge of the task can
be supplied to a learning algorithm
The training of both learning algorithms required several months of computer time and
was achieved by constant selfplay No exploration strategy was usedthe system always
greedily chose the move with the largest expected probability of victory This naive explo
ration strategy proved entirely adequate for this environment which is perhaps surprising
given the considerable work in the reinforcementlearning literature which has produced
numerous counterexamples to show that greedy exploration can lead to poor learning per
formance Backgammon however has two important properties Firstly whatever policy
is followed every game is guaranteed to end in time meaning that useful reward
information is obtained fairly frequently Secondly the state transitions are suciently
stochastic that independent of the policy all states will occasionally be visiteda wrong
initial value function has little danger of starving us from visiting a critical part of state
space from which important information could be obtained
The results of TDGammon are impressive It has competed at the very top
level of international human play Basic TDGammon played respectably but not at a
professional standard

Figure
Schaal and Atkesons devilsticking robot The tapered stick is hit alternately
by each of the two hand sticks The task is to keep the devil stick from falling
for as many hits as possible The robot has three motors indicated by torque
vectors
Although experiments with other games have in some cases produced interesting learning
behavior no success close to that of TDGammon has been repeated Other games that
have been studied include Go Dayan Sejnowski  and Chess
It is still an open question as to if and how the success of TDGammon can be
repeated in other domains
Robotics and Control
In recent years there have been many robotics and control applications that have used
reinforcement learning Here we will concentrate on the following four examples although
many other interesting ongoing robotics investigations are underway
Schaal and Atkeson constructed a twoarmed robot shown in Figure  that
learns to juggle a device known as a devilstick This is a complex nonlinear control
task involving a sixdimensional state space and less than msecs per control deci
sion After about initial attempts the robot learns to keep juggling for hundreds of
hits A typical human learning the task requires an order of magnitude more practice
to achieve prociency at mere tens of hits
The juggling robot learned a world model from experience which was generalized
to unvisited states by a function approximation scheme known as locally weighted
regression Delvin  Moore Atkeson  Between each trial
a form of dynamic programming specic to linear control policies and locally linear
transitions was used to improve the policy The form of dynamic programming is
known as linearquadraticregulator design White

Reinforcement Learning A Survey
Mahadevan and Connell discuss a task in which a mobile robot pushes large
boxes for extended periods of time Boxpushing is a wellknown dicult robotics
problem characterized by immense uncertainty in the results of actions Qlearning
was used in conjunction with some novel clustering techniques designed to enable a
higherdimensional input than a tabular approach would have permitted The robot
learned to perform competitively with the performance of a humanprogrammed so
lution Another aspect of this work mentioned in Section was a preprogrammed
breakdown of the monolithic task description into a set of lower level tasks to be
learned
Mataric describes a robotics experiment with from the viewpoint of theoret
ical reinforcement learning an unthinkably high dimensional state space containing
many dozens of degrees of freedom Four mobile robots traveled within an enclo
sure collecting small disks and transporting them to a destination region There were
three enhancements to the basic Qlearning algorithm Firstly preprogrammed sig
nals called progress estimators were used to break the monolithic task into subtasks
This was achieved in a robust manner in which the robots were not forced to use
the estimators but had the freedom to prot from the inductive bias they provided
Secondly control was decentralized Each robot learned its own policy independently
without explicit communication with the others Thirdly state space was brutally
quantized into a small number of discrete states according to values of a small num
ber of preprogrammed boolean features of the underlying sensors The performance
of the Qlearned policies were almost as good as a simple handcrafted controller for
the job
Qlearning has been used in an elevator dispatching task Barto  The
problem which has been implemented in simulation only at this stage involved four
elevators servicing ten The objective was to minimize the average squared
wait time for passengers discounted into future time The problem can be posed as a
discrete Markov system but there are  states even in the most simplied version of
the problem Crites and Barto used neural networks for function approximation and
provided an excellent comparison study of their Qlearning approach against the most
popular and the most sophisticated elevator dispatching algorithms The squared wait
time of their controller was approximately less than the best alternative algorithm
the System heuristic with a receding horizon controller and less than half
the squared wait time of the controller most frequently used in real elevator systems
The example concerns an application of reinforcement learning by one of the
authors of this survey to a packaging task from a food processing industry The
problem involves containers with variable numbers of nonidentical products
The product characteristics also vary with time but can be sensed Depending on
the task various constraints are placed on the containerlling procedure Here are
three examples

The mean weight of all containers produced by a shift must not be below the
manufacturers declared weight W

Kaelbling Littman Moore
The number of containers below the declared weight must be less than P
No containers may be produced below weight W
Such tasks are controlled by machinery which operates according to various setpoints
Conventional practice is that setpoints are chosen by human operators but this choice
is not easy as it is dependent on the current product characteristics and the current
task constraints The dependency is often dicult to model and highly nonlinear
The task was posed as a Markov decision task in which the state of the
system is a function of the product characteristics the amount of time remaining in
the production shift and the mean wastage and percent below declared in the shift
so far The system was discretized into discrete states and local weighted
regression was used to learn and generalize a transition model Prioritized sweep
ing was used to maintain an optimal value function as each new piece of transition
information was obtained In simulated experiments the savings were considerable
typically with wastage reduced by a factor of ten Since then the system has been
deployed successfully in several factories within the United States
Some interesting aspects of practical reinforcement learning come to light from these
examples The most striking is that in all cases to make a real system work it proved
necessary to supplement the fundamental algorithm with extra preprogrammed knowledge
Supplying extra knowledge comes at a price
more human eort and insight is required and
the system is subsequently less autonomous But it is also clear that for tasks such as
these a knowledgefree approach would not have achieved worthwhile performance within
the lifetime of the robots
What forms did this preprogrammed knowledge take It included an assumption of
linearity for the juggling robots policy a manual breaking up of the task into subtasks for
the two mobilerobot examples while the boxpusher also used a clustering technique for
the Q values which assumed locally consistent Q values The four diskcollecting robots
additionally used a manually discretized state space The packaging example had far fewer
dimensions and so required correspondingly weaker assumptions but there too the as
sumption of local piecewise continuity in the transition model enabled massive reductions
in the amount of learning data required
The exploration strategies are interesting too The juggler used careful statistical anal
ysis to judge where to protably experiment However both mobile robot applications
were able to learn well with greedy explorationalways exploiting without deliberate ex
ploration The packaging task used optimism in the face of uncertainty None of these
strategies mirrors theoretically optimal computationally intractable exploration and
yet all proved adequate
Finally it is also worth considering the computational regimes of these experiments
They were all very dierent which indicates that the diering computational demands of
various reinforcement learning algorithms do indeed have an array of diering applications
The juggler needed to make very fast decisions with low latency between each hit but
had long periods seconds and more between each trial to consolidate the experiences
collected on the previous trial and to perform the more aggressive computation necessary
to produce a new reactive controller on the next trial The boxpushing robot was meant to

Reinforcement Learning A Survey
operate autonomously for hours and so had to make decisions with a uniform length control
cycle The cycle was suciently long for quite substantial computations beyond simple Q
learning backups The four diskcollecting robots were particularly interesting Each robot
had a short life of less than minutes to battery constraints meaning that substantial
number crunching was impractical and any signicant combinatorial search would have
used a signicant fraction of the robots learning lifetime The packaging task had easy
constraints One decision was needed every few minutes This provided opportunities for
fully computing the optimal value function for the system between every
control cycle in addition to performing massive crossvalidationbased optimization of the
transition model being learned
A great deal of further work is currently in progress on practical implementations of
reinforcement learning The insights and task constraints that they produce will have an
important eect on shaping the kind of algorithms that are developed in future
Conclusions
There are a variety of reinforcementlearning techniques that work eectively on a variety
of small problems But very few of these techniques scale well to larger problems This is
not because researchers have done a bad job of inventing learning techniques but because
it is very dicult to solve arbitrary problems in the general case In order to solve highly
complex problems we must give up tabula rasa learning techniques and begin to incorporate
bias that will give leverage to the learning process
The necessary bias can come in a variety of forms including the following

shaping The technique of shaping is used in training animals Bower  a
teacher presents very simple problems to solve then gradually exposes the learner
to more complex problems Shaping has been used in supervisedlearning systems
and can be used to train hierarchical reinforcementlearning systems from the bottom
up  and to alleviate problems of delayed reinforcement by decreasing the
delay until the problem is well understood Colombetti  Dorigo
local reinforcement signals Whenever possible agents should be given reinforcement
signals that are local In applications in which it is possible to compute a gradient
rewarding the agent for taking steps up the gradient rather than just for achieving
the goal can speed learning signicantly
imitation An agent can learn by another agent perform the task
For real robots this requires perceptual abilities that are not yet available But
another strategy is to have a human supply appropriate motor commands to a robot
through a joystick or steering wheel
problem decomposition Decomposing a huge learning problem into a collection of smaller
ones and providing useful reinforcement signals for the subproblems is a very power
ful technique for biasing learning Most interesting examples of robotic reinforcement
learning employ this technique to some extent Mahadevan
reexes One thing that keeps agents that know nothing from learning anything is that
they have a hard time even the interesting parts of the space they wander

Kaelbling Littman Moore
around at random never getting near the goal or they are always immediately
These problems can be ameliorated by programming a set of that cause the
agent to act initially in some way that is reasonable  Singh Barto
Grupen Connolly  These reexes can eventually be overridden by more
detailed and accurate learned knowledge but they at least keep the agent alive and
pointed in the right direction while it is trying to learn Recent work by Millan
explores the use of reexes to make robot learning safer and more ecient
With appropriate biases supplied by human programmers or teachers complex reinforcement
learning problems will eventually be solvable There is still much work to be done and many
interesting questions remaining for learning techniques and especially regarding methods for
approximating decomposing and incorporating bias into problems
Acknowledgements
Thanks to Marco Dorigo and three anonymous reviewers for comments that have helped
to improve this paper Also thanks to our many colleagues in the reinforcementlearning
community who have done this work and explained it to us
Leslie Pack Kaelbling was supported in part by NSF grants IRI and IRI
Michael Littman was supported in part by Bellcore Andrew Moore was supported
in part by an NSF Research Initiation Award and by Corporation
References
Ackley D H Littman M L Generalization and scaling in reinforcement learn
ing In Touretzky D S Advances in Neural Information Processing Systems
pp San Mateo CA Morgan Kaufmann
Albus J S A new approach to manipulator control
Cerebellar model articulation
controller Journal of Dynamic Systems Measurement and Control

Albus J S Brains Behavior and Robotics BYTE Books Subsidiary of McGraw
Hill Peterborough New Hampshire
Anderson C W Learning and Problem Solving with Multilayer Connectionist
Systems PhD thesis University of Massachusetts Amherst MA
Ashar R R Hierarchical learning in stochastic domains Masters thesis Brown
University Providence Rhode Island
Baird L Residual algorithms
Reinforcement learning with function approxima
tion In Prieditis A Russell S Proceedings of the Twelfth International
Conference on Machine Learning pp San Francisco CA Morgan Kaufmann
Baird L C Klopf A H Reinforcement learning with highdimensional con
tinuous actions Tech rep WLTR WrightPatterson Air Force Base Ohio

Wright Laboratory

Reinforcement Learning A Survey
Barto A G Bradtke S J Singh S P Learning to act using realtime dynamic
programming Articial Intelligence
Barto A G Sutton R S Anderson C W Neuronlike adaptive elements that
can solve dicult learning control problems IEEE Transactions on Systems Man
and Cybernetics SMC
Bellman R Dynamic Programming Princeton University Press Princeton NJ
Berenji H R Articial neural networks and approximate reasoning for intelligent
control in space In American Control Conference pp
Berry D A Fristedt B Bandit Problems	 Sequential Allocation of Experiments
Chapman and Hall London UK
Bertsekas D P Dynamic Programming	 Deterministic and Stochastic Models
PrenticeHall Englewood Clis NJ
Bertsekas D P Dynamic Programming and Optimal Control Athena Scientic
Belmont Massachusetts Volumes 	 and
Bertsekas D P Castanon D A Adaptive aggregation for innite horizon
dynamic programming IEEE Transactions on Automatic Control

Bertsekas D P Tsitsiklis J N Parallel and Distributed Computation	 Numer
ical Methods PrenticeHall Englewood Clis NJ
Box G E P Draper N R Empirical ModelBuilding and Response Surfaces
Wiley
Boyan J A Moore A W Generalization in reinforcement learning
Safely
approximating the value function In Tesauro G Touretzky D S Leen T K
Advances in Neural Information Processing Systems Cambridge MA The
MIT Press
Burghes D Graham A Introduction to Control Theory including Optimal
Control Ellis Horwood
Cassandra A R Kaelbling L P Littman M L Acting optimally in partially
observable stochastic domains In Proceedings of the Twelfth National Conference on
Articial Intelligence Seattle WA
Chapman D Kaelbling L P Input generalization in delayed reinforcement
learning
An algorithm and performance comparisons In Proceedings of the Interna
tional Joint Conference on Articial Intelligence Sydney Australia
Chrisman L Reinforcement learning with perceptual aliasing
The perceptual
distinctions approach In Proceedings of the Tenth National Conference on Articial
Intelligence pp  San Jose CA AAAI Press

Kaelbling Littman Moore
Chrisman L Littman M Hidden state and shortterm memory Presentation
at Reinforcement Learning Workshop Machine Learning Conference
Cichosz P Mulawka J J Fast and ecient reinforcement learning with trun
cated temporal dierences In Prieditis A Russell S Proceedings of the
Twelfth International Conference on Machine Learning pp San Francisco
CA Morgan Kaufmann
Cleveland W S Delvin S J Locally weighted regression
An approach to
regression analysis by local Journal of the American Statistical Association

Cli D Ross S Adding temporary memory to ZCS Adaptive Behavior

Condon A The complexity of stochastic games Information and Computation

Connell J Mahadevan S Rapid task learning for real robots In Robot Learning
Kluwer Academic Publishers
Crites R H Barto A G Improving elevator performance using reinforcement
learning In Touretzky D Mozer M Hasselmo M Neural Information
Processing Systems
Dayan P The convergence of TD for general Machine Learning

Dayan P Hinton G E Feudal reinforcement learning In Hanson S J Cowan
J D Giles C L Advances in Neural Information Processing Systems
San Mateo CA Morgan Kaufmann
Dayan P Sejnowski T J TD converges with probability  Machine Learn
ing

Dean T Kaelbling L P Kirman J Nicholson A Planning with deadlines in
stochastic domains In Proceedings of the Eleventh National Conference on Articial
IntelligenceWashington DC
DEpenoux F A probabilistic production and inventory problem Management
Science
Derman C Finite State Markovian Decision Processes Academic Press New York
Dorigo M Bersini H A comparison of qlearning and classier systems In
From Animals to Animats	 Proceedings of the Third International Conference on the
Simulation of Adaptive Behavior Brighton UK
Dorigo M Colombetti M Robot shaping
Developing autonomous agents
through learning Articial Intelligence

Reinforcement Learning A Survey
Dorigo M Alecsys and the AutonoMouse
Learning to control a real robot by
distributed classier systems Machine Learning
Fiechter CN Ecient reinforcement learning In Proceedings of the Seventh
Annual ACM Conference on Computational Learning Theory pp Association
of Computing Machinery
Gittins J C Multiarmed Bandit Allocation Indices WileyInterscience series in
systems and optimization Wiley Chichester NY
Goldberg D Genetic algorithms in search optimization and machine learning
AddisonWesley MA
Gordon G J Stable function approximation in dynamic programming In Priedi
tis A Russell S Proceedings of the Twelfth International Conference on
Machine Learning pp San Francisco CA Morgan Kaufmann
Gullapalli V A stochastic reinforcement learning algorithm for learning realvalued
functions Neural Networks
Gullapalli V Reinforcement learning and its application to control PhD thesis
University of Massachusetts Amherst MA
Hilgard E R Bower G H Theories of Learning edition PrenticeHall
Englewood Clis NJ
Homan A J Karp R M On nonterminating stochastic games Management
Science
Holland J H Adaptation in Natural and Articial Systems University of Michigan
Press Ann Arbor MI
Howard R A Dynamic Programming and Markov Processes The MIT Press
Cambridge MA
Jaakkola T Jordan M I Singh S P On the convergence of stochastic iterative
dynamic programming algorithms Neural Computation
Jaakkola T Singh S P Jordan M I Montecarlo reinforcement learning in
nonMarkovian decision problems In Tesauro G Touretzky D S Leen T K
Advances in Neural Information Processing Systems Cambridge MA The
MIT Press
Kaelbling L P Hierarchical learning in stochastic domains
Preliminary results
In Proceedings of the Tenth International Conference on Machine Learning Amherst
MA Morgan Kaufmann
Kaelbling L P Learning in Embedded Systems The MIT Press Cambridge MA
Kaelbling L P Associative reinforcement learning
A generate and test algorithm
Machine Learning


Kaelbling Littman Moore
Kaelbling L P Associative reinforcement learning
Functions in kDNF Machine
Learning
Kirman J Predicting RealTime Planner Performance by Domain Characterization
PhD thesis Department of Computer Science Brown University
Koenig S Simmons R G Complexity analysis of realtime reinforcement
learning In Proceedings of the Eleventh National Conference on Articial Intelligence
pp Menlo Park California AAAI PressMIT Press
Kumar P R Varaiya P P Stochastic Systems	 Estimation Identication and
Adaptive Control Prentice Hall Englewood Clis New Jersey
Lee C C A self learning rulebased controller employing approximate reasoning
and neural net concepts International Journal of Intelligent Systems
Lin LJ Programming robots using reinforcement learning and teaching In
Proceedings of the Ninth National Conference on Articial Intelligence
Lin LJ Hierachical learning of robot skills by reinforcement In Proceedings of
the International Conference on Neural Networks
Lin LJ Reinforcement Learning for Robots Using Neural Networks PhD thesis
Carnegie Mellon University Pittsburgh PA
Lin LJ Mitchell T M Memory approaches to reinforcement learning in non
Markovian domains Tech rep CMUCS Carnegie Mellon University School
of Computer Science
Littman M L Markov games as a framework for multiagent reinforcement learn
ing In Proceedings of the Eleventh International Conference on Machine Learning
pp  San Francisco CA Morgan Kaufmann
Littman M L Memoryless policies
Theoretical limitations and practical results
In Cli D Husbands P Meyer JA Wilson S W From Animals
to Animats Proceedings of the Third International Conference on Simulation of
Adaptive Behavior Cambridge MA The MIT Press
Littman M L Cassandra A Kaelbling L P Learning policies for partially
observable environments
Scaling up In Prieditis A Russell S Proceed
ings of the Twelfth International Conference on Machine Learning pp San
Francisco CA Morgan Kaufmann
Littman M L Dean T L Kaelbling L P On the complexity of solving
Markov decision problems In Proceedings of the Eleventh Annual Conference on
Uncertainty in Articial Intelligence Montreal Quebec Canada
Lovejoy W S A survey of algorithmic methods for partially observable Markov
decision processes Annals of Operations Research

Reinforcement Learning A Survey
Maes P Brooks R A Learning to coordinate behaviors In Proceedings Eighth
National Conference on Articial Intelligence pp Morgan Kaufmann
Mahadevan S To discount or not to discount in reinforcement learning
A case
study comparing R learning and Q learning In Proceedings of the Eleventh Inter
national Conference on Machine Learning pp  San Francisco CA Morgan
Kaufmann
Mahadevan S Average reward reinforcement learning
Foundations algorithms
and empirical results Machine Learning
Mahadevan S Connell J Automatic programming of behaviorbased robots
using reinforcement learning In Proceedings of the Ninth National Conference on
Articial Intelligence Anaheim CA
Mahadevan S Connell J Scaling reinforcement learning to robotics by ex
ploiting the subsumption architecture In Proceedings of the Eighth International
Workshop on Machine Learning pp
Mataric M J Reward functions for accelerated learning In Cohen W W
Hirsh H Proceedings of the Eleventh International Conference on Machine
Learning Morgan Kaufmann
McCallum A K Reinforcement Learning with Selective Perception and Hidden
State PhD thesis Department of Computer Science University of Rochester
McCallum R A Overcoming incomplete perception with utile distinction memory
In Proceedings of the Tenth International Conference on Machine Learning pp
Amherst Massachusetts Morgan Kaufmann
McCallum R A Instancebased utile distinctions for reinforcement learning with
hidden state In Proceedings of the Twelfth International Conference Machine Learn
ing pp San Francisco CA Morgan Kaufmann
Meeden L McGraw G Blank D Emergent control and planning in an au
tonomous vehicle In Touretsky D Proceedings of the Fifteenth Annual Meeting
of the Cognitive Science Society pp Lawerence Erlbaum Associates Hills
dale NJ
Millan J d R Rapid safe and incremental learning of navigation strategies IEEE
Transactions on Systems Man and Cybernetics
Monahan G E A survey of partially observable Markov decision processes
Theory
models and algorithms Management Science
Moore A W Variable resolution dynamic programming
Eciently learning ac
tion maps in multivariate realvalued spaces In Proc Eighth International Machine
Learning Workshop

Kaelbling Littman Moore
Moore A W The partigame algorithm for variable resolution reinforcement learn
ing in multidimensional statespaces In Cowan J D Tesauro G Alspector J
in Neural Information Processing Systems pp San Mateo
CA Morgan Kaufmann
Moore A W Atkeson C G An investigation of memorybased function ap
proximators for learning control Tech rep MIT Artical Intelligence Laboratory
Cambridge MA
Moore A W Atkeson C G Prioritized sweeping
Reinforcement learning with
less data and less real time Machine Learning
Moore A W Atkeson C G Schaal S Memorybased learning for control
Tech rep CMURITR CMU Robotics Institute
Narendra K Thathachar M A L Learning Automata	 An Introduction
PrenticeHall Englewood Clis NJ
Narendra K S Thathachar M A L Learning automataa survey IEEE
Transactions on Systems Man and Cybernetics

Peng J Williams R J Ecient learning and planning within the Dyna frame
work Adaptive Behavior
Peng J Williams R J Incremental multistep Qlearning In Proceedings of the
Eleventh International Conference on Machine Learning pp San Francisco
CA Morgan Kaufmann
Pomerleau D A Neural network perception for mobile robot guidance Kluwer
Academic Publishing
Puterman M L Markov Decision ProcessesDiscrete Stochastic Dynamic Pro
gramming John Wiley Sons Inc New York NY
Puterman M L Shin M C Modied policy iteration algorithms for discounted
Markov decision processes Management Science

Ring M B Continual Learning in Reinforcement Environments PhD thesis
University of Texas at Austin Austin Texas
R ude U Mathematical and computational techniques for multilevel adaptive meth
ods Society for Industrial and Applied Mathematics Philadelphia Pennsylvania
Rumelhart D E McClelland J L Parallel Distributed Processing
Explorations in the microstructures of cognition Volume Foundations The MIT
Press Cambridge MA
Rummery G A Niranjan M Online Qlearning using connectionist systems
Tech rep CUEDFINFENGTR Cambridge University

Reinforcement Learning A Survey
Rust J Numerical dynamic programming in economics In Handbook of Computa
tional Economics Elsevier North Holland
Sage A P White C C Optimum Systems Control Prentice Hall
Salganico M Ungar L H Active exploration and learning in realvalued
spaces using multiarmed bandit allocation indices In Prieditis A Russell S
Proceedings of the Twelfth International Conference on Machine Learning
pp San Francisco CA Morgan Kaufmann
Samuel A L Some studies in machine learning using the game of checkers IBM
Journal of Research and Development Reprinted in E A Feigenbaum
and J Feldman editors Computers and Thought McGrawHill New York
Schaal S Atkeson C Robot juggling
An implementation of memorybased
learning Control Systems Magazine

Schmidhuber J A general method for multiagent learning and incremental self
improvement in unrestricted environments In Yao X Evolutionary Computa
tion	 Theory and Applications Scientic Publ Co Singapore
Schmidhuber J H Curious modelbuilding control systems In Proc International
Joint Conference on Neural Networks Singapore Vol pp  IEEE
Schmidhuber J H Reinforcement learning in Markovian and nonMarkovian
environments In Lippman D S Moody J E Touretzky D S Advances
in Neural Information Processing Systems pp San Mateo CA Morgan
Kaufmann
Schraudolph N N Dayan P Sejnowski T J Temporal dierence learning of
position evaluation in the game of Go In Cowan J D Tesauro G Alspector
J Advances in Neural Information Processing Systems pp San
Mateo CA Morgan Kaufmann
Schrijver A Theory of Linear and Integer Programming WileyInterscience New
York NY
Schwartz A A reinforcement learning method for maximizing undiscounted re
wards In Proceedings of the Tenth International Conference on Machine Learning
pp Amherst Massachusetts Morgan Kaufmann
Singh S P Barto A G Grupen R Connolly C Robust reinforcement
learning in motion planning In Cowan J D Tesauro G Alspector J
Advances in Neural Information Processing Systems pp San Mateo CA
Morgan Kaufmann
Singh S P Sutton R S Reinforcement learning with replacing eligibility traces
Machine Learning

Kaelbling Littman Moore
Singh S P Reinforcement learning with a hierarchy of abstract models In
Proceedings of the Tenth National Conference on Articial Intelligence pp
San Jose CA AAAI Press
Singh S P Transfer of learning by composing solutions of elemental sequential
tasks Machine Learning
Singh S P Learning to Solve Markovian Decision Processes PhD thesis Depart
ment of Computer Science University of Massachusetts Also CMPSCI Technical
Report
Stengel R F Stochastic Optimal Control John Wiley and Sons
Sutton R S Generalization in Reinforcement Learning
Successful Examples Using
Sparse Coarse Coding In Touretzky D Mozer M Hasselmo M Neural
Information Processing Systems
Sutton R S Temporal Credit Assignment in Reinforcement Learning PhD thesis
University of Massachusetts Amherst MA
Sutton R S Learning to predict by the method of temporal dierences Machine
Learning
Sutton R S Integrated architectures for learning planning and reacting based
on approximating dynamic programming In Proceedings of the Seventh International
Conference on Machine Learning Austin TX Morgan Kaufmann
Sutton R S Planning by incremental dynamic programming In Proceedings
of the Eighth International Workshop on Machine Learning pp Morgan
Kaufmann
Tesauro G Practical issues in temporal dierence learning Machine Learning

Tesauro G TDGammon a selfteaching backgammon program achieves master
level play Neural Computation
Tesauro G Temporal dierence learning and TDGammon Communications of
the ACM
Tham CK Prager R W A modular qlearning architecture for manipula
tor task decomposition In Proceedings of the Eleventh International Conference on
Machine Learning San Francisco CA Morgan Kaufmann
Thrun S Learning to play the game of chess In Tesauro G Touretzky D S
Leen T K Advances in Neural Information Processing Systems Cambridge
MA The MIT Press

Reinforcement Learning A Survey
Thrun S Schwartz A Issues in using function approximation for reinforcement
learning In Mozer M Smolensky P Touretzky D Elman J Weigend A
Proceedings of the Connectionist Models Summer School Hillsdale NJ
Lawrence Erlbaum
Thrun S B The role of exploration in learning control In White D A
Sofge D A Handbook of Intelligent Control	 Neural Fuzzy and Adaptive
Approaches Van Nostrand Reinhold New York NY
Tsitsiklis J N Asynchronous stochastic approximation and Qlearning Machine
Learning
Tsitsiklis J N Van Roy B Featurebased methods for large scale dynamic
programming Machine Learning
Valiant L G A theory of the learnable Communications of the ACM

Watkins C J C H Learning from Delayed Rewards PhD thesis Kings College
Cambridge UK
Watkins C J C H Dayan P Qlearning Machine Learning
Whitehead S D Complexity and cooperation in Qlearning In Proceedings of the
Eighth International Workshop on Machine Learning Evanston IL Morgan Kauf
mann
Williams R J A class of gradientestimating algorithms for reinforcement learning
in neural networks In Proceedings of the IEEE First International Conference on
Neural Networks San Diego CA
Williams R J Simple statistical gradientfollowing algorithms for connectionist
reinforcement learning Machine Learning
Williams R J Baird III L C Analysis of some incremental variants of policy
iteration
First steps toward understanding actorcritic learning systems Tech rep
NUCCS Northeastern University College of Computer Science Boston MA
Williams R J Baird III L C Tight performance bounds on greedy policies
based on imperfect value functions Tech rep NUCCS Northeastern Univer
sity College of Computer Science Boston MA
Wilson S Classier based on accuracy Evolutionary Computation

Zhang W Dietterich T G A reinforcement learning approach to jobshop
scheduling In Proceedings of the International Joint Conference on Articial Intel
lience


