SemiSupervised Learning Using Gaussian Fields and Harmonic FunctionsXiaojin ZhuZHUXJCS.CMU.EDUZoubin GhahramaniZOUBINGATSBY.UCL.AC.UKJohn LaffertyLAFFERTYCS.CMU.EDUSchool of Computer Science, Carnegie Mellon University, Pittsburgh PA 15213, USAGatsby Computational Neuroscience Unit, University College London, London WC1N 3AR, UKAbstractAn approach to semisupervised learning is proposed that is based on a Gaussian random fieldmodel. Labeled and unlabeled data are represented as vertices in a weighted graph, withedge weights encoding the similarity between instances. The learning problem is then formulatedin terms of a Gaussian random field on this graph,where the mean of the field is characterized interms of harmonic functions, and is efficientlyobtained using matrix methods or belief propagation. The resulting learning algorithms haveintimate connections with random walks, electric networks, and spectral graph theory. We discuss methods to incorporate class priors and thepredictions of classifiers obtained by supervisedlearning. We also propose a method of parameterlearning by entropy minimization, and show thealgorithms ability to perform feature selection.Promising experimental results are presented forsynthetic data, digit classification, and text classification tasks.1. IntroductionIn many traditional approaches to machine learning, a target function is estimated using labeled data, which can bethought of as examples given by a teacher to a student.Labeled examples are often, however, very time consuming and expensive to obtain, as they require the efforts ofhuman annotators, who must often be quite skilled. For instance, obtaining a single labeled example for protein shapeclassification, which is one of the grand challenges of biological and computational science, requires months of expensive analysis by expert crystallographers. The problemof effectively combining unlabeled data with labeled datais therefore of central importance in machine learning.The semisupervised learning problem has attracted an increasing amount of interest recently, and several novel approaches have been proposed we refer to Seeger, 2001for an overview. Among these methods is a promising family of techniques that exploit the manifold structure of thedata such methods are generally based upon an assumptionthat similar unlabeled examples should be given the sameclassification. In this paper we introduce a new approachto semisupervised learning that is based on a random fieldmodel defined on a weighted graph over the unlabeled andlabeled data, where the weights are given in terms of a similarity function between instances.Unlike other recent work based on energy minimizationand random fields in machine learning Blum  Chawla,2001 and image processing Boykov et al., 2001, weadopt Gaussian fields over a continuous state space ratherthan random fields over the discrete label set. This relaxation to a continuous rather than discrete sample spaceresults in many attractive properties. In particular, the mostprobable configuration of the field is unique, is characterized in terms of harmonic functions, and has a closed formsolution that can be computed using matrix methods orloopy belief propagation Weiss et al., 2001. In contrast,for multilabel discrete random fields, computing the lowest energy configuration is typically NPhard, and approximation algorithms or other heuristics must be used Boykovet al., 2001. The resulting classification algorithms forGaussian fields can be viewed as a form of nearest neighbor approach, where the nearest labeled examples are computed in terms of a random walk on the graph. The learningmethods introduced here have intimate connections withrandom walks, electric networks, and spectral graph theory, in particular heat kernels and normalized cuts.In our basic approach the solution is solely based on thestructure of the data manifold, which is derived from datafeatures. In practice, however, this derived manifold structure may be insufficient for accurate classification. WeProceedings of the Twentieth International Conference on Machine Learning ICML2003, Washington DC, 2003.Figure 1. The random fields used in this work are constructed onlabeled and unlabeled examples. We form a graph with weightededges between instances in this case scanned digits, with labeleddata items appearing as special boundary points, and unlabeledpoints as interior points. We consider Gaussian random fieldson this graph.show how the extra evidence of class priors can help classification in Section 4. Alternatively, we may combine external classifiers using vertex weights or assignment costs,as described in Section 5. Encouraging experimental results for synthetic data, digit classification, and text classification tasks are presented in Section 7. One difficultywith the random field approach is that the right choice ofgraph is often not entirely clear, and it may be desirable tolearn it from data. In Section 6 we propose a method forlearning these weights by entropy minimization, and showthe algorithms ability to perform feature selection to bettercharacterize the data manifold.2. Basic FrameworkWe suppose there are  labeled points  ,and  unlabeled points   typically  .Let   be the total number of data points. To begin, we assume the labels are binary ,. . Considera connected graph 01132456 with nodes 2 corresponding to the  data points, with nodes 789.9 corresponding to the labeled points with labels     , andnodes .9A corresponding to the unlabeled points. Our task is to assign labels to nodes  . Weassume an  CBD symmetric weight matrix E on the edgesof the graph is given. For example, when GFIH , theweight matrix can beJLKNMOPRQSUTHV WYXKWTMWZZW 1where  KWis the  th component of instance  K representedas a vector  K F H , and   Hare length scalehyperparameters for each dimension. Thus, nearby pointsin Euclidean space are assigned large edge weight. Otherweightings are possible, of course, and may be more appropriate when  is discrete or symbolic. For our purposes thematrix E fully specifies the data manifold structure seeFigure 1.Our strategy is to first compute a realvalued function2TaF on 0 with certain nice properties, and tothen assign labels based on. We constrainto take valuesbcbcdK on the labeled data bDe.f .Intuitively, we want unlabeled points that are nearby in thegraph to have similar labels. This motivates the choice ofthe quadratic energy function5cI.gVKh MJLKNMbTjiZ 2To assign a probability distribution on functions, we formthe Gaussian field klI1monpqRrstup, where v is an inversetemperature parameter, and wxl is the partition functionwxlyez XOPRQTv5c, which normalizes overall functions constrained to on the labeled data.It is not difficult to show that the minimum energy function arg min  X5c is harmonic namely, it satisfies on unlabeled data points  , and is equal toon the labeled data points 7 . Hereis the combinatorialLaplacian, given in matrix form asTE where diag  K  is the diagonal matrix with entries  K  M J KNMand E J KM is the weight matrix.The harmonic property means that the value ofat eachunlabeled data point is the average ofat neighboringpointsjiI.MVKMJKMb for i.fo 3which is consistent with our prior notion of smoothness ofwith respect to the graph. Expressed slightly differently,, where C  E . Because of the maximumprinciple of harmonic functions Doyle  Snell, 1984,isunique and is either a constant or it satisfies ci.for i .To compute the harmonic solution explicitly in terms ofmatrix operations, we split the weight matrix E and similarly   into 4 blocks after the  th row and columnEEEE,E4LettingAwhere denotes the values on the unlabeled data points, the harmonic solution subjectto  is given byDTERE,TU,R, 50 1 2 300.511.522.533.520220201234Figure 2. Demonstration of harmonic energy minimization on twosynthetic datasets. Large symbols indicate labeled data, otherpoints are unlabeled.In this paper we focus on the above harmonic function as abasis for semisupervised classification. However, we emphasize that the Gaussian random field model from whichthis function is derived provides the learning frameworkwith a consistent probabilistic semantics.In the following, we refer to the procedure described aboveas harmonic energy minimization, to underscore the harmonic property 3 as well as the objective function beingminimized. Figure 2 demonstrates the use of harmonic energy minimization on two synthetic datasets. The left figureshows that the data has three bands, with 4 ,  . ,and   gfg the right figure shows two spirals, with g,  . , and  1  . Here we see harmonicenergy minimization clearly follows the structure of data,while obviously methods such as kNN would fail to do so.3. Interpretation and ConnectionsAs outlined briefly in this section, the basic framework presented in the previous section can be viewed in several fundamentally different ways, and these different viewpointsprovide a rich and complementary set of techniques for reasoning about this approach to the semisupervised learningproblem.3.1. Random Walks and Electric NetworksImagine a particle walking along the graph 0 . Startingfrom an unlabeled node b , it moves to a node i with probability  KNM after one step. The walk continues until the particle hits a labeled node. Thenb is the probability thatthe particle, starting from node b , hits a labeled node withlabel 1. Here the labeled data is viewed as an absorbingboundary for the random walk.This view of the harmonic solution indicates that it isclosely related to the random walk approach of Szummerand Jaakkola 2001, however there are two major differences. First, we fix the value ofon the labeled points,and second, our solution is an equilibrium state, expressedin terms of a hitting time, while in Szummer  Jaakkola,2001 the walk crucially depends on the time parameter  .We will return to this point when discussing heat kernels.An electrical network interpretation is given in Doyle Snell, 1984. Imagine the edges of 0 to be resistors withconductance E . We connect nodes labeled . to a positivevoltage source, and points labeled  to ground. Thenis the voltage in the resulting electric network on each ofthe unlabeled nodes. Furthermore minimizes the energydissipation of the electric network 0 for the given . Theharmonic property here follows from Kirchoffs and Ohmslaws, and the maximum principle then shows that this isprecisely the same solution obtained in 5.3.2. Graph KernelsThe solutioncan be viewed from the viewpoint of spectral graph theory. The heat kernel with time parameter on the graph 0 is defined as 9  . Here YbY i isthe solution to the heat equation on the graph with initialconditions being a point source at b at time  . Kondorand Lafferty 2002 propose this as an appropriate kernelfor machine learning with categorical data. When used in akernel method such as a support vector machine, the kernelclassifierji KKKbY i can be viewed as asolution to the heat equation with initial heat sources  K  Kon the labeled data. The time parameter  must, however,be chosen using an auxiliary technique, for example crossvalidation.Our algorithm uses a different approach which is independent of  , the diffusion time. Let, be the lower rightB submatrix of. SinceTE , it is theLaplacian restricted to the unlabeled nodes in 0 . Considerthe heat kernel on this submatrix 9 . Thendescribes heat diffusion on the unlabeled subgraph withDirichlet boundary conditions on the labeled nodes. TheGreens function  is the inverse operator of the restrictedLaplacian,   , which can be expressed in terms ofthe integral over time of the heat kernel 4I 3TE 6The harmonic solution 5 can then be written as  xE, orjiIVKXVKJK  i 7Expression 7 shows that this approach can be viewed asa kernel classifier with the kernel  and a specific form ofkernel machine. See also Chung  Yau, 2000, where anormalized Laplacian is used instead of the combinatorialLaplacian. From 6 we also see that the spectrum of  isK , where  K  is the spectrum of . This indicatesa connection to the work of Chapelle et al. 2002, who manipulate the eigenvalues of the Laplacian to create variouskernels. A related approach is given by Belkin and Niyogi2002, who propose to regularize functions on 0 by selecting the top k normalized eigenvectors ofcorrespondingto the smallest eigenvalues, thus obtaining the best fit toin the least squares sense. We remark that ourfits thelabeled data exactly, while the order k approximation maynot.3.3. Spectral Clustering and Graph MincutsThe normalized cut approach of Shi and Malik 2000 hasas its objective function the minimization of the RaleighquotientI  KMJ KMbTiZKKbZ8subject to the constraint. The solution is the secondsmallest eigenvector of the generalized eigenvalue problem  . Yu and Shi 2001 add a grouping bias tothe normalized cut to specify which points should be inthe same group. Since labeled data can be encoded intosuch pairwise grouping constraints, this technique can beapplied to semisupervised learning as well. In general,when E is close to block diagonal, it can be shown thatdata points are tightly clustered in the eigenspace spannedby the first few eigenvectors ofNg et al., 2001a Meila Shi, 2001, leading to various spectral clustering algorithms.Perhaps the most interesting and substantial connection tothe methods we propose here is the graph mincut approachproposed by Blum and Chawla 2001. The starting pointfor this work is also a weighted graph 0 , but the semisupervised learning problem is cast as one of finding aminimum  cut, where negative labeled data is connectedwith large weight to a special source node  , and positivelabeled data is connected to a special sink node  . A minimum   cut, which is not necessarily unique, minimizes the7 objective function 5  ZKh MJLKM bTiand corresponds to a function2aT.9Y.f  thesolutions can be obtained using linear programming. Thecorresponding random field model is a traditional fieldover the label space T.f. , but the field is pinned onthe labeled entries. Because of this constraint, approximation methods based on rapidly mixing Markov chains thatapply to the ferromagnetic Ising model unfortunately cannot be used. Moreover, multilabel extensions are generallyNPhard in this framework. In contrast, the harmonic solution can be computed efficiently using matrix methods,even in the multilabel case, and inference for the Gaussianrandom field can be efficiently and accurately carried outusing loopy belief propagation Weiss et al., 2001.4. Incorporating Class Prior KnowledgeTo go fromto labels, the obvious decision rule is toassign label 1 to node b ifbZ, and label 0 otherwise. We call this rule the harmonic threshold abbreviatedthresh below. In terms of the random walk interpretation, ifbZ, then starting at b , the random walk ismore likely to reach a positively labeled point before a negatively labeled point. This decision rule works well whenthe classes are well separated. However in real datasets,classes are often not ideally separated, and usingas istends to produce severely unbalanced classification.The problem stems from the fact that E , which specifiesthe data manifold, is often poorly estimated in practice anddoes not reflect the classification goal. In other words, weshould not fully trust the graph structure. The class priorsare a valuable piece of complementary information. Letsassume the desirable proportions for classes 1 and 0 are and .T , respectively, where these values are either givenby an oracle or estimated from labeled data. We adopt asimple procedure called class mass normalization CMNto adjust the class distributions to match the priors. Definethe mass of class 1 to be  Kb , and the mass of class 0to be  K .T b . Class mass normalization scales thesemasses so that an unlabeled point b is classified as class 1iffbKb.T.TbK.Tb9This method extends naturally to the general multilabelcase.5. Incorporating External ClassifiersOften we have an external classifier at hand, which is constructed on labeled data alone. In this section we suggesthow this can be combined with harmonic energy minimization. Assume the external classifier produces labels  onthe unlabeled data   can be 01 or soft labels in  .  . Wecombine   with harmonic energy minimization by a simple modification of the graph. For each unlabeled node b inthe original graph, we attach a dongle node which is a labeled node with value  K , let the transition probability fromb to its dongle be  , and discount all other transitions from bby .T . We then perform harmonic energy minimizationon this augmented graph. Thus, the external classifier introduces assignment costs to the energy function, whichplay the role of vertex potentials in the random field. Itis not difficult to show that the harmonic solution on theaugmented graph is, in the random walk view,GT.T.T, 10We note that throughout the paper we have assumed thelabeled data to be noise free, and so clamping their valuesmakes sense. If there is reason to doubt this assumption, itwould be reasonable to attach dongles to labeled nodes aswell, and to move the labels to these new nodes.6. Learning the Weight Matrix Previously we assumed that the weight matrix E is givenand fixed. In this section, we investigate learning weightfunctions of the form given by equation 1. We will learnthe Ws from both labeled and unlabeled data this will beshown to be useful as a feature selection mechanism whichbetter aligns the graph structure with the data.The usual parameter learning criterion is to maximize thelikelihood of labeled data. However, the likelihood criterion is not appropriate in this case because thevalues forlabeled data are fixed during training, and moreover likelihood doesnt make sense for the unlabeled data because wedo not have a generative model. We propose instead to useaverage label entropy as a heuristic criterion for parameterlearning. The average label entropy  of the fieldisdefined asI.VKXKb 11where  K bITbbT.Tb.Tbis the entropy of the field at the individual unlabeled datapoint b . Here we use the random walk interpretation of,relying on the maximum principle of harmonic functionswhich guarantees that Cb. for b. . Smallentropy implies thatb is close to 0 or 1 this capturesthe intuition that a good E equivalently, a good set of hyperparameters  W  should result in a confident labeling.There are of course many arbitrary labelings of the data thathave low entropy, which might suggest that this criterionwill not work. However, it is important to point out thatwe are constrainingon the labeled datamost of thesearbitrary low entropy labelings are inconsistent with thisconstraint. In fact, we find that the space of low entropylabelings achievable by harmonic energy minimization issmall and lends itself well to tuning the Wparameters.There is a complication, however, which is that  has aminimum at 0 as Wa . As the length scale approacheszero, the tail of the weight function 1 is increasingly sensitive to the distance. In the end, the label predicted for anunlabeled example is dominated by its nearest neighborslabel, which results in the following equivalent labelingprocedure 1 starting from the labeled data set, find theunlabeled point  that is closest to some labeled point  2 label  with   s label, put  in the labeled set and repeat. Since these are hard labels, the entropy is zero. Thissolution is desirable only when the classes are extremelywell separated, and can be expected to be inferior otherwise.This complication can be avoided by smoothing the transition matrix. Inspired by analysis of the PageRank algorithm in Ng et al., 2001b, we replace  with the smoothedmatrix C.TfR , where  is the uniform matrixwith entries  KNM  . .We use gradient descent to find the hyperparameters Wthatminimize  . The gradient is computed asW.VKXj.Tbb bW12where the values bWcan be read off the vector W, which is given by W T , S W  ,W13using the fact that    T 3 . BothWand,Ware submatrices ofW.Tf . Since the original transition matrix  is obtained by normalizing the weight matrix E , we have thatkKMW TkKMjX XJK14Finally,   gJKMWKTWMZW.In the above derivation we use as label probabilities directly that is, k class  K   .b . If we incorporate class prior information, or combine harmonic energyminimization with other classifiers, it makes sense to minimize entropy on the combined probabilities. For instance,if we incorporate a class prior using CMN, the probabilityis given by,bIRTbRT b.T.Tji15and we use this probability in place ofb in 11. Thederivation of the gradient descent rule is a straightforwardextension of the above analysis.7. Experimental ResultsWe first evaluate harmonic energy minimization on a handwritten digits dataset, originally from the Cedar Buffalobinary digits database Hull, 1994. The digits were preprocessed to reduce the size of each image down to a. B . grid by downsampling and Gaussian smoothing, with pixel values ranging from 0 to 255 Le Cunet al., 1990. Each image is thus represented by a 256dimensional vector. We compute the weight matrix 1 withW  9 . For each labeled set size  tested, we perform0 20 40 60 80 1000.50.550.60.650.70.750.80.850.90.951labeled set sizeaccuracyCMN1NNRBFthresh0 20 40 60 80 100 120 140 160 180 2000.50.550.60.650.70.750.80.850.90.951labeled set sizeaccuracyCMN1NNRBFthresh0 10 20 30 40 50 60 70 80 90 1000.50.550.60.650.70.750.80.850.90.951labeled set sizeaccuracyCMN  VPthresh  VPVPCMNthreshFigure 3. Harmonic energy minimization on digits 1 vs. 2 left and on all 10 digits middle and combining votedperceptron withharmonic energy minimization on odd vs. even digits right0 20 40 60 80 1000.50.550.60.650.70.750.80.850.90.951labeled set sizeaccuracyCMNthreshVP1NN0 20 40 60 80 1000.50.550.60.650.70.750.80.850.90.951labeled set sizeaccuracyCMNthreshVP1NN0 20 40 60 80 1000.50.550.60.650.70.750.80.850.90.951labeled set sizeaccuracyCMNthreshVP1NNFigure 4. Harmonic energy minimization on PC vs. MAC left, baseball vs. hockey middle, and MSWindows vs. MAC right10 trials. In each trial we randomly sample labeled datafrom the entire dataset, and use the rest of the images asunlabeled data. If any class is absent from the sampled labeled set, we redo the sampling. For methods that incorporate class priors  , we estimate  from the labeled set withLaplace add one smoothing.We consider the binary problem of classifying digits 1vs. 2, with 1100 images in each class. We report average accuracy of the following methods on unlabeled datathresh, CMN, 1NN, and a radial basis function classifierRBF which classifies to class 1 iff E  E.T .RBF and 1NN are used simply as baselines. The results areshown in Figure 3. Clearly thresh performs poorly, becausethe values ofji are generally close to 1, so the majority of examples are classified as digit 1. This shows theinadequacy of the weight function 1 based on pixelwiseEuclidean distance. However the relative rankings ofjiare useful, and when coupled with class prior informationsignificantly improved accuracy is obtained. The greatestimprovement is achieved by the simple method CMN. Wecould also have adjusted the decision threshold on threshssolution , so that the class proportion fits the prior  . Thismethod is inferior to CMN due to the error in estimating  ,and it is not shown in the plot. These same observationsare also true for the experiments we performed on severalother binary digit classification problems.We also consider the 10way problem of classifying digits0 through 9. We report the results on a dataset with intentionally unbalanced class sizes, with 455, 213, 129, 100,754, 970, 275, 585, 166, 353 examples per class, respectively noting that the results on a balanced dataset are similar. We report the average accuracy of thresh, CMN, RBF,and 1NN. These methods can handle multiway classification directly, or with slight modification in a oneagainstallfashion. As the results in Figure 3 show, CMN again improves performance by incorporating class priors.Next we report the results of document categorization experiments using the 20 newsgroups dataset. We pickthree binary problems PC number of documents 982vs. MAC 961, MSWindows 958 vs. MAC, and baseball 994 vs. hockey 999. Each document is minimallyprocessed into a tf.idf vector, without applying header removal, frequency cutoff, stemming, or a stopword list. Twodocuments  are connected by an edge if  is among  s10 nearest neighbors or if  is among  s 10 nearest neighbors, as measured by cosine similarity. We use the following weight function on the edgesJOPQT.  .Tj 16We use onenearest neighbor and the voted perceptron algorithm Freund  Schapire, 1999 10 epochs with a linear kernel as baselinesour results with support vector machines are comparable. The results are shown in Figure4. As before, each point is the average of 10 random trials. For this data, harmonic energy minimization performsmuch better than the baselines. The improvement from theclass prior, however, is less significant. An explanation forwhy this approach to semisupervised learning is so effective on the newsgroups data may lie in the common use ofquotations within a topic thread document Zquotes partof document   , quotes part of Z, and so on. Thus,although documents far apart in the thread may be quitedifferent, they are linked by edges in the graphical representation of the data, and these links are exploited by thelearning algorithm.7.1. Incorporating External ClassifiersWe use the votedperceptron as our external classifier. Foreach random trial, we train a votedperceptron on the labeled set, and apply it to the unlabeled set. We then use the01 hard labels for dongle values  , and perform harmonicenergy minimization with 10. We use j. .We evaluate on the artificial but difficult binary problemof classifying odd digits vs. even digits that is, we group1,3,5,7,9 and 2,4,6,8,0 into two classes. There are 400images per digit. We use second order polynomial kernelin the votedperceptron, and train for 10 epochs. Figure 3shows the results. The accuracy of the votedperceptronon unlabeled data, averaged over trials, is marked VP inthe plot. Independently, we run thresh and CMN. Next wecombine thresh with the votedperceptron, and the resultis marked threshVP. Finally, we perform class mass normalization on the combined result and get CMNVP. Thecombination results in higher accuracy than either methodalone, suggesting there is complementary information usedby each.7.2. Learning the Weight Matrix ETo demonstrate the effects of estimating E , results on a toydataset are shown in Figure 5. The upper grid is slightlytighter than the lower grid, and they are connected by a fewdata points. There are two labeled examples, marked withlarge symbols. We learn the optimal length scales for thisdataset by minimizing entropy on unlabeled data.To simplify the problem, we first tie the length scales inthe two dimensions, so there is only a single parameter to learn. As noted earlier, without smoothing, the entropyapproaches the minimum at 0 as a . Under such conditions, the results of harmonic energy minimization areusually undesirable, and for this dataset the tighter gridinvades the sparser one as shown in Figure 5a. Withsmoothing, the nuisance minimum at 0 gradually disappears as the smoothing factor  grows, as shown in Figure4 2 0 2 43210123454 2 0 2 4321012345a b0.2 0.4 0.6 0.8 1 1.2 1.40.70.750.80.850.90.951entropy0.10.010.0010.0001unsmoothedcFigure 5. The effect of parameter  on harmonic energy minimization. a If unsmoothed,as , and the algorithmperforms poorly. b Result at optimal  , smoothed with c Smoothing helps to remove the entropy minimum.5c. When we set  . , the minimum entropy is 0.898bits at     . Harmonic energy minimization under thislength scale is shown in Figure 5b, which is able to distinguish the structure of the two grids.If we allow a separate  for each dimension, parameterlearning is more dramatic. With the same smoothing of  . ,  keeps growing towards infinity we use . for computation while  stabilizes at 0.65,and we reach a minimum entropy of 0.619 bits. In thiscase  a is legitimate it means that the learning algorithm has identified the  direction as irrelevant, basedon both the labeled and unlabeled data. Harmonic energyminimization under these parameters gives the same classification as shown in Figure 5b.Next we learn  s for all 256 dimensions on the 1 vs. 2digits dataset. For this problem we minimize the entropywith CMN probabilities 15. We randomly pick a split of92 labeled and 2108 unlabeled examples, and start with alldimensions sharing the same   f as in previous experiments. Then we compute the derivatives of  for eachdimension separately, and perform gradient descent to minimize the entropy. The result is shown in Table 1. Asentropy decreases, the accuracy of CMN and thresh bothincrease. The learned  s shown in the rightmost plot ofFigure 6 range from 181 black to 465 white. A small  Kblack indicates that the weight is more sensitive to variations in that dimension, while the opposite is true for largeK white. We can discern the shapes of a black 1 anda white 2 in this figure that is, the learned parameters bits CMN threshstart 0.6931 97.25  0.73  94.70  1.19 end 0.6542 98.56  0.43  98.02  0.39 Table 1. Entropy of CMN and accuracies before and after learning s on the 1 vs. 2 dataset.Figure 6. Learned  s for 1 vs. 2 dataset. From left to rightaverage 1, average 2, initial  s, learned  s.exaggerate variations within class 1 while suppressingvariations within class 2. We have observed that withthe default parameters, class 1 has much less variationthan class 2 thus, the learned parameters are, in effect,compensating for the relative tightness of the two classes infeature space.8. ConclusionWe have introduced an approach to semisupervised learning based on a Gaussian random field model defined withrespect to a weighted graph representing labeled and unlabeled data. Promising experimental results have been presented for text and digit classification, demonstrating thatthe framework has the potential to effectively exploit thestructure of unlabeled data to improve classification accuracy. The underlying random field gives a coherent probabilistic semantics to our approach, but this paper has concentrated on the use of only the mean of the field, which ischaracterized in terms of harmonic functions and spectralgraph theory. The fully probabilistic framework is closelyrelated to Gaussian process classification, and this connection suggests principled ways of incorporating class priorsand learning hyperparameters in particular, it is naturalto apply evidence maximization or the generalization error bounds that have been studied for Gaussian processesSeeger, 2002. Our work in this direction will be reportedin a future publication.ReferencesBelkin, M.,  Niyogi, P. 2002. Using manifold structurefor partially labelled classification. Advances in NeuralInformation Processing Systems, 15.Blum, A.,  Chawla, S. 2001. Learning from labeled andunlabeled data using graph mincuts. Proc. 18th International Conf. on Machine Learning.Boykov, Y., Veksler, O.,  Zabih, R. 2001. Fast approximate energy minimization via graph cuts. IEEE Trans.on Pattern Analysis and Machine Intelligence, 23.Chapelle, O., Weston, J.,  Scholkopf, B. 2002. Clusterkernels for semisupervised learning. Advances in Neural Information Processing Systems, 15.Chung, F.,  Yau, S. 2000. Discrete Greens functions.Journal of Combinatorial Theory A pp. 191214.Doyle, P.,  Snell, J. 1984. Random walks and electricnetworks. Mathematical Assoc. of America.Freund, Y.,  Schapire, R. E. 1999. Large margin classification using the perceptron algorithm. Machine Learning, 373, 277296.Hull, J. J. 1994. A database for handwritten text recognition research. IEEE Transactions on Pattern Analysisand Machine Intelligence, 16.Kondor, R. I.,  Lafferty, J. 2002. Diffusion kernels ongraphs and other discrete input spaces. Proc. 19th International Conf. on Machine Learning.Le Cun, Y., Boser, B., Denker, J. S., Henderson, D.,Howard, R. E., Howard, W.,  Jackel, L. D. 1990.Handwritten digit recognition with a backpropagationnetwork. Advances in Neural Information ProcessingSystems, 2.Meila, M.,  Shi, J. 2001. A random walks view of spectral segmentation. AISTATS.Ng, A., Jordan, M.,  Weiss, Y. 2001a. On spectral clustering Analysis and an algorithm. Advances in NeuralInformation Processing Systems, 14.Ng, A. Y., Zheng, A. X.,  Jordan, M. I. 2001b. Linkanalysis, eigenvectors and stability. International JointConference on Artificial Intelligence IJCAI.Seeger, M. 2001. Learning with labeled and unlabeleddata Technical Report. University of Edinburgh.Seeger, M. 2002. PACBayesian generalization errorbounds for Gaussian process classification. Journal ofMachine Learning Research, 3, 233269.Shi, J.,  Malik, J. 2000. Normalized cuts and imagesegmentation. IEEE Transactions on Pattern Analysisand Machine Intelligence, 22, 888905.Szummer, M.,  Jaakkola, T. 2001. Partially labeled classification with Markov random walks. Advances in Neural Information Processing Systems, 14.Weiss, Y., ,  Freeman, W. T. 2001. Correctness of beliefpropagation in Gaussian graphical models of arbitrarytopology. Neural Computation, 13, 21732200.Yu, S. X.,  Shi, J. 2001. Grouping with bias. Advancesin Neural Information Processing Systems, 14.
