Ensemble Methods in Machine LearningThomas G DietterichOregon State University Corvallis Oregon USAtgdcsorsteduWWW home page httpwwwcsorstedutgdAbstract Ensemble methods are learning algorithms that construct aset of classiers and then classify new data points by taking a weightedvote of their predictions The original ensemble method is Bayesian averaging but more recent algorithms include errorcorrecting output codingBagging and boosting This paper reviews these methods and explainswhy ensembles can often perform better than any single classier Someprevious studies comparing ensemble methods are reviewed and somenew experiments are presented to uncover the reasons that Adaboostdoes not overt rapidly IntroductionConsider the standard supervised learning problem A learning program is giventraining examples of the form fx y     xm ymg for some unknown function y  fx The xi values are typically vectors of the form hxi xi     xiniwhose components are discrete or realvalued such as height weight color ageand so on These are also called the features of xi Let us use the notation xijto refer to the jth feature of xi In some situations we will drop the i subscriptwhen it is implied by the contextThe y values are typically drawn from a discrete set of classes f    Kgin the case of classication or from the real line in the case of regression Inthis chapter we will consider only classication The training examples may becorrupted by some random noiseGiven a set S of training examples a learning algorithm outputs a classierThe classier is an hypothesis about the true function f  Given new x values itpredicts the corresponding y values I will denote classiers by h     hLAn ensemble of classiers is a set of classiers whose individual decisions arecombined in some way typically by weighted or unweighted voting to classifynew examples One of the most active areas of research in supervised learning hasbeen to study methods for constructing good ensembles of classiers The maindiscovery is that ensembles are often much more accurate than the individualclassiers that make them upA necessary and sucient condition for an ensemble of classiers to be moreaccurate than any of its individual members is if the classiers are accurate anddiverse Hansen  Salamon  An accurate classier is one that has anerror rate of better than random guessing on new x values Two classiers arediverse if they make dierent errors on new data points To see why accuracyand diversity are good imagine that we have an ensemble of three classiersfh h hg and consider a new case x If the three classiers are identical ienot diverse then when hx is wrong hx and hx will also be wrongHowever if the errors made by the classiers are uncorrelated then when hxis wrong hx and hx may be correct so that a majority vote will correctlyclassify x More precisely if the error rates of L hypotheses h are all equal top   and if the errors are independent then the probability that the majorityvote will be wrong will be the area under the binomial distribution where morethan L hypotheses are wrong Figure  shows this for a simulated ensembleof  hypotheses each having an error rate of  The area under the curve for or more hypotheses being simultaneously wrong is  which is much lessthan the error rate of the individual hypotheses00.020.040.060.080.10.120.140.160.180.20 5 10 15 20ProbabilityNumber of classifiers in errorFig  The probability that exactly  of  hypotheses will make an error assumingeach hypothesis has an error rate of  and makes its errors independently of the otherhypothesesOf course if the individual hypotheses make uncorrelated errors at rates exceeding  then the error rate of the voted ensemble will increase as a result ofthe voting Hence one key to successful ensemble methods is to construct individual classiers with error rates below  whose errors are at least somewhatuncorrelatedThis formal characterization of the problem is intriguing but it does notaddress the question of whether it is possible in practice to construct good ensembles Fortunately it is often possible to construct very good ensembles Thereare three fundamental reasons for thisThe rst reason is statistical A learning algorithm can be viewed as searching a space H of hypotheses to identify the best hypothesis in the space Thestatistical problem arises when the amount of training data available is too smallcompared to the size of the hypothesis space Without sucient data the learning algorithm can nd many dierent hypotheses in H that all give the sameaccuracy on the training data By constructing an ensemble out of all of theseaccurate classiers the algorithm can average their votes and reduce the riskof choosing the wrong classier Figure top left depicts this situation Theouter curve denotes the hypothesis space H The inner curve denotes the set ofhypotheses that all give good accuracy on the training data The point labeled fis the true hypothesis and we can see that by averaging the accurate hypotheseswe can nd a good approximation to f H HHStatistical ComputationalRepresentationalh1h3h4h2f ffh1h2 h3h1h2h3Fig  Three fundamental reasons why an ensemble may work better than a singleclassierThe second reason is computational Many learning algorithms work by performing some form of local search that may get stuck in local optima For example neural network algorithms employ gradient descent to minimize an errorfunction over the training data and decision tree algorithms employ a greedysplitting rule to grow the decision tree In cases where there is enough trainingdata so that the statistical problem is absent it may still be very dicultcomputationally for the learning algorithm to nd the best hypothesis Indeedoptimal training of both neural networks and decisions trees is NPhard Hyal Rivest  Blum  Rivest  An ensemble constructed by running thelocal search from many dierent starting points may provide a better approximation to the true unknown function than any of the individual classiers asshown in Figure  top rightThe third reason is representational In most applications of machine learning the true function f cannot be represented by any of the hypotheses in HBy forming weighted sums of hypotheses drawn from H it may be possibleto expand the space of representable functions Figure  bottom depicts thissituationThe representational issue is somewhat subtle because there are many learning algorithms for whichH is in principle the space of all possible classiers Forexample neural networks and decision trees are both very exible algorithmsGiven enough training data they will explore the space of all possible classiersand several people have proved asymptotic representation theorems for themHornik Stinchcombe  White  Nonetheless with a nite training sample these algorithms will explore only a nite set of hypotheses and they willstop searching when they nd an hypothesis that ts the training data Hencein Figure  we must consider the space H to be the eective space of hypothesessearched by the learning algorithm for a given training data setThese three fundamental issues are the three most important ways in whichexisting learning algorithms fail Hence ensemble methods have the promise ofreducing and perhaps even eliminating these three key shortcomings of standard learning algorithms Methods for Constructing EnsemblesMany methods for constructing ensembles have been developed Here we willreview general purpose methods that can be applied to many dierent learningalgorithms Bayesian Voting Enumerating the HypothesesIn a Bayesian probabilistic setting each hypothesis h denes a conditional probability distribution hx  P fx  yjx h Given a new data point x and atraining sample S the problem of predicting the value of fx can be viewedas the problem of computing P fx  yjSx We can rewrite this as weightedsum over all hypotheses in HP fx  yjSx XhHhxP hjSWe can view this as an ensemble method in which the ensemble consists of all ofthe hypotheses inH each weighted by its posterior probability P hjS By Bayesrule the posterior probability is proportional to the likelihood of the trainingdata times the prior probability of hP hjS  P SjhP hIn some learning problems it is possible to completely enumerate each h  Hcompute P Sjh and P h and after normalization evaluate this Bayesiancommittee Furthermore if the true function f is drawn from H according toP h then the Bayesian voting scheme is optimalBayesian voting primarily addresses the statistical component of ensembles When the training sample is small many hypotheses h will have significantly large posterior probabilities and the voting process can average these tomarginalize away the remaining uncertainty about f  When the training sample is large typically only one hypothesis has substantial posterior probabilityand the ensemble eectively shrinks to contain only a single hypothesisIn complex problems whereH cannot be enumerated it is sometimes possibleto approximate Bayesian voting by drawing a random sample of hypothesesdistributed according to P hjS Recent work on Markov chain Monte Carlomethods Neal  seeks to develop a set of tools for this taskThe most idealized aspect of the Bayesian analysis is the prior belief P h Ifthis prior completely captures all of the knowledge that we have about f beforewe obtain S then by denition we cannot do better But in practice it is oftendicult to construct a space H and assign a prior P h that captures our priorknowledge adequately Indeed often H and P h are chosen for computationalconvenience and they are known to be inadequate In such cases the Bayesiancommittee is not optimal and other ensemble methods may produce betterresults In particular the Bayesian approach does not address the computationaland representational problems in any signicant way Manipulating the Training ExamplesThe second method for constructing ensembles manipulates the training examples to generate multiple hypotheses The learning algorithm is run several timeseach time with a dierent subset of the training examples This technique worksespecially well for unstable learning algorithmsalgorithms whose output classier undergoes major changes in response to small changes in the training dataDecisiontree neural network and rule learning algorithms are all unstable Linear regression nearest neighbor and linear threshold algorithms are generallyvery stableThe most straightforward way of manipulating the training set is called Bagging On each run Bagging presents the learning algorithm with a training setthat consists of a sample of m training examples drawn randomly with replacement from the original training set of m items Such a training set is called abootstrap replicate of the original training set and the technique is called bootstrap aggregation from which the term Bagging is derived Breiman  Eachbootstrap replicate contains on the average  of the original training setwith several training examples appearing multiple timesAnother training set sampling method is to construct the training sets byleaving out disjoint subsets of the training data For example the training setcan be randomly divided into  disjoint subsets Then  overlapping trainingsets can be constructed by dropping out a dierent one of these  subsetsThis same procedure is employed to construct training sets for fold crossvalidation so ensembles constructed in this way are sometimes called crossvalidated committees Parmanto Munro  Doyle The third method for manipulating the training set is illustrated by theAdaBoost algorithm developed by Freund and Schapire    Like Bagging AdaBoost manipulates the training examples to generatemultiple hypotheses AdaBoost maintains a set of weights over the trainingexamples In each iteration  the learning algorithm is invoked to minimizethe weighted error on the training set and it returns an hypothesis h Theweighted error of h is computed and applied to update the weights on thetraining examples The eect of the change in weights is to place more weighton training examples that were misclassied by h and less weight on examplesthat were correctly classied In subsequent iterations therefore AdaBoostconstructs progressively more dicult learning problemsThe nal classier hf x P whx is constructed by a weighted voteof the individual classiers Each classier is weighted by w according to itsaccuracy on the weighted training set that it was trained onRecent research Schapire  Singer  has shown that AdaBoost can beviewed as a stagewise algorithm for minimizing a particular error function Todene this error function suppose that each training example is labeled as or  corresponding to the positive and negative examples Then the quantitymi  yihxi is positive if h correctly classies xi and negative otherwise Thisquantity mi is called the margin of classier h on the training data AdaBoostcan be seen as trying to minimizeXiexpyiXwhxi which is the negative exponential of the margin of the weighted voted classierThis can also be viewed as attempting to maximize the margin on the trainingdata Manipulating the Input FeaturesA third general technique for generating multiple classiers is to manipulatethe set of input features available to the learning algorithm For example in aproject to identify volcanoes on Venus Cherkauer  trained an ensembleof  neural networks The  networks were based on  dierent subsets ofthe  available input features and  dierent network sizes The input featuresubsets were selected by hand to group together features that were based ondierent image processing operations such as principal component analysis andthe fast fourier transform The resulting ensemble classier was able to matchthe performance of human experts in identifying volcanoes Tumer and Ghosh applied a similar technique to a sonar dataset with  input featuresHowever they found that deleting even a few of the input features hurt theperformance of the individual classiers so much that the voted ensemble didnot perform very well Obviously this technique only works when the inputfeatures are highly redundant Manipulating the Output TargetsA fourth general technique for constructing a good ensemble of classiers is tomanipulate the y values that are given to the learning algorithm Dietterich Bakiri  describe a technique called errorcorrecting output coding Supposethat the number of classes K is large Then new learning problems can beconstructed by randomly partioning the K classes into two subsets A and BThe input data can then be relabeled so that any of the original classes in setA are given the derived label  and the original classes in set B are giventhe derived label  This relabeled data is then given to the learning algorithmwhich constructs a classier h By repeating this process L times generatingdierent subsets A and B we obtain an ensemble of L classiers h     hLNow given a new data point x how should we classify it The answer is tohave each h classify x If hx   then each class in A receives a vote Ifhx   then each class in B receives a vote After each of the L classiershas voted the class with the highest number of votes is selected as the predictionof the ensembleAn equivalent way of thinking about this method is that each class j isencoded as an Lbit codeword Cj  where bit  is  if and only if j  B Theth learned classier attempts to predict bit  of these codewords When the Lclassiers are applied to classify a new point x their predictions are combinedinto an Lbit string We then choose the class j whose codeword Cj is closest inHamming distance to the Lbit output string Methods for designing good errorcorrecting codes can be applied to choose the codewords Cj or equivalentlysubsets A and BDietterich and Bakiri report that this technique improves the performance ofboth the C decision tree algorithm and the backpropagation neural networkalgorithm on a variety of dicult classication problems Recently Schapire has shown how AdaBoost can be combined with errorcorrecting output coding to yield an excellent ensemble classication method that he calls AdaBoostOC The performance of the method is superior to the ECOC methodand to Bagging but essentially the same as another quite complex algorithmcalled AdaBoostM Hence the main advantage of AdaBoostOC is implementation simplicity It can work with any learning algorithm for solving classproblemsRicci and Aha  applied a method that combines errorcorrecting output coding with feature selection When learning each classier h they applyfeature selection techniques to choose the best features for learning that classierThey obtained improvements in  out of  tasks with this approach Injecting RandomnessThe last general purpose method for generating ensembles of classiers is toinject randomness into the learning algorithm In the backpropagation algorithmfor training neural networks the initial weights of the network are set randomlyIf the algorithm is applied to the same training examples but with dierentinitial weights the resulting classier can be quite dierent Kolen  PollackWhile this is perhaps the most common way of generating ensembles of neural networks manipulating the training set may be more eective A study byParmanto Munro and Doyle  compared this technique to Bagging and tofold crossvalidated committees They found that crossvalidated committeesworked best Bagging second best and multiple random initial weights thirdbest on one synthetic data set and two medical diagnosis data setsFor the C decision tree algorithm it is also easy to inject randomnessKwok  Carter  Dietterich  The key decision of C is to choose afeature to test at each internal node in the decision tree At each internal nodeC applies a criterion known as the information gain ratio to rankorder thevarious possible feature tests It then chooses the topranked featurevalue testFor discretevalued features with V values the decision tree splits the data intoV subsets depending on the value of the chosen feature For realvalued featuresthe decision tree splits the data into  subsets depending on whether the valueof the chosen feature is above or below a chosen threshold Dietterich implemented a variant of C that chooses randomly with equal probabilityamong the top  best tests Figure  compares the performance of a singlerun of C to ensembles of  classiers over  dierent data sets For eachdata set a point is plotted If that point lies below the diagonal line then theensemble has lower error rate than C We can see that nearly all of the pointslie below the line A statistical analysis shows that the randomized trees dostatistically signicantly better than a single decision tree on  of the data setsand statistically the same in the remaining  data setsAli  Pazzani  injected randomness into the FOIL algorithm for learning Prologstyle rules FOIL works somewhat like C in that it ranks possibleconditions to add to a rule using an informationgain criterion Ali and Pazzani01020304050600 10 20 30 40 50 60200fold Randomized C4.5 percent errorC4.5 percent errorFig  Comparison of the error rate of C to an ensemble of  decision treesconstructed by injecting randomness into C and then taking a uniform votecomputed all candidate conditions that scored within  of the topranked candidate and then applied a weighted random choice algorithm to choose amongthem They compared ensembles of  classiers to a single run of FOIL andfound statistically signicant improvements in  out of  tasks and statisticallysignicant loss of performance in only one task They obtained similar resultsusing fold crossvalidation to construct the training setsRaviv and Intrator  combine bootstrap sampling of the training datawith injecting noise into the input features for the learning algorithm To traineach member of an ensemble of neural networks they draw training exampleswith replacement from the original training data The x values of each trainingexample are perturbed by adding Gaussian noise to the input features Theyreport large improvements in a synthetic benchmark task and a medical diagnosistaskFinally note that Markov chain Monte Carlo methods for constructing Bayesianensembles also work by injecting randomness into the learning process Howeverinstead of taking a uniform vote as we did with the randomized decision treeseach hypothesis receives a vote proportional to its posterior probability Comparing Dierent Ensemble MethodsSeveral experimental studies have been performed to compare ensemble methodsThe largest of these are the studies by Bauer and Kohavi  and by Dietterich Table  summarizes the results of Dietterichs study The table showsthat AdaBoost often gives the best results Bagging and randomized trees givesimilar performance although randomization is able to do better in some casesthan Bagging on very large data setsTable  All pairwise combinations of the four ensemble methods Each cell containsthe number of wins losses and ties between the algorithm in that row and the algorithmin that columnC AdaBoost C Bagged CRandom C               Bagged C          AdaBoost C     Most of the data sets in this study had little or no noise When  articialclassication noise was added to the  domains where Bagging and AdaBoostgave dierent performance the results shifted radically as shown in Table Under these conditions AdaBoost overts the data badly while Bagging isshown to work very well in the presence of noise Randomized trees did not dovery wellTable  All pairwise combinations of C AdaBoosted C Bagged C andRandomized C on  domains with  synthetic class label noise Each cell containsthe number of wins losses and ties between the algorithm in that row and the algorithmin that columnC AdaBoost C Bagged CRandom C               Bagged C          AdaBoost C     The key to understanding these results is to return again to the three shortcomings of existing learning algorithms statistical support computation andrepresentation For the decisiontree algorithm C all three of these problems can arise Decision trees essentially partition the input feature space intorectangular regions whose sides are perpendicular to the coordinate axes Eachrectangular region corresponds to one leaf node of the treeIf the true function f can be represented by a small decision tree thenC will work well without any ensemble If the true function can be correctlyrepresented by a large decision tree then C will need a very large trainingdata set in order to nd a good t and the statistical problem will ariseThe computational problem arises because nding the best ie smallestdecision tree consistent with the training data is computationally intractable soC makes a series of decisions greedily If one of these decisions is made incorrectly then the training data will be incorrectly partitioned and all subsequentdecisions are likely to be aected Hence C is highly unstable and smallchanges in the training set can produce large changes in the resulting decisiontreeThe representational problem arises because of the use of rectangular partitions of the input space If the true decision boundaries are not orthogonal tothe coordinate axes then C requires a tree of innite size to represent thoseboundaries correctly Interestingly a voted combination of small decision treesis equivalent to a much larger single tree and hence an ensemble method canconstruct a good approximation to a diagonal decision boundary using severalsmall trees Figure  shows an example of this On the left side of the gureare plotted three decision boundaries constructed by three decision trees eachof which uses  internal nodes On the right is the boundary that results froma simple majority vote of these trees It is equivalent to a single tree with internal nodes and it is much more accurate than any one of the three individualtreesClass 1Class 2Class 1Class 2Fig  The left gure shows the true diagonal decision boundary and three staircaseapproximations to it of the kind that are created by decision tree algorithms Theright gure shows the voted decision boundary which is a much better approximationto the diagonal boundaryNow let us consider the three algorithms AdaBoost Bagging and Randomized trees Bagging and Randomization both construct each decision treeindependently of the others Bagging accomplishes this by manipulating the input data and Randomization directly alters the choices of C These methodsare acting somewhat like Bayesian voting they are sampling from the space ofall possible hypotheses with a bias toward hypotheses that give good accuracyon the training data Consequently their main eect will be to address the statistical problem and to a lesser extent the computational problem But they donot directly attempt to overcome the representational problemIn contrastAdaBoost constructs each new decision tree to eliminate residual errors that have not been properly handled by the weighted vote of thepreviouslyconstructed treesAdaBoost is directly trying to optimize the weightedvote Hence it is making a direct assault on the representational problem Directly optimizing an ensemble can increase the risk of overtting because thespace of ensembles is usually much larger than the hypothesis space of the original algorithmThis explanation is consistent with the experimental results given above Inlownoise cases AdaBoost gives good performance because it is able to optimize the ensemble without overtting However in highnoise cases AdaBoostputs a large amount of weight on the mislabeled examples and this leads it toovert very badly Bagging and Randomization do well in both the noisy andnoisefree cases because they are focusing on the statistical problem and noiseincreases this statistical problemFinally we can understand that in very large datasets Randomization canbe expected to do better than Bagging because bootstrap replicates of a largetraining set are very similar to the training set itself and hence the learneddecision tree will not be very diverse Randomization creates diversity under allconditions but at the risk of generating lowquality decision treesDespite the plausibility of this explanation there is still one important openquestion concerning AdaBoost Given that AdaBoost aggressively attemptsto maximize the margins on the training set why doesnt it overt more oftenPart of the explanation may lie in the stagewise nature of AdaBoost Ineach iteration it reweights the training examples constructs a new hypothesisand chooses a weight w for that hypothesis It never backs up and modiesthe previous choices of hypotheses or weights that it has made to compensatefor this new hypothesisTo test this explanation I conducted a series of simple experiments on synthetic data Let the true classier f be a simple decision rule that tests just onefeature feature  and assigns the example to class  if the feature is  andto class  if the feature is  Now construct training and testing examples bygenerating feature vectors of length  at random as follows Generate feature the important feature at random Then generate each of the other featuresrandomly to agree with feature  with probability  and to disagree otherwiseAssign labels to each training example according to the true function f  butwith  random classication noise This creates a dicult learning problemfor simple decision rules of this kind decision stumps because all  featuresare correlated with the class Still a large ensemble should be able to do well onthis problem by voting separate decision stumps for each featureI constructed a version ofAdaBoost that works more aggressively than standard AdaBoost After every new hypothesis h is constructed and its weightassigned my version performs a gradient descent search to minimize the negativeexponential margin equation  Hence this algorithm reconsiders the weightsof all of the learned hypotheses after each new hypothesis is added Then itreweights the training examples to reect the revised hypothesis weightsFigure  shows the results when training on a training set of size  The plotconrms our explanation The Aggressive AdaBoost initially has much highererror rates on the test set than Standard AdaBoost It then gradually improves Meanwhile Standard AdaBoost initially obtains excellent performanceon the test set but then it overts as more and more classiers are added to theensemble In the limit both ensembles should have the same representationalproperties because they are both minimizing the same function equation But we can see that the exceptionally good performance of StandardAdaBooston this problem is due to the stagewise optimization process which is slow tot the data1601651701751801851901952002052101 10 100 1000Errors out of 1000 on the test data setIterations of AdaboostStandard AdaboostAggressive AdaboostFig  Aggressive AdaBoost exhibits much worse performance than Standard AdaBoost on a challenging synthetic problem ConclusionsEnsembles are wellestablished as a method for obtaining highly accurate classiers by combining less accurate ones This paper has provided a brief survey ofmethods for constructing ensembles and reviewed the three fundamental reasonswhy ensemble methods are able to outperform any single classier within theensemble The paper has also provided some experimental results to elucidateone of the reasons why AdaBoost performs so wellOne open question not discussed in this paper concerns the interaction between AdaBoost and the properties of the underlying learning algorithm Mostof the learning algorithms that have been combined with AdaBoost have beenalgorithms of a global character ie algorithms that learn a relatively lowdimensional decision boundary It would be interesting to see whether localalgorithms such as radial basis functions and nearest neighbor methods can beprotably combined viaAdaBoost to yield interesting new learning algorithmsBibliographyAli K M  Pazzani M J  Error reduction through learning multipledescriptions Machine Learning    Bauer E  Kohavi R  An empirical comparison of voting classicationalgorithms Bagging boosting and variants Machine Learning   Blum A  Rivest R L  Training a node neural network is NPComplete Extended abstract In Proceedings of the  Workshop onComputational Learning Theory pp   San Francisco CA MorganKaufmannBreiman L  Bagging predictors Machine Learning    Cherkauer K J  Human expertlevel performance on a scienticimage analysis task by a system using combined articial neural networks In Chan P Ed Working Notes of the AAAI Workshopon Integrating Multiple Learned Models pp   Available fromhttpwwwcsfiteduimlmDietterich T G  An experimental comparison of three methods forconstructing ensembles of decision trees Bagging boosting and randomization Machine LearningDietterich T G  Bakiri G  Solving multiclass learning problems viaerrorcorrecting output codes Journal of Articial Intelligence Research  Freund Y  Schapire R E  A decisiontheoretic generalization ofonline learning and an application to boosting Tech rep ATT BellLaboratories Murray Hill NJFreund Y  Schapire R E  Experiments with a new boosting algorithm In Proc th International Conference on Machine Learning pp  Morgan KaufmannHansen L  Salamon P  Neural network ensembles IEEE TransPattern Analysis and Machine Intell   Hornik K Stinchcombe M  White H  Universal approximationof an unknown mapping and its derivatives using multilayer feedforwardnetworks Neural Networks   Hyal L  Rivest R L  Constructing optimal binary decision trees isNPComplete Information Processing Letters    Kolen J F  Pollack J B  Back propagation is sensitive to initialconditions In Advances in Neural Information Processing Systems Vol pp   San Francisco CA Morgan KaufmannKwok S W  Carter C  Multiple decision trees In Schachter R DLevitt T S Kannal L N  Lemmer J F Eds Uncertainty in Articial Intelligence  pp   Elsevier Science AmsterdamNeal R  Probabilistic inference using Markov chain Monte Carlo methods Tech rep CRGTR Department of Computer Science University of Toronto Toronto CAParmanto B Munro P W  Doyle H R  Improving committeediagnosis with resampling techniques In Touretzky D S Mozer M C Hesselmo M E Eds Advances in Neural Information ProcessingSystems Vol  pp   Cambridge MA MIT PressRaviv Y  Intrator N  Bootstrapping with noise An eective regularization technique Connection Science     Ricci F  Aha D W  Extending local learners with errorcorrectingoutput codes Tech rep Naval Center for Applied Research in ArticialIntelligence Washington DCSchapire R E  Using output codes to boost multiclass learning problems In Proceedings of the Fourteenth International Conference on Machine Learning pp   San Francisco CA Morgan KaufmannSchapire R E Freund Y Bartlett P  Lee W S  Boosting the margin A new explanation for the eectiveness of voting methods In FisherD Ed Machine Learning Proceedings of the Fourteenth InternationalConference Morgan KaufmannSchapire R E  Singer Y  Improved boosting algorithms usingcondencerated predictions In Proc th Annu Conf on Comput Learning Theory pp   ACM Press New York NYTumer K  Ghosh J  Error correlation and error reduction in ensembleclassiers Connection Science
