Learning with Multiple Similarity Functions
Maria-Florina Balcan
Computer Science Department
Carnegie Mellon University
ninamfcscmuedu
Avrim Blum
Computer Science Department
Carnegie Mellon University
avrimcscmuedu
Nathan Srebro
Toyota Technological Institute at Chicago
natiuchicagoedu
1 Introduction
Kernel functions have become an extremely popular tool in machine learning, with many applica
tions and an attractive theory [1, 12, 10]. There has also been substantial work on learning kernel
functions from data [7, 11, 2]. A sufficient condition for a kernel to allow for good generalization on
a given learning problem is that it induce a large margin of separation between positive and negative
classes in its implicit space. In recent work [4, 5, 3] we have developed a theory that more broadly
holds for general similarity functions that are not necessarily legal kernel functions. In particular
we have introduced a notion of a good similarity function for a given learning problem that (a) is
fairly natural and intuitive (it does not require an implicit space and allows for functions that are not
positive semi-definite), (b) is a sufficient condition for learning well, and (c) strictly generalizes the
notion of a large-margin kernel function in that any such kernel is also a good similarity function
though not necessarily vice-versa. We also have partial progress on extending the theory of learning
with multiple kernel functions to this more general notion. In this note, we describe the main defi
nitions and results of [4], give our results on learning with multiple similarity functions, and present
several open questions
2 Good Similarity Functions
We consider classification problems specified by a joint distribution P over labeled examples (x,
where ℓ ∈ {−1, 1}. We consider learning a predictor based on both labeled examples drawn from
this distribution, as well as unlabeled examples drawn from the marginal over x. Our goal is to
obtain a predictor with low expected error with respect to P
Our main notion of a good similarity function K is summarized in the following definition. This
definition is based on the intuitive idea that ideally we would like most examples to be substantially
more similar on average to examples of their own label than to examples of the opposite label
However, this is too strong a requirement to capture all large-margin kernel function. So, instead we
relax the condition to instead ask for existence of a non-negligble set R of “representative points
such that most examples are on average more similar to those in R of their own label than to those
in R of the opposite label (this set R need not be known in advance). More formally
Definition 1 A similarity function K : X ×X → [−1, 1] is an (ǫ, γ, τ)-good similarity function
for a learning problem P if there exists a subset R of the domain such that the following conditions
hold
1. A 1 − ǫ probability mass of examples (x, ℓ) satisfy
E(x′,ℓ′)∼P
′K(x, x′) | x′ ∈ R] ≥ γ

2. Pr[R] ≥ τ
where formally we allow membership in R to be probabilistic (R(x) is a probabilistic indicator
function, and all probabilities are over both P and any randomization in R). We say K is (ǫ, γ,
good in hinge loss if we can replace (1) with
1′. ExP

[1 − gx

≤ ǫ, where g(x) = ExPR
′K(x, x′) | x′ ∈ R
That is, rather than defining ǫ to be the probability mass of examples that fail, ǫ is the expected
degree of failure
If the set R of “representative points” is 50/50 positive and negative, we can interpret condition
as stating that most examples x are on average 2γ more similar to random representative examples
x′ of their own label than to random representative examples x′ of the other label. Condition (2) is
that at least a τ fraction of the points should be representative
These definitions have two important properties: (a) they are sufficient for learning, and (b) they are
are satisfied—albeit with some loss in parameters—by any large-margin kernel function. We begin
with sufficiency for learning: in particular, we show that by choosing a set of “landmark” points at
random and then using similarity to these as explicit features, we can convert our learning problem
to one of learning a linear separator of good Lmargin
Theorem 1 Let K be an (ǫ, γ, τ)-good similarity function for a learning problem P . Let S
{x′1, x

2, . . . , x

d} be a (potentially unlabeled) sample of
d



log(2/δ) +
log


points (“landmarks”) drawn from P . Consider the mapping φS : X → Rd defined as follows
φSi(x) = K(x, x

i), i ∈ {1, . . . , d}. Then, with probability at least 1 − δ over the random sample
S, the induced distribution φS(P ) in Rd has a separator of error at most ǫ+ δ relative to L1 margin
at least γ/2, and all examples have L∞ norm at most
Proof Sketch: First, note that since |K(x, x′)| ≤ 1 for all x, x′, we have

Sx



≤
Consider the linear separator α · φS(x) ≥ 0 for α ∈ Rd given by αi = xiRx

i)/d1 where
d1

iRx

i) is the number of landmarks with Rx
′) = 1. This normalization ensures ‖α‖1
1. Moreover, the dot-product ℓ(x)α · φS(x) is an empirical estimate (over S) of the conditional
expectation in condition (1) of Definition 1. Our sample size d is sufficiently large so that with
probability at least 1 − δ/2, the number of representative points d1 in S is large enough so that for
any given x, by Hoeffding bounds, the empirical estimate is within γ/2 of the expectation in
with probability at least 1− δ2/2. This in turn, by Markov’s inequality, implies that with probability
at least 1 − δ, at most a δ fraction of the points x satisfying condition (1) have φS(x) with margin
less than γ/2. Therefore, at most an ǫ + δ fraction of points overall have margin less than γ/2, as
desired
Similarly, for hinge-loss, we can show the following
Theorem 2 Let K be an (ǫ, γ, τ)-good similarity function in hinge-loss for a learning problem
P . For any ǫ1 > 0 and 0 < λ < γǫ1/4 let S = {x′1, x′2, . . . , x′d} be a sample of size d



log(2/δ) + 16 log


drawn from P . With probability at least 1− δ over the random
sample S, the induced distribution φS(P ) in Rd, for φS as defined in Theorem 1, has a separator
achieving hinge-loss at most ǫ + ǫ1 at L1 margin
In particular, the above theorem implies that we can use algorithms designed for learning linear sepa
rators of good L1 margin (for example, Winnow), and get generalization bounds that are logarithmic
in the dimension d = |S| and quadratic in 1/γ
Turning to the connection to large-margin kernels, if a similarity function is positive semidefinite
and “good” in the traditional kernel sense, then we can show it also satisfies Definition 1, though

with some loss in parameters. Recall that a function K : X × X is positive semidefinite iff there
exists a mapping φ : X → H into a Hilbert space H such that K(x, x′) = 〈φ(x), φ(x′)〉. With this
representation of K in mind
Definition 2 We say that a positive semidefinite K is (ǫ, γ)-kernel good in hinge-loss if there exists
a vector β ∈ H, ‖β‖ ≤ 1/γ such that
E(x,y)∼P [[1 − ℓ〈β, φ(x)〉]+] ≤
Theorem 3 If a positive semidefinite K is an (ǫ0, γ)-good kernel in hinge loss for learning problem
(with deterministic labels), then for any ǫ1 > 0 there exists c > 1 such that K is also a (ǫ0

c

, 2ǫ1+ǫ0c )-good similarity function in hinge loss
Note that Theorem 3 implies that the learning procedure given by Theorem 2 can also be applied to
good kernel functions, with roughly a quadratic increase in sample complexity. Despite the increase
in sample complexity, this can be useful for other purposes. For example, inspired by results in our
earlier work [6] this approach has been used for transfer learning by Quattoni, Collins, and Darrell
et al. [9]. In addition, the intuitiveness of Definition 1 may make it easier to design good kernel
functions for a given learning problem
3 Learning with Multiple Similarity Functions
We consider here as in Lanckriet et al. [7] the problem of learning with multiple similarity func
tions. In particular, suppose that rather than having a single similarity function, we were instead
given n functions K1, ...,Kn, and our hope is that some convex combination of them will satisfy
Definition 1. Is this sufficient to be able to learn well, and how does performance degrade with n
The following generalization of Theorem 1 shows that indeed we can still learn well and that the
degradation with n is only slight. (The analog of Theorem 2 can be derived similarly
Theorem 4 Suppose K1, . . . ,Kn are similarity functions such that some (unknown) convex com
bination of them is (ǫ, γ, τ)-good. For any δ > 0, let S = {x′1, x

2, . . . , x

d} be a sample of size
d = 16 log(1/δ)τγ2 drawn from P . Consider the mapping
S : X → Rnd defined as follows
φSi(x) = (K1(x, x

1), . . . ,Kn(x, x

1), . . . ,K1(x, x

d), . . . ,Kn(x, x

d
With probability at least 1 − δ over the random sample S, the induced distribution φS(P ) in Rnd
has a separator of error at most ǫ + δ at L1, L∞ margin at least
Before proving Theorem 4, notice that the margin achieved is identical to that in Theorem 1 when we
had just a single similarity function. The only degradation is in the dimension, which has increased
by a factor of n. However, because algorithms for learning linear separators with good L1 margin
have sample-size bounds that are only logarithmic in dimension, this causes only an O(log n) mul
tiplicative penalty in the number of labeled examples needed to achieve good generalization though
running time would be impacted linearly with n
Proof: Let K = α1K1 + . . . + αnKn be an (ǫ, γ, τ)-good convex-combination of the Ki. By
Theorem 1, had we instead performed the mapping: φ̃S : X → Rd defined as
φ̃S(x) = (K(x, x′1), . . . ,K(x, x

d
then with probability 1 − δ, the induced distribution φ̃S(P ) in Rd would have a separator of error
at most ǫ + δ at margin at least γ/2. Let β̂ be the vector corresponding to such a separator in
that space. Now, let us convert β̂ into a vector in Rnd by replacing each coordinate β̂j with the
n values (α1β̂j , . . . , αnβ̂j). Call the resulting vector β̃. Notice that by design, for any x we have

β̃, Sx



β̂, Sx

. Furthermore

















. Thus, the vector β̃ under distribution
φS(P ) has the same properties as the vector β̂ under φ̃S(P ). This implies the desired result

This result can be viewed as analogous to the idea of learning a kernel matrix studied by [7]. How
ever, rather than explicitly learning the best convex combination, we are simply folding the learning
process into the second stage of the algorithm. Of particular interest is that due to the use of L1 mar
gins, the number of labeled examples needed for good generalization grows only logarithmically
with the number of similarity functions at hand. It is interesting to compare this with the results of
[11] who give general sample-complexity bounds for learned kernels. Whereas our approach gives
an O(log n) multiplicative penalty, the results of [11] on convex combinations of kernels give an
O(n) additive penalty. Thus, in a sense the results are incomparable. On the other hand, the loga
rithmic penalty of Theorem 4 means that one could even use more similarity functions than we have
labeled data
4 Open Questions and Discussion
While the result given in Theorem 4 is in a sense analogous to results of [7] and [11] on learning
a convex combination of kernel functions, we do not actually produce a convex combination of
similarity functions; rather, we just use its existence to learn a linear separator in the landmark
feature space. This suggests the following natural open question: Can bounds comparable to those
resulting from Theorem 4 for learning the target function be obtained for algorithms that explicitly
output a convex combination of similarity functions satisfying some approximation of Definition
More broadly, Srebro and Ben-David [11] discuss the problem of learning over a general space K of
kernel functions. One could obtain roughly comparable bounds for spaces of similarity functions by
explicitly listing similarity functions making up an ǫ-net of K and then performing a mapping using
this explicit list as in Theorem 4. However, this would be incredibly computationally inefficient. A
second open question is whether for natural classes of similarity functions one can combine learning
of the target function with learning the best similarity function in the class into a single, and ideally
computationally efficient, optimization problem
The approach of Theorem 4 may also have applications to multi-task or transfer learning. Suppose
one has a set of similarity functions and a family of classification tasks, and for each classification
task there is some (perhaps different) convex combination of the similarity functions that is good for
it. Then one could reuse the same embedding and same landmarks for all of them. If furthermore
one believes that the sets of “representative points” for the different tasks have high overlap, then
following the approach of [9] one could use a joint regularization penalty when learning all of them
It would be interesting to better understand when this can be expected to perform well and more
generally to explore transfer learning in this setting
References
[1] httpwwwkernelmachinesorg
[2] A. Argyriou, R. Hauser, C.A. Micchellio, and M. Pontil. A DC algorithm for kernel selection. In ICML

[3] M.-F. Balcan and A. Blum. On a theory of learning with similarity functions. In ICML,
[4] M.-F. Balcan, A. Blum, and N. Srebro. Improved guarantees for learning via similarity functions. In
COLT,
[5] M.-F. Balcan, A. Blum, and N. Srebro. A theory of learning with similarity functions. Machine Learning

[6] M.-F. Balcan, A. Blum, and S. Vempala. Kernels as features: On kernels, margins, and lowdimensional
mappings. Machine Learning, 65(1):79 – 94,
[7] G. R. G. Lanckriet, N. Cristianini, P. L. Bartlett, L. El Ghaoui, and M. I. Jordan. Learning the kernel
matrix with semidefinite programming. Journal of Machine Learning Research, 5:27–72,
[8] N. Littlestone. From online to batch learning. In COLT, pages 269–284,
[9] A. Quattoni, M. Collins, and T. Darrell. Transfer learning for image classification with sparse prototype
representations. In CVPR,
[10] A. J. Smola and B. Schölkopf. Learning with Kernels. MIT Press,
[11] N. Srebro and S. Ben-David. Learning bounds for support vector machines with learned kernels. In
COLT,
[12] V. N. Vapnik. Statistical Learning Theory. John Wiley and Sons Inc.,


