PreferenceBased Reinforcement LearningA preliminary surveyChristian Wirth and Johannes FrnkranzKnowledge Engineering, Technische Universitt Darmstadt, Germanycwirth,fuernkranzke.tudarmstadt.deAbstract. Preferencebased reinforcement learning has gained significant popularity over the years, but it is still unclear what exactly preference learning is and how it relates to other reinforcement learning tasks.In this paper, we present a general definition of preferences as well assome insight how these approaches compare to reinforcement learning, inverse reinforcement learning and other related approaches. Additionally,we are offering a coarse categorization of preferencebased reinforcementlearning algorithms and a preliminary survey based on this allocation.1 IntroductionPreferencebased reinforcement learning PBRL is usually considered as learning a policy for a Markov decision processes MDP, which satisfies a given setof preferences over trajectories or partial trajectories, but this is not a clearlydefined problem. There is no common definition for preferences or a consensuswhat can be assumed concerning the underlying decision process. Therefore, wewill try to identify similarities and differences by relating PBRL to conventionalreinforcement learning. In both scenarios, MDPs are assumed, but are not necessary. Their advantage lies in the welldefined theory of MDPs which comeswith a large amount of theoretical and empirical analysis, allowing to build onexisting work. All algorithms for PBRL, known to the authors, are using MDPs,hence we are also sticking to this assumption. However, the common definitionfor MDPs like the one given by Sutton and Barto 15 implies a notation of rewards, concerning single stateaction pairs. Those rewards are not available ornot existent in preferencebased approaches, hence we are turning to a rewardfree definition MDPR 1, which is used by inverse reinforcement learningIRL approaches in general.After formalising the notion of MDPR in section 2, we will take a closer lookat what preferences are and how they can be described in section 3. In section 4,we will see that the key step for defining a learning problem based on an MDPRand a preference relation on sequences of stateaction pairs is the definition ofa preference relation over policies, which can either be explicitly modeled or bedefined via a utility function. Existing preferencebased reinforcement learningalgorithms can be categorised by this difference. In section 5, we are presentingalgorithms employing a utility function while the approaches in section 6 are2explicitly modelling a relation between trajectory and policypreferences. Beforewe conclude the paper in section 8, we also take a brief look at the relationbetween the different reinforcement learning subtasks and PBRL in section 7.2 MDPRAbbeel and Ng 1 have introduced the notion of Markov decision processeswithout rewards MDPR. A finitestate MDPR is defined by a quadrupleS,A, , . Given are a set of states S  si, actions A  aj, and a probabilistic state transition function   SAS  0, 1 such thats s, a, s  1for all s, a  S A.A policy   S  A  0, 1 is a probabilistic function that assigns probabilities to state action pairs s.t.a s, a  1 for all s  S. A trajectory is analternating sequence of states and actionsT  s0, a0, s1, a1, . . . , sn1, an1, sn .The space of all trajectories is denoted as  , the set of trajectories that can begenerated with a given policy  is denoted as . Most MDPs have goal statesandor a finite horizon, resulting in finite length trajectories. In the following,we assume that all trajectories start in the same start state s0. Multiple startstates can be included by using a single, virtual s0 with a nullaction that hasprobabilistic transitions to the true start states. Let As denote the set of actionsthat are possible in state s, i.e., As  a  AsS s, a, s  0. A terminalstate is a state in which no action is available, i.e., where As  . A trajectoryis called complete if it leads from a start state s0 to a terminal state sn. As aslight generlization of trajectories, we will, in the following, also consider stateaction sequences C  N S A, where Cn denotes the nth stateaction pairin the sequence.We assume that states and actions are represented by a kdimensional normalized feature vector of the form   S  A  0, 1k over states and actions.The standard tabular representation of a stateaction space can, e.g., be obtainedwith the feature sets, ai,j 1 if s  si and a  aj0 else1  0, 1 is a discount factor, which is relevant if we assume an underlying,hidden reward function as is, e.g., the case in inverse reinforcement learning.2.1 Rewards  UtilitySeveral of the algorithms surveyed here assume that a numeric utility can becalculated for a trajectory. In particular, we assume that a utility function U SA Rm exists, but is not known. Of course, the result is nonscalar ifm  1,3but the value of m is usually not known. We further assume this function to belinear in terms of the features, i.e., Us, a  W  s, a .In the case of classic reinforcement learning, the utility function correspondsto the reward function. However, as pointed out by Akrour et al. 2, a rewardfunction, as defined in the common MDP scenario is not changing, but the utilityof a trajectory can change with the occurrence of new preferences.2.2 Feature ExpectationsThe idea behind feature expectations is to break down the utility of a sequenceinto a linear sum over the stateaction features , as described by Abbeel andNg 1. Ct is the stateaction pair, encountered at index t in the sequence C. Cis the number stateaction pairs in the sequence. As mentioned, the last state isnot forming a pair in the case of complete trajectories and is therefore omitted.The utility of a sequence C isUC Ct0tUCt Ct0tW  s, at  W Ct0ts, at 2Let us now define the vector C as the weighted sum of all values for theindividual feature vectors in the sequence.C Ct0ts, at 3The feature expectations for a policy  can now be defined via the expectedvalue of T  over all trajectories T that can be observed when starting in startstate s0 and following policy  thereafter.  E T   E t0ts, at4If we know the feature expectations of a policy, we can use 2 to determineits utility asU  W   5 can, e.g., be approximated by calculating a sample based estimate forthe expectation.3 PreferenceBased FeedbackOne of the remaining questions is, how to define preferences in general. Weare suggesting a formal representation based on sequences of stateaction pairsCt  N  S  A. A preference is a relation C1  C2 between two sequencesC1, C2. We also use the following notations4C1  C2 The first sequence is strictly preferred, i.e., C1  C2 but not C2  C1.C1  C2 The second sequence is strictly preferred, i.e., C2  C1.C1  C2 Both sequences are indifferent, i.e., both C1  C2 and C2  C1 hold.C1  C2 The sequences are incomparable, i.e., neither C1  C2 nor C2  C1holds.In many cases, the preference relation is assumed to be a total order, i.e.,for each pair C1 and C2, either C1  C2 or C2  C1, or, in other words, thereare no incomparable pairs of sequences. This is, e.g., the case if we assume thatthe preference relation can be modeled with an underlying, singledimensionalutility function, i.e.,C1  C2  UC1  UC2 6Total orders guarantee a single optimal solution. If there are incomparable pairsC1  C2, the preferences form a partial order. Partial orders do not have aunique, highest rank, preventing the determination of a single optimum. Instead,a set of nondominated solutions, the socalled Pareto front can be determined.An agent will observe a subset   C  C of these preferences based on thevisited states and the obtained feedback. Often, special types of preferences areobserved, which fit into the proposed framework as followsAction Preferences An action preference s, a1  s, a2 means that it ispreferred to select action a1 opposed to a2, when in state s. Within oursequence based representation, this is a preference over single element sequences C1  C2, C1  s, a1, C2  s, a2.State Preferences A state preference s1  s2 means that it is preferred to visitstate s1 opposed to state s2. This can be turned into a set of sequence preferences s1, a1  s2, a2a1  As1a2  As2 because the preferenceis valid no matter what action has been performed in each state.Trajectory Preferences A trajectory T  s0, a0, s1, a1, . . . , sn1, an1, snis associated with the sequence s0, a0, . . . , sn1, an1, hence we candirectly interpret a trajectory preference as a sequence preference.4 The Learning ProblemWe can now define a preferencebased decision process PBDP as a quintuplePBDP  S,A, ,,. Informally, the learning problem now is to identify apolicy  in the space of policies  which generates trajectories that agree withthe observed preferences . However, it is nontrivial to specify what it meansthat one or more policies conform to the observed preferences. A simple approachis to associate them with the trajectories they generate.The key step that has to be addressed when solving a PBDP is how tolift preferences on the trajectory or sequence level up to preferences on thepolicy level. In general, we can discriminate two different approaches relationalapproaches try to define a preference relation A on policies, directly utilizingthe preferences, whereas valuebased approaches attempt to associate numericalvalues with policies. In each case there are several possible approaches, whichwe briefly discuss in the following.54.1 Relational ApproachesThe goal of relational approaches to preferencebased reinforcement learning isto define a relation A   over the space of policies, without determining avalue for them. Several options are possible, we list a few of them in the following.Dominance A simple way of defining a partial preference relation on policiesis to postulate that a policy 1 is preferred over a policy 2 if it is guaranteedthat 1 always produces better trajectories than 2.1 A 2  T1  1 , T2  2  T1  T2. 7Obviously, this is a quite conservative definition. For example, if every policyhas nonzero probabilities for all possible actions in a state, no policy will bepreferred over any other policy.On the other hand, when policies and state transitions are deterministic, i.e.,when each policy generates a unique trajectory from each starting state, this isthe most natural choice.Stochastic Dominance 8 suggested the use of stochastic dominance as asomehwat less conservative alternative. Essentially, a policy 1 dominates a policy 2, if the probability that for any given trajectory T , the probability that1 generates a trajectory T1 that is at least as good as T is greater than theprobability that 2 generates a trajectory T2 that is at least as good as T .1 A 2  T    PrT1  T   PrT2  T , where T1  1 , T2  2 8Probabilistic Dominance Another way to weaken dominance is to only require that the probability of generating the preferred trajectory is higher thanthe probability of generating the dominated trajectory.1 A 2  T1  T1 , T2  T2  PrT1  T2  PrT2  T1 9Note that this relation is not transitive, i.e., cyclic relationships 1 A 2 A3 A 1 are possible.4.2 ValueBased ApproachesValuebased approaches assume that each policy can be associated with a numerical utility value U. This is usually achieved by assuming a utility function,that can be described as a function over stateaction features, as discussed insection 2.2. Its now possible to either use U for searching in the policy spaceor to use the implicitly given utility Us, a as stateaction value for defining apolicy, comparable to a Qfunction in classic reinforcement learning.6Statebased Policy Evaluation Statebased policy evaluation defines for eachpolicy  a value U, s for each state s  S, which indicates how good it is tofollow policy  in state s. One approach is to associate with each state theprobability with which a trajectory starting in this state is preferred over someother trajectory in this state.U, s  PrT  T , where T  s, T    s 10Thus, a policy 1 is preferred over some other policy 2, if 1s probability ofgenerating a preferred hypothesis is higher than the corresponding probabilityof 2.These probabilities can be estimated via policy rollouts.U, s 1NNn1IT  T , where T  s, T   s,  6  11This is essentially the approach that was followed in 8, where such rolloutswhere used to provide training information for a label ranker that learns a modelfor ranking the actions in each state.Trajectorybased Policy Evaluation Trajectorybased policy evaluation defines for each trajectory T a value UT . It seems natural to require that apolicy  maximises the value of the trajectories it can generate. Hence, we canevaluate a policy based on the expected sum over all trajectories, subject to theprobability that the trajectory gets generated by .U  ETUT 12A trajectory preference is now a sample concerning the value of the trajectory,meaning T1  T2  UT1  UT2. By assuming a stateaction based utility function Us, a, it is also possible to employ the stateaction preferencesdescribed in section 3 with e.g. s, a1  s, a2 Us, a1  Us, a2.5 Valuebased AlgorithmsValuebased approaches have been predominantly used so far in approaches topreferencebased reinforcement learning. As described in the previous section,they evaluate policies with a utility function. We can discriminate two mainapproaches algorithms that operate in the policy space aka as policy searchin conventional reinforcement learning, utilizing U, or approaches implicitlydefining a policy by a value Us, a. The PPL approach, presented in section 5.3belongs to the first category while the two other algorithms sections 5.1 and5.2 are concerned with the second variant.7One of the main problems of these approaches is how to utilize the preferencesefficiently. Preferences are usually given over complete trajectories, preventingany feedback before a trajectory rollout has finished. Additionally, preferencesare not valid for subsequences of the the sampled trajectories, because it is notknown how the missing stateaction pairs are influencing the utility.5.1 Preferencebased Policy Iteration PBPIFrnkranz et al. 8 have defined a MonteCarlobased approach. The algorithmworks within the framework of policy iteration, where the policy is evaluatedin each iteration and refined based on the outcome 15. The basic idea is todetermine the utilityvalue of an stateaction pair relative to the other actionsavailable, because the absolute values are not known. This means, U  S  AA  0, 1 is used instead of the Qfunction Q  S  A  R common in classicReinforcement learning. The implicit meaning isUs, a, a 1 if Qs, a  Qs, a0 elseIn each iteration, a limited subset of all available states is evaluated, and itsactions are compared in a pairwise manner by always sampling rollouts for twoactions for the same state s. The evaluation determines if Qs, a  Qs, aholds, which is the case if the sampled trajectory starting with s, a is preferredover the one starting with s, a. This is then a sample for the relative utilityfunction and used as training information for a label ranking algorithm 11 withs as features. The optimal action is now determined by the predicted rankingof the actions.This approach updates only a single action pair for a single state by eachpreference encountered, requiring a high amount of evaluations for convergence.5.2 A Policy Iteration Algorithm for Learning fromPreferencebased FeedbackThe algorithm by Wirth and Frnkranz 18 is closely related to PBPI, but differs in some essential points. The main advantage lies in a possibility to reuse apreference feedback for updating multiple states, which is achieved by introducing a probability theorem that states the probability of a stateaction selectionbeing the cause for the observed preference. Additionally, a modified version ofthe EXP3 5 policy for exploration exploitation tradeoff was utilized for furtherimprovements. However, these approaches are not directly comparable, becausethe PBPI algorithm assumes a parameterized state space, whereas Wirth andFrnkranz 18 uses a simple tabular representation which is not suited for generalization over states, rendering the usage of a ranker infeasible.85.3 Preferencebased Policy Learning PPLA wellknown example of preference based policy search is the approach byAkrour et al. 2, 3. They describe states by the features of the sensorimotorstate SMS of the underlying robotics agent system. An SMS is defined asthe combination of the measured sensor values with the actuator values. Forsimplifying the problem, all encountered SMS are clustered, which reduces theamount of states in the MDPR. This representation is closely related to featureexpectations, as described in sec. 2.2, but not identical because of the clustering.The clustering results in  being a dynamic function, due to the clustering onlydependent on the already observed SMS. A clustering of all available SMS couldbe seen as discretization of the state space. In spite of this difference, we are stillconsidering those clusters as tabular features in the form of eq. 2.The first step of PPL is to determine the weight vector w of the scalar utilityversion of eq. 2. This can be achieved by solving eq. 13 with Ti  Tj asthe preference of trajectory Ti over trajectory Tj with the trajectories beingcomplete in this case. Ci and Cj are the state, action sequences of Ti and Tj .A preference is interpreted as constraint on the utility function.min12w2  ci,j,CiCji,js.t. w, Ci  w, Cj  1 i,j and i,j  for all Ci  Cj  13This formulation is conveniently the same optimization problem as solved bySVMRank 12, which is used to solve this problem. New policies are now generated using an evolutionary strategy, more specifically the 1ES algorithmby Auger 6. For determining the value of an policy, its utility estimate Usec. 2.2 is calculated summed up with an exploration term E. This trades offthe expected utility, as measure for exploitation, with additional exploration.Concerning details of the exploration function and how the tradeoff with theutility is realised, we want to point the interested reader to 2, because it is notessential for understanding the approach.It should be noted, the search is not performed in the policy space directly,but in the feature space . The optimization is preformed for determining w,which is dependent on the feature space, and only the resulting utility functionis used for evaluating the policies.6 Relationbased AlgorithmsRelationbased approaches compare policies, without determining a utility. Considering that a policy can be described by the set of all trajectories it can generateor the probability distribution over the trajectories in the stochastic case, wecan use trajectories as samples for the policy. Hence, it is sufficient to comparethe trajectories created by different policies to determine an approximation oftheir relation.9The main difference between the algorithms in this class is how the policyrelation is defined relative to the observed samples of the preference relationsand how to create new policies based on this information. It should be noted, allalgorithms presented in this class are requiring parameterized policies, becausethis enables generalization of policy relations to the complete policy space.6.1 Preferencebased Evolutionary Direct Policy SearchThe algorithm of BusaFekete et al. 7 is closely related to the work of Akrouret al. 2. It also employs an evolutionary strategy for optimizing the parametersof a parametric policy, but CMAES 9 was used in this case. The main differencelies in the evaluation of the policies, which is performed directly in policy space.Each candidate policy of the current iteration is used to sample a limited amountof trajectories. The pairwise preference relation is now used to estimate how ofteni yields a trajectory that is preferred over a trajectory produced by j . Using aracing algorithm witch utilizes Hoeffeding bounds enables the determination ofa ranking for the policies based on the fraction of dominating trajectories 10.This ranking is then use within the CMAES framework to create new policies.6.2 Bayesian Preference Learning from Trajectory PreferenceQueriesWilson et al. 17 tries to learn the distribution of the policy space, according tothe expert preferences. The sequences C are limited to trajectories of length K,with K typically much smaller than the horizon. The posterior distribution ofthe policy space 6.2 is defined by the expected distance between the observedtrajectories and an ones created by an expert policy. This means Ci  Cj fCi, Cj ,   0 with the comparison functionfCi, Cj ,   EdCi, C EdCj , C 14where C is a random trajectory generated by . The trajectory distance function d is defined utilizing the realvalued vector space s, adCi, Cj Kt0s, ai,t s, aj,tHence, eq. 14 can be seen as the expected difference of the undiscounted featureexpectations between Ci and an optimal trajectory C.It is now possible to determine the expected policy distribution, using BayestheoremPr  PrPrCi  Cj ICiCj1 PrCi  Cj 1ICjCjPrCi  Cj Ci, Cj,   IfCi, Cj ,   N0, 2rd10PrCi  Cj  denotes the probability of Ci  Cj concerning samples of expertpolicy .The posterior distribution is approximated using Hybrid Monte Carlo 4,which is a form of Markov Chain Monte Carlo. It should be noted that theexperiments have been performed with explicit knowledge of  for determiningthe expert feedback.7 The Preference Problem CycleAn interesting question concerns the relation between reinforcement learningRL, preferencebased reinforcement learning PBRL, inverse reinforcementlearning IRL, and apprenticeship learning. IRL is about learning the rewardfunction of the MDP from demonstrated solutions 13. Apprenticeship Learningtries to directly recover the policy that was used to create the demonstrated solutions, meaning the demonstrations are assumed to be quasi optimal 1. This isusually achieved by applying IRL for learning the reward function which allowsthe application of classic RL algorithms for determining the policy, but this iscan also be achieved in other ways. Demonstrated solutions can be seen as an implicit preference with the meaning of the demonstrated solutions are preferredover all others, turning those approaches into preferencebased methods. Themain difference is that PBRL does not assume that solutions are demonstrated,Fig. 1. The Preference Problem Cycle11but that the training information is generated with some sampling policy. Thismainly concerns the explorationexploitation tradeoff 15 that has to be considered in reinforcement learning, but can also be seen as a case of Active Learning14. Together with the preference feedback, determined by some kind of oracle,this results in a cycle, as show in figure 1. The edge labels are describing theoutput of the last element, which is also the input for the next part of the cycle,together with the globally available information, like the MDPR. Currently allPBRL publications, known to the authors, are trying to find a solution for everysubtask of the cycle, but this is arguably not required. Due to the possibility ofmodularizing the cycle, we can identify the subtask which are not solvable withalready existing methods. In fact, the only new problems are preferencebasedinverse reinforcement learning and the preferencebased apprenticeship learning.Determining an optimal policy according to a reward function can be solvedby utilizing classic reinforcement learning. Methods for sampling new trajectories, which are beneficial for the learning process, are also existent but they canprobably be improved for preference based algorithms. It should also be noted,that the relationbased methods from section 6 are an instance of preferencebased apprenticeship learning, while learning a utility function can be seen aspreferencebased inverse reinforcement learning.8 ConclusionIn our view, PBRL can be seen as an algorithm that is in between conventionalreinforcement learning and apprenticeship learning. The key difference is thatwe are not shown optimal or at least desirable trajectories as in apprenticeshiplearning or inverse reinforcement learning, but may also see suboptimal andundesirable policies like in traditional reinforcement learning. On the other hand,unlike in reinforcement learning, we cannot directly observe the utility of thesepolicies but can only observe instances of preferences between trajectories, statesor actions. These preferences can be modeled as preferences between stateactionsequences, resulting in a generalized representation which unifies several differenttypes of preferences. Other aspects like the definition of optimality still lack aunifying framework.PBRL algorithms can be divided into two coarse categories relationbasedand valuebased approaches where preferences are samples for a policy relationor the value of a trajectory or stateaction pair. Both approaches have theirown advantages and shortcomings. The most prominent question for relationbased algorithms is how trajectory or other preferences are raised to the levelof policies. Value based methods are usually having difficulties to utilize thetraining information efficiently.AcknowledgmentsThis work was supported by the German Research Foundation DFG as part of thePriority Programme 1527. We would like to thank Eye Hllermeier and Robert BusaFekete for interesting and stimulating discussions that greatly influenced this paper.12References1 Pieter Abbeel and Andrew Y. Ng. Apprenticeship learning via inverse reinforcement learning. In Proceedings of the 21th international conference onMachine learning ICML04, page 1, 2004.2 Riad Akrour, Marc Schoenauer, and Michle Sebag. Preferencebased policylearning. In Proceedings of the European Conference on Machine Learningand Knowledge Discovery in Databases ECMLPKDD11, volume 6911 ofLNCS, pages 1227. Springer, 2011.3 Riad Akrour, Marc Schoenauer, and Michle Sebag. APRIL Active preference learningbased reinforcement learning. In Proceedings of the EuropeanConference on Machine Learning and Knowledge Discovery in DatabasesECMLPKDD12, volume 7524 of LNCS, pages 116131. Springer, 2012.4 Christophe Andrieu, Nando de Freitas, Arnaud Doucet, and Michael I. Jordan. An introduction to MCMC for machine learning. Machine Learning,5012543, 2003.5 Peter Auer, Nicol CesaBianchi, Yoav Freund, and Robert E. Schapire.Gambling in a rigged casino The adversarial multiarm bandit problem.In Proceedings of the 36th Annual Symposium on Foundations of ComputerScience, pages 322331. IEEE Computer Society Press, 1995.6 Anne Auger. Convergence results for the 1,SAES using the theory ofirreducible markov chains. Theoretical Computer Science, 3341335 69, 2005.7 Robert BusaFekete, Balazs Szorenyi, Paul Weng, Weiwei Cheng, and EykeHllermeier. Preferencebased evolutionary direct policy search. ICRAWorkshop on Autonomous Learning, 2013.8 Johannes Frnkranz, Eyke Hllermeier, Weiwei Cheng, and SangHyeunPark. Preferencebased reinforcement learning a formal framework and apolicy iteration algorithm. Machine Learning, 8912123156, 2012. Special Issue of Selected Papers from ECML PKDD 2011.9 Nikolaus Hansen and Stefan Kern. Evaluating the CMA evolution strategyon multimodal test functions. In Parallel Problem Solving from Nature PPSN VIII, volume 3242 of LNCS, pages 282291. Springer, 2004.10 Verena HeidrichMeisner and Christian Igel. Hoeffding and Bernstein racesfor selecting policies in evolutionary direct policy search. In Proceedings ofthe 26th Annual International Conference on Machine Learning ICML09,pages 401408. ACM, 2009.11 Eyke Hllermeier, Johannes Frnkranz, Weiwei Cheng, and Klaus Brinker.Label ranking by learning pairwise preferences. Artificial Intelligence, 172161718971916, 2008.12 Thorsten Joachims. Optimizing search engines using clickthrough data. InProceedings of the 8th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining KDD02, pages 133142. ACM Press,2002.13 Stuart Russell. Learning agents for uncertain environments extended abstract. In Proceedings of the 11th Annual Conference on ComputationalLearning Theory COLT98, pages 101103. ACM, 1998.1314 Burr Settles. Active learning literature survey. Computer Sciences TechnicalReport 1648, University of WisconsinMadison, 2009.15 Richard S. Sutton and Andrew Barto. Reinforcement Learning An Introduction. MIT Press, Cambridge, MA, 1998.16 Ronald J. Williams. Simple statistical gradientfollowing algorithms forconnectionist reinforcement learning. Machine Learning, 834229256,1992.17 Aaron Wilson, Alan Fern, and Prasad Tadepalli. A bayesian approach forpolicy learning from trajectory preference queries. In Proceedings of the 26thAnnual Conference on Neural Information Processing Systems NIPS12,pages 11421150, 2012.18 Christian Wirth and Johannes Frnkranz. A policy iteration algorithmfor learning from preferencebased feedback. In Proceedings of the 12thInternational Symposium on Intelligent Data Analysis IDA13, London,England, October 2013. IOPress.
