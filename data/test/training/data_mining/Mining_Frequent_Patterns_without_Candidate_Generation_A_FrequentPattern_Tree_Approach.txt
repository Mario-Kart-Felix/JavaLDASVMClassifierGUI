Data Mining and Knowledge Discovery, 8, 5387, 2004c 2004 Kluwer Academic Publishers. Manufactured in The Netherlands.Mining Frequent Patterns without CandidateGeneration A FrequentPattern TreeApproachJIAWEI HAN hanjcs.uiuc.eduUniversity of Illinois at UrbanaChampaignJIAN PEI jianpeicse.buffalo.eduState University of New York at BuffaloYIWEN YIN yiwenycs.sfu.caSimon Fraser UniversityRUNYING MAO runyingmmicrosoft.comMicrosoft CorporationEditor Heikki MannilaReceived May 21, 2000 Revised April 21, 2001Abstract. Mining frequent patterns in transaction databases, timeseries databases, and many other kinds ofdatabases has been studied popularly in data mining research. Most of the previous studies adopt an Apriorilikecandidate set generationandtest approach. However, candidate set generation is still costly, especially when thereexist a large number of patterns andor long patterns.In this study, we propose a novel frequentpattern tree FPtree structure, which is an extended prefixtreestructure for storing compressed, crucial information about frequent patterns, and develop an efficient FPtreebased mining method, FPgrowth, for mining the complete set of frequent patterns by pattern fragment growth.Efficiency of mining is achieved with three techniques 1 a large database is compressed into a condensed,smaller data structure, FPtree which avoids costly, repeated database scans, 2 our FPtreebased mining adoptsa patternfragment growth method to avoid the costly generation of a large number of candidate sets, and 3 apartitioningbased, divideandconquer method is used to decompose the mining task into a set of smaller tasks formining confined patterns in conditional databases, which dramatically reduces the search space. Our performancestudy shows that the FPgrowth method is efficient and scalable for mining both long and short frequent patterns,and is about an order of magnitude faster than the Apriori algorithm and also faster than some recently reportednew frequentpattern mining methods.Keywords frequent pattern mining, association mining, algorithm, performance improvements, data structureThe work was done at Simon Fraser University, Canada, and it was supported in part by the Natural Sciencesand Engineering Research Council of Canada, and the Networks of Centres of Excellence of Canada.To whom correspondence should be addressed.54 HAN ET AL.1. IntroductionFrequentpattern mining plays an essential role in mining associations Agrawal et al.,1993, 1996 Agrawal and Srikant, 1994 Mannila et al., 1994, correlations Brin et al.,1997, causality Silverstein et al., 1998, sequential patterns Agrawal and Srikant, 1995,episodes Mannila et al., 1997, multidimensional patterns Lent et al., 1997 Kamberet al., 1997, maxpatterns Bayardo, 1998, partial periodicity Han et al., 1999, emergingpatterns Dong and Li, 1999, and many other important data mining tasks.Most of the previous studies, such as Agrawal and Srikant 1994, Mannila et al. 1994,Agrawal et al. 1996, Savasere et al. 1995, Park et al. 1995, Lent et al. 1997, Sarawagiet al. 1998, Srikant et al. 1997, Ng et al. 1998 and Grahne et al. 2000, adopt anApriorilike approach, which is based on the antimonotone Apriori heuristic Agrawal andSrikant, 1994 if any length k pattern is not frequent in the database, its length k  1superpattern can never be frequent. The essential idea is to iteratively generate the set ofcandidate patterns of length k 1 from the set of frequentpatterns of length k for k  1,and check their corresponding occurrence frequencies in the database.The Apriori heuristic achieves good performance gained by possibly significantly reducing the size of candidate sets. However, in situations with a large number of frequentpatterns, long patterns, or quite low minimum support thresholds, an Apriorilike algorithmmay suffer from the following two nontrivial costs It is costly to handle a huge number of candidate sets. For example, if there are 104frequent 1itemsets, the Apriori algorithm will need to generate more than 107 length2candidates and accumulate and test their occurrence frequencies. Moreover, to discovera frequent pattern of size 100, such as a1, . . . , a100, it must generate 2100  2  1030candidates in total. This is the inherent cost of candidate generation, no matter whatimplementation technique is applied. It is tedious to repeatedly scan the database and check a large set of candidates by patternmatching, which is especially true for mining long patterns.Can one develop a method that may avoid candidate generationandtest and utilize somenovel data structures to reduce the cost in frequentpattern mining This is the motivationof this study.In this paper, we develop and integrate the following three techniques in order to solvethis problem.First, a novel, compact data structure, called frequentpattern tree, or FPtree in short,is constructed, which is an extended prefixtree structure storing crucial, quantitative information about frequent patterns. To ensure that the tree structure is compact and informative,only frequent length1 items will have nodes in the tree, and the tree nodes are arranged insuch a way that more frequently occurring nodes will have better chances of node sharingthan less frequently occurring ones. Our experiments show that such a tree is compact,and it is sometimes orders of magnitude smaller than the original database. Subsequentfrequentpattern mining will only need to work on the FPtree instead of the whole data set.Second, an FPtreebased patternfragment growth mining method is developed, whichstarts from a frequent length1 pattern as an initial suffix pattern, examines only itsMINING FREQUENT PATTERNS WITHOUT CANDIDATE GENERATION 55conditionalpattern base a subdatabase which consists of the set of frequent items cooccurring with the suffix pattern, constructs its conditional FPtree, and performs miningrecursively with such a tree. The pattern growth is achieved via concatenation of the suffixpattern with the new ones generated from a conditional FPtree. Since the frequent itemsetin any transaction is always encoded in the corresponding path of the frequentpattern trees,pattern growth ensures the completeness of the result. In this context, our method is notApriorilike restricted generationandtest but restricted test only. The major operations ofmining are count accumulation and prefix path count adjustment, which are usually muchless costly than candidate generation and pattern matching operations performed in mostApriorilike algorithms.Third, the search technique employed in mining is a partitioningbased, divideandconquer method rather than Apriorilike levelwise generation of the combinations of frequent itemsets. This dramatically reduces the size of conditionalpattern base generated atthe subsequent level of search as well as the size of its corresponding conditional FPtree.Moreover, it transforms the problem of finding long frequent patterns to looking for shorterones and then concatenating the suffix. It employs the least frequent items as suffix, whichoffers good selectivity. All these techniques contribute to substantial reduction of searchcosts.A performance study has been conducted to compare the performance of FPgrowth withtwo representative frequentpattern mining methods, Apriori Agrawal and Srikant, 1994and TreeProjection Agarwal et al., 2001. Our study shows that FPgrowth is about anorder of magnitude faster than Apriori, especially when the data set is dense containingmany patterns andor when the frequent patterns are long also, FPgrowth outperformsthe TreeProjection algorithm. Moreover, our FPtreebased mining method has been implemented in the DBMiner system and tested in large transaction databases in industrialapplications.Although FPgrowth was first proposed briefly in Han et al. 2000, this paper makesadditional progress as follows. The properties of FPtree are thoroughly studied. Also, we point out the fact that, althoughit is often compact, FPtree may not always be minimal. Some optimizations are proposed to speed up FPgrowth, for example, in Section 3.2,a technique to handle single path FPtree has been further developed for performanceimprovements. A database projection method has been developed in Section 4 to cope with the situationwhen an FPtree cannot be held in main memorythe case that may happen in a verylarge database. Extensive experimental results have been reported. We examine the size of FPtree aswell as the turning point of FPgrowth on data projection to building FPtree. We alsotest the fully integrated FPgrowth method on large datasets which cannot fit in mainmemory.The remainder of the paper is organized as follows. Section 2 introduces the FPtreestructure and its construction method. Section 3 develops an FPtreebased frequentpatternmining algorithm, FPgrowth. Section 4 explores techniques for scaling FPgrowth in large56 HAN ET AL.databases. Section 5 presents our performance study. Section 6 discusses the issues onfurther improvements of the method. Section 7 summarizes our study and points out somefuture research issues.2. Frequentpattern tree Design and constructionLet I  a1, a2, . . . , am be a set of items, and a transaction database DB  T1, T2, . . . ,Tn, where Ti i  1 . . . n is a transaction which contains a set of items in I . The support1or occurrence frequency of a pattern A, where A is a set of items, is the number oftransactions containing A in DB. A pattern A is frequent if As support is no less than apredefined minimum support threshold,  .Given a transaction database DB and a minimum support threshold  , the problem offinding the complete set of frequent patterns is called the frequentpattern mining problem.2.1. Frequentpattern treeTo design a compact data structure for efficient frequentpattern mining, lets first examinean example.Example 1. Let the transaction database, DB, be the first two columns of Table 1, and theminimum support threshold be 3 i.e.,   3.A compact data structure can be designed based on the following observations1. Since only the frequent items will play a role in the frequentpattern mining, it is necessaryto perform one scan of transaction database DB to identify the set of frequent items withfrequency count obtained as a byproduct.2. If the set of frequent items of each transaction can be stored in some compact structure,it may be possible to avoid repeatedly scanning the original transaction database.3. If multiple transactions share a set of frequent items, it may be possible to merge theshared sets with the number of occurrences registered as count. It is easy to check whethertwo sets are identical if the frequent items in all of the transactions are listed accordingto a fixed order.Table 1. A transaction database as running example.TID Items bought Ordered frequent items100 f, a, c, d, g, i, m, p f, c, a, m, p200 a, b, c, f, l, m, o f, c, a, b, m300 b, f, h, j, o f, b400 b, c, k, s, p c, b, p500 a, f, c, e, l, p, m, n f, c, a, m, pMINING FREQUENT PATTERNS WITHOUT CANDIDATE GENERATION 574. If two transactions share a common prefix, according to some sorted order of frequentitems, the shared parts can be merged using one prefix structure as long as the count isregistered properly. If the frequent items are sorted in their frequency descending order,there are better chances that more prefix strings can be shared.With the above observations, one may construct a frequentpattern tree as follows.First, a scan of DB derives a list of frequent items,  f 4, c4, a3, b3, m3, p3the number after  indicates the support, in which items are ordered in frequencydescending order. This ordering is important since each path of a tree will follow this order.For convenience of later discussions, the frequent items in each transaction are listed in thisordering in the rightmost column of Table 1.Second, the root of a tree is created and labeled with null. The FPtree is constructedas follows by scanning the transaction database DB the second time.1. The scan of the first transaction leads to the construction of the first branch of the tree f 1, c1, a1, m1, p1. Notice that the frequent items in the transaction arelisted according to the order in the list of frequent items.2. For the second transaction, since its ordered frequent item list  f, c, a, b, m shares acommon prefix  f, c, a with the existing path  f, c, a, m, p, the count of each nodealong the prefix is incremented by 1, and one new node b1 is created and linked as achild of a2 and another new node m1 is created and linked as the child of b1.3. For the third transaction, since its frequent item list  f, b shares only the node  f  withthe f prefix subtree, f s count is incremented by 1, and a new node b1 is created andlinked as a child of  f 3.4. The scan of the fourth transaction leads to the construction of the second branch of thetree, c1, b1, p1.5. For the last transaction, since its frequent item list  f, c, a, m, p is identical to the firstone, the path is shared with the count of each node along the path incremented by 1.To facilitate tree traversal, an item header table is built in which each item points to itsfirst occurrence in the tree via a nodelink. Nodes with the same itemname are linked insequence via such nodelinks. After scanning all the transactions, the tree, together with theassociated nodelinks, are shown infigure 1.Based on this example, a frequentpattern tree can be designed as follows.Definition 1 FPtree. A frequentpattern tree or FPtree in short is a tree structuredefined below.1. It consists of one root labeled as null, a set of itemprefix subtrees as the children ofthe root, and a frequentitemheader table.2. Each node in the itemprefix subtree consists of three fields itemname, count, andnodelink, where itemname registers which item this node represents, count registersthe number of transactions represented by the portion of the path reaching this node, and58 HAN ET AL.Figure 1. The FPtree in Example 1.nodelink links to the next node in the FPtree carrying the same itemname, or null ifthere is none.3. Each entry in the frequentitemheader table consists of two fields, 1 itemname and2 head of nodelink a pointer pointing to the first node in the FPtree carrying theitemname.Based on this definition, we have the following FPtree construction algorithm.Algorithm 1 FPtree construction.Input A transaction database DB and a minimum support threshold  .Output FPtree, the frequentpattern tree of DB.Method The FPtree is constructed as follows.1. Scan the transaction database DB once. Collect F , the set of frequent items, and thesupport of each frequent item. Sort F in supportdescending order as FList, the list offrequent items.2. Create the root of an FPtree, T , and label it as null. For each transaction Trans in DBdo the following.Select the frequent items in Trans and sort them according to the order of FList. Let thesorted frequentitem list in Trans be p  P, where p is the first element and P is theremaining list. Call insert treep  P, T .The function insert treep  P, T  is performed as follows. If T has a child N suchthat N.itemname  p.itemname, then increment N s count by 1 else create a new nodeN , with its count initialized to 1, its parent link linked to T , and its nodelink linked tothe nodes with the same itemname via the nodelink structure. If P is nonempty, callinsert treeP, N  recursively.Analysis. The FPtree construction takes exactly two scans of the transaction database Thefirst scan collects the set of frequent items, and the second scan constructs the FPtree. TheMINING FREQUENT PATTERNS WITHOUT CANDIDATE GENERATION 59cost of inserting a transaction Trans into the FPtree is OfreqTrans, where freqTransis the set of frequent items in Trans. We will show that the FPtree contains the completeinformation for frequentpattern mining.2.2. Completeness and compactness of FPtreeThere are several important properties of FPtree that can be derived from the FPtreeconstruction process.Given a transaction database DB and a support threshold  . Let F be the frequent items inDB. For each transaction T , freqT  is the set of frequent items in T , i.e., freqT   T  F ,and is called the frequent item projection of transaction T . According to the Aprioriprinciple, the set of frequent item projections of transactions in the database is sufficientfor mining the complete set of frequent patterns, because an infrequent item plays no rolein frequent patterns.Lemma 2.1. Given a transaction database DB and a support threshold , the completeset of frequent item projections of transactions in the database can be derived from DBsFPtree.Rationale. Based on the FPtree construction process, for each transaction in the DB, itsfrequent item projection is mapped to one path in the FPtree.For a path a1a2 . . . ak from the root to a node in the FPtree, let cak be the count at thenode labeled ak and cak be the sum of counts of children nodes of ak . Then, according tothe construction of the FPtree, the path registers frequent item projections of cak  caktransactions.Therefore, the FPtree registers the complete set of frequent item projections withoutduplication.Based on this lemma, after an FPtree for DB is constructed, it contains the completeinformation for mining frequent patterns from the transaction database. Thereafter, only theFPtree is needed in the remaining mining process, regardless of the number and length ofthe frequent patterns.Lemma 2.2. Given a transaction database DB and a support threshold  . Without considering the null root, the size of an FPtree is bounded byT DB freqT , and theheight of the tree is bounded by maxT DBfreqT , where freqT  is the frequent itemprojection of transaction T .Rationale. Based on the FPtree construction process, for any transaction T in DB, thereexists a path in the FPtree starting from the corresponding item prefix subtree so that the setof nodes in the path is exactly the same set of frequent items in T . The root is the only extranode that is not created by frequentitem insertion, and each node contains one nodelinkand one count. Thus we have the bound of the size of the tree stated in the Lemma.The height of any pprefix subtree is the maximum number of frequent items in anytransaction with p appearing at the head of its frequent item list. Therefore, the height of60 HAN ET AL.the tree is bounded by the maximal number of frequent items in any transaction in thedatabase, if we do not consider the additional level added by the root.Lemma 2.2 shows an important benefit of FPtree the size of an FPtree is bounded by thesize of its corresponding database because each transaction will contribute at most one pathto the FPtree, with the length equal to the number of frequent items in that transaction. Sincethere are often a lot of sharings of frequent items among transactions, the size of the tree isusually much smaller than its original database. Unlike the Apriorilike method which maygenerate an exponential number of candidates in the worst case, under no circumstances,may an FPtree with an exponential number of nodes be generated.FPtree is a highly compact structure which stores the information for frequentpatternmining. Since a single path a1  a2      an in the a1prefix subtree registers allthe transactions whose maximal frequent set is in the form of a1  a2      ak forany 1  k  n, the size of the FPtree is substantially smaller than the size of the databaseand that of the candidate sets generated in the association rule mining.The items in the frequent item set are ordered in the supportdescending order Morefrequently occurring items are more likely to be shared and thus they are arranged closerto the top of the FPtree. This ordering enhances the compactness of the FPtree structure.However, this does not mean that the tree so constructed always achieves the maximal compactness. With the knowledge of particular data characteristics, it is sometimes possibleto achieve even better compression than the frequencydescending ordering. Consider thefollowing example. Let the set of transactions be adef , bdef , cdef , a, a, a, b, b, b, c, c, c,and the minimum support threshold be 3. The frequent item set associated with support count becomes a4, b4, c4, d3, e3, f 3. Following the item frequency orderinga  b  c  d  e  f , the FPtree constructed will contain 12 nodes, as shown infigure 2a. However, following another item ordering f  d  e  a  b  c, it willcontain only 9 nodes, as shown in figure 2b.The compactness of FPtree is also verified by our experiments. Sometimes a rather smallFPtree is resulted from a quite large database. For example, for the database Connect4 usedin MaxMiner Bayardo, 1998, which contains 67,557 transactions with 43 items in eachtransaction, when the support threshold is 50 which is used in the MaxMiner experimentsFigure 2. FPtree constructed based on frequency descending ordering may not always be minimal.MINING FREQUENT PATTERNS WITHOUT CANDIDATE GENERATION 61Bayardo, 1998, the total number of occurrences of frequent items is 2,219,609, whereasthe total number of nodes in the FPtree is 13,449 which represents a reduction ratio of165.04, while it still holds hundreds of thousands of frequent patterns Notice that fordatabases with mostly short transactions, the reduction ratio is not that high. Therefore,it is not surprising some gigabyte transaction database containing many long patterns mayeven generate an FPtree that fits in main memory. Nevertheless, one cannot assume thatan FPtree can always fit in main memory no matter how large a database is. Methods forhighly scalable FPgrowth mining will be discussed in Section 5.3. Mining frequent patterns using FPtreeConstruction of a compact FPtree ensures that subsequent mining can be performed witha rather compact data structure. However, this does not automatically guarantee that it willbe highly efficient since one may still encounter the combinatorial problem of candidategeneration if one simply uses this FPtree to generate and check all the candidate patterns.In this section, we study how to explore the compact information stored in an FPtree,develop the principles of frequentpattern growth by examination of our running example, explore how to perform further optimization when there exists a single prefix path inan FPtree, and propose a frequentpattern growth algorithm, FPgrowth, for mining thecomplete set of frequent patterns using FPtree.3.1. Principles of frequentpattern growth for FPtree miningIn this subsection, we examine some interesting properties of the FPtree structure whichwill facilitate frequentpattern mining.Property 3.1 Nodelink property. For any frequent item ai , all the possible patternscontaining only frequent items and ai can be obtained by following ai s nodelinks, startingfrom ai s head in the FPtree header.This property is directly from the FPtree construction process, and it facilitates the accessof all the frequentpattern information related to ai by traversing the FPtree once followingai s nodelinks.To facilitate the understanding of other properties of FPtree related to mining, we firstgo through an example which performs mining on the constructed FPtree figure 1 inExample 1.Example 2. Let us examine the mining process based on the constructed FPtree shownin figure 1. Based on Property 3.1, all the patterns containing frequent items that a node aiparticipates can be collected by starting at ai s nodelink head and following its nodelinks.We examine the mining process by starting from the bottom of the nodelink header table.For node p, its immediate frequent pattern is p3, and it has two paths in the FPtree f 4, c3, a3, m2, p2 and c1, b1, p1. The first path indicates that string f, c, a, m, p appears twice in the database. Notice the path also indicates that string62 HAN ET AL. f, c, a appears three times and  f  itself appears even four times. However, they onlyappear twice together with p. Thus, to study which string appear together with p, only psprefix path  f 2, c2, a2, m2 or simply,  f cam2 counts. Similarly, the second pathindicates string c, b, p appears once in the set of transactions in DB, or ps prefix pathis cb1. These two prefix paths of p,  f cam2, cb1, form ps subpatternbase,which is called ps conditional pattern base i.e., the subpatternbase under the condition ofps existence. Construction of an FPtree on this conditional patternbase which is calledps conditional FPtree leads to only one branch c3. Hence, only one frequent patterncp3 is derived. Notice that a pattern is an itemset and is denoted by a string here. Thesearch for frequent patterns associated with p terminates.For node m, its immediate frequent pattern is m3, and it has two paths,  f 4, c3,a3, m2 and  f 4, c3, a3, b1, m1. Notice p appears together with m as well, however,there is no need to include p here in the analysis since any frequent patterns involving phas been analyzed in the previous examination of p. Similar to the above analysis, msconditional patternbase is fca2, fcab1. Constructing an FPtree on it, we derive msconditional FPtree,  f 3, c3, a3, a single frequent pattern path, as shown in figure 3.This conditional FPtree is then mined recursively by calling mine f 3, c3, a3  m.Figure 3 shows that mine f 3, c3, a3  m involves mining three items a, c,  f in sequence. The first derives a frequent pattern am3, a conditional patternbase fc3,and then a call mine f 3, c3  am the second derives a frequent pattern cm3, aconditional patternbase  f 3, and then a call mine f 3  cm and the third derivesonly a frequent pattern fm3. Further recursive call of mine f 3, c3  am derives twopatterns cam3 and fam3, and a conditional patternbase  f 3, which then leadsto a call mine f 3  cam, that derives the longest pattern fcam3. Similarly, the callof mine f 3  cm derives one pattern fcm3. Therefore, the set of frequent patternsinvolving m is m3, am3, cm3,  f m3, cam3, fam3, fcam3, fcm3. Thisindicates that a single path FPtree can be mined by outputting all the combinations of theitems in the path.Similarly, node b derives b3 and it has three paths  f 4, c3, a3, b1,  f 4, b1, andc1, b1. Since bs conditional patternbase fca1,  f 1, c1 generates no frequentitem, the mining for b terminates. Node a derives one frequent pattern a3 and onesubpattern base  f c3, a singlepath conditional FPtree. Thus, its set of frequent patternsFigure 3. Mining FPtree  m, a conditional FPtree for item m.MINING FREQUENT PATTERNS WITHOUT CANDIDATE GENERATION 63Table 2. Mining frequent patterns by creating conditional subpatternbases.Item Conditional patternbase Conditional FPtreep  f cam2, cb1 c3pm  f ca2, fcab1  f 3, c3, a3mb  f ca1,  f 1, c1 a  f c3  f 3, c3ac  f 3  f 3cf  can be generated by taking their combinations. Concatenating them with a3, we have f a3, ca3, fca3. Node c derives c4 and one subpatternbase  f 3, and theset of frequent patterns associated with c3 is fc3. Node f derives only  f 4 but noconditional patternbase.The conditional patternbases and the conditional FPtrees generated are summarized inTable 2.The correctness and completeness of the process in Example 2 should be justified.This is accomplished by first introducing a few important properties related to the miningprocess.Property 3.2 Prefix path property. To calculate the frequent patterns with suffix ai , onlythe prefix subpathes of nodes labeled ai in the FPtree need to be accumulated, and thefrequency count of every node in the prefix path should carry the same count as that in thecorresponding node ai in the path.Rationale. Let the nodes along the path P be labeled as a1, . . . , an in such an order thata1 is the root of the prefix subtree, an is the leaf of the subtree in P , and ai 1  i  n isthe node being referenced. Based on the process of FPtree construction presented in Algorithm 1, for each prefix node ak 1  k  i, the prefix subpath of the node ai in P occurstogether with ak exactly ai .count times. Thus every such prefix node should carry the samecount as node ai . Notice that a postfix node am for i  m  n along the same path alsocooccurs with node ai . However, the patterns with am will be generated when examiningthe suffix node am , enclosing them here will lead to redundant generation of the patterns thatwould have been generated for am . Therefore, we only need to examine the prefix subpathof ai in P .For example, in Example 2, node m is involved in a path  f 4, c3, a3, m2, p2, tocalculate the frequent patterns for node m in this path, only the prefix subpath of node m,which is  f 4, c3, a3, need to be extracted, and the frequency count of every node in theprefix path should carry the same count as node m. That is, the node counts in the prefixpath should be adjusted to  f 2, c2, a2.64 HAN ET AL.Based on this property, the prefix subpath of node ai in a path P can be copied andtransformed into a countadjusted prefix subpath by adjusting the frequency count of everynode in the prefix subpath to the same as the count of node ai . The prefix path so transformedis called the transformed prefix path of ai for path P .Notice that the set of transformed prefix paths of ai forms a small database of patternswhich cooccur with ai . Such a database of patterns occurring with ai is called ai s conditional patternbase, and is denoted as pattern base  ai . Then one can compute allthe frequent patterns associated with ai in this ai conditional patternbase by creating asmall FPtree, called ai s conditional FPtree and denoted as FPtree  ai . Subsequentmining can be performed on this small conditional FPtree. The processes of construction ofconditional patternbases and conditional FPtrees have been demonstrated in Example 2.This process is performed recursively, and the frequent patterns can be obtained by apatterngrowth method, based on the following lemmas and corollary.Lemma 3.1 Fragment growth. Let  be an itemset in DB, B be s conditional patternbase, and  be an itemset in B. Then the support of   in DB is equivalent to the supportof  in B.Rationale. According to the definition of conditional patternbase, each subtransactionin B occurs under the condition of the occurrence of  in the original transaction databaseDB. If an itemset  appears in B  times, it appears with  in DB  times as well. Moreover,since all such items are collected in the conditional patternbase of ,    occurs exactly times in DB as well. Thus we have the lemma.From this lemma, we can directly derive an important corollary.Corollary 3.1 Pattern growth. Let  be a frequent itemset in DB, B be s conditionalpatternbase, and  be an itemset in B. Then    is frequent in DB if and only if  isfrequent in B.Based on Corollary 3.1, mining can be performed by first identifying the set of frequent1itemsets in DB, and then for each such frequent 1itemset, constructing its conditionalpatternbases, and mining its set of frequent 1itemsets in the conditional patternbase, andso on. This indicates that the process of mining frequent patterns can be viewed as firstmining frequent 1itemset and then progressively growing each such itemset by miningits conditional patternbase, which can in turn be done similarly. By doing so, a frequentkitemset mining problem is successfully transformed into a sequence of k frequent 1itemset mining problems via a set of conditional patternbases. Since mining is done bypattern growth, there is no need to generate any candidate sets in the entire mining process.Notice also in the construction of a new FPtree from a conditional patternbase obtainedduring the mining of an FPtree, the items in the frequent itemset should be ordered in thefrequency descending order of node occurrence of each item instead of its support whichrepresents item occurrence. This is because each node in an FPtree may represent manyoccurrences of an item but such a node represents a single unit i.e., the itemset whoseelements always occur together in the construction of an itemassociated FPtree.MINING FREQUENT PATTERNS WITHOUT CANDIDATE GENERATION 653.2. Frequentpattern growth with single prefix path of FPtreeThe frequentpattern growth method described above works for all kinds of FPtrees. However, further optimization can be explored on a special kind of FPtree, called single prefixpath FPtree, and such an optimization is especially useful at mining long frequent patterns.A single prefixpath FPtree is an FPtree that consists of only a single path or a singleprefix path stretching from the root to the first branching node of the tree, where a branchingnode is a node containing more than one child.Let us examine an example.Example 3. Figure 4a is a single prefixpath FPtree that consists of one prefix path,a10  b8  c7, stretching from the root of the tree to the first branching node c7.Although it can be mined using the frequentpattern growth method described above, a bettermethod is to split the tree into two fragments the single prefixpath, a10  b8 c7, as shown in figure 4b, and the multipath part, with the root replaced by a pseudoroot R, as shown in figure 4c. These two parts can be mined separately and then combinedtogether.Let us examine the two separate mining processes. All the frequent patterns associatedwith the first part, the single prefixpath P  a10  b8  c7, can be mined byenumeration of all the combinations of the subpaths of P with the support set to the minimumsupport of the items contained in the subpath. This is because each such subpath is distinctand occurs the same number of times as the minimum occurrence frequency among theitems in the subpath which is equal to the support of the last item in the subpath. Thus, pathP generates the following set of frequent patterns, freq pattern setP  a10, b8,c7, ab8, ac7, bc7, abc7.Let Q be the second FPtree figure 4c, the multipath part rooted with R. Q can bemined as follows.First, R is treated as a null root, and Q forms a multipath FPtree, which can be minedusing a typical frequentpattern growth method. The mining result is freq pattern setQ d4, e3,  f 3, d f 3.Figure 4. Mining an FPtree with a single prefix path.66 HAN ET AL.Second, for each frequent itemset in Q, R can be viewed as a conditional frequentpatternbase, and each itemset in Q with each pattern generated from R may form a distinct frequent pattern. For example, for d4 in freq pattern setQ, P can be viewed asits conditional patternbase, and a pattern generated from P , such as a10, will generatewith it a new frequent itemset, ad4, since a appears together with d at most four times.Thus, for d4 the set of frequent patterns generated will be d4  freq pattern setP ad4, bd4, cd4, abd4, acd4, bcd4, abcd4, where X  Y means that every pattern in X is combined with every one in Y to form a crossproductlike largeritemset with the support being the minimum support between the two patterns. Thus,the complete set of frequent patterns generated by combining the results of P and Qwill be freq pattern setQ  freq pattern setP, with the support being the support ofthe itemset in Q which is always no more than the support of the itemsetfrom P.In summary, the set of frequent patterns generated from such a single prefix path consistsof three distinct sets 1 freq pattern setP, the set of frequent patterns generated from thesingle prefixpath, P 2 freq pattern setQ, the set of frequent patterns generated fromthe multipath part of the FPtree, Q and 3 freq pattern setQ  freq pattern setP, theset of frequent patterns involving both parts.We first show if an FPtree consists of a single path P , one can generate the set of frequentpatterns according to the following lemma.Lemma 3.2 Pattern generation for an FPtree consisting of single path. Suppose anFPtree T consists of a single path P. The complete set of the frequent patterns of T canbe generated by enumeration of all the combinations of the subpaths of P with the supportbeing the minimum support of the items contained in the subpath.Rationale. Let the single path P of the FPtree be a1s1  a2s2      ak sk. Sincethe FPtree contains a single path P , the support frequency si of each item ai for 1  i  kis the frequency of ai cooccurring with its prefix string. Thus, any combination of the itemsin the path, such as ai , . . . , a j  for 1  i, j  k, is a frequent pattern, with their cooccurrence frequency being the minimum support among those items. Since every item ineach path P is unique, there is no redundant pattern to be generated with such a combinational generation. Moreover, no frequent patterns can be generated outside the FPtree.Therefore, we have the lemma.We then show if an FPtree consists of a single prefixpath, the set of frequent patternscan be generated by splitting the tree into two according to the following lemma.Lemma 3.3 Pattern generation for an FPtree consisting of single prefix path. Supposean FPtree T, similar to the tree in figure 4a, consists of 1 a single prefix path P, similarto the tree P in figure 4b, and 2 the multipath part, Q, which can be viewed as anindependent FPtree with a pseudoroot R, similar to the tree Q in figure 4c.MINING FREQUENT PATTERNS WITHOUT CANDIDATE GENERATION 67The complete set of the frequent patterns of T consists of the following three portions1. The set of frequent patterns generated from P by enumeration of all the combinationsof the items along path P, with the support being the minimum support among all theitems that the pattern contains.2. The set of frequent patterns generated from Q by taking root R as null.3. The set of frequent patterns combining P and Q formed by taken the crossproductof the frequent patterns generated from P and Q, denoted as freq pattern setP freq pattern setQ, that is, each frequent itemset is the union of one frequent itemsetfrom P and one from Q and its support is the minimum one between the supports of thetwo itemsets.Rationale. Based on the FPtree construction rules, each node ai in the single prefix pathof the FPtree appears only once in the tree. The single prefixpath of the FPtree forms anew FPtree P , and the multipath part forms another FPtree Q. They do not share nodesrepresenting the same item. Thus, the two FPtrees can be mined separately.First, we show that each pattern generated from one of the three portions by followingthe pattern generation rules is distinct and frequent. According to Lemma 3.2, each patterngenerated from P , the FPtree formed by the single prefixpath, is distinct and frequent.The set of frequent patterns generated from Q by taking root R as null is also distinctand frequent since such patterns exist without combining any items in their conditionaldatabases which are in the items in P . The set of frequent patterns generated by combiningP and Q, that is, taking the crossproduct of the frequent patterns generated from P andQ, with the support being the minimum one between the supports of the two itemsets, isalso distinct and frequent. This is because each frequent pattern generated by P can beconsidered as a frequent pattern in the conditional patternbase of a frequent item in Q,and whose support should be the minimum one between the two supports since this is thefrequency that both patterns appear together.Second, we show that no patterns can be generated out of this three portions. Sinceaccording to Lemma 3.1, the FPtree T without being split into two FPtrees P and Q generates the complete set of frequent patterns by pattern growth. Since each pattern generatedfrom T will be generated from either the portion P or Q or their combination, the methodgenerates the complete set of frequent patterns.3.3. The frequentpattern growth algorithmBased on the above lemmas and properties, we have the following algorithm for miningfrequent patterns using FPtree.Algorithm 2 FPgrowth Mining frequent patterns with FPtree by pattern fragmentgrowth.Input A database DB, represented by FPtree constructed according to Algorithm 1, anda minimum support threshold  .Output The complete set of frequent patterns.68 HAN ET AL.Method call FPgrowthFPtree, null.Procedure FPgrowthTree, 1 if Tree contains a single prefix path  Mining single prefixpath FPtree2 then 3 let P be the single prefixpath part of Tree4 let Q be the multipath part with the top branching node replaced by a null root5 for each combination denoted as  of the nodes in the path P do6 generate pattern    with support  minimum support of nodes in 7 let freq pattern setP be the set of patterns so generated 8 else let Q be Tree9 for each item ai in Q do   Mining multipath FPtree10 generate pattern   ai   with support  ai .support11 construct s conditional patternbase and then s conditional FPtree Tree 12 if Tree  13 then call FPgrowthTree, 14 let freq pattern setQ be the set of patterns so generated 15 returnfreq pattern setP  freq pattern setQ  freq pattern setP freq pattern setQAnalysis. With the properties and lemmas in Sections 2 and 3, we show that the algorithmcorrectly finds the complete set of frequent itemsets in transaction database DB.As shown in Lemma 2.1, FPtree of DB contains the complete information of DB inrelevance to frequent pattern mining under the support threshold  .If an FPtree contains a single prefixpath, according to Lemma 3.3, the generation of thecomplete set of frequent patterns can be partitioned into three portions the single prefixpathportion P , the multipath portion Q, and their combinations. Hence we have lines 14 andline 15 of the procedure. According to Lemma 3.2, the generated patterns for the singleprefix path are the enumerations of the subpaths of the prefix path, with the support being theminimum support of the nodes in the subpath. Thus we have lines 57 of the procedure.After that, one can treat the multipath portion or the FPtree that does not contain the singleprefixpath as portion Q lines 4 and 8 and construct conditional patternbase and mineits conditional FPtree for each frequent itemset ai . The correctness and completeness ofthe prefix path transformation are shown in Property 3.2. Thus the conditional patternbasesstore the complete information for frequent pattern mining for Q. According to Lemmas 3.1and its corollary, the patterns successively grown from the conditional FPtrees are the setof sound and complete frequent patterns. Especially, according to the fragment growthproperty, the support of the combined fragments takes the support of the frequent itemsetsgenerated in the conditional patternbase. Therefore, we have lines 914 of the procedure.Line 15 sums up the complete result according to Lemma 3.3.Lets now examine the efficiency of the algorithm. The FPgrowth mining process scansthe FPtree of DB once and generates a small patternbase Bai for each frequent item ai ,MINING FREQUENT PATTERNS WITHOUT CANDIDATE GENERATION 69each consisting of the set of transformed prefix paths of ai . Frequent pattern mining is thenrecursively performed on the small patternbase Bai by constructing a conditional FPtreefor Bai . As reasoned in the analysis of Algorithm 1, an FPtree is usually much smaller thanthe size of DB. Similarly, since the conditional FPtree, FPtree  ai , is constructed on thepatternbase Bai , it should be usually much smaller and never bigger than Bai . Moreover, apatternbase Bai is usually much smaller than its original FPtree, because it consists of thetransformed prefix paths related to only one of the frequent items, ai . Thus, each subsequentmining process works on a set of usually much smaller patternbases and conditional FPtrees. Moreover, the mining operations consist of mainly prefix count adjustment, countinglocal frequent items, and pattern fragment concatenation. This is much less costly thangeneration and test of a very large number of candidate patterns. Thus the algorithm isefficient.From the algorithm and its reasoning, one can see that the FPgrowth mining process isa divideandconquer process, and the scale of shrinking is usually quite dramatic. If theshrinking factor is around 20100 for constructing an FPtree from a database, it is expectedto be another hundreds of times reduction for constructing each conditional FPtree fromits already quite small conditional frequent patternbase.Notice that even in the case that a database may generate an exponential number offrequent patterns, the size of the FPtree is usually quite small and will never grow exponentially. For example, for a frequent pattern of length 100, a1, . . . , a100, the FPtreeconstruction results in only one path of length 100 for it, possibly a1,     a100 ifthe items are ordered in the list of frequent items as a1, . . . , a100. The FPgrowth algorithmwill still generate about 1030 frequent patterns if time permits, such as a1, a2, . . ., a1a2,. . ., a1a2a3, . . ., a1 . . . a100. However, the FPtree contains only one frequent pattern path of100 nodes, and according to Lemma 3.2, there is even no need to construct any conditionalFPtree in order to find all the patterns.4. Scaling FPtreebased FPgrowth by database projectionFPgrowth proposed in the last section is essentially a main memorybased frequent pattern mining method. However, when the database is large, or when the minimum supportthreshold is quite low, it is unrealistic to assume that the FPtree of a database can fit inmain memory. A diskbased method should be worked out to ensure that mining is highlyscalable. In this section, a method is developed to first partition the database into a set of projected databases, and then for each projected database, construct and mine its correspondingFPtree.Let us revisit the mining problem in Example 1.Example 4. Suppose the FPtree in figure 1 cannot be held in main memory. Instead ofconstructing a global FPtree, one can project the transaction database into a set of frequentitemrelated projected databases as follows.Starting at the tail of the frequent item list, p, the set of transactions that contain itemp can be collected into pprojected database. Infrequent items and item p itself can beremoved from them because the infrequent items are not useful in frequent pattern mining,70 HAN ET AL.Table 3. Projected databases and their FPtrees.Item Projected database Conditional FPtreep fcam, cb, fcam c3pm fca, fcab, fca  f 3, c3, a3mb fca, f, c a fc, fc, fc  f 3, c3ac  f, f, f   f 3cf  and item p is by default associated with each projected transaction. Thus, the pprojecteddatabase becomes fcam, cb, fcam. This is very similar to the the pconditional patternbase shown in Table 2 except fcam and fcam are expressed as fcam2 in Table 2. Afterthat, the pconditional FPtree can be built on the pprojected database based on the FPtreeconstruction algorithm.Similarly, the set of transactions containing item m can be projected into mprojecteddatabase. Notice that besides infrequent items and item m, item p is also excluded from theset of projected items because item p and its association with m have been considered in thepprojected database. For the same reason, the bprojected database is formed by collectingtransactions containing item b, but infrequent items and items f , m and b are excluded. Thisprocess continues for deriving aprojected database, cprojected database, and so on. Thecomplete set of itemprojected databases derived from the transaction database are listed inTable 3, together with their corresponding conditional FPtrees. One can easily see that thetwo processes, construction of the global FPtree and projection of the database into a setof projected databases, derive identical conditional FPtrees.As shown in Section 2, a conditional FPtree is usually orders of magnitude smaller thanthe global FPtree. Thus, construction of a conditional FPtree from each projected databaseand then mining on it will dramatically reduce the size of FPtrees to be handled. Whatabout that a conditional FPtree of a projected database still cannot fit in main memoryOne can further project the projected database, and the process can go on recursively untilthe conditional FPtree fits in main memory.Let us define the concept of projected database formally.Definition 2 Projected database. Let ai be a frequent item in a transaction database, DB. The ai projected database for aiis derived from DB by collecting all the transactions containing ai and removing fromthem 1 infrequent items, 2 all frequent items after ai in the list of frequent items, and3 ai itself. Let a j be a frequent item in projected database. Then the a jprojected database isderived from theprojected database by collecting all entries containing a j and removingMINING FREQUENT PATTERNS WITHOUT CANDIDATE GENERATION 71from them 1 infrequent items, 2 all frequent items after a j in the list of frequent items,and 3 a j itself.According to the rules of construction of FPtree and that of construction of projecteddatabase, the ai projected database is derived by projecting the same set of items in thetransactions containing ai into the projected database as those collected in the constructionof the ai subtree in the FPtree. Thus, the two methods derive the same sets of conditionalFPtrees.There are two methods for database projection parallel projection and partition projection.Parallel projection is implemented as follows Scan the database to be projected once,where the database could be either a transaction database or an projected database. Foreach transaction T in the database, for each frequent item ai in T , project T to the ai projected database based on the transaction projection rule, specified in the definition ofprojected database. Since a transaction is projected in parallel to all the projected databasesin one scan, it is called parallel projection. The set of projected databases shown in Table 3of Example 4 demonstrates the result of parallel projection. This process is illustrated infigure 5a.Parallel projection facilitates parallel processing because all the projected databases areavailable for mining at the end of the scan, and these projected databases can be minedin parallel. However, since each transaction in the database is projected to multiple projected databases, if a database contains many long transactions with multiple frequentitems, the total size of the projected databases could be multiple times of the original one.Let each transaction contains on average l frequent items. A transaction is then projectedto l  1 projected database. The total size of the projected data from this transaction is1  2      l  1  ll12 . This implies that the total size of the single itemprojecteddatabases is about l12 times of that of the original database.To avoid such an overhead, we propose a partition projection method. Partition projectionis implemented as follows. When scanning the database original or projected to beprojected, a transaction T is projected to the ai projected database only if ai is a frequentitem in T and there is no any other item after ai in the list of frequent items appearingFigure 5. Parallel projection vs. partition projection.72 HAN ET AL.in the transaction. Since a transaction is projected to only one projected database at thedatabase scan, after the scan, the database is partitioned by projection into a set of projecteddatabases, and hence it is called partition projection.The projected databases are mined in the reversed order of the list of frequent items. Thatis, the projected database of the least frequent item is mined first, and so on. Each time whena projected database is being processed, to ensure the remaining projected databases obtainthe complete information, each transaction in it is projected to the a j projected database,where a j is the item in the transaction such that there is no any other item after a j in thelist of frequent items appearing in the transaction. The partition projection process for thedatabase in Example 4 is illustrated in figure 5b.The advantage of partition projection is that the total size of the projected databases ateach level is smaller than the original database, and it usually takes less memory and IOs tocomplete the partition projection. However, the processing order of the projected databasesbecomes important, and one has to process these projected databases in a sequential manner.Also, during the processing of each projected database, one needs to project the processedtransactions to their corresponding projected databases, which may take some IO as well.Nevertheless, due to its low memory requirement, partition projection is still a promisingmethod in frequent pattern mining.Example 5. Let us examine how the database in Example 4 can be projected by partitionprojection.First, by one scan of the transaction database, each transaction is projected to only oneprojected database. The first transaction, facdgimp, is projected to the pprojected databasesince p is the last frequent item in the list of frequent items. Thus, fcam i.e., with infrequentitems removed is inserted into the pprojected database. Similarly, transaction abcflmo isprojected to the mprojected database as fcab, bfhjo to the bprojected database as f ,bcksp to the pprojected database as cb, and finally, afcelpmn to the pprojected databaseas fcam. After this phrase, the entries in every projected databases are shown in Table 4.With this projection, the original database can be replaced by the set of singleitemprojected databases, and the total size of them is smaller than that of the original database.Second, the pprojected database is first processed i.e., construction of pconditionalFPtree, where p is the last item in the list of frequent items. During the processing of thepprojected database, each transaction is projected to the corresponding projected databaseTable 4. Singleitem projected databases by partition projection.Item Projected databasesp fcam, cb, fcamm fcabb  f a c f MINING FREQUENT PATTERNS WITHOUT CANDIDATE GENERATION 73according to the same partition projection rule. For example, fcam is projected to the mprojected database as fca, cb is projected to the bprojected database as c, and so on. Theprocess continues until every singleitem projected database is completely processed.5. Experimental evaluation and performance studyIn this section, we present a performance comparison of FPgrowth with the classicalfrequent pattern mining algorithm Apriori, and an alternative database projectionbased algorithm, TreeProjection. We first give a concise introduction and analysis to TreeProjection,and then report our experimental results.5.1. A comparative analysis of FPgrowth and TreeProjection methodsThe TreeProjection algorithm proposed by Agarwal et al. 2001 constructs a lexicographicaltree and projects a large database into a set of reduced, itembased subdatabases basedon the frequent patterns mined so far. Since it applies a tree construction method andperforms mining recursively on progressively smaller databases, it shares some similaritieswith FPgrowth. However, the two methods have some fundamental differences in treeconstruction and mining methodologies, and will lead to notable differences on efficiencyand scalability. We will explain such similarities and differences by working through thefollowing example.Example 6. For the same transaction database presented in Example 1, we construct thelexicographic tree according to the method described in Agarwal et al. 2001. The resulttree is shown in figure 6, and the construction process is presented as follows.By scanning the transaction database once, all frequent 1itemsets are identified. Asrecommended in Agarwal et al. 2001, the frequency ascending order is chosen as theFigure 6. A lexicographical tree built for the same transactional database DB.74 HAN ET AL.ordering of the items. So, the order is pmbacf , which is exactly the reverse order ofwhat is used in the FPtree construction. The top level of the lexicographic tree is constructed,i.e. the root and the nodes labeled by length1 patterns. At this stage, the root node labelednull and all the nodes which store frequent 1itemsets are generated. All the transactionsin the database are projected to the root node, i.e., all the infrequent items are removed.Each node in the lexicographical tree contains two pieces of information i the patternthat node represents, and ii the set of items that may generate longer patterns by addingthem to the pattern. The latter piece information is recorded as active extensions and activeitems.Then, a matrix at the root node is created, as shown below. The matrix computes thefrequencies of length2 patterns, thus all pairs of frequent items are included in the matrix.The items in pairs are arranged in the ordering. The matrix is built by adding counts fromevery transaction, i.e., computing frequent 2itemsets based on transactions stored in theroot node.p m b a c fpm 2b 1 1a 2 3 1c 3 3 2 3f 2 3 2 3 3At the same time of building the matrix, transactions in the root are projected to level1nodes as follows. Let t  a1a2 . . . an be a transaction with all items listed in ordering. t isprojected to node ai 1  i  n  1 as t ai  ai1ai2 . . . an .From the matrix, all the frequent 2itemsets are found as pc, ma, mc, mf , ac, af , cf .The nodes in lexicographic tree for them are generated. At this stage, the only nodesfor 1itemsets which are active are those for m and a, because only they contain enoughdescendants to potentially generate longer frequent itemsets. All nodes up to and includinglevel1 except for these two nodes are pruned.In the same way, the lexicographic tree is grown level by level. From the matrix at nodem, nodes labeled mac, ma f, and mcf are added, and only ma is active in all the nodes forfrequent 2itemsets. It is easy to see that the lexicographic tree in total contains 19 nodes.The number of nodes in a lexicographic tree is exactly that of the frequent itemsets.TreeProjection proposes an efficient way to enumerate frequent patterns. The efficiency ofTreeProjection can be explained by two main factors 1 the transaction projection limitsthe support counting in a relatively small space, and only related portions of transactionsare considered and 2 the lexicographical tree facilitates the management and counting ofcandidates and provides the flexibility of picking efficient strategy during the tree generation phase as well as transaction projection phase. Agarwal et al. 2001 reports that theiralgorithm is up to one order of magnitude faster than other recent techniques in literature.MINING FREQUENT PATTERNS WITHOUT CANDIDATE GENERATION 75However, in comparison with the FPgrowth method, TreeProjection suffers from someproblems related to efficiency, and scalability. We analyze them as follows.First, TreeProjection may encounter difficulties at computing matrices when the databaseis huge, when there are a lot of transactions containing many frequent items, andor when thesupport threshold is very low. This is because in such cases there often exist a large numberof frequent items. The size of the matrices at high level nodes in the lexicographical tree canbe huge, as shown in our introduction section. The study in TreeProjection Agarwal et al.,2001 has developed some smart memory caching methods to overcome this problem.However, it could be wise not to generate such huge matrices at all instead of findingsome smart caching techniques to reduce the cost. Moreover, even if the matrix can becached efficiently, its computation still involves some nontrivial overhead. To compute amatrix at node P with n projected transactions, the cost is Oni1Ti 22 , where Ti  isthe length of the transaction. If the number of transaction is large and the length of eachtransaction is long, the computation is costly. The FPgrowth method will never need tobuild up matrices and compute 2itemset frequency since it avoids the generation of anycandidate kitemsets for any k by applying a pattern growth method. Pattern growth can beviewed as successive computation of frequent 1itemset of the database and conditionalpattern bases and assembling them into longer patterns. Since computing frequent 1itemsets is much less expensive than computing frequent 2itemsets, the cost is substantiallyreduced.Second, since one transaction may contain many frequent itemsets, one transaction inTreeProjection may be projected many times to many different nodes in the lexicographicaltree. When there are many long transactions containing numerous frequent items, transactionprojection becomes a nontrivial cost of TreeProjection. The FPgrowth method constructsFPtree which is a highly compact form of transaction database. Thus both the size and thecost of computation of conditional pattern bases, which corresponds roughly to the compactform of projected transaction databases, are substantially reduced.Third, TreeProjection creates one node in its lexicographical tree for each frequent itemset. At the first glance, this seems to be highly compact since FPtree does not ensure thateach frequent node will be mapped to only one node in the tree. However, each branch of theFPtree may store many hidden frequent patterns due to the potential generation of manycombinations using its prefix paths. Notice that the total number of frequent kitemsets canbe very large in a large database or when the database has quite long frequent itemsets.For example, for a frequent itemset a1, a2, . . . , a100, the number of frequent itemsets atthe 50thlevel of the lexicographic tree will be  10050   1005050  1.0  1029. For the samefrequent itemset, FPtree and FPgrowth will only need one path of 100 nodes.In summary, FPgrowth mines frequent itemsets by 1 constructing highly compactFPtrees which share numerous projected transactions and hide or carry numerousfrequent patterns, and 2 applying progressive pattern growth of frequent 1itemsets whichavoids the generation of any potential combinations of candidate itemsets implicitly orexplicitly, whereas TreeProjection must generate candidate 2itemsets for each projecteddatabase. Therefore, FPgrowth is more efficient and more scalable than TreeProjection,especially when the number of frequent itemsets becomes really large. These observationsand analyses are well supported by our experiments reported in this section.76 HAN ET AL.5.2. Environments of experimentsAll the experiments are performed on a 266MHz Pentium PC machine with 128 megabytesmain memory, running on Microsoft WindowsNT. All the programs are written in MicrosoftVisual C6.0. Notice that we do not directly compare our absolute number ofruntime with those in some published reports running on the RISC workstations becausedifferent machine architectures may differ greatly on the absolute runtime for the samealgorithms. Instead, we implement their algorithms to the best of our knowledge based onthe published reports on the same machine and compare in the same running environment.Please also note that run time used here means the total execution time, that is, the period between input and output, instead of CPU time measured in the experiments in someliterature. We feel that run time is a more comprehensive measure since it takes the totalrunning time consumed as the measure of cost, whereas CPU time considers only the costof the CPU resource. Also, all reports on the runtime of FPgrowth include the time ofconstructing FPtrees from the original databases.The experiments are pursued on both synthetic and real data sets. The synthetic datasets which we used for our experiments were generated using the procedure described inAgrawal and Srikant 1994. We refer readers to it for more details on the generation ofdata sets.We report experimental results on two synthetic data sets. The first one is T10.I4.D100Kwith 1K items. In this data set, the average transaction size and average maximal potentiallyfrequent itemset size are set to 10 and 4, respectively, while the number of transactions inthe dataset is set to 100 K. It is a sparse dataset. The frequent itemsets are short and notnumerous.The second synthetic data set we used is T25.I20.D100K with 10 K items. The averagetransaction size and average maximal potentially frequent itemset size are set to 25 and 20,respectively. There exist exponentially numerous frequent itemsets in this data set whenthe support threshold goes down. There are also pretty long frequent itemsets as well asa large number of short frequent itemsets in it. It contains abundant mixtures of short andlong frequent itemsets.To test the capability of FPgrowth on dense datasets with long patterns, we use thereal data set Connect4, compiled from the Connect4 game state information. The data setis from the UCIrvine Machine Learning Database Repository httpwww.ics.uci.edumlearnMLRepository.html. It contains 67, 557 transactions, while each transaction is with43 items. It is a dense dataset with a lot of long frequent itemsets.5.3. Compactness of FPtreeTo test the compactness of FPtrees, we compare the sizes of the following structures. Alphabetical FPtree. It includes the space of all the links. However, in such an FPtree,the alphabetical order of items are used instead of frequency descending order. Ordered FPtree. Again, the size covers that of all links. In such an FPtree, the items aresorted according to frequency descending order.MINING FREQUENT PATTERNS WITHOUT CANDIDATE GENERATION 77Figure 7. Compactness of FPtree over data set Connect4. Transaction database. Each item in a transaction is stored as an integer. It is simply thesum of occurrences of items in transactions. Frequent transaction database. That is the subdatabase extracted from the original oneby removing all infrequent items.In real dataset Connect4, FPtree achieves good compactness. As seen from the resultshown in figure 7, the size of ordered FPtree is always smaller than the size of the transactiondatabase and the frequent transaction database. In a dense database, the size of the databaseand that of its frequent database are close. The size of the alphabetical FPtree is smaller thanthat of the two databases in most cases but is slightly larger about 1.5 to 2.5 times largerthan the size of the ordered FPtree. It indicates that the frequencydescending ordering ofthe items benefits data compression in this case.In dataset T25.I20.D100k, which contains abundant mixture of long and short frequentpatterns, FPtree is compact most of the time. The result is shown in figure 8. Only whenFigure 8. Compactness of FPtree over data set T25.I20.D100k.78 HAN ET AL.Figure 9. Compactness of FPtree over data set T10.I4.D100k.the support threshold lower than 2.5, is the size of FPtree larger than that of frequentdatabase. Moreover, as long as the support threshold is over 1.5, the FPtree is smallerthan the transaction database. The difference of sizes of ordered FPtree and alphabeticalFPtree is quite small in this dataset. It is about 2.In sparse dataset T10.I4.D100k, FPtree achieves good compactness when the supportthreshold is over 3.5. Again, the difference of ordered FPtree and alphabetical FPtree istrivial. The result is shown in figure 9.The above experiments lead to the following conclusions. FPtree achieves good compactness most of the time. Especially in dense datasets, itcan compress the database many times. Clearly, there is some overhead for pointers andcounters. However, the gain of sharing among frequent projections of transactions issubstantially more than the overhead and thus makes FPtree space more efficient inmany cases. When support is very low, FPtree becomes bushy. In such cases, the degree of sharingin branches of FPtree becomes low. The overhead of links makes the size of FPtreelarge. Therefore, instead of building FPtree, we should construct projected databases.That is the reason why we build FPtree for transaction databaseprojected database onlywhen it passes certain density threshold. From the experiments, one can see that such athreshold is pretty low, and easy to touch. Therefore, even for very large andor sparsedatabase, after one or a few rounds of database projection, FPtree can be used for all theremaining mining tasks.In the following experiments, we employed an implementation of FPgrowth that integrates both database projection and FPtree mining. The density threshold is set to 3, anditems are listed in frequency descending order.5.4. Scalability studyThe runtime of Apriori, TreeProjection, and FPgrowth on synthetic data set T10.I4.D100Kas the support threshold decreases from 0.15 to 0.01 is shown in figure 10.MINING FREQUENT PATTERNS WITHOUT CANDIDATE GENERATION 79Figure 10. Scalability with threshold over sparse data set.FPgrowth is faster than both Apriori and TreeProjection. TreeProjection is faster andmore scalable than Apriori. Since the dataset is sparse, as the support threshold is high,the frequent itemsets are short and the set of such itemsets is not large, the advantages ofFPgrowth and TreeProjection over Apriori are not so impressive. However, as the supportthreshold goes down, the gap becomes wider. FPgrowth can finish the computation forsupport threshold 0.01 within the time for Apriori over 0.05. TreeProjection is alsoscalable, but is slower than FPgrowth.The advantages of FPgrowth over Apriori becomes obvious when the dataset containsan abundant number of mixtures of short and long frequent patterns. Figure 11 shows theexperimental results of scalability with threshold over dataset T25.I20.D100k. FPgrowthcan mine with support threshold as low as 0.05, with which Apriori cannot work outwithin reasonable time. TreeProjection is also scalable and faster than Apriori, but is slowerthan FPgrowth.Figure 11. Scalability with threshold over dataset with abundant mixtures of short and long frequent patterns.80 HAN ET AL.Figure 12. Scalability with threshold over Connect4.The advantage of FPgrowth is dramatic in datasets with long patterns, which is challenging to the algorithms that mine the complete set of frequent patterns. The result on miningthe real dataset Connect4 is shown in figure 12. To the best of our knowledge, this is thefirst algorithm that handles such dense real dataset in performance study. From the figure,one can see that FPgrowth is scalable even when there are many long patterns. Withoutcandidate generation, FPgrowth enumerates long patterns efficiently. In such datasets, neither Apriori nor TreeProjection are comparable to the performance of FPgrowth. To dealwith long patterns, Apriori has to generate a tremendous number of candidates, that is verycostly. The main costs in TreeProjection are matrix computation and transaction projection.In a database with a large number of frequent items, the matrices become quite large, andthe computation cost jumps up substantially. In contrast, the height of FPtree is limited bythe maximal length of the transactions, and many transactions share the prefix paths of anFPtree. This explains why FPgrowth has distinct advantages when the support thresholdis low and when the number of transactions is large.To test the scalability of FPgrowth against the number of transactions, a set of syntheticdatasets are generated using the same parameters of T10.I4 and T25.I20, and the numberof transactions ranges from 100 k to 1 M. FPgrowth is tested over them using the samesupport threshold in percentage. The result is in figure 13, which shows the linear increaseof runtime with the number of transactions. Please note that unlike the way reported in someliterature, we do not replicate transactions in real data sets to test the scalability. This isbecause no matter how many times the transactions are replicated, FPgrowth builds up anFPtree with the size identical to that of the original nonreplicated one, and the scalingupof such databases becomes trivial.6. DiscussionsThe frequentpattern growth method introduced here represents an interesting approach forscalable frequentpattern mining. In this section, we discuss some additional issues relatedto its implementation, usage, and extension.MINING FREQUENT PATTERNS WITHOUT CANDIDATE GENERATION 81Figure 13. Scalability of FPgrowth with number of transactions.6.1. Materialization and maintenance of FPtreesAlthough we have studied the dynamic generation of FPtrees, it is possible to materializeand incrementally update an FPtree. We examine the related issues here.6.1.1. Construction of diskresident FPtrees. When the database grows very large, itis unrealistic to construct a main memorybased FPtree. Database projection has beenintroduced in Section 3.4 as an effective approach. An interesting alternative is to constructa diskresident FPtree.The Btree structure, popularly used in relational database systems, can be used to indexFPtree as well. Since there are many operations localized to single paths or individualitem prefix subtrees, such as pattern matching for node insertion, creation of transformedprefix paths for each node ai , etc., it is important to cluster FPtree nodes according to thetreesubtree structure. That is, one should 1 store each item prefix subtree on the samepage, if possible, or at least on a sequence of continuous pages on disk 2 store eachsubtree on the same page, and put the shared prefix path as the header information of thepage and 3 cluster the nodelinks belonging to the same paged nodes together, etc. Thisalso facilitates a breadthfirst search fashion for mining all the patterns starting from all thenodes in the header in parallel.To reduce the IO costs by following nodelinks, mining should be performed in a groupaccessing mode, that is, when accessing nodes following nodelinks, one should exhaustthe node traversal tasks in main memory before fetching the nodes on disks.Notice that one may also construct nodelinkfree FPtrees. In this case, when traversinga tree path, one should project the prefix subpaths of all the nodes into the correspondingconditional pattern bases. This is feasible if both FPtree and one page of each of its onelevel conditional pattern bases can fit in memory. Otherwise, additional IOs will be neededto swap in and out the conditional pattern bases.6.1.2. Materialization of an FPtree for frequentpattern mining. Although an FPtreeis rather compact, its construction needs two scans of a transaction database, which may82 HAN ET AL.represent a nontrivial overhead. It could be beneficial to materialize an FPtree for regularfrequent pattern mining.One difficulty for FPtree materialization is how to select a good minimum support threshold  in materialization since  is usually querydependent. To overcome this difficulty, onemay use a low  that may usually satisfy most of the mining queries in the FPtree construction. For example, if we notice that 98 queries have   20, we may choose   20as the FPtree materialization threshold that is, only 2 of queries may need to construct anew FPtree. Since an FPtree is organized in the way that less frequently occurring itemsare located at the deeper paths of the tree, it is easy to select only the upper portions of theFPtree or drop the low portions which do not satisfy the support threshold when miningthe queries with higher thresholds. Actually, one can directly work on the materializedFPtree by starting at an appropriate header entry since one just need to get the prefix pathsno matter how low support the original FPtree is.6.1.3. Incremental update of an FPtree. Another issue related to FPtree materializationis how to incrementally update an FPtree, such as when adding daily new transactions intoa database containing records accumulated for months.If the materialized FPtree takes 1 as its minimum support i.e., it is just a compact versionof the original database, the update will not cause any problem since adding new records isequivalent to scanning additional transactions in the FPtree construction. However, a fullFPtree may be an undesirably large. Thus setting 1 as its minimum support may not be agood solution.In the general case, we can register the occurrence frequency of every items in F1 andtrack them in updates. This is not too costly but it benefits the incremental updates of anFPtree as follows. Suppose an FPtree was constructed based on a validity support thresholdcalled watermark   0.1 in a DB with 108 transactions. Suppose an additional 106transactions are added in. The frequency of each item is updated. If the highest relativefrequency among the originally infrequent items i.e., not in the FPtree goes up to, say12, the watermark will need to go up accordingly to   0.12 to exclude such items.However, with more transactions added in, the watermark may even drop since an itemsrelative support frequency may drop with more transactions added in. Only when the FPtreewatermark is raised to some undesirable level, the reconstruction of the FPtree for the newDB becomes necessary.6.2. Extensions of frequentpattern growth method in data miningThe philosophy of database compression and partitionbased frequentpattern mining canbe extended to constraintbased mining and mining other kinds of frequent patterns, suchas maxpatterns, sequential patterns.6.2.1. FPtree mining with constraints. Constraintbased frequentpattern mining represents an important direction towards usercontrolled data mining. Constraintbased association mining using the Apriorilike mining methodology has been studied extensivelySrikant et al., 1997 Ng et al., 1998. With the introduction of FPgrowth method, oneMINING FREQUENT PATTERNS WITHOUT CANDIDATE GENERATION 83may wonder whether constraintbased mining may benefit with FPtreelike structures. Athorough study of this issue, such as classification of various kinds of constraints and development of methods of FPtreebased mining with sophisticated constraints, such as thosein Ng et al. 1998, should be the task of another research paper.2 Here we only showhow to apply FPtree structure to mining frequent patterns by incorporation of constraintsassociated with a set of items.Suppose one may just like to derive frequent patterns only associated with a particularset of items, S, such as mining the set of frequent patterns containing c or m in Example 1.Instead of mining frequent patterns for all the frequent items, one may explore the FPtreebased mining as follows. With the same FPtree, the FPgrowth mining method may justneed to be modified minorly. The only additional care is when computing a transformedprefix path for an item m, one also needs to look down the path to include the items, suchas p, which are not in S. Our previous computation for the whole database will not needto consider ms pairing with p since it would have been checked when examining node p.However, since p is not in S now, such a pair would have been missed if ms computationdid not look down the path to include p.6.2.2. FPtree mining of other frequent patterns. FPtreebased mining method can beextended to mining many other kinds of interesting frequent patterns. We examine a fewsuch examples.The first example is on mining frequent closed itemsets. Since frequent pattern miningoften generates a very large number of frequent itemsets, it hinders the effectiveness ofmining since users have to sift through a large number of mined rules to find useful ones.An interesting alternative method proposed recently by Pasquier et al. 1999 is to minefrequent closed itemsets, where an itemset  is a closed itemset if there exists no propersuperset of  that has the same support as  in the database. Mining frequent closed itemsetshas the same power as mining the complete set of frequent itemsets, but it may substantiallyreduce redundant rules to be generated and increase the effectiveness of mining. A study atmining closed items using an Apriorilike philosophy but adopting a vertical data format,i.e., viewing database as item id a set of transactions instead of transaction id a setof items, has been studied in Zaki and Hsiao 2002. The FPtreebased frequentpatterngrowth method can be extended and further optimized for mining such closed itemsets,which has been reported in our subsequent study, as a new closed pattern mining algorithm,called CLOSET Pei et al., 2000.The second example is on mining sequential patterns. A sequential patterns is a frequentpattern in an event sequence database where a sequence is a set of events happening atdifferent times. Most of the previously developed sequential pattern mining methods, suchas Agrawal and Srikant 1995, Srikant and Agrawal 1996 and Mannila et al. 1997, followthe methodology of Apriori since the Aprioribased method may substantially reduce thenumber of combinations to be examined. However, Apriori still encounters problems when asequence database is large andor when sequential patterns to be mined are numerous andorlong. Our frequentpattern growth method can be extended to mining sequential patternsusing the ideas of projection of sequence database and growth of subsequence fragmentsto confine search space. An efficient sequential pattern method, called PrefixSpan Pei84 HAN ET AL.et al., 2001, has been developed in this direction and our performance study has shown asubstantial performance improvement over the Aprioribased GSP algorithm Srikant andAgrawal, 1996.7. ConclusionsWe have proposed a novel data structure, frequent pattern tree FPtree, for storing compressed, crucial information about frequent patterns, and developed a pattern growth method,FPgrowth, for efficient mining of frequent patterns in large databases.There are several advantages of FPgrowth over other approaches 1 It constructs ahighly compact FPtree, which is usually substantially smaller than the original databaseand thus saves the costly database scans in the subsequent mining processes. 2 It appliesa pattern growth method which avoids costly candidate generation and test by successivelyconcatenating frequent 1itemset found in the conditional FPtrees. This ensures that itnever generates any combinations of new candidate sets which are not in the databasebecause the itemset in any transaction is always encoded in the corresponding path ofthe FPtrees. In this context, mining is not Apriorilike restricted generationandtest butfrequent pattern fragment growth only. The major operations of mining are count accumulation and prefix path count adjustment, which are usually much less costly than candidategeneration and pattern matching operations performed in most Apriorilike algorithms.3 It applies a partitioningbased divideandconquer method which dramatically reducesthe size of the subsequent conditional pattern bases and conditional FPtree. Several otheroptimization techniques, including direct pattern generation for single treepath and employing the least frequent events as suffix, also contribute to the efficiency of the method.We have implemented the FPgrowth method, studied its performance in comparison withseveral influential frequent pattern mining algorithms in large databases. Our performancestudy shows that the method mines both short and long patterns efficiently in large databases,outperforming the current candidate pattern generationbased algorithms. The FPgrowthmethod has also been implemented in the DBMiner system and been tested in large industrialdatabases, such as a retail chain database, with satisfactory performance.Since our first publication of FPgrowth method for mining frequent patterns withoutcandidate generation Han et al., 2000, there have been many subsequent studies on improvements of performance of frequent patterns based on the patterngrowth philosophy, aswell as extension of the scope of the method to cover other kinds of pattern mining tasks.The patterngrowth framework has been extended towards 1 mining closed itemsets asproposed in the CLOSET algorithm Pei et al., 2000, 2 mining sequential patterns as proposed in the PrefixSpan algorithm Pei et al., 2001, and 3 pushing tough constraints deepinto frequent pattern mining processes Pei et al., 2001. Moreover, a notable effort is theproposal of the Hmine algorithm Pei et al., 2001 for mining frequent patterns efficientlyin sparse data sets. FPgrowth, though efficient at mining dense data sets, may incur unnecessary overhead due to its recursive construction of FPtrees. Following the philosophyof frequent pattern growth, but not constructing FPtrees, the Hmine algorithm constructsanother data structure, called Hstruct, and mines directly on the Hstruct without recursivegeneration of numerous conditional FPtrees. The experiments reported in Pei et al. 2001MINING FREQUENT PATTERNS WITHOUT CANDIDATE GENERATION 85shows that Hmine outperforms FPgrowth when database is sparse. A suggested approachis to integrate the two algorithms and dynamically select the FPtreebased and Hstructbased algorithms based on the characteristics of current data distribution. Recently, somestudies also show that various FPtree mining strategies such as bottomup vs. topdownmethods may lead to different efficiency over data sets of different data distributions.There are still many interesting research issues related to the extensions of patterngrowthapproach, such as mining structured patterns by further development of the frequent patterngrowth approach, mining approximate or faulttolerant patterns in noisy environments,frequentpatternbased clustering and classification, and so on.AcknowledgmentsWe would like to express our thanks to anonymous reviewers of our conference and journalsubmissions on this theme. Their constructive comments have improved the quality of thiswork.Notes1. Notice that support is defined here as absolute occurrence frequency, not the relative one as in some literature.2. One such study has been performed by us in Pei et al. 2001.ReferencesAgarwal, R., Aggarwal, C., and Prasad, V.V.V. 2001. A tree projection algorithm for generation of frequentitemsets. Journal of Parallel and Distributed Computing, 61350371,Agrawal, R., Imielinski, T., and Swami, A. 1993. Mining association rules between sets of items in large databases.In Proc. 1993 ACMSIGMOD Int. Conf. Management of Data SIGMOD93, Washington, DC, pp. 207216.Agrawal, R., Mannila, H., Srikant, R., Toivonen, H., and Verkamo, A.I. 1996. Fast discovery of association rules.In Advances in Knowledge Discovery and Data Mining, U.M. Fayyad, G. PiatetskyShapiro, P. Smyth, and R.Uthurusamy Eds., AAAIMIT Press, pp. 307328.Agrawal, R. and Srikant, R. 1994. Fast algorithms for mining association rules. In Proc. 1994 Int. Conf. VeryLarge Data Bases VLDB94, Santiago, Chile, pp. 487499.Agrawal, R. and Srikant, R. 1995. Mining sequential patterns. In Proc. 1995 Int. Conf. Data Engineering ICDE95,Taipei, Taiwan, pp. 314.Bayardo, R.J. 1998. Efficiently mining long patterns from databases. In Proc. 1998 ACMSIGMOD Int. Conf.Management of Data SIGMOD98, Seattle, WA, pp. 8593.Brin, S., Motwani, R., and Silverstein, C. 1997. Beyond market basket Generalizing association rules to correlations. In Proc. 1997 ACMSIGMOD Int. Conf. Management of Data SIGMOD97, Tucson, Arizona,pp. 265276.Dong, G. and Li, J. 1999. Efficient mining of emerging patterns Discovering trends and differences. In Proc. 1999Int. Conf. Knowledge Discovery and Data Mining KDD99, San Diego, CA, pp. 4352.Grahne, G., Lakshmanan, L., and Wang, X. 2000. Efficient mining of constrained correlated sets. In Proc. 2000Int. Conf. Data Engineering ICDE00, San Diego, CA, pp. 512521.Han, J., Dong, G., and Yin, Y. 1999. Efficient mining of partial periodic patterns in time series database. In Proc.1999 Int. Conf. Data Engineering ICDE99, Sydney, Australia, pp. 106115.Han, J., Pei, J., and Yin, Y. 2000. Mining frequent patterns without candidate generation. In Proc. 2000 ACMSIGMOD Int. Conf. Management of Data SIGMOD00, Dallas, TX, pp. 112.86 HAN ET AL.Kamber, M., Han, J., and Chiang, J.Y. 1997. Metaruleguided mining of multidimensional association rules usingdata cubes. In Proc. 1997 Int. Conf. Knowledge Discovery and Data Mining KDD97, Newport Beach, CA,pp. 207210.Lent, B., Swami, A., and Widom, J. 1997. Clustering association rules. In Proc. 1997 Int. Conf. Data EngineeringICDE97, Birmingham, England, pp. 220231.Mannila, H., Toivonen, H., and Verkamo, A.I. 1994. Efficient algorithms for discovering association rules. In Proc.AAAI94 Workshop Knowledge Discovery in Databases KDD94, Seattle, WA, pp. 181192.Mannila, H., Toivonen, H., and Verkamo, A.I. 1997. Discovery of frequent episodes in event sequences. DataMining and Knowledge Discovery, 1259289.Ng, R., Lakshmanan, L.V.S., Han, J., and Pang, A. 1998. Exploratory mining and pruning optimizations ofconstrained associations rules. In Proc. 1998 ACMSIGMOD Int. Conf. Management of Data SIGMOD98,Seattle, WA, pp. 1324.Pasquier, N., Bastide, Y., Taouil, R., and Lakhal, L. 1999. Discovering frequent closed itemsets for associationrules. In Proc. 7th Int. Conf. Database Theory ICDT99, Jerusalem, Israel, pp. 398416.Park, J.S., Chen, M.S., and Yu, P.S. 1995. An effective hashbased algorithm for mining association rules. In Proc.1995 ACMSIGMOD Int. Conf. Management of Data SIGMOD95, San Jose, CA, pp. 175186.Pei, J., Han, J., and Lakshmanan, L.V.S. 2001. Mining frequent itemsets with convertible constraints. In Proc.2001 Int. Conf. Data Engineering ICDE01, Heidelberg, Germany, pp. 433332.Pei, J., Han, J., Lu, H., Nishio, S., Tang, S., and Yang, D. 2001. HMine Hyperstructure mining of frequentpatterns in large databases. In Proc. 2001 Int. Conf. Data Mining ICDM01, San Jose, CA, pp. 441448.Pei, J., Han, J., and Mao, R. 2000. CLOSET An efficient algorithm for mining frequent closed itemsets. InProc. 2000 ACMSIGMOD Int. Workshop Data Mining and Knowledge Discovery DMKD00, Dallas, TX,pp. 1120.Pei, J., Han, J., MortazaviAsl, B., Pinto, H., Chen, Q., Dayal, U., and Hsu, M.C. 2001. PrefixSpan Miningsequential patterns efficiently by prefixprojected pattern growth. In Proc. 2001 Int. Conf. Data EngineeringICDE01, Heidelberg, Germany, pp. 215224.Srikant, R. and Agrawal, R. 1996. Mining sequential patterns Generalizations and performance improvements.In Proc. 5th Int. Conf. Extending Database Technology EDBT96, Avignon, France, pp. 317.Silverstein, C., Brin, S., Motwani, R., and Ullman, J. 1998. Scalable techniques for mining causal structures. InProc. 1998 Int. Conf. Very Large Data Bases VLDB98, New York, NY, pp. 594605.Savasere, A., Omiecinski, E., and Navathe, S. 1995. An efficient algorithm for mining association rules in largedatabases. In Proc. 1995 Int. Conf. Very Large Data Bases VLDB95, Zurich, Switzerland, pp. 432443.Sarawagi, S., Thomas, S., and Agrawal, R. 1998. Integrating association rule mining with relational databasesystems Alternatives and implications. In Proc. 1998 ACMSIGMOD Int. Conf. Management of Data SIGMOD98, Seattle, WA, pp. 343354.Srikant, R., Vu, Q., and Agrawal, R. 1997. Mining association rules with item constraints. In Proc. 1997 Int. Conf.Knowledge Discovery and Data Mining KDD97, Newport Beach, CA, pp. 6773.Zaki, M.J. and Hsiao, C.J. 2002. CHARM An efficient algorithm for closed itemset mining. In Proc. 2002 SIAMInt. Conf. Data Mining, Arlington, VA, pp. 457473.Jiawei Han is a Professor in the Department of Computer Science at the University of Illinois at UrbanaChampaign. Previously, he was an Endowed University Professor at Simon Fraser University, Canada. He hasbeen working on research into data mining, data warehousing, spatial and multimedia databases, deductive andobjectoriented databases, and biomedical databases, with over 250 journal and conference publications. He haschaired or served in over 90 program committees of international conferences and workshops. He also servedor is serving on the editorial boards for Data Mining and Knowledge Discovery An International Journal, IEEETransactions on Knowledge and Data Engineering, and Journal of Intelligent Information Systems. He is currentlyserving on the Board of Directors for the Executive Committee of ACM Special Interest Group on KnowledgeDiscovery and Data Mining SIGKDD and chairman of ACM SIGKDD Curriculum Committee. Jiawei has received IBM Faculty Awards, the Outstanding Contribution Award at the 2002 International Conference on DataMining, and an ACM Service Award.MINING FREQUENT PATTERNS WITHOUT CANDIDATE GENERATION 87Jian Pei received the B.Eng. and the M.Eng. degrees, both in Computer Science, from Shanghai Jiao TongUniversity, China, in 1991 and 1993, respectively, and the Ph.D. degree in Computing Science from Simon FraserUniversity, Canada, in 2002. He was a Ph.D. candidate in Peking University in 19971999. He is currently anAssistant Professor of Computer Science and Engineering, the State University of New York at Buffalo, USA.His research interests include data mining, data warehousing, online analytical processing, database systems, andbioinformatics. His current research is supported in part by the National Science Foundation NSF. He haspublished over 40 research papers in refereed journals, conferences, and workshops. He has served in the programcommittees of over 30 international conferences and workshops. He has been a reviewer for some leading academicjournals. He is a member of the ACM, the ACM SIGMOD, the ACM SIGKDD and the IEEE Computer Society.Yiwen Yin received his M.Sc. degree in Computing Science at Simon Fraser University in 2001 and has beenworking as a software engineering in B.C., Canada.Runying Mao received her B.Eng degree from Zhejiang University, China, in 1997, and her M.Sc. degree fromSimon Fraser University, Canada, in 2001, both in Computer Science. She is currently working in the SQL ServerGroup of Microsoft Corp. She also worked in China Telecom from 1997 to 1999.
