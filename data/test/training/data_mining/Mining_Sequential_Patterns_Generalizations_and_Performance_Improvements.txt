Mining Sequential Patterns: Generalizations and
Performance Improvements
Ramakrishnan Srikant? and Rakesh Agrawal
fsrikant, ragrawalgalmadenibmcom
IBM Almaden Research Center
650 Harry Road, San Jose, CA
Abstract. The problem of mining sequential patterns was recently in
troduced in [3]. We are given a database of sequences, where each se
quence is a list of transactions ordered by transaction-time, and each
transaction is a set of items. The problem is to discover all sequential
patterns with a userspecied minimum support, where the support of
a pattern is the number of data-sequences that contain the pattern. An
example of a sequential pattern is \5% of customers bought Foundation
and `Ringworld' in one transaction, followed by `Second Foundation' in
a later transaction". We generalize the problem as follows. First, we add
time constraints that specify a minimum and/or maximum time period
between adjacent elements in a pattern. Second, we relax the restric
tion that the items in an element of a sequential pattern must come
from the same transaction, instead allowing the items to be present in
a set of transactions whose transaction-times are within a userspecied
time window. Third, given a userdened taxonomy (is-a hierarchy) on
items, we allow sequential patterns to include items across all levels of
the taxonomy
We present GSP, a new algorithm that discovers these generalized se
quential patterns. Empirical evaluation using synthetic and real-life data
indicates that GSP is much faster than the AprioriAll algorithm pre
sented in [3]. GSP scales linearly with the number of datasequences
and has very good scale-up properties with respect to the average data
sequence size
1 Introduction
Data mining, also known as knowledge discovery in databases, has been recog
nized as a promising new area for database research. This area can be dened
as eciently discovering interesting rules from large databases
A new data mining problem, discovering sequential patterns, was introduced
in [3]. The input data is a set of sequences, called data-sequences. Each data
sequence is a list of transactions, where each transaction is a sets of literals, called
items. Typically there is a transaction-time associated with each transaction. A
sequential pattern also consists of a list of sets of items. The problem is to all

Also, Department of Computer Science, University of Wisconsin, Madison
sequential patterns with a userspecied minimum support, where the support of
a sequential pattern is the percentage of data-sequences that contain the pattern
For example, in the database of a book-club, each data-sequence may corre
spond to all book selections of a customer, and each transaction to the books
selected by the customer in one order. A sequential pattern might be \5% of
customers bought `Foundation', then `Foundation and Empire', and then Sec
ond Foundation' ". The data-sequence corresponding to a customer who bought
some other books in between these books still contains this sequential pattern
the data-sequence may also have other books in the same transaction as one of
the books in the pattern. Elements of a sequential pattern can be sets of items
for example, \ `Foundation' and `Ringworld', followed by `Foundation and Em
pire' and `Ringworld Engineers', followed by `Second Foundation'". However
all the items in an element of a sequential pattern must be present in a single
transaction for the data-sequence to support the pattern
This problem was motivated by applications in the retailing industry, includ
ing attached mailing, add-on sales, and customer satisfaction. But the results
apply to many scientic and business domains. For instance, in the medical do
main, a data-sequence may correspond to the symptoms or diseases of a patient
with a transaction corresponding to the symptoms exhibited or diseases diag
nosed during a visit to the doctor. The patterns discovered using this data could
be used in disease research to help identify symptoms/diseases that precede cer
tain diseases
However, the above problem denition as introduced in [3] has the following
limitations
1. Absence of time constraints. Users often want to specify maximum
and/or minimum time gaps between adjacent elements of the sequential
pattern. For example, a book club probably does not care if someone bought
\Foundation", followed by \Foundation and Empire" three years later; they
may want to specify that a customer should support a sequential pattern only
if adjacent elements occur within a specied time interval, say three months
(So for a customer to support this pattern, the customer should have bought
\Foundation and Empire" within three months of buying Foundation
2. Rigid denition of a transaction. For many applications, it does not
matter if items in an element of a sequential pattern were present in two
dierent transactions, as long as the transaction-times of those transactions
are within some small time window. That is, each element of the pattern can
be contained in the union of the items bought in a set of transactions, as long
as the dierence between the maximum and minimum transaction-times is
less than the size of a sliding time window. For example, if the bookclub
species a time window of a week, a customer who ordered the Foundation
on Monday, \Ringworld" on Saturday, and then \Foundation and Empire
and \Ringworld Engineers" in a single order a few weeks later would still
support the pattern \ `Foundation' and `Ringworld', followed by Foundation
and Empire' and `Ringworld Engineers'
3. Absence of taxonomies. Many datasets have a userdened taxonomy
Foundation
Science Fiction
and Empire
Foundation
People
NivenAsimov
Smileyâ€™sPerfect SpyRingworld
Engineers
RingworldSecond
Foundation
Le Carre
Spy
Fig. 1. Example of a Taxonomy
(is-a hierarchy) over the items in the data, and users want to pat
terns that include items across dierent levels of the taxonomy. An example
of a taxonomy is given in Figure 1. With this taxonomy, a customer who
bought \Foundation" followed by \Perfect Spy" would support the patterns
\ `Foundation' followed by `Perfect Spy' ", \ `Asimov' followed by Perfect
Spy' ", \ `Science Fiction' followed by `Le Carre' ", etc
In this paper, we generalize the problem denition given in [3] to incorporate
time constraints, sliding time windows, and taxonomies in sequential patterns
We present GSP (Generalized Sequential Patterns), a new algorithm that dis
covers all such sequential patterns. Empirical evaluation shows that GSP scales
linearly with the number of data-sequences, and has very good scale-up proper
ties with respect to the number of transactions per data-sequence and number
of items per transaction
1.1 Related Work
In addition to introducing the problem of sequential patterns, [3] presented three
algorithms for solving this problem, but these algorithms do not handle time
constraints, sliding windows, or taxonomies. Two of these algorithms were de
signed to only maximal sequential patterns; however, many applications
require all patterns and their supports. The third algorithm, AprioriAll,
all patterns; its performance was better than or comparable to the other two
algorithms. Briey AprioriAll is a three-phase algorithm. It all item
sets with minimum support (frequent itemsets), transforms the database so that
each transaction is replaced by the set of all frequent itemsets contained in the
transaction, and then sequential patterns. There are two problems with this
approach. First, it is computationally expensive to do the data transformation
onthey during each pass while sequential patterns. The alternative, to
transform the database once and store the transformed database, will be infea
sible or unrealistic for many applications since it nearly doubles the disk space
requirement which could be prohibitive for large databases. Second, while it is
possible to extend this algorithm to handle time constraints and taxonomies
it does not appear feasible to incorporate sliding windows. For the cases that
the extended AprioriAll can handle, our empirical evaluation shows that GSP is
upto 20 times faster
Somewhat related to our work is the problem of mining association rules
[1]. Association rules are rules about what items are bought together within
a transaction, and are thus intra-transaction patterns, unlike intertransaction
sequential patterns. The problem of association rules when there is a
userdened taxonomy on items has been addressed in [6]
The problem of discovering similarities in a database of genetic sequences
presented in [8], is relevant. However, the patterns they wish to discover are sub
sequences made up of consecutive characters separated by a variable number of
noise characters. A sequence in our problem consists of list of sets of characters
(items), rather than being simply a list of characters. In addition, we are inter
ested in all sequences with minimum support rather than some frequent
patterns
A problem of discovering frequent episodes in a sequence of events was pre
sented in [5]. Their patterns are arbitrary DAG (directed acyclic graphs), where
each vertex corresponds to a single event (or item) and an edge from event A
to event B denotes that A occurred before B. They move a time window across
the input sequence, and all patterns that occur in some userspecied per
centage of windows. Their algorithm is designed for counting the number of
occurrences of a pattern when moving a window across a single sequence, while
we are interested in patterns that occur in many dierent datasequences
1.2 Organization of the Paper
We give a formal description of the problem of mining generalized sequential pat
terns in Section 2. In Section 3, we describe GSP, an algorithm for such
patterns. We empirically compared the performance of GSP with the AprioriAll
algorithm [3], studied the scale-up properties of GSP, and examined the perfor
mance impact of time constraints and sliding windows. Due to space limitations
we could not include the details of these experiments which are reported in
However, we include the gist of the main results in Section 4. We conclude with
a summary in Section
2 Problem Statement
Denitions Let I = fi1; i2; : : : ; img be a set of literals, called items. Let T
be a directed acyclic graph on the literals. An edge in T represents an isa
relationship, and T represents a set of taxonomies. If there is an edge in T from
p to c, we call p a parent of c and c a child of p. (p represents a generalization of
c.) We model the taxonomy as a DAG rather than a tree to allow for multiple
taxonomies. We call bx an ancestor of x (and x a descendant of bx) if there is an
edge from bx to x in transitive-closure(T
An itemset is a non-empty set of items. A sequence is an ordered list of
itemsets. We denote a sequence s by h s1s2:::sn i, where sj is an itemset. We
also call sj an element of the sequence. We denote an element of a sequence by
(x1; x2; :::; xm), where xj is an item. An item can occur only once in an element
of a sequence, but can occur multiple times in dierent elements. An itemset is
considered to be a sequence with a single element. We assume without loss of
generality that items in an element of a sequence are in lexicographic order
A sequence h a1a2:::an i is a subsequence of another sequence h b1b2:::bm i if
there exist integers i1 < i2 < ::: < in such that a1 bi1 , a2 bi2 , ..., an bin
For example, the sequence h (3) (4 5) (8) i is a subsequence of h (7) (3, 8) (9)
5, 6) (8) i, since (3) (3, 8), (4, 5) (4, 5, 6) and (8) (8). However, the
sequence h (3) (5) i is not a subsequence of h (3, 5) i (and vice versa
Input We are given a database D of sequences called data-sequences. Each
data-sequence is a list of transactions, ordered by increasing transactiontime
A transaction has the following sequence-id, transaction-id, transaction
time, and the items present in the transaction. While we expect the items in a
transaction to be leaves in T , we do not require this
For simplicity, we assume that no data-sequence has more than one trans
action with the same transaction-time, and use the transaction-time as the
transactionidentier We do not consider quantities of items in a transaction
Support The support count (or simply support) for a sequence is dened as the
fraction of total data-sequences that \contain" this sequence. (Although the word
\contains" is not strictly accurate once we incorporate taxonomies, it captures
the spirt of when a data-sequence contributes to the support of a sequential
pattern.) We now dene when a data-sequence contains a sequence, starting
with the denition as in [3], and then adding taxonomies, sliding windows, and
time constraints
as in [3]: In the absence of taxonomies, sliding windows and time con
straints, a data-sequence contains a sequence s if s is a subsequence of the
datasequence
plus taxonomies: We say that a transaction T contains an item x 2 I if
x is in T or x is an ancestor of some item in T . We say that a transaction
T contains an itemset y I if T contains every item in y. A datasequence
d = h d1:::dm i contains a sequence s = h s1:::sn i if there exist integers i1 lt
i2 < ::: < in such that s1 is contained in di1 , s2 is contained in di2, ..., sn
is contained in din . If there is no taxonomy, this degenerates into a simple
subsequence test
plus sliding windows: The sliding window generalization relaxes the def
inition of when a data-sequence contributes to the support of a sequence
by allowing a set of transactions to contain an element of a sequence, as
long as the dierence in transaction-times between the transactions in the
set is less than the userspecied window-size. Formally, a datasequence
d = h d1:::dm i contains a sequence s = h s1:::sn i if there exist integers
l1 u1 < l2 u2 < ::: < ln un such that
1. si is contained in
ui
kli
dk, 1 i n, and
2. transactiontimedui transaction-time(dli) window-size, 1 i n
plus time constraints: Time constraints restrict the time gap between
sets of transactions that contain consecutive elements of the sequence
Given userspecied window-size, max-gap and min-gap, a datasequence
d = h d1:::dm i contains a sequence s = h s1:::sn i if there exist integers
l1 u1 < l2 u2 < ::: < ln un such that
1. si is contained in
ui
kli
dk, 1 i n
2. transactiontimedui transaction-time(dli) window-size, 1 i n
3. transaction-time(dli ) transactiontimedui > min-gap, 2 i n
and
4. transactiontimedui transactiontimedli ) max-gap, 2 i n
The two conditions are the same as in the earlier denition of when a
data-sequence contains a pattern. The third condition species the minimum
time-gap constraint, and the last the maximum time-gap constraint
We will refer to transaction-time(dli) as start-time(si), and transaction
time(dui) as end-time(si). In other-words, start-time(si) and endtimesi
correspond to the and last transaction-times of the set of transactions
that contain si
Note that if there is no taxonomy, min-gap = 0, max-gap = 1 and
window-size = 0 we get the notion of sequential patterns as introduced in
where there are no time constraints and items in an element come from a single
transaction
2.1 Problem Denition
Given a database D of data-sequences, a taxonomy T , userspecied min-gap and
max-gap time constraints, and a userspecied sliding-window size, the problem
of mining sequential patterns is to all sequences whose support is greater
than the userspecied minimum support. Each such sequence represents a se
quential pattern, also called a frequent sequence
Given a frequent sequence s = h s1:::sn i, it is often useful to know the sup
port relationship" between the elements of the sequence. That is, what fraction
of the data-sequences that support h s1:::si i support the entire sequence s. Since
h s1:::si i must also be a frequent sequence, this relationship can easily be com
puted
2.2 Example
Consider the data-sequences shown in Figure 2. For simplicity, we have assumed
that the transaction-times are integers; they could represent, for instance, the
number of days after January 1, 1995. We have used an abbreviated version of
the taxonomy given in Figure 1. Assume that the minimum support has been
set to 2 datasequences
With the [3] problem denition the only 2-element sequential patterns is
h (Ringworld) (Ringworld Engineers) i
Database D
Sequence-Id Transaction Items
Time
C1 1 Ringworld
C1 2 Foundation
C1 15 Ringworld Engineers, Second Foundation
C2 1 Foundation, Ringworld
C2 20 Foundation and Empire
C2 50 Ringworld Engineers
Taxonomy T
Foundation
and Empire
Foundation
Foundation
NivenAsimov
Ringworld
Engineers
RingworldSecond
Fig. 2. Example
Setting a sliding-window of 7 days adds the pattern
h (Foundation, Ringworld) (Ringworld Engineers) i
since C1 now supports this pattern. (\Foundation" and \Ringworld" are present
within a period of 7 days in data-sequence C
Further setting a max-gap of 30 days results in both the patterns being
dropped, since they are no longer supported by customer C
If we only add the taxonomy, but no sliding-window or time constraints, one
of the patterns added is
h (Foundation) (Asimov) i
Observe that this pattern is not simply a replacement of an item with its ancestor
in an existing pattern
3 Algorithm GSP
The basic structure of the GSP algorithm for sequential patterns is as
follows. The algorithm makes multiple passes over the data. The pass de
termines the support of each item, that is, the number of data-sequences that
include the item. At the end of the pass, the algorithm knows which items
are frequent, that is, have minimum support. Each such item yields a element
frequent sequence consisting of that item. Each subsequent pass starts with a
seed set: the frequent sequences found in the previous pass. The seed set is used
to generate new potentially frequent sequences, called candidate sequences. Each
candidate sequence has one more item than a seed sequence; so all the candidate
sequences in a pass will have the same number of items. The support for these
candidate sequences is found during the pass over the data. At the end of the
pass, the algorithm determines which of the candidate sequences are actually
frequent. These frequent candidates become the seed for the next pass. The al
gorithm terminates when there are no frequent sequences at the end of a pass
or when there are no candidate sequences generated
We need to specify two key details
1. Candidate generation: how candidates sequences are generated before the
pass begins. We want to generate as few candidates as possible while main
taining completeness
2. Counting candidates: how the support count for the candidate sequences is
determined
Candidate generation is discussed in Section 3.1, and candidate counting in Sec
tion 3.2. We incorporate time constraints and sliding windows in this discussion
but do not consider taxonomies. Extensions required to handle taxonomies are
described in Section
Our algorithm is not a main-memory algorithm. If the candidates do not
in memory, the algorithm generates only as many candidates as will in mem
ory and the data is scanned to count the support of these candidates. Frequent
sequences resulting from these candidates are written to disk, while those candi
dates without minimum support are deleted. This procedure is repeated until all
the candidates have been counted. Further details about memory management
can be found in
3.1 Candidate Generation
We refer to a sequence with k items as a k-sequence. (If an item occurs multiple
times in dierent elements of a sequence, each occurrence contributes to the
value of k.) Let Lk denote the set of all frequent k-sequences, and Ck the set of
candidate ksequences
Given Lk the set of all frequent ksequences we want to generate a
superset of the set of all frequent k-sequences. We dene the notion of a
contiguous subsequence
Denition Given a sequence s = h s1s2:::sn i and a subsequence c, c is a con
tiguous subsequence of s if any of the following conditions hold
1. c is derived from s by dropping an item from either s1 or sn
2. c is derived from s by dropping an item from an element si which has at
least 2 items
3. c is a contiguous subsequence of c0, and c0 is a contiguous subsequence of s
For example, consider the sequence s = h (1, 2) (3, 4) (5) (6) i. The sequences
h (2) (3, 4) (5) i, h (1, 2) (3) (5) (6) i and h (3) (5) i are some of the contiguous
subsequences of s. However, h (1, 2) (3, 4) (6) i and h (1) (5) (6) i are not
We show in [7] that any data-sequence that contains a sequence s will also
contain any contiguous subsequence of s. If there is no max-gap constraint
the data-sequence will contain all subsequences of s (including noncontiguous
Frequent Candidate Sequences
3-Sequences after join after pruning
h (1, 2) (3) i h (1, 2) (3, 4) i h (1, 2) (3, 4) i
h (1, 2) (4) i h (1, 2) (3) (5) i
h (1) (3, 4) i
h (1, 3) (5) i
h (2) (3, 4) i
h (2) (3) (5) i
Fig. 3. Candidate Generation: Example
subsequences). This property provides the basis for the candidate generation
procedure
Candidates are generated in two steps
1. Join Phase. We generate candidate sequences by joining Lk with Lk
A sequence s1 joins with s2 if the subsequence obtained by dropping the
item of s1 is the same as the subsequence obtained by dropping the last item
of s2. The candidate sequence generated by joining s1 with s2 is the sequence
s1 extended with the last item in s2. The added item becomes a separate
element if it was a separate element in s2, and part of the last element of
s1 otherwise. When joining L1 with L1, we need to add the item in s2 both
as part of an itemset and as a separate element, since both h (x) (y) i and
h (x y) i give the same sequence h (y) i upon deleting the item. Observe
that s1 and s2 are contiguous subsequences of the new candidate sequence
2. Prune Phase.We delete candidate sequences that have a contiguous k
subsequence whose support count is less than the minimum support. If there
is no max-gap constraint, we also delete candidate sequences that have any
subsequence without minimum support
The above procedure is reminiscent of the candidate generation procedure
for association rules [2]; however details are quite dierent A proof of
correctness of this procedure is given in
Example Figure 3 shows L3, and C4 after the join and prune phases. In
the join phase, the sequence h (1, 2) (3) i joins with h (2) (3, 4) i to generate
h (1, 2) (3, 4) i and with h (2) (3) (5) i to generate h (1, 2) (3) (5) i. The remain
ing sequences do not join with any sequence in L3. For instance, h (1, 2) (4) i does
not join with any sequence since there is no sequence of the form h (2) (4 x) i or
h (2) (4) (x) i. In the prune phase, h (1, 2) (3) (5) i is dropped since its contiguous
subsequence h (1) (3) (5) i is not in L
3.2 Counting Candidates
While making a pass, we read one data-sequence at a time and increment the
support count of candidates contained in the data-sequence. Thus, given a set
of candidate sequences C and a data-sequence d, we need to all sequences
in C that are contained in d. We use two techniques to solve this problem
1. We use a hash-tree data structure to reduce the number of candidates in C
that are checked for a datasequence
2. We transform the representation of the data-sequence d so that we can e
ciently whether a specic candidate is a subsequence of d
3.2.1 Reducing the number of candidates that need to be checked
We adapt the hash-tree data structure of [2] for this purpose. A node of the
hash-tree either contains a list of sequences (a leaf node) or a hash table an
interior node). In an interior node, each non-empty bucket of the hash table
points to another node. The root of the hash-tree is dened to be at depth
An interior node at depth p points to nodes at depth p
Adding candidate sequences to the hash-tree. When we add a sequence
s, we start from the root and go down the tree until we reach a leaf. At an
interior node at depth p, we decide which branch to follow by applying a hash
function to the pth item of the sequence. Note that we apply the hash function
to the pth item, not the pth element. All nodes are initially created as leaf nodes
When the number of sequences in a leaf node exceeds a threshold, the leaf node
is converted to an interior node
Finding the candidates contained in a data-sequence. Starting from the
root node, we all the candidates contained in a data-sequence d. We apply
the following procedure, based on the type of node we are at
Interior node, if it is the root: We apply the hash function to each item in
d, and recursively apply this procedure to the node in the corresponding
bucket. For any sequence s contained in the data-sequence d, the item
of s must be in d. By hashing on every item in d, we ensure that we only
ignore sequences that start with an item not in d
Interior node, if it is not the root: Assume we reached this node by hash
ing on an item x whose transaction-time is t. We apply the hash func
tion to each item in d whose transaction-time is in twindowsize t
max(window-size;max-gap)] and recursively apply this procedure to the
node in the corresponding bucket
To see why this returns the desired set of candidates, consider a candidate
sequence s with two consecutive items x and y. Let x be contained in a trans
action in d whose transaction-time is t. For d to contain s, the transaction
time corresponding to y must be in twindowsize t+window-size] if y is
part of the same element as x, or in the interval (t; t+max-gap] if y is part of
the next element. Hence if we reached this node by hashing on an item x with
transaction-time t, y must be contained in a transaction whose transaction
time is in the interval twindowsize t+max(window-size;max-gap)] for the
data-sequence to support the sequence. Thus we only need to apply the hash
function to the items in d whose transaction-times are in the above interval
and check the corresponding nodes
Leaf node: For each sequence s in the leaf, we check whether d contains s
and add s to the answer set if necessary. (We will discuss below exactly how
to whether d contains a specic candidate sequence.) Since we check
each sequence contained in this node, we don't miss any sequences
3.2.2 Checking whether a data-sequence contains a specic sequence
Let d be a data-sequence, and let s = h s1:::sn i be a candidate sequence. We
describe the algorithm for checking if d contains s, assuming existence of
a procedure that the occurrence of an element of s in d after a given
time, and then describe this procedure
Contains test: The algorithm for checking if the data-sequence d contains a
candidate sequence s alternates between two phases. The algorithm starts in the
forward phase from the element
Forward phase: The algorithm successive elements of s in d as long
as the dierence between the end-time of the element just found and the
start-time of the previous element is less than max-gap. (Recall that for an
element si, start-time(si) and end-time(si) correspond to the and last
transaction-times of the set of transactions that contain si.) If the dierence
is more than max-gap, the algorithm switches to the backward phase. If an
element is not found, the data-sequence does not contain s
Backward phase: The algorithm backtracks and \pulls up" previous ele
ments. If si is the current element and end-time(si) = t, the algorithm
the set of transactions containing si whose transaction-times are after
tmaxgap The start-time for si (after si is pulled up) could be after
the end-time for si. Pulling up si may necessitate pulling up si because
the max-gap constraint between si and si may no longer be satised
The algorithmmoves backwards until either the max-gap constraint between
the element just pulled up and the previous element is satised or the
element has been pulled up. The algorithm then switches to the forward
phase, elements of s in d starting from the element after the last
element pulled up. If any element cannot be pulled up (that is, there is no
subsequent set of transactions which contain the element), the datasequence
does not contain s
This procedure is repeated, switching between the backward and forward phases
until all the elements are found. Though the algorithm moves back and forth
among the elements of s, it terminates because for any element si, the algo
rithm always checks whether a later set of transactions contains si; thus the
transaction-times for an element always increase
Example Consider the data-sequence shown in Figure 4. Consider the case
when max-gap is 30, min-gap is 5, and window-size is 0. For the candidate
sequence h (1, 2) (3) (4) i, we would (1, 2) at transaction-time 10, and
then (3) at time 45. Since the gap between these two elements (35 days
Transaction-Time Items
10 1,
25 4,
45
50 1,
65
90 2,
95
Fig. 4. Example DataSequence
Item Times
1 ! 10 ! 50 ! NULL
2 ! 10 ! 50 ! 90 ! NULL
3 ! 45 ! 65 ! NULL
4 ! 25 ! 90 ! NULL
5 ! NULL
6 ! 25 ! 95 ! NULL
7 ! NULL
Fig. 5. Alternate Representation
is more than max-gap, we \pull up" (1, 2). We search for the occurrence
of (1, 2) after time 15, because end-time((3)) = 45 and max-gap is 30, and so
even if (1, 2) occurs at some time before 15, it still will not satisfy the maxgap
constraint. We (1, 2) at time 50. Since this is the element, we do not
have to check to see if the max-gap constraint between (1, 2) and the element
before that is satised We now move forward. Since (3) no longer occurs more
than 5 days after (1, 2), we search for the next occurrence of (3) after time
We (3) at time 65. Since the max-gap constraint between (3) and (1, 2) is
satised we continue to move forward and (4) at time 90. The maxgap
constraint between (4) and (3) is satised so we are done
Finding a single element: To describe the procedure for the
occurrence of an element in a data sequence, we discuss how to eciently
a single item. A straightforward approach would be to scan consecutive
transactions of the data-sequence until we the item. A faster alternative is
to transform the representation of d as follows
Create an array that has as many elements as the number of items in the
database. For each item in the data-sequence d, store in this array a list of
transaction-times of the transactions of d that contain the item. To the
occurrence of an item after time t, the procedure simply traverses the
list corresponding to the item till it a transaction-time greater than t
Assuming that the dataset has 7 items, Figure 5 shows the tranformed repre
sentation of the data-sequence in Figure 4. This transformation has a onetime
overhead of O(total-number-of-items-in-dataset) over the whole execution (to al
locate and initialize the array), plus an overhead of O(no-of-items-in-d) for each
datasequence
Now, to the occurrence of an element after time t, the algorithm
makes one pass through the items in the element and the transaction
time greater than t for each item. If the dierence between the start-time and
end-time is less than or equal to the window-size, we are done. Otherwise, t is
set to the end-time minus the window-size, and the procedure is repeated

An alternate approach would be to \pull up" previous items as soon as we that
the transaction-time for an item is too high. Such a procedure would be similar to
the algorithm that does the contains test for a sequence
Example Consider the data-sequence shown in Figure 4. Assume windowsize
is set to 7 days, and we have to the occurrence of the element (2,
after time t = 20.We 2 at time 50, and 6 at time 25. Since endtime
start-time((2,6)) > 7, we set t to 43 (= end-time((2,6)) window-size) and try
again. Item 2 remains at time 50, while item 6 is found at time 95. The time gap
is still greater than the window-size, so we set t to 88, and repeat the procedure
We now item 2 at time 90, while item 6 remains at time 95. Since the time
gap between 90 and 95 is less than the window size, we are done
3.3 Taxonomies
The ideas presented in [6] for discovering association rules with taxonomies carry
over to the current problem. The basic approach is to replace each datasequence
d with an \extended-sequence" d0, where each transaction d
i
of d0 contains the
items in the corresponding transaction di of d, as well as all the ancestors of
each item in di. For example, with the taxonomy shown in Figure 1, a data
sequence h (Foundation, Ringworld) (Second Foundation) i would be replaced
with the extended-sequence h (Foundation, Ringworld, Asimov, Niven, Science
Fiction) (Second Foundation, Asimov, Science Fiction) i. We now run GSP on
these extendedsequences
There are two optimizations that improve performance considerably. The
is to pre-compute the ancestors of each item and drop ancestors which are not
in any of the candidates being counted before making a pass over the data. For
instance, if \Ringworld", \Second Foundation" and \Niven" are not in any of the
candidates being counted in the current pass, we would replace the datasequence
h (Foundation, Ringworld) (Second Foundation) i with the extendedsequence
h (Foundation, Asimov, Science Fiction) (Asimov, Science Fiction) i (instead of
the extended-sequence h (Foundation, Ringworld, Asimov, Niven, Science Fic
tion) (Second Foundation, Asimov, Science Fiction) i). The second optimization
is to not count sequential patterns with an element that contains both an item
x and its ancestor y, since the support for that will always be the same as the
support for the sequential pattern without y. (Any transaction that contains x
will also contain y
A related issue is that incorporating taxonomies can result in many redun
dant sequential patterns. For example, let the support of \Asimov" be
the support of \Foundation" 10% and the support of the pattern h Asimov
(Ringworld) i 15%. Given this information, we would \expect" the support of
the pattern h (Foundation) (Ringworld) i to be 7.5%, since half the Asimovs
are \Foundation"s. If the actual support of h (Foundation) (Ringworld) i is close
to 7.5%, the pattern can be considered \redundant". The interest measure intro
duced in [6] also carries over and can be used to prune such redundant patterns
The essential idea is that given a userspecied interest-level I, we display pat
terns that have no ancestors, or patterns whose actual support is at least I times
their expected support (based on the support of their ancestors
4 Performance Evaluation
We compared the performance of GSP to the AprioriAll algorithm given in
[3], using both synthetic and real-life datasets. Due to lack of space, we only
summarize the main results in this section. Details of the experiments, including
performance graphs and detailed explanations of the results, can be found in
Comparison of GSP and AprioriAll. On the synthetic datasets, GSP was
between 30% to 5 times faster than AprioriAll, with the performance gap often
increasing at low levels of minimum support. The results were similar on the
three customer datasets, with GSP running 2 to 20 times faster than AprioriAll
There are two main reasons why GSP does better than AprioriAll
1. GSP counts fewer candidates than AprioriAll
2. AprioriAll has to which frequent itemsets are present in each ele
ment of a data-sequence during the data transformation, and then which
candidate sequences are present in it. This is typically somewhat slower than
directly the candidate sequences
Scaleup. GSP scales linearly with the number of data-sequences. For a constant
database size, the execution time of GSP increases with the number of items in
the data-sequence, but only gradually
Eects of Time Constraints and Sliding Windows. To see the eect of
the sliding window and time constraints on performance, we ran GSP on the
three customer datasets, with and without the min-gap,max-gap, slidingwindow
constraints. The sliding-windowwas set to 1 day, so that the eect on the number
of sequential patterns would be small. Similarly, the max-gap was set to more
than the total time-span of the transactions in the dataset, and the min-gap was
set to 1 day. We found that the min-gap constraint comes for \free"; there was
no performance degradation due to specifying a min-gap constraint. However
there was a performance penalty of 5% to 30% for using the max-gap constraint
or sliding windows
5 Summary
We are given a database of sequences, where each sequence is a list of transac
tions ordered by transaction-time, and each transaction is a set of items. The
problem of mining sequential patterns introduced in [3] is to discover all sequen
tial patterns with a userspecied minimum support, where the support of a
pattern is the number of data-sequences that contain the pattern
We addressed some critical limitations of the earlier work in order to make
sequential patterns useful for real applications. In particular, we generalized the
denition of sequential patterns to admit max-gap and min-gap time constraints
between adjacent elements of a sequential pattern. We also relaxed the restriction
that all the items in an element of a sequential pattern must come from the same
transaction, and allowed a userspecied window-size within which the items can
be present. Finally, if a userdened taxonomy over the items in the database is
available, the sequential patterns may include items across dierent levels of the
taxonomy
We presented GSP, a new algorithm that discovers these generalized sequen
tial patterns. It is a complete algorithm in that it guarantees all rules that
have a userspecied minimumsupport. Empirical evaluation using synthetic and
real-life data indicates that GSP is much faster than the AprioriAll algorithm
presented in [3]. GSP scales linearly with the number of data-sequences, and has
very good scale-up properties with respect to the average data-sequence size
The GSP algorithm has been implemented as part of the Quest data mining
prototype at IBM Research, and is incorporated in the IBM data mining prod
uct. It runs on several platforms, including AIX and MVS DB2/CS and
DB2/MVS. It has also been parallelized for the SP/2 shared-nothing multipro
cessor
References
1. R. Agrawal, T. Imielinski, and A. Swami. Mining association rules between sets of
items in large databases. In Proc. of the ACM SIGMOD Conference on Management
of Data, pages 207{216, Washington, D.C., May
2. R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. In Proc
of the 20th Int'l Conference on Very Large Databases, Santiago, Chile, September

3. R. Agrawal and R. Srikant. Mining Sequential Patterns. In Proc. of the 11th Intl
Conference on Data Engineering, Taipei, Taiwan, March
4. J. Han and Y. Fu. Discovery of multiple-level association rules from large databases
In Proc. of the 21st Int'l Conference on Very Large Databases, Zurich, Switzerland
September
5. H. Mannila, H. Toivonen, and A. I. Verkamo. Discovering frequent episodes in se
quences. In Proc. of the Int'l Conference on Knowledge Discovery in Databases and
Data Mining (KDD-95), Montreal, Canada, August
6. R. Srikant and R. Agrawal. Mining Generalized Association Rules. In Proc. of
the 21st Int'l Conference on Very Large Databases, Zurich, Switzerland, September

7. R. Srikant and R. Agrawal. Mining Sequential Patterns: Generalizations and Per
formance Improvements. Research Report RJ 9994, IBM Almaden Research Center
San Jose, California, December
8. J. T.-L. Wang, G.-W. Chirn, T. G. Marr, B. Shapiro, D. Shasha, and K. Zhang
Combinatorial pattern discovery for scientic data: Some preliminary results. In
Proc. of the ACM SIGMOD Conference on Management of Data, Minneapolis, May


