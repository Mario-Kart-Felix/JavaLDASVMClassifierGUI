TITLE PAGETITLE Clustering with Obstacles for GeographicalData MiningSHORT TITLE Clustering with Obstacles for GeographicalData MiningAUTHORSAFFILIATION 1AFFILIATION 2TOPICS IN SPECIALISSUENew techniques to derive information andknowledge from geospatial data advanceddata mining techniques. New methods andapproaches to spatial analysis of vector data,raster data and hybrid dataKEYWORDS Geographical Data Mining, Clustering, Delaunay Triangulation, ObstaclesTITLE PAGEClustering with Obstacles for Geographical Data MiningClustering with ObstaclesforGeographical Data MiningAbstract100 wordsClustering algorithms typically use the Euclidean distance. However, spatial proximity is dependent on obstacles recorded in other layers. We present a clustering algorithm suitable for largespatial databases with obstacles. The algorithm is free of usersupplied arguments and incorporatesglobal and local variations. The algorithm detects clusters in complex scenarios and successfullysupports association analyzes between layers. All this within On log n  s  t log n expectedtime, where n is the number of points, s is the number of linesegments that determine obstaclesand t is the number of Delaunay edges intersecting obstacles. Performance evaluations illustratethe power of our approach.ABSTRACT PAGE Page 1Clustering with Obstacles for Geographical Data MiningClustering with ObstaclesforGeographical Data Mining1 IntroductionGeographic Information Systems GIS are advancing from mere repositories of georeferenced datato powerful analysis tools for real world phenomena. However, advances in the automation of analysisare behind the advances in the technologies for collecting and manipulating data. Fast data collection processes result in massive spatial databases with many geographical layers that far exceed theexploration capabilities of human analysis 33 37 38. Hence, the recent interest for automatic orsemiautomatic tools to enhance the exploratory and analytical capabilities of GIS under the umbrellaof Geographical Data Mining 37 or Spatial Data Mining Koperski et al.. GIS needs to expand inthe direction of a sophisticated pattern spotter, that suggests highly likely plausible hypotheses, andis capable of handling hundreds of different layers that may contain thousands or millions of datapoints.Traditionally, spatial statistics have been the core of analytical methods to identify spatial patterns, but these traditional approaches are computationally expensive and essentially confirmatory.In addition, they require prior knowledge and domain knowledge 33. While some have indicatedwhy statistical analysis becomes inappropriate or unsuitable for datarich environments 37 38, thisvery much depends on what can be considered an statistical approach, since data mining overlapssignificantly with the aims of statistics for understanding and analyzing data 24.As an alternative, Geographical Data Mining offers approaches like SpatioDominant Generalization, where clustering point data is a frequent and fundamental operation Koperski et al.. Clusteringfor large spatial datasets is so central to data mining in GIS that several algorithms have been developed 9 23 26 35 50 51 54. In SpatioDominant Generalization, clustering works as a summarization or generalization operation on the spatial component of the data. Later, an agent computer orhuman can explore associations between layers 17 14 28 44 45 46 or conduct exploratory dataanalysis. In fact, other layers are explored to attempt to suggest hypotheses that may explain theconcentrations found by the clustering algorithm.The benefit is that unsuspected relationships between the layers are identified discovered as potential candidates for causal relationships perhaps to be submitted to a further confirmatory analysis.For example, when clusters are found in the point data, we expect to be able to suggest explanationsfor them from the interaction of the layers.One crucial interaction between the themes represented in layers is that one theme may work asPage 2Clustering with Obstacles for Geographical Data Miningan obstacle for the entities represented in another theme. For clustering, in particular, it is not onlyrelevant but necessary to consider and explore obstacles as part of the elements that define spatialproximity and spatial heterogeneity. Clearly, clustering point data in one layer, on the assumptionthat the Euclidean distance is the optimal evaluation of proximity when another layer creates barriers,is bound to be unsuccessful. This paper extends a spatial clustering algorithm AUTOCLUST 13into a framework of algorithms that can incorporate the information of other layers as obstacles. Theresult is a generic algorithm that allows the exploration of the effect of the obstacles on the clusteringresults and allows the regulation of the influence of the obstacles. It also makes computationallyefficient such exploration for different levels of effect. Note that a simple but crisp consideration ofthe effect of the obstacles can directly use an algorithm that clusters disregarding the obstacles, anduse the Euclidean distance. This crisp approach later overlays the obstacles to cut separate theclusters. While this approach is one extreme of the possibilities of our algorithm, we suggest here thatthe spatial heterogeneity resulting from measuring distance along paths avoiding obstacles is to beconsidered in the evaluation and formation of clusters. We also make the point that the result willbe more informative.We refer to our algorithms as AUTOCLUST because its fundamental methods are based onAUTOCLUST 15, but they incorporates the consideration of obstacles while preserving crucialproperties in AUTOCLUST. The user does not have to supply argument values. The values of potentialarguments are derived from the analysis of Delaunay Diagrams. For all those directions in which thepossibility of exploration exists because there are modeling assumptions, the algorithm still uses whatwe have found to be very robust defaults and for which several justifications have been supplied inearlier work. The default form of AUTOCLUST suggests quality clusters to further the investigationof relative information. All this is performed within On log n  s  t log n expected time 1, wheren is the number of data points, s is the number of linesegments that determine the obstacles and tis the number of Delaunay edges intersecting at least one obstacle. This is suitable for mining largespatial databases. Moreover, if the set of obstacles changes, the algorithm recomputes the clusteringonly in On  s1 log n expected time where s1 is the number of line segments defining the new set ofobstacles.The remaining sections of the paper are organized as follows. Section 2 presents the context ofclustering within Geographical Data Mining and datarich environments. Section 3 summarizes theworking principle of AUTOCLUST. Section 4 describes its extension for settings with obstacles. Inthis section, we also justify our claims for the time complexity of the algorithm. Section 5 offers anexamination of the performance of AUTOCLUST with an image processing application. A series ofdetailed evaluations with GIS data have been presented for the default form of AUTOCLUST 13.1A function fn is Ogn if Cgn bounds from above fn except on a finite set for a suitable constant C thatis, there is n0 such that fn  Cgn for all n  n0. We use the standard notions of algorithmic complexity 1 forexpectedcase performance.Page 3Clustering with Obstacles for Geographical Data MiningThese evaluations illustrate the power of our approach and confirm the virtues of the proposed method.Section 6 discusses the differences between our approach and COE 47 48.2 The clustering problemClustering a point dataset P  p1, p2, . . . , pn in some study region R can be regarded as findingsmaller homogeneous subgroups according to spatial proximity. Alternatively, any result of a clusteringalgorithm is a hypothesis that explains the data in some way. It is a selection, among a family of models,for that model that best fits the data. All clustering algorithms optimize an induction principle 6that defines the family of models and the criteria by which a model is to be regarded as the onethat best fits the data. Criteria are usually defined in terms of a similarity or dissimilarity measureand if the algorithms proceeds to construct a hierarchical structure that optimizes the criteria or apartition has been repeatedly used to categorize families of clustering methods 22. Clustering is inthe eye of the beholder as much as there are inductive principles 11 52. The algorithms here findboundaries between regions with different density. Thus, a cluster is a region of relatively uniformdensity regardless of the shape of its boundary.Clustering algorithms are applied because they reveal two types of information. First, informationthat we will refer to as indicative, which includes the number of clusters that reside in P , the extent,scatter, size of each cluster, location in spatial dimension, relative proximity among clusters, andrelative density and perhaps location in a temporal dimension. Second, information that we referto as relational, which consists of causal factors, potential explanations, cluster justifications, andassociation to other themes.Naturally there are several dimensions in this, and some algorithms will be more efficient thanothers mainly because they either restrict the models by which they will explain the data or they willsimplify their search space by asking the user to supply some of the indicative or relational informationthat they are to discover. For example, an algorithm like Expectation Maximization EM 7 mayrestrict the models for fitting the data to multivariate normal distributions of common variance, andrequest that the user supply the number of clusters components in the mixture. In fact, restrictingthe models that can fit the data is a form of asking for domain knowledge because the user mustknow or be aware that the true structure of the data is among the family of models the algorithmwill attempt to fit if the data does not fit a mixture of multivariate exponential distributions, EM isunsuitable.Thus, for exploratory analysis, when the agent human or artificial is a data miner we would liketo impose the least assumptions or artificial constraints on the data. We hope this will maximize theexploration and reduce external biases. This principle is based on the philosophy to let the dataspeak for themselves 36.Because of users domain knowledge, the view in the geography research community is that clusPage 4Clustering with Obstacles for Geographical Data MiningFigure 1 Generic schema of SpatioDominant Generalizationters are, at least in part, in the eye of the beholder. Nevertheless for Geographical Data Miningapplications, it is very important that the clustering method detects and suggests as many alternativeclusterings as possible. In traditionally humandriven geographical analysis, clustering is very muchguided by reference to the background layers. For example, epidemiologists judge high densities ofcases in the disease layer as a cluster only with respect to the layer that records the population atrisk. However, this approach requires the user to select and incorporate the background layers. Incontrast, our clustering methods are for exploration by Geographical Data Mining engines that usethe clustering results in SpatioDominant Generalization Koperski et al.. This overcomes issues ofbackground layers and populations at risk. This is specially relevant for datarich environments wherethere are many other layers that could be explored as the possible population at risk.The algorithm here allows this flexibility while maintaining subquadratic time complexity crucialfor the massive datasets of Geographical Data Mining applications. It minimizes the need for usersto explore arguments of the algorithm in order to obtain good results, but allows for the explorationof scenarios in which the influence of obstacles may not be as crisp or as transparent as if they werenot there.Clustering is the starting point of many exploratory analyzes in point data 16. The context ofour first illustration is SpatioDominant Generalization Koperski et al.. In this setting, the agentminer uses clustering to generalize the spatial component of the data Figure 1 step 1 and step 2. Theconcentrations or clusters found are then contrasted with the many other layers integrated into a GISfor discovering associations and therefore suggesting relations for further confirmatory analysis referto Figure 1, step 3. The exploration phase attempts to find autonomously the unexpected relationsbetween the different layers, and thus, many possible associations are tested. Since clustering is used inthis approach to extract point patterns in one layer for association analysis between themes, clusteringresults must be of highquality. Otherwise, spatial associations between themes is undetected. Inparticular, obstacles may be crucial to determining the shape of clusters, and thus, they impactdirectly the opportunity for discovering associations since these are revealed by autonomous analysisPage 5Clustering with Obstacles for Geographical Data Miningwith overlays. A motivation of scenarios for clustering analysis with obstacles can be found in 13.3 The working principle of AUTOCLUSTTwodimensional point clustering is central to spatial analysis processes. Goodchild 21 emphasizedthe importance of spatial analysis and modeling for the application of GIS as a spatial informationscience rather than as a technology. The modeling and structuring of georeferenced data is stronglyrelated to the analysis capability and functionality of GIS 40. Thus, data modeling, data structuringand spatial analysis are interrelated and play major roles for informative mining of spatial data.3.1 Background on AUTOCLUSTModeling topological relations, for instance pi is nearby neighbor of pj, is central for spatial clusters. We expect that the relation pi is in the same spatial cluster as pj has some association to theis nearby neighbor relation. Topological methods, like saying that two objects are neighbors if theyintersect or if they share a common boundary, apply to line or area objects but not directly to pointdata, since points neither intersect nor share boundaries in common unless they coincide. Topologicalinformation is derived for point data through pointtoarea transformations. Thus, two points areregarded as neighbors if their transformed areas share at least a common boundary. A widely adoptedtransformation is to assign every location in the study region R to the nearest pi  P based on a certain metric. This transformation divides R into nonoverlapping regions except for boundaries. Theresulting tessellation is the wellknown Voronoi Diagram. Thus, two points are neighbors if, and onlyif, their corresponding Voronoi regions share a common Voronoi edge. By connecting two neighboringpoints in the Voronoi Diagram, another tessellation, the Delaunay Triangulation, is obtained. Thus,the dual graph explicitly encodes spatial neighborhood information.Being conscious of the interplay between modeling and analysis capability, AUTOCLUST 15uses Voronoi Diagrams and their dual Delaunay Diagrams as the data model and data structure,respectively these remove ambiguities from Delaunay Triangulations when cocircularity2 occurs.The model, Voronoi Diagram, overcomes the limitations of conventional vector and raster models andguarantees unique modeling of discrete point data 2 3 16 15 19 20. The structure, DelaunayDiagram, is succinct and efficient for clustering 3 8 25.Clustering finds sudden global changes in point density. In Delaunay Diagrams points in the borderof clusters tend to have greater standard deviation on the length of their incident edges. This is becauseborder points have both short edges and long edges. The short edges connect points within a clusterwhile the long edges straddle between clusters or between a cluster and noise points. AUTOCLUSTaccounts for local and global variations to ensure that relatively homogeneous clusters are not broken2Four or more points are cocircular if there exists a circle through them.Page 6Clustering with Obstacles for Geographical Data Mininginto tiny uninteresting subsets, and relatively heterogeneous subsets are split into meaningful clusters.This analysis allows AUTOCLUST to be argument free. In the next subsection, we briefly introducehow AUTOCLUST discriminates intracluster edges from intercluster edges.3.2 The ThreePhase clustering in AUTOCLUSTAUTOCLUST proceeds in three phases. Each phase is an edge correction phase. The idea is to startwith the Delaunay Diagram and produce a planar graph that synthesizes the discrete relation pi andpj are in the same cluster with the condition that pi is connected to pj by a path in the planar graph.AUTOCLUST first computes a global variation indicator Mean St DevP  that is the average of thestandard deviations in the length of incident edges for all points pi in the Delaunay Diagram. Theprocess to automatically find cluster boundaries uses this global variation as global information inthe first two phases. Phase I eliminates edges that are evidently too long. In order to detect this,AUTOCLUST computes a local indicator Local Meanpi for each pi. The value of Local Meanpi isthe mean length of edges incident to point pi. The first phase classifies incident edges for each pi intothree groups. Short Edgespi are those edges whose lengths are less than Local Meanpi  Mean St DevP , Long Edgespi are those edges whose lengths are greater than Local Meanpi  Mean St DevP and Other Edgespi  incident edges   Short Edgespi  Long Edgespi.Criteria for these three groups Short Edgespi, Long Edgespi and Other Edgespi are not static,but rather dynamic over the study region, since Local Meanpi varies with pi. This dynamic nature ofAUTOCLUST overcomes the static nature of peakinference clustering methods 8 9 23 25 39 50and the lack of local considerations in partitioning clustering methods. This constitutes the balanceof local and global information. AUTOCLUST obtains rough boundaries of clusters by removinginconsistently long and short edges Long Edgespi and Short Edgespi for all points pi. BecauseLong Edgespi are too long to join points in the same cluster, they are permanently removed. Onthe other hand, Short Edgespi may correspond to links between points within a cluster but alsomay correspond to bridges3 between clusters, or closeby noise points. The removal of Short Edgespienables AUTOCLUST to detect clusters linked by multiple bridges. This is a great improvement overtraditional graphbased clustering methods 53 that use cutpoints to detect bridges.Phase II recuperates edges in Short Edgespi that are intracluster links. It computes connectedcomponents and reconnects isolated singleton connected components connected components havingonly one element to nontrivial connected components connected components having more than one3Here, bridges is used in the graph theoretic sense, and means edges that connect connected components.Page 7Clustering with Obstacles for Geographical Data Miningelement, if and only if they are very close. The reconnection is performed if and only if singletonconnected component pi is very close to to pj specifically, e  pi, pj  Short Edgespi and pj some nontrivial connected component.Phase III extends the notion of neighborhood of a point pi to those points reachable by paths oflength 2 or less. Then, the indicators of local variation are recalculated. The third phase detects andremoves inconsistently long edges for each pi by the use of a new indicator Local Mean 2,Gpi theaverage length of edges in paths of 2 or less edges starting at pi.Thus, the threephase clustering removes all globally long edges and bridges, and reports clustersthat are interconnected by intracluster edges.4 AUTOCLUSTIn a proximity graph, points are linked by edges if and only if they seem to be close by some proximitymeasure 31. In the Euclidean Delaunay Diagramof the point set P also called ordinary DelaunayDiagram, each Delaunay edge represents a relationship that indicates that two endpoints are relativeneighbors. In particular, any circle through these two points of P  is either empty of other pointsfrom P or includes some other points from P . Moreover, any three points of P are mutual neighborsif and only if the circumcircle through them is void of any other points from P . Thus if pi and pjboth in P  have an edge in the Euclidean Delaunay Diagram, they are as close as possible. So closethat you can draw a small circle trough them and exclude any other point point in P . Of course youcan draw a large circle and include other points in P . However, when there is no edge between twopoints in P for the Euclidean Delaunay Diagrams, it means that you can not draw a circle throughthem that does not include somebody else from P . In such a case, they are not neighbors becausethere is always some other point between them. Intuitively, the edges are between closest neighborssince there are no other points in between.Strong interactions short edges indicate that two endpoints belong to the same cluster whileweak ones long edges imply that endpoints belong to distinct clusters. But, in the presence ofobstacles, interactions Delaunay edges and paths are blocked by obstacles. An ordinary Delaunayedge which intersects the defining lines of obstacles no longer suffices to link its endpoints as membersof the same cluster even if it is very short. The criteria to evaluate now depend also on the lengthof edges on the detour path and the transparency associated to the obstacle. We use detour pathfor the shortest path between two points in the Delaunay Diagram where the edges on the pathare ordinary Delaunay edges that avoid all obstacles. This is a path in a graph embedded in theplane. In contrast, the detour distance is the length of the shortest path in the plane between twopoints. Figure 2 illustrates the distinction. In Figure 2a, thick solid lines represent the path whoselength constitutes the detour distance. The detour path for the same pair of points is highlightedin Figure 2b. Dotted edges represent Delaunay edges that traverse an obstacle. Note that, thePage 8Clustering with Obstacles for Geographical Data Mininglines determining the detour distance are not edges in the Delaunay Diagram. The detour distance isapproximated by the length of existing Delaunay edges detour path. A graph embedded in the planeis a spanner if the length of the shorted path on the graph between any pair of nodes approximateswithin a constant the Euclidean distance between those nodes. The length of a detour path in theDelaunay Diagram is a good approximation to the detour distance because the Delaunay Diagram isa spanner 27. Thus, two endpoints of a Delaunay edge traversing some obstacles would not belongto the same cluster if Delaunay edges on a detour path of the Delaunay edge are long.Figure 2 Differentiation between measuring the detour distance and the length of thedetour path a Path that determines the detour distance thick solid lines b Detourpath thick solid lines.4.1 AlgorithmsHere we generalize AUTOCLUST 13 into at least four policies to incorporate the effect of obstacles.Two statistics are major components of AUTOCLUST.1. The local cohesion Local Meanpi evaluates pi as a spatial median4 of it and its neighbors.2. The global degree of variation is measured with Mean St DevP .The four proposed methods correspond to levels of transparency in the obstacles. The participationof the obstacles into the analysis is implemented as degrees of participation or removal into thecomputation of the two statistics or into the construction of the proximity graph. To show thesedegrees of inclusion of obstacles we summarize AUTOCLUST in pseudocode next.Algorithm AUTOCLUSTInput A set P of points and a set B of obstacles4Given a set P  p1, . . . , pn of points, the spatial median of FermatWeber point 29 of P is the point q thatminimizes FW q ni1dq, pi. The smaller the value of FW pi, the better nearly pi approximates the spatialmedian.Page 9Clustering with Obstacles for Geographical Data MiningOutput A graph outG with intracluster edges1 begin2v1 G  BuildGeneralizedDelaunayDiagramP,Bv2 G  BuildDelaunayDiagramP  ProcessObstaclesG, B3 Mean St DevP   ComputeMeanStDevG ProcessObstaclesG, B4 outG  ThreePhaseClusteringG, Mean St DevP  ProcessObstaclesoutG, B5 endIn this template for AUTOCLUST, we have indicated with a left pointing arrow  the possible location of an optional step. We have indicated that the second step has two versions, with aversion identifier. One or the other version exclusively in step 2 is to be used. Now, the step labeledBuildDelaunayDiagram computes an ordinary Delaunay Diagram of the point dataset that it receives as an argument. This step alone simply ignores obstacles. However, BuildGeneralizedDelaunayDiagramreturns a generalized Delaunay Diagram derived from a generalized Voronoi Diagram that considersobstacles, and where distances are computed as detour distances to define Voronoi regions. Thispointtoarea step will be fully aware of crisp obstacles. One or the other but not both will definethe starting proximity graph G.We proceed now to describe and discuss four approaches that result from the proposed template.We will refer to Approach 1 as the instantiation of AUTOCLUST that uses step 2.v1 to determinethe initial proximity graph. We will refer to Approach 2 as the use of step 2.v2 to build the initialproximity graph. The process labeled ComputeMeanStDev calculates the global variation indicatorMean St DevP . This indicator takes as input a proximity graph and uses only the graph informationto determine the points adjacent to each point pi. The length of these links is the Euclidean distance.From here, the standard deviation at each pi is derived and their mean average is the result of thisprocess.The process labeled ThreePhaseClustering procedure represents the threephase clustering ofAUTOCLUST, which also only requires a proximity graph and the calculation of the global informationvalue. The process ProcessObstacles incorporates the obstacles into the template by modifyingat that stage the proximity graph with the removal of all Delaunay edges that traverse an obstacle.The routine ProcessObstacles can be placed in one of three possibilities.1. Between step 2 and step 32. Between step 3 and step 4.Page 10Clustering with Obstacles for Geographical Data Mining1 1a b2 23 34 455Figure 3 Voronoi regions change in the presence of obstacles, but neighborhood information may not change in the constrained Voronoi Diagram.3. Between step 4 and step 5.We will see now that the two options for step 2 and the three insertion points for ProcessObstaclesresult in essentially four approaches to incorporate obstacles.1. Constrained Voronoi Diagram based approach step 2.v1.Although this approach constructs a Voronoi Diagram with obstacles and the resulting generalized Voronoi Diagram is typically different from the ordinary Voronoi Diagram without obstacles,the resulting proximity graph may not be informative. In fact, when obstacles are small withrespect to Delaunay triangles, even if obstacles block straight lines between two points, the proximity graph may be the same. For the construction of the generalized Voronoi Diagram, themetric used evaluates the distance between two points pi and pj as the length of the shortestpath in the plane between pi and pj that does not cut any obstacle. The geometry is determinedby the location, size and every aspect of the obstacles as well as the metric used to measure thelength of segments of the path that does not touch obstacles. Thus, computing the proximitygraph is more laborious. We illustrate this in Figure 3. Figure 3 a shows 5 data points. TheVoronoi regions are indicated by the dark lines, while the Delaunay Diagram is shown in dottedlines. Figure 3 b shows the same configuration but with 3 rectangular obstacles. We do notshow the constrained Voronoi Diagram but the edges of the proximity graph the dotted linesremain although they intersect the obstacles. This is counterintuitive since Point 1 is almostsurrounded by obstacles. However, the point equidistant to Points 1, 2 and 3 highlighted witha  remains equidistant to these points despite the obstacles, and thus in the boundary of theconstrained Voronoi regions. Similarly, there is still a path to get from Point 1 to Point 4, andany position points along this path have as their nearest point, Point 1 or Point 4. So this pathmust go over the boundary of the constrained Voronoi regions for 1 and 4. So, Points 1 and 4remain neighbors.Page 11Clustering with Obstacles for Geographical Data MiningThus, because the proximity graph may be very similar to the ordinary Delaunay Triangulation,AUTOCLUST with step 2.v1 may reduce to the application of the threephase of AUTOCLUST to a Delaunay Diagram which may look like Delaunay Diagram without obstacles. Theadditional complexity in computing the generalized Voronoi Diagram does not pay off. Theinclusion of ProcessObstacles is not effective because the obstacles are not reflected in theproximity graph. Moreover, we will see that the algorithms described below are faster for equivalent or better results.Also, this first approach Approach 1 considers the neighboring information derived directlyfrom the obstacles. This seems to incorporate the influence of obstacles into the proximitymodeling. However, we see that users may be interested in exploring and clustering a targetlayer with respect to various settings. For instance, they may want to inspect a clustering of thetarget layer with rivers as obstacles, later with mountains as obstacles, and further, to test theclusters by overlaying waterways, rivers and lakes as obstacles. This very important exploratoryspatial analysis would be computationally costly because the entire proximity graph is revisedfor each layer used as obstacles. For each obstacle layer, we need to compute the constrainedVoronoi Diagram. Consequently, this approach is not as efficient for whatif analysis. It willnot return results quickly enough for decision making and mining massive spatial databases.Another aspect that is of concern is that the statistic Mean St DevP  is an indication of globaland local effects in the dataset P and not on the obstacles. But, under this approach, it willvary significantly with the type of obstacle considered.2. Edge removal based approach step 2.v2.In this approach, we compute the ordinary Voronoi Diagram without obstacles and derive theordinary Delaunay Diagram. And then, we remove Delaunay edges that intersect obstacles usingProcessObstacles.a Computing Mean St DevP  after overlaying obstacles.ProcessObstacles between step 2 and step 3.This approach removes Delaunay edges first with the edge removal procedure ProcessObstaclesand then calculates the global variation indicator Mean St DevP  after the edge removal.Thus, the obstacles influence the global variation indicator. Finally, we apply the threephase clustering to the modified Delaunay Diagram.Users shall consider some features of this alternative. Here again, the method incorporatesan association between layers when computing Mean St DevP . However, this alternativehas several drawbacks. First, it is still slightly more expensive in terms of computationalefficiency than the two approaches we describe next. This is because here we must computeMean St DevP  for each new set of obstacles. Second, clustering results are sometimes tooPage 12Clustering with Obstacles for Geographical Data Miningsensitive to the types of obstacles. Typically, this approach cannot separate closely locatedclusters, because most obstacles are located between clusters or on sparse regions whereobstacles intersect relatively long Delaunay edges. Therefore this approach computes avalue for Mean St DevP  that is smaller than what is actually needed. As a consequence,fewer edges belong to Short Edgespi and Long Edgespi. Therefore, fewer edges areremoved in Phase I and Phase III of the AUTOCLUST procedure. This eventually causesclosely located clusters to merge into the same cluster.b Computing Mean St DevP  before overlaying obstacles.ProcessObstacles between step 3 and step 4.This approach is the same as Approach 2a except it calculates the global indicator Mean St DevP before removing Delaunay edges intersecting overlaid obstacles. This approach obtains aglobal indicator of variation in P that is not affected by obstacles, but allows obstaclesto change the topological information incident edges. Thus, the inverse local strengthindicator Local Meanpi is affected by obstacles and this new local indicator is used in thethreephase clustering.This alternative is efficient for clustering with different sets of obstacles. Once the DelaunayDiagram is built, removal of Delaunay edges that traverse obstacles requires Os t log n,where s is the number of line segments that determine the obstacles and t is the totalnumber of edges removed from the Delaunay Diagram. Typically, s and t are much smallerthan n unless the set of obstacles is extremely complex. Once we have the DelaunayDiagram, clustering with respect to new obstacles can be obtained in linear time. Thus,this approach is suitable for exploratory data analysis and whatif analysis. Another goodaspect of Approach 2b is that Mean St DevP  is now invariable, regardless of the types ofobstacles considered in clustering. Namely, Mean St DevP  is the same whether obstaclesare present or absent, so this approach is able to automatically detect globally significantwithin the context of R and P  spatial concentrations irrespective of types of obstacles.Thus, we can compare and contrast clustering in the absence of obstacles with clusteringin the presence of obstacles with the same global variation indicator.c Removing Delaunay edges after computing spatial clusters.ProcessObstacles between step 4 and step 5.We compute spatial clusters with total disregard for the obstacles. When the results withobstacles are to be presented, the obstacles are used to cut, divide or separate clusters.Clusters are built with a geometry that is oblivious to obstacles.This last approach is the most efficient computationally if one considers that it is verylikely that the obstacles are actually a different geographical theme and they are stored ina different layer. This approach computes the clusters independently of the obstacles andPage 13Clustering with Obstacles for Geographical Data Miningthus, the computational effort for clustering depends only on the size n of the dataset P .It depends on the complexity of the obstacles only later when an overlay of the clusters andthe obstacles is being performed however, this is very fast because the data in P has beensummarized to the clusters and the complexity of the obstacles is usually much smallerthan n. Thus, exploration with alternative sets of obstacles becomes very fast the pointclustering needs to be computed only once. However, this approach incorporates obstaclesonly as separators of clusters and not as elements that affect the neighboring informationof the point data.Each approach offers valid modeling for settings with obstacles. The user may have applicationspecific information in order to prefer one above the other. For example, is housing distributed alonga rail track, or does the rail track cut through and separate an apparent cluster of houses Thedistinction may be historical. If the rail track was there before many of the houses, because of thepresence of a station and a bridge5 houses may tend to form the cluster located along the track. Ifthe need for a fast train has developed the train line after the cluster of houses, the rail track willeffectively split a cluster.Nevertheless, each decision to model the geometry, and to allow its impact on the topology willresult in slightly different results. With AUTOCLUST, the scenarios of different geometries canall be explored and contrasted. They offer different geometric interpretations and they offer differentcomputational tradeoffs.Table 1 summarizes the differences brought into the two statistics Local Meanpi and Mean St DevP by the four proposed approaches. Approach 1 and Approach 2a have different values of Local Meanp iTable 1 Comparison of the four approaches the status of value in the presence of obstacles.Approach 1 Approach 2a Approach 2b Approach 2cLocal Mean changed changed changed not changedMean St Dev changed changed not changed not changedand Mean St DevP  in the presence of obstacles from those values they hold in the absence of obstacles. Different values of Mean St DevP  for different sets of obstacles imply difficulties in exploratorydata analysis. Also, the computational cost of Approach 1 makes it less attractive. Approach 2b isseen as a hybrid approach of Approach 2a and Approach 2c. We have explored these alternativesand found Approach 2b the most adequate in terms of the indicative information provided as wellas the significantly lower computational cost with respect to Approach 1 and Approach 2a. We believe this approach is the most suitable for exploratory data analysis and whatif analysis. Thus,AUTOCLUST uses this approach as a default method for clustering in the presence of obstacles.5Here, a bridge is a physical structure allowing crossing over railtracks.Page 14Clustering with Obstacles for Geographical Data Mining4.2 Time complexity analysisIn this section, we review in detail the time complexity of Approach 2b. AUTOCLUST requiresOn log n time to construct Delaunay Diagram if Approach 1 is used, this is more costly. Next,computing Mean St DevP  is bounded by twice the number of edges. Thus, this takes linear timethat is On time. Removal of edges that traverse obstacles requires Os  t log n, where s is thenumber of line segments that determine the obstacles and t is the total number of edges removed fromthe Delaunay Diagram. This is because for each linesegment l defining an obstacle, its endpointscan be located within the Delaunay Diagram in Olog n expected time. Then, the planar structureof the Delaunay Diagram can be navigated along the linesegment l collecting edges to remove inOlog n expected time. Our implementation uses LEDA 32. Typically, s is much smaller thann, resulting in On time unless the set of obstacles is extremely complex. Finally, the threephasecleaning process works in linear time 15. Thus, AUTOCLUST in default mode only requiresOn log n  s  t log n time. The time complexity of AUTOCLUST is dominated by computingDelaunay Diagram, and in default mode, this does not depend on the obstacles. This means that oncewe have the Delaunay Diagram, it can be stored, and analysis with clustering can be obtained in Ontime for new sets of obstacles.5 Performance evaluationTo illustrate the contrast in the alternatives of incorporating obstacles into AUTOCLUST, we usea real dataset from Sydney, Australia refer to Figure 4 a. The region of study has extensivewaterways and is bounded by ocean on the East. Several roads cross water via bridges. The pointdataset that is the target of clustering is presented in Figure 4 b. Clustering without obstaclesresults in just one cluster as illustrated in Figure 4 c using kmeans, kmedoids, CLARANS 35,COE 47 48 requires users to provide k. Analysis of separation 4 suggest k  1. Clearly, obstaclesmust be incorporated for the analysis. Unfortunately, if obstacles are incorporated as Approach 1, theconstrained Voronoi Diagram computes distances between the points that are very similar for manycases to the distances that would exist if there were no obstacles. This is due to the pathways overthe rivers via roads and bridges. Thus, once again, the result is just one cluster as in Figure 4 c.The last approach Approach 2c that ignores obstacles even after clusters are formed, results in onlytwo clusters, because the streams going EastWest cut a North section of the data. Approaches thatcompute the ordinary Voronoi Diagram but use obstacles to remove edges of the Delaunay Diagramproduce much better results. These are shown in Figure 4 d. Note that the North section is shownas two clusters one labeled with 4 and the other with   although there is no obstacle between thesetwo sets of points. Nevertheless, these points in isolation from the others which happens because ofthe EastWest rivers do seem more like two clusters. Similarly on the region south of the EastWestPage 15Clustering with Obstacles for Geographical Data Mininga bc dFigure 4 Illustration with real dataset from Sydney, Australia a The region of studyhas wide waterways and is bounded by the ocean. b Clustering is to be performed forpoint features in the presence of obstacles. c If obstacles are ignored, one cluster isobtained involving all points. d If obstacles are considered, 6 clusters are found and 4objects are isolated.rivers. We actually have four clusters labeled with ,  3 and . The geography of the coastlinemakes these relevant subgroups. In the light of the obstacles, we see the NorthWest cluster labeledwith  is indeed relative far from other points, similarly the points in the east peninsula labeled with, as well as the points in the west peninsula labeled with 3. Note that we have removed pointsthat are placed in a cluster by themselves singletons. Because Approach 2b is the more efficientcomputationally, and produces the type of informative results that we observe in this example, itis the default mode for AUTOCLUST. Experimental results reported below are produced withApproach 2b.AUTOCLUST with its default settings Approach 2b produces robust outcomes for complexscenarios with obstacles and noise 13. In particular, AUTOCLUST produced remarkable resultswith the CHAMELEONs benchmark data 26.We present here results in the domain of image segmentation that are interesting for remote sensing scenarios where noisy images prevent the use of edge detection. The application of clusteringalgorithms for large data sets in this context was previously explored in BIRCH 54. We use datafrom an application of objectrecognition based on color in a controlled illumination environment.This is the vision system for the legged league RoboCup competition and associated research symposium 49. Algorithms must be very fast, because of the realtime requirements. Figure 5 illustratesPage 16Clustering with Obstacles for Geographical Data Mininga Original b Edge detectionc Color Segmentation d Red pixels e Black pixelsf Red clusters with no obstacles g Clusters with ObstaclesFigure 5 Object identification in an image by clustering points of the same color.Page 17Clustering with Obstacles for Geographical Data Miningthe applicability of our algorithm where 25 frames a second must be analyzed. Figure 5a showsan image captured by the camera onboard one of these robots. Although in this environment images have little noise with respect to color segmentation, edge detection algorithms have difficulty infinding the patches of the same color that allow the identification of objects. Figure 5b shows thatedges appear on shadows on the ground and produce some incomplete polygon boundaries on colors.The lack of noise can be confirmed by the color segmentation image Figure 5c. Unfortunately, toconvert color classified point data into polygons, clustering algorithms that only produce convex clusters are inapplicable. Moreover, we suggest that the results of clustering say, the red pixels, is muchbetter if we use say, the black pixels, as obstacles. Figure 5d shows the red pixels, while Figure 5eshows the black pixels. Clustering algorithms without obstacles have difficulty identifying differentpolygons. Even our powerful AUTOCLUST, when clustering the red pixels, does not distinguish thepatches in the heads of the robots from the large body, placing all this in only one cluster refer toFigure 5f. Also, the patches in the legs of the closest red dog are merged with the patch for thebody. However, clustering the black pixels first, and using them as obstacles for clustering the redwith AUTOCLUST results in separate head patches as well as a separate leg path on the right, andtwo on the left Figure 5g.6 DiscussionWe now review the advantages of the proposed algorithm with respect to other known recent approaches to clustering georeferenced point data.First, we believe AUTOCLUST better supports the fact that all information is extracted fromthe data. Other commonly used clustering methods impose a number of assumptions and require anumber of usersupplied arguments prior to clustering. The arguments parameters required are asfollows. The number of clusters typically in partition or representativebased clustering. Termination conditions as in hierarchical clustering methods 23 26 54. Values for density thresholds as in densitybased clustering. Values for resolution as in gridbased clustering. The form of underlying probability distributions as in modelbased clustering 5 18 41. Threshold values for lengths of edges as in graphbased clustering.Determining suitable values for these arguments in order for the algorithm to produce its best resultsis usually a difficult task. The need to find bestfit arguments in these semiautomatic clusteringsdemands preprocessing andor several trial and error steps. This is very expensive for massivePage 18Clustering with Obstacles for Geographical Data Miningspatial datasets. These semiautomatic algorithms inherit their argumenttuning to search bestfitarguments when they are modified for clustering in the presence of obstacles. Minimizing the need toadjust arguments not only reduces the risk of user bias polluting exploratory analysis, but promiseshigher user friendliness. Our algorithm AUTOCLUST is able to find these values from the dataautomatically, so that novice users can expect quality clusters in the presence of obstacles. It couldbe argued that the definition of Short Edgespi in Section 3 should introduce parameters 1 and2 to the clustering approach by defining Short Edgespi as those edges whose lengths is less thanLocal Meanpi  1 Mean St DevP , and Long Edgespi as those edges whose lengths are greaterthan Local Meanpi2 Mean St DevP . However, several experiments with even many proximitygraphs have suggested that 1  2  1 is a very robust starting point 30.Second, we believe AUTOCLUST is more adequate for data mining under schemes that findassociations across layers, as in SpatioDominant Generalization. Partitioning algorithms 12 3547 provide limited opportunity for advancing the clustering to association analysis. They producerepresentatives of clusters that can be used to explore associations with other point data layers 17.However, for other themes coded as areas, they are not as effective, since these algorithms produceconvexshaped clusters. Also, these algorithms are very sensitive to large discrepancies in the sizeor the density. In these scenarios, their results are not as good and big clusters pass undetectedsince they are split into many fragments. Thus, these clustering methods fail to detect clusters ofarbitrary shapes and clusters of different sizes. Similarly, densitybased clustering algorithms 939 and gridbased clustering algorithms 43 50 51 do not serve association analysis well becausethey fail to detect clusters of different densities, sparse clusters near highdensity clusters and closelylocated highdensity clusters. Graphbased clustering methods 8 23 25 26 53 are not as robust asAUTOCLUST at handling bridges. AUTOCLUST is robust to noise. It can detect local densityvariations. AUTOCLUST not only detects clusters of arbitrary shapes, but also clusters of differentdensities. Further, it successfully finds sparse clusters near to highdensity clusters, clusters linked bymultiple bridges and closely located highdensity clusters.Third, AUTOCLUST has several direct improvements over COE 47 48 the only method forclustering large georeferenced point data with obstacles. COE is a modification of a partitioning algorithm named CLARANS 35. This embodies randomized heuristics to solve the pmedian problem,under the assumption that the number p of clusters is known the facility location literature uses pfor the number of representatives, but here p  k. Searching for p or k increases the complexity ofthe algorithm by a factor of k and care must be taken not to favor larger p since p  n is an absoluteminimum. The first advantage is that AUTOCLUST uses the Euclidean distance and not the squareof the Euclidean distance. Using the square of the Euclidean distance is typically motivated by considerations like speeding the implementation by a constant factor andor by an inductive principle ofleastsquares as is the case with kmeans 10. However, using squares of the Euclidean distance inthe modeling makes the approach extremely sensitive to outliers andor noise 12 34 42. A secondPage 19Clustering with Obstacles for Geographical Data Miningadvantage of AUTOCLUST over COE is that COE needs to experiment with different values for thenumber of clusters. This is a significant benefit, both in CPUtime and in useranalysis time. As athird advantage, AUTOCLUST operates with all the data and does not require the preprocessing ofCOE to form miniclusters. This construction of miniclusters introduces numerical error and biasesto the grouping. Thus, the quality of the clustering is reduced and the exploratory capability is limitedin COE. Fourth, COE uses a randomized interchange heuristic to approximate the optimum set ofk representatives for the clusters. This heuristics is a very poor hillclimber that can not guaranteeeven local optimality and that sacrifices significantly the quality of the clustering for speed in theoptimization. Much better variants of interchange heuristics are well known from the literature offacility location, and they have been applied to medoidbased clustering 17. Fifth, because of therepresentativebased nature of COE, the algorithm has a bias for convex clusters, and in particularfor spherical clusters. This is probably acceptable when statistical justifications allow the assumptionthat the data has a probabilistic distribution from a mixture model. However, in the presence ofobstacles, this assumption will certainly be very difficult to sustain. AUTOCLUST is not restrictedto convex clusters. Moreover, even without obstacles on the dataset from Figure 4, AUTOCLUSTdetects several edges of the Delaunay Diagram that are too long and although it produces only onecluster, its shape is not convex and the water bays around the dataset are identified.One thing in which AUTOCLUST especially Approach 1, Approach 2a and Approach 2b andCOE are similar is that they are a tightlycoupled approach to including obstacles into the clustering. That is, rather than computing the clustering ignoring the obstacles and then cutting theresulting groups by the obstacles, the obstacles are used to modify parameters the distance or neighboring information when considering the cluster. In the case of COE, there is no real explanation toavoiding the looselycoupled approach Approach 2c, specially since Approach 2c is more efficientand produces similar results to the tightlycoupled approach with COE. On the other hand, in AUTOCLUST, the computational efficiency does not suffer as much across the approaches that involvethe obstacles. However, in AUTOCLUST, we set the default to the tightlycoupled Approach 2bbecause the evaluation of global effects is considered with all Delaunay edges, even those that intersect obstacles, since this is what allows the discovery of density estimates over the region of study.But when analyzing the neighbors of a data point, obstacles are considered. An edge intersecting anobstacle is not an edge that justifies that its endpoints are neighbors.References1 Aho, A., Hopcroft, J., and Ullman, J. 1974. The Design and Analysis of Computer Algorithms.AddisonWesley Publishing Co., Reading, MA.2 Ahuja, N. 1982. Dot Pattern Processing Using Voronoi Neighborhoods. IEEE Transactions onPage 20Clustering with Obstacles for Geographical Data MiningPattern Analysis and Machine Intelligence, PAMI43336343.3 Ahuja, N. and Tuckeryan, M. 1989. Extraction of Early Perceptual Structure in Dot PattenrsIntegrating Region, Boundary, and Component Gestalt. Computer Vision, Graphics, and ImageProcessing, 48304356.4 Bezdek, J. 1981. Pattern Recognition with Fuzzy Objective Function Algorithms. Plenum Press,New York.5 Celeux, G. and Govaret, G. 1995. Gaussian parsimonious clustering models. Pattern Recognition,285781793.6 Cherkassky, V. and Muller, F. 1998. Learning from Data  Concept, Theory and Methods. JohnWiley  Sons, NY, USA.7 Dempster, A.P., Laird, N.M. and Rubin D.B. 1977. Maximum likelihood from incomplete datavia the EM algorithm. Journal of the Royal Statistical Society B, 39138.8 Eldershaw, C. and Hegland, M. 1997. Cluster analysis using triangulation. In Noye, B., Teibner,M., and Gill, A., editors, Proceedings of Computational Techniques with Applications CTAC97,pages 201208. World Scientific Singapore.9 Ester, M., Kriegel, H., Sander, S., and Xu, X. 1996. A densitybased algorithm for discoveringclusters in large spatial databases with noise. In Simoudis, E., Han, J., and Fayyad, U., editors,Proceedings of the 2nd International Conference on Knowledge Discovery and Data Mining KDD96, pages 226231, Menlo Park, CA. AAAI, AAAI Press.10 EstivillCastro, V. 2000. Hybrid genetic algorithms are better for spatial clustering. In Mizoguchi, R. and Slaney, J., editors, Proceedings Sixth Pacific Rim International Conference onArtificial Intelligence PRICAI 2000, pages 424434, Melbourne, Australia. SpringerVerlag LectureNotes in Artificial Intelligence 1886.11 EstivillCastro, V. 2002. Why so many clustering algorithms  a position paper. SIGKDDExplorations, 416575.12 EstivillCastro, V. and Houle, M. 1999. Robust clustering of large georeferenced data sets. InZhong, N. and Zhow, L., editors, Proceedings of the 3rd PacificAsia Conference on Knowledge Discovery and Data Mining PAKDD99, pages 327337. SpringerVerlag Lecture Notes in ArtificialIntelligence 1574.13 EstivillCastro, V. and Lee, I. 2000. AUTOCLUST Automatic clustering of pointdata sets inthe presence of obstacles. In Roddick, J. and Hornsby, K., editors, Proceedings of the InternationalPage 21Clustering with Obstacles for Geographical Data MiningWorkshop on Temporal, Spatial and SpatioTemporal Data Mining  TSDM2000, in conjunction withthe 4th European Conference on Principles and Practices of Knowledge Discovery and Databases,pages 131144, Lyon, France. SpringerVerlag Lecture Notes in Artificial Intelligence 2007.14 EstivillCastro, V. and Lee, I. 2001. Data mining techniques for autonomous exploration oflarge volumes of georeferenced crime data. In Pullar, D., editor, 6th International Conference onGeocomputation, Brisbane. University of Queensland. CDROM, ISBN 1864995637.15 EstivillCastro, V. and Lee, I. 2002a. Argument free clustering for large spatial pointdata sets.Computers, Environment and Urban Systems, 264315334.16 EstivillCastro, V. and Lee, I. 2002b. Multilevel clustering and its visualization for exploratorydata analysis. GeoInformatica, 62123152.17 EstivillCastro, V. and Murray, A. 1998. Discovering associations in spatial data  an efficientmedoid based approach. In Wu, X., Kotagiri, R., and Korb, K., editors, Proceedings of the 2ndPacificAsia Conference on Knowledge Discovery and Data Mining PAKDD98, pages 110121,Melbourne, Australia. SpringerVerlag Lecture Notes in Artificial Intelligence 1394.18 Fraley, C. and Raftery, A. 1998. How many clusters which clustering method answers viamodelbased cluster analysis. Computer Journal, 418578588.19 Gold, C. M. 1991. Problems with handling spatial data  The Voronoi approach. CISM JournalACSGC, 4516580.20 Gold, C. M. 1992. The meaning of Neighbour. In Tinhofer, G. and Schmidt, G., editors,Theories and Methods of SpatioTemporal Reasoning in Geographic Space, pages 220235, Berlin.SpringerVerlag Lecture Notes in Computer Science 639.21 Goodchild, M. 1992. Geographical information science. International Journal of GeographicalInformation Systems, 63145.22 Grabmeier, J. and Rudolph, A. 2002. Techniques of Cluster Algorithms in Data Mining, DataMining and Knowledge Discovery, 64 303360.23 Guha, S., Rastogi, R., and Shim, K. 1998. CURE An efficient clustering algorithm for largedatabases. In Proceedings of ACM SIGMOD International Conference on Management of Data,volume 27, pages 7384, New York. ACM, ACM Press.24 Hastie, T., Tibshirani, R. and Friedman J.H. 2001. The Elements of Statistical Learning DataMining, Inference, and Prediction SpringerVerlag, NY.Page 22Clustering with Obstacles for Geographical Data Mining25 Kang, I.S., Kim, T.W., and Li, K.J. 1997. A spatial data mining method by Delaunaytriangulation. In Proceedings of the Fifth ACM Workshop on Geographic Information Systems, LasVegas, Nevada.26 Karypis, G., Han, E.H., and Kumar, V. 1999. Chameleon Hierarchical clustering using dynamic modeling. IEEE Computer, 3286875. Special Issue on Data Analysis and Mining.27 Keil, M. and Gutwin, G. 1989. The Delaunay triangulation closely approximates the completegraph. In Denhe, F., Sack, J.R., and Snatoro, N., editors, Proceedings of the First Workshop onAlgorithms and Data Structures WADS89, pages 4756. SpringerVerlag Lecture Notes in ComputerScience. 382.28 Knorr, E., Ng, R., and Shilvock, D. 1997. Finding boundary shape matching relations in spatialdata. In School, A. and Voisard, A., editors, Advances in Spatial Databases, 5th InternationalSymposium, SDD97, pages 2946, Berlin, Germany. SpringerVerlag Lecture Notes in ComputerScience 1262.Koperski et al. Koperski, K., Han, J., and Adhikari, J. Mining knowledge in geographical data.Communications of the ACM. to appear.29 Kuhn, H. 1973. A note on Fermats problem. Mathematical Programming, 4198107.30 Lee, I. and EstivillCastro, V. 2001. Effective and Efficient Boundarybased Clustering forThreeDimensional Geoinformation Studies. Proceedings of the The Third International Symposiumon Cooperative Database Systems for Advanced Applications, CODAS April 2324, Beijing, China.8796. Lu, H. and Spaccapietra, S., editors.31 Liotta, G. 1996. Low Degree Algorithm for Computing and Checking Gabriel Graphs. TechnicalReport 9628, Department of Computer Science, Brown University.32 Mehlhorn, K. and Naher, S. 1999. LEDA A platform for combinatorial and geometric computing.Cambridge University Press, UK.33 Miller, H. and Han, J., editors 2001. Geographic Data Mining and Knowledge Discovery. Research Monographs in Geographic Information Systems. Taylor and Francis.34 Murray, A. and EstivillCastro, V. 1998. Cluster discovery techniques for exploratory spatialdata analysis. International Journal of Geographic Information Systems, 125431443.35 Ng, R. and Han, J. 1994. Efficient and effective clustering methods for spatial data mining.In Bocca, J., Jarke, M., and Zaniolo, C., editors, Proceedings of the 20th Conference on VeryLarge Data Bases VLDB, pages 144155, San Francisco, CA Santiago, Chile, Morgan KaufmannPublishers.Page 23Clustering with Obstacles for Geographical Data Mining36 Openshaw, S. 1994. Two exploratory spacetimeattribute pattern analysers relevant to GIS.In Fotheringham, S. and Rogerson, P., editors, Spatial Analysis and GIS, pages 83104, London,UK. Taylor and Francis.37 Openshaw, S. 1999. Geographical data mining key design issues. In Proceedings of 4thGeoComputation 99.38 Openshaw, S. and Alvanides, S. 1999. Applying geocomputation to the analysis of spatialdistributions. In Longley, P. A., Goodchild, M. F., Maguire, D. J., and Rhind, D. W., editors,Geographical Information Systems Principles and Technical Issues, volume 1, pages 267282. JohnWiley  Sons, New York, second edition.39 Openshaw, S., Charlton, M., Wymer, C., and Craft, A. 1987. A Mark 1 geographical analysis machine for the automated analysis of point data sets. International Journal of GeographicalInformation Systems, 14335358.40 Paper, J. F. and Maguire, D. J. 1992. Design models and functionality in GIS. Computers andGeosciences, 184387394.41 Posse, C. 1999. Hierarchical ModelBased Clustering for Large Datasets. Technical Report 363,Department of Statistics, University of Washington.42 Rousseeuw, P. and Leroy, A. 1987. Robust regression and outlier detection. John Wiley  Sons,NY, USA.43 Schikuta, E. and Erhart, M. 1997. The BANGclustering system Gridbased data analysis.In Proceedings of the Second International Symposium IDA97. SpringerVerlag Lecture Notes inComputer Science 1280.44 Shekhar, S. and Huang, Y. 2001. Discovering spatial colocation patterns A summary ofresults. In Jensen, C., Schneider, M., Seeger, B., and Tsotras, V., editors, Proceedings of the 7thInternational Symposium SSTD2001, pages 236256, Redondo beach, CA. SpringerVerlag LectureNotes in Computer Science 2121.45 Son, E.J., Kang, I.S., Kim, T.W., and Li, K.J. 1998. A spatial data mining method byclustering analysis. In Proceedings of the sixth International symposium on Advances in GeographicalInformation Systems, pages 157158. ACMGIS.46 Tsoukatos, I. and Gunopulos, D. 2001. Efficient mining of spatiotemporal patterns. In Jensen,C., Schneider, M., Seeger, B., and Tsotras, V., editors, Proceedings of the 7th International Symposium SSTD2001, pages 425442, Redondo beach, CA. SpringerVerlag Lecture Notes in ComputerScience 2121.Page 24Clustering with Obstacles for Geographical Data Mining47 Tung, A., Hou, J., and Han, J. 2000. COE Clustering with obstacles entities A prelimianrystudy. In Terano, T., Liu, H., and Chen, A., editors, Proceedings of the 4th PacificAsia conferenceon Knowledge Discovery and Data Mining, pages 165168, Kyoto, Japan. SpringerVerlag LectureNotes in Computer Science 1805.48 Tung, A. K. H., Hou, J., and Han, J. 2001. Spatial Clustering in the Presence of Obstacles. InProceedings of the International Conference on Data Engineering, pages 359367.49 Veloso, M., Uther, W., Fujita, M., Asada, M., and Kitano, H. 1998. Playing soccer with leggedrobots. In In Proceedings of IROS98, Intelligent Robots and Systems Conference, Victoria, Canada.50 Wang, W., Yang, J., and Muntz, R. 1997. STING A statistical information grid approach tospatial data mining. In Jarke, M., editor, Proceedings of the 23rd International Conference on VeryLarge Data Bases, pages 186195, Athens, Greece. VLDB, Morgan Kaufmann Publishers.51 Wang, W., Yang, J., and Muntz, R. 1999. STING An approach to active spatial data mining.In Kitsuregawa, M., Maciaszek, L., Papazoglou, K., and Pu, C., editors, Proceedings of the FifteenthInternational Conference on Data Engineering, pages 116125, Los Alamitos, CA. IEEE ComputerSociety. Sydney, Australia.52 Witten, I. and Frank, E. 2000. Data Mining  Practical Machine Learning Tools and Technologies with JAVA implementations. Morgan Kaufmann Publishers, San Mateo, CA.53 Zahn, C. 1971. Graphtheoretical methods for detecting and describing gestalt clusters. IEEETransactions on Computers, 2016886.54 Zhang, T., Ramakrishnan, R., and Livny, M. 1996. BIRCH An efficient data clustering methodfor very large databases. SIGMOD Record, 252103114. Proceedings of the 1996 ACM SIGMODInternational Conference on Management of Data.Page 25
