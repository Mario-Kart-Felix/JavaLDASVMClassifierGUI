A modified version of this technical report will appear in ACM Computing Surveys, September 2009.Anomaly Detection  A SurveyVARUN CHANDOLAUniversity of MinnesotaARINDAM BANERJEEUniversity of MinnesotaandVIPIN KUMARUniversity of MinnesotaAnomaly detection is an important problem that has been researched within diverse research areasand application domains. Many anomaly detection techniques have been specifically developedfor certain application domains, while others are more generic. This survey tries to provide astructured and comprehensive overview of the research on anomaly detection. We have groupedexisting techniques into different categories based on the underlying approach adopted by eachtechnique. For each category we have identified key assumptions, which are used by the techniquesto differentiate between normal and anomalous behavior. When applying a given technique to aparticular domain, these assumptions can be used as guidelines to assess the effectiveness of thetechnique in that domain. For each category, we provide a basic anomaly detection technique, andthen show how the different existing techniques in that category are variants of the basic technique. This template provides an easier and succinct understanding of the techniques belongingto each category. Further, for each category, we identify the advantages and disadvantages of thetechniques in that category. We also provide a discussion on the computational complexity of thetechniques since it is an important issue in real application domains. We hope that this surveywill provide a better understanding of the different directions in which research has been done onthis topic, and how techniques developed in one area can be applied in domains for which theywere not intended to begin with.Categories and Subject Descriptors H.2.8 Database Management Database ApplicationsData MiningGeneral Terms AlgorithmsAdditional Key Words and Phrases Anomaly Detection, Outlier Detection1. INTRODUCTIONAnomaly detection refers to the problem of finding patterns in data that do notconform to expected behavior. These nonconforming patterns are often referred toas anomalies, outliers, discordant observations, exceptions, aberrations, surprises,peculiarities or contaminants in different application domains. Of these, anomaliesand outliers are two terms used most commonly in the context of anomaly detectionsometimes interchangeably. Anomaly detection finds extensive use in a wide varietyof applications such as fraud detection for credit cards, insurance or health care,intrusion detection for cybersecurity, fault detection in safety critical systems, andmilitary surveillance for enemy activities.The importance of anomaly detection is due to the fact that anomalies in datatranslate to significant and often critical actionable information in a wide varietyof application domains. For example, an anomalous traffic pattern in a computerTo Appear in ACM Computing Surveys, 09 2009, Pages 172.2  Chandola, Banerjee and Kumarnetwork could mean that a hacked computer is sending out sensitive data to anunauthorized destination Kumar 2005. An anomalous MRI image may indicatepresence of malignant tumors Spence et al. 2001. Anomalies in credit card transaction data could indicate credit card or identity theft Aleskerov et al. 1997 oranomalous readings from a space craft sensor could signify a fault in some component of the space craft Fujimaki et al. 2005.Detecting outliers or anomalies in data has been studied in the statistics community as early as the 19th century Edgeworth 1887. Over time, a variety of anomalydetection techniques have been developed in several research communities. Many ofthese techniques have been specifically developed for certain application domains,while others are more generic.This survey tries to provide a structured and comprehensive overview of theresearch on anomaly detection. We hope that it facilitates a better understandingof the different directions in which research has been done on this topic, and howtechniques developed in one area can be applied in domains for which they werenot intended to begin with.1.1 What are anomaliesAnomalies are patterns in data that do not conform to a well defined notion ofnormal behavior. Figure 1 illustrates anomalies in a simple 2dimensional data set.The data has two normal regions, N1 and N2, since most observations lie in thesetwo regions. Points that are sufficiently far away from the regions, e.g., points o1and o2, and points in region O3, are anomalies.xyN1N2o1o2O3Fig. 1. A simple example of anomalies in a 2dimensional data set.Anomalies might be induced in the data for a variety of reasons, such as maliciousactivity, e.g., credit card fraud, cyberintrusion, terrorist activity or breakdown of asystem, but all of the reasons have a common characteristic that they are interestingto the analyst. The interestingness or real life relevance of anomalies is a keyfeature of anomaly detection.Anomaly detection is related to, but distinct from noise removal Teng et al.1990 and noise accommodation Rousseeuw and Leroy 1987, both of which dealTo Appear in ACM Computing Surveys, 09 2009.Anomaly Detection  A Survey  3with unwanted noise in the data. Noise can be defined as a phenomenon in datawhich is not of interest to the analyst, but acts as a hindrance to data analysis.Noise removal is driven by the need to remove the unwanted objects before anydata analysis is performed on the data. Noise accommodation refers to immunizinga statistical model estimation against anomalous observations Huber 1974.Another topic related to anomaly detection is novelty detection Markou andSingh 2003a 2003b Saunders and Gero 2000 which aims at detecting previouslyunobserved emergent, novel patterns in the data, e.g., a new topic of discussionin a news group. The distinction between novel patterns and anomalies is thatthe novel patterns are typically incorporated into the normal model after beingdetected.It should be noted that solutions for above mentioned related problems are oftenused for anomaly detection and viceversa, and hence are discussed in this reviewas well.1.2 ChallengesAt an abstract level, an anomaly is defined as a pattern that does not conform toexpected normal behavior. A straightforward anomaly detection approach, therefore, is to define a region representing normal behavior and declare any observationin the data which does not belong to this normal region as an anomaly. But severalfactors make this apparently simple approach very challengingDefining a normal region which encompasses every possible normal behavior isvery difficult. In addition, the boundary between normal and anomalous behavioris often not precise. Thus an anomalous observation which lies close to theboundary can actually be normal, and viceversa.When anomalies are the result of malicious actions, the malicious adversariesoften adapt themselves to make the anomalous observations appear like normal,thereby making the task of defining normal behavior more difficult.In many domains normal behavior keeps evolving and a current notion of normalbehavior might not be sufficiently representative in the future.The exact notion of an anomaly is different for different application domains. Forexample, in the medical domain a small deviation from normal e.g., fluctuationsin body temperature might be an anomaly, while similar deviation in the stockmarket domain e.g., fluctuations in the value of a stock might be considered asnormal. Thus applying a technique developed in one domain to another is notstraightforward.Availability of labeled data for trainingvalidation of models used by anomalydetection techniques is usually a major issue.Often the data contains noise which tends to be similar to the actual anomaliesand hence is difficult to distinguish and remove.Due to the above challenges, the anomaly detection problem, in its most generalform, is not easy to solve. In fact, most of the existing anomaly detection techniquessolve a specific formulation of the problem. The formulation is induced by variousfactors such as nature of the data, availability of labeled data, type of anomalies tobe detected, etc. Often, these factors are determined by the application domain inTo Appear in ACM Computing Surveys, 09 2009.4  Chandola, Banerjee and Kumarwhich the anomalies need to be detected. Researchers have adopted concepts fromdiverse disciplines such as statistics, machine learning, data mining, informationtheory, spectral theory, and have applied them to specific problem formulations.Figure 2 shows the above mentioned key components associated with any anomalydetection technique.Anomaly Detection TechniqueApplication DomainsMedical InformaticsIntrusion Detection. . .FaultDamage DetectionFraud DetectionResearch AreasInformation TheoryMachine LearningSpectral TheoryStatisticsData Mining. . .Problem CharacteristicsLabels Anomaly TypeNature of Data OutputFig. 2. Key components associated with an anomaly detection technique.1.3 Related WorkAnomaly detection has been the topic of a number of surveys and review articles,as well as books. Hodge and Austin 2004 provide an extensive survey of anomalydetection techniques developed in machine learning and statistical domains. Abroad review of anomaly detection techniques for numeric as well as symbolic datais presented by Agyemang et al. 2006. An extensive review of novelty detectiontechniques using neural networks and statistical approaches has been presentedin Markou and Singh 2003a and Markou and Singh 2003b, respectively. Patchaand Park 2007 and Snyder 2001 present a survey of anomaly detection techniquesTo Appear in ACM Computing Surveys, 09 2009.Anomaly Detection  A Survey  5used specifically for cyberintrusion detection. A substantial amount of research onoutlier detection has been done in statistics and has been reviewed in several booksRousseeuw and Leroy 1987 Barnett and Lewis 1994 Hawkins 1980 as well asother survey articles Beckman and Cook 1983 Bakar et al. 2006.Table I shows the set of techniques and application domains covered by our surveyand the various related survey articles mentioned above.1 2 3 4 5 6 7 8TechniquesClassification Based    Clustering Based   Nearest Neighbor Based    Statistical      Information TheoreticSpectralApplicationsCyberIntrusion Detection Fraud DetectionMedical Anomaly DetectionIndustrial Damage DetectionImage ProcessingTextual Anomaly DetectionSensor NetworksTable I. Comparison of our survey to other related survey articles.1  Our survey 2  Hodge andAustin 2004, 3  Agyemang et al. 2006, 4  Markou and Singh 2003a, 5  Markou and Singh2003b, 6  Patcha and Park 2007, 7  Beckman and Cook 1983, 8  Bakar et al 20061.4 Our ContributionsThis survey is an attempt to provide a structured and a broad overview of extensiveresearch on anomaly detection techniques spanning multiple research areas andapplication domains.Most of the existing surveys on anomaly detection either focus on a particularapplication domain or on a single research area. Agyemang et al. 2006 and Hodgeand Austin 2004 are two related works that group anomaly detection into multiplecategories and discuss techniques under each category. This survey builds uponthese two works by significantly expanding the discussion in several directions.We add two more categories of anomaly detection techniques, viz., informationtheoretic and spectral techniques, to the four categories discussed in Agyemanget al. 2006 and Hodge and Austin 2004. For each of the six categories, we notonly discuss the techniques, but also identify unique assumptions regarding thenature of anomalies made by the techniques in that category. These assumptionsare critical for determining when the techniques in that category would be able todetect anomalies, and when they would fail. For each category, we provide a basicanomaly detection technique, and then show how the different existing techniques inthat category are variants of the basic technique. This template provides an easierand succinct understanding of the techniques belonging to each category. Further,for each category we identify the advantages and disadvantages of the techniquesin that category. We also provide a discussion on the computational complexity ofthe techniques since it is an important issue in real application domains.To Appear in ACM Computing Surveys, 09 2009.6  Chandola, Banerjee and KumarWhile some of the existing surveys mention the different applications of anomalydetection, we provide a detailed discussion of the application domains where anomalydetection techniques have been used. For each domain we discuss the notion of ananomaly, the different aspects of the anomaly detection problem, and the challengesfaced by the anomaly detection techniques. We also provide a list of techniquesthat have been applied in each application domain.The existing surveys discuss anomaly detection techniques that detect the simplest form of anomalies. We distinguish the simple anomalies from complex anomalies. The discussion of applications of anomaly detection reveals that for most application domains, the interesting anomalies are complex in nature, while most ofthe algorithmic research has focussed on simple anomalies.1.5 OrganizationThis survey is organized into three parts and its structure closely follows Figure2. In Section 2 we identify the various aspects that determine the formulationof the problem and highlight the richness and complexity associated with anomalydetection. We distinguish simple anomalies from complex anomalies and define twotypes of complex anomalies, viz., contextual and collective anomalies. In Section3 we briefly describe the different application domains where anomaly detectionhas been applied. In subsequent sections we provide a categorization of anomalydetection techniques based on the research area which they belong to. Majorityof the techniques can be categorized into classification based Section 4, nearestneighbor based Section 5, clustering based Section 6, and statistical techniquesSection 7. Some techniques belong to research areas such as information theorySection 8, and spectral theory Section 9. For each category of techniques we alsodiscuss their computational complexity for training and testing phases. In Section10 we discuss various contextual anomaly detection techniques. We discuss variouscollective anomaly detection techniques in Section 11. We present some discussionon the limitations and relative performance of various existing techniques in Section12. Section 13 contains concluding remarks.2. DIFFERENT ASPECTS OF AN ANOMALY DETECTION PROBLEMThis section identifies and discusses the different aspects of anomaly detection. Asmentioned earlier, a specific formulation of the problem is determined by severaldifferent factors such as the nature of the input data, the availability or unavailability of labels as well as the constraints and requirements induced by the applicationdomain. This section brings forth the richness in the problem domain and justifiesthe need for the broad spectrum of anomaly detection techniques.2.1 Nature of Input DataA key aspect of any anomaly detection technique is the nature of the input data.Input is generally a collection of data instances also referred as object, record, point,vector, pattern, event, case, sample, observation, entity Tan et al. 2005, Chapter2 . Each data instance can be described using a set of attributes also referredto as variable, characteristic, feature, field, dimension. The attributes can be ofdifferent types such as binary, categorical or continuous. Each data instance mightconsist of only one attribute univariate or multiple attributes multivariate. InTo Appear in ACM Computing Surveys, 09 2009.Anomaly Detection  A Survey  7the case of multivariate data instances, all attributes might be of same type ormight be a mixture of different data types.The nature of attributes determine the applicability of anomaly detection techniques. For example, for statistical techniques different statistical models have tobe used for continuous and categorical data. Similarly, for nearest neighbor basedtechniques, the nature of attributes would determine the distance measure to beused. Often, instead of the actual data, the pairwise distance between instancesmight be provided in the form of a distance or similarity matrix. In such cases,techniques that require original data instances are not applicable, e.g., many statistical and classification based techniques.Input data can also be categorized based on the relationship present among datainstances Tan et al. 2005. Most of the existing anomaly detection techniques dealwith record data or point data, in which no relationship is assumed among thedata instances.In general, data instances can be related to each other. Some examples aresequence data, spatial data, and graph data. In sequence data, the data instancesare linearly ordered, e.g., timeseries data, genome sequences, protein sequences. Inspatial data, each data instance is related to its neighboring instances, e.g., vehiculartraffic data, ecological data. When the spatial data has a temporal sequentialcomponent it is referred to as spatiotemporal data, e.g., climate data. In graphdata, data instances are represented as vertices in a graph and are connected toother vertices with edges. Later in this section we will discuss situations wheresuch relationship among data instances become relevant for anomaly detection.2.2 Type of AnomalyAn important aspect of an anomaly detection technique is the nature of the desiredanomaly. Anomalies can be classified into following three categories2.2.1 Point Anomalies. If an individual data instance can be considered asanomalous with respect to the rest of data, then the instance is termed as a pointanomaly. This is the simplest type of anomaly and is the focus of majority ofresearch on anomaly detection.For example, in Figure 1, points o1 and o2 as well as points in region O3 lieoutside the boundary of the normal regions, and hence are point anomalies sincethey are different from normal data points.As a real life example, consider credit card fraud detection. Let the data setcorrespond to an individuals credit card transactions. For the sake of simplicity,let us assume that the data is defined using only one feature amount spent. Atransaction for which the amount spent is very high compared to the normal rangeof expenditure for that person will be a point anomaly.2.2.2 Contextual Anomalies. If a data instance is anomalous in a specific context but not otherwise, then it is termed as a contextual anomaly also referredto as conditional anomaly Song et al. 2007.The notion of a context is induced by the structure in the data set and has to bespecified as a part of the problem formulation. Each data instance is defined usingfollowing two sets of attributesTo Appear in ACM Computing Surveys, 09 2009.8  Chandola, Banerjee and Kumar1 Contextual attributes. The contextual attributes are used to determine thecontext or neighborhood for that instance. For example, in spatial data sets,the longitude and latitude of a location are the contextual attributes. In timeseries data, time is a contextual attribute which determines the position of aninstance on the entire sequence.2 Behavioral attributes. The behavioral attributes define the noncontextual characteristics of an instance. For example, in a spatial data set describing theaverage rainfall of the entire world, the amount of rainfall at any location is abehavioral attribute.The anomalous behavior is determined using the values for the behavioral attributeswithin a specific context. A data instance might be a contextual anomaly in a givencontext, but an identical data instance in terms of behavioral attributes couldbe considered normal in a different context. This property is key in identifyingcontextual and behavioral attributes for a contextual anomaly detection technique.Monthly TempTimeMar Jun Sept Dec Mar Jun Sept Dec Mar Jun Sept Dect2t1Fig. 3. Contextual anomaly t2 in a temperature time series. Note that the temperature at timet1 is same as that at time t2 but occurs in a different context and hence is not considered as ananomaly.Contextual anomalies have been most commonly explored in timeseries dataWeigend et al. 1995 Salvador and Chan 2003 and spatial data Kou et al. 2006Shekhar et al. 2001. Figure 3 shows one such example for a temperature time serieswhich shows the monthly temperature of an area over last few years. A temperatureof 35F might be normal during the winter at time t1 at that place, but the samevalue during summer at time t2 would be an anomaly.A similar example can be found in the credit card fraud detection domain. Acontextual attribute in credit card domain can be the time of purchase. Suppose anindividual usually has a weekly shopping bill of 100 except during the Christmasweek, when it reaches 1000. A new purchase of 1000 in a week in July will beconsidered a contextual anomaly, since it does not conform to the normal behaviorof the individual in the context of time even though the same amount spent duringChristmas week will be considered normal.The choice of applying a contextual anomaly detection technique is determined bythe meaningfulness of the contextual anomalies in the target application domain.To Appear in ACM Computing Surveys, 09 2009.Anomaly Detection  A Survey  9Another key factor is the availability of contextual attributes. In several casesdefining a context is straightforward, and hence applying a contextual anomalydetection technique makes sense. In other cases, defining a context is not easy,making it difficult to apply such techniques.2.2.3 Collective Anomalies. If a collection of related data instances is anomalouswith respect to the entire data set, it is termed as a collective anomaly. The individual data instances in a collective anomaly may not be anomalies by themselves,but their occurrence together as a collection is anomalous. Figure 4 illustrates anexample which shows a human electrocardiogram output Goldberger et al. 2000.The highlighted region denotes an anomaly because the same low value exists for anabnormally long time corresponding to an Atrial Premature Contraction. Notethat that low value by itself is not an anomaly.0 500 1000 1500 2000 2500 30007.576.565.554.54Fig. 4. Collective anomaly corresponding to an Atrial Premature Contraction in an human electrocardiogram output.As an another illustrative example, consider a sequence of actions occurring in acomputer as shown below. . . httpweb, bufferoverflow, httpweb, httpweb, smtpmail, ftp, httpweb, ssh, smtpmail, httpweb, ssh, bufferoverflow, ftp, httpweb, ftp, smtpmail,httpweb . . .The highlighted sequence of events bufferoverflow, ssh, ftp correspond to atypical web based attack by a remote machine followed by copying of data from thehost computer to remote destination via ftp. It should be noted that this collectionof events is an anomaly but the individual events are not anomalies when theyoccur in other locations in the sequence.Collective anomalies have been explored for sequence data Forrest et al. 1999Sun et al. 2006, graph data Noble and Cook 2003, and spatial data Shekhar et al.2001.To Appear in ACM Computing Surveys, 09 2009.10  Chandola, Banerjee and KumarIt should be noted that while point anomalies can occur in any data set, collectiveanomalies can occur only in data sets in which data instances are related. Incontrast, occurrence of contextual anomalies depends on the availability of contextattributes in the data. A point anomaly or a collective anomaly can also be acontextual anomaly if analyzed with respect to a context. Thus a point anomalydetection problem or collective anomaly detection problem can be transformed toa contextual anomaly detection problem by incorporating the context information.2.3 Data LabelsThe labels associated with a data instance denote if that instance is normal oranomalous1. It should be noted that obtaining labeled data which is accurate aswell as representative of all types of behaviors, is often prohibitively expensive.Labeling is often done manually by a human expert and hence requires substantialeffort to obtain the labeled training data set. Typically, getting a labeled set ofanomalous data instances which cover all possible type of anomalous behavior ismore difficult than getting labels for normal behavior. Moreover, the anomalousbehavior is often dynamic in nature, e.g., new types of anomalies might arise, forwhich there is no labeled training data. In certain cases, such as air traffic safety,anomalous instances would translate to catastrophic events, and hence will be veryrare.Based on the extent to which the labels are available, anomaly detection techniques can operate in one of the following three modes2.3.1 Supervised anomaly detection. Techniques trained in supervised mode assume the availability of a training data set which has labeled instances for normalas well as anomaly class. Typical approach in such cases is to build a predictivemodel for normal vs. anomaly classes. Any unseen data instance is comparedagainst the model to determine which class it belongs to. There are two major issues that arise in supervised anomaly detection. First, the anomalous instances arefar fewer compared to the normal instances in the training data. Issues that arisedue to imbalanced class distributions have been addressed in the data mining andmachine learning literature Joshi et al. 2001 2002 Chawla et al. 2004 Phua et al.2004 Weiss and Hirsh 1998 Vilalta and Ma 2002. Second, obtaining accurateand representative labels, especially for the anomaly class is usually challenging.A number of techniques have been proposed that inject artificial anomalies in anormal data set to obtain a labeled training data set Theiler and Cai 2003 Abeet al. 2006 Steinwart et al. 2005. Other than these two issues, the supervisedanomaly detection problem is similar to building predictive models. Hence we willnot address this category of techniques in this survey.2.3.2 SemiSupervised anomaly detection. Techniques that operate in a semisupervised mode, assume that the training data has labeled instances for only thenormal class. Since they do not require labels for the anomaly class, they are morewidely applicable than supervised techniques. For example, in space craft faultdetection Fujimaki et al. 2005, an anomaly scenario would signify an accident,which is not easy to model. The typical approach used in such techniques is to1Also referred to as normal and anomalous classes.To Appear in ACM Computing Surveys, 09 2009.Anomaly Detection  A Survey  11build a model for the class corresponding to normal behavior, and use the modelto identify anomalies in the test data.A limited set of anomaly detection techniques exist that assume availability ofonly the anomaly instances for training Dasgupta and Nino 2000 Dasgupta andMajumdar 2002 Forrest et al. 1996. Such techniques are not commonly used,primarily because it is difficult to obtain a training data set which covers everypossible anomalous behavior that can occur in the data.2.3.3 Unsupervised anomaly detection. Techniques that operate in unsupervisedmode do not require training data, and thus are most widely applicable. Thetechniques in this category make the implicit assumption that normal instances arefar more frequent than anomalies in the test data. If this assumption is not truethen such techniques suffer from high false alarm rate.Many semisupervised techniques can be adapted to operate in an unsupervisedmode by using a sample of the unlabeled data set as training data. Such adaptationassumes that the test data contains very few anomalies and the model learnt duringtraining is robust to these few anomalies.2.4 Output of Anomaly DetectionAn important aspect for any anomaly detection technique is the manner in whichthe anomalies are reported. Typically, the outputs produced by anomaly detectiontechniques are one of the following two types2.4.1 Scores. Scoring techniques assign an anomaly score to each instance in thetest data depending on the degree to which that instance is considered an anomaly.Thus the output of such techniques is a ranked list of anomalies. An analyst maychoose to either analyze top few anomalies or use a cutoff threshold to select theanomalies.2.4.2 Labels. Techniques in this category assign a label normal or anomalousto each test instance.Scoring based anomaly detection techniques allow the analyst to use a domainspecific threshold to select the most relevant anomalies. Techniques that providebinary labels to the test instances do not directly allow the analysts to make sucha choice, though this can be controlled indirectly through parameter choices withineach technique.3. APPLICATIONS OF ANOMALY DETECTIONIn this section we discuss several applications of anomaly detection. For each application domain we discuss the following four aspectsThe notion of anomaly.Nature of the data.Challenges associated with detecting anomalies.Existing anomaly detection techniques.To Appear in ACM Computing Surveys, 09 2009.12  Chandola, Banerjee and Kumar3.1 Intrusion DetectionIntrusion detection refers to detection of malicious activity breakins, penetrations,and other forms of computer abuse in a computer related system Phoha 2002.These malicious activities or intrusions are interesting from a computer securityperspective. An intrusion is different from the normal behavior of the system, andhence anomaly detection techniques are applicable in intrusion detection domain.The key challenge for anomaly detection in this domain is the huge volume ofdata. The anomaly detection techniques need to be computationally efficient tohandle these large sized inputs. Moreover the data typically comes in a streamingfashion, thereby requiring online analysis. Another issue which arises because ofthe large sized input is the false alarm rate. Since the data amounts to millions ofdata objects, a few percent of false alarms can make analysis overwhelming for ananalyst. Labeled data corresponding to normal behavior is usually available, whilelabels for intrusions are not. Thus, semisupervised and unsupervised anomalydetection techniques are preferred in this domain.Denning 1987 classifies intrusion detection systems into host based and network based intrusion detection systems.3.1.1 Host Based Intrusion Detection Systems. Such systems also referred toas system call intrusion detection systems deal with operating system call traces.The intrusions are in the form of anomalous subsequences collective anomalies ofthe traces. The anomalous subsequences translate to malicious programs, unauthorized behavior and policy violations. While all traces contain events belongingto the same alphabet, it is the cooccurrence of events which is the key factor indifferentiating between normal and anomalous behavior.The data is sequential in nature and the alphabet consists of individual systemcalls as shown in Figure 5. These calls could be generated by programs Hofmeyret al. 1998 or by users Lane and Brodley 1999. The alphabet is usually large 183system calls for SunOS 4.1x Operating System. Different programs execute thesesystem calls in different sequences. The length of the sequence for each programvaries. Figure 5 illustrates a sample set of operating system call sequences. A keycharacteristic of the data in this domain is that the data can be typically profiled atdifferent levels such as program level or user level. Anomaly detection techniquesopen, read, mmap, mmap, open, read, mmap . . .open, mmap, mmap, read, open, close . . .open, close, open, close, open, mmap, close . . .Fig. 5. A sample data set comprising of three operating system call traces.applied for host based intrusion detection are required to to handle the sequentialnature of data. Moreover, point anomaly detection techniques are not applicablein this domain. The techniques have to either model the sequence data or computesimilarity between sequences. A survey of different techniques used for this problemis presented by Snyder 2001. A comparative evaluation of anomaly detection forhost based intrusion detection presented in Forrest et al. 1996 and Dasgupta andTo Appear in ACM Computing Surveys, 09 2009.Anomaly Detection  A Survey  13Technique Used Section ReferencesStatistical Profilingusing HistogramsSection 7.2.1 Forrest et al 1996 2004 1996 19941999,Hofmeyr et al. 1998 Kosoresow andHofmeyr 1997 Jagadish et al. 1999 Cabreraet al. 2001 Gonzalez and Dasgupta 2003 Dasgupta et al 2000 2002 Ghosh et al 1999a 19981999b Debar et al. 1998 Eskin et al. 2001Marceau 2000 Endler 1998 Lane et al 19991997b 1997aMixture of Models Section 7.1.3 Eskin 2000Neural Networks Section 4.1 Ghosh et al. 1998Support Vector MachinesSection 4.3 Hu et al. 2003 Heller et al. 2003Rulebased Systems Section 4.4 Lee et al1997 1998 2000Table II. Examples of anomaly detection techniques used for host based intrusion detection.Nino 2000. Some anomaly detection techniques used in this domain are shown inTable II.3.1.2 Network Intrusion Detection Systems. These systems deal with detectingintrusions in network data. The intrusions typically occur as anomalous patternspoint anomalies though certain techniques model the data in a sequential fashionand detect anomalous subsequences collective anomalies Gwadera et al. 2005b2004. The primary reason for these anomalies is due to the attacks launched byoutside hackers who want to gain unauthorized access to the network for informationtheft or to disrupt the network. A typical setting is a large network of computerswhich is connected to the rest of the world via the Internet.The data available for intrusion detection systems can be at different levels ofgranularity, e.g., packet level traces, CISCO netflows data, etc. The data hasa temporal aspect associated with it but most of the techniques typically do nothandle the sequential aspect explicitly. The data is high dimensional typically witha mix of categorical as well as continuous attributes.A challenge faced by anomaly detection techniques in this domain is that thenature of anomalies keeps changing over time as the intruders adapt their networkattacks to evade the existing intrusion detection solutions.Some anomaly detection techniques used in this domain are shown in Table III.3.2 Fraud DetectionFraud detection refers to detection of criminal activities occurring in commercialorganizations such as banks, credit card companies, insurance agencies, cell phonecompanies, stock market, etc. The malicious users might be the actual customersof the organization or might be posing as a customer also known as identity theft.The fraud occurs when these users consume the resources provided by the organization in an unauthorized way. The organizations are interested in immediatedetection of such frauds to prevent economic losses.Fawcett and Provost 1999 introduce the term activity monitoring as a generalapproach to fraud detection in these domains. The typical approach of anomalyTo Appear in ACM Computing Surveys, 09 2009.14  Chandola, Banerjee and KumarTechnique Used Section ReferencesStatistical Profilingusing HistogramsSection 7.2.1 NIDES Anderson et al. 1994 Anderson et al. 1995Javitz and Valdes 1991, EMERALD Porras andNeumann 1997, Yamanishi et al 2001 2004, Hoet al. 1999, Kruegel at al 2002 2003, Mahoneyet al 2002 2003 2003 2007, Sargor 1998Parametric Statistical ModelingSection 7.1 Gwadera et al 2005b 2004, Ye and Chen 2001Nonparametric Statistical ModelingSection 7.2.2 Chow and Yeung 2002Bayesian Networks Section 4.2 Siaterlis and Maglaris 2004, Sebyala et al. 2002,Valdes and Skinner 2000, Bronstein et al. 2001Neural Networks Section 4.1 HIDE Zhang et al. 2001, NSOM Labib and Vemuri 2002, Smith et al. 2002, Hawkins et al.2002, Kruegel et al. 2003, Manikopoulos and Papavassiliou 2002, Ramadas et al. 2003Support Vector MachinesSection 4.3 Eskin et al. 2002Rulebased Systems Section 4.4 ADAM Barbara et al. 2001a Barbara et al. 2003Barbara et al. 2001b, Fan et al. 2001, Helmeret al. 1998, Qin and Hwang 2004, Salvador andChan 2003, Otey et al. 2003Clustering Based Section 6 ADMIT Sequeira and Zaki 2002, Eskin et al.2002, Wu and Zhang 2003, Otey et al. 2003Nearest NeighborbasedSection 5 MINDS Ertoz et al. 2004 Chandola et al. 2006,Eskin et al. 2002Spectral Section 9 Shyu et al. 2003, Lakhina et al. 2005, Thottanand Ji 2003,Sun et al. 2007Information TheoreticSection 8 Lee and Xiang 2001,Noble and Cook 2003Table III. Examples of anomaly detection techniques used for network intrusion detection.Technique Used Section ReferencesNeural Networks Section 4.1 CARDWATCH Aleskerov et al. 1997, Ghosh andReilly 1994,Brause et al. 1999,Dorronsoro et al.1997Rulebased Systems Section 4.4 Brause et al. 1999Clustering Section 6 Bolton and Hand 1999Table IV. Examples of anomaly detection techniques used for credit card fraud detection.detection techniques is to maintain a usage profile for each customer and monitorthe profiles to detect any deviations. Some of the specific applications of frauddetection are discussed below.3.2.1 Credit Card Fraud Detection. In this domain, anomaly detection techniques are applied to detect fraudulent credit card applications or fraudulent creditcard usage associated with credit card thefts. Detecting fraudulent credit cardapplications is similar to detecting insurance fraud Ghosh and Reilly 1994.To Appear in ACM Computing Surveys, 09 2009.Anomaly Detection  A Survey  15Technique Used Section ReferencesStatistical Profilingusing HistogramsSection 7.2.1 Fawcett and Provost 1999,Cox et al. 1997Parametric Statistical ModelingSection 7.1 Agarwal 2005,Scott 2001Neural Networks Section 4.1 Barson et al. 1996,Taniguchi et al. 1998Rulebased Systems Section 4.4 Phua et al. 2004,Taniguchi et al. 1998Table V. Examples of anomaly detection techniques used for mobile phone fraud detection.The data typically comprises of records defined over several dimensions such asthe user ID, amount spent, time between consecutive card usage, etc. The fraudsare typically reflected in transactional records point anomalies and correspond tohigh payments, purchase of items never purchased by the user before, high rate ofpurchase, etc. The credit companies have complete data available and also havelabeled records. Moreover, the data falls into distinct profiles based on the creditcard user. Hence profiling and clustering based techniques are typically used in thisdomain.The challenge associated with detecting unauthorized credit card usage is that itrequires online detection of fraud as soon as the fraudulent transaction takes place.Anomaly detection techniques have been applied in two different ways to addressthis problem. The first one is known as byowner in which each credit card useris profiled based on hisher credit card usage history. Any new transaction iscompared to the users profile and flagged as an anomaly if it does not match theprofile. This approach is typically expensive since it requires querying a centraldata repository, every time a user makes a transaction. Another approach knownas byoperation detects anomalies from among transactions taking place at a specificgeographic location. Both byuser and byoperation techniques detect contextualanomalies. In the first case the context is a user, while in the second case thecontext is the geographic location.Some anomaly detection techniques used in this domain are listed in Table IV.3.2.2 Mobile Phone Fraud Detection. Mobilecellular fraud detection is a typical activity monitoring problem. The task is to scan a large set of accounts,examining the calling behavior of each, and to issue an alarm when an accountappears to have been misused.Calling activity may be represented in various ways, but is usually describedwith call records. Each call record is a vector of features, both continuous e.g.,CALLDURATION and discrete e.g., CALLINGCITY. However, there is noinherent primitive representation in this domain. Calls can be aggregated by time,for example into callhours or calldays or user or area depending on the granularitydesired. The anomalies correspond to high volume of calls or calls made to unlikelydestinations.Some techniques applied to cell phone fraud detection are listed in Table V.3.2.3 Insurance Claim Fraud Detection. An important problem in the propertycasualty insurance industry is claims fraud, e.g. automobile insurance fraud. Individuals and conspiratorial rings of claimants and providers manipulate the claimTo Appear in ACM Computing Surveys, 09 2009.16  Chandola, Banerjee and Kumarprocessing system for unauthorized and illegal claims. Detection of such fraud hasbeen very important for the associated companies to avoid financial losses.The available data in this domain are the documents submitted by the claimants.The techniques extract different features both categorical as well as continuousfrom these documents. Typically, claim adjusters and investigators assess theseclaims for frauds. These manually investigated cases are used as labeled instancesby supervised and semisupervised techniques for insurance fraud detection.Insurance claim fraud detection is quite often handled as a generic activity monitoring problem Fawcett and Provost 1999. Neural network based techniques havealso been applied to identify anomalous insurance claims He et al. 2003 Brockettet al. 1998.3.2.4 Insider Trading Detection. Another recent application of anomaly detection techniques has been in early detection of Insider Trading. Insider trading is aphenomenon found in stock markets, where people make illegal profits by acting onor leaking inside information before the information is made public. The insideinformation can be of different forms Donoho 2004. It could refer to the knowledgeof a pending mergeracquisition, a terrorist attack affecting a particular industry, apending legislation affecting a particular industry or any information which wouldaffect the stock prices in a particular industry. Insider trading can be detected byidentifying anomalous trading activities in the market.The available data is from several heterogenous sources such as option tradingdata, stock trading data, news. The data has temporal associations since the data iscollected continuously. The temporal and streaming nature has also been exploitedin certain techniques Aggarwal 2005.Anomaly detection techniques in this domain are required to detect fraud inan online manner and as early as possible, to prevent peopleorganizations frommaking illegal profits.Some anomaly detection techniques used in this domain are listed in Table VI.Technique Used Section ReferencesStatistical Profilingusing HistogramsSection 7.2.1 Donoho 2004,Aggarwal 2005Information TheoreticSection 8 Arning et al. 1996Table VI. Examples of different anomaly detection techniques used for insider trading detection.3.3 Medical and Public Health Anomaly DetectionAnomaly detection in the medical and public health domains typically work with patient records. The data can have anomalies due to several reasons such as abnormalpatient condition or instrumentation errors or recording errors. Several techniqueshave also focussed on detecting disease outbreaks in a specific area Wong et al.2003. Thus the anomaly detection is a very critical problem in this domain andrequires high degree of accuracy.The data typically consists of records which may have several different typesof features such as patient age, blood group, weight. The data might also haveTo Appear in ACM Computing Surveys, 09 2009.Anomaly Detection  A Survey  17Technique Used Section ReferencesParametric Statistical ModelingSection 7.1 Horn et al. 2001,Laurikkala et al. 2000,Solbergand Lahti 2005,Roberts 2002,Suzuki et al. 2003Neural Networks Section 4.1 Campbell and Bennett 2001Bayesian Networks Section 4.2 Wong et al. 2003Rulebased Systems Section 4.4 Aggarwal 2005Nearest Neighborbased TechniquesSection 5 Lin et al. 2005Table VII. Examples of different anomaly detection techniques used in medical and public healthdomain.temporal as well as spatial aspect to it. Most of the current anomaly detectiontechniques in this domain aim at detecting anomalous records point anomalies.Typically the labeled data belongs to the healthy patients, hence most of the techniques adopt semisupervised approach. Another form of data handled by anomalydetection techniques in this domain is time series data, such as ElectrocardiogramsECG Figure 4 and Electroencephalograms EEG. Collective anomaly detectiontechniques have been applied to detect anomalies in such data Lin et al. 2005.The most challenging aspect of the anomaly detection problem in this domain isthat the cost of classifying an anomaly as normal can be very high.Some anomaly detection techniques used in this domain are listed in Table VII.3.4 Industrial Damage DetectionIndustrial units suffer damage due to continuous usage and the normal wear andtear. Such damages need to be detected early to prevent further escalation andlosses. The data in this domain is usually referred to as sensor data because itis recorded using different sensors and collected for analysis. Anomaly detectiontechniques have been extensively applied in this domain to detect such damages.Industrial damage detection can be further classified into two domains, one whichdeals with defects in mechanical components such as motors, engines, etc., and theother which deals with defects in physical structures. The former domain is alsoreferred to as system health management.3.4.1 Fault Detection in Mechanical Units. The anomaly detection techniquesin this domain monitor the performance of industrial components such as motors,turbines, oil flow in pipelines or other mechanical components and detect defectswhich might occur due to wear and tear or other unforseen circumstances.The data in this domain has typically a temporal aspect and timeseries analysisis also used in some techniques Keogh et al. 2002 Keogh et al. 2006 Basu andMeckesheimer 2007. The anomalies occur mostly because of an observation in aspecific context contextual anomalies or as an anomalous sequence of observationscollective anomalies.Typically, normal data pertaining to components without defects is readilyavailable and hence semisupervised techniques are applicable. Anomalies are required to be detected in an online fashion as preventive measures are required tobe taken as soon as an anomaly occurs.Some anomaly detection techniques used in this domain are listed in Table VIII.To Appear in ACM Computing Surveys, 09 2009.18  Chandola, Banerjee and KumarTechnique Used Section ReferencesParametric Statistical ModelingSection 7.1 Guttormsson et al. 1999, Keogh et al 1997 20022006Nonparametric Statistical ModelingSection 7.2.2 Desforges et al. 1998Neural Networks Section 4.1 Bishop 1994, Campbell and Bennett 2001, Diazand Hollmen 2002, Harris 1993, Jakubek andStrasser 2002, King et al. 2002, Li et al. 2002,Petsche et al. 1996, Streifel et al. 1996, Whitehead and Hoyt 1993Spectral Section 9 Parra et al. 1996,Fujimaki et al. 2005Rule Based Systems Section 4.4 Yairi et al. 2001Table VIII. Examples of anomaly detection techniques used for fault detection in mechanical units.Technique Used Section ReferencesStatistical Profilingusing HistogramsSection 7.2.1 Manson 2002,Manson et al. 2001,Manson et al.2000Parametric Statistical ModelingSection 7.1 Ruotolo and Surace 1997Mixture of Models Section 7.1.3 Hickinbotham et al 2000a 2000b,Hollier andAustin 2002Neural Networks Section 4.1 Brotherton et al 1998 2001, Nairac et al 19991997, Surace et al 1998 1997, Sohn et al. 2001,Worden 1997Table IX. Examples of anomaly detection techniques used for structural damage detection.3.4.2 Structural Defect Detection. Structural defect and damage detection techniques detect structural anomalies in structures, e.g., cracks in beams, strains inairframes.The data collected in this domain has a temporal aspect. The anomaly detectiontechniques are similar to novelty detection or change point detection techniquessince they try to detect change in the data collected from a structure. The normaldata and hence the models learnt are typically static over time. The data mighthave spatial correlations.Some anomaly detection techniques used in this domain are listed in Table IX.3.5 Image ProcessingAnomaly detection techniques dealing with images are either interested in anychanges in an image over time motion detection or in regions which appear abnormal on the static image. This domain includes satellite imagery Augusteijn andFolkert 2002 Byers and Raftery 1998 Moya et al. 1993 Torr and Murray 1993Theiler and Cai 2003, digit recognition Cun et al. 1990, spectroscopy Chen et al.2005 Davy and Godsill 2002 Hazel 2000 Scarth et al. 1995, mammographic imageanalysis Spence et al. 2001 Tarassenko 1995, and video surveillance Diehl andHampshire 2002 Singh and Markou 2004 Pokrajac et al. 2007. The anomalies arecaused by motion or insertion of foreign object or instrumentation errors. The datahas spatial as well as temporal characteristics. Each data point has a few continuTo Appear in ACM Computing Surveys, 09 2009.Anomaly Detection  A Survey  19Technique Used Section ReferencesMixture of Models Section 7.1.3 Byers and Raftery 1998,Spence et al.2001,Tarassenko 1995Regression Section 7.1.2 Chen et al. 2005, Torr and Murray 1993Bayesian Networks Section 4.2 Diehl and Hampshire 2002Support Vector MachinesSection 4.3 Davy and Godsill 2002,Song et al. 2002Neural Networks Section 4.1 Augusteijn and Folkert 2002,Cun et al.1990,Hazel 2000,Moya et al. 1993,Singhand Markou 2004Clustering Section 6 Scarth et al. 1995Nearest Neighborbased TechniquesSection 5 Pokrajac et al. 2007,Byers and Raftery 1998Table X. Examples of anomaly detection techniques used in image processing domain.Technique Used Section ReferencesMixture of Models Section 7.1.3 Baker et al. 1999Statistical Profilingusing HistogramsSection 7.2.1 Fawcett and Provost 1999Support Vector MachinesSection 4.3 Manevitz and Yousef 2002Neural Networks Section 4.1 Manevitz and Yousef 2000Clustering Based Section 6 Allan et al. 1998,Srivastava and ZaneUlman2005,Srivastava 2006Table XI. Examples of anomaly detection techniques used for anomalous topic detection in textdata.ous attributes such as color, lightness, texture, etc. The interesting anomalies areeither anomalous points or regions in the images point and contextual anomalies.One of the key challenges in this domain is the large size of the input. Whendealing with video data, online anomaly detection techniques are required.Some anomaly detection techniques used in this domain are listed in Table X.3.6 Anomaly Detection in Text DataAnomaly detection techniques in this domain primarily detect novel topics or eventsor news stories in a collection of documents or news articles. The anomalies arecaused due to a new interesting event or an anomalous topic.The data in this domain is typically high dimensional and very sparse. The dataalso has a temporal aspect since the documents are collected over time.A challenge for anomaly detection techniques in this domain is to handle thelarge variations in documents belonging to one category or topic.Some anomaly detection techniques used in this domain are listed in Table XI.3.7 Sensor NetworksSensor networks have lately become an important topic of research more fromthe data analysis perspective, since the sensor data collected from various wirelesssensors has several unique characteristics. Anomalies in data collected from a sensorTo Appear in ACM Computing Surveys, 09 2009.20  Chandola, Banerjee and KumarTechnique Used Section ReferencesBayesian Networks Section 4.2 Janakiram et al. 2006Rulebased Systems Section 4.4 Branch et al. 2006Parametric Statistical ModelingSection 7.1 Phuong et al. 2006, Du et al. 2006Nearest Neighborbased TechniquesSection 5 Subramaniam et al. 2006, Kejia Zhang and Li2007, Ide et al. 2007Spectral Section 9 Chatzigiannakis et al. 2006Table XII. Examples of anomaly detection techniques used for anomaly detection in sensor networks.network can either mean that one or more sensors are faulty, or they are detectingevents such as intrusions that are interesting for analysts. Thus anomaly detectionin sensor networks can capture sensor fault detection or intrusion detection or both.A single sensor network might comprise of sensors that collect different types ofdata, such as binary, discrete, continuous, audio, video, etc. The data is generatedin a streaming mode. Often times the environment in which the various sensors aredeployed, as well as the communication channel, induces noise and missing valuesin the collected data.Anomaly detection in sensor networks poses a set of unique challenges. Theanomaly detection techniques are required to operate in an online approach. Dueto severe resource constraints, the anomaly detection techniques need to be lightweight. Another challenge is that data is collected in a distributed fashion, andhence a distributed data mining approach is required to analyze the data Chatzigiannakis et al. 2006. Moreover, the presence of noise in the data collected from thesensor makes anomaly detection more challenging, since it has to now distinguishbetween interesting anomalies and unwanted noisemissing values.Table XII lists some anomaly detection techniques used in this domain.3.8 Other DomainsAnomaly detection has also been applied to several other domains such as speechrecognition Albrecht et al. 2000 Emamian et al. 2000, novelty detection in robotbehavior Crook and Hayes 2001 Crook et al. 2002 Marsland et al. 1999 2000b2000a, traffic monitoring Shekhar et al. 2001, click through protection Ihler et al.2006, detecting faults in web applications Ide and Kashima 2004 Sun et al. 2005,detecting anomalies in biological data Kadota et al. 2003 Sun et al. 2006 Gwaderaet al. 2005a MacDonald and Ghosh 2007 Tomlins et al. 2005 Tibshirani and Hastie2007, detecting anomalies in census data Lu et al. 2003, detecting associationsamong criminal activities Lin and Brown 2003, detecting anomalies in CustomerRelationship Management CRM data He et al. 2004b, detecting anomalies inastronomical data Dutta et al. 2007 Escalante 2005 Protopapas et al. 2006 anddetecting ecosystem disturbances Blender et al. 1997 Kou et al. 2006 Sun andChawla 2004.4. CLASSIFICATION BASED ANOMALY DETECTION TECHNIQUESClassification Tan et al. 2005 Duda et al. 2000 is used to learn a model classifierfrom a set of labeled data instances training and then, classify a test instance intoTo Appear in ACM Computing Surveys, 09 2009.Anomaly Detection  A Survey  21one of the classes using the learnt model testing. Classification based anomalydetection techniques operate in a similar twophase fashion. The training phaselearns a classifier using the available labeled training data. The testing phaseclassifies a test instance as normal or anomalous using the classifier.Classification based anomaly detection techniques operate under the followinggeneral assumptionAssumption A classifier that can distinguish between normal and anomalousclasses can be learnt in the given feature space.Based on the labels available for training phase, classification based anomalydetection techniques can be grouped into two broad categories multiclass andoneclass anomaly detection techniques.Multiclass classification based anomaly detection techniques assume that thetraining data contains labeled instances belonging to multiple normal classes Stefano et al. 2000 Barbara et al. 2001b. Such anomaly detection techniques learna classifier to distinguish between each normal class against the rest of the classes.See Figure 6a for illustration. A test instance is considered anomalous if its notclassified as normal by any of the classifiers. Some techniques in this subcategoryassociate a confidence score with the prediction made by the classifier. If none ofthe classifiers are confident in classifying the test instance as normal, the instanceis declared to be anomalous.Oneclass classification based anomaly detection techniques assume that all training instances have only one class label. Such techniques learn a discriminativeboundary around the normal instances using a oneclass classification algorithm,e.g., oneclass SVMs Scholkopf et al. 2001, oneclass Kernel Fisher DiscriminantsRoth 2004 2006, as shown in Figure 6b. Any test instance that does not fallwithin the learnt boundary is declared as anomalous.Normal Class 3Normal Class 2Normal Class 1AnomaliesMulticlass Classifiera Multiclass Anomaly DetectionAnomaliesOneclass ClassifierNormal Instancesb Oneclass Anomaly DetectionFig. 6. Using classification for anomaly detection.In the following subsections, we discuss a variety of anomaly detection techniquesthat use different classification algorithms to build classifiers4.1 Neural Networks BasedNeural networks have been applied to anomaly detection in multiclass as well asoneclass setting.To Appear in ACM Computing Surveys, 09 2009.22  Chandola, Banerjee and KumarNeural Network Used ReferencesMulti Layered Perceptrons Augusteijn and Folkert 2002 Cun et al. 1990 Sykacek1997 Ghosh et al. 1999a Ghosh et al. 1998 Barsonet al. 1996 He et al. 1997 Nairac et al. 1997 Hickinbotham and Austin 2000b Vasconcelos et al. 19951994Neural Trees Martinez 1998Autoassociative Networks Aeyels 1991 Byungho and Sungzoon 1999 Japkowicz et al. 1995 Hawkins et al. 2002 Ko and Jacyna2000 Manevitz and Yousef 2000 Petsche et al. 1996Sohn et al. 2001 Song et al. 2001 Streifel et al. 1996Thompson et al. 2002 Worden 1997 Williams et al.2002 Diaz and Hollmen 2002Adaptive Resonance Theory Based Moya et al. 1993 Dasgupta and Nino 2000 Caudelland Newman 1993Radial Basis Function Based Albrecht et al. 2000 Bishop 1994 Brotherton et al.1998 Brotherton and Johnson 2001 Li et al. 2002Nairac et al. 1999 Nairac et al. 1997 Ghosh and Reilly1994 Jakubek and Strasser 2002Hopfield Networks Jagota 1991 Crook and Hayes 2001 Crook et al. 2002Addison et al. 1999 Murray 2001Oscillatory Networks Ho and Rouat 1997 1998 Kojima and Ito 1999Borisyuk et al. 2000 Martinelli and Perfetti 1994Table XIII. Some examples of classification based anomaly detection techniques using neuralnetworks.A basic multiclass anomaly detection technique using neural networks operatesin two steps. First, a neural network is trained on the normal training data to learnthe different normal classes. Second, each test instance is provided as an inputto the neural network. If the network accepts the test input, it is normal and ifthe network rejects a test input, it is an anomaly Stefano et al. 2000 Odin andAddison 2000. Several variants of the basic neural network technique have beenproposed that use different types of neural networks, as summarized in Table XIII.Replicator Neural Networks have been used for oneclass anomaly detectionHawkinset al. 2002 Williams et al. 2002. A multilayer feed forward neural network is constructed that has the same number of input and output neurons corresponding tothe features in the data. The training involves compressing data into three hiddenlayers. The testing phase involves reconstructing each data instance xi using thelearnt network to obtain the reconstructed output oi. The reconstruction error ifor the test instance xi is then computed asi 1nnj1xij  oij2where n is the number of features over which the data is defined. The reconstructionerror i is directly used as an anomaly score for the test instance.4.2 Bayesian Networks BasedBayesian networks has been used for anomaly detection in the multiclass setting.A basic technique for a univariate categorical data set using a nave Bayesiannetwork estimates the posterior probability of observing a class label from a setTo Appear in ACM Computing Surveys, 09 2009.Anomaly Detection  A Survey  23of normal class labels and the anomaly class label, given a test data instance.The class label with largest posterior is chosen as the predicted class for the giventest instance. The likelihood of observing the test instance given a class, and theprior on the class probabilities, are estimated from the training data set. Thezero probabilities, especially for the anomaly class, are smoothed using LaplaceSmoothing.The basic technique can be generalized to multivariate categorical data set byaggregating the perattribute posterior probabilities for each test instance and usingthe aggregated value to assign a class label to the test instance.Several variants of the basic technique has been proposed for network intrusiondetection Barbara et al. 2001b Sebyala et al. 2002 Valdes and Skinner 2000Mingming 2000 Bronstein et al. 2001, for novelty detection in video surveillanceDiehl and Hampshire 2002, for anomaly detection in text data Baker et al. 1999,and for disease outbreak detection Wong et al. 2002 2003.The basic technique described above assumes independence between the different attributes. Several variations of the basic technique have been proposed thatcapture the conditional dependencies between the different attributes using morecomplex Bayesian networks Siaterlis and Maglaris 2004 Janakiram et al. 2006Das and Schneider 2007.4.3 Support Vector Machines BasedSupport Vector Machines SVMs Vapnik 1995 have been applied to anomalydetection in the oneclass setting. Such techniques use one class learning techniquesfor SVM Ratsch et al. 2002 and learn a region that contains the training datainstances a boundary. Kernels, such as radial basis function RBF kernel, canbe used to learn complex regions. For each test instance, the basic techniquedetermines if the test instance falls within the learnt region. If a test instance fallswithin the learnt region, it is declared as normal, else it is declared as anomalous.Variants of the basic technique have been proposed for anomaly detection inaudio signal data Davy and Godsill 2002, novelty detection in power generationplants King et al. 2002 and system call intrusion detection Eskin et al. 2002Heller et al. 2003 Lazarevic et al. 2003. The basic technique also been extendedto detect anomalies in temporal sequences Ma and Perkins 2003a 2003b.A variant of the basic technique Tax and Duin 1999a 1999b Tax 2001 findsthe smallest hypersphere in the kernel space, which contains all training instances,and then determines which side of that hypersphere does a test instance lie. If atest instance lies outside the hypersphere, it is declared to be anomalous.Song et al. 2002 use Robust Support Vector Machines RSVM which are robustto the presence of anomalies in the training data. RSVM have been applied tosystem call intrusion detection Hu et al. 2003.4.4 Rule BasedRule based anomaly detection techniques learn rules that capture the normal behavior of a system. A test instance that is not covered by any such rule is consideredas an anomaly. Rule based techniques have been applied in multiclass as well asoneclass setting.A basic multiclass rule based technique consists of two steps. First step is toTo Appear in ACM Computing Surveys, 09 2009.24  Chandola, Banerjee and Kumarlearn rules from the training data using a rule learning algorithm, such as RIPPER,Decision Trees, etc. Each rule has an associated confidence value which is proportional to ratio between the number of training instances correctly classified by therule and the total number of training instances covered by the rule. Second step isto find, for each test instance, the rule that best captures the test instance. Theinverse of the confidence associated with the best rule is the anomaly score of thetest instance. Several minor variants of the basic rule based technique have beenproposed Fan et al. 2001 Helmer et al. 1998 Lee et al. 1997 Salvador and Chan2003 Teng et al. 1990.Association rule mining Agrawal and Srikant 1995 has been used for oneclassanomaly detection by generating rules from the data in an unsupervised fashion.Association rules are generated from a categorical data set. To ensure that the rulescorrespond to strong patterns, a support threshold is used to prune out rules withlow support Tan et al. 2005. Association rule mining based techniques have beenused for network intrusion detection Mahoney and Chan 2002 2003 Mahoneyet al. 2003 Tandon and Chan 2007 Barbara et al. 2001a Otey et al. 2003, systemcall intrusion detection Lee et al. 2000 Lee and Stolfo 1998 Qin and Hwang 2004,credit card fraud detection Brause et al. 1999, and fraud detection in spacecrafthouse keeping data Yairi et al. 2001. Frequent itemsets are generated in the intermediate step of association rule mining algorithms. He et al. 2004a propose ananomaly detection algorithm for categorical data sets in which the anomaly scoreof a test instance is equal to the number of frequent itemsets it occurs in.Computational ComplexityThe computational complexity of classification based techniques depends on theclassification algorithm being used. For a discussion on the complexity of trainingclassifiers, see Kearns 1990. Generally, training decision trees tends to be fasterwhile techniques that involve quadratic optimization, such as SVMs, are more expensive, though linear time SVMs Joachims 2006 have been proposed that havelinear training time. The testing phase of classification techniques is usually veryfast since the testing phase uses a learnt model for classification.Advantages and Disadvantages of Classification Based TechniquesThe advantages of classification based techniques are as follows1 Classification based techniques, especially the multiclass techniques, can makeuse of powerful algorithms that can distinguish between instances belonging todifferent classes.2 The testing phase of classification based techniques is fast since each test instance needs to be compared against the precomputed model.The disadvantages of classification based techniques are as follows1 Multiclass classification based techniques rely on availability of accurate labelsfor various normal classes, which is often not possible.2 Classification based techniques assign a label to each test instance, which canalso become a disadvantage when a meaningful anomaly score is desired forthe test instances. Some classification techniques that obtain a probabilisticTo Appear in ACM Computing Surveys, 09 2009.Anomaly Detection  A Survey  25prediction score from the output of a classifier, can be used to address thisissue Platt 2000.5. NEAREST NEIGHBOR BASED ANOMALY DETECTION TECHNIQUESThe concept of nearest neighbor analysis has been used in several anomaly detectiontechniques. Such techniques are based on the following key assumptionAssumption Normal data instances occur in dense neighborhoods, while anomalies occur far from their closest neighbors.Nearest neighbor based anomaly detection techniques require a distance or similarity measure defined between two data instances. Distance or similarity betweentwo data instances can be computed in different ways. For continuous attributes,Euclidean distance is a popular choice but other measures can be used Tan et al.2005, Chapter 2. For categorical attributes, simple matching coefficient is oftenused but more complex distance measures can be used Boriah et al. 2008 Chandola et al. 2008. For multivariate data instances, distance or similarity is usuallycomputed for each attribute and then combined Tan et al. 2005, Chapter 2.Most of the techniques that will be discussed in this section, as well as theclustering based techniques Section 6 do not require the distance measure to bestrictly metric. The measures are typically required to be positivedefinite andsymmetric, but they are not required to satisfy the triangle inequality.Nearest neighbor based anomaly detection techniques can be broadly groupedinto two categories1 Techniques that use the distance of a data instance to its kth nearest neighboras the anomaly score.2 Techniques that compute the relative density of each data instance to computeits anomaly score.Additionally there are some techniques that use the distance between data instancesin a different manner to detect anomalies and will be briefly discussed later.5.1 Using Distance to kth Nearest NeighborA basic nearest neighbor anomaly detection technique is based on the followingdefinition  The anomaly score of a data instance is defined as its distance to itskth nearest neighbor in a given data set. This basic technique has been appliedto detect land mines from satellite ground images Byers and Raftery 1998 andto detect shorted turns anomalies in the DC field windings of large synchronousturbinegenerators Guttormsson et al. 1999. In the latter paper the authors usek  1. Usually, a threshold is then be applied on the anomaly score to determine ifa test instance is anomalous or not. Ramaswamy et al. 2000, on the other hand,select n instances with the largest anomaly scores as the anomalies.The basic technique has been extended by researchers in three different ways. Thefirst set of variants modify the above definition to obtain the anomaly score of adata instance. The second set of variants use different distancesimilarity measuresto handle different data types. The third set of variants focus on improving theefficiency of the basic technique the complexity of the basic technique is ON2,where N is the data size in different ways.To Appear in ACM Computing Surveys, 09 2009.26  Chandola, Banerjee and KumarEskin et al. 2002, Angiulli and Pizzuti 2002 and Zhang and Wang 2006 compute the anomaly score of a data instance as the sum of its distances from its knearest neighbors. A similar technique has been applied to detect credit card fraudsby Bolton and Hand 1999 called Peer Group Analysis.A different way to compute the anomaly score of a data instance is to countthe number of nearest neighbors n that are not more than d distance apart fromthe given data instance Knorr and Ng 1997 1998 1999 Knorr et al. 2000. Thismethod can also be viewed as estimating the global density for each data instancesince it involves counting the number of neighbors in a hypersphere of radius d.For example, in a 2D data set, the density of a data instance  nd2 . The inverseof the density is the anomaly score for the data instance. Instead of computing theactual density, several techniques fix the radius d and use 1n as the anomaly score,while several techniques fix n and use 1d as the anomaly score.While most techniques discussed in this category so far have been proposed tohandle continuous attributes, several variants have been proposed to handle otherdata types. A hypergraph based technique is proposed by Wei et al. 2003 calledHOT where the authors model the categorical values using a hypergraph, andmeasure distance between two data instances by analyzing the connectivity of thegraph. A distance measure for data containing a mix of categorical and continuousattributes has been proposed for anomaly detection Otey et al. 2006. The authorsdefine links between two instances by adding distance for categorical and continuous attributes separately. For categorical attributes, the number of attributes forwhich the two instances have same values defines the distance between them. Forcontinuous attributes, a covariance matrix is maintained to capture the dependencies between the continuous values. Palshikar 2005 adapts the technique proposedin Knorr and Ng 1999 to continuous sequences. Kou et al. 2006 extend thetechnique proposed in Ramaswamy et al. 2000 to spatial data.Several variants of the basic technique have been proposed to improve the efficiency. Some techniques prune the search space by either ignoring instances thatcannot be anomalous or by focussing on instances that are most likely to be anomalous. Bay and Schwabacher 2003 show that for a sufficiently randomized data, asimple pruning step could result in the average complexity of the nearest neighborsearch to be nearly linear. After calculating the nearest neighbors for a data instance, the algorithm sets the anomaly threshold for any data instance to the scoreof the weakest anomaly found so far. Using this pruning procedure, the techniquediscards instances that are close, and hence not interesting.Ramaswamy et al. 2000 propose a partition based technique, which first clustersthe instances and computes lower and upper bounds on distance of a instance fromits kth nearest neighbor for instances in each partition. This information is thenused to identify the partitions that cannot possibly contain the top k anomaliessuch partitions are pruned. Anomalies are then computed from the remaininginstances belonging to unpruned partitions in a final phase. Similar cluster basedpruning has been proposed by Eskin et al. 2002,McCallum et al. 2000, Ghotinget al. 2006, and Tao et al. 2006.Wu and Jermaine 2006 use sampling to improve the efficiency of the nearestneighbor based technique. The authors compute the nearest neighbor of everyTo Appear in ACM Computing Surveys, 09 2009.Anomaly Detection  A Survey  27instance within a smaller sample from the data set. Thus the complexity of theproposed technique is reduced to OMN where M is the sample size chosen.To prune the search space for nearest neighbors, several techniques partitionthe attribute space into a hypergrid consisting of hypercubes of fixed sizes. Theintuition behind such techniques is that if a hypercube contains many instances,such instances are likely to be normal. Moreover, if for a given instance, the hypercube that contains the instance and its adjoining hypercubes contain very fewinstances, the given instance is likely to be anomalous. Techniques based on thisintuition have been proposed by Knorr and Ng 1998. Angiulli and Pizzuti 2002extend by linearizing the search space through the Hilbert space filling curve. Theddimensional data set is fitted in a hypercube D  0, 1d. This hypercube is thenmapped to the interval I  0, 1 using the Hilbert Space Filling Curve and theknearest neighbors of a data instance are obtained by examining its successors andpredecessors in I.5.2 Using Relative DensityDensity based anomaly detection techniques estimate the density of the neighborhood of each data instance. An instance that lies in a neighborhood with low densityis declared to be anomalous while an instance that lies in a dense neighborhood isdeclared to be normal.For a given data instance, the distance to its kth nearest neighbor is equivalentto the radius of a hypersphere, centered at the given data instance, which contains k other instances. Thus the distance to the kth nearest neighbor for a givendata instance can be viewed as an estimate of the inverse of the density of theinstance in the data set and the basic nearest neighbor based technique describedin the previous subsection can be considered as a density based anomaly detectiontechnique.Density based techniques perform poorly if the data has regions of varying densities. For example, consider a 2 dimensional data set shown in Figure 7. Due to thelow density of the cluster C1 it is apparent that for every instance q inside the cluster C1, the distance between the instance q and its nearest neighbor is greater thanthe distance between the instance p2 and the nearest neighbor from the cluster C2,and the instance p2 will not be considered as anomaly. Hence, the basic techniquewill fail to distinguish between p2 and instances in C1. However, the instance p1may be detected.To handle the issue of varying densities in the data set, a set of techniqueshave been proposed to compute density of instances relative to the density of theirneighbors.Breunig et al 1999 2000 assign an anomaly score to a given data instance,known as Local Outlier Factor LOF. For any given data instance, the LOF scoreis equal to ratio of average local density of the k nearest neighbors of the instanceand the local density of the data instance itself. To find the local density for a datainstance, the authors first find the radius of the smallest hypersphere centered atthe data instance, that contains its k nearest neighbors. The local density is thencomputed by dividing k by the volume of this hypersphere. For a normal instancelying in a dense region, its local density will be similar to that of its neighbors, whilefor an anomalous instance, its local density will be lower than that of its nearestTo Appear in ACM Computing Surveys, 09 2009.28  Chandola, Banerjee and KumarC2C1p2p1Fig. 7. Advantage of local density based techniques over global density based techniques.o1n1LOF NeighborhoodCOF NeighborhoodFig. 8. Difference between the neighborhoodscomputed by LOF and COF.neighbors. Hence the anomalous instance will get a higher LOF score.In the example shown in Figure 7, LOF will be able to capture both anomaliesp1 and p2 due to the fact that it considers the density around the data instances.Several researchers have proposed variants of LOF technique. Some of thesevariants estimate the local density of an instance in a different way. Some variantshave adapted the original technique to more complex data types. Since the originalLOF technique is ON2 N is the data size, several techniques have been proposedthat improve the efficiency of LOF.Tang et al. 2002 discuss a variation of the LOF, which they call Connectivitybased Outlier Factor COF. The difference between LOF and COF is the manner inwhich the k neighborhood for an instance is computed. In COF, the neighborhoodfor an instance is computed in an incremental mode. To start, the closest instanceto the given instance is added to the neighborhood set. The next instance addedto the neighborhood set is such that its distance to the existing neighborhood setis minimum among all remaining data instances. The distance between an instanceand a set of instances is defined as the minimum distance between the given instanceand any instance belonging to the given set. The neighborhood is grown in thismanner until it reaches size k. Once the neighborhood is computed, the anomalyscore COF is computed in the same manner as LOF. COF is able to captureregions such as straight lines, as shown in Figure 8.A simpler version of LOF was proposed by Hautamaki et al. 2004 which calculates a quantity called Outlier Detection using Indegree Number ODIN for eachdata instance. For a given data instance, ODIN is equal to the number of k nearestneighbors of the data instance which have the given data instance in their k nearestneighbor list. The inverse of ODIN is the anomaly score for the data instance. Asimilar technique was proposed by Brito et al. 1997.Papadimitriou et al. 2002 propose a measure called Multigranularity DeviationFactor MDEF which is a variation of LOF. MDEF for a given data instanceis equal to the standard deviation of the local densities of the nearest neighborsof the given data instance including the data instance itself. The inverse ofthe standard deviation is the anomaly score for the data instance. The anomalydetection technique presented in the paper is called LOCI, which not only findsanomalous instances but also anomalous microclusters.To Appear in ACM Computing Surveys, 09 2009.Anomaly Detection  A Survey  29Several variants of LOF have been proposed to handle different data types. Avariant of LOF is applied for detecting spatial anomalies in climate data by Sun andChawla 2004 2006. Yu et al. 2006 use a similarity measure instead of distanceto handle categorical attributes. Similar technique has been proposed to detectsequential anomalies in protein sequences by Sun et al. 2006. This technique usesProbabilistic Suffix Trees PST to find the nearest neighbors for a given sequence.Pokrajac et al. 2007 extend LOF to work in an incremental fashion to detectanomalies in video sensor data.Some variants of the LOF technique have been proposed to improve its efficiency.Jin et al. 2001 propose a variant, in which only the top n anomalies are foundinstead of finding LOF score for every data instance. The technique includes findingmicroclusters in the data and then finding upper and lower bound on LOF for eachof the microclusters. Chiu and chee Fu 2003 proposed three variants of LOF whichenhance its performance by making certain assumptions about the problem to pruneall those clusters which definitely do not contain instances which will figure in thetop n anomaly list. For the remaining clusters a detailed analysis is done to findthe LOF score for each instance in these clusters.Computational ComplexityA drawback of the basic nearest neighbor based technique and the LOF technique,is the ON2 complexity required. Since these techniques involve finding nearestneighbors for each instance efficient data structures such as kd trees Bentley 1975and Rtrees Roussopoulos et al. 1995 can be used. But such techniques do notscale well as the number of attributes increases. Several techniques have directlyoptimized the anomaly detection technique under the assumption that only top fewanomalies are interesting. If an anomaly score is required for every test instance,such techniques are not applicable. Techniques that partition the attribute spaceinto a hypergrid, are linear in data size but are exponential in the number ofattributes, and hence are not well suited for large number of attributes. Samplingtechniques try to address the ON2 complexity issue by determining the nearestneighbors within a small sample of the data set. But sampling might result inincorrect anomaly scores if the size of the sample is very small.Advantages and Disadvantages of Nearest Neighbor Based TechniquesThe advantages of nearest neighbor based techniques are as follows1 A key advantage of nearest neighbor based techniques is that they are unsupervised in nature and do not make any assumptions regarding the generativedistribution for the data. Instead, they are purely data driven.2 Semisupervised techniques perform better than unsupervised techniques interms of missed anomalies, since the likelihood of an anomaly to form a closeneighborhood in the training data set is very low.3 Adapting nearest neighbor based techniques to a different data type is straightforward, and primarily requires defining an appropriate distance measure forthe given data.The disadvantages of nearest neighbor based techniques are as follows1 For unsupervised techniques, if the data has normal instances that do notTo Appear in ACM Computing Surveys, 09 2009.30  Chandola, Banerjee and Kumarhave enough close neighbors or if the data has anomalies that have enoughclose neighbors, the technique fails to label them correctly, resulting in missedanomalies.2 For semisupervised techniques, if the normal instances in test data do not haveenough similar normal instances in the training data, the false positive rate forsuch techniques is high.3 The computational complexity of the testing phase is also a significant challengesince it involves computing the distance of each test instance with all instancesbelonging to either the test data itself, or to the training data, to compute thenearest neighbors.4 Performance of a nearest neighbor based technique greatly relies on a distancemeasure, defined between a pair of data instances, that can effectively distinguish between normal and anomalous instances. Defining distance measuresbetween instances can be challenging when the data is complex, e.g. graphs,sequences, etc.6. CLUSTERING BASED ANOMALY DETECTION TECHNIQUESClustering Jain and Dubes 1988 Tan et al. 2005 is used to group similar datainstances into clusters. Clustering is primarily an unsupervised technique thoughsemisupervised clustering Basu et al. 2004 has also been explored lately. Eventhough clustering and anomaly detection appear to be fundamentally different fromeach other, several clustering based anomaly detection techniques have been developed. Clustering based anomaly detection techniques can be grouped into threecategories.First category of clustering based techniques rely on the following assumptionAssumption Normal data instances belong to a cluster in the data, while anomalies either do not belong to any cluster.Techniques based on the above assumption apply a known clustering based algorithm to the data set and declare any data instance that does not belong to anycluster as anomalous. Several clustering algorithms that do not force every datainstance to belong to a cluster, such as DBSCAN Ester et al. 1996, ROCK Guhaet al. 2000, and SNN clustering Ertoz et al. 2003 can be used. The FindOutalgorithm Yu et al. 2002 is an extension of the WaveCluster algorithm Sheikholeslami et al. 1998 in which the detected clusters are removed from the data andthe residual instances are declared as anomalies.A disadvantage of such techniques is that they are not optimized to find anomalies, since the main aim of the underlying clustering algorithm is to find clusters.Second category of clustering based techniques rely on the following assumptionAssumption Normal data instances lie close to their closest cluster centroid,while anomalies are far away from their closest cluster centroid.Techniques based on the above assumption consist of two steps. In the first step,the data is clustered using a clustering algorithm. In the second step, for eachdata instance, its distance to its closest cluster centroid is calculated as its anomalyscore.To Appear in ACM Computing Surveys, 09 2009.Anomaly Detection  A Survey  31A number of anomaly detection techniques that follow this two step approachhave been proposed using different clustering algorithms. Smith et al. 2002 studiedSelfOrganizing Maps SOM, Kmeans Clustering, and Expectation MaximizationEM to cluster training data and then use the clusters to classify test data. Inparticular, SOM Kohonen 1997 has been widely used to detect anomalies in asemisupervised mode in several applications such as intrusion detection Labiband Vemuri 2002 Smith et al. 2002 Ramadas et al. 2003, fault detection Harris1993 Ypma and Duin 1998 Emamian et al. 2000, and fraud detection Brockettet al. 1998. Barbara et al. 2003 propose a technique is robust to anomalies inthe training data. The authors first separate normal instances from anomaliesin the training data, using frequent itemset mining, and then use the clusteringbased technique to detect anomalies. Several techniques have also been proposedto handle sequence data Blender et al. 1997 Bejerano and Yona 2001 Vinuezaand Grudic 2004 Budalakoti et al. 2006.Techniques based on the second assumption can also operate in semisupervisedmode, in which the training data is clustered and instances belonging to the testdata are compared against the clusters to obtain an anomaly score for the testdata instance Marchette 1999 Wu and Zhang 2003 Vinueza and Grudic 2004Allan et al. 1998. If the training data has instances belonging to multiple classes,semisupervised clustering can be applied to improve the clusters. He et al. 2002incorporate the knowledge of labels to improve on their unsupervised clusteringbased anomaly detection technique He et al. 2003 by calculating a measure calledsemantic anomaly factor which is high if the class label of an object in a cluster isdifferent from the majority of the class labels in that cluster.Note that if the anomalies in the data form clusters by themselves, the abovediscussed techniques will not be able to detect such anomalies. To address thisissue a third category of clustering based techniques have been proposed that relyon the following assumptionAssumption Normal data instances belong to large and dense clusters, whileanomalies either belong to small or sparse clusters.Techniques based on the above assumption declare instances belonging to clusterswhose size andor density is below a threshold as anomalous.Several variations of the third category of techniques have been proposed Piresand SantosPereira 2005 Otey et al. 2003 Eskin et al. 2002 Mahoney et al. 2003Jiang et al. 2001 He et al. 2003. The technique proposed by He et al. 2003, calledFindCBLOF, assigns an anomaly score known as ClusterBased Local Outlier FactorCBLOF for each data instance. The CBLOF score captures the size of the clusterto which the data instance belongs, as well as the distance of the data instance toits cluster centroid.Several clustering based techniques have been proposed to improve the efficiencyof the existing techniques discussed above. Fixed width clustering is a linear timeONd approximation algorithm used by various anomaly detection techniquesEskin et al. 2002 Portnoy et al. 2001 Mahoney et al. 2003 He et al. 2003. Aninstance is assigned to a cluster whose center is within a prespecified distance to thegiven instance. If no such cluster exists then a new cluster with the instance as theTo Appear in ACM Computing Surveys, 09 2009.32  Chandola, Banerjee and Kumarcenter is created. Then they determine which clusters are anomalies based on theirdensity and distance from other clusters. The width can either be a userspecifiedparameter Eskin et al. 2002 Portnoy et al. 2001 or can be derived from the dataMahoney et al. 2003. Chaudhary et al. 2002 propose an anomaly detectiontechnique using kd trees which provide a partitioning of the data in linear time.They apply their technique to detect anomalies in astronomical data sets wherecomputational efficiency is an important requirement. Another technique whichaddresses this issue is proposed by Sun et al. 2004. The authors propose anindexing technique called CDtrees to efficiently partition data into clusters. Thedata instances which belong to sparse clusters are declared as anomalies.6.1 Distinction between Clustering Based and Nearest Neighbor Based TechniquesSeveral clustering based techniques require distance computation between a pairof instances. Thus, in that respect, they are similar to nearest neighbor basedtechniques. The choice of the distance measure is critical to the performance ofthe technique hence the discussion in the previous section regarding the distancemeasures hold for clustering based techniques also. The key difference between thetwo techniques, however, is that clustering based techniques evaluate each instancewith respect to the cluster it belongs to, while nearest neighbor based techniquesanalyze each instance with respect to its local neighborhood.Computational ComplexityThe computational complexity of training a clustering based anomaly detectiontechnique depends on the clustering algorithm used to generate clusters from thedata. Thus such techniques can have quadratic complexity if the clustering technique requires computation of pairwise distances for all data instances, or linearwhen using heuristic based techniques such as kmeans Hartigan and Wong 1979or approximate clustering techniques Eskin et al. 2002. The test phase of clustering based techniques is fast, since it involves comparing a test instance with asmall number of clusters.Advantages and Disadvantages of Clustering Based TechniquesThe advantages of clustering based techniques are as follows1 Clustering based techniques can operate in an unsupervised mode.2 Such techniques can often be adapted to other complex data types by simplyplugging in a clustering algorithm that can handle the particular data type.3 The testing phase for clustering based techniques is fast since the number ofclusters against which every test instance needs to be compared is a smallconstant.The disadvantages of clustering based techniques are as follows1 Performance of clustering based techniques is highly dependent on the effectiveness of clustering algorithm in capturing the cluster structure of normalinstances.2 Many techniques detect anomalies as a byproduct of clustering, and hence arenot optimized for anomaly detection.To Appear in ACM Computing Surveys, 09 2009.Anomaly Detection  A Survey  333 Several clustering algorithms force every instance to be assigned to some cluster.This might result in anomalies getting assigned to a large cluster, therebybeing considered as normal instances by techniques that operate under theassumption that anomalies do not belong to any cluster.4 Several clustering based techniques are effective only when the anomalies donot form significant clusters among themselves.5 The computational complexity for clustering the data is often a bottleneck,especially if ON2d clustering algorithms are used.7. STATISTICAL ANOMALY DETECTION TECHNIQUESThe underlying principle of any statistical anomaly detection technique is Ananomaly is an observation which is suspected of being partially or wholly irrelevant because it is not generated by the stochastic model assumed Anscombe andGuttman 1960. Statistical anomaly detection techniques are based on the followingkey assumptionAssumption Normal data instances occur in high probability regions of a stochastic model, while anomalies occur in the low probability regions of the stochasticmodel.Statistical techniques fit a statistical model usually for normal behavior to thegiven data and then apply a statistical inference test to determine if an unseeninstance belongs to this model or not. Instances that have a low probability to begenerated from the learnt model, based on the applied test statistic, are declared asanomalies. Both parametric as well as nonparametric techniques have been appliedto fit a statistical model. While parametric techniques assume the knowledge ofunderlying distribution and estimate the parameters from the given data Eskin2000, nonparametric techniques do not generally assume knowledge of underlyingdistribution Desforges et al. 1998. In the next two subsection we will discussparametric and nonparametric anomaly detection techniques.7.1 Parametric TechniquesAs mentioned before, parametric techniques assume that the normal data is generated by a parametric distribution with parameters  and probability densityfunction fx, , where x is an observation. The anomaly score of a test instanceor observation x is the inverse of the probability density function, fx, . Theparameters  are estimated from the given data.Alternatively, a statistical hypothesis test also referred to as discordancy test instatistical outlier detection literature Barnett and Lewis 1994 maybe used. Thenull hypothesis H0 for such tests is that the data instance x has been generatedusing the estimated distribution with parameters . If the statistical test rejectsH0, x is declared to be anomaly. A statistical hypothesis test is associated witha test statistic, which can be used to obtain a probabilistic anomaly score for thedata instance x.Based on the type of distribution assumed, parametric techniques can be furthercategorized as followsTo Appear in ACM Computing Surveys, 09 2009.34  Chandola, Banerjee and KumarMaxMinQ1MedianQ3AnomaliesAnomalyFig. 9. A box plot for a univariate data set.7.1.1 Gaussian Model Based. Such techniques assume that the data is generated from a Gaussian distribution. The parameters are estimated using MaximumLikelihood Estimates MLE. The distance of a data instance to the estimated meanis the anomaly score for that instance. A threshold is applied to the anomaly scoresto determine the anomalies. Different techniques in this category calculate the distance to the mean and the threshold in different ways.A simple outlier detection technique, often used in process quality control domainShewhart 1931, is to declare all data instances that are more than 3 distance awayfrom the distribution mean , where  is the standard deviation for the distribution.The  3 region contains 99.7 of the data instances.More sophisticated statistical tests have also been used to detect anomalies, asdiscussed in Barnett and Lewis 1994 Barnett 1976 Beckman and Cook 1983. Wewill describe a few tests here.The box plot rule Figure 9 is the simplest statistical technique that has beenapplied to detect univariate and multivariate anomalies in medical domain dataLaurikkala et al. 2000 Horn et al. 2001 Solberg and Lahti 2005 and turbinerotors data Guttormsson et al. 1999. A boxplot graphically depicts the data usingsummary attributes such as smallest nonanomaly observation min, lower quartileQ1, median, upper quartile Q3, and largest nonanomaly observation max.The quantity Q3Q1 is called the Inter Quartile Range IQR. The box plots alsoindicates the limits beyond which any observation will be treated as an anomaly. Adata instance that lies more than 1.5IQR lower than Q1 or 1.5IQR higher thanQ3 is declared as an anomaly. The region between Q1  1.5IQR and Q3  1.5IQRcontains 99.3 of observations, and hence the choice of 1.5IQR boundary makesthe box plot rule equivalent to the 3 technique for Gaussian data.Grubbs test also known as the maximum normed residual test is used to detectanomalies in a univariate data set Grubbs 1969 Stefansky 1972 Anscombe andGuttman 1960 under the assumption that the data is generated by a Gaussiandistribution. For each test instance x, its z score is computed as followsz x xs1where x and s are the mean and standard deviation of the data sample, respectively.To Appear in ACM Computing Surveys, 09 2009.Anomaly Detection  A Survey  35A test instance is declared to be anomalous ifz N  1N t22N,N2N  2  t22N,N22where N is the data size and t2N,N2 is a threshold used to declare an instanceto be anomalous or normal. This threshold is the value taken by a tdistribution ata significance level of 2N . The significance level reflects the confidence associatedwith the threshold and indirectly controls the number of instances declared asanomalous.A variant of the Grubbs test for multivariate data was proposed by Laurikkalaet al. 2000, which uses the Mahalanobis distance of a test instance x to the samplemean x, to reduce multivariate observations to univariate scalars.y2  x xS1x x, 3where S is the sample covariance matrix. The univariate Grubbs test is appliedto y to determine if the instance x is anomalous or not. Several other variants ofGrubbs test have been proposed to handle multivariate data sets Aggarwal andYu 2001 2008 Laurikkala et al. 2000, graph structured data Shekhar et al. 2001,and Online Analytical Processing OLAP data cubes Sarawagi et al. 1998.The students ttest has also been applied for anomaly detection in Surace andWorden 1998 Surace et al. 1997 to detect damages in structural beams. A normalsample, N1 is compared with a test sample, N2 using the ttest. If the test showssignificant difference between them, it signifies the presence of an anomaly in N2.The multivariate version of students ttest called the Hotelling t2test is also usedas an anomaly detection test statistic in Liu and Weng 1991 to detect anomaliesin bioavailabilitybioequivalence studies.Ye and Chen 2001 use a 2 statistic to determine anomalies in operating systemcall data. The training phase assumes that the normal data has a multivariatenormal distribution. The value of the 2 statistic is determined as2 ni1Xi  Ei2Ei4where Xi is the observed value of the ith variable, Ei is the expected value of theith variable obtained from the training data and n is the number of variables. Alarge value of X2 denotes that the observed sample contains anomalies.Several other statistical anomaly detection techniques that assume that the datafollows a Gaussian distribution have been proposed that use other statistical tests,such as Rosner test Rosner 1983, Dixon test Gibbons 1994, Slippage Detectiontest Hawkins 1980, etc.7.1.2 Regression Model Based. Anomaly detection using regression has beenextensively investigated for timeseries data Abraham and Chuang 1989 Abrahamand Box 1979 Fox 1972.The basic regression model based anomaly detection technique consists of twosteps. In the first step, a regression model is fitted on the data. In the second step,for each test instance, the residual for the test instance is used to determine theTo Appear in ACM Computing Surveys, 09 2009.36  Chandola, Banerjee and Kumaranomaly score. The residual is the part of the instance which is not explained bythe regression model. The magnitude of the residual can be used as the anomalyscore for the test instance, though statistical tests have been proposed to determineanomalies with certain confidence Anscombe and Guttman 1960 Beckman andCook 1983 Hawkins 1980 Torr and Murray 1993. Certain techniques detect thepresence of anomalies in a data set by analyzing the Akaike Information ContentAIC during model fitting Kitagawa 1979 Kadota et al. 2003.Presence of anomalies in the training data can influence the regression parameters and hence the regression model might not produce accurate results. A populartechnique to handle such anomalies while fitting regression models is called robustregression Rousseeuw and Leroy 1987 estimation of regression parameters whileaccommodating anomalies. The authors argue that the robust regression techniques not only hide the anomalies, but can also detect the anomalies, becausethe anomalies tend to have larger residuals from the robust fit. A similar robustanomaly detection approach has been applied in Autoregressive Integrated MovingAverage ARIMA models Bianco et al. 2001 Chen et al. 2005.Variants of the basic regression models based technique have been proposed tohandle multivariate timeseries data. Tsay et al. 2000 discuss the additional complexity in multivariate timeseries over the univariate timeseries and come up withstatistics that can be applied to detect anomalies in multivariate ARIMA models.This is a generalization of statistics proposed earlier by Fox 1972.Another variant that detect anomalies in multivariate timeseries data generatedby an Autoregressive Moving Average ARMA model, was proposed by Galeanoet al. 2004. In this technique the authors transform the multivariate timeseriesto univariate timeseries by linearly combining the components of the multivariate timeseries. The interesting linear combinations projections in 1d space areobtained using a projection pursuit technique Huber 1985 that maximizes theKurtosis coefficient a measure for the degree of peakednessflatness in the variabledistribution of the timeseries data. The anomaly detection in each projection isdone by using univariate test statistics as proposed by Fox 1972.7.1.3 Mixture of Parametric Distributions Based. Such techniques use a mixture of parametric statistical distributions to model the data. Techniques in thiscategory can be grouped into two subcategories. The first subcategory of techniques model the normal instances and anomalies as separate parametric distributions, while the second subcategory of techniques model only the normal instancesas a mixture of parametric distributions.For the first subcategory of techniques, the testing phase involves determiningwhich distributionnormal or anomalousthe test instance belongs to. Abrahamand Box 1979 assume that the normal data is generated from a Gaussian distribution N0,2 and the anomalies are also generated from a Gaussian distributionwith same mean but with larger variance, N0,k22. A test instance is testedusing the Grubbs test on both distributions, and accordingly labeled as normalor anomalous. Similar techniques have been proposed in Lauer 2001 Eskin 2000Abraham and Box 1979 Box and Tiao 1968 Agarwal 2005. Eskin 2000 use Expectation Maximization EM algorithm to develop a mixture of models for the twoclasses, assuming that each data point is an anomaly with apriori probability , andTo Appear in ACM Computing Surveys, 09 2009.Anomaly Detection  A Survey  37normal with apriori probability 1 . Thus, if D represents the actual probabilitydistribution of the entire data, and M and A represent the distributions of thenormal and anomalous data respectively, then D  A  1 M. M is learntusing any distribution estimation technique, while A is assumed to be uniform.Initially all points are considered to be in M. The anomaly score is assigned to apoint based on how much the distributions change if that point is removed from Mand added to A.The second subcategory of techniques model the normal instances as a mixtureof parametric distributions. A test instance which does not belong to any of thelearnt models is declared to be anomaly. Gaussian mixture models have been mostlyused for such techniques Agarwal 2006, and have been used to detect strains inairframe data Hickinbotham and Austin 2000a Hollier and Austin 2002, to detectanomalies in mammographic image analysis Spence et al. 2001 Tarassenko 1995and for network intrusion detection Yamanishi and ichi Takeuchi 2001 Yamanishiet al. 2004. Similar techniques have been applied to detecting anomalies in biomedical signal data Roberts and Tarassenko 1994 Roberts 1999 2002, where extremevalue statistics2 are used to determine if a test point is an anomaly with respectto the learnt mixture of models or not. Byers and Raftery 1998 use a mixture ofPoisson distributions to model the normal data and then detect anomalies.7.2 Nonparametric TechniquesThe anomaly detection techniques in this category use nonparametric statistical models, such that the model structure is not defined a prioiri, but is insteaddetermined from given data. Such techniques typically make fewer assumptionsregarding the data, such as smoothness of density, when compared to parametrictechniques.7.2.1 Histogram Based. The simplest nonparametric statistical technique is touse histograms to maintain a profile of the normal data. Such techniques are alsoreferred to as frequency based or counting based. Histogram based techniques areparticularly popular in intrusion detection community Eskin 2000 Eskin et al.2001 Denning 1987 and fraud detection Fawcett and Provost 1999, since thebehavior of the data is governed by certain profiles user or software or systemthat can be efficiently captured using the histogram model.A basic histogram based anomaly detection technique for univariate data consistsof two steps. The first step involves building a histogram based on the differentvalues taken by that feature in the training data. In the second step, the techniquechecks if a test instance falls in any one of the bins of the histogram. If it does, thetest instance is normal, otherwise it is anomalous. A variant of the basic histogrambased technique is to assign an anomaly score to each test instance based on theheight frequency of the bin in which it falls.2Extreme Value Theory EVT Pickands 1975 is a similar concept as anomaly detection, anddeals with extreme deviations of a probability distribution. EVT has been applied to risk management McNeil 1999 as a method for modeling and measuring extreme risks. The key differencebetween extreme values and statistical anomalies is that extreme values are known to occur at theextremities of a probability distribution, while anomalies are more general. Anomalies can alsobe generated from a different distribution altogether.To Appear in ACM Computing Surveys, 09 2009.38  Chandola, Banerjee and KumarThe size of the bin used when building the histogram is key for anomaly detection.If the bins are small, many normal test instances will fall in empty or rare bins,resulting in a high false alarm rate. If the bins are large, many anomalous testinstances will fall in frequent bins, resulting in a high false negative rate. Thus akey challenge for histogram based techniques is to determine an optimal size of thebins to construct the histogram which maintains low false alarm rate and low falsenegative rate.Histogram based techniques require normal data to build the histograms Anderson et al. 1994 Javitz and Valdes 1991 Helman and Bhangoo 1997. Sometechniques even construct histograms for the anomalies Dasgupta and Nino 2000,if labeled anomalous instances are available.For multivariate data, a basic technique is to construct attributewise histograms.During testing, for each test instance, the anomaly score for each attribute valueof the test instance is calculated as the height of the bin that contains the attribute value. The perattribute anomaly scores are aggregated to obtain an overallanomaly score for the test instance.The basic histogram based technique for multivariate data has been applied tosystem call intrusion detection Endler 1998, network intrusion detection Ho et al.1999 Yamanishi and ichi Takeuchi 2001 Yamanishi et al. 2004, fraud detectionFawcett and Provost 1999, damage detection in structures Manson 2002 Mansonet al. 2001 Manson et al. 2000, detecting webbased attacks Kruegel and Vigna2003 Kruegel et al. 2002, and anomalous topic detection in text data Allan et al.1998. A variant of the simple technique is used in Packet Header Anomaly Detection PHAD and Application Layer Anomaly Detection ALAD Mahoney andChan 2002, applied to network intrusion detection.The SRI Internationals realtime Network Intrusion Detection System NIDESAnderson et al. 1994 Anderson et al. 1995 Porras and Neumann 1997 has a subsystem that maintains longterm statistical profiles to capture the normal behaviorof a computer system Javitz and Valdes 1991. The authors propose a Q statisticto compare a longterm profile with a short term profile observation. The statisticis used to determine another measure called S statistic which reflects the extent towhich the behavior in a particular feature is anomaly with respect to the historicalprofile. The featurewise S statistics are combined to get a single value called ISstatistic which determines if a test instance is anomalous or not. A variant has beenproposed by Sargor 1998 for anomaly detection in linkstate routing protocols.7.2.2 Kernel Function Based. A nonparametric technique for probability density estimation is parzen windows estimation Parzen 1962. This involves usingkernel functions to approximate the actual density. Anomaly detection techniquesbased on kernel functions are similar to parametric methods described earlier. Theonly difference is the density estimation technique used. Desforges et al. 1998 proposed a semisupervised statistical technique to detect anomalies which uses kernelfunctions to estimate the probability distribution function pdf for the normal instances. A new instance which lies in the low probability area of this pdf is declaredto be anomalous.Similar application of parzen windows is proposed for network intrusion detectionChow and Yeung 2002, for novelty detection in oil flow data Bishop 1994, andTo Appear in ACM Computing Surveys, 09 2009.Anomaly Detection  A Survey  39for mammographic image analysis Tarassenko 1995.Computational ComplexityThe computational complexity of statistical anomaly detection techniques dependson the nature of statistical model that is required to be fitted on the data. Fittingsingle parametric distributions from the exponential family, e.g., Gaussian, Poisson,Multinomial, etc., is typically linear in data size as well as number of attributes.Fitting complex distributions such as mixture models, HMM, etc. using iterativeestimation techniques such as Expectation Maximization EM, are also typicallylinear per iteration, though they might be slow in converging depending on theproblem andor convergence criterion. Kernel based techniques can potentiallyhave quadratic time complexity in terms of the data size.Advantages and Disadvantages of Statistical TechniquesThe advantages of statistical techniques are1 If the assumptions regarding the underlying data distribution hold true, statistical techniques provide a statistically justifiable solution for anomaly detection.2 The anomaly score provided by a statistical technique is associated with aconfidence interval, which can be used as additional information while makinga decision regarding any test instance.3 If the distribution estimation step is robust to anomalies in data, statisticaltechniques can operate in a unsupervised setting without any need for labeledtraining data.The disadvantages of statistical techniques are1 The key disadvantage of statistical techniques is that they rely on the assumption that the data is generated from a particular distribution. This assumptionoften does not hold true, especially for high dimensional real data sets.2 Even when the statistical assumption can be reasonably justified, there are several hypothesis test statistics that can be applied to detect anomalies choosingthe best statistic is often not a straightforward task Motulsky 1995. In particular, constructing hypothesis tests for complex distributions that are requiredto fit high dimensional data sets is nontrivial.3 Histogram based techniques are relatively simple to implement, but a key shortcoming of such techniques for multivariate data is that they are not able tocapture the interactions between different attributes. An anomaly might haveattribute values that are individually very frequent, but their combination isvery rare, but an attributewise histogram based technique would not be ableto detect such anomalies.8. INFORMATION THEORETIC ANOMALY DETECTION TECHNIQUESInformation theoretic techniques analyze the information content of a data set usingdifferent information theoretic measures such as Kolomogorov Complexity, entropy,relative entropy, etc. Such techniques are based on the following key assumptionAssumption Anomalies in data induce irregularities in the information contentof the data set.To Appear in ACM Computing Surveys, 09 2009.40  Chandola, Banerjee and KumarLet CD denote the complexity of a given data set, D. A basic informationtheoretic technique can be described as follows. Given a data set D, find theminimal subset of instances, I, such that CDCDI is maximum. All instancesin the subset thus obtained, are deemed as anomalous. The problem addressed bythis basic technique is to find a paretooptimal solution, which does not have asingle optima, since there are two different objectives that need to be optimized.In the above described technique, the complexity of a data set C can be measured in different ways. Kolomogorov complexity Li and Vitanyi 1993 has beenused by several techniques Arning et al. 1996 Keogh et al. 2004. Arning et al.1996 use the size of the regular expression to measure the Kolomogorov Complexity of data represented as a string for anomaly detection. Keogh et al. 2004 usethe size of the compressed data file using any standard compression algorithm, asa measure of the data sets Kolomogorov Complexity. Other information theoreticmeasures such as entropy, relative uncertainty, etc., have also been used to measurethe complexity of a categorical data set Lee and Xiang 2001 He et al. 2005 Heet al. 2006 Ando 2007.The basic technique described above, involves dual optimization to minimizethe subset size while maximizing the reduction in the complexity of the data set.Thus an exhaustive approach in which every possible subset of the data set isconsidered would run in exponential time. Several techniques have been proposedthat perform approximate search for the most anomalous subset. He et al. 2006use an approximate algorithm called Local Search Algorithm LSA He et al. 2005to approximately determine such a subset in a linear fashion, using entropy asthe complexity measure. A similar technique that uses an information bottleneckmeasure was proposed by Ando 2007.Information theoretic techniques have also been used in data sets in which datainstances are naturally ordered, e.g., sequential data, spatial data. In such cases, thedata is broken into substructures segments for sequences, subgraphs for graphs,etc., and the anomaly detection technique finds the substructure, I, such thatCD  CD  I is maximum. This technique has been applied to sequences Linet al. 2005 Chakrabarti et al. 1998 Arning et al. 1996, graph data Noble andCook 2003, and spatial data Lin and Brown 2003. A key challenge of such techniques is to find the optimal size of the substructure which would result in detectinganomalies.Computational ComplexityAs mentioned earlier, the basic information theoretic anomaly detection techniquehas exponential time complexity, though approximate techniques have been proposed that have linear time complexity.Advantages and Disadvantages of Information Theoretic TechniquesThe advantages of information theoretic techniques are as follows1 They can operate in an unsupervised setting.2 They do not make any assumptions about the underlying statistical distributionfor the data.The disadvantages of information theoretic techniques are as followsTo Appear in ACM Computing Surveys, 09 2009.Anomaly Detection  A Survey  411 The performance of such techniques is highly dependent on the choice of theinformation theoretic measure. Often, such measures can detect the presence ofanomalies only when there are significantly large number of anomalies presentin the data.2 Information theoretic techniques applied to sequences and spatial data sets relyon the size of the substructure, which is often nontrivial to obtain.3 It is difficult to associate an anomaly score with a test instance using an information theoretic technique.9. SPECTRAL ANOMALY DETECTION TECHNIQUESSpectral techniques try to find an approximation of the data using a combinationof attributes that capture the bulk of variability in the data. Such techniques arebased on the following key assumptionAssumption Data can be embedded into a lower dimensional subspace in whichnormal instances and anomalies appear significantly different.Thus the general approach adopted by spectral anomaly detection techniques is todetermine such subspaces embeddings, projections, etc. in which the anomalousinstances can be easily identified Agovic et al. 2007. Such techniques can work inan unsupervised as well as semisupervised setting.Several techniques use Principal Component Analysis PCA Jolliffe 2002 forprojecting data into a lower dimensional space. One such technique Parra et al.1996 analyzes the projection of each data instance along the principal componentswith low variance. A normal instance that satisfies the correlation structure of thedata will have a low value for such projections while an anomalous instances thatdeviates from the correlation structure will have a large value. Dutta et al. 2007adopt this approach to detect anomalies in astronomy catalogs.Ide and Kashima 2004 propose a spectral technique to detect anomalies in atime series of graphs. Each graph is represented as an adjacency matrix for a giventime. At every time instance, the principle component of the matrix is chosen asthe activity vector for the given graph. The timeseries of the activity vectors isconsidered as a matrix and the principal left singular vector is obtained to capturethe normal dependencies over time in the data. For a new test graph, then angle between its activity vector and the principal left singular vector obtained fromthe previous graphs is computed and used to determine the anomaly score of thetest graph. In a similar approach, Sun et al. 2007 propose an anomaly detectiontechnique on a sequence of graphs by performing Compact Matrix DecompositionCMD on the adjacency matrix for each graph and thus obtaining an approximation of the original matrix. For each graph in the sequence, the authors performCMD and compute the approximation error between the original adjacency matrixand the approximate matrix. The authors construct a time series of the approximation errors and detect anomalies in the time series of errors the graph correspondingto anomalous approximation error is declared to be anomalous.Shyu et al. 2003 present an anomaly detection technique where the authorsperform robust PCA Huber 1974 to estimate the principal components from thecovariance matrix of the normal training data. The testing phase involves comparTo Appear in ACM Computing Surveys, 09 2009.42  Chandola, Banerjee and Kumaring each point with the components and assigning an anomaly score based on thepoints distance from the principal components. Thus if the projection of x on theprincipal components are y1, y2, . . ., yp and the corresponding eigenvalues are 1,2, . . ., p, thenqi1y2iiy211y222 . . . y2qq, q  p 5has a chisquare distribution Hawkins 1974. Using this result, the authors proposethat, for a given significance level , observation x is an anomaly ifqi1y2ii 2q 6It can be shown that the quantity calculated in Equation 5 is equal to the Mahalanobis distance of the instance x from the sample mean See Equation 3 whenq  p Shyu et al. 2003. Thus the robust PCA based technique is same as astatistical technique discussed in Section 7.1.1 in a smaller subspace.The robust PCA based technique has been applied to the network intrusion detection domain Shyu et al. 2003 Lakhina et al. 2005 Thottan and Ji 2003 andfor detecting anomalies in space craft components Fujimaki et al. 2005.Computational ComplexityStandard PCA based techniques are typically linear in data size but often quadraticin the number of dimensions. Non linear techniques can improve the time complexity to be linear in the number of dimensions but polynomial in the number ofprincipal components Gunter et al. 2007. Techniques that perform SVD on thedata typically quadratic in data size.Advantages and Disadvantages of Spectral TechniquesThe advantages of spectral anomaly detection techniques are as follows1 Spectral techniques automatically perform dimensionality reduction and henceare suitable for handling high dimensional data sets. Moreover, they can alsobe used as a preprocessing step followed by application of any existing anomalydetection technique in the transformed space.2 Spectral techniques can be used in an unsupervised setting.The disadvantages of spectral anomaly detection techniques are as follows1 Spectral techniques are useful only if the normal and anomalous instances areseparable in the lower dimensional embedding of the data.2 Spectral techniques typically have high computational complexity.10. HANDLING CONTEXTUAL ANOMALIESThe anomaly detection techniques discussed in the previous sections primarily focuson detecting point anomalies. In this section, we will discuss anomaly detectiontechniques that handle contextual anomalies.As discussed in Section 2.2.2, contextual anomalies require that the data has a setof contextual attributes to define a context, and a set of behavioral attributes toTo Appear in ACM Computing Surveys, 09 2009.Anomaly Detection  A Survey  43detect anomalies within a context. Song et al. 2007 use the terms environmentaland indicator attributes which are analogous to our terminology. Some of the waysin which contextual attributes can be defined are1 Spatial The data has spatial attributes, which define the location of a datainstance and hence a spatial neighborhood. A number of context based anomalydetection techniques Lu et al. 2003 Shekhar et al. 2001 Kou et al. 2006 Sunand Chawla 2004 have been proposed for data with spatial data.2 Graphs The edges that connect nodes data instances define neighborhoodfor each node. Contextual anomaly detection techniques have been applied tograph based data by Sun et al. 2005.3 Sequential The data is sequential, i.e., the contextual attributes of a datainstance is its position in the sequence.Timeseries data has been extensively explored in the contextual anomaly detection category Abraham and Chuang 1989 Abraham and Box 1979 Rousseeuwand Leroy 1987 Bianco et al. 2001 Fox 1972 Salvador and Chan 2003 Tsayet al. 2000 Galeano et al. 2004 Zeevi et al. 1997.Another form of sequential data for which anomaly detection techniques havebeen developed is event data, in which each event has a timestamp such asoperating system call data or web data Ilgun et al. 1995 Vilalta and Ma2002 Weiss and Hirsh 1998 Smyth 1994. The difference between timeseriesdata and event sequences is that for the latter, the interarrival time betweenconsecutive events is uneven.4 Profile Often times the data might not have an explicit spatial or sequentialstructure, but can still be segmented or clustered into components using a setof contextual attributes. These attributes are typically used to profile andgroup users in activity monitoring systems, such as cellphone fraud detectionFawcett and Provost 1999 Teng et al. 1990, CRM databases He et al. 2004band creditcard fraud detection Bolton and Hand 1999. The users are thenanalyzed within their group for anomalies.In comparison to the rich literature on point anomaly detection techniques, theresearch on contextual anomaly detection has been limited. Broadly, such techniques can be classified in two categories. The first category of techniques reduce acontextual anomaly detection problem to a point anomaly detection problem whilethe second category of techniques model the structure in the data and use the modelto detect anomalies.10.1 Reduction to Point Anomaly Detection ProblemSince contextual anomalies are individual data instances like point anomalies, butare anomalous only with respect to a context, one approach is to apply a knownpoint anomaly detection technique within a context.A generic reduction based technique consists of two steps. First, identify a contextfor each test instance using the contextual attributes. Second, compute anomalyscore for the test instance within the context using a known point anomaly detectiontechnique.To Appear in ACM Computing Surveys, 09 2009.44  Chandola, Banerjee and KumarAn example of the generic reduction based technique has been proposed for thescenario where identifying the context is not straightforward Song et al. 2007.The authors assume that the attributes are already partitioned into contextualand behavioral attributes. Thus each data instance d can be represented as x, y.The contextual data is partitioned using a mixture of Gaussian model, say U . Thebehavioral data is also partitioned using another mixture of Gaussian model, say V .A mapping function, pVj Ui is also learnt. This mapping indicates the probabilityof the indicator part of a data point y to be generated from a mixture componentVj , when the environmental part x is generated by Ui. Thus for a given test instanced  x, y, the anomaly score is given byAnomaly Score nUi1px  UinVj1py  VjpVj Uiwhere nU is the number of mixture components in U and NV is the number ofmixture components in V . px  Ui indicates the probability of a sample pointx to be generated from the mixture component Ui while py  Uj indicates theprobability of a sample point y to be generated from the mixture component Vj .Another example of the generic technique is applied to cellphone fraud detectionFawcett and Provost 1999. The data in this case consists of cellphone usagerecords. One of the attributes in the data is the cellphone user which is used asthe contextual attribute. The activity of each user is then monitored to detectanomalies using other attributes. A similar technique is adopted for computersecurity Teng et al. 1990, where the contextual attributes are user id, time ofthe day. The remaining attributes are compared with existing rules representingnormal behavior to detect anomalies. Peer group analysis Bolton and Hand 1999is another similar technique where users are grouped together as peers and analyzedwithin a group for fraud. He et al. 2004b propose the concept of class anomalydetection, which is essentially segmenting the data using the class labels, and thenapplying a known clustering based anomaly detection technique He et al. 2002 todetect anomalies within this subset.For spatial data, neighborhoods are intuitive and straightforward to detect Ngand Han 1994 by using the location coordinates. Graphbased anomaly detectionShekhar et al. 2001 Lu et al. 2003 Kou et al. 2006 use Grubbs score Grubbs1969 or similar statistical point anomaly detection techniques to detect anomalieswithin a spatial neighborhood. Sun and Chawla 2004 use a distance based measurecalled SLOM Spatial Local Outlier Measure Sun and Chawla 2006 to detectspatial anomalies within a neighborhood.Another example of the generic technique applied to timeseries data is proposedby Basu and Meckesheimer 2007. For a given instance in a timeseries the authorscompare the observed value to the median of the neighborhood values. A transformation technique for timeseries data has been proposed by using phase spaces Maand Perkins 2003b. This technique converts a timeseries into a set of vectors byunfolding the timeseries into a phase space using a timedelay embedding process.The temporal relations at any time instance are embedded in the phase vector forthat instance. The authors use this technique to transform a timeseries into feature space and then use oneclass SVMs to detect anomalies. Each anomaly can beTo Appear in ACM Computing Surveys, 09 2009.Anomaly Detection  A Survey  45translated to a value at certain time instance in the original timeseries.10.2 Utilizing the Structure in DataIn several scenarios, breaking up data into contexts is not straightforward. This istypically true for timeseries data and event sequence data. In such cases, timeseries modeling and sequence modeling techniques are extended to detect contextualanomalies in the data.A generic technique in this category can be described as follows. A model is learntfrom the training data which can predict the expected behavior with respect to agiven context. If the expected behavior is significantly different from the observedbehavior, an anomaly is declared. A simple example of this generic technique isregression in which the contextual attributes can be used to predict the behavioralattribute by fitting a regression line on the data.For time series data, several regression based techniques for timeseries modeling such as robust regression Rousseeuw and Leroy 1987, autoregressive modelsFox 1972, ARMA models Abraham and Chuang 1989 Abraham and Box 1979Galeano et al. 2004 Zeevi et al. 1997, and ARIMA models Bianco et al. 2001Tsay et al. 2000, have been developed for contextual anomaly detection. Regression based techniques have been extended to detect contextual anomalies in a setof coevolving sequences by modeling the regression as well as correlation betweenthe sequences Yi et al. 2000.One of the earliest works in timeseries anomaly detection was proposed by Fox1972, where a timeseries was modeled as a stationary autoregressive process. Anyobservation is tested to be anomaly by comparing it with the covariance matrix ofthe autoregressive process. If the observation falls outside the modeled error forthe process, it is declared to be an anomaly. An extension to this technique is madeby using Support Vector Regression to estimate the regression parameters and thenusing the learnt model to detect novelties in the data Ma and Perkins 2003a.A technique to detect a single anomaly discord in a sequence of alphabets wasproposed by Keogh et al. 2004. The technique adopts a divide and conquer approach. The sequence is divided into two parts and the Kolmogorov Complexity iscalculated for each. The one with higher complexity contains the anomaly. The sequence is recursively divided until they are left with a single event which is declaredto be the anomaly in the sequence.Weiss and Hirsh 1998 propose a technique to detect rare events in sequentialdata, where they use events occurring before a particular time to predict the eventoccurring at that time instance. If the prediction does not match the actual event,it is declared to be rare. This idea is extended in other areas, where the authorshave used Frequent Itemset Mining Vilalta and Ma 2002, Finite State AutomatonFSA Ilgun et al. 1995 Salvador and Chan 2003 and Markov Models Smyth 1994to determine conditional probabilities for events based on the history of events.Marceau 2000 use FSA to predict the next event of a sequence based on theprevious n events. They apply this technique to the domain of system call intrusiondetection. Hollmen and Tresp 1999 employ HMM for cell phone fraud detection.The authors use a hierarchical regime switching call model to model the cell phoneactivity of a user. The model predicts the probability of a fraud taking place fora call using the learnt model. The parameter estimation is done using the EMTo Appear in ACM Computing Surveys, 09 2009.46  Chandola, Banerjee and Kumaralgorithm.A model to detect intrusions in telephone networks was proposed by Scott 2001and for modeling web click data by Ihler et al. 2006. Both papers follow a technique in which they assume that the normal behavior in a timeseries is generatedby a nonstationary Poisson process while the anomalies are generated by a homogenous Poisson process. The transition between normal and anomalous behavior ismodeled using a Markov process. The proposed techniques in each of these papersuse Markov Chain Monte Carlo MCMC estimation technique to estimate the parameters for these processes. For testing, a time series is modeled using this processand the time instances for which the anomalous behavior was active are consideredas anomalies.Bipartite graph structure in P2P networks has been used to first identify a neighborhood for any node in the graph Sun et al. 2005, and then detecting the relevanceof that node within the neighborhood. A node with a low relevance score is treatedas an anomaly. The authors also propose an approximate technique where thegraph is first partitioned into nonoverlapping subgraphs using graph partitioningalgorithm such as METIS Karypis and Kumar 1998. The neighborhood of a nodeis then computed within its partition.Computational ComplexityThe computational complexity of the training phase in reduction based contextual anomaly detection techniques depends on the reduction technique as well asthe point anomaly detection technique used within each context. While segmentingpartitioning techniques have a fast reduction step, techniques that use clustering, or mixture of models estimation, are relatively slower. Since the reductionsimplifies the anomaly detection problem, fast point anomaly detection techniquescan be used to speed up the second step. The testing phase is relatively expensivesince for each test instance, its context is determined, and then an anomaly labelor score is assigned using a point anomaly detection technique.The computational complexity of training phase in contextual anomaly detectiontechniques that utilize the structure in the data to build models, is typically higherthat of techniques that reduce the problem to point anomaly detection. An advantage for such techniques is the testing phase is relatively fast, since each instanceis just compared to the single model and assigned an anomaly score or an anomalylabel.Advantages and Disadvantages of Contextual Anomaly Detection TechniquesThe key advantage of contextual anomaly detection techniques is that they allow anatural definition of an anomaly in many real life applications where data instancestend to be similar within a context. Such techniques are able to detect anomalies that might not be detected by point anomaly detection techniques that take aglobal view of the data.The disadvantage of contextual anomaly detection techniques is that they areapplicable only when a context can be defined.To Appear in ACM Computing Surveys, 09 2009.Anomaly Detection  A Survey  4711. HANDLING COLLECTIVE ANOMALIESThis section discusses the anomaly detection techniques which focus on detectingcollective anomalies. As mentioned earlier, collective anomalies are a subset ofinstances that occur together as a collection and whose occurrence is not normalwith respect to a normal behavior. The individual instances belonging to thiscollection are not necessarily anomalies by themselves, but it is their cooccurrencein a particular form that makes them anomalies. Collective anomaly detectionproblem is more challenging than point and contextual anomaly detection becauseit involves exploring structure in the data for anomalous regions.A primary data requirement for collective anomaly detection, is the presenceof relationship between data instances. Three types of relations that have beenexploited most frequently are sequential, spatial, and graphsSequential Anomaly Detection Techniques These techniques work with sequential data and find subsequences as anomalies also referred to as sequential anomalies. Typical data sets include event sequence data, such as system call dataForrest et al. 1999 or numerical timeseries data Chan and Mahoney 2005.Spatial Anomaly Detection Techniques These techniques work with spatial dataand find connected subregions within the data as anomalies also referred to asspatial anomalies. Anomaly detection techniques have been applied to multispectral imagery data Hazel 2000.Graph Anomaly Detection Techniques These techniques work with graph dataand find connected subgraphs within the data as anomalies also referred to asgraph anomalies. Anomaly detection techniques have been applied to graphdata Noble and Cook 2003.Substantial research has been done in the field of sequential anomaly detectionthis can be attributed to the existence of sequential data in several importantapplication domains. Spatial anomaly detection has been explored primarily inthe domain of image processing. The following subsections discuss each of thesecategories in detail.11.1 Handling Sequential AnomaliesAs mentioned earlier, collective anomaly detection in sequence data involves detecting sequences that are anomalous with respect to a definition of normal behavior.Sequence data is very common in a wide range of domains where a natural ordering is imposed on data instances by either time or position. In anomaly detectionliterature, two types of sequences are dealt with. First type of sequences are symbolic, such as a sequence of operating system calls, or a sequence of biologicalentities. Second type of sequences are continuous, or time series. Sequences canalso be univariate, in which each event in the sequence is a univariate observation,or multivariate, in which each event in the sequence is a multivariate observation.The anomaly detection problem for sequences can be defined in different waysand are discussed below.11.1.1 Detecting anomalous sequence in a set of sequences. The objective of thetechniques in this category is to detect anomalous sequences from a given set ofTo Appear in ACM Computing Surveys, 09 2009.48  Chandola, Banerjee and Kumarsequences. Such techniques can either operate in a semisupervised mode, or anunsupervised mode.Key challenges faced by techniques in this category areThe sequences might not be of equal length.The test sequences may not be aligned with each other or with normal sequences.For example, the first event in one sequence might correspond to the third eventin another sequence. Comparing such sequences is a fundamental problem withbiological sequences Gusfield 1997 where different sequence alignment and sequence matching techniques are explored.Techniques addressing this problem follow one of the following two approachesReduction to Point Anomaly Detection ProblemA general approach to solve the above problem would be to transform the sequencesto a finite feature space and then use a point anomaly detection technique in thenew space to detect anomalies.Certain techniques assume that all sequences are of equal lengths. Thus theytreat each sequence as a vector of attributes and employ a point anomaly detection technique to detect anomalies. For example, if a data set contains length 10sequences, they can be treated as data records with 10 features. A similarity or distance measure can be defined between a pair of sequences and any point anomalydetection technique can be applied to such data sets. This approach has beenadopted for timeseries data sets Caudell and Newman 1993 Blender et al. 1997.In the former paper, the authors apply ART Adaptive Resonance Theory neuralnetworks based anomaly detection technique to detect anomalies in a timeseriesdata set, while the latter paper uses a clustering based anomaly detection techniqueto identify cyclone regimes anomalies in weather data.As mentioned earlier, the given sequences may not be of equal length. Certaintechniques address this issue by transforming each sequence into a record of equalnumber of attributes. A transformation technique has been proposed for multipletimeseries data Chan and Mahoney 2005, known as Box Modeling. In a boxmodel, for each timeseries, each instance of this timeseries is assigned to a boxdepending on its value. These boxes are then treated as features the number ofboxes is the number of features in the transformed feature space. The authors thenapply point anomaly detection techniques  a Euclidean distance based techniqueand a classification based technique using RIPPER to detect anomalous time seriesin the data.Several techniques address the issue of unequal length of sequences by using asimilarity or distance metric that can compute similarity or distance between twounequal length sequences. For example, Budalakoti et al. 2006 employ length oflongest common subsequence as the similarity measure for symbolic sequences. Theauthors subsequently apply a clustering based anomaly detection technique, usingthis similarity measure.11.1.1.1 Modeling Sequences. The transformations discussed in the previoussection are appropriate when all the sequences are properly aligned. Often times thealignment assumption becomes too prohibitive. Research dealing with system calldata, biological data, etc., explore other alternatives to detect collective anomalies.To Appear in ACM Computing Surveys, 09 2009.Anomaly Detection  A Survey  49Such techniques operate in a semisupervised mode, and hence require a trainingset of normal sequences.Sequential association modeling has been used to generate sequential rules fromsequences Teng et al. 1990. The authors use an approach called timebased inductive learning to generate rules from the set of normal sequences. The test sequenceis compared to these rules and is declared an anomaly if it contains patterns forwhich no rules have been generated.Markovian modeling of sequences has been the most popular approach in thiscategory. The modeling techniques used in this category range from Finite StateAutomatons FSA to Markov models. FSA have been used to detect anomalies innetwork protocol data Sekar et al. 2002 Sekar et al. 1999. Anomalies are detectedwhen a given sequence of events does not result in reaching one of the final states.The authors also apply their technique to operating system call intrusion detectionSekar et al. 2001.Ye 2004 proposes a simple 1order markov chain modeling approach to detectif a given sequence S is an anomaly. The author determines the likelihood of S,P S using the following equationP S  qS1St2pSt1Stwhere qS1 is the probability of observing the symbol S1 in the training set andpSt1St is the probability of observing the symbol St after symbol St1 in thetraining set. The inverse of P S is the anomaly score for S. The drawback of thistechnique is that single order markov chain cannot model higher order dependenciesin the sequences.Forrest et al. 1999 propose a Hidden Markov Model HMM based techniqueto detect anomalous program traces in operating system call data. The authorstrain an HMM using the training sequences. The authors propose two testingtechniques. In the first technique they compute the likelihood of a test sequenceS to be generated by the learnt HMM using the Viterbi algorithm. The secondtechnique is to use the underlying Finite State Automaton FSA of the HMM.The state transitions and the outputs made by the HMM to produce the testsequence are recorded. The authors count the number of times the HMM had tomake an unlikely state transition or output an unlikely symbol using a userdefinedthreshold as mismatches. The total number of mismatches denote the anomalyscore for that sequence.A Probabilistic Suffix Trees PST is another modeling tool which has been applied to detect collective anomalies in sequential databases. A PST is a compactrepresentation of a variable order markov chain. Yang and Wang 2003 use PSTto cluster sequences and detect anomalous sequences as a byproduct. Similarly,Smyth 1997 and Cadez et al. 2000 use HMMs to cluster the set of sequences anddetect any sequences which do not belong to any cluster as anomalies.Another modeling tool used for sequential anomaly detection is Sparse MarkovTrees SMT, which is similar to a PST with the difference that it allows wild cardsymbols within a path. This technique has been used by Eskin et al. 2001, whotrain a mixture of SMT using the training set. Each SMT has a different location ofTo Appear in ACM Computing Surveys, 09 2009.50  Chandola, Banerjee and Kumarwildcards. Testing phase involves predicting the probability P SnSn1 . . . S1 using the best SMT from the mixture. If this probability is below a certain threshold,the test sequence is declared as an anomaly.11.1.2 Detecting anomalous subsequences in a long sequence. The objective oftechniques belonging to this category is to detect a subsequence within a givensequence which is anomalous with respect to the rest of the sequence. Such anomalous subsequences have also been referred as discords Bu et al. 2007 Fu et al. 2006Keogh et al. 2005 Yankov et al. 2007.This problem formulation occurs in event and timeseries data sets where thedata is in the form of a long sequence and contains regions that are anomalous.The techniques that address this problem, typically work in an unsupervised mode,due to the lack of any training data. The underlying assumption is that the normalbehavior of the timeseries follows a defined pattern. A subsequence within thelong sequence which does not conform to this pattern is an anomaly.Key challenges faced by techniques in this category areThe length of the anomalous subsequence to be detected is not generally defined.A long sequence could contain anomalous regions of variable lengths. Thus fixedlength segmenting of the sequence is often not useful.Since the input sequence contains anomalous regions, it becomes challenging tocreate a robust model of normalcy.Chakrabarti et al. 1998 propose a surprise detection technique in market baskettransactions. The data is a sequence of itemsets, ordered by time. The authorspropose to segment the sequence of itemsets such that the sum of number of bitsrequire to encode each segment using Shannons classical Information Theorem isminimized. The authors show that an optimal solution exists to find such segmentation. The segments which require highest number of bits for encoding are treatedas anomalies.Keogh et al. 2004 propose an algorithm called Window Comparison AnomalyDetection WCAD, where the authors extract subsequences out of a given sequenceof continuous observations using a sliding window. The authors compare each subsequence with the entire sequence using a compression based dissimilarity measure.The anomaly score of each subsequence is its dissimilarity with the entire sequence.Keogh et al 2005 2006 propose a related technique HOT SAX to solve theabove problem for continuous timeseries. The basic approach followed by theauthors is to extract subsequences out of the given sequence using sliding window,and then computing the distance of each subsequence to its closest nonoverlappingsubsequence within the original sequence. The anomaly score of a subsequenceis proportional to its distance from its nearest neighbors. Distance between twosequences is measured using Euclidean measure. Similar approach is also appliedto the domain of medical data by Lin et al. 2005. The same authors propose theuse of Haar Wavelet based transformation to make the previous technique moreefficient Fu et al. 2006 Bu et al. 2007.Maximum Entropy Markov Models Maxent McCallum et al. 2000 Pavlov andPennock 2002 Pavlov 2003 as well as Conditional Random Fields CRF Laffertyet al. 2001, have been used for segmenting text data. The problem formulationTo Appear in ACM Computing Surveys, 09 2009.Anomaly Detection  A Survey  51there is to predict the most likely state sequence for a given observation sequence.Any anomalous segment within the observation sequence will have a low conditionalprobability for any state sequence.11.1.3 Determining if the frequency of a query pattern in a given sequence isanomalous w.r.t its expected frequency. Such formulation of the anomaly detectionproblem is motivated from the case vs control type of data Helman and Bhangoo1997 Gwadera et al. 2005b 2004. The idea is to detect patterns whose occurrencein a given test data set case is different from its occurrence in a normal data setcontrol. Keogh et al. 2002 extract substrings from a given string of alphabetsusing a sliding window. For each of these substrings they determine if this substringis anomalous with respect to a normal database of strings. The authors use suffixtrees to estimate the expected frequency of a substring in the normal database ofstrings. In a similar approach Gwadera et al. 2005a, the authors use InterpolatedMarkov Models IMM to estimate the expected frequency.11.2 Handling Spatial Anomaliescollective anomaly detection in spatial data involves finding subgraphs or subcomponents in the data that are anomalous. A limited amount of research has beendone in this category so we will discuss them individually.Hazel 2000 propose a technique to detect regions in an image that are anomalouswith respect to rest of the image. The proposed technique makes use of MultivariateGaussian Random Markov Fields MGMRF to segment a given image. The authors make an assumption that each pixel belonging to an anomalous region of theimage is also a contextual anomaly within its segment. These pixels are detected ascontextual anomalies with respect to the segments by estimating the conditionalprobability of each pixel, and then connected using a spatial structure available,to find the collective anomalies.Anomaly detection for graphs has been explored in application domains wherethe data can be modeled as graphs. Noble and Cook 2003 address two distinctcollective anomaly detection problems for graph data. The first problem involvesdetecting anomalous subgraphs in a given large graph. The authors use a bottomup subgraph enumeration technique and analyze the frequency of a subgraph in thegiven graph to determine if it is an anomaly or not. The size of the subgraph is alsotaken into account, since a large subgraph such as the graph itself is bound tooccur very rarely in the graph while a small subgraph such as an individual nodewill be more frequent. The second problem involves detecting if a given subgraphis an anomaly with respect to a large graph. The authors measure the regularity orentropy of the subgraph in the context of the entire graph to determine its anomalyscore.12. RELATIVE STRENGTHS AND WEAKNESSES OF ANOMALY DETECTIONTECHNIQUESEach of the large number of anomaly detection techniques discussed in previoussections have their unique strengths and weaknesses. It is important to know whichanomaly detection technique is best suited for a given anomaly detection problem.Given the complexity of the problem space, it is not feasible to provide such anTo Appear in ACM Computing Surveys, 09 2009.52  Chandola, Banerjee and Kumarunderstanding for every anomaly detection problem. In this section we analyze therelative strengths and weakenesses of different categories of techniques for a fewsimple problem settings.a Data Set 1 b Data Set 2 c Data Set 3Fig. 10. 2D data sets. Normal instances are shown as circles and anomalies are shown as squares.For example, let us consider the following anomaly detection problem. The inputis 2D continuous data Figure 10a. The normal data instances are generated froma Gaussian distribution and are located in a tight cluster in the 2D space. Theanomalies are a very few instances generated from another Gaussian distributionwhose mean is very far from the first distribution. A representative training dataset that contains instances from the normal data set is also available. Thus theassumptions made by techniques in Sections 49 hold for this data set, and henceany anomaly detection techniques belonging to these categories will detect theanomalies in such a scenario.Now let us consider another 2D data set Figure 10b. Let the normal instancesbe such that they are generated by a large number of different Gaussian distributions with means arranged on a circle and very low variance. Thus the normal datawill be a set of tight clusters arranged on a circle. A oneclass classification basedtechnique might learn a circular boundary around the entire data set and hence willnot be able to detect the anomalies that lie within the circle of clusters. On theother hand if each cluster was labeled as a different class, a multiclass classificationbased technique might be able to learn boundaries around each cluster, and hencebe able to detect the anomalies in the center. A statistical technique that uses amixture model approach to model the data, may be able to detect the anomalies.Similarly, clustering based and nearest neighbor based techniques will be able todetect the anomalies since they are far from all other instances. In a similar example Figure 10c, if the anomalous instances form a tight cluster of significantsize at the center of the circle, both clustering based and nearest neighbor basedtechniques will treat these instances as normal, thus exhibiting poor performance.For more complex data sets, different types of techniques face different challenges.Nearest neighbor and clustering based techniques suffer when the number of dimensions is high because the distance measures in high number of dimensions are notable to differentiate between normal and anomalous instances. Spectral techniquesexplicitly address high dimensionality problem by mapping data to a lower dimensional projection. But their performance is highly dependent on the assumptionthat the normal instances and anomalies are distinguishable in the projected space.Classification based techniques can be a better choice in such scenario. But to beTo Appear in ACM Computing Surveys, 09 2009.Anomaly Detection  A Survey  53most effective, classification based techniques require labels for both normal andanomalous instances, which are not often available. Even if the labels for bothnormal and anomalous instances are available, the imbalance in the distribution ofthe two labels often makes learning a classifier quite challenging. Semisupervisednearest neighbor and clustering techniques, that only use the normal labels, canoften be more effective than the classification based techniques. Statistical techniques, though unsupervised, are effective only when the dimensionality of datais low and statistical assumptions hold. Information theoretic techniques requirea measure that is sensitive enough to detect the effects of even a single anomaly.Otherwise, such techniques can detect anomalies only when there are significantlyenough number of anomalies.Nearest neighbor and clustering based techniques require distance computationbetween a pair of data instances. Thus, such techniques assume that the distancemeasure can discriminate between the anomalies and normal instances well enough.In situations where identifying a good distance measure is difficult, classificationbased or statistical techniques might be a better choice.The computational complexity of an anomaly detection technique is a key aspect,especially when the technique is applied to a real domain. While classificationbased, clustering based, and statistical techniques have expensive training times,testing is usually fast. Often this is acceptable, since models can be trained in an offline fashion while testing is required to be in real time. In contrast, techniques suchas nearest neighbor based, information theoretic, and spectral techniques which donot have a training phase, have expensive testing phase which can be a limitationin a real setting.Anomaly detection techniques typically assume that anomalies in data are rarewhen compared to normal instances. Though this assumption is generally true,anomalies are not always rare. For example, when dealing with worm detection incomputer networks, the anomalous worm traffic is actually more frequent thanthe normal traffic. Unsupervised techniques are not suited for such bulk anomalydetection. Techniques operating in supervised or semisupervised modes can beapplied to detect bulk anomalies Sun et al. 2007 Soule et al. 2005.13. CONCLUDING REMARKS AND FUTURE WORKIn this survey we have discussed different ways in which the problem of anomalydetection has been formulated in literature, and have attempted to provide anoverview of the huge literature on various techniques. For each category of anomalydetection techniques, we have identified a unique assumption regarding the notionof normal and anomalous data. When applying a given technique to a particulardomain, these assumptions can be used as guidelines to assess the effectiveness ofthe technique in that domain. Ideally, a comprehensive survey on anomaly detection should allow a reader to not only understand the motivation behind using aparticular anomaly detection technique, but also provide a comparative analysisof various techniques. But the current research has been done in an unstructuredfashion, without relying on a unified notion of anomalies, which makes the job ofproviding a theoretical understanding of the anomaly detection problem very difficult. A possible future work would be to unify the assumptions made by differenttechniques regarding the normal and anomalous behavior into a statistical or maTo Appear in ACM Computing Surveys, 09 2009.54  Chandola, Banerjee and Kumarchine learning framework. A limited attempt in this direction is provided by Knorrand Ng 1997, where the authors show the relation between distance based andstatistical anomalies for twodimensional data sets.There are several promising directions for further research in anomaly detection.Contextual and collective anomaly detection techniques are beginning to find increasing applicability in several domains and there is much scope for developmentof new techniques in this area. The presence of data across different distributedlocations has motivated the need for distributed anomaly detection techniques Zimmermann and Mohay 2006. While such techniques process information availableat multiple sites, they often have to simultaneously protect the information presentin each site, thereby requiring privacy preserving anomaly detection techniquesVaidya and Clifton 2004. With the emergence of sensor networks, processing dataas it arrives has become a necessity. Many techniques discussed in this survey require the entire test data before detecting anomalies. Recently, techniques havebeen proposed that can operate in an online fashion Pokrajac et al. 2007 suchtechniques not only assign an anomaly score to a test instance as it arrives, but alsoincrementally update the model. Another upcoming area where anomaly detectionis finding more and more applicability is in complex systems. An example of suchsystem would be an aircraft system with multiple components. Anomaly detection in such systems involves modeling the interaction between various componentsBronstein et al. 2001.ACKNOWLEDGMENTSThe authors thank Shyam Boriah and Gang Fang for extensive comments on thefinal draft of the paper.This work was supported by NASA under award NNX08AC36A, NSF grant number CNS0551551, NSF ITR Grant ACI0325949, NSF IIS0713227 and NSF GrantIIS0308264. Access to computing facilities was provided by the Digital TechnologyConsortium.REFERENCESAbe, N., Zadrozny, B., and Langford, J. 2006. Outlier detection by active learning. InProceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery andData Mining. ACM Press, New York, NY, USA, 504509.Abraham, B. and Box, G. E. P. 1979. Bayesian analysis of some outlier problems in time series.Biometrika 66, 2, 229236.Abraham, B. and Chuang, A. 1989. Outlier detection and time series modeling. Technometrics 31, 2, 241248.Addison, J., Wermter, S., and MacIntyre, J. 1999. Effectiveness of feature extraction in neuralnetwork architectures for novelty detection. In Proceedings of the 9th International Conferenceon Artificial Neural Networks. Vol. 2. 976981.Aeyels, D. 1991. On the dynamic behaviour of the novelty detector and the novelty filter. InAnalysis of Controlled Dynamical Systems Progress in Systems and Control Theory, B. Bonnard, B. Bride, J. Gauthier, and I. Kupka, Eds. Vol. 8. Springer, Berlin, 110.Agarwal, D. 2005. An empirical bayes approach to detect anomalies in dynamic multidimensional arrays. In Proceedings of the 5th IEEE International Conference on Data Mining. IEEEComputer Society, Washington, DC, USA, 2633.Agarwal, D. 2006. Detecting anomalies in crossclassified streams a bayesian approach. Knowledge and Information Systems 11, 1, 2944.To Appear in ACM Computing Surveys, 09 2009.Anomaly Detection  A Survey  55Aggarwal, C. 2005. On abnormality detection in spuriously populated data streams. In Proceedings of 5th SIAM Data Mining. 8091.Aggarwal, C. and Yu, P. 2001. Outlier detection for high dimensional data. In Proceedings ofthe ACM SIGMOD International Conference on Management of Data. ACM Press, 3746.Aggarwal, C. C. and Yu, P. S. 2008. Outlier detection with uncertain data. In SDM. 483493.Agovic, A., Banerjee, A., Ganguly, A. R., and Protopopescu, V. 2007. Anomaly detectionin transportation corridors using manifold embedding. In First International Workshop onKnowledge Discovery from Sensor Data. ACM Press.Agrawal, R. and Srikant, R. 1995. Mining sequential patterns. In Proceedings of the 11thInternational Conference on Data Engineering. IEEE Computer Society, Washington, DC,USA, 314.Agyemang, M., Barker, K., and Alhajj, R. 2006. A comprehensive survey of numeric andsymbolic outlier mining techniques. Intelligent Data Analysis 10, 6, 521538.Albrecht, S., Busch, J., Kloppenburg, M., Metze, F., and Tavan, P. 2000. Generalized radialbasis function networks for classification and novelty detection selforganization of optionalbayesian decision. Neural Networks 13, 10, 10751093.Aleskerov, E., Freisleben, B., and Rao, B. 1997. Cardwatch A neural network based databasemining system for credit card fraud detection. In Proceedings of IEEE Computational Intelligence for Financial Engineering. 220226.Allan, J., Carbonell, J., Doddington, G., Yamron, J., and Yang, Y. 1998. Topic detection and tracking pilot study. In Proceedings of DARPA Broadcast News Transcription andUnderstanding Workshop. 194218.Anderson, Lunt, Javitz, Tamaru, A., and Valdes, A. 1995. Detecting unusual program behavior using the statistical components of NIDES. Tech. Rep. SRICSL9506, Computer ScienceLaboratory, SRI International. may.Anderson, D., Frivold, T., Tamaru, A., and Valdes, A. 1994. Nextgeneration intrusiondetection expert system nides, software users manual, betaupdate release. Tech. Rep. SRICSL9507, Computer Science Laboratory, SRI International. may.Ando, S. 2007. Clustering needles in a haystack An information theoretic analysis of minorityand outlier detection. In Proceedings of 7th International Conference on Data Mining. 1322.Angiulli, F. and Pizzuti, C. 2002. Fast outlier detection in high dimensional spaces. In Proceedings of the 6th European Conference on Principles of Data Mining and Knowledge Discovery.SpringerVerlag, 1526.Anscombe, F. J. and Guttman, I. 1960. Rejection of outliers. Technometrics 2, 2, 123147.Arning, A., Agrawal, R., and Raghavan, P. 1996. A linear method for deviation detection inlarge databases. In Proceedings of 2nd International Conference of Knowledge Discovery andData Mining. 164169.Augusteijn, M. and Folkert, B. 2002. Neural network classification and novelty detection.International Journal on Remote Sensing 23, 14, 28912902.Bakar, Z., Mohemad, R., Ahmad, A., and Deris, M. 2006. A comparative study for outlier detection techniques in data mining. Cybernetics and Intelligent Systems, 2006 IEEE Conferenceon, 16.Baker, D., Hofmann, T., McCallum, A., and Yang, Y. 1999. A hierarchical probabilisticmodel for novelty detection in text. In Proceedings of International Conference on MachineLearning.Barbara, D., Couto, J., Jajodia, S., and Wu, N. 2001a. Adam a testbed for exploring theuse of data mining in intrusion detection. SIGMOD Rec. 30, 4, 1524.Barbara, D., Couto, J., Jajodia, S., and Wu, N. 2001b. Detecting novel network intrusionsusing bayes estimators. In Proceedings of the First SIAM International Conference on DataMining.Barbara, D., Li, Y., Couto, J., Lin, J.L., and Jajodia, S. 2003. Bootstrapping a data miningintrusion detection system. In Proceedings of the 2003 ACM symposium on Applied computing.ACM Press, 421425.To Appear in ACM Computing Surveys, 09 2009.56  Chandola, Banerjee and KumarBarnett, V. 1976. The ordering of multivariate data with discussion. Journal of the RoyalStatistical Society. Series A 139, 318354.Barnett, V. and Lewis, T. 1994. Outliers in statistical data. John Wiley and sons.Barson, P., Davey, N., Field, S. D. H., Frank, R. J., and McAskie, G. 1996. The detectionof fraud in mobile phone networks. Neural Network World 6, 4.Basu, S., Bilenko, M., and Mooney, R. J. 2004. A probabilistic framework for semisupervisedclustering. In Proceedings of the tenth ACM SIGKDD international conference on Knowledgediscovery and data mining. ACM Press, New York, NY, USA, 5968.Basu, S. and Meckesheimer, M. 2007. Automatic outlier detection for time series an applicationto sensor data. Knowledge and Information Systems 11, 2 February, 137154.Bay, S. D. and Schwabacher, M. 2003. Mining distancebased outliers in near linear timewith randomization and a simple pruning rule. In Proceedings of the ninth ACM SIGKDDinternational conference on Knowledge discovery and data mining. ACM Press, 2938.Beckman, R. J. and Cook, R. D. 1983. Outlier...s. Technometrics 25, 2, 119149.Bejerano, G. and Yona, G. 2001. Variations on probabilistic suffix trees statistical modelingand prediction of protein families . Bioinformatics 17, 1, 2343.Bentley, J. L. 1975. Multidimensional binary search trees used for associative searching. Communications of the ACM 18, 9, 509517.Bianco, A. M., Ben, M. G., Martinez, E. J., and Yohai, V. J. 2001. Outlier detection inregression models with arima errors using robust estimates. Journal of Forecasting 20, 8,565579.Bishop, C. 1994. Novelty detection and neural network validation. In Proceedings of IEEEVision, Image and Signal Processing. Vol. 141. 217222.Blender, R., Fraedrich, K., and Lunkeit, F. 1997. Identification of cyclonetrack regimes inthe north atlantic. Quarterly Journal of the Royal Meteorological Society 123, 539, 727741.Bolton, R. and Hand, D. 1999. Unsupervised profiling methods for fraud detection. In CreditScoring and Credit Control VII.Boriah, S., Chandola, V., and Kumar, V. 2008. Similarity measures for categorical data Acomparative evaluation. In Proceedings of the eighth SIAM International Conference on DataMining. 243254.Borisyuk, R., Denham, M., Hoppensteadt, F., Kazanovich, Y., and Vinogradova, O. 2000.An oscillatory neural network model of sparse distributed memory and novelty detection.Biosystems 58, 265272.Box, G. E. P. and Tiao, G. C. 1968. Bayesian analysis of some outlier problems.Biometrika 55, 1, 119129.Branch, J., Szymanski, B., Giannella, C., Wolff, R., and Kargupta, H. 2006. Innetworkoutlier detection in wireless sensor networks. In 26th IEEE International Conference on Distributed Computing Systems.Brause, R., Langsdorf, T., and Hepp, M. 1999. Neural data mining for credit card fraud detection. In Proceedings of IEEE International Conference on Tools with Artificial Intelligence.103106.Breunig, M. M., Kriegel, H.P., Ng, R. T., and Sander, J. 1999. Opticsof Identifying localoutliers. In Proceedings of the Third European Conference on Principles of Data Mining andKnowledge Discovery. SpringerVerlag, 262270.Breunig, M. M., Kriegel, H.P., Ng, R. T., and Sander, J. 2000. Lof identifying densitybasedlocal outliers. In Proceedings of 2000 ACM SIGMOD International Conference on Managementof Data. ACM Press, 93104.Brito, M. R., Chavez, E. L., Quiroz, A. J., and Yukich, J. E. 1997. Connectivity of themutual knearestneighbor graph in clustering and outlier detection. Statistics and ProbabilityLetters 35, 1, 3342.Brockett, P. L., Xia, X., and Derrig, R. A. 1998. Using kohonens selforganizing feature mapto uncover automobile bodily injury claims fraud. Journal of Risk and Insurance 65, 2 June,245274.To Appear in ACM Computing Surveys, 09 2009.Anomaly Detection  A Survey  57Bronstein, A., Das, J., Duro, M., Friedrich, R., Kleyner, G., Mueller, M., Singhal, S.,and Cohen, I. 2001. Bayesian networks for detecting anomalies in internetbased services. InInternational Symposium on Integrated Network Management,.Brotherton, T. and Johnson, T. 2001. Anomaly detection for advance military aircraft usingneural networks. In Proceedings of 2001 IEEE Aerospace Conference.Brotherton, T., Johnson, T., and Chadderdon, G. 1998. Classification and novelty detection using linear models and a class dependent elliptical basis function neural network. InProceedings of the IJCNN Conference. Anchorage AL.Bu, Y., Leung, T.W., Fu, A., Keogh, E., Pei, J., and Meshkin, S. 2007. Wat Finding topkdiscords in time series database. In Proceedings of 7th SIAM International Conference on DataMining.Budalakoti, S., Srivastava, A., Akella, R., and Turkov, E. 2006. Anomaly detection in largesets of highdimensional symbol sequences. Tech. Rep. NASA TM2006214553, NASA AmesResearch Center.Byers, S. D. and Raftery, A. E. 1998. Nearest neighbor clutter removal for estimating featuresin spatial point processes. Journal of the American Statistical Association 93, 577584.Byungho, H. and Sungzoon, C. 1999. Characteristics of autoassociative mlp as a novelty detector. In Proceedings of IEEE International Joint Conference on Neural Networks. Vol. 5.30863091.Cabrera, J. B. D., Lewis, L., and Mehra, R. K. 2001. Detection and classification of intrusionsand faults using sequences of system calls. SIGMOD Records 30, 4, 2534.Cadez, I., Heckerman, D., Meek, C., Smyth, P., and White, S. 2000. Visualization of navigation patterns on a web site using modelbased clustering. In Proceedings of the sixth ACMSIGKDD international conference on Knowledge discovery and data mining. ACM Press, NewYork, NY, USA, 280284.Campbell, C. and Bennett, K. 2001. A linear programming approach to novelty detection. InProceedings of Advances in Neural Information Processing. Vol. 14. Cambridge Press.Caudell, T. and Newman, D. 1993. An adaptive resonance architecture to define normality anddetect novelties in time series and databases. In IEEE World Congress on Neural Networks.IEEE, Portland, OR, 166176.Chakrabarti, S., Sarawagi, S., and Dom, B. 1998. Mining surprising patterns using temporaldescription length. In Proceedings of the 24rd International Conference on Very Large DataBases. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 606617.Chan, P. K. and Mahoney, M. V. 2005. Modeling multiple time series for anomaly detection.In Proceedings of the Fifth IEEE International Conference on Data Mining. IEEE ComputerSociety, Washington, DC, USA, 9097.Chandola, V., Boriah, S., and Kumar, V. 2008. Understanding categorical similarity measuresfor outlier detection. Tech. Rep. 08008, University of Minnesota. Mar.Chandola, V., Eilertson, E., Ertoz, L., Simon, G., and Kumar, V. 2006. Data mining forcyber security. In Data Warehousing and Data Mining Techniques for Computer Security,A. Singhal, Ed. Springer.Chatzigiannakis, V., Papavassiliou, S., Grammatikou, M., and Maglaris, B. 2006. Hierarchical anomaly detection in distributed largescale sensor networks. In ISCC 06 Proceedingsof the 11th IEEE Symposium on Computers and Communications. IEEE Computer Society,Washington, DC, USA, 761767.Chaudhary, A., Szalay, A. S., and Moore, A. W. 2002. Very fast outlier detection in largemultidimensional data sets. In Proceedings of ACM SIGMOD Workshop in Research Issues inData Mining and Knowledge Discovery DMKD. ACM Press.Chawla, N. V., Japkowicz, N., and Kotcz, A. 2004. Editorial special issue on learning fromimbalanced data sets. SIGKDD Explorations 6, 1, 16.Chen, D., Shao, X., Hu, B., and Su, Q. 2005. Simultaneous wavelength selection and outlierdetection in multivariate regression of nearinfrared spectra. Analytical Sciences 21, 2, 161167.Chiu, A. and chee Fu, A. W. 2003. Enhancements on local outlier detection. In Proceedings of7th International Database Engineering and Applications Symposium. 298307.To Appear in ACM Computing Surveys, 09 2009.58  Chandola, Banerjee and KumarChow, C. and Yeung, D.Y. 2002. Parzenwindow network intrusion detectors. In Proceedingsof the 16th International Conference on Pattern Recognition. Vol. 4. IEEE Computer Society,Washington, DC, USA, 40385.Cox, K. C., Eick, S. G., Wills, G. J., and Brachman, R. J. 1997. Visual data mining Recognizing telephone calling fraud. Journal of Data Mining and Knowledge Discovery 1, 2,225231.Crook, P. and Hayes, G. 2001. A robot implementation of a biologically inspired method fornovelty detection. In Proceedings of Towards Intelligent Mobile Robots Conference. Manchester,UK.Crook, P. A., Marsland, S., Hayes, G., and Nehmzow, U. 2002. A tale of two filters  onlinenovelty detection. In Proceedings of International Conference on Robotics and Automation.38943899.Cun, Y. L., Boser, B., Denker, J. S., Howard, R. E., Habbard, W., Jackel, L. D., and Henderson, D. 1990. Handwritten digit recognition with a backpropagation network. Advancesin neural information processing systems, 396404.Das, K. and Schneider, J. 2007. Detecting anomalous records in categorical datasets. InProceedings of the 13th ACM SIGKDD international conference on Knowledge discovery anddata mining. ACM Press.Dasgupta, D. and Majumdar, N. 2002. Anomaly detection in multidimensional data using negative selection algorithm. In Proceedings of the IEEE Conference on Evolutionary Computation.Hawaii, 10391044.Dasgupta, D. and Nino, F. 2000. A comparison of negative and positive selection algorithmsin novel pattern detection. In Proceedings of the IEEE International Conference on Systems,Man, and Cybernetics. Vol. 1. Nashville, TN, 125130.Davy, M. and Godsill, S. 2002. Detection of abrupt spectral changes using support vectormachines. an application to audio signal segmentation. In Proceedings of IEEE InternationalConference on Acoustics, Speech, and Signal Processing. Orlando, USA.Debar, H., Dacier, M., Nassehi, M., and Wespi, A. 1998. Fixed vs. variablelength patternsfor detecting suspicious process behavior. In Proceedings of the 5th European Symposium onResearch in Computer Security. SpringerVerlag, London, UK, 115.Denning, D. E. 1987. An intrusion detection model. IEEE Transactions of Software Engineering 13, 2, 222232.Desforges, M., Jacob, P., and Cooper, J. 1998. Applications of probability density estimationto the detection of abnormal conditions in engineering. In Proceedings of Institute of MechanicalEngineers. Vol. 212. 687703.Diaz, I. and Hollmen, J. 2002. Residual generation and visualization for understanding novelprocess conditions. In Proceedings of IEEE International Joint Conference on Neural Networks.IEEE, Honolulu, HI, 20702075.Diehl, C. and Hampshire, J. 2002. Realtime object classification and novelty detection forcollaborative video surveillance. In Proceedings of IEEE International Joint Conference onNeural Networks. IEEE, Honolulu, HI.Donoho, S. 2004. Early detection of insider trading in option markets. In Proceedings of thetenth ACM SIGKDD international conference on Knowledge discovery and data mining. ACMPress, New York, NY, USA, 420429.Dorronsoro, J. R., Ginel, F., Sanchez, C., and Cruz, C. S. 1997. Neural fraud detection incredit card operations. IEEE Transactions On Neural Networks 8, 4 July, 827834.Du, W., Fang, L., and Peng, N. 2006. Lad localization anomaly detection for wireless sensornetworks. J. Parallel Distrib. Comput. 66, 7, 874886.Duda, R. O., Hart, P. E., and Stork, D. G. 2000. Pattern Classification 2nd Edition. WileyInterscience.Dutta, H., Giannella, C., Borne, K., and Kargupta, H. 2007. Distributed topk outlier detection in astronomy catalogs using the demac system. In Proceedings of 7th SIAM InternationalConference on Data Mining.Edgeworth, F. Y. 1887. On discordant observations. Philosophical Magazine 23, 5, 364375.To Appear in ACM Computing Surveys, 09 2009.Anomaly Detection  A Survey  59Emamian, V., Kaveh, M., and Tewfik, A. 2000. Robust clustering of acoustic emission signalsusing the kohonen network. In Proceedings of the IEEE International Conference of Acoustics,Speech and Signal Processing. IEEE Computer Society.Endler, D. 1998. Intrusion detection Applying machine learning to solaris audit data. InProceedings of the 14th Annual Computer Security Applications Conference. IEEE ComputerSociety, 268.Ertoz, L., Eilertson, E., Lazarevic, A., Tan, P.N., Kumar, V., Srivastava, J., and Dokas,P. 2004. MINDS  Minnesota Intrusion Detection System. In Data Mining  Next GenerationChallenges and Future Directions. MIT Press.Ertoz, L., Steinbach, M., and Kumar, V. 2003. Finding topics in collections of documents Ashared nearest neighbor approach. In Clustering and Information Retrieval. 83104.Escalante, H. J. 2005. A comparison of outlier detection algorithms for machine learning. InProceedings of the International Conference on Communications in Computing.Eskin, E. 2000. Anomaly detection over noisy data using learned probability distributions. InProceedings of the Seventeenth International Conference on Machine Learning. Morgan Kaufmann Publishers Inc., 255262.Eskin, E., Arnold, A., Prerau, M., Portnoy, L., and Stolfo, S. 2002. A geometric framework for unsupervised anomaly detection. In Proceedings of Applications of Data Mining inComputer Security. Kluwer Academics, 78100.Eskin, E., Lee, W., and Stolfo, S. 2001. Modeling system call for intrusion detection usingdynamic window sizes. In Proceedings of DISCEX.Ester, M., Kriegel, H.P., Sander, J., and Xu, X. 1996. A densitybased algorithm for discovering clusters in large spatial databases with noise. In Proceedings of Second InternationalConference on Knowledge Discovery and Data Mining, E. Simoudis, J. Han, and U. Fayyad,Eds. AAAI Press, Portland, Oregon, 226231.Fan, W., Miller, M., Stolfo, S. J., Lee, W., and Chan, P. K. 2001. Using artificial anomalies todetect unknown and known network intrusions. In Proceedings of the 2001 IEEE InternationalConference on Data Mining. IEEE Computer Society, 123130.Fawcett, T. and Provost, F. 1999. Activity monitoring noticing interesting changes in behavior. In Proceedings of the 5th ACM SIGKDD International Conference on Knowledge Discoveryand Data Mining. ACM Press, 5362.Forrest, S., Dhaeseleer, P., and Helman, P. 1996. An immunological approach to changedetection Algorithms, analysis and implications. In Proceedings of the 1996 IEEE Symposiumon Security and Privacy. IEEE Computer Society, 110.Forrest, S., Esponda, F., and Helman, P. 2004. A formal framework for positive and negativedetection schemes. In IEEE Transactions on Systems, Man and Cybernetics, Part B. IEEE,357373.Forrest, S., Hofmeyr, S. A., Somayaji, A., and Longstaff, T. A. 1996. A sense of self forunix processes. In Proceedinges of the ISRSP96. 120128.Forrest, S., Perelson, A. S., Allen, L., and Cherukuri, R. 1994. Selfnonself discriminationin a computer. In Proceedings of the 1994 IEEE Symposium on Security and Privacy. IEEEComputer Society, Washington, DC, USA, 202.Forrest, S., Warrender, C., and Pearlmutter, B. 1999. Detecting intrusions using systemcalls Alternate data models. In Proceedings of the 1999 IEEE ISRSP. IEEE Computer Society,Washington, DC, USA, 133145.Fox, A. J. 1972. Outliers in time series. Journal of the Royal Statistical Society. SeriesBMethodological 34, 3, 350363.Fu, A. W.C., Leung, O. T.W., Keogh, E. J., and Lin, J. 2006. Finding time series discordsbased on haar transform. In Proceeding of the 2nd International Conference on Advanced DataMining and Applications. Springer Verlag, 3141.Fujimaki, R., Yairi, T., and Machida, K. 2005. An approach to spacecraft anomaly detectionproblem using kernel feature space. In Proceeding of the eleventh ACM SIGKDD internationalconference on Knowledge discovery in data mining. ACM Press, New York, NY, USA, 401410.To Appear in ACM Computing Surveys, 09 2009.60  Chandola, Banerjee and KumarGaleano, P., Pea, D., and Tsay, R. S. 2004. Outlier detection in multivariate time series viaprojection pursuit. Statistics and Econometrics Working Papers ws044211, Universidad CarlosIII, Departamento de Estadstica y Econometrca. Sep.Ghosh, A. K., Schwartzbard, A., and Schatz, M. 1999a. Learning program behavior profilesfor intrusion detection. In Proceedings of 1st USENIX Workshop on Intrusion Detection andNetwork Monitoring. 5162.Ghosh, A. K., Schwartzbard, A., and Schatz, M. 1999b. Using program behavior profiles forintrusion detection. In Proceedings of SANS Third Conference and Workshop on IntrusionDetection and Response.Ghosh, A. K., Wanken, J., and Charron, F. 1998. Detecting anomalous and unknown intrusions against programs. In Proceedings of the 14th Annual Computer Security ApplicationsConference. IEEE Computer Society, 259.Ghosh, S. and Reilly, D. L. 1994. Credit card fraud detection with a neuralnetwork. InProceedings of the 27th Annual Hawaii International Conference on System Science. Vol. 3.Los Alamitos, CA.Ghoting, A., Parthasarathy, S., and Otey, M. 2006. Fast mining of distancebased outliersin high dimensional datasets. In Proceedings of the SIAM International Conference on DataMining.Gibbons, R. D. 1994. Statistical Methods for Groundwater Monitoring. John Wiley  Sons, Inc.Goldberger, A. L., Amaral, L. A. N., Glass, L., Hausdorff, J. M., Ivanov, P. C., Mark,R. G., Mietus, J. E., Moody, G. B., Peng, C.K., and Stanley, H. E. 2000. PhysioBank, PhysioToolkit, and PhysioNet Components of a new research resource for complex physiologic signals. Circulation 101, 23, e215e220. Circulation Electronic Pageshttpcirc.ahajournals.orgcgicontentfull10123e215.Gonzalez, F. A. and Dasgupta, D. 2003. Anomaly detection using realvalued negative selection.Genetic Programming and Evolvable Machines 4, 4, 383403.Grubbs, F. 1969. Procedures for detecting outlying observations in samples. Technometrics 11, 1,121.Guha, S., Rastogi, R., and Shim, K. 2000. ROCK A robust clustering algorithm for categoricalattributes. Information Systems 25, 5, 345366.Gunter, S., Schraudolph, N. N., and Vishwanathan, S. V. N. 2007. Fast iterative kernelprincipal component analysis. J. Mach. Learn. Res. 8, 18931918.Gusfield, D. 1997. Algorithms on strings, trees, and sequences computer science and computational biology. Cambridge University Press, New York, NY, USA.Guttormsson, S., II, R. M., and ElSharkawi, M. 1999. Elliptical novelty grouping for onlineshortturn detection of excited running rotors. IEEE Transactions on Energy Conversion 14, 1March.Gwadera, R., Atallah, M. J., and Szpankowski, W. 2004. Detection of significant sets ofepisodes in event sequences. In Proceedings of the Fourth IEEE International Conference onData Mining. IEEE Computer Society, Washington, DC, USA, 310.Gwadera, R., Atallah, M. J., and Szpankowski, W. 2005a. Markov models for identificationof significant episodes. In Proceedings of 5th SIAM International Conference on Data Mining.Gwadera, R., Atallah, M. J., and Szpankowski, W. 2005b. Reliable detection of episodes inevent sequences. Knowledge and Information Systems 7, 4, 415437.Harris, T. 1993. Neural network in machine health monitoring. Professional Engineering.Hartigan, J. A. and Wong, M. A. 1979. A kmeans clustering algorithm. Applied Statistics 28,100108.Hautamaki, V., Karkkainen, I., and Franti, P. 2004. Outlier detection using knearest neighbour graph. In Proceedings of 17th International Conference on Pattern Recognition. Vol. 3.IEEE Computer Society, Washington, DC, USA, 430433.Hawkins, D. 1980. Identification of outliers. Monographs on Applied Probability and Statistics.Hawkins, D. M. 1974. The detection of errors in multivariate data using principal components.Journal of the American Statistical Association 69, 346 june, 340344.To Appear in ACM Computing Surveys, 09 2009.Anomaly Detection  A Survey  61Hawkins, S., He, H., Williams, G. J., and Baxter, R. A. 2002. Outlier detection using replicatorneural networks. In Proceedings of the 4th International Conference on Data Warehousing andKnowledge Discovery. SpringerVerlag, 170180.Hazel, G. G. 2000. Multivariate gaussian mrf for multispectral scene segmentation and anomalydetection. GeoRS 38, 3 May, 11991211.He, H., Wang, J., Graco, W., and Hawkins, S. 1997. Application of neural networks to detectionof medical fraud. Expert Systems with Applications 13, 4, 329336.He, Z., Deng, S., and Xu, X. 2002. Outlier detection integrating semantic knowledge. In Proceedings of the Third International Conference on Advances in WebAge Information Management.SpringerVerlag, London, UK, 126131.He, Z., Deng, S., Xu, X., and Huang, J. Z. 2006. A fast greedy algorithm for outlier mining.In Proceedings of 10th PacificAsia Conference on Knowledge and Data Discovery. 567576.He, Z., Xu, X., and Deng, S. 2003. Discovering clusterbased local outliers. Pattern RecognitionLetters 24, 910, 16411650.He, Z., Xu, X., and Deng, S. 2005. An optimization model for outlier detection in categoricaldata. In Proceedings of International Conference on Intelligent Computing. Vol. 3644. Springer.He, Z., Xu, X., Huang, J. Z., and Deng, S. 2004a. A frequent pattern discovery method foroutlier detection. 726732.He, Z., Xu, X., Huang, J. Z., and Deng, S. 2004b. Mining class outliers Concepts, algorithmsand applications. 588589.Heller, K. A., Svore, K. M., Keromytis, A. D., and Stolfo, S. J. 2003. One class supportvector machines for detecting anomalous windows registry accesses. In Proceedings of theWorkshop on Data Mining for Computer Security.Helman, P. and Bhangoo, J. 1997. A statistically based system for prioritizing informationexploration under uncertainty. In IEEE International Conference on Systems, Man, and Cybernetics. Vol. 27. IEEE, 449466.Helmer, G., Wong, J., Honavar, V., and Miller, L. 1998. Intelligent agents for intrusiondetection. In Proceedings of IEEE Information Technology Conference. 121124.Hickinbotham, S. J. and Austin, J. 2000a. Novelty detection in airframe strain data. InProceedings of 15th International Conference on Pattern Recognition. Vol. 2. 536539.Hickinbotham, S. J. and Austin, J. 2000b. Novelty detection in airframe strain data. InProceedings of the IEEEINNSENNS International Joint Conference on Neural Networks.Vol. 6. 2427.Ho, L. L., Macey, C. J., and Hiller, R. 1999. A distributed and reliable platform for adaptive anomaly detection in ip networks. In Proceedings of the 10th IFIPIEEE InternationalWorkshop on Distributed Systems Operations and Management. SpringerVerlag, London, UK,3346.Ho, T. V. and Rouat, J. 1997. A novelty detector using a network of integrate and fire neurons.Lecture Notes in Computer Science 1327, 103108.Ho, T. V. and Rouat, J. 1998. Novelty detection based on relaxation time of a network ofintegrateandfire neurons. In Proceedings of Second IEEE World Congress on ComputationalIntelligence. Anchorage, AK, 15241529.Hodge, V. and Austin, J. 2004. A survey of outlier detection methodologies. Artificial Intelligence Review 22, 2, 85126.Hofmeyr, S. A., Forrest, S., and Somayaji, A. 1998. Intrusion detection using sequences ofsystem calls. Journal of Computer Security 6, 3, 151180.Hollier, G. and Austin, J. 2002. Novelty detection for straingauge degradation using maximally correlated components. In Proceedings of the European Symposium on Artificial NeuralNetworks. 257262539.Hollmen, J. and Tresp, V. 1999. Callbased fraud detection in mobile communication networksusing a hierarchical regimeswitching model. In Proceedings of the 1998 conference on Advancesin neural information processing systems II. MIT Press, Cambridge, MA, USA, 889895.Horn, P. S., Feng, L., Li, Y., and Pesce, A. J. 2001. Effect of outliers and nonhealthy individualson reference interval estimation. Clinical Chemistry 47, 12, 21372145.To Appear in ACM Computing Surveys, 09 2009.62  Chandola, Banerjee and KumarHu, W., Liao, Y., and Vemuri, V. R. 2003. Robust anomaly detection using support vector machines. In Proceedings of the International Conference on Machine Learning. MorganKaufmann Publishers Inc., San Francisco, CA, USA, 282289.Huber, P. 1974. Robust Statistics. Wiley, New York.Huber, P. J. 1985. Projection pursuit with discussions. The Annals of Statistics 13, 2 June,435475.Ide, T. and Kashima, H. 2004. Eigenspacebased anomaly detection in computer systems. InProceedings of the 10th ACM SIGKDD international conference on Knowledge discovery anddata mining. ACM Press, New York, NY, USA, 440449.Ide, T., Papadimitriou, S., and Vlachos, M. 2007. Computing correlation anomaly scoresusing stochastic nearest neighbors. In Proceedings of International Conference Data Mining.523528.Ihler, A., Hutchins, J., and Smyth, P. 2006. Adaptive event detection with timevarying poissonprocesses. In Proceedings of the 12th ACM SIGKDD international conference on Knowledgediscovery and data mining. ACM Press, New York, NY, USA, 207216.Ilgun, K., Kemmerer, R. A., and Porras, P. A. 1995. State transition analysis A rulebasedintrusion detection approach. IEEE Transactions on Software Engineering 21, 3, 181199.Jagadish, H. V., Koudas, N., and Muthukrishnan, S. 1999. Mining deviants in a time seriesdatabase. In Proceedings of the 25th International Conference on Very Large Data Bases.Morgan Kaufmann Publishers Inc., 102113.Jagota, A. 1991. Novelty detection on a very large number of memories stored in a hopfieldstylenetwork. In Proceedings of the International Joint Conference on Neural Networks. Vol. 2.Seattle, WA, 905.Jain, A. K. and Dubes, R. C. 1988. Algorithms for Clustering Data. PrenticeHall, Inc.Jakubek, S. and Strasser, T. 2002. Faultdiagnosis using neural networks with ellipsoidal basisfunctions. In Proceedings of the American Control Conference. Vol. 5. 38463851.Janakiram, D., Reddy, V., and Kumar, A. 2006. Outlier detection in wireless sensor networksusing bayesian belief networks. In First International Conference on Communication SystemSoftware and Middleware. 16.Japkowicz, N., Myers, C., and Gluck, M. A. 1995. A novelty detection approach to classification. In Proceedings of International Joint Conference on Artificial Intelligence. 518523.Javitz, H. S. and Valdes, A. 1991. The sri ides statistical anomaly detector. In Proceedings ofthe 1991 IEEE Symposium on Research in Security and Privacy. IEEE Computer Society.Jiang, M. F., Tseng, S. S., and Su, C. M. 2001. Twophase clustering process for outliersdetection. Pattern Recognition Letters 22, 67, 691700.Jin, W., Tung, A. K. H., and Han, J. 2001. Mining topn local outliers in large databases. InProceedings of the seventh ACM SIGKDD international conference on Knowledge discoveryand data mining. ACM Press, 293298.Joachims, T. 2006. Training linear svms in linear time. In KDD 06 Proceedings of the 12thACM SIGKDD international conference on Knowledge discovery and data mining. ACM, NewYork, NY, USA, 217226.Jolliffe, I. T. 2002. Principal Component Analysis, 2nd ed. Springer.Joshi, M. V., Agarwal, R. C., and Kumar, V. 2001. Mining needle in a haystack classifying rareclasses via twophase rule induction. In Proceedings of the 2001 ACM SIGMOD internationalconference on Management of data. ACM Press, New York, NY, USA, 91102.Joshi, M. V., Agarwal, R. C., and Kumar, V. 2002. Predicting rare classes can boosting makeany weak learner strong In Proceedings of the eighth ACM SIGKDD international conferenceon Knowledge discovery and data mining. ACM, New York, NY, USA, 297306.Kadota, K., Tominaga, D., Akiyama, Y., and Takahashi, K. 2003. Detecting outlying samplesin microarray data A critical assessment of the effect of outliers on sample classification. ChemBio Informatics 3, 1, 3045.Karypis, G. and Kumar, V. 1998. Multilevel kway partitioning scheme for irregular graphs.Journal of Parallel and Distributed Computing 48, 1, 96129.To Appear in ACM Computing Surveys, 09 2009.Anomaly Detection  A Survey  63Kearns, M. J. 1990. Computational Complexity of Machine Learning. MIT Press, Cambridge,MA, USA.Kejia Zhang, Shengfei Shi, H. G. and Li, J. 2007. Unsupervised outlier detection in sensornetworks using aggregation tree. Advanced Data Mining and Applications 4632, 158169.Keogh, E., Lin, J., and Fu, A. 2005. Hot sax Efficiently finding the most unusual time seriessubsequence. In Proceedings of the Fifth IEEE International Conference on Data Mining.IEEE Computer Society, Washington, DC, USA, 226233.Keogh, E., Lin, J., Lee, S.H., and Herle, H. V. 2006. Finding the most unusual time seriessubsequence algorithms and applications. Knowledge and Information Systems 11, 1, 127.Keogh, E., Lonardi, S., and chi Chiu, B. Y. 2002. Finding surprising patterns in a time seriesdatabase in linear time and space. In Proceedings of the eighth ACM SIGKDD internationalconference on Knowledge discovery and data mining. ACM Press, New York, NY, USA, 550556.Keogh, E., Lonardi, S., and Ratanamahatana, C. A. 2004. Towards parameterfree datamining. In Proceedings of the 10th ACM SIGKDD international conference on Knowledgediscovery and data mining. ACM Press, New York, NY, USA, 206215.Keogh, E. and Smyth, P. 1997. A probabilistic approach to fast pattern matching in timeseries databases. In Proceedings of Third International Conference on Knowledge Discoveryand Data Mining, D. Heckerman, H. Mannila, D. Pregibon, and R. Uthurusamy, Eds. AAAIPress, Menlo Park, California., Newport Beach, CA, USA, 2430.King, S., King, D., P. Anuzis, K. A., Tarassenko, L., Hayton, P., and Utete, S. 2002. Theuse of novelty detection techniques for monitoring highintegrity plant. In Proceedings of the2002 International Conference on Control Applications. Vol. 1. Cancun, Mexico, 221226.Kitagawa, G. 1979. On the use of aic for the detection of outliers. Technometrics 21, 2 may,193199.Knorr, E. M. and Ng, R. T. 1997. A unified approach for mining outliers. In Proceedings of the1997 conference of the Centre for Advanced Studies on Collaborative research. IBM Press, 11.Knorr, E. M. and Ng, R. T. 1998. Algorithms for mining distancebased outliers in largedatasets. In Proceedings of the 24rd International Conference on Very Large Data Bases.Morgan Kaufmann Publishers Inc., 392403.Knorr, E. M. and Ng, R. T. 1999. Finding intensional knowledge of distancebased outliers. InThe VLDB Journal. 211222.Knorr, E. M., Ng, R. T., and Tucakov, V. 2000. Distancebased outliers algorithms andapplications. The VLDB Journal 8, 34, 237253.Ko, H. and Jacyna, G. 2000. Dynamical behavior of autoassociative memory performing noveltyfiltering. In IEEE Transactions on Neural Networks. Vol. 11. 11521161.Kohonen, T., Ed. 1997. Selforganizing maps. SpringerVerlag New York, Inc., Secaucus, NJ,USA.Kojima, K. and Ito, K. 1999. Autonomous learning of novel patterns by utilizing chaotic dynamics. In IEEE International Conference on Systems, Man, and Cybernetics. Vol. 1. IEEE,Tokyo, Japan, 284289.Kosoresow, A. P. and Hofmeyr, S. A. 1997. Intrusion detection via system call traces. IEEESoftware 14, 5, 3542.Kou, Y., Lu, C.T., and Chen, D. 2006. Spatial weighted outlier detection. In Proceedings ofSIAM Conference on Data Mining.Kruegel, C., Mutz, D., Robertson, W., and Valeur, F. 2003. Bayesian event classificationfor intrusion detection. In Proceedings of the 19th Annual Computer Security ApplicationsConference. IEEE Computer Society, 14.Kruegel, C., Toth, T., and Kirda, E. 2002. Service specific anomaly detection for networkintrusion detection. In Proceedings of the 2002 ACM symposium on Applied computing. ACMPress, 201208.Kruegel, C. and Vigna, G. 2003. Anomaly detection of webbased attacks. In Proceedings ofthe 10th ACM conference on Computer and communications security. ACM Press, 251261.To Appear in ACM Computing Surveys, 09 2009.64  Chandola, Banerjee and KumarKumar, V. 2005. Parallel and distributed computing for cybersecurity. Distributed SystemsOnline, IEEE 6, 10.Labib, K. and Vemuri, R. 2002. Nsom A realtime networkbased intrusion detection usingselforganizing maps. Networks and Security.Lafferty, J. D., McCallum, A., and Pereira, F. C. N. 2001. Conditional random fieldsProbabilistic models for segmenting and labeling sequence data. In Proceedings of the Eighteenth International Conference on Machine Learning. Morgan Kaufmann Publishers Inc., SanFrancisco, CA, USA, 282289.Lakhina, A., Crovella, M., and Diot, C. 2005. Mining anomalies using traffic feature distributions. In Proceedings of the 2005 conference on Applications, technologies, architectures, andprotocols for computer communications. ACM Press, New York, NY, USA, 217228.Lane, T. and Brodley, C. E. 1997a. An application of machine learning to anomaly detection. InProceedings of 20th NISTNCSC National Information Systems Security Conference. 366380.Lane, T. and Brodley, C. E. 1997b. Sequence matching and learning in anomaly detection forcomputer security. In Proceedings of AI Approaches to Fraud Detection and Risk Management,Fawcett, Haimowitz, Provost, and Stolfo, Eds. AAAI Press, 4349.Lane, T. and Brodley, C. E. 1999. Temporal sequence learning and data reduction for anomalydetection. ACM Transactions on Information Systems and Security 2, 3, 295331.Lauer, M. 2001. A mixture approach to novelty detection using training data with outliers. InProceedings of the 12th European Conference on Machine Learning. SpringerVerlag, London,UK, 300311.Laurikkala, J., Juhola1, M., and Kentala., E. 2000. Informal identification of outliers inmedical data. In Fifth International Workshop on Intelligent Data Analysis in Medicine andPharmacology. 2024.Lazarevic, A., Ertoz, L., Kumar, V., Ozgur, A., and Srivastava, J. 2003. A comparativestudy of anomaly detection schemes in network intrusion detection. In Proceedings of SIAMInternational Conference on Data Mining. SIAM.Lee, W. and Stolfo, S. 1998. Data mining approaches for intrusion detection. In Proceedingsof the 7th USENIX Security Symposium. San Antonio, TX.Lee, W., Stolfo, S., and Chan, P. 1997. Learning patterns from unix process execution tracesfor intrusion detection. In Proceedings of the AAAI 97 workshop on AI methods in Fraud andrisk management.Lee, W., Stolfo, S. J., and Mok, K. W. 2000. Adaptive intrusion detection A data miningapproach. Artificial Intelligence Review 14, 6, 533567.Lee, W. and Xiang, D. 2001. Informationtheoretic measures for anomaly detection. In Proceedings of the IEEE Symposium on Security and Privacy. IEEE Computer Society, 130.Li, M. and Vitanyi, P. M. B. 1993. An Introduction to Kolmogorov Complexity and Its Applications. SpringerVerlag, Berlin.Li, Y., Pont, M. J., and Jones, N. B. 2002. Improving the performance of radial basis functionclassifiers in condition monitoring and fault diagnosis applications where unknown faults mayoccur. Pattern Recognition Letters 23, 5, 569577.Lin, J., Keogh, E., Fu, A., and Herle, H. V. 2005. Approximations to magic Finding unusualmedical time series. In Proceedings of the 18th IEEE Symposium on ComputerBased MedicalSystems. IEEE Computer Society, Washington, DC, USA, 329334.Lin, S. and Brown, D. E. 2003. An outlierbased data association method for linking criminalincidents. In Proceedings of 3rd SIAM Data Mining Conference.Liu, J. P. and Weng, C. S. 1991. Detection of outlying data in bioavailabilitybioequivalencestudies. Statistics Medicine 10, 9, 137589.Lu, C.T., Chen, D., and Kou, Y. 2003. Algorithms for spatial outlier detection. In Proceedingsof 3rd International Conference on Data Mining. 597600.Ma, J. and Perkins, S. 2003a. Online novelty detection on temporal sequences. In Proceedingsof the 9th ACM SIGKDD international conference on Knowledge discovery and data mining.ACM Press, New York, NY, USA, 613618.To Appear in ACM Computing Surveys, 09 2009.Anomaly Detection  A Survey  65Ma, J. and Perkins, S. 2003b. Timeseries novelty detection using oneclass support vectormachines. In Proceedings of the International Joint Conference on Neural Networks. Vol. 3.1741 1745.MacDonald, J. W. and Ghosh, D. 2007. Copacancer outlier profile analysis. Bioinformatics 22, 23, 29502951.Mahoney, M. V. and Chan, P. K. 2002. Learning nonstationary models of normal network trafficfor detecting novel attacks. In Proceedings of the 8th ACM SIGKDD international conferenceon Knowledge discovery and data mining. ACM Press, 376385.Mahoney, M. V. and Chan, P. K. 2003. Learning rules for anomaly detection of hostile network traffic. In Proceedings of the 3rd IEEE International Conference on Data Mining. IEEEComputer Society, 601.Mahoney, M. V., Chan, P. K., and Arshad, M. H. 2003. A machine learning approach toanomaly detection. Tech. Rep. CS200306, Department of Computer Science, Florida Instituteof Technology Melbourne FL 32901. march.Manevitz, L. M. and Yousef, M. 2000. Learning from positive data for document classificationusing neural networks. In Proceedings of Second BarIlan Workshop on Knowledge Discoveryand Learning. Jerusalem.Manevitz, L. M. and Yousef, M. 2002. Oneclass svms for document classification. Journal ofMachine Learning Research 2, 139154.Manikopoulos, C. and Papavassiliou, S. 2002. Network intrusion and fault detection a statistical anomaly approach. IEEE Communication Magazine 40.Manson, G. 2002. Identifying damage sensitive, environment insensitive features for damagedetection. In Proceedings of the IES Conference. Swansea, UK.Manson, G., Pierce, G., and Worden, K. 2001. On the longterm stability of normal conditionfor damage detection in a composite panel. In Proceedings of the 4th International Conferenceon Damage Assessment of Structures. Cardiff, UK.Manson, G., Pierce, S. G., Worden, K., Monnier, T., Guy, P., and Atherton, K. 2000.Longterm stability of normal condition data for novelty detection. In Proceedings of SmartStructures and Integrated Systems. 323334.Marceau, C. 2000. Characterizing the behavior of a program using multiplelength ngrams. InProceedings of the 2000 workshop on New Security Paradigms. ACM Press, New York, NY,USA, 101110.Marchette, D. 1999. A statistical method for profiling network traffic. In Proceedings of 1stUSENIX Workshop on Intrusion Detection and Network Monitoring. Santa Clara, CA, 119128.Markou, M. and Singh, S. 2003a. Novelty detection a reviewpart 1 statistical approaches.Signal Processing 83, 12, 24812497.Markou, M. and Singh, S. 2003b. Novelty detection a reviewpart 2 neural network basedapproaches. Signal Processing 83, 12, 24992521.Marsland, S., Nehmzow, U., and Shapiro, J. 1999. A model of habituation applied to mobilerobots. In Proceedings of Towards Intelligent Mobile Robots. Department of Computer Science,Manchester University, Technical Report Series, ISSN 13616161, Report UMCS9931.Marsland, S., Nehmzow, U., and Shapiro, J. 2000a. Novelty detection for robot neotaxis. InProceedings of the 2nd International Symposium on Neural Compuatation. 554  559.Marsland, S., Nehmzow, U., and Shapiro, J. 2000b. A realtime novelty detector for a mobilerobot. In Proceedings of the EUREL Conference on Advanced Robotics Systems.Martinelli, G. and Perfetti, R. 1994. Generalized cellular neural network for novelty detection.IEEE Transactions on Circuits Systems I Fundamental Theory Application 41, 2, 187190.Martinez, D. 1998. Neural tree density estimation for novelty detection. IEEE Transactions onNeural Networks 9, 2, 330338.McCallum, A., Freitag, D., and Pereira, F. C. N. 2000. Maximum entropy markov models forinformation extraction and segmentation. In Proceedings of the 17th International Conferenceon Machine Learning. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 591598.To Appear in ACM Computing Surveys, 09 2009.66  Chandola, Banerjee and KumarMcCallum, A., Nigam, K., and Ungar, L. H. 2000. Efficient clustering of highdimensionaldata sets with application to reference matching. In Proceedings of the 6th ACM SIGKDDinternational conference on Knowledge discovery and data mining. ACM Press, 169178.McNeil, A. 1999. Extreme value theory for risk managers. Internal Modelling and CAD II ,93113.Mingming, N. Y. 2000. Probabilistic networks with undirected links for anomaly detection.In Proceedings of IEEE Systems, Man, and Cybernetics Information Assurance and SecurityWorkshop. 175179.Motulsky, H. 1995. Intuitive Biostatistics Choosing a statistical test. Oxford University Press,Chapter 37.Moya, M., Koch, M., and Hostetler, L. 1993. Oneclass classifier networks for target recognition applications. In Proceedings on World Congress on Neural Networks, International NeuralNetwork Society. Portland, OR, 797801.Murray, A. F. 2001. Novelty detection using products of simple experts  a potential architecturefor embedded systems. Neural Networks 14, 9, 12571264.Nairac, A., CorbettClark, T., Ripley, R., Townsend, N., and Tarassenko, L. 1997. Choosing an appropriate model for novelty detection. In Proceedings of the 5th IEEE InternationalConference on Artificial Neural Networks. 227232.Nairac, A., Townsend, N., Carr, R., King, S., Cowley, P., and Tarassenko, L. 1999. A system for the analysis of jet engine vibration data. Integrated ComputerAided Engineering 6, 1,5356.Ng, R. T. and Han, J. 1994. Efficient and effective clustering methods for spatial data mining. InProceedings of the 20th International Conference on Very Large Data Bases. Morgan KaufmannPublishers Inc., San Francisco, CA, USA, 144155.Noble, C. C. and Cook, D. J. 2003. Graphbased anomaly detection. In Proceedings of the9th ACM SIGKDD international conference on Knowledge discovery and data mining. ACMPress, 631636.Odin, T. and Addison, D. 2000. Novelty detection using neural network technology. In Proceedings of the COMADEN Conference. Houston, TX.Otey, M., Parthasarathy, S., Ghoting, A., Li, G., Narravula, S., and Panda, D. 2003.Towards nicbased intrusion detection. In Proceedings of the 9th ACM SIGKDD internationalconference on Knowledge discovery and data mining. ACM Press, New York, NY, USA, 723728.Otey, M. E., Ghoting, A., and Parthasarathy, S. 2006. Fast distributed outlier detection inmixedattribute data sets. Data Mining and Knowledge Discovery 12, 23, 203228.Palshikar, G. K. 2005. Distancebased outliers in sequences. Lecture Notes in Computer Science 3816, 547552.Papadimitriou, S., Kitagawa, H., Gibbons, P. B., and Faloutsos, C. 2002. Loci Fast outlier detection using the local correlation integral. Tech. Rep. IRPTR0209, Intel ResearchLaboratory, Pittsburgh, PA. July.Parra, L., Deco, G., and Miesbach, S. 1996. Statistical independence and novelty detectionwith information preserving nonlinear maps. Neural Computing 8, 2, 260269.Parzen, E. 1962. On the estimation of a probability density function and mode. Annals ofMathematical Statistics 33, 10651076.Patcha, A. and Park, J.M. 2007. An overview of anomaly detection techniques Existingsolutions and latest technological trends. Comput. Networks 51, 12, 34483470.Pavlov, D. 2003. Sequence modeling with mixtures of conditional maximum entropy distributions. In Proceedings of the Third IEEE International Conference on Data Mining. IEEEComputer Society, Washington, DC, USA, 251.Pavlov, D. and Pennock, D. 2002. A maximum entropy approach to collaborative filtering indynamic, sparse, highdimensional domains. In Proceedings of Advances in Neural InformationProcessing. MIT Press.To Appear in ACM Computing Surveys, 09 2009.Anomaly Detection  A Survey  67Petsche, T., Marcantonio, A., Darken, C., Hanson, S., Kuhn, G., and Santoso, I. 1996.A neural network autoassociator for induction motor failure prediction. In Proceedings ofAdvances in Neural Information Processing. Vol. 8. 924930.Phoha, V. V. 2002. The Springer Internet Security Dictionary. SpringerVerlag.Phua, C., Alahakoon, D., and Lee, V. 2004. Minority report in fraud detection classificationof skewed data. SIGKDD Explorer Newsletter 6, 1, 5059.Phuong, T. V., Hung, L. X., Cho, S. J., Lee, Y., and Lee, S. 2006. An anomaly detectionalgorithm for detecting attacks in wireless sensor networks. Intelligence and Security Informatics 3975, 735736.Pickands, J. 1975. Statistical inference using extreme order statistics. The Annals of Statistics 3, 1 Jan, 119131.Pires, A. and SantosPereira, C. 2005. Using clustering and robust estimators to detect outliersin multivariate data. In Proceedings of International Conference on Robust Statistics. Finland.Platt, J. 2000. Probabilistic outputs for support vector machines and comparison to regularizedlikelihood methods. A. Smola, P. Bartlett, B. Schoelkopf, and D. Schuurmans, Eds. 6174.Pokrajac, D., Lazarevic, A., and Latecki, L. J. 2007. Incremental local outlier detection fordata streams. In Proceedings of IEEE Symposium on Computational Intelligence and DataMining.Porras, P. A. and Neumann, P. G. 1997. EMERALD Event monitoring enabling responsesto anomalous live disturbances. In Proceedings of 20th NISTNCSC National InformationSystems Security Conference. 353365.Portnoy, L., Eskin, E., and Stolfo, S. 2001. Intrusion detection with unlabeled data usingclustering. In Proceedings of ACM Workshop on Data Mining Applied to Security.Protopapas, P., Giammarco, J. M., Faccioli, L., Struble, M. F., Dave, R., and Alcock, C.2006. Finding outlier light curves in catalogues of periodic variable stars. Monthly Notices ofthe Royal Astronomical Society 369, 2, 677696.Qin, M. and Hwang, K. 2004. Frequent episode rules for internet anomaly detection. In Proceedings of the 3rd IEEE International Symposium on Network Computing and Applications.IEEE Computer Society.Ramadas, M., Ostermann, S., and Tjaden, B. C. 2003. Detecting anomalous network trafficwith selforganizing maps. In Proceedings of Recent Advances in Intrusion Detection. 3654.Ramaswamy, S., Rastogi, R., and Shim, K. 2000. Efficient algorithms for mining outliersfrom large data sets. In Proceedings of the 2000 ACM SIGMOD international conference onManagement of data. ACM Press, 427438.Ratsch, G., Mika, S., Scholkopf, B., and Muller, K.R. 2002. Constructing boosting algorithms from svms An application to oneclass classification. IEEE Transactions on PatternAnalysis and Machine Intelligence 24, 9, 11841199.Roberts, S. 1999. Novelty detection using extreme value statistics. In Proceedings of IEEE Vision, Image and Signal processing. Vol. 146. 124129.Roberts, S. 2002. Extreme value statistics for novelty detection in biomedical signal processing. InProceedings of the 1st International Conference on Advances in Medical Signal and InformationProcessing. 166172.Roberts, S. and Tarassenko, L. 1994. A probabilistic resource allocating network for noveltydetection. Neural Computing 6, 2, 270284.Rosner, B. 1983. Percentage points for a generalized esd manyoutlier procedure. Technometrics 25, 2 may, 165172.Roth, V. 2004. Outlier detection with oneclass kernel fisher discriminants. In NIPS.Roth, V. 2006. Kernel fisher discriminants for outlier detection. Neural Computation 18, 4,942960.Rousseeuw, P. J. and Leroy, A. M. 1987. Robust regression and outlier detection. John Wiley Sons, Inc., New York, NY, USA.Roussopoulos, N., Kelley, S., and Vincent, F. 1995. Nearest neighbor queries. In Proceedingsof ACMSIGMOD International Conference on Management of Data.To Appear in ACM Computing Surveys, 09 2009.68  Chandola, Banerjee and KumarRuotolo, R. and Surace, C. 1997. A statistical approach to damage detection through vibrationmonitoring. In Proceedings of the 5th Pan American Congress of Applied Mechanics. PuertoRico.Salvador, S. and Chan, P. 2003. Learning states and rules for timeseries anomaly detection.Tech. Rep. CS200305, Department of Computer Science, Florida Institute of TechnologyMelbourne FL 32901. march.Sarawagi, S., Agrawal, R., and Megiddo, N. 1998. Discoverydriven exploration of olap datacubes. In Proceedings of the 6th International Conference on Extending Database Technology.SpringerVerlag, London, UK, 168182.Sargor, C. 1998. Statistical anomaly detection for linkstate routing protocols. In Proceedings ofthe Sixth International Conference on Network Protocols. IEEE Computer Society, Washington,DC, USA, 62.Saunders, R. and Gero, J. 2000. The importance of being emergent. In Proceedings of ArtificialIntelligence in Design.Scarth, G., McIntyre, M., Wowk, B., and Somorjai, R. 1995. Detection of novelty in functional images using fuzzy clustering. In Proceedings of the 3rd Meeting of International Societyfor Magnetic Resonance in Medicine. Nice, France, 238.Scholkopf, B., Platt, J. C., ShaweTaylor, J. C., Smola, A. J., and Williamson, R. C. 2001.Estimating the support of a highdimensional distribution. Neural Comput. 13, 7, 14431471.Scott, S. L. 2001. Detecting network intrusion using a markov modulated nonhomogeneouspoisson process. Submitted to the Journal of the American Statistical Association.Sebyala, A. A., Olukemi, T., and Sacks, L. 2002. Active platform security through intrusiondetection using naive bayesian network for anomaly detection. In Proceedings of the 2002London Communications Symposium.Sekar, R., Bendre, M., Dhurjati, D., and Bollineni, P. 2001. A fast automatonbased methodfor detecting anomalous program behaviors. In Proceedings of the IEEE Symposium on Securityand Privacy. IEEE Computer Society, 144.Sekar, R., Guang, Y., Verma, S., and Shanbhag, T. 1999. A highperformance network intrusion detection system. In Proceedings of the 6th ACM conference on Computer and communications security. ACM Press, 817.Sekar, R., Gupta, A., Frullo, J., Shanbhag, T., Tiwari, A., Yang, H., and Zhou, S. 2002.Specificationbased anomaly detection a new approach for detecting network intrusions. InProceedings of the 9th ACM conference on Computer and communications security. ACMPress, 265274.Sequeira, K. and Zaki, M. 2002. Admit anomalybased data mining for intrusions. In Proceedings of the 8th ACM SIGKDD international conference on Knowledge discovery and datamining. ACM Press, 386395.Sheikholeslami, G., Chatterjee, S., and Zhang, A. 1998. Wavecluster A multiresolutionclustering approach for very large spatial databases. In Proceedings of the 24rd InternationalConference on Very Large Data Bases. Morgan Kaufmann Publishers Inc., San Francisco, CA,USA, 428439.Shekhar, S., Lu, C.T., and Zhang, P. 2001. Detecting graphbased spatial outliers algorithmsand applications a summary of results. In Proceedings of the 7th ACM SIGKDD internationalconference on Knowledge discovery and data mining. ACM Press, New York, NY, USA, 371376.Shewhart, W. A. 1931. Economic Control of Quality of Manufactured Product. D. Van NostrandCompany, New York NY.Shyu, M.L., Chen, S.C., Sarinnapakorn, K., and Chang, L. 2003. A novel anomaly detectionscheme based on principal component classifier. In Proceedings of 3rd IEEE InternationalConference on Data Mining. 353365.Siaterlis, C. and Maglaris, B. 2004. Towards multisensor data fusion for dos detection. InProceedings of the 2004 ACM symposium on Applied computing. ACM Press, 439446.Singh, S. and Markou, M. 2004. An approach to novelty detection applied to the classificationof image regions. IEEE Transactions on Knowledge and Data Engineering 16, 4, 396407.To Appear in ACM Computing Surveys, 09 2009.Anomaly Detection  A Survey  69Smith, R., Bivens, A., Embrechts, M., Palagiri, C., and Szymanski, B. 2002. Clusteringapproaches for anomaly based intrusion detection. In Proceedings of Intelligent EngineeringSystems through Artificial Neural Networks. ASME Press, 579584.Smyth, P. 1994. Markov monitoring with unknown states. IEEE Journal on Selected Areasin Communications, Special Issue on Intelligent Signal Processing for Communications 12, 9december, 16001612.Smyth, P. 1997. Clustering sequences with hidden markov models. In Advances in NeuralInformation Processing. Vol. 9. MIT Press.Snyder, D. 2001. Online intrusion detection using sequences of system calls. M.S. thesis, Department of Computer Science, Florida State University.Sohn, H., Worden, K., and Farrar, C. 2001. Novelty detection under changing environmental conditions. In Proceedings of Eighth Annual SPIE International Symposium on SmartStructures and Materials. Newport Beach, CA.Solberg, H. E. and Lahti, A. 2005. Detection of outliers in reference distributions Performanceof horns algorithm. Clinical Chemistry 51, 12, 23262332.Song, Q., Hu, W., and Xie, W. 2002. Robust support vector machine with bullet hole imageclassification. IEEE Transactions on Systems, Man, and Cybernetics  Part CApplicationsand Reviews 32, 4.Song, S., Shin, D., and Yoon, E. 2001. Analysis of novelty detection properties of autoassociators. In Proceedings of Condition Monitoring and Diagnostic Engineering Management.577584.Song, X., Wu, M., Jermaine, C., and Ranka, S. 2007. Conditional anomaly detection. IEEETransactions on Knowledge and Data Engineering 19, 5, 631645.Soule, A., Salamatian, K., and Taft, N. 2005. Combining filtering and statistical methodsfor anomaly detection. In IMC 05 Proceedings of the 5th ACM SIGCOMM conference onInternet measurement. ACM, New York, NY, USA, 114.Spence, C., Parra, L., and Sajda, P. 2001. Detection, synthesis and compression in mammographic image analysis with a hierarchical image probability model. In Proceedings of the IEEEWorkshop on Mathematical Methods in Biomedical Image Analysis. IEEE Computer Society,Washington, DC, USA, 3.Srivastava, A. 2006. Enabling the discovery of recurring anomalies in aerospace problem reportsusing highdimensional clustering techniques. Aerospace Conference, 2006 IEEE , 1734.Srivastava, A. and ZaneUlman, B. 2005. Discovering recurring anomalies in text reportsregarding complex space systems. Aerospace Conference, 2005 IEEE , 38533862.Stefano, C., Sansone, C., and Vento, M. 2000. To reject or not to reject that is the questionan answer in case of neural classifiers. IEEE Transactions on Systems, Management andCybernetics 30, 1, 8494.Stefansky, W. 1972. Rejecting outliers in factorial designs. Technometrics 14, 2, 469479.Steinwart, I., Hush, D., and Scovel, C. 2005. A classification framework for anomaly detection.Journal of Machine Learning Research 6, 211232.Streifel, R., Maks, R., and ElSharkawi, M. 1996. Detection of shortedturns in the field ofturbinegenerator rotors using novelty detectorsdevelopment and field tests. IEEE Transactions on Energy Conversations 11, 2, 312317.Subramaniam, S., Palpanas, T., Papadopoulos, D., Kalogeraki, V., and Gunopulos, D.2006. Online outlier detection in sensor data using nonparametric models. In VLDB 06Proceedings of the 32nd international conference on Very large data bases. VLDB Endowment,187198.Sun, H., Bao, Y., Zhao, F., Yu, G., and Wang, D. 2004. Cdtrees An efficient index structurefor outlier detection. 600609.Sun, J., Qu, H., Chakrabarti, D., and Faloutsos, C. 2005. Neighborhood formation andanomaly detection in bipartite graphs. In Proceedings of the 5th IEEE International Conferenceon Data Mining. IEEE Computer Society, Washington, DC, USA, 418425.To Appear in ACM Computing Surveys, 09 2009.70  Chandola, Banerjee and KumarSun, J., Xie, Y., Zhang, H., and Faloutsos, C. 2007. Less is more Compact matrix representation of large sparse graphs. In Proceedings of 7th SIAM International Conference on DataMining.Sun, P. and Chawla, S. 2004. On local spatial outliers. In Proceedings of 4th IEEE InternationalConference on Data Mining. 209216.Sun, P. and Chawla, S. 2006. Slom a new measure for local spatial outliers. Knowledge andInformation Systems 9, 4, 412429.Sun, P., Chawla, S., and Arunasalam, B. 2006. Mining for outliers in sequential databases. InIn SIAM International Conference on Data Mining.Surace, C. and Worden, K. 1998. A novelty detection method to diagnose damage in structuresan application to an offshore platform. In Proceedings of Eighth International Conference ofOffshore and Polar Engineering. Vol. 4. Colorado, USA, 6470.Surace, C., Worden, K., and Tomlinson, G. 1997. A novelty detection approach to diagnosedamage in a cracked beam. In Proceedings of SPIE. Vol. 3089. 947953.Suzuki, E., Watanabe, T., Yokoi, H., and Takabayashi, K. 2003. Detecting interesting exceptions from medical test data with visual summarization. In Proceedings of the 3rd IEEEInternational Conference on Data Mining. 315322.Sykacek, P. 1997. Equivalent error bars for neural network classifiers trained by bayesian inference. In Proceedings of the European Symposium on Artificial Neural Networks. 121126.Tan, P.N., Steinbach, M., and Kumar, V. 2005. Introduction to Data Mining. AddisonWesley.Tandon, G. and Chan, P. 2007. Weighting versus pruning in rule validation for detecting networkand host anomalies. In Proceedings of the 13th ACM SIGKDD international conference onKnowledge discovery and data mining. ACM Press.Tang, J., Chen, Z., chee Fu, A. W., and W.Cheung, D. 2002. Enhancing effectiveness ofoutlier detections for low density patterns. In Proceedings of the PacificAsia Conference onKnowledge Discovery and Data Mining. 535548.Taniguchi, M., Haft, M., Hollmn, J., and Tresp, V. 1998. Fraud detection in communicationsnetworks using neural and probabilistic methods. In Proceedings of IEEE International Conference in Acoustics, Speech and Signal Processing. Vol. 2. IEEE Computer Society, 12411244.Tao, Y., Xiao, X., and Zhou, S. 2006. Mining distancebased outliers from large databasesin any metric space. In Proceedings of the 12th ACM SIGKDD international conference onKnowledge discovery and data mining. ACM Press, New York, NY, USA, 394403.Tarassenko, L. 1995. Novelty detection for the identification of masses in mammograms. InProceedings of the 4th IEEE International Conference on Artificial Neural Networks. Vol. 4.Cambridge, UK, 442447.Tax, D. and Duin, R. 1999a. Data domain description using support vectors. In Proceedings ofthe European Symposium on Artificial Neural Networks, M. Verleysen, Ed. Brussels, 251256.Tax, D. and Duin, R. 1999b. Support vector data description. Pattern Recognition Letters 20, 1113, 11911199.Tax, D. M. J. 2001. Oneclass classification conceptlearning in the absence of counterexamples.Ph.D. thesis, Delft University of Technology.Teng, H., Chen, K., and Lu, S. 1990. Adaptive realtime anomaly detection using inductivelygenerated sequential patterns. In Proceedings of IEEE Computer Society Symposium on Research in Security and Privacy. IEEE Computer Society Press, 278284.Theiler, J. and Cai, D. M. 2003. Resampling approach for anomaly detection in multispectralimages. In Proceedings of SPIE 5093, 230240, Ed.Thompson, B., II, R. M., Choi, J., ElSharkawi, M., Huang, M., and Bunje, C. 2002. Implicitlearning in autoencoder novelty assessment. In Proceedings of International Joint Conferenceon Neural Networks. Honolulu, 28782883.Thottan, M. and Ji, C. 2003. Anomaly detection in ip networks. IEEE Transactions on SignalProcessing 51, 8, 21912204.Tibshirani, R. and Hastie, T. 2007. Outlier sums for differential gene expression analysis.Biostatistics 8, 1, 28.To Appear in ACM Computing Surveys, 09 2009.Anomaly Detection  A Survey  71Tomlins, S. A., Rhodes, D. R., Perner, S., Dhanasekaran, S. M., Mehra, R., Sun, X. W.,Varambally, S., Cao, X., Tchinda, J., Kuefer, R., Lee, C., Montie, J. E., Shah, R.,Pienta, K. J., Rubin, M., and Chinnaiyan, A. M. 2005. Recurrent fusion of tmprss2 and etstranscription factor genes in prostate cancer. Science 310, 5748, 603611.Torr, P. and Murray, D. 1993. Outlier detection and motion segmentation. In Proceedings ofSPIE, Sensor Fusion VI, Paul S. Schenker Ed. Vol. 2059. 432443.Tsay, R. S., Pea, D., and Pankratz, A. E. 2000. Outliers in multivariate time series.Biometrika 87, 4, 789804.Vaidya, J. and Clifton, C. 2004. Privacypreserving outlier detection. In Proceedings of the 4thIEEE International Conference on Data Mining. 233240.Valdes, A. and Skinner, K. 2000. Adaptive, modelbased monitoring for cyber attack detection.In Proceedings of the 3rd International Workshop on Recent Advances in Intrusion Detection.SpringerVerlag, 8092.Vapnik, V. N. 1995. The nature of statistical learning theory. SpringerVerlag New York, Inc.,New York, NY, USA.Vasconcelos, G., Fairhurst, M., and Bisset, D. 1994. Recognizing novelty in classificationtasks. In Proceedings of Neural Information Processing Systems Workshop on Novelty Detection and Adaptive Systems monitoring. Denver, CO.Vasconcelos, G. C., Fairhurst, M. C., and Bisset, D. L. 1995. Investigating feedforward neuralnetworks with respect to the rejection of spurious patterns. Pattern Recognition Letters 16, 2,207212.Vilalta, R. and Ma, S. 2002. Predicting rare events in temporal domains. In Proceedings of the2002 IEEE International Conference on Data Mining. IEEE Computer Society, Washington,DC, USA, 474.Vinueza, A. and Grudic, G. 2004. Unsupervised outlier detection and semisupervised learning.Tech. Rep. CUCS97604, Univ. of Colorado at Boulder. May.Wei, L., Qian, W., Zhou, A., and Jin, W. 2003. Hot Hypergraphbased outlier test for categorical data. In Proceedings of the 7th PacificAsia Conference on Knowledge and Data Discovery.399410.Weigend, A. S., Mangeas, M., and Srivastava, A. N. 1995. Nonlinear gated experts fortimeseries  discovering regimes and avoiding overfitting. International Journal of NeuralSystems 6, 4, 373399.Weiss, G. M. and Hirsh, H. 1998. Learning to predict rare events in event sequences. In Proceedings of 4th International Conference on Knowledge Discovery and Data Mining, R. Agrawal,P. Stolorz, and G. PiatetskyShapiro, Eds. AAAI Press, Menlo Park, CA, New York, NY,359363.Whitehead, B. and Hoyt, W. 1993. A function approximation approach to anomaly detectionin propulsion system test data. In In Proceedings of 29th AIAASAEASMEASEE JointPropulsion Conference. IEEE Computer Society, Monterey, CA, USA.Williams, G., Baxter, R., He, H., Hawkins, S., and Gu, L. 2002. A comparative study of rnnfor outlier detection in data mining. In Proceedings of the 2002 IEEE International Conferenceon Data Mining. IEEE Computer Society, Washington, DC, USA, 709.Wong, W.K., Moore, A., Cooper, G., and Wagner, M. 2002. Rulebased anomalypattern detection for detecting disease outbreaks. In Proceedings of the 18th National Conference on Artificial Intelligence. MIT Press. Also available online fromhttpwww.cs.cmu.edusimawmantiterror.Wong, W.K., Moore, A., Cooper, G., and Wagner, M. 2003. Bayesian network anomalypattern detection for disease outbreaks. In Proceedings of the 20th International Conferenceon Machine Learning. AAAI Press, Menlo Park, California, 808815.Worden, K. 1997. Structural fault detection using a novelty measure. Journal of Sound Vibration 201, 1, 85101.Wu, M. and Jermaine, C. 2006. Outlier detection by sampling with accuracy guarantees. InProceedings of the 12th ACM SIGKDD international conference on Knowledge discovery anddata mining. ACM, New York, NY, USA, 767772.To Appear in ACM Computing Surveys, 09 2009.72  Chandola, Banerjee and KumarWu, N. and Zhang, J. 2003. Factor analysis based anomaly detection. In Proceedings of IEEEWorkshop on Information Assurance. United States Military Academy, West Point, NY, USA.Yairi, T., Kato, Y., and Hori, K. 2001. Fault detection by mining association rules from housekeeping data. In In Proceedings of International Symposium on Artificial Intelligence, Roboticsand Automation in Space.Yamanishi, K. and ichi Takeuchi, J. 2001. Discovering outlier filtering rules from unlabeleddata combining a supervised learner with an unsupervised learner. In Proceedings of the 7thACM SIGKDD international conference on Knowledge discovery and data mining. ACM Press,389394.Yamanishi, K., Takeuchi, J.I., Williams, G., and Milne, P. 2004. Online unsupervisedoutlier detection using finite mixtures with discounting learning algorithms. Data Mining andKnowledge Discovery 8, 275300.Yang, J. and Wang, W. 2003. Cluseq Efficient and effective sequence clustering. In Proceddingsof International Conference on Data Engineering. 101112.Yankov, D., Keogh, E. J., and Rebbapragada, U. 2007. Disk aware discord discovery Findingunusual time series in terabyte sized datasets. In Proceedings of International Conference onData Mining. 381390.Ye, N. 2004. A markov chain model of temporal behavior for anomaly detection. In Proceedingsof the 5th Annual IEEE Information Assurance Workshop. IEEE.Ye, N. and Chen, Q. 2001. An anomaly detection technique based on a chisquare statistic fordetecting intrusions into information systems. Quality and Reliability Engineering International 17, 105112.Yi, B.K., Sidiropoulos, N., Johnson, T., Jagadish, H. V., Faloutsos, C., and Biliris, A.2000. Online data mining for coevolving time sequences. In Proceedings of the 16th International Conference on Data Engineering. IEEE Computer Society, Washington, DC, USA,13.Ypma, A. and Duin, R. 1998. Novelty detection using selforganizing maps. In Progress inConnectionist Based Information Systems. Vol. 2. Springer, 13221325.Yu, D., Sheikholeslami, G., and Zhang, A. 2002. Findout finding outliers in very large datasets.Knowledge And Information Systems 4, 4, 387412.Yu, J. X., Qian, W., Lu, H., and Zhou, A. 2006. Finding centric local outliers in categoricalnumerical spaces. Knowledge and Information Systems 9, 3, 309338.Zeevi, A. J., Meir, R., and Adler, R. 1997. Time series prediction using mixtures of experts.In Advances in Neural Information Processing. Vol. 9. MIT Press.Zhang, J. and Wang, H. 2006. Detecting outlying subspaces for highdimensional data the newtask, algorithms, and performance. Knowledge and Information Systems 10, 3, 333355.Zhang, Z., Li, J., Manikopoulos, C., Jorgenson, J., and Ucles, J. 2001. Hide a hierarchicalnetwork intrusion detection system using statistical preprocessing and neural network classification. In Proceedings of IEEE Workshop on Information Assurance and Security. West Point,8590.Zimmermann, J. and Mohay, G. 2006. Distributed intrusion detection in clusters based onnoninterference. In ACSW Frontiers 06 Proceedings of the 2006 Australasian workshops onGrid computing and eresearch. Australian Computer Society, Inc., Darlinghurst, Australia,Australia, 8995.To Appear in ACM Computing Surveys, 09 2009.
