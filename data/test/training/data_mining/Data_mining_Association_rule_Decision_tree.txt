ARTMED1014 No of Pages 13Efficient discovery of risk patterns in medical dataJiuyong Li a,, Ada Waichee Fu b, Paul Fahey ca School of Computer and Information Science, University of South Australia, Mawson Lakes,Adelaide 5095, South Australia, AustraliabDepartment of Computer Science and Engineering, Chinese University of Hong Kong,Shatin, New Territories, Hong KongcDepartment of Mathematics and Computing, University of Southern Queensland,Toowoomba 4350, Queensland, AustraliaReceived 8 November 2007 received in revised form 30 June 2008 accepted 4 July 2008Artificial Intelligence in Medicine 2008 xxx, xxxxxxhttpwww.intl.elsevierhealth.comjournalsaiimKEYWORDSRelative riskRisk patternData miningAssociation ruleDecision treeEpidemiologySummaryObjective This paper studies a problem of efficiently discovering risk patterns inmedical data. Risk patterns are defined by a statistical metric, relative risk, which hasbeen widely used in epidemiological research.Methods To avoid fruitless search in the complete exploration of risk patterns, wedefine optimal risk pattern set to exclude superfluous patterns, i.e. complicatedpatterns with lower relative risk than their corresponding simpler form patterns. Weprove that mining optimal risk pattern sets conforms an antimonotone property thatsupports an efficient mining algorithm. We propose an efficient algorithm for miningoptimal risk pattern sets based on this property. We also propose a hierarchicalstructure to present discovered patterns for the easy perusal by domain experts.Results The proposed approach is compared with two wellknown rule discoverymethods, decision tree and association rule mining approaches on benchmark datasets and applied to a real world application. The proposed method discovers more andbetter quality risk patterns than a decision tree approach. The decision treemethod isnot designed for such applications and is inadequate for pattern exploring. Theproposed method does not discover a large number of uninteresting superfluouspatterns as an association mining approach does. The proposed method is moreefficient than an association rule mining method. A real world case study shows thatthe method reveals some interesting risk patterns to medical practitioners.Conclusion The proposed method is an efficient approach to explore risk patterns. Itquickly identifies cohorts of patients that are vulnerable to a risk outcome from a largedata set. The proposedmethod is useful for exploratory study on largemedical data togenerate and refine hypotheses. The method is also useful for designing medicalsurveillance systems. 2008 Elsevier B.V. All rights reserved. Corresponding author. Tel. 61 8 8302 3898 Fax 61 8 8302 3381.Email address jiuyong.liunisa.edu.au J. Li.09333657  see front matter  2008 Elsevier B.V. All rights reserved.doi10.1016j.artmed.2008.07.008Please cite this article in press as Li J, et al. Efficient discovery of risk patterns inmedical data. Artif Intell Med 2008,doi10.1016j.artmed.2008.07.008ARTMED1014 No of Pages 132 J. Li et al.1. Introduction1.1. Background and aimsHospitals and clinics accumulate a huge amount ofpatient data over the years. These data provide abasis for the analysis of risk factors for many diseases. For example, we can compare cancerpatients with noncancer patients to find patternsassociated with cancer. This method has been common practice in evidencebased medicine, which isan approach to the practice of medicine in which aclinician is aware of the evidence in support ofclinical practice, and the strength of that evidence.The analysis of the data from comparative studieshas usually been done by using statistical softwaretools, such as SPSS. This is a laborintensive process.It is inefficient to run an exhaustive analysis ofinteractions of three or more exposure variables.Therefore, an automatic datamining tool isrequired to perform such tedious and timeconsuming tasks.The interpretability of results is a requirementfor designing a data mining method for medicalapplications. In general, medical practitioners andresearchers do not care how sophisticated a datamining method is, but they do care how understandable its results are.Rules are a type of the most humanunderstandable knowledge, and therefore they are suitable formedical applications. There following are twowidely used approaches to extract rules from data.Decision trees, typified by C4.5 1, can be extendedto rules. Decision trees are usually used for buildingdiagnosis models for medical applications 24.The main objective is to minimise the overall errorsin classification. Rules from a decision tree areusually accurate but some are not statistically significant. Furthermore, a decision tree only represents one model among a number of possiblemodels. Rules from a decision tree may fail topresent relationships that are of interest to users.Association rule mining 5 is a general purposerule discovery scheme. It has been widely used fordiscovering rules in medical applications 68.Three challenges of association rule miningapproaches in these applications are 1 most widelyused interestingness criteria, such as confidence andlift, do not make sense to medical practitioners, 2too many trivial rules discovered overwhelm trulyinteresting rules, and 3 an association rule miningapproach is inefficient when the frequency requirement, the minimum support, is set low.To tackle the above problems, we use a widelyused epidemiological term, relative risk, to definerisk patterns. We propose optimal risk pattern setsPlease cite this article in press as Li J, et al. Efficient discovedoi10.1016j.artmed.2008.07.008to exclude superfluous patterns that are of no interest to medical practitioners. We present an efficientalgorithm to discover optimal risk pattern sets. Wealso study a way to present structured risk patternsto medical practitioners. The proposed method hasbeen applied to a real world application and produced some interesting results. This paper extendsour previous work 9.1.2. Related workDecision trees are a popular logical method forclassification. A decision tree is a hierarchical structure that partitions data into some disjoint groupsbased on their different attribute values. Leafs of adecision tree contain records of one or nearly oneclass, and so it has been used for classification. Anadvantage of decision tree methods is that decisiontrees can be converted into understandable rules. Amost widely used decision tree system is C4.5 1, itsancestor ID3 10, and a commercial version C5.0.Decision trees have been mainly used to builddiagnosis models for medical data 24. When it isused for exploring patterns in medical data, work in11 shows that it is inadequate for such exploration.One reason is that the objective of decision trees isnot to explore data but to build a simple classification model on the data. Another reason is that theheuristic search of decision tree prevents its findingmany quality rules. Decision trees only follow onepath in tree construction, and hence may missbetter rules along alternative paths. Recently, avariant decision tree algorithm, highyieldpartitiontree method, has been proposed to discover hiutility patterns for business intelligence 12. Itsapplication to medical data is to be explored.Association rule mining is a major data miningtechnique, and is a most commonly used patterndiscovery method. It retrieves all frequent patternsin a data set and forms interesting rules amongfrequent patterns. Most frequently used associationrule mining methods are Apriori 13 and FPgrowth14.Association rule mining has been widely used inmedical data analysis. Brossette et al. 6 uncoveredassociation rules in hospital infection control andpublic surveillance data. Paetz and Brause 8 discovered association rules in septic shock patientdata. Sequential patterns have been found inchronic hepatitis data by Ohsaki et al. 7, and inadverse drug reaction data by Chen et al. 15.Ordonez et al. used association rules to predictheart disease 16. However, the discovery of toomany rules is a major problem in all applications.Too many trivial and repetitive rules hide trulyinteresting rules. Association rule mining is ineffiry of risk patterns inmedical data. Artif Intell Med 2008,Efficient discovery of risk patterns in medical data 3ARTMED1014 No of Pages 13cient when the frequency requirement, i.e. theminimum support, is set low. Furthermore, the lackof the right interestingness measurements for medical application is another problem.Some efficient variants of association rule mininghave been presented in the last few years, forexample, mining nonredundant association rules17, mining constraint association rules 18,mining most interesting rules 19, mining top Nassociation rules 20, and mining koptimal rules21 or patterns 22. The rules are defined byconfidence, lift or leverage, and hence their resultsare not directly understandable to medical practitioners. Apart from the first two methods, they havea data coverage problem. For example, the top krules may come from the same section of data, andthis leaves some records in a data set uncovered. Asa result, some records in data are not represented inthe results.To our best knowledge, there is only one paperin data mining literature discussing finding patterns defined by relative risk. Li et al. 23 studieda number of algorithms to discover the most general and the most specific patterns defined byrelative risk using the convex property of plateausof support. The most efficient algorithm in 23 iscomparable to that of mining minimal generators.We will show theoretically that our approach ismore efficient than mining minimal generators inSection 2.2.2. Methods2.1. Problem definitions2.1.1. Risk patternsLet us assume that there is a collection of patientrecords. Each record is described by a number ofdiscrete attributes, one of which is the target attribute. The target attribute takes two values abnormal and nonabnormal. Records for patients with adisease or risk under study are labelled as abnormal,otherwise records are labelled as nonabnormal. Anexample of such a data set is listed as Table 1.In the following we refer to the abnormal class asa and the nonabnormal class as n.Please cite this article in press as Li J, et al. Efficient discovedoi10.1016j.artmed.2008.07.008Table 1 An example of medical data setGender Age Smoking BM 4050 Y HM 2040 N NF 2040 N N... ... ...A pattern is defined as a set of attributevaluepairs. For example, Gender  M, Age in 40,50 is apattern with two attributevalue pairs. The supportof pattern P is the ratio of the number of recordscontaining P to the number of all records in the dataset, denoted by suppP. When the data set is large,we have suppP probP.A pattern is usually called frequent if its supportis greater than a given threshold. However, in amedical data set, a pattern in the abnormal groupwould hardly be frequent when the abnormal casesare themselves rare. Therefore, we define the localsupport of P as the support of P in the abnormalgroup, represented aslsuppP a  suppPasuppawhere Pa is an abbreviation for P  a. Others havecalled this the recall of the rule P a 24. Weprefer to call it local support since it observes theantimonotone property of support the support of asuper pattern is less than or equal to the support ofits any subpattern. In this paper, a pattern is frequent if its local support is greater than a giventhreshold.A risk pattern in this paper refers to the antecedent of a rule with the consequence of abnormal.For the convenience of our discussions, we introduce another important concept for associationrules, confidence, in the followingconfP a  suppPasuppPA pattern separates all records into two groups, agroup with the pattern and the other without thepattern, e.g., males between 40 and 50 and therest. Cohorts separated by a pattern and two classesform a contingency table, see Table 2.Relative risk is a metric often used in epidemiological studies. It is often used to compare the risk ofdeveloping a disease of a group people with acertain characteristic to the other group withoutthe characteristic. The relative risk RR for thecohort with pattern P being abnormal is definedas followsry of risk patterns inmedical data. Artif Intell Med 2008,lood pressure . . . Classigh . . . Abnormalormal . . . Nonabnormalormal . . . Nonabnormal... . . . ...4ARTMED1014 No of Pages 13Table 2 A contingency table of a pattern and outcomesAbnormal a Nonabnormal n TotalP probP a probP n probP P prob  P a prob  P n prob  PTotal proba probn 1RRP a  probajPprobaj  P probP aprobPprob  P aprob  P suppPasuppPsupp  Pasupp  P suppPasupp  Psupp  PasuppP P means that P does not occur. Pa is an abbreviation of P a. supp  P is the fraction of all recordsthat do not contain P, and Pa refers to the recordscontaining a but not P.For example, if P  smoking, a  lung cancer, and RR  30, then this means that peoplewho smoke are three times more likely to get lungcancer than those who do not.A relative risk of less than 1 means the groupdescribed by the pattern is less likely to be abnormal. A relative risk of grater than 1 means the groupdescribed by the pattern is more likely to be abnormal. Confidence interval of relative risk is determined by the numbers in four cells of thecontingency table 25We give a formal definition of risk patterns usingrelative risk in the following.Definition 1. Risk patterns are patterns whose localsupport and relative risk are higher than the userspecified minimum local support and relative riskthresholds, respectively.A primitive goal is to find all risk patterns. However, mining all risk patterns suffers two similarproblems as association rule mining too many discovered patterns and low efficiency for low support.Mining optimal risk pattern sets alleviates the problems.2.1.2. Optimal risk pattern setMany risk patterns are of no interest to users. Forexample, we have two patterns, SEX  M andHRTFAIL  T and LIVER  T with relative risk 2.3,and HRTFAIL  Tand LIVER  T with relative risk 2.4.SEX  M in the first pattern does not increase relativerisk and hence we say that the first pattern is superfluous. Thus we introduce the optimal risk patternset to exclude these superfluous patterns.Please cite this article in press as Li J, et al. Efficient discovedoi10.1016j.artmed.2008.07.008Definition 2. A risk pattern set is optimal if itincludes all risk patterns except those whose relative risks are less than or equal to that of one of theirsubpatterns.In the above example, the first pattern will not bein the optimal risk pattern set because it is a superset of the second pattern but has lower relative risk.Optimal pattern set will exclude many superfluous and uninteresting risk patterns, for example, ifpattern symptom  x is a risk pattern, manypatterns, like gender  m, symptom  x, gender f, symptom  x, gender  m, age  middle age,symptom  x with the same or a lower relative riskwill be excluded from the optimal risk pattern set.Practically, a pattern with a slight improvement inrelative risk over its subpatterns is uninteresting. Aminimum improvement requirement can be definedby users. The optimal pattern set makes use of thezero minimum improvement. Mining a pattern setwith a nonzero minimum improvement can beextended by postpruning the optimal pattern set.In the optimal risk pattern set, the relative risk ofa pattern has to be greater than the relative risk ofits every subpattern. Note that the set of recordscovered by a pattern is a subset or at most an equalset of the set of records covered by a subpattern.Therefore, every record in a data set will be coveredby a pattern with the highest relative risk. In otherwords, the optimal pattern set does not include allpatterns, but does include patterns with the highestrelative risk for all records.Another important reason for defining the optimal risk pattern set is that it supports a property forefficient pattern discovery. We will present theproperty in the following section.2.2. Antimonotone property of optimalrisk pattern setsIn this section, we will prove that optimal riskpattern set satisfies an antimonotone property,which supports efficient optimal pattern discovery.We first introduce notation used in the followinglemma and corollary. Px is a proper super pattern ofP with one additional attributevalue pair x. We usea to stand for class a, and  a to stand for a class thatis not a. We can use n instead of  a for a twoclassproblem. We use  a because conclusions in thissection are true for the multiple class problem too.We have supp  a  1 suppa and suppP  a suppP  suppPa. Furthermore, we have supp  Px1 suppPxsupp  Px supp  P  x  suppP x  suppPx  suppPx supp  Px  supp  P  x  suppP  x.J. Li et al.ry of risk patterns inmedical data. Artif Intell Med 2008,5ARTMED1014 No of Pages 13Lemma 1 Antimonotone property for optimal riskpattern sets. If suppPx  a  suppP  a thenpattern Px and all its super patterns do not occur inthe optimal risk pattern set.Proof. We first present a proof scheme.Let PQx be a proper super pattern of PQ. PQx Px and PQ  P when Q  . To prove the Lemma,we need to show that RRPQx a  RRPQ aRRPQa  suppPQasupp PQ supp PQasuppPQ  confPQaconf PQ a confPQxaconf PQa1RRPQ a confPQx aconf  PQx a  RRPQx a 2We can deduce that suppPQ  a suppPQx  a for any Q from suppP  a suppPx  a.Next we prove Step 1. Consider fy  yy a monotonically increases with y when constanta 0 and suppPQ suppPQx 0confPQ a suppPQasuppPQ  suppPQasuppPQa  suppPQ  a suppPQasuppPQa  suppPQx  a suppPQxasuppPQxa  suppPQx  a  confPQx aWe then prove Step 2. Note that fromsuppPQ  a  suppPQx  a, we can deduce thatsuppPQ   x a  0. Another property we shallmake use of is that fy  y  ay monotonicallyincreases with y when constant a 0 andsupp  PQx supp  PQ 0confPQxa suppPQxasuppPQx suppPQxsuppPQxasuppPQxsuppPQx suppPQ xa suppPQxasuppPQxsincesuppPQ xa  0 suppPQx suppPQ asuppPQx suppPQ  suppPQasuppPQ suppPQasuppPQ   confPQ aEfficient discovery of risk patterns in medical dataPlease cite this article in press as Li J, et al. Efficient discovedoi10.1016j.artmed.2008.07.008The Lemma has been proved. From the above lemma, we can adopt a pruningtechnique as follows once we observe that anypattern, e.g., Px, satisfying suppPx  a suppP  a, we do not need to search for its superpatterns, e.g., PQx, since they do not occur in anoptimal risk pattern set.Corollary 1 Closure property. if suppPx suppP then pattern Px and all its super patternsdo not occur in the optimal risk pattern set.Proof. If suppPx  suppP, then suppPx  a suppP  a. Therefore, all its super patterns do notoccur in the optimal risk pattern set according toLemma 1. From the above corollary, we can adopt a pruningtechnique as follows once suppPx  suppP isobserved, we do not need to search for its superpatterns, e.g., PxQ since they will not be in theoptimal risk set.This corollary is closely associated with miningminimal generators 26. P is a proper generator ofPx when suppPx  suppP. P is called a minimalgenerator if there is no P0  P such thatsuppP0  suppP. According to Corollary 1, a pattern in an optimal risk pattern set has to be aminimalgenerator. Corollary 1 is a special case of Lemma 1.Lemma 1 disqualifies many minimal generators frombeing considered tobe in the optimal risk pattern set.As a result, mining optimal risk pattern sets does notsearch all minimal generators, and therefore is moreefficient than mining minimal generators.2.3. Risk pattern mining and presentingWe now discuss how to discover optimal pattern setsefficiently, and how to present risk patterns in a easyto peruse structure. The algorithm makes use of theantimonotone property to find optimal risk patternsets efficiently.2.3.1. MORE algorithmA naive method to find an optimal risk pattern setundergoes the following three steps. Firstly, discovering all frequent patterns in the abnormal group.Secondly, forming rules using relative risk to replaceconfidence. Thirdly, postpruning a large number ofuninteresting rules. This procedure is normally inefficient when the minimum support is low.Our optimal risk pattern mining algorithm makesuse of the antimonotone property to efficientlyprune the search space, and this distinguishes itfrom an association rule mining algorithm.ry of risk patterns inmedical data. Artif Intell Med 2008,6 J. Li et al.ARTMED1014 No of Pages 13The efficiency of an association rule mining algorithm lies in its efficient forward pruning of infrequent itemsets. An itemset is frequent if its supportis greater than the minimum support. An itemset ispotentially frequent only if all its subsets are frequent, and this property is used to limit the numberof itemsets to be searched. This antimonotoneproperty of frequent itemsets makes forward pruning possible.Lemma 1 and Corollary 1 are used to forwardlyprune risk patterns that do not occur in the optimalrisk pattern set. When a pattern satisfies the condition of Lemma 1 or Corollary 1, all its superpatterns are pruned. Pseudocode for mining optimal risk pattern sets is presented in the following.Algorithm 1 MORE Mining Optimal Risk pattErnsets.Input data set D, the minimum support s inabnormal class a, and the minimum relative riskthreshold u.Output optimal risk pattern set RGlobal data structure lpattern sets for 1  l Anlpattern contains l attributevalue pairs.1 Set R  2 Count support of 1patterns in the abnormalclass3 Generate1pattern set4 Select risk patterns and add them to R5 new pattern set  Generate2pattern set6 While new pattern set is not empty7 Count supports of candidates in new pattern set8 Prunenew pattern set9 Add patterns with relative risk greater than uto R10 Prune remaining superfluous patterns in R11 new pattern set  Generatenext level pattern set12 Return RThe above algorithm is selfexplanatory. We listtwo important functions as follows.Function 1 Generate l 1pattern setCombining1 Let l 1pattern set be empty set2 For each pair of patterns Sl1 p and Sl1q in lpattern set3 Insert candidate Sl1 pq inl 1 pattern setPruning4 For all Sl Sl1 pqPlease cite this article in press as Li J, et al. Efficient discovedoi10.1016j.artmed.2008.07.0085 If Sl does not exist in lpattern set6 Then remove candidate Sl1 pq7 Return l 1pattern setLine 5 is implemented by antimonotone properties of frequent patterns and optimal risk patterns. A nonexisting pattern in a lpattern set is aninfrequent pattern or a pattern satisfying Lemma 1or Corollary 1. They are pruned in the followingfunction.Function 2 Prune l 1pattern set1 For each pattern S in l 1pattern set2 If suppSasuppa  s then remove pattern S3 Else if there is a subpattern S0 in lpattern setsuch that suppS0  suppS or suppS0  a suppS a4 Then remove pattern S5 ReturnLines 3 and 4 are implemented according toLemma 1 and Corollary 1. Not only an infrequentpattern but also a pattern satisfying Lemma 1 orCorollary 1 is removed. Both Lemma 1 and Corollary1 are very effective and the resultant algorithm ismore efficient than an association rule mining algorithm.In the following, we use an example to show howthe algorithm works.Example 1. Consider the data set D in Table 3, andassume s  04 and u  20.After line 5 in MORE, the 1pattern set containsfb c d eg and the 2pattern set comprisesfbc bd be cd ce deg. Line 8 in MORE calls function Prune2candidate set. Pattern bc is prunedbecause suppbc a  suppc a, and pattern deis pruned because suppde a  suppd  a. Afterthe pruning, the 2pattern set becomesfbdbe cd ceg. Line 10 in the MORE calls FunctionGenerate 3pattern set. Candidate bde is generated in line 3 of Function Generator, and thenpruned in line 6 of Function Generator becausepattern de does not exist in the 2pattern set.The same procedure repeats on pattern cde.No 3pattern is generated and hence the programterminated. The output optimal risk patternset contains fcRR  24 dRR  24 bdRR 25 cdRR  25 ceRR  25g. An illustration ofthe searched patterns and output risk patterns byMORE is shown in Fig. 1.As a comparison, we show how to use an association rule mining method to achieve the same goal.We may generate all frequent patterns in class a andform association rules targeting a with the relativerisk as the strength. An association rule miningry of risk patterns inmedical data. Artif Intell Med 2008,Efficient discovery of risk patterns in medical dataARTMED1014 No of Pages 13Table 3 The data set of Example 1B C D E Ab1 c d e ab c d1 e ab c d e1 ab c d e ab c1 d e2 ab c d2 e3  ab c2 d3 e  ab2 c3 d e  aFigure 1 An illustration of the searched patterns andoutput risk patterns by MORE in Example 1. Patternscrossed are pruned. Patterns in bold are output riskpatterns.algorithm will examine candidate patternsfb c d e bc bd be cd ce de bcd bce cdeg andreturn a set of all risk patterns, fcRR  24 dRR 24 bdRR  25 cdRR  25 ceRR  25bcdRR  20 bceRR  20 cdeRR  20g. Anillustration of the searched patterns and the outputrisk patterns by an association mining algorithm isshown in Fig. 2.We see that the proposed algorithm, MORE,searches a smaller space, and returns a smaller riskpattern set than an association rule mining algorithm. This is a small data set including only a fewitems. For a large real world data set, differences ofthe searched spaces and output pattern setsbetween the two methods are significant.Please cite this article in press as Li J, et al. Efficient discovedoi10.1016j.artmed.2008.07.008Figure 2 An illustration of the searched patterns andoutput risk patterns by an association rule miningbasedapproach in Example 1. Only frequent patterns withinclass a are considered. Patterns crossed are pruned.Patterns in bold are output risk patterns.2.3.2. Pattern presentationAn optimal risk pattern set is smaller than anassociation rule set, but is still big for medicalpractitioners to review them. We may only returnthe top k patterns with the highest relative riskbut they may all come from the same section ofthe data set and lack representatives for allabnormal cases.In order to account for all known abnormal cases,we aim to retain a risk pattern with the highestrelative risk for each case. We use the followingmethod to select a small set of representativepatterns to present to users.Algorithm 2 Selecting representative risk patterns.Input data set D, and optimal risk pattern set R.Output representative risk pattern set R01 Set R0  2 For each record r in D belonging to class a3 Find all patterns in R that are subsets of r4 Add the pattern with the highest relative risk toR05 Sort all patterns in R0 in the RR decreasing order6 Return R0As a result, each abnormal record in D has its ownrepresentative risk pattern in R0. Through the aboveselection, the number of patterns becomes manageable.We organise the remaining risk patterns into atree structure and hide them behind each representative pattern by using a hyper link. An example isshown in Fig. 3.As a result, medical practitioners can easilyexamine the representative patterns and find theirrelated patterns. This is very useful for finding theevolution of relative risks.3. Experiments and discussion3.1. Experimental resultsPurposes of experiments are to compare MORE to arulebased classification system C5.0, a commercialversion of C4.5 1, and an association rule miningbased approach. We used two benchmark medicaldata sets from UCML repository 27, which aredescribed in Table 4.3.1.1. Comparison with C5.0For C5.0, we first used the default setting and thenset differential misclassification costs as 20 and 15for data sets Hypothyroid and Sick, respectively.7ry of risk patterns inmedical data. Artif Intell Med 2008,8 J. Li et al.ARTMED1014 No of Pages 13Table 4 A brief description of data sets used inexperimentsName Records Attributes DistributionsHypothyroid 3163 25 4.8 and 95.2Sick 2800 29 6.1 and 93.9Figure 3 A representative pattern and its subpatterns in a tree structure. Users are presented with a small list ofrepresentative patterns. All subpatterns are hidden from users initially, and are brought out when the user clicks therepresentative pattern in order to know the evolution of the relative risks. There are some repetitions in the tree tomakeit easy to follow.The purpose of differential misclassification costs isto penalise misclassifications in some groups. If wedo not set differential misclassification costs DMCin Hypothyroid data set, it only results in 4.8overall error rate that all cases in the hypothyroidgroup are classified as negative. This small overallerror rate reduces the chance of forming rules inhypothyroid group. When we set the differentialPlease cite this article in press as Li J, et al. Efficient discovedoi10.1016j.artmed.2008.07.008Table 5 Comparison with C5.0 by the number of patternsData set C5.0Default number With DMC numberHypothyroid 3 5Sick 3 7misclassification costs as 20 in Hypothyroid dataset, 1 error in the abnormal group is equivalentto 20 errors in the nonabnormal group. As a result,both types of cases have an equal chance for forming rules.We set the minimum local support as 5, and theminimum relative risk as 1.5 for MORE. To comparewith C5.0 fairly, we set the maximum number ofattributevalue pairs in a pattern as four since mostpatterns from C5.0 have four or less attributevaluepairs. Rules discovered by C5.0 with the relative riskless than 1.5 andor with the local support less than5 are filtered. We use this setting since the numberof representative patterns of MORE is comparable tothe number of C5.0 rules. If we set the minimumlocal support low, the number of risk patterns ofry of risk patterns inmedical data. Artif Intell Med 2008,discoveredMORE Optimal number Representative number462 4304 3Efficient discovery of risk patterns in medical data 9ARTMED1014 No of Pages 13Table 7 Comparison with an association rule miningapproachData set Associationpattern numberMOREpattern numberHypothyroid 21807 462Sick 24833 304Table 6 Comparison with C5.0 by the quality of discovered patterns using the average local support and relative riskData set C5.0 default C5.0 with DMC MORE representativeavelsupp aveRR avelsupp aveRR avelsupp aveRRHypothyroid 0.31 29.4 0.43 23.1 0.78 33.3Sick 0.40 27.0 0.29 18.6 0.95 43.1MORE will be larger. This setting is identical to thatin our real world case study in the following section,where setting has been advised by domain experts.Table 5 reports the summary of patterns discovered rules targeting abnormal of C5.0 and MORE onboth data sets. Table 6 lists the average local support and relative risk of discovered patterns by bothmethods.Firstly, C5.0 produces fewer patterns than MORE.The total number patterns discovered by MORE is upto 150 times larger than that of C5.0. Setting differential misclassification costs DMC does not resultin more rules. The exploratory power C5.0 is limitedsince it discovers few rules. C5.0 is designed forbuilding classification models rather than discovering patterns. In contrast, MORE algorithm isdesigned for exploring the data to generate hypotheses for further studying. It is not designed forclassification.Secondly, C5.0 fails to find patterns with thehighest relative risks. The objective of classification is different from identifying risk patterns.Further, rules discovered by C5.0 tend to be specificand are not supported in data. In contrast, MOREcan find patterns with highest relative risk andhighest support. This has been demonstrated inTable 6 that both the average local support andthe average relative risk of representative patternsare higher than those from C5.0. A finetuned decision tree can uncover some interesting patterns in adata set. However, a decision tree does not guarantee the discovery of the patterns with the highestrelative risk nor all patterns with the relative riskabove a threshold because of its heuristic searchtrait. The way of search dictates the difference oftwo methods.3.1.2. Comparison with variant associationrule miningbased approachesAnother approach to discover risk pattern sets isbased on association rule mining. Firstly, find allfrequent patterns in the abnormal group. Secondly,form association rules targeting the abnormal byreplacing the confidence with the relative risk. Wewill show that this approach generates too manypatterns and is inefficient in comparison with MORE.For both MORE and the association rule miningbased approach, we set the minimum local supportPlease cite this article in press as Li J, et al. Efficient discovedoi10.1016j.artmed.2008.07.008as 5, the minimum relative risk as 1.5, and themaximum length of patterns as 4. We implementedthe association rule miningbased approach byApriori 13. However, results reported in this section are independent from the implementationsince the number of discovered rules and frequentpatterns are identical among association rule miningmethods. The summary of discovered patterns arelisted in Table 7.The association rule mining approach producestoo many patterns and many provide superfluousinformation. For example, T3  115 is a riskpattern because T3 is an indicator for sick. Theassociation rule miningbased approach discovers37 patterns with additional conditions, like T3 115, TBGmeasured  f and T3  115, TBGmeasured  f, pregnant f, which have exactly the samerelative risk as pattern T3  115. There areanother 4742 patterns containing T3  115 whichhave lower relative risk in the association rule set.All these patterns are not included in the optimalrisk pattern set. An optimal risk patterns set issmaller than its corresponding risk pattern set discovered by association rule mining, but includeshighest relative risk patterns for all records.Nonredundant association rule mining 17makes use of candidates of minimal generatorsinstead of frequent patterns. It avoids generatinga lot of superfluous rules and is more efficient thanassociation rule mining. However, nonredundantassociation rule mining searches all minimal generators that are a superset of candidates searchedby MORE. Therefore, nonredundant associationrule mining is also less efficient than MORE formining risk patterns.To demonstrate the efficiency improvementobtained by MORE over the association rule miningand nonredundant association rule miningapproaches, we conducted more experiments usingdifferent support settings and high interactions. Wery of risk patterns inmedical data. Artif Intell Med 2008,10 J. Li et al.ARTMED1014 No of Pages 13Figure 4 Comparison of the number of patterns searchedbyMOREwith the number of frequent patterns andminimalgenerators searched by other approaches. MORE searchesfewer candidates and hence more efficient.searched for risk patterns containing up to tenattributevalue pairs. To make the comparison independent of implementation and computers, weshow the number of searched candidates frequentpatterns and minimal generators instead of theexecution time. As a result, the conclusion is general because theoretically the reduction of thesearched candidates is the reduction of computational cost. Fig. 4 shows that MORE searches fewercandidates than both frequent patterns andminimalgenerators, and hence is more efficient than theassociation rule mining and nonredundant association rule miningbased approaches. This is moreevident when the support is lower.In sum, C5.0 is not designed for exploring riskpatterns. Association rule mining is not efficient forexploring risk patterns, and produces too many riskpatterns. MORE is efficient, and produces a manageable number of risk patterns.Please cite this article in press as Li J, et al. Efficient discovedoi10.1016j.artmed.2008.07.0083.2. A case studyThis method has been applied to a real world applications for analysing emergency department administration data.The hospital in this case study is a regional hospital in Australia. Its emergency department has a10bed ultra short stay unit for a short periodobservation. A patient may stay at the ultra shortstay unit for up to 20 hours. Patients staying in theultra short stay unit may be admitted to the hospitalfor further treatment, or may be discharged after abrief observation. In some occasions, the beds arenot enough to cope with a large demand, and doctors need to transfer some patients to the ward. Asignificant reduction in administrative work can beachieved if patients who eventually end up at thehospital are admitted to the ward without staying inultra short stay units after the initial assessment.The emergency department under study has collected 4321 records of patients who have stayed inthe ultra short stay unit over 2 years. 808 records arefor patients who were eventually admitted to thehospital and 3513 records are for patients who weredischarged after a short stay at the ultra short stayunit. Doctors are interested in knowing patterns ofpatients who are admitted. We have done a pilotstudy on this data set.Patients are described by 16 attributes. A triageattribute classify patients into five groups. Somedisease related attributes indicating whetherpatients have the following problems renal, cardio,diabetes, and asthma. Some attributes describepersonal related information, such as gender, agecategorised into four age groups, marital status,and indigenous status. An attribute indicates location information, in town or off town. Some temporal related attributes show season, month, andweek date. An attribute shows whether the patienthas visited the hospital within a week. All values arebinary or categorical.We have used C4.5 to analyse the data set firstly.C4.5 builds a model with an accuracy of 82 on thisdata set. We have not conducted crossvalidation toevaluate the model since it is not our objective tobuild a predictive model. Instead, we are interestedin the rules discovered by C4.5 targeting admittedclass. 20 rules are discovered by C4.5. After we filterrules with 5 local support and 1.5 relative riskthresholds, only two rules are left. Two rules arenot enough for doctors to understand the data set.Furthermore, the two rules do not include thepattern with the highest relative risk.Many rules discovered by C4.5 are of no interestto doctors since the rules do not have sufficientsupport from data. For example, the first two rulesry of risk patterns inmedical data. Artif Intell Med 2008,Efficient discovery of risk patterns in medical data 11ARTMED1014 No of Pages 13Figure 5 Correlation between the rank by accuracy and the rank by relative risk. Two ranks are loosely correlated. Thisexplains why C4.5 does not find the pattern with the highest relative risk.from C4.5 have local supports in number of 11 and3, respectively. They are good classification rulessince their confidence are 100. However, they areof no interest to doctors since they only explain fewcases although their relative risk are high. If weconsider risk patterns at such low minimum supportlevel, there are thousands of them, i.e. 4105 riskpatterns when theminimumnumber of local supportis 9.Furthermore high accurate rules discovered byC4.5 may not be high relative risk patterns. We showthis by the following experiment.We ranked 20 rulesdiscovered by C4.5 first by accuracy and then byrelative risk. We calculated the Spearmans rankcorrelation between the two ranks. The resultsare shown in Fig. 5. We can see that two ranksare loosely correlated. Therefore, discovering accurate classification rules is not suitable for the discovery of risk patterns.When we applied MORE algorithm to the data setwith a support threshold of 5 and relative riskthreshold of 1.5, we discovered 131 risk patternstoward admitting to the hospital where 75 patternsare representatives.Discovered patterns reconfirm many known factors by doctors. For example, patients with cardiovascular or renal related disease are more than twotimes more likely to be admitted to the hospitalthan other patients. Patients with skinsubcutaneousjoint infections are nearly three times morelikely to be admitted to the hospital.Please cite this article in press as Li J, et al. Efficient discovedoi10.1016j.artmed.2008.07.008Discovered patterns show some common practices used by doctors. Patients who live off townare more likely to be admitted to the hospitaleven though their situations are not urgent relative risk of 1.7. This is due to the extra caution ofdoctors.Discovered patterns reveal some interesting phenomena. Male patients with limb injuries are nearlytwo times more likely admitted to the hospitalrelative risk of 1.83. Note that neither malepatients nor limb injuries alone are risky. Thismay be attribute to serious injuries in sports or abias against female limb injury admissions. Patientspresented to the department on Mondays in businesshours are 1.57 times more likely to be admitted tothe hospital than other patients. This shows that thelack of medical service on weekend causes somedelayed admissions. Old male patients are very riskyrelative risk of 2.23 to be admitted to the hospital.This may be due too the fact that male patients arereluctant to see doctors until there is a pressingurgency.Since many combinations have been tested in therisk pattern mining process, some patterns becomessignificant just by chance. Validation is important toaccept or reject them. MORE presents a small set ofwell structured representative hypotheses quicklyfrom a data set. They can be either validated bydomain experts or by further statistical studies.MORE is an efficient data exploratory tool for initialdata analysis.ry of risk patterns inmedical data. Artif Intell Med 2008,12 J. Li et al.ARTMED1014 No of Pages 134. ConclusionsThis paper has discussed a problem of finding riskpatterns in medical data. Risk patterns are definedby an epidemiological metric, relative risk, andhence are understandable to medical practitioners.We define optimal risk pattern set to exclude superfluous patterns that are of no interesting to users.The definition of optimal risk patterns leads to anantimonotone property for efficient discovery, andwe proposed an efficient algorithm for mining optimal risk pattern sets. We have also proposed a wayto organise and present discovered patterns tousers in an easy to explore structure. The proposedmethod has been compared with two well knownrule discovery methods. The method has also beenapplied to a real world medical data set andhas revealed a number of interesting patterns tomedical practitioners. We have the following conclusions from the work a decision tree approachis unsuitable for discovering risk patterns an association rule mining approach is inefficient indiscovering risk patterns and produces too manyuninteresting superfluous patterns and the proposed algorithm discovers a small set of risk patternsefficiently, which includes the highest relative riskpatterns for all records.The method is useful for exploratory study onlarge medical data sets. It quickly discovers somerisk spots in a large medical data set. The resultsare understandable to medical practitioners. It canbe used to generate and refine hypotheses forfurther time consuming statistical studies.AcknowledgementsThis research has been supported by ARCDP0559090, the RGC Earmarked Research Grant ofHKSAR CUHK 417901E, and the Innovation andTechnology Fund ITF in the HKSAR ITS06903.We thank Dr Mark Laverty for providing the opportunity for the case study.References1 Quinlan JR. C4. 5 programs for machine learning. SanMateo, CA Morgan Kaufmann 1993.2 Kononenko I. Machine learning for medical diagnosis history, state of the art and perspective. Artificial Intelligencein Medicine 2001189109.3 Li J, Wong L. Using rules to analyse biomedical data acomparison between c4. 5 and PCL. In Dong G, Tang C,Wang W, editors. Advances in webage information management, proceedings of 4th international conference. BerlinHeidelberg Springer 2003. p. 25465.Please cite this article in press as Li J, et al. Efficient discovedoi10.1016j.artmed.2008.07.0084 Zhou Z, Jiang Y. Medical diagnosis with C4.5 rule preceded byartificial neural network ensemble. IEEE Transactions onInformation Technology in Biomedicine 200313742.5 Agrawal R, Imielinski T, Swami A. Mining association rulesbetween sets of items in large databases. In Buneman P,Jajodia S, editors. Proceedings of ACM SIGMOD internationalconference on management of data. New York ACM 1993.p. 20716.6 Brossette SE, Sprague AP, Hardin JM, Jones KWT, Moser SA.Association rules and data mining in hospital infection control and public health surveillance. Journal of AmericanMedical Informatics Association 1998537381.7 Ohsaki M, Sato Y, Yokoi H, Yamaguchi T. A rule discoverysupport system for sequential medical data in the casestudy of a chronic hepatitis dataset. In Proceedings of theECMLPKDD2003 discovery challenge workshop,  httpwww.lisp.vse.czchallengeecmlpkdd2003, accessed26.3.08.8 Paetz J, Brause RW. A frequent patterns tree approach forrule generation with categorical septic shock patient data.In Crespo J, Maojo V, Martin F, editors. Proceedings of thesecond international symposium on medical data analysis.London SpringerVerlag 2001. p. 20712.9 Li J, chee Fu AW, He H, Chen J, Jin H, McAullay D, et al.Mining risk patterns in medical data. In Grossman R,Bayardo RJ, Bennett KP, editors. Proceedings of the 11thACM SIGKDD international conference on knowledge discovery and data mining. New York ACM 2005. p. 7705.10 Quinlan JR. Induction of decision trees. Machine Learning1986181106.11 Ordonez C. Comparing association rules and decision trees fordisease prediction. In Xiong L, Xia Y, editors. Proceedings ofthe international workshop on healthcare information andknowledge management. New York ACM 2006. p. 1724.12 Hu J, Mojsilovic A. Highutility pattern mining a method fordiscovery of highutility item sets. Pattern Recognition200711331724.13 Agrawal R, Srikant R. Fast algorithms for mining associationrules. In Bocca JB, Jarke M, Zaniolo C, editors. Proceedingsof 20th international conference on very large data bases.San Mateo, CA Morgan Kaufmann 1994. p. 48799.14 Han J, Pei J, Yin Y, Mao R. Mining frequent patterns withoutcandidate generation a frequentpattern tree approach.Data Mining and Knowledge Discovery Journal 200415387.15 Chen J, He H, Williams GJ, Jin H. Temporal sequenceassociations for rare events. In Dai H, Srikant R, ZhangC, editors. Advances in knowledge discovery and datamining, eighth PacificAsia conference. BerlinHeidelbergSpringer 2004. p. 2359.16 Ordonez C, Ezquerra NF, Santana CA. Constraining andsummarizing association rules in medical data. Knowledgeand Information Systems 2006312.17 Zaki MJ. Mining nonredundant association rules. DataMining and Knowledge Discovery Journal 2004322348.18 Bayardo R, Agrawal R, Gunopulos D. Constraintbased rulemining in large, dense database. Data Mining and KnowledgeDiscovery Journal 23 200021740.19 Roberto J, Bayardo J, Agrawal R. Mining the most interestingrules. In Fayyad U, Chaudhuri S, Madigan D, editors.Proceedings of the fifth ACM SIGKDD international conference on knowledge discovery and data mining. New YorkACM 1999. p. 14554.20 Webb GI. Efficient search for association rules. In Ramakrishnan R, Stolfo S, Bayardo R, Parsa I, editors. Proceedingsof the sixth ACM SIGKDD international conference on knowledge discovery and data mining. New York ACM 2000. p.99107.ry of risk patterns inmedical data. Artif Intell Med 2008,Efficient discovery of risk patterns in medical data 13ARTMED1014 No of Pages 1321 Webb GI, Zhang S. Koptimal rule discovery. Data Mining andKnowledge Discovery Journal 200513979.22 Cheung Y, Fu A. Mining association rules without supportthreshold with and without item constraints. IEEE Transactions on Knowledge and Data Engineering 20049105269.23 Li H, Li J, Wong L, Feng M, Tan YP. Relative risk and oddsratio a data mining perspective. In Li C, editor. Proceedings of the 24th ACM SIGMODSIGACTSIGART symposium onprinciples of database systems. New York ACM 2005. p.36877.24 Ohsaki M, Kitaguchi S, Okamoto K, Yokoi H, Yamaguchi T.Evaluation of rule interestingness measures with a clinicaldataset on hepatitis. In Boulicaut JFF., Esposito F, Giannotti F, Pedreschi D, editors. Proceedings of the eighthPlease cite this article in press as Li J, et al. Efficient discovedoi10.1016j.artmed.2008.07.008European conference on principles and practice of knowledge discovery in databases. New York SpringerVerlag2004. p. 36273.25 Triola MM, Triola MF. Biostatistics for the biological andhealth sciences, 2nd ed, Boston AddisonWesley 2005.26 Wang J, Han J, Pei J. Closet searching for the best strategies for mining frequent closed itemsets. In Getoor L,Senator TE, Domingos P, Faloutsos C, editors. Proceedings ofthe ninth ACM SIGKDD international conference on knowledge discovery and data mining. New York ACM 2003. p.23645.27 Asuncion A, Newman DJ. UCI repository of machine learningdatabases,  httpwww.archive.ics.uci.eduml, accessed26.6.08.ry of risk patterns inmedical data. Artif Intell Med 2008,
