14PrivacyPreserving Data Publishing A Survey ofRecent DevelopmentsBENJAMIN C. M. FUNGConcordia University, MontrealKE WANGSimon Fraser University, BurnabyRUI CHENConcordia University, MontrealandPHILIP S. YUUniversity of Illinois at ChicagoThe collection of digital information by governments, corporations, and individuals has created tremendousopportunities for knowledge and informationbased decision making. Driven by mutual benefits, or by regulations that require certain data to be published, there is a demand for the exchange and publication of dataamong various parties. Data in its original form, however, typically contains sensitive information about individuals, and publishing such data will violate individual privacy. The current practice in data publishingrelies mainly on policies and guidelines as to what types of data can be published and on agreements on theuse of published data. This approach alone may lead to excessive data distortion or insufficient protection.Privacypreserving data publishing PPDP provides methods and tools for publishing useful informationwhile preserving data privacy. Recently, PPDP has received considerable attention in research communities, and many approaches have been proposed for different data publishing scenarios. In this survey, wewill systematically summarize and evaluate different approaches to PPDP, study the challenges in practical data publishing, clarify the differences and requirements that distinguish PPDP from other relatedproblems, and propose future research directions.Categories and Subject Descriptors H.2.7 Database Management Database AdministrationSecurity,integrity, and protection H.2.8 Database Management Database ApplicationsData miningGeneral Terms Performance, SecurityAdditional Key Words and Phrases Information sharing, privacy protection, anonymity, sensitive information, data miningThe research is supported in part by NSERC Discovery Grants 3560652008.Authors addresses Concordia Institute for Information Systems Engineering, Concordia University,Montreal, QC, Canada H3G 1M8 email fungciise.concordia.ca.Permission to make digital or hard copies of part or all of this work for personal or classroom use is grantedwithout fee provided that copies are not made or distributed for profit or commercial advantage and thatcopies show this notice on the first page or initial screen of a display along with the full citation. Copyrightsfor components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any componentof this work in other works requires prior specific permission andor a fee. Permissions may be requestedfrom Publications Dept., ACM, Inc., 2 Penn Plaza, Suite 701, New York, NY 101210701 USA, fax 1 2128690481, or permissionsacm.org.c2010 ACM 03600300201006ART14 10.00DOI 10.11451749603.1749605 httpdoi.acm.org10.11451749603.1749605ACM Computing Surveys, Vol. 42, No. 4, Article 14, Publication date June 2010.142 B. C. M. Fung et al.ACM Reference FormatFung, B. C. M., Wang, K., Chen, R., and Yu, P. S. 2010. PrivacyPreserving data publishing A survey ofrecent developments. ACM Comput. Surv. 42, 4, Article 14 June 2010, 53 pages.DOI  10.11451749603.1749605 httpdoi.acm.org10.11451749603.17496051. INTRODUCTIONThe collection of digital information by governments, corporations, and individualshas created tremendous opportunities for knowledgebased decision making. Drivenby mutual benefits, or by regulations that require certain data to be published, thereis a demand for the exchange and publication of data among various parties. For example, licensed hospitals in California are required to submit specific demographic dataon every patient discharged from their facility Carlisle et al. 2007. In June 2004, theInformation Technology Advisory Committee released a report entitled Revolutionizing Health Care Through Information Technology President Information TechnologyAdvisory Committee 2004. One of its key points was to establish a nationwide systemof electronic medical records that encourages sharing of medical knowledge throughcomputerassisted clinical decision support. Data publishing is equally ubiquitous inother domains. For example, Netflix, a popular online movie rental service, recentlypublished a data set containing movie ratings of 500,000 subscribers, in a drive toimprove the accuracy of movie recommendations based on personal preferences NewYork Times, Oct. 2, 2006 AOL published a release of query logs but quickly removedit due to the reidentification of a searcher Barbaro and Zeller 2006.Detailed personspecific data in its original form often contains sensitive informationabout individuals, and publishing such data immediately violates individual privacy.The current practice primarily relies on policies and guidelines to restrict the typesof publishable data and on agreements on the use and storage of sensitive data. Thelimitation of this approach is that it either distorts data excessively or requires a trustlevel that is impractically high in many datasharing scenarios. For example, contractsand agreements cannot guarantee that sensitive data will not be carelessly misplacedand end up in the wrong hands.A task of the utmost importance is to develop methods and tools for publishing datain a more hostile environment, so that the published data remains practically usefulwhile individual privacy is preserved. This undertaking is called privacypreservingdata publishing PPDP. In the past few years, research communities have respondedto this challenge and proposed many approaches. While the research field is stillrapidly developing, it is a good time to discuss the assumptions and desirable properties for PPDP, clarify the differences and requirements that distinguish PPDP fromother related problems, and systematically summarize and evaluate different approaches to PPDP. This survey aims to achieve these goals.1.1. PrivacyPreserving Data PublishingA typical scenario for data collection and publishing is described in Figure 1. In thedata collection phase, the data publisher collects data from record owners e.g., Aliceand Bob. In the data publishing phase, the data publisher releases the collected datato a data miner or to the public, called the data recipient, who will then conduct datamining on the published data. In this survey, data mining has a broad sense, not necessarily restricted to pattern mining or model building. For example, a hospital collectsdata from patients and publishes the patient records to an external medical center. Inthis example, the hospital is the data publisher, patients are record owners, and theACM Computing Surveys, Vol. 42, No. 4, Article 14, Publication date June 2010.PrivacyPreserving Data Publishing A Survey of Recent Developments 143Fig. 1. Data collection and data publishing.medical center is the data recipient. The data mining conducted at the medical center could be anything from a simple count of the number of men with diabetes to asophisticated cluster analysis.There are two models of data publishers Gehrke 2006. In the untrusted model, thedata publisher is not trusted and may attempt to identify sensitive information fromrecord owners. Various cryptographic solutions Yang et al. 2005 anonymous communications Chaum 1981 Jakobsson et al. 2002 and statistical methods Warner 1965were proposed to collect records anonymously from their owners without revealing theowners identity. In the trusted model, the data publisher is trustworthy and recordowners are willing to provide their personal information to the data publisher however, the trust is not transitive to the data recipient. In this survey, we assume thetrusted model of data publishers and consider privacy issues in the data publishingphase.In practice, every data publishing scenario has its own assumptions and requirements of the data publisher, the data recipients, and the data publishing purpose.The following are several desirable assumptions and properties in practical datapublishingThe nonexpert data publisher. The data publisher is not required to have the knowledge to perform data mining on behalf of the data recipient. Any data mining activitieshave to be performed by the data recipient after receiving the data from the data publisher. Sometimes, the data publisher does not even know who the recipients are atthe time of publication, or has no interest in data mining. For example, the hospitalsin California publish patient records on the Web Carlisle et al. 2007. The hospitals donot know who the recipients are and how the recipients will use the data. The hospitalpublishes patient records because it is required by regulations Carlisle et al. 2007or because it supports general medical research, not because the hospital needs theresult of data mining. Therefore, it is not reasonable to expect the data publisher to domore than anonymize the data for publication in such a scenario.In other scenarios, the data publisher is interested in the data mining result, butlacks the inhouse expertise to conduct the analysis, and hence outsources the datamining activities to some external data miners. In this case, the data mining taskperformed by the recipient is known in advance. In the effort to improve the qualityof the data mining result, the data publisher could release a customized data set thatpreserves specific types of patterns for such a data mining task. Still, the actual datamining activities are performed by the data recipient, not by the data publisher.The data recipient could be an attacker. In PPDP, one assumption is that the datarecipient could also be an attacker. For example, the data recipient, say a drug researchcompany, is a trustworthy entity however, it is difficult to guarantee that all staff inthe company is trustworthy as well. This assumption makes the PPDP problems andACM Computing Surveys, Vol. 42, No. 4, Article 14, Publication date June 2010.144 B. C. M. Fung et al.solutions very different from the encryption and cryptographic approaches, in whichonly authorized and trustworthy recipients are given the private key for accessing thecleartext. A major challenge in PPDP is to simultaneously preserve both the privacyand information usefulness in the anonymous data.Publish data, not the data mining result. PPDP emphasizes publishing data recordsabout individuals i.e., micro data. Clearly, this requirement is more stringent thanpublishing data mining results, such as classifiers, association rules, or statistics aboutgroups of individuals. For example, in the case of the Netflix data release, useful information may be some type of associations of movie ratings. However, Netflix decided topublish data records instead of such associations because the participants, with datarecords, have greater flexibility in performing the required analysis and data exploration, such as mining patterns in one partition but not in other partitions visualizingthe transactions containing a specific pattern trying different modeling methods andparameters, and so forth. The assumption for publishing data and not the data mining results, is also closely related to the assumption of a nonexpert data publisher.For example, Netflix does not know in advance how the interested parties might analyze the data. In this case, some basic information nuggets should be retained in thepublished data, but the nuggets cannot replace the data.Truthfulness at the record level. In some data publishing scenarios, it is importantthat each published record corresponds to an existing individual in real life. Considerthe example of patient records. The pharmaceutical researcher the data recipientmay need to examine the actual patient records to discover some previously unknownside effects of the tested drug Emam 2006. If a published record does not correspondto an existing patient in real life, it is difficult to deploy data mining results in thereal world. Randomized and synthetic data do not meet this requirement. Although anencrypted record corresponds to a real life patient, the encryption hides the semanticsrequired for acting on the patient represented.1.2. The Anonymization ApproachIn the most basic form of PPDP, the data publisher has a table of the formDExplicit Identifier, Quasi Identifier, Sensitive Attributes,NonSensitive Attributes,where Explicit Identifier is a set of attributes, such as name and social security number SSN, containing information that explicitly identifies record ownersQuasi Identifier QID is a set of attributes that could potentially identify record owners Sensitive Attributes consists of sensitive personspecific information such as disease, salary, and disability status and NonSensitive Attributes contains all attributesthat do not fall into the previous three categories Burnett et al. 2003. The four setsof attributes are disjoint. Most works assume that each record in the table representsa distinct record owner.Anonymization Cox 1980 Dalenius 1986 refers to the PPDP approach that seeksto hide the identity andor the sensitive data of record owners, assuming that sensitive data must be retained for data analysis. Clearly, explicit identifiers of recordowners must be removed. Even with all explicit identifiers being removed, Sweeney2002a showed a reallife privacy threat to William Weld, former governor of the stateof Massachusetts. In Sweeneys example, an individuals name in a public voter listwas linked with his record in a published medical database through the combinationof zip code, date of birth, and sex, as shown in Figure 2. Each of these attributesdoes not uniquely identify a record owner, but their combination, called the quasiidentifier Dalenius 1986, often singles out a unique or a small number of recordACM Computing Surveys, Vol. 42, No. 4, Article 14, Publication date June 2010.PrivacyPreserving Data Publishing A Survey of Recent Developments 145Fig. 2. Linking to reidentify record owner Sweeney 2002a.owners. According to Sweeney 2002a, 87 of the U.S. population had reported characteristics that likely made them unique based on only such quasiidentifiers.In the above example, the owner of a record is reidentified by linking his quasiidentifier. To perform such linking attacks, the attacker needs two pieces of priorknowledge the victims record in the released data and the quasiidentifier of the victim. Such knowledge can be obtained by observation. For example, the attacker noticedthat his boss was hospitalized, and therefore knew that his bosss medical record wouldappear in the released patient database. Also, it was not difficult for the attacker to obtain his bosss zip code, date of birth, and sex, which could serve as the quasiidentifierin linking attacks.To prevent linking attacks, the data publisher provides an anonymous table,T QID, Sensitive Attributes, NonSensitive Attributes,QID is an anonymous version of the original QID obtained by applying anonymizationoperations to the attributes in QID in the original table D. Anonymization operationshide some detailed information so that several records become indistinguishable withrespect to QID. Consequently, if a person is linked to a record through QID, thatperson is also linked to all other records that have the same value for QID, making thelinking ambiguous. Alternatively, anonymization operations could generate syntheticdata table T based on the statistical properties of the original table D, or add noise tothe original table D. The anonymization problem is to produce an anonymous T thatsatisfies a given privacy requirement determined by the chosen privacy model and toretain as much data utility as possible. An information metric is used to measure theutility of an anonymous table. Note that the NonSensitive Attributes are published ifthey are important to the data mining task.1.3. The ScopeA closely related research area is privacypreserving data mining Aggarwal and Yu2008c. The term, privacypreserving data mining PPDM, emerged in 2000 Agrawaland Srikant 2000. The initial idea of PPDM was to extend traditional data miningtechniques to work with the data modified to mask sensitive information. The key issues were how to modify the data and how to recover the data mining result fromthe modified data. The solutions were often tightly coupled with the data mining algorithms under consideration. In contrast, PPDP may not necessarily be tied to a specificdata mining task, and the data mining task may be unknown at the time of data publishing. Furthermore, some PPDP solutions emphasize preserving the data truthfulness at the record level as discussed earlier, but often PPDM solutions do not preservesuch a property. In recent years, the term PPDM has evolved to cover many otherACM Computing Surveys, Vol. 42, No. 4, Article 14, Publication date June 2010.146 B. C. M. Fung et al.privacy research problems, even though some of them may not directly relate to datamining.Another related area is the study of the noninteractive query model in statisticaldisclosure control Adam and Wortman 1989 Brand 2002, in which the data recipients can submit one query to the system. This type of noninteractive query model maynot fully address the information needs of data recipients because, in some cases, itis very difficult for a data recipient to accurately construct a query for a data miningtask in one shot. Consequently, there are a series of studies on the interactive querymodel Blum et al. 2005 Dwork 2008 Dinur and Nissim 2003, in which the data recipients, unfortunately including attackers, can submit a sequence of queries basedon previously received query results. One limitation of any privacypreserving querysystem is that it can only answer a sublinear number of queries in total otherwise,an attacker or a group of corrupted data recipients will be able to reconstruct all but1  o1 fraction of the original data Blum et al. 2008, which is a very strong violation of privacy. When the maximum number of queries is reached, the system must beclosed to avoid privacy leak. In the case of a noninteractive query model, the attackercan issue an unlimited number of queries and, therefore, a noninteractive query modelcannot achieve the same degree of privacy defined by the interactive model. This survey focuses mainly on the noninteractive query model, but the interactive query modelwill also be briefly discussed in Section 8.1.In this survey, we review recent work on anonymization approaches to privacypreserving data publishing PPDP and provide our own insights into this topic. Thereare several fundamental differences between the recent work on PPDP and the previous work proposed by the official statistics community. Recent work on PPDP considersbackground attacks, inference of sensitive attributes, generalization, and variousnotions of data utility measures, but the work of the official statistics communitydoesnot. The term privacypreserving data publishing has been widely adopted bythe computer science community to refer to the recent work discussed in this survey article. In fact, the official statistics community seldom uses the term privacypreservingdata publishing to refer to their work. In this survey, we do not intend to provide a detailed coverage of the official statistics methods because some decent surveys alreadyexist Adam and Wortman 1989 DomingoFerrer 2001 Moore 1996 Zayatz 2007.We focus on several key issues in PPDP attack models and privacy modelsSection 2 anonymization operations Section 3 information metrics Section 4and anonymization algorithms Section 5. Most research focuses on a single releasefrom a single data publisher. We also consider the work for more practical scenariosSection 6 that deals with dynamic data, multiple releases, and multiple publishers.Much realworld data is nonrelational. We study some recently proposed anonymization techniques for transaction data, moving objects data, and textual data Section 7.Then, we briefly discuss other privacypreserving techniques that are orthogonal toPPDP. Section 8. Finally, we conclude with a summary and discussion of future research directions Section 9.2. ATTACK MODELS AND PRIVACY MODELSWhat is privacy protection Dalenius 1977 provided a very stringent definition access to the published data should not enable the attacker to learn anything extra aboutany target victim compared to no access to the database, even with the presence of anyattackers background knowledge obtained from other sources. Dwork 2006 showedthat such absolute privacy protection is impossible due to the presence of backgroundknowledge. Suppose the age of an individual is sensitive information. Assume an attacker knows that Alices age is 5 years younger than the average age of AmericanACM Computing Surveys, Vol. 42, No. 4, Article 14, Publication date June 2010.PrivacyPreserving Data Publishing A Survey of Recent Developments 147Table I. Privacy ModelsAttack ModelPrivacy Model Record Linkage Attribute Linkage Table Linkage Probabilistic AttackkAnonymity MultiR kAnonymity Diversity  Confidence Bounding ,kAnonymity  X,Y Privacy  k, eAnonymity ,mAnonymity Personalized Privacy tCloseness  Presence c, tIsolation  Differential Privacy  d,  Privacy  Distributional Privacy  women. If the attacker has access to a statistical database that discloses the averageage of American women, then Alices privacy is considered compromised according toDalenius definition, regardless whether or not Alices record is in the database Dwork2006.Most literature on PPDP considers a more relaxed, more practical notion of privacyprotection by assuming the attacker has limited background knowledge. Below, theterm victim refers to the record owner targeted by the attacker. We can broadly classify privacy models into two categories based on their attack principles.The first category considers that a privacy threat occurs when an attacker is ableto link a record owner to a record in a published data table, to a sensitive attributein a published data table, or to the published data table itself. We call these recordlinkage, attribute linkage, and table linkage, respectively. In all three types of linkages,we assume that the attacker knows the QID of the victim. In record and attributelinkages, we further assume that the attacker knows that the victims record is in thereleased table, and seeks to identify the victims record andor sensitive informationfrom the table. In table linkage, the attack seeks to determine the presence or absenceof the victims record in the released table. A data table is considered to be privacypreserving if it can effectively prevent the attacker from successfully performing theselinkages. Sections 2.1 to 2.3 study this category of privacy models.The second category aims at achieving the uninformative principleMachanavajjhala et al. 2006 The published table should provide the attackerwith little additional information beyond the background knowledge. If the attackerhas a large variation between the prior and posterior beliefs, we call it the probabilisticattack. Many privacy models in this family do not explicitly classify attributes in adata table into QID and Sensitive Attributes, but some of them could also thwartthe sensitive linkages in the first category, so the two categories overlap. Section 2.4studies this family of privacy models. Table I summarizes the attack models addressedby the privacy models.2.1. Record LinkageIn the attack of record linkage, some value qid on QID identifies a small number ofrecords in the released table T , called a group. If the victims QID matches the valueqid, the victim is vulnerable to being linked to the small number of records in thegroup. In this case, the attacker faces only a small number of possibilities for theACM Computing Surveys, Vol. 42, No. 4, Article 14, Publication date June 2010.148 B. C. M. Fung et al.Table II. Examples Illustrating Various Attacksa Patient tableJob Sex Age DiseaseEngineer Male 35 HepatitisEngineer Male 38 HepatitisLawyer Male 38 HIVWriter Female 30 FluWriter Female 30 HIVDancer Female 30 HIVDancer Female 30 HIVb External tableName Job Sex AgeAlice Writer Female 30Bob Engineer Male 35Cathy Writer Female 30Doug Lawyer Male 38Emily Dancer Female 30Fred Engineer Male 38Gladys Dancer Female 30Henry Lawyer Male 39Irene Dancer Female 32c 3anonymous patient tableJob Sex Age DiseaseProfessional Male 3540 HepatitisProfessional Male 3540 HepatitisProfessional Male 3540 HIVArtist Female 3035 FluArtist Female 3035 HIVArtist Female 3035 HIVArtist Female 3035 HIVd 4anonymous external tableName Job Sex AgeAlice Artist Female 3035Bob Professional Male 3540Cathy Artist Female 3035Doug Professional Male 3540Emily Artist Female 3035Fred Professional Male 3540Gladys Artist Female 3035Henry Professional Male 3540Irene Artist Female 3035victims record, and with the help of additional knowledge, there is a chance that theattacker could uniquely identify the victims record from the group.Example 2.1. Suppose that a hospital wants to publish the patient records inTable IIa to a research center. Suppose that the research center has access to theexternal table Table IIb and knows that every person with a record in Table IIb hasa record in Table IIa. Joining the two tables on the common attributes Job, Sex, andAge may link the identity of a person to hisher Disease. For example, Doug, a malelawyer who is 38 years old, is identified as an HIV patient by qid  Lawyer, Male,38after the join.kAnonymity. To prevent record linkage through QID, Samarati andSweeney 1998a, 1998b proposed the notion of kanonymity if one record in thetable has some value qid, at least k 1 other records also have the value qid. In otherwords, the minimum group size on QID is at least k. A table satisfying this requirement is called kanonymous. In a kanonymous table, each record is indistinguishablefrom at least k 1 other records with respect to QID. Consequently, the probability oflinking a victim to a specific record through QID is at most 1k.kanonymity cannot be replaced by the privacy models in attribute linkageSection 2.2. Consider a table T that contains no sensitive attributes such as thevoter list in Figure 2. An attacker could possibly use the QID in T to link to the sensitive information in an external source. A kanonymous T can still effectively preventthis type of record linkage without considering the sensitive information. In contrast,the privacy models in attribute linkage assume the existence of sensitive attributesin T .Example 2.2. Table IIc shows a 3anonymous table by generalizing QID  Job,Sex,Age from Table IIa using the taxonomy trees in Figure 3. It has two distinctgroups on QID, namely Professional,Male, 3540 and Artist, Female, 3035.Since each group contains at least 3 records, the table is 3anonymous. If we link therecords in Table IIb to the records in Table IIc through QID, each record is linkedto either no record or at least 3 records in Table IIc.ACM Computing Surveys, Vol. 42, No. 4, Article 14, Publication date June 2010.PrivacyPreserving Data Publishing A Survey of Recent Developments 149Fig. 3. Taxonomy trees for Job, Sex, Age.The kanonymity model assumes that QID is known to the data publisher. Most workconsiders a single QID containing all attributes that can be potentially used in thequasiidentifier. The more attributes included in QID, the more protection kanonymitywould provide. On the other hand, this also implies that more distortion is needed toachieve kanonymity because the records in a group have to agree on more attributes.To address this issue, Fung et al. 2005, 2007 allow the specification of multiple QIDs,assuming that the data publisher knows the potential QIDs for record linkage. Thefollowing example illustrates the use of this specification.Example 2.3. The data publisher wants to publish a table T A, B,C, D, S, whereS is the sensitive attribute, and knows that the data recipient has access to previouslypublished tables T 1A, B, X and T 2C, D,Y , where X and Y are attributes not in T .To prevent linking the records in T to the information on X or Y , the data publishercan specify kanonymity on QID1  A, B and QID2  C, D for T . This means thateach record in T is indistinguishable from a group of at least k records with respect toQID1 and is indistinguishable from a group of at least k records with respect to QID2.The two groups are not necessarily the same. Clearly, this requirement is implied bykanonymity on QID  A, B,C, D, but having kanonymity on both QID1 and QID2does not imply kanonymity on QID.Specifying multiple QIDs is practical only if the data publisher knows how the attackermight perform the linking. Nevertheless, often the data publisher does not have suchinformation. A wrong decision may cause higher privacy risks or higher informationloss. Later, we discuss the dilemma and implications of choosing attributes in QID. Inthe presence of multiple QIDs, some QIDs may be redundant and can be removed bythe following subset propertyObservation 2.1 Subset Property. Let QID  QID. If a table T is kanonymous onQID, then T is also kanonymous on QID. In other words, QID is covered by QID, soQID can be removed from the privacy requirement Fung et al. 2005, 2007 LeFevreet al. 2005.The kanonymity model assumes that each record represents a distinct individual. Ifseveral records in a table represent the same record owner, a group of k records mayrepresent fewer than k record owners, and the record owner may be underprotected.The following example illustrates this point.Example 2.4. A record in the table InpatientPid,Job,Sex,Age, Disease represents that a patient identified by Pid has Job, Sex, Age, and Disease. A patient mayhave several records, one for each disease. In this case, QID  Job,Sex,Age is not akey and kanonymity on QID fails to ensure that each group on QID contains at leastk distinct patients. For example, if each patient has at least 3 diseases, a group of krecords will involve no more than k3 patients.X,Y Anonymity. To address this problem of kanonymity, Wang and Fung 2006proposed the notion of X,Y anonymity, where X and Y are disjoint sets of attributes.ACM Computing Surveys, Vol. 42, No. 4, Article 14, Publication date June 2010.1410 B. C. M. Fung et al.Definition 2.1. Let x be a value on X. The anonymity of x with respect to Y , denoted aY x, is the number of distinct values on Y that cooccur with x. Let AY X minaY x  x  X. A table T satisfies the X,Y anonymity for some specified integerk if AY X  k.X,Y anonymity specifies that each value on X is linked to at least k distinct valueson Y . The kanonymity is the special case where X is the QID and Y is a key in T thatuniquely identifies record owners. X,Y anonymity provides a uniform and flexibleway to specify different types of privacy requirements. If each value on X describesa group of record owners e.g., X  Job,Sex,Age and Y represents the sensitiveattribute e.g., Y  Disease, this means that each group is associated with a diverseset of sensitive values, making it difficult to infer a specific sensitive value. The nextexample shows the usefulness of X,Y anonymity for modeling kanonymity in thecase that several records may represent the same record owner.Example 2.5. Continue from Example 2.4. With X,Y anonymity, we specifykanonymity with respect to patients by letting X  Job,Sex,Age and Y  Pid.That is, each X group is linked to at least k distinct patient IDs, therefore, k distinctpatients.MultiRelational kAnonymity. Most work on kanonymity focuses on anonymizinga single data table however, a reallife database usually contains multiple relationaltables. Nergiz et al. 2007 proposed a privacy model called MultiR kanonymity to ensure kanonymity on multiple relational tables. Their model assumes that a relationaldatabase contains a personspecific table PT and a set of tables T1, . . . ,Tn, where PTcontains a person identifier Pid and some sensitive attributes, and Ti, for 1  i  n,contains some foreign keys, some attributes in QID, and sensitive attributes. The general privacy notion is to ensure that for each record owner o contained in the join of alltables PT  T1      Tn, there exists at least k 1 other record owners who sharethe same QID with o. It is important to emphasize that the kanonymization is appliedat the record owner level, not at the record level in traditional kanonymity. This ideais similar to X,Y anonymity, where X  QID and Y  Pid.Dilemma in choosing QID. One challenge faced by a data publisher is how to classify the attributes in a data table into three disjoint sets QID, Sensitive Attributes,and NonSensitive Attributes. In principle, QID should contain an attribute A if theattacker could potentially obtain A from other external sources. After the QID isdetermined, remaining attributes are grouped into Sensitive Attributes and NonSensitive Attributes based on their sensitivity. There is no definite answer to the question of how a data publisher can determine whether or not an attacker can obtain anattribute A from some external sources, but it is important to understand the implications of a misclassification misclassifying an attribute A into Sensitive Attributes orNonSensitive Attributes may compromise another sensitive attribute S because anattacker may obtain A from other sources and then use A to perform record linkage orattribute linkage on S. On the other hand, misclassifying a sensitive attribute S intoQID may directly compromise sensitive attribute S of some target victim because anattacker may use attributes in QID  S to perform attribute linkage on S. Furthermore, incorrectly including S in QID causes unnecessary information loss due to thecurse of dimensionality Aggarwal 2005.Motwani and Xu 2007 presented a method to determine the minimal set of quasiidentifiers for a data table T . The intuition is to identify a minimal set of attributesfrom T that has the ability to almost distinctly identify a record and the abilityto separate two data records. Nonetheless, the minimal set of QID does not implyACM Computing Surveys, Vol. 42, No. 4, Article 14, Publication date June 2010.PrivacyPreserving Data Publishing A Survey of Recent Developments 1411the most appropriate privacy protection setting because the method does not considerwhat attributes the attacker could potentially have. If the attacker can obtain a bitmore information about the target victim beyond the minimal set, then he may able toconduct a successful linking attack. The choice of QID remains an open issue.kanonymity, X,Y anonymity, and MultiR kanonymity prevent record linkage byhiding the record of a victim in a large group of records with the same QID. However, ifmost records in a group have similar values on a sensitive attribute, the attacker canstill associate the victim to her sensitive value without having to identify her record.This situation is illustrated in Table IIc, which is 3anonymous. For a victim matchingqid  Artist,Female, 3035, the confidence of inferring that the victim has HIV is75 because 3 out of the 4 records in the group have HIV. Though X,Y anonymityrequires that each X group is linked to at least k distinct Y values, if some Y valuesoccur more frequently than others, there is a higher confidence of inferring the morefrequent values. This leads us to the next family of privacy models for preventing thistype of attribute linkage.2.2. Attribute LinkageIn the attack of attribute linkage, the attacker may not precisely identify the record ofthe target victim, but could infer hisher sensitive values from the published data T ,based on the set of sensitive values associated to the group that the victim belongs to.In case some sensitive values predominate in a group, a successful inference becomesrelatively easy even if kanonymity is satisfied. Clifton 2000 suggested eliminatingattribute linkages by limiting the released data size. Limiting data size may not bedesirable if data records such as HIV patient data, are valuable and are difficult toobtain. Several other approaches have been proposed to address this type of threat.The general idea is to diminish the correlation between QID attributes and sensitiveattributes.Example 2.6. From Table IIa, an attacker can infer that all female dancers atage 30 have HIV, i.e., Dancer,Female,30  HIV with 100 confidence. Applyingthis knowledge to Table IIb, the attacker can infer that Emily has HIV with 100confidence provided that Emily comes from the same population in Table IIa.Diversity. Machanavajjhala et al. 2006, 2007 proposed the diversity principle,called diversity, to prevent attribute linkage. The diversity requires every qid groupto contain at least  wellrepresented sensitive values. A similar idea was previouslydiscussed in Ohrn and OhnoMachado 1999. There are several instantiations of thisprinciple, which differ in the definition of being wellrepresented. The simplest understanding of wellrepresented is to ensure that there are at least  distinct valuesfor the sensitive attribute in each qid group. This distinct diversity privacy modelalso known as psensitive kanonymity Truta and Bindu 2006 automatically satisfies kanonymity, where k  , because each qid group contains at least  records.Distinct diversity cannot prevent probabilistic inference attacks because some sensitive values are naturally more frequent than others in a group, enabling an attackerto conclude that a record in the group is very likely to have those values. For example,Flu is more common than HIV. This motivates the following two stronger notions ofdiversity.A table is entropy diverse if for every qid groupsSPqid, slogPqid, s  log 1ACM Computing Surveys, Vol. 42, No. 4, Article 14, Publication date June 2010.1412 B. C. M. Fung et al.where S is a sensitive attribute, and Pqid, s is the fraction of records in a qid grouphaving the sensitive value s. The lefthand side, called the entropy of the sensitiveattribute, has the property that more evenly distributed sensitive values in a qid groupproduce a larger value. Therefore, a large threshold value  implies less certainty ofinferring a particular sensitive value in a group. Note that the inequality does notdepend on the choice of the log base.Example 2.7. Consider Table IIc. For the first group Professional, Male, 35 40,  23log 23  13log 13  log1.9, and for the second group Artist, Female, 3035, 34log 34  14log 14  log1.8. So the table satisfies entropy diversity if   1.8.One limitation of entropy diversity is that it does not provide a probability based riskmeasure, which tends to be more intuitive to the human data publisher. For example,in Table IIc, being entropy 1.8diverse in Example 2.7 does not convey the risk levelthat the attacker has 75 probability of success to infer HIV where 3 out of the 4 recordowners in the qid group have HIV. Also, it is difficult to specify different protectionlevels based on varied sensitivity and frequency of sensitive values.The recursive c, diversity makes sure that the most frequent value does not appear too frequently, and that the less frequent values do not appear too rarely. Letm be the number of sensitive values in a qid group. Let fi denote the frequency ofthe ith most frequent sensitive value in a qid group. A qid group is c, diverse if thefrequency of the most frequent sensitive value is less than the sum of the frequencies of the m   1 least frequent sensitive values multiplying by some publisherspecified constant c, that is, f1  cmi fi. The intuition is that even if the attackerexcludes some possible sensitive values of a victim by applying background knowledge, the remaining ones remain hard to infer. A table is considered to have recursivec, diversity if all of its groups have c, diversity.Machanavajjhala et al. 2006, 2007 also presented two other instantiations, calledpositive disclosurerecursive c, diversity and negativepositive disclosurerecursivec, diversity to capture the attackers background knowledge. Suppose a victim isin a qid group that contains three different sensitive values Flu, Cancer,HIV , andsuppose the attacker knows that the victim has no symptom of having a flu. Giventhis piece of background knowledge, the attacker can eliminate Flu from the set ofcandidatesensitive values of the victim. Martin et al. 2007 proposed a language tocapture this type of background knowledge and to represent the knowledge as k unitsof information. Furthermore, the language could capture the type of implication knowledge. For example, given that Alice, Bob, and Cathy have flu, the attacker infers thatDoug is very likely to have flu, too, because all four of them live together. This implication is considered to be one unit of information. Given an anonymous table T andk units of background knowledge, Martin et al. 2007 estimated the maximum disclosure risk of T , which is the probability of the most likely predicted sensitive valueassignment of any record owner in T .diversity has the limitation of implicitly assuming that each sensitive attributetakes values uniformly over its domain, that is, the frequencies of the various valuesof a confidential attribute are similar. When this is not the case, achieving diversitymay cause a large data utility loss. Consider a data table containing data of 1000patients on some QID attributes and a single sensitive attribute HIV with two possiblevalues, Yes or No. Assume that there are only 5 patients with HIV  Yes in the table.To achieve kanonymity with k  , at least one patient with HIV is needed in each qidgroup therefore, at most 5 groups can be formed DomingoFerrer and Torra 2008.Enforcing kanonymity with k   may lead to high information loss in this case.ACM Computing Surveys, Vol. 42, No. 4, Article 14, Publication date June 2010.PrivacyPreserving Data Publishing A Survey of Recent Developments 1413Confidence bounding. Wang et al. 2005, 2007 considered bounding the confidence ofinferring a sensitive value from a qid group by specifying one or more privacy templatesof the form, QID  s,h s is a sensitive value, QID is a quasiidentifier, and h is athreshold. Let Conf QID  s be maxconf qid  s over all qid groups on QID,where conf qid  s denotes the percentage of records containing s in the qid group.A table satisfies QID  s,h if Conf QID  s  h. In other words, QID  s,hbounds the attackers confidence of inferring the sensitive value s in any group on QIDto the maximum h.For example, with QID  Job,Sex,Age, QID  HIV,10 states that the confidence of inferring HIV from any group on QID is no more than 10. For the data inTable IIc, this privacy template is violated because the confidence of inferring HIV is75 in the group Artist,Female, 3035.The confidence measure has two advantages over recursive c, diversity and entropy diversity. First, the confidence measure is more intuitive because the risk ismeasured by the probability of inferring a sensitive value. The data publisher relieson this intuition to specify the acceptable maximum confidence threshold. Second, itallows the flexibility for the data publisher to specify a different threshold h for eachcombination of QID and s according to the perceived sensitivity of inferring s from agroup on QID. The recursive c, diversity cannot be used to bound the frequency ofsensitive values that are not the most frequent. Confidence bounding provides greaterflexibility than diversity in this aspect. However, recursive c, diversity can stillprevent attribute linkages, even in the presence of background knowledge discussedearlier. Confidence bounding does not share the same merit.X,Y Privacy. X,Y anonymity in Section 2.1 states that each group on X has atleast k distinct values on Y e.g., diseases. However, if some Y values occur more frequently than others, the probability of inferring a particular Y value can be higherthan 1k. To address this issue, Wang and Fung 2006 proposed a general privacymodel, called X,Y Privacy, which combines both X,Y anonymity and confidencebounding. The general idea is to require each group x on X to contain at least k recordsand conf x  y  h for any y  Y , where Y is a set of selected sensitive values and his a maximum confidence threshold.,kAnonymity. Wong et al. 2006 proposed a similar integrated privacy model,called ,kanonymity, requiring every qid in a Table T to be shared by at least krecords and conf qid  s   for any sensitive value s, where k and  are datapublisherspecified thresholds. Nonetheless, both X,Y Privacy and ,kanonymitymay result in high distortion if the sensitive values are skewed.k, eAnonymity. Most work on kanonymity and its extensions assumes categoricalsensitive attributes. Zhang et al. 2007 proposed the notion of k, eanonymity to address numerical sensitive attributes such as salary. The general idea is to partitionthe records into groups so that each group contains at least k different sensitive valueswith a range of at least e. However, k, eanonymity ignores the distribution of sensitive values within the range . If some sensitive values occur frequently within a subrange of , then the attacker could still confidently infer the subrange in a group. Thistype of attribute linkage attack is called the proximity attack Li et al. 2008. Considera qid group of 10 data records with 7 different sensitive values, where 9 records havesensitive values in 3035, and 1 record has value 80. The group is 7,50anonymousbecause 80  30  50. Still, the attacker can infer that a victim inside the group has asensitive value falling into 3035 with 90 confidence. Li et al. 2008 proposed an alternative privacy model, called ,manonymity. Given any numerical sensitive values in T , this privacy model bounds the probability of inferring s , s  to be at most1m.ACM Computing Surveys, Vol. 42, No. 4, Article 14, Publication date June 2010.1414 B. C. M. Fung et al.tCloseness. In a spirit similar to the uninformative principle discussed earlier, Liet al. 2007 observed that when the overall distribution of a sensitive attribute isskewed, diversity does not prevent attribute linkage attacks. Consider a patient tablewhere 95 of records have Flu and 5 of records have HIV. Suppose that a qid grouphas 50 of Flu and 50 of HIV and, therefore, satisfies 2diversity. However, thisgroup presents a serious privacy threat because any record owner in the group couldbe inferred as having HIV with 50 confidence, compared to 5 in the overall table.To prevent skewness attack, Li et al. 2007 proposed a privacy model, called tCloseness, which requires the distribution of a sensitive attribute in any group onQID to be close to the distribution of the attribute in the overall table. tcloseness usesthe Earth Mover Distance EMD function to measure the closeness between two distributions of sensitive values, and requires the closeness to be within t. tcloseness hasseveral limitations and weaknesses. First, it lacks the flexibility of specifying differentprotection levels for different sensitive values. Second, the EMD function is not suitable for preventing attribute linkage on numerical sensitive attributes Li et al. 2008.Third, enforcing tcloseness would greatly degrade the data utility because it requiresthe distribution of sensitive values to be the same in all qid groups. This would significantly damage the correlation between QID and sensitive attributes. One way todecrease the damage is to relax the requirement by adjusting the thresholds with theincreased risk of skewness attack DomingoFerrer and Torra 2008, or to employ theprobabilistic privacy models in Section 2.4.Personalized Privacy. Xiao and Tao 2006b proposed the notion of personalized privacy to allow each record owner to specify her own privacy level. This model assumesthat each sensitive attribute has a taxonomy tree and that each record owner specifiesa guarding node in this tree. The record owners privacy is violated if an attacker isable to infer any domain sensitive value within the subtree of her guarding node witha probability, called breach probability, greater than a certain threshold. For example,suppose HIV and SARS are child nodes of Infectious Disease in the taxonomy tree. AHIV patient Alice can set the guarding node to Infectious Disease, meaning that sheallows people to infer that she has some infectious diseases, but not any specific typeof infectious disease. Another HIV patient, Bob, does not mind disclosing his medicalinformation, so he does not set any guarding node for this sensitive attribute.Although both confidence bounding and personalized privacy take an approach tobound the confidence or probability of inferring a sensitive value from a qid group,they have differences. In the confidence bounding approach, the data publisher imposes a universal privacy requirement on the entire data set, so the minimum levelof privacy protection is the same for every record owner. In the personalized privacyapproach, a guarding node is specified for each record by its owner. The advantage isthat each record owner may specify a guarding node according to her own toleranceon sensitivity. Experiments show that this personalized privacy requirement could result in lower information loss than the universal privacy requirement Xiao and Tao2006b. In practice, however, it is unclear how individual record owners would settheir guarding node. Often, a reasonable guarding node depends on the distribution ofsensitive values in the whole table or in a group. For example, knowing that her disease is very common, a record owner may set a more special lower privacy protectedguarding node for her record. Nonetheless, the record owners usually have no accessto the distribution of sensitive values in their qid group or in the whole table beforethe data is published. Without such information, the tendency is to play safe by settinga more general higher privacy protected guarding node, which may negatively affectthe utility of data.FFAnonymity. All previous work assumes that the data table can be divided intoquasiidentifying QID attributes and sensitive attributes. Yet, this assumption doesACM Computing Surveys, Vol. 42, No. 4, Article 14, Publication date June 2010.PrivacyPreserving Data Publishing A Survey of Recent Developments 1415not hold when an attribute contains both sensitive values and quasiidentifying values.Wang et al. 2009 identify a class of freeform attacks of the form X  s, where s and thevalues in X can be any value of any attribute in the table T . X  s is a privacy breachif any record in T matching X can infer a sensitive value s with a high probability.Their proposed privacy model, FFanonymity, bounds the probability of all potentialprivacy breaches in the form X  s to be below a given threshold.2.3. Table LinkageBoth record linkage and attribute linkage assume that the attacker already knows thevictims record is in the released table T . However, in some cases, the presence or theabsence of the victims record in T already reveals the victimss sensitive information.Suppose a hospital releases a data table with a particular type of disease. Identifyingthe presence of the victims record in the table is already damaging. A table linkageoccurs if an attacker can confidently infer the presence or the absence of the victimsrecord in the released table. The following example illustrates the privacy threat of atable linkage.Example 2.8. Suppose the data publisher has released a 3anonymous patient table T Table IIc. To launch a table linkage on a target victim, for instance, Alice,on T , the attacker is presumed to also have access to an external public table ETable IId where T  E. The probability that Alice is present in T is 45  0.8because there are 4 records in T Table IIc and 5 records in E Table IId containing Artist,Female, 30  35. Similarly, the probability that Bob is present in Tis 34  0.75.Presence. To prevent table linkage, Nergiz et al. 2007 proposed to bound the probability of inferring the presence of any potential victims record within a specified range  min, max. Formally, given an external public table E and a private table T , whereT  E, a generalized table T  satisfies min, maxpresence if min  Pt  T T   maxfor all t  E. presence can indirectly prevent record and attribute linkages because ifthe attacker has at most  of confidence that the target victims record is present inthe released table, then the probability of a successful linkage to her record and sensitive attribute is at most . Though presence is a relatively safe privacy model,it assumes that the data publisher has access to the same external table E as theattacker does. This may not be a practical assumption.2.4. Probabilistic AttackThere is another family of privacy models that does not focus on exactly what records,attributes, and tables the attacker can link to a target victim, but focuses on howthe attacker would change hisher probabilistic belief on the sensitive information ofa victim after accessing the published data. In general, this group of privacy models aims at achieving the uninformative principle Machanavajjhala et al. 2006,whose goal is to ensure that the difference between the prior and posterior beliefs issmall.c, tIsolation. Chawla et al. 2005 suggested that having access to the publishedanonymous data table should not enhance an attackers power of isolating any recordowner. Consequently, they proposed a privacy model to prevent c, tisolation in a statistical database. Suppose p is a data point of a target victim v in a data table, and q isthe attackers inferred data point of v by using the published data and the backgroundinformation. Let p be the distance between p and q. We say that point q c, tisolatespoint p if Bq, cp contains fewer than t points in the table, where Bq, cp is a ball ofACM Computing Surveys, Vol. 42, No. 4, Article 14, Publication date June 2010.1416 B. C. M. Fung et al.radius cp centered at point q. Preventing c, tisolation can be viewed as preventingrecord linkages. Their model considers distances among data records and, therefore, ismore suitable for numerical attributes in statistical databases.Differential privacy. Dwork 2006 proposed an insightful privacy notion the riskto the record owners privacy should not substantially increase as a result of participating in a statistical database. Instead of comparing the prior probability and theposterior probability before and after accessing the published data, Dwork proposedto compare the risk with and without the record owners data in the published data.Consequently, Dwork 2006 proposed a privacy model called differential privacy toensure that the removal or addition of a single database record does not significantlyaffect the outcome of any analysis. It follows that no risk is incurred by joining different databases. Based on the same intuition, if a record owner does not provide hisheractual information to the data publisher, it will not make much difference in the resultof the anonymization algorithm.The following is a more formal definition of differential privacy Dwork 2006 Arandomized function F ensures differential privacy if for all data sets T1 and T2 differing on at most one record, lnPFT1SPFT2S    for all S  RangeF, where RangeF isthe set of possible outputs of the randomized function F. Although differential privacy does not prevent record and attribute linkages studied in earlier sections, it assures record owners that they may submit their personal information to the databasesecurely in the knowledge that nothing, or almost nothing, can be discovered from thedatabase with their information that could not have been discovered without theirinformation. Dwork 2006 formally proved that differential privacy can provide aguarantee against attackers with arbitrary background knowledge. This strong guarantee is achieved by comparison with and without the record owners data in thepublished data. Dwork 2007 proved that if the number of queries is sublinear inn, the noise to achieve differential privacy is bounded by on, where n is the number of records in the database. Dwork 2008 further showed that the notion of differential privacy is applicable to both interactive and noninteractive query models,discussed in Sections 1.1 and 8.1 refer to Dwork 2008 for a survey on differentialprivacy.d,  Privacy. Rastogi et al. 2007 presented a probabilistic privacy definition d,  privacy. Let Pr and PrT  be the prior probability and the posterior probability ofthe presence of a victims record in the data table T before and after examining thepublished table T . d,  privacy bounds the difference of the prior and posterior probabilities and provides a provable guarantee on privacy and information utility, whilemost previous work lacks such a formal guarantee. Rastogi et al. 2007 showed that areasonable tradeoff between privacy and utility can be achieved only when the priorbelief is small. Nonetheless, d,  privacy is designed to protect only against attacksthat are dindependent an attack is dindependent if the prior belief Pr satisfies theconditions Pr  1 or Pr  d for all records r, where Pr  1 means that the attacker already knows that r is in T and no protection on r is needed. Machanavajjhalaet al. 2008 pointed out that this dindependence assumption may not hold in somereallife applications. Differential privacy in comparison does not have to assume thatrecords are independent or that an attacker has a prior belief bounded by a probabilitydistribution.Distributional privacy. Motivated by the learning theory, Blum et al. 2008 presented a privacy model called distributional privacy for a noninteractive query model.The key idea is that when a data table is drawn from a distribution, the table shouldreveal only information about the underlying distribution, and nothing else. Distributional privacy is a strictly stronger privacy notion than differential privacy, andACM Computing Surveys, Vol. 42, No. 4, Article 14, Publication date June 2010.PrivacyPreserving Data Publishing A Survey of Recent Developments 1417can answer all queries over a discretized domain in a concept class of polynomialVCdimension.1 Yet, the algorithm has high computational cost. Blum et al. 2008 presented an efficient algorithm specifically for simple interval queries with limited constraints. The problems of developing efficient algorithms for more complicated queriesremain open.3. ANONYMIZATION OPERATIONSTypically, the original table does not satisfy a specified privacy requirement and the table must be modified before being published. The modification is done by applying a sequence of anonymization operations to the table. An anonymization operation comes inseveral flavors generalization, suppression, anatomization, permutation, and perturbation. Generalization and suppression replace values of specific description, typicallythe QID attributes, with less specific description. Anatomization and permutation deassociate the correlation between QID and sensitive attributes by grouping and shuffling sensitive values in a qid group. Perturbation distorts the data by adding noise,aggregating values, swapping values, or generating synthetic data based on some statistical properties of the original data. Below, we discuss these anonymization operations in detail.3.1. Generalization and SuppressionEach generalization or suppression operation hides some details in QID. For a categorical attribute, a specific value can be replaced with a general value according to agiven taxonomy. In Figure 3, the parent node Professional is more general than thechild nodes Engineer and Lawyer. The root node, ANY Job, represents the most general value in Job. For a numerical attribute, exact values can be replaced with aninterval that covers exact values. If a taxonomy of intervals is given, the situation issimilar to categorical attributes. More often, however, no predetermined taxonomy isgiven for a numerical attribute. Different classes of anonymization operations havedifferent implications on privacy protection, data utility, and search space. But theyall result in a less precise but consistent representation of the original data.A generalization replaces some values with a parent value in the taxonomy of anattribute. The reverse operation of generalization is called specialization. A suppression replaces some values with a special value, indicating that the replaced valuesare not disclosed. The reverse operation of suppression is called disclosure. Below, wesummarize five generalization schemes.Fulldomain generalization scheme LeFevre et al. 2005 Samarati 2001 Sweeney2002b. In this scheme, all values in an attribute are generalized to the same level ofthe taxonomy tree. For example, in Figure 3, if Lawyer and Engineer are generalized toProfessional, then it also requires generalizing Dancer and Writer to Artist. The searchspace for this scheme is much smaller than the search space for other schemes below,but the data distortion is the largest because of the same granularity level requirementon all paths of a taxonomy tree.Subtree generalization scheme Bayardo and Agrawal 2005 Fung et al. 2005, 2007Iyengar 2002 LeFevre et al. 2005. In this scheme, at a nonleaf node, either all childvalues or none are generalized. For example, in Figure 3, if Engineer is generalized toProfessional, this scheme also requires the other child node, Lawyer, to be generalizedto Professional, but Dancer and Writer, which are child nodes of Artist, can remainungeneralized. Intuitively, a generalized attribute has values that form a cut through1VapnikChervonenkis dimension is a measure of the capacity of a statistical classification algorithm.ACM Computing Surveys, Vol. 42, No. 4, Article 14, Publication date June 2010.1418 B. C. M. Fung et al.its taxonomy tree. A cut of a tree is a subset of values in the tree that contains exactlyone value on each roottoleaf path.Sibling generalization scheme LeFevre et al. 2005. This scheme is similar to thesubtree generalization, except that some siblings may remain ungeneralized. A parent value is then interpreted as representing all missing child values. For example, inFigure 3, if Engineer is generalized to Professional, and Lawyer remains ungeneralized, Professional is interpreted as all jobs covered by Professional except for Lawyer.This scheme produces less distortion than subtree generalization schemes because itonly needs to generalize the child nodes that violate the specified threshold.Cell generalization scheme LeFevre et al. 2005 Wong et al. 2006 Xu et al. 2006.In all of the above schemes, if a value is generalized, all its instances are generalized.Such schemes are called global recoding. In cell generalization, also known as local recoding, some instances of a value may remain ungeneralized while other instances aregeneralized. For example, in Table IIa the Engineer in the first record is generalizedto Professional, while the Engineer in the second record can remain ungeneralized.Compared with global recoding schemes, this scheme is more flexible and thereforeit produces a smaller data distortion. Nonetheless, it is important to note that theutility of data is adversely affected by this flexibility, which causes a data explorationproblem most standard data mining methods treat Engineer and Professional as twoindependent values, but, in fact, they are not. For example, building a decision treefrom such a generalized table may result in two branches, Professional  class2 andEngineer  class1. It is unclear which branch should be used to classify a new engineer. Though very important, this aspect of data utility has been ignored by all workthat employed the local recoding scheme. Data produced by global recoding does notsuffer from this data exploration problem.Multidimensional generalization LeFevre et al. 2006a, 2006b. Let Di be the domain of an attribute Ai. A singledimensional generalization, such as fulldomaingeneralization and subtree generalization, is defined by a function fi  DAi  Dfor each attribute Ai in QID. In contrast, a multidimensional generalization is defined by a single function f  DA1      DAn  D, which is used to generalizeqid  v1, . . . , vn to qid  u1, . . . ,un where for every vi, either vi  ui or vi is achild node of ui in the taxonomy of Ai. This scheme flexibly allows two qid groups,even having the same value v, to be independently generalized into different parentgroups. For example Engineer,Male can be generalized to Engineer, ANY Sex whileEngineer,Female can be generalized to Professional,Female. The generalized tablecontains both Engineer and Professional. This scheme produces less distortion thanthe fulldomain and subtree generalization schemes because it needs to generalize onlythe qid groups that violate the specified threshold. Note that in this multidimensionalscheme, all records in a qid are generalized to the same qid, but cell generalizationdoes not have such constraint. Although both schemes suffer from the data explorationproblem, Nergiz and Clifton 2007 further evaluated a family of clusteringbased algorithms that even attempted to improve data utility by ignoring the restrictions ofthe given taxonomies.There are also different suppression schemes. Record suppression Bayardo andAgrawal 2005 Iyengar 2002 LeFevre et al. 2005 Samarati 2001 refers to suppressing an entire record. Value suppression Wang et al. 2005, 2007 refers to suppressingevery instance of a given value in a table. Cell suppression or local suppression Cox1980 Meyerson and Williams 2004 refers to suppressing some instances of a givenvalue in a table.In summary, the choice of anonymization operations has an implication on thesearch space of anonymous tables and data distortion. The fulldomain generalizationACM Computing Surveys, Vol. 42, No. 4, Article 14, Publication date June 2010.PrivacyPreserving Data Publishing A Survey of Recent Developments 1419has the smallest search space but the largest distortion, and the local recoding schemehas the largest search space but the least distortion. For a categorical attribute witha taxonomy tree H, the number of possible cuts in subtree generalization, denotedCH, is equal to CH1      CHu  1 where H1, . . . , Hu are the subtrees rooted atthe children of the root of H, and 1 is for the trivial cut at the root of H. The number ofpotential modified tables is equal to the product of such numbers for all the attributesin QID. The corresponding number is much larger if a local recoding scheme is adoptedbecause any subset of values can be generalized while the rest remains ungeneralizedfor each attribute in QID.A table is minimally anonymous if it satisfies the given privacy requirement and itssequence of anonymization operations cannot be reduced without violating the requirement. A table is optimally anonymous if it satisfies the given privacy requirement andcontains most information according to the chosen information metric among all satisfying tables. See Section 4 for different types of information metrics. Various workshave shown that finding the optimal anonymization is NPhard Samarati 2001showed that the optimal kanonymity by fulldomain generalization is very costlyMeyerson and Williams 2004 and Aggarwal et al. 2005 proved that the optimal kanonymity by cell suppression, value suppression, and cell generalization is NPhardWong et al. 2006 proved that the optimal ,kanonymity by cell generalization isNPhard. In most cases, finding a minimally anonymous table is a reasonable solution, and can be done efficiently.3.2. Anatomization and PermutationAnatomization Xiao and Tao 2006a. Unlike generalization and suppression, anatomization does not modify the quasiidentifier or the sensitive attribute, but deassociates the relationship between the two. Precisely, the method releases the data onQID and the data on the sensitive attribute in two separate tables a quasiidentifiertable QIT contains the QID attributes, a sensitive table ST contains the sensitiveattributes, and both QIT and ST have one common attribute, GroupID. All records inthe same group will have the same value on GroupID in both tables, and thereforeare linked to the sensitive values in the group in the exact same way. If a group has distinct sensitive values and each distinct value occurs exactly once in the group, thenthe probability of linking a record to a sensitive value by GroupID is 1. The attributelinkage attack can be distorted by increasing .Example 3.1. Suppose that the data publisher wants to release the patient datain Table IIIa, where Disease is a sensitive attribute and QID  Age,Sex. First,partition or generalize the original records into qid groups so that, in each group, atmost 1 of the records contain the same Disease value. This intermediate Table IIbcontains two qid groups 3035,Male and 3540,Female. Next, create QITTable IIIc to contain all records from the original Table IIIa, but replace the sensitive values by the GroupIDs, and create ST Table IIId to contain the count of eachDisease for each qid group. QIT and ST satisfy the privacy requirement with   2because each qid group in QIT infers any associated Disease in ST with probability atmost 1  12  50.The major advantage of anatomy is that the data in both QIT and ST is unmodified.Xiao and Tao 2006a showed that the anatomized tables can more accurately answeraggregate queries involving domain values of the QID and sensitive attributes thanthe generalization approach. The intuition is that, in a generalized table, domain values are lost, and without additional knowledge, the uniform distribution assumptionis the best that can be used to answer a query about domain values. In contrast, allACM Computing Surveys, Vol. 42, No. 4, Article 14, Publication date June 2010.1420 B. C. M. Fung et al.Table III. Anatomya Original patient dataDiseaseAge Sex sensitive30 Male Hepatitis30 Male Hepatitis30 Male HIV32 Male Hepatitis32 Male HIV32 Male HIV36 Female Flu38 Female Flu38 Female Heart38 Female Heartb Intermediate QIDgrouped tableDiseaseAge Sex sensitive3035 Male Hepatitis3035 Male Hepatitis3035 Male HIV3035 Male Hepatitis3035 Male HIV3035 Male HIV3540 Female Flu3540 Female Flu3540 Female Heart3540 Female Heartc Quasiidentifier table QITfor releaseAge Sex GroupID30 Male 130 Male 130 Male 132 Male 132 Male 132 Male 136 Female 238 Female 238 Female 238 Female 2d Sensitive table ST for releaseDiseaseGroupID sensitive Count1 Hepatitis 31 HIV 32 Flu 22 Heart 2domain values are retained in the anatomized tables, which give the exact distributionof domain values. For instance, suppose that the data recipient wants to count thenumber of patients of age 38 having heart disease. The correct count from the originalTable IIIa is 2. The expected count from the anatomized Table IIIc and Table IIIdis 3  24  1.5, since 2 out of the 4 records in GroupID  2 in Table IIId have heartdisease. This count is more accurate than the expected count 2 15  0.4, from the generalized Table IIIb, where the 15 comes from the fact that the 2 patients with heartdisease have an equal chance to be of age 35,36,37,38,39.Yet, with the data published in two tables, it is unclear how standard data mining tools such as classification, clustering, and association mining tools can be applied to the published data, and new tools and algorithms need to be designed. Also,anatomy is not suitable for continuous data publishing, which will be discussed furtherin Section 6.3. The generalization approach does not suffer from the same problem because all attributes are released in the same table.Permutation. Sharing the same spirit of anatomization, Zhang et al. 2007 proposedan approach called permutation. The idea is to deassociate the relationship betweena quasiidentifier and a numerical sensitive attribute by partitioning a set of datarecords into groups and shuffling their sensitive values within each group.3.3. PerturbationPerturbation has a long history in statistical disclosure control Adam and Wortman1989 due to its simplicity, efficiency, and ability to preserve statistical information.The general idea is to replace the original data values with some synthetic data values, so that the statistical information computed from the perturbed data does not differ significantly from the statistical information computed from the original data. Theperturbed data records do not correspond to realworld record owners, so the attackerACM Computing Surveys, Vol. 42, No. 4, Article 14, Publication date June 2010.PrivacyPreserving Data Publishing A Survey of Recent Developments 1421cannot perform the sensitive linkages or recover sensitive information from the published data.Compared to the other anonymization operations discussed earlier, one limitation ofthe perturbation approach is that the published records are synthetic in that theydo not correspond to the realworld entities represented by the original data therefore, individual records in the perturbed data are basically meaningless to the humanrecipients. Only the statistical properties explicitly selected by the data publisher arepreserved. In such a case, the data publisher may consider releasing the statistical information or the data mining results rather than the perturbed data DomingoFerrer2008. In contrast, generalization and suppression make the data less precise than, butsemantically consistent with, the raw data, and hence preserve the truthfulness of thedata. For example, after analyzing the statistical properties of a collection of perturbedpatient records, a drug company wants to focus on a small number of patients for further analysis. This stage requires the truthful record information instead of perturbedrecord information. Below, we discuss several commonly used perturbation methods,including additive noise, data swapping, and synthetic data generation.Additive noise. Additive noise is a widely used privacy protection method in statistical disclosure control Adam and Wortman 1989 Brand 2002. It is often used forhiding sensitive numerical data e.g., salary. The general idea is to replace the originalsensitive value s with s, r where r is a random value drawn from some distribution.Privacy was measured by how closely the original values of a modified attribute can beestimated Agrawal and Aggarwal 2001. Fuller 1993 and Kim and Winkler 1995showed that some simple statistical information, like means and correlations, can bepreserved by adding random noise. Experiments in Agrawal and Srikant 2000, Duand Zhan 2003, and Evfimievski et al. 2002 further suggested that some data mining information can be preserved in the randomized data. However, Kargupta et al.2003 pointed out that some reasonably close sensitive values can be recovered fromthe randomized data when the correlation among attributes is high but the noise isnot. Huang et al. 2005 presented an improved randomization method to limit thistype of privacy breach. Some representative statistical disclosure control methods thatemploy additive noise are discussed in Sections 2.4 and 5.5.Data swapping. The general idea of data swapping is to anonymize a data table by exchanging values of sensitive attributes among individual records, while theswaps maintain the loworder frequency counts or marginals for statistical analysis.It can be used to protect numerical attributes Reiss et al. 1982 and categorical attributes Reiss 1984. An alternative swapping method is rank swapping First rankthe values of an attribute A in ascending order. Then for each value v  A, swap vwith another value u  A, where u is randomly chosen within a restricted range p ofv. Rank swapping can better preserve statistical information than the ordinary dataswapping DomingoFerrer and Torra 2002.Synthetic data generation. Many statistical disclosure control methods use syntheticdata generation to preserve record owners privacy and retain useful statistical information Rubin. The general idea is to build a statistical model from the data andthen to sample points from the model. These sampled points form the synthetic datafor data publication instead of the original data. An alternative synthetic data generation approach is condensation Aggarwal and Yu 2008a, 2008b. The idea is to firstcondense the records into multiple groups. For each group, extract some statisticalinformation, such as sum and covariance, that suffices to preserve the mean and correlations across the different attributes. Then, based on the statistical information, forpublication generate points for each group following the statistical characteristics ofthe group.ACM Computing Surveys, Vol. 42, No. 4, Article 14, Publication date June 2010.1422 B. C. M. Fung et al.4. INFORMATION METRICSPrivacy preservation is one side of anonymization. The other side is retaining information so that the published data remains practically useful. There are broad categoriesof information metrics for measuring data usefulness. A data metric measures thedata quality in the entire anonymous table with respect to the data quality in the original table. A search metric guides each step of an anonymization search algorithmto identify an anonymous table with maximum information or minimum distortion.Often, this is achieved by ranking a set of possible anonymization operations and thengreedily performing the best one at each step in the search. Since the anonymoustable produced by a search metric is eventually evaluated by a data metric, the twotypes of metrics usually share the same principle for measuring data quality.Alternatively, an information metric can be categorized by its information purposes,including general purpose, special purpose, or tradeoff purpose. Below, we discuss somecommonly used data and search metrics according to their purposes.4.1. General Purpose MetricsIn many cases, the data publisher does not know how the published data will be analyzed by the recipient. This is very different from privacypreserving data miningPPDM, which assumes that the data mining task is known. In PPDP, for example, the data may be published on the Web and a recipient may analyze the dataaccording to her own purpose. An information metric good for one recipient maynot be good for another recipient. In such scenarios, a reasonable information metric is to measure similarity between the original data and the anonymous data,which underpins the principle of minimal distortion Samarati 2001 Sweeney 1998,2002b. In the minimal distortion metric or MD, a penalty is charged to each instance of a value that is generalized or suppressed. For example, generalizing 10instances of Engineer to Professional causes 10 units of distortion, and further generalizing these instances to ANY Job causes another 10 units of distortion. Thismetric is a single attribute measure, and was previously used in Samarati 2001,Sweeney 2002a, 2002b, and Wang and Fung 2006 as a data metric and searchmetric.ILoss is a data metric proposed in Xiao and Tao 2006b to capture the informationloss of generalizing a specific value to a general value vg ILossvg  vg1DA where vgis the number of domain values that are descendants of vg, and DA is the numberof domain values in the attribute A of vg. This data metric requires all original datavalues to be at the leaves in the taxonomy. ILossvg  0 if vg is an original datavalue in the table. In words, ILossvg measures the fraction of domain values generalized by vg. For example, generalizing one instance of Dancer to Artist in Figure 3 hasILossArtist  214  0.25. The loss of a generalized record r is given byILossr vgrwi  ILossvg, 2where wi is a positive constant specifying the penalty weight of attribute Ai of vg. Theoverall loss of a generalized table T is given byILossT  rTILossr. 3ACM Computing Surveys, Vol. 42, No. 4, Article 14, Publication date June 2010.PrivacyPreserving Data Publishing A Survey of Recent Developments 1423Both MD and ILoss charge a penalty for generalizing a value in a record independently of other records. For example, generalizing 99 instances of Engineer and1 instance of Lawyer to Professional will have the same penalty as generalizing 50instances of Engineer and 50 instances of Lawyer. In both cases, 100 instances aremade indistinguishable. The difference is that, before the generalization, 99 instanceswere already indistinguishable in the first case, whereas only 50 instances are indistinguishable in the second case. Therefore, the second case makes more originally distinguishable records become indistinguishable. The discernibility metric, orDM Skowron and Rauszer 1992, addresses this notion of loss by charging a penaltyto each record for being indistinguishable from other records with respect to QID.If a record belongs to a group of size s, the penalty for the record will be s. Thisdata metric, used in Bayardo and Agrawal 2005, LeFevre et al. 2006a, Machanavajjhala et al. 2006, 2007, Vinterbo 2004, and Xu et al. 2006, works exactly againstthe kanonymization that seeks to make records indistinguishable with respect toQID.A simple search metric, called distinctive attribute, or DA, was employed in Sweeney1998 to guide the search for a minimally anonymous table in a fulldomain generalization scheme. The heuristic selects the attribute having the most number of distinctive values in the data for generalization. Note that this type of simple heuristic onlyserves the purpose of guiding the search, but does not quantify the utility of an anonymous table.4.2. Special Purpose MetricsIf the purpose of the data is known at the time of publication, the purpose can betaken into account during anonymization to better retain information. For example, ifthe data is published for modeling the classification of a target attribute in the table,then it is important not to generalize the values whose distinctions are essential fordiscriminating the class labels in the target attribute. An oftenasked question is ifthe purpose of data is known, why not extract and publish a data mining result forthat purpose such as a classifier instead of the data Nergiz and Clifton 2007 Theanswer is that publishing a data mining result is a commitment at the algorithmiclevel, which is neither practical for the nonexpert data publisher nor desirable forthe data recipient. In practice, there are many ways to mine the data even for a givenpurpose, and typically it is unknown which one is the best until the data is received anddifferent ways are tried. A reallife example is the release of the Netflix data New YorkTimes, Oct. 2, 2006 discussed in Section 1. Netflix wanted to provide the participantsthe greatest flexibility in performing their desired analysis, instead of limiting themto a specific type of analysis.For concreteness, let us consider the classification problem where the goal is to classify future cases into some predetermined classes, drawn from the same underlyingpopulation as the training cases in the published data. The training cases contain boththe useful classification information that can improve the classification model, and theuseless noise that can degrade the classification model. Specifically, the useful classification information is the information that can differentiate the target classes, andholds not only for training cases, but also for future cases. In contrast, the useless noiseholds only for training cases. Clearly, only the useful classification information thathelps classification should be retained. For example, a patients birth year is likely tobe part of information for classifying lung cancer if the disease occurs more frequentlyamong elderly people, but the exact birth date is likely to be noise. In this case, generalizing birth date to birth year helps classification because it eliminates the noise.This example shows that simply minimizing the distortion to the data, as adopted byACM Computing Surveys, Vol. 42, No. 4, Article 14, Publication date June 2010.1424 B. C. M. Fung et al.all general purpose metrics and optimal kanonymization, is not addressing the rightproblem.To address the classification goal, the distortion should be measured by the classification error on future cases. Since future data is not available in most scenarios,most developed methods Fung et al. 2005, 2007 Iyengar 2002 measure accuracy onthe training data. Research results in Fung et al. 2005, 2007 suggest that the usefulclassification knowledge is captured by different combinations of attributes. Generalization and suppression may destroy some of these useful classification structures,but other useful structures may emerge to help. In some cases, generalization and suppression may even improve the classification accuracy because some noise has beenremoved.Iyengar 2002 presented the first work on PPDP for classification. He proposed theclassification metric, or CM, to measure the classification error on the training data.The idea is to charge a penalty for each record suppressed or generalized to a group inwhich the records class is not the majority class. The intuition is that a record havinga nonmajority class in a group will be classified as the majority class, which is an errorbecause it disagrees with the records original class.CM is a data metric, and hence penalizes modification to the training data. This doesnot quite address the classification goal, which is actually better off by generalizinguseless noise into useful classification information. For classification, a more relevantapproach is searching for a good anonymization according to some heuristics. In otherwords, instead of optimizing a data metric, this approach employs a search metric torank anonymization operations at each step in the search. An anonymization operation is ranked high if it retains useful classification information. The search metriccould be adapted by different anonymization algorithms. For example, a greedy algorithm or a hillclimbing optimization algorithm can be used to identify a minimal sequence of anonymization operations for a given search metric. We discuss anonymization algorithms in Section 5.Neither a data metric nor a search metric guarantees a good classification for future cases. It is essential to experimentally evaluate the impact of anonymization bybuilding a classifier from the anonymous data and seeing how it performs on testingcases. Few works Fung et al. 2005, 2007 Iyengar 2002 LeFevre et al. 2006b Wanget al. 2004 have actually conducted such experiments, although many, such as Bayardo and Agrawal 2005, adopted CM in an attempt to address the classificationproblem.4.3. Tradeoff MetricsThe special purpose information metrics aim at preserving data usefulness for a givendata mining task. The catch is that the anonymization operation that gains maximuminformation may also lose so much privacy that no other anonymization operation canbe performed. The idea of tradeoff metrics is to consider both the privacy and information requirements at every anonymization operation and to determine an optimaltradeoff between the two requirements.Fung et al. 2005, 2007 proposed a search metric based on the principle of informationprivacy tradeoff. Suppose that the anonymous table is searched by iterativelyspecializing a general value into child values. Each specialization operation splitseach group containing the general value into a number of groups, one for each childvalue. Each specialization operation s gains some information, denoted IGs, and losessome privacy, PLs. This search metric prefers the specialization s that maximizes theACM Computing Surveys, Vol. 42, No. 4, Article 14, Publication date June 2010.PrivacyPreserving Data Publishing A Survey of Recent Developments 1425information gained per each loss of privacyIGPLs  IGsPLs  1 . 4The choice of IGs and PLs depends on the information metric and privacy model.For example, in classification analysis, IGs could be the information gain Quinlan1993 defined as the decrease of the class entropy Shannon 1948 after specializinga general group into several specialized groups. Alternatively, IGs could be the decrease of distortion measured by MD, described in Section 4.1, after performing s. Forkanonymity, Fung et al. 2005, 2007 measured the privacy loss PLs by the averagedecrease of anonymity over all QIDj that contain the attribute of s, that is,PLs  avgAQIDj  AsQIDj,where AQIDj and AsQIDj denote the anonymity of QIDj before and after the specialization. One variant is to maximize the gain of information by setting PLs to zero.The catch is that the specialization that gains maximum information may also lose somuch privacy that no other specializations can be performed. Note that the principleof informationprivacy tradeoff can also be used to select a generalization g, in whichcase it will minimizeILPGg  ILgPGg  1 , 5where ILg denotes the information loss and PGg denotes the privacy gain by performing g.5. ANONYMIZATION ALGORITHMSIn this section, we examine some representative anonymization algorithms. Refer toTable IV for a characterization based on the privacy model Section 2, anonymizationoperation Section 3, and information metric Section 4. Our presentation of algorithms is organized according to linkage models we then discuss the potential privacythreats, even though a data table has been optimally anonymized.5.1. Algorithms for the Record Linkage ModelWe broadly classify record linkage anonymization algorithms into three families thefirst two, optimal anonymization and minimal anonymization, use generalization andsuppression methods the third family uses perturbation methods.5.1.1. Optimal Anonymization Algorithms. The first family finds an optimal kanonymization, for a given data metric by limiting to fulldomain generalization andrecord suppression. Since the search space for the fulldomain generalization schemeis much smaller than other schemes, finding an optimal solution is feasible for smalldata sets. This type of exhaustive search, however, is not scalable to large data sets,especially if a more flexible anonymization scheme is employed.MinGen. Sweeneys 2002b MinGen algorithm exhaustively examines all potentialfulldomain generalizations to identify the optimal generalization measured in MD.Sweeney acknowledged that this exhaustive search is impractical even for the modestsized data sets, motivating the second family of kanonymization algorithms for laterdiscussion. Samarati 2001 proposed a binary search algorithm that first identifiesall minimal generalizations, and then finds the optimal generalization measured inACM Computing Surveys, Vol. 42, No. 4, Article 14, Publication date June 2010.1426 B. C. M. Fung et al.Table IV. Characterization of Anonymization AlgorithmsAlgorithm Operation Metric OptimalityRecord LinkageBinary Search Samarati 2001 FG,RS MD optimalMinGen Sweeney 2002b FG,RS MD optimalIncognito LeFevre et al. 2005 FG,RS MD optimalKOptimize Bayardo and Agrawal 2005 SG,RS DM,CM optimalargus Hundepool and Willenborg 1996 SG,CS MD minimalDatafly Sweeney 1998 FG,RS DA minimalGenetic Algorithm Iyengar 2002 SG,RS CM minimalBottomUp Generalization Wang et al. 2004 SG ILPG minimalTopDown Specialization TDS Fung et al. 2005, 2007 SG,VS IGPL minimalTDS for Cluster Analysis Fung et al. 2009 SG,VS IGPL minimalMondrian Multidimensional LeFevre et al. 2006a MG DM minimalBottomUp  TopDown Greedy Xu et al. 2006 CG DM minimalTDS2P Wang et al. 2005 Mohammed et al. 2009 SG IGPL minimalCondensation Aggarwal and Yu 2008a, 2008b CD heuristics minimalrGather Clustering Aggarwal et al. 2006 CL heuristics minimalAttribute LinkageTopDown Disclosure Wang et al. 2005, 2007 VS IGPL minimalProgressive Local Recoding Wong et al. 2006 CG MD minimalDiversity Incognito Machanavajjhala et al. 2007 FG,RS MD,DM optimalInfoGain Mondrian LeFevre et al. 2006b MG IG minimalAnatomy Xiao and Tao 2006a AM heuristics minimalk, eAnonymity Permutation Zhang et al. 2007 PM min. error optimalGreedy Personalized Xiao and Tao 2006b SG,CG ILoss minimaltCloseness Incognito Li et al. 2007 FG,RS DM optimalTable LinkageSPALM Nergiz et al. 2007 FG DM optimalMPALM Nergiz et al. 2007 MG heuristics minimalProbabilistic AttackCrossTraining Round Sanitization Chawla et al. 2005 AN statistical NADifferential Privacy Additive Noise Dwork 2006 AN statistical NA Algorithm Rastogi et al. 2007 AN,SP statistical NAFG  Fulldomain Generalization, SG  Subtree Generalization, CG  Cell Generalization,MG  Multidimensional Generalization, RS  Record Suppression, VS Value Suppression,CS  Cell Suppression, AM  Anatomization, PM  Permutation, AN  Additive Noise, SP Sampling, CD  Condensation, CLClusteringMD. Enumerating all minimal generalizations is an expensive operation, and hencenot scalable for large data sets.Incognito. LeFevre et al. 2005 presented a suite of optimal bottomup generalization algorithms, called Incognito, to generate all possible kanonymous fulldomaingeneralizations. These algorithms exploit the rollup property for computing the size ofqid groups.Observation 5.1 Rollup Property. If qid is a generalization of qid1, . . . ,qidc,then qid  ci1 qidi.The rollup property states that the parent group size qid can be directly computedfrom the sum of all child group sizes qidi, implying that the group size qid of allpossible generalizations can be incrementally computed in a bottomup manner. Thisproperty not only allows efficient computation of group sizes, but also provides aterminating condition for further generalizations, leading to the generalizationpropertyACM Computing Surveys, Vol. 42, No. 4, Article 14, Publication date June 2010.PrivacyPreserving Data Publishing A Survey of Recent Developments 1427Observation 5.2 Generalization Property. Let T  be a table not more specific thantable T on all attributes in QID. If T is kanonymous on QID, then T  is also kanonymous on QID.The generalization property provides the basis for effectively pruning the search spaceof generalized tables. This property is essential for efficiently determining an optimalkanonymization LeFevre et al. 2005 Samarati 2001. Consider a qid in a table T . Ifqid is a generalization of qid and qid  k, then qid  k. Thus, if T is kanonymous,there is no need to generalize T further because any further generalizations of T mustalso be kanonymous but with higher distortion, and therefore not optimal according to,for example, the minimal distortion metric MD. Although Incognito significantly outperforms the binary search in efficiency Samarati 2001, the complexity of all three algorithms, namely MinGen, binary search, and Incognito, increases exponentially withthe size of QID.KOptimize. Another algorithm called KOptimize Bayardo and Agrawal 2005 effectively prunes nonoptimal anonymous tables by modeling the search space using aset enumeration tree. Each node represents a kanonymous solution. The algorithmassumes a totally ordered set of attribute values and examines the tree in a topdownmanner, starting from the most general table, and prunes a node in the tree when noneof its descendants could be a global optimal solution based on discernibility metric DMand classification metric CM. Unlike the above algorithms, KOptimize employs thesubtree generalization and record suppression schemes. It is the only efficient optimalalgorithm that uses the flexible subtree generalization.5.1.2. Minimal Anonymization Algorithms. The second family of algorithms produces aminimal kanonymous table by employing a greedy search guided by a search metric.Being heuristic in nature, these algorithms find a minimally anonymous solution, butare more scalable than the previous family.argus. The argus algorithm Hundepool and Willenborg 1996 computes the frequency of all 3value combinations of domain values, then greedily applies subtreegeneralizations and cell suppressions to achieve kanonymity. Since the method limitsthe size of attribute combination, the resulting data may not be kanonymous whenmore than 3 attributes are considered.Datafly. Sweeneys 1998 Datafly system was the first kanonymization algorithmscalable to handle reallife large data sets. It achieves kanonymization by generatingan array of qid group sizes and greedily generalizing those combinations with less thank occurrences based on a heuristic search metric DA that selects the attribute withthe largest number of distinct values. Datafly employs fulldomain generalization andrecord suppression schemes.Genetic. Iyengar 2002 was among the first to aim at preserving classification information in kanonymous data by employing a genetic algorithm with an incomplete stochastic search based on classification metric CM and a subtree generalizationscheme. The idea is to encode each state of generalization as a chromosome and encode data distortion by a fitness function. The search process is a genetic evolutionthat converges to the fittest chromosome. Iyengars experiments suggested that, byconsidering the classification purpose, the classifier built from the anonymous dataproduces lower classification error than the classifier built from the anonymous datausing a general purpose metric. However, experiments also showed that this geneticalgorithm is inefficient for large data sets.BottomUp Generalization. To address the efficiency issue in kanonymization, abottomup generalization algorithm was proposed in Wang et al. 2004 for findinga minimal kanonymization for classification. The algorithm starts from the originalACM Computing Surveys, Vol. 42, No. 4, Article 14, Publication date June 2010.1428 B. C. M. Fung et al.data that violates kanonymity and greedily selects a generalization operation at eachstep according to a search metric similar to ILPG in Eq. 5. Each operation increasesthe group size according to the rollup property in Observation 5.1. The generalizationprocess is terminated as soon as all groups have the minimum size k. To select a generalization operation, it first considers those that will increase the minimum groupsize, called critical generalizations, with the intuition that a loss of information shouldtrade for some gain on privacy. When there are no critical generalizations, it considers other generalizations. Wang et al. 2004 showed that this heuristic significantlyreduces the search space.TopDown Specialization. Instead of bottomup, the topdown specialization TDSmethod Fung et al. 2005, 2007 generalizes a table by specializing it from the mostgeneral state in which all values are generalized to the most general values of theirtaxonomy trees. At each step, TDS selects the specialization according to the searchmetric IGPL in Eq. 4. The specialization process terminates if no specialization canbe performed without violating kanonymity. The data on termination is a minimal kanonymization according to the generalization property in Observation 5.2. TDS handles both categorical and numerical attributes in a uniform way, except that the taxonomy tree for a numerical attribute is grown onthefly as specializations are searchedat each step.Fung et al. 2008, 2009 further extended the kanonymization algorithm to preservethe information for cluster analysis. The major challenge in anonymizing data for cluster analysis is the lack of class labels that could be used to guide the anonymizationprocess. Fung et al.s solution is to first partition the original data into clusters on theoriginal data convert the problem into the counterpart problem for classification analysis, where class labels encode the cluster information in the data and then apply TDSto preserve kanonymity and the encoded cluster information.In contrast to the bottomup approach LeFevre et al. 2005 Samarati 2001 Wanget al. 2004, the topdown approach has several advantages. First, the user can stop thespecialization process at any time and have a kanonymous table. In fact, every step inthe specialization process produces a kanonymous solution. Second, TDS handles multiple QIDs, which is essential for avoiding the excessive distortion suffered by a singlehighdimensional QID. Third, the topdown approach is more efficient by going fromthe most generalized table to a more specific table. Once a group cannot be specialized further, all data records in the group can be discarded. In contrast, the bottomupapproach has to keep all data records until the end of computation. However, datapublishers employing TDS may encounter the dilemma of choosing multiple QID,discussed in Section 2.1.Mondrian Multidimensional. LeFevre et al. 2006a presented a greedy topdownspecialization algorithm for finding a minimal kanonymization in the case of the multidimensional generalization scheme. This algorithm is very similar to TDS. Both algorithms perform a specialization on a value v one at a time. The major difference isthat TDS specializes in all qid groups containing v. In other words, a specializationis performed only if each specialized qid group contains at least k records. In contrast, Mondrian performs a specialization on one qid group if each of its specializedqid groups contains at least k records. Due to such a relaxed constraint, the resulting anonymous data in multidimensional generalization usually has a better qualitythan in single generalization. The tradeoff is that multidimensional generalization isless scalable than other schemes due to the increased search space. Xu et al. 2006showed that employing cell generalization could further improve the data quality.Although the multidimensional and cell generalization schemes cause less information loss, they suffer from the data exploration problem discussed in Section 3.ACM Computing Surveys, Vol. 42, No. 4, Article 14, Publication date June 2010.PrivacyPreserving Data Publishing A Survey of Recent Developments 14295.1.3. Perturbation Algorithms. This family of anonymization methods employs perturbation to deassociate the linkages between a target victim and a record while preserving some statistical information.Condensation. Aggarwal and Yu 2008a, 2008b presented a condensation method tothwart record linkages. The method first assigns records into multiple nonoverlappinggroups in which each group has a size of at least k records. For each group, extractsome statistical information, such as sum and covariance, that suffices to preservethe mean and correlations across the different attributes. Then, for publishing, basedon the statistical information, generate points for each group following the statisticalcharacteristics of the group. This method does not require the use of taxonomy treesand can be effectively used in situations with dynamic data updates as in the case ofdata streams. As each new data record is received, it is added to the nearest group,as determined by the distance to each group centroid. As soon as the number of datarecords in the group equals 2k, the corresponding group needs to be split into twogroups of k records each. The statistical information of the new group is then incrementally computed from the original group.rGather Clustering. In a similar spirit, Aggarwal et al. 2006 proposed a perturbation method called rgather clustering. This method partitions records into severalclusters such that each cluster contains at least r data points i.e., records. Insteadof generalizing individual records, this approach releases the cluster centers, togetherwith their size, radius, and a set of associated sensitive values. To eliminate the impactof outliers, they relaxed this requirement to r, gather clustering so that at most fraction of data records in the data set can be treated as outliers for removal from thereleased data.CrossTraining Round Sanitization. Recall from Section 2.1 that point q c, tisolatespoint p if Bq, cp contains fewer than t points in the table, where Bq, cp is aball of radius cp centered at point q. Chawla et al. 2005 proposed two sanitization anonymization techniques, recursive histogram sanitization and densitybasedperturbation, to prevent c, tisolation.Recursive histogram sanitization recursively divides original data into a set ofsubcubes according to local data density until all subcubes have no more than 2tdata points. The method outputs the boundaries of the subcubes and the number ofpoints in each subcube. However, this method cannot handle highdimensional spheresand balls. Chawla et al. 2005 proposed an extension to handle highdimensionality.Densitybased perturbation, a variant of the one proposed by Agrawal and Srikant2000, in which the magnitude of the added noise is relatively fixed, takes into consideration the local data density near the point that needs to be perturbed. Points indense areas are perturbed much less than points in sparse areas. Although the privacyof the perturbed points is protected, the privacy of the points in the tneighborhoodof the perturbed points could be compromised because the sanitization radius itself could leak information about these points. To prevent such privacy leakage fromtneighborhood points, Chawla et al. 2005 further suggested a crosstraining roundsanitization method by combining recursive histogram sanitization and densitybasedperturbation. In crosstraining round sanitization, a dataset is randomly divided intotwo subsets, A and B. B is sanitized using only recursive histogram sanitization,while A is perturbed by adding Gaussian noise generated according to the histogramof B.5.2. Algorithms for the Attribute Linkage ModelThe following algorithms anonymize the data to prevent attribute linkages. They usethe privacy models discussed in Section 2.2. Though their privacy models are differentACM Computing Surveys, Vol. 42, No. 4, Article 14, Publication date June 2010.1430 B. C. M. Fung et al.from those of record linkage, many algorithms for attribute linkage are simple extensions from algorithms for record linkage.The following algorithms adopt diversity as the privacy model. Recall that diversity requires every qid group to contain at least  wellrepresented sensitivevalues.Diversity Incognito. Machanavajjhala et al. 2006, 2007 modified the bottomupIncognito LeFevre et al. 2005 to identify an optimal diverse table. The DiversityIncognito operates based on the generalization property, similar to Observation 5.2,that diversity is nondecreasing with respect to generalization. In other words, generalizations help to achieve diversity, just as generalizations help achieve kanonymity.Therefore, kanonymization algorithms that employ fulldomain and subtree generalization can also be extended into diversity algorithms.InfoGain Mondrian. LeFevre et al. 2006b proposed a suite of greedy algorithmsto identify a minimally anonymous table satisfying kanonymity andor entropy diversity with the consideration of a specific data analysis task such as classificationmodeling multiple target attributes and query answering with minimal imprecision.Their topdown algorithms are similar to TDS Fung et al. 2005, but LeFevre et al.2006b employed multidimensional generalization.TopDown Disclosure. Recall that a privacy template has the form QID  s,h, andstates that the confidence of inferring the sensitive value s from any group on QID isno more than h. Wang et al. 2005, 2007 proposed an efficient algorithm to minimallysuppress a table to satisfy a set of privacy templates. Their algorithm, called TopDownDisclosure TDD, iteratively discloses domain values starting from the table in whichall domain values are suppressed. In each iteration, it discloses the suppressed domainvalue that maximizes the search metric IGPL in Eq. 4, and terminates the iterativeprocess when a further disclosure leads to a violation of some privacy templates. Thisapproach is based on the following key observation.Observation 5.3 Disclosure Property. Consider a privacy template QID  s,h.If a table violates the privacy template, so does any table obtained by disclosing asuppressed value Wang et al. 2007.This property ensures that the algorithm finds a minimally suppressed table. Thisproperty, and therefore the algorithm, is extendable to fulldomain, subtree, and sibling generalization schemes, with the disclosure operation being replaced by the specialization operation. The basic observation is that the confidence in at least one of thespecialized groups will be as large as the confidence in the general group. Based on asimilar idea, Wong et al. 2006 employed the cell generalization scheme and proposedsome greedy topdown and bottomup methods to identify a minimally anonymous solution that satisfies ,kanonymity.k, eAnonymity Permutation. To achieve k, eanonymity, Zhang et al. 2007 proposed an optimal permutation method to assign data records into groups together,so that the sum of error E is minimized, where E, for example, could be measuredby the range of sensitive values in each group. The optimal algorithm has time andspace complexity in On2, where n is the number of data records. k, eanonymity isalso closely related to a range coding technique, which is used in both process control Rosen et al. 1992 and official statistics Hegland et al. 1999. In process control,range coding also known as coarse coding permits generalization by allowing thewhole numerical area to be mapped to a set of groups defined by a set of boundaries,which is similar to the idea of grouping data records by ranges and keeping boundaries of each group for fast computation in k, eanonymity. Hegland et al. 1999 alsosuggested handling large data sets as population census data, by dividing them intoACM Computing Surveys, Vol. 42, No. 4, Article 14, Publication date June 2010.PrivacyPreserving Data Publishing A Survey of Recent Developments 1431generalized groups blocks and applying a computational model to each group. Anyaggregate computation can hence be performed based on manipulation of individualgroups. Similarly, k, eanonymity exploits the group boundaries to efficiently answeraggregate queries.Personalized Privacy. Refer to the requirement of personalized privacy discussed inSection 2.2. Xiao and Tao 2006b proposed a greedy algorithm to achieve every recordowners privacy requirement in terms of a guarding node, as follows initially, all QIDattributes are generalized to the most general values, and the sensitive attributesremain ungeneralized. At each iteration, the algorithm performs a topdown specialization on a QID attribute and, for each qid group, performs cell generalization on thesensitive attribute to achieve the personalized privacy requirement the breach probability of inferring any domainsensitive values within the subtree of guarding nodes isbelow a certain threshold. Since the breach probability is nonincreasing with respectto generalization on the sensitive attribute and the sensitive values could possibly begeneralized to the most general values, the generalized table found at every iterationis publishable without violating the privacy requirement, although a table with lowerinformation loss ILoss, measured by Eq. 3, is preferable. When no better solutionwith lower ILoss is found, the greedy algorithm terminates and outputs a minimalanonymization. Since this approach generalizes the sensitive attribute, ILoss is measured on both QID and sensitive attributes.5.3. Algorithms for the Table Linkage ModelThe following algorithms aim at preventing table linkages, that is, preventing attackers from determining the presence or the absence of a target victims record in a released table.Presence Algorithms SPALM and MPALM Recall that a generalized table T  satisfies min,  maxpresence or simply presence with respect to an external table E ifmin  Pt  T T   max for all t  E. To achieve presence, Nergiz et al. 2007presented two anonymization algorithms, SPALM and MPALM. SPALM is an optimal algorithm that employs a fulldomain singledimensional generalization scheme.Nergiz et al. 2007 proved the antimonotonicity property of presence with respectto fulldomain generalization if table T is present, then a generalized version of T is also present. SPALM is a topdown specialization approach and exploits the antimonotonicity property of presence to prune the search space effectively. MPALM is aminimal algorithm that employs a multidimensional generalization scheme, with complexity OCElog2E, where C is the number of attributes in private table T andE is the number of records in the external table E. Their experiments showed thatMPALM usually results in much lower information loss than SPALM because MPALMemploys a more flexible generalization scheme.5.4. Minimality Attack on Anonymous DataMost privacy models assume that the attacker knows the QID of a target victimandor the presence of the victims record in the published data. In addition to thisbackground knowledge, the attacker can possibly determine the privacy requiremente.g., 10anonymity or 5diversity, the anonymization operations e.g., subtree generalization scheme to achieve the privacy requirement, and the detailed mechanismof an anonymization algorithm. The attacker can possibly determine the privacy requirement and anonymization operations by examining the published data, or itsdocumentation, and learn the mechanism of the anonymization algorithm by, for example, reading research papers. Wong et al. 2007 pointed out that such additionalACM Computing Surveys, Vol. 42, No. 4, Article 14, Publication date June 2010.1432 B. C. M. Fung et al.Table V. Example Illustrating Minimality Attacksa Original patient tableJob Sex DiseaseEngineer Male HIVEngineer Male HIVLawyer Male FluLawyer Male FluLawyer Male FluLawyer Male FluLawyer Male Flub Published anonymous tableJob Sex DiseaseProfessional Male HIVProfessional Male FluProfessional Male FluProfessional Male FluProfessional Male FluProfessional Male FluProfessional Male HIVc External tableName Job SexAndy Engineer MaleCalvin Lawyer MaleBob Engineer MaleDoug Lawyer MaleEddy Lawyer MaleFred Lawyer MaleGabriel Lawyer Malebackground knowledge can lead to extra information that facilitates an attack to compromise data privacy. This is called the minimality attack.Many anonymization algorithms discussed in this section follow an implicit minimality principle. For example, when a table is generalized from bottomup to achievekanonymity, the table is not further generalized once it minimally meets the kanonymity requirement. Minimality attack exploits this minimality principle to reverse the anonymization operations and filter out the impossible versions of the original table Wong et al. 2007. The following example illustrates a minimality attack onconfidence bounding Wang et al. 2007.Example 5.1. Consider the original patient Table Va, the anonymous Table Vb,and an external Table Vc in which each record has a corresponding original recordin Table Va. Suppose the attacker knows that the confidence bounding requirement is Job,Sex  HIV,60. With the minimality principle, the attacker caninfer that Andy and Bob have HIV based on the following reason From Table Va,qid  Lawyer,Male has 5 records, and qid  Engineer,Male has 2 records. Thus,Lawyer,Male in the original table must already satisfy Job,Sex  HIV,60because even if both records with HIV have Lawyer,Male, the confidence for inferring HIV is only 25  40. Since a subtree generalization has been performed,Engineer,Male must be the qid that has violated the 60 confidence requirementon HIV, and that is possible only if both records with Engineer,Male have a diseasevalue of HIV.To thwart minimality attack, Wong et al. 2007 proposed a privacy model, called mconfidentiality, that limits the probability of the linkage from any record owner toany sensitive value set in the sensitive attribute. Wong et al. 2007 also showed thatthis type of minimality attack is applicable to both optimal and minimal anonymization algorithms that employ generalization, suppression, anatomization, or permutation to achieve privacy models, including, but not limited to, diversity Machanavajjhala et al. 2007 ,kanonymity Wong et al. 2006 k, eanonymity Zhang et al.2007 personalized privacy Xiao and Tao 2006b anatomy Xiao and Tao 2006a tcloseness Li et al. 2007 minvariance Xiao and Tao 2007 and X,Y privacy Wangand Fung 2006. To avoid minimality attack on diversity, Wong et al. 2007 proposedto first kanonymize the table, then, for each qid group in the kanonymous table thatviolates diversity, their method distorts the sensitive values to satisfy diversity.5.5. Algorithms for the Probabilistic Attack ModelMany algorithms for achieving the probabilistic privacy models studied in Section 2.4employ perturbation methods, so they do not suffer from the problem of minimality attacks. The perturbation algorithms are nondeterministic therefore, the anonymizationoperations are nonreversible. The perturbation algorithms for the probabilistic attackmodel can be divided into two groups. The first group is local perturbation AgrawalACM Computing Surveys, Vol. 42, No. 4, Article 14, Publication date June 2010.PrivacyPreserving Data Publishing A Survey of Recent Developments 1433and Haritsa 2005, which assumes that a record owner does not trust anyone excepthimself and perturbs his own data record by adding noise before submission to theuntrusted data publisher. The second group is to perturb all records together by atrusted data publisher, which is the data publishing scenario studied in this survey.Although the methods in the first group are also applicable to the second by addingnoise to each individual record, Rastogi et al. 2007 and Dwork 2007 demonstratedthat the information utility can be improved with a stronger lower bounds by assuming a trusted data publisher who has the capability to access all records and exploit the overall distribution to perturb the data, rather than perturbing the recordsindividually.A number of PPDP methods Agrawal and Srikant 2000 Zhang et al. 2005 havebeen proposed for preserving classification information with randomization. Agrawaland Srikant 2000 presented a randomization method for decision tree classificationwith the use of the aggregate distributions reconstructed from the randomized distribution. The general idea is to construct the distribution separately from the differentclasses. Then, a special decision tree algorithm is developed to determine the splitting conditions based on the relative presence of the different classes, derived fromthe aggregate distributions. Zhang et al. 2005 presented a randomization methodfor a naive Bayes classifier. The major shortcoming of this approach is that ordinaryclassification algorithms will not work on this randomized data.The statistics community conducts substantial research in the disclosure control ofstatistical information and aggregate query results Cox 1980 Chawla et al. 2005Duncan and Fienberg 1998 Matloff 1988 Ozsoyoglu and Su 1990. The goal is toprevent attackers from obtaining sensitive information by correlating different published statistics. Cox 1980 proposed the kdominance rule which suppresses a sensitive cell if the values of two or three entities in the cell contribute more than kof the corresponding SUM statistic. The proposed mechanisms include query size andquery overlap control, aggregation, data perturbation, and data swapping. Nevertheless, such techniques are often complex and difficult to implement Farkas and Jajodia2003, or address privacy threats that are unlikely to occur. There are some decentsurveys Adam and Wortman 1989 DomingoFerrer 2001 Moore 1996 Zayatz 2007in the statistics community.Differential Additive Noise One representative work that aims to thwart probabilistic attack is differential privacy Dwork 2006 its definition can be found in Section 2.4. Dwork 2006 proposed an additive noise method to achieve differentialprivacy. The added noise is chosen over a scaled symmetric exponential distributionwith variance  2 in each component, and    f , where  f is the maximum difference of outputs of a query f caused by the removal or addition of a single data record.Machanavajjhala et al. 2008 proposed a revised version of differential privacy, calledprobabilistic differential privacy, that yields a practical privacy guarantee for syntheticdata generation. The idea is to first build a model from the original data, then samplepoints from the model to substitute for original data. The key idea is to filter unrepresentative data and shrink the domain. Other algorithms Blum et al. 2005 Dinurand Nissim 2003 Dwork et al. 2006 Dwork and Nissim 2004 have been proposed toachieve differential privacy refer to Dwork 2008 for a decent survey on the recentdevelopments in this line of privacy model. Algorithm Recall that d,  privacy in Section 2.4 bounds the difference of Prand PrT , where Pr and PrT  are the prior probability and the posterior probability of the presence of a victims record in the data table T before and after examiningthe published table T . To achieve d,  Privacy, Rastogi et al. 2007 proposed a perturbation method, called  algorithm, consisting of two steps. The first step is to selecta subset of records from the original table D with probability    and insert themACM Computing Surveys, Vol. 42, No. 4, Article 14, Publication date June 2010.1434 B. C. M. Fung et al.into the data table T , which is to be published. The second step is to generate somecounterfeit records from the domain of all attributes. If the counterfeit records are notin the original table D, then insert them into T with probability . Hence, the resulting perturbed table T consists of both records randomly selected from the originaltable and counterfeit records from the domain. The number of records in the perturbeddata could be larger than the original data table, in comparison with FRAPP Agrawaland Haritsa 2005 which has a fixed table size. The drawback of inserting counterfeits is that the released data can no longer preserve the truthfulness of the original data at the record level, which is important in some applications, as explained inSection 1.1.6. EXTENDED SCENARIOSAll the work discussed so far focuses on anonymizing and publishing a single release.In practical applications, data publishing is more complicated. For example, the samedata may be published several times. Each time, the data is anonymized differentlyfor different purposes, or the data is published incrementally as new data is collected.In this section, we consider such extended publishing scenarios.6.1. Multiple Release PublishingDifferent data recipients may be interested in different attributes of a data table.Suppose there is a personspecific data table T Job,Sex,Age,Race,Disease,Salary. Adata recipient for example, a pharmaceutical company is interested in classificationmodeling the target attribute Disease with attributes Job,Sex,Age. Another data recipient such as a social service department is interested in clustering analysis onJob,Age,Race. One approach is to publish a single release on Job,Sex,Age,Racefor both purposes. A drawback is that information is released unnecessarily, in thatneither of the two purposes needs all four attributes, which makes it is more vulnerable to attacks. Moreover, if the information needed in the two cases is different, thedata anonymized in a single release may not be good for either of the two cases. A better approach is to anonymize and publish a customized release for each data miningpurpose each release is anonymized to best address the specific purpose. Given thatboth releases are published, there is a possibility that the data recipients have accessto both releases it is difficult to prevent them from colluding with each other behindthe scenes. In particular, an attacker can combine attributes from the two views toform a sharper QID that contains attributes from both views. The following exampleillustrates the join attack in multiple releases.Example 6.1. Consider the data in Table VIa. Suppose that the data publisherreleases one projection view T1 to one data recipient and releases another projectionview T2 to another data recipient. Both views are from the same underlying patienttable. Further suppose that the data publisher does not want Age,Birthplace to belinked to Diseaset. When T1 and T2 are examined separately, the Age  40 groupand the Birthplace  France group have size 2. However, by joining T1 and T2 usingT1.Job  T2.Job, an attacker can uniquely identify the record owner in the 40,Francegroup, thus linking Age,Birthplace to Disease without difficulty. Moreover, the joinreveals the inference 30,US  Cancer with 100 confidence for the record ownersin the 30,US group. Such an inference cannot be made when T1 and T2 are examinedseparately Wang and Fung 2006.Several works measured information disclosure arising from linking two or moreviews. Yao et al. 2005 presented a method for detecting kanonymity violation on aACM Computing Surveys, Vol. 42, No. 4, Article 14, Publication date June 2010.PrivacyPreserving Data Publishing A Survey of Recent Developments 1435Table VI. MultipleSequential Releasea T1Age Job Class30 Lawyer c130 Lawyer c140 Carpenter c240 Electrician c350 Engineer c450 Clerk c4b T2Job Birthplace DiseaseLawyer US CancerLawyer US CancerCarpenter France HIVElectrician UK CancerEngineer France HIVClerk US HIVc The join of T1 and T2Age Job Birthplace Disease Class30 Lawyer US Cancer c130 Lawyer US Cancer c140 Carpenter France HIV c240 Electrician UK Cancer c350 Engineer France HIV c450 Clerk US HIV c430 Lawyer US Cancer c130 Lawyer US Cancer c1Table VII. Marginalsa Job marginalJob CountEngineer 2Lawyer 1Writer 2Dancer 2b Sex marginalSex CountMale 3Female 4set of views, each view was obtained from a projection and selection query they alsoconsidered functional dependency as prior knowledge.In addition to the anonymous base table, Kifer and Gehrke 2006 proposed increasing the utility of published data by releasing several anonymous marginals thatare essentially duplicate preserving projection views. For example, Table VIIa andTable VIIb are the Job and Sex marginals for the kanonymous base Table IIc. Theavailability of additional marginals views provides additional information for datamining, but also poses new privacy threats. For example, if a combination of attributevalues has a low count, it can be used as QID to reveal sensitive attributes in otherdatabases. Thus, Kifer and Gehrke 2006 extended kanonymity and diversity formarginals and presented a method to check whether published marginals violate theprivacy requirement on the anonymous base table.Barak et al. 2007 also studied the privacy threats caused by marginals, but alongthe lines of differential privacy Dwork 2006. Their primary contribution was providing a formal guarantee to preserve all the privacy, accuracy, and consistency in thepublished marginals. Accuracy bounds the difference between the original marginalsand published marginals. Consistency ensures that there exists a contingency tablewhose marginals equal the published marginals. Instead of adding noise to the original data records at the cost of accuracy, or adding noise to the published marginalsat the cost of consistency, they have proposed transforming the original data into theFourier domain, applying differential privacy to the transformed data by perturbation,and employing linear programming to obtain a nonnegative contingency table basedon the given Fourier coefficients.ACM Computing Surveys, Vol. 42, No. 4, Article 14, Publication date June 2010.1436 B. C. M. Fung et al.6.2. Sequential Release PublishingIn the multiple release publishing scenario, several releases, for different purposes,are published at one time. In some other scenarios, the data is released continuouslyand sequentially as new information becomes available. Consider the problem ofsequential anonymization Wang and Fung 2006 a data publisher has previouslyreleased T1, . . . ,Tp1 and now wants to publish the next release Tp, where all Tiare projections of the same underlying table, and each individual release, not thejoin, serves a data mining purpose. The data publisher wants to prevent record andattribute linkages through the join of T1, . . . ,Tp. This requirement is specified by aprivacy model such as X,Y privacy Section 2.2 on the join of all releases because theattacker has access to all releases. Unlike the multiple release publishing scenario, theprevious releases T1, . . . ,Tp1 have been published and, therefore, cannot be modified.Any attempt at prevention of privacy violation has to rely on anonymizing the nextrelease Tp.To address the sequential anonymization problem, Wang and Fung 2006 introduced the lossy join, a negative property in relational database design, as a way tohide the join relationship among releases. A lossy join of T1 and T2 will result in atable containing some records that are not original records in the underlying tables T1and T2. Such records are the result of matching some records in T1 and T2 that belongto different owners. The next example illustrates this point.Example 6.2. Example 6.1 shows that the record owner in the 40,France groupbecomes uniquely identifiable, thus revealing hisher contracted disease, after the joinof T1 and T2 in Table VI. In fact, the join is a doubleedged sword, in that a lossy joincould also weaken identification. For example, after the join, the 30,US group hassize 4 because the records for different owners are matched i.e., the last two recordsin the join table, whereas T1 and T2 are examined separately, both Age  30 groupand Birthplace  US group have a smaller size.A join attack depends critically on matching the records in T1 and T2 that represent thesame record owner therefore, a lossy join, which matches records of different recordowners, can be used to combat the join attack. To make the join of T1 and T2 lossy, Wangand Fung 2006 proposed generalizing the join attributes in T2 recall that T1 has beenpublished and cannot be modified so that a generalized record in T2 will match morerecords in T1. For example, for the join attribute Job, all records in T2 generalized toProfessional will match all records in T1 that contain Professional or an ancestor or adescendant of Professional in the taxonomy of Job. Intuitively, two records, one in T1and one in T2, match if their Job values are on the same generalization path in thetaxonomy of Job. Note that this match condition is more relaxed than the traditionalequality join which requires an exact match.To satisfy a given requirement on X,Y privacy, the anonymization algorithm generalizes T2 on the attributes X  attT2, where attT2 denotes the set of attributesin T2. A topdown specialization process is employed to iteratively specialize T2 onX  attT2 starting from the most general state of T2. Recall from Section 2.2 thatX,Y privacy is composed of X,Y anonymity and confidence bounding defined on thejoin of T1 and T2. Significantly, the generalization property holds in X,Y privacy forthe subtree generalization scheme, X,Y anonymity is nonincreasing and confidencebounding is nondecreasing with respect to a specialization on T2 on X  attT2. Essentially, this property means that if any of these requirements is violated, it remainsviolated after a specialization. Therefore, the topdown specialization approach canprune the remaining search space and efficiently identify a minimal anonymizationof T2.ACM Computing Surveys, Vol. 42, No. 4, Article 14, Publication date June 2010.PrivacyPreserving Data Publishing A Survey of Recent Developments 1437Table VIII. Continuous Data Publishinga 2diverse T1Job Sex DiseaseProfessional Female CancerProfessional Female DiabetesArtist Male FeverArtist Male Cancerb 2diverse T2 after an insertion to T1Job Sex DiseaseProfessional Female CancerProfessional Female DiabetesProfessional Female HIVArtist Male FeverArtist Male Cancerc 2diverse T2 after a deletion fromand an insertion to T1Job Sex DiseaseProfessional Female CancerProfessional Female FeverArtist Male FeverArtist Male CancerThis sequential anonymization problem was briefly discussed in some pioneeringwork on kanonymity, but none provided a practical solution. For example, Samaratiand Sweeney 1998b suggested to kanonymize all potential join attributes as theQID in the next release Tp. Sweeney 2002a suggested generalizing Tp based on theprevious releases T1, . . . ,Tp1 to ensure that all values in Tp are not more specificthan in any T1, . . . ,Tp1. Both solutions suffer from monotonically distorting the datain a later release. The third solution is to release a complete cohort in which allpotential releases are anonymized at one time, after which no additional mechanism isrequired. This requires predicting future releases. The underprediction means thatthere will be no room for additional releases and the overprediction means there willbe unnecessary data distortion. Also, this solution does not accommodate the new dataadded at a later time.6.3. Continuous Data PublishingIn the model of continuous data publishing, the data publisher has previously published T1, . . . ,Tp1 and now wants to publish Tp, where Ti is an updated release ofTi1 with record insertions andor deletions. The problem assumes that all recordsfor the same individual remain the same in all releases. Even though each releaseT1, . . . ,Tp is individually anonymous, the privacy requirement could be compromisedby comparing different releases and eliminating some possible sensitive values for avictim. This problem assumes that the data is dynamically updated, unlike the sequential anonymization problem which assumes all data is static and is available atthe time of release. Furthermore, this problem assumes all releases share the samedatabase schema, while the sequential problem assumes all releases are projections ofthe same underlying data table.This continuous data publishing problem assumes that the attacker knows thetimestamp and QID of the victim, so the attacker knows exactly which releases contain the victims data record. The following examples show the privacy threats causedby record insertions and deletions.Example 6.3. Let Table VIIIa be the first release T1. Let Table VIIIb be thesecond release T2 after inserting a new record. Both T1 and T2 satisfy 2diversity independently. Suppose the attacker knows that a female lawyer, Alice, has a record in T2but not in T1, based on the timestamp that Alice was admitted to a hospital. From T2,the attacker can infer that Alice must have contracted either Flu, Fever, or HIV. Bycomparing T2 with T1, the attacker can identify that the first two records in T2 mustbe old records from T1 and, thus, infer that Alice must have contracted HIV.ACM Computing Surveys, Vol. 42, No. 4, Article 14, Publication date June 2010.1438 B. C. M. Fung et al.Example 6.4. Let Table VIIIa be the first release T1. Let Table VIIIb be the second release T2 after deleting the record Professional,Female,Diabetes and insertinga new record Professional,Female,Fever. Both T1 and T2 satisfy 2diversity independently. Suppose the attacker knows that a female engineer, Beth, must be in both T1and T2. From T1, the attacker can infer that Beth must have contracted either Canceror Diabetes. Since T2 contains no Diabetes, the attacker can infer that Beth must havecontracted Cancer.Byun et al. 2006 were the pioneers who proposed an anonymization technique thatenables privacypreserving continuous data publishing after new records have beeninserted. Specifically, it guarantees every release to satisfy diversity, which requireseach qid group contain at least  distinct sensitive values. Since this instantiation ofdiversity does not consider the frequencies of sensitive values, an attacker could stillconfidently infer a sensitive value of a victim if the value occurs frequently in a qidgroup. Thus, this instantiation cannot prevent attribute linkage attacks.Byun et al. 2006 addressed the threats caused by record insertions but not deletions, so the current release Tp contains all records in previous releases. The algorithminserts new records into the current release Tp only if two privacy requirements remain satisfied after the insertion 1 Tp is diverse 2 given any previous release Tiand the current release Tp together, there are at least  distinct sensitive values inthe remaining records that could potentially be the victims record. This requirementcan be verified by comparing the difference and intersection of the sensitive valuesin any two comparable qid groups in Ti and Tp. The algorithm prefers to specializeTp as much as possible to improve the data quality, provided that the two privacy requirements are satisfied. If the insertion of some new records would violate any of theprivacy requirements, even after generalization, the insertions are delayed until laterreleases. Nonetheless, this strategy may sometimes run into a situation in which nonew data could be released. Also, it requires a very large memory buffer to store thosedelayed data records.Xiao and Tao 2007 proposed a new privacy notion called minvariance and ananonymization method, addressing both record insertions and deletions. In this continuous data publishing model, a sequence of releases T1, . . . ,Tp is minvariant if 1 everyqid group in any Ti contains at least mrecords and all records in qid have different sensitive values and 2 for any record r with published lifespan x, y where 1  x, y  p,qidx, . . . ,qidy have the same set of sensitive values where qidx, . . . ,qidy are the generalized qid groups containing r in Tx, . . . ,Ty. The rationale of minvariance is that, if arecord r has been published in Tx, . . . ,Ty, then all qid groups containing r must havethe same set of sensitive values. This will ensure the intersection of sensitive valuesover all such qid groups does not reduce the set of sensitive values compared to eachqid group.Given a sequence of minvariant T1, . . . ,Tp1, Xiao and Tao 2007 maintained asequence of minvariant T1, . . . ,Tp by minimally adding counterfeit data records andgeneralizing the current release Tp. A table with counterfeit records could no longerpreserve the data truthfulness at the record level, which is important in some applications, as explained in Section 1.1.Recently, Fung et al. 2008 showed a method to systematically quantify the exact number of records that can be cracked by comparing all kanonymous releases.A record in a kanonymous release is cracked if it is impossible to be a candidaterecord of the target victim. After excluding the cracked records from a release, a tablemay no longer be kanonymous. In some cases, data records, with sensitive information of some victims, can even be uniquely identified from the releases. Fung et al.2008 proposed a privacy requirement, called BCFanonymity, to measure the trueACM Computing Surveys, Vol. 42, No. 4, Article 14, Publication date June 2010.PrivacyPreserving Data Publishing A Survey of Recent Developments 1439Fig. 4. Collaborative data publishing.anonymity in a release after excluding the cracked records, and presented a generalization method to achieve BCFanonymity without delaying record publication orinserting counterfeit reords. Some work misinterprets Fung et al. 2008 to mean thatthey allow only record insertion but not record deletion. Indeed, T1 and T2 are independent of each other i.e., T2 is not an insertion or deletion of T1. Fung et al. 2008anonymize T1  T2 as one release for utility on the whole data set. In contrast, allother work Xiao and Tao 2007 anonymizes each Ti independently, so the publishingmodel in Xiao and Tao 2007 does not benefit from new data because each Ti is small,resulting in a large distortion. Bu et al. 2008 further relax the PPDP scenario and assume that the QID and sensitive values of a record owner could change in subsequentreleases.6.4. Collaborative Data PublishingSo far, we have considered only a single data publisher. In reallife data publishing,a single organization often does not hold the complete data. Organizations need toshare data for mutual benefits or for publishing to a third party. For example, twocredit card companies want to integrate their customer data for developing a frauddetection system or for publishing to a bank. However, the credit card companies donot want to indiscriminately disclose their data to each other or to the bank for reasons such as privacy protection and business competitiveness. Figure 4 depicts thisscenario, called collaborative data publishing, where several data publishers own different sets of attributes on the same set of records and want to publish the integrateddata on all attributes. Say, publisher 1 owns RecID,Job,Sex,Age, and publisher 2owns RecID,Salary,Disease, where RecID, such as the SSN, is the record identifiershared by all data publishers. They want to publish an integrated kanonymous tableon all attributes. Also, no data publisher should learn more specific information, ownedby the other data publishers, than the information that appears in the final integratedtable.There are two obvious but insecure approaches. The first one is integratethengeneralize that is, first integrate the tables and then generalize the integrated tableusing any single table kanonymization method discussed in previous sections. Thisapproach does not preserve privacy because the data publisher holding the integratedtable will immediately know all the private information of all data publishers. Thesecond approach is generalizethenintegrate that is, first generalize each table locally and then integrate the generalized tables. This approach does not work if thekanonymity involves a global QID spanning two or more data publishers.ACM Computing Surveys, Vol. 42, No. 4, Article 14, Publication date June 2010.1440 B. C. M. Fung et al.Wang et al. 2005 proposed an algorithm called TopDown Specialization for 2PartyTDS2P to solve the collaborative publishing problem. Essentially, TDS2P producesthe same final anonymous table as the integratethengeneralize approach, but doesnot reveal local data until the data has been generalized to satisfy a given kanonymityrequirement. First, all data publishers generalize their attributes in QID to the mostgeneral value ANY. At each iteration, each data publisher identifies a local specialization that has the highest IGPL, measured by Eq. 4, based on her own data,and then collaboratively identifies the global specialization w that has the maximumIGPL across all local specializations. The data publisher P, who owns the attributeof w, first performs the specialization w on her own data. The other data publishers,who do not own the attribute of w, have to get the instruction from P to partitiontheir local data. The instruction, represented by GroupNo,RecID, tells how recordsidentified by RecID are specialized into different groups identified by GroupNo.Repeat this process, and stop if any further specialization leads to a violation ofkanonymity. Mohammed et al. 2009 extended the idea to distributed data mashupapplications.Jiang and Clifton 2005, 2006 addressed a similar problem by using a cryptographicapproach. First, each data publisher determines a locally kanonymous table. Then,the intersection of RecIDs for the qid groups in the two locally kanonymous tablesis determined. If the intersection size of each pair of the qid group is at least k,then the algorithm returns the join of the two locally kanonymous tables that isglobally kanonymous otherwise, further generalization is performed on both tablesand the RecID comparison procedure is repeated. To prevent the other data publisherfrom learning more specific information than that appearing in the final integratedtable through RecID, a commutative encryption scheme Pohlig and Hellman 1978is employed to encrypt the RecIDs for comparison. This scheme ensures the equality of two values encrypted in a different order on the same set of keys, that is,EKey1EKey2RecID  EKey2EKey1RecID.7. ANONYMIZING OTHER TYPES OF DATAAll the work discussed so far focuses on anonymizing relational and statistical data.What about other types of nonrelational data Recent studies have shown that publishing transaction data, moving object data, and textual data may also result in privacy threats and sensitive information leakages. Below, we discuss the privacy threats,together with some privacypreserving solutions, on these nonrelational data types.7.1. HighDimensional Transaction DataPublishing highdimensional data is part of the daily operations in commercial andpublic activity. A classic example of highdimensional data is transaction databases.Each transaction corresponds to a record owner and consists of a set of items selected from a large universe. Examples of transactions are web queries, click streams,emails, market baskets, and medical notes. Such data often contains rich information and is an excellent source for data mining. Detailed transaction data provides an electronic image of a record owners life, possibly containing sensitiveinformation.A recent case demonstrates the privacy threats caused by publishing transactiondata AOL released a database of query logs to the public for research purposesBarbaro and Zeller 2006. However, by examining query terms, AOL user No. 4417749was traced back to Ms. Thelma Arnold, a 62yearold widow who lives in Lilburn. Evenif a query does not contain an address or name, a record owner the AOL user inACM Computing Surveys, Vol. 42, No. 4, Article 14, Publication date June 2010.PrivacyPreserving Data Publishing A Survey of Recent Developments 1441this example may still be reidentified from combinations of query terms that areadequately unique to the record owner. This scandal led not only to the disclosure ofprivate information of AOL users, but also damaged data publishers enthusiasm in offering anonymized transaction data for research purposes. Kumar et al. 2007 furthershowed that some tokenhashed anonymous query logs could be cracked by inverting the hash function based on the cooccurrences of tokens in some other referencequery logs. Clearly, there is a need for a proper anonymization method for transactiondata.Transaction data is usually highdimensional. For example, Amazon.com has several million catalog items. Each dimension could be a potential QID attribute usedfor record or attribute linkages therefore, employing traditional privacy models, suchas kanonymity, would require including all dimensions into a single QID. Due to thecurse of highdimensionality Aggarwal 2005, it is very likely that lots of data has tobe suppressed or generalized to the topmost values in order to satisfy kanonymity,even if k is small. Obviously, such anonymous data is useless for data analysis.There are some recent studies on anonymizing highdimensional data. Ghinita et al.2008 proposed a permutation method whose general idea is to first group transactions with close proximity and then associate each group to a set of diversified sensitivevalues. In any reallife privacy attack, it is unlikely that the attacker would know allquasiidentifying attributes of a target victim due to the effort it would take to gatherevery piece of background knowledge. Thus, it is reasonable to bound the attackersbackground knowledge in the privacy model. Terrovitis et al. 2008 proposed an algorithm to kanonymize transactions by generalization. Xu et al. 2008 extended thetraditional kanonymity model by assuming that the attacker knows at most m transaction items of the target victim. Specifically, the privacy model in Xu et al. 2008ensures that 1 every itemset I with size not greater than m in the published tableis shared by at least k records and 2 that the confidence of inferring the sensitivevalue s from I is less than a maximum confidence threshold h. Their results show thatthis relaxation can substantially improve data utility. In another work, Xu et al. 2008consider preserving frequent itemsets as data utility. To deal with the scalability bottleneck caused by exponential explosion of itemsets, Xu et al. 2008 use sets of maximal and minimal itemsets, called borders, to represent the itemsets that violate theprivacy requirement and the frequent itemsets. Both papers Xu et al. 2008 Xu et al.2008 use item suppression, instead of generalization, because the taxonomy trees fortransaction data tend to be flat and fanout. In this case, employing generalization losesmore information than employing item suppression.Aggarwal and Yu 2007 formalized an anonymity model for the sketchbased approach, and utilized it to construct sketchbased privacypreserving representationsof the original data. The sketchbased approach Alon et al. 1999 reduces the dimensionality of the data by generating a new representation with a much smaller number of features, where each one uses a different set of random weights to produce aweighted sum of the original feature values. This technique is quite effective for highdimensional data sets, as long as the data is sparse. The sketchbased method providesprivacy protection while allowing effective reconstruction of many aggregate distancemeasures. Therefore, it can be used for a variety of data mining algorithms such asclustering and classification.7.2. Moving Object DataLocationbased services LBS are information services provided to mobile subscribersbased on their specific physical locations. In recent years, a variety of locationbasedservices has been developed due to increasing demand from subscribers. Although theACM Computing Surveys, Vol. 42, No. 4, Article 14, Publication date June 2010.1442 B. C. M. Fung et al.Table IX. PatientSpecific Path Table TPid Path Disease ...1 a1  d2  b3  e4  f 6  c7 HIV2 b3  e4  f 6  e8 Flu3 b3  c7  e8 Flu4 d2  f 6  c7  e8 Allergy5 d2  c5  f 6  c7 HIV6 c5  f 6  e9 Allergy7 d2  c5  c7  e9 Fever8  f 6  c7  e9 FeverFig. 5. Time and spatial trajectory volume Abul et al. 2008.advancement of telecommunication technology has improved our quality of life, research has shown that 24 of potential LBS users are seriously concerned about theprivacy implications of disclosing their locations in conjunction with other personaldata Beinat 2001. Moving object data poses new challenges to traditional database,data mining, and privacypreserving technologies due to its unique characteristicsit is timedependent, locationdependent, and is generated in large volumes of highdimensional stream data. The following example shows the privacy threats caused bypublishing moving object data.Example 7.1. A hospital wants to release the patientspecific path table, Table IX,to a third party for data analysis. Explicit identifiers, such as patient names and Pid,have been removed. Each record contains the moving path of a patient in the hospitaland some patientspecific sensitive information, for example, contracted diseases. Amoving path contains a sequence of pairs lociti indicating the patients visited locationloci at timestamp ti. For example, Pid3 has a path b3  c7  e8, meaning that thepatient has visited locations b, c, and e at timestamps 3, 7, and 8, respectively.An attacker seeks to perform record andor attribute linkages by using the movingpath as QID for matching. 1 Record linkage suppose the attacker knows that thetarget victim, Alice, has visited e and c at timestamps 4 and 7, respectively. Alicesrecord, together with her sensitive value HIV in this case, can be uniquely identifiedbecause Pid1 is the only record that contains e4 and c7. 2 Attribute linkage supposethe attacker knows that another target victim, Bob, has visited d2 and f 6, matchingPid1,4,5, the attacker can infer that Bob has HIV with 23  67 confidence.There are a few recent works on anonymizing moving objects. Abul et al. 2008 extended the traditional kanonymity model to anonymize a set of moving objects. Theintuition is to have at least k moving objects appearing within the radius  of the pathof every moving object in the same period of time, as depicted in Figure 5. In addition tothe traditional anonymization operations discussed in Section 3, Abul et al. 2008 alsoACM Computing Surveys, Vol. 42, No. 4, Article 14, Publication date June 2010.PrivacyPreserving Data Publishing A Survey of Recent Developments 1443explored space translation by adding noise to the original paths so that more objects appear at the same time and spatial trajectory volume. Terrovitis and Mamoulis 2008assumed that the locations are sensitive information, and that the attacker will attempt to infer some sensitive locations visited by the target victim which are unknownto the attacker. Malin and Airoldi 2006 studied the privacy threats in locationbaseddata in the hospital environment.Fung et al. 2009 presented the first work to anonymize highdimensional RFIDmoving object data. Their proposed privacy model, LKCprivacy, ensures that everyRFID moving path with length not greater than L is shared by least K1 other movingpaths, and the confidence in inferring any prespecified sensitive value is not greaterthan C.Papadimitriou et al. 2007 studied the privacy issue in publishing timeseriesdata and examined the tradeoffs between timeseries compressibility and partialinformation hiding and their fundamental implications for how one should introduceuncertainty about individual values by perturbing them. The study found that by making the perturbation similar to the original data, we can both preserve the structureof the data better, and simultaneously make breaches harder. However, as data becomes more compressible, a fraction of the uncertainty can be removed if true valuesare leaked, revealing how they were perturbed.7.3. Textual DataMost previous work focused on anonymizing the structural or semistructural data.What about the unstructural data, such as text documents Saygin et al. 2006 describes implicit and explicit privacy threats in text document repositories. Sanitizationof text documents involves removing sensitive information or removing potential linking information that can associate an individual person to the sensitive information ina document. This research direction is in its infancy.Kokkinakis and Thurin 2007 implemented a system for automatically anonymizinghospital discharge letters by identifying and deliberately removing all phrases fromclinical text that satisfy some predefined types of sensitive entities. The identificationphase is achieved by collaborating with an underlying generic named entity recognition system.Instead of simply removing phases containing predefined types of sensitive entities,Chakaravarthy et al. 2008 presented the ERASE system to sanitize a document withthe least distortion. External knowledge is required to associate a database of entities with their context. ERASE prevents disclosure of protected entities by removingcertain terms of their context so that no protected entity can be inferred from theremaining document text. ksafety, in the same spirit of kanonymity, is thereafter defined. A set of terms is ksafe if its intersection with every protected entity containsat least k entities. Then the proposed problem is to find the maximum cardinalitysubset of a document satisfying ksafety. Chakaravarthy et al. 2008 proposed andevaluated both a global optimal algorithm and an efficient greedy algorithm to achieveksafety.8. PRIVACYPRESERVING TECHNIQUES IN OTHER DOMAINS8.1. Interactive Query ModelClosely related, but orthogonal to PPDP, is the extensive literature on inference control in multilevel secure databases Farkas and Jajodia 2003 Jajodia and Meadows 1995. Attribute linkages are identified and eliminated either at the databasedesign phase Goguen and Meseguer 1984 Hinke 1988 Hinke et al. 1995, byACM Computing Surveys, Vol. 42, No. 4, Article 14, Publication date June 2010.1444 B. C. M. Fung et al.Table X. Interactive Query Modela Original examination dataID University Department Score1 Concordia CS 922 Simon Fraser EE 913 Concordia CS 974 Illinois CS 96b Added one new recordID University Department Score1 Concordia CS 922 Simon Fraser EE 913 Concordia CS 974 Illinois CS 965 Illinois CS 99modifying the schemes and metadata, or during the interactive query time Denning1985 Thuraisingham 1987, by restricting and modifying queries. These techniques,which focus on query databaseanswering, are not readily applicable to PPDP, wherethe data publisher may not have sophisticated database management knowledge, ordoes not want to provide an interface for database query. A data publisher, such as ahospital, has no intention of being a database server answering database queries isnot part of its normal business. Therefore, queryanswering is quite different from thePPDP scenarios studied in this survey. Here, we briefly discuss the interactive querymodel.In the interactive query model, the user can submit a sequence of queries based onpreviously received query results. Although this query model could improve the satisfaction of the data recipients information needs Dwork et al. 2006, the dynamicnature of queries makes the returned results even more vulnerable to attack, as illustrated in the following example. Refer to Blum et al. 2005, 2008, Dwork 2008,Dinur and Nissim 2003 for more privacypreserving techniques on the interactivequery model.Example 8.1. Suppose that an examination center allows a data miner to accessits database, Table Xa, for research purposes. The attribute Score is sensitive. Anattacker wants to identify the Score of a target victim, Bob, who is a student at thecomputer science department at Illinois. The attacker can first submit the queryQ1 COUNT University  Illinois AND Department  CSSince the count is 1, the attacker can determine Bobs Score  96 by the followingqueryQ2 AVERAGE Score WHERE University  Illinois AND Department  CS.Suppose that the data publisher has inserted a new record as shown in Table 9b.Now the attacker tries to identify another victim by resubmitting query Q1. Since theanswer is 2, the attacker knows another student at the computer science departmentat Illinois took this exam and can then submit the queryQ3 SUM Score WHERE University  Illinois AND Department  CSBenefiting from this update, the attacker can learn the Score of the new record bycalculating Q3  Q2  99.Query auditing has a long history in statistical disclosure control. It can be broadlydivided into two categories online auditing and offline auditing.Online auditing The objective of online query auditing is to detect and deny queriesthat violate privacy requirements. Miklau and Suciu 2004 measured information disclosure of a view set, V , with respect to a secret view S. S is secure if publishing Vdoes not alter the probability of inferring the answer to S. Deutsch and Papakonstantinou 2005 studied whether a new view disclosed more information than the existingviews with respect to a secret view. To put the data publishing scenario consideredin this survey in their terms the anonymous release can superficially be consideredas the view and the underlying data can be considered as the secret query. However, the two problems have two major differences First, the anonymous release isACM Computing Surveys, Vol. 42, No. 4, Article 14, Publication date June 2010.PrivacyPreserving Data Publishing A Survey of Recent Developments 1445obtained by anonymization operations, not by conjunctive queries as in Deutsch andPapakonstantinou 2005 and Miklau and Suciu 2004. Second, the publishing scenarios employ anonymity as the privacy measure, whereas Miklau and Suciu 2004and Deutsch and Papakonstantinou 2005 adopted the perfect secrecy for the securitymeasure. The released data satisfies perfect secrecy if the probability that the attackerfinds the original data after observing the anonymous data is the same as the probability or difficulty of getting the original data before observing the anonymous data.Kenthapadi et al. 2005 proposed another privacy model, called stimulatable auditing, as an interactive query model. If the attacker has access to all previous queryresults, the method denies the new query if it leaks any information beyond whatthe attacker already knows. Although this detect and deny approach is practical,Kenthapadi et al. 2005 pointed out that the denials themselves may implicitly disclose sensitive information, making the privacy protection problem even more complicated. This motivates the offline query auditing.Offline auditing In offline query auditing Evfimievski et al. 2008, the data recipients submit their queries and receive their results. The auditor checks if a privacyrequirement has been violated after the queries have been executed. The data recipients have no access to the audit results and, therefore, the audit results do not triggerextra privacy threats as in the online mode. The objective of offline query auditionis to check for compliance of privacy requirements, not to prevent the attackers fromaccessing the sensitive information.8.2. Privacy Threats Caused by Data Mining ResultsThe release of data mining results or patterns could pose privacy threats. There aretwo broad research directions in this family.The first direction is to anonymize the data so that sensitive data mining patternscannot be generated. Aggarwal et al. 2006 pointed out that simply suppressing thesensitive values chosen by individual record owners is insufficient because an attackercan use association rules learnt from the data to estimate the suppressed values. Theyproposed a heuristic algorithm to suppress a minimal set of values to combat suchattacks. Verykios et al. 2004 proposed algorithms for hiding sensitive associationrules in a transaction database. The general idea is to hide one rule at a time by eitherdecreasing its support or its confidence, achieved by removing items from transactions.Rules satisfying a specified minimum support and minimum confidence are removed.However, in the notion of anonymity, a rule applying to a small group of individualsi.e., low support presents a more serious threat because record owners from a smallgroup are more identifiable.The second direction is to directly anonymize the data mining patterns. Atzori et al.2008 proposed the insightful suggestion that if the goal is to release data mining results, such as frequent patterns, then it is sufficient to anonymize the patterns ratherthan the data. Their study suggested that anonymizing the patterns yields much better information utility than performing data mining on anonymous data. This opensup a new research direction for privacypreserving patterns publishing. Kantarciogluet al. 2004 defined an evaluation method to measure the loss of privacy due to releasing data mining results.8.3. PrivacyPreserving Distributed Data MiningPrivacypreserving distributed data mining PPDDM is a cousin to the research topicof privacypreserving data publishing PPDP. PPDDM assumes a scenario that multiple data holders want to collaboratively perform data mining on the union of theirACM Computing Surveys, Vol. 42, No. 4, Article 14, Publication date June 2010.1446 B. C. M. Fung et al.data without revealing their sensitive information. PPDDM usually employs cryptographic solutions. Although the ultimate goal of both PPDDM and PPDP is to performdata mining, they have very different assumptions on data ownerships, attack models, privacy models, and solutions, so PPDDM is out of the scope of this survey. Werefer readers interested in PPDDM to work by Clifton et al. 2002 Kantarcioglu 2008Pinkas 2002 Vaidya 2008 Wright et al. 2005.9. SUMMARY AND FUTURE RESEARCH DIRECTIONSInformation sharing has become part of the routine activity of many individuals, companies, organizations, and government agencies. Privacypreserving data publishingis a promising approach to information sharing, while preserving individual privacyand protecting sensitive information. In this survey, we reviewed the recent developments in the field. The general objective is to transform the original data into someanonymous form to prevent from inferring its record owners sensitive information.We presented our views on the difference between privacypreserving data publishing and privacypreserving data mining, and gave a list of desirable properties of aprivacypreserving data publishing method. We reviewed and compared existing methods in terms of privacy models, anonymization operations, information metrics, andanonymization algorithms. Most of these approaches assumed a single release from asingle publisher, and thus only protected the data up to the first release or the firstrecipient. We also reviewed several works on more challenging publishing scenarios,including multiple release publishing, sequential release publishing, continuous datapublishing, and collaborative data publishing.Privacy protection is a complex social issue, which involves policymaking, technology, psychology, and politics. Privacy protection research in computer science canprovide only technical solutions to the problem. Successful application of privacypreserving technology will rely on the cooperation of policy makers in governments anddecision makers in companies and organizations. Unfortunately, while the deploymentof privacythreatening technology, such as RFID and social networks, grows quickly,the implementation of privacypreserving technology in reallife applications is verylimited. As the gap becomes larger, we foresee that the number of incidents and thescope of privacy breach will increase in the near future. Below, we identify a few potential research directions in privacy preservation, together with some desirable properties that could facilitate the general public, decision makers, and systems engineersto adopt privacypreserving technology.Privacypreserving tools for individuals. Most previous privacypreserving techniques were proposed for data publishers, but individual record owners should alsohave the right and responsibility to protect their own private information. There isan urgent need for personalized privacypreserving tools, such as privacypreservingweb browsers and minimal information disclosure protocols for ecommerce activities. It is important that the privacypreserving notions and tools developed are intuitive for novice users. Xiao and Tao 2006bs work on personalized privacy preservation provides a good start, but little work has been conducted on this directionsince.Privacy protection in emerging technologies. Emerging technologies, like locationbased services Atzori et al. 2007 Hengartner 2007 You et al. 2007, RFID Wanget al. 2006, bioinformatics, and mashup web applications, enhance our quality of life.These new technologies allow corporations and individuals to have access to previouslyunavailable information and knowledge however, such benefits also bring up manynew privacy issues. Nowadays, once a new technology has been adopted by a smallcommunity, it can become very popular in a short period of time. A typical example isACM Computing Surveys, Vol. 42, No. 4, Article 14, Publication date June 2010.PrivacyPreserving Data Publishing A Survey of Recent Developments 1447the social network application called Facebook.2 Since its deployment in 2004, it hasacquired 70 million active users. Due to the massive number of users, the harm couldbe extensive if the new technology is misused. One research direction is to customizeexisting privacypreserving models for emerging technologies.Incorporating privacy protection in engineering process. The issue of privacy protection is often considered after the deployment of a new technology. Typical examples are the deployments of mobile devices with locationbased services Abul et al.2008 Atzori et al. 2007 Hengartner 2007 You et al. 2007, sensor networks, and social networks. The privacy issue should be considered as a primary requirement inthe engineering process for developing new technology. This involves formal specification of privacy requirements and formal verification tools to prove the correctness of aprivacypreserving system.Finally, we emphasize that privacypreserving technology solves only one side of theproblem. It is equally important to identify and overcome the nontechnical difficulties faced by decision makers when they deploy a privacypreserving technology. Theirtypical concerns include the degradation of dataservice quality, loss of valuable information, increased costs, and increased complexity. We believe that crossdisciplinaryresearch is the key to remove these obstacles, and urge computer scientists in theprivacy protection field to conduct crossdisciplinary research with social scientists insociology, psychology, and public policy studies. Having a better understanding of theprivacy problem from different perspectives can help realize successful applications ofprivacypreserving technology.ACKNOWLEDGMENTSWe sincerely thank the reviewers of this manuscript for greatly improving the quality of this survey.REFERENCESABUL, O., BONCHI, F., AND NANNI, M. 2008. Never walk alone Uncertainty for anonymity in moving objectsdatabases. In Proceedings of the 24th IEEE International Conference on Data Engineering ICDE. 376385.ADAM, N. R. AND WORTMAN, J. C. 1989. Security control methods for statistical databases. ACM Comput.Surv. 21, 4, 515556.AGGARWAL, C. C. AND YU, P. S. 2008a. A framework for condensationbased anonymization of string data.Data Min. Knowl. Discov. 13, 3, 251275.AGGARWAL, C. C. AND YU, P. S. 2008b. On static and dynamic methods for condensationbased privacypreserving data mining. ACM Trans. Datab. Syst. 33, 1.AGGARWAL, C. C. AND YU, P. S. 2008c. PrivacyPreserving Data Mining Models and Algorithms. Springer,Berlin.AGGARWAL, C. C. AND YU, P. S. 2007. On privacypreservation of text and sparse binary data with sketches.In Proceedings of the SIAM International Conference on Data Mining SDM.AGGARWAL, C. C., PEI, J., AND ZHANG, B. 2006. On privacy preservation against adversarial data mining.In Proceedings of the 12th ACM SIGKDD. ACM, New York.AGGARWAL, C. C. 2005. On kanonymity and the curse of dimensionality. In Proceedings of the 31stConference on Very Large Data Bases VLDB. 901909.AGGARWAL, G., FEDER, T., KENTHAPADI, K., MOTWANI, R., PANIGRAHY, R., THOMAS, D., AND ZHU, A. 2006.Achieving anonymity via clustering. In Proceedings of the 25th ACM SIGMODSIGACTSIGART PODSConference. ACM, New York.AGGARWAL, G., FEDER, T., KENTHAPADI, K., MOTWANI, R., PANIGRAHY, R., THOMAS, D., AND ZHU, A. 2005.Anonymizing tables. In Proceedings of the 10th International Conference on Database Theory ICDT.246258.2httpwww.facebook.comACM Computing Surveys, Vol. 42, No. 4, Article 14, Publication date June 2010.1448 B. C. M. Fung et al.AGRAWAL, D. AND AGGARWAL, C. C. 2001. On the design and quantification of privacy preserving datamining algorithms. In Proceedings of the 20th ACM Symposium on Principles of Database SystemsPODS. ACM, New York, 247255.AGRAWAL, R. AND SRIKANT, R. 2000. Privacy preserving data mining. In Proceedings of the ACM SIGMOD.ACM, New York, 439450.AGRAWAL, S. AND HARITSA, J. R. 2005. A framework for highaccuracy privacypreserving mining. In Proceedings of the 21st IEEE International Conference on Data Engineering ICDE. 193204.ALON, N., MATIAS, Y., AND SZEGEDY, M. 1999. The space complexity of approximating the frequency moments. J. Comput. Syst. Sci. 58, 1, 137147.ATZORI, M., BONCHI, F., GIANNOTTI, F., AND PEDRESCHI, D. 2008. Anonymity preserving pattern discovery.Int. J. Very Large Data Bases 17, 4, 703727.ATZORI, M., BONCHI, F., GIANNOTTI, F., PEDRESCHI, D., AND ABUL, O. 2007. Privacyaware knowledge discovery from location data. In Proceedings of the International Workshop on PrivacyAware LocationbasedMobile Services PALMS. 283287.BARAK, B., CHAUDHURI, K., DWORK, C., KALE, S., MCSHERRY, F., AND TALWAR, K. 2007. Privacy, accuracy,and consistency too A holistic solution to contingency table release. In Proceedings of the 26th ACMSymposium on Principles of Database Systems PODS. ACM, New York, 273282.BARBARO, M. AND ZELLER, T. 2006. A face is exposed for AOL searcher no. 4417749. New York Times Aug.9.BAYARDO, R. J. AND AGRAWAL, R. 2005. Data privacy through optimal kanonymization. In Proceedings ofthe 21st IEEE International Conference on Data Engineering ICDE. 217228.BEINAT, E. 2001. Privacy and locationbased Stating the policies clearly. GeoInformatics.BLUM, A., LIGETT, K., AND ROTH, A. 2008. A learning theory approach to noninteractive database privacy.In Proceedings of the 40th Annual ACM Symposium on Theory of Computing STOC. ACM, New York,609618.BLUM, A., DWORK, C., MCSHERRY, F., AND NISSIM, K. 2005. Practical privacy The sulq framework. In Proceedings of the 24th ACM Symposium on Principles of Database Systems PODS. ACM, New York,128138.BRAND, R. 2002. Microdata protection through noise addition. In Inference Control in StatisticalDatabases, From Theory to Practice, London, 97116.BU, Y., FU, A. W. C., WONG, R. C. W., CHEN, L., AND LI, J. 2008. Privacy preserving serial data publishingby role composition. Proc. VLDB Endowment 1, 1, 845856.BURNETT, L., BARLOWSTEWART, K., PROS, A., AND AIZENBERG, H. 2003. The gene trustee A universal identification system that ensures privacy and confidentiality for human genetic databases. J. Law andMedicine 10, 506513.BYUN, J.W., SOHN, Y., BERTINO, E., AND LI, N. 2006. Secure anonymization for incremental datasets. InProceedings of the VLDB Workshop on Secure Data Management SDM.CARLISLE, D. M., RODRIAN, M. L., AND DIAMOND, C. L. 2007. California inpatient data reporting manual,medical information reporting for California 5th Ed, Tech. rep., Office of Statewide Health Planningand Development.CHAKARAVARTHY, V. T., GUPTA, H., ROY, P., AND MOHANIA, M. 2008. Efficient techniques for documentssanitization. In Proceedings of the 17th ACM Conference on Information and Knowledge ManagementCIKM. ACM, New York.CHAUM, D. 1981. Untraceable electronic mail, return addresses, and digital pseudonyms. Comm. ACM 24,2, 8488.CHAWLA, S., DWORK, C., MCSHERRY, F., SMITH, A., AND WEE, H. 2005. Toward privacy in public databases.In Proceedings of the Theory of Cryptography Conference TCC. 363385.CHAWLA, S., DWORK, C., MCSHERRY, F., AND TALWAR, K. 2005. On privacypreserving histograms. In Proceedings of the Uncertainty in Artificial Intelligence Coference UAI.CLIFTON, C., KANTARCIOGLU, M., VAIDYA, J., LIN, X., AND ZHU, M. Y. 2002. Tools for privacy preservingdistributed data mining. ACM SIGKDD Explor. Newsl. 4, 2, 2834.CLIFTON, C. 2000. Using sample size to limit exposure to data mining. J. Comput. Security 8, 4,281307.COX, L. H. 1980. Suppression methodology and statistical disclosure control. J. Am. Statistical Assoc. 75,370, 377385.DALENIUS, T. 1986. Finding a needle in a haystack  or identifying anonymous census record. J. OfficialStatistics 2, 3, 329336.ACM Computing Surveys, Vol. 42, No. 4, Article 14, Publication date June 2010.PrivacyPreserving Data Publishing A Survey of Recent Developments 1449DALENIUS, T. 1977. Towards a methodology for statistical disclosure control. Statistik Tidskrift 15, 429444.DENNING, D. E. 1985. Commutative filters for reducing inference threats in multilevel database systems.In Proceedings of the IEEE Symposium on Security and Privacy.DEUTSCH, A. AND PAPAKONSTANTINOU, Y. 2005. Privacy in database publishing. In Proceedings of the 10thInternational Conference on Database Theory ICDT. 230245.DINUR, I. AND NISSIM, K. 2003. Revealing information while preserving privacy. In Proceedings of the 22ndACM Symposium on Principles of Database Systems PODS. 202210.DOMINGOFERRER, J. 2008. PrivacyPreserving Data Mining Models and Algorithms. Springer, Berlin, 5380.DOMINGOFERRER, J. AND TORRA, V. 2008. A critique of kanonymity and some of its enhancements. InProceedings of the 3rd International Conference on Availability, Reliability and Security ARES. 990993.DOMINGOFERRER, J. AND TORRA, V. 2002. Theory and Practical Applications for Statistical Agencies. NorthHolland, Amsterdam, 113134.DOMINGOFERRER, J. 2001. Confidentiality, Disclosure and Data Access Theory and Practical Applicationsfor Statistical Agencies, 9111.DU, W. AND ZHAN, Z. 2003. Using randomized response techniques for privacypreserving data mining. InProceedings of the 9th ACM SIGKDD. ACM, New York.DUNCAN, G. AND FIENBERG, S. 1998. Obtaining information while preserving privacy A Markov perturbation method for tabular data. In Statistical Data Protection, 351362.DWORK, C. 2008. Differential privacy A survey of results. In Proceedings of the 5th International Conference on Theory and Applications of Models of Computation TAMC. 119.DWORK, C. 2007. Ask a better question, get a better answer A new approach to private data analysis. InProceedings of the International Conference on Database Theory ICDT. 1827.DWORK, C. 2006. Differential privacy. In Proceedings of the 33rd International Colloquium on Automata,Languages and Programming ICALP. 112.DWORK, C., MCSHERRY, F., NISSIM, K., AND SMITH, A. 2006. Calibrating noise to sensitivity in private dataanalysis. In Proceedings of the 3rd Theory of Cryptography Conference TCC. 265284.DWORK, C. AND NISSIM, K. 2004. Privacypreserving data mining on vertically partitioned databases. InProceedings of the 24th International Cryptology Conference CRYPTO. 528544.EMAM, K. E. 2006. Data anonymization practices in clinical research A descriptive study. Tech. rep. Access to Information and Privacy Division of Health in Canada.EVFIMIEVSKI, A., FAGIN, R., AND WOODRUFF, D. P. 2008. Epistemic privacy. In Proceedings of the 27th ACMSymposium on Principles of Database Systems PODS. ACM, New York, 171180.EVFIMIEVSKI, A., SRIKANT, R., AGRAWAL, R., AND GEHRKE, J. 2002. Privacy preserving mining of associationrules. In Proceedings of the 8th ACM SIGKDD. ACM, New York, 217228.FARKAS, C. AND JAJODIA, S. 2003. The inference problem A survey. ACM SIGKDD Explor. Newsl. 4, 2,611.FULLER, W. A. 1993. Masking procedures for microdata disclosure limitation. Official Statistics 9, 2, 383406.FUNG, B. C. M., CAO, M., DESAI, B. C., AND XU, H. 2009. Privacy protection for RFID data. In Proceedingsof the 24th ACM SIGAPP Symposium on Applied Computing SAC. ACM, New York.FUNG, B. C. M., WANG, K., WANG, L., AND HUNG, P. C. K. 2009. Privacypreserving data publishing forcluster analysis. Data Knowl. Engin. 68, 6, 552575.FUNG, B. C. M., WANG, K., FU, A. W. C., AND PEI, J. 2008. Anonymity for continuous data publishing. InProceedings of the 11th International Conference on Extending Database Technology EDBT. ACM, NewYork, 264275.FUNG, B. C. M., WANG, K., WANG, L., AND DEBBABI, M. 2008. A framework for privacypreserving clusteranalysis. In Proceedings of the 2008 IEEE International Conference on Intelligence and Security Informatics ISI. 4651.FUNG, B. C. M., WANG, K., AND YU, P. S. 2007. Anonymizing classification data for privacy preservation.IEEE Trans. Knowl. Data Engin. 19, 5, 711725.FUNG, B. C. M., WANG, K., AND YU, P. S. 2005. Topdown specialization for information and privacy preservation. In Proceedings of the 21st IEEE International Conference on Data Engineering ICDE. 205216.GEHRKE, J. 2006. Models and methods for privacypreserving data publishing and analysis. Tutorial atthe 12th ACM SIGKDD.ACM Computing Surveys, Vol. 42, No. 4, Article 14, Publication date June 2010.1450 B. C. M. Fung et al.GHINITA, G., TAO, Y., AND KALNIS, P. 2008. On the anonymization of sparse highdimensional data. InProceedings of the 24th IEEE International Conference on Data Engineering ICDE. 715724.GOGUEN, J. AND MESEGUER, J. 1984. Unwinding and inference control. In Proceedings of the IEEE Symposium on Security and Privacy.HEGLAND, M., MCINTOSH, I., AND TURLACH, B. A. 1999. A parallel solver for generalized additive models.Comput. Statistics Data Anal. 31, 4, 377396.HENGARTNER, U. 2007. Hiding location information from locationbased services. In Proceedings of theInternational Workshop on PrivacyAware Locationbased Mobile Services PALMS. 268272.HINKE, T. 1988. Inference aggregation detection in database management systems. In Proceedings of theIEEE Symposium on Security and Privacy. 96107.HINKE, T., DEGULACH, H., AND CHANDRASEKHAR, A. 1995. A fast algorithm for detecting second paths indatabase inference analysis. J. Comput. Security.HUANG, Z., DU, W., AND CHEN, B. 2005. Deriving private information from randomized data. In Proceedingsof the ACM SIGMOD. ACM, New York, 3748.HUNDEPOOL, A. AND WILLENBORG, L. 1996. 1 and argus Software for statistical disclosure control.In Proceedings of the 3rd International Seminar on Statistical Confidentiality.IYENGAR, V. S. 2002. Transforming data to satisfy privacy constraints. In Proceedings of the 8th ACMSIGKDD. ACM, New York, 279288.JAJODIA, S. AND MEADOWS, C. 1995. Inference problems in multilevel database management systems. InIEEE Information Security An Integrated Collection of Essays. 570584.JAKOBSSON, M., JUELS, A., AND RIVEST, R. L. 2002. Making mix nets robust for electronic voting by randomized partial checking. In Proceedings of the 11th USENIX Security Symposium. 339353.JIANG, W. AND CLIFTON, C. 2005. Privacypreserving distributed kanonymity. In Proceedings of the 19thAnnual IFIP WG 11.3 Working Conference on Data and Applications Security. 166177.JIANG, W. AND CLIFTON, C. 2006. A secure distributed framework for achieving kanonymity. Very LargeData Bases J. 15, 4, 316333.KANTARCIOGLU, M. 2008. PrivacyPreserving Data Mining Models and Algorithms. Springer, Berlin, 313335.KANTARCIOGLU, M., JIN, J., AND CLIFTON, C. 2004. When do data mining results violate privacy In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM,New York, 599604.KARGUPTA, H., DATTA, S., WANG, Q., AND SIVAKUMAR, K. 2003. On the privacy preserving properties ofrandom data perturbation techniques. In Proceedings of the 3rd IEEE International Conference on DataMining ICDM. 99106.KENTHAPADI, K., MISHRA, N., AND NISSIM, K. 2005. Simulatable auditing. In Proceedings of the 24thACM SIGMODSIGACTSIGART Symposium on Principles of Database Systems. ACM, New York,118127.KIFER, D. AND GEHRKE, J. 2006. Injecting utility into anonymized datasets. In Proceedings of ACM SIGMOD. ACM, New York.KIM, J. AND WINKLER, W. 1995. Masking microdata files. In Proceedings of the ASA Section on SurveyResearch Methods. 114119.KOKKINAKIS, D. AND THURIN, A. 2007. Anonymization of Swedish clinical data. In Proceedings of the 11thConference on Artificial Intelligence in Medicine AIME. 237241.KUMAR, R., NOVAK, J., PANG, B., AND TOMKINS, A. 2007. On anonymizing query logs via tokenbased hashing. In Proceedings of the 16th World Wide Wed Conference. 628638.LEFEVRE, K., DEWITT, D. J., AND RAMAKRISHNAN, R. 2006a. Mondrian multidimensional kanonymity. InProceedings of the 22nd IEEE International Conference on Data Engineering ICDE.LEFEVRE, K., DEWITT, D. J., AND RAMAKRISHNAN, R. 2006b. Workloadaware anonymization. In Proceedingsof the 12th ACM SIGKDD. ACM, New York.LEFEVRE, K., DEWITT, D. J., AND RAMAKRISHNAN, R. 2005. Incognito Efficient fulldomain kanonymity. InProceedings of ACM SIGMOD. ACM, New York, 4960.LI, J., TAO, Y., AND XIAO, X. 2008. Preservation of proximity privacy in publishing numerical sensitive data. In Proceedings of the ACM Conference on Management of Data SIGMOD. 437486.LI, N., LI, T., AND VENKATASUBRAMANIAN, S. 2007. tcloseness Privacy beyond kanonymity and ldiversity.In Proceedings of the 21st IEEE International Conference on Data Engineering ICDE.ACM Computing Surveys, Vol. 42, No. 4, Article 14, Publication date June 2010.PrivacyPreserving Data Publishing A Survey of Recent Developments 1451MACHANAVAJJHALA, A., KIFER, D., ABOWD, J. M., GEHRKE, J., AND VILHUBER, L. 2008. Privacy Theory meetspractice on the map. In Proceedings of the 24th IEEE International Conference on Data EngineeringICDE. 277286.MACHANAVAJJHALA, A., KIFER, D., GEHRKE, J., AND VENKITASUBRAMANIAM, M. 2007. ldiversity Privacy beyond kanonymity. ACM Trans. Knowl. Discov. Data 1, 1.MACHANAVAJJHALA, A., GEHRKE, J., KIFER, D., AND VENKITASUBRAMANIAM, M. 2006. ldiversity Privacybeyond kanonymity. In Proceedings of the 22nd IEEE International Conference on Data EngineeringICDE.MALIN, B. AND AIROLDI, E. 2006. The effects of location access behavior on reidentification risk in a distributed environment. In Proceedings of the 6th Workshop on Privacy Enhancing Technologies PET.413429.MARTIN, D., KIFER, D., MACHANAVAJJHALA, A., GEHRKE, J., AND HALPERN, J. 2007. Worstcase backgroundknowledge in privacypreserving data publishing. In Proceedings of the 23rd IEEE International Conference on Data Engineering ICDE.MATLOFF, N. S. 1988. Inference control via query restriction vs. data modification A perspective. InDatabase Security Status and Prospects. 159166.MEYERSON, A. AND WILLIAMS, R. 2004. On the complexity of optimal kanonymity. In Proceedings of the23rd ACM SIGMODSIGACTSIGART PODS. ACM, New York, 223228.MIKLAU, G. AND SUCIU, D. 2004. A formal analysis of information disclosure in data exchange. In Proceedings of the ACM SIGMOD. ACM, New York, 575586.MOHAMMED, N., FUNG, B. C. M., WANG, K., AND HUNG, P. C. K. 2009. Privacypreserving data mashup. InProceedings of the 12th International Conference on Extending Database Technology EDBT.MOORE, R. A., JR. 1996. Controlled dataswapping techniques for masking public use microdata sets.Statistical Research Division Report Series RR 9604, U.S. Bureau of the Census, Washington, D.C.MOTWANI, R. AND XU, Y. 2007. Efficient algorithms for masking and finding quasiidentifiers. In Proceedings of the Conference on Very Large Data Bases VLDB.NERGIZ, M. E., ATZORI, M., AND CLIFTON, C. W. 2007. Hiding the presence of individuals from shareddatabases. In Proceedings of ACM SIGMOD Conference. ACM, New York, 665676.NERGIZ, M. E. AND CLIFTON, C. 2007. Thoughts on kanonymization. Data Knowl. Engin. 63, 3, 622645.NERGIZ, M. E., CLIFTON, C., AND NERGIZ, A. E. 2007. Multirelational kanonymity. In Proceedings of the23rd International Conference on Data Engineering ICDE. 14171421.OHRN, A. AND OHNOMACHADO, L. 1999. Using Boolean reasoning to anonymize databases. Artif. Intell.Medicine 15, 235254.OZSOYOGLU, G. AND SU, T. 1990. On inference control in semantic data models for statistical databases. J.Comput. Syst. Sci. 40, 3, 405443.PAPADIMITRIOU, S., LI, F., KOLLIOS, G., AND YU, P. S. 2007. Time series compressibility and privacy. In Proceedings of the 33rd International Conference on Very Large Data Bases VLDB,459470.PINKAS, B. 2002. Cryptographic techniques for privacypreserving data mining. ACM SIGKDD Explor.Newsl. 4, 2, 1219.POHLIG, S. AND HELLMAN, M. 1978. An improved algorithm for computing logarithms over gfp and itscryptographic significance. IEEE Trans. Inform. Theory IT24, 106110.PRESIDENT INFORMATION TECHNOLOGY ADVISORY COMMITTEE. 2004. Revolutionizing health care throughinformation technology. Tech. rep., Executive Office of the President of the United States.QUINLAN, J. R. 1993. C4.5 Programs for Machine Learning. Morgan Kaufmann.RASTOGI, V., SUCIU, D., AND HONG, S. 2007. The boundary between privacy and utility in data publishing. In Proceedings of the 33rd International Conference on Very Large Data Bases VLDB. 531542.REISS, S. P. 1984. Practical dataswapping The first steps. ACM Trans. Datab. Syst. 9, 1, 2037.REISS, S. P., POST, M. J., AND DALENIUS, T. 1982. Nonreversible privacy transformations. In Proceedings ofthe 1st ACM Symposium on Principles of Database Systems PODS. 139146.ROSEN, B. E., GOODWIN, J. M., AND VIDAL, J. J. 1992. Process control with adaptive range coding. BiologicalCyber. 67, 419428.RUBIN, D. B. Discussion statistical disclosure limitation. J. Official Statistics 9, 2.SAMARATI, P. 2001. Protecting respondents identities in microdata release. IEEE Trans. Knowl. Data Engin. 13, 6, 10101027.ACM Computing Surveys, Vol. 42, No. 4, Article 14, Publication date June 2010.1452 B. C. M. Fung et al.SAMARATI, P. AND SWEENEY, L. 1998a. Generalizing data to provide anonymity when disclosing information. In Proceedings of the 17th ACM SIGACTSIGMODSIGART PODS. ACM, New York, 188.SAMARATI, P. AND SWEENEY, L. 1998b. Protecting privacy when disclosing information kanonymity andits enforcement through generalization and suppression. Tech. rep., SRI International.SAYGIN, Y., HAKKANITUR, D., AND TUR, G. 2006. Web and Information Security. IRM Press, 133148.SHANNON, C. E. 1948. A mathematical theory of communication. The Bell Syst. Tech. J. 27, 379 and 623.SKOWRON, A. AND RAUSZER, C. 1992. Intelligent Decision Support Handbook of Applications and Advancesof the Rough Set Theory.SWEENEY, L. 2002a. Achieving kanonymity privacy protection using generalization and suppression. Int.J. Uncertainty, Fuzziness, Knowl.Based Syst. 10, 5, 571588.SWEENEY, L. 2002b. kAnonymity A model for protecting privacy. Int. J. Uncertainty, Fuzziness, Knowl.Based Syst. 10, 557570.SWEENEY, L. 1998. Datafly A system for providing anonymity in medical data. In Proceedings of the IFIPTC11 WG11.3 11th International Conference on Database Securty XI Status and Prospects. 356381.TERROVITIS, M. AND MAMOULIS, N. 2008. Privacy preservation in the publication of trajectories. In Proceedings of the 9th International Conference on Mobile Data Management MDM. 6572.TERROVITIS, M., MAMOULIS, N., AND KALNIS, P. 2008. Privacypreserving anonymization of setvalued data.Proc. VLDB Endowment 1, 1, 115125.THURAISINGHAM, B. M. 1987. Security checking in relational database management systems augmentedwith inference engines. Comput. Security 6, 479492.TRUTA, T. M. AND BINDU, V. 2006. Privacy protection psensitive kanonymity property. In Proceedings ofthe Workshop on Privacy Data Management PDM. 94.VAIDYA, J. 2008. PrivacyPreserving Data Mining Models and Algorithms. Springer, Berlin, 337358.VERYKIOS, V. S., ELMAGARMID, A. K., BERTINO, E., SAYGIN, Y., AND DASSENI, E. 2004. Association rule hiding.IEEE Trans. Knowl. Data Engin. 16, 4, 434447.VINTERBO, S. A. 2004. Privacy A machine learning view. IEEE Trans. Knowl. Data Engin. 16, 8, 939948.WANG, K., XU, Y., FU, A. W. C., AND WONG, R. C. W. 2009. ffanonymity When quasiidentifiers are missing.In Proceedings of the 25th IEEE International Conference on Data Engineering ICDE.WANG, K., FUNG, B. C. M., AND YU, P. S. 2007. Handicapping attackers confidence An alternative tokanonymization. Knowl. Inform. Syst. 11, 3, 345368.WANG, K. AND FUNG, B. C. M. 2006. Anonymizing sequential releases. In Proceedings of the 12th ACMSIGKDD Conference. ACM, New York.WANG, K., FUNG, B. C. M., AND DONG, G. 2005. Integrating private databases for data analysis. In Proceedings of the IEEE International Conference on Intelligence and Security Informatics ISI. 171182.WANG, K., FUNG, B. C. M., AND YU, P. S. 2005. Templatebased privacy preservation in classification problems. In Proceedings of the 5th IEEE International Conference on Data Mining ICDM. 466473.WANG, K., YU, P. S., AND CHAKRABORTY, S. 2004. Bottomup generalization A data mining solution toprivacy protection. In Proceedings of the 4th IEEE International Conference on Data Mining ICDM.WANG, S.W., CHEN, W.H., ONG, C.S., LIU, L., AND CHUANG, Y. 2006. RFID applications in hospitals Acase study on a demonstration RFID project in a Taiwan hospital. In Proceedings of the 39th HawaiiInternational Conference on System Sciences.WARNER, S. L. 1965. Randomized response A survey technique for eliminating evasive answer bias. J.Am. Statistical Assoc. 60, 309, 6369.WONG, R. C. W., FU, A. W. C., WANG, K., AND PEI, J. 2007. Minimality attack in privacy preserving datapublishing. In Proceedings of the 33rd International Conference on Very Large Data Bases VLDB. 543554.WONG, R. C. W., LI., J., FU, A. W. C., AND WANG, K. 2006. a,kanonymity An enhanced kanonymity modelfor privacy preserving data publishing. In Proceedings of the 12th ACM SIGKDD. ACM, New York,754759.WRIGHT, R. N., YANG, Z., AND ZHONG, S. 2005. Distributed data mining protocols for privacy A reviewof some recent results. In Proceedings of the Secure Mobile AdHoc Networks and Sensors WorkshopMADNES.XIAO, X. AND TAO, Y. 2007. minvariance Towards privacy preserving republication of dynamic datasets.In Proceedings of the ACM SIGMOD Conference. ACM, New York.XIAO, X. AND TAO, Y. 2006a. Anatomy Simple and effective privacy preservation. In Proceedings of the32nd Conference on Very Large Data Bases VLDB.ACM Computing Surveys, Vol. 42, No. 4, Article 14, Publication date June 2010.PrivacyPreserving Data Publishing A Survey of Recent Developments 1453XIAO, X. AND TAO, Y. 2006b. Personalized privacy preservation. In Proceedings of the ACM SIGMOD Conference. ACM, New York.XU, J., WANG, W., PEI, J., WANG, X., SHI, B., AND FU, A. W. C. 2006. Utilitybased anonymization using localrecoding. In Proceedings of the 12th ACM SIGKDD Conference. ACM, New York.XU, Y., FUNG, B. C. M., WANG, K., FU, A. W. C., AND PEI, J. 2008. Publishing sensitive transactions foritemset utility. In Proceedings of the 8th IEEE International Conference on Data Mining ICDM.XU, Y., WANG, K., FU, A. W. C., AND YU, P. S. 2008. Anonymizing transaction databases for publication. InProceedings of the 14th ACM SIGKDD Conference. ACM, New York.YANG, Z., ZHONG, S., AND WRIGHT, R. N. 2005. Anonymitypreserving data collection. In Proceedings of the11th ACM SIGKDD Conference. ACM, New York, 334343.YAO, C., WANG, X. S., AND JAJODIA, S. 2005. Checking for kanonymity violation by views. In Proceedingsof the 31st Conference on Very Large Data Bases VLDB. 910921.YOU, T.H., PENG, W.C., AND LEE, W.C. 2007. Protect moving trajectories with dummies. In Proceedingsof the International Workshop on PrivacyAware LocationBased Mobile Services PALMS. 278282.ZAYATZ, L. 2007. Disclosure avoidance practices and research at the U.S. Census Bureau An update. J.Official Statistics 23, 2, 253265.ZHANG, P., TONG, Y., TANG, S., AND YANG, D. 2005. Privacypreserving naive Bayes classifier. Lecture Notesin Computer Science, vol. 3584.ZHANG, Q., KOUDAS, N., SRIVASTAVA, D., AND YU, T. 2007. Aggregate query answering on anonymized tables.In Proceedings of the 23rd IEEE International Conference on Data Engineering ICDE.Received April 2008 revised December 2008 accepted December 2008ACM Computing Surveys, Vol. 42, No. 4, Article 14, Publication date June 2010.
