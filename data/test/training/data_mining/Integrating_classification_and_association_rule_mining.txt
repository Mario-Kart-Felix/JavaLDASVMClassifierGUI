Integrating Classification and Association Rule MiningBing Liu        Wynne Hsu        Yiming MaDepartment of Information Systems and Computer ScienceNational University of SingaporeLower Kent Ridge Road, Singapore 119260liub, whsu, mayimingiscs.nus.edu.sgAbstractClassification rule mining aims to discover a small set ofrules in the database that forms an accurate classifier.Association rule mining finds all the rules existing in thedatabase that satisfy some minimum support and minimumconfidence constraints. For association rule mining, thetarget of discovery is not predetermined, while forclassification rule mining there is one and only one predetermined target. In this paper, we propose to integratethese two mining techniques. The integration is done byfocusing on mining a special subset of association rules,called class association rules CARs. An efficientalgorithm is also given for building a classifier based on theset of discovered CARs. Experimental results show that theclassifier built this way is, in general, more accurate thanthat produced by the stateoftheart classification systemC4.5. In addition, this integration helps to solve a numberof problems that exist in the current classification systems.Introduction   Classification rule mining and association rule mining aretwo important data mining techniques. Classification rulemining aims to discover a small set of rules in the databaseto form an accurate classifier e.g., Quinlan 1992 Breimanet al 1984. Association rule mining finds all rules in thedatabase that satisfy some minimum support and minimumconfidence constraints e.g., Agrawal and Srikant 1994.For association rule mining, the target of mining is not predetermined, while for classification rule mining there isone and only one predetermined target, i.e., the class.Both classification rule mining and association rule miningare indispensable to practical applications. Thus, greatsavings and conveniences to the user could result if the twomining techniques can somehow be integrated. In thispaper, we propose such an integrated framework, calledassociative classification. We show that the integration canbe done efficiently and without loss of performance, i.e.,the accuracy of the resultant classifier.The integration is done by focusing on a special subsetof association rules whose righthandside are restricted tothe classification class attribute. We refer to this subset of                                                Copyright  1998, American Association for Artificial Intelligence www.aaai.org. All rights reserved.rules as the class association rules CARs. An existingassociation rule mining algorithm Agrawal and Srikant1994 is adapted to mine all the CARs that satisfy theminimum support and minimum confidence constraints.This adaptation is necessary for two main reasons1. Unlike a transactional database normally used inassociation rule mining Agrawal and Srikant 1994 thatdoes not have many associations, classification datatends to contain a huge number of associations.Adaptation of the existing association rule miningalgorithm to mine only the CARs is needed so as toreduce the number of rules generated, thus avoidingcombinatorial explosion see the evaluation section.2. Classification datasets often contain many continuousor numeric attributes. Mining of association rules withcontinuous attributes is still a major research issueSrikant and Agrawal 1996 Yoda et al 1997 Wang,Tay and Liu 1998. Our adaptation involves discretizingcontinuous attributes based on the classification predetermined class target. There are many gooddiscretization algorithms for this purpose Fayyad andIrani 1993 Dougherty, Kohavi and Sahami 1995.Data mining in the proposed associative classificationframework thus consists of three steps discretizing continuous attributes, if any generating all the class association rules CARs, and building a classifier based on the generated CARs.This work makes the following contributions1. It proposes a new way to build accurate classifiers.Experimental results show that classifiers built thisway are, in general, more accurate than thoseproduced by the stateoftheart classification systemC4.5 Quinlan 1992.2. It makes association rule mining techniquesapplicable to classification tasks.3. It helps to solve a number of important problemswith the existing classification systems.Let us discuss point 3 in greater detail below The framework helps to solve the understandabilityproblem Clark and Matwin 1993 Pazzani, Mani andShankle 1997 in classification rule mining. Many rulesproduced by standard classification systems aredifficult to understand because these systems usedomain independent biases and heuristics to generate asmall set of rules to form a classifier. These biases,Appeared in KDD98, New York, Aug 2731, 1998however, may not be in agreement with the existingknowledge of the human user, thus resulting in manygenerated rules that make no sense to the user, whilemany understandable rules that exist in the data are leftundiscovered. With the new framework, the problem offinding understandable rules is reduced to a postprocessing task since we generate all the rules.Techniques such as those in Liu and Hsu 1996 Liu,Hsu and Chen 1997 can be employed to help the useridentify understandable rules. A related problem is the discovery of interesting oruseful rules. The quest for a small set of rules of theexisting classification systems results in manyinteresting and useful rules not being discovered. Forexample, in a drug screening application, the biologistsare very interested in rules that relate the color of asample to its final outcome. Unfortunately, theclassification system we used C4.5 just could not findsuch rules even though such rules do exist asdiscovered by our system. In the new framework, the database can reside on diskrather than in the main memory. Standard classificationsystems need to load the entire database into the mainmemory e.g., Quinlan 1992, although some work hasbeen done on the scaling up of classification systemsMahta, Agrawal and Rissanen 1996.Problem StatementOur proposed framework assumes that the dataset is anormal relational table, which consists of N cases describedby l distinct attributes. These N cases have been classifiedinto q known classes. An attribute can be a categorical ordiscrete or a continuous or numeric attribute.In this work, we treat all the attributes uniformly. For acategorical attribute, all the possible values are mapped to aset of consecutive positive integers. For a continuousattribute, its value range is discretized into intervals, andthe intervals are also mapped to consecutive positiveintegers. With these mappings, we can treat a data case as aset of attribute, integervalue pairs and a class label. Wecall each attribute, integervalue pair an item.Discretization of continuous attributes will not be discussedin this paper as there are many existing algorithms in themachine learning literature that can be used seeDougherty, Kohavi and Sahami 1995.Let D be the dataset. Let I be the set of all items in D,and Y be the set of class labels. We say that a data case dD contains X  I, a subset of items, if X  d. A classassociation rule CAR is an implication of the form X  y,where X  I, and y  Y. A rule X  y holds in D withconfidence c if c of cases in D that contain X are labeledwith class y. The rule X  y has support s in D if s of thecases in D contain X and are labeled with class y.Our objectives are 1 to generate the complete set ofCARs that satisfy the userspecified minimum supportcalled minsup and minimum confidence called minconfconstraints, and 2 to build a classifier from the CARs.Generating the Complete Set of CARsThe proposed algorithm is called algorithm CBAClassification Based on Associations. It consists of twoparts, a rule generator called CBARG, which is basedon algorithm Apriori for finding association rules inAgrawal and Srikant 1994, and a classifier buildercalled CBACB. This section discusses CBARG. Thenext section discusses CBACB.Basic concepts used in the CBARG algorithmThe key operation of CBARG is to find all ruleitems thathave support above minsup. A ruleitem is of the formcondset, ywhere condset is a set of items, y  Y is a class label. Thesupport count of the condset called condsupCount is thenumber of cases in D that contain the condset. The supportcount of the ruleitem called rulesupCount is the numberof cases in D that contain the condset and are labeled withclass y. Each ruleitem basically represents a rulecondset  y,whose support is rulesupCount  D 100, where D isthe size of the dataset, and whose confidence isrulesupCount  condsupCount100.Ruleitems that satisfy minsup are called frequentruleitems, while the rest are called infrequent ruleitems.For example, the following is a ruleitemA, 1, B, 1, class, 1,where A and B are attributes. If the support count of thecondset A, 1, B, 1 is 3, the support count of theruleitem is 2, and the total number of cases in D is 10, thenthe support of the ruleitem is 20, and the confidence is66.7. If minsup is 10, then the ruleitem satisfies theminsup criterion. We say it is frequent.For all the ruleitems that have the same condset, theruleitem with the highest confidence is chosen as thepossible rule PR representing this set of ruleitems. Ifthere are more than one ruleitem with the same highestconfidence, we randomly select one ruleitem. For example,we have two ruleitems that have the same condset1. A, 1, B, 1, class 1.2.  A, 1, B, 1, class 2.Assume the support count of the condset is 3. The supportcount of the first ruleitem is 2, and the second ruleitem is1. Then, the confidence of ruleitem 1 is 66.7, while theconfidence of ruleitem 2 is 33.3 With these tworuleitems, we only produce one PR assume D  10A, 1, B, 1  class, 1 supt  20, confd 66.7If the confidence is greater than minconf, we say the rule isaccurate. The set of class association rules CARs thusconsists of all the PRs that are both frequent and accurate.The CBARG algorithmThe CBARG algorithm generates all the frequentruleitems by making multiple passes over the data. In thefirst pass, it counts the support of individual ruleitem anddetermines whether it is frequent. In each subsequent pass,it starts with the seed set of ruleitems found to be frequentin the previous pass. It uses this seed set to generate newpossibly frequent ruleitems, called candidate ruleitems.The actual supports for these candidate ruleitems arecalculated during the pass over the data. At the end of thepass, it determines which of the candidate ruleitems areactually frequent. From this set of frequent ruleitems, itproduces the rules CARs.Let kruleitem denote a ruleitem whose condset has kitems. Let Fk denote the set of frequent kruleitems. Eachelement of this set is of the following formcondset, condsupCount, y, rulesupCount.Let Ck be the set of candidate kruleitems. The CBARGalgorithm is given in Figure 1.1 F1  large 1ruleitems2 CAR1  genRulesF13 prCAR1  pruneRulesCAR1 4 for k  2 Fk1   k do5 Ck  candidateGenFk16 for each data case d  D do7 Cd  ruleSubsetCk, d8 for each candidate c  Cd do9 c.condsupCount10 if d.class  c.class then c.rulesupCount11 end12 end13 Fk  c  Ck  c.rulesupCount  minsup14 CARk  genRulesFk15 prCARk  pruneRulesCARk16 end17 CARs  Uk CARk18 prCARs   Uk prCARkFigure 1 The CBARG algorithmLine 13 represents the first pass of the algorithm. It countsthe item and class occurrences to determine the frequent 1ruleitems line 1. From this set of 1ruleitems, a set ofCARs called CAR1 is generated by genRules line 2 seeprevious subsection. CAR1 is subjected to a pruningoperation line 3 which can be optional. Pruning is alsodone in each subsequent pass to CARk line 15. Thefunction pruneRules uses the pessimistic error rate basedpruning method in C4.5 Quinlan 1992. It prunes a rule asfollows If rule rs pessimistic error rate is higher than thepessimistic error rate of rule r obtained by deleting onecondition from the conditions of r, then rule r is pruned.This pruning can cut down the number of rules generatedsubstantially see the evaluation section.For each subsequent pass, say pass k, the algorithmperforms 4 major operations. First, the frequent ruleitemsFk1 found in the k1th pass are used to generate thecandidate ruleitems Ck using the condidateGen functionline 5. It then scans the database and updates varioussupport counts of the candidates in Ck line 612. Afterthose new frequent ruleitems have been identified to formFk line 13, the algorithm then produces the rules CARkusing the genRules function line 14. Finally, rule pruningis performed line 15 on these rules.The candidateGen function is similar to the functionApriorigen in algorithm Apriori. The ruleSubset functiontakes a set of candidate ruleitems Ck and a data case d tofind all the ruleitems in Ck whose condsets are supportedby d. This and the operations in line 810 are also similarto those in algorithm Apriori. The difference is that weneed to increment the support counts of the condset and theruleitem separately whereas in algorithm Apriori only onecount is updated. This allows us to compute the confidenceof the ruleitem. They are also useful in rule pruning.The final set of class association rules is in CARs line17. Those remaining rules after pruning are in prCARsline 18.Building a ClassifierThis section presents the CBACB algorithm for building aclassifier using CARs or prCARs. To produce the bestclassifier out of the whole set of rules would involveevaluating all the possible subsets of it on the training dataand selecting the subset with the right rule sequence thatgives the least number of errors. There are 2m such subsets,where m is the number of rules, which can be more than10,000, not to mention different rule sequences. This isclearly infeasible. Our proposed algorithm is a heuristicone. However, the classifier it builds performs very well ascompared to that built by C4.5. Before presenting thealgorithm, let us define a total order on the generated rules.This is used in selecting the rules for our classifier.Definition Given two rules, ri and rj, ri f rj also called riprecedes rj or ri has a higher precedence than rj if1. the confidence of ri is greater than that of rj, or2. their confidences are the same, but the support of riis greater than that of rj, or3. both the confidences and supports of ri and rj arethe same, but ri is generated earlier than rjLet R be the set of generated rules i.e., CARs or pCARs,and D the training data. The basic idea of the algorithm isto choose a set of high precedence rules in R to cover D.Our classifier is of the following formatr1, r2, , rn, defaultclass,where ri  R, ra f rb if b  a. defaultclass is the defaultclass. In classifying an unseen case, the first rule thatsatisfies the case will classify it. If there is no rule thatapplies to the case, it takes on the default class as in C4.5.A naive version of our algorithm called M1 for buildingsuch a classifier is shown in Figure 2. It has three stepsStep 1 line 1 Sort the set of generated rules R accordingto the relation f. This is to ensure that we will choosethe highest precedence rules for our classifier.Step 2 line 213 Select rules for the classifier from Rfollowing the sorted sequence. For each rule r, we gothrough D to find those cases covered by r they satisfythe conditions of r line 5. We mark r if it correctlyclassifies a case d line 6. d.id is the uniqueidentification number of d. If r can correctly classify atleast one case i.e., if r is marked, it will be a potentialrule in our classifier line 78. Those cases it covers arethen removed from D line 9. A default class is alsoselected the majority class in the remaining data,which means that if we stop selecting more rules for ourclassifier C this class will be the default class of C line10. We then compute and record the total number oferrors that are made by the current C and the defaultclass line 11. This is the sum of the number of errorsthat have been made by all the selected rules in C andthe number of errors to be made by the default class inthe training data. When there is no rule or no trainingcase left, the rule selection process is completed.Step 3 line 1415 Discard those rules in C that do notimprove the accuracy of the classifier. The first rule atwhich there is the least number of errors recorded on Dis the cutoff rule. All the rules after this rule can bediscarded because they only produce more errors. Theundiscarded rules and the default class of the last rule inC form our classifier.1 R  sortR2 for each rule r  R in sequence do3 temp  4 for each case d  D do5 if d satisfies the conditions of r then6 store d.id in temp and mark r if it correctlyclassifies d7 if r is marked then  8 insert r at the end of C9 delete all the cases with the ids in temp from D10 selecting a default class for the current C 11 compute the total number of errors of C12 end13 end14 Find the first rule p in C with the lowest total numberof errors and drop all the rules after p in C15 Add the default class associated with p to end of C,and return C our classifier.Figure 2. A nave algorithm for CBACB M1This algorithm satisfies two main conditionsCondition 1. Each training case is covered by the rule withthe highest precedence among the rules that can coverthe case. This is so because of the sorting done in line 1.Condition 2.  Every rule in C correctly classifies at leastone remaining training case when it is chosen. This is sodue to line 57.This algorithm is simple, but is inefficient especiallywhen the database is not resident in the main memorybecause it needs to make many passes over the database.Below, we present an improved version of the algorithmcalled M2, whereby only slightly more than one pass ismade over D. The key point is that instead of making onepass over the remaining data for each rule in M1, we nowfind the best rule in R to cover each case. M2 consists ofthree stages see Liu, Hsu and Ma 1998 for more detailsStage 1. For each case d, we find both the highestprecedence rule called cRule that correctly classifies d,and also the highest precedence rule called wRule thatwrongly classifies d. If cRule f wRule, the case should becovered by cRule. This satisfies Condition 1 and 2 above.We also mark the cRule to indicate that it classifies a casecorrectly. If wRule f cRule, it is more complex because wecannot decide which rule among the two or some other rulewill eventually cover d. In order to decide this, for each dwith wRule f cRule, we keep a data structure of the formdID, y, cRule, wRule, where dID is the uniqueidentification number of the case d, y is the class of d. LetA denote the collection of dID, y, cRule, wRules, U theset of all cRules, and Q the set of cRules that have a higherprecedence than their corresponding wRules. This stage ofthe algorithm is shown in Figure 3.The maxCoverRule function finds the highestprecedence rule that covers the case d. Cc or Cw is the setof rules having the same or different class as d. d.id andd.class represent the identification number and the class ofd respectively. For each cRule, we also remember howmany cases it covers in each class using the fieldclassCasesCovered of the rule.1 Q   U   A  2 for each case d  D do3 cRule  maxCoverRuleCc, d4 wRule  maxCoverRuleCw, d5 U  U  cRule6 cRule.classCasesCoveredd.class7 if cRule f wRule then8 Q  Q  cRule9 mark cRule10 else  A  A  d.id, d.class, cRule, wRule11 endFigure 3 CBACB M2 Stage 1Stage 2. For each case d that we could not decide whichrule should cover it in Stage 1, we go through d again tofind all rules that classify it wrongly and have a higherprecedence than the corresponding cRule of d line 5 inFigure 4. That is the reason we say that this method makesonly slightly more than one pass over D. The details are asfollows Figure 41 for each entry  dID, y, cRule, wRule  A do2 if wRule is marked then3 cRule.classCasesCoveredy4 wRule.classCasesCoveredy5 else wSet  allCoverRulesU, dID.case, cRule6 for each rule w  wSet do7 w.replace  w.replace  cRule, dID, y8 w.classCasesCoveredy9 end10 Q  Q  wSet11 end12 endFigure 4 CBACB M2 Stage 2If wRule is marked which means it is the cRule of at leastone case line 2, then it is clear that wRule will cover thecase represented by dID. This satisfies the two conditions.The numbers of data cases, that cRule and wRule cover,need to be updated line 34. Line 5 finds all the rules thatwrongly classify the dID case and have higher precedencesthan that of its cRule note that we only need to use therules in U. This is done by the function allCoverRules.The rules returned are those rules that may replace cRule tocover the case because they have higher precedences. Weput this information in the replace field of each rule line7. Line 8 increments the count of w.classCasesCoveredyto indicate that rule w may cover the case. Q contains allthe rules to be used to build our classifier.Stage 3. Choose the final set of rules to form ourclassifier Figure 5. It has two stepsStep 1 line 117 Choose the set of potential rules to formthe classifier. We first sort Q according to the relationf. This ensures that Condition 1 above is satisfied inthe final rule selection. Line 1 and 2 are initializations.The compClassDistr function counts the number oftraining cases in each class line 1 in the initial trainingdata. ruleErrors records the number of errors made sofar by the selected rules on the training data.In line 5, if rule r no longer correctly classifies anytraining case, we discard it. Otherwise, r will be a rule inour classifier. This satisfies Condition 2. In line 6, r willtry to replace all the rules in r.replace because rprecedes them. However, if the dID case has alreadybeen covered by a previous rule, then the current r willnot replace rul to cover the case. Otherwise, r willreplace rul to cover the case, and the classCasesCoveredfields of r and rul are updated accordingly line 79.For each selected rule, we update ruleErrors andclassDistr line 1011. We also choose a default classi.e., defaultClass, which is the majority class in theremaining training data, computed using classDistr line12. After the default class is chosen, we also know thenumber of errors called defaultError that the defaultclass will make in the remaining training data line 13.The total number of errors denoted by totalErrors thatthe selected rules in C and the default class will make isruleErrors  defaultErrors line 14.Step 2 line 1820 Discard those rules that introducemore errors, and return the final classifier C this is thesame as in M1.1 classDistr  compClassDistriD2 ruleErrors  03 Q  sortQ4 for each rule r in Q in sequence do5 if r.classCasesCoveredr.class   0 then6 for each entry rul, dID, y in r.replace do7 if the dID case has been covered by aprevious r then8 r.classCasesCoveredy9 else rul.classCasesCoveredy10 ruleErrors  ruleErrors  errorsOfRuler11 classDistr  updater, classDistr12 defaultClass  selectDefaultclassDistr13 defaultErrors  defErrdefaultClass, classDistr14 totalErrors  ruleErrors  defaultErrors15 Insert r, defaultclass, totalErrors at end of C16 end17 end18 Find the first rule p in C with the lowest totalErrors,and then discard all the rules after p from C19 Add the default class associated with p to end of C20 Return C without totalErrors and defaultclassFigure 5 CBACB M2 Stage 3Empirical EvaluationWe now compare the classifiers produced by algorithmCBA with those produced by C4.5 tree and rule Release8. We used 26 datasets from UCI ML Repository Merzand Murphy 1996 for the purpose. The execution timeperformances of CBARG and CBACB are also shown.In our experiments, minconf is set to 50. For minsup,it is more complex. minsup has a strong effect on thequality of the classifier produced. If minsup is set too high,those possible rules that cannot satisfy minsup but withhigh confidences will not be included, and also the CARsmay fail to cover all the training cases. Thus, the accuracyof the classifier suffers. From our experiments, we observethat once minsup is lowered to 12, the classifier built ismore accurate than that built by C4.5. In the experimentsreported below, we set minsup to 1. We also set a limit of80,000 on the total number of candidate rules in memoryincluding both the CARs and those droppedoff rules thateither do not satisfy minsup or minconf. 16 marked with a in Table 1 of the 26 datasets reported below cannot becompleted within this limit. This shows that classificationdata often contains a huge number of associations. Discretization of continuous attributes is done usingthe Entropy method in Fayyad and Irani 1993. The codeis taken from MLC machine learning library Kohavi etal 1994. In the experiments, all C4.5 parameters had theirdefault values. All the error rates on each dataset areobtained from 10fold crossvalidations. The experimentalresults are shown in Table 1. The execution times here arebased on datasets that reside in the main memory.Column 1 It lists the names of the 26 datasets. See Liu,Hsu and Ma 1998 for the description of the datasets.Column 2 It shows C4.5rules mean error rates over tencomplete 10fold crossvalidations using the originaldatasets i.e., without discretization. We do not showC4.5 trees detailed results because its average error rateover the 26 datasets is higher 17.3.Column 3 It shows C4.5rules mean error rate afterdiscretization. The error rates of C4.5 tree are not usedhere as its average error rate 17.6 is higher.Column 4 It gives the mean error rates of the classifiersbuilt using our algorithm with minsup  1 over the tencrossvalidations, using both CARs and infrequent rulesdropped off rules that satisfy minconf. We useinfrequent rules because we want to see whether theyaffect the classification accuracy. The first value is theerror rate of the classifier built with rules that are notsubjected to pruning in rule generation, and the secondvalue is the error rate of the classifier built with rulesthat are subjected to pruning in rule generation.Column 5 It shows the error rates using only CARs in ourclassifier construction without or with rule pruning i.e.,prCARs in rule generation.It is clear from these 26 datasets that CBA produces moreaccurate classifiers. On average, the error rate decreasesfrom 16.7 for C4.5rules without discretization to 15.615.8  for  CBA.  Furthermore, our  system  is  superior toC4.5rules on 16 of the 26 datasets. We also observe thatwithout or with rule pruning the accuracy of the resultantclassifier is almost the same. Thus, those prCARs afterpruning are sufficient for building accurate classifiers.Note that when compared with the error rate 17.1 ofC4.5rules after discretization, CBA is even more superior.Let us now see the rest of the columns, which give thenumber of rules generated and the run time performancesof our system running on 192MB DEC alpha 500.Column 6 It gives the average numbers of rules generatedby algorithm CBARG in each crossvalidation. The firstvalue is the number of CARs. The second value is thenumber of prCARs after pruning. We see that thenumber of rules left after pruning is much smaller.Column 7 It gives the average time taken to generate therules in each crossvalidation. The first value is the timetaken when no pruning is performed. The second valueis the time taken when pruning is used. With pruning,algorithm CBARG only runs slightly slower.Column 8 It shows the average times taken to build eachclassifier using only prCARs. The first value is therunning time of method 1 M1, and the second value isthat of method 2 M2. We see that M2 is much moreefficient than M1.Column 9 It gives the average number of rules in theclassifier built by CBACB using prCARs. There aregenerally  more rules in our  classifier than that producedby C4.5 not shown here. But this is not a problem asthese rules are only used to classify future cases.Understandable and useful rules can be found in CARsor prCARs. These rules may or may not be generatedby C4.5 since C4.5 does not generate all the rules.Below, we summarize two other important results. Although we cannot find all the rules in 16 of the 26datasets using the 80,000 limit, the classifiersconstructed with the discovered rules are already quiteaccurate. In fact, when the limit reaches 60,000 in the26 datasets we have experimented with manydifferent limits, the accuracy of the resultingclassifiers starts to stabilize. Proceeding further onlygenerate rules with many conditions that are hard tounderstand and difficult to use. We also ran the CBA algorithm with the datasets ondisk rather than in the main memory, and increasedthe number of cases of all datasets by up to 32 timesthe largest dataset reaches 160,000 cases.Experimental results show that both CBARG andCBACB M2 have linear scaleup.Related WorkOur system is clearly different from the existingclassification systems, such as C4.5 Quinlan 1992 andCART Breiman et al 1984, which only produce a small c4.5rules   c4.5rules  CBA  CBA    No. of         Run time sec Run time sec No. ofDatasets wo discr.     discr.   CARs  infreq  CARs   CARs         CBARG    CBACB  Rules wo pru. pru. wo pru pru. wo pru. pru. wo pru pru. M1 M2   in Canneal 5.2 6.5 1.9 1.9 3.2 3.6 65081 611 14.33 14.36 0.08 0.06 34australian 15.3 13.5 13.5 13.4 13.2 13.4 46564 4064 5.00 5.05 0.20 0.22 148auto 19.9 29.2 21.0 23.1 24.0 27.2 50236 3969 3.30 3.55 0.12 0.06 54breastw 5.0 3.9 3.9 3.9 4.2 4.2 2831 399 0.30 0.33 0.02 0.03 49cleve 21.8 18.2 18.1 19.1 16.7 16.7 48854 1634 4.00 4.30 0.04 0.06 78crx 15.1 15.9 14.3 14.3 14.1 14.1 42877 4717 4.90 5.06 0.43 0.30 142diabetes 25.8 27.6 24.8 25.5 24.7 25.3 3315 162 0.25 0.28 0.03 0.01 57german 27.7 29.5 27.2 26.5 25.2 26.5 69277 4561 5.60 6.00 1.04 0.28 172glass 31.3 27.5 27.4 27.4 27.4 27.4 4234 291 0.20 0.22 0.02 0.00 27heart 19.2 18.9 19.6 19.6 18.5 18.5 52309 624 4.70 4.60 0.04 0.03 52hepatitis 19.4 22.6 15.1 15.1 15.1 15.1 63134 2275 2.80 2.79 0.09 0.05 23horse 17.4 16.3 18.2 17.9 18.7 18.7 62745 7846 3.2 3.33 0.35 0.19 97hypo 0.8 1.2 1.6 1.6 1.9 1.7 37631 493 45.60 45.30 1.02 0.40 35ionosphere 10.0 8.0 7.9 7.9 8.2 8.2 55701 10055 3.75 4.00 0.56 0.41 45iris 4.7 5.3 7.1 7.1 7.1 7.1 72 23 0.00 0.00 0.00 0.00 5labor 20.7 21.0 17.0 17.0 17.0 17.0 5565 313 0.17 0.20 0.00 0.00 12led7 26.5 26.5 27.8 27.8 27.8 27.8 464 336 0.40 0.45 0.11 0.10 71lymph 26.5 21.0 20.3 18.9 20.3 19.6 40401 2965 2.70 2.70 0.07 0.05 36pima 24.5 27.5 26.9 27.0 27.4 27.6 2977 125 0.23 0.25 0.04 0.02 45sick 1.5 2.1 2.8 2.8 2.7 2.7 71828 627 32.60 33.40 0.62 0.40 46sonar 29.8 27.8 24.3 21.7 24.3 21.7 57061 1693 5.34 5.22 0.30 0.12 37tictactoe 0.6 0.6 0.0 0.0 0.0 0.0 7063 1378 0.62 0.71 0.12 0.08 8vehicle 27.4 33.6 31.3 31.2 31.5 31.3 23446 5704 6.33 6.33 1.40 0.40 125waveform 21.9 24.6 20.2 20.2 20.4 20.6 9699 3396 13.65 13.55 2.72 1.12 386wine 7.3 7.9 8.4 8.4 8.4 8.4 38070 1494 2.34 2.65 0.11 0.04 10zoo 7.8 7.8 5.4 5.4 5.4 5.4 52198 2049 2.73 2.70 0.61 0.32 7Average 16.7 17.1 15.6 15.6 15.7 15.8 35140 2377 6.35 6.44 0.39 0.18 69Table 1 Experiment Resultsset of biased rules. The proposed technique aims togenerate the complete set of potential classification rules.Several researchers, e.g., Schlimmer 1993, Webb1993, and Murphy and Pazzani 1994 have tried to buildclassifiers by performing extensive search. None of them,however, uses association rule mining techniques.Our classifier building technique is related to thecovering method in Michalski 1980. The coveringmethod works as follows. For each class in turn, it findsthe best rule for the class and then removes those trainingcases covered by the rule. The strategy is then recursivelyapplied to the remaining cases in the class until no moretraining case is left. Searching for the rule is doneheuristically. In Webb 1993 Quinlan and CameronJones1995, large scale beamsearch is used to search for thebest rule. The results were, however, not encouraging. Ourmethod is different. We first find all the rules, and thenselect the best rules to cover the training cases. Test resultsshow that our classifier performs better than that built byC4.5. Our best rules are global best rules because theyare generated using all the training data, while the bestrules of the covering method are local best rules becauseit removes those covered cases after each best rule isfound. We also implemented the covering method in ourframework, but the performance is not good. We believe itis because local best rules tend to overfit the data.This research is closely related to association rulemining Agrawal and Srikant 1994. The Apriori algorithmin Agrawal and Srikant 1994 has been adapted fordiscovering CARs. In CBARG, we do not use itemset aset of items as in algorithm Apriori. Instead, we useruleitem, which consists of a condset a set of items and aclass. We also use the rule pruning technique in Quinlan1992 to prune off those nonpredictive and overfittingrules. This is not used in association rule mining. Inaddition, we build an accurate classifier that can be usedfor prediction from the generated rules. The associationrule discovery is not concerned with building classifiers.Bayardo 1997 uses an association rule miner togenerate highconfidence classification rules confidence 90. Ali, Manganaris and Srikant 1997 uses anassociation rule miner to form rules that can describeindividual classes. Both works are not concerned withbuilding a classifier from the rules. This paper has shownthat it is possible to build an accurate classifier forprediction from the set of generated rules.ConclusionThis paper proposes a framework to integrate classificationand association rule mining. An algorithm is presented togenerate all class association rules CARs and to build anaccurate classifier. The new framework not only gives anew way to construct classifiers, but also helps to solve anumber of problems that exist in current classificationsystems. In our future work, we will focus on buildingmore accurate classifiers by using more sophisticatedtechniques and to mine CARs without prediscretization.ReferencesAgrawal, R. and Srikant, R. 1994. Fast algorithms formining association rules. VLDB94, 1994.Ali, K., Manganaris, S. and Srikant, R. 1997. Partialclassification using association rules. KDD97, 115118.Bayardo, R. J. 1997. Bruteforce mining of highconfidence classification rules. KDD97, 123126.Breiman, L, Friedman, J., Olshen, R. and Stone C. 1984.Classification and regression trees. BelmontWadsworth.Clark, P. and Matwin, S. 1993. Using qualitative models toguide induction learning.  ICML93, 4956.Dougherty, J., Kohavi, R. Sahami, M. 1995. Supervisedand unsupervised discretization of continuous features.ICML95.Fayyad, U. M. and Irani, K. B. 1993. Multiintervaldiscretization of continuousvalued attributes forclassification learning. IJCAI93, 10221027.Kohavi, R., John, G., Long, R., Manley, D., and Pfleger,K. 1994. MLC a machine learning library in C.Tools with artificial intelligence, 740743.Liu, B. and Hsu, W. 1996. Postanalysis of learned rules.AAAI96, 828834.Liu, B., Hsu, W. and Chen, S. 1997. Using generalimpressions to analyze discovered classification rules.KDD97, 3136.Liu, B., Hsu, W. and Ma, Y. 1998. Building an accurateclassifier using association rules. Technical report.Mahta, M., Agrawal, R. and Rissanen, J. 1996. SLIQ Afast scalable classifier for data mining. Proc. of the fifthIntl Conference on Extending Database Technology.Merz, C. J, and Murphy, P. 1996. UCI repository ofmachine learning database. httpwww.cs.uci.edumlearnMLRepository.html.Michalski, R. 1980. Pattern recognition as ruleguidedinduction inference. IEEE Transaction On PatternAnalysis and Machine Intelligence 2, 349361.Murphy, P. and Pazzani, M. 1994. Exploring the decisionforest an empirical investigation of Occams razor indecision tree induction. J. of AI Research 1257275.Pazzani, M., Mani, S. and Shankle, W. R. 1997. Beyondconcise and colorful learning intelligible rules. KDD97.Quinlan, J. R. 1992. C4.5 program for machine learning.Morgan Kaufmann.Quinlan, R. and CameronJones, M. 1995. Oversearchingand layered search in empirical learning. IJCAI95.Schlimmer, J 1993. Efficiently inducing determinations acomplete and systematic search algorithm that usesoptimal pruning. ICML93, 268275.Srikant, R. and Agrawal, R. 1996. Mining quantitativeassociation rules in large relational tables. SIGMOD96.Wang, K., Tay, W. and Liu, B. 1998. An interestingnessbased interval merger for numeric association rules.KDD98.Yoda, K., Fukuda, T. Morimoto, Y. Morishita, S. andTokuyama, T. 1997. Computing optimized rectilinearregions for association rules. KDD97.
