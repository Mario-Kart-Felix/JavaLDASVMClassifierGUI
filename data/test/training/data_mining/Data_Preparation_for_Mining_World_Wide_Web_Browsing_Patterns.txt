Data Preparation for Mining World Wide WebBrowsing PatternsRobert Cooley, Bamshad Mobasher, and Jaideep SrivastavaDepartment of Computer Science and EngineeringUniversity of Minnesota4192 EECS Bldg., 200 Union St. SEMinneapolis, MN 55455, USAAbstract. The World Wide Web WWW continues to grow at an astounding rate in both the sheer volume of traffic and the size and complexity of Web sites. The complexity of tasks such as Web site design,Web server design, and of simply navigating through a Web site haveincreased along with this growth. An important input to these designtasks is the analysis of how a Web site is being used. Usage analysis includes straightforward statistics, such as page access frequency, as well asmore sophisticated forms of analysis, such as finding the common traversal paths through a Web site. Web Usage Mining is the application ofdata mining techniques to usage logs of large Web data repositories inorder to produce results that can be used in the design tasks mentionedabove. However, there are several preprocessing tasks that must be performed prior to applying data mining algorithms to the data collectedfrom server logs. This paper presents several data preparation techniquesin order to identify unique users and user sessions. Also, a method to divide user sessions into semantically meaningful transactions is definedand successfully tested against two other methods. Transactions identified by the proposed methods are used to discover association rules fromreal world data using the WEBMINER system 15.1 Introduction and BackgroundThe World Wide Web WWW continues to grow at an astounding rate inboth the sheer volume of traffic and the size and complexity of Web sites. Thecomplexity of tasks such as Web site design, Web server design, and of simplynavigating through a Web site have increased along with this growth. An important input to these design tasks is analysis of how a Web site is being used. Usageanalysis includes straightforward statistics, such as page access frequency, as wellas more sophisticated forms of analysis, such as finding the common traversalpaths through a Web site. Usage information can be used to restructure a Website in order to better serve the needs of users of a site. Long convoluted traversalpaths or low usage of a page with important site information could suggest thatthe site links and information are not laid out in an intuitive manner. The design Supported by NSF grant EHR9554517.of a phsysical data layout or caching scheme for a distributed or parallel Webserver can be enchanced by knowledge of how users typically navigate throughthe site. Usage information can also be used to directly aide site navigation byproviding a list of popular destinations from a particular Web page.Web Usage Mining is the application of data mining techniques to large Webdata repositories in order to produce results that can be used in the design tasksmentioned above. Some of the data mining algorithms that are commonly usedin Web Usage Mining are association rule generation, sequential pattern generation, and clustering. Association Rule mining techniques 1 discover unorderedcorrelations between items found in a database of transactions. In the context ofWeb Usage Mining a transaction is a group of Web page accesses, with an itembeing a single page access. Examples of association rules found from an IBManalysis of the server log of the Official 1996 Olympics Web site 7 are 45 of the visitors who accessed a page about Indoor Volleyball also accesseda page on Handball. 59.7 of the visitors who accessed pages about Badminton and Diving alsoaccessed a page about Table Tennis.The percentages reported in the examples above are referred to as confidence.Confidence is the number of transactions containing all of the items in a rule,divided by the number of transactions containing the rule antecedants The antecedants are Indoor Volleyball for the first example and Badminton and Divingfor the second example.The problem of discovering sequential patterns 17, 26 is that of finding intertransaction patterns such that the presence of a set of items is followed byanother item in the timestamp ordered transaction set. By analyzing this information, a Web Usage Mining system can determine temporal relationshipsamong data items such as the following Olympics Web site examples 9.81 of the site visitors accessed the Atlanta home page followed by theSneakpeek main page. 0.42 of the site visitors accessed the Sports main page followed by theSchedules main page.The percentages in the second set of examples are referred to as support.Support is the percent of the transactions that contain a given pattern. Bothconfidence and support are commonly used as thresholds in order to limit thenumber of rules discovered and reported. For instance, with a 1 support threshold, the second sequential pattern example would not be reported.Clustering analysis 12, 19 allows one to group together users or data itemsthat have similar characteristics. Clustering of user information or data fromWeb server logs can facilitate the development and execution of future marketingstrategies, both online and offline, such as automated return mail to visitorsfalling within a certain cluster, or dynamically changing a particular site for avisitor on a return visit, based on past classification of that visitor.As the examples above show, mining for knowledge from Web log data hasthe potential of revealing information of great value. While this certainly isan application of existing data mining algorithms, e.g. discovery of associationrules or sequential patterns, the overall task is not one of simply adapting existingalgorithms to new data. Ideally, the input for the Web Usage Mining process is afile, referred to as a user session file in this paper, that gives an exact accountingof who accessed the Web site, what pages were requested and in what order, andhow long each page was viewed. A user session is considered to be all of thepage accesses that occur during a single visit to a Web site. The informationcontained in a raw Web server log does not reliably represent a user session filefor a number of reasons that will be discussed in this paper. Specifically, thereare a number of difficulties involved in cleaning the raw server logs to eliminateoutliers and irrelevant items, reliably identifying unique users and user sessionswithin a server log, and identifying semantically meaningful transactions withina user session.This paper presents several data preparation techniques and algorithms thatcan be used in order to convert raw Web server logs into user session files inorder to perform Web Usage Mining. The specific contributions include i development of models to encode both the Web site developers and users view ofhow a Web site should be used, ii discussion of heuristics that can be used toidentify Web site users, user sessions, and page accesses that are missing from aWeb server log, iii definition of several transaction identification approaches,and iv and evaluation of the different transaction identification approachesusing synthetic server log data with known association rules.The rest of this paper is organized as follows Section 2 reviews related work.Section 3 briefly discusses the architecture of the Web Usage Mining processand the WEBMINER system 15. Section 4 presents a model for user browsingbehavior and a method for encoding a Web site designers view of how a siteshould be used. Section 5 gives a detailed breakdown of the steps involved inpreprocessing data for Web Usage Mining. Section 6 presents a general modelfor identifying transactions along with some specific transaction identificationapproaches. Section 7 discusses a method used to generate Web server log datain order to compare the different transaction identification approaches. Section 8presents the experimental results of using the WEBMINER system to minefor association rules with transactions identified with the different approaches.Finally, Section 9 provides conclusions.2 Related WorkThere are several commercially available Web server log analysis tools, suchas 8, 10, 18, that provide limited mechanisms for reporting user activity, i.e.it is possible to determine the number of accesses to individual files and thetimes of visits. However, these tools are not designed for very high traffic Webservers, and usually provide little analysis of data relationships among accessedfiles, which is essential to fully utilizing the data gathered in the server logs. Theconcept of applying data mining techniques to Web server logs was first proposedin 6, 16, and 29. Mannila et. al. 16 use page accesses from a Web serverlog as events for discovering frequent episodes 17. Chen et. al. 6 introducethe concept of using the maximal forward references in order to break down usersessions into transactions for the mining of traversal patterns. A maximal forwardreference is the last page requested by a user before backtracking occurs, wherethe user requests a page previously viewed during that particular user session.For example, if a user session consists of requests for pages ABACDC, in thatorder, the maximal forward references for the session would be B and D. Both 6and 16 concentrate on developing data mining algorithms and assume that theserver logs, after filtering out image files, represent an accurate picture of siteusage. The Analog system 29 uses Web server logs to assign site visitors toclusters. The links that are presented to a given user are dynamically selectedbased on what pages other users assigned to the same cluster have visited.The difficulty of identifying users and user sessions from Web server logs hasbeen addressed in research performed by Pitkow 5, 21, 23. Two of the biggest impediments to collecting reliable usage data are local caching and proxy servers. Inorder to improve performance and minimize network traffic, most Web browserscache the pages that have been requested. As a result, when a user hits theback button, the cached page is displayed and the Web server is not aware ofthe repeat page access. Proxy servers provide an intermediate level of cachingand create even more problems with identifying site usage. In a Web server log,all requests from a proxy server have the same identifier, even though the requests potentially represent more than one user. Also, due to proxy server levelcaching, a single request from the server could actually be viewed by multipleusers throughout an extended period of time. The data mining results from the1996 Olympics site 7 were obtained using cookies to identify site users and usersessions. Cookies are markers that are used to tag and track site visitors automatically. Another approach to getting around the problems created by cachingand proxy servers is to use a remote agent, as described in 28. Instead of sending a cookie, 28 sends a Java agent that is run on the client side browser inorder to send back accurate usage information to the Web server. The majordisadvantage of the methods that rely on implicit user cooperation stem fromprivacy issues. There is a constant struggle between the Web users desire for privacy and the Web providers desire for information about who is using their site.Many users choose to disable the browser features that enable these methods.Han et. al. 30 have loaded Web server logs into a data cube structure 9 inorder to perform data mining as well as traditional OnLine Analytical Processing OLAP activities such as rollup and drilldown of the data. While both 29and 30 acknowledge the difficulties involved in identifying users and user sessions, no specific methods are presented for solving the problem. Smith et. al. 27use path profiles in order to generate dynamic content in advance of a userrequest. The problems that caching causes are also identified in 27, but becauseof the focus on dynamic content, caching is assumed to have minimal impact,and is hence ignored.The SiteHelper system 20 learns from information extracted from a Websites server log in tandem with search cues explicitly given by a user in orderto recommend pages within the Web site. Several research efforts, such as 3, 11,13, 22, maintain logs of user actions as part of the process of using the contentof Web pages and hypertext links in order to provide page recommendationsto users. These projects can be classified as Web Content Mining as definedby 4, since the learning and inference is predominantly based on the contentof the pages and links, and not strictly the behavior of the users. Because usersexplicitly request the services of these recommendation systems, the problem ofidentifying user sessions is greatly simplified. Effectively, a request for a pagerecommendation on the part of a user serves as a registration, enabling thetracking of the session. While users are often willing to register in order to receivecertain benefits which in this case is in the form of navigation assistance, fortasks such as usage analysis that provide no immediate feedback to the user,registration is often looked upon as an invasion of privacy, similar to the use ofcookies and remote agents.3 The WEBMINER SystemThe WEBMINER system 4, 15 divides the Web Usage Mining process into threemain parts, as shown in Figs. 1 and 2. Input data consists of the three serverlogs  access, referrer, and agent, the HTML files that make up the site, and anyoptional data such as registration data or remote agent logs. The first part ofWeb Usage Mining, called preprocessing, includes the domain dependent tasksof data cleaning, user identification, session identification, and path completion.Data cleaning is the task of removing log entries that are not needed for themining process. User identification is the process of associating page references,even those with the same IP address, with different users. The site topology isrequired in addition to the server logs in order to perform user identification.Session identification takes all of the page references for a given user in a log andbreaks them up into user sessions. As with user identification, the site topologyis needed in addition to the server logs for this task. Path completion fills inpage references that are missing due to browser and proxy server caching. Thisstep differs from the others in that information is being added to the log. Theother preprocessing tasks remove data or divide the data up according to usersand sessions. Each of these tasks are performed in order to create a user sessionfile which will be used as input to the knowledge discovery phase.As shown in Fig. 2, mining for association rules requires the added step oftransaction identification, in addition to the other preprocessing tasks. Transaction identification is the task of identifying semantically meaningful groupingsof page references. In a domain such as market basket analysis, a transactionhas a natural definition  all of the items purchased by a customer at one time.However, the only natural transaction definition in the Web domain is a userPreprocessing Pattern AnalysisMining AlgorithmsSite FilesInterestingRules, Patterns,and StatisticsRules, Patterns,and StatisticsUser SessionFileRaw LogsFig. 1. High Level Usage Mining Processsession, which is often too coarse grained for mining tasks such as the discoveryof association rules. Therefore, specialized algorithms are needed to refine singleuser sessions into smaller transactions.The knowledge discovery phase uses existing data mining techniques to generate rules and patterns. Included in this phase is the generation of general usagestatistics, such as number of hits per page, page most frequently accessed, mostcommon starting page, and average time spent on each page. Association ruleand sequential pattern generation are the only data mining algorithms currentlyimplemented in the WEBMINER system, but the open architecture can easilyaccommodate any data mining or path analysis algorithm. The discovered information is then fed into various pattern analysis tools. The site filter is used toidentify interesting rules and patterns by comparing the discovered knowledgewith the Web site designers view of how the site should be used, as discussedin the next section. As shown in Fig. 2, the site filter can be applied to the datamining algorithms in order to reduce the computation time, or the discoveredrules and patterns.4 Browsing Behavior ModelsIn some respects, Web Usage Mining is the process of reconciling the Web sitedevelopers view of how the site should be used with the way users are actuallybrowsing through the site. Therefore, the two inputs that are required for theAccess Log Referrer Log Agent LogData CleaningPath CompletionSession IdentificationUser IdentificationSite TopologyUser Session FileSQLQueryTransaction FileTransactionIdentificationSequential Patterns Page Clusters Association Rules Usage StatisticsClusteringSequentialPatternMiningStandardStatisticsPackageAssociationRule MiningSite FilterPage ClassificationSite FilesClassificationAlgorithmSite CrawlerInteresting Rules, Patterns,and StatisticsRegistration orRemote AgentDataINPUTPREPROCESSINGKNOWLEDGEDISCOVERYPATTERN ANALYSIS KnowledgeQueryMechanismOLAPVisualizationFig. 2. Architecture for WEBMINERTable 1. Common Characteristics of Web PagesPage Type Physical Characteristics Usage CharacteristicsHead  Inlinks from most site pages  First page in user sessions Root of site file structureContent  Large textgraphic to link ratio  Long average reference lengthNavigation  Small textgraphic to link ratio  Short average reference length Not a maximal forward referenceLookup  Large number of inlinks  Short average reference length Few or no outlinks  Maximal forward reference Very little contentPersonal  No common characteristics  Low usageWeb Usage Mining process are an encoding of the site developers view of browsing behavior and an encoding of the actual browsing behaviors. These inputs arederived from the site files and the server logs respectively.4.1 Developers ModelThe Web site developers view of how the site should be used is inherent inthe structure of the site. Each link between pages exists because the developerbelieves that the pages are related in some way. Also, the content of the pagesthemselves provide information about how the developer expects the site to beused. Hence, an integral step of the preprocessing phase is the classifying of thesite pages and extracting the site topology from the HTML files that make upthe web site. The topology of a Web site can be easily obtained by means of asite crawler, that parses the HTML files to create a list of all of the hypertextlinks on a given page, and then follows each link until all of the site pages aremapped. The pages are classified using an adaptation of the classification schemepresented in 23. The WEBMINER system recognizes five main types of pages Head Page  a page whose purpose is to be the first page that users visit,i.e. home pages. Content Page  a page that contains a portion of the information contentthat the Web site is providing. Navigation Page  a page whose purpose is to provide links to guide userson to content pages. Lookup Page  a page used to provide a definition or acronym expansion. Personal Page  a page used to present information of a biographical orpersonal nature for individuals associated with the organization running theWeb site.Each of these types of pages is expected to exhibit certain physical characteristics. For example, a head page is expected to have inlinks from most of theother pages in the site, and is often at the root of the site file structure. Table 1lists some common physical characteristics for each page type. Note that theseare only rulesofthumb, and that there will be pages of a certain type that donot match the common physical characteristics. Personal pages are not expectedto exhibit any common characteristics. Each personal page is expected to be acombination of one of the other page types. For example, personal pages oftenhave content in the form of biographical information followed by a list of favorite links. The important distinction for personal pages is that they are notcontrolled by the site designer, and are thus not expected to contribute heavilyto discovered rules. This is because usage is expected to be low for any givenpersonal page compared to the overall site traffic. Thresholds such as supportwould be expected to filter out rules that contain personal pages. There areseveral valid combinations of page types that can be applied to a single page,such as a headnavigation page or a contentnavigation page. The page classifications should represent the Web site designers view of how each page willbe used. The classifications can be assigned manually by the site designer, orautomatically using supervised learning techniques. In order to automate theclassification of site pages, the common physical characteristics can be learnedby a classification algorithm such as C4.5 25 using a training set of pages. Another possibility is that a classification tag can be added to each page by thesite designer, using a data structure markup language such as XML ExtensibleMarkup Language 2.4.2 Users ModelAnalogous to each of the common physical characteristics for the different pagetypes, there is expected to be common usage characteristics among differentusers. These are also shown in Table 1. The reference length of a page is theamount of time a user spends viewing that particular page for a specific log entry.Some of the challenges involved in calculating reference lengths and maximalforward references will be discussed in Sect. 6.In order to group individual Web page references into meaningful transactionsfor the discovery of patterns such as association rules, an underlying model of theusers browsing behavior is needed. For the purposes of association rule discovery,it is really the content page references that are of interest. The other page typesare just to facilitate the browsing of a user while searching for information, andwill be referred to as auxiliary pages. What is merely an auxiliary page for oneuser may be a content page for another. Transaction identification assumes thatuser sessions have already been identified see Sect. 5.2.Using the concept of auxiliary and content page references, there are twoways to define transactions, as shown in Fig. 3. The first would be to define atransaction as all of the auxiliary references up to and including each contentreference for a given user. Mining these auxiliarycontent transactions wouldessentially give the common traversal paths through the web site to a givencontent page. The second method would be to define a transaction as all of thecontent references for a given user. Mining these contentonly transactions wouldgive associations between the content pages of a site, without any informationas to the path taken between the pages. It is important to note that resultsgenerated from contentonly transactions only apply when those pages are usedas content references. For example, an association rule, A  B, is normally takento mean that when A is in a set, so is B. However, if this rule has been generatedwith contentonly transactions, it has a more specific meaning, namely that Aimplies B only when both A and B are used as content references. This propertyallows data mining on contentonly transactions to produce rules that might bemissed by including all of the page references in a log. If users that treat page Aas an auxiliary page do not generally go on to page B, inclusion of the auxiliaryreferences into the data mining process would reduce the confidence of the rule A B, possibly to the point where it is not reported. Depending on the goals ofthe analysis, this can be seen as an advantage or a disadvantage. The key is thatauxiliarycontent transactions can be used when this property is undesirable. Thechallenge of identifying transactions is to dynamically determine which referencesin a server log are auxiliary and which are content. Three different approachesare presented and compared in Sects. 6 and 8.A AA A AA AA CC CCTransaction 1A AA A AA AA CC CCTransaction 1 Transaction 2 Transaction 3 Transaction 4AuxiliaryContent TransactionsContentonly TransactionFig. 3. Transaction Types Auxiliary and Content page references are labeled alongthe time axes with an A or C respectively4.3 Site FilterAs shown in Fig. 2, the site topology is used during the user identification andpath completion preprocessing tasks. Both site topology and page classificationswill be needed for applying a site filter to the mining algorithms, or the generatedrules and patterns during the pattern analysis phase. The site filter method usedin the WEBMINER system compares the web site designers view of how thesite should be used with the way users are actually browsing through the site,and reports any discrepancies. For example, the site filter simply checks headpages to see if a significant number of users are starting on that page, or if thereare other pages in the site which are a more common starting point for browsing.The start statistics for the head page are only flagged as interesting if it is notbeing treated as such by the users. Conversely, any page that is being used as ahead page, but is not classified as one is also reported.The site filter also uses the site topology to filter out rules and patternsthat are uninteresting. Any rule that confirms direct hypertext links betweenpages is filtered out. The sensitivity of the task can be adjusted by increasingthe length of the paths that are considered uninteresting. In other words, rulesconfirming pages that are up to n links away can be pruned, instead of just thosethat are directly linked. In addition, rules that are expected but not presentcan also be identified. If two pages are directly linked and not confirmed by arule or pattern, this information is as important as the existence of rules thatare not supported by the site structure. By using the site filter, the confidenceand support thresholds of the standard data mining algorithms can be loweredsignificantly to generate more potential rules and patterns, which are then culledto provide the data analyst with a reasonable number of results to examine.5 PreprocessingFigure 4 shows the preprocessing tasks of Web Usage Mining in greater detailthan Figs. 1 and 2. The inputs to the preprocessing phase are the server logs,site files, and optionally usage statistics from a previous analysis. The outputsare the user session file, transaction file, site topology, and page classifications.As mentioned in Sect. 2, one of the major impediments to creating a reliableuser session file is browser and proxy server caching. Current methods to collectinformation about cached references include the use of cookies and cache busting.Cache busting is the practice of preventing browsers from using stored localversions of a page, forcing a new download of a page from the server everytime it is viewed. As detailed in 21, none of these methods are without seriousdrawbacks. Cookies can be deleted by the user and cache busting defeats thespeed advantage that caching was created to provide, and is likely to be disabledby the user. Another method to identify users is user registration, as discussedin section 2. Registration has the advantage of being able to collect additionaldemographic information beyond what is automatically collected in the serverlog, as well as simplifying the identification of user sessions. However, againdue to privacy concerns, many users choose not to browse sites that requireregistration and logins, or provide false information. The preprocessing methodsused in the WEBMINER system are all designed to function with only theinformation supplied by the Common Log Format specified as part of the HTTPprotocol by CERN and NCSA 14.Transaction FileAgent andReferrer LogsData CleaningSQL QueryTransactionIdentificationUserIdentificationSessionIdentificationPathCompletionAccess LogSite TopologyUser Session FileUsage StatisticsSite FilesClassificationAlgorithmSite CrawlerPage ClassificationFig. 4. Details of Web Usage Mining Preprocessing5.1 Data CleaningTechniques to clean a server log to eliminate irrelevant items are of importancefor any type of Web log analysis, not just data mining. The discovered associations or reported statistics are only useful if the data represented in the serverlog gives an accurate picture of the user accesses to the Web site. The HTTPprotocol requires a separate connection for every file that is requested from theWeb server. Therefore, a users request to view a particular page often resultsin several log entries since graphics and scripts are downloaded in addition tothe HTML file. In most cases, only the log entry of the HTML file request isrelevant and should be kept for the user session file. This is because, in general, auser does not explicitly request all of the graphics that are on a Web page, theyare automatically downloaded due to the HTML tags. Since the main intent ofWeb Usage Mining is to get a picture of the users behavior, it does not makesense to include file requests that the user did not explicitly request. Eliminationof the items deemed irrelevant can be reasonably accomplished by checking thesuffix of the URL name. For instance, all log entries with filename suffixes suchas, gif, jpeg, GIF, JPEG, jpg, JPG, and map can be removed. In addition, common scripts such as count.cgi can also be removed. The WEBMINER systemuses a default list of suffixes to remove files. However, the list can be modifieddepending on the type of site being analyzed. For instance, for a Web site thatcontains a graphical archive, an analyst would probably not want to automatically remove all of the GIF or JPEG files from the server log. In this case, logentries of graphics files may very well represent explicit user actions, and shouldbe retained for analysis. A list of actual file names to remove or retain can beused instead of just file suffixes in order to distinguish between relevant andirrelevant log entries.5.2 User IdentificationNext, unique users must be identified. As mentioned previously, this task isgreatly complicated by the existence of local caches, corporate firewalls, andproxy servers. The Web Usage Mining methods that rely on user cooperation arethe easiest ways to deal with this problem. However, even for the logsite basedmethods, there are heuristics that can be used to help identify unique users. Aspresented in 23, even if the IP address is the same, if the agent log shows achange in browser software or operating system, a reasonable assumption to makeis that each different agent type for an IP address represents a different user. Forexample, consider the Web site shown in figure 5 and the sample informationcollected from the access, agent, and referrer logs shown in figure 6. All of thelog entries have the same IP address and the user ID is not recorded. However,the fifth, sixth, eighth, and tenth entries were accessed using a different agentthan the others, suggesting that the log represents at least two user sessions.The next heuristic for user identification is to use the access log in conjunctionwith the referrer log and site topology to construct browsing paths for each user.If a page is requested that is not directly reachable by a hyperlink from any ofthe pages visited by the user, again, the heuristic assumes that there is anotheruser with the same IP address. Looking at the figure 6 sample log again, thethird entry, page L, is not directly reachable from pages A or B. Also, the seventhentry, page R is reachable from page L, but not from any of the other previous logentries. This would suggest that there is a third user with the same IP address.Therefore, after the user identification step with the sample log, three uniqueusers are identified with browsing paths of ABFOGAD, ABCJ, and LR,respectively. It is important to note that these are only heuristics for identifyingusers. Two users with the same IP address that use the same browser on thesame type of machine can easily be confused as a single user if they are lookingat the same set of pages. Conversely, a single user with two different browsersrunning, or who types in URLs directly without using a sites link structure canbe mistaken for multiple users.5.3 Session IdentificationFor logs that span long periods of time, it is very likely that users will visitthe Web site more than once. The goal of session identification is to dividethe page accesses of each user into individual sessions. The simplest methodof achieving this is through a timeout, where if the time between page requestsexceeds a certain limit, it is assumed that the user is starting a new session. Manycommercial products use 30 minutes as a default timeout, and 5 establisheda timeout of 25.5 minutes based on empirical data. Once a site log has beenAQF NLOIDCBRTKSMJPHGE Content Page Auxiliary Page Multiple Purpose PageXXXFig. 5. Sample Web Site  Arrows between the pages represent hypertext links         IP Address    Userid       Time Method URL Protocol  Status Size Referred               Agent 1      123.456.78.9   25Apr1998030441 0500 GET A.html HTTP1.0    200 3290          Mozilla3.04 Win95, I 2      123.456.78.9   25Apr1998030534 0500 GET B.html HTTP1.0    200 2050  A.html     Mozilla3.04 Win95, I 3      123.456.78.9   25Apr1998030539 0500 GET L.html HTTP1.0    200 4130          Mozilla3.04 Win95, I 4      123.456.78.9   25Apr1998030602 0500 GET F.html HTTP1.0    200 5096  B.html     Mozilla3.04 Win95, I 5      123.456.78.9   25Apr1998030658 0500 GET A.html HTTP1.0    200 3290          Mozilla3.01 X11, I, IRIX6.2, IP22 6      123.456.78.9   25Apr1998030742 0500 GET B.html HTTP1.0    200 2050  A.html     Mozilla3.01 X11, I, IRIX6.2, IP22 7      123.456.78.9   25Apr1998030755 0500 GET R.html HTTP1.0    200 8140  L.html     Mozilla3.04 Win95, I 8      123.456.78.9   25Apr1998030950 0500 GET C.html HTTP1.0    200 1820  A.html     Mozilla3.01 X11, I, IRIX6.2, IP22 9      123.456.78.9   25Apr1998031002 0500 GET O.html HTTP1.0    200 2270  F.html     Mozilla3.04 Win95, I 10    123.456.78.9   25Apr1998031045 0500 GET J.html HTTP1.0     200 9430  C.html     Mozilla3.01 X11, I, IRIX6.2, IP22 11   123.456.78.9   25Apr1998031223 0500 GET G.html HTTP1.0    200 7220  B.html     Mozilla3.04 Win95, I 12   123.456.78.9   25Apr1998050522 0500 GET A.html HTTP1.0    200 3290          Mozilla3.04 Win95, I 13   123.456.78.9   25Apr1998050603 0500 GET D.html HTTP1.0    200 1680  A.html     Mozilla3.04 Win95, IFig. 6. Sample Information from Access, Referrer, and Agent Logs The first columnis for referencing purposes and would not be part of an actual log.analyzed and usage statistics obtained, a timeout that is appropriate for thespecific Web site can be fed back into the the session identification algorithm.This is the reason usage statistics are shown as an input to session identificationin Fig. 4. Using a 30 minute timeout, the path for user 1 from the sample logis broken into two separate sessions since the last two references are over anhour later than the first five. The session identification step results in four usersessions consisting of ABFOG, AD, ABCJ, and LR.5.4 Path CompletionAnother problem in reliably identifying unique user sessions is determining ifthere are important accesses that are not recorded in the access log. This problem is referred to as path completion. Methods similar to those used for useridentification can be used for path completion. If a page request is made thatis not directly linked to the last page a user requested, the referrer log can bechecked to see what page the request came from. If the page is in the users recent request history, the assumption is that the user backtracked with the backbutton available on most browsers, calling up cached versions of the pages untila new page was requested. If the referrer log is not clear, the site topology canbe used to the same effect. If more than one page in the users history contains alink to the requested page, it is assumed that the page closest to the previouslyrequested page is the source of the new request. Missing page references that areinferred through this method are added to the user session file. An algorithmis then required to estimate the time of each added page reference. A simplemethod of picking a timestamp is to assume that any visit to a page alreadyseen will be effectively treated as an auxiliary page. The average reference lengthfor auxiliary pages for the site can be used to estimate the access time for themissing pages. Looking at Figs. 5 and 6 again, page G is not directly accessiblefrom page O. The referrer log for the page G request lists page B as the requesting page. This suggests that user 1 backtracked to page B using the back buttonbefore requesting page G. Therefore, pages F and B should be added into thesession file for user 1. Again, while it is possible that the user knew the URLfor page G and typed it in directly, this is unlikely, and should not occur oftenenough to affect the mining algorithms. The path completion step results in userpaths of ABFOFBG, AD, ABACJ, and LR. The results of each of thepreprocessing steps are summarized in Table 2.5.5 FormattingOnce the appropriate preprocessing steps have been applied to the server log, afinal preparation module can be used to properly format the sessions or transactions for the type of data mining to be accomplished. For example, since temporalinformation is not needed for the mining of association rules, a final associationrule preparation module would strip out the time for each reference, and do anyother formatting of the data necessary for the specific data mining algorithm tobe used.Table 2. Summary of Sample Log Preprocessing ResultsTask ResultClean Log  ABLFABRCOJGADUser Identification  ABFOGAD ABCJ LRSession Identification  ABFOG AD ABCJ LRPath Completion  ABFOFBG AD ABACJ LR6 Transaction Identification6.1 General ModelEach user session in a user session file can be thought of in two ways eitheras a single transaction of many page references, or a set of many transactionseach consisting of a single page reference. The goal of transaction identificationis to create meaningful clusters of references for each user. Therefore, the task ofidentifying transactions is one of either dividing a large transaction into multiplesmaller ones or merging small transactions into fewer larger ones. This processcan be extended into multiple steps of merge or divide in order to create transactions appropriate for a given data mining task. A transaction identificationapproach can be defined as either a merge or a divide approach. Both types ofapproaches take a transaction list and possibly some parameters as input, andoutput a transaction list that has been operated on by the function in the approach in the same format as the input. The requirement that the input andoutput transaction format match allows any number of approaches to be combined in any order, as the data analyst sees fit. Let L be a set of user session fileentries. A session entry l  L includes the client IP address l.ip, the client userid l.uid, the URL of the accessed page l.url, and the time of access l.time. If theactual user id is not available, which is often the case, an arbitrarily assignedidentifier from the user session file is used. There are other fields in user sessionfile entries, such as the request method used e.g., POST or GET and the size ofthe file transmitted, however these fields are not used in the transaction model.A General Transaction t is a triple, as shown in 1.t  ipt, uidt, lt1.url, lt1.time, . . . , ltm.url, ltm.time where, for 1  k  m, ltk  L, ltk.ip  ipt, ltk.uid  uidt 1Table 3. Summary of Sample Transaction Identification Results Transaction type isnot defined for the Time Window ApproachTransactionsApproach Contentonly AuxiliaryContentReference Length FG, D, LR, J ABF, OFBG, ADL, R, ABACJMaximal Forward OG, R, BJ, D ABFO, ABG, LRReference AB, ACJ, ADTime Window ABF, OFBG, AD, LR, ABACJSince the initial input to the transaction identification process consists of allof the page references for a given user session, the first step in the transactionidentification process will always be the application of a divide approach. Thenext sections describe three divide transaction identification approaches. Thefirst two, reference length and maximal forward reference, make an attempt toidentify semantically meaningful transactions. The third, time window, is notbased on any browsing model, and is mainly used as a benchmark to comparewith the other two algorithms. The results of using the three different approacheson the sample Fig. 6 data are shown in Table 3.6.2 Transaction Identification by Reference LengthThe reference length transaction identification approach is based on the assumption that the amount of time a user spends on a page correlates to whether thepage should be classified as a auxiliary or content page for that user. Figure 7shows a histogram of the lengths of page references between 0 and 600 seconds fora server log from the Global Reach Internet Productions GRIP Web site 24.Qualitative analysis of several other server logs reveals that like Fig. 7, theshape of the histogram has a large exponential component. It is expected thatthe variance of the times spent on the auxiliary pages is small, and the auxiliaryreferences make up the lower end of the curve. The length of content references isexpected to have a wide variance and would make up the upper tail that extendsout to the longest reference. If an assumption is made about the percentage ofauxiliary references in a log, a reference length can be calculated that estimatesthe cutoff between auxiliary and content references. Specifically, given a percentof auxiliary references, the reference length method uses a maximum likelihoodestimate to calculate the time length t as shown in 2.t  ln1where    of auxiliary references,  reciprocal of observed mean reference length. 2The definition of 2 comes from integrating the formula for an exponential distribution, from  to zero. The maximum likelihood estimate for the exponential0 100 200 300 400 500 6000200400600800100012001400160018002000Reference Length secondsReference CountFig. 7. Histogram of Web Page Reference Lengths secondsdistribution is the observed mean. The time length t could be calculated exactlyby sorting all of the reference lengths from a log and then selecting the referencelength that is located in the log size position. However, this increases thecomplexity of the algorithm from linear to Onlogn while not necessarily increasing the accuracy of the calculation since the value of  is only an estimate.Although the exponential distribution does not fit the histograms of server logdata exactly, it provides a reasonable estimate of the cutoff reference length. Itwould be interesting to examine the costbenefit tradeoffs for distributions thatfit the histograms more accurately.The definition of a transaction within the reference length approach is aquadruple, and is given in 3. It has the same structure as 1 with the referencelength added for each page.trl  iptrl , uidtrl , ltrl1 .url, ltrl1 .time, ltrl1 .length,. . . , ltrlm .url, ltrlm .time, ltrlm .length where, for 1  k  m, ltrlk  L, ltrlk .ip  iptrl , ltrlk .uid  uidtrl 3The length of each reference is estimated by taking the difference between thetime of the next reference and the current reference. Obviously, the last referencein each transaction has no next time to use in estimating the reference length.The reference length approach makes the assumption that all of the last references are content references, and ignores them while calculating the cutoff time.This assumption can introduce errors if a specific auxiliary page is commonlyused as the exit point for a Web site. While interruptions such as a phone call orlunch break can result in the erroneous classification of a auxiliary reference asa content reference, it is unlikely that the error will occur on a regular basis forthe same page. A reasonable minimum support threshold during the applicationof a data mining algorithm would be expected to weed out these errors.Once the cutoff time is calculated, the two types of transactions discussedin Sect. 4 can be formed by comparing each reference length against the cutofftime. Depending on the goal of the analysis, the auxiliarycontent transactionsor the contentonly transactions can be identified. If C is the cutoff time, forauxiliarycontent transactions the conditions,for 1  k  m  1  ltrlk .length  Cand k  m  ltrlk .length  Care added to 3, and for contentonly transactions, the condition,for 1  k  m  ltrlk .length  Cis added to 3. Using the example presented in Sect. 5 with the assumption thatthe multiple purpose pages are used as content pages half of the time they areaccessed, a cutoff time of 78.4 seconds is calculated Of course, with such a smallexample, the calculation is statistically meaningless. This results in contentonlytransactions of FG, D, LR, and J, as shown in Table 3. Notice that page L isclassified as a content page instead of an auxiliary page. This could be due toany of the reasons discussed above.The one parameter that the reference length approach requires is an estimation of the overall percentage of references that are auxiliary. The estimation ofthe percentage of auxiliary references can be based on the structure and contentof the site or experience of the data analyst with other server logs. The resultspresented in Sect. 8 show that the approach is fairly robust and a wide range ofauxiliary percentages will yield reasonable sets of association rules.6.3 Transaction Identification by Maximal Forward ReferenceThe maximal forward reference transaction identification approach is based onthe work presented in 6. Instead of time spent on a page, each transaction isdefined to be the set of pages in the path from the first page in a user sessionup to the page before a backward reference is made. A forward reference isdefined to be a page not already in the set of pages for the current transaction.Similarly, a backward reference is defined to be a page that is already containedin the set of pages for the current transaction. A new transaction is started whenthe next forward reference is made. The underlying model for this approach isthat the maximal forward reference pages are the content pages, and the pagesleading up to each maximal forward reference are the auxiliary pages. Like thereference length approach, two sets of transactions, namely auxiliarycontent orcontentonly, can be formed. The definition of a general transaction shown in1 is used within the maximal forward reference approach. Again, using theSect. 5 example, auxiliarycontent transactions of ABFO, ABG, LR, AB,ACJ, and AD would be formed. The contentonly transactions would be OG,R, BJ, and D. The maximal forward reference approach has an advantage overthe reference length in that it does not require an input parameter that is basedon an assumption about the characteristics of a particular set of data.6.4 Transaction Identification by Time WindowThe time window transaction identification approach partitions a user sessioninto time intervals no larger than a specified parameter. The approach doesnot try to identify transactions based on the model of Sect. 4, but instead assumes that meaningful transactions have an overall average length associatedwith them. For a sufficiently large specified time window, each transaction willcontain an entire user session. Since the time window approach is not based onthe model presented in Sect. 4, it is not possible to create two separate sets oftransactions. The last reference of each transaction does not correspond to acontent reference, the way it does for the auxiliarycontent transactions of thereference length and maximal forward reference approaches. If W is the lengthof the time window, definition 1 applies for transactions identified with the timewindow approach with the following added conditionltm.time lt1.time  WSince there is some standard deviation associated with the length of each realtransaction, it is unlikely that a fixed time window will break a log up appropriately. However, the time window approach can also be used as a merge approachin conjunction with one of the other divide approaches. For example, after applying the reference length approach, a merge time window approach with a 10minute input parameter could be used to ensure that each transaction has someminimum overall length.7 Creation of Test Server Log DataIn order to compare the performance of the transaction identification approachesfor the mining of association rules presented in Sect. 6, a server log with knownrules is needed. As can be seen in Table 3, the three different approaches resultin different sets of transactions, even for a simple example. Mining of associationrules from actual Web server logs also tends to result in different lists of rules foreach approach, and even for the same approach with different input parameters.There is no quantitative method for determining which of the results are betterthan the others. It was decided to create server logs with generated data for thepurpose of comparing the three approaches. The user browsing behavior modelpresented in Sect. 4 is used to create the data. The data generator takes a filewith a description of a Web site as a directed tree or graph and some embeddedassociation rules. The embedded association rules become the interesting rulesthat are checked for during experiments with different transaction identificationapproaches. The interesting rules in this case are associations that are not bornout by the site structure, which will be identified by the site filter discussed inSect. 4. For each user, the next log entry is one of three choices, a forward reference, backward reference, or exit. The probability of each choice is taken fromthe input file, and a random number generator is used to make the decision. Ifthe page reference is going to be a content reference, the time is calculated usinga normal distribution and a mean value for the time spent on a page taken fromthe input file. The times for auxiliary hits are calculated using a gamma distribution. The reason for using a gamma distribution for the auxiliary referencesand a normal distribution with different averages for the content references isto create an overall distribution that is similar to those seen in real server logs.A pure exponential distribution does not model the initial steep slope of thehistogram particularly well. Figure 8 shows a histogram of the reference lengthsfor a generated data set, which is very similar to the histogram of the real serverlog data shown in Fig. 7.0 100 200 300 400 500 600050100150200250300350400450Fig. 8. Histogram of Generated Data Reference Lengths secondsBesides prior knowledge of the interesting association rules, the other advantage of using the generated data for testing the transaction identificationapproaches is that the actual percentage of auxiliary references is also known.The obvious disadvantage is that it is, after all, only manufactured data andshould not be used as the only tool to evaluate the transaction identificationTable 4. Number of Interesting Rules Discovered number discoveredtotal possibleApproach Parameter Sparse Medium DenseTime 10 min. 04 03 03Window 20 min. 24 23 1330 min. 24 23 23Reference 50 44 33 33Length 65 44 33 3380 44 33 33M. F. R. 44 23 13approaches. Since the data is created from the user behavior model of Sect. 4,it is expected that the transaction identification approach based on the samemodel will perform best.Three different types of web sites were modeled for evaluation, a sparselyconnected graph, a densely connected graph, and a graph with a medium amountof connectivity. The sparse graph, with an average incoming order of one for thenodes, is the site used for the examples in Sects. 5 and 6, and is shown in Fig. 5.The medium and dense graphs use the same nodes as the sparse graph, but havemore edges added for an average node degree of four and eight, respectively.8 Experimental Evaluation8.1 Comparison using Synthetic DataTable 4 shows the results of using the three transactions identification approachesdiscussed in section 3 to mine for association rules from data created from thethree different web site models discussed in Sect. 7. The reference length approach performed the best, even though it uses a pure exponential distributionto estimate the cutoff time, while the synthetic data was created with a combination of normal and gamma distributions. The maximal forward referenceapproach performs well for sparse data, but as the connectivity of the graph increases, its performance degrades. This is because as more forward paths becomeavailable, a content reference is less likely to be the maximal forward reference.For dense graphs, auxiliarycontent transactions would probably give better results with the maximal forward reference approach. The performance of the timewindow approach is relatively poor, but as the time window increases, so doesthe performance.Table 5 shows the average ratio of reported confidence to actual confidencefor the interesting rules discovered. The differences between the reference lengthand maximal forward reference approaches stand out in Table 5. The reportedconfidence of rules discovered by the reference length approach are consistentlyclose to the actual values. Note that even though the created data has an actualauxiliary page ratio of 70, inputs of 50 and 80 produce reasonable results.Table 5. Ratio of Reported Confidence to Actual ConfidenceApproach Parameter Sparse Medium DenseTime 10 min. null null nullWindow 20 min. 0.82 0.87 0.8730 min. 0.98 0.90 0.88Reference 50 0.99 0.95 0.96Length 65 1.0 0.99 0.9680 0.97 0.99 0.96M. F. R. 0.79 0.47 0.44Table 6. Transaction Identification Run Time sec  Total Run Time secApproach Parameter Sparse Medium DenseTime 10 min. 0.814.38 0.824.65 0.753.94Window 20 min. 0.847.06 0.807.06 0.734.4230 min. 0.797.16 0.779.95 0.725.17Ref. 50 1.824.62 1.664.46 1.474.09Length 65 1.684.29 1.724.35 1.454.0280 1.624.14 1.664.26 1.484.03M. F. R. 1.263.98 1.303.95 1.203.87The reported confidence for the rules discovered by the maximal forward reference approach is significantly lower than the actual confidence, and similar tothe results of Table 4, it degrades as the connectivity of the graph increases.Table 6 shows the running time of each transaction identification approach,and the total run time of the data mining process. The total run times do notinclude data cleaning since the data was generated in a clean format. Althoughthe data cleaning step for real data can comprise a significant portion of thetotal run time, it generally only needs to be performed once for a given setof data. The time window approach shows the fastest run time, but a muchslower overall data mining process due to the number of rules discovered. Thereference length approach has the slowest approach times due to an extra set offile readwrites in order to calculate the cutoff time. All three of the approachesare On algorithms and are therefore linearly scalable.8.2 Association Rules from Real DataTransactions identified with the reference length and maximal forward referenceapproaches were used to mine for association rules from a server log with datafrom the Global Reach Web site. The server log used contained 20.3 Mb ofraw data, which when cleaned corresponded to about 51.7K references. Becausethe Global Reach Web server hosts Web sites for other companies, the serverlog is really a collection of smaller server logs and the overall support for mostdiscovered association rules is low. Accordingly, the association rule generationTable 7. Examples of Association Rules from www.globalreach.comApproach Used Conf. Supp. Association RulesReference Length 61.54 0.18 mticlinres.htm mtinew.htmcontentonly  mtiprodinfo.htmReference Length 100.00 0.15 mtiQA.htm mtiprodinfo.htmcontentonly mtipubs.htm  mticlinres.htmReference Length 26.09 0.14 cyprusonlinedailynews.htmcontentonly  mtiQA.htmMaximal Forward 52.17 0.14 cyprusonlineMagazines.htmReference cyprusonlineRadio.htm contentonly cyprusonlineNews.htmMaximal Forward 73.50 1.32 mticlinres.htm mtinew.htmReference  mtiprodinfo.htmauxcontentalgorithm was run with thresholds of 0.1 support and 20 confidence. This ledto a fairly large number of computed rules 1150 for the reference length approachand 398 for the maximal forward reference approach. The site filter was notapplied to the discovered rules. Table 7 shows some examples of associationrules discovered.The first two rules shown in Table 7 are straight forward association rulesthat could have been predicted by looking at the structure of the web site.However, the third rule shows an unexpected association between a page of thecyprusonline site and a page from the MTI site. Approximately one fourth of theusers visiting cyprusonlinedailynews.htm also chose to visit mtiQA.htmduring the same session. However, since the cyprusonline page no longer existson the Global Reach server, it is not clear if the association is the result of anadvertisement, a link to the MTI site, or some other factor. The fourth rulelisted in Table 7 is one of the 150 rules that the maximal forward referenceapproach discovered that was not discovered by the reference length approach.While the reference length approach discovered many rules involving the MTIweb site, the maximal forward reference approach discovered relatively few rulesinvolving the MTI site. An inspection of the MTI site revealed that the site isa fully connected graph. Consistent with the results of Sect. 8.1, the maximalforward reference approach does not perform well under these conditions. Theassociation rule algorithm was run with auxiliarycontent transactions createdfrom the maximal forward reference approach to confirm the theory that therules missed by the contentonly transactions would be discovered. The last rulelisted in Table 7 is the same as the first rule listed, and shows that the auxiliarycontent transactions from the maximal forward reference approach can discoverrules in a highly connected graph. However, at thresholds of 0.1 support and20 confidence, approximately 25,000 other rules were also discovered with theauxiliarycontent approach.9 ConclusionsThis paper has presented the details of preprocessing tasks that are necessary forperforming Web Usage Mining, the application of data mining and knowledgediscovery techniques to WWW server access logs. This paper also presentedexperimental results on synthetic data for the purpose of comparing transactionidentification approaches, and on realworld industrial data to illustrate some ofits applications Sect. 8. The transactions identified with the reference lengthapproach performed consistently well on both the real data and the created data.For the real data, only the reference length transactions discovered rules thatcould not be reasonably inferred from the structure of the Web sites. Since theimportant page in a traversal path is not always the last one, the contentonlytransactions identified with the maximal forward reference approach did not workwell with real data that had a high degree of connectivity. The auxiliarycontenttransactions led to an overwhelmingly large set of rules, which limits the valueof the data mining process. Future work will include further tests to verify theuser browsing behavior model discussed in Sect. 4 and a more rigorous analysisof the shape of reference length histograms in order to refine the reference lengthtransaction identification approach.References1. R. Agrawal and R. Srikant. Fast algorithms for mining association rules. In Proc.of the 20th VLDB Conference, pages 487499, Santiago, Chile, 1994.2. T. Bray, J. Paoli, and C. M. SperbergMcQueen. Extensible markup languageXML 1.0 W3C recommendation. Technical report, W3C, 1998.3. M. Balabanovic and Y. Shoham. Learning information retrieval agents Experimentswith automated Web browsing. In Online Working Notes of the AAAI SpringSymposium Series on Information Gathering from Distributed, Heterogeneous Environments, 1995.4. R. Cooley, B. Mobasher, and J. Srivastava. Web mining Information and patterndiscovery on the World Wide Web. In International Conference on Tools withArtificial Intelligence, pages 558567, Newport Beach, CA, 1997.5. L. Catledge and J. Pitkow. Characterizing browsing behaviors on the World WideWeb. Computer Networks and ISDN Systems, 276, 1995.6. M.S. Chen, J.S. Park, and P.S. Yu. Data mining for path traversal patterns in a Webenvironment. In Proceedings of the 16th International Conference on DistributedComputing Systems, pages 385392, 1996.7. S. EloDean and M. Viveros. Data mining the IBM official 1996 Olympics Web site.Technical report, IBM T.J. Watson Research Center, 1997.8. e.g. Software Inc. Webtrends. httpwww.webtrends.com, 1995.9. J. Gray, A. Bosworth, A. Layman, and H. Pirahesh. Data cube A relational aggregation operator generalizing groupby, crosstab, and subtotals. In IEEE 12thInternational Conference on Data Engineering, pages 152159, 1996.10. Open Market Inc. Open Market Web reporter. httpwww.openmarket.com, 1996.11. T. Joachims, D. Freitag, and T. Mitchell. Webwatcher A tour guide for the WorldWide Web. In Proc. of the 15th International Conference on Artificial Intelligence,pages 770  775, Nagoya, Japan, 1997.12. L. Kaufman and P.J. Rousseeuw. Finding Groups in Data an Introduction toCluster Analysis. John Wiley  Sons, 1990.13. H. Lieberman. Letizia An agent that assists Web browsing. In Proc. of the 1995International Joint Conference on Artificial Intelligence, Montreal, Canada, 1995.14. A. Luotonen. The common log file format. httpwww.w3.orgpubWWW, 1995.15. B. Mobasher, N. Jain, E. Han, and J. Srivastava. Web Mining Pattern discoveryfrom World Wide Web transactions. Technical Report TR 96050, University ofMinnesota, Dept. of Computer Science, Minneapolis, 1996.16. H. Mannila and H. Toivonen. Discovering generalized episodes using minimalocurrences. In Proc. of the Second Intl Conference on Knowledge Discovery andData Mining, pages 146151, Portland, Oregon, 1996.17. H. Mannila, H. Toivonen, and A. I. Verkamo. Discovering frequent episodes insequences. In Proc. of the First Intl Conference on Knowledge Discovery and DataMining, pages 210215, Montreal, Quebec, 1995.18. net.Genesis. net.analysis desktop. httpwww.netgen.com, 1996.19. R. Ng and J. Han. Efficient and effective clustering method for spatial data mining.In Proc. of the 20th VLDB Conference, pages 144155, Santiago, Chile, 1994.20. D.S.W. Ngu and X. Wu. Sitehelper A localized agent that helps incremental exploration of the World Wide Web. In 6th International World Wide Web Conference,pages 691700, Santa Clara, CA, 1997.21. J. Pitkow. In search of reliable usage data on the WWW. In Sixth InternationalWorld Wide Web Conference, pages 451463, Santa Clara, CA, 1997.22. M. Pazzani, L. Nguyen, and S. Mantik. Learning from hotlists and coldlists Towards a WWW information filtering and seeking agent. In IEEE 1995 InternationalConference on Tools with Artificial Intelligence, 1995.23. P. Pirolli, J. Pitkow, and R. Rao. Silk from a sows ear Extracting usable structures from the Web. In Proc. of 1996 Conference on Human Factors in ComputingSystems CHI96, Vancouver, British Columbia, Canada, 1996.24. Global Reach Internet Productions. GRIP. httpwww.globalreach.com, 1997.25. J. Ross Quinlan. C4.5 Programs for Machine Learning. Morgan Kaufmann, SanMateo, CA, 1993.26. R. Srikant and R. Agrawal. Mining sequential patterns Generalizations and performance improvements. In Proc. of the Fifth Intl Conference on Extending DatabaseTechnology, Avignon, France, 1996.27. S. Schechter, M. Krishnan, and M. D. Smith. Using path profiles to predict HTTPrequests. In 7th International World Wide Web Conference, Brisbane, Australia,1998.28. C. Shahabi, A. Zarkesh, J. Adibi, and V. Shah. Knowledge discovery from usersWebpage navigation. In Workshop on Research Issues in Data Engineering, Birmingham, England, 1997.29. T. Yan, M. Jacobsen, H. GarciaMolina, and U. Dayal. From user access patternsto dynamic hypertext linking. In Fifth International World Wide Web Conference,Paris, France, 1996.30. O. R. Zaiane, M. Xin, and J. Han. Discovering Web access patterns and trends byapplying OLAP and data mining technology on Web logs, 1998.
