Data mining and knowledge discovery indatabases have been attracting a significantamount of research, industry, and media attention of late. What is all the excitement aboutThis article provides an overview of this emergingfield, clarifying how data mining and knowledgediscovery in databases are related both to eachother and to related fields, such as machinelearning, statistics, and databases. The articlementions particular realworld applications,specific datamining techniques, challenges involved in realworld applications of knowledgediscovery, and current and future research directions in the field.Across a wide variety of fields, data arebeing collected and accumulated at adramatic pace. There is an urgent needfor a new generation of computational theories and tools to assist humans in extractinguseful information knowledge from therapidly growing volumes of digital data.These theories and tools are the subject of theemerging field of knowledge discovery indatabases KDD.At an abstract level, the KDD field is concerned with the development of methods andtechniques for making sense of data. The basicproblem addressed by the KDD process is oneof mapping lowlevel data which are typicallytoo voluminous to understand and digest easily into other forms that might be more compact for example, a short report, more abstract for example, a descriptiveapproximation or model of the process thatgenerated the data, or more useful for example, a predictive model for estimating the value of future cases. At the core of the process isthe application of specific datamining methods for pattern discovery and extraction.1This article begins by discussing the historical context of KDD and data mining and theirintersection with other related fields. A briefsummary of recent KDD realworld applications is provided. Definitions of KDD and data mining are provided, and the general multistep KDD process is outlined. This multistepprocess has the application of datamining algorithms as one particular step in the process.The datamining step is discussed in more detail in the context of specific datamining algorithms and their application. Realworldpractical application issues are also outlined.Finally, the article enumerates challenges forfuture research and development and in particular discusses potential opportunities for AItechnology in KDD systems.Why Do We Need KDDThe traditional method of turning data intoknowledge relies on manual analysis and interpretation. For example, in the healthcareindustry, it is common for specialists to periodically analyze current trends and changesin healthcare data, say, on a quarterly basis.The specialists then provide a report detailingthe analysis to the sponsoring healthcare organization this report becomes the basis forfuture decision making and planning forhealthcare management. In a totally different type of application, planetary geologistssift through remotely sensed images of planets and asteroids, carefully locating and cataloging such geologic objects of interest as impact craters. Be it science, marketing, finance,health care, retail, or any other field, the classical approach to data analysis relies fundamentally on one or more analysts becomingArticlesFALL 1996    37From Data Mining toKnowledge Discovery inDatabasesUsama Fayyad, Gregory PiatetskyShapiro, and Padhraic Smyth Copyright  1996, American Association for Artificial Intelligence. All rights reserved. 073846021996  2.00areas is astronomy. Here, a notable successwas achieved by SKICAT, a system used by astronomers to perform image analysis,classification, and cataloging of sky objectsfrom skysurvey images Fayyad, Djorgovski,and Weir 1996. In its first application, thesystem was used to process the 3 terabytes1012 bytes of image data resulting from theSecond Palomar Observatory Sky Survey,where it is estimated that on the order of 109sky objects are detectable. SKICAT can outperform humans and traditional computationaltechniques in classifying faint sky objects. SeeFayyad, Haussler, and Stolorz 1996 for a survey of scientific applications.In business, main KDD application areasincludes marketing, finance especially investment, fraud detection, manufacturing,telecommunications, and Internet agents.Marketing In marketing, the primary application is database marketing systems,which analyze customer databases to identifydifferent customer groups and forecast theirbehavior. Business Week Berry 1994 estimated that over half of all retailers are using orplanning to use database marketing, andthose who do use it have good results for example, American Express reports a 10 to 15percent increase in creditcard use. Anothernotable marketing application is marketbasket analysis Agrawal et al. 1996 systems,which find patterns such as, If customerbought X, heshe is also likely to buy Y andZ. Such patterns are valuable to retailers.Investment Numerous companies use data mining for investment, but most do notdescribe their systems. One exception is LBSCapital Management. Its system uses expertsystems, neural nets, and genetic algorithmsto manage portfolios totaling 600 millionsince its start in 1993, the system has outperformed the broad stock market Hall, Mani,and Barr 1996.Fraud detection HNC Falcon and NestorPRISM systems are used for monitoring creditcard fraud, watching over millions of accounts. The FAIS system Senator et al. 1995,from the U.S. Treasury Financial Crimes Enforcement Network, is used to identify financial transactions that might indicate moneylaundering activity.Manufacturing The CASSIOPEE troubleshooting system, developed as part of ajoint venture between General Electric andSNECMA, was applied by three major European airlines to diagnose and predict problems for the Boeing 737. To derive families offaults, clustering methods are used. CASSIOPEEreceived the European first prize for innovaintimately familiar with the data and servingas an interface between the data and the usersand products.For these and many other applications,this form of manual probing of a data set isslow, expensive, and highly subjective. Infact, as data volumes grow dramatically, thistype of manual data analysis is becomingcompletely impractical in many domains.Databases are increasing in size in two ways1 the number N of records or objects in thedatabase and 2 the number d of fields or attributes to an object. Databases containing onthe order of N  109 objects are becoming increasingly common, for example, in the astronomical sciences. Similarly, the number offields d can easily be on the order of 102 oreven 103, for example, in medical diagnosticapplications. Who could be expected to digest millions of records, each having tens orhundreds of fields We believe that this job iscertainly not one for humans hence, analysiswork needs to be automated, at least partially.The need to scale up human analysis capabilities to handling the large number of bytesthat we can collect is both economic and scientific. Businesses use data to gain competitive advantage, increase efficiency, and provide more valuable services to customers.Data we capture about our environment arethe basic evidence we use to build theoriesand models of the universe we live in. Because computers have enabled humans togather more data than we can digest, it is only natural to turn to computational techniques to help us unearth meaningful patterns and structures from the massivevolumes of data. Hence, KDD is an attempt toaddress a problem that the digital information era made a fact of life for all of us dataoverload. Data Mining and KnowledgeDiscovery in the Real WorldA large degree of the current interest in KDDis the result of the media interest surroundingsuccessful KDD applications, for example, thefocus articles within the last two years inBusiness Week, Newsweek, Byte, PC Week, andother largecirculation periodicals. Unfortunately, it is not always easy to separate factfrom media hype. Nonetheless, several welldocumented examples of successful systemscan rightly be referred to as KDD applicationsand have been deployed in operational useon largescale realworld problems in scienceand in business.In science, one of the primary applicationThere is anurgent needfor a new generation ofcomputational theoriesand tools toassist humans inextractinguseful informationknowledgefrom therapidly growing volumes of digital data. Articles38 AI MAGAZINEtive applications Manago and Auriol 1996.Telecommunications The telecommunications alarmsequence analyzer TASA wasbuilt in cooperation with a manufacturer oftelecommunications equipment and threetelephone networks Mannila, Toivonen, andVerkamo 1995. The system uses a novelframework for locating frequently occurringalarm episodes from the alarm stream andpresenting them as rules. Large sets of discovered rules can be explored with flexible informationretrieval tools supporting interactivityand iteration. In this way, TASA offers pruning,grouping, and ordering tools to refine the results of a basic bruteforce search for rules.Data cleaning The MERGEPURGE systemwas applied to the identification of duplicatewelfare claims Hernandez and Stolfo 1995.It was used successfully on data from the Welfare Department of the State of Washington.In other areas, a wellpublicized system isIBMs ADVANCED SCOUT, a specialized datamining system that helps National Basketball Association NBA coaches organize and interpret data from NBA games U.S. News 1995.ADVANCED SCOUT was used by several of theNBA teams in 1996, including the Seattle Supersonics, which reached the NBA finals.Finally, a novel and increasingly importanttype of discovery is one based on the use of intelligent agents to navigate through an informationrich environment. Although the ideaof active triggers has long been analyzed in thedatabase field, really successful applications ofthis idea appeared only with the advent of theInternet. These systems ask the user to specifya profile of interest and search for related information among a wide variety of publicdomain and proprietary sources. For example,FIREFLY is a personal musicrecommendationagent It asks a user hisher opinion of severalmusic pieces and then suggests other musicthat the user might like httpwww.ffly.com. CRAYON httpcrayon.netallows users to create their own free newspapersupported by ads NEWSHOUND httpwww.sjmercury.comhound from the San JoseMercury News and FARCAST httpwww.farcast.com automatically search informationfrom a wide variety of sources, includingnewspapers and wire services, and email relevant documents directly to the user.These are just a few of the numerous suchsystems that use KDD techniques to automatically produce useful information from largemasses of raw data. See PiatetskyShapiro etal. 1996 for an overview of issues in developing industrial KDD applications.Data Mining and KDDHistorically, the notion of finding useful patterns in data has been given a variety ofnames, including data mining, knowledge extraction, information discovery, informationharvesting, data archaeology, and data patternprocessing. The term data mining has mostlybeen used by statisticians, data analysts, andthe management information systems MIScommunities. It has also gained popularity inthe database field. The phrase knowledge discovery in databases was coined at the first KDDworkshop in 1989 PiatetskyShapiro 1991 toemphasize that knowledge is the end productof a datadriven discovery. It has been popularized in the AI and machinelearning fields.In our view, KDD refers to the overall process of discovering useful knowledge from data, and data mining refers to a particular stepin this process. Data mining is the applicationof specific algorithms for extracting patternsfrom data. The distinction between the KDDprocess and the datamining step within theprocess is a central point of this article. Theadditional steps in the KDD process, such asdata preparation, data selection, data cleaning,incorporation of appropriate prior knowledge,and proper interpretation of the results ofmining, are essential to ensure that usefulknowledge is derived from the data. Blind application of datamining methods rightly criticized as data dredging in the statistical literature can be a dangerous activity, easilyleading to the discovery of meaningless andinvalid patterns. The Interdisciplinary Nature of KDDKDD has evolved, and continues to evolve,from the intersection of research fields such asmachine learning, pattern recognition,databases, statistics, AI, knowledge acquisitionfor expert systems, data visualization, andhighperformance computing. The unifyinggoal is extracting highlevel knowledge fromlowlevel data in the context of large data sets.The datamining component of KDD currently relies heavily on known techniquesfrom machine learning, pattern recognition,and statistics to find patterns from data in thedatamining step of the KDD process. A natural question is, How is KDD different from pattern recognition or machine learning and related fields The answer is that these fieldsprovide some of the datamining methodsthat are used in the datamining step of theKDD process. KDD focuses on the overall process of knowledge discovery from data, including how the data are stored and accessed, howalgorithms can be scaled to massive data setsThe basicproblem addressed bythe KDD process is one of mapping lowlevel data  intoother formsthat might bemore compact,more abstract, or more useful. ArticlesFALL 1996   39A driving force behind KDD is the databasefield the second D in KDD. Indeed, theproblem of effective data manipulation whendata cannot fit in the main memory is of fundamental importance to KDD. Database techniques for gaining efficient data access,grouping and ordering operations when accessing data, and optimizing queries constitute the basics for scaling algorithms to largerdata sets. Most datamining algorithms fromstatistics, pattern recognition, and machinelearning assume data are in the main memory and pay no attention to how the algorithmbreaks down if only limited views of the dataare possible.A related field evolving from databases isdata warehousing, which refers to the popularbusiness trend of collecting and cleaningtransactional data to make them available foronline analysis and decision support. Datawarehousing helps set the stage for KDD intwo important ways 1 data cleaning and 2data access.Data cleaning As organizations are forcedto think about a unified logical view of thewide variety of data and databases they possess, they have to address the issues of mapping data to a single naming convention,uniformly representing and handling missingdata, and handling noise and errors whenpossible.Data access Uniform and welldefinedmethods must be created for accessing the data and providing access paths to data thatwere historically difficult to get to for example, stored offline.Once organizations and individuals havesolved the problem of how to store and access their data, the natural next step is thequestion, What else do we do with all the data This is where opportunities for KDD naturally arise.A popular approach for analysis of datawarehouses is called online analytical processingOLAP, named for a set of principles proposed by Codd 1993. OLAP tools focus onproviding multidimensional data analysis,which is superior to SQL in computing summaries and breakdowns along many dimensions. OLAP tools are targeted toward simplifying and supporting interactive data analysis,but the goal of KDD tools is to automate asmuch of the process as possible. Thus, KDD isa step beyond what is currently supported bymost standard database systems.Basic DefinitionsKDD is the nontrivial process of identifyingvalid, novel, potentially useful, and ultimateand still run efficiently, how results can be interpreted and visualized, and how the overallmanmachine interaction can usefully bemodeled and supported. The KDD processcan be viewed as a multidisciplinary activitythat encompasses techniques beyond thescope of any one particular discipline such asmachine learning. In this context, there areclear opportunities for other fields of AI besides machine learning to contribute toKDD. KDD places a special emphasis on finding understandable patterns that can be interpreted as useful or interesting knowledge.Thus, for example, neural networks, althougha powerful modeling tool, are relativelydifficult to understand compared to decisiontrees. KDD also emphasizes scaling and robustness properties of modeling algorithmsfor large noisy data sets. Related AI research fields include machinediscovery, which targets the discovery of empirical laws from observation and experimentation Shrager and Langley 1990 see Kloesgen and Zytkow 1996 for a glossary of termscommon to KDD and machine discovery,and causal modeling for the inference ofcausal models from data Spirtes, Glymour,and Scheines 1993. Statistics in particularhas much in common with KDD see Elderand Pregibon 1996 and Glymour et al.1996 for a more detailed discussion of thissynergy. Knowledge discovery from data isfundamentally a statistical endeavor. Statisticsprovides a language and framework for quantifying the uncertainty that results when onetries to infer general patterns from a particular sample of an overall population. As mentioned earlier, the term data mining has hadnegative connotations in statistics since the1960s when computerbased data analysistechniques were first introduced. The concernarose because if one searches long enough inany data set even randomly generated data,one can find patterns that appear to be statistically significant but, in fact, are not. Clearly,this issue is of fundamental importance toKDD. Substantial progress has been made inrecent years in understanding such issues instatistics. Much of this work is of direct relevance to KDD. Thus, data mining is a legitimate activity as long as one understands howto do it correctly data mining carried outpoorly without regard to the statistical aspects of the problem is to be avoided. KDDcan also be viewed as encompassing a broaderview of modeling than statistics. KDD aims toprovide tools to automate to the degree possible the entire process of data analysis andthe statisticians art of hypothesis selection.Data miningis a step inthe KDD process thatconsists of applying dataanalysis anddiscovery algorithms thatproduce a particular enumeration ofpatterns or modelsover the data. Articles40 AI MAGAZINEly understandable patterns in data Fayyad,PiatetskyShapiro, and Smyth 1996.Here, data are a set of facts for example,cases in a database, and pattern is an expression in some language describing a subset ofthe data or a model applicable to the subset.Hence, in our usage here, extracting a patternalso designates fitting a model to data finding structure from data or, in general, making any highlevel description of a set of data.The term process implies that KDD comprisesmany steps, which involve data preparation,search for patterns, knowledge evaluation,and refinement, all repeated in multiple iterations. By nontrivial, we mean that somesearch or inference is involved that is, it isnot a straightforward computation ofpredefined quantities like computing the average value of a set of numbers.The discovered patterns should be valid onnew data with some degree of certainty. Wealso want patterns to be novel at least to thesystem and preferably to the user and potentially useful, that is, lead to some benefit tothe user or task. Finally, the patterns shouldbe understandable, if not immediately thenafter some postprocessing. The previous discussion implies that we candefine quantitative measures for evaluatingextracted patterns. In many cases, it is possible to define measures of certainty for example, estimated prediction accuracy on newdata or utility for example, gain, perhaps indollars saved because of better predictions orspeedup in response time of a system. Notions such as novelty and understandabilityare much more subjective. In certain contexts,understandability can be estimated by simplicity for example, the number of bits to describe a pattern. An important notion, calledinterestingness for example, see Silberschatzand Tuzhilin 1995 and PiatetskyShapiro andMatheus 1994, is usually taken as an overallmeasure of pattern value, combining validity,novelty, usefulness, and simplicity. Interestingness functions can be defined explicitly orcan be manifested implicitly through an ordering placed by the KDD system on the discovered patterns or models. Given these notions, we can consider apattern to be knowledge if it exceeds some interestingness threshold, which is by nomeans an attempt to define knowledge in thephilosophical or even the popular view. As amatter of fact, knowledge in this definition ispurely user oriented and domain specific andis determined by whatever functions andthresholds the user chooses.Data mining is a step in the KDD processthat consists of applying data analysis anddiscovery algorithms that, under acceptablecomputational efficiency limitations, produce a particular enumeration of patterns ormodels over the data. Note that the space ofArticlesFALL 1996   41DataTransformedDataPatternsPreprocessingData MiningInterpretation  EvaluationTransformationSelection      KnowledgePreprocessed DataTarget DateFigure 1. An Overview of the Steps That Compose the KDD Process.methods, the effective number of variablesunder consideration can be reduced, or invariant representations for the data can befound.Fifth is matching the goals of the KDD process step 1 to a particular dataminingmethod. For example, summarization, classification, regression, clustering, and so on,are described later as well as in Fayyad, PiatetskyShapiro, and Smyth 1996.Sixth is exploratory analysis and modeland hypothesis selection choosing the datamining algorithms and selecting methodsto be used for searching for data patterns.This process includes deciding which modelsand parameters might be appropriate for example, models of categorical data are different than models of vectors over the reals andmatching a particular datamining methodwith the overall criteria of the KDD processfor example, the end user might be more interested in understanding the model than itspredictive capabilities.Seventh is data mining searching for patterns of interest in a particular representational form or a set of such representations,including classification rules or trees, regression, and clustering. The user can significantly aid the datamining method by correctlyperforming the preceding steps.Eighth is interpreting mined patterns, possibly returning to any of steps 1 through 7 forfurther iteration. This step can also involvevisualization of the extracted patterns andmodels or visualization of the data given theextracted models.Ninth is acting on the discovered knowledge using the knowledge directly, incorporating the knowledge into another system forfurther action, or simply documenting it andreporting it to interested parties. This processalso includes checking for and resolving potential conflicts with previously believed orextracted knowledge.The KDD process can involve significantiteration and can contain loops betweenany two steps. The basic flow of steps although not the potential multitude of iterations and loops is illustrated in figure 1.Most previous work on KDD has focused onstep 7, the data mining. However, the othersteps are as important and probably moreso for the successful application of KDD inpractice. Having defined the basic notionsand introduced the KDD process, we nowfocus on the datamining component,which has, by far, received the most attention in the literature.patterns is often infinite, and the enumeration of patterns involves some form ofsearch in this space. Practical computationalconstraints place severe limits on the subspace that can be explored by a dataminingalgorithm.The KDD process involves using thedatabase along with any required selection,preprocessing, subsampling, and transformations of it applying datamining methodsalgorithms to enumerate patterns from itand evaluating the products of data miningto identify the subset of the enumerated patterns deemed knowledge. The dataminingcomponent of the KDD process is concernedwith the algorithmic means by which patterns are extracted and enumerated from data. The overall KDD process figure 1 includes the evaluation and possibleinterpretation of the mined patterns to determine which patterns can be considerednew knowledge. The KDD process also includes all the additional steps described inthe next section. The notion of an overall userdriven process is not unique to KDD analogous proposals have been put forward both in statisticsHand 1994 and in machine learning Brodley and Smyth 1996.The KDD ProcessThe KDD process is interactive and iterative,involving numerous steps with many decisions made by the user. Brachman and Anand1996 give a practical view of the KDD process, emphasizing the interactive nature ofthe process. Here, we broadly outline some ofits basic stepsFirst is developing an understanding of theapplication domain and the relevant priorknowledge and identifying the goal of theKDD process from the customers viewpoint.Second is creating a target data set selecting a data set, or focusing on a subset of variables or data samples, on which discovery isto be performed.Third is data cleaning and preprocessing.Basic operations include removing noise ifappropriate, collecting the necessary information to model or account for noise, decidingon strategies for handling missing data fields,and accounting for timesequence information and known changes.Fourth is data reduction and projectionfinding useful features to represent the datadepending on the goal of the task. With dimensionality reduction or transformationArticles42 AI MAGAZINEThe DataMining Step of the KDD ProcessThe datamining component of the KDD process often involves repeated iterative application of particular datamining methods. Thissection presents an overview of the primarygoals of data mining, a description of themethods used to address these goals, and abrief description of the datamining algorithms that incorporate these methods.The knowledge discovery goals are definedby the intended use of the system. We candistinguish two types of goals 1 verificationand 2 discovery. With verification, the system is limited to verifying the users hypothesis. With discovery, the system autonomouslyfinds new patterns. We further subdivide thediscovery goal into prediction, where the system finds patterns for predicting the futurebehavior of some entities, and description,where the system finds patterns for presentation to a user in a humanunderstandableform. In this article, we are primarily concerned with discoveryoriented data mining. Data mining involves fitting models to, ordetermining patterns from, observed data.The fitted models play the role of inferredknowledge Whether the models reflect usefulor interesting knowledge is part of the overall, interactive KDD process where subjectivehuman judgment is typically required. Twoprimary mathematical formalisms are used inmodel fitting 1 statistical and 2 logical.The statistical approach allows for nondeterministic effects in the model, whereas a logical model is purely deterministic. We focusprimarily on the statistical approach to datamining, which tends to be the most widelyused basis for practical datamining applications given the typical presence of uncertainty in realworld datagenerating processes. Most datamining methods are based ontried and tested techniques from machinelearning, pattern recognition, and statisticsclassification, clustering, regression, and soon. The array of different algorithms undereach of these headings can often be bewildering to both the novice and the experienceddata analyst. It should be emphasized that ofthe many datamining methods advertised inthe literature, there are really only a few fundamental techniques. The actual underlyingmodel representation being used by a particular method typically comes from a composition of a small number of wellknown options polynomials, splines, kernel and basisfunctions, thresholdBoolean functions, andso on. Thus, algorithms tend to differ primarily in the goodnessoffit criterion used toevaluate model fit or in the search methodused to find a good fit. In our brief overview of datamining methods, we try in particular to convey the notionthat most if not all methods can be viewedas extensions or hybrids of a few basic techniques and principles. We first discuss the primary methods of data mining and then showthat the data mining methods can be viewedas consisting of three primary algorithmiccomponents 1 model representation, 2model evaluation, and 3 search. In the discussion of KDD and datamining methods,we use a simple example to make some of thenotions more concrete. Figure 2 shows a simple twodimensional artificial data set consisting of 23 cases. Each point on the graph represents a person who has been given a loanby a particular bank at some time in the past.The horizontal axis represents the income ofthe person the vertical axis represents the total personal debt of the person mortgage, carpayments, and so on. The data have beenclassified into two classes 1 the xs represent persons who have defaulted on theirloans and 2 the os represent persons whoseloans are in good status with the bank. Thus,this simple artificial data set could represent ahistorical data set that can contain usefulknowledge from the point of view of thebank making the loans. Note that in actualKDD applications, there are typically manymore dimensions as many as several hundreds and many more data points manythousands or even millions.ArticlesFALL 1996   43xxxxooooIncomeDebtoxooooo oxxxxxooFigure 2. A Simple Data Set with Two Classes Used for Illustrative Purposes.The purpose here is to illustrate basic ideason a small problem in twodimensionalspace.DataMining MethodsThe two highlevel primary goals of data mining in practice tend to be prediction and description. As stated earlier, prediction involves using some variables or fields in thedatabase to predict unknown or future valuesof other variables of interest, and descriptionfocuses on finding humaninterpretable patterns describing the data. Although theboundaries between prediction and description are not sharp some of the predictivemodels can be descriptive, to the degree thatthey are understandable, and vice versa, thedistinction is useful for understanding theoverall discovery goal. The relative importance of prediction and description for particular datamining applications can vary considerably. The goals of prediction anddescription can be achieved using a variety ofparticular datamining methods. Classification is learning a function thatmaps classifies a data item into one of several predefined classes Weiss and Kulikowski1991 Hand 1981. Examples of classificationmethods used as part of knowledge discoveryapplications include the classifying of trendsin financial markets Apte and Hong 1996and the automated identification of objects ofinterest in large image databases Fayyad,Djorgovski, and Weir 1996. Figure 3 shows asimple partitioning of the loan data into twoclass regions note that it is not possible toseparate the classes perfectly using a lineardecision boundary. The bank might want touse the classification regions to automaticallydecide whether future loan applicants will begiven a loan or not.Regression is learning a function that mapsa data item to a realvalued prediction variable. Regression applications are many, forexample, predicting the amount of biomasspresent in a forest given remotely sensed microwave measurements, estimating the probability that a patient will survive given the results of a set of diagnostic tests, predictingconsumer demand for a new product as afunction of advertising expenditure, and predicting time series where the input variablescan be timelagged versions of the predictionvariable. Figure 4 shows the result of simplelinear regression where total debt is fitted as alinear function of income The fit is poor because only a weak correlation exists betweenthe two variables.Clustering is a common descriptive taskArticles44 AI MAGAZINEFigure 3. A Simple Linear Classification Boundary for the Loan Data Set.The shaped region denotes class no loan.xxxxooooIncomeDebtoxooooo oxxxxxoNo LoanLoanoFigure 4. A Simple Linear Regression for the Loan Data Set.xxxxooooIncomeDebtoxooooo oxxxxxooRegression Linewhere one seeks to identify a finite set of categories or clusters to describe the data Jainand Dubes 1988 Titterington, Smith, andMakov 1985. The categories can be mutuallyexclusive and exhaustive or consist of a richerrepresentation, such as hierarchical or overlapping categories. Examples of clustering applications in a knowledge discovery contextinclude discovering homogeneous subpopulations for consumers in marketing databasesand identifying subcategories of spectra frominfrared sky measurements Cheeseman andStutz 1996. Figure 5 shows a possible clustering of the loan data set into three clustersnote that the clusters overlap, allowing datapoints to belong to more than one cluster.The original class labels denoted by xs andos in the previous figures have been replacedby a  to indicate that the class membershipis no longer assumed known. Closely relatedto clustering is the task of probability densityestimation, which consists of techniques forestimating from data the joint multivariateprobability density function of all the variables or fields in the database Silverman1986.Summarization involves methods for finding a compact description for a subset of data. A simple example would be tabulating themean and standard deviations for all fields.More sophisticated methods involve thederivation of summary rules Agrawal et al.1996, multivariate visualization techniques,and the discovery of functional relationshipsbetween variables Zembowicz and Zytkow1996. Summarization techniques are oftenapplied to interactive exploratory data analysis and automated report generation.Dependency modeling consists of finding amodel that describes significant dependenciesbetween variables. Dependency models existat two levels 1 the structural level of themodel specifies often in graphic form whichvariables are locally dependent on each otherand 2 the quantitative level of the modelspecifies the strengths of the dependenciesusing some numeric scale. For example, probabilistic dependency networks use conditional independence to specify the structural aspect of the model and probabilities orcorrelations to specify the strengths of the dependencies Glymour et al. 1987 Heckerman1996. Probabilistic dependency networks areincreasingly finding applications in areas asdiverse as the development of probabilisticmedical expert systems from databases, information retrieval, and modeling of the humangenome.Change and deviation detection focuses ondiscovering the most significant changes inthe data from previously measured or normative values Berndt and Clifford 1996 Guyon,Matic, and Vapnik 1996 Kloesgen 1996Matheus, PiatetskyShapiro, and McNeill1996 Basseville and Nikiforov 1993.The Components of DataMining AlgorithmsThe next step is to construct specific algorithms to implement the general methods weoutlined. One can identify three primarycomponents in any datamining algorithm1 model representation, 2 model evaluation, and 3 search.This reductionist view is not necessarilycomplete or fully encompassing rather, it is aconvenient way to express the key conceptsof datamining algorithms in a relativelyunified and compact manner. Cheeseman1990 outlines a similar structure.Model representation is the language used todescribe discoverable patterns. If the representation is too limited, then no amount oftraining time or examples can produce an accurate model for the data. It is important thata data analyst fully comprehend the representational assumptions that might be inherentin a particular method. It is equally important that an algorithm designer clearly statewhich representational assumptions are beingmade by a particular algorithm. Note that increased representational power for models increases the danger of overfitting the trainingdata, resulting in reduced prediction accuracyon unseen data.Modelevaluation criteria are quantitativeArticlesFALL 1996   45IncomeDebtCluster 2Cluster 3Cluster 1Figure 5. A Simple Clustering of the Loan Data Set into Three Clusters.Note that original labels are replaced by a .Decision Trees and RulesDecision trees and rules that use univariatesplits have a simple representational form,making the inferred model relatively easy forthe user to comprehend. However, the restriction to a particular tree or rule representationcan significantly restrict the functional formand, thus, the approximation power of themodel. For example, figure 6 illustrates the effect of a threshold split applied to the incomevariable for a loan data set It is clear that using such simple threshold splits parallel tothe feature axes severely limits the type ofclassification boundaries that can be induced.If one enlarges the model space to allow moregeneral expressions such as multivariate hyperplanes at arbitrary angles, then the modelis more powerful for prediction but can bemuch more difficult to comprehend. A largenumber of decision tree and ruleinductionalgorithms are described in the machinelearning and applied statistics literatureQuinlan 1992 Breiman et al. 1984.To a large extent, they depend on likelihoodbased modelevaluation methods, withvarying degrees of sophistication in terms ofpenalizing model complexity. Greedy searchmethods, which involve growing and pruning rule and tree structures, are typically usedto explore the superexponential space of possible models. Trees and rules are primarilyused for predictive modeling, both for classification Apte and Hong 1996 Fayyad, Djorgovski, and Weir 1996 and regression, although they can also be applied to summarydescriptive modeling Agrawal et al. 1996.Nonlinear Regression and Classification MethodsThese methods consist of a family of techniques for prediction that fit linear and nonlinear combinations of basis functions sigmoids, splines, polynomials to combinationsof the input variables. Examples include feedforward neural networks, adaptive splinemethods, and projection pursuit regressionsee Elder and Pregibon 1996, Cheng andTitterington 1994, and Friedman 1989 formore detailed discussions. Consider neuralnetworks, for example. Figure 7 illustrates thetype of nonlinear decision boundary that aneural network might find for the loan dataset. In terms of model evaluation, althoughnetworks of the appropriate size can universally approximate any smooth function toany desired degree of accuracy, relatively littleis known about the representation propertiesof fixedsize networks estimated from finitedata sets. Also, the standard squared error andstatements or fit functions of how well a particular pattern a model and its parametersmeets the goals of the KDD process. For example, predictive models are often judged bythe empirical prediction accuracy on sometest set. Descriptive models can be evaluatedalong the dimensions of predictive accuracy,novelty, utility, and understandability of thefitted model. Search method consists of two components1 parameter search and 2 model search.Once the model representation or family ofrepresentations and the modelevaluationcriteria are fixed, then the datamining problem has been reduced to purely an optimization task Find the parameters and modelsfrom the selected family that optimize theevaluation criteria. In parameter search, thealgorithm must search for the parametersthat optimize the modelevaluation criteriagiven observed data and a fixed model representation. Model search occurs as a loop overthe parametersearch method The model representation is changed so that a family ofmodels is considered. Some DataMining MethodsA wide variety of datamining methods exist,but here, we only focus on a subset of popular techniques. Each method is discussed inthe context of model representation, modelevaluation, and search.Articles46 AI MAGAZINExxxxooooIncomeDebtoxooooo oxxxxxoNo LoanLoanotFigure 6. Using a Single Threshold on the Income Variable to Try to Classify the Loan Data Set.crossentropy loss functions used to trainneural networks can be viewed as loglikelihood functions for regression andclassification, respectively Ripley 1994 Geman, Bienenstock, and Doursat 1992. Backpropagation is a parametersearch methodthat performs gradient descent in parameterweight space to find a local maximum ofthe likelihood function starting from randominitial conditions. Nonlinear regression methods, although powerful in representationalpower, can be difficult to interpret.For example, although the classificationboundaries of figure 7 might be more accurate than the simple threshold boundary offigure 6, the threshold boundary has the advantage that the model can be expressed, tosome degree of certainty, as a simple rule ofthe form if income is greater than threshold,then loan will have good status.ExampleBased MethodsThe representation is simple Use representative examples from the database to approximate a model that is, predictions on new examples are derived from the properties ofsimilar examples in the model whose prediction is known. Techniques include nearestneighbor classification and regression algorithms Dasarathy 1991 and casebasedreasoning systems Kolodner 1993. Figure 8illustrates the use of a nearestneighbor classifier for the loan data set The class at anynew point in the twodimensional space isthe same as the class of the closest point inthe original training data set.A potential disadvantage of examplebasedmethods compared with treebased methodsis that a welldefined distance metric for evaluating the distance between data points is required. For the loan data in figure 8, thiswould not be a problem because income anddebt are measured in the same units. However, if one wished to include variables such asthe duration of the loan, sex, and profession,then it would require more effort to define asensible metric between the variables. Modelevaluation is typically based on crossvalidation estimates Weiss and Kulikowski 1991 ofa prediction error Parameters of the model tobe estimated can include the number ofneighbors to use for prediction and the distance metric itself. Like nonlinear regressionmethods, examplebased methods are oftenasymptotically powerful in terms of approximation properties but, conversely, can bedifficult to interpret because the model is implicit in the data and not explicitly formulated. Related techniques include kerneldensityArticlesFALL 1996   47xxxxooooIncomeDebtoxooooo oxxxxxoNo LoanLoanoFigure 7. An Example of Classification Boundaries Learned by a NonlinearClassifier Such as a Neural Network for the Loan Data Set.xxxxooooIncomeDebtoxooooo oxxxxxoNo LoanLoanoFigure 8. Classification Boundaries for a NearestNeighbor Classifier for the Loan Data Set.evitably limited in scope many dataminingtechniques, particularly specialized methodsfor particular types of data and domains, werenot mentioned specifically. We believe thegeneral discussion on datamining tasks andcomponents has general relevance to a variety of methods. For example, consider timeseries prediction, which traditionally hasbeen cast as a predictive regression task autoregressive models, and so on. Recently,more general models have been developed fortimeseries applications, such as nonlinear basis functions, examplebased models, and kernel methods. Furthermore, there has beensignificant interest in descriptive graphic andlocal data modeling of time series rather thanpurely predictive modeling Weigend andGershenfeld 1993. Thus, although differentalgorithms and applications might appear different on the surface, it is not uncommon tofind that they share many common components. Understanding data mining and modelinduction at this component level clarifiesthe behavior of any datamining algorithmand makes it easier for the user to understandits overall contribution and applicability tothe KDD process.An important point is that each techniquetypically suits some problems better thanothers. For example, decision tree classifierscan be useful for finding structure in highdimensional spaces and in problems withmixed continuous and categorical data because tree methods do not require distancemetrics. However, classification trees mightnot be suitable for problems where the truedecision boundaries between classes are described by a secondorder polynomial for example. Thus, there is no universal datamining method, and choosing a particularalgorithm for a particular application is something of an art. In practice, a large portion ofthe application effort can go into properlyformulating the problem asking the rightquestion rather than into optimizing the algorithmic details of a particular dataminingmethod Langley and Simon 1995 Hand1994.Because our discussion and overview of datamining methods has been brief, we wantto make two important points clearFirst, our overview of automated search focused mainly on automated methods for extracting patterns or models from data. Although this approach is consistent with thedefinition we gave earlier, it does not necessarily represent what other communitiesmight refer to as data mining. For example,some use the term to designate any manualestimation Silverman 1986 and mixturemodeling Titterington, Smith, and Makov1985.Probabilistic Graphic Dependency ModelsGraphic models specify probabilistic dependencies using a graph structure Whittaker1990 Pearl 1988. In its simplest form, themodel specifies which variables are directly dependent on each other. Typically, these models are used with categorical or discretevaluedvariables, but extensions to special cases, suchas Gaussian densities, for realvalued variablesare also possible. Within the AI and statisticalcommunities, these models were initially developed within the framework of probabilisticexpert systems the structure of the model andthe parameters the conditional probabilitiesattached to the links of the graph were elicited from experts. Recently, there has been significant work in both the AI and statisticalcommunities on methods whereby both thestructure and the parameters of graphic models can be learned directly from databasesBuntine 1996 Heckerman 1996. Modelevaluation criteria are typically Bayesian in form,and parameter estimation can be a mixture ofclosedform estimates and iterative methodsdepending on whether a variable is directlyobserved or hidden. Model search can consistof greedy hillclimbing methods over variousgraph structures. Prior knowledge, such as apartial ordering of the variables based oncausal relations, can be useful in terms of reducing the model search space. Although stillprimarily in the research phase, graphic modelinduction methods are of particular interest toKDD because the graphic form of the modellends itself easily to human interpretation.Relational Learning ModelsAlthough decision trees and rules have a representation restricted to propositional logic, relational learning also known as inductive logicprogramming uses the more flexible patternlanguage of firstorder logic. A relational learner can easily find formulas such as X  Y. Mostresearch to date on modelevaluation methodsfor relational learning is logical in nature. Theextra representational power of relationalmodels comes at the price of significant computational demands in terms of search. SeeDzeroski 1996 for a more detailed discussion.DiscussionGiven the broad spectrum of dataminingmethods and algorithms, our overview is inUnderstanding data mining andmodel induction atthis componentlevel clarifiesthe behaviorof any dataminingalgorithmand makes iteasier for theuser to understand itsoverall contributionand applicabilityto the KDD process.Articles48 AI MAGAZINEsearch of the data or search assisted by queriesto a database management system or to referto humans visualizing patterns in data. Inother communities, it is used to refer to theautomated correlation of data from transactions or the automated generation of transaction reports. We choose to focus only onmethods that contain certain degrees ofsearch autonomy.Second, beware the hype The state of theart in automated methods in data mining isstill in a fairly early stage of development.There are no established criteria for decidingwhich methods to use in which circumstances, and many of the approaches arebased on crude heuristic approximations toavoid the expensive search required to findoptimal, or even good, solutions. Hence, thereader should be careful when confrontedwith overstated claims about the great abilityof a system to mine useful information fromlarge or even small databases.Application IssuesFor a survey of KDD applications as well asdetailed examples, see PiatetskyShapiro et al.1996 for industrial applications and Fayyad,Haussler, and Stolorz 1996 for applicationsin science data analysis. Here, we examinecriteria for selecting potential applications,which can be divided into practical and technical categories. The practical criteria for KDDprojects are similar to those for other applications of advanced technology and include thepotential impact of an application, the absence of simpler alternative solutions, andstrong organizational support for using technology. For applications dealing with personal data, one should also consider the privacyand legal issues PiatetskyShapiro 1995. The technical criteria include considerations such as the availability of sufficient datacases. In general, the more fields there areand the more complex the patterns beingsought, the more data are needed. However,strong prior knowledge see discussion latercan reduce the number of needed cases significantly. Another consideration is the relevance of attributes. It is important to have data attributes that are relevant to the discoverytask no amount of data will allow predictionbased on attributes that do not capture therequired information. Furthermore, low noiselevels few data errors are another consideration. High amounts of noise make it hard toidentify patterns unless a large number of cases can mitigate random noise and help clarifythe aggregate patterns. Changing and timeoriented data, although making the application development more difficult, make it potentially much more useful because it is easierto retrain a system than a human. Finally,and perhaps one of the most important considerations, is prior knowledge. It is useful toknow something about the domain whatare the important fields, what are the likelyrelationships, what is the user utility function, what patterns are already known, and soon. Research and Application ChallengesWe outline some of the current primary research and application challenges for KDD.This list is by no means exhaustive and is intended to give the reader a feel for the typesof problem that KDD practitioners wrestlewith. Larger databases Databases with hundreds of fields and tables and millions ofrecords and of a multigigabyte size are commonplace, and terabyte 1012 bytes databasesare beginning to appear. Methods for dealingwith large data volumes include moreefficient algorithms Agrawal et al. 1996,sampling, approximation, and massively parallel processing Holsheimer et al. 1996.High dimensionality Not only is there often a large number of records in the database,but there can also be a large number of fieldsattributes, variables so, the dimensionalityof the problem is high. A highdimensionaldata set creates problems in terms of increasing the size of the search space for model induction in a combinatorially explosive manner. In addition, it increases the chances thata datamining algorithm will find spuriouspatterns that are not valid in general. Approaches to this problem include methods toreduce the effective dimensionality of theproblem and the use of prior knowledge toidentify irrelevant variables.Overfitting When the algorithm searchesfor the best parameters for one particularmodel using a limited set of data, it can model not only the general patterns in the databut also any noise specific to the data set, resulting in poor performance of the model ontest data. Possible solutions include crossvalidation, regularization, and other sophisticated statistical strategies.Assessing of statistical significance Aproblem related to overfitting occurs whenthe system is searching over many possiblemodels. For example, if a system tests modelsat the 0.001 significance level, then on average, with purely random data, N1000 ofthese models will be accepted as significant.ArticlesFALL 1996   49edge is important in all the steps of the KDDprocess. Bayesian approaches for example,Cheeseman 1990 use prior probabilitiesover data and distributions as one form of encoding prior knowledge. Others employ deductive database capabilities to discoverknowledge that is then used to guide the datamining search for example, Simoudis,Livezey, and Kerber 1995.Integration with other systems A standalone discovery system might not be veryuseful. Typical integration issues include integration with a database management systemfor example, through a query interface, integration with spreadsheets and visualizationtools, and accommodating of realtime sensorreadings. Examples of integrated KDD systems are described by Simoudis, Livezey, andKerber 1995 and Stolorz, Nakamura, Mesrobiam, Muntz, Shek, Santos, Yi, Ng, Chien,Mechoso, and Farrara 1995. Concluding Remarks The Potential Role of AI in KDDIn addition to machine learning, other AI fields can potentially contribute significantly tovarious aspects of the KDD process. We mention a few examples of these areas hereNatural language presents significant opportunities for mining in freeform text, especially for automated annotation and indexingprior to classification of text corpora. Limitedparsing capabilities can help substantially inthe task of deciding what an article refers to.Hence, the spectrum from simple natural language processing all the way to language understanding can help substantially. Also, natural language processing can contributesignificantly as an effective interface for stating hints to mining algorithms and visualizing and explaining knowledge derived by aKDD system. Planning considers a complicated dataanalysis process. It involves conducting complicated dataaccess and datatransformationoperations applying preprocessing routinesand, in some cases, paying attention to resource and dataaccess constraints. Typically,data processing steps are expressed in terms ofdesired postconditions and preconditions forthe application of certain routines, whichlends itself easily to representation as a planning problem. In addition, planning abilitycan play an important role in automatedagents see next item to collect data samplesor conduct a search to obtain needed data sets.Intelligent agents can be fired off to collect necessary information from a variety ofThis point is frequently missed by many initial attempts at KDD. One way to deal withthis problem is to use methods that adjustthe test statistic as a function of the search,for example, Bonferroni adjustments for independent tests or randomization testing.Changing data and knowledge Rapidlychanging nonstationary data can make previously discovered patterns invalid. In addition, the variables measured in a given application database can be modified, deleted, oraugmented with new measurements overtime. Possible solutions include incrementalmethods for updating the patterns and treating change as an opportunity for discoveryby using it to cue the search for patterns ofchange only Matheus, PiatetskyShapiro, andMcNeill 1996. See also Agrawal and Psaila1995 and Mannila, Toivonen, and Verkamo1995. Missing and noisy data This problem isespecially acute in business databases. U.S.census data reportedly have error rates asgreat as 20 percent in some fields. Importantattributes can be missing if the database wasnot designed with discovery in mind. Possiblesolutions include more sophisticated statistical strategies to identify hidden variables anddependencies Heckerman 1996 Smyth et al.1996.Complex relationships between fieldsHierarchically structured attributes or values,relations between attributes, and more sophisticated means for representing knowledge about the contents of a database will require algorithms that can effectively use suchinformation. Historically, datamining algorithms have been developed for simple attributevalue records, although new techniques for deriving relations betweenvariables are being developed Dzeroski 1996Djoko, Cook, and Holder 1995.Understandability of patterns In manyapplications, it is important to make the discoveries more understandable by humans.Possible solutions include graphic representations Buntine 1996 Heckerman 1996, rulestructuring, natural language generation, andtechniques for visualization of data andknowledge. Rulerefinement strategies for example, Major and Mangano 1995 can beused to address a related problem The discovered knowledge might be implicitly or explicitly redundant.User interaction and prior knowledgeMany current KDD methods and tools are nottruly interactive and cannot easily incorporate prior knowledge about a problem exceptin simple ways. The use of domain knowlArticles50 AI MAGAZINEsources. In addition, information agents canbe activated remotely over the network orcan trigger on the occurrence of a certainevent and start an analysis operation. Finally,agents can help navigate and model theWorldWide Web Etzioni 1996, another areagrowing in importance.Uncertainty in AI includes issues for managing uncertainty, proper inference mechanisms in the presence of uncertainty, and thereasoning about causality, all fundamental toKDD theory and practice. In fact, the KDD96conference had a joint session with the UAI96conference this year Horvitz and Jensen 1996.Knowledge representation includes ontologies, new concepts for representing, storing, and accessing knowledge. Also includedare schemes for representing knowledge andallowing the use of prior human knowledgeabout the underlying process by the KDDsystem.These potential contributions of AI are buta sampling many others, including humancomputer interaction, knowledgeacquisitiontechniques, and the study of mechanisms forreasoning, have the opportunity to contribute to KDD.In conclusion, we presented some definitions of basic notions in the KDD field. Ourprimary aim was to clarify the relation between knowledge discovery and data mining.We provided an overview of the KDD processand basic datamining methods. Given thebroad spectrum of datamining methods andalgorithms, our overview is inevitably limited in scope There are many dataminingtechniques, particularly specialized methodsfor particular types of data and domain. Although various algorithms and applicationsmight appear quite different on the surface,it is not uncommon to find that they sharemany common components. Understandingdata mining and model induction at thiscomponent level clarifies the task of any datamining algorithm and makes it easier forthe user to understand its overall contribution and applicability to the KDD process.This article represents a step toward acommon framework that we hope will ultimately provide a unifying vision of the common overall goals and methods used inKDD. We hope this will eventually lead to abetter understanding of the variety of approaches in this multidisciplinary field andhow they fit together.AcknowledgmentsWe thank Sam Uthurusamy, Ron Brachman, andKDD96 referees for their valuable suggestionsand ideas. Note1. Throughout this article, we use the term patternto designate a pattern found in data. We also referto models. One can think of patterns as components of models, for example, a particular rule in aclassification model or a linear component in a regression model.  ReferencesAgrawal, R., and Psaila, G. 1995. Active Data Mining. In Proceedings of the First International Conference on Knowledge Discovery and Data MiningKDD95, 38. Menlo Park, Calif. American Association for Artificial Intelligence.Agrawal, R. Mannila, H. Srikant, R. Toivonen, H.and Verkamo, I. 1996. Fast Discovery of AssociationRules. In Advances in Knowledge Discovery and DataMining, eds. U. Fayyad, G. PiatetskyShapiro, P.Smyth, and R. Uthurusamy, 307328. Menlo Park,Calif. AAAI Press.Apte, C., and Hong, S. J. 1996. Predicting EquityReturns from Securities Data with Minimal RuleGeneration. In Advances in Knowledge Discovery andData Mining, eds. U. Fayyad, G. PiatetskyShapiro, P.Smyth, and R. Uthurusamy, 514560. Menlo Park,Calif. AAAI Press.Basseville, M., and Nikiforov, I. V. 1993. Detectionof Abrupt Changes Theory and Application. Englewood Cliffs, N.J. Prentice Hall.Berndt, D., and Clifford, J. 1996. Finding Patternsin Time Series A Dynamic Programming Approach.In Advances in Knowledge Discovery and Data Mining,eds. U. Fayyad, G. PiatetskyShapiro, P. Smyth, andR. Uthurusamy, 229248. Menlo Park, Calif. AAAIPress.Berry, J. 1994. Database Marketing. Business Week,September 5, 5662.Brachman, R., and Anand, T. 1996. The Process ofKnowledge Discovery in Databases A HumanCentered Approach. In Advances in Knowledge Discoveryand Data Mining, 3758, eds. U. Fayyad, G. PiatetskyShapiro, P. Smyth, and R. Uthurusamy. MenloPark, Calif. AAAI Press.Breiman, L. Friedman, J. H. Olshen, R. A. andStone, C. J. 1984. Classification and Regression Trees.Belmont, Calif. Wadsworth.Brodley, C. E., and Smyth, P. 1996. Applying Classification Algorithms in Practice. Statistics and Computing. Forthcoming.Buntine, W. 1996. Graphical Models for Discovering Knowledge. In Advances in Knowledge Discoveryand Data Mining, eds. U. Fayyad, G. PiatetskyShapiro, P. Smyth, and R. Uthurusamy, 5982.Menlo Park, Calif. AAAI Press.Cheeseman, P. 1990. On Finding the Most ProbableModel. In Computational Models of Scientific Discovery and Theory Formation, eds. J. Shrager and P. Langley, 7395. San Francisco, Calif. Morgan Kaufmann.Cheeseman, P., and Stutz, J. 1996. Bayesian Classification AUTOCLASS Theory and Results. In Advances in Knowledge Discovery and Data Mining, eds.ArticlesFALL 1996   51ering Informative Patterns and Data Cleaning. InAdvances in Knowledge Discovery and Data Mining,eds. U. Fayyad, G. PiatetskyShapiro, P. Smyth, andR. Uthurusamy, 181204. Menlo Park, Calif. AAAIPress.Hall, J. Mani, G. and Barr, D. 1996. ApplyingComputational Intelligence to the Investment Process. In Proceedings of CIFER96 ComputationalIntelligence in Financial Engineering. Washington,D.C. IEEE Computer Society.Hand, D. J. 1994. Deconstructing Statistical Questions. Journal of the Royal Statistical Society A. 1573317356.Hand, D. J. 1981. Discrimination and Classification.Chichester, U.K. Wiley.Heckerman, D. 1996. Bayesian Networks for Knowledge Discovery. In Advances in Knowledge Discoveryand Data Mining, eds. U. Fayyad, G. PiatetskyShapiro, P. Smyth, and R. Uthurusamy, 273306.Menlo Park, Calif. AAAI Press.Hernandez, M., and Stolfo, S. 1995. The MERGEPURGE Problem for Large Databases. In Proceedingsof the 1995 ACMSIGMOD Conference, 127138.New York Association for Computing Machinery.Holsheimer, M. Kersten, M. L. Mannila, H. andToivonen, H. 1996. Data Surveyor Searching theNuggets in Parallel. In Advances in Knowledge Discovery and Data Mining, eds. U. Fayyad, G. PiatetskyShapiro, P. Smyth, and R. Uthurusamy,447471. Menlo Park, Calif. AAAI Press.Horvitz, E., and Jensen, F. 1996. Proceedings of theTwelfth Conference of Uncertainty in Artificial Intelligence. San Mateo, Calif. Morgan Kaufmann.Jain, A. K., and Dubes, R. C. 1988. Algorithms forClustering Data. Englewood Cliffs, N.J. PrenticeHall.Kloesgen, W. 1996. A Multipattern and Multistrategy Discovery Assistant. In Advances in KnowledgeDiscovery and Data Mining, eds. U. Fayyad, G. PiatetskyShapiro, P. Smyth, and R. Uthurusamy,249271. Menlo Park, Calif. AAAI Press.Kloesgen, W., and Zytkow, J. 1996. Knowledge Discovery in Databases Terminology. In Advances inKnowledge Discovery and Data Mining, eds. U. Fayyad,G. PiatetskyShapiro, P. Smyth, and R. Uthurusamy,569588. Menlo Park, Calif. AAAI Press.Kolodner, J. 1993. CaseBased Reasoning. San Francisco, Calif. Morgan Kaufmann.Langley, P., and Simon, H. A. 1995. Applications ofMachine Learning and Rule Induction. Communications of the ACM 385564.Major, J., and Mangano, J. 1995. Selecting amongRules Induced from a Hurricane Database. Journalof Intelligent Information Systems 41 3952.Manago, M., and Auriol, M. 1996. Mining for OR.ORMS Today Special Issue on Data Mining, February, 2832.Mannila, H. Toivonen, H. and Verkamo, A. I.1995. Discovering Frequent Episodes in Sequences.In Proceedings of the First International Conference on Knowledge Discovery and Data MiningKDD95, 210215. Menlo Park, Calif. AmericanU. Fayyad, G. PiatetskyShapiro, P. Smyth, and R.Uthurusamy, 7395. Menlo Park, Calif. AAAI Press.Cheng, B., and Titterington, D. M. 1994. NeuralNetworksA Review from a Statistical Perspective.Statistical Science 91 230.Codd, E. F. 1993. Providing OLAP OnLine Analytical Processing to UserAnalysts An IT Mandate. E.F. Codd and Associates.Dasarathy, B. V. 1991. Nearest Neighbor NNNorms NN Pattern Classification Techniques.Washington, D.C. IEEE Computer Society.Djoko, S. Cook, D. and Holder, L. 1995. Analyzingthe Benefits of Domain Knowledge in SubstructureDiscovery. In Proceedings of KDD95 First International Conference on Knowledge Discovery andData Mining, 7580. Menlo Park, Calif. AmericanAssociation for Artificial Intelligence.Dzeroski, S. 1996. Inductive Logic Programming forKnowledge Discovery in Databases. In Advances inKnowledge Discovery and Data Mining, eds. U.Fayyad, G. PiatetskyShapiro, P. Smyth, and R.Uthurusamy, 5982. Menlo Park, Calif. AAAI Press.Elder, J., and Pregibon, D. 1996. A Statistical Perspective on KDD. In Advances in Knowledge Discovery and Data Mining, eds. U. Fayyad, G. PiatetskyShapiro, P. Smyth, and R. Uthurusamy, 83116.Menlo Park, Calif. AAAI Press.Etzioni, O. 1996. The World Wide Web Quagmireor Gold Mine Communications of the ACM SpecialIssue on Data Mining. November 1996. Forthcoming.Fayyad, U. M. Djorgovski, S. G. and Weir, N. 1996.From Digitized Images to OnLine Catalogs DataMining a Sky Survey. AI Magazine 172 5166.Fayyad, U. M. Haussler, D. and Stolorz, Z. 1996.KDD for Science Data Analysis Issues and Examples. In Proceedings of the Second InternationalConference on Knowledge Discovery and DataMining KDD96, 5056. Menlo Park, Calif. American Association for Artificial Intelligence.Fayyad, U. M. PiatetskyShapiro, G. and Smyth, P.1996. From Data Mining to Knowledge DiscoveryAn Overview. In Advances in Knowledge Discoveryand Data Mining, eds. U. Fayyad, G. PiatetskyShapiro, P. Smyth, and R. Uthurusamy, 130. Menlo Park, Calif. AAAI Press.Fayyad, U. M. PiatetskyShapiro, G. Smyth, P. andUthurusamy, R. 1996. Advances in Knowledge Discovery and Data Mining. Menlo Park, Calif. AAAIPress.Friedman, J. H. 1989. Multivariate Adaptive Regression Splines. Annals of Statistics 191141.Geman, S. Bienenstock, E. and Doursat, R. 1992.Neural Networks and the BiasVariance Dilemma.Neural Computation 4158.Glymour, C. Madigan, D. Pregibon, D. andSmyth, P. 1996. Statistics and Data Mining. Communications of the ACM Special Issue on Data Mining. November 1996. Forthcoming.Glymour, C. Scheines, R. Spirtes, P. Kelly, K. 1987.Discovering Causal Structure. New York Academic.Guyon, O. Matic, N. and Vapnik, N. 1996. DiscovArticles52 AI MAGAZINEAssociation for Artificial Intelligence.Matheus, C. PiatetskyShapiro, G. and McNeill, D.1996. Selecting and Reporting What Is InterestingThe KEfiR Application to Healthcare Data. In Advances in Knowledge Discovery and Data Mining, eds.U. Fayyad, G. PiatetskyShapiro, P. Smyth, and R.Uthurusamy, 495516. Menlo Park, Calif. AAAIPress. Pearl, J. 1988. Probabilistic Reasoning in IntelligentSystems. San Francisco, Calif. Morgan Kaufmann.PiatetskyShapiro, G. 1995. Knowledge Discoveryin Personal Data versus PrivacyA MiniSymposium. IEEE Expert 105.PiatetskyShapiro, G. 1991. Knowledge Discoveryin Real Databases A Report on the IJCAI89 Workshop. AI Magazine 115 6870. PiatetskyShapiro, G., and Matheus, C. 1994. TheInterestingness of Deviations. In Proceedings ofKDD94, eds. U. M.  Fayyad and R. Uthurusamy.Technical Report WS03. Menlo Park, Calif. AAAIPress.PiatetskyShapiro, G. Brachman, R. Khabaza, T.Kloesgen, W. and Simoudis, E., 1996. An Overviewof Issues in Developing Industrial Data Mining andKnowledge Discovery Applications. In Proceedingsof the Second International Conference on Knowledge Discovery and Data Mining KDD96, eds. J.Han and E. Simoudis, 8995. Menlo Park, Calif.American Association for Artificial Intelligence.Quinlan, J. 1992. C4.5 Programs for Machine Learning. San Francisco, Calif. Morgan Kaufmann.Ripley, B. D. 1994. Neural Networks and RelatedMethods for Classification. Journal of the Royal Statistical Society B. 563 409437. Senator, T. Goldberg, H. G. Wooton, J. Cottini, M.A. Umarkhan, A. F. Klinger, C. D. Llamas, W. M.Marrone, M. P. and Wong, R. W. H. 1995. The Financial Crimes Enforcement Network AI SystemFAIS Identifying Potential Money Launderingfrom Reports of Large Cash Transactions. AI Magazine 164 2139.Shrager, J., and Langley, P., eds. 1990. Computational Models of Scientific Discovery and Theory Formation. San Francisco, Calif. Morgan Kaufmann.Silberschatz, A., and Tuzhilin, A. 1995. On Subjective Measures of Interestingness in Knowledge Discovery. In Proceedings of KDD95 First International Conference on Knowledge Discovery andData Mining, 275281. Menlo Park, Calif. American Association for Artificial Intelligence.Silverman, B. 1986. Density Estimation for Statisticsand Data Analysis. New York Chapman and Hall.Simoudis, E. Livezey, B. and Kerber, R. 1995. UsingRecon for Data Cleaning. In Proceedings of KDD95First International Conference on Knowledge Discovery and Data Mining, 275281. Menlo Park, Calif.American Association for Artificial Intelligence.Smyth, P. Burl, M. Fayyad, U. and Perona, P.1996. Modeling Subjective Uncertainty in ImageAnnotation. In Advances in Knowledge Discovery andData Mining, 517540. Menlo Park, Calif. AAAIPress.Spirtes, P. Glymour, C. and Scheines, R. 1993.Causation, Prediction, and Search. New YorkSpringerVerlag.Stolorz, P. Nakamura, H. Mesrobian, E. Muntz, R.Shek, E. Santos, J. Yi, J. Ng, K. Chien, S. Mechoso, C. and Farrara, J.  1995. Fast SpatioTemporal Data Mining of Large Geophysical Datasets. InProceedings of KDD95 First International Conference on Knowledge Discovery and Data Mining,300305. Menlo Park, Calif. American Associationfor Artificial Intelligence.Titterington, D. M. Smith, A. F. M. and Makov, U.E. 1985. Statistical Analysis of FiniteMixture Distributions. Chichester, U.K. Wiley.U.S. News. 1995. Basketballs New HighTech GuruIBM Software Is Changing Coaches Game Plans.U.S. News and World Report, 11 December.Weigend, A., and Gershenfeld, N., eds. 1993. Predicting the Future and Understanding the Past. Redwood City, Calif. AddisonWesley.Weiss, S. I., and Kulikowski, C. 1991. Computer Systems That Learn Classification and Prediction Methods from Statistics, Neural Networks, Machine Learning, and Expert Systems. San Francisco, Calif.Morgan Kaufmann.Whittaker, J. 1990. Graphical Models in Applied Multivariate Statistics. New York Wiley.Zembowicz, R., and Zytkow, J. 1996. From Contingency Tables to Various Forms of Knowledge inDatabases. In Advances in Knowledge Discovery andData Mining, eds. U. Fayyad, G. PiatetskyShapiro, P.Smyth, and R. Uthurusamy, 329351. Menlo Park,Calif. AAAI Press.Usama Fayyad is a senior researcher at Microsoft Research.He received his Ph.D. in 1991from the University of Michiganat Ann Arbor. Prior to joining Microsoft in 1996, he headed theMachine Learning Systems Groupat the Jet Propulsion LaboratoryJPL, California Institute of Technology, where he developed datamining systemsfor automated science data analysis. He remainsaffiliated with JPL as a distinguished visiting scientist. Fayyad received the JPL 1993 Lew Allen Awardfor Excellence in Research and the 1994 NationalAeronautics and Space Administration ExceptionalAchievement Medal. His research interests includeknowledge discovery in large databases, data mining, machinelearning theory and applications, statistical pattern recognition, and clustering. He wasprogram cochair of KDD94 and KDD95 the FirstInternational Conference on Knowledge Discoveryand Data Mining. He is general chair of KDD96,an editor in chief of the journal Data Mining andKnowledge Discovery, and coeditor of the 1996 AAAIPress book Advances in Knowledge Discovery and Data Mining.ArticlesFALL 1996   53cal Engineering Departments at Caltech 1994 andregularly conducts tutorials on probabilistic learning algorithms at national conferences includingUAI93, AAAI94, CAIA95, IJCAI95. He is generalchair of the Sixth International Workshop on AIand Statistics, to be held in 1997. Smyths researchinterests include statistical pattern recognition, machine learning, decision theory, probabilistic reasoning, information theory, and the application ofprobability and statistics in AI. He has published 16journal papers, 10 book chapters, and 60 conference papers on these topics.Gregory PiatetskyShapiro is aprincipal member of the technicalstaff at GTE Laboratories and theprincipal investigator of theKnowledge Discovery in Databases KDD Project, which focuseson developing and deploying advanced KDD systems for businessapplications. Previously, heworked on applying intelligent front ends to heterogeneous databases. PiatetskyShapiro receivedseveral GTE awards, including GTEs highest technical achievement award for the KEfiR system forhealthcare data analysis. His research interests include intelligent database systems, dependencynetworks, and Internet resource discovery. Prior toGTE, he worked at Strategic Information developing financial database systems. PiatetskyShapiro received his M.S. in 1979 and his Ph.D. in 1984, bothfrom New York University NYU. His Ph.D. dissertation on selforganizing database systems receivedNYU awards as the best dissertation in computerscience and in all natural sciences. PiatetskyShapiro organized and chaired the first three 1989,1991, and 1993 KDD workshops and helped in developing them into successful conferences KDD95and KDD96. He has also been on the programcommittees of numerous other conferences andworkshops on AI and databases. He edited andcoedited several collections on KDD, including twobooksKnowledge Discovery in Databases AAAIPress, 1991 and Advances in Knowledge Discovery inDatabases AAAI Press, 1996and has many otherpublications in the areas of AI and databases. He isa coeditor in chief of the new Data Mining andKnowledge Discovery journal. PiatetskyShapirofounded and moderates the KDD Nuggets electronicnewsletter kddgte.com and is the web master forKnowledge Discovery Mine httpinfo.gte.comkdd index.html.Padhraic Smyth received a firstclasshonors Bachelor of Engineering from the National University of Ireland in 1984 and anMSEE and a Ph.D. from the Electrical Engineering Department atthe California Institute of Technology Caltech in 1985 and1988, respectively. From 1988 to1996, he was a technical group leader at the JetPropulsion Laboratory JPL. Since April 1996, hehas been a faculty member in the Information andComputer Science Department at the University ofCalifornia at Irvine. He is also currently a principalinvestigator at JPL parttime and is a consultant toprivate industry. Smyth received the Lew AllenAward for Excellence in Research at JPL in 1993and has been awarded 14 National Aeronautics andSpace Administration certificates for technical innovation since 1991. He was coeditor of the bookAdvances in Knowledge Discovery and Data MiningAAAI Press, 1996. Smyth was a visiting lecturer inthe Computational and Neural Systems and ElectriArticles54 AI MAGAZINEAAAI 97Providence, Rhode IslandJuly 2731, 1997Title pages due January 6, 1997 Papers due January 8, 1997Camera copy due April 2, 1997ncaiaaaai.orghttpwww.aaai.orgConferencesNational1997aaai97.html
