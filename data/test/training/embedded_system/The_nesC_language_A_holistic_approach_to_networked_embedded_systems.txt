The nesC LanguageA Holistic Approach to Networked Embedded Systemshttpnescc.sourceforge.netDavid Gaydgayintelresearch.netPhilip Levispalcs.berkeley.eduRobert von Behrenjrvbcs.berkeley.eduMatt Welshmdwintelresearch.netEric Brewerbrewercs.berkeley.eduDavid Cullercullercs.berkeley.eduEECS Department Intel Research, BerkeleyUniversity of California, Berkeley 2150 Shattuck Ave, Suite 1300Berkeley, CA 94720 Berkeley, CA 94704ABSTRACTWe present nesC, a programming language for networked embedded systems that represent a new design space for application developers. An example of a networked embedded system is a sensornetwork, which consists of potentially thousands of tiny, lowpower motes, each of which execute concurrent, reactive programs that must operate with severe memory and power constraints.nesCs contribution is to support the special needs of this domainby exposing a programming model that incorporates eventdrivenexecution, a flexible concurrency model, and componentorientedapplication design. Restrictions on the programming model allow the nesC compiler to perform wholeprogram analyses, including datarace detection which improves reliability and aggressivefunction inlining which reduces resource consumption.nesC has been used to implement TinyOS, a small operating system for sensor networks, as well as several significant sensor applications. nesC and TinyOS have been adopted by a large number ofsensor network research groups, and our experience and evaluationof the language shows that it is effective at supporting the complex, concurrent programming style demanded by this new class ofdeeply networked systems.Categories and Subject DescriptorsD.3 Programming Languages GeneralGeneral TermsDesign, LanguagesKeywordsProgramming Languages, C, Concurrency, Data Races, Components, Modules, Firstorder, nesC, TinyOSPermission to make digital or hard copies of all or part of this work forpersonal or classroom use is granted without fee provided that copies arenot made or distributed for profit or commercial advantage and that copiesbear this notice and the full citation on the first page. To copy otherwise, torepublish, to post on servers or to redistribute to lists, requires prior specificpermission andor a fee.PLDI03, June 911, 2003, San Diego, California, USA.Copyright 2003 ACM 1581136625030006 ...5.00.1. INTRODUCTIONAdvances in networking and integration have enabled small, flexible, lowcost nodes that interact with their environment throughsensors, actuators and communication. Singlechip systems arenow emerging that integrate a lowpower CPU and memory, radio or optical communication, and substantial MEMsbased onchip sensors these nodes are colloquially referred to as motesor smart dust 49. Target costs for singlechip motes are lessthan 10 cents per unit, which enables networks with potentiallytens of thousands of motes. Target power consumption means thatmotes can last years with lowbandwidth communication, or evenbe batteryfree when fueled by ambient power e.g., heat from theenvironment.In this paper, we present nesC, a systems programming languagefor networked embedded systems such as motes. nesC supports aprogramming model that integrates reactivity to the environment,concurrency, and communication. By performing wholeprogramoptimizations and compiletime data race detection, nesC simplifies application development, reduces code size, and eliminatesmany sources of potential bugs.A key focus of nesC is holistic system design. Mote applicationsare deeply tied to hardware, and each mote runs a single applicationat a time. This approach yields three important properties. First, allresources are known statically. Second, rather than employing ageneralpurpose OS, applications are built from a suite of reusablesystem components coupled with applicationspecific code. Third,the hardwaresoftware boundary varies depending on the application and hardware platform it is important to design for flexibledecomposition.There are a number of unique challenges that nesC must addressDriven by interaction with environment Unlike traditional computers, motes are used for data collection and control of the localenvironment, rather than generalpurpose computation. This focusleads to two observations. First, motes are fundamentally eventdriven, reacting to changes in the environment message arrival,sensor acquisition rather than driven by interactive or batch processing. Second, event arrival and data processing are concurrentactivities, demanding an approach to concurrency management thataddresses potential bugs such as race conditions.Limited resources Motes have very limited physical resources,due to the goals of small size, low cost, and low power consumption. We do not expect new technology to remove these limitations1the benefits of Moores Law will be applied to reduce size andcost, rather than increase capability. Although our current motesare measured in square centimeters, a version has been built thatmeasures less than 5 mm2.Reliability Although we expect individual motes to fail due tohardware issues, we must enable very longlived applications. Forexample, environmental monitoring applications must collect datawithout human interaction for months at a time. An important goalis to reduce runtime errors, since there is no real recovery mechanism in the field except for automatic reboot.Soft realtime requirements Although there are some tasks thatare time critical, such as radio management or sensor polling, wedo not focus on hard realtime guarantees. Our experience so farindicates that timing constraints are easily met by having completecontrol over the application and OS, and limiting utilization. One ofthe few timingcritical aspects in sensor networks is radio communication however, given the fundamental unreliability of the radiolink, it is not necessary to meet hard deadlines in this domain.Although nesC is a synthesis of many existing language conceptstargeted at the above problems, it provides three broad contributions. First, nesC defines a component model that supports eventdriven systems the model provides bidirectional interfaces to simplify event flow, supports a flexible hardwaresoftware boundary,and admits efficient implementation that avoids virtual functionsand dynamic component creation. Second, nesC defines a simplebut expressive concurrency model coupled with extensive compiletime analysis the nesC compiler detects most data races at compiletime. This combination allows applications to exhibit highly concurrent behavior with very limited resources. Third, nesC providesa unique balance between accurate program analysis to improve reliability and reduce code size, and expressive power for buildingreal applications. In addition to static data race detection, the nesCcompiler performs static component instantiation, wholeprograminlining, and deadcode elimination. We prohibit many featuresthat hinder static analysis, including function pointers and dynamicmemory allocation, but are capable of supporting complex applications and a substantial user community.nesC is used as the programming language for TinyOS 16, asmall operating system for sensor network applications that is inuse by more than 100 research groups worldwide. Several significant sensor network applications have been implemented in nesC,including TinyDB 29, a sensor network query processing engine,and Mate 28, a small virtual machine that allows rapid reprogramming of sensor networks.Section 2 presents background material on sensor networks andintroduces Surge, a sample nesC application used as a running example throughout the paper. Section 3 presents the nesC designSection 4 summarizes our experience with nesC and evaluates theeffectiveness of datarace detection and inlining. We conclude witha survey of related work Section 5 and a discussion of nesC andits future directions Section 6.2. BACKGROUNDWireless sensor networks are composed of large numbers of tinyresourcelimited devices motes. A first application of these networks is data collection in uncontrolled environments, such as nature reserves 30 or seismically threatened structures 25. Fourkey features have emerged in networks we have deployed interaction with the local environment though sensors, communication viaa wireless network, lifetime requirements of months to a year, andphysical inaccessibility.Table 1 presents several generations of motes designed at UCMote Type WeC rene2 rene2 dot micaDate 999 1000 601 801 202MicrocontrollerType AT90LS8535 ATMega163 ATMega103Prog. mem. KB 8 16 128RAM KB 0.5 1 4CommunicationRadio RFM TR1000Rate Kbps 10 10 10 10 1040Modulation type OOK OOKASKTable 1 The family of TinyOS motes.Berkeley. Although very resource constrained, motes must be veryreactive and participate in complex distributed algorithms, such asdata aggregation 19, 29 or spatial localization 50. This combination of requirements makes traditional operating systems andprogramming models inappropriate for sensor networks. Mote hardware evolves rapidly Table 1 covers five platforms in three years,with different sensors and varying levels of hardware support foroperations such as radiobased messaging. A sensor network operating system and programming language must make it easy forapplications to adapt to these changes.2.1 TinyOSTinyOS 16 is an operating system specifically designed for network embedded systems. TinyOS has a programming model tailored for eventdriven applications as well as a very small footprintthe core OS requires 400 bytes of code and data memory, combined. Two of our motivations in designing nesC were to support and evolve TinyOSs programming model and to reimplementTinyOS in the new language. TinyOS has several important features that influenced nesCs design a componentbased architecture, a simple eventbased concurrency model, and splitphase operations.Componentbased architecture TinyOS provides a set of reusable system components. An application connects components usinga wiring specification that is independent of component implementations each application customizes the set of components it uses.Although most OS components are software modules, some arethin wrappers around hardware the distinction is invisible to thedeveloper. Decomposing different OS services into separate components allows unused services to be excluded from the application.We present nesCs support for components in Sections 3.1 and 3.2.Tasks and eventbased concurrency There are two sources ofconcurrency in TinyOS tasks and events. Tasks are a deferredcomputation mechanism. They run to completion and do not preempt each other. Components can post tasks the post operationimmediately returns, deferring the computation until the schedulerexecutes the task later. Components can use tasks when timing requirements are not strict this includes nearly all operations exceptlowlevel communication. To ensure low task execution latency,individual tasks must be short lengthy operations should be spreadacross multiple tasks. The lifetime requirements of sensor networksprohibit heavy computation, keeping the system reactive.Events also run to completion, but may preempt the executionof a task or another event. Events signify either completion of asplitphase operation discussed below or an event from the environment e.g. message reception or time passing. TinyOS execution is ultimately driven by events representing hardware interrupts.We discuss the refinement and inclusion of TinyOSs concurrencymodel into nesC in Sections 3.3 and 3.4.2Splitphase operations Because tasks execute nonpreemptively,TinyOS has no blocking operations. All longlatency operations aresplitphase operation request and completion are separate functions. Commands are typically requests to execute an operation.If the operation is splitphase, the command returns immediatelyand completion will be signaled with an event nonsplitphase operations e.g. toggle an LED do not have completion events. Atypical example of a splitphase operation is a packet send a component may invoke the send command to initiate the transmissionof a radio message, and the communication component signals thesendDone event when transmission has completed. Each component implements one half of the splitphase operation and calls theother the wiring connects both commands and events across component boundaries. We discuss how nesC captures splitphase operations in Section 3.1.Resource contention is typically handled through explicit rejection of concurrent requests. In the example above, if the communication component cannot handle multiple concurrent send operations, it signals an error when a concurrent send is attempted.Alternately, the communication component could queue the requestfor future processing.The simple concurrency model of TinyOS allows high concurrency with low overhead, in contrast to a threadbased concurrencymodel in which thread stacks consume precious memory whileblocking on a contended service. However, as in any concurrentsystem, concurrency and nondeterminism can be the source ofcomplex bugs, including deadlock e.g. the Mars Rover 22 anddata races e.g. the Therac25 27. One of the primary benefits ofnesC is helping the programmer use concurrency safely by ensuringthe absence of most data races. This is discussed in Section 3.3.2.2 Surge A Sensor Network ApplicationA common application of sensor networks is to sample a sensor periodically e.g., light or temperature and report readings to abase station, which is typically a node with a wired network connection and power source. As a running example throughout thispaper, we present Surge, a simple application that performs periodic sensor sampling and uses adhoc multihop routing over thewireless network to deliver samples to the base station. Surge isintentionally simple and does not perform advanced functions suchas innetwork data aggregation 19, 29.Surge motes organize themselves into a spanning tree rooted atthe base station. Each mote maintains the address of its parent andits depth in the tree, advertising its depth in each radio message either sensor sample or forwarded message that it transmits. A nodeselects an initial parent by listening to messages and choosing thenode with the smallest depth to seed the creation of the spanningtree, the base station periodically broadcasts beacon messages withdepth 0. Nodes estimate parent link quality when the link qualityfalls below some threshold, nodes select a new parent from theirneighbor set based on linkquality estimates and depth.Once a second, each mote samples its light sensor and sends thesample to its parent. Parents acknowledge received packets. Surgeuses the acknowledgments to provide a reliable transport layer parent link quality is calculated as the fraction of transmitted messagesthat are acknowledged. When a node receives a message from another node, it forwards the message to its parent. Sensor samplesare collected at the base station where they can be analyzed or visualized.Figure 1 shows the components of Surge and the interfaces bywhich they are wired. This example shows several important advantages of components. The Surge application only needs to include the parts of TinyOS that it needs, i.e., system boot codeSurgeTimerTimerMultihopSendMsgLedsLedsPhotoADCHWClockClockQueuedSendSendMsgGenericCommSendMsgReceiveMsgFigure 1 Simplified view of the Surge application. Nodesrepresent components, and edges represent interface wiring.Edges are labeled with the corresponding interface name.Main, the timer Timer, a sensor Photo, access to the LEDsLeds, and multihop message routing Multihop. Secondly, theapplication code  the Surge component  explicitly specifies itsenvironmental dependencies in terms of interfaces. Surge requiresa timer Timer interface, sensor ADC interface, LEDs Leds interface, and communication Send interface. The code is therefore independent of the particular sensor hardware used. For example, ADC could easily be wired to a temperature sensor rather thana light sensor, as long as the two provide the same interface.3. nesC DESIGNIn this section we discuss the primary concepts in nesCs design. First, nesC applications are built out of components withwelldefined, bidirectional interfaces. Second, nesC defines a concurrency model, based on tasks and events, and detects data racesat compile time.A few basic principles underlie nesCs designnesC is an extension of C C 23 produces efficient code for allthe target microcontrollers that are likely to be used in sensor networks. C provides all the lowlevel features necessary for accessinghardware, and interaction with existing C code is simplified. Lastbut not least, many programmers are familiar with C.C does have significant disadvantages it provides little help inwriting safe code or in structuring applications. nesC addressessafety through reduced expressive power and structure through components. None of the new features in nesC are tied to C the sameideas could be added to other imperative programming languagessuch as Modula2 52.Wholeprogram analysis nesC programs are subject to wholeprogram analysis for safety and optimization for performance.Therefore we do not consider separate compilation in nesCs design. The limited program size on motes makes this approachtractable.nesC is a static language There is no dynamic memory allocation and the callgraph is fully known at compiletime. Theserestrictions make whole program analysis and optimization significantly simpler and more accurate. They sound more onerous thanthey are in practice nesCs component model and parameterizedinterfaces eliminate many needs for dynamic memory allocation3TimerMStdControl TimerHWClockmodule TimerM provides interface StdControlinterface Timeruses interface Clock as Clk ...Figure 2 Specification and graphical depiction of the TimerMcomponent.and dynamic dispatch. We have, so far, implemented one optimization and one analysis a simple wholeprogram inliner and adatarace detector. Details are given in Section 4.nesC supports and reflects TinyOSs design nesC is based onthe concept of components, and directly supports TinyOSs eventbased concurrency model. Additionally, nesC explicitly addressesthe issue of concurrent access to shared data Section 3.3. In practice, nesC resolved many ambiguities in the TinyOS concepts ofcomponents and concurrency, and TinyOS evolved to the nesC versions as it was reimplemented.3.1 Component SpecificationnesC applications are built by writing and assembling components. A component provides and uses interfaces. These interfacesare the only point of access to the component. An interface generally models some service e.g., sending a message and is specified by an interface type. Figure 2 shows the TimerM component,part of the TinyOS timer service, that provides the StdControland Timer interfaces and uses a Clock interface all shown in Figure 3. TimerM provides the logic that maps from a hardware clockClock into TinyOSs timer abstraction Timer.Interfaces in nesC are bidirectional they contain commands andevents, both of which are essentially functions. The providers oran interface implement the commands, while the users implementsthe events. For instance, the Timer interface Figure 3 definesstart and stop commands and a fired event. In Figure 2 provided interfaces are shown above the TimerM component and usedinterfaces are below downwardpointing arrows depict commandsand upwardpointing arrows depict events. Although this same interaction between the timer and its client could have been providedvia two separate interfaces one for start and stop, and one forfired, grouping these commands and events in the same interfacemakes the specification much clearer and helps prevent bugs whenwiring components together. Splitphase operations are cleanlymodeled by placing the command request and event response inthe same interface. Figure 3 shows two examples of this. The Sendinterface has the send command and sendDone event of the splitphased packet send Section 2.1. The ADC interface is similarlyused to model splitphase sensor value reads.The separation of interface type definitions from their use incomponents promotes the definition of standard interfaces, makingcomponents more reusable and flexible. A component can provideand use the same interface type e.g., when interposing a component between a client and service, or provide the same interfacemultiple times. In these cases, the component must give each interface instance a separate name using the as notation shown for Clkin Figure 2.Components are also a clean way to abstract the boundary between hardware and software. For instance, on one sensor board,the temperature sensor accessed via a component named Tempis mostly in hardware Temp is a thin layer of software accessingonchip hardware registers. On another it is accessed over an I2Cinterface StdControl command resultt initinterface Timer command resultt startchar type, uint32t intervalcommand resultt stopevent resultt firedinterface Clock command resultt setRatechar interval, char scaleevent resultt fireinterface Send command resultt sendTOSMsg msg, uint16t lengthevent resultt sendDoneTOSMsg msg, resultt successinterface ADC command resultt getDataevent resultt dataReadyuint16t dataFigure 3 Some interface types.module SurgeM provides interface StdControluses interface ADCuses interface Timeruses interface Sendimplementation uint16t sensorReadingcommand resultt StdControl.init return call Timer.startTIMERREPEAT, 1000event resultt Timer.fired call ADC.getDatareturn SUCCESSevent resultt ADC.dataReadyuint16t data sensorReading  data... send message with data in it ...return SUCCESS...Figure 4 Simplified excerpt from SurgeM.bus Temp is implemented as a number of interacting componentsincluding a generic I2C access component. A subtle but importantpoint is that bidirectional interfaces make it very easy to supporthardware interrupts. In contrast, oneway interfaces based on procedure calls force hardware polling or having two separate interfaces for hardware operations and the corresponding interrupts.3.2 Component ImplementationThere are two types of components in nesC modules and configurations. Modules provide application code, implementing one ormore interfaces. Configurations are used to wire other componentstogether, connecting interfaces used by components to interfacesprovided by others. Every nesC application is described by a toplevel configuration that wires together the components used.The body of a module is written in Clike code, with straightforward extensions. A command or event f in an interface i is namedi.f . A command call is like a regular function call prefixed withthe keyword call, similarly an event signal is like a functioncall prefixed by signal. The definition of a commands or event4HWClockClockTimerCTimerMClockStdControl TimerStdControl Timer configuration TimerC provides interface StdControlinterface Timerimplementation components TimerM, HWClockStdControl  TimerM.StdControlTimer  TimerM.TimerTimerM.Clk  HWClock.ClockFigure 5 TinyOSs timer service the TimerC configuration.LedsLedsCStdControlSurgeMADC Timer LedsSendMsgStdControl ADCPhotoStdControlMultihopSendMsgTimerCStdControl TimerMainSurgeCStdControlFigure 6 The SurgeC configuration A toplevel configuration.named i.f is prefixed with command or event. We require theseannotations to improve code clarity. Figure 4 is a simplified excerptfrom SurgeM, which is part of the Surge application. It defines theStdControl.init command, called at boottime, and two of theevents handled by Surge the firing of the timer Timer.firedand sensor data acquisition ADC.dataReady. The code calls theTimer.start command to setup periodic timer events and theADC.getData command to request a new sensor sample. Moduleshave private state, in this example the sensorReading variable.TimerC, the TinyOS timer service, is implemented as a configuration, shown in Figure 5. TimerC is built by wiring the two subcomponents given by the components declaration TimerM fromFigure 2 and HWClock access to the onchip clock. It maps itsStdControl and Timer interfaces to those of TimerM StdControl  TimerM.StdControl, Timer  TimerM.Timer and connects the hardware clock interface used by TimerM to that provided by HWClock TimerM.Clk  HWClock.Clock. Figure 6shows a more elaborate example the toplevel configuration for theSurge application.An interface of a component may be wired zero, one or moretimes. As a result, an arbitrary number of command call expressions may be wired to a single command implementation fanin, and a single command call expression may be connected to anarbitrary number of command implementations fanout. For instance, Figure 6 shows that calls to StdControl.init in Main areconnected to four different implementations in SurgeM, Photo,TimerC and Multihop. nesC allows a fanout degree of zero nowires if the module implementer provides a default implementation for the unwired command. Fanout degrees greater than oneare allowed as long as the return type of the command is associatedwith a function for combining the results of all the calls. In the caseof StdControl.init, the result type result t Figure 3 represents success or failure. Its combining function implements thelogical AND of the results, thus the result of the call to StdControl.init in Main is success exactly if all four implementationssucceed. The analogous situations for event signal expressions ishandled identically.The explicit wiring of components via interfaces, combined withthe removal of function pointer types1, makes the controlflow between components explicit. Module variables are private and, as adesign style in TinyOS, we discourage sharing of data among components. Taken together, this makes it much easier to write correctcomponents and understand their behavior when wired in an application.Most components in TinyOS represent services such as the timeror pieces of hardware such as the LEDs and therefore exist only ina single instance. However, it is sometimes useful to create severalinstances of a component. In nesC, this is achieved by declaringan abstract component with optional parameters abstract components are created at compiletime in configurations. For instance,the QueuedSend component used by the multihop communication layer Multihop Figure 1 is an abstract component that takesa maximum retransmit count parameterabstract module QueuedSendint maxAttempts  ... configuration Multihop provides interface Sendimplementation components MultihopM,QueuedSend10 as newQueue, ... Send  MultihopM.SendMultihopM.QueuedSendMsg  newQueue.Send...3.3 Concurrency and AtomicityData races occur due to concurrent updates to shared state. In order to prevent them, a compiler must 1 understand the concurrencymodel, and 2 determine the target of every update. In this sectionwe present the concurrency model and the key invariant that thecompiler must enforce to avoid data races. We achieve tractabletarget analysis by reducing the expressive power of the languageand performing alias analysis. In particular, nesC has no dynamicmemory allocation and no function pointers.In TinyOS, code runs either asynchronously in response to aninterrupt, or in a synchronously scheduled task. To facilitate thedetection of race conditions, we distinguish synchronous and asynchronous codeAsynchronous Code AC code that is reachable fromat least one interrupt handler.Synchronous Code SC code that is only reachablefrom tasks.The runtocompletion rule and sequential execution of tasks leadimmediately to a key invariantInvariant Synchronous Code is atomic with respect toother Synchronous Code.By atomic, we mean that any shared state between the twowill be updated atomically. This essentially provides atomicity bydefault for tasks. Code that includes splitphase operations, whichby definition must include at least two tasks, is not atomic as a1At this point our implementation issues a warning when functionpointers are used.5whole, although each half is atomic. We discuss larger units ofatomicity in Section 6.Although nonpreemption allows us to avoid races among tasks,there are still potential races between SC and AC, as well as ACand AC. We claim thatClaim 1 Any update to shared state from AC is a potential race condition.Claim 2 Any update to shared state from SC that isalso updated from AC is a potential race condition.To reinstate atomicity in these cases, the programmer has twooptions either to convert all of the sharing code to tasks SC only,or to use atomic sections to update the shared state. An atomic section is a small code sequence that nesC ensures will run atomically.We present the syntax and implementation issues for atomic sections in Section 3.4. We require that any update to shared state thatis a potential race condition based on the claims above, must occurwithin an atomic section. This gives us our basic invariantRaceFree Invariant Any update to shared state is either not a potential race condition SC only, or occurswithin an atomic section.In nesC, we enforce this invariant at compile time, which avoidsmost data races. Note that this invariant only guarantees that individual accesses are racefree incorrect use of atomic sections canstill lead to race conditions.3.4 Concurrency in nesCConcurrency is central to nesC components events and commands may be signaled directly or indirectly by an interrupt, whichmakes them asynchronous code. To handle this concurrency, nesCprovides two tools atomic sections and tasks. Figure 7 illustratestheir use, showing the core logic of the Surge application. Here,Timer.fired and ADC.dataReady are asynchronous code.The Timer.fired event is signaled periodically. If Surge is notalready busy, ADC.getData is called to get a new sensor value. Asbusy is accessed in asynchronous code, its use is protected by anatomic statement that performs a testandset on busy. When thesensor value is available, the ADC.dataReady event is signaled.Sending the message with the sensor reading is not a timecriticaloperation, and the TinyOS communication layer is not designed tobe executed as asynchronous code. Therefore, SurgeM posts a task,sendData, which sends the sensor reading message. Posted tasksare executed by the TinyOS scheduler when the processor is idle.We currently implement atomic by disabling and enabling interrupts, which has very low overhead a few cycles. However,leaving interrupts disabled for a long period delays interrupt handling, which makes the system less responsive. To minimize thiseffect, atomic statements are not allowed to call commands or signal events, either directly or in a called function. This confines thecode executed by an atomic statement to a single module, makingit possible for the module implementer to bound atomic statementexecution time.As discussed in Section 3.3, atomic prevents concurrent accessto the shared data accessed in the statement. Given atomic sections,we define the key rule that enforces our racefree invariantIf a variable x is accessed by AC, then any access of xoutside of an atomic statement is a compiletime error.The compiler reveals the specific conflict in addition to signalingthe error. To remove the error, the programmer must either add anatomic section, or move the offending code into a task.module SurgeM  ... implementation bool busynorace uint16t sensorReadingevent resultt Timer.fired bool localBusyatomic localBusy  busybusy  TRUEif localBusycall ADC.getDatareturn SUCCESStask void sendData   send sensorReadingadcPacket.data  sensorReadingcall Send.sendadcPacket, sizeof adcPacket.datareturn SUCCESSevent resultt ADC.dataReadyuint16t data sensorReading  datapost sendDatareturn SUCCESS...Figure 7 Concurrency and atomicity in SurgeM. Changes fromFigure 4 are highlighted.There are some cases in which there is a potential data race on avariable that the programmer knows is not an actual data race, forinstance sensorReading in Figure 7. The declaration of sensorReading includes the norace qualifier to suppress errors aboutthis particular false positive. This avoids uselessly protecting allaccesses to sensorReading with atomic statements. Section 4.2shows that we can effectively detect a large number of data races.3.5 Parameterized InterfacesParameterized interfaces are nesCs mechanism for introducingruntime command and event dispatch within a first order language.A component declares an interface with a parameter list which creates a separate interface for each tuple of parameter values. Parameterized interfaces are used to model Active Messages 47 inTinyOS in Active Messages, packets contain a numeric identifierthat specifies which event handler should be executed. Figure 8shows a very simplified definition of the communication component GenericComm. Wires to a parameterized interface mustspecify a specific interface with a compiletime constant the multihop message routing uses Active Message 42 for all its communication, hence wires to GenericComm.Send42 in Figure 8.In a module, the implemented commands and events of a parameterized interface receive extra parameters specifying the selectedinterface see Send.send in the ActiveMessages module of Figure 8 and select a specific interface when invoking a command orevent in a parameterized interface see sendComplete. This lastconstruction translates into a runtime dispatch to one of the functions connected to Send.sendDone by the applications configurations. However the set of possible dispatch targets is explicitlyspecified by the programs configurations. An additional benefit isthat no RAM is needed to store the active message dispatch table.4. EVALUATIONIn this section we evaluate nesCs component model, concurrency model, and wholeprogram inlining with respect to a set ofrepresentative TinyOS applications, including Surge, TinyDB and6module GenericComm   id is the Active Message IDprovides interface Senduint8t idprovides interface Receiveuint8t id implementation TOSMsg msgcommand resulttSend.senduint8t iduint8t length, TOSMsg data dataamId  id msg  data ... void sendCompleteTOSMsg packet signal Send.sendDonemsgamIdmsg, SUCCESS...configuration Multihop  ... implementation components QueuedSend10 as newQueue, GenericComm,...newQueue.RealSend  GenericComm.Send42Figure 8 Active Messages using Parameterized Interfaces.Mate. We have implemented a compiler for nesC that generates asingle C source file for a whole application, resolving all interfaceconnections to direct function calls. In the results below, this C fileis compiled with gcc 3.1.1 for the Atmel ATmega 103, an 8bitRISC microprocessor used on Mica mote Table 1.4.1 Component ModelAnecdotally, nesCs component model has been invaluable foreventdriven sensor applications. The success of the componentmodel is shown by the way in which components are used in theTinyOS code applications are small, and make use of a large number of reusable components. Moreover, nesCs component modelmakes it possible to pick and choose which parts of the OS areincluded with each application.For the purposes of the discussion below, we divided the TinyOSsource into application code and core OS code. We consider coreOS code to be fundamental things such as the TinyOS schedulerand radio stack as well as commonly used support routines fortimers, sensor management, etc. All other code is considered to beapplication specific. Additionally, we restricted our calculations toone hardware platform to avoid doublecounting hardware supportroutines.The core TinyOS source consists of 172 components, of which108 are code modules and 64 are configurations. The number ofmodules per application in the TinyOS 1.x release ranges from8 to 67, with an average of 24. Modules are generally quite small on average only 120 lines of code. This small size indicates theexpressive power of nesCs components, as programmers have notneeded to break the model by producing large monolithic components, which are more difficult to analyze and more error prone.Figure 9 illustrates the effectiveness of the component model forour three sample applications. It shows the total number of modules and lines of code for each application, and the number of thesemodules and lines of code that come from OS components. We alsoinclude with the OS numbers the fraction of the OS each application ended up including. These numbers show that only a smallportion of the OS is used by each application. nesCs componentmodel restricts the code included in each application to the bareminimum, hence reducing the applications memory footprint.nesCs bidirectional interfaces are an excellent fit for eventdrivensystems, since they provide a clean syntax for grouping relatedcomputation in the presence of splitphase and asynchronous operations. As evidence of this, bidirectional interfaces are pervasiveApplication Modules OS Modules Lines OS Lines of full OS  of full OSSurge 31 27 25 2860 2160 14Mate 35 28 25 4736 2524 17TinyDB 65 38 35 11681 4160 28Figure 9 Application component and OS use breakdown.Application Task Event  interruptcount count codeSurge 7 221 64Mate 13 317 41TinyDB 18 722 57Figure 10 Application concurrency summary.in TinyOS of the 172 components in the TinyOS core, 74 haveat least one bidirectional interface.4.2 ConcurrencynesCs component model makes it simple to express the complex concurrent actions in sensor network applications. Figure 10gives the number of tasks and event handlers for each sample application each of which represents a potentially concurrent activity.Figure 10 also gives the percentage of code measured in bytes thatis reachable from an interrupt context. These numbers demonstratethat these applications have a high degree of concurrency.Our implementation of race detection uses a simple typebasedalias analysis to detect which variables are accessed by asynchronous code. We report errors if any of these variables are accessedoutside atomic sections.Initially, nesC included neither an explicit atomic statement northe analysis to detect potential race conditions TinyOS and its applications had many data races. Once race detection was implemented, we analyzed every application in the TinyOS source tree,finding 156 variables that potentially had a race condition. Of these,53 were false positives discussed below and 103 were genuinerace conditions, a frequency of about six per thousand code statements. We fixed each of these bugs by moving code into tasks orby using atomic statements. We tested each TinyOS applicationand verified that the presence of atomic sections has not interferedwith correct operation.Figure 11 shows the locations of data races in the TinyOS tree.Component Type Datarace variablesRandomLFSR System 1UARTM System 1AMStandard System 2AMPromiscious System 2BAPBaseM Application 2ChirpM Application 2MicaHighSpeedRadioM System 2TestTimerM Application 2ChannelMonC System 3NoCrcPacket System 3OscilloscopeM Application 3QueuedSend System 3SurgeM Application 3SenseLightToLogM Application 3TestTemp Application 3MultihopM System 10eepromM System 17TinyAlloc System 18IdentC Application 23Total 103Figure 11 Component locations of race condition variables.7 Contains a race   Fixed version if state  IDLE  uint8t oldStatestate  SENDING atomic count oldState  state send a packet if state  IDLE  state  SENDINGif oldState  IDLE count send a packetFigure 12 Fixing a race condition in a state transition.Half of the races existed in systemlevel components used by manyapplications, while the other half were application specific. Modules MultihopM, eepromM, and TinyAlloc had a disproportionatenumber of races due to the amount of internal state they maintainthrough complex concurrent operations. IdentC tracks node interactions, records them in flash, and periodically sends them to thebasestation it has complex concurrency, lots of state, and was written before most of the concurrency issues were well understood.The finitestatemachine style of decomposition in TinyOS led tothe most common form of bug, a nonatomic state transition. Statetransitions are typically implemented using a readmodifywrite ofthe state variable, which must be atomic. A canonical example ofthis race is shown in Figure 12, along with the fix.The original versions of the communication, TinyAlloc andEEPROM components contained large numbers of variable accessesin asynchronous code. Rather than using large atomic sections,which might decrease overall responsiveness, we promoted manyof the offending functions to synchronous code, by posting a fewadditional tasks.False positives fell into three major categories statebased guards,buffer swaps, and causal relationships. The first class, statebasedguards, occurred when access to a module variable is serialized atrun time by a state variable. The state transition example of Figure 12 illustrates this in this function, the variable count is safedue to the monitor created by state.TinyOSs communication primitives use a bufferswapping policy for memory management. When a network packet is received,the radio component passes a buffer to the application the application returns a separate buffer to the component for the next receiveevent. This raises problems with alias analysis although only onecomponent has a reference to a given buffer at any time, components swap them back and forth. Although alias analysis wouldconclude that both components could concurrently read and writethe same buffer, swapping ensures that only one component hasa reference to it at any time. To resolve this issue, we annotatedmessage buffers with the norace qualifier.The last class of false positives, causal relationships, comes fromthe splitphase operation of TinyOS components. The commandand event pair of a splitphase operation might share a variable, buttheir causal relationship means that the event will not fire while thecommand is executing. The event could fire during the commandonly if another TinyOS component accessed the underlying hardware although this would violate the TinyOS programming model,nesC does not enforce this limitation.These last two classes of false positives are actually high risk,and therefore worth verifying by hand. The absence of data racesdepends on strict use of the two idioms it is easy to access messagebuffers concurrently by accident, and it is also possible to have agiven splitphase operation running concurrently with itself breaking the assumption that the two phases do not overlap.App Code size Code Data size CPUinlined noninlined reduction reductionSurge 14794 16984 12 1188 15Mate 25040 27458 9 1710 34TinyDB 64910 71724 10 2894 30Figure 13 Effect of inlining on application footprint and performance. All sizes are in bytes. The CPU reduction columnshows the reduction in CPU cycles spent executing tasks withoptimizations enabled.Cycles Optimized Unoptimized ReductionWork 371 520 29Boundary crossing 109 258 57Noninterrupt 8 194 95Interrupt 101 64 36Total 480 778 38Figure 14 Optimization effects on timer event handling. Thisfigure shows the breakdown, in CPU cycles, for both work andboundary crossing for timer event handling, which requires7 module crossings. Optimization reduces the overall cyclecount by 38.We hope that we will be able to reduce the number of false positives by extending nesC with ideas from the static checking of software protocols in Vault 6.4.3 OptimizationAs with datarace detection, nesC exploits the restrictions of thecomponent model to perform static analysis and optimization. ThenesC compiler uses the application callgraph to eliminate unreachable code and module boundary crossings as well as to inline smallfunctions. This allows extensive crosscomponent optimizations,including constant propagation and common subexpression elimination, to be performed by the backend C compiler. These optimizations greatly reduce memory footprint and execution overheads, which are key savings for embedded systems. We evaluate the effectiveness of our inlining by first giving overall reductions in code size and CPU usage for our three representative applications, including a detailed componentbycomponent codesizebreakdown for Surge. Then we show, by analysis of a single butimportant code path, that our inlining is effective at reducing theoverhead of the nesC component system.Figure 13 shows for each of our three applications the code sizein bytes with and without these optimizations, the data sizes, andthe decrease in execution cycles spent in tasks obtained by inlining.Because the networking stack dominates execution time in our applications, we show the reduction in CPU cycles for time spent executing tasks, nearly all of which are applicationlevel code. nesCsoptimizations lead to a 1534 reduction in execution time, and a915 reduction in overall footprint.Figure 14 gives a breakdown of the CPU cycle reduction for handling a timer event, which requires crossing 7 module boundaries.nesCs optimizations save not only 57 of the boundary crossingoverhead, but also 29 of the work required to handle the timerevent, for a total savings of 38. The increase in boundary crossing overhead for the interrupt occurs because the inlining requiresthe handler to save more registers however, the total time spent inthe handler goes down.Figure 15 provides a detailed breakdown of the code and datasize for each component in the Surge application. The TinyOS rowrepresents the core TinyOS initialization code and task scheduler,which fits into 400 bytes. C Runtime represents necessary runtime8Component Code size Data sizeSizes in bytes inlined noninlinedRuntimeTinyOS 368 646 32C Runtime 1068 1078 13RealMain  72 0Application componentsSurgeM 80 240 44Multihop communicationAMPromiscuous 456 654 9MultihopM 2646 2884 223NoCRCPacket 370 484 50QueuedSend 786 852 461Radio stackChannelMonC 454 486 9CrcFilter  34 0MicaHighSpeedRadioM 1162 1250 61PotM 50 82 1RadioTimingC 42 56 0SecDedEncoding 662 684 3SpiByteFifoC 344 438 2Sensor acquisitionADCM 156 260 2PhotoTempM 226 320 2MiscellaneousNoLeds  18 0RandomLFSR 134 134 6TimerM 1964 1958 168AbsoluteTimerM 1338 1276 50LogicalTimeM 1758 1806 32SysTimeC 90 110 2Hardware presentationHPLADCC 214 268 11HPLClock 108 132 4HPLInit  10 0HPLInterrupt  22 0HPLPotC  66 0HPLSlavePinC  28 0HPLUARTM 160 212 0LedsC  164 1SlavePinM 80 124 1UARTM 78 136 1Totals 14794 16984 1188Figure 15 Breakdown of code and data size by component forthe Surge application. A  in the inlined column indicatesthat the corresponding component was entirely inlined.routines, including floatingpoint libraries currently used by multihop routing. Note that inlining increases code size slightly for onlytwo components  TimerM and AbsoluteTimerM  but reduces oreliminates many others, leading to an overall reduction in footprint.5. RELATED WORKThe module systems of languages such as Modula2 plus descendants 12, 52, 53 and Ada 20 explicitly import and exportinterfaces. However these systems are less flexible than nesC, asthere is no explicit binding of interfaces an exported interface Iis automatically linked to all importers of I . Standard MLs module system offers similar functionality to nesC, except that circularcomponent structure assemblies cannot be expressed. ThenesC module system is very close to Mesas 35, and coincidentally uses essentially the same terminology modules contain executable code, configurations connect components configurationsor modules by binding their interfaces, whose interface types mustmatch.The Giotto 13, Esterel 4, Lustre 11, Signal 3 and EFRP 48languages target embedded, hard realtime, control systems. Theyexplicitly model the concept of input events and output control signals, and offer much stronger time guarantees than nesC.However, they are not generalpurpose programming languages inwhich one would implement, e.g., a multihop radio stack.The VHDL 18 hardware description language is based on assembling components architecture with welldefined interfaces.Architectures are often specified as independent processes architectures that are built out of components always encapsulate theinner components. In contrast, nesC configurations do not encapsulate the components they use, and concurrency crosses component boundaries. VHDLs model matches that of actual hardware,while nesCs is inspired by hardware. Another language targetedto generating hardware is SAFL 38, a firstorder functional language. To allow hardware generation, SAFL has no recursion andstatic data allocation.Distributed systems 14, 21, 40, 45, including the Corba Component Model 41 and Microsofts COM 34, and software engineering 2 often model systems as interacting sets of components.These components are specified by the interfaces they provide oruse. However, the focus is very different from nesC componentsare largescale e.g., a database, dynamically loaded andor linked,and possibly accessed remotely. The component models are moreheavyweight than nesCs, and do not include all the features required for TinyOS.ArchJava 1 has bidirectional interfaces ports. There are nointerface types port connections match methods by name and signature, fanout is limited to methods with no result, and dispatch ofport methods is dynamic. C 32 and BCOOPL 5 support eventsin classes and interfaces. However the two directions are asymmetric registration of an event handler with an event producer isdynamic.The languages above, and languages commonly used for programming embedded systems such as C, Ada, Forth, do not offerthe set of features desired in nesC an interruptbased concurrencymodel, lowlevel hardware access, componentbased programmingand static concurrency checking.A number of operating systems have explored the use of component architectures. The Flux OSKit 10 is a collection of components for building operating systems, but provided componentstarget workstationlike machines. Flux was subsequently reimplemented with the help of Knit 43, a language for constructing andlinking components implemented in C. Knits component model,based on that of units 9, is similar to nesC in that components provide and use interfaces and that new components can be assembledout of existing ones. Unlike nesC, Knit lacks bidirectional interfaces and data race detection. THINK 8 takes the Flux model onestep further by allowing explicit modeling of calls between components. This allows clean linking of THINK components acrossprotection domains or networks. THINK does not employ wholeprogram optimization and relies on dynamic dispatch. We do notbelieve this model, with its associated runtime cost, is appropriate for network embedded systems. Other componentoriented systems include Click 36, Scout 37, and the xkernel 17. Thesesystems are more specialized than nesC and do not support wholeprogram optimization apart from several optimizations in Click24 or bidirectional interfaces.Traditional realtime and embedded operating systems, such asVxWorks 51, QNX 42, and WinCE 33, differ from TinyOSin a number of respects. These systems are generally much largerand provide greater functionality than TinyOS, and are intended forlargerscale embedded systems. We refer the reader to TinyOS 16for a thorough discussion of the differences among these systems.There have been a few attempts at static detection of race conditions. ESC 7 is in between a type checker and a program verifierit has been used to verify usersupplied associations between locksand variables, and also to enforce ordering constraints on lock acquisition. Suns LockLint 46 statically checks for inconsistent9use of locks or lock ordering. As expected, these tools have troublewith firstclass functions and aliasing, and tend to report a subset ofthe errors with false positives as well. They also focus on lockingrather than atomicity we chose the latter to enable more freedomof implementation, which is particularly important for interrupts.The next section covers the use of monitors in Mesa.There are also tools for dynamic detection of races. Eraser 44detects unprotected shared variables using a modified binary. Onthe fly race detectors 31, 39 serialize all accesses to a variable toverify serializability. These approaches only catch errors that actually occur during the test run. In addition, dynamic approaches areless appealing for motes due to their number, resource limitations,and UI constraints. All of these race detection systems, includingnesC, validate individual variable accesses. They cannot detect areadmodifywrite through a temporary variable in which the readand write occur in distinct atomic sections.6. DISCUSSION AND FUTURE WORKThe nesC language is wellsuited to the unique challenges of programming networked embedded systems. nesC was originally designed to express the concepts embodied in TinyOS, and by reimplementing the operating system in nesC, those concepts were refined. nesCs component model supports holistic system design bymaking it easy to assemble applications that include only the necessary OS support. The component model also allows alternate implementations and a flexible hardwaresoftware boundary. nesCsconcurrency model allows us to write highly concurrent programson a platform with very limited resources the use of bidirectionalinterfaces and atomic statements permit a tight integration of concurrency and componentoriented design. Careful restrictions onthe programming model, including the lack of dynamic allocationand explicit specification of an applications callgraph, facilitatewholeprogram analyses and optimizations. Aggressive inlining reduces both memory footprint and CPU usage, and static dataracedetection allows the developer to identify and fix concurrency bugs.The nesC design opens up several key areas for future work.These broadly fall into the areas of concurrency support, enhancements to language features, and application to domains other thannetworked embedded systems.Concurrency supportThe nesC concurrency model provides short atomic actions, whichcan be used to build higherlevel synchronization mechanisms suchas semaphores, condition variables, atomic queues, and locks. Someof these mechanisms imply blocking, but there is nothing in the language per se that prevents support for blocking we would need toprohibit blocking calls in atomic sections as well as treat blockingcalls as yield points for task scheduling.Our current implementation of atomic sections, which workswell for embedded systems, is to disable interrupts. This is acceptable in part because we prevent blocking and limit the lengthof atomic sections. It also depends on the assumption of a uniprocessor and on the lack of virtual memory, since a page fault shouldnot occur within an atomic section. However, these assumptionscan be relaxed by considering alternate implementations of atomicsections, for example, using nonblocking synchronization primitives 15.The use of monitors in Mesa 26 is the most directly comparable concurrency model to ours. In Mesa, the authors consideredand rejected atomicity based on nonpreemption for several reasons, including the desire to support multiprocessors and virtualmemory. Also, nonpreemption alone does not handle interruptswe use atomic sections to handle asynchronous updates to sharedstate. Finally, Mesa was unable to prevent calls within atomic sections from yielding the processor. This is not an issue for nesC.Language enhancementsThere are a number of idioms common in TinyOS that are notwell expressed in nesC. Multiclient services with perclient stateare not wellsupported. For example, consider a general timer service where each client wishes to receive timer events at a differentfrequency. Abstract components can be used for this purpose, although they are currently limited in that the internal state of eachinstance is private to that instance. We currently use parameterizedinterfaces to implement such multiclient services, where the parameter corresponds to the client number. We are not wholly satisfied with this approach, and we plan to investigate a better mechanism in the future.Splitphase operations provide high concurrency with low overhead, but are difficult to program reintroducing the convenienceof a threaded model would greatly simplify programming. By automatically transforming blocking operations into splitphase calls,we could preserve expressive lightweight concurrency without forcing the programmer to manually build continuations within components as they do now. As it stands, many components are written as small finite state machines atomic state transitions result inreplicated control flow, separating state transitions from their corresponding actions. A future direction for nesC is to provide explicit support for FSMstyle decomposition that simplifies component design and allows properties of FSM behavior to be staticallyverified.Application to other platformsWe believe that nesC is not limited to the domain of embeddedsystems. nesCs componentoriented structure, focus on concurrency, and bidirectional interfaces are valuable concepts in programming larger systems, such as enterpriseclass applications andInternet services. To effectively support this broader class of applications, several extensions to nesC are needed. First, nesCscompiletime analyses would need to be extended to handle dynamic memory and component allocation, as well as patterns suchas message buffer swap. The static checking of software protocols in Vault 6 may provide an approach to solving these problems and may also help in reducing false positives in data racedetection. nesCs concurrency model should be extended to admitmultiprocessors, blocking operations, and a more general notion ofthreads, as discussed above. Such an approach would lead to a richset of concurrency primitives specifically tailored for componentoriented programming of largescale systems.AcknowledgementsThis work was supported, in part, by the Defense Department Advanced Research Projects Agency grants F3361501C1895 andN66019928913, the National Science Foundation under GrantNo. 0122599, California MICRO program, and Intel Corporation.Research infrastructure was provided by the National Science Foundation grant EIA9802069.7. REFERENCES1 J. Aldrich, C. Chambers, and D. Notkin. Architectural Reasoning inArchJava. In European Conference on Object Oriented ProgrammingECOOP, June 2002.2 F. Bachmann, L. Bass, C. Buhrman, S. CornellaDorda, F. Long,J. Robert, R. Seacord, and K. Wallnau. Volume II TechnicalConcepts of ComponentBased Software Engineering, 2nd Edition.Technical Report CMUSEI2000TR008, Carnegie MellonSoftware Engineering Institute, May 2000.103 A. Benveniste, P. L. Guernic, and C. Jacquemot. Synchronousprogramming with events and relations the SIGNAL language andits semantics. Science of Computer Programming, 162103149,Sept. 1991.4 F. Boussinot and R. de Simone. The ESTEREL Language.Proceedings of the IEEE, 79912931304, Sept. 1991.5 H. de Bruin. BCOOPL A Langage for Controlling ComponentInteractions. Journal of Supercomputing, 2002. Accepted forpublication.6 R. Deline and M. Fahndrich. Enforcing Highlevel Protocols inLowLevel Software. In Proceedings of the ACM SIGPLAN 01Conference on Programming Language Design and Implementation,pages 5969, June 2001.7 D. L. Detlefs, K. R. M. Leino, G. Nelson, and J. B. Saxe. Extendedstatic checking. Technical Report 159, Palo Alto, USA, 1998.8 J.P. Fassino, J.B. Stefani, J. Lawall, and G. Muller. THINK ASoftware Framework for Componentbased Operating SystemKernels. In Proceedings of Usenix Annual Technical Conference,June 2002.9 M. Flatt and M. Felleisen. Units Cool modules for HOT languages.In Proceedings of the ACM SIGPLAN 98 Conference onProgramming Language Design and Implementation, pages236248, 1998.10 B. Ford, G. Back, G. Benson, J. Lepreau, A. Lin, and O. Shivers. TheFlux OSKit A Substrate for Kernel and Language Research. InProceedings of the Symposium on Operating Systems Principles,pages 3851, 1997.11 N. Halbwachs, P. Caspi, P. Raymond, and D. Pilaud. Thesynchronous dataflow programming language LUSTRE.Proceedings of the IEEE, 79913051320, September 1991.12 S. P. Harbison. Modula3. Prentice Hall, 1991.13 T. A. Henzinger, B. Horowitz, and C. M. Kirsch. Embedded ControlSystems Development with Giotto. In Proceedings of the ACMWorkshop on Languages, Compilers and Tools for EmbeddedSystems LCTES, pages 6472, June 2001.14 A. Herbert. An ANSA Overview. IEEE Network, 811823, 1994.15 M. Herlihy. A methodology for implementing highly concurrent dataobjects. ACM Transactions on Programming Languages andSystems, 155745770, November 1993.16 J. Hill, R. Szewczyk, A. Woo, S. Hollar, D. E. Culler, and K. S. J.Pister. System Architecture Directions for Networked Sensors. InArchitectural Support for Programming Languages and OperatingSystems, pages 93104, 2000. TinyOS is available athttpwebs.cs.berkeley.edu.17 N. C. Hutchinson and L. L. Peterson. Design of the xkernel. InProceedings of SIGCOMM 88, pages 6575, Aug. 1988.18 IEEE Standard 10762002. VHDL Language Reference Manual.19 C. Intanagonwiwat, D. Estrin, R. Govindan, and J. Heidemann.Impact of network density on data aggregation in wireless sensornetworks. In Proceedings of International Conference on ComputingSystems ICDCS, July 2002.20 International Organisation for Standardization. Ada 95 ReferenceManual, Jan. 1995.21 ISOIEC International Standard 107463. ODP Reference ModelArchitecture, 1995.22 M. Jones. What really happened on mars rover pathfinder. The RisksDigest, 1949.23 B. W. Kernighan and D. M. Ritchie. The C Programming Language,Second Edition. Prentice Hall, 1988.24 E. Kohler, B. Chen, M. F. Kaashoek, R. Morris, and M. Poletto.Programming language techniques for modular router configurations.Technical Report MITLCSTR812, MIT Laboratory for ComputerScience, Aug. 2000.25 Lab Notes Research from the College of Engineering, UC Berkeley.Smart buildings admit their faults., 2001.httpcoe.berkeley.edulabnotes1101.smartbuildings.html.26 B. W. Lampson and D. D. Redell. Experience with processes andmonitors in mesa. In Proceedings of the 7th ACM Symposium onOperating Systems Principles SOSP, pages 4344, 1979.27 N. Leveson and C. S. Turner. An investigation of the therac25accidents. IEEE Computer, 2671841, July 1993.28 P. Levis and D. Culler. Mate A Tiny Virtual Machine for SensorNetworks. In Proceedings of the ACM Conference on ArchitecturalSupport for Programming Languages and Operating SystemsASPLOS, Oct. 2002.29 S. R. Madden, M. J. Franklin, J. M. Hellerstein, and W. Hong. TAGa Tiny AGgregation Service for AdHoc Sensor Networks. InProceedings of the ACM Symposium on Operating System Designand Implementation OSDI, Dec. 2002.30 A. Mainwaring, J. Polastre, R. Szewczyk, D. Culler, and J. Anderson.Wireless Sensor Networks for Habitat Monitoring. In Proceedings ofthe ACM International Workshop on Wireless Sensor Networks andApplications, Sept. 2002.31 J. MellorCrummey. Onthefly detection of data races for programswith nested forkjoin parallelism. In Proc. of Supercomputing 91,pages 2433, 1991.32 Microsoft C Language Specification. Microsoft Press. ISBN0735614482.33 Microsoft. Windows CE. httpwww.microsoft.com.34 OLE2 Programmers Reference, Volume One. Microsoft Press, 1994.35 J. Mitchell. Mesa language manual. Technical Report CSL793,Xerox PARC, 1979.36 R. Morris, E. Kohler, J. Jannotti, and M. F. Kaashoek. The ClickModular Router. In Proceedings of the ACM Symposium onOperating Systems Principles SOSP, pages 217231, 1999.37 D. Mosberger and L. L. Peterson. Making Paths Explicit in the ScoutOperating System. In Operating Systems Design andImplementation, pages 153167, 1996.38 A. Mycroft and R. Sharp. A Statically Allocated Parallel FunctionalLanguage. In Proceedings of the Internal Conference on Automata,Languages and Programming ICALP, pages 3748, 2000.39 R. H. B. Netzer. Race condition detection for debuggingsharedmemory parallel programs. Technical ReportCSTR19911039, 1991.40 Object Management Group. Common Object Request BrokerArchitecture. Available at httpwww.omg.org.41 Object Management Group. CORBA Component Model CCMSpecification. Available at httpwww.omg.org.42 QNX Software Systems, Ltd, Kanata, Ontatio, Canada.httpwww.qnx.com.43 A. Reid, M. Flatt, L. Stoller, J. Lepreau, and E. Eide. KnitComponent composition for systems software. In Proceedings of the4th ACM Symposium on Operating System Design andImplementation, pages 347360, Oct. 2000.44 S. Savage, M. Burrows, G. Nelson, P. Sobalvarro, and T. Anderson.Eraser A dynamic data race detector for multithreaded programs.ACM Transactions on Computer Systems, 154391411, 1997.45 Sun Microsystems. Enterprise Java Beans. Available athttpjava.sun.comejb.46 SunSoft. Sun Workshop 5.0 Collection Chapter 5, Lock AnalysisTool, 2000.47 T. von Eicken, D. E. Culler, S. C. Goldstein, and K. E. Schauser.Active Messages A Mechanism for Integrated Communication andComputation. In Proceedings of the International Symposium onComputer Architecture, pages 256266, 1992.48 Z. Wan, W. Taha, and P. Hudak. EventDriven FRP. In Proceedingsof the Internation Symposium on Principles of DeclarativeLanguages PADL, volume 2257, pages 155172, 2001.49 B. Warneke, M. L. andl B. Liebowitz, and K. Pister. Smart dustCommunicating with a cubicmillimeter computer. IEEE ComputerMagazine, pages 4451, January 2001.50 K. Whitehouse and D. Culler. Calibration as Parameter Estimation inSensor Networks. In Proceedings of the ACM InternationalWorkshop on Wireless Sensor Networks and Applications, Sept. 2002.51 Wind River Systems, Inc, Alameda, CA, USA.httpwww.vxworks.com.52 N. Wirth. Programming in Modula2. Springer Verlag, 1992.53 N. Wirth and M. Reiser. Programming in Oberon  Steps BeyondPascal and Modula. AddisonWesley, 1992. ISBN 0201565439.11
