System Architecture Directions for Networked SensorsJason Hill, Robert Szewczyk, Alec Woo, Seth Hollar, David Culler, Kristofer PisterApril 27, 2000AbstractTechnological progress in integrated, lowpower, CMOS communication devices and sensors makesa rich design space of networked sensors viable. They can be deeply embedded in the physical worldor spread throughout our environment. The missing elements are an overall system architecture and amethodology for systematic advance. To this end, we identify key requirements, develop a small devicethat is representative of the class, design a tiny eventdriven operating system, and show that it providessupport for efficient modularity and concurrencyintensive operation. Our operating system fits in 178bytes of memory, propagates events in the time it takes to copy 1.25 bytes of memory, context switchesin the time it takes to copy 6 bytes of memory and supports two level scheduling. The analysis lays agroundwork for future architectural advances.1 IntroductionAs the postPC era emerges, several new niches of computer system design are taking shape with characteristics that are quite different from traditional desktop and server regimes. One of the most interesting ofthese new design regimes is networked sensors. The networked sensor is enabled, in part, by Moores Lawpushing a given level of computing and storage into a smaller, cheaper, lowerpower unit. However, threeother trends are equally important complete systems on a chip, integrated lowpower communication, andintegrated lowpower devices that interact with the physical world. The basic microcontroller building blockincludes not just memory and processing, but nonvolatile memory and interface resources, such as DAC,ADCs, UARTs, interrupt controllers, and counters. Communication may take the form of wired, shortrangeRF, infrared, optical, or various techniques 18. Sensors interact with various fields and forces to detectlight, heat, position, movement, chemical presence, and so on. In each of these areas, the technology iscrossing a critical threshold that makes networked sensors an exciting regime to apply systematic designmethods.Today, networked sensors can be constructed using commercial components on the scale of a square inch insize and a fraction of a watt in power, using one or more microcontrollers connected to various sensor devices,using I2C, SPI, or device specific protocols, and to small transceiver chips. One such sensor is described inthis study. It is also possible to construct the processing and storage equivalent of a 90s PC on the orderof 10 in2 and 510 watts of power. Simple extrapolation suggests that the equivalent will eventually fit inthe square inch and subwatt spacepower niche and will run a scaled down Unix 12, 42 or an embeddedmicrokernel 13, 26. However, many researchers envision driving the networked sensor down into microscopicscales, as communication becomes integrated onchip and microelectrical mechanical MEMS devices makea rich set of sensors available on the same CMOS chip at extremely low cost 38, 5. networked sensors willbe integrated into their physical environment, perhaps even powered by ambient energy 32, and used inmany smart space scenarios. Others see ramping up the power associated with oneinch devices dramatically.In either scenario, it is essential that the network sensor design regime be subjected to the same rigorous,workloaddriven, quantitative analysis that allowed microprocessor performance to advance so dramaticallyover the past 15 years. It should not be surprising that the unique characteristics of this regime give rise tovery different design tradeoffs than current generalpurpose systems.1This paper provides an initial exploration of system architectures for networked sensors. The investigationis grounded in a prototype current generation device constructed from offtheshelf components. Otherresearch projects 38, 5 are trying to compress this class of devices onto a single chip however, the keymissing technology is the system support to manage and operate the device. To address this problem, wehave developed a tiny microthreaded OS, called TinyOS, on the prototype platform. It draws strongly onprevious architectural work on lightweight thread support and efficient network interfaces. While workingin this design regime two issues emerge strongly these devices are concurrency intensive  several differentflows of data must be kept moving simultaneously, and the system must provide efficient modularity hardware specific and application specific components must snap together with little processing and storageoverhead. We address these two problems in the context of current network sensor technology and our tinymicrothreaded OS. Analysis of this solution provides valuable initial directions for architectural innovation.Section 2 outlines the design requirements that characterize the networked sensor regime and guide ourmicrothreading approach. Section 3 describes our baseline, currenttechnology hardware design point. Section 4 develops our TinyOS for devices of this general class. Section 5 evaluates the effectiveness of the designagainst a collection of preliminary benchmarks. Section 6 contrasts our approach with that of prevailingembedded operating systems. Finally, Section 7 draws together the study and considers its implications forarchitectural directions.2 Networked Sensor CharacteristicsThis section outlines the requirements that shape the design of network sensor systems these observationsare made more concrete by later sections.Small physical size and low power consumption At any point in technological evolution, size and powerconstrain the processing, storage, and interconnect capability of the basic device. Obviously, reducing thesize and power required for a given capability are driving factors in the hardware design. At a system level,the key observation is that these capabilities are limited and scarce. This sets the background for the restof the solution, which must be austere and efficient.Concurrencyintensive operation The primary mode of operation for these devices is to flow information fromplace to place with a modest amount of processing onthefly, rather than to accept a command, stop, think,and respond. For example, information may be simultaneously captured from sensors, manipulated, andstreamed onto a network. Alternatively, data may be received from other nodes and forwarded in multihoprouting or bridging situations. There is little internal storage capacity, so buffering large amounts of databetween the inbound and the outbound flows is unattractive. Moreover, each of the flows generally involve alarge number of lowlevel events interleaved with higherlevel processing. Some of these events have realtimerequirements, such as bounded jitter some processing will extend over many such timecritical events.Limited Physical Parallelism and Controller Hierarchy The number of independent controllers, the capabilities of the controllers, and the sophistication of the processormemoryswitch level interconnect are muchlower than in conventional systems. Typically, the sensor or actuator provides a primitive interface directly to a singlechip microcontroller. In contrast, conventional systems distribute the concurrent processingassociated with the collection of devices over multiple levels of controllers interconnected by an elaboratebus structure. Although future architectural developments may recreate a low dutycycle analog of theconventional federation of controllers and interconnect, space and power constraints and limited physicalconfigurability onchip are likely to retain the need to support concurrencyintensive management of flowsthrough the embedded microprocessor.Diversity in Design and Usage Networked sensor devices will tend to be application specific, rather thangeneral purpose, and carry only the available hardware support actually needed for the application. Asthere is a wide range of potential applications, the variation in physical devices is likely to be large. Onany particular device, it is important to easily assemble just the software components required to synthesizethe application from the hardware components. Thus, these devices require an unusual degree of software2modularity that must also be very efficient. A generic development environment is needed which allows specialized applications to be constructed from a spectrum of devices without heavyweight interfaces. Moreover,it should be natural to migrate components across the hardwaresoftware boundary as technology evolves.Robust Operation These devices will be numerous, largely unattended, and expected to be operational a largefraction of the time. The application of traditional redundancy techniques is constrained by space and powerlimitations. Although redundancy across devices is more attractive than within devices, the communicationcost for cross device redundancy is prohibitive. Thus, enhancing the reliability of individual devices isessential. This reinforces the need for efficient modularity the components should be as independent aspossible and connected with narrow interfaces.3 Example Design PointTo ground our system design study, we have developed a small, flexible networked sensor platform thatexpresses many of the key characteristics of the general class and represents the various internal interfacesusing currently available components 34. A photograph and schematic for the hardware configuration ofthis device appear in Figure 1. It consists of a microcontroller with internal flash program memory, dataSRAM and data EEPROM, connected to a set of actuator and sensor devices, including LEDs, a lowpowerradio transceiver, an analog photosensor, a digital temperature sensor, a serial port, and a small coprocessorunit. While not a breakthrough in its own right, this prototype has been invaluable in developing a feel forthe salient issues in this design regime.3.1 Hardware OrganizationThe processor within the MCU ATMEL 90LS8535 2, which conventionally receives so much attention, isnot particularly noteworthy. It is an 8bit Harvard architecture with 16bit addresses. It provides 32 8bitgeneral registers and runs at 4 MHz and 3.0 V. The system is very memory constrained it has 8 KB of flashas the program memory, and 512 bytes of SRAM as the data memory. The MCU is designed such that aprocessor cannot write to instruction memory our prototype uses a coprocessor to perform that function.Additionally, the processor integrates a set of timers and counters which can be configured to generateinterrupts at regular time intervals. More noteworthy are the three sleep modes idle, which just shuts offthe processor, power down, which shuts off everything but the watchdog and asynchronous interrupt logicnecessary for wake up, and power save, which is similar to the power down mode, but leaves an asynchronoustimer running.Three LEDs represent analog outputs connected through a general IO port they may be used to displaydigital values or status. The photosensor represents an analog input device with simple control lines. Inthis case, the control lines eliminate power drain through the photo resistor when not in use. The inputsignal can be directed to an internal ADC in continuous or sampled modes.The radio is the most important component. It represents an asynchronous inputoutput device with hardreal time constraints. It consists of an RF Monolithics 916.50 MHz transceiver TR1000 10, antenna, andcollection of discrete components to configure the physical layer characteristics such as signal strength andsensitivity. It operates in an ONOFF key mode at speeds up to 19.2 Kbps. Control signals configure theradio to operate in either transmit, receive, or poweroff mode. The radio contains no buffering so each bitmust be serviced by the controller on time. Additionally, the transmitted value is not latched by the radio,so jitter at the radio input is propagated into the transmission signal.The temperature sensor Analog Devices AD7418 represents a large class of digital sensors which haveinternal AD converters and interface over a standard chiptochip protocol. In this case, the synchronous,twowire I2C 40 protocol is used with software on the microcontroller synthesizing the I2C master overgeneral IO pins. In general, up to eight different I2C devices can be attached to this serial bus, each witha unique ID. The protocol is rather different from conventional bus protocols, as there is no explicit arbiter.3Component Active mA Idle mA Inactive  AMCU core AT90S8535 5 2 1MCU pins 1.5  LED 4.6 each  Photocell .3  Radio RFM TR1000 12 tx, 4.5 rcv  5Temp AD7416 1 0.6 1.5Coproc AT90LS2343 2.4 .5 1EEPROM 24LC256 3  1Table 1 Current per hardware component of baseline networked sensor platform. Our prototype is powered by anEnergizer CR2450 lithium battery rated at 575 mAh 30. At peak load, the system consumes 19.5 mA of current,or can run about 30 hours on a single battery. In the idle mode, the system can run for 200 hours. When switchedinto inactive mode, the system draws only 10 A of current, and a single battery can run for over a year.Bus negotiations must be carried out by software on the microcontroller.The serial port represents an important asynchronous bitlevel device with bytelevel controller support. Ituses IO pins that are connected to an internal UART controller. In transmit mode, the UART takes abyte of data and shifts it out serially at a specified interval. In receive mode, it samples the input pin for atransition and shifts in bits at a specified interval from the edge. Interrupts are triggered in the processorto signal completion events.The coprocessor represents a synchronous bitlevel device with bytelevel support. In this case, it is a verylimited MCU AT90LS2343 2, with 2 KB flash instruction memory, 128 bytes of SRAM and EEPROMthat uses IO pins connected to an SPI controller. SPI is a synchronous serial data link, providing high speedfullduplex connections up to 1 Mbit between various peripherals. The coprocessor is connected in a waythat allows it to reprogram the main microcontroller. The sensor can be reprogrammed by transferring datafrom the network into the coprocessors 256 KB EEPROM 24LC256. Alternatively the main processor canuse the coprocessor as a gateway to extra storage.Future extensions to the design will include the addition of battery strength monitoring via voltage andtemperature measurements, radio signal strength sensor, radio transmission strength actuator, and a generalI2C sensor extension bus.3.2 Power CharacteristicsTable 1 shows the current drawn by each hardware component under three scenarios peak load when active,load in idle mode, and inactive. When active, the power consumption of the LED and radio reception areabout equal to the processor. The processor, radio, and sensors running at peak load consume 19.5 mA at 3volts, or about 60 mW. If all the LEDs are on, this increases to 100 mW. This figure should be contrastedwith the 10 A current draw in the inactive mode. Clearly, the biggest savings are obtained by makingunused components inactive whenever possible. The system must embrace the philosophy of getting thework done as quickly as possible and going to sleep.The minimum pulse width for the RFM radio is 52 s. Thus, it takes 1.9 J of energy to transmit a single bitof one. Transmitting a zero is free, so at equal DC balance which is roughly what the transmitter requiresfor proper operation, it costs about a 1 J to transmit a bit and 0.5 J to receive a bit. During this time,the processor can execute 208 cycles roughly 100 instructions and can consume up to .8 J. A fraction ofthis instruction count is devoted to bit level processing. The remainder can go to higher level processingbytelevel, packet level, application level amortized over several bit times. Unused time can be spent inidle or powerdown mode.To broaden the coverage of our study, we deploy these networked sensors in two configurations. One is a4CoprocessorAT90L2313EEPROMSPISerialPortUARTADCLight SensorPwrdataIO pinsTempAD7418IO pinsI2CRFM TR100916 MHz transceiverIO pinsCtrlTXRXLEDsIO pinsInst.RegisterPgm. mem.flashInst.DecoderPCSRAMRegsSRALUSPEEPROMIntunitTimerUnitReferenceVoltage4 MHzclockTXRXI2CSPIAT 90LS853532.768 MHzclock8bit data busCtrl linesFigure 1 Photograph and schematic for representative network sensor platform5mobile sensor that picks up temperature and light readings and periodically presents them on the wirelessnetwork as tagged data objects. It needs to conserve its limited energy. The second is a stationary sensorthat bridges the radio network through the serial link to a host on the Internet. It has power supplied byits host, but also has more demanding data flows.4 Tiny Microthreading Operating System TinyOSThe core challenge we face is to meet the requirements for networked sensors put forth in Section 2 upon theclass of platforms represented by the design in Section 3 in manner that scales forward to future technology.Small physical size, modest active power load and tiny inactive load are provided by the hardware design.An operating system framework is needed that will retain these characteristics by managing the hardwarecapabilities effectively, while supporting concurrencyintensive operation in a manner that achieves efficientmodularity and robustness.For reasons described in Section 6, existing embedded device operating systems do not meet this challenge.Also, we desire a clean open platform to explore alternatives. The problem we must tackle is strikingly similarto that of building efficient network interfaces, which also must maintain a large number of concurrent flowsand juggle numerous outstanding events 20. This has been tackled through physical parallelism 21 andvirtual machines 27. We tackle it by building an extremely efficient multithreading engine. As in TAM 22and CILK 23 it maintains a twolevel scheduling structure, so a small amount of processing associatedwith hardware events can be performed immediately. The execution model is similar to FSM models, butconsiderably more programmable.Our system is designed to scale with the current technology trends supporting both smaller, tightly integrateddesigns as well as the crossover of software components into hardware. This is in contrast to traditionalnotions of scalability that are centered on scaling up total powerresourceswork for a given computingparadigm. It is essential that network sensor architectures plan for the eventual integration of sensors,processing and communication. The days of sensor packs being dominated by interconnect and supporthardware, as opposed to physical sensors, are numbered.In TinyOS, we have chosen an event model that allows for high concurrency to be handled in a very smallamount of space. A stackbased threaded approach would require orders of magnitude more memory thanwe expect to have avilable. Worstcase memory usage must be reserved for each execution context, orsophisticated memory management support is required. Additionally, we need to be able to multitaskbetween these execution contexts at a rate of 40,000 switches per second, or twice every 50 s  once toservice the radio and once to perform all other work. It is clear that an eventbased regime is required. It isnot surprising that researchers in the area of high performance computing have seen this same phenomena that event based programming must be used to achieve high performance 28, 43.In this design space, power is the most precious resource. We believe that the eventbased approach createsa system that uses CPU resources efficiently. The collection of tasks associated with an event are handledrapidly, and no blocking or polling is permitted. Unused CPU cycles are spent in the sleep state as opposedto actively looking for some interesting event. Additionally, with realtime constraints the calculation of CPUutilization becomes simple  allowing for algorithms that adjust processor speed and voltage accordingly 37,45.4.1 Tiny OS DesignA complete system configuration consists of a tiny scheduler and a graph of components. A componenthas four interrelated parts a set of command handlers, a set of event handlers, an encapsulated fixedsizeframe, and a bundle of simple threads. Threads, commands, and handlers execute in the context of the frameand operate on its state. To facilitate modularity, each component also declares the commands it uses andthe events it signals. These declarations are used to compose the modular components in a perapplication6configuration. The composition process creates layers of components where higher level components issuecommands to lower level components and lower level components signal events to the higher level components.Physical hardware represents the lowest level of components. The entire system is written in a structuredsubset of C.The use of static memory allocation allows us to know the memory requirements of a component at compiletime. Additionally, it prevents the overhead associated with dynamic allocation. This savings manifestsitself many ways, including execution time savings because variable locations can be statically compiled intothe program instead of accessing thread state via pointers.Commands are nonblocking requests made to lower level components. Typically, a command will depositrequest parameters into its frame and conditionally post a thread for later execution. It may also invokelower commands, but it must not wait for long or indeterminate latency actions to take place. A commandmust provide feedback to its caller by returning status indicating whether it was successful or not, e.g., bufferoverrun.Event handlers are invoked to deal with hardware events, either directly or indirectly. The lowest levelcomponents have handlers connected directly to hardware interrupts, which may be external interrupts,timer events, or counter events. An event handler can deposit information into its frame, post threads,signal higher level events or call lower level commands. A hardware event triggers a fountain of processingthat goes upward through events and can bend downward through commands. In order to avoid cycles inthe commandevent chain, commands cannot signal events. Both commands and events are intended toperform a small, fixed amount of work, which occurs within the context of an executing thread.Threads perform the primary work. They are atomic with respect to other threads and run to completion,though they can be preempted by events. Threads can call lower level commands, signal higher level events,and schedule other threads within a component. The runtocompletion semantics of threads make it possibleto allocate a single stack that is assigned to the currently executing thread. This is essential in memoryconstrained systems. Threads allow us to simulate concurrency within each component, since they executeasynchronously with respect to events. However, threads must never block or spin wait or they will preventprogress in other components. Thread bundles provide a way to incorporate arbitrary computation into theevent driven model.The thread scheduler is currently a simple FIFO scheduler, utilizing a bounded size scheduling data structure. Depending on the requirements of the application, more sophisticated prioritybased or deadlinebasedstructures can be used. It is crucial that the scheduler is power aware our prototype puts the processorto sleep when the thread queue is empty, but leaves the peripherals operating, so that any of them canwake up the system. This behavior enables us to provide efficient battery usage. see Section 5. Once thequeue is empty, another thread can be scheduled only as a result of an event, thus there is no need for thescheduler to wake up until a hardware event triggers activity. More aggressive power management is left tothe application.4.2 Example ComponentA typical component including a frame, event handlers, commands and threads for a message handlingcomponent is pictured in Figure 2. Like most components, it exports commands for initialization and powermanagement. Additionally, it has a command for initiating a message transmission, and signals events onthe completion of a transmission or the arrival of a message. In order to perform its function, the messagecomponent issues commands to a packet level component and handles two types of events one that indicatesa message has been transmitted and one that signals that a message has been received.Since the components describe both the resources they provide and the resources they require, connectingthem together is very simple. The programmer simply matches the signatures of events and commandsrequired by one component with the signatures of events and commands provided by another component.The communication across the components takes the form of a function call, which has low overhead and7addr,type,datasendmsgsendmsgthread internalstateRXpacketdonebufferTXpacketdonesuccessMessagingComponentmsgsenddonesuccessmsgrectype, datainitpowermodeinitpowermodeTXpacketbuf Messaging Component Declaration   ACCEPTS char TOSCOMMANDAMsendmsgint addr,int type,        char data void TOSCOMMANDAMpowerchar mode char TOSCOMMANDAMinit SIGNALS char AMmsgrecint type, char data char AMmsgsenddonechar success HANDLES char AMTXpacketdonechar success char AMRXpacketdonechar packet USES char TOSCOMMANDAMSUBTXpacketchar data void TOSCOMMANDAMSUBpowerchar mode char TOSCOMMANDAMSUBinit Figure 2 A sample messaging component. Pictorially, we represent the component as a bundle of threads, a blockof state component frame a set of commands upsidedown triangles, a set of handlers triangles, solid downwardarcs for commands they use, and dashed upward arcs for events they signal. All of these elements are explicit in thecomponent code.provides compile time type checking.4.3 Component TypesIn general, components fall into one of three categories hardware abstractions, synthetic hardware, and highlevel software components.Hardware abstraction components map physical hardware into our component model. The RFM radiocomponent shown in lower left corner of Figure 3 is representative of this class. This component exports commands to manipulate the individual IO pins connected to the RFM transceiver and posts eventsinforming other components about the transmission and reception of bits. The frame of the componentcontains about the current state the transceiver is in sending or receiving mode, the current bit rate, etc..The RFM consumes the hardware interrupt, which is transformed into either the RX bit evt or into theTX bit evt. There are no threads within the RFM because the hardware itself provides the concurrency.This model of abstracting over the hardware resources can scale from very simple resources, like individualIO pins, to quite complex ones, like UARTs.Synthetic hardware components simulate the behavior of advanced hardware. A good example of suchcomponent is the Radio Byte component see Figure 3. It shifts data into or out of the underlying RFMmodule and signals when an entire byte has completed. The internal threads perform simple encoding anddecoding of the data. 1 Conceptually, this component is an enhanced state machine that could be directlycast into hardware. From the point of view of the higher levels, this component provides an interface andfunctionality very similar to the UART hardware abstraction component they provide the same commandsand signal the same events, deal with data of the same granularity, and internally perform similar taskslooking for a start bit or symbol, perform simple encoding, etc..The high level software components perform control, routing and all data transformations. A representativeof this class is the messaging module presented above, in Figure 2. It performs the function of filling in apacket buffer prior to transmission and dispatches received messages to their appropriate place. Additionally,components that perform calculations on data or data aggregation fall into this category.1The radio requires that the data transmitted is DCbalanced. We currently use Manchester encoding.8This component model allows for easy migration of the hardwaresoftware boundary. This is possible becauseour event based model is complementary to the underlying hardware. Additionally, the use of fixed size,preallocated storage is a requirement for hardware based implementations. This ease of migration fromsoftware to hardware will be particularly important for networked sensors, where the system designers willwant to explore the tradeoffs between the scale of integration, power requirements, and the cost of thesystem.4.4 Putting it all togetherNow, that we have shown a few sample components, we will examine their composition and their interactionwithin a complete configuration. To illustrate the interaction of the components, we describe a networkedsensor application we have developed. The application consists of a number of sensors distributed within alocalized area. They monitor the temperature and light conditions and periodically broadcast their measurements onto the radio network. Each sensor is configured with routing information that will guide packets toa central base station. Thus, each sensor can act as a router for packets traveling from sensors that are outof range of the base station. The internal component graph of one of these sensors is shown in Figure 3.There are three IO devices that this application must service the network, the light sensor, and thetemperature sensor. Each of these devices is represented by a vertical stack of components. The stacksare tied together by the application layer. We chose an abstraction similar to active messages 43 for ourtop level communication model. The active message model includes handler identifiers with each message.The networking layer invokes the indicated handler when a message arrives. This integrates well with ourexecution model because the invocation of message handlers takes the form of events being signaled in theapplication. Our application data is broadcasted in the form of fixed length active messages. If the receiveris an intermediate hop on the way to the base station, the message handler initiates the retransmission ofthe message to the next recipient. Once at the base station, the handler forwards the packet to the attachedcomputer.When our application is running, a timer event is used to periodically start data collection. Once the temperature and light information have been collected, the application uses the messaging layers send messagecommand to initiate a transfer. This command records the message location in the AM components frameand schedules a thread to handle the transmission. When executed, this thread composes a packet, andinitiates a downward chain of commands by calling the TX packet command in the Packet component. Inturn, the command calls TX byte within the Radio Byte component to start the bytebybyte transmission.The Packet component internally acts as a data drain, handing bytes down to the Radio Byte componentwhenever the previous byte transmission is complete. Internally, Radio Byte prepares for transmission byputting the RFM component into the transmission state if appropriate and scheduling the encode threadto prepare the byte for transmission. When the encode thread is scheduled, it encodes the data, and sendsthe first bit of data to the RFM component for transmission. The Radio Byte also acts as a data drain, providing bits to the RFM in response to the TX bit evt event. If the byte transmission is complete, then theRadio Byte will propagate the TX bit evt signal to the packetlevel controller through the TX byte doneevent. When all the bytes of the packet have been drained, the packet level will signal the TX packet doneevent, which will signal the the application through the msg send done event.When a transmission is not in progress, and the sensor is active, the Radio Byte component receives bitsfrom the RFM component. If the start sequence is detected, the transmission process is reversed bits arecollected into bytes and bytes are collected into packets. Each component acts as a datapump it activelysignals the incoming data to the higher levels of the system, rather than respond to a read operation fromabove. Once a packet is available, the address of the packet is checked and if it matches the local address,the appropriate handler is invoked.9internalstateRFMRXbitevtdataTXbitevttimer interruptencodethrdecodethr internalstateinitsetbitratelevelRadio byteTXbytereadysuccessTXdoneRXbytereadydata,errpowermodeRXmodeTXbitdataTXmodeinternalstatePacketpowermodeTXbytesinitRXpacketdonebufTXpacketdonesuccessinternalstateinternalstateTemperaturereadaddrinitdatareadydatagetdatainitinternalstateI2C busI2C threadwriteaddr,datawritedonesuccessreaddonedatainternalstateinternalstatemsgsenddonesuccessAMsendmsgthrTXpacketbufpowermodeinitmsgrectype, data interrupttimerApplicationinitdatareadydataPhotopowergetdatasendmsgaddr,type,datapowermodeinitADC readyinterruptFigure 3 A sample configuration of a networked sensor10Component Name Code Size Data Sizebytes bytesMultihop router 88 0AM dispatch 40 0AM temperature 78 32AM light 146 8AM 356 40Packet 334 40RADIO byte 810 8RFM 310 1Photo 84 1Temperature 64 1UART 196 1UART packet 314 40I2C bus 198 8Procesor init 172 30TinyOS scheduler 178 16C runtime 82 0Total 3450 226Table 2 Code and data size breakdown for our complete system. Only the processor init, the TinyOS scheduler,and the C runtime are required for every application, the other components are included as needed.5 EvaluationSmall physical size Table 2 shows the code and data size for each of the components in our system. It isclear that the code size of our complete system, including a network sensor application with simple multihoprouting, is remarkable. In particular, our scheduler only occupies 178 bytes and our complete network sensorapplication requires only about 3KB of instruction memory. Furthermore, the data size of our scheduler isonly 16 bytes, which utilizes only 3 of the available data memory. Our entire application comes in at 226bytes of data, still under 50 of the 512 bytes available.Concurrencyintensive operations As we argued in Section 2, network sensors need to handle multiple flowsof information simultaneously. In this context, an important baseline characteristic of a network sensor isits context switch speed. Table 3 shows this aspect calibrated against the intrinsic hardware cost for movingbytes in memory. The cost of propagating an event is roughly equivalent to that of copying one byte of data.This low overhead is essential for achieving modular efficiency. Posting a thread and switching context costsabout as much as moving 6 bytes of memory. Our most expensive operation involves the lowlevel aspects ofinterrupt handling. Though the hardware operations for handling interrupts are fast, the software operationsthat save and restore registers in memory impose a significant overhead. Several techniques can be used toreduce that overhead partitioning the register set 22 or use of register windows 14.Efficient modularity One of the key characteristics of our systems is that events and commands can propagatethrough components quickly. Projects such as paths, in Scout 36, and stackable systems 29, 25, 24 havehad similar goals in other regimes. Table 3 gives the cost of individual component crossing, while Figure 4shows the dynamic composition of these crossings. It contains a timing diagram from a logic analyzer ofan event chain that flows through the system at the completion of a radio transmission. The events fireup through our component stack eventually causing a command to trasmit a second message. The totalpropagation delay up the five layer radio communication stack is 40 s or about 80 instructions. This isdiscussed in detail in Figure 4 steps 0 through 4 show the event crossing these layers. The entire eventpropagation delay plus the cost of posting a command to schedule a thread to send the next packet step 0through 6 is about 90 s.Limited physical parallelism and controller hierarchy We have successfully demonstrated a system managing11RFMRadio bytePacketTX pin123456interrupttimerApplication0AM s endmsgthreads ampling for packet starts ymbol every 50usAMs endmsgFigure 4 A timing diagram from a logic analyzer capturing event propagation across networking components at agranularity of 50 s per division. The graph shows the send message scenario described in Section 4.4 focusing ontransmission of the last bit of the packet. Starting from the hardware timer interrupt of step 0, events propagateup through the TX bit evt in step 1, into bytelevel processing. The handler issues a command to transmit the finalbit and then fires the TX byte ready event in step 2 to signal the end of the byte. This triggers TX packet done instep 3. Step 4 signals the application that the send msg command has finished. The application then issues anotherasynchronous send msg command in step 5 which post a thread at step 6 to send the packet. While send msg threadprepares the message, the RFM component is periodically scheduled to listen for incoming packets. The eventpropagation delay from step 0 to step 4 is about 40 s while for the entire event and command fountain starting fromstep 0 to step 6 to be completed, the total elapsed time is about 95 s.multiple flows of data through a single microcontroller. Table 4 shows the work and energy distributionamong each of our software components while engaged in active data transmission. Even during this highlyactive period, the processor is idle approximately 50 of the time. The remaining time can be used toaccess other sensors, like the photo sensor, or the I2C temperature controller. Even if other IO devicesprovide an interface as primitive as our radio, a single controller can support flows of data at rates up to40 s per bit or 25Kbps. Furthermore, this data can be used to make design choices about the amount ofphysical parallelism necessary. For example, while the low level bit and byte processing utilize significantCPU resources, the CPU is not the system bottleneck. If bit level functions were implemented on a separatemicrocontroller, we would not realize a performance gain because of the radio bandwidth limitations. Wewould also incur additional power and time expense in transferring data between microcontrollers. However,if these components were implemented by dedicated hardware, we would be able to make several powersaving design choices including sleeping, which would save 690 J per bit, or lowering the frequency of theprocessor 20fold.Diversity in usage and robust operation Finally, we have been able to test the versatility of this architectureby creating sample applications that exploit the modular structure of our system. These include source basedmultihop routing applications, activebadgelike 44 location detection applications and sensor networkmonitoring applications. Additionally by developing our system in C, we have the ability to target multipleCPU architectures in future systems.6 Related WorkThere is a large amount of work on developing micromechanical sensors and new communication devices 39,38. The development of these new devices make a strong case for the development of a software platformto support and connect them. TinyOS is designed to fill this role. We believe that current realtime12Operations Average Cost Time Normalized tocycles s byte copyByte copy 8 2 1Post an Event 10 2.5 1.25Call a Command 10 2.5 1.25Post a thread to scheduler 46 11.5 6Context switch overhead 51 12.75 6Interrupt hardware cost 9 2.25 1Interrupt software cost 71 17.75 9Table 3 Overheads of primitive operations in TinyOSComponents Packet reception Percent CPU Energywork breakdown Utilization nJbitAM 0.05 0.02 0.33Packet 1.12 0.51 7.58Radio handler 26.87 12.16 182.38Radio decode thread 5.48 2.48 37.2RFM 66.48 30.08 451.17Radio Reception   1350Idle  54.75 Total 100.00 100.00 2028.66Components Packet transmission Percent CPU Energywork breakdown Utilization nJbitAM 0.03 0.01 0.18Packet 3.33 1.59 23.89Radio handler 35.32 16.90 253.55Radio encode thread 4.53 2.17 32.52RFM 56.80 27.18 407.17Radio Transmission   1800Idle  52.14 Total 100.00 100.00 4317.89Table 4 Details breakdown of work distribution and energy consumption across each layer for packet transmissionand reception. For example, 66.48 of the work in receiving packets is done in the RFM bitlevel component and itutilizes 30.08 of the CPU time during the entire period of receiving the packet. It also consumes 451.17nJ per bitit processes. Note that these measurements are done with respect to raw bits at the physical layer with the bit rateof the radio set to 100 sbit using DCbalanced ONOFF keying.13Name Preemption Protection ROM Size Configurable TargetspOSEK Tasks No 2K Static MicrocontrollerspSOSystem POSIX Optional Dynamic PII  ARM ThumbVxWorks POSIX Yes  286K Dynamic Pentium  Strong ARMQNX Neutrino POSIX Yes  100K Dynamic Pentium II  NEC chipsQNX Realtime POSIX Yes 100K Dynamic Pentium II  386sOS9 Process Yes Dynamic Pentium  SH4Chorus OS POSIX Optional 10K Dynamic Pentium  Strong ARMAriel Tasks No 19K Static SH2, ARM ThumbCREEM dataflow No 560 bytes Static ATMEL 8051Table 5 A comparison of selected architecture features of several embedded OSes.operating systems do not meet the needs of this emerging integrated regime. Many of them have followedthe performance growth of the wallet size device.Traditional real time embedded operating systems include VxWorks 13, WinCE 19, PalmOS 4, andQNX 26 and many others 8, 33, 35. Table 5 shows the characteristics for a handful of these systems.Many are based on microkernels that allow for capabilities to be added or removed based on system needs.They provide an execution environment that is similar to traditional desktop systems. Their POSIX 41compatible thread packages allow system programmers to reuse existing code and multiprogramming techniques. The largest RTOSes provide memory protection given the appropriate hardware support. Thisbecomes increasingly important as the size of the embedded applications grow. In addition to providingfault isolation, memory protection prevents corrupt pointers from causing seemingly unrelated errors in other parts of the program allowing for easier software development. These systems are a popular choice forPDAs, cell phones and settopboxes. However, they do not come close to meeting our requirements theyare more suited to the world of embedded PCs. For example, a QNX context switch requires over 2400cycles on a 33MHz 386EX processor. Additionally, the memory footprint of VxWorks is in the hundreds ofkilobytes. 2 Both of these statistics are more than an order of magnitude beyond our required limits.There is also a collection of smaller real time executives including Creem 31, pOSEK 7, and Ariel 3, whichare minimal operating systems designed for deeply embedded systems, such as motor controllers or microwaveovens. While providing support for preemptive tasks, they have severely constrained execution and storagemodels. pOSEK, for example, provides a taskbased execution model that is statically configured to meetthe requirements of a specific application. Generally, these systems approach the space requirements andrepresent designs closest to ours. However, they tend to be control centric  controlling access to hardwareresources  as opposed to movementcentric. Even the pOSEK, which meets our memory requirements,exceeds the limitations we have on context switch time. At its optimal performance level and with theassumption that the CPI and instructions per program of the PowerPC are equivalent to that of the 8bitATMEL the context switch time would be over 40 s.Other related work includes 17 where a finite state machine FSM description language is used to expresscomponent designs that are compiled down to software. However, they assume that this software will thenoperate on top of a realtime OS that will give them the necessary concurrency. This work is complementaryto our own in that the requirements of an FSM based design maps well onto our eventcommand structure.We also have the ability to support the high levels of concurrency inherent in many finite state machines.On the device side, 6 is developing a cubic millimeter integrated network sensors. Additionally, 39, 15has developed low power hardware to support the streaming of sensor readings over wireless communicationchannels. In their work, they explicitly mention the need for the inclusion of a microcontroller and thesupport of multihop routing. Both of these systems require the support of an efficient software architecturethat allows high levels of concurrency to manage communication and data collection. Our system is designed2It is troubling to note that while there is a large amount of information on code size of embedded OSes, there are veryfew hard performance numbers published. 9 has started a program to test various realtime operating systems yet they arekeeping the results confidential  you can view them for a fee.14to scale down to the types of devices they envision.A final class of related work is that of applications that will be enabled by networked sensors. Piconet16 and The Active Badge Location System 44 have explored the utility of networked sensors. Theirapplications include personnel tracking and information distribution from wireless, portable communicationdevices. However, they have focused on the applications of such devices as opposed to the system architecturethat will allow a heterogeneous group of devices to scale down to the cubic millimeter category.7 Architectural ImplicationsA major architectural question in the design of network sensors is whether or not individual microcontrollersshould be used to manage each IO device. We have demonstrated that it is possible to maintain multipleflows of data with a single microcontroller. This shows that it is an architectural option  not a requirement to utilize individual microcontrollers per device. Moreover, the interconnect of such a system will need tosupport an efficient event based communication model. Tradeoffs quickly arise between power consumption,speed of off chip communication, flexibility and functionality. Additionally, our quantitative analysis hasenabled us to consider the effects of using alternative microcontrollers. We believe that the use of a higherperformance ARM Thumb 1 would not change our architecture, while we can calculate at what point aprocessor will not meet our requirements. Along similar lines, we can extrapolate how our technology willperform in the presence of higher speed radio components. It is clear that bit level processing cannot beused with the transfer rates of Bluetooth radios 11 the Radio Byte component needs to become a hardwareabstraction rather than synthetic hardware.Further analysis of our timing breakdown in Table 4 can reveal the impact of architectural changes in microcontrollers. For example, the inclusion of hardware support for events would make a significant performanceimpact. An additional register set for the execution of events would save us about 20 s per event or about20 of our total CPU load. This savings could be directly transferred to either higher performance or lowerpower consumption.Additionally, we are able to quantify the effects of additional hardware support for managing data transmission. Table 4 shows that hardware support for the byte level collection of data from the radio would saveus a total of about 690 J per bit in processor overhead. This represents the elimination of the bit levelprocessing from the CPU. Extension of this analysis can reveal the implication of several other architecturalchanges including the use of radios that can automatically wake themselves at the start of an incomingtransmission or a hardware implementation of a MAC layer.Furthermore, the impact of reconfigurable computing can be investigated relative to our design point. Intraditional systems, the interconnect and controller hierarchy is configured for a particular system niche,where as in future network sensors it will be integrated on chip. Reconfigurable computing has the potentialof making integrated network sensors highly versatile. The Radio byte component is a perfect candidate forreconfigurable support. It consumes a significant amount of CPU time and must be radio protocol specific.A standard UART or DMA controller is much less effective in this situation because the component mustsearch for the complex start symbol prior to clocking in the bits of the transmission. However, it could betrivially implemented in a FPGA.All of this extrapolation is the product of fully developing and analyzing quantitatively a specific designpoint in the network sensor regime. It is clear that there is a strong tie between the software executionmodel and the hardware architecture that supports it. Just as SPEC benchmarks attempted to evaluate theimpact of architectural changes on the entire system in the workstation regime, we have attempted to beginthe systematic analysis architectural alternatives in the network sensor regime.15References1 Atmel AT91 Arm Thumb. httpwww.atmel.comatmelproductsprod35.htm.2 Atmel AVR 8Bit RISC processor. httpwww.atmel.comatmelproductsprod23.htm.3 Microware Ariel Technical Overview. httpwww.microware.comProductsServicesTechnologiesarieltechnologybrief.html.4 PalmOS Software 3.5 Overview. httpwww.palm.comdevzonedocspalmos35.html.5 Pico Radio. httpbwrc.eecs.berkeley.eduResearchPicoRadio.6 Pister, K.S.J. Smart Dust. httpwww.atmel.comatmelproductsprod23.htm.7 pOSEK, A supersmall, scalable realtime operating system for highvolume, deeply embedded applications. httpwww.isi.comproductsposekindex.htm.8 pSOSystem Datasheet. httpwww.windriver.comproductshtmlpsosystemds.html.9 RealTime Consult. httpwww.realtimeinfo.comencycmarketrtosevalintroduction.htm.10 RF Monolithics. httpwww.rfm.comproductsdatatr1000.pdf.11 The Official Bluetooth Website. httpwww.bluetooth.com.12 uClinux, The LinuxMicrocontroller Project. httpwww.uclinux.org.13 VxWorks 5.4 Datasheet. httpwww.windriver.comproductshtmlvxwks54ds.html.14 Anant Agarwal, Geoffrey DSouza, Kirk Johnson, David Kranz, John Kubiatowicz, Kiyoshi Kurihara,BengHong Lim, Gino Maa, Daniel Nussbaum, Mike Parkin, and Donald Yeung. The MIT alewifemachine  A largescale distributedmemory multiprocessor. In Proceedings of Workshop on ScalableShared Memory Multiprocessors. Kluwer Academic, 1991.15 B. Atwood, B.Warneke, and K.S.J. Pister. Preliminary circuits for smart dust. In Proceedings of the2000 Southwest Symposium on MixedSignal Design, San Diego, California, February 2729 2000.16 F. Bennett, D. Clarke, J. Evans, A. Hopper, A. Jones, and D. Leask. Piconet Embedded mobilenetworking, 1997.17 M. Chiodo. Synthesis of software programs for embedded control applications, 1995.18 Chu, P.B., Lo, N.R., Berg, E., Pister, K.S.J. Optical communication link using micromachined cornercuber reflectors. In Proceedings of SPIE vol.300820., 1997.19 Microsoft Corp. Microsoft Windows CE. httpwww.microsoft.comwindowsceembedded.20 D. Culler, J. Singh, and A. Gupta. Parallel computer architecture a hardwaresoftware approach, 1999.21 R. Esser and R. Knecht. Intel Paragon XPS  architecture and software environment. Technical ReportKFAZAMIB9305, 1993.22 D. Culler et. al. Fine grain parallelism with minimal hardware support A compilercontrolled treadedabstract machine. In Proceedings of 4th International Conference on Architectural Support for Programming Languages and Operating Systems, April 1991.23 R.D. Blumofe et. al. Cilk An efficient multithreaded runtime system. In Proceedings of the Fifth ACMSIGPLAN Symposium on Principles and Practice of Parallel Programming PPoPP, pages 207216,Santa Barbara, California, July 1995.1624 Richard G. Guy, John S. Heidemann, Wai Mak, Thomas W. Page Jr., Gerald J. Popek, and DieterRothmeier. Implementation of the ficus replicated file system. In Proceedings of the Summer USENIXConference, pages pages 6371, Anaheim, CA, June 1990.25 J. S. Heidemann and G. J. Popek. Filesystem development with stackable layers. In ACM Transactionson Computer Systems, pages 1215889, Feb. 1994.26 Dan Hildebrand. An Architectural Overview of QNX. httpwww.qnx.comliteraturewhitepapersarchoverview.html.27 M. Homewood and M. McLaren. Meiko cs2 interconnect elanelite design, 1993.28 James Hu, Irfan Pyarali, and Douglas C. Schmidt. Measuring the impact of event dispatching andconcurrency models on web server performance over highspeed networks. In In Proceedings of the 2 ndGlobal Internet Conference. IEEE, November 1997.29 N. C. Hutchinson and L. L. Peterson. The xkernel An architecture for implementing network protocols.In IEEE Transactions on Software Engineering, pages 1716476, Jan. 1991.30 Energizer Battery Company Inc. Energizer cr2450, engineering data. httpdata.energizer.comdatasheetslibraryprimarylithiumcoincr2450.pdf.31 Barry Kauler. CREEM Concurrent Realitme Embedded Executive for Microcontrollers. httpwww.goofee.comcreem.htm.32 J. Kymissis, C. Kendall, J. Paradiso, and N. Gershenfeld. Parasitic power harvesting in shoes. In Proc.of the Second IEEE International Conference on Wearable Computing ISWC, IEEE Computer SocietyPress, pages pp. 132139, October 1998.33 QNX Software Systems Ltd. QNX Neutrino Realtime OS . httpwww.qnx.comproductsosneutrino.html.34 James McLurkin. Algorithms for distributed sensor networks. In Masters Thesis for Electrical Engineering at the Univeristy of California, Berkeley, December 1999.35 Microware. Microware OS9. httpwww.microware.comProductsServicesTechnologiesos91.html.36 A. B. Montz, D. Mosberger, S. W. OMalley, L. L. Peterson, and T. A. Proebsting. Scout Acommunicationsoriented operating system. In Hot OS, May 1995.37 T. Pering, T. Burd, and R. Brodersen. The simulation and evaluation of dynamic voltage scalingalgorithms. In Proc. Intl Symposium on Low Power Electronics and Design, pages pp. 7681, Aug.1998.38 K. S. J. Pister, J. M. Kahn, and B. E. Boser. Smart dust Wireless networks of millimeterscale sensornodes, 1999.39 G. Pottie, W. Kaiser, L. Clare, and H. Marcy. Wireless integrated network sensors, 1998.40 Philips Semiconductors. The i2cbus specification, version 2.1. httpwwwus.semiconductors.comacrobatvariousI2CBUSSPECIFICATION3.pdf, 2000.41 I. Standard. Realtime extensions to posix, 1991.42 EMJ EMBEDDED SYSTEMS. White Dwarf Linux. httpwww.emjembedded.comlinuxdimmpc.html.43 T. von Eicken, D. Culler, S. Goldstein, and K. Schauser. Active messages a mechanism for integratedcommunication and computation, 1992.1744 R. Want and A. Hopper. Active badges and personal interactive computing objects, 1992.45 M. Weiser, B. Welch, A. Demers, and S. Shenker. Scheduling for reduced cpu energy. In Proceedings ofthe First Symposium on Operating Systems Design and Implementation OSDI, pages 1323.18
