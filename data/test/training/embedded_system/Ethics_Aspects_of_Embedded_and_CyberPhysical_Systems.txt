Ethics Aspects of Embedded and CyberPhysicalSystemsAbhilash Thekkilakattil1 and Gordana DodigCrnkovic21Malardalen University, Vasteras, Sweden2Chalmers Technical University and University of Gothenburg, Gothenburg, SwedenAbstractThe growing complexity of software employed in thecyberphysical domain is calling for a thorough study of bothits functional and extrafunctional properties. Ethical aspectsare among important extrafunctional properties, that cover thewhole life cycle with different stages from design, development,deploymentproduction to use of cyber physical systems. Oneof the ethical challenges involved is the question of identifyingthe responsibilities of each stakeholder associated with thedevelopment and use of a cyberphysical system. This challengeis made even more pressing by the introduction of autonomousincreasingly intelligent systems that can perform functionalitieswithout human intervention, because of the lack of experience,best practices and policies for such technology.In this article, we provide a framework for responsibilityattribution based on the amount of autonomy and automationinvolved in AI based cyberphysical systems. Our approach enables traceability of anomalous behaviors back to the responsibleagents, be they human or software, allowing us to identify andseparate the responsibility of the decisionmaking softwarefrom human responsibility. This provides us with a framework toaccommodate the ethical responsibility of the software for AIbased cyberphysical systems that will be deployed in the future,underscoring the role of ethics as an important extrafunctionalproperty. Finally, this systematic approach makes apparent theneed for rigorous communication protocols between differentactors associated with the development and operation of cyberphysical systems that further identifies the ethical challengesinvolved in the form of group responsibilities.I. INTRODUCTIONThe increasing use of computerbased systems in daytoday applications has revolutionized human life. Today, theinfluence of computing extends to cyberphysical systemsCPS that interact with the physical world, and thereforemust operate dependably, safely, securely, and efficiently 1.Some of the advantages offered by computing in cyberphysical systems include environmental sustainability throughefficiency, high performance, cost reduction, and flexibility.Cyberphysical systems are found in transportation, aerospace,robotic systems, manufacturing, process control, environmental control and smart cities. CPS are considered to be thebasis of the next computing revolution and are expected todevelop rapidly in coming years. Embedded systems canbe considered as precursors of CPS. An example of suchsystem is drivebywire system that replaces large amountsof hardware, such as wires and cables, reducing cost whileThis work was supported by the Swedish Research Council project CONTESSE 20104276enabling flexibility in the design of the system. In addition toreplacing traditional hardware components, computer systemsenable improved user experience through driver assistancefunctionalities in automobiles, as well as, in recent years,performing more advanced tasks of autonomous driving. Inthe future, it is expected that highly intelligent systems independent from human control will be developed, that willcoexist with humans and perform traditionally human taskslike decisionmaking and control. Case of special interest forour analysis are safety critical systems, for which a failureof the system may lead to catastrophic consequences suchas loss of human lives. Both embedded and cyberphysicalsystems can be safety critical. Up to now most lessonslearned regarding safetycriticality are related to embeddedsystems, and we can draw conclusions about cyberphysicalsystems by extrapolation. Safetycritical systems have to behighly dependable. Particularly important among the attributesof dependability are reliability, availability and security. Inorder to guarantee dependability of cyberphysical systems,the personnel involved in their research, development and useneeds to conform to the highest ethical standards. Bowen 2examines the ethical aspects of safetycritical systems. The article identifies and presents a detailed analysis of the practicesthat should be avoided while building traditional safetycriticalsystems. Bowen notes that many decisions in building safetycritical systems directly depend on economic considerations,rather than on safety criteria. Leveson 3 examines the roleof software in spacecraft accidents and notes that the flaws insafety culture present an important cause of majority of theseaccidents. Moreover, the author discusses some methods toprevent the flaws in future development efforts. Leveson 4also relates the situation of lack of safe engineering practicesin developing software to a similar situation involving earlysteam engines in which safety features were introduced aftergaining sufficient experience with their use.One of the challenges involved in making informed decisions while building cyberphysical systems and embeddedsystem as a special case is to be able to trace decisions takenby various stakeholders into specific anomalous behaviorsexhibited by the system. In other words, engineers building asafetycritical system must be able to predetermine the impactof their design decisions on the safety of the system. Thisenables the engineer or any actor involved to make betterethical judgments. In particular, it is necessary for engineersto assess the impact of their design choices taking ethicalaspects as a part of nonfunctional requirements. Similarly,other stakeholders such as users operating cyberphysicalsystems must be aware of the impact of their own decisions.For example, police forces deploying autonomous drones todestroy drug plantations must assess the safety risk to theenvironment surrounding the plantation, since the potentialdamage caused by the drone can be unpredictable. Finally,in the case of systems that employ Artificial In telligenceAI in their design, manufacturing or operation, it must bepossible to separate inherent design errors from the decisionstaken by the AI software. In order to be able to trace backunsafe behaviors of such system to its builders, users or tothe system itself, a clear demarcation of the responsibilitymust be made, especially with the projected use of AI incyberphysical systems. Such a classification can serve asa template for structuring the ethical responsibility of thedifferent stakeholders associated with cyberphysical systems,as well as for accommodating the ethical responsibility of thesoftware when highly intelligent systems are used in the future.The resistance to attribute functional responsibility to softwarecan be detrimental 5 since the most likely the industry willbe reluctant to rely on the advances of AI, fearing litigationresulting from potential accidents due to decisions taken bythe AI software. In this paper, we classify cyberphysicalsystems based on the amount of autonomy of the system andautomation involved. This allows us to isolate responsibilitiesof the different stakeholders both humans and softwareinvolved with the development and use of cyberphysicalsystems. Our classification framework enables us to considerthe ethical responsibilities of software alongside humans, whentruly intelligent software systems will be deployed that areindependent of human control. It also highlights the criticalimportance of rigorous communication infrastructure betweenvarious stakeholders, requiring the study of the ethical aspectsof such a communication.II. ETHICSThe field of ethics concerns the study of what constitutes anideal conduct in various situations, while taking decisions thatmay affect other people or the environment. The main goalis to examine the moral aspects of conduct, and to determineactions that are deemed morally acceptable. The question ofwhat is morally acceptable needs a continuous evaluation, andso is the study of ethical aspects of major changes introducedinto the society. In particular, it is important to understandand rigorously study the ethical aspects of new technologies,and their implications on the society. Cyberphysical systemsare such a radically new and complex emerging technologythat deserves careful ethical analysis. In particular, for thoseworking in the areas of science and technology, it is importantto know ethical implications of the knowledge and informationdata they produce and products that they design and build.It is therefore necessary that people working in technologydevelop ethical autonomy to be aware about the ethical aspectsof the decisions that they make. Professional ethics is one ofimportant constitutive elements of professionalism that aims atproviding necessary basis for dealing with ethical challenges.One of the first steps towards addressing ethical aspects isto identify a moral problem, and then examine the variousalternative actions at hand after collecting relevant facts. Thenspecific actions are then performed based on the decisiontaken. The effect of the decision are then evaluated andfed back into the process of ethical assessment. In otherwords, the process of addressing ethical issues need to be acontinuous activity, typically based on professional groupscodes of ethics. Moor 6 notes that the contemporary rapiddevelopment of computing technology has resulted in policyvacuums that is lack of policies, due to lack of experiencewith radically new artifacts and situations related to them sowe need to learn how to deal with the ethical issues thathave emerged. Moor argues that the analysis methods usedto deal with the new ethical challenges arising out of theuse of technology must unify the classical approaches towardssolving ethical problems, e.g., the deontological and utilitarianapproaches. He further points out that any ethical policy ofaction must be based on justice just consequentialism  itis unjust to act in a certain way that one would not acceptfrom others. Therefore, policies and actions must be based onimpartiality, even if it may bring harm to some, and must besuch that everyone could be allowed to follow it. Moor alsopoints out that policies and actions for computing technologyshould change as the technology advances.Critics of computer ethics sometimes argue that it dealswith problems that are unsolved or perhaps even unsolvable.Nevertheless, even though technology is constantly developingand one cannot expect a definite set of rules to guide ethicalbehavior, we can develop strategies and policies as a basis ofan adaptive, learning, intelligent framework for ethical assessment. Within the domain of professional ethics of computingDodigCrnkovic 7 identifies the need for ethical education incomputing curricula, in order to assure good ethical judgmentin the computing profession. Georgiadou and Oriogun 8present case studies that underline the need for teachingprofessional ethics as part of software engineering courses.Professional ethics aspects are part of yet a bigger picturewhich includes methods and tools of particular ethical analysis.While some ethicists claim that classical ethical approachesconsequentialism, virtue ethics, deontological ethics, justiceethics, etc. can be applied to deal with contemporary fieldslike computing technology 9, others argue that radicallynew ways of ethical analysis are needed since the problemspresented by computers are unique 6. Floridi 10 supportsuniqueness of computer ethics and shows that Information andCommunication Technologies ICT bring about fundamentally new dimensions to old ethical problems and force us tothink about the very foundations of our ethical policies.III. TERMINOLOGYIn order to address the ethics aspects of embedded andcyberphysical systems, we first define some basic terminologythat we will use consistently in the rest of the paper. First, forclarity, we present the formal definition of a stakeholder.Definition 1. A stakeholder associated with a cyberphysicalsystem is defined as an agent who may interact with the cyberphysical system at various stages of its design, development,deployment and operation.A stakeholder can be a developer, a user or a softwareagentthat interacts with the cyberphysical system at some level.Definition 2. A developer is defined as an individual ororganization associated with the development of a cyberphysical system, whose decisions affect the design of thesystem.As mentioned before, Leveson observed that most softwarerelated accidents are due to a lack of safety culture in theassociated software development 3. Therefore, according toour definition, a designerdeveloper is responsible for safetyrelated accidents that are manifested as a result of ignorance ofsafety in the design and development. For example, engineersare responsible for evaluating the safety of their design, projectmanagers are responsible for making sure that sufficient consideration is given to safety, company executives must ensurethat sufficient time and money is spend in establishing a safetyculture in the company.Definition 3. A user is defined as any stakeholder person orsoftware that uses or operates a cyberphysical system, whosedecisions affect how the cyberphysical system is used.For example, a pilot of an aircraft is referred to as a user.The user is responsible for following the safety practicesduring the use of a cyberphysical system. The safety practicesare typically established by the developers own best practice,guided by safety department in the software developmentorganization and by a government regulatory body such asthe Federal Aviation Authority in the USA that decides thesafety practices for flying aircrafts in the American airspace.Definition 4. A software agent is defined as any softwareassociated with the development and use of a cyberphysicalsystem, capable of making decisions that affect the system itselfand its behavior.In the future, systems with increasingly advanced AI will beused in the development and use of cyberphysical systems.Any software, that is highly intelligent and involved in thedecision making process is referred to as a software agent.Note that current software development tools do not fall intothis category. We assume that they are highly intelligent andcapable of taking autonomous decisions independent of humancontrol.IV. CLASSIFICATION OF SAFETY CRITICAL SYSTEMSIn order to facilitate traceability of decisions, we classifycyberphysical systems based on the amount of autonomyinvolved and based on whether or not a human is involvedin the decision making. We give simple examples on what wemean by each, and prepare foundations on which the rest ofthe paper is based.A. Automatic SystemsAutomation has been one of the key driving forces behindthe widespread adoption of computing in safety and missioncritical systems. Although automation has its modest beginning in data processing, it was soon applied in the context ofprocess automation and control in industrial, vehicular, avionics and aerospace systems. These types of systems replace aspecific hardware, performing a particular task, and do notimplement any amount of autonomy apart from imitating thespecific hardware that it replaces. We refer to such systems asautomatic systems. An automatic system can be a subsystemof a larger system. In automatic systems, there is no decisionmaking involved. Once the system is built, it is expected toperform the specified task repeatedly and no decisions, otherthan that specified at design time, are made by the systemitself. Noorman and Johnson 11 refer to automatic systemsas autonomous systems. We however want to differentiatesystems that are truly AIbased from systems that merelyautomate the required process.An example of an automatic system is the adaptive cruisecontroller that automatically adjusts the vehicle speed in orderto maintain a safe distance from the vehicle ahead.B. Semiautomatic SystemsAlthough computing has extensively been used in industrialautomation and control, many systems need human feedbackthus requiring a human in the loop. These types of systems canbe typically seen as a set of automatic subsystems coordinatedby a human being. The key decision maker is the human, asshe specifies what task the system should perform, as wellas how the task is to be performed.Modern cars are an example of such systems. In modern carsmany automatic subsystems such as the CAN, adaptive cruisecontroller, automatic gear shifter and antilock brakes are coordinated by the driver. Here, the driver is the decision makerwhile the subsystems merely perform the preprogrammedtasks. According to the definitions presented in Section III,the driver is a user.C. Semiautonomous SystemsThe increasing automation enables various automatic subsystems to be controlled and coordinated by computer software. Semiautonomous systems are systems that are capableof autonomously performing tasks specified by humans. Thiskind of systems have limited autonomy in the sense that thesystem needs to be instructed with the specific task, and thetask is carried out by the system based on its own decisions.For example, consider an unmanned aerial vehicle taskedwith taking pictures of a territory. These vehicles are usuallyprovided with the mission, e.g., the coordinates of the targetterritory. The vehicle then autonomously computes the routeand navigates to the target, takes pictures and returns withouthuman intervention.D. Autonomous SystemsAdvances in artificial intelligence are expected to enablebuilding of fully autonomous systems, such as humanoids,that reason and act like humans. They are expected to behighly intelligent learning systems that will be able to takeautonomous decisions without direct human intervention, andwill eventually be outside human control. Another example isautonomous cars that are currently being developed. We referto such systems, that are capable of deciding what task thesystem should perform and how the task is to be performed bythemselves, as autonomous systems. In this kind of systems,the software can be referred to as a softwareagent accordingto the definitions presented in Section III.V. ASSIGNING RESPONSIBILITY AND ACCOUNTABILITYIn this section, based on the classification in the previousone, we assign the responsibility of failure to the differentstakeholders in the lifecycle of a cyberphysical system.A. Automatic System FailuresAutomatic systems are designed to perform a specific task,free from human control. Failures of automatic systems can beattributed to the designersdevelopers of the system, since theyspecify what task the system will perform and how the system will accomplish that. Therefore, the designersdevelopersinvolved must conform to the highest ethical standards. Theymust adopt safety culture as a part of their designdevelopmentprocess and must consult with a safety engineering teamthroughout the development and plan for the use of a cyberphysical system, even under budget and deadline constraints.For example, if an engineer feels that a subsystem has notbeen thoroughly tested or a proper safety study has not beenundertaken, she should discuss with the project manager.Similarly, a project manager must listen to hisher engineeringteam since they are in a better position to identify flaws in thesystem.B. Semiautomatic System FailuresSemiautomatic systems are typically composed of severalautomatic subsystems and one or more humans in the loopusers. Failures in semiautomatic systems can be attributedto either the designerdeveloper that includes training anddocumentation crew, as well as testers, product maintenancefunction or the user. Production is the next link in the chainof responsibilities. The continuous maintenance of the productmight be the responsibility of either producer or the user.Additionally, the users must adequately use the system, e.g.,follow the safety procedures described by the producer. This isquite important because, for example, incorrect use of cyberphysical systems can also result in undesirable consequences.The Bhopal gas tragedy is a classic example of how ignoranceof safety procedures led to a major catastrophe 12. Given thatlarge parts of process industry are controlled and operated ascyberphysical systems this should be kept in mind.In order to understand the attribution of responsibility,let us take as a next example user responsibility of a pilotflying a plane. The pilot must follow the safety instructionsand protocols laid down by the developers and producersof the system, incorporated in safety practices defined andmonitored by a safety department of the company operatingthe aircraft. In that case, the failure of subsystems, such asfuel meters, in spite of the humanintheloop following allsafety instructions, are attributable to the previous links inthe responsibility chain  either maintenance or producers anddesignersdevelopers.C. Semiautonomous System FailuresThe advances in the field of artificial intelligence haveenabled building of systems that are capable of performingintelligent decisionmaking. Many autonomous drones existtoday that are able to perform tasks without human intervention. However, their intelligence is limited to specific tasks,such as surveillance, and cannot be used for other purposes.In these types of systems a user decides what the systemwill perform. Therefore, the user is responsible for the anomalous behaviors caused by its deployment for nonspecifiedtasks. Nonetheless, if the artificial intelligence in the systemtakes a decision that causes an anomalous behavior, e.g., anautonomous drone decides to crash into a building, theaccountability can be traced back to the AI software i.e., thesoftwareagent. When the system consists of automatic subsystems its failures are attributable to the designerdeveloper,if the softwareagent, such as an assisting robot, is unable todetect failures in the subsystems.D. Autonomous System FailuresIn fully autonomous systems, the anomalous behavior of thesystem is the primary responsibility of the softwareagent. Inthis case, there must exist strong evidence that the autonomoussystem is truly autonomous. However, as developer other thana softwareagent is involved in the development of the system,the developer can be deemed responsible only for the failure ofthe automatic subsystems, when it is established that the failuredid not depend on the decisions made by the autonomoussystem. For example, if the braking system of an autonomouscar fails, the autonomous software will be unable to preventa catastrophe. This holds particularly if the softwareagentis unable to automatically detect failures in the subsystems.Therefore the developers involved must be cautious whiledesigning the subsystems and their mutual communications.Nevertheless, the softwareagent has the overarching responsibility in this case and should be given particular care. Ourargument analysing the character of machine responsibilityor accountability of autonomous agents is based on 513 and7. It is argued in 5 and 13 that increasingly autonomousand intelligent agents must have builtin ethical properties,otherwise they can have severe unwanted consequences.For example, in the future, if drone strikes are mandated byautonomous highly intelligent software based on some threatperception, the responsibility which in case of a machineamounts to accountability of an incorrect decision should bewith the software as it is the agent making decision. Whilemany may argue against this assignment of accountabilitymachine responsibility to a machine, we believe that apolicy vacuum in this regard is even more dangerous if weallow autonomous agents be without any moral guidelines.VI. THE ETHICAL CHALLENGE OF EMBEDDED ANDCYBERPHYSICAL SYSTEMSIn this section, we structure the ethical challenges facedby designersdevelopersproducers, users and software actors.This enables us to take the very first steps towards formulatingguidelines and policies in order to fill the policy vacuumas defined by Moor 6. The idea of ethical responsibilityaccountability of software agents is strongly criticized andrefuted by many 11. Critics argue that the designers anddevelopers can still influence the decisions of the software,as they develop and deploy its first version, which then isallowed to learn from its interactions with the environment.Underlying is the idea that we can predict possible behaviorsof an autonomous intelligent system. That idea, however isunjustified. For a future autonomous intelligent cyberphysicalsystem we can be sure that we will not be able to predict itsbehavior  that lies in the nature of autonomy. We point outthat the class of systems whose behavior can be predictedlie in the context of what we refer to as automatic systems.An example of such a system is an automatic missile defensesystem. In this paper, we clearly differentiate automatic systems from autonomous systems that are highly intelligent andoutside of immediate human control. While developers can,to some extent, influence the behavior of software agents,the negligence of responsibilityaccountability of softwarecan leave a hazardous policy vacuum in the projected use ofAI in cyberphysical systems. Therefore, in order to formulate effective policies regarding the ethical aspects of cyberphysical systems, the responsibility that is accountabilityof software agents cannot be ignored as they act in the domainwhere natural intelligent agents have moral responsibility andtechnological artifactual agents should have correspondingartifactual moral responsibility designedbuilt in as long aspossible 5. Deploying highly autonomous intelligent cyberphysical systems without any moralethical considerations bydesign can have unforeseeable negative consequences 5 13.For the sake of discussion, we structure the ethical boundaries of the different stakeholders based on the responsibilityassociated with the safe functioning of the cyberphysicalsystem for each of them. Each stakeholder, be it a designerdeveloperproducer, user or a software agent is boundedby the responsibilities described in Section V that make themcommitted to all the other stakeholders e.g., passengers inan aircraft or the environment. This is important since eachstakeholder acts assuming that the other stakeholder havedone their job properly e.g., a pilot flies an aircraft assumingthe designerdeveloperproducer have designed, developed andmanufactured properly that the maintenance crew have donetheir job and that flight control functions flawlessly and soon. It is the moral duty of all stakeholders to dischargetheir duties with the highest integrity and rigor. Since inthis framework it is in principle possible to map anomalousbehavior to a particular stakeholder or a group of stakeholders,each stakeholder must stick to the highest ethical standards toprevent the anomalous behaviors within hisher responsibility.A. Communicating Information ResponsiblyWhen there are multiple stakeholders in the picture,there is a collective responsibility that requires the designersdevelopersproducers, users and software actors to interactresponsibly, especially without ambiguity, to ensure safe operation of the cyberphysical system 13. There should bewell defined interfaces so that relevant information is madeavailable completely, and without ambiguity, especially bymanufacturers who may not want to disclose certain information for various reasons. Communication protocols mustbe developed for this purpose, and communication must bemaintained at least until the operation of the cyberphysicalsystem is stable e.g., the users are adequately trained. Additionally, there must be sufficient infrastructure in place tocommunicate back diagnostic information to the developersto improve safety.This highlights another important area for which ethicalpolicies need to be formulated type and amount of information that different stakeholders must disclose. For example,manufacturers may not want to disclose certain details ofthe cyberphysical system e.g., so that they dont lose theircompetitive edge. Also, many users may not want to spendtoo much time and money in getting trained at using thesystem because of financial constraints. Similarly, there couldbe inconsistencies in the communication interfaces, especiallybetween AI based softwareagents and humans, due to whichinformation may get lost, remain unattended or is misinterpreted. For example, in the case of classical safetycriticalsystems, Leveson cites poor information flow as one of thereasons for software related accidents 3. Therefore, the needfor communication among different stakeholders requires usto investigate the ethical aspects of communication withemphasis on what information needs to be disclosed forethical reasons, while protecting business secrets. To summarize, the main challenge involved in such a communicationinfrastructure is that there should be an adequate transferof relevant information pertaining to the safe operation ofsystems especially between AI based software agents andhumans.B. Attributing Responsibility to SoftwareIn this context, the responsibility of softwareagents canbecome a contentious issue 11 5 13. The main claimof our article is that new autonomous and intelligent cyberphysical systems must be accompanied by suitable selfregulatory mechanism that will assure their ethical behavior.It is still a widely debated topic and some ethicists refuseany possibility of machine ethicsethical machines 11 whilesome others defend the same line of argument that we present141516. We however, reiterate our argument that ignoringresponsibility of software can only leave the process of addressing the policy vacuum incomplete. Softwareagents thatare highly intelligent must be programmed to understandand apply ethics in order to guarantee that they learn andevolve just like humans do. Such an openminded approachtowards the ethical responsibility of softwareagents enablessocial sustainability, also in the context of legal proceduresarising out of accidents. On the contrary, if the softwareagents associated with cyberphysical systems are providedimmunity from responsibility, accidents that occur due towrong decisions taken by softwareagents will remain acontroversial issue. Also, other stakeholders who are in no wayresponsible for such accidents risk being penalized. The risk oflitigation may also make the stakeholders, such as developers,reluctant to adopt the advanced capabilities provided by thefield of artificial intelligence in building safer and moreefficient systems e.g., the developers may be discouragedfrom using AI in systems to achieve energy efficiency byimproving operational efficiency which may require completeautonomy for the softwareagents. This reluctance may proveto be detrimental since it may further undermine the goal ofsocial sustainability. Aircraft accidents suspected to be causedby pilots actualize discussions if not completely automaticsystems would be safer.In this regard, it is important that attributing the responsibility of failures to fully autonomous software gain widespreadacceptance among diverse communities such as governmental,legal, usercommunities etc. This can be quite challengingbecause of two main concerns fear of the unknown andinertia to change. While the first of the concerns can beaddressed to a great extent by proper research and education,the second problem is more difficult. For example, there arehuge legal obstacles for autonomous cars in some countries.In order to adopt truly revolutionary technologies, the societymust be prepared to take risks associated with attributingresponsibility of failures to software while continuouslylearning from experience and improving technology.VII. CONCLUSIONSIn this paper, we presented a framework for assessment andattribution of responsibility based on classification of cyberphysical systems with respect to the amount of autonomy andautomation involved. Our framework allows the traceability ofanomalous behaviors to the responsible agents, particularly AIbased softwareagents, making it possible to structure and demarcate ethical responsibilities. Besides specific stakeholdersresponsibility we also highlight the importance of collectiveresponsibility 1415 that addresses the central problem ofinformation communication between different stakeholders.It requires developers, users and softwareactors to interactresponsibly, using unambiguous communication protocols toprevent incomplete or incorrect information exchange. Thecentral role of communication as a basis of distributed responsibility presents a topic for future research. We advocatea casebycase study to delineate ethical aspects of such acommunication protocol. Finally, we elaborate the risks associated with the resistance to attribute responsibility machineresponsibility or accountability of failure to the AI software.The main obstacle is the reluctance to adopt advances in AIfor emerging autonomous intelligent cyberphysical systemsas moral agents.We conclude by emphasizing that our framework is the firstand novel attempt to base the reasoning about attribution ofresponsibility including artifactual responsibility of softwareagents on the systematization of types of cyberphysicalsystems with respect to the type of decision making into automatic, semiautomatic, semiautonomous and autonomous.Future work is expected to develop a more detailed schemewhich involves identifying the applicationspecific ethical responsibilities of each entity for some sample of typical cyberphysical applications.ACKNOWLEDGMENTSWe would like to thank an anonymous reviewer who suggested us connection to the topic of Intelligent regulatorycompliance and I modeling language which could be used tointelligently assure compliance of a complex system to safetyrequirements. This will add one more layer of regulatorycompliance AI on the top of the decisionmaking AI as apart of cyberphysical system.REFERENCES1 R. Rajkumar, I. Lee, L. Sha, and J. Stankovic, Cyberphysical systems The next computing revolution, in The 47th ACMIEEE DesignAutomation Conference, June 2010.2 J. Bowen, The ethics of safetycritical systems, Communications ofthe ACM, 2000.3 N. G. Leveson, The role of software in spacecraft accidents, AIAAJournal of Spacecraft and Rockets, 2004.4 , Highpressure steam engines and computer software, in Proceedings of the 14th International Conference on Software Engineering.ACM, 1992.5 G. DodigCrnkovic and D. Persson, Sharing moral responsibility withrobots A pragmatic approach, In Proceedings of the Tenth Scandinavian Conference on Artificial Intelligence Vol. 173, 2008.6 J. Moor, Just consequentialism and computing, Ethics and InformationTechnology, 1999.7 G. DodigCrnkovic, Computing curricula Social, ethical and professional issues, In proceedings of the Conference for the Promotion ofResearch in IT in Sweden, 2003.8 E. Georgiadou and P. Oriogun, Professional issues in software engineering curricula case studies on ethical decision making, in Proceedingsof the International Symposium on Technology and Society, 2001.9 D. G. Johnson, Ethics online, Communnications of ACM, 1997.10 L. Floridi, Information ethics On the philosophical foundation ofcomputer ethics, Ethics and Information Technology, 1999.11 M. Noorman and D. Johnson, Negotiating autonomy and responsibilityin military robots, Ethics and Information Technology, 2014.12 N. Leveson, A new accident model for engineering safer systems,Safety Science, 2004.13 G. DodigCrnkovic and B. Curuklu, Robots  ethical by design,Springer Special issue on Requirements Engineering Ethics and Information Technology, August 2011.14 A. Adam, Delegating and distributing morality Can we inscribe privacyprotection in a machine Ethics and Information Technology, 2005.15 C. Allen, I. Smit, and W. Wallach, Artificial morality Topdown,bottomup, and hybrid approaches, Ethics and Information Technology,2005.16 M. Anderson and S. L. Anderson, Artificial morality Topdown,bottomup, and hybrid approaches, AI Magazine, 2007.
