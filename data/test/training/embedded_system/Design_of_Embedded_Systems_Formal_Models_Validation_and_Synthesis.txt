Design of Embedded Systems FormalModels, Validation, and SynthesisS. Edwards, L. Lavagno, E. A. Lee, and A. SangiovanniVincentelliNovember 5, 1999AbstractThis paper addresses the design of reactive realtime embedded systems. Such systems are often heterogeneous in implementation technologiesand design styles, for example by combining hardware ASICs with embedded software. The concurrent design process for such embedded systemsinvolves solving the specification, validation, and synthesis problems. Wereview the variety of approaches to these problems that have been taken.1 IntroductionReactive realtime embedded systems are pervasive in the electronics system industry. Applications include vehicle control, consumer electronics, communication systems, remote sensing, and household appliances. In such applications,specifications may change continuously, and timetomarket strongly affects success. This calls for the use of software programmable components with behaviorthat can be fairly easily changed. Such systems, which use a computer to perform a specific function, but are neither used nor perceived as a computer, aregenerically known as embedded systems. More specifically, we are interested inreactive embedded systems. Reactive systems are those that react continuously totheir environment at the speed of the environment. They can be contrasted withinteractive systems, which react with the environment at their own speed, andtransformational systems, which take a body of input data and transform it into abody of output data Ber89.1A large percentage of the worldwide market for microprocessors is filled bymicrocontrollers that are the programmable core of embedded systems. In addition to microcontrollers, embedded systems may consist of ASICs andor fieldprogrammable gate arrays as well as other programmable computing units such asDigital Signal Processors DSPs. Since embedded systems interact continuouslywith an environment that is analog in nature, there must typically be componentsthat perform AD and DA conversions. A significant part of the design problemconsists of deciding the software and hardware architecture for the system, as wellas deciding which parts should be implemented in software running on the programmable components and which should be implemented in more specializedhardware.Embedded systems often are used in life critical situations, where reliabilityand safety are more important criteria than performance. Today, embedded systems are designed with an ad hoc approach that is heavily based on earlier experience with similar products and on manual design. Use of higher level languagessuch as C helps somewhat, but with increasing complexity, it is not sufficient. Formal verification and automatic synthesis of implementations are the surest ways toguarantee safety. However, both formal verification and synthesis from high levels of abstraction have been demonstrated only for small, specialized languageswith restricted semantics. This is at odds with the complexity and heterogeneityfound in typical embedded systems.We believe that the design approach should be based on the use of one ormore formal models to describe the behavior of the system at a high level ofabstraction, before a decision on its decomposition into hardware and softwarecomponents is taken. The final implementation of the system should be made asmuch as possible using automatic synthesis from this high level of abstraction toensure implementations that are correct by construction. Validation throughsimulation or verification should be done as much as possible at the higher levelsof abstraction.A typical hardware architecture for an embedded system is illustrated in Figure 1. This type of architecture combines custom hardware with embedded software, lending a certain measure of complexity and heterogeneity to the design.Even within the software or hardware portions themselves, however, there is oftenheterogeneity. In software, controloriented processes might be mixed under thesupervision of a multitasking realtime kernel running on a microcontroller. In addition, hardrealtime tasks may run cooperatively on one or more programmableDSPs. The design styles used for these two software subsystems are likely to2system busASIC microcontrollercontrol panelrealtimeoperatingsystemcontrollerprocessuser interfaceprocessprogrammableDSPDSPassemblycodeprogrammableDSPdualported memoryDSPassemblycodeCODEChardware softwareFigure 1 A typical reactive realtime embedded system architecture.be quite different from one another, and testing the interaction between them isunlikely to be trivial.The hardware side of the design will frequently contain one or more ASICs,perhaps designed using logic or behavioral synthesis tools. On the other hand, asignificant part of the hardware design most likely consists of interconnections ofcommodity components, such as processors and memories. Again, this time onthe hardware side, we find heterogeneity. The design styles used to specify andsimulate the ASICs and the interconnected commodity components are likely tobe quite different. A typical system, therefore, not only mixes hardware designwith software design, but also mixes design styles within each of these categories.Most often the set of tasks that the system implements are not specified in arigorous and unambiguous fashion, so the design process requires several iterations to obtain convergence. Moreover, during the design process, the level ofabstraction, detail, and specificity in different parts of the design varies. To complicate matters further, the skill sets and design styles used by different engineerson the project are likely to be different. The net result is that during the designprocess, many different specification and modeling techniques will be used.Managing the design complexity and heterogeneity is the key problem. We3believe that the use of formal models and highlevel synthesis for ensuring safeand correct designs depends on understanding the interaction between diverse formal models. Only then can the simplicity of modeling required by verification andsynthesis be reconciled with the complexity and heterogeneity of realworld design.The concurrent design process for mixed hardwaresoftware embedded systems involves solving the following subproblems specification, validation, andsynthesis. Although these problems cannot be entirely separated, we deal withthem below in three successive sections.2 Specification and modelingThe design process is often viewed as a sequence of steps that transforms a set ofspecifications described informally into a detailed specification that can be usedfor manufacturing. All the intermediate steps are characterized by a transformation from a more abstract description to a more detailed one.A designer can perform one or more steps in this process. For the designer,the input description is a specification, the final description of the design is animplementation. For example, a software designer may see a set of routines written in C as an implementation of herhis design even though several other stepsmay be taken before the design is ready for manufacturing. During this process,verification of the quality of the design with respect to the demands placed onits performance and functionality has to be carried out. Unfortunately, the descriptions of the design at its various stages are often informal and not logicallyconnected by a set of precise relationships.We advocate a design process that is based on representations with precisemathematical meaning so that both the verification and the map from the initialdescription to the various intermediate steps can be carried out with tools of guaranteed performance. Such an approach is standard in certain communities, wherelanguages with strong formal properties are used to ensure robust design. Examples include ML MTH90, dataflow languages e.g. Lucid WA85, HaskellDav92 and synchronous languages e.g., Lustre, Signal, Esterel Hal93.There is a broad range of potential formalizations of a design, but most toolsand designers describe the behavior of a design as a relation between a set of inputsand a set of outputs. This relation may be informal, even expressed in naturallanguage. It is easy to find examples where informal specifications resulted in4unnecessary redesigns. In our opinion, a formal model of a design should consistof the following components1. A functional specification, given as a set of explicit or implicit relationswhich involve inputs, outputs and possibly internal state information.2. A set of properties that the design must satisfy, given as a set of relationsover inputs, outputs, and states, that can be checked against the functionalspecification.3. A set of performance indices that evaluate the quality of the design in termsof cost, reliability, speed, size, etc., given as a set of equations involving,among other things, inputs and outputs.4. A set of constraints on performance indices, specified as a set of inequalities.The functional specification fully characterizes the operation of a system, whilethe performance constraints bound the cost in a broad sense. The set of properties is redundant, in that in a properly constructed design, the functional specification satisfies these properties. However, the properties are listed separatelybecause they are simpler and more abstract and also incomplete compared to thefunctional specification. A property is an assertion about the behavior, rather thana description of the behavior. It is an abstraction of the behavior along a particularaxis. For example, when designing a network protocol, we may require that thedesign never deadlock this is also called a liveness property. Note that livenessdoes not completely specify the behavior of the protocol it is instead a propertywe require our protocol to have. For the same protocol, we may require that anyrequest will eventually be satisfied this is also called fairness. Again this doesnot completely specify the behavior of the protocol but it is a required property.Given a formal model of the functional specifications and of the properties,we can classify properties in three groups1. Properties that are inherent in the model of computation i.e., they can beshown formally to hold for all specifications described using that model.We will define later on what we mean exactly by inputs, outputs and state information. Fornow, consider them as sequences of values.52. Properties that can be verified syntactically for a given specification i.e.,they can be shown to hold with a simple, usually polynomialtime, analysisof the specification.3. Properties that must be verified semantically for a given specification i.e.,they can be shown to hold by executing, at least implicitly, the specificationfor all inputs that can occur.For example, consider the property of determinate behavior, i.e., the fact thatthe output of a system depends only on its inputs and not on some internal, hidden choice. Any design described by a dataflow network a formal model to bedescribed later is determinate, and hence this property need not be checked. Ifthe design is represented by a network of FSMs, determinacy can be assessed byinspection of the state transition function. In some discrete event models for example those embodied in Verilog and VHDL determinacy is difficult to prove itmust be checked by exhaustive simulation.The design process takes a model of the design at a level of abstraction andrefines it to a lower one. In doing so, the designer must ensure that the propertiesat that level of abstraction are verified, that the constraints are satisfied, and thatthe performance indices are satisfactory. The refinement process involves alsomapping constraints, performance indices and properties to the lower level so thatthey can be computed for the next level down.Figure 2 shows a key refinementstage in embedded system design. The more abstract specification in this case isan executable functional model that is closer to the problem level. The specification undergoes a synthesis process which may be partly manual that generatesa model of an implementation in hardware. That model itself may still be fairlyabstract, capturing for example only timing properties. In this example the modelis presumably used for hardwaresoftware partitioning.While figure 2 suggests a purely topdown process, any real design needsmore interaction between specification and implementation. Nonetheless, when adesign is complete, the best way to present and document it is top down. This isenough to require that the methodology support topdown design.The refinement process can be defined formally once the models of the design are formallyspecified, see McMillan McM93.6imperative FSMs dataflowdiscreteeventcompilersoftwaresynthesisbehavioralsynthesislogicsynthesispartitioningprocessormodelprocessormodelHW simul.modelHW simul.modelSpecificationRefinementImplementationdecreasing abstractionFigure 2 An example of a design refinement stage, which uses hardware andsoftware synthesis to translate a functional specification into a model of hardware.72.1 Elements of a Model of ComputationA language is a set of symbols, rules for combining them its syntax, and rulesfor interpreting combinations of symbols its semantics. Two approaches to semantics have evolved, denotational and operational. A language can have bothideally they are consistent with one another, although in practice this can be difficult to achieve. Operational semantics, which dates back to Turing machines,gives meaning of a language in terms of actions taken by some abstract machine,and is typically closer to the implementation. Denotational semantics, first developed by Scott and Strachey Sto77, gives the meaning of the language in termsof relations.How the abstract machine in an operational semantics can behave is a featureof what we call the model of computation underlying the language. The kinds ofrelations that are possible in a denotational semantics is also a feature of the model of computation. Other features include communication style, how individualbehavior is aggregated to make more complex compositions, and how hierarchyabstracts such compositions.A design at all levels of the abstraction hierarchy from functional specification to final implementation is generally represented as a set of components,which can be considered as isolated monolithic blocks, interacting with each otherand with an environment that is not part of the design. The model of computationdefines the behavior and interaction of these blocks.In the sections that follow, we present a framework for comparing elementsof different models of computation, called the taggedsignal model, and use it tocontrast different styles of sequential behavior, concurrency, and communication.We will give precise definitions for a number of terms, but these definitions willinevitably conflict with standard usage in some communities. We have discoveredthat, short of abandoning the use of most common terms, no terminology can beconsistent with standard usage in all related communities. Thus we attempt toavoid confusion by being precise, even at the risk of being pedantic.2.1.1 The TaggedSignal ModelTwo of the authors Lee and SangiovanniVincentelli have proposed the taggedsignal model LSV96, a formalism for describing aspects of models of computation for embedded system specification. It is denotational in the Scott andStrachey Sto77 sense, and it defines a semantic framework of signals and pro8cesses within which models of computation can be studied and compared. It isvery abstractdescribing a particular model of computation involves imposingfurther constraints that make it more concrete.The fundamental entity in the TaggedSignal Model is an eventa valuetagpair. Tags are often used to denote temporal behavior. A set of events an abstractaggregation is a signal. Processes are relations on signals, expressed as sets of tuples of signals. A particular model of computation is distinguished by theorder it imposes on tags and the character of processes in the model.Given a set of valuesand a set of tags  , an event is a member of ,i.e., an event has a tag and a value. A signal  is a set of events. A signal can beviewed as a subset of . A functional signal is a possibly partial functionfrom  to. The set of all signals is denoted  . A tuple of  signals is denoted , and the set of all such tuples is denoted  .The different models of time that have been used to model embedded systemscan be translated into different order relations on the set of tags  in the taggedsignal model. In particular, in a timed system  is totally ordered, i.e., there is abinary relation  on members of  such that if      and     , then either or     . In an untimed system,  is only partially ordered.A process  with  signals is a subset of the set of all  tuples of signals, for some  . A particular    is said to satisfy the process if    . An  thatsatisfies a process is called a behavior of the process. Thus a process is a set ofpossible behaviors, or a relation between signals.For many but not all applications, it is natural to partition the signals associated with a process into inputs and outputs. Intuitively, the process does notdetermine the values of the inputs, and does determine the values of the outputs.If   , then    is a partition of  . A process with  inputs and  outputs is a subset of     . In other words, a process defines a relation betweeninput signals and output signals. A    tuple      is said to satisfy  if . It can be written     ,    , where     is an  tuple of input signalsfor process  and  .   is an  tuple of output signals for process  . If theinput signals are given by     , then the set 0 21   3   4  5  76 describesthe inputs, and 098 is the set of behaviors consistent with the input   .A process  is functional with respect to a partition if it is a singlevalued,possibly partial, mapping from   to   . That is, if   3      and   3     ,then     . In this case, we can write        , where   is a possibly partial function. Given the input signals, the output signals aredetermined or there is unambiguously no behavior.9Consider, as a motivating example introducing these several mechanisms todenote temporal behavior, the problem of modeling a timeinvariant dynamicalsystem on a computer. The underlying mathematical model, a set of differentialequations over continuous time, is not directly implementable on a digital computer, due to the double quantization of real numbers into finite bit strings, andof time into clock cycles. Hence a first translation is required, by means of anintegration rule, from the differential equations to a set of difference equations,that are used to compute the values of each signal with a given tag from the valuesof some other signals with previous andor current tags.If it is possible to identify several strongly connected components in the dependency graph, then the system is decoupled. It becomes then possible to gofrom the total order of tags implicit in physical time to a partial order imposedby the depthfirst ordering of the components. This partial ordering gives us somefreedom in implementing the integration rule on a computer. We could, for example, play with scheduling by embedding the partial order into the total orderamong clock cycles. It is often convenient, for example, to evaluate a componentcompletely, for all tags, before evaluating components that depend on it. Or it ispossible to spread the computation among multiple processors.In the end, time comes back into the picture, but the double mapping, fromtotal to partial order, and back to total order again, is essential to1. prove properties about the implementation e.g., stability of the integrationmethod, a bound on the maximum execution time, . . . ,2. optimize the implementation with respect to a given cost function e.g., sizeof the buffers required to hold intermediate signals versus execution time,satisfaction of a constraint on the maximum execution time, . . . ,2.1.2 StateMost models of computation include components with state, where behavior isgiven as a sequence of state transitions. In order to formalize this notion, let usconsider a process  that is functional with respect to partition       . Let usassume for the moment that  belongs to a timed system, in which tags are totallyordered. Then for any tuple of signals  , we can define  to be a tuple of thepossibly empty subset of the events in  with tags greater than  .A directed graph with a node for each signal, and an edge between two signals whenever theequation for the latter depends on the former.10Two input signal tuples       are in relation denoted        if      implies          . This definition intuitively means thatprocess  cannot distinguish between the histories of  and  prior to time  .Thus, if the inputs are identical after time  , then the outputs will also be identical. is an equivalence relation, partitioning the set of input signal tuples intoequivalence classes for each  . Following a long tradition, we call these equivalence classes the states of  . In the hardware community, components with onlyone state for each  are called combinational, while components with more thanone state for some  are called sequential. Note however that the term sequentialis used in very different ways in other communities.2.1.3 DecidabilityComponents with a finite number of states differ significantly from those with aninfinite number of states. For certain infinitestate models those that are Turingcomplete, many desirable properties are undecidablethey cannot be determinedin a finite amount of time for all systems. These properties include whether asystem will need more memory than is available, whether a system will halt, andhow fast a system will run. Hopcroft and Ullman HU79 discuss these issues atlength.Undecidability is not an insurmountable barrier, and decidability is not sufficient to answer all questions in practice e.g., because the required runtime maybe prohibitive. Many successful systems have been designed using undecidablelanguages i.e., those in which questions about some programs are undecidable.Although no algorithm can solve an undecidable problem for all systems, algorithms exist that can solve them for most systems. Bucks Boolean Dataflowscheduler Buc93, for example, can answer the halting and bounded memoryproblems for many systems specified in a Turingcomplete dataflow model, although it does, necessarily, fail to reach a conclusion for some systems.The nonterminating nature of embedded systems opens the possibility of using infinite time to solve certain undecidable problems. Parks Par95 scheduler,for example, will execute a potentially infinitestate system forever in boundedmemory if it is possible to do so. However, it does not answer the question of howmuch memory is needed or whether the program will eventually halt.The classical von Neumann model of computation  is a familiar model ofsequential behavior. A memory stores the state and a processor advances the stateIt is formalized in the abstract model called random access machine or random access stored11through a sequence of memory operations. Most commonlyused programminglanguages e.g., C, C, Lisp, Pascal, FORTRAN use this model of computation.Often, the memory is viewed as having an unbounded number of finitevaluedwords, which, when coupled with an appropriate choice of processor instructions,makes the model Turing complete  . Modern computer systems make this modelpractical by simulating unbounded memory with an elaborate hierarchy registers,cache, RAM, hard disk. Few embedded systems, however, can currently affordsuch a scheme.2.1.4 Concurrency and CommunicationWhile sequential or combinational behavior is related to individual processes, embedded systems will typically contain several coordinated concurrent processes.At the very least, such systems interact with an environment that evolves independently, at its own speed. But it is also common to partition the overall modelinto tasks that also evolve more or less independently, occasionally or frequentlyinteracting with one another.Communication between processes can be explicit or implicit. In explicit communication, a sender process informs one or more receiver processes about somepart of its state. In implicit communication, two or more processes share a common notion of state.Time plays a larger role in embedded systems than in classical computation.In classical transformational systems, the correct result is the primary concernwhen it arrives is less important although whether it arrives, the termination question, is important. By contrast, embedded systems are usually realtime systems,where the time at which a computation takes place can be more important than thecomputation itself.As we discussed above, different models of time become different order relations on the set of tags  in the tagged signal model. Recall that in a timed system is totally ordered, while in an untimed system  is only partially ordered. Implicit communication generally requires totally ordered tags, usually identifiedwith physical time.The tags in a metrictime system have the notion of a distance between them,much like physical time. Formally, there exists a partial function   program SS63.Turingcompleteness can be obtained also with a finite number of infinitevalued words.12mapping pairs of tags to real numbers such that , , 3  and 3   , .A discreteevent system is a timed system where the tags in each signal areorderisomorphic with the integers for a twosided system or the natural numbersfor a onesided system LSV96. Intuitively, this means that any pair of orderedtags has a finite number of intervening tags.Two events are synchronous if they have the same tag. Two signals are synchronous if each event in one signal is synchronous with an event in the othersignal and vice versa. A system is synchronous if every signal in the system issynchronous with every other signal in the system. A discretetime system is asynchronous discreteevent system.Synchronousreactive languages see e.g. Hal93 are synchronous in exactlythis sense. The set of tags in a behavior of the system denotes a global clock forthe system. Every signal conceptually has an event at every tag, although in somemodels this event could have a value denoting the absence of an event calledbottom. At each clock tick, each process maps input values to output values.If cyclic communication is allowed, then some mechanism must be provided toresolve or prevent circular dependencies. One possibility is to constrain the outputvalues to have tags corresponding to the next tick. Another possibility all toocommon is to leave the result unspecified, resulting in nondeterminacy or worse,infinite computation within one tick. A third possibility is to use fixedpointsemantics, where the behavior of the system is defined as a set of events thatsatisfy all processes.Concurrency in physical implementations of systems occurs through somecombination of parallelism, having physically distinct computational resources,and interleaving, sharing of a common physical resource. Mechanisms for achieving interleaving vary widely, ranging from operating systems that manage contextswitches to fullystatic interleaving in which concurrent processes are convertedcompiled into a single nonconcurrent process. We focus here on the mechanisms used to manage communication between concurrent processes.Parallel physical systems naturally share a common notion of time, accordingto the laws of physics. The time at which an event in one subsystem occurs hasa natural ordering relationship with the time at which an event occurs in anothersubsystem. Physically interleaved systems also share a natural common notion oftime.Logical systems, on the other hand, need a mechanism to explicitly share anotion of time. Consider two imperative programs interleaved on a single pro13cessor under the control of timesharing operating system. Interleaving creates anatural ordering between events in the two processes, but this ordering is generally unreliable, because it heavily depends on scheduling policy, system load andso on. Some synchronization mechanism is required if those two programs needto cooperate.More generally, in logically concurrent systems, maintaining a coherent globalnotion of time as a total order on events, can be extremely expensive. Hence inpractice this is replaced whenever possible with an explicit synchronization, inwhich this total order is replaced by a partial order. Returning to the example oftwo processes running under a timesharing operating system, we take precautionsto ensure an ordering of two events only if the ordering of these two events matters.A variety of mechanisms for managing the order of events, and hence forcommunicating information between processes, has arisen. Some of the mostcommon ones are UnsynchronizedIn an unsynchronized communication, a producer of information and a consumer of the information are not coordinated. There is no guarantee that theconsumer reads valid information produced by the producer, and there is noguarantee that the producer will not overwrite previously produced data before the consumer reads the data. In the taggedsignal model, the repositoryfor the data is modeled as a process, and the reading and writing events haveno enforced ordering relationship between their tags. ReadmodifywriteCommonly used for accessing shared data structures, this strategy locks adata structure between a read and write from a process, preventing any otheraccesses. In other words, the actions of reading, modifying, and writing areatomic indivisible. In the taggedsignal model, the repository for the datais modeled as a process where events associated with this process are totallyordered resulting in a globally partially ordered model. The readmodifywrite is modeled as a single event. Unbounded FIFO bufferedThis is a pointtopoint communication strategy, where a producer generatesa sequence of data tokens and consumer consumes these tokens, but onlyafter they have been generated. In the taggedsignal model, this is a simple14connection where the signal on the connection is constrained to have totallyordered tags. The tags model the ordering imposed by the FIFO model. Ifthe consumer implements blocking reads, then it imposes a total order onevents at all its input signals. This model captures essential properties ofboth Kahn process networks and dataflow Kah74. Bounded FIFO bufferedIn this case, the data repository is modeled as a process that imposes ordering constraints on its inputs which come from the producer and theoutputs which go to the consumer. Each of the input and output signalsare internally totally ordered. The simplest case is where the size of thebuffer is one, in which case the input and output events must be interleavedso that each output event lies between two input events. Larger buffers impose a maximum difference often called synchronic distance between thenumber of input and output events.Note that some implementations of this communication mechanism may notreally block the writing process when the buffer is full, thus requiring somehigher level of flow control to ensure that this never happens, or that it doesnot cause any harm. RendezvousIn the simplest form of rendezvous, implemented for example in Occam andLotos, a single writing process and a single reading process must simultaneously be at the point in their control flow where the write and the readoccur. It is a convenient communication mechanism, because it has the semantics of a single assignment, in which the writer provides the righthandside, and the reader provides the lefthand side. In the taggedsignal model, this is imposed by events with identical tags LSV96. Lotos offers, inaddition, multiple rendezvous, in which one among multiple possible communications is nondeterministically selected. Multiple rendezvous is moreflexible than single rendezvous, because it allows the designer to specifymore easily several expected communication ports at any given time, butit is very difficult and expensive to implement correctly.Of course, various combinations of the above models are possible. For example, in a partially unsynchronized model, a consumer of data may be required to15Transmitters Receivers Buffer Blocking Blocking SingleSize Reads Writes ReadsUnsynchronized many many one no no noReadModifyWrite many many one yes yes noUnbounded FIFO one one unbounded yes no yesBounded FIFO one one bounded yes maybe yesSingle Rendezvous one one one yes yes yesMultiple Rendezvous one one one no no yesTable 1 A comparison of concurrency and communication schemes.wait until the first time a producer produces data, after which the communicationis unsynchronized.The essential features of the concurrency and communication styles describedabove are presented in Table 1. These are distinguished by the number of transmitters and receivers e.g., broadcast versus pointtopoint communication, thesize of the communication buffer, whether the transmitting or receiving processmay continue after an unsuccessful communication attempt blocking reads andwrites, and whether the result of each write can be read at most once singlereads.2.2 Common Models of ComputationWe are now ready to use the scheme developed in the previous Section to classifyand analyze several models of computation that have been used to describe embedded systems. We will consider issues such as ease of modeling, efficiency ofanalysis simulation or formal verification, automated synthesizability, optimization space versus overspecification, and so on.2.2.1 DiscreteEventTime is an integral part of a discreteevent model of computation. Events usuallycarry a totallyordered time stamp indicating the time at which the event occurs.A DE simulator usually maintains a global event queue that sorts events by timestamp.Digital hardware is often simulated using a discreteevent approach. The Verilog language, for example, was designed as an input language for a discreteevent16simulator. The VHDL language also has an underlying discreteevent model ofcomputation.Discreteevent modeling can be expensivesorting time stamps can be timeconsuming. Moreover, ironically, although discreteevent is ideally suited to modeling distributed systems, it is very challenging to build a distributed discreteevent simulator. The global ordering of events requires tight coordination betweenparts of the simulation, rendering distributed execution difficult.Discreteevent simulation is most efficient for large systems with large, frequently idle or autonomously operating sections. Under discreteevent simulation,only the changes in the system need to be processed, rather than the whole system. As the activity of a system increases, the discreteevent paradigm becomesless efficient because of the overhead inherent in processing time stamps.Simultaneous events, especially those arising from zerodelay feedback loops,present a challenge for discreteevent models of computation. In such a situation,events may need to be ordered, but are not.Consider the discreteevent system shown in Figure 3. Process B has zerodelay, meaning that its output has the same time stamp as its input. If process Aproduces events with the same time stamp on each output, there is ambiguity aboutwhether B or C should be invoked first, as shown in Figure 3a.Suppose B is invoked first, as shown in Figure 3b. Now, depending on thesimulator, C might be invoked once, observing both input events in one invocation,or it might be invoked twice, processing the events one at a time. In the latter case,there is no clear way to determine which event should be processed first.The addition of delta delay makes such nondeterminacy easier to prevent, butdoes not avoid it completely. It introduces a twolevel model of time in which eachinstant of time is broken into a potentially infinite number of totallyordereddelta steps. The simulated time reported to the user, however, does not includedelta information. A zerodelay process in this model actually has delta delay.For example, Process B would have delta delay, so firing A followed by B wouldresult in the situation in Figure 3c. The next firing of C will see the event from Aonly the firing after that will see the delaydelayed event from B.Other simulators, including the DE simulator in Ptolemy BHLM94, attemptto statically analyze data precedences within a single time instant. Such precedence analysis is similar to that done in synchronous languages Esterel, Lustre,and Signal to ensure that simultaneous events are processed deterministically. Itdetermines a partial ordering of events with the same time stamp by examiningdata precedences.17A B CA B Ca bA B CA B Cc dFigure 3 Simultaneous events in a discreteevent system. a Process A producesevents with the same time stamp. Should B or C be fired next b Zerodelayprocess B has fired. How many times should C be fired c Deltadelay processB has fired C will consume As output next. d C has fired once it will fire againto consume Bs output.18Adding a feedback loop from Process C to A in Figure 3 would create aproblem if events circulate through the loop without any increment in time stamp. The same problem occurs in synchronous languages, where such loops arecalled causality loops. No precedence analysis can resolve the ambiguity. In synchronous languages, the compiler may simply fail to compile such a program.Some discreteevent simulators will execute the program nondeterministically,while others support tighter control over the sequencing through graph annotations.2.2.2 Communicating Finite State MachinesFinite State Machines FSMs are an attractive model for embedded systems. Theamount of memory required by such a model is always decidable, and is often anexplicit part of its specification. Halting and performance questions are alwaysdecidable since each state can, in theory, be examined in finite time. In practice,however, this may be prohibitively expensive.A traditional FSM consists of a set of input symbols the Cartesian product of the sets of values of theinput signals, a set of output signals the Cartesian product of the sets of values of theoutput signals, a finite set of states with a distinguished initial state, an output function mapping inputs and states to outputs, and a nextstate function mapping inputs and states to next states.The input to such a machine is a sequence of input symbols, and the output is asequence of output symbols.Traditional FSMs are good for modeling sequential behavior, but are impractical for modeling concurrency or memory because of the socalled state explosionproblem. A single machine mimicking the concurrent execution of a group of machines has a number of states equal to the product of the number of states of eachmachine. A memory has as many states as the number of values that can be storedat each location raised to the power of the number of locations. The number of19states alone is not always a good indication of complexity, but it often has a strongcorrelation.Harel advocated the use of three major mechanisms that reduce the size andhence the visual complexity of finite automata for modeling practical systems HLN  90b. The first one is hierarchy, in which a state can represent an enclosed state machine. That is, being in a particular state  has the interpretationthat the state machine enclosed by  is active. Equivalently, being in state  meansthat the machine is in one of the states enclosed by  . Under the latter interpretation, the states of  are called or states. Or states can exponentially reduce thecomplexity the number of states required to represent a system. They compactlydescribe the notion of preemption a highpriority event suspending or killing alower priority task, that is fundamental in embedded control applications.The second mechanism is concurrency. Two or more state machines are viewedas being simultaneously active. Since the system is in one state of each parallelstate machine simultaneously, these are sometimes called and states. They alsoprovide a potential exponential reduction in the size of the system representation.The third mechanism is nondeterminism. While often nondeterminism issimply the result of an imprecise maybe erroneous specification, it can be anextremely powerful mechanism to reduce the complexity of a system model byabstraction. This abstraction can either be due to the fact that the exact functionality must still be defined, or that it is irrelevant to the properties currentlyconsidered of interest. E.g., during verification of a given system component, other components can be modeled as nondeterministic entities to compactlyconstrain the overall behavior. A system component can also be described nondeterministically to permit some optimization during the implementation phase.Nondeterminism can also provide an exponential reduction in complexity.These three mechanisms have been shown in DH94b to cooperate synergistically and orthogonally, to provide a potential triple exponential reduction in thesize of the representation with respect to a single, flat deterministic FSM.Harels Statecharts model uses a synchronous concurrency model also calledsynchronous composition. The set of tags is a totally ordered countable set thatdenotes a global clock for the system. The events on signals are either producedThe exact claim in DH94b was that and type nondeterminism in which all nondeterministic choices must be successful, rather than hierarchical states, was the third sourceof exponential reduction together with or type nondeterminism and concurrency. Hierarchicalstates, on the other hand, were shown in that paper to be able to simulate and nondeterminismwith only a polynomial increase in size.20by state transitions or inputs. Events at a tick of the clock can trigger state transitions in other parallel state machines at the same clock. Unfortunately, Harel leftopen some questions about the semantics of causality loops and chains of instantaneous same tick events, triggering a flurry of activity in the community thathas resulted in at least twenty variants of Statecharts vdB94.Most of these twenty variants use the synchronous concurrency model. However, for many applications, the tight coordination implied by the synchronousmodel is inappropriate. In response to this, a number of more loosely coupledasynchronous FSM models have evolved, including behavioral FSMs TW95,SDL process networks TW95, and codesign FSMs CGH94a.A model that is closely related to FSMs is Finite Automata. FAs emphasize theacceptance or rejection of a sequence of inputs rather than the sequence of outputsymbols produced in response to a sequence of input symbols. Most notions, suchas composition and so on, can be naturally extended from one model to the other.In fact, any of the concurrency models described in this paper can be usefullycombined with FSMs. In the Ptolemy project BHLM94, FSMs are hierarchicallynested with dataflow, discreteevent, or synchronousreactive models CKL95.The nesting is arbitrarily deep and can mix concurrency models at different levelsof the hierarchy. This very flexible model is called charts, pronounced starcharts, where the asterisk is meant to suggest a wildcard.2.2.3 SynchronousReactiveIn a synchronous model of computation, all events are synchronous, i.e., all signals have events with identical tags. The tags are totally ordered, and globallyavailable. Simultaneous events those in the same clock tick may be totally ordered, partially ordered, or unordered, depending on the model of computation.Unlike the discreteevent model, all signals have events at all clock ticks, simplifying the simulator by requiring no sorting. Simulators that exploit this simplification are called cyclebased or cycledriven simulators. Processing all events ata given clock tick constitutes a cycle. Within a cycle, the order in which eventsare processed may be determined by data precedences, which define microsteps. These precedences are not allowed to be cyclic, and typically impose a partialorder leaving some arbitrary ordering decisions to the scheduler. Cyclebasedmodels are excellent for clocked synchronous circuits, and have also been appliedsuccessfully at the system level in certain signal processing applications.A cyclebased model is inefficient for modeling systems where events do not21occur at the same rate in all signals. While conceptually such systems can bemodeled using, for example, special tokens to indicate the absence of an event,the cost of processing such tokens is considerable. Fortunately, the cyclebasedmodel is easily generalized to multirate systems. In this case, every  th event inone signal aligns with the events in another.A multirate cyclebased model is still somewhat limited. It is an excellentmodel for synchronous signal processing systems where sample rates are relatedby constant rational multiples, but in situations where the alignment of events indifferent signals is irregular, it can be inefficient.The more general synchronousreactive model is embodied in the socalledsynchronous languages BB91. Esterel BS91 is a textual imperative languagewith sequential and concurrent statements that describe hierarchicallyarrangedprocesses. Lustre HCRP91 is a textual declarative language with a dataflow flavor and a mechanism for multirate clocking. Signal BG90 is a textual relationallanguage, also with a dataflow flavor and a more powerful clocking system. Argos Mar91, a derivative of Harels Statecharts Har87, is a graphical languagefor describing hierarchical finite state machines. Halbwachs Hal93 gives a goodsummary of this group of languages.The synchronousreactive languages describe systems as a set of concurrentlyexecuting synchronized modules. These modules communicate through signalsthat are either present or absent in each clock tick. The presence of a signal iscalled an event, and often carries a value, such as an integer. The modules are reactive in the sense that they only perform computation and produce output eventsin instants with at least one input event.Every signal in these languages is conceptually or explicitly accompaniedby a clock signal, which has meaning relative to other clock signals and definesthe global ordering of events. Thus, when comparing two signals, the associatedclock signals indicate which events are simultaneous and which precede or followothers. In the case of Signal and Lustre, clocks have complex interrelationships,and a clock calculus allows a compiler to reason about these ordering relationshipsand to detect inconsistencies in the definition. Esterel and Argos have simplerclocking schemes and focus instead on finitestate control.Most of these languages are static in the sense that they cannot request additional storage nor create additional processes while running. This makes themwellsuited for bounded and speedcritical embedded applications, since their behavior can be extensively analyzed at compile time. This static property makes asynchronous program finitestate, greatly facilitating formal verification.22Verifying that a synchronous program is causal noncontradictory and deterministic is a fundamental challenge with these languages. Since computation inthese languages is delayfree and arbitrary interconnection of processes is possible, it is possible to specify a program that has either no interpretation a contradiction where there is no consistent value for some signal or multiple interpretationssome signal has more than one consistent value. Both situations are undesirable, and usually indicate a design error. A conservative approach that checks forcausality problems structurally flags an unacceptably large number of programsas incorrect because most will manifest themselves only in unreachable programstates. The alternative, to check for a causality problem in any reachable state,can be expensive since it requires an exhaustive check of the state space of theprogram.In addition to the ability to translate these languages into finitestate descriptions, it is possible to compile these languages directly into hardware. Techniquesfor translating both Esterel Ber91 and Lustre RH92 into hardware have beenproposed. The result is a logic network consisting of gates and flipflops that canbe optimized using traditional logic synthesis tools. To execute such a system insoftware, the resulting network is simply simulated. The technique is also the basis to perform more efficiently causality checks, by means of implicit state spacetraversal techniques SBT96.2.2.4 Dataflow Process NetworksIn dataflow, a program is specified by a directed graph where the nodes calledactors represent computations and the arcs represent totally ordered sequencescalled streams of events called tokens. In figure 4a, the large circles representactors, the small circle represents a token and the lines represent streams. Thegraphs are often represented visually and are typically hierarchical, in that anactor in a graph may represent another directed graph. The nodes in the graphcan be either language primitives or subprograms specified in another language,such as C or FORTRAN. In the latter case, we are mixing two of the models ofcomputation from figure 2, where dataflow serves as the coordination languagefor subprograms written in an imperative host language.Dataflow is a special case of Kahn process networks Kah74, LP95. In a Kahnprocess network, communication is by unbounded FIFO buffering, and processesare constrained to be continuous mappings from input streams to output streams.Continuous in this usage is a topological property that ensures that the program23is determinate Kah74. Intuitively, it implies a form of causality without timespecifically, a process can use partial information about its input streams to produce partial information about its output streams. Adding more tokens to the inputstream will never result in having to change or remove tokens on the output streamthat have already been produced. One way to ensure continuity is with blockingreads, where any access to an input stream results in suspension of the process ifthere are no tokens. One consequence of blocking reads is that a process cannottest an input channel for the availability of data and then branch conditionally toa point where it will read a different input.In dataflow, each process is decomposed into a sequence of firings, indivisiblequanta of computation. Each firing consumes and produces tokens. Dividing processes into firings avoids the multitasking overhead of context switching in directimplementations of Kahn process networks. In fact, in many of the signal processing environments, a major objective is to statically at compile time schedulethe actor firings, achieving an interleaved implementation of the concurrent model of computation. The firings are organized into a list for one processor or setof lists for multiple processors. Figure 4a shows a dataflow graph, and Figure 4b shows a single processor schedule for it. This schedule is a list of firingsthat can be repeated indefinitely. One cycle through the schedule should returnthe graph to its original state here, state is defined as the number of tokens oneach arc. This is not always possible, but when it is, considerable simplificationresults BML96. In many existing environments, what happens within a firingcan only be specified in a host language with imperative semantics, such as Cor C. In the Ptolemy system BHLM94, it can also consist of a quantum ofcomputation specified with any of several models of computation, such as FSMs,a synchronousreactive subsystem, or a discreteevent subsystem CHL96.A useful formal device is to constrain the operation of a firing to be functional,i.e., a simple, stateless mapping from input values to output values. Note, however, that this does not constrain the process to be stateless, since it can maintainstate in a selfloop an output that is connected back to one of its inputs. An initialtoken on this selfloop provides the initial value for the state.Many possibilities have been explored for precise semantics of dataflow coordination languages, including Karp and Millers computation graphs KM66,Lee and Messerschmitts synchronous dataflow graphs LM87, Lauwereins etal.s cyclostatic dataflow model LWAP94, BELP94, Kaplan et al.s Processing Graph Method PGM K87, Granular Lucid Jag92, and others Ack82,CG89, CH71, SBKB90. Many of these limit expressiveness in exchange for for24A CBDaA B C DbFigure 4 a A dataflow process network b A singleprocessor static schedulefor itmal properties e.g., provable liveness and bounded memory.Synchronous dataflow SDF and cyclostatic dataflow require processes toconsume and produce a fixed number of tokens for each firing. Both have theuseful property that a finite static schedule can always be found that will returnthe graph to its original state. This allows for extremely efficient implementations BML96. For more general dataflow models, it is undecidable whether such aschedule exists Buc93.A looser model of dataflow is the taggedtoken model, in which the partialorder of tokens is explicitly carried with the tokens AG82. A significant advantage of this model is that while it logically preserves the FIFO semantics of thechannels, it permits outoforder execution.Some examples of graphical dataflow programming environments intendedfor signal processing including image processing are Khoros RW91, and Ptolemy BHLM94.2.2.5 Other modelsAnother commonly used partially ordered concurrency model is based on rendezvous. Two or more concurrent sequential processes proceed autonomously,but at certain points in their control flow, coordinate so that they are simultane25ously at specified points. Rendezvous has been developed into elaborate processcalculi e.g., Hoares CSP Hoa78 and Milners CCS Mil89. It has also beenimplemented in the Occam and Lotos programming languages. Ada also uses rendezvous, although the implementation is stylistically quite different, using remoteprocedure calls rather than more elementary synchronization primitives.Rendezvousbased models of computation are often called synchronous. However, by the definition we have given, they are not synchronous. Events are partially ordered, not totally ordered, with rendezvous points imposing the partialordering constraints.No discussing of concurrent models of computation would be complete without mentioning Petri nets Pet81, Rei85. Petri nets are, in their basic form, neitherTuring complete nor finite state. They are interesting as uninterpreted model forseveral very different classes of problems, including some relevant to embeddedsystem design e.g., process control, asynchronous communication, scheduling,. . . . Many questions about Petri nets can be answered in finite time. Moreover, alarge user community has developed a large body of theoretical results and practical design aids and methods based on them. In particular, partial orderbasedverification methods e.g. Val92, God90, McM93 are one possible answerto the state explosion problem plaguing FSMbased verification techniques.2.3 LanguagesThe distinction between a language and its underlying model of computation isimportant. The same model of computation can give rise to fairly different languages e.g., the imperative Algollike languages C, C, Pascal, and FORTRAN.Some languages, such as VHDL and Verilog, support two or more models ofcomputation.The model of computation affects the expressiveness of a language  whichbehaviors can be described in the language, whereas the syntax affects compactness, modularity, and reusability. Thus, for example, objectoriented propertiesof imperative languages like C are more a matter of syntax than a model ofcomputation.The expressiveness of a language is an important issue. At one extreme, aThey directly support the Imperative model within a process, and the Discrete Event model among processes. They can also support Extended Finite State Machines under suitable restrictionsknown as the synthesizable subset.26language that is not expressive enough to specify a particular behavior is clearlyunsuitable, but the other extreme also raises problems. A language that is tooexpressive often raises the complexity of analysis and synthesis. In fact, for veryexpressive languages, many analysis and synthesis problems become undecidableno algorithm will solve all problem instances in finite time.A language in which a desired behavior cannot be represented succinctly isalso problematic. The difficulty of solving analysis and synthesis problems is atleast linear in the size of the problem description, and can be as bad as severaltimes exponential, so choosing a language in which the desired behavior of thesystem is compact can be critical.A language may be very incomplete andor very abstract. For example, it mayspecify only the interaction between computational modules, and not the computation performed by the modules. Instead, it provides an interface to a hostlanguage that specifies the computation, and is called a coordination languageexamples include Linda CG89, Granular Lucid Jag92, and Ptolemy domains BHLM94. Or the language may specify only the causality constraints of theinteractions without detailing the interactions themselves nor providing an interface to a host language. In this case, the language is used as a tool to prove properties of systems, as done, for example, in process calculi Hoa78, Mil89 and Petrinets Pet81, Rei85. In still more abstract modeling, components in the system arereplaced with nondeterminate specifications that give constraints on the behavior,but not the behavior itself. Such abstraction provides useful simplifications thathelp formal verification.2.4 Heterogeneous Models of ComputationThe variety of models of computation that have been developed is only partially due to immaturity in the field. It appears that different models fundamentallyhave different strengths and weaknesses, and that attempts to find their commonfeatures result in models that are very low level, difficult to use. These low levelmodels such as Dijkstras PV systems Dij68 provide a good theoretical foundation, but not a good basis for design.Thus we are faced with two alternatives in designing complex, heterogeneoussystems. We can either use a single unified approach and suffer the consequences,or we can mix approaches. To use the unified approach today we could choosebetween VHDL and C for a mixed hardware and software design, doing the entiredesign in one or the other i.e. specifying the software in VHDL or the hardware27in C. Or worse, we could further bloat the VHDL language by including a subsetdesigned for software specification e.g. by making Ada a subset of VHDL. In thealternative that we advocate, we mix approaches while keeping them conceptuallydistinct, for example by using both VHDL and C in a mixed hardwaresoftwaredesign.The key problem in the mixed approach, then, is to define the semantics of theinteraction of fundamentally different models of computation. It is not simply aproblem of interfacing languages. It is easy, for example, to provide a mechanismfor calling C procedures from VHDL. But what does it mean if two concurrentVHDL entities call C procedures that interact The problem is exacerbated by thelack of agreedupon semantics for C or VHDL.Studying the interaction semantics of mixed models of computation is themain objective of the Ptolemy project BHLM94. There, a hierarchical framework is used, where a specification in one model of computation can contain aprimitive that is internally implemented using another model of computation. Theobjectoriented principle of information hiding is used to isolate the models fromone another as much as possible.3 ValidationValidation loosely refers to the process of determining that a design is correct.Simulation remains the main tool to validate a model, but the importance of formalverification is growing, especially for safetycritical embedded systems. Althoughstill in its infancy, it shows more promise than verification of arbitrary systems,such as generic software programs, because embedded systems are often specifiedin a more restricted way. For example, they are often finitestate.Many safety properties including deadlock detection can be detected in atimeindependent way using existing model checking and language containment methods see, e.g., Kurshan Kur94 and Burch et al. BCMD90. Unfortunately, verifying most temporal properties is much more difficult Alur and Henzinger AH92 provide a good summary. Much more research is needed beforethis is practical.283.1 SimulationSimulating embedded systems is challenging because they are heterogeneous. Inparticular, most contain both software and hardware components that must besimulated at the same time. This is the cosimulation problem.The basic cosimulation problem is reconciling two apparently conflicting requirements to execute the software as fast as possible, often on a host machine that maybe faster than the final embedded CPU, and certainly is very different fromit and to keep the hardware and software simulations synchronized, so that theyinteract just as they will in the target system.One approach, often taken in practice, is to use a generalpurpose softwaresimulator based, e.g., on VHDL or Verilog to simulate a model of the targetCPU, executing the software program on this simulation model. Different modelscan be employed, with a tradeoff between accuracy and performance Gatelevel modelsThese are viable only for small validation problems, where either the processor is a simple one, or very little code needs to be run on it, or both. Instructionset architecture ISA models augmented with hardware interfacesAn ISA model is a standard processor simulator often written in C augmented with hardware interface information for coupling to a standard logicsimulator. Busfunctional modelsThese are hardware models only of the processor interface they cannotrun any software. Instead, they are configured programmed to make theinterface appear as if software were running on the processor. A stochasticmodel of the processor and of the program can be used to determine the mixof bus transactions. Translationbased models29paper hardware software synchronizationsimul. simul. mechanismGupta GJM92 logic custom buscycle custom single simul.Rowson Row94 logic commercial hostcompiled handshakeWilson Wil94 logic commercial hostcompiled handshakeThomas TAS93 logic commercial hostcompiled handshaketen Hagen 1 tHM93 logic commercial hostcompiled handshaketen Hagen 2 tHM93 cyclebased cyclecounting tagged messagesKalavade 1 KL92 logic custom hostcompiled single simul.Kalavade 2 KL92 logic custom ISA single simul.Lee KL92 logic custom hostcompiled single simul.Sutarwala SP94 logic commercial ISA on HW simul. single simul.Table 2 A comparison of cosimulation methods.These convert the code to be executed on a processor into code that can beexecuted natively on the computer doing the simulation. Preserving timinginformation and coupling the translated code to a hardware simulator arethe major challenges.When more accuracy is required, and acceptable simulation performance isnot achievable on standard computers, designers sometimes resort to emulation.In this case, configurable hardware emulates the behavior of the system beingdesigned.Another problem is the accurate modeling of a controlled electromechanicalsystem, which is generally governed by a set of differential equations. This oftenrequires interfacing to an entirely different kind of simulator.3.1.1 Cosimulation MethodsIn this section, we present a survey of some of the representative cosimulationmethods, summarized in Table 2. A unified approach, where the entire system istranslated into a form suitable for a single simulator, is conceptually simple, butcomputationally inefficient. Making better use of computational resources oftenmeans distributing the simulation, but synchronization of the processes becomesa challenge.The method proposed by Gupta et al. GJM92 is typical of the unified approach to cosimulation. It relies on a single custom simulator for hardware and30software that uses a single event queue and a highlevel, buscycle model of thetarget CPU.Rowson Row94 takes a more distributed approach that loosely links a hardware simulator with a software process, synchronizing them with the standardinterprocess communication mechanisms offered by the host operating system.One of the problems with this approach is that the relative clocks of software andhardware simulation are not synchronized. This requires the use of handshakingprotocols, which may impose an undue burden on the implementation. This mayhappen, for example, because hardware and software would not need such handshaking since the hardware part runs in reality much faster than in the simulation.Wilson Wil94 describes the use of a commercial hardware simulator. In thisapproach, the simulator and software compiled on the host processor interact viaa buscycle emulator inside the hardware simulator. The software and hardwaresimulator execute in separate processes and the two communicate via UNIX pipes.Thomas et al. TAS93 take a similar approach.Another approach keeps track of time in software and hardware independently,using various mechanisms to synchronize them periodically. For example, ten Hagen et al. tHM93 describe a twolevel cosimulation environment that combinesa timed and untimed level. The untimed level is used to verify timeindependentproperties of the system, such as functional correctness. At this level, softwareand hardware run independent of each other, passing messages whenever needed.This allows the simulation to run at the maximum speed, while taking full advantage of the native debugging environments both for software and for hardware.The timed level is used to verify timedependent properties, requiring the definition of time in hardware and software. In hardware, time can be measured eitheron the basis of clock cycles cyclebased simulation, assuming synchronous operation for maximum performance, or on the basis of estimated or extracted timinginformation for maximum precision. In software, on the other hand, time can bemeasured either by profiling or clock cycle counting information for maximumperformance, or by executing a model of the CPU for maximum precision. Theauthors propose two basic mechanisms for synchronizing time in hardware andsoftware.1. Software is the master and hardware is the slave. In this case, software decides when to send a message, tagged with the current software clock cycle,to the hardware simulator. Depending on the relation between software andhardware time, the hardware simulator can either continue simulation un31til software time or backup the simulation to software time this requirescheckpointing capabilities, which few hardware simulators currently have.2. Hardware is the master and software is the slave. In this case, the hardwaresimulator directly calls communication procedures which, in turn, call usersoftware code.Kalavade and Lee KL92 and Lee and Rabaey LR93 take a similar approach. The simulation and design environment Ptolemy BHLM94 is used toprovide an interfacing mechanism between different domains. In Ptolemy, objectsdescribed at different levels of abstraction and using different semantic models arecomposed hierarchically. Each abstraction level, with its own semantic model, isa domain e.g., dataflow, discreteevent. Atomic objects called stars are theprimitives of the domain e.g., dataflow operators, logic gates. They can be usedeither in simulation mode reacting to events by producing events or in synthesismode producing software or a hardware description. Galaxies are collections of instances of stars or other galaxies. An instantiated galaxy can belong to adomain different than the instantiating domain. Each domain includes a scheduler, which decides the order in which stars are executed, both in simulation andin synthesis. For synthesis, it must be possible to construct the schedule statically. Whenever a galaxy instantiates a galaxy belonging to another domain typicalin cosimulation, Ptolemy provides a mechanism called a wormhole for the two schedulers to communicate. The simplest form of communication is to passtimestamped events across the interface between domains, with the appropriatedatatype conversion.Kalavade and Lee KL92 perform cosimulation at the specification level byusing a dataflow model and at the implementation level by using an ISA processor model augmented with the interfaces within a hardware simulator, both builtwithin Ptolemy.Lee and Rabaey LR93 simulate the specification by using concurrent processes communicating via queues within a timed model the Ptolemy communicating processes domain. The same message exchanging mechanism is retainedin the implementation using a mix of microprocessorbased boards, DSPs, andASICs, thus performing cosimulation of one part of the implementation with asimulation model of the rest. For example, the software running on the microprocessor can also be run on a host computer, while the DSP software runs on theDSP itself.32Sutarwala and Paulin SP94 describe an environment coupled with a retargetable compiler LMP94b for cyclebased simulation of a userdefinable DSParchitecture. The user only provides a description of the DSP structure and functionality, while the environment generates a behavioral buscycle VHDL modelfor it, which can then be used to run the code on a standard hardware simulator.3.2 Formal VerificationFormal verification is the process of mathematically checking that the behavior ofa system, described using a formal model, satisfies a given property, also describedusing a formal model. The two models may or may not be the same, but must sharea common semantic interpretation. The ability to carry out formal verification isstrongly affected by the model of computation, which determines decidability andcomplexity bounds. Two distinct types of verification arise Specification Verification checking an abstract property of a highlevelmodel. An example checking whether a protocol modeled as a networkof communicating FSMs can ever deadlock. Implementation Verification checking if a relatively lowlevel model correctly implements a higherlevel model or satisfies some implementationdependent property. For example checking whether a piece of hardwarecorrectly implements a given FSM, or whether a given dataflow networkimplementation on a given DSP completely processes an input sample before the next one arrives.Implementation verification for hardware is a relatively welldeveloped area,with the first industrialstrength products beginning to appear. For example,most logic synthesis systems have a mechanism to verify a gatelevel implementation against a set of Boolean equations or an FSM, to detect bugsin the synthesis software.While simulation could fall under these definitions if the property is the behavior under this stimulus is as expected, the term formal verification is usuallyreserved for checking properties of the system that must hold for all or a broadclass of inputs. The properties are traditionally broken into two classes Safety properties, which state that no matter what inputs are given, and nomatter how nondeterministic choices are resolved inside the system mod33el, the system will not get into a specific undesirable configuration e.g.,deadlock, emission of undesired outputs, etc. Liveness properties, which state that some desired configuration will be visited eventually or infinitely often e.g., expected response to an input, etc.More complex checks, such as the correct implementation of a specification,can usually be done in terms of those basic properties. For example, Dill Dil88describes a method to define and check correct implementation for asynchronouslogic circuits in an automatatheoretic framework.In this section we only summarize the major approaches that have been or canbe applied to embedded system verification. These can be roughly divided intothe following classes Theorem proving methods provide an environment that assists the designerin carrying out a formal proof of specification or implementation correctness. The assistance can be either in the form of checking the correctnessof the proof, or in performing some steps of the proof automatically e.g.,Gordon and Melhams HOL GM92, the BoyerMoore system BKM95and PVS ORS92. The main problems with this approach are the undecidability of some higher order logics and the large size of the search spaceeven for decidable logics. Finite automata methods restrict the power of the model in order to automateproofs. A Finite Automaton, in its simplest form, consists of a set of states,connected by a set of edges labeled with symbols from an alphabet. Variouscriteria can be used to define which finite or infinite sequences of symbolsare accepted by the automaton. The set of accepted sequences is generallycalled the language of the automaton. The main verification methods usedin this case are language containment and model checking. In language containment, both the system and the property to be verified are described as a synchronous composition of automata. Theproof is carried out by testing whether the language of one is containedin the language of the other Kurshans approach is typical Kur94.One particularly simple case occurs when comparing a synchronousFSM with its hardware implementation. Then both automata are on finite strings, and the proof of equivalence can be performed by traversing the state space of their product CBM89.34 Simulation relations are an efficient sufficient i.e., conservative criterion to establish language containment properties between automata,originating from the process algebraic community Mil89, Hoa78.Informally, a simulation relation is a relationbetween the statesof the two automata such that for each pair of states    in, foreach symbol labeling an edge from  , the pair of next states under thatsymbol is also in. This relation can be computed much more quickly than the exact language containment test that in the case of nondeterministic automata requires an exponential determinization step,and hence can be used as a fast heuristic check.If two automata simulate each other, then there exists a bisimulationrelation between them, that is also a sufficient criterion for languageequivalence. This criterion can be used, for example, to heuristically minimize nondeterministic automata while the exact procedure isagain exponential. In model checking see, e.g., CES86, QS82, BCMD90, McM93, thesystem is modeled as a synchronous or asynchronous composition ofautomata, and the property is described as a formula in some temporallogic Pnu77, MP92. The proof is again carried out by traversing thestate space of the automaton and marking the states that satisfy theformula. Infinite automata methods can deal with infinite state spaces when someminimization to a finite form is possible. One example of this class are thesocalled timed automata AD90, in which a set of realvalued clocks isused to measure time. Severe restrictions are applied, in order to make thismodel decidable. Clocks can only be tested, started, and reset as part of theedge labels of a finite automaton. Also, clocks can only be compared againstinteger values and initialized to integer values. In this case, it is possible toshow that only a finite set of equivalence class representatives is sufficientto represent exactly the behavior of the timed automaton CC77, AD90.McManis and Varaiya MV94 introduced the notion of suspension, whichextends the class of systems that can be modeled with variations of timedautomata. It is then possible, in principle, to verify timing constraint satisfaction by using preemptive scheduling, which allows a lowpriority processto be stopped in the middle of a computation by a highpriority one.35The main obstacles to the widespread application of finite automatabasedmethods are the inherent complexity of the problem, and the difficulty for designers, generally accustomed to simulationbased models, to formally model thesystem or its properties. The synchronous composition of automata, which is thebasis of all known automatabased methods, is inherently sensitive to the numberof states in the component automata, since the size of the total state space is theproduct of the sizes of the component state spaces.Abstraction is the most promising technique to tackle this problem, generallyknown as statespace explosion. Abstraction replaces generally requiring extensive user intervention some system components with simpler versions, exhibitingnondeterministic behavior. Nondeterminism is used to reduce the size of the statespace without losing the possibility of verifying the desired property. The basicidea is to build provably conservative approximations of the exact behavior of thesystem model, such that the complexity of the verification is lower, but no falsepositive results are possible. I.e., the verification system may say that the approximate model does not satisfy the property, while the original one did, thus requiringa better approximation, but it will never say that the approximate model satisfiesthe property, while the original one did not Bur92, CGL94.In particular, the systematic application of formal verification techniques since the early stages of a design may lead to a new definition of optimal size fora module apart from those currently in use, that are generally related to humanunderstanding, synthesis or compilation. A good leaflevel module must besmall enough to admit verification, and large enough to possess interesting verifiable properties. The possibility of meaningfully applying abstraction would alsodetermine the appropriate size and contents of modules at the upper levels of thehierarchy.Another interesting family of formal verification techniques, useful for heterogeneous systems with multiple concurrent agents, is based on the notion of partial ordering between computations in an execution of a process network. Directuse of available concurrency information can be used during the verification process to reduce the number of explicitly explored states McM93, God90, Val92.Some such methods are based on the socalled Mazurkiewicz traces, in which atrace is an equivalence class of sequences of state transitions where concurrenttransitions are permuted Maz84, dSdS95.Model checking and language containment have been especially useful in verifying the correctness of protocols, which are particularly wellsuited to the finiteautomaton model due to their relative data independence. One may claim that36these two closely related paradigms represent about the only solutions to thespecification verification problem that are currently close to industrial applicability, thanks to The development of extremely efficient implicit representation methods forthe state space, based on Binary Decision Diagrams Bry86, CBM89,that do not require to represent and store every reachable state of the modeled system explicitly. The good degree of automation, at least of the property satisfaction or language containment checks themselves once a suitable abstraction has beenfound by hand. The good match between the underlying semantics statetransition objectsand the finitestate behavior of digital systems.The verification problem becomes much more difficult when one must takeinto account either the actual value of data and the operations performed on them,or the timing properties of the system. The first problem can be tackled by firstassuming equality of arithmetic functions with the same name used at different levels of modeling e.g., specification and implementation, see Burch and Dill BD94 and then separately verifying that a given piece of hardware implementscorrectly a given arithmetic function see Bryant BC95. The timing verificationproblem for sequential systems, on the other hand, still needs to be formulatedin a way that permits the solution of practical problems in a reasonable amountof space and time. One possibility, proposed almost simultaneously by BSV92and AIKY93, is to incrementally add timing constraints to an initially untimedmodel, rather than immediately building the fullblown timed automaton. Thisaddition should be done iteratively, to gradually eliminate all false violationsof the desired properties due to the fact that some timing properties of the model have been ignored. The iteration can be shown to converge, but the speed ofconvergence still depends heavily on the ingenuity of the designer in providinghints to the verification system about the next timing information to consider.As with many young technologies, optimism about verification techniques initially led to excessive claims about their potential, particularly in the area of software verification, where the term proving programs was broadly touted. Formany reasons, including the undecidability of many verification problems and thefact that verification can only be as good as the properties the designer specifies,37this optimism has been misplaced. Berry has suggested using the term automaticbug detection in place of verification to underscore that it is too much to hopefor a conclusive proof of any nontrivial design. Instead, the goal of verificationshould be a technology that will help designers preventing problems in deployedsystems.4 SynthesisBy synthesis, we mean broadly a stage in the design refinement where a moreabstract specification is translated into a less abstract specification, as suggestedin Figure 2. For embedded systems, synthesis is a combination of manual andautomatic processes, and is often divided into three stages mapping to architecture, in which the general structure of an implementation is chosen partitioning,in which the sections of a specification are bound to the architectural units andhardware and software synthesis, in which the details of the units are filled out.We informally distinguish between software synthesis and software compilation, according to the type of input specification. The term software compilationis generally associated with an input specification using C or Pascallike imperative, generally nonconcurrent, languages. These languages have a syntax andsemantics that is very close to that of the implementation assembly or executablecode. In some sense, they already describe, at a fairly detailed level, the desired implementation of the software. We will use the term software synthesis todenote an optimized translation process from a highlevel specification that describes the function that must be performed, rather than the way in which it mustbe implemented. Examples of software synthesis can be, for example, the C orassembly code generation capabilities of Digital Signal Processing graphical programming environments such as Ptolemy BHLM90, of graphical FSM designenvironments such as StateCharts HLN  90a, or of synchronous programmingenvironments such as Esterel, Lustre and Signal Hal93.Recently, higher and higher level synthesis approaches have started to appear.One particularly promising technique for embedded systems is supervisory control, pioneered by Ramadge and Wonham RW89. While most synthesis methods start from an explicit model of how the system that is being designed mustbehave, supervisory control describes what it must achieve. It cleverly combinesa classical control system view of the world with automatatheoretic techniques,to synthesize a control algorithm that is, in some sense, optimum.38Supervisory control distinguishes between the plant an abstraction of thephysical system that must be controlled and the controller the embedded systemthat must be synthesized. Given a finiteautomaton model of the plant possiblyincluding limitations on what a controller can do and of the expected behavior ofthe complete system plant plus controller, it is possible to determine if a finitestate controller satisfying that specification exists, and a best finitestate controller, under some cost function e.g., minimumestimated implementation cost.Recent papers dealing with variations on this problem are, for example, HWT92,BSSV95.4.1 Mapping from Specification to ArchitectureThe problem of architecture selection andor design is one of the key aspects ofthe design of embedded systems. Supporting the designer in choosing the rightmix of components and implementation technologies is essential to the success ofthe final product, and hence of the methodology that was used to design it. Generally speaking, the mapping problem takes as input a functional specification andproduces as output an architecture and an assignment of functions to architecturalunits.An architecture is generally composed of hardware components e.g., microprocessors, microcontrollers, memories,IO devices, ASICs, and FPGAs, software components e.g., an operating system, device drivers, procedures,and concurrent programs, and interconnection media e.g., abstract channels, busses, and shared memories.Partitioning determines which parts of the specification will be implemented onthese components, while their actual implementation will be created by softwareand hardware synthesis.The cost function optimized by the mapping process includes a mixture oftime, area, component cost, and power consumption, where the relative importance depends heavily on the type of application. Time cost may be measured39either as execution time for an algorithm, or as missed deadlines for a soft realtime system  . Area cost may be measured as chip, board, or memory size. Thecomponents of the cost function may take the form of a hard constraint or a quantity to be minimized.Current synthesisbased methods almost invariably impose some restrictions on the target architecture in order to make the mapping problem manageable.For example, the architecture may be limited to a library of predefined components due to vendor restrictions or interfacing constraints. Few papers havebeen published on automating the design of, say, a memory hierarchy or an IOsubsystem based on standard components. Notable exceptions to this rule arepapers dealing with retargetable compilation e.g., Theissinger et al. TSV94,or with a very abstract formulation of partitioning for codesign e.g., Kumaret al. KAJW92, KAJW93, Prakash and Parker PP91, and Vahid and Gajski VG92. The structure of the applicationspecific hardware components, onthe other hand, is generally much less constrained.Often, the communication mechanisms are also standardized for a given methodology. Few choices, often closely tied to the communication mechanism used atthe specification level, are offered to the designer. Nonetheless, some work hasbeen done on the design of interfaces e.g., Chou et al. CWB94.4.2 PartitioningPartitioning is a problem with any design using more than one component. Itis a particularly interesting problem in embedded systems because of the heterogeneous hardwaresoftware mixture. Partitioning methods can be classified, asshown in Table 3, according to four main characteristics the specification models supported, the granularity, the cost function, and the algorithm.Realtime systems, and individual timing constraints within such systems, are classified assoft or hard according to whether missing a deadline just degrades the system performance orcauses a catastrophic failure.40Explored algorithm classes include greedy heuristics, clustering methods, iterative improvement, and mathematical programming.So far, there seems to be no clear winner among partitioning methods, partly due to the early stage of research in this area, and partly due to the intrinsiccomplexity of the problem, which seems to preclude an exact formulation with arealistic cost function in the general case.Ernst et al. EH92, HBE93, HEHB94 use a graphbased model, with nodescorresponding to elementary operations statements in C, a Clike language extended with concurrency. The cost is derived by profiling, aimed at discovering the bottlenecks that can be eliminatedfrom the initial, allsoftware partition by moving some operations to hardware by estimating the closeness between operations, including control localitythe distance in number of control nodes between activations of the sameoperation in the control flow graph, data locality the number of commonvariables among operations, and operator closeness the similarities, e.g.,an add and a subtract are close and by estimating the communication overhead incurred when blocks are movedacross partitions. This is approximated by the static number of data itemsexchanged among partitions, assuming a simple memorymapped communication mechanism between hardware and software.Partitioning is done in two loops. The inner loop uses simulated annealing,with a quick estimation of the gain derived by moving an operation between hardware and software, to improve an initial partition. The outer loop uses synthesisto refine the estimates used in the inner loop.Olokutun et al. OHLR94 perform performancedriven partitioning workingon a blockbyblock basis. The specification model is a hardware descriptionlanguage. This allows them to use synthesis for hardware cost estimation, andprofiling of a compiledcode simulator for software cost estimation. Partitioningis done together with scheduling, since the overall goal is to minimize responsetime in the context of using emulation to speed up simulation. An initial partition is obtained by classifying blocks according to whether or not they are synthesizable, and whether or not the communication overhead justifies a hardwareimplementation. This determines some blocks which must either go into software41paper model granularity cost function algorithmHenkel HEHB94 CDFG C operation profiling SW hand outersynthesis and sim. anneal.similarity HW innercommunication costOlokutun OHLR94 HDL task profiling SW Kernighansynthesis HW and LinKumar KAJW93 setbased task profiling math. prog.Hu HDMT94 task list task profiling branchsched. analysis and boundVahid VG92 acyclic DFG operation profiling SW Mixedprocessor cost HW IntegerLinearcommunication cost Prog.Barros 1 BRX93 Unity HDL operation similarity clusteringconcurr.sequenc.Barros 2 BS94 Occam operation similarity clusteringhierarchy concurr.sequenc.hierarchyKalavade KL94 acyclic DFG operation schedulability heuristic withlookaheadAdams AST93 HDL  task profiling SW handsynthesis HWEles EPD94 VHDL task profiling sim. anneal.Luk LW94 Ruby HDL operation rate matching handhierarchySteinhausen SCG93 CDFG HDL, C operation profiling handBen Ismail IAJ94 communicating task  handprocessesAntoniazzi ABFS94 FSMs task  handChou CWB94 timing diagram operation time SW area HW mincutGupta GJM94 CDFG HDL operation time heuristicTable 3 A comparison of partitioning methods.42or hardware. Uncommitted blocks are assigned to hardware or software startingfrom the block which has most to gain from a specific choice. The initial partitionis then improved by a Kernighan and Linlike iterative swapping procedure.Kumar et al. KAJW92, KAJW93, on the other hand, consider partitioningin a very general and abstract form. They use a complex, setbased representationof the system, its various implementation choices and the various costs associated with them. Cost attributes are determined mainly by profiling. The systembeing designed is represented by four sets available software functions hardware resources communications between the software andor hardware unitsand functions to be implemented, each of which can be assigned a set of softwarefunctions, hardware resources and communications. This means that the givensoftware runs on the given hardware and uses the given communications to implement the function. The partitioning process is followed by a decomposition ofeach function into virtual instruction sets, followed by design of an implementation for the set using the available resources, and followed again by an evaluationphase.DAmbrosio et al. DH94a, HDMT94 tackle the problem of choosing a setof processors on which a set of cooperating tasks can be executed while meeting realtime constraints. They also use a mathematical formulation, but provide an optimal solution procedure by using branchandbound. The cost of asoftware partition is estimated as a lower and an upper bound on processor utilization. The upper bound is obtained by ratemonotonic analysis see Liu andLayland LL73, while the lower bound is obtained by various refinements of thesum of task computation times divided by task periods. The branchandboundprocedure uses the bounds to prune the search space, while looking for optimalassignments of functions to components, and satisfying the timing constraints.Other optimization criteria can be included beside schedulability, such as responsetimes to tasks with soft deadlines, hardware costs, and expandability, which favorssoftware solutions.Barros et al. BRX93 use a graphbased finegrained representation, witheach unit corresponding to a simple statement in the Unity specification language.They cluster units according to a variety of sometimes vague criteria similaritybetween units, based on concurrency control and data independence, sequencingcontrol or data dependence, mutual exclusion, and vectorization of a sequenceof related assignments. They cluster the units to minimize the cost of cuts in theclustering tree, and then improve the clustering by considering pipelining opportunities, allocations done at the previous stage, and cost savings due to resource43sharing.Kalavade and Lee KL94 use an acyclic dependency graph derived from adataflow graph to simultaneously map each node task to software or hardwareand schedule the execution of the tasks. The approach is heuristic, and can give anapproximate solution to very large problem instances. To guide the search process,it uses both critical path information and the suitability of a node to hardwareor software. For example, bit manipulations are better suited to hardware whilerandom accesses to a data structure are better suited to software.Vahid, Gajski et al. VG92, GNRV96 perform graphbased partitioning of avariablegrained specification. The specification language is SpecCharts, a hierarchical model in which the leaves are states of a hierarchical Statechartslikefinite state machine. These states can contain arbitrarily complex behavioralVHDL processes, written in a highlevel specification style. Cost function estimation is done at the leaf level. Each level is assigned an estimated numberof IO pins, an estimated area based on performing behavioral, RTL and logicsynthesis in isolation, and an estimated execution time obtained by simulatingthat initial implementation, and considering communication delay as well. Thearea estimate can be changed if more leaves are mapped onto the same physicalentity, due to potential sharing. The cost model is attached to a graph, in whichnodes represent leaves and edges represent control activationdeactivation anddata communication dependencies. Classical clustering and partitioning algorithms are then applied, followed by a refinement phase. During refinement, eachpartition is synthesized, to get better area and timing estimates, and peripheralgraph nodes are moved among partitions greedily to reduce the overall cost. Thecost of a given partition is a simple weighted sum of area, pin, chip count, andperformance constraint satisfaction measures.Steinhausen et al. SCG  93, TSV94, WCR94 describe a complete cosynthesisenvironment in which a CDFG representation is derived from an array of specification formats, such as Verilog, VHDL and C. The CDFG is partitioned by hand,based on the results of profiling, and then mapped onto an architecture that can include generalpurpose microprocessors, ASIPs applicationspecific instructionprocessor, softwareprogrammable components designed ad hoc for an application, and ASICs applicationspecific integrated circuits. An interesting aspectof this approach is that the architecture itself is not fixed, but synthesis is driven bya userdefined structural description. ASIC synthesis is done with a commercialtool, while software synthesis, both for generalpurpose and specialized processors, is done with an existing retargetable compiler developed by Hoogerbrugge44et al. HC94.Ben Ismail et al. IAJ94 and Voss et al. VIJK94 start from a system specification described in SDL SSR89. The specification is then translated into theSolar internal representation, based on a hierarchical interconnection of communicating processes. Processes can be merged and split, and the hierarchy can bechanged by splitting, moving and clustering of subunits. The sequencing of theseoperations is currently done by the user.Finally, Chou et al. CWB94 and Walkup and Borriello WB93 describe aspecialized, schedulingbased algorithm for interface partitioning. The algorithmis based on a graph model derived from a formalized timing diagram. Nodes represent lowlevel events in the interface specification. Edges represent constraints,and can either be derived from causality links in the specification, or be addedduring the partitioning process for example to represent events that occur on thesame wire, and hence should be moved together. The cost function is time forsoftware and area for hardware. The algorithm is based on a mincut procedureapplied to the graph, in order to reduce congestion. Congestion in this case isdefined as software being required to produce events more rapidly than the targetprocessor can do, which implies the need for some hardware assistance.4.3 Hardware and Software SynthesisAfter partitioning and sometimes before partitioning, in order to provide cost estimates the hardware and software components of the embedded system must beimplemented. The inputs to the problem are a specification, a set of resources andpossibly a mapping onto an architecture. The objective is to realize the specification with the minimum cost.Generally speaking, the constraints and optimization criteria for this step arethe same as those used during partitioning. Area and code size must be tradedoff against performance, which often dominates due to the realtime characteristics of many embedded systems. Cost considerations generally suggest the useof software running on offtheshelf processors, whenever possible. This choice,among other things, allows one to separate the software from the hardware synthesis process, relying on some form of predesigned or customized interfacingmechanism.One exception to this rule are authors who propose the simultaneous designof a computer architecture and of the program that must run on it e.g., Menezet al. MABC92, Marwedel Mar93, and Wilberg et al. WCR94. Since the45designers of generalpurpose CPUs face different problems than the designers of embedded systems, we will only consider those authors who synthesize anApplicationSpecific Instruction Processor ASIP, Pau95 and the microcodethat runs on it. The designer of a generalpurpose CPU must worry about backward compatibility, compiler support, and optimal performance for a wide varietyof applications, whereas the embedded system designer must worry about additionof new functionality in the future, user interaction, and satisfaction of a specificset of timing constraints.Note that by using an ASIP rather than a standard Application Specific Integrated Circuit ASIC, which generally has very limited programming capabilities, the embedded system designer can couple some of the advantages ofhardware and software. For example, performance and power consumption canbe improved with respect to a software implementation on a generalpurposemicrocontroller or DSP, while flexibility can be improved with respect to a hardware implementation. Another method to achieve the same goal is to use reprogrammable hardware, such as Field Programmable Gate Arrays. FPGAs can bereprogrammed either offline just like embedded software is upgraded by changing a ROM, or even online to speed up the algorithm that is currently beingexecuted.The hardware synthesis task for ASICs used in embedded systems whetherthey are implemented on FPGAs or not is generally performed according to theclassical highlevel and logic synthesis methods. These techniques have beenworked on extensively for example, recent books by De Micheli Mic94, Devadas, Gosh and Keutzer DGK94, and Camposano and Wolf CW91 describethem in detail. Marwedel and Goossens MG95 present a good overview of codegeneration strategies for DSPs and ASIPs.The software synthesis task for embedded systems, on the other hand, is arelatively new problem. Traditionally, software synthesis has been regarded withsuspicion, mainly due to excessive claims made during its infancy. In fact, theproblem is much more constrained for embedded systems compared to generalpurpose computing. For example, embedded software often cannot use virtualmemory, due to physical constraints e.g., the absence of a swapping device,to realtime constraints, and to the need to partition the specification betweensoftware and hardware. This severely limits the applicability of dynamic taskcreation and memory allocation. For some highly critical applications even theuse of a stack may be forbidden, and everything must be dealt with by pollingand static variables. Algorithms also tend to be simpler, with a clear division into46cooperating tasks, each solving one specific problem e.g., digital filtering of agiven input source, protocol handling over a channel, and so on. In particular, theproblem of translating cooperating finitestate machines into software has beensolved in a number of ways.Software synthesis methods proposed in the literature can be classified, asshown in Table 4, according to the following general lines the specification formalism, interfacing mechanisms at the specification and the implementation levels, when the scheduling is done, and the scheduling method.Almost all software synthesis methods perform some sort of schedulingsequencing the execution of a set of originally concurrent tasks. Concurrent tasksare an excellent specification mechanism, but cannot be implemented as suchon a standard CPU. The scheduling problem reviewed e.g. by Halang and Stoyenko HS91 amounts to finding a linear execution order for the elementaryoperations composing the tasks, so that all the timing constraints are satisfied. Depending on how and when this linearization is performed, scheduling algorithmscan be classified as Static, where all scheduling decisions are made at design or compiletime. Quasistatic, where some scheduling decisions are made at runtime, someat compiletime. Dynamic, where all decision are made at runtime.Dynamic schedulers take many forms, but in particular they are distinguishedas preemptive or nonpreemptive, depending on whether a task can be interrupted at arbitrary points. For embedded systems, there are compelling motivationsfor using static or quasistatic scheduling, or at least for minimizing preemptivescheduling in order to minimize scheduling overhead and to improve reliabilityand predictability. There are, of course, cases in which preemption cannot beavoided, because it is the only feasible solution to the problem instance HS91,but such cases should be carefully analyzed to limit preemption to a minimum.47paper model interface constraint schedulinggranularity algorithmCochran Coc92 task list none task RMA runtimeChou CWB94 task list synthesized task heuristic staticoperationGupta GJM94 CDFG  operation heuristicwith lookaheadstatic  runtimeChiodo CGH  94b task list synthesized task RMA runtimeMenez MABC92 CDFG  operation exhaustiveTable 4 A comparison of software scheduling methods.Many static scheduling methods have been developed. Most somehow construct a precedence graph and then apply or adapt classical methods. We referthe reader to Bhattacharyya et al. BML96 and Sih and Lee SL93b, SL93a as astarting point for scheduling of dataflow graphs.Many approaches to software synthesis for embedded systems divide the computation into cooperating tasks that are scheduled at run time. This scheduling canbe done1. either by using classical scheduling algorithms,2. or by developing new techniques based on a better knowledge of the domain. Embedded systems with fairly restricted specification paradigms arean easier target for specialized scheduling techniques than fully general algorithms written in an arbitrary highlevel language.The former approach uses, for example, Rate Monotonic Analysis RMA LL73to perform schedulability analysis. In the pure RMA model, tasks are invoked periodically, can be preempted, have deadlines equal to their invocation period, andsystem overhead context switching, interrupt response time, and so on is negligible. The basic result by Liu and Layland states that under these hypotheses, ifa given set of tasks can be successfully scheduled by a static priority algorithm,then it can be successfully scheduled by sorting tasks by invocation period, withthe highest priority given to the task with the shortest period.The basic RMA model must be augmented to be practical. Several resultsfrom the realtime scheduling literature can be used to develop a scheduling en48vironment supporting process synchronization, interrupt service routines, contextswitching time, deadlines different from the task invocation period, mode changeswhich may cause a change in the number andor deadlines of tasks, and parallelprocessors. Parallel processor support generally consists of analyzing the schedulability of a given assignment of tasks to processors, providing the designer withfeedback about potential bottlenecks and sources of deadlocks.Chou et al. CWB94 advocate developing new techniques based on a betterknowledge of the domain. The problem they consider is to find a valid scheduleof processes specified in Verilog under given timing constraints. This approach,like that of Gupta et al. described below, and unlike classical taskbased scheduling methods, can take into account both finegrained and coarsegrained timingconstraints. The specification style chosen by the authors uses Verilog constructs that provide structured concurrency with watchdogstyle preemption. In thisstyle, multiple computation branches are started in parallel, and some of themthe watchdogs can kill others upon occurrence of a given condition. A setof safe recovery points is defined for each branch, and preemption is allowedonly at those points. Timing constraints are specified by using modes, which represent different states for the computation as in SpecCharts, e.g., initialization,normal operation and error recovery. Constraints on the minimum and maximumtime separation between events even of the same type, to describe occurrencerates can be defined either within a mode or among events in different modes.Scheduling is performed within each mode by finding a cyclic order of operationswhich preserves IO rates and timing constraints. Each mode is transformed intoan acyclic partial order by unrolling, and possibly splitting if it contains parallelloops with harmonically unrelated repetition counts. Then the partial order islinearized by using a longestpath algorithm to check feasibility and assign starttimes to the operations.The same group describes in CB94 a technique for device driver synthesis,targeted towards microcontrollers with specialized IO ports. It takes as input aspecification of the system to be implemented, a description of the function andstructure of each IO port a list of bits and directions, and a list of communicationinstructions. It can also exploit specialized functions such as parallelserial andserial parallel conversion capabilities. The algorithm assigns communications inthe specification to physical entities in the microcontroller. It first tries to usespecial functions, then assigns IO ports, and finally resorts to the more expensivememorymapped IO for overflow communications. It takes into account resourceconflicts e.g. among different bits of the same port, and allocates hardware49components to support memorymapped IO. The output of the algorithm is anetlist of hardware components, initialization routines and IO driver routines thatcan be called by the software generation procedure whenever a communicationbetween software and hardware must take place.Gupta et al. GJM92, GJM94, GM94 develop a scheduling method for reactive realtime systems. The input specification is a Control and DataFlow GraphCDFG derived from a Clike HDL called HardwareC. The cost model takes into account the processor type, the memory model, and the instruction executiontime. The latter is derived bottomup from the CDFG by assigning a processorand memorydependent cost to each leaf operation in the CDFG. Some operationshave an unbounded execution time, because they are either datadependent loops or synchronization IO operations. Timing constraints are basically data rateconstraints on externally visible Input Output operations. Boundedtime operations within a process are linearized by a heuristic method the problem is knownto be NPcomplete. The linearization procedure selects the next operation to beexecuted among those whose predecessors have all been scheduled, according towhether or not their immediate selection for scheduling can cause some timingconstraint to be missed, and a measure of urgency that performs some limitedtiming constraint lookahead. Unboundedtime operations, on the other hand, areimplemented by a call to the runtime scheduler, which may cause a context switchin favor of another more urgent thread.Chiodo et al. CGH95 also propose a software synthesis method from extended asynchronous Finite State Machines called Codesign Finite State Machines, CFSMs. The method takes advantage of optimization techniques fromthe hardware synthesis domain. It uses a model based on multiple asynchronously communicating CFSMs, rather than a single FSM, enabling it to handle systemswith widely varying data rates and response time requirements. Tasks are organized with different priority levels, and scheduled according to classical runtimealgorithms like RMA. The software synthesis technique is based on a very simple CDFG, representing the state transition and output functions of the CFSM.The nodes of the CDFG can only be of two types TEST nodes, which evaluatean expression and branch according to its result, and ASSIGN nodes, which evaluate an expression and assign its result to a variable. The authors develop amapping from a representations of the state transition and output functions using Binary Decision Diagrams Bry86 to the CDFG form, and can thus use abody of welldeveloped optimization techniques to minimize memory occupationandor execution time. The simple CDFG form permits also an easy and relatively50accurate prediction of software cost and performance, based on cost assignment to each CDFG node SSV96. The cost code and data memory occupationand performance clock cycles of each node type can be evaluated with a gooddegree of accuracy, based on a handful of systemspecific parameters e.g., thecost of a variable assignment, of an addition, of a branch. These parameters canbe derived by compiling and running a few carefully designed benchmarks on thetarget processor, or on a cycleaccurate emulator or simulator.Liem et al. LMP94b tackle a very different problem, that of retargetablecompilation for a generic processor architecture. They focus their optimizationtechniques towards highly asymmetric processors, such as commercial DigitalSignal Processors in which, for example, one register may only be used for multiplication, another one only for memory addressing, and so on. Their registerassignment scheme is based on the notion of classes of registers, describing whichtype of operation can use which register. This information is used during CDFGcovering with processor instructions LMP94a to minimize the number of movesrequired to save registers into temporary locations.Marwedel Mar93 also uses a similar CDFG covering approach. The sourcespecification can be written in VHDL or in the Pascallike language Mimola. Thepurpose is microcode generation for Very Long Instruction Word VLIW processors, and in this case the instruction set has not been defined yet. Rather, aminimum encoding of the control word is generated for each control step. Control steps are allocated using an As Soon As Possible policy ASAP, meaning thateach microoperation is scheduled to occur as soon as its operands have been computed, compatibly with resource utilization conflicts. The control word containsall the bits necessary to steer the execution units in the specified architecture toperform all the microoperations in each step. Register allocation is done in orderto minimize the number of temporary locations in memory due to register spills.Tiwari et al. TMW94 describe a software analysis rather than synthesismethod aimed at estimating the power consumption of a program on a given processor. Their power consumption model is based on the analysis of single instructions, addressing modes, and instruction pairs a simple way of modelingthe effect of the processor state. The model is evaluated by running benchmarkprograms for each of these characteristics, and measuring the current flow to andfrom the power and ground pins.515 ConclusionsIn this paper we outlined some important aspects of the design process for embedded systems, including specification models and languages, simulation, formalverification, partitioning and hardware and software synthesis.The design process is iterativea design is transformed from an informal description into a detailed specification usable for manufacturing. The specificationproblem is concerned with the representation of the design at each of these stepsthe validation problem is to check that the representation is consistent both withina step and between steps and the synthesis problem is to transform the designbetween steps.We argued that formal models are necessary at each step of a design, and thatthere is a distinction between the language in which the design is specified andits underlying model of computation. Many models of computation have beendefined, due not just to the immaturity of the field but also to fundamental differences the best model is a function of the design. The heterogeneous nature ofmost embedded systems makes multiple models of computation a necessity. Manymodels of computation are built by combining three largely orthogonal aspectssequential behavior, concurrency, and communication.We presented an outline of the taggedsignal model LSV96, a frameworkdeveloped by two of the authors to contrast different models of computation. Thefundamental entity in the model is an event a valuetag pair. Tags usually denotetemporal behavior, and different models of time appear as structure imposed onthe set of all possible tags. Processes appear as relations between signals setsof events. The character of such a relation follows from the type of process itdescribes.Simulation and formal verification are two key validation techniques. Mostembedded systems contain both hardware and software components, and it is achallenge to efficiently simulate both components simultaneously. Using separate simulators for each is often more efficient, but synchronization becomes achallenge.Formal verification can be roughly divided into theorem proving methods, finite automata methods, and infinite automata methods. Theorem provers generally assist designers in constructing a proof, rather than being fully automatic, butare able to deal with very powerful languages. Finiteautomata schemes represent either explicitly or implicitly all states of the system and check properties onthis representation. Infiniteautomata schemes usually build finite partitions of the52state space, often by severely restricting the input language.In this paper, synthesis refers to a step in the design refinement process wherethe design representation is made more detailed. This can be manual andorautomated, and is often divided into mapping to architecture, partitioning, andcomponent synthesis. Automated architecture mapping, where the overall systemstructure is defined, often restricts the result to make the problem manageable.Partitioning, where sections of the design are bound to different parts of the system architecture, is particularly challenging for embedded systems because of theelaborate cost functions due to their heterogeneity. Assigning an execution order to concurrent modules, and finding a sequence of instructions implementing afunctional module are the primary challenges in software synthesis for embeddedsystems.6 AcknowledgementsEdwards and Lee participated in this study as part of the Ptolemy project, whichis supported by the Advanced Research Projects Agency and the U.S. Air Forceunder the RASSP program, contract F3361593C1317, the State of California MICRO program, and the following companies Cadence, Dolby, Hitachi,LG Electronics, Mitsubishi, Motorola, NEC, Philips, and Rockwell. Lavagnoand SangiovanniVincentelli were partially supported by grants from Cadence,Magneti Marelli, DaimlerBenz, Hitachi, Consiglio Nazionale delle Ricerche, theMICRO program, and SRC.ReferencesABFS94 S. Antoniazzi, A. Balboni, W. Fornaciari, and D. Sciuto. A methodology for controldominated systems codesign. In Proceedings ofthe International Workshop on HardwareSoftware Codesign, 1994.Ack82 W. B. Ackerman. Data flow languages. Computer, 152, 1982.AD90 R. Alur and D. Dill. Automata for Modeling RealTime Systems. InAutomata, Languages and Programming 17th Annual Colloquium,volume 443 of Lecture Notes in Computer Science, pages 322335,1990. Warwick University, July 1620.53AG82 Arvind and K. P. Gostelow. The UInterpreter. Computer, 152,1982.AH92 R. Alur and T.A. Henzinger. Logics and models of real time A survey. In J.W. de Bakker, C. Huizing, W.P. de Roever, and G. Rozenberg, editors, RealTime Theory in Practice. REX Workshop Proceedings, 1992.AIKY93 R. Alur, A. Itai, R. Kurshan, and M. Yannakakis. Timing verificationby successive approximation. In Proceedings of the Computer AidedVerification Workshop, pages 137150, 1993.AST93 J.K. Adams, H. Schmitt, and D.E. Thomas. A model and methodology for hardwaresoftware codesign. In Proceedings of the International Workshop on HardwareSoftware Codesign, October 1993.BB91 A. Benveniste and G. Berry. The synchronous approach to reactiveand realtime systems. Proceedings of the IEEE, 79912701282,1991.BC95 R.E. Bryant and YA Chen. Verification of arithmetic circuits withBinary Moment Diagrams. In Proceedings of the Design AutomationConference, pages 535541, 1995.BCMD90 J. Burch, E. Clarke, K. McMillan, and D. Dill. Sequential circuitverification using symbolic model checking. In Proceedings of theDesign Automation Conference, pages 4651, 1990.BD94 J.R. Burch and D.L. Dill. Automatic verification of pipelined microprocessor control. In Proceedings of the Sixth Workshop onComputerAided Verification, pages 6880, 1994.BELP94 G. Bilsen, M. Engels, R. Lauwereins, and J. A. Peperstraete. Staticscheduling of multirate and cyclostatic DSP applications. In Proc.1994 Workshop on VLSI Signal Processing. IEEE Press, 1994.Ber89 G. Berry. Information Processing, volume 89, chapter Real Timeprogramming Special purpose or general purpose languages, pages1117. North HollandElsevier Science Publishers, 1989.54Ber91 G. Berry. A hardware implementation of pure Esterel. In Proceedings of the International Workshop on Formal Methods in VLSI Design, January 1991.BG90 A. Benveniste and P. Le Guernic. Hybrid dynamical systems theory and the SIGNAL language. IEEE Transactions on AutomaticControl, 355525546, May 1990.BHLM90 J. Buck, S. Ha, E.A. Lee, and D.G. Masserschmitt. Ptolemy aframework for simulating and prototyping heterogeneous systems.Interntional Journal of Computer Simulation, special issue on Simulation Software Development, January 1990.BHLM94 J. T. Buck, S. Ha, E. A. Lee, and D. G. Messerschmitt. Ptolemy A framework for simulating and prototyping heterogeneoussystems. Int. Journal of Computer Simulation, 4155155182,April 1994. Special issue on simulation software development.httpptolemy.eecs.berkeley.edupapersJEurSim.ps.Z.BKM95 R.S. Boyer, M. Kaufmann, and J.S. Moore. The BoyerMoore theorem prover and its interactive enhancement. Computers  Mathematics with Applications, pages 2762, January 1995.BML96 S. S. Bhattacharyya, P. K. Murthy, and E. A. Lee. Software Synthesisfrom Dataflow Graphs. Kluwer Academic Press, Norwood, Mass,1996.BRX93 E. Barros, W. Rosenstiel, and X. Xiong. Hardwaresoftware partitioning with UNITY. In Proceedings of the International Workshopon HardwareSoftware Codesign, October 1993.Bry86 R. Bryant. Graphbased algorithms for boolean function manipulation. IEEE Transactions on Computers, C358677691, August1986.BS91 F. Boussinot and R. De Simone. The ESTEREL language. Proceedings of the IEEE, 799, 1991.BS94 E. Barros and A. Sampaio. Towards provably correct hardwaresoftware partitioning using OCCAM. In Proceedings of the55International Workshop on HardwareSoftware Codesign, October1994.BSSV95 M. Di Benedetto, A. Saldanha, and A. SangiovanniVincentelli.Strong model matching for finite state machines. In Proceedingsof the Third European Control Conference, September 1995.BSV92 F. Balarin and A. SangiovanniVincentelli. A verification strategyfor timingconstrained systems. In Proceedings of the Fourth Workshop on ComputerAided Verification, pages 148163, 1992.Buc93 J. T. Buck. Scheduling Dynamic Dataflow Graphs with BoundedMemory Using the Token Flow Model. PhD thesis, University ofCalifornia, Berkeley, 1993. Dept. of EECS, Tech. Report UCBERL9369.Bur92 J. R. Burch. Automatic Symbolic Verification of RealTime Concurrent Systems. PhD thesis, Carnegie Mellon University, August 1992.CB94 P. Chou and G. Borriello. Software scheduling in the cosynthesis ofreactive realtime systems. In Proceedings of the Design AutomationConference, June 1994.CBM89 O. Coudert, C. Berthet, and J. C. Madre. Verification of Sequential Machines Using Boolean Functional Vectors. In IMECIFIP Intl Workshop on Applied Formal Methods for Correct VLSI Design,pages 111128, November 1989.CC77 P. Cousot and R. Cousot. Abstract interpretation a unified latticemodel for static analysis of programs by construction or approximation of fixpoints. In 4th ACM Symp. on Principles of ProgrammingLanguages, Los Angeles, January 1977.CES86 E. M. Clarke, E. A. Emerson, and A. P. Sistla. Automatic verificationof finitestate concurrent systems using temporal logic specifications. ACM TOPLAS, 82, 1986.CG89 N. Carriero and D. Gelernter. Linda in context. Comm. of the ACM,324444458, April 1989.56CGH  94a M. Chiodo, P. Giusto, H. Hsieh, A. Jurecska, L. Lavagno, andA. SangiovanniVincentelli. A formal methodology for hardwaresoftware codesign of embedded systems. IEEE Micro, August1994.CGH  94b M. Chiodo, P. Giusto, H. Hsieh, A. Jurecska, L. Lavagno, andA. SangiovanniVincentelli. Hardwaresoftware codesign of embedded systems. IEEE Micro, 1442636, August 1994.CGH95 M. Chiodo, P. Giusto, H. Hsieh, A. Jurecska, L. Lavagno, andA. SangiovanniVincentelli. Synthesis of software programs fromCFSM specifications. In Proceedings of the Design AutomationConference, June 1995.CGL94 E. Clarke, O. Grumberg, and D. Long. Model checking and abstraction. ACM Transactions on Programming Languages and Systems,16515121542, September 1994.CH71 F. Commoner and A. W. Holt. Marked directed graphs. Journal ofComputer and System Sciences, 5511523, 1971.CHL96 W.T. Chang, S.H. Ha, and E. A. Lee. Heterogeneous simulation mixing discreteevent models with dataflow. J. on VLSI SignalProcessing, 1996. to appear.CKL95 W.T. Chang, A. Kalavade, and E. A. Lee. Effective heterogeneousdesign and cosimulation. In NATO Advanced Study Institute Workshop on HardwareSoftware Codesign, Lake Como, Italy, June 1995.httpptolemy.eecs.berkeley.edupaperseffective.Coc92 M. Cochran. Using the rate monotonic analysis to analyze theschedulability of ADARTS realtime software designs. In Proceedings of the International Workshop on HardwareSoftware Codesign, September 1992.CW91 R. Camposano and W. Wolf, editors. Highlevel VLSI synthesis. Kluwer Academic Publishers, 1991.CWB94 P. Chou, E.A. Walkup, and G. Borriello. Scheduling for reactiverealtime systems. IEEE Micro, 1443747, August 1994.57Dav92 A. Davie. An introduction to functional programming systems usingHaskell. Cambridge University Press, 1992.DGK94 S. Devadas, A. Ghosh, and K. Keutzer. Logic synthesis. McGrawHill, 1994.DH94a J.G. DAmbrosio and X.B. Hu. Configurationlevel hardwaresoftware partitioning for realtime embedded systems. InProceedings of the International Workshop on HardwareSoftwareCodesign, 1994.DH94b D. Drusinski and D. Harel. On the power of bounded concurrency. I.Finite automata. Journal of the Association for Computing Machinery, 413517539, May 1994.Dij68 E. Dijkstra. Cooperating sequential processes. In E. F. Genuys,editor, Programming Languages. Academic Press, New York, 1968.Dil88 D.L. Dill. Trace Theory for Automatic Hierarchical Verification ofSpeedIndependent Circuits. The MIT Press, Cambridge, Mass.,1988. An ACM Distinguished Dissertation 1988.dSdS95 M. L. de Souza and R. de Simone. Using partial orders for verifyingbehavioral equivalences. In Proceedings of CONCUR 95, 1995.EH92 R. Ernst and J. Henkel. Hardwaresoftware codesign of embeddedcontrollers based on hardware extraction. In Proceedings of the International Workshop on HardwareSoftware Codesign, September1992.EPD94 P. Eles, Z. Peng, and A. Doboli. VHDL systemlevel specificationand partitioning in a hardwaresoftware cosynthesis environment. InProceedings of the International Workshop on HardwareSoftwareCodesign, September 1994.GJM92 R. K. Gupta, C. N. Coelho Jr., and G. De Micheli. Synthesis andsimulation of digital systems containing interacting hardware andsoftware components. In Proceedings of the Design AutomationConference, June 1992.58GJM94 R. K. Gupta, C. N. Coelho Jr., and G. De Micheli. Program implementation schemes for hardwaresoftware systems. IEEE Computer,pages 4855, January 1994.GM92 M.J.C. Gordon and T.F. Melham, editors. Introduction to HOL atheorem proving environment for higher order logic. Cambridge University Press, 1992.GM94 R.K. Gupta and G. De Micheli. Constrained software generationfor hardwaresoftware systems. In Proceedings of the InternationalWorkshop on HardwareSoftware Codesign, 1994.GNRV96 D. D. Gajski, S. Narayan, L. Ramachandran, and F. Vahid. System design methodologies aiming at the 100 h design cycle. IEEETransactions on VLSI, 41, March 1996.God90 P. Godefroid. Using partial orders to improve automatic verificationmethods. In E.M Clarke and R.P. Kurshan, editors, Proceedings ofthe Computer Aided Verification Workshop, 1990. DIMACS Seriesin Discrete Mathematica and Theoretical Computer Science, 1991,pages 321340.Hal93 N. Halbwachs. Synchronous Programming of Reactive Systems. Kluwer Academic Publishers, 1993.Har87 D. Harel. Statecharts A visual formalism for complex systems. Sci.Comput. Program., 8231274, 1987.HBE93 J. Henkel, T. Benner, and R. Ernst. Hardware generation and partitioning effects in the COSYMA system. In Proceedings of theInternational Workshop on HardwareSoftware Codesign, October1993.HC94 J. Hoogerbrugge and H. Corporaal. Transporttriggering vs.operationtriggering. In 5th International Conference on CompilerConstruction, April 1994.HCRP91 N. Halbwachs, P. Caspi, P. Raymond, and D. Pilaud. The synchronous data flow programming language LUSTRE. Proceedingsof the IEEE, 79913051319, 1991.59HDMT94 X. Hu, J.G. DAmbrosio, B. T. Murray, and DL Tang. Codesignof architectures for powertrain modules. IEEE Micro, 1444858,August 1994.HEHB94 J. Henkel, R. Ernst, U. Holtmann, and T. Benner. Adaptation of partitioning and highlevel synthesis in hardwaresoftware cosynthesis.In Proceedings of the International Conference on ComputerAidedDesign, November 1994.HLN  90a D. Harel, H. Lachover, A. Naamad, A. Pnueli, et al. STATEMATE a working environment for the development of complex reactive systems. IEEE Transactions on Software Engineering, 164,April 1990.HLN90b D. Harel, H. Lachover, A. Naamad, A. Pnueli, M. Politi, R. Sherman,A. ShtullTrauring, and M. Trakhtenbrot. Statemate A workingenvironment for the development of complex reactive systems. IEEETrans. on Software Engineering, 164, April 1990.Hoa78 C. A. R. Hoare. Communicating sequential processes. Communications of the ACM, 218, 1978.HS91 W.A. Halang and A.D. Stoyenko. Constructing predictable real timesystems. Kluwer Academic Publishers, 1991.HU79 J. E. Hopcroft and J. D. Ullman. Introduction to Automata Theory,Languages, and Computation. AddisonWesley, 1979.HWT92 G. Hoffmann and H. WongToi. Symbolic synthesis of supervisorycontrollers. In American Control Conference, Chicago, June 1992.IAJ94 T.B. Ismail, M. Abid, and A.A. Jerraya. COSMOS a codesign approach for communicating systems. In Proceedings of the International Workshop on HardwareSoftware Codesign, 1994.Jag92 R. Jagannathan. Parallel execution of GLU programs. In 2ndInternational Workshop on Dataflow Computing, Hamilton Island,Queensland, Australia, May 1992.60K  87 D. J. Kaplan et al. Processing graph method specification version1.0. The Naval Research Laboratory, Washington D.C., December1987.Kah74 G. Kahn. The semantics of a simple language for parallel programming. In Proc. of the IFIP Congress 74. NorthHolland PublishingCo., 1974.KAJW92 S. Kumar, J. H. Aylor, B. W. Johnson, and W. A. Wulf. A frameworkfor hardwaresoftware codesign. In Proceedings of the InternationalWorkshop on HardwareSoftware Codesign, September 1992.KAJW93 S. Kumar, J. H. Aylor, B. Johnson, and W. Wulf. Exploringhardwaresoftware abstractions and alternatives for codesign. InProceedings of the International Workshop on HardwareSoftwareCodesign, October 1993.KL92 A. Kalavade and E. A. Lee. Hardwaresoftware codesign using Ptolemy  a case study. In Proceedings of the International Workshopon HardwareSoftware Codesign, September 1992.KL94 A. Kalavade and E.A. Lee. A global criticalitylocal phase drivenalgorithm for the constrained hardwaresoftware partitioning problem. In Proceedings of the International Workshop on HardwareSoftware Codesign, 1994.KM66 R. M. Karp and R. E. Miller. Properties of a model for parallelcomputations Determinacy, termination, queueing. SIAM Journal,1413901411, November 1966.Kur94 R. P. Kurshan. AutomataTheoretic Verification of Coordinating Processes. Princeton University Press, 1994.LL73 C. Liu and J.W Layland. Scheduling algorithms for multiprogramming in a hard realtime environment. Journal of the ACM,2014461, January 1973.LM87 E. A. Lee and D. G. Messerschmitt. Synchronous data flow. IEEEProceedings, September 1987.61LMP94a C. Liem, T. May, and P. Paulin. Instruction set matching and selection for DSP and ASIP code generation. In European Design andTest Conference, February 1994.LMP94b C. Liem, T. May, and P. Paulin. Register assignment through resource classification for ASIP microcode generation. In Proceedings of the International Conference on ComputerAided Design,November 1994.LP95 E. A. Lee and T. M. Parks.Dataflow process networks. Proceedings of the IEEE, May 1995.httpptolemy.eecs.berkeley.edupapersprocessNets.LR93 S. Lee and J.M. Rabaey. A hardwaresoftware cosimulation environment. In Proceedings of the International Workshop onHardwareSoftware Codesign, October 1993.LSV96 E. A. Lee and A. SangiovanniVincentelli. The tagged signal model a preliminary version of a denotational framework for comparingmodels of computation. Technical report, Electronics Research Laboratory, University of California, Berkeley, CA 94720, May 1996.LW94 W. Luk and T. Wu. Towards a declarative framework for hardwaresoftware codesign. In Proceedings of the International Workshop onHardwareSoftware Codesign, 1994.LWAP94 R. Lauwereins, P. Wauters, M. Ade, and J. A. Peperstraete. Geometric parallelism and cyclostatic dataflow in GRAPEII. In Proc. 5thInt. Workshop on Rapid System Prototyping, Grenoble, France, June1994.MABC92 G. Menez, M. Auguin, F Boeri, and C. Carriere. A partitioning algorithm for systemlevel synthesis. In Proceedings of the InternationalConference on ComputerAided Design, November 1992.Mar91 F. Maraninchi. The Argos language Graphical representation ofautomata and description of reactive systems. In Proc. of the IEEEWorkshop on Visual Languages, Kobe, Japan, October 1991.62Mar93 P. Marwedel. Treebased mapping of algorithms to predefined structures. In Proceedings of the International Conference on ComputerAided Design, November 1993.Maz84 A. Mazurkiewicz. Traces, histories, graphs Instances of a processmonoid. In M. P. Chytil and V. Koubek, editors, Proc. Conf. onMathematical Foundations of Computer Science, volume 176 of LNCS. SpringerVerlag, 1984.McM93 K. McMillan. Symbolic model checking. Kluwer Academic, 1993.MG95 P. Marwedel and G. Goossens, editors. Code generation for embedded processors. Kluwer Academic Publishers, 1995.Mic94 G. De Micheli. Synthesis and optimization of digital circuits.McGrawHill, 1994.Mil89 R. Milner. Communication and Concurrency. PrenticeHall, Englewood Cliffs, NJ, 1989.MP92 Z. Manna and A. Pnueli. The temporal logic of reactive and concurrent systems. SpringerVerlag, 1992.MTH90 R. Milner, M. Tofte, and R. Harper. The definition of Standard ML.MIT Press, 1990.MV94 J. McManis and P. Varaiya. Suspension automata a decidableclass of hybrid automata. In Proceedings of the Sixth Workshop onComputerAided Verification, pages 105117, 1994.OHLR94 K. Olokutun, R. Helaihel, J. Levitt, and R. Ramirez. A softwarehardware cosynthesis approach to digital system simulation. IEEEMicro, 1444858, August 1994.ORS92 S. Owre, J.M. Rushby, and N. Shankar. PVS a prototype verificationsystem. In 11th International Conference on Automated Deduction.SpringerVerlag, June 1992.Par95 T. M. Parks. Bounded Scheduling of Process Networks. PhD thesis,University of California, Berkeley, December 1995. Dept. of EECS,Tech. Report UCBERL 95105.63Pau95 P. Paulin. DSP design tool requirements for embedded systems atelecommunications industrial perspective. Journal of VLSI SignalProcessing, 9122247, January 1995.Pet81 J. L. Peterson. Petri Net Theory and the Modeling of Systems.PrenticeHall Inc., Englewood Cliffs, NJ, 1981.Pnu77 A. Pnueli. The temporal logics of programs. In Proceedings of theAnnual Symposium on Foundations of Computer Science. IEEEPress, May 1977.PP91 S. Prakash and A. Parker. Synthesis of applicationspecific multiprocessor architectures. In Proceedings of the Design AutomationConference, June 1991.QS82 J. P. Queille and J. Sifakis. Specification and verification of concurrent systems in Cesar. In International Symposium on Programming.LNCS 137, Springer Verlag, April 1982.Rei85 W. Reisig. Petri Nets An Introduction. SpringerVerlag, 1985.RH92 F. Rocheteau and N. Halbwachs. Implementing reactive programs on circuits A hardware implementation of LUSTRE. In RealTime, Theory in Practice, REX Workshop Proceedings, volume 600of LNCS, pages 195208, Mook, Netherlands, June 1992. SpringerVerlag.Row94 J. Rowson. Hardwaresoftware cosimulation. In Proceedings of theDesign Automation Conference, pages 439440, 1994.RW89 P. J. Ramadge and W. M. Wonham. The control of discrete eventsystems. Proceedings of the IEEE, 771, January 1989.RW91 J. Rasure and C. S. Williams. An integrated visual language andsoftware development environment. Journal of Visual Languagesand Computing, 2217246, 1991.SBKB90 P. A. Suhler, J. Biswas, K. M. Korner, and J. C. Browne. Tdfl Atasklevel dataflow language. J. on Parallel and Distributed Systems,92, June 1990.64SBT96 T. R. Shiple, G. Berry, and H. Touati. Constructive analysis of cycliccircuits. In Proceedings of the European Design and Test Conference, March 1996.SCG  93 U. Steinhausen,R. Camposano, H Gunther, P. Ploger, M. Theissinger, et al. Systemsynthesis using hardwaresoftware codesign. In Proceedings of theInternational Workshop on HardwareSoftware Codesign, October1993.SL93a G. C. Sih and E. A. Lee. A compiletime scheduling heuristic forinterconnectionconstrained heterogeneous processor architectures.IEEE Trans. on Parallel and Distributed Systems, 42, February1993.SL93b G. C. Sih and E. A. Lee. Declustering A new multiprocessorscheduling technique. IEEE Trans. on Parallel and Distributed Systems, 46625637, June 1993.SP94 S. Sutarwala and P. Paulin. Flexible modeling environment for embedded systems design. In Proceedings of the International Workshop on HardwareSoftware Codesign, 1994.SS63 J.C. Shepherdson and H. E. Sturgis. Computability of recursivefunctions. Journal of the ACM, 102217255, 1963.SSR89 S. Saracco, J. R. W. Smith, and R. Reed. Telecommunications Systems Engineering Using SDL. NorthHolland  Elsevier, 1989.SSV96 K. Suzuki and A. SangiovanniVincentelli. Efficient software performance estimation methods for hardwaresoftware codesign. InProceedings of the Design Automation Conference, 1996.Sto77 J. E. Stoy. Denotational Semantics The ScottStrachey Approach toProgramming Language Theory. The MIT Press, Cambridge, MA,1977.TAS93 D.E. Thomas, J.K. Adams, and H. Schmitt. A model and methodology for hardwaresoftware codesign. IEEE Design and Test ofComputers, 103615, September 1993.65tHM93 K. ten Hagen and H. Meyr. Timed and untimed hardwaresoftwarecosimulation application and efficient implementation. In Proceedings of the International Workshop on HardwareSoftware Codesign, October 1993.TMW94 V. Tiwari, S. Malik, and A. Wolfe. Power analysis of embeddedsoftware a first step towards software power minimization. IEEETransactions on VLSI Systems, 24437445, December 1994.TSV94 M. Theissinger, P. Stravers, and H. Veit. CASTLE an interactiveenvironment for hardwaresoftware codesign. In Proceedings ofthe International Workshop on HardwareSoftware Codesign, 1994.TW95 W. Takach and A. Wolf. An automaton model for scheduling constraints in synchronous machines. IEEE Tr. on Computers, 441112, January 1995.Val92 A. Valmari. A stubborn attack on state explosion. Formal Methodsin System Design, 14297322, 1992.vdB94 M. von der Beeck. A comparison of statecharts variants. In Proc. ofFormal Techniques in Real Time and Fault Tolerant Systems, volume863 of LNCS, pages 128148. SpringerVerlag, 1994.VG92 F. Vahid and D. G. Gajski. Specification partitioning for systemdesign. In Proceedings of the Design Automation Conference, June1992.VIJK94 M. Voss, T. Ben Ismail, A.A. Jerraya, and KH. Kapp. Towards atheory for hardwaresoftware codesign. In Proceedings of the International Workshop on HardwareSoftware Codesign, 1994.WA85 W. Wadge and E.A. Ashcroft. Lucid, the dataflow programminglanguage. Academic Press, 1985.WB93 E. Walkup and G. Borriello. Automatic synthesis of device driversfor hardwaresoftware codesign. In Proceedings of the InternationalWorkshop on HardwareSoftware Codesign, October 1993.66WCR94 J. Wilberg, R. Camposano, and W. Rosenstiel. Design flow forhardwaresoftware cosynthesis of a video compression system. InProceedings of the International Workshop on HardwareSoftwareCodesign, 1994.Wil94 J. Wilson. Hardwaresoftware selected cycle solution. In Proceedings of the International Workshop on HardwareSoftware Codesign, 1994.67
