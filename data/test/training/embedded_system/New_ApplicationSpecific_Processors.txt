EMBEDDED SYSTEMS form a market that isalready larger and growing more rapidly thanthat of generalpurpose computers. In fact, realtime multimedia and signal processing embedded applications currently account for over 90of all computer cycles.8 Our focus in this articlewill be on an increasingly important set ofembedded applications, consisting of portablesystems in the areas of digital communicationsand multimedia consumer electronics e.g., cellular phones, personal digital assistants, digitalvideo cameras, and multimedia terminals.These complex systems rely on powerhungryalgorithms for highbandwidth wireless communications, video compression and decompression, handwriting recognition, speech and imageprocessing, etc. The portability of these systemsmakes energy consumption a particularly critical design concern as it reduces battery life.Moreover, high power dissipation leads to moreexpensive packaging and decreases reliability.At the same time, levels of microelectronic integration continue to rise, enabling more integrated functionality on a single chip. Such integration has significant advantages from the point ofview of performance, energy consumption, andreliability, but poses a basic challenge how toeffectively design firsttimeright complex systemsonachip that meet multiple stringentdesign constraints.In order to successfully design complex systems within the short timetomarket windowscharacteristic of the embedded systems industry, it is important to maximize the flexibility orprogrammability of the target system architecture. In other words, it is desirable to move asmuch functionality as possible to embeddedsoftware. This minimizes or eliminates the needfor applicationspecific hardware accelerators,which due to their lack of flexibility may compromise timetomarket. In particular, since thespecification of such products frequentlyevolves over time, a programmable systemarchitecture diminishes the impact of suchchanges on the design. Moreover, feature differentiation within families of products canbe done in software, which greatly decreasesdevelopment time and cost.Unfortunately, the use of offtheshelf embedded processor cores is often not viable for ourapplications, because generalpurpose embedded processors even topoftheline reducedinstructionset computers andor DSP coresmay not be able to deliver the performancerequired by the application and because theymay be prohibitively expensive or inefficient,Design Challenges for NewApplicationSpecificProcessorsNew ApplicationSpecific Processors40This article discusses challenges in developingretargetable compilers and synthesis tools forapplicationspecific processor cores targeted atembedded portable digital communications andmultimedia systems. Margarida F. Jacome Gustavo de VecianaUniversity of Texas at Austin074074750010.00  2000 IEEE IEEE Design  Test of Computersparticularly with respect to energy consumption. Thus, the embedded systems industry hasshown an increasing interest in ApplicationSpecific InstructionSet Processors ASIPs, i.e.,processors tailored or specialized to the needsof a specific product or family of products. Byspending silicon where it truly matters, theseprocessors are smaller and simpler than theirgeneralpurpose counterparts, are able to runat higher clock frequencies, and are more energy efficient. The target applications in this articledigital communications and multimedia consumerelectronicsoften spend most of their cyclesexecuting a few timecritical code segments withwelldefined characteristics, making themamenable to processor specialization.Moreover, these computationintensive components often exhibit a high degree of inherentparallelism, i.e., computations that can be executed concurrently. Very Large InstructionWord VLIW ASIPs are particularly effective inexploiting such finegrained instructionlevelparallelism. See, e.g., httpwwwus2.semiconductors.philips.comtrimedia. Theseprocessors see Figure 1 comprise a largenumber of functional units such as multipliersand arithmetic logic units that can processmultiple operations and data transfers oraccesses simultaneously, thus enabling them toachieve performance commensurate with thatof dedicated hardware accelerators.4Consider the block diagram of a hypothetical programmable system architecture for amultimedia application shown in Figure 1. TheVLIW ASIP performs numbercrunching functions required by our hypothetical application,including discrete cosine and inverse discretecosine transforms and motion estimation algorithms. An offtheshelf DSP is used for the lesscomputationally demanding modem andsound codec functions the applicationrequires. A third programmable component, anapplicationspecific microcontroller, is used toprovide timely interleaving of memory andmaster control functions. Examples of embed41AprilJune 2000Register File 1 Register File 2 Register File 3Crossbar  BusTo memory systemIO IO IO IO IO IOIO IO IO IOIOALUA1ALUA5ALUA3ALUA2ALUA4MULTM1MULTM2MULTM5MACMA1MULTM3MULTM4Cluster 1 Cluster 2 Cluster 3Glue logicAD and DA C  ASIPFunctionalityDSPSystemarchitecture DSPmodem andsound codecVLIW ASIPDCT, IDCTand motionestimationMicrocontrollerASIP  Cmemory andmaster controllerVLIWprocessorASIPOnchipmemoryInternal storageand interconnectstructureFigure 1. Block diagram of a multimedia system architecture containing a VLIW ASIP witha clustered datapath.ded systems such as this onewherein three oreven more processor cores are instantiatedtwo ASIPs and one offtheshelf DSPare notunusual in practice. However, current industryefforts to develop such system architectures,and the corresponding specialized processors,require excessive manpower and resources,resulting in unnecessarily high costs that onlya few can afford. In this article, we focus on VLIW ASIPs specialized to support the timecritical, processingintensive components of our target embeddedapplications. Although these processors arepotentially attractive to implement these systemcomponents, their promise can be fully realizedonly if methodologies and tools for the synthesis of specialized processors and associatedhighquality retargetable compilers are developed. We argue here for the need to recognizethe complementarity between processor designor specialization and retargetable compilation.In simple terms, a compiler is said to be retargetable if it can generate efficient assemblycode for various target processors. We discussthis in more detail below. Indeed, observe thatwhen considering processor specialization fora group of functions or algorithms, the benefitsof a datapath structure and memory organization can be achieved only with an effectivecompiler. In turn, the effectiveness of a compiler depends on its ability to properly exploitthe specialized features that make the processor suitable to the application at hand. We will use this complementarity as a springboard to discuss the challenges associated withprocessor specialization. For concreteness, wediscuss possible solutions based on our ongoing work For information on the NOVA project, seehttphorizon.ece.utexas.edujacomenova.on a novel methodology that jointly addressesthe synthesis of specialized VLIW ASIPs andassociated memory systems and the development of highquality retargetable compilers forsuch specialized processors. As will be seen,the promise of this joint approach lies inenabling a systematic and aggressive reusebased, compilerassisted optimization of aprocessors complex costefficiency tradeoffs.As such, this is not a tutorial article, but ratherpresents our view of the various challengeslying ahead, based on past and ongoing workin this growing research area. VLIW ASIPs design spaceexploration supported byretargetable code generation The datapath of a VLIW machine is an interconnection of multiple, horizontally microcoded, possibly pipelined functional units e.g.,multipliers, arithmetic logic units, and multiplyaccumulate units that are centrally controlled.Specifically, each functional unit has dedicated, fixed control fields in a very long machineinstruction that can be independently set. VLIWmachines are microcoded in that these longinstructions execute in one cycle i.e., everyinstruction word specifies all datapath andmemory actions to be executed during thatcycle. Thus, setting up and maintaining theinstruction pipeline is the responsibility of theprogrammer or code generator accordingly,the resulting pipeline schedule is fully visible inthe machine code. A centralized controllerissues a sequence of very long instructions during program execution. VLIW machines can be seen as a hybridbetween standard SingleInstruction, MultipleData SIMD and MultipleInstruction, MultipleData MIMD parallel architectures. Indeed, aSIMD processor can be viewed as a collectionof processing elements marching in lockstepunder the orders of a centralized controllerwith each performing identical operations ondifferent data elements. By contrast, in a VLIWarchitecture, heterogenous functional units caneach be performing different operations on various data elements. Thus, the flexibility of aVLIW architecture is one step up from aSIMDvector processor. A MIMD machine, onthe other hand, has multiple, typically identicalprocessing elements, each with its own threadof control. Thus, since a VLIW machine hasonly one thread of control, it can be viewed asa carefully preplanned MIMD machine withheterogenous processing elements. Below, we will discuss the specializationdimensions that are worth pursuing for ourtarget applications and present a compilerassisted methodology for effectively exploringthe VLIW ASIP design space. New ApplicationSpecific Processors42 IEEE Design  Test of ComputersKey specialization dimensions for VLIWASIPs In tuning the microarchitecture of a VLIWmachine to an applications timecritical functions or components, three fundamental specialization dimensions must be considered  the number and type of functional units thatshould be instantiated in the machines datapath  the organization of internal storage registerfiles and corresponding interconnect structure among such register files and to or fromthe memory system  the organization of the memory system Traditionally, datapaths have been based ona single register file shared by all functionalunits. This central register file provides internalstorage as well as switching i.e., interconnection among the functional units and to or fromthe memory system. Unfortunately, this simpleorganization does not scale well with the largenumber of functional units typical of a VLIWmachine. Indeed, it has been shown that for Narithmetic units connected to a centralized register file, the area of the register file grows as N3,the delays as N32, and power requirements asN3. See Rixner et al.8 In short, as the number offunctional units increases, internal storage andcommunication between functional unitsquickly become the dominant, if not the prohibitive factor in terms of area, delay, andpower requirements. Thus, highperformanceVLSI computing systems have become limitednot by arithmetic capacity, but rather by communication bandwidth.8 Indeed, as deepsubmicron microelectronic technologies evolve,enabling very large numbers of functional unitsto be placed on small, inexpensive chips, thechallenge is to devise microarchitectures capable of costeffectively keeping these functionalunits busy. A fundamental observation is that the area,delay, and power associated with the storageorganization can be dramatically reduced byrestricting the connectivity between functionalunits and registers, so that each functional unitcan only read and write from or to a limitedsubset of registers. Thus, a key dimension ofprocessor specialization to be explored is clusteringi.e., the development of datapaths comprising clusters of functional units connectedto local storage register files. A sketch of aVLIW processor with three such clusters isshown in Figure 1.Although by moving from a centralized to adistributed register file organization one canreap significant delay, area, and power savings,this type of specialization may come at a cost.In particular, one may have to transfer dataamong these register files i.e., clusters, possibly resulting in increased execution latency.Our premise is that by carefully considering thespecifics of the target embedded applicationand by using powerful optimizing compilers,one can avoid these penalties and enjoy thebenefits. The memory system organization is also amajor specialization dimension that should becarefully considered when designing a VLIWASIP for the set of applications of interest. Thisis so because the large amounts of data parallelism typically exhibited by such applicationsprovide major opportunities for performanceenhancement. However, in order to effectivelyexplore such parallelism, one needs to be ableto stream data to the clusters or functional unitsat a sufficiently high rate i.e., high memorybandwidth is required. The good news is thatthe algorithms of interest typically perform welldefined access patterns on simple, regular datastructures i.e., vectors and matrices whosedimensions are known at compile time. Thisstrongly suggests that a distributed memory organization comprised of a number of small, fastmemory banks properly designed to exploitlocality or regularity in these data access patterns can provide the required memory bandwidth at a reduced cost i.e., power or energy,area, and delay. As in the previous case,though, the timely design of such specializedmemory organizations encompassing datapartitioning, memory allocation and assignment, scheduling of data operations, loop transformations, and other complex tasks requiresthe availability of effective memory synthesissystems working in conjunction with powerfuloptimizing compilers. Finally, the interconnect structure among43AprilJune 2000clusters and from clusters to memory is a thirdmajor specialization dimension that should beexplored. By choosing an appropriate interconnect structure i.e., a bus or crossbar configuration matching the parallelism and accesspatterns in the algorithms of interest and thenappropriately binding the algorithms operations to clusters, one can explore relevantcostperformance tradeoffs. In summary, the key specialization dimensions for the clustered VLIW machines we introduced above are as follows. First, one needs todefine an effective organization for the memorysystem. Then, one needs to determine the number of clusters to be instantiated in the datapathand the interconnect structure among the clusters and to or from the memory system.Moreover, for each cluster, one needs to determine the number and type of functional units tobe locally instantiated, as well as the capacity ofthe clusters local register file. Devising an optimal datapath and memory system organizationfor a given application i.e., one that meets thetarget performance while achieving low siliconcost and high energy efficiency is an exceedingly complex, multiobjective optimization problem. In practice, this requires intensive designspace exploration with tradeoffs involving physNew ApplicationSpecific Processors44 IEEE Design  Test of ComputersRetargetable compilerOptimized assembly codeApplication specificationtime critical segments and requirementsperformance, energypower, etc.VLIW ASIP designarchitectural levelAlternativesexplorationAlternatives selectionSolution evaluationSpecialized datapathand memory system   Data partitioning andbindingassignmentOperationdatabinding assignmentto clustersPerformance enhancingcode transformationsBounds on executionlatencyenergycode sizeCoarse scheduling anddata routingRegister allocationand detailed scheduling   Library of parameterizeddatapathmemorycomponentsFast floorplannerAreadelaypowerestimatorsDatapathData memoryProgram memoryIncreased programmemory requirementsMemory requirements Increased data transfers Cluster 2Increase in requiredinternal storageIncreased accessesspills to memory Memorysystem Measures of compiler effectivenesscode size, energy consumption,and execution latencyCompiler issues for embedded VLIW ASIPs       Optimizationand codegenerationtechniquesSufficient memorybandwidthApplication levelperformanceestimatesPhysicalestimatesRegister File 1IO IO IOALUA2IOALUA3MULTM4MULTM3Cluster 1Register File 1IO IO IOALUA1MULTM1MULTM2Figure 2. Research overview VLIW ASIP design and retargetable compilers.ical figures of merit combinational delay, area,power dissipation as well as applicationlevelperformance metrics throughput or latency,code size, energy consumption. In the next subsection, we discuss these challenges and propose a reusebased, compilerassisted designspace exploration methodology for specializinga VLIW ASIPs datapath and memory system toa given class of applications.Methodology As shown in Figure 2, given a characterizationof the application of interest, our goal is to support both an iterative search design spaceexploration for optimal specialized memoryorganizations, datapaths, and interconnect structures i.e., solutions that deliver the required performance at low silicon cost and high energyefficiency and the generation of highly optimized assembly code. The key elements of theproblem are as follows. To effectively explorethe huge space of specialized datapaths andmemory systems, one must have an infrastructure enabling a structured selection and evaluation process. To evaluate candidate solutions,one must have reliable estimates of the costefficiency physical metrics of a datapath, memorysystem, and interconnect configuration, as wellas associated applicationlevel performance metrics. Since these estimates will drive the search,they should be performed early i.e., prior to thedetailed datapath and memory system designand codegeneration process. To enable the selection of alternatives, wepropose to structure the solution space by usinga hierarchical parameterization of candidatesolutions. The idea is to build a library of fundamental components parameterized with respectto relevant features e.g., clusters parameterizedby the number of functional units and registerfiles and memory banks parameterized by technology, speed, and access modes size andports. Such components can then be composedonthefly to define the datapath, memory, andinterconnect alternatives. Note that such anapproach narrows the solution space along thecritical specialization dimensions discussedabove i.e., supports the search for effective distributed organizations of computationalresources, such as clusters, and data storageresources. Moreover, as shown on the right sideof Figure 2, the proposed parameterization ofcomponents can facilitate early reliable estimation of physical figures of merit, such as delay,area, and power. Specifically, a fast floorplanner together with a database of statistically characterized clusters and memory banks can beused to derive estimates with improved reliability. This approach is a natural application of previous research on design reuse and earlyestimation see, e.g., Jacome and Peixoto6.To evaluate the suitability of a specializedmemory organization and datapath to a targetapplication, one must estimate the executionlatency, code size, and energy consumptionthat can be achieved for the applications target code segments. As shown in Figure 2, thetasks required to derive such application estimates and the retargetable compilation problem are closely linked. The first such task is datapartitioning and allocation or binding to memory banks so as to maximize raw memory bandwidth given the expected memory accesspatterns. Then, one needs to determine a binding or assignment of an applications operations to the datapaths clusters that is likely toreduce execution latency i.e., what to executewhere. As mentioned earlier, for datapathswith distributed register files, careful attentionshould be paid to penalties incurred by bindings that introduce data transfers among clusters, as well as significant imbalances betweencomputation and memory operations. Thethird task is to determine optimal behavior preserving code transformations so as to enhanceperformance e.g., increase parallelism or concurrency by pipelining loop iterations. Notethat such optimizations should take intoaccount both computation operations and therequired memory access operations so as toproperly balance the load on the datapath andon the memory system over time. As we discussbelow, such transformations may significantlyincrease program size, a major drawback formemoryconstrained embedded systems, aswell as internal data storage requirements i.e.,register pressure and energy consumption andthus must be carefully assessed for the class ofembedded components of interest. Given a set of bindings or assignments and45AprilJune 2000parallelismenhancing transformations, oneneeds to quickly generate bounds estimatesfor an applications execution latency and otherapplicationspecific metrics of interest. Suchbounds may be analytical e.g., Jacome and deVeciana4 andor simulationbased, in whichcase the proper level of abstraction for suchfast simulation must be devised seehttpwww.trimaran.orgdocs.html. Note thatwhile the above tasks are required to select andevaluate candidate memory organizations anddatapaths, they also represent the initial phasesof the retargetable compilation processthusour argument that the two problems are closelylinked. The final research challenge is to enablethe use of information generated at each iteration of this process to guide or assist the designspace exploration process itself i.e., help determine which alternative clustered datapath andmemory system configurations are most promising and thus should be considered next. Retargetable compilation for VLIWASIP cores Emerging retargetable compiler technologyfor clustered VLIW ASIPs has two challengingroles as an essential component to assist thesynthesis of specialized processors and as ameans to produce highquality embedded software for clustered VLIW machines. In this section, we argue that meeting these newchallenges may require fundamental changesin the traditional compilation process and wediscuss some research directions. Traditional compilers typically include threemain modules a languagedependent frontend, an intermediate optimization stage, and amachinedependent backend.1 The frontendmodule takes source code written in a highlevel programming language such as C or Cand generates an internal or intermediate representation of the behavioral description. Theintermediate optimization stage performsmachineindependent powerful performanceenhancing transformations on this internal representation. Finally, the backendgenerates machine code for the target processor architecture. Thus, in a traditional compiler, the specifics of the target machineinstruction set and structural details are takeninto account only during the last codegeneration stage of the compilation process. The broad aim of traditional compilers is toquickly produce fast code. Thus, the time complexity of compilation algorithms is a major concern. By contrast, when considering compilersfor embedded processors, the quality of the produced machine code is much more importantthan the speed of the compilation process. Thus,the use of more powerful optimization algorithms, even if timeconsuming, becomes necessary and justifiable. Moreover, for the embeddedapplications of interest, in addition to throughput, the code size and energy consumption areimportant for the generated code. Thus, the compilation process for embedded applications has abroader set of goals and constraints. Most current compilers target a specificprocessor architecture, which is, for the mostpart, hardcoded in the codegeneration module. However, in order to enable design spaceexploration for specialized VLIW machines, thistarget specificity is inadequate. Indeed, as weargued above, processor specialization can beexplored only via an automatically retargetable compiler. A compiler is said to be automatically retargetable if the same compiler i.e.,same executable can be used for a range possibly limited of target architectures. This isachieved by providing the compiler with adescription of the target processor architectureusing a specialpurpose language.7 See, e.g.,httpwww.trimaran.orgdocs.html.In summary, compiler technology forembedded VLIW ASIPs requires both retargetability as well as the ability to deal with a larger set of applicationlevel performance issues.Moreover, when compilers are used to drive adatapath and a memory system synthesis or specialization process, they should encompass orprovide a broader set of tools to assist the various phases of this process. Below, we discussthe impact of these new requirements on the traditional compilation framework. Code generation background andchallenges posed by VLIW ASIPs Traditional code generation. Traditional codegeneration consists of three main phasesinstruction selection, register allocation andNew ApplicationSpecific Processors46 IEEE Design  Test of Computersassignment, and scheduling and compaction.During the instruction selection phase, an intermediate representation of the source programis mapped to atomic machine operations, ormicrooperations, each typically specifyingthe transfer of a computed value to a register ormemory location. Next, during the register allocation and assignment phase, program variables and intermediate results are mapped tosets of machine registers and then to specificphysical registers. When scheduling is performed, a partial order for the execution of thepreviously obtained microoperations is established that maintains the semantics of the original program. Finally, during compaction,microoperations are mapped to i.e., compacted into actual machine instructions. Notethat compaction takes place only for machineswith some degree of parallelism e.g., superscalar or VLIW processors. These three phases are each exceedinglycomplex as well as mutually dependent, soaddressing them sequentially can result in suboptimal code. This is known as the phasecoupling problem.1 Although there is someconsensus on the adequacy of the above phasing in the context of traditional compilers, theproblem is far from solved for clustered VLIWASIPs. Moreover, traditional algorithms orapproaches to address compilation steps maynot work well for such machines. Code generation for embedded VLIW ASIPs.A key aspect driving the need to reevaluate thecodegeneration process for VLIW ASIPs is thehierarchical organization of the datapath intoclusters i.e., distributed smaller datapathswith limited communication bandwidth.Indeed, in this context, a particularly criticalsubproblem is the binding or assignment ofoperations to clusters, since it is crucial to effectively exploit the finegrain parallelism presentin the program. Although the cluster binding orassignment phase can be viewed as an abstractform of instruction selection, this critical stepis not present in traditional code generation.The quality of a cluster binding or assignmentdepends on achieving a good tradeoff betweenmaximizing parallelism i.e., allowing executionof as many concurrent operations as possible,even if that means spreading them across theclusters and minimizing data transfers acrossclusters or to memory, since these may harmlatency and energy consumption. In order toassess the quality of a binding, some relaxedform of early scheduling or compaction anddata routing is required. Thus, there is a need torevise the manner in which the coupling amongvarious compilation phases is handled in thecontext of clustered machines. The clustered nature of these machines alsoimpacts the effectiveness of traditional compilation algorithms. For example, coloringbasedapproaches have been successfully used forregister allocation or assignment.7,1Unfortunately, such algorithms may performquite poorly in the context of distributed register file organizationsin particular, when minimizing memory spills is important so as toreduce energy consumption. Hence, there isthe need to develop approaches that considerdatapaths with several register files and effectively route streams of data to or from andacross register files clusters. In doing so, oneshould attempt to minimize the need for spillsto memory to reduce energy consumption aswell as maximize the time windows for completing the required memory accesses to avoidcongestion on the interconnection structure.Along these lines, we propose2 a technique thatexplores tradeoffs between the size of potential prefetching windows and minimization ofspills, given a maximally concurrent scheduleof operations. Taking a broader view of this problem, effective code generation for clustered machineswith distributed storage requires not only revising the problem decomposition and associated algorithms but also taking a fresh look at theway dependencies among the resulting subproblems are handled. Specifically, the traditional sequential approach to code generation,using monolithic algorithms, needs to bereplaced by a hierarchical, iterative compilationprocess. In such an approach, early compilation tasks e.g., cluster binding or assignmentmight be based on an abstract, coarse view ofthe datapath, permitting a rough assessment ofquality without completely carrying out scheduling, register assignment, and data routing.47AprilJune 2000The subsequent compilation tasks would useincreasingly precise models of the processordatapath, memory system, and interconnectstructure to achieve highquality code generation. Thus, a hierarchy is needed that enablesone to easily move from coarse to finegrainmodels and back. Of particular importance isthe ability to backannotate highlevel modelswith key information extracted from moredetailed ones. This not only allows a moreeffective exploration of possible compiled codebut also is compatible with the need to supportthe datapath and memory design process, aswe discussed above. We propose4 and briefly illustrate below analgorithm and model that exhibits some ofthese desirable characteristics, while addressing the critical cluster binding or assignmentproblem. Given a timecritical loop body segment for which code is to be generated, themodel and associated algorithm realize a careful decomposition and relaxation of the globalscheduling problem, both in time machinecycles and space clusters and interconnection network. This decomposition, called awindow dependency graph, allows one to control complexity while reasoning about bindings. In particular, it is useful to exploretradeoffs between achieving high instructionlevel parallelism ILP versus data transferpenalties required to achieve such parallelism.The key idea is to judiciously relax both capacity and scheduling constraints so as to efficiently reason early on about binding. Thus, forexample, one might initially assume unlimitedlocal storage capacity but retain a finite aggregated capacity constraint for the interconnection network. This relaxed model for thedatapath allows one to bring in both scheduling and a crude form of data routing, while handling important cluster assignment decisions.This model is currently being extended so as tosupport other phases of the codegenerationprocess as well as data partitioning. In addition to the codegeneration issues discussed above, many of the socalled intermediate performance optimizations traditionallyperformed in a machineindependent fashionwill have to be merged into the codegeneration phase, at least during the generation offinal or optimized machine code. Next, we discuss the rationale and challenges in thisdomain. Performanceenhancing optimizationsbackground and challenges posed byVLIW ASIPSAs summarized in Figure 2, there are multipleissues specific to compilation for embeddedclustered VLIW ASIPs that make the use of traditional machineindependent optimization techniques problematic. In particular, embeddedapplications may have stringent throughput aswell as program memory andor energy consumption constraints. When not used judiciously, these techniques may result in prohibitiveincreases in code size. Moreover, some suchtransformations may increase memory accesses, including spills due to insufficient local storage, which in turn may reduce throughputandor increase energy consumption. This suggests that during initial design spaceexploration or compilation steps, it may makesense to apply these techniques in a quasimachineindependent context in order toquickly obtain upper bounds on achievablethroughput and general guidance on themachine specializations that would be morefavorable. Then, as the specifics of the targetmachine under construction are fleshed out,they should be incorporated into these algorithms, so as to provide moreprecise tradeoffinformation e.g., tighter bounds on the applicationspecific metrics of interest. Eventually,when the microarchitecture of the VLIW ASIPis fully defined and the assembly code for thetarget machine is to be generated by the compiler, such algorithms should be moved into thefirst phases of the codegeneration process andinclude latency, code size, and energy consumption constraints associated with theembedded application or system. We will illustrate the inadequacies ofmachineindependent optimizations and drawbacks of focusing solely on latency by discussing software pipelining, a technique thathas traditionally been used quite effectively toincrease ILP for timecritical loops.7 In softwarepipelining, direct data dependencies betweenoperations are reduced and thus ILPNew ApplicationSpecific Processors48 IEEE Design  Test of Computersincreased by creating a modified loop bodythat pipelines operations from several loop iterations. Unfortunately, software pipelining mayresult in significant increases in code size.Software pipelining requires an epilogue to fillin the iterations pipe and a prologue to emptythe pipe. Moreover, software pipelining usually increases internal storage requirements,7,5thus when applied to clustered machines withlimited local storage, spills to memory andordata transfers may be introduced, which compromise the gains expected from increased ILP.At the extreme, these additional spills and datatransfers can result in decreased throughput,while unnecessarily increasing code size andenergy consumption. An example of an algorithm that capturesmachine specifics and incorporates constraintsbeyond latency is the software pipelining algorithm proposed in Jacome et al.5 This algorithm,to be executed after cluster assignment, canminimize latency under code size and resourceconstraints and considers the effects on performance of memory operations. We are currently working on extensions that control theincrease in the lifetime of data objects, sincethese can lead to costly spills to memory. As alluded to previously, the applications ofinterest typically also exhibit large amounts ofdata parallelism, providing major opportunitiesfor performance enhancement. To exploit dataparallelism, one must adequately partition dataand assign them to memory banks in the distributed memory system and adequately schedule the associated memory operations. Thispermits one to costeffectively maximize therate at which data can be streamed to the datapath, while balancing computation and memory operations. Although some work has beendeveloped in the datapartitioning, assignment,and scheduling areas see, e.g., Catthoor etal.3, robust optimization techniques to addressthese problems in the context of a distributedorganization of computational resources i.e.,clusters and data storage resources are stilllacking. Due to the firstorder impact on performance and power or energy consumption ofmemory accesses for our class of applications,the need for such techniques cannot beoveremphasized. In this article, we have identified the main challenges posed by clustered VLIW ASIPs, havediscussed promising research directions, andhave briefly presented some of our work in thisarea. We focused on clustered VLIW machinesfor two reasons. First, they constitute an important class of new machines that are particularly effective in the context of increasinglypervasive portable digital communications andmultimedia consumer electronics. Second,they introduce a form of hierarchical aggregation that we believe not only is here to stay butalso will be increasingly critical, as microelectronic technology enables increasing levels ofintegration. With the possibility of placing avery large number of processors on a singlebilliontransistor chip on the near horizon, wesee a vast arena for research in applicationspecific, highperformance computing systems.Specifically, the future challenges might lie inthe development of singlechip specialpurpose networks of loosely connected supernodes i.e., clusters or aggregates of specializedprocessors such as the VLIW ASIPs we discussed here, cooperating on a MIMD or quasiMIMD schema and supported by an effectivedistributed main memory system that is partially or fully placed on a chip. Still, the startingpoint is to develop a solid understanding onhow to control the complexity associated withdesigning and compiling for a single such specialized processor. AcknowledgmentsThis work is supported by a National ScienceFoundation NSF Career Award MIP9624231,NSF Award CCR9901255, and Grant 00365806491999 of the Texas Higher Education CoordinatingBoard Advanced Technology Program. References 1. A. Aho, R. Sethi, and J. Ullman, Compilers Principles, Techniques and Tools. Reading, Mass.AddisonWesley, 1988. 2. R. Anand, M.F. Jacome, and G. de Veciana,Heuristic Tradeoffs Between Latency and EnergyConsumption in Register Assignment, IEEEACM8th Intl Workshop HardwareSoftware Codesign,May 2000. 3. F. Catthoor, S. Wuyack, E. Degreef, F. Balasa, L.49AprilJune 2000Nachtergaele, and A. Vandecappelle, CustomMemory Management Methodology Explorationof Memory Organization for Embedded Multimedia System Design. Kluwer Academic Publishers,1998. 4. M. Jacome and G. de Veciana, Lower Bound onLatency for VLIW ASIPs, Proc. ACMIEEE IntlConf. Computer Aided Design ICCAD, Nov.1999. 5. M. Jacome, G. de Veciana, and C. Akturan,Resource Constrained Dataflow Retiming Heuristics for VLIW ASIPs, 7th Intl WorkshopHardwareSoftware Codesign, May 1999, pp.1216. 6. M. Jacome and H. Peixoto, Design Reuse, HowFar from Delivering the Promise, IEEE Designand Test of Computers, to appear. 7. P. Marwedel and G. Goossens, eds, Code Generation for Embedded Processors. Kluwer Academic Publishers, 1995. 8. S. Rixner, W. Dally, B. Khailany, P. Mattson, U.Kapasi, and J. Owens, Register Organization forMedia Processing, Proc. 26th Intl Symp. HighPerformance Computer Architecture, May 1999.Margarida F. Jacome isan assistant professor in theDepartment of Electrical andComputer Engineering at theUniversity of Texas at Austin.She received the BS and theMS degrees from the Technical University ofLisbon in 1981 and 1988, respectively, and thePhD degree in electrical and computer engineering from Carnegie Mellon University in 1993.Her research focuses on CAD for hardwaresoftware codesign of embedded systems, applicationspecific highperformance programmablearchitectures, and retargetable compilers. In1992, she was the recipient of the ACMIEEEDesign Automation Conference Best PaperAward. She is the recipient of a HalliburtonFoundation Award of Excellence and a 1996National Science Foundation Career Award.Gustavo de Vecianareceived his BS, MS, andPhD in electrical engineeringfrom the University ofCalifornia at Berkeley in1987, 1990, and 1993,respectively. In 1993, he joined the Departmentof Electrical and Computer Engineering at theUniversity of Texas at Austin, where he is currently an associate professor. His researchfocuses on issues in the design and control oftelecommunication networks and developingalgorithms for CAD. He is an editor for theIEEEACM Transactions on Networking. He is therecipient of a General Motors FoundationCentennial Fellowship in Electrical Engineeringand a 1996 National Science Foundation CareerAward.Direct comments and questions to MargaridaJacome, Department of Electrical and ComputerEngineering, University of Texas, Austin, TX78712 jacomeece.utexas.edu.New ApplicationSpecific Processors50 IEEE Design  Test of Computers
