QUALITY AND RELIABILITY ENGINEERING INTERNATIONAL
Qual. Reliab. Engng. Int. 2008;
Published online 1 April 2008 inWiley InterScience (www.interscience.wiley.com). DOI: qre
Research ComputerSystemsAvailability
EvaluationUsingaSegregated
FailuresModel
Sergiy A. Vilkomir1,∗,†, David L. Parnas2, Veena B. Mendiratta3 and Eamonn Murphy
1Software Quality Research Laboratory (SQRL), Department of Electrical Engineering and Computer Science
University of Tennessee, Knoxville, TN, USA
2Software Quality Research Laboratory (SQRL), Department of Computer Science and Information Systems
University of Limerick, Limerick, Ireland
3Chief Technology Office, Alcatel-Lucent, Naperville, IL, USA
4Department of Mathematics and Statistics, University of Limerick, Limerick, Ireland
This paper presents the segregated failures model (SFM) of availability of fault
tolerant computer systems with several recovery procedures. This model is compared
with a Markov chain model and its advantages are explained. The basic model is
then extended for the situation when the coverage factor is unknown and the failure
escalation rates must be used instead. A simple practical analytical approach to avail
ability evaluation is provided and illustrated in detail by estimating the availability
of two versions of a reliable clustered computing architecture. For these examples
numeric values of availability indexes are computed and the contribution of each
recovery procedure to total system availability is analysed. Copyright © 2008 John
Wiley & Sons, Ltd
Received 23 November 2006; Revised 28 January 2008; Accepted 31 January
KEY WORDS: software; fault tolerance; availability; reliability; recovery; fault model
1. INTRODUCTION
Usually, when a computer system fails, a variety of recovery procedures are available. Some proce-dures are expensive, whereas others are cheap; some result in loss of data, whereas others minimizeloss of data; some provide full service after restoration whereas others provide reduced service. In
this paper we look only at characteristics important for system availability evaluation: time and applicability
to various classes of failures. Thus, it is important for us that some recovery procedures are fast, but apply
only to a limited class of failures and other procedures are slow but will restore service in more cases
The main reason for using several recovery procedures is to reduce restoration (recovery) time. A recovery
strategy involves applying recovery procedures in a specific order until recovery is successful. In other
words, a recovery strategy is divided into several levels, with one recovery procedure assigned to each level
∗Correspondence to: Sergiy A. Vilkomir, Software Quality Research Laboratory (SQRL), Department of Electrical Engineering and
Computer Science, University of Tennessee, Knoxville, TN, USA
†E-mail: vilkomircsutkedu
Copyright q 2008 John Wiley & Sons, Ltd
448 S. A. VILKOMIR ET AL
When a failure has occurred, the procedure with the shortest recovery time, for example, switching to a
waiting redundant computer (hot spare), is usually applied first (level 1). If the first recovery attempt is not
successful, the level 2 procedure, for example, a computer restart, is applied, and so on. We assume that the
highest-level procedure always guarantees a recovery. This way of defining recovery levels is close to1 but
is more general because it does not specify the content of procedures
The first recovery procedures are usually automatic. The final procedure is usually a manual repair. In
this paper, we assume that the order of procedures is fixed, i.e. there is no examination of the cause of the
failure except for specific hardware failures when it can be determined that the further use of automatic
recovery is not expedient. In this case, the intermediate levels are skipped and the highest-level procedure
(the manual repair) is applied
Several recovery procedures are often used for telecommunication systems. A specific example of a
practical use is the reliable clustered computing (RCC)2. RCC methodology provides an implementation
of various fault-tolerance recovery strategies to achieve high availability of commercial nonfaulttolerant
systems. We use this system to illustrate our new approach (see Sections 3 and
Other application areas of this recovery method are database systems and operating systems. Thus, a
database system with the three-level recovery could, for example, use the following recovery technique
• With built-in redundant pointers in data structures to be able to recover from certain types of failures
• That maintains backup copies of parts of the data structures
• That keeps a complete backup copy of the database on a separate device
An example of the operating system with several recovery procedures is Sprite, a distributed UNIX
compatible operating system. The following two-level recovery mechanism is used
• The system first tries to recover quickly from backup data that it stored in the main memory
• If this fast recovery fails, the system returns to the traditional disk-based hard reboot
For availability evaluation of such systems, Markov chains5–8, Matrix-Geometric solutions9, and Petri
nets10 have been used. These are powerful mathematical methods. However, analysis using these methods
can be quite complicated and often requires special software tools. When such tools are used, calculations
that require careful scrutiny are hidden from users. Both the models and the algorithms inside the tools
are often based on implicit assumptions; if these assumptions are not valid for the actual application, the
results are of doubtful value. In this paper, we propose a new model and a simple analytical approach to
availability. The hardware/software failures are divided into several types, and the availability of the system
is calculated separately for each type of failures. This model makes calculations more understandable for
users and allows determining the impact of each type of failures on the availability of the whole system
This paper is partially based on an extension and refinement of our earlier investigations11,12. In Section
we consider SFM and present a mathematical approach that allows us to calculate the availability of the
hardware/software systemwith several recovery procedures. In Sections 3 and 4, examples of the applications
of the proposed approach to two various versions of the RCC product are considered. Availability of these
products was previously analysed in6,7 using a Markov chain model. In this paper, we illustrate our approach
using the same applications and the same input data. Detailed results of the numerical availability evaluation
are provided and the impact of (1) various types of failures and (2) coverage factors on the system down
time is analysed. General conclusions are presented in Section
2. SFM OF A SYSTEM WITH SEVERAL RECOVERY PROCEDURES
2.1. Comparison with traditional systems with one recovery procedure
The difference between a traditional system with one recovery procedure and a system with several recovery
procedures is illustrated in Figure 1. As usual, it is assumed that the system can be in only two states
a normal (working) state NS and a failed state FS. A transition from the normal state to the failed state is
Copyright q 2008 John Wiley & Sons, Ltd. Qual. Reliab. Engng. Int. 2008;
DOI: qre
COMPUTER SYSTEMS AVAILABILITY EVALUATION
a
b
Figure 1. System with one recovery procedure (a) and with several recovery procedures b
described by failure rate For the traditional system (Figure 1(a)), transition from the failed state to the
working state is characterized by a restoration rate or, equivalently, mean restoration time It is
assumed that only one recovery procedure exists and that a result of recovery is always successful. In these
circumstances, the term restoration refers to the process of restoration as well as its result
For the system in Figure 1(b), n (n>1) different recovery procedures exist. When a failure occurs, usually
recovery procedures are applied sequentially starting from level 1. For every procedure except the nth
the result of the recovery can be either successful or unsuccessful. If the recovery procedure at level 1 is
unsuccessful, the level 2 procedure is applied, i.e. the failure is escalated from level 1 to level 2, etc. It is
assumed that level n recovery is always successful
Similar to a traditional system, every recovery level i is described by restoration rate or mean restoration
time i . However, the meaning of these indexes is slightly different here. Because time here is the
time required for the attempt, whether or not it succeeds, the term restoration refers here only to the process
of restoration, not to its result (successful or unsuccessful
It is important to note that the use of several recovery procedures is different from the recovery block
approach (one of the fault-tolerance software techniques13,14) despite some similarities between them. The
recovery block approach also uses several procedures (alternates). However, all alternates perform the same
desired operation and are executed sequentially until the operation performance is accepted. Thus, in systems
with several recovery procedures, these procedures are used for restoration, i.e. the procedures restore service
so that the normal programs can be run. In the recovery block approach, the procedures are used to perform
some system functionalities (operations), i.e. the alternates are used instead of the normal code. Markov
chains have been mainly used to model recovery blocks
2.2. Segregated failures model
Let F be the complete set of all possible failures of the system. As it was mentioned, the result of recovery
from a failure can be either successful or unsuccessful. We say that a failure is served at level i if the level
i procedure is applied (with two possible results) to this failure. This means that recovery at previous levels
has been unsuccessful. Let Fi be a set of failures that are served at level i , Fi ⊆F . Now consider only
failures, for which the result of recovery at level i is successful
Definition 1. A failure f is said to be a failure of type i if and only if i is the lowest level where this failure
is successfully served
Denote a set of such failures as Ftypei . It follows from Definition 1 that Ftypei ⊆Fi and a set of Ftypei
partitions F , ie
F
n
i
Ftypei
Copyright q 2008 John Wiley & Sons, Ltd. Qual. Reliab. Engng. Int. 2008;
DOI: qre
450 S. A. VILKOMIR ET AL
and
∀i, j :1≤ i, j≤n, Ftypei ∩Ftype j =∅
The ability of the recovery procedure to successfully restore a normal operation after a failure is often
described by a coverage factor. Adapting it for our model, consider the following definition
Definition 2. A coverage factor prec,i of the recovery level i is a conditional probability that a failure is
successfully served at level i given that this failure is served at level i
More formally
prec,i = P( f ∈Ftypei | f ∈Fi )
We mentioned above that usually failures are escalated sequentially, from level i to level i+1. We assume
that the recovery procedure is independent of the nature of the failure and is applied to all hardware and
software failures. However, if at any level it is diagnosed for a specific failure that the usage of next recovery
levels is not expedient, these levels can be skipped and this failure can be escalated directly to the last
level n. Thus, there are three possibilities when a failure recovery is attempted at level i,1≤ iltn
• The recovery is successful
• The recovery is unsuccessful and the next-level procedure will be applied (the failure is escalated from
level i to the next level i
• The recovery is unsuccessful and the highest-level procedure will be applied (the failure is escalated
to level n
These assumptions reflect RCC applications as discussed in the case study sections. The model that we
present here can be extended to consider additional hypothetic possibilities such as
• Skipping some restoration levels but not all of them
• Using diagnostics that allows changing the order of recovery procedures for every specific failure
depending on the nature of the failure
We do not address these extensions in this paper
According to Definition 2, the probability of the first possibility is prec,i . Probability pnext,i of the second
possibility and probability plast,i of the third possibility are determined as follows
pnext,i = P( f ∈Fi+1 | f ∈Fi )
plast,i = P( f ∈Fn∧¬ f ∈Fi+1 | f ∈Fi )
Probability prec,i is defined for 1≤ i≤n, and probabilities pnext,i and plast,i are defined only for 1≤ iltn
For consistency, we additionally determine that pnext,n= plast,n=0. It follows from the definitions that
prec,n=1, plast,n−1=0 and prec,i+ pnext,i+ plast,i
Some failures can be escalated to the last level even before applying the procedure of level 1. For consis
tency with previously introduced notation, let us denote the probability of this event as plast,0. Accordingly
we denote the probability that the procedure of level 1 is applied as pnext,0, pnext,0=1− plast
The described division of failures into the types and the main parameters of the model are shown in
Figure
To summarize, every level i is defined as 4-tuple i , prec,i , pnext,i , plast,i ). In turn, the whole system is
defined as (n+2)-tuple  pnext,0, i , prec,i , pnext,i , plast,i ni
We do not assume that all failures are independent. In other words, the model can contain commoncause
and/or correlated failures. Such failures tend to belong to the same type and hence may have an influence
on numerical values of the parameters of the model. However, the same model and the same approach to
availability evaluation are used for both independent and correlated failures
Copyright q 2008 John Wiley & Sons, Ltd. Qual. Reliab. Engng. Int. 2008;
DOI: qre
COMPUTER SYSTEMS AVAILABILITY EVALUATION
Figure 2. Classifying failures into the types
2.3. Availability evaluation
The main idea of SFM is classifying processor failures into several types in accordance with the described
model and evaluating the influence of each type of failure on the availability of the whole system. The
approach to availability evaluation based on SFM consists of the following six main steps
• Step 1: separating all failures into different types corresponding to the lowest recovery level where
faults are successfully recovered as it is considered in Section
• Step 2: for each type k, evaluating the probability ptypek that a failure belongs to type k, i.e. ptypek
P( f ∈Ftypek ). For type
ptype1 = prec,1× pnext,0
For types k, 1<k<n, the probability is evaluated as
ptypek = preck
k
i
pnext,i× pnext,0
For type n
ptypen

plast
n
j

plast, j
j
i
pnexti


n
i
pnexti

× pnext,0+ plast,0
It is possible to prove that
n
k
ptypek =1
which follows immediately from classifying failures into mutual exclusive types
Copyright q 2008 John Wiley & Sons, Ltd. Qual. Reliab. Engng. Int. 2008;
DOI: qre
452 S. A. VILKOMIR ET AL
• Step 3: evaluating the failure rate for failures of each type k
ptypek
where ptypek are determined by
From (9) it follows that

n
k

• Step 4: evaluating the restoration rate for failures of each type k. For failures of type k, the mean
restoration time should include mean service time at level k and also include time that has
been unsuccessfully spent for recovery at the previous levels

k
i

As the mean restoration time typek , the restoration rate is

k
ii

where is the restoration (service) rate for the recovery procedure at level i .• Step 5: evaluating the availability. As a measure of availability, calculate the expected down time Tdk
during a fixed period of time T relative to failures of type k
Tdk(T )=T (1−Ak)=T
typek

where Aktypektypek typek ) is the availability factor. The down time can be measured in minutes
per year. In that case T =365×24×60=525600min
Usually typek . Then, instead of formula (14), it is possible to use the approximate value of
the down time per year
Tdktypek
calculating in ‘failures per year’and in minutes.• Step 6: evaluating the down time of the system based on the results of the previous step. To calculate
the total down time, just sum up the down time for every type of failures
Td
n
k
Tdk
It is necessary to mention that even when the exact formula (14) is used, formula (16) can give an
approximate result. Because failures of different types are considered separately, we take into account the
situation when a failure of one type can occur during recovery after a failure of another type. If the real
situation or an assumption of a model is opposite (i.e. the system never fails during recovery) then our
approach of availability evaluation gives a slightly pessimistic result. However, because in practice is
much less than T (see the example in Section 3), the difference between approximate and exact results is
not important
2.4. Rates of escalation
In this section we propose an extension of SFM to deal with a different form of input data. In the previous
section, we assumed that some conditional probabilities are known, specifically, the probabilities prec,i
pnext,i , and plast,i . For applications of the Markov chain model, explicit rates of transitions between system
states are often used as input data instead of these probabilities. This situation is also possible for SFM
Copyright q 2008 John Wiley & Sons, Ltd. Qual. Reliab. Engng. Int. 2008;
DOI: qre
COMPUTER SYSTEMS AVAILABILITY EVALUATION
In that case, the input data for the model are
• , the successful failure recovery rate,• , the rate of escalation from level i to the next level i+1.• , the rate of escalation from level i to the last level n
All these three possibilities are mutually exclusive and exhaustive. Therefore, the failure processing rate
at level i (i.e. failure exit rate regardless of the results of recovery) is the sum of rates for these possibilities
recinextilasti
The best way to calculate the availability from these data is to express prec,i , pnext,i , and plast,i using
, , and and then apply the basic approach described in Section 2.3. Each probability is
determined as a ratio of the corresponding rate to the failure proceeding rate of the whole procedure
prec,i =

=


pnext,i =

=


plast,i =

=


To apply the approach from Section 2.3, we also need to express mean service (processing) time at
level i . For traditional systems with one recovery procedure, the mean restoration time is a reciprocal value
of the failure restoration rate. For systems with several recovery procedures, there are different rates for
each level, in particular, rates of successful failure recovery and failure proceeding rate . Because
we assume that the mean processing time for a specific level is the same for all failures independent of the
results of restoration, this time is a reciprocal value of failure processing rate, not of successful recovery
rate. In other words
=

=


Using (18)–(21) allows us to calculate the availability of a system with known explicit rates of escalation
leading to the situation described in Section
2.5. Comparison with a Markov chain model
In this section we illustrate similarities and differences between the Markov chain model and SFM. The
Markov chain model for a system with several recovery levels is illustrated in Figure 3. Despite a superficial
similarity between Figure 2 (SFM) and Figure 3 (Markov chain model), they have different meanings
The circles in Figure 3 represent states of a system: one working state and several faulted states for every
recovery level. The arrows represent transitions between system states. In contrast, the circles in Figure
represent a set of failures. The arrows represent the relationships between these sets, i.e. how one set of
failures is divided into other sets (with corresponding probabilities). The two models use the same input
data and should lead to the same results but use different approaches to availability evaluation
The Markov chain model is a powerful mathematical approach and allows modelling many aspects of a
system’s behaviour, not just availability. Using the Markov chain model, the probabilities of system states
are evaluated. Knowing the probability of a normal (working) state, the system availability can be evaluated
However, calculations according to this model can be quite complicated (solving the ChapmanKolmogorov
equations). The use of special tools is often required for this analysis. SFM is designed for a narrow
specific purpose—availability evaluation of systems with several recovery procedures. System states are
Copyright q 2008 John Wiley & Sons, Ltd. Qual. Reliab. Engng. Int. 2008;
DOI: qre
454 S. A. VILKOMIR ET AL
Figure 3. Markov chain model
(a) b
Figure 4. System with two recovery procedures: (a) segregated failures model and (b) Markov chain model
not considered and an impact of different types of failures on system availability is considered instead. In
contrast to the Markov chain model, calculations are very simple and do not require the use of any tools
SFM is proposed as a supplement to Markov chain models. We believe that it is useful to have a simple
analytical method of availability evaluation. To illustrate these approaches, consider an application of both
of them in the following toy example
• A system has two different recovery procedures
• The probability that a failure is recovered by the first procedure is 23 .• The mean restoration time for the first recovery procedure is 30 times less than that for the second
recovery procedure
The representations of both models are shown in Figure 4, where
• is the system failure rate
• is the recovery rate for the second procedure
• Pi , i=0,1,2, are the probabilities of system states
Copyright q 2008 John Wiley & Sons, Ltd. Qual. Reliab. Engng. Int. 2008;
DOI: qre
COMPUTER SYSTEMS AVAILABILITY EVALUATION
The application of SFM for this example requires only mental calculations. Thus, the failure rates for
failures of each type from (10) are
=  =
The mean restoration times for failures of each type from (12) are



,


+

=


Finally, the system down time can be found using
Td=




+




=


The application of the Markov chain model is more complicated. We need to solve the following simul
taneous equations
= PP
P =
= P
P0+P1+P2 = 1
Transposing (26) for P1 and (27) for P2 and substituting P1 and P2 into (28) give
P0+

P0+

P0=1
and
P0=

The system down time during time period T can be found as
Td=(1−P0)T
Considering down time during T =1 year and using (30) , we finally have
Td=

In practice, usually typek and from (32) the approximate value of the down time is
Td=


which completely coincides with the result (24) of SFM
The following conclusions can be drawn from the example
• SFM provides the approximate values of the down time, which are very close to the accurate values
Thus, if per year and per hour, the accurate value of Td according to (32) is 219.91 min
per year. The approximate value of Td according to (24) or (33) is 220.00 min per year. The difference
is only 0.04% and is not significant, especially taking into account an approximation of the input data
Copyright q 2008 John Wiley & Sons, Ltd. Qual. Reliab. Engng. Int. 2008;
DOI: qre
456 S. A. VILKOMIR ET AL
• The complexity of calculations according to the Markov chain model increases when the number of
recovery procedures increases. At the same time, calculations using SFM remain simple. Thus, the
benefit of using SFM increases when more recovery procedures are used
• Both the Markov chain model and the SFM allow us to evaluate the system down time. However, SFM
additionally provides the evaluation of the down times separately for every recovery procedure. This
allows us to analyse availability more deeply and to find ways for availability improvement
3. CASE STUDY 1: RCC
3.1. Cluster failure model
Every system developer or manufacturer wants to increase the availability of his/her system. Consequently
methods of availability evaluation are important not only for ensuring a required level of availability but also
as an instrument for availability improvement. The big advantage of SFM is that the model can be easily
used for this purpose
For systems with several recovery procedures, there are several different ways of improving availability
As developer resources are restricted, it is important to choose the most effective ways. With that end of
view, a developer should be able to answer the following questions
• Which recovery procedure has the most negative effect on availability
• Which characteristics of a recovery procedure would be most beneficial to improve (e.g. coverage
factor or recovery time
SFM can be used to answer these questions. With SFM, availability is evaluated for each recover level
providing insight into the first question. Because the calculations are simple, the calculation can be repeated
with changes to each of the parameters to see how each one affects the recovery time
As an example of the application of the proposed approach, consider the RCC product2, which has been
analysed in7 using a Markov chain model
The goal of RCC is to achieve high levels of availability and reliability using commercial offtheshelf
computers in a cost-effective manner. A system using RCC is a collection of processors connected by
standard interconnects such as an Ethernet bus. In this paper we consider an example with one active and
one spare (standby) node, but generally RCC supports various architectures with several active and/or spare
nodes
RCC includes two hardware devices (PowerDog andWatchDog) and software components. The PowerDog
can turn electrical power on/off for a single processor. The WatchDog monitors the state of hardware
(processors) and forces a recovery if necessary. Software components of RCC monitor the state and manage
recovery procedures of individual software applications. RCC detects and recovers failures at several levels
A specific recovery strategy varies for different RCC applications. In this example based on7 we model the
following sequence of recovery procedures
• A switchover from a failed active node to a spare node
• An automatic processor restart
• An automatic processor restart after data reload from the disk
• A manual processor repair
A diagrammatic representation of the RCC failure model is shown in Figure
Model inputs are as follows (taking the notation and inputs values from
• is the processor failure rate, which represents all hardware and software failures for a single processor
• are the recovery rates, correspondingly, after switchover, processor restart, and processor
restart with data reload
• is the processor manual repair rate
Copyright q 2008 John Wiley & Sons, Ltd. Qual. Reliab. Engng. Int. 2008;
DOI: qre
COMPUTER SYSTEMS AVAILABILITY EVALUATION
Figure 5. Cluster failure model
• (1−ca) is the proportion of processor failures’ impact on all processors
• c1,c2,c3 are the coverage factors, i.e. proportions of processor recoveries after switchover, processor
restart, and restart with data reload
The values of input data are as follows
failures per year
recoveries per hour
recoveries per hour
recoveries per hour
repair per hour
ca
ccc
3.2. Availability evaluation of RCC system
Step 1: According to our approach, we consider the following four types of failures
• Type 1: failures recovered by switchover
• Type 2: failures recovered by processor restart
• Type 3: failures recovered by restart with reload
• Type 4: failures recovered by processor repair
Steps 2–3: Using (6)–(8) and (10) for failures of types 1–4 gives the following equations for failure rates

=
=
=
=
Copyright q 2008 John Wiley & Sons, Ltd. Qual. Reliab. Engng. Int. 2008;
DOI: qre
458 S. A. VILKOMIR ET AL
Step 4: Using (12) gives the following values of the restoration times
s hmin
typepminminmin
typegminminmin
typeRminminmin
Steps 5–6: The intermediate results and values Tdi and Td of the down time according to (15)–(16) are
presented in Table I
It is clear that system availability is strongly affected by the values of the coverage factors. This is
illustrated in Table I, where increasing the coverage factor from 0.75 to 0.9 reduces the system down time
from 92 to 46 minutes per year
It is also important to pay attention to another index, namely the mean restoration time, which is also
important for availability. It follows from Table I that the largest contribution into the system down time is
made by the down time after the manual processor repairs: 24.4min from total 46min (53%) for ci
and 56.4min from total 92min (61%) for ci =0.75. Reducing the coverage factor increases the influence of
the down time after manual repairs
The effect of the mean manual repair time on the system down time is shown in Figure 6. There is a linear
dependence between these quantities. Reducing the mean manual repair time significantly decreases the
Table I. Failure rates (per year) and expected down time (minutes per year
Type of failures Failure rate and down time ci =0.9 ci
1 7.13
Td1 14.3
2 0.71
Td2 5.0
3 0.07
Td3 2.6
4 0.09
Td4 24.4
System down time Td 46
Figure 6. Case study 1: Effect of the mean manual repair time on the system down time
Copyright q 2008 John Wiley & Sons, Ltd. Qual. Reliab. Engng. Int. 2008;
DOI: qre
COMPUTER SYSTEMS AVAILABILITY EVALUATION
system down time. Thus, for ci =0.9, if the repair time is reduced from 4 to 3 h, then the expected down time
after repairs becomes equal to 19.5min (decreasing 20%) and the total system down time becomes min
(decreasing 11%). If the repair time is reduced from 4 to 2 h, then the expected down time after repairs
becomes equal to 14.1min (decreasing 42%) and the total system down time becomes 36min decreasing

4. CASE STUDY 2: DESCRIBING RECOVERY POLICY BY RATES OF
ESCALATION WITHOUT THE USE OF COVERAGE FACTOR
4.1. A model
As an example of a model with rates of escalation we consider a hypothetical typical RCC application
which has been analysed in6 using a Markov chain model. The application uses two types of data. Volatile
data (the first type) are kept in the memory temporarily only during execution of the application. Persistent
data (the second type) are permanently written to the disk. Data of both types are recovered when a hardware
or software failure occurs. Below we reuse notation and inputs from6 in our model that gives opportunities
to compare different approaches with availability evaluation
The model has four recovery procedures
• Fault detection and recovery. A small number of hardware and software faults are detected by the
watchdog and recovery is fully automatic. The internal data are not saved and the application is restarted
at the initial internal state. The restart for this procedure is usually slow
• Volatile data recovery. Periodic checkpointing is used and the critical volatile data are saved. When a
hardware failure occurs, the system is reconfigured. At the next step, for both hardware and software
failures, the process automatically restarts at the most recent check-pointed internal state
• Persistent data recovery. Replicating the persistent data on a backup disk is used. It ensures data
consistency when the application is automatically recovered on the backup node
• Manual repair. For all hardware and software faults when the attempts of automatic recovering are not
successful, manual intervention is used. In addition, a small set of faults is detected for manual repair
before applying procedures of the automatic recovery
A diagrammatic representation of the model is shown in Figure
Model inputs are as follows
• is the total failure rate
• , i=1,2,3, is the level i to i+1 escalation rate
• , i=1,2,3, is the recovery rate at level i .• is the manual repair rate
• c is the fault detection coverage
• ci , i=1,2, is the level i to i+1 coverage
The values of input data are as follows
failures per year
exits per hour
exits per hour
exits per hour
recoveries per hour
recoveries per hour
recoveries per hour
repair per hour
ccc
Copyright q 2008 John Wiley & Sons, Ltd. Qual. Reliab. Engng. Int. 2008;
DOI: qre
460 S. A. VILKOMIR ET AL
Figure 7. Segregated failure model with rates of escalation
4.2. Availability evaluation
Step 1: The model has the following four types of failures
• Type 1: failures restored by the fault detection and recovery
• Type 2: failures restored by the volatile data recovery
• Type 3: failures restored by the persistent data recovery
• Type 4: failures restored by processor manual repair
Step 2: To turn from rates of escalation to coverage factors, let us calculate the conditional probabilities
prec,i , pnext,i , and plast,i . The application of (18)–(20) gives the following
prec,i =

, i=1,2,3
pnext,i = ci

, i=1,2
pnext,3 =


plast,i = (1−ci

, i=1,2
Now we can calculate ptypei using
ptype1
c


ptype2
cc


Copyright q 2008 John Wiley & Sons, Ltd. Qual. Reliab. Engng. Int. 2008;
DOI: qre
COMPUTER SYSTEMS AVAILABILITY EVALUATION
ptype3
ccc


ptype4 = c



+

+


+1−c
Step 3: Using (10) and the values of prec,i , pnext,i , and plast,i obtained at Step 2, calculate
Step 4: For the evaluation of the mean restoration time for failures of every type i , we first calculate
the mean service (i.e. successful or unsuccessful restoration) time for every level i using formulas
ii ), i=1,2,3, and Then we find using (12). The results of the calculation are
summarized in Table II
Steps 5–6: The intermediate results and values Tdi and Td of the down time according to (15)–(16) are
presented in Table III for and Table IV for
The comparison of these results with the results from6, where the same case study has been analysed
using the Markov chain model, shows that the values of the down time from both models are practically
the same. The difference is on average less than 0.5%, which can be explained by rounding off the decimal
However, the results from6 give only the values of the system down time but Tables III and IV, as opposed
to6, additionally provide the down time for every recovery procedure
Two conclusions can be directly drawn from Tables III and IV
• System down time is proportional to the value of the total failure rate, which is also clear from formulas
(10) and
• Increasing the value of the fault detection coverage ci significantly decreases down time
Table II. Mean restoration time minutes
Level/Type Mean restoration time
1 1.0
1.0
2 0.02
1.02
3 0.02
1.03
4 240
241.03
Table III. Failure rates (per year) and expected down time (minutes per year) for
ci =0.99 ci
Type of failures Failure rate and down time
1 4.95 9.9 14.85 4.50 9.00
Td1 5.0 9.9 14.9 4.5 9.0
2 2.45 4.9 7.35 2.03 4.05
Td2 2.5 5.0 7.5 2.1 4.1
3 2.36 4.72 7.08 1.77 3.55
Td3 2.4 4.9 7.3 1.8 3.7
4 0.24 0.48 0.72 1.7 3.40
Td4 57.9 115.7 173.6 410.2 820.5
System down time Td 68 135 203 419 837
Copyright q 2008 John Wiley & Sons, Ltd. Qual. Reliab. Engng. Int. 2008;
DOI: qre
462 S. A. VILKOMIR ET AL
The value of the fault detection coverage ci determines the number of failures that are escalated from
level i to the next level (not to the last level directly). Increasing ci from 0.9 to 0.99 decreases down
time several times (6.2 times from 419 to 68min for 7.2 times from 346 to min
for However, the value of the fault detection coverage depends on the nature of soft
ware failures and can hardly be changed by system designers. Even more, systems with a bad diag
nostic subsystem (which cannot determine failures immediately required to be manually repaired) have
Table IV. Failure rates (per year) and expected down time (minutes per year) for
ci =0.99 ci
Type of failures Failure rate and down time
1 6.60 13.20 19.80 6.00 12.00
Td1 4.4 8.8 13.3 4.0 8.0
2 2.18 4.36 6.53 1.80 3.60
Td2 1.5 2.9 4.4 1.2 2.4
3 1.05 2.10 3.15 0.79 1.58
Td3 0.7 1.5 2.2 0.6 1.1
4 0.17 0.34 0.52 1.41 2.82
Td4 41.6 83.3 124.9 339.9 679.7
System down time Td 48 97 145 346 691
Figure 8. Case study 2: Effect of the mean manual repair time on the system down time. (1) ci =0.9,
(2) ci =0.9, (3) ci =0.99, and (4) ci =0.99,
Copyright q 2008 John Wiley & Sons, Ltd. Qual. Reliab. Engng. Int. 2008;
DOI: qre
COMPUTER SYSTEMS AVAILABILITY EVALUATION
a greater value of fault detection coverage. However, if some failures are eventually (step by step) esca
lated through all levels to the last one, system down time will increase because of wasting time at every
level
There are at least two ways for the designers to improve the availability of a system
• Changing recovery strategy
• Improving any existing recovery procedure
Changing recovery strategy, as a creation of a new recovery procedure, can require significant efforts from
designers. However, in some cases it is possible to improve availability just by changing the order of
application of recovery procedures. SFM can be useful for studying such changes
Improving an existing recovery procedure can be achieved by reducing the mean recovery time for this
procedure, but it is important to evaluate how it affects the whole system. The dependence of the system
down time on the mean manual repair time is shown in Figure
High recovery time for a specific procedure influences system availability even when part of the failures
restored at this level is negligible. Thus, according to Tables III and IV for ci =0.99, on average only
of all failures are restored at level 4, i.e. by the manual repair. However, the contribution of this level to
system down time comes to 86% (85.1% for and 86.7% for The
reason is that, according to Table II, the mean restoration time for the manual repair increases two hundred
times as much as the mean restoration time for other procedures
Mean restoration time can be reduced by using better diagnostic equipment, speeding up a delivery of
spare parts, etc. Thus, if the mean restoration time for the manual repair is reduced at 20% (from 240 to
192min) then, according to (15) and (16), the system down time will be reduced on average at
As it follows from Figure 8, a variation in the system down time (when the mean manual repair time
varies) is greater for ci =0.9 comparing with ci =0.99. It illustrates the fact that reducing the repair time is
especially effective in case of low values of the coverage factor
5. CONCLUSIONS
In this paper, we consider availability assessment of a specific type of fault-tolerant computer systems
namely systems with several recovery procedures. Such systems are widely used in the telecommunications
area where a very high level of availability is required
Previous research in this area used powerful mathematical methods (mainly, the Markov chains approach
which are at the same time quite complicated and usually require the use of software tools. We have described
a method for availability evaluation based on a new model. This model gives a simple analytical way to
evaluate availability and can often be used as an alternative to Markov chain models. We analyse both
approaches and show that the SFM not only allows calculating down time of a whole system but also makes
it possible to use the intermediate results of the calculation to determine the impact of possible changes on
the future availability of the product
As case studies, various versions of the RCC architecture are considered. Different values of failure
restoration rates were considered and their influence on down time is analysed. The values of down time
for every type of failure as well as for the whole system are calculated. For these specific case studies, the
main factor that impacts system availability is the mean restoration time for manual repair. Thus, reducing
this time is an important practical task to improve availability. The case studies show that SFM provides a
simple and convenient practical approach to availability prediction
The approach also allows finding weakest links of a system reliability architecture and recovery strategy
As a result, one can determine where techniques of availability improvement (increasing coverage factors
decreasing recovery time, etc.) can be applied to a specific system in the most effective way. In future work
we will consider an application of this model for the analysis of different recovery strategies and choosing
the optimal one for any given system
Copyright q 2008 John Wiley & Sons, Ltd. Qual. Reliab. Engng. Int. 2008;
DOI: qre
464 S. A. VILKOMIR ET AL
REFERENCES
1. Huang Y, Kintala C. Software fault-tolerance in the application layer. Software Fault Tolerance, ch. 10, Lyu MR ed
Wiley: New York, 1995;
2. Hughes-Fenchel G. A flexible clustered approach to high availability. Digest of Papers Twenty-Seventh Annual
International Symposium on Fault-Tolerant Computing, Seattle, WA, June 1997;
3. Verhofstad J. Recovery techniques for database systems. ACM Computing Surveys 1978;
4. Baker M, Sullivan M. The recovery box: Using fast recovery to provide high availability in the UNIX environment
Proceedings of the Summer 1992 USENIX Conference, San Antonio, TX, June 1992;
5. Ibe O, Howe R, Trivedi KS. Approximate availability analysis of VAXCluster systems. IEEE Transactions on Reliability
1989;
6. Lyu MR, Mendiratta VB. Software fault tolerance in a clustered architecture: Techniques and reliability modeling
Proceedings of the IEEE Aerospace Conference, Snowmass, CO, 1999;
7. Mendiratta VB. Reliability analysis of clustered computing systems. Proceedings of the Ninth International Symposium
on Software Reliability Engineering, Paderborn, Germany, November 1998;
8. Sun H, Han JJ, Levendel H. Availability requirement for a fault-management server in high-availability communication
systems. IEEE Transactions on Reliability 2003;
9. Hoeflin DA, Mendiratta VB. Elementary model for predicting switching system outage durations. Proceedings of the
XV International Switching Symposium, Berlin, Germany, April
10. Sun H, Han JJ, Levendel H. A generic availability model for clustered computing systems. Proceedings of the Pacific
Rim International Symposium on Dependable Computing, Seoul, Korea, December 2001;
11. Vilkomir SA, Parnas DL, Mendiratta VB, Murphy E. Availability evaluation of hardware/software systems with
several recovery procedures. Proceedings of the 29th IEEE Annual International Computer Software and Applications
Conference, Edinburgh, Scotland, July 2005;
12. Vilkomir SA, Parnas DL, Mendiratta VB, Murphy E. Segregated failures model for availability evaluation of fault
tolerant systems. Proceedings of the 29th Australasian Computer Science Conference, CRPIT, vol. 48, ACS, Hobart
Tasmania, Australia, January 2006;
13. Randell B. System structure for software fault tolerance. IEEE Transactions on Software Engineering 1975; SE

14. Randell B. The evolution of the recovery block concept. Software Fault Tolerance, Lyu M (ed.). Wiley: New York
1995;
15. Arlat J, Kanoun K, Laprie J-C. Dependability modeling and evaluation of software fault-tolerant systems. IEEE
Transactions on Computers 1990;
16. Goseva-Popstojanova K, Grnarov A. Dependability modeling and evaluation of recovery block systems. Proceedings
of the 4th IEEE International Symposium on Software Reliability Engineering, Denver, CO, November 1993;
17. Pucci G. A new approach to the modeling of recovery block structures. IEEE Transactions on Software Engineering
1992;
Authors’ Biographies
Sergiy A. Vilkomir is a Research Associate Professor in the Software Quality Research Laboratory at
the University of Tennessee, Knoxville, U.S.A. He was formerly a Senior Researcher in the Software
Quality Research Laboratory at the University of Limerick, Ireland and a Research Fellow at the University
of Wollongong, Australia and at London South Bank University, U.K. Dr Vilkomir obtained his PhD in
Computer Systems from Kharkov Polytechnic University, Ukraine and MSc in Mathematics from Kharkov
State University, Ukraine. He is the author or coauthor of 60 research papers. His research interests include
software testing, system and software reliability, requirement engineering, and formal methods
David L. Parnas was the director of the Software Quality Research Laboratory, an SFI fellow, and a
professor of software engineering at the University of Limerick, Limerick, Republic of Ireland. He is now
retired. He received the PhD degree in electrical engineering from Carnegie Mellon University and honorary
doctorates from the ETH in Zurich, Switzerland, and the Catholic University of Louvain, Belgium. The
Copyright q 2008 John Wiley & Sons, Ltd. Qual. Reliab. Engng. Int. 2008;
DOI: qre
COMPUTER SYSTEMS AVAILABILITY EVALUATION
author of more than 250 papers and reports, Dr Parnas is interested in most aspects of computer system
design and documentation
Veena B. Mendiratta is a consulting member of technical staff in the Chief Technology Office at Alcatel
Lucent in Naperville, Illinois, U.S.A. She holds a BTech in engineering from the Indian Institute of Tech
nology in New Delhi and a PhD in operations research from Northwestern University in Evanston, Illinois
Dr Mendiratta’s research interests include architecture, reliability and survivability, fault tolerant computing
and software reliability engineering
Eamonn Murphy is the Boart Longyear Professor of Quality and Applied Statistics at the University of
Limerick. He holds a BSc, MSc and PhD from University College Dublin. He has consulted extensively in
quality management and quality systems throughout Europe. He is the director of the Enterprise Research
Centre. His research interests focus on adapting and deploying statistical tools and techniques as a means
of significantly improving quality and reliability
Copyright q 2008 John Wiley & Sons, Ltd. Qual. Reliab. Engng. Int. 2008;
DOI: qre

