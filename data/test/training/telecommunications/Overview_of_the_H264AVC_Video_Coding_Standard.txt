560 IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY, VOL. 13, NO. 7, JULY 2003Overview of the H.264AVC Video Coding StandardThomas Wiegand, Gary J. Sullivan, Senior Member, IEEE, Gisle Bjntegaard, and Ajay Luthra, Senior Member, IEEEAbstractH.264AVC is newest video coding standard of theITUT Video Coding Experts Group and the ISOIEC MovingPicture Experts Group. The main goals of the H.264AVC standardization effort have been enhanced compression performanceand provision of a networkfriendly video representationaddressing conversational video telephony and nonconversational storage, broadcast, or streaming applications.H.264AVC has achieved a significant improvement in ratedistortion efficiency relative to existing standards. This article providesan overview of the technical features of H.264AVC, describesprofiles and applications for the standard, and outlines the historyof the standardization process.Index TermsAVC, H.263, H.264, JVT, MPEG2, MPEG4,standards, video.I. INTRODUCTIONH .264AVC is the newest international video coding standard 1. By the time of this publication, it is expected tohave been approved by ITUT as Recommendation H.264 andby ISOIEC as International Standard 14 49610 MPEG4 part10 Advanced Video Coding AVC.The MPEG2 video coding standard also known as ITUTH.262 2, which was developed about ten years ago primarilyas an extension of prior MPEG1 video capability with supportof interlaced video coding, was an enabling technology for digital television systems worldwide. It is widely used for the transmission of standard definition SD and high definition HDTV signals over satellite, cable, and terrestrial emission and thestorage of highquality SD video signals onto DVDs.However, an increasing number of services and growingpopularity of high definition TV are creating greater needsfor higher coding efficiency. Moreover, other transmissionmedia such as Cable Modem, xDSL, or UMTS offer muchlower data rates than broadcast channels, and enhanced codingefficiency can enable the transmission of more video channelsor higher quality video representations within existing digitaltransmission capacities.Video coding for telecommunication applications hasevolved through the development of the ITUT H.261, H.262MPEG2, and H.263 video coding standards and laterenhancements of H.263 known as and ,Manuscript received April 15, 2002 revised May 10, 2003.T. Wiegand is with the FraunhoferInstitute for Telecommunications,HeinrichHertzInstitute, Einsteinufer 37, 10587 Berlin, Germany emailwiegandhhi.de.G. J. Sullivan is with the Microsoft Corporation, Redmond, WA 98052 USAemail garysullmicrosoft.com.G. Bjntegaard is with the Tandberg, N1324 Lysaker, Norway emailgbjtandberg.noA. Luthra is with the Broadband Communications Sector, Motorola, Inc., SanDiego, CA 92121 USA. email aluthramotorola.comDigital Object Identifier 10.1109TCSVT.2003.815165Fig. 1. Scope of video coding standardization.and has diversified from ISDN and T1E1 service to embracePSTN, mobile wireless networks, and LANInternet networkdelivery. Throughout this evolution, continued efforts havebeen made to maximize coding efficiency while dealing withthe diversification of network types and their characteristicformatting and losserror robustness requirements.Recently the MPEG4 Visual MPEG4 part 2 standard 5has also begun to emerge in use in some application domains ofthe prior coding standards. It has provided video shape codingcapability, and has similarly worked toward broadening therange of environments for digital video use.In early 1998, the Video Coding Experts Group VCEGITUT SG16 Q.6 issued a call for proposals on a project calledH.26L, with the target to double the coding efficiency whichmeans halving the bit rate necessary for a given level of fidelityin comparison to any other existing video coding standards fora broad variety of applications. The first draft design for thatnew standard was adopted in October of 1999. In December of2001, VCEG and the Moving Picture Experts Group MPEGISOIEC JTC 1SC 29WG 11 formed a Joint Video TeamJVT, with the charter to finalize the draft new video codingstandard for formal approval submission as H.264AVC 1 inMarch 2003.The scope of the standardization is illustrated in Fig. 1, whichshows the typical video codingdecoding chain excluding thetransport or storage of the video signal. As has been the casefor all ITUT and ISOIEC video coding standards, only thecentral decoder is standardized, by imposing restrictions on thebitstream and syntax, and defining the decoding process of thesyntax elements such that every decoder conforming to the standard will produce similar output when given an encoded bitstream that conforms to the constraints of the standard. This limitation of the scope of the standard permits maximal freedomto optimize implementations in a manner appropriate to specific applications balancing compression quality, implementation cost, time to market, etc.. However, it provides no guarantees of endtoend reproduction quality, as it allows even crudeencoding techniques to be considered conforming.This paper is organized as follows. Section II provides a highlevel overview of H.264AVC applications and highlights somekey technical features of the design that enable improved operation for this broad variety of applications. Section III explainsthe network abstraction layer NAL and the overall structure105182150317.00  2003 IEEEWIEGAND et al. OVERVIEW OF THE H.264AVC VIDEO CODING STANDARD 561Fig. 2. Structure of H.264AVC video encoder.of H.264AVC coded video data. The video coding layer VCLis described in Section IV. Section V explains the profiles supported by H.264AVC and some potential application areas ofthe standard.II. APPLICATIONS AND DESIGN FEATURE HIGHLIGHTSThe new standard is designed for technical solutions including at least the following application areas Broadcast over cable, satellite, cable modem, DSL, terrestrial, etc. Interactive or serial storage on optical and magnetic devices, DVD, etc. Conversational services over ISDN, Ethernet, LAN, DSL,wireless and mobile networks, modems, etc. or mixturesof these. Videoondemand or multimedia streaming services overISDN, cable modem, DSL, LAN, wireless networks, etc. Multimedia messaging services MMS over ISDN, DSL,ethernet, LAN, wireless and mobile networks, etc.Moreover, new applications may be deployed over existing andfuture networks. This raises the question about how to handlethis variety of applications and networks.To address this need for flexibility and customizability, theH.264AVC design covers a VCL, which is designed to efficiently represent the video content, and a NAL, which formatsthe VCL representation of the video and provides header information in a manner appropriate for conveyance by a variety oftransport layers or storage media see Fig. 2.Relative to prior video coding methods, as exemplified byMPEG2 video, some highlighted features of the design that enable enhanced coding efficiency include the following enhancements of the ability to predict the values of the content of a picture to be encoded. Variable blocksize motion compensation with smallblock sizes This standard supports more flexibility in theselection of motion compensation block sizes and shapesthan any previous standard, with a minimum luma motioncompensation block size as small as 4 4. Quartersampleaccurate motion compensation Mostprior standards enable halfsample motion vector accuracyat most. The new design improves up on this by addingquartersample motion vector accuracy, as first found inan advanced profile of the MPEG4 Visual part 2 standard, but further reduces the complexity of the interpolation processing compared to the prior design. Motion vectors over picture boundaries While motionvectors in MPEG2 and its predecessors were required topoint only to areas within the previouslydecoded reference picture, the picture boundary extrapolation techniquefirst found as an optional feature in H.263 is included inH.264AVC. Multiple reference picture motion compensation Predictively coded pictures called P pictures in MPEG2and its predecessors used only one previous picture to predict the values in an incoming picture. The new design extends upon the enhanced reference picture selection technique found in to enable efficient coding by allowing an encoder to select, for motion compensation purposes, among a larger number of pictures that have beendecoded and stored in the decoder. The same extensionof referencing capability is also applied to motioncompensated biprediction, which is restricted in MPEG2 tousing two specific pictures only one of these being theprevious intra I or P picture in display order and the otherbeing the next I or P picture in display order. Decoupling of referencing order from display orderIn prior standards, there was a strict dependency betweenthe ordering of pictures for motion compensation referencing purposes and the ordering of pictures for displaypurposes. In H.264AVC, these restrictions are largely removed, allowing the encoder to choose the ordering ofpictures for referencing and display purposes with a highdegree of flexibility constrained only by a total memorycapacity bound imposed to ensure decoding ability. Removal of the restriction also enables removing the extradelay previously associated with bipredictive coding. Decoupling of picture representation methods frompicture referencing capability In prior standards,pictures encoded using some encoding methods namelybipredictivelyencoded pictures could not be used asreferences for prediction of other pictures in the videosequence. By removing this restriction, the new standardprovides the encoder more flexibility and, in many cases,an ability to use a picture for referencing that is a closerapproximation to the picture being encoded. Weighted prediction A new innovation in H.264AVCallows the motioncompensated prediction signal to beweighted and offset by amounts specified by the encoder.This can dramatically improve coding efficiency forscenes containing fades, and can be used flexibly forother purposes as well. Improved skipped and direct motion inference Inprior standards, a skipped area of a predictivelycodedpicture could not motion in the scene content. This hada detrimental effect when coding video containing globalmotion, so the new H.264AVC design instead infers motion in skipped areas. For bipredictively coded areascalled B slices, H.264AVC also includes an enhancedmotion inference method known as direct motion compensation, which improves further on prior direct prediction designs found in and MPEG4 Visual.562 IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY, VOL. 13, NO. 7, JULY 2003 Directional spatial prediction for intra coding A newtechnique of extrapolating the edges of the previouslydecoded parts of the current picture is applied in regions ofpictures that are coded as intra i.e., coded without reference to the content of some other picture. This improves the quality of the prediction signal, and also allowsprediction from neighboring areas that were not codedusing intra coding something not enabled when usingthe transformdomain prediction method found inand MPEG4 Visual. Intheloop deblocking filtering Blockbased videocoding produces artifacts known as blocking artifacts.These can originate from both the prediction and residualdifference coding stages of the decoding process. Application of an adaptive deblocking filter is a wellknownmethod of improving the resulting video quality, and whendesigned well, this can improve both objective and subjective video quality. Building further on a concept froman optional feature of , the deblocking filter inthe H.264AVC design is brought within the motioncompensated prediction loop, so that this improvement inquality can be used in interpicture prediction to improvethe ability to predict other pictures as well.In addition to improved prediction methods, other parts ofthe design were also enhanced for improved coding efficiency,including the following. Small blocksize transform All major prior videocoding standards used a transform block size of 8 8,while the new H.264AVC design is based primarily ona 4 4 transform. This allows the encoder to representsignals in a more locallyadaptive fashion, which reducesartifacts known colloquially as ringing. The smallerblock size is also justified partly by the advances in theability to better predict the content of the video usingthe techniques noted above, and by the need to providetransform regions with boundaries that correspond tothose of the smallest prediction regions. Hierarchical block transform While in most cases,using the small 4 4 transform block size is perceptuallybeneficial, there are some signals that contain sufficientcorrelation to call for some method of using a representation with longer basis functions. The H.264AVCstandard enables this in two ways 1 by using a hierarchical transform to extend the effective block size usefor lowfrequency chroma information to an 8 8 arrayand 2 by allowing the encoder to select a special codingtype for intra coding, enabling extension of the length ofthe luma transform for lowfrequency information to a16 16 block size in a manner very similar to that appliedto the chroma. Short wordlength transform All prior standard designs have effectively required encoders and decoders touse more complex processing for transform computation.While previous designs have generally required 32bitprocessing, the H.264AVC design requires only 16bitarithmetic. Exactmatch inverse transform In previous videocoding standards, the transform used for representingthe video was generally specified only within an errortolerance bound, due to the impracticality of obtaining anexact match to the ideal specified inverse transform. Asa result, each decoder design would produce slightly different decoded video, causing a drift between encoderand decoder representation of the video and reducingeffective video quality. Building on a path laid out as anoptional feature in the effort, H.264AVC isthe first standard to achieve exact equality of decodedvideo content from all decoders. Arithmetic entropy coding An advanced entropycoding method known as arithmetic coding is includedin H.264AVC. While arithmetic coding was previouslyfound as an optional feature of H.263, a more effectiveuse of this technique is found in H.264AVC to create avery powerful entropy coding method known as CABACcontextadaptive binary arithmetic coding. Contextadaptive entropy coding The two entropycoding methods applied in H.264AVC, termed CAVLCcontextadaptive variablelength coding and CABAC,both use contextbased adaptivity to improve performancerelative to prior standard designs.Robustness to data errorslosses and flexibility for operationover a variety of network environments is enabled by a numberof design aspects new to the H.264AVC standard, including thefollowing highlighted features. Parameter set structure The parameter set design provides for robust and efficient conveyance header information. As the loss of a few key bits of information such assequence header or picture header information could havea severe negative impact on the decoding process whenusing prior standards, this key information was separatedfor handling in a more flexible and specialized manner inthe H.264AVC design. NAL unit syntax structure Each syntax structure inH.264AVC is placed into a logical data packet called aNAL unit. Rather than forcing a specific bitstream interface to the system as in prior video coding standards, theNAL unit syntax structure allows greater customizationof the method of carrying the video content in a mannerappropriate for each specific network. Flexible slice size Unlike the rigid slice structure found inMPEG2 which reduces coding efficiency by increasingthe quantity of header data and decreasing the effectiveness of prediction, slice sizes in H.264AVC are highlyflexible, as was the case earlier in MPEG1. Flexible macroblock ordering FMO A new ability topartition the picture into regions called slice groups hasbeen developed, with each slice becoming an independentlydecodable subset of a slice group. When used effectively, flexible macroblock ordering can significantlyenhance robustness to data losses by managing the spatialrelationship between the regions that are coded in eachslice. FMO can also be used for a variety of other purposes as well. Arbitrary slice ordering ASO Since each slice of acoded picture can be approximately decoded independently of the other slices of the picture, the H.264AVCWIEGAND et al. OVERVIEW OF THE H.264AVC VIDEO CODING STANDARD 563design enables sending and receiving the slices of thepicture in any order relative to each other. This capability,first found in an optional part of , can improveendtoend delay in realtime applications, particularlywhen used on networks having outoforder deliverybehavior e.g., internet protocol networks. Redundant pictures In order to enhance robustness todata loss, the H.264AVC design contains a new abilityto allow an encoder to send redundant representations ofregions of pictures, enabling a typically somewhat degraded representation of regions of pictures for which theprimary representation has been lost during data transmission. Data Partitioning Since some coded information for representation of each region e.g., motion vectors and otherprediction information is more important or more valuable than other information for purposes of representingthe video content, H.264AVC allows the syntax of eachslice to be separated into up to three different partitions fortransmission, depending on a categorization of syntax elements. This part of the design builds further on a path takenin MPEG4 Visual and in an optional part of .Here, the design is simplified by having a single syntaxwith partitioning of that same syntax controlled by a specified categorization of syntax elements. SPSI synchronizationswitching pictures TheH.264AVC design includes a new feature consistingof picture types that allow exact synchronization of thedecoding process of some decoders with an ongoing videostream produced by other decoders without penalizingall decoders with the loss of efficiency resulting fromsending an I picture. This can enable switching a decoderbetween representations of the video content that useddifferent data rates, recovery from data losses or errors,as well as enabling trick modes such as fastforward,fastreverse, etc.In Sections III and IV, a more detailed description of the keyfeatures is given.III. NALThe NAL is designed in order to provide network friendliness to enable simple and effective customization of the use ofthe VCL for a broad variety of systems.The NAL facilitates the ability to map H.264AVC VCL datato transport layers such as RTPIP for any kind of realtime wireline and wirelessInternet services conversational and streaming File formats, e.g., ISO MP4 for storage and MMS H.32X for wireline and wireless conversational services MPEG2 systems for broadcasting services, etc.The full degree of customization of the video content to fit theneeds of each particular application is outside the scope of theH.264AVC standardization effort, but the design of the NALanticipates a variety of such mappings. Some key concepts ofthe NAL are NAL units, byte stream, and packet format uses ofNAL units, parameter sets, and access units. A short description of these concepts is given below whereas a more detaileddescription including error resilience aspects is provided in 6and 7.A. NAL UnitsThe coded video data is organized into NAL units, each ofwhich is effectively a packet that contains an integer numberof bytes. The first byte of each NAL unit is a header byte thatcontains an indication of the type of data in the NAL unit, andthe remaining bytes contain payload data of the type indicatedby the header.The payload data in the NAL unit is interleaved as necessarywith emulation prevention bytes, which are bytes inserted witha specific value to prevent a particular pattern of data called astart code prefix from being accidentally generated inside thepayload.The NAL unit structure definition specifies a generic formatfor use in both packetoriented and bitstreamoriented transportsystems, and a series of NAL units generated by an encoder isreferred to as a NAL unit stream.B. NAL Units in ByteStream Format UseSome systems e.g., H.320 and MPEG2H.222.0 systemsrequire delivery of the entire or partial NAL unit stream as an ordered stream of bytes or bits within which the locations of NALunit boundaries need to be identifiable from patterns within thecoded data itself.For use in such systems, the H.264AVC specification definesa byte stream format. In the byte stream format, each NAL unitis prefixed by a specific pattern of three bytes called a start codeprefix. The boundaries of the NAL unit can then be identified bysearching the coded data for the unique start code prefix pattern.The use of emulation prevention bytes guarantees that start codeprefixes are unique identifiers of the start of a new NAL unit.A small amount of additional data one byte per video picture is also added to allow decoders that operate in systems thatprovide streams of bits without alignment to byte boundaries torecover the necessary alignment from the data in the stream.Additional data can also be inserted in the byte stream formatthat allows expansion of the amount of data to be sent and canaid in achieving more rapid byte alignment recovery, if desired.C. NAL Units in PacketTransport System UseIn other systems e.g., internet protocolRTP systems, thecoded data is carried in packets that are framed by the systemtransport protocol, and identification of the boundaries of NALunits within the packets can be established without use of startcode prefix patterns. In such systems, the inclusion of start codeprefixes in the data would be a waste of data carrying capacity,so instead the NAL units can be carried in data packets withoutstart code prefixes.D. VCL and NonVCL NAL UnitsNAL units are classified into VCL and nonVCL NAL units.The VCL NAL units contain the data that represents the valuesof the samples in the video pictures, and the nonVCL NALunits contain any associated additional information such as parameter sets important header data that can apply to a large564 IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY, VOL. 13, NO. 7, JULY 2003Fig. 3. Parameter set use with reliable outofband parameter set exchange.number of VCL NAL units and supplemental enhancement information timing information and other supplemental data thatmay enhance usability of the decoded video signal but are notnecessary for decoding the values of the samples in the videopictures.E. Parameter SetsA parameter set is supposed to contain information that isexpected to rarely change and offers the decoding of a largenumber of VCL NAL units. There are two types of parametersets sequence parameter sets, which apply to a series of consecutive coded video pictures called a coded video sequence picture parameter sets, which apply to the decoding of oneor more individual pictures within a coded video sequence.The sequence and picture parameterset mechanism decouplesthe transmission of infrequently changing information from thetransmission of coded representations of the values of the samples in the video pictures. Each VCL NAL unit contains an identifier that refers to the content of the relevant picture parameterset and each picture parameter set contains an identifier thatrefers to the content of the relevant sequence parameter set. Inthis manner, a small amount of data the identifier can be usedto refer to a larger amount of information the parameter setwithout repeating that information within each VCL NAL unit.Sequence and picture parameter sets can be sent well aheadof the VCL NAL units that they apply to, and can be repeated toprovide robustness against data loss. In some applications, parameter sets may be sent within the channel that carries the VCLNAL units termed inband transmission. In other applications see Fig. 3, it can be advantageous to convey the parameter sets outofband using a more reliable transport mechanism than the video channel itself.F. Access UnitsA set of NAL units in a specified form is referred to as anaccess unit. The decoding of each access unit results in one decoded picture. The format of an access unit is shown in Fig. 4.Each access unit contains a set of VCL NAL units that together compose a primary coded picture. It may also be prefixedwith an access unit delimiter to aid in locating the start of theaccess unit. Some supplemental enhancement information containing data such as picture timing information may also precedethe primary coded picture.Fig. 4. Structure of an access unit.The primary coded picture consists of a set of VCL NAL unitsconsisting of slices or slice data partitions that represent thesamples of the video picture.Following the primary coded picture may be some additionalVCL NAL units that contain redundant representations of areasof the same video picture. These are referred to as redundantcoded pictures, and are available for use by a decoder in recovering from loss or corruption of the data in the primary codedpictures. Decoders are not required to decode redundant codedpictures if they are present.Finally, if the coded picture is the last picture of a codedvideo sequence a sequence of pictures that is independentlydecodable and uses only one sequence parameter set, an endof sequence NAL unit may be present to indicate the end of thesequence and if the coded picture is the last coded picture inthe entire NAL unit stream, an end of stream NAL unit may bepresent to indicate that the stream is ending.G. Coded Video SequencesA coded video sequence consists of a series of access unitsthat are sequential in the NAL unit stream and use only one sequence parameter set. Each coded video sequence can be decoded independently of any other coded video sequence, giventhe necessary parameter set information, which may be conveyed inband or outofband. At the beginning of a codedvideo sequence is an instantaneous decoding refresh IDR access unit. An IDR access unit contains an intra picturea codedWIEGAND et al. OVERVIEW OF THE H.264AVC VIDEO CODING STANDARD 565Fig. 5. Progressive and interlaced frames and fields.picture that can be decoded without decoding any previous pictures in the NAL unit stream, and the presence of an IDR accessunit indicates that no subsequent picture in the stream will require reference to pictures prior to the intra picture it containsin order to be decoded.A NAL unit stream may contain one or more coded videosequences.IV. VCLAs in all prior ITUT and ISOIEC JTC1 video standardssince H.261 3, the VCL design follows the socalled blockbased hybrid video coding approach as depicted in Fig. 8, inwhich each coded picture is represented in blockshaped units ofassociated luma and chroma samples called macroblocks. Thebasic sourcecoding algorithm is a hybrid of interpicture prediction to exploit temporal statistical dependencies and transform coding of the prediction residual to exploit spatial statistical dependencies. There is no single coding element in theVCL that provides the majority of the significant improvementin compression efficiency in relation to prior video coding standards. It is rather a plurality of smaller improvements that addup to the significant gain.A. Pictures, Frames, and FieldsA coded video sequence in H.264AVC consists of a sequenceof coded pictures. A coded picture in 1 can represent either anentire frame or a single field, as was also the case for MPEG2video.Generally, a frame of video can be considered to contain twointerleaved fields, a top and a bottom field. The top field containsevennumbered rows 0, 2,,H21 with H being the number ofrows of the frame. The bottom field contains the oddnumberedrows starting with the second line of the frame. If the two fieldsof a frame were captured at different time instants, the frame isreferred to as an interlaced frame, and otherwise it is referred toas a progressive frame see Fig. 5. The coding representation inH.264AVC is primarily agnostic with respect to this video characteristic, i.e., the underlying interlaced or progressive timing ofthe original captured pictures. Instead, its coding specifies a representation based primarily on geometric concepts rather thanbeing based on timing.B. YCbCr Color Space and 420 SamplingThe human visual system seems to perceive scene content interms of brightness and color information separately, and withgreater sensitivity to the details of brightness than color. Videotransmission systems can be designed to take advantage of this.This is true of conventional analog TV systems as well as digital ones. In H.264AVC as in prior standards, this is done byusing a YCbCr color space together with reducing the samplingresolution of the Cb and Cr chroma information.The video color space used by H.264AVC separates a colorrepresentation into three components called Y, Cb, and Cr. Component Y is called luma, and represents brightness. The twochroma components Cb and Cr represent the extent to whichthe color deviates from gray toward blue and red, respectively.The terms luma and chroma are used in this paper and in thestandard rather than the terms luminance and chrominance, inorder to avoid the implication of the use of linear light transfercharacteristics that is often associated with the terms luminanceand chrominance.Because the human visual system is more sensitive to lumathan chroma, H.264AVC uses a sampling structure in whichthe chroma component has one fourth of the number of samplesthan the luma component half the number of samples in boththe horizontal and vertical dimensions. This is called 420 sampling with 8 bits of precision per sample. The sampling structureused is the same as in MPEG2 Mainprofile video. Proposalsfor extension of the standard to also support higherresolutionchroma and a larger number of bits per sample are currentlybeing considered.C. Division of the Picture Into MacroblocksA picture is partitioned into fixedsize macroblocks that eachcover a rectangular picture area of 16 16 samples of the lumacomponent and 8 8 samples of each of the two chroma components. This partitioning into macroblocks has been adopted intoall previous ITUT and ISOIEC JTC1 video coding standardssince H.261 3. Macroblocks are the basic building blocks ofthe standard for which the decoding process is specified. Thebasic coding algorithm for a macroblock is described after weexplain how macroblocks are grouped into slices.D. Slices and Slice GroupsSlices are a sequence of macroblocks which are processedin the order of a raster scan when not using FMO which is described in the next paragraph. A picture maybe split into one orseveral slices as shown in Fig. 6. A picture is therefore a collection of one or more slices in H.264AVC. Slices are selfcontained in the sense that given the active sequence and pictureparameter sets, their syntax elements can be parsed from thebitstream and the values of the samples in the area of the picturethat the slice represents can be correctly decoded without use ofdata from other slices provided that utilized reference picturesare identical at encoder and decoder. Some information fromother slices maybe needed to apply the deblocking filter acrossslice boundaries.566 IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY, VOL. 13, NO. 7, JULY 2003Fig. 6. Subdivision of a picture into slices when not using FMO.Fig. 7. Subdivision of a QCIF frame into slices when utilizing FMO.FMO modifies the way how pictures are partitioned intoslices and macroblocks by utilizing the concept of slice groups.Each slice group is a set of macroblocks defined by a macroblock to slice group map, which is specified by the contentof the picture parameter set and some information from sliceheaders. The macroblock to slice group map consists of a slicegroup identification number for each macroblock in the picture,specifying which slice group the associated macroblock belongs to. Each slice group can be partitioned into one or moreslices, such that a slice is a sequence of macroblocks withinthe same slice group that is processed in the order of a rasterscan within the set of macroblocks of a particular slice group.The case when FMO is not in use can be viewed as the simplespecial case of FMO in which the whole picture consists of asingle slice group.Using FMO, a picture can be split into many macroblockscanning patterns such as interleaved slices, a dispersed macroblock allocation, one or more foreground slice groups anda leftover slice group, or a checkerboard type of mapping.The latter two are illustrated in Fig. 7. The lefthand sidemacroblock to slice group mapping has been demonstratedfor use in regionofinterest type of coding applications. Therighthand side macroblock to slice group mapping has beendemonstrated useful for concealment in video conferencingapplications where slice group 0 and slice group 1 aretransmitted in separate packets and one of them is lost. Formore details on the use of FMO, see 14.Regardless of whether FMO is in use or not, each slice canbe coded using different coding types as follows. I slice A slice in which all macroblocks of the slice arecoded using intra prediction. P slice In addition to the coding types of the I slice, somemacroblocks of the P slice can also be coded using interprediction with at most one motioncompensated prediction signal per prediction block. B slice In addition to the coding types available in a Pslice, some macroblocks of the B slice can also be codedusing inter prediction with two motioncompensated prediction signals per prediction block.The above three coding types are very similar to those in previous standards with the exception of the use of reference pictures as described below. The following two coding types forslices are new. SP slice A socalled switching P slice that is coded suchthat efficient switching between different precoded pictures becomes possible. SI slice A socalled switching I slice that allows an exactmatch of a macroblock in an SP slice for random accessand error recovery purposes.For details on the novel concept of SP and SI slices, the reader isreferred to 5, while the other slice types are further describedbelow.E. Encoding and Decoding Process for MacroblocksAll luma and chroma samples of a macroblock are eitherspatially or temporally predicted, and the resulting predictionresidual is encoded using transform coding. For transformcoding purposes, each color component of the predictionresidual signal is subdivided into smaller 4 4 blocks. Eachblock is transformed using an integer transform, and thetransform coefficients are quantized and encoded using entropycoding methods.Fig. 8 shows a block diagram of the VCL for a macroblock.The input video signal is split into macroblocks, the associationof macroblocks to slice groups and slices is selected, and theneach macroblock of each slice is processed as shown. An efficient parallel processing of macroblocks is possible when thereare various slices in the picture.F. Adaptive FrameField Coding OperationIn interlaced frames with regions of moving objects or cameramotion, two adjacent rows tend to show a reduced degree ofstatistical dependency when compared to progressive frames in.In this case, it may be more efficient to compress each fieldseparately. To provide high coding efficiency, the H.264AVCdesign allows encoders to make any of the following decisionswhen coding a frame.1 To combine the two fields together and to code them asone single coded frame frame mode.2 To not combine the two fields and to code them as separate coded fields field mode.3 To combine the two fields together and compress them asa single frame, but when coding the frame to split the pairsof two vertically adjacent macroblocks into either pairs oftwo field or frame macroblocks before coding them.The choice between the three options can be made adaptivelyfor each frame in a sequence. The choice between the first twooptions is referred to as pictureadaptive framefield PAFFcoding. When a frame is coded as two fields, each field is partitioned into macroblocks and is coded in a manner very similarto a frame, with the following main exceptions motion compensation utilizes reference fields rather thanreference frames the zigzag scan of transform coefficients is differentWIEGAND et al. OVERVIEW OF THE H.264AVC VIDEO CODING STANDARD 567Fig. 8. Basic coding structure for H.264AVC for a macroblock.Fig. 9. Conversion of a frame macroblock pair into a field macroblock pair. the strong deblocking strength is not used for filtering horizontal edges of macroblocks in fields, because the fieldrows are spatially twice as far apart as frame rows and thelength of the filter thus covers a larger spatial area.During the development of the H.264AVC standard, PAFFcoding was reported to reduce bit rates in the range of 16 to20 over frameonly coding mode for ITUR 601 resolutionsequences like Canoa, Rugby, etc.If a frame consists of mixed regions where some regions aremoving and others are not, it is typically more efficient to codethe nonmoving regions in frame mode and the moving regionsin the field mode. Therefore, the framefield encoding decisioncan also be made independently for each vertical pair of macroblocks a 16 32 luma region in a frame. This coding option is referred to as macroblockadaptive framefield MBAFFcoding. For a macroblock pair that is coded in frame mode, eachmacroblock contains frame lines. For a macroblock pair that iscoded in field mode, the top macroblock contains top field linesand the bottom macroblock contains bottom field lines. Fig. 9illustrates the MBAFF macroblock pair concept.Note that, unlike in MPEG2, the framefield decision is madeat the macroblock pair level rather than within the macroblocklevel. The reasons for this choice are to keep the basic macroblock processing structure intact, and to permit motion compensation areas as large as the size of a macroblock.Each macroblock of a field macroblock pair is processedvery similarly to a macroblock within a field in PAFF coding.However, since a mixture of field and frame macroblock pairsmay occur within an MBAFF frame, the methods that are usedfor zigzag scanning, prediction of motion vectors, prediction568 IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY, VOL. 13, NO. 7, JULY 2003of intra prediction modes, intraframe sample prediction,deblocking filtering, and context modeling in entropy codingare modified to account for this mixture. The main idea is topreserve as much spatial consistency as possible. It should benoted that the specification of spatial neighbors in MBAFFframes is rather complicated please refer to 1 and thatin Sections IVGL spatial neighbors are only described fornonMBAFF frames.Another important distinction between MBAFF and PAFF isthat in MBAFF, one field cannot use the macroblocks in theother field of the same frame as a reference for motion predictionbecause some regions of each field are not yet available whena field macroblock of the other field is coded. Thus, sometimesPAFF coding can be more efficient than MBAFF coding particularly in the case of rapid global motion, scene change, or intrapicture refresh, although the reverse is usually true.During the development of the standard, MBAFF was reported to reduce bit rates in the range of 14 to 16 over PAFFfor ITUR 601 resolution sequences like Mobile and Calendarand MPEG4 World News.G. IntraFrame PredictionEach macroblock can be transmitted in one of several codingtypes depending on the slicecoding type. In all slicecodingtypes, the following types of intra coding are supported, whichare denoted as Intra4 4 or Intra16 16 together with chromaprediction and IPCM prediction modes.The Intra4 4 mode is based on predicting each 4 4 lumablock separately and is well suited for coding of parts of apicture with significant detail. The Intra16 16 mode, on theother hand, performs prediction of the whole 16 16 lumablock and is more suited for coding very smooth areas of apicture. In addition to these two types of luma prediction, aseparate chroma prediction is conducted. As an alternative toIntra4 4 and Intra16 16, the IPCM coding type allows theencoder to simply bypass the prediction and transform codingprocesses and instead directly send the values of the encodedsamples. The IPCM mode serves the following purposes.1 It allows the encoder to precisely represent the values ofthe samples.2 It provides a way to accurately represent the values ofanomalous picture content without significant data expansion3 It enables placing a hard limit on the number of bits adecoder must handle for a macroblock without harm tocoding efficiencyIn contrast to some previous video coding standards namelyH.263 and MPEG4 Visual, where intra prediction hasbeen conducted in the transform domain intra prediction inH.264AVC is always conducted in the spatial domain, byreferring to neighboring samples of previouslycoded blockswhich are to the left andor above the block to be predicted.This may incur error propagation in environments with transmission errors that propagate due to motion compensationinto intercoded macroblocks. Therefore, a constrained intracoding mode can be signaled that allows prediction only fromintracoded neighboring macroblocks.a bFig. 10. a Intra44 prediction is conducted for samples ap of a block usingsamples AQ. b Eight prediction directions for Intra44 prediction.Fig. 11. Five of the nine Intra44 prediction modes.When using the Intra4 4 mode, each 4 4 block is predicted from spatially neighboring samples as illustrated on thelefthand side of Fig. 10. The 16 samples of the 4 4 block whichare labeled as ap are predicted using prior decoded samples inadjacent blocks labeled as AQ. For each 4 4 block, one of nineprediction modes can be utilized. In addition to DC predictionwhere one value is used to predict the entire 4 4 block, eightdirectional prediction modes are specified as illustrated on therighthand side of Fig. 10. Those modes are suitable to predictdirectional structures in a picture such as edges at various angles.Fig. 11 shows five of the nine Intra4 4 prediction modes.For mode 0 vertical prediction, the samples above the 4 4block are copied into the block as indicated by the arrows. Mode1 horizontal prediction operates in a manner similar to verticalprediction except that the samples to the left of the 4 4 blockare copied. For mode 2 DC prediction, the adjacent samplesare averaged as indicated in Fig. 11. The remaining six modesare diagonal prediction modes which are called diagonaldownleft, diagonaldownright, verticalright, horizontaldown, verticalleft, and horizontalup prediction. As their names indicate,they are suited to predict textures with structures in the specifieddirection. The first two diagonal prediction modes are also illustrated in Fig. 11. When samples EH Fig. 10 that are used forthe diagonaldownleft prediction mode are not available because they have not yet been decoded or they are outside ofthe slice or not in an intracoded macroblock in the constrainedintramode, these samples are replaced by sample D. Note thatin earlier draft versions of the Intra4 4 prediction mode thefour samples below sample L were also used for some prediction modes. However, due to the need to reduce memory access,WIEGAND et al. OVERVIEW OF THE H.264AVC VIDEO CODING STANDARD 569Fig. 12. Segmentations of the macroblock for motion compensation. Topsegmentation of macroblocks, bottom segmentation of 88 partitions.these have been dropped, as the relative gain for their use is verysmall.When utilizing the Intra16 16 mode, the whole luma component of a macroblock is predicted. Four prediction modes aresupported. Prediction mode 0 vertical prediction, mode 1 horizontal prediction, and mode 2 DC prediction are specifiedsimilar to the modes in Intra4 4 prediction except that insteadof 4 neighbors on each side to predict a 4 4 block, 16 neighborson each side to predict a 16 16 block are used. For the specification of prediction mode 4 plane prediction, please refer to1.The chroma samples of a macroblock are predicted usinga similar prediction technique as for the luma component inIntra16 16 macroblocks, since chroma is usually smooth overlarge areas.Intra prediction and all other forms of prediction across sliceboundaries is not used, in order to keep all slices independent ofeach other.H. InterFrame Prediction1 InterFrame Prediction in P Slices In addition to theintra macroblock coding types, various predictive or motioncompensated coding types are specified as P macroblocktypes. Each P macroblock type corresponds to a specificpartition of the macroblock into the block shapes used for motioncompensated prediction. Partitions with luma block sizesof 16 16, 16 8, 8 16, and 8 8 samples are supported by thesyntax. In case partitions with 8 8 samples are chosen, oneadditional syntax element for each 8 8 partition is transmitted.This syntax element specifies whether the corresponding 8 8partition is further partitioned into partitions of 8 4, 4 8, or4 4 luma samples and corresponding chroma samples. Fig. 12illustrates the partitioning.The prediction signal for each predictivecoded M N lumablock is obtained by displacing an area of the correspondingreference picture, which is specified by a translational motionvector and a picture reference index. Thus, if the macroblockis coded using four 8 8 partitions and each 8 8 partition isfurther split into four 4 4 partitions, a maximum of 16 motionvectors may be transmitted for a single P macroblock.The accuracy of motion compensation is in units of onequarter of the distance between luma samples. In case the motion vector points to an integersample position, the predictionsignal consists of the corresponding samples of the referencepicture otherwise the corresponding sample is obtained usinginterpolation to generate noninteger positions. The predictionvalues at halfsample positions are obtained by applying aFig. 13. Filtering for fractionalsample accurate motion compensation.Uppercase letters indicate samples on the fullsample grid, while lower casesamples indicate samples in between at fractionalsample positions.onedimensional 6tap FIR filter horizontally and vertically.Prediction values at quartersample positions are generated byaveraging samples at integer and halfsample positions.Fig. 13 illustrates the fractional sample interpolation for samples ak and nr. The samples at half sample positions labeledand are derived by first calculating intermediate values and, respectively by applying the 6tap filter as followsThe final prediction values for locations and are obtained asfollows and clipped to the range of 0255The samples at half sample positions labeled as are obtainedbywhere intermediate values denoted as , , , , and areobtained in a manner similar to . The final prediction valueis then computed as and is clipped to therange of 0 to 255. The two alternative methods of obtaining thevalue of illustrate that the filtering operation is truly separablefor the generation of the halfsample positions.The samples at quarter sample positions labeled as a, c, d, n,f, i, k, and q are derived by averaging with upward rounding ofthe two nearest samples at integer and half sample positions as,for example, byThe samples at quarter sample positions labeled as e, g, p, andr are derived by averaging with upward rounding of the twonearest samples at half sample positions in the diagonal direction as, for example, by570 IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY, VOL. 13, NO. 7, JULY 2003The prediction values for the chroma component are alwaysobtained by bilinear interpolation. Since the sampling grid ofchroma has lower resolution than the sampling grid of the luma,the displacements used for chroma have oneeighth sample position accuracy.The more accurate motion prediction using full sample, halfsample and onequarter sample prediction represent one of themajor improvements of the present method compared to earlierstandards for the following two reasons.1 The most obvious reason is more accurate motion representation.2 The other reason is more flexibility in prediction filtering.Full sample, half sample and onequarter sample prediction represent different degrees of low pass filtering whichis chosen automatically in the motion estimation process.In this respect, the 6tap filter turns out to be a much bettertradeoff between necessary prediction loop filtering andhas the ability to preserve highfrequency content in theprediction loop.A more detailed investigation of fractional sample accuracy ispresented in 8.The syntax allows socalled motion vectors over pictureboundaries, i.e., motion vectors that point outside the imagearea. In this case, the reference frame is extrapolated beyondthe image boundaries by repeating the edge samples beforeinterpolation.The motion vector components are differentially coded usingeither median or directional prediction from neighboring blocks.No motion vector component prediction or any other form ofprediction takes place across slice boundaries.The syntax supports multipicture motioncompensated prediction 9, 10. That is, more than one prior coded picture canbe used as reference for motioncompensated prediction. Fig. 14illustrates the concept.Multiframe motioncompensated prediction requires both encoder and decoder to store the reference pictures used for interprediction in a multipicture buffer. The decoder replicates themultipicture buffer of the encoder according to memory management control operations specified in the bitstream. Unlessthe size of the multipicture buffer is set to one picture, the indexat which the reference picture is located inside the multipicture buffer has to be signalled. The reference index parameter istransmitted for each motioncompensated 16 16, 16 8, 8 16,or 8 8 luma block. Motion compensation for smaller regionsthan 8 8 use the same reference index for prediction of allblocks within the 8 8 region.In addition to the motioncompensated macroblock modesdescribed above, a P macroblock can also be coded in thesocalled PSkip type. For this coding type, neither a quantizedprediction error signal, nor a motion vector or reference indexparameter is transmitted. The reconstructed signal is obtainedsimilar to the prediction signal of a P16 16 macroblock typethat references the picture which is located at index 0 in themultipicture buffer. The motion vector used for reconstructingthe PSkip macroblock is similar to the motion vector predictorfor the 16 16 block. The useful effect of this definition ofthe PSkip coding type is that large areas with no change orFig. 14. Multiframe motion compensation. In addition to the motion vector,also picture reference parameters  are transmitted. The concept is alsoextended to B slices.constant motion like slow panning can be represented with veryfew bits.2 InterFrame Prediction in B Slices In comparison toprior video coding standards, the concept of B slices is generalized in H.264AVC. This extension refers back to 11 andis further investigated in 12. For example, other pictures canreference pictures containing B slices for motioncompensatedprediction, depending on the memory management controloperation of the multipicture buffering. Thus, the substantialdifference between B and P slices is that B slices are codedin a manner in which some macroblocks or blocks may use aweighted average of two distinct motioncompensated prediction values for building the prediction signal. B slices utilizetwo distinct lists of reference pictures, which are referred toas the first list 0 and second list 1 reference picture lists,respectively. Which pictures are actually located in each reference picture list is an issue of the multipicture buffer controland an operation very similar to the conventional MPEG2 Bpictures can be enabled if desired by the encoder.In B slices, four different types of interpicture predictionare supported list 0, list 1, bipredictive, and direct prediction.For the bipredictive mode, the prediction signal is formed by aweighted average of motioncompensated list 0 and list 1 prediction signals. The direct prediction mode is inferred from previously transmitted syntax elements and can be either list 0 orlist 1 prediction or bipredictive.B slices utilize a similar macroblock partitioning as P slices.Beside the P16 16, P16 8, P8 16, P8 8, and the intracoding types, bipredictive prediction and another type of prediction called direct prediction, are provided. For each 16 16,16 8, 8 16, and 8 8 partition, the prediction method list 0,list 1, bipredictive can be chosen separately. An 8 8 partitionof a B macroblock can also be coded in direct mode. If no prediction error signal is transmitted for a direct macroblock mode,it is also referred to as BSkip mode and can be coded veryefficiently similar to the PSkip mode in P slices. The motionvector coding is similar to that of P slices with the appropriatemodifications because neighboring blocks may be coded usingdifferent prediction modes.I. Transform, Scaling, and QuantizationSimilar to previous video coding standards, H.264AVC utilizes transform coding of the prediction residual. However, inH.264AVC, the transformation is applied to 4 4 blocks, andWIEGAND et al. OVERVIEW OF THE H.264AVC VIDEO CODING STANDARD 571Fig. 15. Repeated transform for chroma blocks. The four blocks numbered03 indicate the four chroma blocks of a chroma component of a macroblock.instead of a 4 4 discrete cosine transform DCT, a separableinteger transform with similar properties as a 4 4 DCT is used.The transform matrix is given asSince the inverse transform is defined by exact integer operations, inversetransform mismatches are avoided. The basictransform coding process is very similar to that of previousstandards. At the encoder, the process includes a forward transform, zigzag scanning, scaling, and rounding as the quantization process followed by entropy coding. At the decoder, theinverse of the encoding process is performed except for therounding. More details on the specific aspects of the transformin H.264AVC can be found in 17.It has already been mentioned that Intra16 16 predictionmodes and chroma intra modes are intended for coding ofsmooth areas. For that reason, the DC coefficients undergoa second transform with the result that we have transformcoefficients covering the whole macroblock. An additional2 2 transform is also applied to the DC coefficients of the four4 4 blocks of each chroma component. The procedure for achroma block is illustrated in Fig. 15. The small blocks insidethe larger blocks represent DC coefficients of each of the four4 4 chroma blocks of a chroma component of a macroblocknumbered as 0, 1, 2, and 3. The two indices correspond to theindices of the 2 2 inverse Hadamard transform.To explain the idea behind these repeated transforms, let uspoint to a general property of a twodimensional transform ofvery smooth content where sample correlation approaches 1.In that situation, the reconstruction accuracy is proportional tothe inverse of the onedimensional size of the transform. Hence,for a very smooth area, the reconstruction error with a transform covering the complete 8 8 block is halved compared tousing only 4 4 transform. A similar rationale can be used forthe second transform connected to the INTRA16 16 mode.There are several reasons for using a smaller size transform. One of the main improvements of the present standard isthe improved prediction process both for inter and intra.Consequently, the residual signal has less spatial correlation. This generally means that the transform has less tooffer concerning decorrelation. This also means that a 4 4transform is essentially as efficient in removing statisticalcorrelation as a larger transform With similar objective compression capability, the smaller4 4 transform has visual benefits resulting in less noisearound edges referred to as mosquito noise or ringingartifacts. The smaller transform requires less computations and asmaller processing wordlength. Since the transformationprocess for H.264AVC involves only adds and shifts, itis also specified such that mismatch between encoder anddecoder is avoided this has been a problem with earlier8 8 DCT standardsA quantization parameter is used for determining the quantization of transform coefficients in H.264AVC. The parameter cantake 52 values. Theses values are arranged so that an increase of 1in quantization parameter means an increase of quantization stepsize by approximately 12 an increase of 6 means an increaseof quantization step size by exactly a factor of 2. It can be noticed that a change of step size by approximately 12 also meansroughly a reduction of bit rate by approximately 12.The quantized transform coefficients of a block generallyare scanned in a zigzag fashion and transmitted using entropycoding methods. The 2 2 DC coefficients of the chroma component are scanned in rasterscan order. All inverse transformoperations in H.264AVC can be implemented using onlyadditions and bitshifting operations of 16bit integer values.Similarly, only 16bit memory accesses are needed for a goodimplementation of the forward transform and quantizationprocess in the encoder.J. Entropy CodingIn H.264AVC, two methods of entropy coding are supported.The simpler entropy coding method uses a single infiniteextent codeword table for all syntax elements except the quantized transform coefficients. Thus, instead of designing a different VLC table for each syntax element, only the mapping tothe single codeword table is customized according to the datastatistics. The single codeword table chosen is an expGolombcode with very simple and regular decoding properties.For transmitting the quantized transform coefficients, a moreefficient method called ContextAdaptive Variable LengthCoding CAVLC is employed. In this scheme, VLC tables forvarious syntax elements are switched depending on alreadytransmitted syntax elements. Since the VLC tables are designedto match the corresponding conditioned statistics, the entropycoding performance is improved in comparison to schemesusing a single VLC table.In the CAVLC entropy coding method, the number of nonzeroquantized coefficients N and the actual size and position ofthe coefficients are coded separately. After zigzag scanningof transform coefficients, their statistical distribution typicallyshows large values for the low frequency part decreasing tosmall values later in the scan for the highfrequency part. Anexample for a typical zigzag scan of quantized transform coefficients could be given as followsBased on this statistical behavior, the following data elementsare used to convey information of quantized transform coefficients for a luma 4 4 block.1 Number of Nonzero Coefficients N and Trailing1s Trailing 1s T1s indicate the number of coefficientswith absolute value equal to 1 at the end of the scan. In the572 IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY, VOL. 13, NO. 7, JULY 2003example and the number of coefficients is .These two values are coded as a combined event. One out of4 VLC tables is used based on the number of coefficients inneighboring blocks.2 Encoding the Value of Coefficients The values of the coefficients are coded. The T1s need only sign specification sincethey all are equal to or . Please note that the statisticsof coefficient values has less spread for the last nonzero coefficients than for the first ones. For this reason, coefficient valuesare coded in reverse scan order. In the examples above, 2 is thefirst coefficient value to be coded. A starting VLC is used forthat. When coding the next coefficient having value of 6 in theexample a new VLC may be used based on the just coded coefficient. In this way adaptation is obtained in the use of VLCtables. Six expGolomb code tables are available for this adaptation.3 Sign Information One bit is used to signal coefficientsign. For T1s, this is sent as single bits. For the other coefficients, the sign bit is included in the expGolomb codes.Positions of each nonzero coefficient are coded by specifyingthe positions of 0s before the last nonzero coefficient. It is splitinto two parts4 TotalZeroes This codeword specifies the number ofzeros between the last nonzero coefficient of the scan and itsstart. In the example the value of TotalZeros is 3. Since it isalready known that , the number must be in the range011. 15 tables are available for in the range 115. Ifthere is no zero coefficient.5 RunBefore In the example it must be specified how the3 zeros are distributed. First the number of 0s before the last coefficient is coded. In the example the number is 2. Since it mustbe in the range 03 a suitable VLC is used. Now there is onlyone 0 left. The number of 0s before the second last coefficientmust therefore be 0 or 1. In the example the number is 1. At thispoint there are no 0s left and no more information is codedThe efficiency of entropy coding can be improved furtherif the ContextAdaptive Binary Arithmetic Coding CABACis used 16. On the one hand, the usage of arithmetic codingallows the assignment of a noninteger number of bits toeach symbol of an alphabet, which is extremely beneficialfor symbol probabilities that are greater than 0.5. On theother hand, the usage of adaptive codes permits adaptation tononstationary symbol statistics. Another important property ofCABAC is its context modeling. The statistics of already codedsyntax elements are used to estimate conditional probabilities.These conditional probabilities are used for switching severalestimated probability models. In H.264AVC, the arithmeticcoding core engine and its associated probability estimation arespecified as multiplicationfree lowcomplexity methods usingonly shifts and table lookups. Compared to CAVLC, CABACtypically provides a reduction in bit rate between 515 Thehighest gains are typically obtained when coding interlaced TVsignals. More details on CABAC can be found in 16.K. InLoop Deblocking FilterOne particular characteristic of blockbased coding is theaccidental production of visible block structures. Block edgesare typically reconstructed with less accuracy than interiorFig. 16. Principle of deblocking filter.pixels and blocking is generally considered to be one of themost visible artifacts with the present compression methods.For this reason, H.264AVC defines an adaptive inloop deblocking filter, where the strength of filtering is controlled bythe values of several syntax elements. A detailed description ofthe adaptive deblocking filter can be found in 18.Fig. 16 illustrates the principle of the deblocking filter usinga visualization of a onedimensional edge. Whether the samplesand as well as and are filtered is determined usingquantization parameter dependent thresholds and. Thus, filtering of and only takes place if each ofthe following conditions is satisfiedwhere the is considerably smaller than . Accordingly, filtering of or takes place if the corresponding following condition is satisfiedThe basic idea is that if a relatively large absolute difference between samples near a block edge is measured, it is quite likelya blocking artifact and should therefore be reduced. However,if the magnitude of that difference is so large that it cannot beexplained by the coarseness of the quantization used in the encoding, the edge is more likely to reflect the actual behavior ofthe source picture and should not be smoothed over.The blockiness is reduced, while the sharpness of the contentis basically unchanged. Consequently, the subjective quality issignificantly improved. The filter reduces the bit rate typicallyby 510 while producing the same objective quality as thenonfiltered video. Fig. 17 illustrates the performance of the deblocking filter.L. Hypothetical Reference DecoderOne of the key benefits provided by a standard is the assurance that all the decoders compliant with the standard will beable to decode a compliant compressed video. To achieve that,it is not sufficient to just provide a description of the coding algorithm. It is also important in a realtime system to specifyhow bits are fed to a decoder and how the decoded picturesWIEGAND et al. OVERVIEW OF THE H.264AVC VIDEO CODING STANDARD 573abFig. 17. Performance of the deblocking filter for highly compressed picturesa without deblocking filter and b with deblocking filter.are removed from a decoder. Specifying input and output buffermodels and developing an implementation independent modelof a receiver achieves this. That receiver model is also called hypothetical reference decoder HRD and is described in detail in19. An encoder is not allowed to create a bitstream that cannotbe decoded by the HRD. Hence, if in any receiver implementation the designer mimics the behavior of HRD, it is guaranteedto be able to decode all the compliant bitstreams.In H.264AVC HRD specifies operation of two buffers 1 thecoded picture buffer CPB and 2 the decoded picture bufferDPB. CPB models the arrival and removal time of the codedbits. The HRD design is similar in spirit to what MPEG2 had,but is more flexible in support of sending the video at a varietyof bit rates without excessive delay.Unlike MPEG2, in H.264AVC, multiple frames can be usedfor reference, the reference frames can be located either in pastor future arbitrarily in display order, the HRD also specifies amodel of the decoded picture buffer management to ensure thatexcessive memory capacity is not needed in a decoder to storethe pictures used as references.V. PROFILES AND POTENTIAL APPLICATIONSA. Profiles and LevelsProfiles and levels specify conformance points. These conformance points are designed to facilitate interoperability betweenvarious applications of the standard that have similar functionalrequirements. A profile defines a set of coding tools or algorithms that can be used in generating a conforming bitstream,whereas a level places constraints on certain key parameters ofthe bitstream.All decoders conforming to a specific profile must support allfeatures in that profile. Encoders are not required to make useof any particular set of features supported in a profile but haveto provide conforming bitstreams, i.e., bitstreams that can bedecoded by conforming decoders. In H.264AVC, three profilesare defined, which are the Baseline, Main, and Extended Profile.The Baseline profile supports all features in H.264AVC except the following two feature sets Set 1 B slices, weighted prediction, CABAC, fieldcoding, and picture or macroblock adaptive switchingbetween frame and field coding. Set 2 SPSI slices, and slice data partitioning.The first set of additional features is supported by the Mainprofile. However, the Main profile does not support the FMO,ASO, and redundant pictures features which are supported bythe Baseline profile. Thus, only a subset of the coded video sequences that are decodable by a Baseline profile decoder can bedecoded by a Main profile decoder. Flags are used in the sequence parameter set to indicate which profiles of decoder candecode the coded video sequence.The Extended Profile supports all features of the Baselineprofile, and both sets of features on top of Baseline profile, except for CABAC.In H.264AVC, the same set of level definitions is used withall profiles, but individual implementations may support adifferent level for each supported profile. There are 15 levelsdefined, specifying upper limits for the picture size in macroblocks ranging from QCIF to all the way to above 4k 2k,decoderprocessing rate in macroblocks per second rangingfrom 250k pixelss to 250M pixelss, size of the multipicturebuffers, video bit rate ranging from 64 kbps to 240 Mbps, andvideo buffer size.B. Areas for the Profiles of the New Standard to be UsedThe increased compression efficiency of H.264AVC offersto enhance existing applications or enables new applications. Alist of possible application areas is provided below. Conversational services which operate typically below 1Mbps with low latency requirements. The ITUT SG16 iscurrently modifying its systems recommendations to support H.264AVC use in such applications, and the IETF isworking on the design of an RTP payload packetization.In the near term, these services would probably utilize theBaseline profile possibly progressing over time to alsouse other profiles such as the Extended profile. Some specific applications in this category are given below.H.320 conversational video services that utilize circuitswitched ISDNbased video conferencing.3GPP conversational H.324M services.H.323 conversational services over the Internet withbest effort IPRTP protocols.3GPP conversational services using IPRTP for transport and SIP for session setup. Entertainment video applications which operate between18 Mbps with moderate latency such as 0.5 to 2 s.574 IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY, VOL. 13, NO. 7, JULY 2003The H.222.0MPEG2 Systems specification is beingmodified to support these application. These applicationswould probably utilize the Main profile and include thefollowing.Broadcast via satellite, cable, terrestrial, or DSL.DVD for standard and highdefinition video.Videoondemand via various channels. Streaming services which typically operate at 501500kbps and have 2 s or more of latency. These services wouldprobably utilize the Baseline or Extended profile and maybe distinguished by whether they are used in wired or wireless environments as follows3GPP streaming using IPRTP for transport andRTSP for session setup. This extension of the 3GPPspecification would likely use Baseline profile only.Streaming over the wired Internet using IPRTP protocol and RTSP for session setup. This domain which iscurrently dominated by powerful proprietary solutionsmight use the Extended profile and may require integration with some future system designs. Other services that operate at lower bit rates and are distributed via file transfer and therefore do not impose delayconstraints at all, which can potentially be served by anyof the three profiles depending on various other systemsrequirements are3GPP multimedia messaging servicesvideo mail.VI. HISTORY AND STANDARDIZATION PROCESSIn this section, we illustrate the history of the standard. Thedevelopment of H.264AVC is characterized by improvementsin small steps over the last 34 years as can be seen from Fig. 18.In Fig. 18, the coding performance is shown for two exampleprogressivescan video sequences, when enabling the typicalcoding options for the various versions of the standard sinceAugust 1999 until completion in April 2003. The dates and creation of the various versions are shown in Table I. The documentand the software versions have been called test model longtermTML when being developed in VCEG and joint model JMwhen the development was continued in the joint video teamJVT as a partnership between MPEG and VCEG. The development took place in small steps between each version of thedesign as can be seen from Fig. 18.The work started in VCEG as a parallel activity to thecompletion of the last version of H.263. The first Test ModelLongTerm TML1, curve with circles in Fig. 18 wasproduced in August 1999. TML1 was similar to H.263 byusing a combination of block prediction and block transformquantizationcoding of the residual signal. The PSNRperformance of TML1 was similar to that of H.263 curvewith diamondshaped markers in Fig. 18 and below MPEG4ASP curve with starshaped markers in Fig. 18. However, thestarting point was considered sufficient due to some perceptualbenefits being shown and a judgment that the design couldbe incrementally improved. The results shown for H.263 andMPEG4 ASP have been optimized using Lagrangian methodsFig. 18. Evolution of H.264AVC since August 1999 until March 2003. TopQCIF sequence Foreman coded at 10 Hz. Bottom CIF sequence tempete codedat 30 Hz. The legend in the top figure indicates the various versions that havebeen run with typical settings.as described in 13. The main coding features of TML1 aresummarized as follows Seven block partitions for inter prediction of a macroblock. The luminance component of a macroblockcould be partitioned into 16 16, 16 8, 8 16, 8 8,8 4, 4 8 or 4 4 blocks in a similar way as depicted inFig. 12. The 16 16, 16 8, 8 16, and 8 8 blocks haveremained in the design. Partitioning into smaller blockswas modified later in JM2.1 into a treestructuredmacroblock partition as described above. 13sample accurate motioncompensated predictionusing 4tap filter in horizontal and vertical direction.The filter taps were  , 12, 6, 16 and  , 6, 12,16. One of the sample positions used a stronger lowpass filter for more flexible prediction loop filtering. Thiswas modified later in TML4 to 14sample accurateprediction using a 6tap as described above. For TML2,a method called adaptive motion accuracy AMA wasadopted which has never been implemented into thesoftware. AMA was dropped due to lack of coding efficiency improvement in TML4. In TML78, 18sampleaccurate motion compensation was introduced which wasthen dropped for complexity reasons in JM5.WIEGAND et al. OVERVIEW OF THE H.264AVC VIDEO CODING STANDARD 575TABLE IHISTORY OF H.264AVC STANDARDIZATION PROCESS. FOR TML VERSIONSWITH A , NO SOFTWARE HAS BEEN CREATED Multiple reference frames, the decoupling of temporalorder and display order, and the decoupling of picturetype from the ability to use pictures as references wereenvisioned from the beginning, but were integrated in thesoftware and draft text rather gradually. Intra prediction was done on 4 4 blocks and based on theneighboring samples. There were five prediction modeswhich were the ones shown in Fig. 9 except with a simpler version of Mode 3. The number of prediction modeswas increased to 7 in TML4 and further increased to 9 inJM2. In TML3, the Intra16 16 prediction mode wasintroduced and in JM3, the various prediction modes forchroma have been introduced. The transform for the residual signal had size 4 4. Thiswas in contrast to all previous standards which used transform size 8 8. The transform was also no longer exactDCT but an integer transform very close to DCT. The integer definition resulted in an exact definition of the inverse transform. The transform had basis vectorsThe transform was later changed to the version describedabove in JM2. An inloop deblocking filter similar to the one used inH.263 was in TML1, but only on intra frames. This filterhas been considerably refined during the entire development of the standard. Entropy coding used one single expGolomb type VLCtable for all syntax elements including transform coefficients. This was extended later by CABAC in TML7 andCAVLC for transform coefficients in JM3.TML1 did not contain many of the features of the final designof JM6 squares in Fig. 18 including interlace support, B pictures and the NAL. The method of handling of interlaced videowas among the last things integrated into the design note thatFig. 18 does not show performance for interlaced video. Theimprovement of JM6 relative to TML1 is typically between23 dB PSNR or between 4060 in bitrate reduction.VII. CONCLUSIONSThe emerging H.264AVC video coding standard hasbeen developed and standardized collaboratively by both theITUT VCEG and ISOIEC MPEG organizations. H.264AVCrepresents a number of advances in standard video codingtechnology, in terms of both coding efficiency enhancementand flexibility for effective use over a broad variety of networktypes and application domains. Its VCL design is based onconventional blockbased motioncompensated hybrid videocoding concepts, but with some important differences relativeto prior standards. We thus summarize some of the importantdifferences enhanced motionprediction capability use of a small blocksize exactmatch transform adaptive inloop deblocking filter enhanced entropy coding methods.When used well together, the features of the new design provide approximately a 50 bit rate savings for equivalent perceptual quality relative to the performance of prior standards especially for higherlatency applications which allow some use ofreverse temporal prediction.1ACKNOWLEDGMENTThe authors thank the experts of ITUT VCEG, ISOIECMPEG, and the ITUTISOIEC Joint Video Team for theircontributions. Thanks to K. Shring and H. Schwarz forproviding the data for the comparison of the various TML andJM versions.REFERENCES1 Draft ITUT recommendation and final draft international standard ofjoint video specification ITUT Rec. H.264ISOIEC 14 49610 AVC,in Joint Video Team JVT of ISOIEC MPEG and ITUT VCEG, JVTG050, 2003.2 Generic Coding of Moving Pictures and Associated Audio Information Part 2 Video, ITUT and ISOIEC JTC 1, ITUT RecommendationH.262 and ISOIEC 13 8182 MPEG2, 1994.3 Video Codec for Audiovisual Services at p64 kbits ITUT Recommendation H.261, Version 1, ITUT, ITUT Recommendation H.261Version 1, 1990.4 Video Coding for Low Bit Rate Communication, ITUT, ITUT Recommendation H.263 version 1, 1995.5 Coding of audiovisual objectsPart 2 Visual, in ISOIEC 14 4962MPEG4 Visual Version 1, Apr. 1999.6 S. Wenger, H.264AVC over IP, IEEE Trans. Circuits Syst. VideoTechnol., vol. 13, pp. 645656, July 2003.7 T. Stockhammer, M. M. Hannuksela, and T. Wiegand, H.264AVC inwireless environments, IEEE Trans. Circuits Syst. Video Technol., vol.13, pp. 657673, July 2003.1Further information and documents of the project is available by ftp atftpftp.imtcfiles.orgjvtexperts.576 IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY, VOL. 13, NO. 7, JULY 20038 T. Wedi, Motion compensation in H.264AVC, IEEE Trans. CircuitsSyst. Video Technol., vol. 13, pp. 577586, July 2003.9 T. Wiegand, X. Zhang, and B. Girod, Longterm memory motioncompensated prediction, IEEE Trans. Circuits Syst. Video Technol., vol. 9,pp. 7084, Feb. 1999.10 T. Wiegand and B. Girod, MultiFrame MotionCompensated Predictionfor Video Transmission. Norwell, MA Kluwer, 2001.11 M. Flierl, T. Wiegand, and B. Girod, A locally optimal design algorithmfor blockbased multihypothesis motioncompensated prediction, inProc. Data Compression Conf., Snowbird, UT, Mar. 1998, pp. 239248.12 M. Flierl and B. Girod, Generalized B pictures and the draft JVTH.264video compression standard, IEEE Trans. Circuits Syst. Video Technol.,vol. 13, pp. 587597, July 2003.13 T. Wiegand, H. Schwarz, A. Joch, F. Kossentini, and G. J. Sullivan,Rateconstrained coder control and comparison of video coding standards, IEEE Trans. Circuits Syst. Video Technol., vol. 13, pp. 688703,July 2003.14 S. Wenger, H.264AVC over IP, IEEE Trans. Circuits Syst. VideoTechnol., vol. 13, pp. 645656, July 2003.15 M. Karczewicz and R. Kureren, The SP and SI frames design forH.264AVC, IEEE Trans. Circuits Syst. Video Technol., vol. 13, pp.637644, July 2003.16 D. Marpe, H. Schwarz, and T. Wiegand, Contextadaptive binary arithmetic coding in the H.264AVC video compression standard, IEEETrans. Circuits Syst. Video Technol., vol. 13, pp. 620636, July 2003.17 H. Malvar, A. Hallapuro, M. Karczewicz, and L. Kerofsky, LowComplexity transform and quantization in H.264AVC, IEEE Trans. CircuitsSyst. Video Technol., vol. 13, pp. 598603, July 2003.18 P. List, A. Joch, J. Lainema, G. Bjntegaard, and M. Karczewicz, Adaptive deblocking filter, IEEE Trans. Circuits Syst. Video Technol., vol.13, pp. 614619, July 2003.19 J. RibasCorbera, P. A. Chou, and S. Regunathan, A generalized hypothetical reference decoder for H.264AVC, IEEE Trans. Circuits Syst.Video Technol., vol. 13, pp. 674687, July 2003.Thomas Wiegand received the Dr.Ing. degree fromthe University of ErlangenNuremberg, Germany,in 2000 and the Dipl.Ing. degree in electricalengineering from the Technical University ofHamburgHarburg, Germany, in 1995.He is the Head of the Image CommunicationGroup in the Image Processing Department, FraunhoferInstitute for Telecommunications  HeinrichHertz Institute HHI, Berlin, Germany. During 1997to 1998, he was a Visiting Researcher at StanfordUniversity, Stanford, CA, and served as a Consultantto 8x8, Inc., Santa Clara, CA. From 1993 to 1994, he was a Visiting Researcherat Kobe University, Kobe, Japan. In 1995, he was a Visiting Scholar at theUniversity of California at Santa Barbara, where he began his research on videocompression and transmission. Since then, he has published several conferenceand journal papers on the subject and has contributed successfully to the ITUTVideo Coding Experts Group ITUT SG16 Q.6VCEGISOIEC MovingPictures Experts Group ISOIEC JTC1SC29WG11MPEGJoint VideoTeam JVT standardization efforts and holds various international patents inthis field. He has been appointed as the Associated Rapporteur of the ITUTVCEG October 2000, the Associated RapporteurCoChair of the JVT thathas been created by ITUT VCEG and ISOIEC MPEG for finalization of theH.264AVC video coding standard December 2001, and the Editor of theH.264AVC video coding standard February 2002.Gary J. Sullivan S83M 91SM01 received theB.S. and M.Eng. degrees in electrical engineeringfrom the University of Louisville, Louisville, KY, in1982 and 1983, respectively, and the Ph.D. and Eng.degrees in electrical engineering from the Universityof California, Los Angeles, in 1991.He is the Chairman of the Joint Video TeamJVT for the development of the nextgenerationH.264MPEG4AVC video coding standard, whichwas recently completed as a result of a joint projectbetween the ITUT video coding experts groupVCEG and the ISOIEC moving picture experts group MPEG. He is alsothe Rapporteur of Advanced Video Coding in the ITUT, where he has ledVCEG ITUT Q.6SG16 for about six years, and the ITUT Video LiaisonRepresentative to MPEG ISOIEC JTC1SC29WG11 and served as MPEGsVideo Chairman during 20012002. He is currently a Program Managerof video standards and technologies in the eHome AV Platforms Groupof Microsoft Corporation, Redmond, WA, where he designed and remainslead engineer for the DirectX Video Acceleration APIDDI feature of theMicrosoft Windows operating system platform. Prior to joining Microsoft in1999, he was the Manager of Communications Core Research at PictureTelCorporation, the quondam world leader in videoconferencing communication.He was previously a Howard Hughes Fellow and Member of the TechnicalStaff in the Advanced Systems Division of Hughes Aircraft Corporation andwas a terrainfollowing radar system software engineer for Texas Instruments.His research interests and areas of publication include image and videocompression, ratedistortion optimization, motion representation, scalar andvector quantization, and error and packetlossresilient video coding.Gisle Bjntegaard received the Dr. Phil. degree inphysics from the University of Oslo, Oslo, Norway,in 1974.From 1974 to 1996, he was a Senior Scientist withTelenor Research and Development, Oslo, Norway.His areas of research included radio link networkdesign, reflector antenna design and construction,digital signal procession, and development of videocompression methods. From 1996 to 2002, he wasa Group Manager at Telenor Broadband Services,Oslo, Norway, where his areas of work includedthe design of pointtopoint satellite communication and development ofsatellite digital TV platform. Since 2002, he has been a Principal Scientist atTandberg Telecom, Lysaker, Norway, working with videocoding developmentand implementation. He has contributed actively to the development of theITU video standards H.261, H.262, H.263, and H.264, as well as to ISOIECMPEG2 and MPEG4.Ajay Luthra S79M81SM89 received the B.E.Hons. degree from BITS, Pilani, India, in 1975, theM.Tech. degree in communications engineering fromIIT Delhi, Delhi, India, in 1977, and the Ph.D. degreefrom Moore School of Electrical Engineering, University of Pennsylvania, Philadelphia, in 1981.From 1981 to 1984, he was a Senior Engineer atInterspec Inc., Philadelphia, PA, where he was involved in applications of digital signal and image processing for Biomedical applications. From 1984 to1995, he was with Tektronix, Beaverton, OR, wherehe was Manager of the Digital Signal and Picture Processing Group during19851990 and then Director of the CommunicationsVideo Systems ResearchLab from 19901995 . He is currently a Senior Director in Advanced Technology Group at Motorola, Broadband Communications Sector formerly General Instrument, San Diego, CA, where he is involved in advanced developmentwork in the areas of digital video compression and processing, streaming video,interactive TV, cable headend system design, and advanced settop box architectures. He has been an active member of the MPEG committee for the lastten years where he has chaired several technical subgroups. He is an AssociateRapporteurCoChair of the Joint Video Team consisting of ISOMPEG andITUTH.26L experts working on developing next generation of video codingstandards.Dr. Luthra was an Associate Editor of the IEEE TRANSACTIONS ON CIRCUITSAND SYSTEMS FOR VIDEO TECHNOLOGY 20002002 and also a Guest Editorfor its Special Issue on Streaming Video March 2001.
