Parallel andDistributedSimulation SystemsRICHARD M. FUJIMOTO .  1 ,, I t I.    iIIWILEY SERIES ON PARALLELAND DISTRIBUTED COMPUTINGEDITORALBERT Y. ZOMAYAFUJIMOTO  Parallel and Distributed Simulation SystemsSAPATY  Mobile Processing in Distributed and Open EnvironmentsXAVIER AND IYENGAR  Introduction to Parallel AlgorithmsPARALLEL ANDDISTRIBUTEDSIMULATION SYSTEMSRichard M. Fujimoto, PhDGeorgia Institute of TechnologyA WILEYINTERSCIENCE PUBLICATIONJOHN WILEY  SONS, INC.New York  Chichester  Weinheim  Brisbane  Singapore  TorontoThis book is printed on acidfree paper. iCopyright  2000 by John Wiley  Sons, Inc. All rights reserved.Published simultaneously in Canada.No part of this publication may be reproduced, stored in a retrieval systemor transmitted in any form or by any means, electronic, mechanical,photocopying, recording, scanning or otherwise, except as permitted underSection 107 or 108 of the 1976 United States Copyright Act, without either theprior written permission of the Publisher, or authorization through paymentof the appropriate percopy fee to the Copyright Clearance Center, 222Rosewood Drive, Danvers, MA 01923, 978 7508400, fax 978 7504744..Requests to the Publisher for permission should be addressed to the Permissions Department, John Wiley  Sons, Inc., 605 Third Avenue, NewYork, NY 101580012, 212 8506011, fax 212 8506008, EMailPERMREQWILEY.COM.For ordering and customer service, call 1800CALLWILEY.Library of Congress CataloginginPublication DataFujimoto, Richard M.Parallel and distributed simulation systems  Richard M.Fujimoto.p. cm.A WileyInterscience publication.Includes bibliographical references.ISBN 0471183830 alk. paper1. Computer simulation. 2. Parallel processing Electroniccomputers 3. Electronic data processingDistributed processing.I. Title.QA76.9.C65F84 2000003 .3435dc21 9925438Printed in the United States of America.10 9 8 7 6 5 4 3 2ToJan, Emily, and AlexCONTENTSPreface xvPART I INTRODUCTION1 Background and Applications 31.1 Why ParallelDistributed Simulation 41.2 Analytic Simulations versus Virtual Environments 61.3 Historical Perspective 81.3.1 HighPerformance Computing Community 81.3.2 Defense Community 91.3.3 Interactive Gaming and Internet Communities 101.4 Applications 111.4.1 Military Applications 121.4.2 Entertainment 131.4.3 Social Interactions and Business Collaborations 131.4.4 Education and Training 141.4.5 Telecommunication Networks 141.4.6 Digital Logic Circuits and Computer Systems 151.4.7 Transportation 161.5 Underlying Technologies 161.6 Hardware Platforms 171.6.1 Parallel versus Distributed Computers 171.6.2 SharedMemory Multiprocessors 191.6.3 DistributedMemory Multicomputers 201.6.4 SIMD Machines 211.6.5 Distributed Computers 221.7 Summary 231.8 Additional Readings 242 Discrete Event Simulation Fundamentals 272.1 Time 272.2 RealTime, Scaled RealTime, and AsFastAsPossible Execution 28viiCONTENTS ixviii CONTENTSRepeatability and Simultaneous Events 842.3 State Changes and Time Flow Mechanisms 30 3.92.3.1 TimeStepped Execution 31 3.9.1 Using Hidden Time Stamp Fields to Order852.3.2 EventDriven Execution 32 Simultaneous Events853.9.2 Priority Numbers862.4 DiscreteEvent Simulation Programs 34 3.9.3 ReceiverSpecified Ordering2.5 An Example Application 36 Performance of Conservative Mechanisms 873.102.6 Starting and Stopping the Simulation 39 Summary and Critique of Conservative Mechanisms 913.112.7 ParallelDistributed Simulation Example 39 Additional Readings 943.122.8 World Views and ObjectOriented Simulation 412.8.1 Simulation Processes 41972.8.2 ObjectBased and ObjectOriented Simulations 45 4 Time Warp2.8.3 Query Events and Push versus Pull Processing 46 Preliminaries 984.12.8.4 Event Retraction 47Local Control Mechanism 984.21002.9 Other Approaches to Exploiting Concurrent Execution 484.2.1 Rolling Back State Variables1022.10 Additional Readings 48 4.2.2 Unsending Messages ..1064.2.3 Zero Lookahead, Simultaneous Events, and RepeatablhtyGlobal Control Mechanism 1084.3110PART II PARALLEL AND DISTRIBUTED DISCRETEEVENTFossil CollectionSIMULATION 4.3.11104.3.2 Error Handling3 Conservative Synchronization Algorithms 51Computing Global Virtual Time 1124.41133.1 Synchronization Problem 52 4.4.1 Transient Message ProblemSimultaneous Reporting Problem 1143.2 Deadlock Avoidance Using Null Messages 54 4.4.21164.4.3 Samadis GVT Algorithm1173.3 Lookahead and the Simulation Model 58 4.4.4 Matterns GVT Algorithm3.4 Deadlock Detection and Recovery 60 Other Mechanisms 1224.53.4.1 Deadlock Detection 60 Dynamic Memory Allocation 1224.5.13.4.2 Deadlock Recovery 63 4.5.2 Infrequent State Saving 1234.5.3 Specifying What to Checkpoint 1263.5 Synchronous Execution 651283.5.1 Centralized Barriers 66 4.5.4 Event Retraction1294.5.5 Lazy Cancellation3.5.2 Tree Barrier 674.5.6 Lazy Reevaluation 1323.5.3 Butterfly Barrier 683.5.4 Transient Messages 70 4.6 Scheduling Logical Processes 1333.5.5 A Simple Synchronous Protocol 744.7 Summary 1353.5.6 Distance between Logical Processes 751353.6 Bounded Lag 79 4.8 Additional Readings3.7 Conditional versus Unconditional Information 811373.8 Dynamic Processes and Interconnections 82 5 Advanced Optimistic Techniques1385.1 Memory Utilization in Time WarpX CONTENTSCONTENTS xi5.1.1 Preliminaries State Vectors and Message Send 6 Time Parallel Simulation 177Time Stamps 1396.1 Time Parallel Cache Simulation Using Fixup Computations 1795.1.2 Memory Management Mechanisms and MessageSendback 140 6.2 Simulation of an ATM Multiplexer Using Regeneration Points 1835.1.3 Storage Optimality 142 Simulation of Queues Using Parallel Prefix 1885.1.4 Cancelback 1456.35.1.5 Artificial Rollback 145 6.4 Summary 1905.1.6 Pruneback 147 1915.1.7 MemoryBased Flow Control 1486.5 Additional Readings5.1.8 Trading Off Performance and Memory 1495.2 Performance Hazards in Time Warp 1515.2.1 Chasing Down Incorrect Computations 151 PART III DISTRIBUTED VIRTUAL ENVIRONMENTS DVEs5.2.2 Rollback Echoes 152 1957 DVEs Introduction5.3 Other Optimistic Synchronization Algorithms 154 7.1 Goals 1955.3.1 Moving Time Window 155 1965.3.2 LookaheadBased Blocking 1567.2 Contrasting DVE and PDES Systems5.3.3 Local Rollback 156 7.3 Server versus Serverless Architectures 1975.3.4 Breathing Time Buckets 157 1995.3.5 Wolf Calls 1597.4 Distributed Interactive Simulation5.3.6 Probabilistic Rollbacks 160 7.4.1 DIS Design Principles2005.3.7 SpaceTime Simulation 160 7.4.2 DIS PDUs2025.3.8 Summary 160 7.4.3 Time Constraints2037.5 Dead Reckoning 2045.4 Putting It All Together Georgia Tech Time Warp GTW 1617.5.1 Dead Reckoning Models 2065.4.1 Programmers Interface 161 7.5.2 Time Compensation 2085.4.2 IO and Dynamic Memory Allocation 162 7.5.3 Smoothing 2085.4.3 GTW Data Structures 1635.4.4 Direct Cancellation 165 7.6 High Level Architecture2095.4.5 EventProcessing Loop 166 7.6.1 Historical Perspective 2105.4.6 Buffer Management 166 7.6.2 Overview of the HLA 2115.4.7 Flow Control 168 7.6.3 HLA Rules 2135.4.8 GVT Computation and Fossil Collection 168 7.6.4 Object Models and the Object Model Template 2135.4.9 Incremental State Saving 169 7.6.5 Interface Specification 2185.4.10 Local Message Sends 169 7.6.6 Typical Federation Execution 2195.4.11 Message Copying 1697.7 Summary 2205.4.12 Batch Event Processing 1695.4.13 Performance Measurements 170 7.8 Additional Readings 2215.5 Summary 1715.6 Comparing Optimistic and Conservative 8 Networking and Data Distribution 223Synchronization 172 8.1 MessagePassing Services 2235.7 Additional Readings 174 8.1.1 Reliable Delivery 223xii CONTENTSxiiiCONTENTS8.1.2 Message Ordering 224 9.4 Summary 2758.1.3 ConnectionOriented versus Connectionless 9.5 Additional Readings 275Communication 2248.1.4 Unicast versus Group Communication 2248.1.5 Examples DIS and NPSNet 225 References 2778.2 Networking Requirements 2268.3 Networking Technologies 227Index 2938.3.1 LAN Technologies 2278.3.2 WAN Technologies 2318.3.3 Quality of Service 2338.4 Communication Protocols 2348.4.1 OSI Protocol Stack 2358.4.2 ATM Protocol Stack 2388.4.3 Internetworking and Internet Protocols 2408.5 Group Communication 2438.5.1 Groups and Group Communication Primitives 2448.5.2 Transport Mechanisms 2448.6 Data Distribution 2458.6.1 Interface to the Data Distribution System 2458.6.2 Example Data Distribution in the High LevelArchitecture 2478.6.3 Implementation Issues 2518.6.4 Dynamic Group Management 2548.6.5 WideArea Viewers and FastMoving Entities 2558.7 Summary 2568.8 Additional Readings 2579 Time Management and Event Ordering 2599.1 The Problem 2599.2 MessageOrdering Services 2619.2.1 Causal Order 2619.2.2 Causal and Totally Ordered 2659.2.3 Delta Causality 2679.2.4 Time Stamp Order 2689.3 Synchronizing Wallclock Time 2699.3.1 Time and Time Sources 2709.3.2 Clock Synchronization Algorithms 2719.3.3 Correcting Clock Synchronization Errors 2739.3.4 Network Time Protocol 274PREFACEThese are exciting times in the parallel and distributed simulation field. After manyyears of research and development in university and industrial laboratories, the fieldhas exploded in the last decade and is now seeing use in many realworld systemsand applications. My goal in writing Parallel and Distributed Simulation Systems isto give an indepth treatment of technical issues concerning the execution of discreteevent simulation programs on computing platforms composed of many processorsinterconnected through a network. The platform may range from tightly coupledmultiprocessor computer systems confined to a single cabinet or room to geographically distributed personal computers or specialized simulators for example,video game systems spread across the world. This technology can be used to speedup the execution of largescale simulations, for example simulations of the nextgeneration of the Internet, or to create distributed synthetic environments for trainingor entertainment.My goal in writing this book was to bring together into one volume thefundamental principles concerning parallel and distributed simulation systems thattoday are scattered across numerous journals and conference proceedings. Theintended audience includes managers and practitioners involved in research andordevelopment ofdistributed simulation systems. The book can serve as a textbook foran advanced undergraduate or a graduate level computer science course. The bookmight be of interest in other disciplines for example, industrial engineering oroperations research although the principal emphasis is on issues concerning paralleland distributed computation. Prior knowledge of discrete event simulation parallel,or distributed computation would be helpful, but is not essential as the book willinclude brief introductions to these fields.ContentsThe book is divided into three parts. The first provides an introduction to the field.Chapter 1 describes typical applications where this technology can be applied, andgives an historical perspective to characterize the communities that developed andrefined this technology. Background information concerning parallel and distributedcomputing systems is reviewed. Chapter 2 reviews fundamental concepts in discreteevent simulation to provide a common basis and terminology that is used in theremainder of the book.YVxvi PREFACEThe second part is primarily concerned with parallel and distributed execution ofsimulations, primarily for analysis applications such as to design large, complexsystems. Here the goal is to use multiple processors to speed up the execution. Muchof the material in these four chapters is concerned with synchronization algorithmsthat are used to ensure a parallel execution of the simulation yields the same resultsas a sequential execution, but hopefully much more quickly. Two principalapproaches to addressing this issue are called conservative and optimistic synchronization. Chapter 3 is concerned with the former, and Chapters 4 and S with thelatter. Chapter 6 is concerned with an altogether different approach to parallelexecution called time parallel simulation that is only suitable for certain classes ofsimulation problems, but can yield dramatic performance improvements when it canbe applied.The third part is concerned with distributed virtual environments DVEs. Herethe emphasis is on realtime simulations, that is, to create virtual environments intowhich humans may be embedded, for example, for training or entertainment.Chapter 7 gives an introduction to this area, focusing primarily on two effortswithin the defense community, namely Distributed Interactive Simulation mS andthe High Level Architecture HLA where much of this technology was developedand has been applied. Chapters 8 and 9 are concerned with two specific issues inDVEs. Chapter 8 covers the problem of efficiently distributing data among theparticipants of the DVE. The first half of the chapter is an introduction to computernetworks which provide the underlying communication support for DVEs. Thesecond half is concerned with techniques to effectively utilize the networkinginfrastructure, particularly for largescale simulations with many interacting components. Finally, Chapter 9 revisits the problem of time synchronization in DVEs aswell as the problem of ensuring that the different computers participating in thesimulation have properly synchronized clocks.Part I lays the groundwork for the remainder of the book, so should be read first.Parts II and III can be read in either order. I have used this book as the text in a 10week course in parallel and distributed simulation taught at Georgia Tech, and planto use it when we transition to ISweek semesters. Alternatively, this book could beused for part of a course in discrete event simulation. When used in this manner,instructors may wish to skip Chapters Sand 6, and the first half of Chapter 8 toobtain a more abbreviated treatment of the subject material.SoftwareInterested readers may wish to try out some of the algorithms discussed in this book.Although software is not included with the text, it is available. In particular, theGeorgia Tech Time Warp GTW software discussed in Chapter 5 and an implementation of a subset of the High level Architecture Run Time Infrastructure arefreely available for education and research purposes. Information concerning thissoftware is available at httpwww.cc.gatech.educomputingpads. To obtain acopy of either or both of these software packages, you may contact me via electronicmail at fujimotocc.gatech.edu.PREFACE xviiAcknowledgmentsObviously, this book would not be possible without the many technical contributionsby numerous individuals in both academia and idustY. I hae .attempted torecognize as many of these contributors as possIble m the blbhography andreferences to additional reading materials. Regrettably, the field has exanded t.othe extent that anything approaching a complete listing of the contnbutors ISimpossible. . .I am in debt to many individuals who contnbuted dIrectly to the developent ofthis book. In particular, many useful comments on early drafts wer provIded bystudents in my graduate class on parallel and distributed simulatlOn taught. atGeorgia Tech. Specific detailed comments from Glenn Oberhauser and KathenneMorse are also appreciated. I am grateful to several funding agencies that spnsoredmy research in parallel and distributed simulation some of te esuIs f thIS workare included in this text. These agencies include the Balhstlc Mlssl1e DefenseOrganization, the Defense Advance Research Projects Aency, the Deense Modeling and Simulation Office, the National Science FoundatlOn, SAIC, MItre Corporation, Bellcore, Army Research Office, the Office of Naval Research, and theStrategic Missile Defense Command. . . .Finally, lowe the greatest gratitude to my famIly, who provIded contlued spportand understanding despite the countless evenings and weekends of dealmg wIth yabsence due to this project which came to be known simply as the book. hlSmanuscript would never have been completed were it not for their love and devotlOn.PARALLEL AND DISTRIBUTEDSIMULATION SYSTEMS PART IINTRODUCTIONCHAPTER 1Background and ApplicationsImagine that you are responsible for monitoring commercial air traffic in the UnitedStates and providing recommendations to air traffic controllers across the country.Your objective is to ensure that the air transportation system remains safe andefficient, and to minimize traveler delays. Severe storms unexpectedly develop inChicago, causing the flow of traffic in and out of OHare airport to be reduced toonly a fraction of its normal capacity. How does this situation affect air traffic acrossthe country Should aircraft about to take off be allowed to depart, potentiallycreating backlogs of planes that are forced to circle, wasting enormous quantities offuel Or should these aircraft be held on the ground, at the risk of causing travelerdelays and frustrations that might not have been really necessary What aboutaircraft already in flight Should they be rerouted to alternate destinationsComputer simulations of the national air traffic space provide a means to testout different strategies in order to determine which will be the most effective. But,existing air traffic simulations may require hours to complete, while decisions mustbe made in minutes.Consider a second scenario. You are an engineer working on designs for the nextgeneration of the Internet. Specifically, you are responsible for designing communication protocols that will be used to carry multimedia traffic for a wide variety ofusers with vastly different requirements, for example, realtime video teleconferences requiring low latency transmissions of compressed video frames, electronicmail that can tolerate higher, more variable transmission delays, or web surferstransferring large data files, voice and audio transmissions. There are manyimportant design issues and costperformance tradeoffs you need to consider thatwill play an important role in determining the success of your designs in acompetitive marketplace. Again, computer simulation tools are available to evaluatedifferent designs, but because of the size and complexity of the networks you areconsidering, even a single execution of the simulation program that models only afew minutes of network operation will require hours, or even days, to complete.Consider still another scenario. You are the commander of a large militaryoperation. You must prepare a variety of officers and enlisted personnel for combatin an engagement that is breaking out in another part of the world. In particular, youmust perform multinational joint mission rehearsals including tank commandersstationed in the United States, aircraft pilots stationed in England, and naval4 BACKGROUND AND APPLICATIONS1.1 WHY PARALLEUDISTRIBUTED SIMULATION 51. Reduced execution tme. By subdividing a large simulation computation intomany sUbcompuatIOns, and executing the subcomputations concurrentlyacross, say, ten dlffeen processors, one can reduce the execution time upt a facor of ten.. hIS IS not unlike subdividing your lawn into ten equallysIzd stnps, and hmng ten people each with their own lawn mower to w ka dIfferent strip. In principle, you can mow the entire lawn in only oneencommanders at sea. It is critical that individuals within each group g . .. . . aIn expenenceUSIng theIr eqUIpment in situations they may encounter as well .k . h . . , s expenencewor Ing WIt personnel from other umts In coordinated military stnkes T db d t . . . Ime anu ge ary restnctIOns preclude largescale field exercises to p tH rac Ice maneuvers.0V ca you properly prepare individuals at geographically distinct locations whelImIted tIme and resources are at your disposal. Tese scnarios. describe realworld situations that exist today where parallel andstnted sImulatIOn technolgies can, and in many cases are playing a critical role.e WI . return to these scenanos momentarily to discuss how such technologies canhelp..FIrst, le us define what is meant by paralleljdistributed simulation andexamIne why It has attracted so much interest in recent years.What is paralleldistriuted imulation technology Simply stated, this is a technology that enables a sImulatIOn program to be executed on paralleldistributede sxse.ms, namely systems composed of multiple interconnected computers.. . e mtIOn suggests, there are two key components to this technolo . 1sImulatIOn, and 2 excuton on parallel or distributed computers. gy.A computer szmulatlOn IS a computation that models the beha f 1. . d VIOr 0 some rea ormgIe system over tIme. Simulations are widely used today to analyze the.e aVIOr of systms such as air traffic control and future generation telecommunicaon twrks wIthout actually constructing the systems and situations of interestinsctIng a pototye may be costly, infeasible, andor dangerous Anothe. tPortthue of sImulatIOns today is to create computergenerated virtul worldsI 0  IC umans ndor physical devices will be embedded. An aircraft flightSimu ator usd to aIn pIlots is one such example.. Pralel sImulatIOn and distributed simulation refer to technologies that enable aSimu atIOn program to execute on a computing system containing multi Ie rocessors, .such as peroal omputers, interconnected by a communication neort Laterwe dISCUSS the dIstInctIOn between parallel and distrzbut d l b .typ f . e Simu atIOn ased on thee 0 computIng system used to execute the simulation For now suffice it tt allel simulaions. excute on a set of computers confined to a single cabiePhiaIlcdsep,cStadgeograThere alr.e pnmanly four principal benefits to executing a simulation pogramacross mu tIple computers1.1 WHY PARALLELDISTRIBUTED SIMULATIONthe time it would have taken using only one lawn mower. In computersimulations it may be necessary to reduce execution time so that an engineerwill not have to wait long periods of time to receive results produced by thesimulation. Alternatively, when used to create a virtual world into whichhumans will be immersed, multiple processors may be needed to complete thesimulation computation fast enough so that the simulated world evolves asrapidly as real life. This is essential to make the computergenerated worldlook and feel to the user just like the real thing.2. Geographical distribution. Executing the simulation program on a set ofgeographically distributed computers enables one to create virtual worlds withmultiple participants that are physically located at different sites. For example,consider a simulated air battle composed of flight simulators executing oncomputers at distinct geographical locations, such as London, New York, andParis. Participants in this simulation exercise can interact with each other as ifthey were located together at a training facility at a single site, but without thetime, expense, and inconvenience of traveling to that site.3. Integrating simulators that execute on machines from different manufacturers.Suppose that flight simulators for different types of aircraft have beendeveloped by different manufacturers. Rather than porting these programs toa single computer, it may be more cost effective to hook together theexisting simulators, each executing on a different computer, to create a newvirtual environment. Again, this requires the simulation computation to bedistributed across multiple computers.4. Fault tolerance. Another potential benefit of utilizing multiple processors isincreased tolerance to failures. If one processor goes down, it may be possiblefor other processors to pick up the work of the failed machine, allowing thesimulation computation to proceed despite the failure. By contrast, if thesimulation is mapped to a single processor, failure of that processor means theentire simulation must stop.Returning to our first scenario involving air traffic control, parallel simulationtechniques can reduce the execution time of simulation tools for modeling air trafficfrom hours to minutes, or even seconds, enabling these tools to be used online intime critical decisionmaking processes. Similarly, in the Internet design scenario,parallel simulation techniques can enable much more extensive, detailed analyses ofnetworks to be performed before a new product is brought to market, therebyresulting in improved performance, reliability, andor reduced cost. In these twoapplications, reduced execution time is the principal benefit. Fault tolerance is also apotential benefit, but, a substantial amount of effort is required to recover and restartthe computation from failed processors in order for the results of the simulation to bevalid.In the scenario involving the training of military personnel, geographic distribution and integrating simulators that execute on different hardware platforms areimportant benefits in utilizing a distributed simulation approach. Fault tolerance is6 BACKGROUND AND APPLICATIONS 1.2 ANALYTIC SIMULATIONS VERSUS VIRTUAL ENVIRONMENTS 7TABLE 1.1 Analytic simulations and virtual environments1.2 ANALYTIC SIMULATIONS VERSUS VIRTUAL ENVIRONMENTSHistorically two classes of simulation applications have received the most attentionanalytic simulations and virtual environments. Characteristics that distinguish thesedifferent domains are summarized in Table 1.1.Analytic simulations usually attempt to capture detailed quantitative dataconcerning the system being simulated. For example, in the air traffic simulation,one might be interested in the average circling time for each aircraft when itarrives at a busy airport. In a telecommunication network simulation, one might beinterested in statistics such as the average delay to perform a file transfer or theamount of data per second transmitted through a typical connection. Analyticsimulations require that the model reproduce as exactly as possible actual systembehaviors so that the generated statistical results are valid. Analytic simulation is thealso more straightforward, because one often does not need to recover thecomputation on failed processors unless they are critical to the entire exercise.Reduced execution time may be important, particularly as one expands the exerciseto include more and more participants. As mentioned earlier, a training simulationmust be able to complete its computations and advance time in the simulatedworld as rapidly as time progresses in the real world, or the simulation will notappear realistic. This becomes impossible if the number of entities in the simulatedworld increases, unless additional computing resources can be brought together toexecute the simulation computations.In the remainder of this first chapter we set the stage of the rest of this book. Inparticular, we introduce terminology, applications, and certain background information such as hardware platforms and underlying technologies that are important tounderstand the chapters that follow.classical approach to simulation it has been used as long as electronic computershave been in existence. For example, the earliest machines were used to computeartillery shell trajectories.Analytic simulation typically includes limited, or no interaction, with humanparticipants or physical devices during the execution of the simulation program. Inmany cases users may merely analyze the statistics produced by the simulator afterexecution has been completed. Alternatively, the user may view an animation of thesystem being modeled, perhaps with an ability to pause the execution and changecertain parameter settings. Analytic simulations typically execute asfastaspossible, meaning that the simulation attempts to complete its computations as quicklyas it can. This could mean that the simulator advances faster than realtime forexample, it might simulate hours of system behavior in only minutes of elapsed timeto the user or that it runs slower than realtime. IA more recent phenomenon in simulation has been to create virtual environmentsinto which humans or devices are embedded. Perhaps the most familiar use ofsimulations for this purpose are video arcade games where the player, oftenrepresented by a character or avatar on the games display, is placed in acomputergenerated world representing a medieval castle or a modem race track.This world may be populated by other characters representing other humans or bycomputergenerated characters whose behaviors and actions are represented byprograms within the simulator. The battlefield training exercises alluded to earlierare an example of a multiple user virtual environment simulation widely used by themilitary today. A variation on this theme is to embed into the virtual environmentactual physical components, possibly in addition to human participants. This is oftenused to test the component, for example, to test a missile defense system forscenarios that might be difficult andor expensive to create with live range tests.Virtual environment simulations with human participants are sometimes referred toas humanintheloop or manintheloop simulations, and simulations includingembedded physical devices are also called hardwareintheloop simulations.Virtual environment simulations differ from traditional analytic simulations inseveral important ways. First, they almost always include human participants oractual physical devices as entities within the simulation itself, as opposed to externalusers viewing or artificially manipulating the simulation as desribed earlier foranalytic simulations. Thus it is important that the simulated world advance in time atapproximately the same rate that time advances are perceived by the humanparticipants. The central goal in most virtual environment simulation to date hasbeen to give users the look and feel ofbeing embedded in the system being modeled.As such, it is not always essential for these simulations to exactly emulate the actualsystem. If the differences between the simulated world and the actual world are notperceptible to human participants, this is usually acceptable. For example, if twoevents occur close enough in time that the human cannot perceive which occurredI More precise meanings of terms such as simulated time, realtime, and wallc10ck time wiJI be given inChapter 2. For now, suffice it to say that simulated time is the simulations representation of time in thesystem being modeled, and realtime corresponds to time during the execution of the simulation program.RealtimeCreate a realistic andor entertammgrepresentation of an environmentHumans integral to controlling thebehavior of entities within the modelVirtual EnvironmentsNeed only reproduce beforeandafterrelationships to the extent thathumans or physical componentsembedded in the environment canperceive themTypically asfastaspossibleQuantitative analysis ofcomplex systemsIf included, human is anexternal observer to themodelAttempt to preciselyreproduce beforeandafter relationshipsAnalytic SimulationsHuman interactionExecution pacingTypical objectiveBeforeandafterrelationships8 BACKGROUND AND APPLICATIONS 1.3 HISTORICAL PERSPECTIVE 9first, it may be acceptable for the simulated world to model these events as occurringin an order that differs from that in which they would actually occur in the realsystem.On the other hand, analytic simulations usually require that the simulatorcorrectly reproduce the orderings ofevents, especially ifthere is a causal relationshipbetween them, for example, an observer should see a weapon fire before it sees thatthe target has been destroyed. Failing to do so may compromise the statistics that arecollected, especially if there is a bias in the outcomes that one observes. Forexample, if tanks for the red army are always observed to fire first when a red and ablue tank fire at approximately the same time, the kill statistics will be biased infavor of the red army.In summary, it is useful to distinguish between analytic and virtual environmentsimulations because they have different objectives, leading to different requirementsand constraints. To some extent, this dichotomy is historic, as much of the basicresearch in these applications has been conducted by different communities. We usethis dichotomy here because these two areas have given rise to different problemsand solutions.1.3 HISTORICAL PERSPECTIVEThe bulk of the research in parallel and distributed simulation has evolved from threeseparate communities. Work in parallel and distributed simulation techniques foranalytic simulation applications grew largely out of the highperformance computingcommunity, which maintained the goal of reducing the execution time of simulationcomputations. Work in distributed virtual environments DVEs grew largely fromtwo separate camps. On one hand, the military establishment can be credited with thedevelopment of sophisticated, though costly, geographically distributed virtualenvironments for training applications. At approximately the same time, much ofthe work originating in the interactive gaming and Internet communities wasfocusing on more economical DVEs that could be used by players on personalcomputers linked via public networks.1.3.1 HighPerformance Computing CommunityParallel and distributed simulation technologies for analytic simulation applicationsoriginated largely from basic research conducted in universities and researchlaboratories in the late 1970s and throughout the 1980s. This research has flourishedin the 1990s. Work in this field began with the development of synchronizationalgorithms to ensure that when the simulation is distributed across multiplecomputers, the same results are produced as when the simulation is executed on asingle machine. Although the first of these synchronization algorithms werepublished in 1977, few implementations on parallel and distributed computersappeared over the next ten years, perhaps due to limited availability of suitablehardware platforms and primitive software development environments on the fewplatforms that were available. Simulation of queuing networks an example of whichwill be presented in Chapter 2 was popular for many years. To some extent, thiscontinues to be a popular benchmark for evaluating the performance of parallelsimulation techniques. Queueing network simulations remain popular benchmarksbecause they require little knowledge of a specific problem domain, they can becoded very quickly, and they are representative of many important applicationdomains, for example, simulation of telecommunication networks and commercialair traffic.Results from other applications began to appear in 1990. Noteworthy amongthese was work in the TWOS project at the Jet Propulsion Laboratory JPL indeveloping parallel war game simulations for the United States Department ofDefense. As elaborated upon below, subsequent work has also focused on applications such as telecommunication networks, transportation systems, and digital logiccircuits. At the time of this writing, work in the field has largely been confined touniversities and industrial research laboratories, with application to a handful ofrealworld military and commercial simulation problems reported. With inclusion ofthese techniques in the U.S. Department of Defense DoD High Level ArchitectureHLA effort, which will be discussed below and in greater depth in Part III of thisbook, impact of this technology is accelerating in the late 1990s, especially formilitary applications.1.3.2 Defense CommunityMuch of the work in distributed simulation for virtual environments began in the1980s, largely independent of the work described above concerning analyticsimulations. Compared to the sluggish infiltration of paralleldistributed simulationtechnology for analytic applications into realworld usage, technology transfer ofdistributed simulation for virtual environments has been rapid, and perhaps mostnotably, has gained widespread acceptance in military establishments and theentertainment industry. A key factor driving the development and adoption ofdistributed simulations for synthetic environments has been the need for the militaryto develop more effective and economical means of training personnel prior todeployment. Field exercises are extremely costly activities, and thus can only beutilized rather sparingly. It is clear that embedding personnel in a virtual environmentprovides a much more costeffective, not to mention safer and environmentallyfriendlier, training facility. This has driven a large amount of RD effort in theUnited States, Western Europe, and other parts of the world toward development ofthe technologies to realize such a capability.Early work in distributed simulation for virtual environments for the militarybegan with the SIMNET SIMulator NETworking project that extended from 1983to 1989. Sponsored by the Defense Advanced Research Projects Agency DARPAin the United States, SIMNET demonstrated the viability of interconnectingautonomous simulators for example, tank simulators in training exercises, and ithas since been deployed for use in actual as opposed to experimental training. The10 BACKGROUND AND APPLICATIONS 1.4 APPLICATIONS 11success of the SIMNET experiment has had farreaching effects throughout thedefense modeling and simulation community in the United States. SIMNET wasreplaced by what came to be known as Distributed Interactive Simulation DISwhere standards were defined to support interoperability among autonomous trainingsimulators in geographically distributed simulation environments.A second major development springing from SIMNET was the Aggregate LevelSimulation Protocol ALSP work that applied the SIMNET concept of interoperability to war game simulations. ALSP enabled war game simulations from theArmy, Air Force, and Navy, for example, to be brought together in a single exerciseto analyze joint military operations. ALSP used synchronization protocols discussedearlier for analytic simulations it represents perhaps the most extensive applicationof that technology to date. Work in the ALSP community proceeded concurrentlywith work in the DIS community that was focused primarily though not exclusivelyon training.The next major milestone in the evolution of this technology was the development of the High Level Architecture HLA which began in 1995 and resulted in thesocalled baseline definition in August 1996. HLA is important for several reasons.From a practical standpoint, HLA was mandated in September 1996 as the standardarchitecture for all modeling and simulation activities in the Department of Defensein the United States. All DoD simulations are required to become HLA compliant orobtain approval for an exception to this requirement by 1999. From a technicalstandpoint, HLA is important because it provides a single architecture that spansboth analytic and virtual environment simulations. In some respects it can be viewedas a merging of DIS and ALSP into a single architecture. Prior to the HLA effort,work in the paralleljdistributed analytic simulation community and the distributedvirtual environment communities proceeded largely independent of each other. HLAwas a landmark effort in that it began integrating these technologies in a significantway. At the time of this writing, the initial baseline definition of the HLA and itsrealization in prototype versions of the Runtime Infrastructure RTI have beencompleted and standardization activities are in progress. Migration of DIS standardsto the HLA is also under way.1.3.3 Interactive Gaming and Internet CommunitiesA second major thread of activity in distributed virtual environments for nonmilitaryapplications grew from the interactive gaming and Internet communities. Just asdefense simulations originated from platformlevel simulators for tanks andaircraft, nonmilitary DVE work originated in immersive games such as Adventureand Dungeons and Dragons. Adventure was a fantasy computer game created atXerox Palo Alto Research Center PARC in California in the mid1970s. In it auserplayer explored a rich computergenerated fantasy world, most of which wasunderground in a maze of caves and hidden passage ways. Adventure was a textbased game where users typed short phrases to describe their actions for example,move up, and were given word descriptions of objects and rooms theyencountered in their journey. This fantasy world was complete with a rich varietyof hidden treasures and a wide assortment of other computergenerated creatures,both friend and foe, that could help or hinder the player from finding and obtainingthe treasures. A skilled player could slay harmful adversaries such as dragons withvarious weapons, such as swords and magic potions giving the partaker specialabilities for a limited amount of time. These weapons could be found in differentareas of the virtual world. Adventure was developed in the 1970s and 1980s beforepowerful personal computers were widely available. Yet despite the limited interaction allowed by a textoriented program, it was a very popular game among thecollege students who were lucky enough to have computer access. Computer andvideo games of this nature continue to thrive today, greatly enhanced with audioeffects and computer graphics.Adventure was a singleplayer game. A second, key ingredient in the development of DVEs was the introduction of multiple players to the virtual world. Thoughnot initially computerized, the popular game of Dungeons and Dragons, also fromthe mid1970s, is credited with being the catalyst for this development. This was apencil and paper roleplaying game where players gathered to play out roles asknights and sorcerers in a madeup world created by one of the players, referred to asthe dungeon master. The actual environment could be as simple as a writtendescription of the various portions of the virtual world, or as elaborate as scalemodels.Computergenerated fantasy games and multiple usersplayers came together inthe early 1980s with the MultiUser Dungeon MUD game developed at theUniversity of Essex in England. Today, the term MUD is associated with multiplayergames of this sort in general, as opposed to any particular game. Further theapplications for DVEs extend far beyond games, and a substantial amount of workhas been geared toward nongaming applications.In addition to computergenerated virtual worlds and the inclusion of multipleplayers, a third critical ingredient in the development of DVEs was the unprecedented expansion and growth of the worldwide network of computer networksknown as the Internet. With the Internet a virtual environment can support multipleusers who may be scattered around the globe. Multiplayer games with geographically distributed players are flourishing in the 1990s, despite limited bandwidth forexample modem lines with as little as 9600 bits per second and relatively highnetwork latencies. Continued increases in modem bandwidth at the time of thiswriting in the late 1990s, 50 Kbitssecond modems are becoming widely availableand megabit per second bandwidths for example, via cable modems or othertechnologies are on their way. The communication bottlenecks that have hithertorestricted widespread use of distributed virtual environments may be a thing of thepast.1.4 APPLICATIONSWith the above historical context, we now survey some of the applications whereparallel and distributed simulation technologies have been applied. While far from12 BACKGROUND AND APPLICATIONS 1.4 APPLICATIONS 13being complete, this list gives a flavor of some of the current and potential uses ofthe technology.1.4.1 Military ApplicationsIt is clear that the military establishment has had a major role in developingdistributed simulation technology for virtual environments, and to a lesser thoughstill significant extent, parallel simulation technology for analytic simulationapplications. Some of the most prominent military applications utilizing thistechnology are as follows1. War gaming simulations. These simulations are often used to evaluate differentstrategies for attacking or defending against an opposing force, or foracquisition decisions to determine the number and type of weapon systemsthat should be purchased to be prepared for future engagements. Thesimulation is typically composed of models for battalions, divisions, and soforth. Because these simulations usually model groups of units rather thanindividual platforms for example, aircraft and tanks, they are sometimes alsoreferred to as aggregated simulations. Two noteworthy examples of theapplication of parallel discrete event simulation techniques to war gamesimulations are the Concurrent Theater Level Simulation CTLS Wieland,Hawley et al. 1989 and Aggregate Level Simulation Protocol ALSP Wilsonand Weatherly 1994 discussed earlier. The underlying execution mechanismfor CTLS was a parallel simulation executive using a synchronizationalgorithm called Time Warp. ALSP used another algorithm called theChandyjMisrajBryant null message protocol. These algorithms will bediscussed in detail in Chapters 4 and 3, respectively.2. Training environments. As discussed earlier, these simulations embed pilots,tank operators, commanding officers and their staffs, and the like, into anenvironment to train personnel for actual combat. In contrast to aggregatedsimulations, many training environments use platformlevel simulations thatdo model individual tanks, aircraft, and so forth.3. Test and evaluation TE. While training simulations embed humans into asynthetic battlefield, TE simulations embed physical components forexample, a new sensor for detecting missile launches into a virtual environment, often to evaluate the effectiveness of proposed new devices or to verifythat manufactured devices operate at reported specifications. The TEsimulation community has sometimes been referred to as the ConsumerReports for the military because they evaluate new products before they aremanufactured and eventually deployed.As discussed earlier, the High Level Architecture effort attempts to integratesimulations from these three domains in order to facilitate reuse of simulationmodels in new contexts, thereby reducing the cost of developing new simulators.1.4.2 EntertainmentThe number of real F15 pilots that can benefit from immersion into a computergenerated dogfight is dwarfed by the number of wannabe pilots that are looking forrecreation on a Saturday night. Application of distributed simulation technology tothe entertainment industry will and already is leading to the most significant impactof this technology on the average citizen. Singleplayer video arcade games cannotprovide the same kind of entertainment as interactively competing with friends orstrangers in a computergenerated virtual world.Distributed simulation technology can be applied in amusement park and arcadecenters where players are colocated but interact with each other and computergenerated entities over a local area network. These systems sometimes use costlycustomdesigned hardware that can only be justified economically by repeated useby many users. Another emerging market is the multiuser home entertainmentindustry where video game machines or personal computers are interconnectedthrough the Internet.Entertainment and training systems employing distributed simulation technologies have much in common, but they also differ in many important respects.Obviously, entertainment systems must be engaging. Unlike training simulators,one does not have a captive audience where players must return to fulfill jobrequirements. Thus the realism of the virtual environment may take second placeto pure excitement and artistic effects. Economic factors playa much more dominantrole in the design of entertainment systems, sometimes requiring compromises thatwould not be necessary in a multimillion dollar training system. Interoperabilityamong separately developed simulations is a fundamental goal in DIS, but it may beviewed as undesirable by some in the entertainment industry. This would be true, forexample, when a company marketing proprietary entertainment systems has control,as a single vendor, over the simulations that will be included in the system.1.4.3 Social Interactions and Business CollaborationsAnother potentially farreaching impact of distributed virtual environments IS mcreating new means for people to interact socially on the Internet. The Internet hasalready made fundamental changes in the way people interact both in the office andat home. Many believe DVEs represent the next logical step in electronic socialinteractions. Already users around the world can meet without ever leaving theirown home through Internet newsgroups and chat rooms. Beyond this, a DVEapplication can create more realistic social settings such as the one known asDiamond Park developed by Mitsubishi Electric Corps MERL research laboratory.Diamond Park provides a virtual park atmosphere where users can meet and interactin various settings such as the parks cafe, walkways, or meeting areas Waters andBarrus 1997. Users can navigate through the park on foot or on bicycle and caneven race against each other Virtual environments like this may be the norm in thefuture for social interaction via the Internet.14 BACKGROUND AND APPLICATIONSVirtual environments can also provide a new means for interactions in thebusiness world between colleagues and clients. Entire virtual corporationscould be created, composed of employees who are based physically at differentlocations or different companies but who are working together on a joint venture. Forexample, one can envision building designers and engineers at different locationswalking through a virtual design of a product a building to discuss and evaluatedesign changes.1.4.4 Education and TrainingNonmilitary applications for DVEs in education and training abound. Much workhas been accomplished in the medical community using virtual environments fortraining as well as treatment of patients. Computergenerated environments canprovide a more costeffective and safe means for doctors to practice surgicaltechniques. Experimental studies have been performedconducted using virtualenvironments to treat patients with various phobias such as a fear of heights.Patients are exposed gradually and in a controlled way to virtual situations thatcause them anxiety. While much of the work to date in these areas has been focusedon singleuser virtual environments i.e., not distributed, extensions to DVEs toallow for users to remain in different geographic locations are clear. Work has alsofocused on using DVE technology developed under DIS for nonmilitary applications, such as training air traffic controllers, or performing exercise drills foremergency procedures, such as recovery from earthquakes or major accidents.1.4.5 Telecommunication NetworksAnalytic simUlations have long been used in the telecommunications industry toevaluate networking hardware, software, protocols, and services. The widespreaddeployment of fiber optics technology has had important impacts on the use ofsimulation in modeling networks. First, this technology has brought about increaseduse of telecommunication networks for applications other than voice communications, namely transmission of still images, data, and video. Socalled BroadbandIntegrated Services Digital Networks BISDN provide a single networking infrastructure to carry these diverse types of traffic. Network designers have had to totallyrethink their designs, and tum toward simulation tools to aid them. Networkingtechnologies such as Asynchronous Transfer Mode or ATM2 have emerged to meetthe challenge of supporting these diverse types of traffic on a single networkinfrastructure, for example, see Partridge 1993.Second, because the underlying network is based on fiber optic links that cancarry orders of magnitude more traffic than copper cables, simulations become moretimeconsuming. This is because one must often model the network for at least theduration of a conversation to collect useful data that is, simulations of minutes to2An unfortunate acronynm, this technology has nothing to do with Automated Teller Machines used in thebaking industry.1.4 APPLICAnONS 15hours of network operation are required. Because BISDN networks carry orders ofmagnitude more traffic during this time period, the computation time to complete thesimulation increases in proportion. Parallel simulation techniques offer one approachtoward alleviating this problem.A typical problem in telecommunications that calls out for te use of parallelsimulation is that of analyzing cell losses in ATM networks. A cell IS a 53byte blckf data that is the basic unit transmitted through an ATM network. Each ATM SWitchoontains buffers to hold cells waiting to be transmitted on links. If a link becomesongested and the buffers become full, subsequent cells that rquire ue f that linkare discarded. The cell loss proability is an important metrIc th.a .mdicates h,frequently this happens. ATM SWitches often target cell loss prob.aIhtIes t be 1 ,that is, only one in 109 cells is lost under anticipated traffic .co.nditIOns. ThiS reqUIressimulation of at least 1011 cell arrivals to obtain reliable statistIcal data. As of the late1990s fast sequential simulation will execute on the order of 105 events per second,so otimistically equating simulation of eah cell arival with  single e.ven, such asimulation will require more than 11 days, Just to Simulate a smgle SWitch.A second major area where parallel simulation may have a signifiant impact is inthe simulation of large networks such as the Internet. Here, highperformancesimulation engines are required because of the large number of entities that mustbe simulated. Simulations of millions of mobile subscribers are sometimes needed.1.4.6 Digital Logic Circuits and Computer SystemsLike telecommunication networks, simulations of digital electronic circuits andcomputer systems is a second area where parallel simulation can play a significtrole. Fast simulation of logic circuits is of considerable interest to the electrolllccomputeraideddesign community because simulation is a major ottleneck in hedesign cycle. Final verification of a computer system may reqUIre weeks usmgconventional sequential simulation techniques.Much of the work in applying parallel simulation techniques to logic circuits hasbeen focused on the VHDL hardware description language that has become widelyused in industry. Several prototype parallel simulation systems have bee develoedthat execute VHDL programs on multiple processor computers, With varymgdegrees of success reported in the literature. Successful demonstrations typicallyreport up to an order of magnitude reduction in execution tie.. .. .,While socalled gatelevel logic simulations focus on modelmg mdividual CirCUItsfor implementing primitive Boolean functions and storage elements, h.igherlevelsimulations of computers using models for switches, processors, memOnes, and soforth are also used extensively in preliminary investigations of design alternatives.Thes higherlevel simulations often include simulated exctions of bencharkprograms on the modeled machine to evaluate it under eahstIc worloads. Directexecution is a technique where the benchmark program IS executed directly on themachine used to perform the simulation rather than use a mch slower softwareinterpreter. Prototype parallel simulation systems .using .technlques sUh. as thesehave been demonstrated to yield very accurate SimulatIon results, withm a few16 BACKGROUND AND APPLICATIONS 1.6 HARDWARE PLATFORMS 17percet of measurements from a realized system, while delivering up to an order ofmagllltude reduction in computation time.1.6 HARDWARE PLATFORMSThe hardware platforms of interest here contain a potentially large number ofprocessors interconnected through a communication network. In most cases theprocessor is a general purpose CPU central processing unit, often identical to thosecommonly found in personal computers and engineering workstations. The switching network may be as specific as a customized switch for a particular multiprocessor system, or as general as the Internet.1.6.1 Parallel versus Distributed ComputersMultipleCPU hardware platforms can be broadly classified into two categoriesparallel and distributed computers. Differences between these platforms are summarized in Table 1.2. Parallel and distributed computing platforms are distinguished bythe physical area occupied by the computer. The processors in parallel computersare in close physical proximity, usually within a single cabinet, or a small number ofadjacent cabinets in a machine room. These are usually homogeneous machines,using processors from a single manufacturer. These machines normally provideswitching hardware tailored to the parallel computer, so the delay in transmitting amessage from one computer to another referred to as the communication latency isrelatively low. This latency is typically a few microseconds to tens of microsecondsfor a message containing a few bytes in contemporary machines. Latency isimportant because it has a large impact on performance if latencies are large, thecomputers may spend much of their time waiting for messages to be delivered. Here,communication latency is perhaps the single most important technical aspectdifferentiating parallel and distributed computers. There are three principal classesof parallel computers that are in use today sharedmemory multiprocessors,distributed memory multicomputers, and SIMD machines see Fig. 1.1, as will beelaborated upon momentarily.Distributed computers cover a much broader geographic area. Their extent maybe confined to a single building, or may be as broad as across an entire nation oreven the world. Unlike parallel computers, each node of a distributed computer isusually a standalone machine that includes its own memory and IO devices.Commercial offtheshelf personal computers or engineering workstations, oftenfrom different manufacturers, are usually used. Communication latencies are usuallyTABLE 1.2 Contrasting parallel and distributed computers1.4.7 TransportationSimulation can play an important role in designing and managing road and airtransportation systems. It can be used as an analysis tool to evaluate the effectivenessof adding a new runway to an airport, or rerouting vehicular traffic after thecompletion of a major sporting event. As alluded to earlier, it may be used onlinein developing strategies to respond to an unexpected event, for example, congestionresulting from adverse weather conditions.1.5 UNDERLYING TECHNOLOGIESParallel and distributed simulation is made possible by the confluence of threeessential, underlying technologies Integrated circuits. The first key ingredient is an inexpensive computer, therebymaking systems composed of tens, hundreds, or even thousands of computerseconomically feasible. Fundamental to this development are steadily decreasing costs of integrated circuits, driven largely by an increasing ability tosqueeze more and more circuits onto a single silicon chip. For example, thecost of random access memory RAM that accounts for a significant portionof the cost of a personal computer or workstation has, over the long term,decreased by 40 per year Hennessy and Patterson 1996. Highspeed intercomputer communications. There are two flavors of technology at work here. On the one hand, highspeed switches enable one toconstruct systems containing tens to hundreds or even thousands of processorsthat reside within a single cabinet or computer room. On the other hand,advances in fiber optics technology is fueling a revolution in the telecommunications industry, making possible computing systems distributed acrosscontinents. These advances enable one to consider developing computerapplications utilizing many geographically distributed machines. Modeling and simulation. The final ingredient are technologies to enableconstruction of models of actual or envisioned realworld systems that can 1be represented in the internal storage of a computer, and 2 be manipulated bycomputer programs to emulate the evolution of the actual system over time.Here, we are primarily concerned with discrete event simulation, wherechanges in the state of the simulation are viewed as occurring at distinctpoints in time.. In many applications other technologies such as graphics, humancomputermterfaces, and databases clearly play a critical role. However, these technologiesare somewhat tangential to the focus ofthis book, and are not discussed further here.Physical extentProcessorsCommunication networkCommunication latencyParallel ComputersMachine roomHomogeneousCustomized switchLess than 100 microsecondsDistributed ComputersSingle building to globalOften heterogeneousCommercial LAN or WANHundreds of microsecondsto seconds18 BACKGROUND AND APPLICATIONS1.6 HARDWARE PLATFORMS 19Hardware PlatformsDlbl COPnetworkedworkstationsCPUcache  CPUcacheCPUcacheSimulations executing on distributed computers are referred to as distributedsimulations. Distributed simulations may be used for analytic purposes, or morecommonly for constructing distributed virtual environments. The latter is perhapsthe more common application for distributed simulation technology, and the termdistributed simulation in the literature sometimes refers exclusively to distributedvirtual environments.1.6.2 SharedMemory MultiprocessorsSharedmemory multiprocessors, distributed memory multicomputers, and SIMDmachines provide different programming models to the application. The distinguishing property of the programming model for sharedmemory multiprocessors is onemay define variables that are accessible by different processors. Thus one can definea variable X that can be autonomously read or modified by one processor without theintervention of another.Shared variables and message passing are the two dominant forms of interprocessor communications used in parallel and distributed simulation. Messagepassingmechanisms, widely used in parallel simulation, can be implemented using sharedmemory by defining shared data structures queues to hold incoming or outgoingmessages.One type of sharedmemory machine, the symmetric multiprocessor SMP, hasbecome increasingly popular. A typical sharedmemory machine is depicted inFigure 1.2. These systems consist of offtheshelf microprocessors connected tomemory through a highspeed switch, such as a bus. Frequently accessed instructions and data are stored in a highspeed cache memory that is attached to eachprocessor. Typically the multiprocessor hardware automatically moves data andinstructions between the cache and main memories, so the programmer need notbe concerned with its operation, except perhaps to tune the program to maximizeperformance. Consistency protocols are required to ensure that multiple copies ofany shared variable residing in different caches remain uptodate if one copy ismodified this is often realized in the hardware by either invalidating or updatingcopies residing in other caches when one copy is changed. The Sun Enterprisesystem is an example of a contemporary SMP. Personal computers PCs containingFigure 1.2 Block diagram of a typical sharedmemory multiprocessor.Parallel ComputerssharIMDmey I mchinesdistributedmemorymulticomputersFigure 1.1 Taxonomy of important classes of parallel and distributed computers.on the order of a few hundreds of microseconds for distributed computers withprocessors in close proximity for example, a single building, but they may be aslarge as hundreds of milliseconds or even seconds for machines covering largegeographical areas. In the latter case, satellites may be used for some communicationlinks, contributing to increased latency. The latency ofdistributed computers is muchhigher than parallel machines because l signals must traverse large physicaldistances and 2 complex software protocols designed for interconnecting autonomous computers from different manufacturers are usually used rather thancustomized hardware and software designed for a specific interconnection scheme.While technological advances may be able to substantially reduce software overheads for communication, latency between geographically distributed machines isfundamentally limited by the speed of light, which is approximately 2.1 x 108 metersper second in optical fiber, or 210 kilometers 131 miles per millisecond. A modemmicroprocessor such as that included in personal computers can execute tens tohundreds of thousands of machine instructions where each instruction can performa simple operation such as an integer addition in one millisecond, so this is asubstantial amount of time from a computing perspective.Recently the distinction between parallel and distributed computers has becomeblurred with the advent of the network of workstations, which is a cluster ofworkstations interconnected through a highspeed switch usually confined to a singleroom. Through the use of new switching techniques that bypass traditionalcommunication protocols, communication latency of these machines approach thatof conventional parallel computers. Are these parallel or distributed computersHere, because of the close physical proximity of the machines, they are characterizedas parallel computers, though often these systems are classified as distributedmachines. Because these machines often include aspects common to both paralleland distributed computers, the characterization is perhaps not so important.Simulations that execute on shared memory multiprocessors, multicomputers, orSIMD machines are referred to as parallel simulation programs. The focus of thisbook with respect to parallel simulations is on discrete event simulations discussedin Chapter 2 used for analysis. The field concerned with this subject is calledparallel discrete event simulation PDES. Here, the terms parallel simulation andparallel discrete event simulation will be used synonymously.20 BACKGROUND AND APPLICATIONS 1.6 HARDWARE PLATFORMS 21multiple CPUs in an SMP organization are also becoming common. Most SMPsonly support a limited number of processors for example, up to 20 or 30, althoughlarger sharedmemory machines containing hundreds of processors have beenconstructed in the past.A second class of sharedmemory multiprocessor is the socalled nonuniformmemory access NUMA machine. These machines are typically constructed byproviding memory with each processor as opposed to separate memory modules asshown in Fig. 1.2 but allow each processor to directly read or write the memoryattached to another processor as well as its own memory. Unlike symmetricmultiprocessors, the programmers interface to these machines distinguishes betweenlocal and remote memory, and provides faster access to local memory. Thismakes these machines more difficult to program than socalled uniform memoryaccess UMA machines where the average access time to any memory location isthe same. This is because in a NUMA machine the programmer must carefully mapprogram variables to memory modules in order to minimize remote references, orelse pay a severe performance penalty in making frequent accesses to remotememory. Access to remote memory typically takes an order of magnitude ormore longer than an access to a local memory location. The Silicon GraphicsOrigin multiprocessor is one example of a NUMA machine.1.6.3 DistributedMemory MulticomputersMulticomputers do not support shared variables. Rather, all communicationsbetween processors must occur via message passing. Messagepassing librariesare provided to send and receive messages between processors. Examples includethe Cray T3D, NCubeTen, and Intel Paragon. Large multicomputers may containhundreds of processors.A block diagram for a typical distributed memory multicomputer is shown inFigure 1.3. Each node of the network is not unlike that found in personalcomputers it includes a CPU, cache memory, and a communications controller thathandles interprocessor communication. Unlike the cache used in sharedmemorymultiprocessors, the cache in each multicomputer node only holds instructions anddata for the local processor, so no cache coherence protocol is needed. The memory. . .interconnection networkFigure 1.3 Block diagram of a typical distributedmemory multicomputer.in each node can only be accessed by the CPU in that node. The communicationscontroller is responsible for sending and receiving messages between nodes andtypically transfers messages directly between that nodes memory and the interconnection network. Such transfers that usually do not require the intervention of theCPU, except to initiate the transfer or to be notified when the transfer is completed,are referred to as direct memory access DMA operations.In principle, one could implement sharedmemory operations in software on topof a distributedmemory architecture, providing the illusion to the programmer ofhaving shared memory. For example, read and write operations could be implemented by sending messages to and from the processor on which a program variableresides, and copies of frequently referenced remote memory locations can be kept inthe local processor with software used to maintain coherence. Software systems thatsupport this capability are often referred to as distributed sharedmemory DSMsystems. Because these mechanisms must be implemented in software, however, theoverhead associated with managing the memory system in this way may beprohibitive.The distinction between sharedmemory multiprocessors and multicomputers isimportant because the common address space provided by shared memory machinesallows global data structures referenced by more than one processor to be used theseare not so easily implemented in distributedmemory multicomputers. Also, differentmemory management techniques may be used in sharedmemory machines. Forexample, as will be seen in Chapter 5, it is possible to define memory managementprotocols in sharedmemory computers that use, to within a constant factor, the sameamount of memory as a sequential execution of the program. Such techniques havenot yet been developed for distributedmemory machines.1.6.4 SIMD MachinesSIMD stands for singleinstructionstream, multipledatastream. The central characteristic of these machines is that all processors must execute the same instructionbut using different data at any instant in the programs execution. Typically thesemachines execute in lockstep, synchronous to a global clock. This means allprocessors must complete execution ofthe current instruction some may choose notto execute that instruction before any is allowed to proceed to the next instruction.Actually lockstep execution need not be strictly adhered to so long as the machinesappears to the application that it operates in this fashion. Lockstep execution and theconstaint that all processors must execute the same instruction distinguish thesemachmes from the others, socalled MIMD multipleinstructionstream, multipledatastream computers that are described here.A block diagram for a typical SIMD machine is depicted in Figure 104. Theontrol unit fetches instructions from the control memory, and then broadcasts thatmstruction to each of the processing elements. Each processing element contains anALU, registers, and control logic to implement machine instructions LOAD,STORE, ADD, etc.. Upon receiving an instruction from the control unit theprocessing element executes that instruction, possibly accessing its local data22 BACKGROUND AND APPLICATIONS 1.7 SUMMARY 23IinstructionImemoryII control unit II I .processing processing processingelement element elementdata data    datamemory memory memoryI I Iinterconnection networkFigure 1.4 Block diagram of a typical SIMD machine.memory or the interconnection network depending on the type of instruction. Forexample, a vector ADD operation might be performed by distributing the vectoracross all of the data memories, then having each processing element execute twoLOAD instructions to fetch an element from each of the arrays into its local registers,next an ADD instruction to add the data values and store the result into another localregister, and finally a STORE instruction to write the result into a location in the datamemory.SIMD machines typically contain more, albeit simpler, processors processingelements than either multiprocessors or multicomputers. Because they are simplerthan complete microprocessors, customdesigned components are usually usedrather than offtheshelf parts.Because all processors in an SIMD machine must execute the same instruction,these machines are more specialized than MIMD machines. The bulk of the effort inparallel simulation has been on MIMD machines, but some techniques are applicable on SIMD machines as well.1.6.5 Distributed ComputersTwo characteristics that distinguish distributed computers from parallel machines areheterogeneity and the network used to interconnect the machines. Unlike parallelcomputers, distributed systems are often composed of standalone computer workstations from different manufacturers. Unixbased workstations for example, Sun,Silicon Graphics, DEC, or IBM workstations, and personal computers are mostcommonly used for distributed simulations today. Heterogeneity is importantbecause many distributed simulators are constructed by interconnecting existingsequential simulators for example, tank simulators in DIS operating on specificworkstations. Heterogeneity eliminates the need to port existing simulators to newplatforms, and it enables participation of users with different computing equipmentin distributed simulation exercises.While parallel computers use interconnection switches customized for theprocessors that they are interconnecting, distributed computers use general interconnects based on widely accepted telecommunication standards such as asynchronous transfer mode ATM or Ethernet for interconnecting equipment from differentmanufacturers. The price of generality is performance, as complex software protocols inflate communication latencies to be one to two orders of magnitude largerthan that of parallel machines, even for equipment in close physical proximity ofeach other. One can expect that this gap will be reduced in the future, however,blurring the distinction between parallel and distributed machines.Different distributed computers are distinguished by the geographical extentcovered by the system, which in tum dictates the type of network used tointerconnect the machines. Local area network LAN based systems consist of acollection of machines in a limited geographic area, such as within a single buildingor a university campus, interconnected through a highspeed network or switch.MAN metropolitan area network based systems have the physical extent of a city,and a WAN wide area network based systems may be distributed across a nation orthe world.1.7 SUMMARYParallel and distributed simulation technology can provide substantial benefit insituations such as the following1. Time critical applications where simulations are used as decision aids forexample, how do I reroute air traffic, and results are needed on very shortnotice.2. Design of large andor complex systems where execution of the simulationprogram is excessively timeconsuming.3. Virtual environments such as for training, where participants andor resourcesare at geographically distant locations.A variety of applications ranging from training to entertainment to the design of thenext generation of the Internet are identified as targets for this technology.Simulation applications were broadly divided into two categories analyticsimulations and distributed virtual environments. This distinction is importantbecause each domain presents different requirements and technical challenges.Dividing simulations applications in this way also provides a convenient meansfor separating work in parallel and distributed simulation technology. Part II of thisbook focuses on analytic applications, while part III focuses on distributed virtualenvironments. Finally the underlying computing platform can be broadly classifiedas being a parallel or distributed computer, with geographical distribution of the24 BACKGROUND AND APPLICATIONS 1.8 ADDITIONAL READINGS 25processors used as the distinguishing characteristic. This distinction is importantbecause different communication latencies are implied, leading to different problemsand solutions. The distinction between parallel simulation and distributedsimulation that is used here is based on this latter categorization.1.8 ADDITIONAL READINGSNumerous conferences report recent advances in the parallel and distributedsimulation field. The ACMjIEEEjSCS Workshop on Parallel and DistributedSimulation PADS is the premier conference in the parallel discrete event simulationarea for analytic simulation applications and has been operating since its inceptionin 1985, but results also appear in a variety of other conferences, including theWinter Simulation Conference, and the Summer Computer Simulation Conferenceand the Annual Simulation Symposium. The ACM Transactions on Modeling andComputer Simulation is a journal that includes articles concerning parallel anddistributed simulation technology.The literature contains numerous studies of applying parallel discrete eventsimulation techniques to specific applications. A few relevant references are listedbelow Battlefield simulation Wieland, Hawley et al. 1989 Morse 1990 Rich andMichelsen 1991 Steinman and Wieland 1994 Hiller and Hartrum 1997. Computer architectures Konas and Yew 1992 Bailey, Pagels et al. 1993Reinhardt, Hill et al. 1993 Agrawal, Choy et al. 1994 Konas and Yew 1994Shah, Ramachandran et al. 1994 Chandrasekaran and Hill 1996 Dickens,Heidelberger et al. 1996 direct execution is described in Fujimoto 1983. Digital logic circuits Su and Seitz 1989 Chamberlain and Franklin 1990 Lin,Lazowska et al. 1990 Briner 1991 Chung and Chung 1991 Soule and Gupta1991 Nandy and Loucks 1992 Willis and Siewiorek 1992 Manjikian andLoucks 1993 Sporrer and Bauer 1993 Chamberlain and Henderson 1994Costa, DeGloria et al. 1994 Kapp, Hartrum et al. 1995 Hering and Haupt1996 Keller, Rauber et al. 1996 Kim and Jean 1996 Krishnaswamy andBanneJjee 1996 Chen, Jha et al. 1997 Frohlich, Schlagenhaft et al. 1997Krishnaswamy, BaneJjee et al. 1997 see Bailey, Briner et al. 1994 for asurvey of this area. Ecological systems Ebling, DiLorento et al. 1989 Deelman and Szymanski1997 Glass, Livingston et al. 1997. Petri networks Kumar and Harous 1990 Nicol and Roy 1991 Thomas andZahoJjan 1991 Baccelli and Canales 1993. Telecommunication networks Mouftah and Sturgeon 1990 Phillips andCuthbert 1991 Tallieu and Verboven 1991 Earnshaw and Hind 1992Turner and Xu 1992 Chai and Ghosh 1993 Ronngren, Rajaei et al. 1994Carothers, Fujimoto et al. 1995 Unger, Gomes et al. 1995 Bagrodia, Chen etal. 1996 Hao, Wilson et al. 1996 Kumaran, Lubachevsky et al. 1996 Clearyand Tsai 1997. Transportation systems Merrifield, Richardson et al. 1990 Wieland, Blair etal. 1995 Mitre Corp. 1997 Wieland 1997.A readable introduction to DIS is provided in DIS Steering Committee 1994. TheAugust 1995 issue of the Proceedings of the IEEE includes several articlesconcerning DIS, and the March 1997 issue of IEEE Spectrum concerning distributedvirtual environments. A historical look at DIS is presented in Voss 1993. Severalconferences include papers on recent work in the field. The Simulation Interoperability Workshop SIW meets every six months in Orlando, Florida, and focuses ondistributed simulation for military applications. An important subject of this workshop concerns definition of standards to facilitate interoperability of simulationspreviously called the DIS Workshop, the IEEE standards were developed inassociation with the forerunner to this meeting. The Defense Modeling andSimulation Offices web cite httpj jwww.dmso.mil is the best source of information concerning the High Level Architecture, though numerous papers on the HLAappear in the SIW workshop. In a nonmilitary setting the bimonthly journalPresence Teleoperators and Virtual Environments published by The MIT pressincludes numerous articles concerning the development of virtual environments,human factors issues, and applications. Recent developments in the field appear inthe IEEEs Virtual Reality Annual International Symposium VRAIS.There are several good textbooks on parallel and distributed computer architectures. For example, Hennessy and Patterson 1996 covers sharedmemory andmessagepassing multicomputers as well as networked workstation platforms. Reedand Fujimoto 1987 is devoted to messagebased multicomputer networks andincludes a chapter on parallel discrete event simulation. Other books providing broadcoverage of parallel computer architecture include Stone 1990, Hwang 1993, andFlynn 1995, among others. A survey of techniques for implementing sharedmemory operations on distributedmemory computers is presented in Nitzberg andLo 1991.CHAPTER 2Discrete Event SimulationFundamentalsA simulation is a system that represents or emulates the behavior of another systemover time. In a computer simulation the system doing the emulating is a computerprogram. The system being emulated is called the physical system. The physicalsystem may be an actual, realized system, or it may only be a hypothetical one, forexample, one of several possible design alternatives that only exists in the mind of itsinventor.A physical system contains some notion of state that evolves over time, forexample, the number of aircraft waiting to land at an airport. The simulation mustprovide l a representation of the state of the physical system, 2 some means ofchanging this representation to model the evolution of the physical system, and 3some representation of time. To address l, computer simulations define a collectionof state variables, namely program variables specified in some highlevel programming language such as C or Java that represents the state of the physical system. Toaddress 2, changes in the state of the physical system are realized by the simulationprogram writing new values into these state variables. Finally, time in the physicalsystem is represented through an abstraction called simulation time. This isdiscussed next.2.1 TIMEThere are several different notions of time that are important when discussing asimulation. It is imperative to keep these concepts distinct, since this is perhaps oneof the greatest sources of confusion when beginning to learn about parallel anddistributed simulations.The following definitions will be used throughout1. Physical time refers to time in the physical system.2. Simulation time is an abstraction used by the simulation to model physicaltime. A more precise definition will be given momentarily.3. Wallclock time refers to time during the execution of the simulation program.2728 DISCRETE EVENT SIMULATION FUNDAMENTALS 2.2 REALTIME. SCALED REALTIME, AND ASFASTASPOSSIBLE EXECUTION 29A simulation program can usually obtain the current value of wallclock timeaccurate to some specifiable amount of error by reading a hardware clockmaintained by the operating system.To illustrate these different notions of time, consider a simulation of the transportation system in Atlanta during the 1996 summer Olympic games. Physical time forthis simulation extends from July 19 to August 4, 1996. Simulation time might berepresented in the simulation program by a double precision floating point numberwith each unit corresponding to a single day. In this case, simulation time advancesfrom 0.00 to 17.00 during each execution of the program. If the simulation programran for three hours on the afternoon of February 25, 1995, while planning for theOlympics, wallclock time might extend from 300 PM to 600 PM on that day.All three definitions of time make use of scales or axes to represent specificinstants of time and to define before and after relationships among these instants.Physical time and wallclock time are essentially the same as time used in theconventional sense, so we do not dwell upon their meaning here. Simulation time is anew concept that only exists in simulated worlds, and is defined as followsDefinition Simulation time is defined as a totally ordered set of values where eachvalue represents an instant of time in the physical system being modeled. Further, forany two values of simulation time TI representing physical time PI and T2representing P 2, if TI  T2, then PI occurs before P 2, and T2  TI  is equal toP2  PI K for some constant K. If TI  T2, then T1 is said to occur before T2,and if TI  T2, then TI is said to occur after T2The linear relationship between intervals of simulation time and intervals ofphysical time ensures durations of simulation time have a proper correspondence todurations in physical time.For any given simulation, one common, global simulation time scale is used thatis recognized by all components of the simulation, just as all countries in the worldrecognize Greenwich Mean Time. This ensures that all parts of the simulation have acommon understanding of before and after relationships among simulated actionsthat occur at specific instants of simulated time.2.2 REALTIME, SCALED REALTIME, AND ASFASTASPOSSIBLEEXECUTIONThe progression of simulation time during the execution of the simulation mayormay not have a direct relationship to the progression of wallclock time. Insimulations used for virtual environments, simulated time must be made to advancein synchrony with wallclock time, or the simulated environment will appearunrealistic. For example, if simulated time advanced more slowly than wallclocktime, the virtual environment would appear to be sluggish and unresponsive to useractions. In a training exercise, tasks would appear to human participants as being tooeasy because more time is allowed for the human to perform operations than wouldbe provided in the actual physical system. Similarly, if simulated time advancedmore rapidly than wallclock time, human participants would be at a disadvantage.Simulation executions where advances in simulation time are paced by wallclocktime are often referred to as realtime executions, and simulators designed to operatein this mode are called realtime simulators. Because of this relationship betweensimulation time and wallclock time in realtime simulations, the two are sometimesviewed as being synonymous in the distributed simulation literature. It is importantto keep these two concepts distinct, however.A variation on the above is scaled realtime execution. Here, simulation timeadvances faster or slower than wallclock time by some constant factor. For example,the simulation may be paced to advance two seconds of simulation time for eachsecond of wallclock time, making the simulation appear to run twice as fast as thereal world, not unlike pressing the fastforward button on a VCR. This might be donein a training session to skip over uninteresting parts of the exercise. Similarly onemight slow down the simulation by a certain constant factor to provide more detailedslow motion views ofother parts. A realtime execution is a special case of a scaledrealtime execution where the scale factor is set to one.Realtime and scaled realtime simulations use a mapping function to translatewallclock time to simulation time. Specifically, the following function can be used toconvert wallclock time to simulation timewhere Tw is a value of wallclock time, TStart is the simulation time at which thesimulation begins, TwStart is the wallclock time at the beginning of the simulation,and Scale is the scale factor. If Scale is 2, then the simulation runs twice as fast aswallclock time, namely advancing one second in wallclock time corresponds toadvancing two seconds in simulation time.In analytic simulations that do not include humans or physical devices ascomponents within the simulation, the progression of simulated time is usuallynot paced by wallclock time a unit of advance in simulated time could require a fewseconds in wallclock time during one part of the simulation, minutes in another part,or even hours in a third. These simulations are sometimes referred to as asfastaspossible simulations because one wishes to complete the execution of the simulationas quickly as possible, without any concern for maintaining a fixed relationshipbetween advances in simulation time and wallclock time. The relationship betweenthe rate of advance in simulation time with advances in wallclock time bears nosignificance, except to the extent that it provides a metric of how long the user has towait to get the results of the simulation.One can design simulation programs that can be used for either realtime or asfastaspossible executions. To accomplish this, one must augment an asfastaspossible simulation with a mechanism to pace its execution with wallclock time.This assumes, of course, that the unpaced execution advances simulation time fasterthan wallclock time advances it is easy to slow down a simulation, but often difficult30 DISCRETE EVENT SIMULATION FUNDAMENTALS 2.3 STATE CHANGES AND TIME FLOW MECHANISMS 31to speed one up The pacing mechanism merely introduces a waiting mechanism toprevent the simulation from advancing simulation time ahead of wallclock time bymore than some prescribed amount. This will be illustrated in the next section.2.3 STATE CHANGES AND TIME FLOW MECHANISMSThe discussion thus far has focused on temporal aspects concerning the execution ofthe simulation. A second, independent classification corresponds to the manner inwhich the state of the model changes as simulation time is advanced, sometimereferred to as the time flow mechanism. This classification of simulation models isdepicted in Figure 2.1. Simulation models may be broadly classified as continuous ordiscrete. In a continuous simulation, the state of the system is viewed as changingcontinuously over time. The behavior of the system is typically described as a set ofdifferential equations that describe how the system state changes as a function ofsimulation time. Typical examples of continuous simulation problems includemodeling weather or climatic conditions, airflow around an aircraft wing, or changesin voltage on wires in an electronic circuit. Continuous models of the motion ofvehicles for example, aircraft, ships, robots were used in some of the earliestdistributed simulation applications for virtual environments. An extensive literaturehas developed concerning the use of parallel and distributed computers forcontinuous simulation problems, primarily to reduce execution time. A goodintroduction to the use of parallel computing in this field is described in Bertsekasand Tsitsik1is 1989.In a discrete simulation, the simulation model views the physical system as onlychanging state at discrete points in simulation time. Conceptually the system isviewed as jumping from one state to the next, much like moving from one frameto another in a cartoon strip.A simulation program defines a collection of state variables, and then defines therules to modifY these variables across simulation time. This concept can berepresented via a spacetime diagram such as that depicted in Figure 2.2. The yaxis of this graph represents the state variables and the xaxis denotes simulationIlIsimulation timeaIlI I II I I II I I II II III II III II I Il I 1111 I I I II 1simulation timebFigure 2.2 Spacetime diagram for a timestepped and b eventdriven discreteeventsimulations.time. A horizontal strip, namely all points ,X3 represents the evolution of thevariable X across simulation time. A change to state variable X at simulation time Tisrepresented by a vertical line in the spacetime diagram at coordinate position T, X.A vertical strip, namely all points T,  represents the state of the simulation modelat simulation time T. The task of the simulation program is to fill in the spacetimediagram by computing the values of each of the state variables across simulationtime. In a discrete simulation, each change to a state variable occurs at a specificinstant in simulation time.4event timedriven steppedFigure 2.1 Classification of simulation paradigms.discretemodelscontinuousmodels2.3.1 TimeStepped ExecutionThe two most common types of discrete simulations are called timestepped andeventdriven or sometimes called eventstepped simulations. These two categoriesof simulation are distinguished by the time flow mechanism used by the simulationto advance simulation time. In a timestepped simulation, simulation time is3The character  denotes all values.4In practice, this is also true for continuous simulations because, even though the state variables are viewedas changing continuously over time, the simulation program will define time steps and typically computenew values only at time step boundaries.32 DISCRETE EVENT SIMULATION FUNDAMENTALS 2.3 STATE CHANGES AND TIME FLOW MECHANISMS 33subdivided as a sequence of equalsized time steps, and the simulation advancesfrom one time step to the next. The execution of a timestepped simulation isdepicted in Figure 2.2a. A timestepped simulation program fills in the spacetimediagram by repeatedly computing a new state for the simulation, time step by timestep, much like a brick layer building a wall one layer of bricks at a time. Actuallynot every state variable need be modified in each time step, but the executionmechanism can only advance from one time step to the next.Actions in the simulation occurring in the same time step are usually consideredto be simultaneous, and are often assumed not to have an effect on each other. This isimportant because it allows actions occurring within each time step to be executedconcurrently by different computers. In this paradigm, if two actions have a causalrelationship that must be accurately modeled in the simulation for example, anaircraft must vacate a runway before the next aircraft can land the actions must besimulated at different time steps. Thus the size of the time step is important becauseit determines the precision of the simulation with respect to time.A variation on the timestepped mechanism is sometimes used in realtime andscaled realtime simulations. Here, the simulation must control advances in simulation time to be in synchrony with wallclock time. A nonrealtime timesteppedsimulation must repeatedly compute the new state of the system at the end of eachtime step. The realtime version is similar, except the program waits until wallclocktime has advanced before advancing to the next time step. The main loop for atypical realtime simulation is shown in Figure 2.3. In this paradigm, the computation for each time step should be completed before wallclock time advances to thenext time step, or else the simulation will lag behind wallclock time.2.3.2 EventDriven ExecutionRather than compute a new value for state variables each time step, it may be moreefficient to only update the variables when something interesting occurs. Thesomething interesting that occurs is referred to as an event. This is the key ideabehind discrete event simulations. An event is an abstraction used in the simulationto model some instantaneous action in the physical system. Each event has a timestamp associated with it that indicates the point in simulation time when the eventoccurs. Each event usually results in some change in one or more state variablesdefined by the simulation.For example, consider a simulation of an aircraft flying from New York to LosAngeles. A timestepped simulation with time step size of ten minutes mightcompute the aircrafts new position every ten minutes. A more efficient approachis to compute the total flight time, and only update the aircrafts position variablewhen simulation time advances to the time the aircraft reaches Los Angeles. Theaircraft arriving in Los Angeles is modeled by an event. In an eventdrivensimulation, changes in state variables only occur as the result of some event. Thisapproach assumes intermediate positions of the aircraft are not needed, or if they areneeded, they can be computed from the time the flight left New York and other stateinformation, for example, the speed and direction of the aircraft.Figure 2.2b shows a spacetime diagram for an event driven simulation. In thisfigure, updates to state variables occur at irregular points in simulation time, namelythe simulation time of the events.In an eventdriven simulation, simulation time does not advance from one timestep to the next but, rather, advances from the time stamp of one event to the next.From a computational standpoint, the simulation can be viewed as a sequence ofcomputations, one for each event, transforming the system across simulated time in amanner representing the behavior of the actual system. For example, Figure 2.4depicts the execution of a discreteevent simulation of air traffic in a single airport,with events denoting the arrival, landing, and departure of different aircraft.An eventdriven simulation can emulate a timestepped simulation by definingevents that happen at each time step, for example, events can be defined with timestamp 1, 2, 3, ... , assuming a time step size of one. In principal, a timesteppedsimulation may emulate an eventdriven simulation by defining a time step size thatis the greatest common divisor among all the time stamps assigned to events in thesimulation. This will ensure that no event lies between two time steps. This may berather inefficient in practice, however, because many time steps may not contain anycomputation to be performed.A variation of the eventdriven paradigm can also be used for realtime simulations. One approach is to prevent simulation time from advancing to the time stampof the next event until wallclock time has advanced to the time ofthis event, that is, ifthe time stamp of the next event is Ts simulation time is not advanced to Ts untilW2STw  reaches Ts where Tw is the current value of wallclock time.while simulation in progresswait until W2Swallclock time  simulation timecompute state of the system at the end of this time stepadvance simulation time to the next time stepFigure 2.3 Main loop in a timestepped simulation where execution is paced to wallclocktime. The W2S function converts wallc10ck time to simulation time.I , I I, , I Ir I I I I I .900 915 930 945 1000simulation timeFigure 2.4 Sequence of events in a discreteevent simulation. The arrival at 900 schedulesthe departure event at 956.34 DISCRETE EVENT SIMULATION FUNDAMENTALS 2.4 DISCRETEEVENT SIMULATION PROGRAMS 352.4 DISCRETEEVENT SIMULATION PROGRAMSThis book is primarily concerned with discrete event simulations. A sequentialdiscrete event simulation program typically utilizes three data structures see Fig.2.51. The state variables that describe the state of the system for example, Fig. 2.5shows variables that will be used later in an example for simulating an airportbriefly, these variables indicate counts of the number of aircraft that are flyingoverhead and are on the ground, and the state of the runway.2. An event list containing events that are to occur some time in the simulatedfuture Fig. 2.5 shows the events depicted in Fig. 2.4 the event with timestamp 956 is absent because it has not been created yet.3. A global clock variable to denote the instant on the simulation time axis atwhich the simulation now resides in Fig. 2.5 the simulation has advanced tosimulation time 845.If the clock variable contains a value T, then this denotes the fact that all activities inthe physical system up to the time represented by T have been simulated, andactivities later than T have not yet been simulated. All events in the event list musthave a time stamp greater than or equal to TOperationally, an event is usually implemented by a data structure that includesthe events time stamp for example, 9 16 AM, some indication of the type of eventfor example, an aircraft arriving at an airport and various parameters elaboratingmore details of the event for example, flight 396 arriving at LAX.In the physical system, events such as an aircraft arrival just happen. In thesimulated world, nothing happens unless the simulation computation makes ithappen. In other words, a mechanism is required to create new events. Themechanism for creating a new event in the simulation is called scheduling anevent. For example, suppose that the simulation depicted in Figure 2.4 and Figure2.5 now advances to simulation time 900 and the event at that time indicates flight200 has landed. The simulation might now schedule a new departure event to denotethe fact that this aircraft departs again at 956. Scheduling an event in thesimulator is implemented by allocating memory for a new event, filling in the fieldsfor the time stamp, event type, and the associated parameters, and adding the eventSimulation Application state variables code modeling system behavior 110 and user interface softwarecalls to Il calls to eventschedule handlersevents IrSimulation Executive event list management managing advances in simulation timeFigure 2.6 Separation of the simulation program into the simulation application andexecutive components.to the event list data structure. Event scheduling is one way that simulation programscan model causal relationships in the physical system.We are now ready to describe the simulation program. This program can bedivided into two components see Fig. 2.6. The lower piece is the simulationexecutive that maintains the event list and clock variable. This portion is independentof the physical system. Commercial vendors often sell the simulation executive as ageneral purpose component that can be used to simulate a variety of systems. Theupper portion includes the state variables and the software for modeling the physicalsystem. This part is called the simulation application, and it is intimately tied to thephysical system. In its simplest form, the simulation executive need only provide asingle primitive to the simulation application a procedure for scheduling events.The program executed by the simulation executive is shown in Figure 2.7. Theheart of the simulation executive is the eventprocessing loop that repeatedlyremoves the event containing the smallest time stamp from the event list, advancesthe simulation time clock to the time stamp of this event, and then calls a proceduredefined in the simulation application that processes the event. This procedure forprocessing the event may do two things1. Modify state variables to model changes in the state of the physical systemthat result from this event.2. Schedule new events into the simulated future.state variablesInteger InTheAirInteger OnTheGroundBoolean RunwayFreeevent list1900 1 1916 1while simulation is in progressremove smallest time stamped event from event listset simulation time clock to time stamp of this eventIClock  845Figure 2.5 Principal data structures in a discreteevent simulation program.execute event handler in application to process eventFigure 2.7 Main eventprocessing loop in a discreteevent simulation program.else36 DISCRETE EVENT SIMULATION FUNDAMENTALSThere are two important points in this discrete event simulation that are worthhighlighting, because they will become very important when we consider executionon parallel and distributed computers. Both relate to ensuring that the simulationfaithfully reproduces causal relationships in the physical system. First, the simulationapplication can only schedule events into the simulated future, namely the timestamp of any new event must be at least as large as the current time of the simulation.Second, the simulation executive always processes the event containing the smallesttime stamp next. These two properties ensure that the simulation will process eventsin time stamp order, and the simulation time clock never decreases in value duringthe execution of the simulation. This is important because it ensures that an eventcomputation at time T cannot affect any event computation with a smaller timestamp. This is certainly a good thing, because, if this were not the case, it would bepossible for future events to affect those in the past2.5 AN EXAMPLE APPLICATIONConsider a simulation of air traffic arriving and departing at an airport. Assume thatthe airport contains a single runway for incoming aircraft. Such a simulation mightbe used to collect statistics such as the average number of aircraft waiting to land orthe average amount of time each aircraft must wait however, we will ignore thecomputation of such statistics here in order to focus on the event processingmechanism. In this example, the state of the airport is characterized by three statevariables1. InTheAir indicates the number of aircraft that are in the process oflanding, or are circling, waiting to land.2. OnTheGround indicates the number of aircraft that have landed and areeither at a gate, or traveling to or from a gate.3. RunwayFree indicates whether or not the runway is currently being usedby a landing aircraft.These state variables and the observation that the state of the simulation can onlychange when an event occurs suggest the types of events that are needed.Specifically, InTheAir is incremented by one when a new aircraft arrives atthe airport, and is decremented by one when an aircraft lands. Similarly OnTheGround is incremented when an aircraft lands, and is decremented when an aircraftdeparts. Finally RunwayFree will become FALSE when an aircraft arrives iftherunway was not already in use, and becomes TRUE when an aircraft lands and thereare no additional aircraft waiting to land. Departing aircraft are not included in theInTheAir state variable. Thus three types of events are defined for thissimulation2.5 AN EXAMPLE APPLICATION 371. An arrival even t denotes the arrival of a new aircraft at the airport.2. A landed even t denotes that an aircraft has landed.3. A departure even t denotes an aircraft leaving to travel to another airport.val nd departure events represent, respectively, the introduction and removal ofaircraft ill the simulation.. Upon arrival, each aircraft must I wait for the runway and land assume that theaircraft uses the runway for R units of time while landing, 2 travel to the gate andunload and load new passengers assume that this requires G units of time, and 3depa .and travl t another airport. Assume that Rand G are fixed, knownquantltles. To SimplIfY the model, queuing at the runway for departing aircraftwill not be considered here. Constants and other values Now urrent simlation time from simulation executiveR tlme runway In use to land aircraft constant G  time required at gate constant State Variables Integer InTheAir number landingwaiting to land Integer OnTheGround number of aircraft within airport BOlan.RunwaYFree TRUE if runway is not being usedInltlallze these variables to 0, 0, and TRUE, respectivelyArrival EventInTheAir  InTheAir  1 compute time alrcraft land d d d .e an one uSlng runway If RunwayFreeRunwaYFree  FALSESchedule Landed Event at time NowRLanded Event update state for the aircraft that has landed InTheAir  InTheAir  1OnTheGround  OnTheGround  1Schedule Departure Event at time NowG land next aircraft if there is one if InTheAir  0Schedule Landed Event at time Now  RRunwayFree . TRUEDeparture EventOnTheGround  OnTheGround  1FilJllr.. 2Jl Simulation annlication nrollram for a inlIle airnort.38 DISCRETE EVENT SIMULATION FUNDAMENTALS2.7 PARALLEUDISTRIBUTED SIMULATION EXAMPLE 392.6 STARTING AND STOPPING THE SIMULATIONsimulation timeFigre 2.9 .Spacetime diaram depicting an aircraft arriving, landing, and then departing atan airport usmg the SimulatIOn application shown in Figure 2.8.A paral.lel r a istributed simulation is typically composed of a collection ofequentIal s.lmulatlOns, ach modling a different part of the physical system and ateast potetIally executlll on a Ifferent processor. Borrowing terminology from thep.arallel. discrete erent Simulation community, let us refer to each sequentialSimulatIOn as a logical process or LP. In other contexts, each sequential simulationo. R ., .. Gstate variables , ,InThcAir 0 0OnTheGround 0RunwayFree TRUE FALSE TRUEPARALLELDISTRIBUTED SIMULATION EXAMPLE2.7Tere are two remaining aspects of the simulation execution that need to bediscussed staing and stopping the simulation. The simulation begins by initializingthe state vanables, and generating initial events. Initialization of variables isaccomplished b traditnl. pOgamming techniques. The initial events may bec.reated .by defimng an ImtlahzatlOn event with time stamp equal to a simulatedtime. pnor to the beginning of the actual simulation. The simulation applicationrrldes a procedre that processes this initialization event by scheduling all otherlllltIal events reqUired by the simulation.There are several techniques for terminating the execution of the simulation Asto simuation event may be used that is defined to be the last event processed bythe slmlatlOn, even if there are other scheduled events remaining in the event list.ltemalvely, an end simulation time may be defined that indicates the simulationI termnated when the simulation clock is about to exceed this time that is, thelmulatlOn ens hen the n.ext event removed from the event list carries a time stamparger than thiS time. In either case, the simulation will always terminate after anevent computation if there are no events in the event list.The simulation application consists of the definitions of the state variables andconstants, and three procedures, Arrival, Landed, and Departure, one tohandle each of the three different types of events. As discussed previously, thesimulation executive repeatedly removes the smallest timestamped event from theevent list, and calls the appropriate procedure for the type of event that was removed.The simulation executive also defines a function called Now that returns the currentvalue of the Clock variable.The code for this simulation application is shown in Figure 2.8. This programclosely follows the behavior of the airport that was just described. When an arrivalevent occurs, InTheAir is incremented, and if the runway is free, the aircraftbegins to land. This is accomplished by setting RunwayFree to FALSE andscheduling a Landed event R time units into the future. If the runway is not free, nofurther action is taken. The Landed event procedure decrements InTheAirand increments OnTheGround to reflect the new status of the aircraft, andschedules a Departure event to represent the final departure of the aircraft fromthe airport. If there are additional aircraft waiting to land, the Landed eventprocedure also schedules a new Landed event to model the next aircraft landing. Ifthere are no more aircraft waiting to land, the runway is marked as being free bysetting RunwayFree to TRUE. Finally the Departure event procedure simplydecrements OnTheGround to represent the fact that the airport has one feweraircraft.It may be noted that in this simple example, no queueing of departing aircraft ismodeled. It is straightforward to extend the model to include this aspect, so this isleft as an exercise for the reader.The above program models the movement of aircraft through the airport but doesnot provide any means for generating new aircraft, that is, generating new Ar r iva1events. This could be accomplished by augmenting the Ar r i val event handlerprocedure so that each arrival event schedules a new arrival event I time units intothe future, where I is the interarrival time, or time between arriving aircraft. I mightbe selected by invoking a random number generator, that is, a procedure that selectsa number in accordance to some probability distribution. Rather than schedulingnew arrival events in this fashion, here, we will assume that the event list isinitialized to contain an arrival event for each aircraft that will pass through theairport in the entire simulation. This will facilitate discussions later when we expandthis sequential simulation program to one that can execute on parallel or distributedcomputers.A sample execution ofthis program is depicted in Figure 2.9. This figure shows aspacetime diagram to illustrate how the state variables are modified by the differentevents. For example, the computation for the Arrival event increments theInTheAir variable and sets RunwayFree to FALSE. It then schedules aLanded event R time units into the future. The Landed event decrementsInTheAir, increments OnTheGround, sets RunwayFree to TRUE,and schedules a Departure event. Finally the departure event decrements OnTheGround.40 DISCRETE EVENT SIMULATION FUNDAMENTALS2.8 WORLD VIEWS AND OBJECTORIENTED SIMULATION 41might be referred to as a simulator. Thus the hysical systm ca be viewed s acollection of physical processes that interacts III some fashlOn wIth each physIcalprocess being modeled by a logical process.As each logical process executes, it processes events and generates new events. Itmay be the case that an event that is generated within one LP is relevant to one.ormore other LPs. When this happens, a message is sent to the other LPs to notifythem of the event.5 Viewed another way, interactions between physical processes remodeled in the distributed simulation by passing messages among the correspondlllglogical processes. . . . . .For example, let us consider extending the alfport slmulatlOn descnbe earher tomodel air traffic in the United States. The U.S. air traffic network can be vIewed as.acollection of airports that interact by having aircraft fly between the.m. An aca alrtraffic system might also include other interactions such as radlO translsslnsbetween aircraft and airports, but these interactions will be ignored here to slmphfythe discussion. .The physical system consists of three airports JFK in New York, LAX III osAngeles, and ORD in Chicago. In this example, each airport is modeled by a 10g1C1process identical to that shown in Figure 2.8, except the departure event procedure ISmodified as described next.The original model shown in Figure 2.8 assumed that once an ircra dparte itleft the simulation and was never heard from again. Here, we modIfy thIS SlmulatlOnby observing that an aircraft departure results in a subsequent aval event at anotherairport. To realize this change, only the proceure for p.rocsslllg departure eventsneeds to be modified. The modified procedure IS shown III FIgure 2.1 O. Rather thandiscarding departing aircraft, each departure event generates a new arrival ev.ent foranother airport. This is accomplished by sending a message to the L mdellllg thedestination airport requesting that it schedule a new arrival event wIth time stampequal to the time of departure plus the amount of time required to fly between thetwo airports. .This completes our example for now. With the small change descnbed abov.e, wehave now constructed a simple distributed simulation program for modehng acollection of airports. This example was intended to illustrate tht dstribuedsimulation is a direct extension of wellknown concepts in the sequential slmulatlOnworld. One can view a paralleldistributed simulation as a collection of sequentialsimulation programs that exchange messages to notify other simu.lations of vents.While in this example each logical process in the distributed executlOn was a dIscreteevent simulation program, one could easily replace each LP with a time stepped or acontinuous model. .At this point we should alert the reader to the fact that the. above exampl ISdeceptively simple. As will be seen in later chapters, extendlllg the underlYlllg5Th ral1el discrete event simulation literature often views events and messages as being synonymous.epa . I hi IThis view is not taken here because it may be that a smgle event may be relevant to severa ot er oglcaprocesses. It is more natural to view this situation as a single event resulting in several LPs being notifiedof the event via messages.Departure Event notify next airport of a new arrival event. Source  ID of this airport Dest  ID of destination airport FlightTime S,D  time to fly from S to DSend Message to Dest to schedule an arrival event attime NowFlightTimeSource,DestFigure 2.10 Modified departure event procedure for distributed simulation.simulation executive to parallel and distributed environments introduces manynontrivial problems.2.8 WORLD VIEWS AND OBJECTORIENTED SIMULATIONThe approach discussed above for modeling air traffic is known as the eventorientedworld view. In this approach the focus of the model is on events, and how they affectthe state of the simulation. Simulation programs using this world view consist ofprocedures or event handlers, one for each different type of event that can occur inthe simulation. The eventoriented world view will be used throughout much of thisbook because it is, in many respects, the machine language of discreteeventsimulation. By this we mean it defines the fundamental mechanisms, specifically themechanisms for handling events and advancing simulation time that are used by theother techniques described later in this chapter. We conclude this section with adiscussion of another important world view, the socalled processoriented approach,and a programming approach called objectoriented simulation.For completeness we mention one other world view, known as the activityscanning approach, that is also sometimes used. This is a variation on the timestepped mechanism. The simulation program consists of a collection of procedures,with a predicate associated with each one. At each time step, each predicate isevaluated, and the associated procedure is executed if its predicate evaluates toTRUE. This process repeats until no predicate evaluates to TRUE. When this happensthe simulation advances to the next time step.2.8.1 Simulation ProcessesConsider the air traffic simulation described earlier in this chapter. Imagine that youare employed by a commercial airline company to modify this simulation program to42 DISCRETE EVENT SIMULATION FUNDAMENTALS2.8 WORLD VIEWS AND OBJECTORIENTED SIMULATION 43include more detail concerning actions performed by the pilot of an aircraft forexample, details concerning procedures for takeoff and landing or for changingaltitude during a flight. While this is a manageable task for the simple simulationshown in Figure 2.8, this would be much more challenging if the simulation weremuch larger and more complex for example, containing tens or hundreds ofthousands of lines of code and hundreds of different event procedures. In orderto make modifications such as those suggested above, you must locate all of the codedescribing the aircrafts behavior for example, take off, landing, and travel betweenairports, and modify these portions of the program. The problem is the behavioraldescription for a single aircraft is scattered across the entire program in the differentevent procedures, and it is not immediately clear from the code what sequence ofevents describes the behavior of a single aircraft. It is difficult to understand andmodify the model for a single aircraft because the simulation program is notorganized in a way to allow one to separate the aircrafts behavior from that ofother types of aircraft for example, flown by different airlines and other activitiesthat go on in the airport.A processoriented simulation attacks this problem through an abstraction calledthe simulation process. A simulation process is intended to model a specific entity inthe simulation with a welldefined behavior for example, an aircraft in the air trafficexample. The behavioral description of the entity is encapsulated by the process.This description describes the actions performed by the process throughout itslifetime.For example, in the simulation for a single airport, the lifetime of an aircraft canbe described as follows First it waits for the runway to become free. Then it lands,using the runway. It next moves to the gate to load and unload passengers, and then itdeparts. The simulation program for an aircraft process is depicted in Figure 2.11,where it can be seen that the program directly reflects this word description of theaircrafts behavior. This is in sharp contrast to the eventoriented descriptiondescribed previously. The simulation program uses two key primitives1. Wai tUntil predicate. This construct causes the process to besuspended blocked while simulation time advances until the specifiedpredicate, in this case the runway becoming free, evaluates to TRUE.2. AdvanceT ime T. This construct causes simulation time for the process toadvance by T units of simulation time. This construct is invoked to signify thatthe entity is busy performing some activity for T units of time.A key point is that the WaicUntil and TimeAdvance primitives cause simulationtime to advance. This is critical because the lifetime of the entity modeled by theprocess is over a certain period of simulation time. By contrast, the lifetime of anevent procedure is a single instant in simulation time.In addition to processes, processoriented simulations often utilize the concept ofa resource. A resource is an abstraction that represents a shared entity for which oneor more processes compete. The runway in the air traffic simulation is an example ofa resource. The scenario described in Figure 2.11 where the aircraft waits until the Constants and other values R time runway in use to land aircraft constant G  time required at gate constant State Variables Integer InTheAir number of landing or waiting to land Integer OnTheGround number of aircraft within airport Boolean RunwayFree TRUE if runway is not being used Initialize these variables to 0, 0, and TRUE, respectivelyProcess Aircraft simulate aircraft arrival, circling, and landing 1 InTheAir InTheAir  12 WaitUntil RunwayFree  circle 3 RunwayFree FALSE  land 4 AdvanceTimeR5 RunwaYFree TRUE simulate aircraft on the ground 6 InTheAir InTheAir  17 OnTheGround OnTheGround  18 AdvanceTimeG simulate aircraft departure 9 OnTheGround OnTheGround  1Figure 2.11 Processoriented simulation of airport.esource becomes available the Wai tUntilRunwaYFree statement, acquirmg the resource the RunwayFree  FALSE statement and releasing theresource the RunwayFree  TRUE statement, is sufficiently common thatprimitives for performing these functions are often included in a library, or built intothe simulation language itself Conceptually a processoriented simulation can beviewed as collections of processes, each advancing in a somewhat autonomousfashion through simulation time and interacting with other processes by competingfor shared resources.As mentioned earlier, processoriented simulations are typically implemented onto of eventoriented simulation mechanisms. Specifically, processoriented simulahons use the same event list and time advance mechanism defined for the eventoriented paradigm but provide additional mechanisms for managing simulationprocesses. The lifetime of a simulation process can be viewed as a kind of miniatureeventoriented simulation in that it consists of a sequence of event computations.Simulation time for the process only advances between these event computations.The key difference is that in an eventoriented simulation, the event computation isencapsulated into a procedure i.e., the event handler. In a processorientedsimulation the event computations are blocks of statements within the code for thesimulation process, and they are terminated by a call to a primitive to advancesimulation time, i.e., the Wai tUn ti 1 and AdvanceT ime statements. In thesimulation in Figure 2.11, there are four event computations1. Statements 1 and 2 modeling the aircraft waiting to land.2. Statements 3 and 4 modeling the aircraft landing.2. Statements 5, 6, 7, and 8 modeling the aircraft on the ground.3. Statement 9 modeling the departure.The eventoriented paradigm provides a very straightforward mapping of thesimulation program to standard programming language constructs that .is, eachevent handler could be simply implemented as a procedure. The mapplllg of asimulation process to language constructs is somewhat more complex. One couldpartition the simulation code into separate procedures and revert back to the eventoriented style of execution. For example, a compiler or preprocessor could transltethe simulation code in Figure 2.11 into four procedures, PI, P2, P3, and P4, Witheach procedure terminated by a call to a simula.tion rimit.ive that results insimulation time to advancing. This ensures that SimulatiOn time only advancesbetween calls to the procedures, which is identical to an eventoriented simulation.The compiler could create an event handler for each process tat is alled wheneveran event pertaining to that process is removed from the event list. ThiS event hadlercalls PI the first time it is invoked, P2 the second time, and P3 and P4, respectiVely,on the final two invocations. The AdvanceTime T primitive schedules a newevent T time units into the future, thereby guaranteeing the event handler for theprocess will be called again at the precise simulation time when the proces souldresume execution. The Wai tUntil primitive updates a data structure withlll thesimulation executive to indicate the condition on which the process is waiting. Priorto processing each event, the simulation executive must check to determine whichwaiting processes are now able to resume execution, and schedules an event at thecurrent simulation time to wake up one such process. Ifthere are several that areeligible to execute for example, there may be several processes waiting for aresource that has now become free, the simulation executive must use soneprioritization rule to determine which process should be resumed. Of course, thiSis something that the modeler must have control over, since it will usually affect thesimulation results, so a queueing discipline may be specified for example, firstcomefirstserve to address this issue. .The above discussion describes a typical implementation of a processonentedsimulation paradigm, with one exception. Suppose that a WaitUntil orAdvanceTime primitive is called within a loop, or within a procedure alledby the simulation process. In this case, decomposing he ode for the process lllto asequence of procedure calls is not so simple. For Situations such as these, a coroutine mechanism or equivalently, a threading mechanism is needed to transfer2.8.2 ObjectBased and ObjectOriented Simulations452.8 WORLD VIEWS AND OBJECTORIENTED SIMULATIONexecution in and out of the process code. A coroutine mechanism is a facility thatallows a computation for example, a simulation process to stop and transferexecution to another computation the simulation executive. Later, the simulationprocess can resume execution at exactly the point at which it had been stopped.To summarize, the principal points concerning processoriented simulations areas follows1. They provide a more convenient paradigm for developing simulation applications for certain types of applications.2. They can be implemented on top of the basic eventoriented style of executionthat was described earlier.3. They incur a certain amount of additional computational overhead to controland manage the execution of simulation processes.Many physical systems can be viewed as collections of components aircraft,controllers, airports, etc. that interact in some fashion. Thus it is natural to modelthese systems as collections of interacting objects. For this reason, objectbased andobjectoriented paradigms have become popular modeling paradigms.An object consists of a collection of fields state variables or attributes and a setof methods, typically implemented as procedures, that model the behavior of thecomponent. Objects are created instantiated dynamically during the execution ofthe program, enabling one to easily model the creation of new components forexample, aircraft in an air traffic simulation during the execution of the simulation.Objects may initiate execution invoke methods of other objects or request querythe current value of another objects fields. Logically, invoking a method can beviewed as sending a message to the object requesting that the method be executed.When executed on a single computer, invoking an objects method can beimplemented by a simple procedure call. On a parallel or distributed computer,messages are used to invoke methods for objects that reside on another computer.The fields of an object can only be modified by that objects methods. Thisprincipal, called encapsulation, greatly simplifies software maintenance and debugging. Systems that require the simulation to be structured as collections ofinteracting objects are often referred to as objectbased systems. Objectorientedsystems go a step further by providing a capability called inheritance to characterizerelationships among collections of similar, but not identical objects.A key aspect of objectoriented languages is that they allow one to define newtypes of objects in terms of already defined object types. The new object type is saidto inherit the properties fields and methods of the original. However, the new objecttype, called the derived type, may replace these properties with new ones, or extendthe object type to include entirely new fields and methods. This allows, for instance,one to define a generic object type and define specific object types that elaborateupon the original, base, type. For example, the base type might be vehicle objectsDISCRETE EVENT SIMULATION FUNDAMENTALS442.8.3 Query Events and Push versus Pull ProcessingWhen simulating collections of interacting objects, such as aircraft and airports, it iscommon for one object to need to collect state information from other objects. Forexample, in the simulation of the three airports discussed earlier, one might define afourth object that monitors traffic conditions at all three airports, and dispatchesrecommendations for example, rescheduling of flights to these airports. Thismonitor object might request the current value of the IDTheAir state variableat each of the other airports to determine which airports are congested. This might beimplemented by invoking an Ask method at each of the airport objects requestingthe value of this variable. In a sequential simulation, this could be implemented by asimple procedure call for each airport. In a distributed simulation, the monitor objectand the airport objects may reside on different processors, so messages must be sentto retrieve the requested information. Adhering to our paradigm of logical processesexchanging timestamped events, requesting the value of the IDTheAir statevariable is viewed as scheduling an event called a query event for each airportprocess with time stamp equal to the current simulation time of the monitor object.The event handler for this event a method in the airport object generates a reply bywith fields indicating the vehicles current position, direction, and velocity. Anaircraft object can be derived from this base type that extends this definition toinclude an altitude field.The derived type may replace or overload methods in the base object type. Boththe derived type and the base type use the same name for the method. Thus an objectcan invoke the move method for a vehicle object without being concernedwhether the vehicle is an aircraft or an automobile. The move method for an aircraftmight cause it to climb 1000 feet, while that of the automobile causes it to travelanother 10 minutes down the freeway. The underlying system ensures that thecorrect method is invoked at runtime. Replacing methods in this fashion allows oneto extend existing libraries and tailor them to suit the purposes of the user. The factthat different object types can use the same name for their move method is importantbecause it allows new object types to be defined and incorporated into the simulationprogram without modifying the object that invokes the method, thereby simplifyingthe addition of new types of objects to the program. The ability to have differentmethods with the same name is called polymorphism.It should be obvious that objectbased and objectoriented languages are naturalvehicles for implementing discreteevent simulations. This should come as nosurprise because many of the ideas in objectoriented simulation can be tracedback to a language called Simula that was designed for discrete event simulation.Methods can be used to implement event procedures. Encapsulation of the state ofan object supports parallel and distributed simulations because it discourages the useof global variables that may be difficult to implement on parallel and distributedcomputers which do not provide shared memory. Moreover the approach ofconstructing simulation programs as collections of objects that interact in somefashion is a natural way to view systems of interacting components.472.8 WORLD VIEWS AND OBJECTORIENTED SIMULATION2.8.4 Event Retractionscheduling a new event at the monitor object containing the requested value. Thetime stamp of this reply is the same as the query.The approach described above using query events is sometimes referred to aspull processing because each LP is responsible for pulling in the information itneeds when it needs this information. The drawback with pull processing indistributed simulations is that two message transmissions are required to collectinformation from another processor. Further the process requesting the informationmust usually block until the query has been satisfied.An alternative approach is to have the airport processes automatically provide themonitor process the value of the required state information whenever the variablechanges. This is sometimes called push processing because the LP that holds thestate variable pushes changes to the variable to other processes. This reduces thetwo messages required for each transmission of state information in the pullapproach to only one, and it eliminates forcing the monitor process to block whilewaiting for the response of its queries. Push processing may require more messagetransmissions than are really needed, however, because the source of the data cannotknow if the user of this information requires each new value of the state variable.Another commonly used mechanism is to retract sometimes called cancel in thediscreteevent simulation literature however, event cancellation is used to denote anentirely different mechanism here, as will be discussed in Chapter 4 previouslyscheduled events. Fundamental to the discrete event simulation paradigm describedearlier is the notion of scheduling events into the simulated future. This is, inessence, predicting what will happen for example, once an aircraft lands, we canpredict that it will later depart again. In some circumstances this may be difficult todo with absolute certainty. In that case the simulation program may schedule eventsthat it believes will occur when the event is scheduled, but later it will retract thescheduled event should this belief tum out to be incorrect.For example, again consider the air traffic example described earlier. Suppose thatWe now introduce a gremlin process that generates airport closings for example,because of bad weather at randomly selected points in time. An airport closurecoulbe easily implemented by the gremlin process scheduling a closing event atan airport with a time stamp indicating when the airport closes. When an airportclosing event is processed by an airport, it may have other events already scheduled,ased on the assumption that the airport did not close. For instance, there may beeparture events scheduled for aircraft that have recently landed. The event retractionmechanism can be used to unschedule these departure events, and reschedule newdeparture events based on the reopening time of the airport. Without the ability toetract previousy sceduled events, the simulator would need to devise a way toIgnore the now InvalId departure events when they are processed. It is possible to dothis, though somewhat cumbersome.DISCRETE EVENT SIMULATION FUNDAMENTALS4648 DISCRETE EVENT SIMULATION FUNDAMENTALS2.9 OTHER APPROACHES TO EXPLOITING CONCURRENTEXECUTIONThis book is primarily concerned with parallel and distributed simulation techniquesthat are composed of simulation models for different parts of the system executingconcurrently on different processors. For completeness, other approaches to exploiting concurrency in simulation problems are mentioned. One approach that has beenproposed is to use dedicated functional units to implement specific sequentialsimulation functions, for example, event list manipulation and random numbergeneration. This method does not scale to large simulation models, however, becauseno provision is made to partition large models into smaller submodels executingconcurrently on different processors.Another wellknown approach is to execute independent, sequential simulationprograms on different processors. This replicated trials approach is useful if one isperforming long simulation runs to reduce variance, or if one is investigating thebehavior of a system across a large number of different parameter settings. Thereplicated trials approach is a very simple and useful technique to exploiting multipleprocessors, and one that is widely used today. A disadvantage of this approach is thateach processor must have enough memory to hold the entire simulation program.Also this approach is obviously not well suited for interactive virtual environments.Finally it is not suitable if results of one experiment are needed to determine theexperiment that should be performed next.2.10 ADDITIONAL READINGSDiscreteevent simulation is a mature field that dates back at least to the 1950s. Thefield includes numerous areas such as model design and development, programminglanguages, experimental design, analysis of output, and random number generation,to mention a few. The focus of this book is limited to one aspect of this field, namelymodel execution on parallel and distributed computing systems. Several goodtextbooks giving broader coverage of the field are available for example, see Lawand Kelton 1991 Fishwick 1994 and Banks, Carson II et al. 1996.The spacetime view of simulation programs, and different execution mechanismsboth sequential and parallel for filling in the graph is described in Chandy andSherman 1989 and elaborated upon in Bagrodia, Liao et al. 1991. The functionaldecomposition approach where different processors of a parallel computer are usedto execute different portions of a sequential simulation event list processing,random number generation, etc. is described in Comfort 1984, and Davis,Sheppard et al. 1988. Use of the replicated trial approach to reduce the time oflong simulation runs are described in Biles, Daniels et al. 1985, Heidelberger1986, Glynn and Heidelberger 1991, and Sunderam and Rego 1991.PART IIPARALLEL AND DISTRIBUTEDDISCRETEEVENT SIMULATION CHAPTER 3Conservative SynchronizationAlgorithmsThis chapter and the three that follow are concerned with the execution of analyticsimulation programs on parallel and distributed computers with the principal goal ofreducing execution time. The emphasis is on asfastaspossible execution however,as was described in the previous chapter, the simulation program could be paced toexecute as a realtime or scaled realtime simulation if the execution is fast enoughto keep up with scaled wallclock time.As discussed in the previous chapter, the physical system is viewed as beingcomposed of some number of physical processes that interact in some fashion. Eachphysical process is modeled by a logical process LP, and interactions betweenphysical processes are modeled by exchanging timestamped messages between thecorresponding logical processes. The computation performed by each LP is asequence of event computations, where each computation may modify state variables andor schedule new events for itself or other LPs.At first glance this paradigm would seem to be ideally suited for paralleldistributed execution one can simply map different logical processes to differentprocessors and let each LP execute forward, event by event, and exchange messagesthat schedule events for other LPs as needed. Unfortunately, there is a catch. Eachlogical process must process all of its events, both those generated locally and thosegenerated by other LPs, in time stamp order. Failure to process the events in timestamp order could cause the computation for one event to affect another event in itspast, clearly an unacceptable situation. While we saw in Chapter 2 that time stampordered event processing was easily accomplished on a sequential computer by usinga centralized list of pending events, this is not so easily accomplished whenexecution is distributed over more than one processor. Errors resulting from outoforder event processing are referred to as causality errors, and the general problemof ensuring that events are processed in a time stamp order is referred to as thesynchronization problem.This chapter describes one major class of algorithms for addressing the synchronization problem, namely conservative synchronization protocols where each LPstrictly avoids processing events out of time stamp order. The two chapters thatfollow describe an alternative approach called optimistic synchronization where3.1 SYNCHRONIZATION PROBLEM5320 simulation time3.1 SYNCHRONIZATION PROBLEM1510LPLAXIt is clear that if one violates the local causality constraint, causality errors mayo.ccur. owever, one could ask the opposite question. If each LP in the parallelsimulatIOn adheres to the local causality constraint, is this sufficient to guarantee thatthe sirlUlation is correct It turns out that the answer to this question is yes, asstated III the following observationObservation If each LP adheres to the local causality constraint, then theparalleydistributed execution will yield exactly the same results as a sequentiale.xecutlOn of the same simulation program provided that events containing the sametime samp are processed in the same order in both the sequential and parallelexecutIOn. Events containing the same time stamp are referred to as simultaneousevents.Figure 3.1 Event E lO affects E20 by sheduling a third event E 15 which modifies a statevariable used by E20 This necessitates sequential execution of all three events.Note that this observation does not guarantee that the simulation produces usefulor even meaningful results. Any simulation model must be validated before itsresults can be trusted. From the standpoint of synchronization, correctness onlygoes so far as to say that the parallel execution will produce identical results as asequential execution of the same program. Also it should be pointed out thatadherence to this constraint is sufficient, but not always necessary, to guarantee thatno causality errors occur. It may not be necessary because two events within a singleLP may be independent of each other, in which case processing them out of timestamp sequence is acceptable.. Operationally one must decide whether or not E 10 can be executed concurrentlywth E20  But how does the simulator determine whether or not E affects Ewithout actually performing the simulation for E10 This is the fundamtal dilehat must be addressed..Te se.nario in which E10 affects E20 can be a complexequence of events, and It IS cntlcally dependent on event time stamps.Assume that the simulati.on cosist of N logical processes, LP0 ... , LPNIClock refers to the current simulatIOn time of LP when an event is processed, theprocesss clock is automatically advanced to the time stamp of that event. If Lp.sends a message to LPj during the simulation, a link is said to exist from LP to LPj CONSERVATIVE SYNCHRONIZATION ALGORITHMS52errors are detected during the execution, and some mechanism is used to recoverfrom them.Consider a simulation program composed of a collection of logical processesexchanging timestamped messages. Consider the execution of this program on asequential computer. The sequential execution ensures that all events across all ofthe logical processes are processed in time stamp order. When the simulation isdistributed over multiple processors, a mechanism is required for the concurrentexecution to produce exactly the same results as the sequential execution. The goalof the synchronization algorithm is to ensure that this is the case. It is important torealize that the synchronization algorithm does not need to actually guarantee thatevents in different processors are processed in time stamp order but only that the endresult is the same as if this had been the case.Consider parallelization of a simulation program that is based on the logicalprocess paradigm discussed above. The greatest opportunity for parallelism arisesfrom processing events from different LPs concurrently on different processors.However, a direct mapping of this paradigm onto say a sharedmemory multiprocessor quickly runs into difficulty. Consider the air traffic example discussed inthe previous chapter with three LPs that model LAX, ORD, and JFK. Consider theconcurrent execution of two arrival events in this example. Specifically, E IO at the LPfor ORD has a time stamp of 10, and E20 at LAX with time stamps 20. IfEIO affectsE20 for example, E IO might write into a state variable that is read by E20, then E IOmust be executed before E20 .To avoid scenarios such as this, the restriction is made that there cannot be anystate variables that are shared between logical processes. The state of the entiresimulator must be partitioned into state vectors, with one state vector per LP. Eachlogical process contains a portion of the state corresponding to the physical processit models, as well as a local clock that denotes how far the process has progressed insimulation time.Although the exclusion of shared state in the logical process paradigm avoidsmany types ofcausality errors, it does not prevent others. Again, consider two arrivalevents, E 10 at logical process LPORD with time stamp 10, and E20 at LPLAX with timestamp 20 see Fig 3.1. IfE IO schedules a new event E I5 for LPLAX that contains timestamp 15, then E I5 could affect E20 , necessitating sequential execution of all threeevents. For example, E 15 might denote the arrival of an aircraft at LAX, delaying thelanding of the flight arriving at time 20.We impose the following local causality constraint on each logical process toavoid errors such as thisLocal Causality Constraint A discreteevent simulation, conslstmg of logicalprocesses LPs that interact exclusively by exchanging time stamped messagesobeys the local causality constraint if and only if each LP processes events in54 CONSERVATIVE SYNCHRONIZATION ALGORITHMS 3.2 DEADLOCK AVOIDANCE USING NULL MESSAGES 55Historically the first synchronization algorithms were based on socalled conservative approaches. The fundamental problem that conservative mechanims ustsolve is to determine when it is safe to process an event. More precisely, If aprocess contains an unprocessed event E IO with time stamp TIO and no other withsmaller time stamp, and that process can determine that it is impossible for it to laterreceive another event with time stamp smaller than T IO , then E IO is said to be safebecause one can guarantee that processing the event now will not later result in aviolation of the local causality constraint. Processes containing no safe eventsmust block. As will be seen momentarily, this can lead to deadlock situations ifappropriate precautions are not taken.3.2 DEADLOCK AVOIDANCE USING NULL MESSAGESLet us assume that one statically specifies the links that indicate which logicalprocesses may communicate with which other logical processes. Further assume thatl the sequence of time stamps on messages sent over a link is nondecreasing, 2the communications facility guarantees that messages are received in the same orderthat they were sent software to reorder messages is necessary if the network doesnot guarantee this property, and that 3 communications are reliable i.e., everymessage that is sent is eventually received. This implies that the stream of messagesarriving on a given link will have nondecreasing time stamp values. It alsoguarantees that the time stamp of the last message received on an incoming linkis a lower bound on the time stamp of any subsequent message that will later bereceived on that link.Messages arriving on each incoming link can be stored in a firstinfirstoutFIFO queue, which is also time stamp order because of the above restrictions.Here, we ignore local events that are scheduled by an LP for itself. In practice,processing of these events must be interleaved with the processing of messages fromother LPs so that all events are processed in time stamp order, however, this is easyto accomplish. 6 Each link has a clock associated with it that is equal to the timestamp of the message at the front of that links queue if the queue contains amessage, or the time stamp of the last received message if the queue is empty. Forexample, a snapshot of the queues for the JFK logical process in our airport exampleis shown in Figure 3.2. This logical process is guaranteed that any subsequentmessage sent to it from ORD has a time stamp of at least 5 and that any subsequentmessage from LAX has a time stamp of at least 9. ..A program for executing incoming messages in time stamp order IS shown mFigure 3.3. Because messages in each FIFO queue are orted by time stamp, the .LPcan guarantee adherence to the local causality constramt by repeatedly processmgthe message containing the smallest time stamp, so long as each queue contains atleast one message. If one of the FIFO queues becomes empty, the LP must wait until6 Local events could be placed in a separate FIFO queue that is similar to the others except the LP shouldnot block if this queue becomes empty.ORO  5 J4 I.JFKlogicalprocessLAX  98 2 I..Figure 3.2 Snapshot of the logical process modeling JFK.a new message is added to this queue because a message could later arrive thatcontains a time stamp as small as the message it just removed and processed fromthat queue. The LP could process messages in other queues containing the same timestamp as the one it just processed however, it cannot process any messagescontaining a larger time stamp. In this way the protocol shown in Figure 3.3guarantees that each process will only process events in nondecreasing time stamporder, thereby ensuring adherence to the local causality constraint.For example, consider the air traffic simulation described earlier. As shown inFigure 3.2, each airport LP will have one queue to hold incoming messages fromeach of the other airports that are simulated. Again, assume that there are only threeairports LAX, ORD, and JFK. Consider the queues in the JFK process. The queueholding messages from ORD contains messages with time stamps 4 and 5, and thequeue for LAX has messages with time stamp 2, 8, and 9. The JFK process will nowprocess arrival messages in the following order 2 LAX, 4 ORD, and 5 ORD.Assuming that no new messages have been received, the JFK simulator will block atthis point, even though there are unprocessed messages with time stamps 8 and 9from LAX. The LP must block because of the possibility that a new message willlater arrive from ORD with time stamp less than 8. As mentioned earlier, we canonly guarantee the next message has a time stamp of at least 5.A cycle of empty queues could develop such as that shown in Figure 3.4 whereeach process in that cycle must block. The simulation is now deadlocked. In Figure3.4 the JFK LP is waiting to receive a message from ORD, ORD is waiting for LAX,and LAX is waiting for JFK. Because no logical process can safely process anyevent, the simulation is frozen in this state, unable to advance forward, even thoughwhile simulation is not overwait until each FIFO contains at least one messageremove smallest time stamped message M from its FIFOclock  time stamp of Mprocess MFigure 3.3 Initial version of central event processing loop for a logical process.56 CONSERVATIVE SYNCHRONIZATION ALGORITHMS 3.2 DEADLOCK AVOIDANCE USING NULL MESSAGES 571iffiFKwaitingon OROFigure 3.4 Deadlock situation. Each process is waiting on the incoming link containing thesmallest link clock value because the corresponding queue is empty. All three processes areblocked, even though there are event messages in other queues that are wating to be processed.there are several events that have not yet been processed. In general, if there arerelatively few unprocessed event messages compared to the number of links in thenetwork, or if the unprocessed events become clustered in one portion of thenetwork, deadlock may occur frequently.This deadlock situation can be broken as follows Suppose that the minimumamount of time to fly from one airport to another is 3 units of simulation time, andJFK is at simulation time 5. This implies that any message JFK sends to LAX in thefuture must have a time stamp of at least 8 i.e., its current time plus the minimumflight time to reach LAX. This information is not sufficient for LAX to safelyprocess its next message with time stamp 10. However, because LAX now knowsits next event must have time stamp of at least 8, any message sent from LAX toORD must have a time stamp of at least 11 i.e., 8 plus the minimum flight time toreach ORD, or 3 units of simulation time. Because ORD is now guaranteed anyfuture message that it will receive from LAX must have a time stamp of 11, it cansafely process its message with time stamp 9, thus breaking the deadlock.Some mechanism is required for an LP to indicate to other LPs a lower bound onthe time stamp of messages it will send that LP in the future. Null messages can beused for this purpose. Null messages are used only for synchronization, and do notcorrespond to any activity in the physical system. In general, a null message withtime stamp Tnull that is sent from LPA to LPB is essentially a promise by LPA that itwill not send a message to LPB carrying a time stamp smaller than Tnull  How does aprocess determine the time stamps of the null messages it sends The clock value ofeach incoming link provides a lower bound on the time stamp of the nextunprocessed event that will be removed from that links buffer. When coupledwith knowledge of the simulation performed by the process for example, theminimum time for an aircraft to fly from one airport to another, this incomingbound can be used to determine a lower bound on the time stamp of the nextoutgoing message on each output link. Whenever a process finishes processing anevent, it sends a null message on each of its output ports indicating this bound thereceiver of the null message can then compute new bounds on its outgoing links,send this information on to its neighbors, and so on. It is typically up to theapplication programmer to determine the time stamps assigned to null messages.This is the essential idea behind the null message or ChandyMisraBryantalgorithm named after its inventors. One question that remains is when should nullmessages be sent One approach is to send a null message on each outgoing linkafter processing each event. This guarantees that processes always have updatedinformation on the time stamp of future messages that can be received from each ofthe other processes. Using this approach, the algorithm shown in Figure 3.3 can berevised to yield the algorithm shown in Figure 3.5. It should be observed thatincoming null messages are processed exactly the same as other messages that is,the LPs clock is updated, but no application code is executed to process themessage.An alternative approach to sending a null message after processing each event is ademanddriven approach. Whenever a process is about to become blocked becausethe incoming link with the smallest link clock value has no messages waiting to beprocessed, it requests the next message null or otherwise from the process on thesending side of the link. The process resumes execution when the response to thisrequest is received. This approach helps to reduce the amount of null message traffic,though a longer delay may be required to receive null messages because twomessage transmissions are required.As mentioned earlier, the air traffic example relied on the fact that the minimumamount of time for an aircraft to fly from one airport to another was 3 units oftime inorder to determine the time stamp of null messages. More generally, this algorithmrelies on a quantity called lookahead, defined below.Lookahead If a logical process at simulation time T can only schedule new eventswith time stamp of at least T L, then L is referred to as the lookahead for thelogical process.while simulation is not overwait until each FIFO contains at least one messageremove smallest time stamped message M from its FIFOclock  time stamp of Mprocess Msend null message to neighboring LPs with time stampequal to lower bound on time stamp of futuremessages clock plus lookaheadlFigure 3.5 ChandyIMisraBryant null message algorithm.58 CONSERVATIVE SYNCHRONIZATION ALGORITHMS 3.3 LOOKAHEAD AND THE SIMULATION MODEL 59In general, the lookahead for a logical process may change during the executionof the simulation. Here, we define lookahead in terms of a single logical process.This concept can easily be extended to define lookahead for all messages sent fromone LP to another. As will be discussed later, lookahead depends on the semantics ofthe simulation model.Returning to the null message algorithm, the time stamp of null messages can beset to the current time of the LP plus its lookahead. Continuing the air trafficexample, the lookahead of each LP is 3. When the JFK LP completes processing itsevent at time 5, it sends a null message to both LAX and ORD with time stamp 8.LAX will then process this null message, advance to simulation time 8, and send anull message to ORD and JFK with time stamp 11. ORD can now process the timestamp 9 nonnull message.The above example illustrates a very important point The performance of the nullmessage algorithm depends critically on the lookahead value. Suppose that thelookahead were 0.5 instead of 3. Then the following sequence of null messageswould be sent JFK to LAX time stamp 5.5, LAX to ORD time stamp 6.0, ORDto JFK time stamp 6.5, JFK to LAX time stamp 7.0, LAX to ORD time stamp7.5. Additional null messages would be generated because the LP must send one toboth LPs with each time advance. Five null messages must be transmitted in order toprocess a single event It is clear that smaller lookahead values would create evenlonger sequences of null messages before a nonnull message can be processed.Further suppose that the lookahead is zero that is, an LP at time T could schedulea new event with time stamp T. The null message algorithm will fail in that casebecause an endless sequence of null messages will be sent, all containing a timestamp of 5, cycling from JFK to LAX to ORD, back to JFK, and so on. Thus animportant limitation of the null message algorithm is that there cannot be any cycleof logical processes with zero lookahead. It can be shown, however, that the nullmessage algorithm will avoid deadlock if no such cycle exists.The restriction that there not be any zero lookahead cycles implies that certaintypes of simulations cannot be easily performed. One problematic situation is insimulations where LPs can request query other LPs for state variables. A query isusually implemented by the logical process requesting the state information LPAsending a message to the process holding the desired state LPB with time stampequal to T, LPAS current time i.e., zero lookahead is used. When LPBs receives themessage, it will have advanced to time T, and it will send a reply containing thedesired value with time stamp T i.e., again with zero lookahead. This creates a zerolookahead cycle. A simple solution is to include a small time stamp increment witheach message. However, while this avoids the zero lookahead cycle, as explainedearlier, small lookahead often leads to very poor performance for the null messagealgorithm.3.3 LOOKAHEAD AND THE SIMULATION MODELThe notion of lookahead is fundamental to conservative synchronization mechanisms. Consider the IOlical nrocess with the smallest clock value at some instant inthe execution of a parallel simulation program. Let the current simulation time ofthis LP be T. This LP could generate events relevant to every other LP in thesimulation with a time stamp of T. This implies that no LP can process any eventwith time stamp larger than T, or it may violate the local causality constraint.Similarly no LP can advance beyond simulation time T because it is then prone toreceiving notification of an event in its past.Lookahead is used to solve this problem. Let Ts be the current time of the LPwith the smallest clock value in the entire simulation. If each LP has a lookahead ofL, then this guarantees that any new message sent by an LP must have a time stampof at least Ts  L. This in tum implies that all events with time stamp in the intervalTs Ts L can be safely processed. L is referred to as the lookahead for the LPbecause it must be able to look ahead L time units into the future and schedule theevents at least L time units prior to when they actually happen.Lookahead is clearly very intimately related to details of the simulation model.Some examples of where lookahead may be derived are described below. Limitations concerning how quickly physical processes can interact with eachother. In the air traffic example, the minimum amount of time for an aircraft tofly from one airport to another determined the minimum amount of simulationtime that must elapse for one logical process to affect another. This minimumflight time was used to define a lookahead value. Physical limitations concerning how quickly one LP can react to a new event.Consider again the air traffic simulation. Suppose that the minimum amount oftime an aircraft must remain on the ground to exchange passengers is one unitof simulation time. Then, if the smallest time stamp of any arrival event thatwill be received in the future is T, no new aircraft will depart until T  1, andthe minimum time stamp of any arrival event it will schedule for anotherprocess is T  1  minimumtransiCtime. Thus the minimum amount of timeany aircraft remains on the ground further enhances that LPs lookahead. Tolerance to temporal inaccuracies. Suppose that an LP produces an event attime T, but errors of up to 1 unit in simulation time can be tolerated by thereceiver while still producing sufficiently accurate results. Then the LP mayschedule events 1 time unit into the future, providing a lookahead of thisamount. Nonpreemptive behavior. In the air traffic example, once an aircraft departedfrom one airport, nothing in the simulation model could prevent that aircraftfrom arriving at its destination airport at the assigned arrival time. If the modelincluded other events that preempted this behavior, such as at midflight, JFKcould divert an LAXbound aircraft to ORD, and arrive at ORD only 0.5 timeunits after the divert flight event, then JFKs lookahead would be reduced toonly 0.5. Thus the nonpreemptive nature of the original air traffic simulationenhances its lookahead. Precomputing simulation activities. If the events produced by an LP over thenext L units of time do not depend on external events but only on internalcomputations, these computations can be performed in advance, enhancing60 CONSERVATIVE SYNCHRONIZATION ALGORITHMS 3.4 DEADLOCK DETECTION AND RECOVERY 61lookahead. For example, if the time for an aircraft to fly to another airport weredrawn from a random number generator, the flight time could be selected inadvance, and this value rather than the smallest number that could be selectedfrom the generator can be used as the lookahead value.Lookahead can change dynamically during the execution. However, lookaheadcannot instantaneously be reduced. At any instant, a lookahead ofL indicates to thesimulation executive that the LP will not generate any new event with time stampless than T L, where T is the LPs current time. If the lookahead is reduced by Kunits of simulation time, the LP must first advance K units before this changedlookahead can take effect, so no events with time stamp less than T  L areproduced.3.4 DEADLOCK DETECTION AND RECOVERYAs discussed earlier, the principal disadvantage of the ChandyMisraBryantalgorithm is that a large number of null messages can be generated, particularly ifthe lookahead is small. Recall that the approach used in this algorithm is to avoiddeadlock situations. Another approach is to use the original algorithm shown inFigure 3.3 that is prone to deadlock but provide a mechanism to detect and recoverfrom deadlock situations. This approach is described next.3.4.1 Deadlock DetectionThe simulation computation depicted in Figure 3.3 is one example of a diffusingcomputation. This means the distributed computation consists of a set of processes,and processes only perform computations upon receiving one or more messages.Once initiated, the process continues with its local computation, sending andreceiving additional messages to other processes, until it again stops. Once a processhas stopped, it cannot spontaneously begin new computations until it receives a newmessage. The computation can be viewed as spreading or diffusing across theprocesses much like a fire spreading through a forest.A single controller process is introduced to the distributed simulation. Thedistributed simulation computation cycles through the following steps1. The computation is initially deadlocked.2. The controller sends messages to one or more LPs informing them thatcertain events are safe to process, thereby breaking the deadlock. More will besaid about this later.3. The LPs process the events that have been declared safe. This typicallygenerates new messages that are sent to other LPs that hopefully cause themto process still more events, and generate additional messages to still otherLPs. The spreading of the computation to previously blocked processes isviewed as constructing a tree. Every process that is not blocked is in the tree.Whenever a message is sent to a process that is not in the tree, that process isadded to the tree, and logically a link is established from the process sendingthe message to the process receiving the message. LPs that are in the tree arereferred to as being engaged. Processes that are not in the tree are referred toas being disengaged.4. Just as the tree expands when the diffusing computation spreads to new LPs, italso contracts when engaged LPs become blocked. Specifically, if an LPbecomes blocked, and that LP is a leafnode in the tree, the LP removes itselffrom the tree and signals its parent the LP that originally sent it the messagethat caused it to become engaged that it is no longer in the tree. An LPbecomes a leaf node in the tree when all of the LPs it added to the tree signalthat they have removed themselves from the tree.5. If the controller becomes a leaf node in the tree, then the computation is againdeadlocked, completing the cycle.A signaling protocol is used to implement the paradigm described above. Specifically, each LP adheres to the following rules When an engaged process a process already in the tree receives a message, itimmediately returns a signal to the sender to indicate the message did not causethe tree to expand. When a disengaged process receives a message, it becomes engaged it doesnot return a signal to the sender until it becomes disengaged. Each LP maintains a count indicating the number of messages that it has sentwithout receiving a signal. When this count is zero, the LP is a leaf node in thetree. If the LP is blocked and its count becomes zero, it becomes disengaged, soit sends a signal to the process that originally caused it to become engaged.An example depicting a scenario with four logical processes is shown in Figure3.6. In a the computation is initially deadlocked and all four processes aredisengaged. The controller sends a message to process 3 to break the deadlock. Inb process 3 is now engaged i.e., is in the tree and the computation diffuses tothe other three processes as process 3 sends a message to each of them. Figure 3.6cshows the engagement tree including all four processes. In d several independentactions occur. Process 2 sends a message to process 4 however, process 4 is alreadyin the tree, so process 4 immediately returns a signal to process 2 not shown.Processes 1 and 3 both become blocked because of an empty FIFO queue. At thispoint, process I is a leaf node in the tree, so it sends a signal to process 3, the processthat originally caused I to become engaged. Process 3, however, is an interiornonleaf node of the tree, so it does not send any signal. The end result of theseactions is process I becomes disengaged, as shown in Figure 3.6e. In this figure,processes 2 and 4 also become idle and send signals to process 3, since they are leafnodes of the tree. As shown in f, both 2 and 4 become disengaged. Process 3 is62 CONSERVATIVE SYNCHRONIZATION ALGORITHMS 3.4 DEADLOCK DETECTION AND RECOVERY 63 C is defined as the number of messages received from neighbors that have notyet been signaled. D is defined as the number of messages sent to other processors from which asignal has yet to be returned the number of descendants in the tree.node of the tree if its C value is greater than zero i.e., it is engaged and its D valueis zero.Sending a message to another process causes D in the sender to be incrementedby one. C in the receiver is also incremented by one when the message is received.When a process sends a signal to another process, C in the sender is decremented by1, and D in the receiver is decremented by 1. It must always be the case that either Cis greater than zero the process is engaged and hasnt returned a signal for themessage that caused it to become engaged, or D is equal to zero. If C is zero, then itmust be the case that D is also zero, and the process is disengaged. When C and D inthe controller are both zero, the simulation is deadlocked.3.4.2 Deadlock RecoveryThe deadlock can be broken by observing that the messages contammg thesmallest time stamp in the entire simulation is are always safe to process. Thisis the event that would be processed next in a sequential execution of the simulationprogram. Thus, to break the deadlock, the simulation executive need only identifythe event containing the smallest time stamp and send a message to the LPsholding the event to indicate that the event can now be safely processed.Locating the smallest timestamped event is relatively straightforward because thecomputation is deadlocked, so no new events are being created while the smallesttime stamped event is being located. The controller can broadcast a message to all ofthe LPs requesting the time stamp of the event within that processor containing thesmallest time stamp. After receiving a message from each processor, the controllerdetermines the smallest timestamped events in the entire simulation, and instructsthe processorss that hold them to process the eventss. This approach assumes thatthere are no messages in transit in the network while the deadlock is being broken.Depending on details of the communication subsystem, which in tum depends onthe hardware architecture, this mayor may not be the case. We will return to thissubject later.The broadcast could be performed by having the controller directly send amessage to every other processor in the system, or by constructing a spanning tree,that is, a tree with processors as the tree nodes and the controller at the root thatincludes all processors in the system as shown in Figure 3.7. The links of the tree canbe defined arbitrarily they need not correspond to links between logical processes.The controller initiates the broadcast by sending a request minimum time stampmessage to each descendent processor in the tree. Upon receiving this message, eachprocessor forwards the message to each of its descendants in the tree. Processors thatare leaf nodes in the tree do not forward the request to other processors. Instead, eachleaf node processor returns a message to its parent in the tree indicating the timestamp of the smallest timestamped event in that processor. Each processor that isnot a leaf node of the tree waits until it has received such a reply message fromeach descendant in the tree, and computes the minimum among 1 the time stamp oflocal events within that processor and 2 the time stamp value in each of the replymessages it receives from descendants in the tree. Thus each processor computes theUiSengagedngagednd busy.,ngagedBand blocked.message send.arc in treeecR..anow a leaf node of the tree and is idle, so it sends a signal to the controller process,indicating that the entire computation is again deadlocked.To implement this signaling protocol, each LP must be able to determine whetherit is engaged or disengaged, and if it is engaged, whether or not it is a leaf node ofthe tree. Two variables are defined for this purposeThe approach used in the signaling protocol is the following An LP assumes thateach message it sends causes the receiver to become engaged. The receiver returns asignal if either 1 it is already engaged or 2 it is becoming disengaged because it isa leafnode ofthe tree and it is blocked. An LP is engaged if C is greater than zero. IfC is equal to 0, the process is disengaged, and D must also be zero. An LP is a leafFigure 3.6 Example of deadlock detection algorithm. a Controller initiates process 3 afterdeadlock, b 3 is added to engagement tree, 3 sends messages to 1, 2, and 4 to spreadcomputation, c engagement tree includes all processors, d 1 and 3 become idle, 2 sends amessage to 4, but 4 is already in tree, e 2 and 4 become idle, j 3 becomes idle and thecomputation is deadlocked.64 CONSERVATIVE SYNCHRONIZATION ALGORITHMS 3.5 SYNCHRONOUS EXECUTION 65processorsI,...IIProcesssor 1  .. processor executes,,arrier primitive , I,   IIProcesssor 4 1 Processsor 3 .Processsor 2 1wallclock timeIn the deadlock detection and recovery algorithm the processors repeatedly cyclethrough phases of 1 one or more processors processing simulation events and2 deadlock resolution. Several conservative synchronization algorithms utilize thisapproach of cycling between phases, but they explicitly control when the entirecomputation stops rather than relying on the system becoming deadlocked. Tocontrol the latter, these algorithms rely on a mechanisms called barrier synchronizations.A barrier is a general parallel programming construct that defines a point inwallclock time when all of the processors participating in the computation havestopped. As shown in Figure 3.8, when a processor executes the barrier primitive, itblocks, and remains blocked until all of the processors have executed the barrierprimitive. The barrier operation is completed when all of the processors haveexecuted the barrier primitive each processor is then allowed to resume execution,starting at the statement immediately following the barrier.available, this information can be used to expand the set of safe events when thedeadlock is broken. For example, if the lookahead of each LP is L and the LPfurthest behind in the execution has a clock value of T, then all events in the intervalT, T  L1are safe and can be processed.An important observation in the deadlock detection and recovery algorithm is thatit utilizes the time stamp of the next unprocessed event. This information was notused in the null message algorithm. Consider a set of logical processes, each withlookahead of 1, all blocked at simulation time 10. Suppose that the time stamp of thenext unprocessed event is 100. Unless this information is used, there is no means toimmediately advance all of the LPs to time 100. Instead, the LPs are doomed toadvance only in increments of the lookahead value until they reach 100.3.5 SYNCHRONOUS EXECUTIONFigure 3.8 Sample execution of processors entering a barrier. The solid horizontal lineindicates that the processor is executing the white space indicates that the processor isblocked, waiting for the barrier to complete.min9minimum among all processors in the subtree rooted by that processor. Thisminimum time stamp value is reported by that processor to its parent in the treein its reply message, as shown in Figure 3.7. In this way the global minimumcomputation propagates up the tree, and the controller computes the globalminimum. The controller can then broadcast this global minimum back down thespanning tree, indicating that all processors with an events with time stamp equalto this global minimum can safely process that events. Thus the treebasedalgorithm uses three rounds of messages to break the deadlock 1 messagesinitiating the global minimum computation flowing down the tree, 2 replymessages to compute the global minimum flowing up the tree, and 3 restartmessages to instruct processors which events are safe to process again flowing downthe tree.One drawback of the algorithm described above is that it is overly conservative inthat it only specifies the smallest time stamped events as being safe to process.Using lookahead information, a larger set of safe events can usually be obtained. Ingeneral, any algorithm that is able to define a set of safe events among a collection ofblocked logical processes can be used. One such algorithm based on a concept calleddistance between processes is described later in this chapter.The deadlock detection and recovery approach described above also relies on theentire computation becoming deadlock before it attempts to break the deadlock. Analternative approach is to detect deadlock among a subset of logical processes, andthen break these local deadlocks as they occur. Detecting partial deadlocks ismore complex than detecting deadlocks of the entire system, however, and the extracomplexity required to perform this computation may not result in a significantperformance improvement. Although some parallel simulation systems have beenproposed using this approach, few have been realized, so this subject will not bepursued further.Unlike the null message algorithm, the deadlock detection and recovery algorithm described above allows the lookahead of the simulation application to be zero.Cycles of zero lookahead LPs are permitted with this algorithm. If lookahead isFigure 3.7 Example of deadlock recovery mechanism. Each node of tree represents aprocessor, and the number in each node indicates the time stamp of the smallest timestampedeven in that processor. Arrows indicate communications in the second round to compute theglobal minimum.66 CONSERVATIVE SYNCHRONIZATION ALGORITHMS 3.5 SYNCHRONOUS EXECUTION 67The barrier synchronization is a useful primitive because it defines a point inwallclock time where all the processors are at a known point in their execution. Onetypical use is to divide the execution of a parallel program into a sequence of steps,where each step involves a parallel computation that must be completed beforeexecution moves on to the next step so that successive steps do not interfere witheach other. Barriers will be used for this purpose here. Specifically, the parallelsimulation program executing on each processor can be structured as shown inFigure 3.9.The barriers ensure that no events are being processed, and thus no new events arebeing created, while the simulation executive is trying to determine which events aresafe to process.In Figure 3.9 processing safe events is identical to what was done before Eachprocessor executes simulation application code to model the occurrence of eachevent. The principal question concerns the method for determining which events aresafe to process. As discussed earlier in the context of deadlock recovery, the smallesttime stamped event in the entire simulation is clearly safe to process. Lookahead isused to identify other events that are safe to process.We next describe techniques for implementing the barrier primitive, particularlyon distributedmemory computers. Two approaches for determining the set of safeevents are then discussed.3.5.1 Centralized BarriersThere are two important issues that must be addressed in implementing the barrierprimitive. The first concerns controlling the blocking of processors entering thebarrier, and releasing the processors after the barrier has been achieved. The secondconcerns ensuring that there are no messages lingering in the network, referred to astransient messages, when the processors are released from the barrier. Threeapproaches are described next.A simple approach to implement the barrier is to designate one of the processorsin the simulation to be a global controller. When a processor enters the barrierprimitive, it sends a synchronization message to the controller processor, and waitsfor a response. When the controller has received such a message from everyprocessor participating in the simulation including itself, it broadcasts a releasewhile simulation in progressidentify all events that are safe to processbarrier synchronizationprocess safe eventsbarrier synchronizationFigure 3.9 Parallel simulation program using barrier synchronizations.message indicating the global synchronization point has been reached. Uponreceiving the release message, each processor continues execution starting at thestatement immediately following the barrier.In a sharedmemory multiprocessor, the equivalent to the central controllerapproach is to utilize global synchronization variables. For example, two counterscan be maintained. A variable called Blocked indicates the number of processorsthat have reached the barrier point. The second counter called Released indicatesthe number of processors that have l reached the barrier point, 2 detected that allof the processors have reached the barrier point, and 3 proceeded beyond thebarrier. Both counters are initialized to zero. Assume that there are N processors inthe system. The condition Blocked equal to N for any value of Released indicatesthat all processors have reached the barrier point, and it is used to signal thatprocessors can proceed beyond the barrier. Before a processor can initiate a barrieroperation, it must wait until Released is equal to o. This is necessary to avoidstarting a new barrier operation before all processors have been released from theprevious one. Once Released is equal to 0, the processor enters the barrier byincrementing Blocked. This must be done as an atomic operation to avoid raceconditions. The processor then waits until Blocked becomes N. When the lastprocessor to reach the barrier point increments Blocked to N, the processors can bereleased from the barrier. At this instant, Released will still be equal to zero. Eachprocessor that is released from the barrier increments Released again as anatomic operation and resumes execution beyond the barrier. The last processor to bereleased from the barrier detects that it is setting Released to become equal to N,so it resets both variables to zero. Setting Released equal to 0 arms the barrierfor the next operation.The principal drawback with the centralized approach is that it does not scale tolarge numbers of processors, since the central controller, or the shared variables in asharedmemory machine, become a bottleneck. The controller must perform N  1message sends and receives on each barrier operation.3.5.2 Tree BarrierThe bottleneck problem is easily solved by organizing the processors as a balancedtree with each node of the tree representing a different processor see Fig. 3.10 forthe case of fourteen processors. When a leaf processor reaches the barrier point, itsends a message to its parent processor. Each interior node of the tree waits until itreceives a synchronization message from each of its children. When it has receivedsuch a message from each child, and has itself reached the barrier point, it sends amessage to its parent. The barrier is achieved when the root of the tree is at thebarrier point and has received a synchronization message from each child node.Once the root detects the achieved barrier, it broadcasts a release message to all ofthe other processors. This can be done by propagating the release messages down thetree, reversing the flow of messages used to detect the achievement of the barrier.This treebased barrier mechanism requires approximately time 2 logk N where k isthe degree of the tree, and 2N  I messages for N processors each processor68 CONSERVATIVE SYNCHRONIZATION ALGORITHMSa3.5 SYNCHRONOUS EXECUTION 69Figure 3.10 Processors organized into a tree to implement the barrier primitive.except the root sends one message up the tree to its parent and receives onebroadcast message coming down the tree. Although Figure 3.10 shows a binary tree,in general, any node degree can be used. In fact the centralized approach describedearlier is an N  1 ary tree, with the controller at the root node, and the remainingN  1 nodes the children of the controller.3.5.3 Butterfly BarrierAnother approach that eliminates the broadcast to notify the processors that a globalsynchronization has been achieved is the butterfly barrier. Assume that an Nprocessor barrier is performed, and the processors are numbered 0, 1, 2, ... , N  1.To simplify the discussion, assume that N is a power of 2 it is straightforward toextend the approach to arbitrary N. The communication pattern among processorsfor this barrier mechanism for the case of eight processors is shown in Figure3.11 a. Each processor executes a sequence of log N pairwise barriers with adifferent processor at each step. A pairwise barrier between processors i and isaccomplished by simply having i or send a message to i when it has reachedthe barrier point, and then wait for a message from i indicating that processor hasalso reached the barrier point. In the first step, processors whose binary addressesdiffer only in the least significant bit perform a pairwise barrier for example,processors 3 011 and 2 OIQ perform a pairwise barrier. In the second step,processors whose addresses differ only in the second least significant bit forexample, processor 3 011 and 1 OI synchronize. In general, in step k processori synchronizes with the processor whose address differs in only the kth bit wherebits are numbered 1,2, ... , 10gN from least significant to most significant. Thesepairwise synchronizations continue for log N steps until all of the address bits havebeen scanned. This communication pattern is referred to as a butterfly. Eachprocessor is released from the barrier once it has completed the log N pairwisebarriers.level 3lcvcl2blevellFigure 3.11 Eightprocessor Butterfly barrier. a Communications pattern, and illustrationof barrier from the perspective of processor 3 b tree abstraction of barrier mechanism.To see why this algorithm works, consider the operation of the barrier mechanismfrom the perspective of a particular processor. The highlighted nodes and arcs inFigure 3.11a illustrate the barrier from the perspective of processor 3. After step 1has completed, processor 3 knows that processor 2 as well as itself has reached thebarrier point. This is illustrated by the dashed box around processors 2 and 3 in step1 of Figure 3.11a. After step 2, processor 3 receives a message from processor 1,so it knows processor 1 has also reached the barrier. However, processor 1 must havesynchronized with processor 0 in step 1 before it could have synchronized withprocessor 3 in step 2, so processor 3 can conclude 0, 1,2, and 3 have all reached thebarrier point. This is represented by the dashed box around these four processors instep 2. Continuing this analysis, after step 3 is completed, processor 3 infers fromreceiving a synchronization message from processor 7 that processors 4, 5, 6, and 7have also reached the barrier, so it can safely conclude that all eight processors havereached the barrier point. In effect, as shown in Figure 3.11 b, a tree is constructedin bottomup fashion with processors at the leaves. Intermediate nodes of the treeindicate the set of processors that are known to have reached the barrier when thatstep of the algorithm has been completed.70 CONSERVATIVE SYNCHRONIZATION ALGORITHMS 3.5 SYNCHRONOUS EXECUTION 71The butterfly barrier mechanism requires flog Nl steps to complete, andtransmission ofNflogNl messages because each processor must send and receiveone message in each step of the algorithm. 73.5.4 Transient MessagesA transient message is a message that has been sent but has not yet been received bythe destination processor. They are, in effect, in the network. Transient messagesare an issue if asynchronous message sends are allowed that is, the sender is allowedto execute after performing a message send without waiting for the receiver toacknowledge receipt of the message. Asynchronous sends are particularly useful indistributed computing systems for example, networks of workstations because thedelay in waiting for the receiver to send an acknowledgment for a message may belarge.Unless properly accounted for, transient messages can cause errors in thesynchronous execution mechanism protocol shown in Figure 3.9. The basic problemis that transient messages may not be properly accounted for in the computation todetermine which events are safe to process. For example, consider the case where allof the LPs advance to simulation time T, one LP sends a message with time stampT  1, and all the others generate messages with time stamp T  10. Assume that allof these messages are transmitted to their destination processor except the messagewith time stamp T  1, which is delayed in the network. All processors now enterthe barrier primitive, are subsequently released from the primitive, and begin thecomputation to determine the events that are safe to process. Because the time stampT  1 message has not been received, it is not taken into account in this computation. This would result in the processors erroneously believing the time stampT  10 events are all safe to process, since there are none with a smaller time stamp.It may be noted that this problem does not exist if synchronous message sends areused, since the processor sending the time stamp T  1 message would block untilthis message is received at its destination. This prevents each processor fromentering the barrier until all messages it has sent have been received.This problem can be solved without giving up asynchronous message sends byusing message counters. Each processor maintains two local counters indicating lthe number of messages it has sent, and 2 the number of messages it has received. 8There are no transient messages in the system when 1 all of the processors havereached the barrier point and thus are not producing new messages, and 2 the sumof all of the send counters across all of the processors is equal to the sum of thereceive counters across all of the processors.Tree Barriers The mechanism for ensuring that the total of the send and receivecounters match can be combined with the barrier primitive. Consider the tree barrier.When a leaf processor sends a message up the tree to indicate that it has reached the7 rXl pronounced ceiling of X denotes the smallest integer greater than or equal to X.8 A simple optimization to this approach is to maintain one variable indicating the difference betweenthese two quantities.barrier point, it also transmits its send and receive counters. Each processor at aninterior node of the tree sums the counters it receives from the child processors in thetree with its own counters, and sends the two sums to its parent in the tree. Using thisapproach, the root will hold the total sum of the send and receive counters across allof the processors. These counters may not match at the root, however, because aprocessor may not have received one or more transient messages when it added thevalues of its local counters into the sum. To address this problem, if any processorreceives a message after it has sent its counters to its parent in the tree, it sends aseparate message to the root denoting this fact, causing the total count of the numberof received messages to increase by one. When the root detects that the total sendand receive counters match, it broadcasts a message indicating that the barrier hasbeen reached and that there are no more transient messages in the system.Butterfly Barriers A more complex mechanism is required when the butterflybarrier is used. The butterfly barrier can be viewed as a sequence of barriers, eachcovering a successively larger set of processors. For example, as shown in Figure3.11 b, from processor 3s perspective the barrier is achieve by first performing abarrier among processors 2, 3, next among processors 0, 1,2,3, and then among0, 1,2, ... ,7. The path up the tree dictates the sets of processors that mustsynchronize in order to achieve global synchronization. Define Gki as the group ofsynchronizing processors at level k of the tree that includes processor i, orequivalently, the kth level node of the tree that includes leaf node i as a descendent.For example, G13  2,3, G23  0, 1,2,3, and G33  0, 1,2, ... , 7. Toaccomplish synchronization at level k of the tree, processor i communicates with theprocessor with the same binary address as i except in bit position k to accomplishsynchronization with half of the processors in Gki. Define this set of siblingprocessors with which processor i is attempting to achieve synchronization in step kas Ski. For example, SI3  2, S23  0,1, and S33  4, 5, 6, 7. Thisnotation is illustrated in Figure 3.12 each shaded box represents a processor. Inlevel klevel kla  TotaISendkib  TotaISendklic  TotaISendk.lnkliFigure 3.12 Notation for detecting transient messages in the butterfly barrier.72 CONSERVATIVE SYNCHRONIZATION ALGORITHMS 3.5 SYNCHRONOUS EXECUTION 73general, SkU is the sibling node to Gk 1U, where level 0 is the level containing theleaf nodes. Let nkU be the processor with the same binary address as i, except for bitposition k k  1, 2, .... nkU is referred to as the neighbor to processor i duringstep k of the algorithm. Thus, in step k of the algorithm for example, step 2,processor i for example, processor 3 must wait for a message from nki nz3 isprocessor 1 see Fig. 3.11b in order to synchronize with SkU Sz3  0, 1,thereby ensuring synchronization among the processors in GkU Gz3 0, 1,2,3. Observe that Gk Ji  SknkU and that GkU  SkU U SknkU,Finally it can be seen that during step k of the barrier processor i is attempting toachieve synchronization among processors in sibling nodes of the tree that is, Skiwhich includes nkU and Sknki which includes i.One can extend the butterfly barrier algorithm to accommodate transientmessages by preventing a processor from advancing to the next step if there areany transient messages among the set of processors trying to synchronize in thecurrent step. Specifically, a processor i is not allowed to advance beyond step k ofthebarrier algorithm unless1. the processors in Gki have all reached the barrier point just as before, and2. there are no messages in transit between any two processors 1 and m where1, mE GkU.When processor i reaches step k, there cannot be any transient messages amongprocessors within Gk JU i.e., SknkU see Fig. 3.12 because processor i could nothave completed step k  1 if there were any. Similarly there cannot be any transientmessages within SkU when nkU completes step k  1. Therefore, to ensure thatthere are no transient messages among processors in Gki, each processor i E GkU,must ensure that there are no transient messages traveling from Sknki to SkU andvice versa. This is accomplished by maintaining four sets of counters. First, SendkUk  1,2, ... , 10gN denotes the number of messages processor i has sent to aprocessor in SkU, and ReceivekU denotes the number of messages processor i hasreceived from a processor in SkU. To guarantee that there are no transient messagesbetween SknkU and SkU, one must verify that1. L SendkU summed over all i E Sknki is equal to L ReceivekJ summedover all j E SkU, and2. L SendkJ for all j E SkU is equal to L ReceivekU for all i E SknkU,To compute these conditions, define TotalSendkU as L SendkU summed over alli E SknkU, and define TotaIReceivekJ  L Receivekj summed over allj E SkU. A key observation is that these quantities can be defined recursively seethe dashed arcs labeled a, b, and c in Fig. 3.121. Tota1Sendki  TotalSendkJi  TotalSendkJnkJ i,2. TotalReceivekj  TotalReceivekJ j  Tota1ReceivekJ nkJ j,3. TotalSendJU  SendJU, and4. TotalReceiveJ U  Receive U.Operationally each processor maintains the log N element Send and Receivearrays as it sends and receives simulation messages. Sending and receivingsynchronization messages are not included in these counters. Consider the operationof processor 3 in executing the barrier algorithm. When it begins the barrier, it sendsits arrays Sendk3 and Receivek3 for all k which are equivalent to TotaISendk3and TotaIReceivek3, respectively, to its neighbor, processor 2, and waits to receivearrays TotaISendk2 and TotaIReceivek2 from processor 2 if they havent alreadybeen received. Processor 3 then compares the first elements of the arrays. IfTotalSendJ3 is equal to TotalReceiveJ 2, and TotalReceiveJ 3 is equal toTotalSendJ2, then there are no messages in transit between processors 2 and 3,so each processor can advance to the next step in the algorithm. In particular,processor 3 adds TotaISendk2 and TotaIReceivek2 to its local arrayTotaISendk3 and TotaIReceivek3. At this point, the TotalSend and TotalReceivearrays in processors 2 and 3 are identical. Processor 3 then sends its new TotalSendand TotalReceive arrays to processor 1, its neighbor in step 2 of the algorithm. Theabove steps are repeated, except now, the second elements of each arrayTotaISendz3 and TotaIReceivez1, and TotaIReceivez3 and TotaISendz1are used to determine if transient messages remain between the sets 0, 1 and2, 3. This process repeats for log N steps. After successfully completing the laststep, each processor can advance beyond the barrier point, with the knowledge thatall processors have reached the barrier point and no transient messages remain.If in step p of the algorithm TotalSendpU and TotaIReceiveinii orTotalReceiveii and TotaISendpnii do not match, then there is at least onemessage in transit from processor i to nii or processor npU to i. In this case, thebarrier operation has failed, and the processor must wait for additional messagesbefore it can proceed. Specifically, if a processor receives a new simulation message,then this indicates that the initial TotalReceive vector it sent in the first step of thealgorithm was incorrect, so the processor aborts the barrier operation and starts overfrom the beginning step 1. If a processor receives new TotalSend and TotalReceivearrays signaling that it was previously passed incorrect information in step k, then itmust return to step k and repeat the barrier from step k onward. This requires aprocessor to maintain a copy of its TotalSend and TotalReceive vectors after eachstep, however. An alternative approach is to simply abort the barrier computation andrestart it from the beginning. In either case, the barrier is, in effect, rolled back to anearlier point in time and restarted. Because of the use of rollback this barriermechanism is sometime called an optimistic barrier. It should be noed, however,that the simulation program is not rolled back, only the barrier computation itself.These mechanisms enable one to define a barrier where it is guaranteed that thereare no transient messages in the system when the processors are released from thebarrier. We next describe algorithms for determining the set of events that are safe toprocess. We will return to the transient message problem again in the next chapter74 CONSERVATIVE SYNCHRONIZATION ALGORITHMS 3.5 SYNCHRONOUS EXECUTION 75when we discuss algorithms for computing a quantity called Global Virtual Time insystems using optimistic synchronization.while simulation in progressS  set of events in the processor wi th time stamp, TMinFigure 3.13 A simple synchronous protocol. Events in the shaded region are safe to process.3.5.5 A Simple Synchronous ProtocolA simple approach using lookahead to determine safe events can be derived byparallelizing the sequential event processing loop. Consider a sequential simulationthat has advanced to simulation time T, which is the time of the next unprocessedevent in the event list. If the constraint is made that an event must be scheduled atleast L units of simulation time into the future, then it can be guaranteed that all newevents that are later scheduled in the simulation will have a time stamp greater thanor equal to T  L, so any event with time stamp less than T  L can be safelyprocessed.In the parallel simulation, assume, as before, that the simulation is composed ofsome number of logical processes. Each logical process LPi defines a lookaheadvalue Li Let Ti be the smallest time stamp of any unprocessed event in LPi . LetTM  LM be the minimum of Ti  Li over all of the LPs in the simulation. This is theminimum time stamp of any new event that will be later generated in the execution.Then, as illustrated in Figure 3.13, all events with time stamp less than or equal toTM  LM those in the shaded rectangle in the figure are safe to process because anynew event must have a time stamp larger than TM  LM beyond the shadedrectangle.The program executed by each processor using this protocol is shown in Figure3.14. A computation is required to compute a global minimum across the simulation.This can be implemented by extending any of the barrier algorithms described earlierto compute a minimum value among the processors entering the barrier. Forexample, using the tree barrier algorithm, each processor in the tree would 1compute the minimum among all child processors in the tree if there are any, and itsown local minimum, and 2 report the minimum of these values to its parent in thetree. The minimum computed by the processor at the root of the tree is the globalminimum, which is then broadcast to all of the other processors. Each processorwould then process all events with time stamp less than TM  LM , the result of theglobal minimum computation.This approach requires few restrictions other than lookahead. The topologyamong logical processes is arbitrary, and can change during the course of thesimulation. Further LPs need not schedule new events in time stamp order. The costof avoiding these restrictions is that there is no opportunity to exploit suchinformation when it is available. Extensions to the synchronous protocol to exploittopology information are described next.Consider application of the synchronous protocol described in the previous sectionto the air traffic simulation discussed earlier including LPs for LAX, ORD, and JFK.Suppose that a fourth airport is added, in San Diego, called SAN. Recall thatlookahead is derived from the minimum amount of time required for an aircraft to flyfrom one airport to another. Suppose that the minimum flight time from SAN to LosAngeles LAX is 30 minutes this is clearly the minimum lookahead of any airportin this example. Further suppose that at some point in the simulation there are onlytwo unprocessed events, one in SAN with time stamp 10 00, and a second in JFKwith time stamp 10 45. It is clear that the event in SAN cannot affect the event inJFK, since an aircraft requires several hours to fly from San Diego to New York, soone should be able to process these two events concurrently. Yet, using the protocoldescribed earlier, no event in the system with time stamp larger than 10 30 can beprocessed in the current iteration of the algorithm, since the minimum time stamp ofthe next event that can be scheduled is 1030. To circumvent this problem, additionalinformation concerning which LPs can schedule events for which others must beprovided.To verify that the event in JFK cannot be affected by the event in SAN, we need toknow the smallest amount of simulation time that must elapse for an event in SAN toaffect JFK. Suppose that there is no link from SAN to JFK because the model doesnot include any direct flights between these two airports. SAN could affect JFK byscheduling an event in LAX, which then schedules another event in JFK. Consideration of all paths from SAN to JFK specifically, SAN to ORD to JFK must also beconsidered allows one to determine the minimum amount of simulation time thatmust elapse for an event in SAN to affect JFK.This idea is captured in the notion of distance between processes which providesa lower bound in the amount of simulated time that must elapse for an event in oneprocess events in S3.5.6 Distance between Logical Processesbarrier synchronizationFigure 3.14 Synchronous simulation protocol.simulation timeTLPALPBLPc76 CONSERVATIVE SYNCHRONIZATION ALGORITHMS 3.5 SYNCHRONOUS EXECUTION 77process to affect another. If the network of LPs is not fully connected, that is, if thereis not a link between every pair of LPs, an event will have to propagate through oneor more additional LPs before it can reach a specific LP. Distance informationprovides a means of exploiting information concerning the topology of logicalprocesses to derive better bounds on the time stamp of events that can arrive in thefuture, and in principle, it allows one to derive larger numbers of events that are safeto process in the synchronous execution approach.Here, it is assumed that there is a fixed network of logical processes. If logicalprocess LPA can send a message to LPB , then there is a link from LPA to LPB . Alookahead LAB is associated with each link that is, messages sent from LPA to LPBmust have a time stamp of at least LAB larger than LPAS current time. For notationalconvenience, we assume that there is at most one link from LPA to LPB Distance is defined as follows If a path exists from LPA to LPz traversing in succession logical processesLPA , LPB , LPc, ... ,LPy,LPz, then DABc...yz is defined as LAB L BC  ... Lyz  DAB, the distance from LPA to LPB , is defined as the minimum ofDpath over allpaths from LPA to LPB The distance between pairs of processes can be encoded in a matrix called thedistance matrix. The entry in row i and column indicates the minimum distance Dijfrom LP j to LP. Figure 3.15b shows the distance matrix for the network shown inFigure 3.15a. For example, there are two paths from LPA to LPo, oflengths 3 viaLPd and 4 via LPB. The distance from LPA to LPo is the minimum of these twovalues, or 3. In general, if there is no path from LP j to LP, then the distance Dij isdefined as 00.In simulations containing a large number of logical processes relative to thenumber of processors there will be many LPs mapped to each processor. In this caseit may be more efficient to consider distances between processors than processes. Inessence the LPs mapped to a single processor can be viewed as a superlogicalprocess that includes many LPs for the purpose of determining which events aresafe to process. This has the disadvantage that it may be overly conservative indetermining which events are safe to process. In other words, each LP implicitlyassumes that all events within that LP must be processed in time stamp order, butthis may not be necessary if the LP models several independent components.The distance from an LP to itself is computed by determining the minimumlength cycle that includes this LP. For example, for LPA in Figure 3.15, there are fourcycles including LPA LPA  LPB  LPA length 7, LPA  LPc  LPA length4, LPA  LPB  LPo  LPc  LPA length 9, and LPA  LPc  LPo LPB  LPA length 11, so DAA is 4.An event in LPo with time stamp TD depends on can be affected by an event inLPA with time stamp TA if TA  DAD  To. For example, in Figure 3.15a the timestamp II event in LPA could cause a new event to be sent to LPo with a time stampas small as 14. Thus the time stamp 15 event in LPo depends on the time stamp IIevent in LPA Conversely, the time stamp 13 event in LPo does not depend on thetime stamp II event in LPA because the minimum distance from LPA to LPo is 3.An event E in LPx is said to be safe if it is not possible for a new event to begenerated and sent to LPx that contains a time stamp smaller than Es time stamp.Each logical process can determine which events within that LP are safe toprocess if it can determine a lower bound on the time stamp of any message that LPwill later receive. Let LBTS j be the lower bound on the time stamp of any messageLP j can receive in the future. All events in LP j with time stamp less than LBTS j aresafe. Let Tj be the smallest time stamped event in LP j . Tj is defined as 00 if there areno events in LP j  ThenNote that the minimum computation includes the case where is equal to i. This isnecessary to account for the possibility of an event in LP j causing a message to besent to one or more other processors that results in another message that is sent backto LPj . Also this equation assumes that there are no transient messages in thenetwork. Thus it is assumed that this equation will be applied after a barriermechanism has been used to ensure there are no such messages.For example, in Figure 3.15, LBTSA is 15 TA  DAA, LBTSB is 14 TA  DAB,LBTSc is 12 TA  DAd and LBTSo is 14 TA  DAD This implies that the timestamp 11 even in LPA and the time stamp 13 event in LPD can be safely processed.The time stamp 15 event in LPD is not safe, verifying the analysis that wasperformed earlier.The approach described above requires each processor to communicate withevery other processor in the system to obtain their T values to compute which events34a 4 12 LPD..2 IUBLPA LPs LPG LPDLPA 4 3 1 3bLPs 4 5 3 1 min 12, 31LPG 3 6 4 2LPo 5 4 2 4Figure 3.15 a Network of logical processes indicating lookahead on each arc. The boxesrepresent events with time stamp II, 13, and 15 b distance matrix for this network of LPs.LBTS  minT D..I all  13.178 CONSERVATIVE SYNCHRONIZATION ALGORITHMS 3.6 BOUNDED LAG 79 15 14c 12 14Figure 3.16 Computation for computing LBTS values. a Each LP sends messagesindicating a lower bound on the time stamp of the sent messages if this value is not 00.b Each LP updates its LBTS, and if its value decreased, it sends messages indicating a newlower bound on the time stamp of these messages. c No new messages are generated thefinl THTS Vll1f hve heen comnuted.are safe to process. This generates N 2 messages each time the set of safe events mustbe computed. This limits the applicability of this algorithm to systems containing amodest number of processors. An approach using time windows will be discussedlater to alleviate this problem.Another drawback with this approach is the distance matrix must be recomputedif lookaheads change during the execution of the simulation program. This problemcan be addressed by using an alternate method for computing LBTS values, depictedin Figure 3.16. An approach reminiscent of the null message algorithm and similarto Dijkstras algorithm for computing shortest paths is to have each LP initialize itsLBTS value to 00. Then each LP sends a message to the other LPs to which it maysend messages indicating the smallest time stamp on any message it may send in thefuture, assuming that it does not receive any new messages initially, the time stampof the next unprocessed event plus the links lookahead. An LP receiving such amessage will use this information to determine if its local LBTS value should beupdated reduced. If the LPs LBTS value is reduced, and the LP now discovers thatit could send a smaller timestamped message than what it had previously reported toits neighbors, it sends new messages indicating a new lower bound on the time stampof messages it could later send. This process continues until no additional messagesare generated, at which time each LP has computed its LBTS value, and thecomputation completes.For example, in Figure 3.l6a, LPA may send messages with time stamp 14 and12 to LPB and LPC, respectively, because of its local event with time stamp 11, andits lookahead to LPB is 3, and to LPc is 1. Similarly LPD may send messages withtime stamp 17 and 15 to LPB and LPC, respectively. No messages are sent by LPBand LPc because they cannot generate a lower bound on the time stamp of futuremessages other than 00. In Figure 3.16b the LBTS values of LPB and LPc areupdated to the smallest value among the messages they received. LPB and LPc nowsend additional messages indicating lower bounds on the time stamp of messages itmay later send. Because LPB may receive a message with time stamp 14 its newLBTS value, it could generate messages with time stamp 18 to LPA lookahead 4and 15 to LPD lookahead 1. Similarly LPc could generate new messages with timestamp 15 to LPA, and 14 to LPD . These messages cause the LBTS values ofLPA andLPD to change, as shown in Figure 3.16c. However, this new LBTS value is higherthan the time stamp of events already buffered in each LP, so the lower bound onfuture messages it might send is not reduced any further. Because no new messagesare generated, the computation is now complete.It is instructive to compare the synchronous style of execution with the deadlockdetection and recovery approach described earlier. Both share the characteristic thatthe simulation moves through phases of 1 processing events and 2 performingsome global synchronization function to decide which events are safe to process.The two methods differ in the way they enter into the synchronization phase.In the best case the detection and recovery strategy will never deadlock,eliminating most of the clock synchronization overhead. In contrast, synchronousmethods will continually block and restart throughout the simulation. On the otherhand, the synchronous methods do not require a deadlock detection mechanism.However, an important disadvantage of the detection and recovery method is thatduring the period leading up to a deadlock when the computation is grinding to ahalt, execution may be largely sequential. This can severely limit speedup.9Synchronous methods have some control over the amount of computation that isperformed during each iteration, so, at least in principle, they offer a mechanism forguarding against such behavior.9 Amdahls law states that no more than kfold speedup is possible if lkth of the computation issequential.3.6 BOUNDED LAGConsider a large network of logical processes where the distance between pairs oflogical processes varies widely from one pair to another. For instance, if we extend...abthe air traffic simulation described earlier to a simulation of the global air trafficnetwork, there is a direct relationship between the physical distance between airportsand the minimum distance in simulation time required for two airport LPs tointeract, assuming as before that interactions occur through aircraft flying betweenthe airports. Consider a parallel simulation where all LPs are at simulation time T. Itis clear that air traffic now departing from Tokyo International airport cannot affect aflight arriving at LAX 30 minutes from now however, a flight that just left SanDiego airport destined for LAX could affect this incoming flight.Thus it is curious that a simulation of the global air traffic network using thesynchronous parallel simulation algorithm based on equation 3.1 must collectinformation from every other processor in the system i.e., every other airport inthe world before it can determine which local events are safe to process.Specifically, in the above scenario where flights are converging on LAX, thesimulation algorithm requires that LAX solicit information from Tokyo Internationalbefore it certifies that the arrival event at LAX 30 minutes from now is safe toprocess. As mentioned earlier, requiring each processor to collect information in thisway from every other processor prevents the algorithm from scaling to largenumbers of processors. Thus a mechanism is needed that will reduce interprocessorcommunication so that one can determine which events are safe.The reason that the algorithm using equation 3.1 must collect information fromevery other processor is because no consideration is made on how far into the futureone should check for the safety of a local event. For example, while it is intuitivelyclear that a recent Tokyo departure cannot affect an arrival at LAX occurring 30minutes from now, such a departure could affect LAX arrivals that occur 24 hoursfrom now. Because the synchronization algorithm makes no distinction betweennearfuture and farfuture events, it must check all logical processes in theentire simulation to determine whether its farfuture events are safe to process.Because farfuture events are unlikely to be safe, expending much effort todetermine if these events are safe to process is usually wasteful.A simple approach to improving the efficiency of the synchronization algorithmis to introduce an interval also commonly referred to as a window of simulationtime extending from the time stamp of the smallest event in the simulation Ts toTs  Tw, where Tw denotes the size of the window. Events with time stamp largerthan Ts  Tware not considered for execution in this iteration of the algorithm. Thusthe simulation executive need not determine the safety of farfuture events theseare events with time stamp larger than Ts  Tw. Because events with time stampbeyond Ts  Tware never processed in the current iteration, no LP can advance itslocal simulation time clock more than Tw units of simulation time ahead of anotherLP. For this reason Tw is also referred to as a bounded lag in the simulation it limitshow far behind one LP can lag behind another.Events with time stamp less than or equal to Ts  Tware called nearfutureevents, and those with time stamp greater than Ts  Tware called farfuture events.Farfuture events are automatically assumed to be unsafe. The synchronizationalgorithm only attempts to determine the safety of nearfuture events.3.7 CONDITIONAL VERSUS UNCONDITIONAL INFORMATIONIfDXY  Tw , then LPy need not check LPx to determine if its near future eventsare safe. This is because LPxs next event must have a time stamp of at least Ts . Thisevent cannot affect any event in LPy with time stamp less than Ts  DXY , which isgreater than Ts  Tw since Dxy  Tw . But LPy is only considering the safety ofnearfuture events that have a time stamp less than Ts  Tw. Thus LPx is too faraway to affect any events that LPy is considering for execution during this iteration,so LPy need not check LPx when checking the safety of its events.Using time windows, LPy need only check LPx if DXY S Tw. Thus equation 3.1is modified to equation 3.2 below813.2LBTS j  min Ij  DjJall whereDjiS.TW3.7 CONDITIONAL VERSUS UNCONDITIONAL INFORMATIONIt is sometimes useful to distinguish between conditional and unconditionalinformation in the simulation. Unconditional information is that which can beguaranteed to be true based on local information. For example, if an LP hasadvanced to simulation time T, and it has a lookahead of L, then the LP canunconditionally guarantee T  L is a lower bound on the time stamp of messagesthat it may generate in the future. Only unconditional information was transmittedamong logical processes via null messages in the ChandyMisraBryant algorithm.Conditional information is information provided by a logical process that is onlyguaranteed to be true if some predicate is true. In the example illustrated in Figure3.16, each LP initially sends a message to neighboring LPs equal to the time stampof its next event plus the lookahead for the link on which the message is sent. Thisinformation is a lower bound on the time stamp of future messages sent over that linkprovided that LP does not receive any messages in the future with time stampsmaller than its next local event. In this sense the lowerbound information sent bythe LP is conditioned on the fact that it could receive a new message in the futurewith time stamp smaller than its next local event. As the algorithm proceeds, LPsmay receive new information concerning events it could receive in the future thatmay cause it to reduce the conditional lowerbound information it had sent inFor example, in Figure 3.15, if Tw is 4 and DCB is 6, then LPB does not need tocheck LPC in computing LBTSB to determine which events are safe to process.An important question concerns setting the size of the time window. If thewindow is too small, there will be too few events within the window that areavailable for concurrent execution. On the other hand, if the window is too large, thebenefits afforded by the window are lost because the simulation mechanism willbehave in much the same way as if no time window were used at all. In general, onemust carefully tune the simulation executive for each application to set the windowto an appropriate size that both achieves a reasonable amount of concurrentexecution and limits the overhead in performing the LBTS calculations.CONSERVATIVE SYNCHRONIZATION ALGORITHMS8082 CONSERVATIVE SYNCHRONIZATION ALGORITHMS 3.8 DYNAMIC PROCESSES AND INTERCONNECTIONS 83previous messages. It is only when each LP has determined its true LBTS value thatit can unconditionally guarantee the lower bound on time stamps of future messagesit will send in the future.The fact that the original ChandyMisraBryant algorithm only transmitsunconditional guarantees on the time stamp of future events, and the synchronousalgorithm described above allows logical processes to transmit conditional information is a key distinction between these algorithms. It allows the simulation executiveto avoid the simulation time creep problem where LPs only advance in lookaheadincrements to advance to the time stamp of the next unprocessed event.3.8 DYNAMIC PROCESSES AND INTERCONNECTIONSThe discussion thus far has assumed a static topology that is, the processes and linksamong processes are known prior to the execution and do not change during theexecution of the simulation program. This is acceptable for certain classes ofapplications, such as a simulation of a wired telecommunication network utilizingsome fixed topology. For other applications the interactions between logicalprocesses may vary over time. For example, if an LP were used to model eachaircraft in the air traffic simulation discussed earlier, interactions among LPs woulddepend on the physical proximity of aircraft to airports, which will changedramatically throughout the simulation.One simple approach to allowing dynamic creation and destruction of logicalprocesses and links is to initially create all logical processes and links that may beneeded during the entire execution. This allows existing synchronization algorithmsto be used more or less as is. A pool of unused LPs is created at the beginning ofthe execution, and is used as new LPs are required. This approach requires one to beable to place an upper bound on the number of processes that will be required.Unless one can guarantee a priori that certain pairs of LPs will never need toexchange messages during the execution, a fully connected topology where each LPcan send a message to any other LP must be used. This can lead to inefficiencies incertain synchronization algorithms. For example, each of the unused logicalprocesses may be required to send null messages to the other LPs.A more flexible approach is to allow new LPs and connections to be establishedand joined to the existing network of logical processes during the execution. Withrespect to synchronization, creation of new LPs does not produce problems so muchas establishing new connections. The central problem that must be addressed is thatany new connection to an existing LP provides a new source of messages for that LP.Let the sending LP refer to the LP on the sending side of the new connection, andthe receiving LP be the LP on the receiving side. Precautions must be taken toensure that no messages are sent on the new connection in the past of the receivingLP. Two approaches to preventing causality errors such as this are described below.The first constrains the behavior of the receiving Lp, and the second constrains thebehavior of the sending LP.1. Receiver constrained. One can prevent the receiving LP from advancing toofar ahead of all potential sending LPs in order to ensure that it does notreceive messages in its past. For example, if the receiving LP is constrained sothat it cannot advance more then L i units of time ahead of each potentialsending process LPi , where Li is the lookahead for LP i , then it is guaranteedthat the receiving LP will not receive any new messages in its past.2. Sender constrained. One can constrain the sending LP so that it cannot sendmessages in the past of the receiving LP. If the sending logical process LPs isat simulation time Ts and the receiving process LPR is at simulation time TRwhen a new connection is established from LPs to LPR, then the lookahead onthe newly established connection must initially be set to be at least TR  Ts toprevent LPR from receiving messages in its past.The first approach described above is somewhat restricting in that it does not allowLPs to advance more than a lookahead amount ahead of other LPs. The secondapproach allows LPs greater flexibility to advance further ahead of others butprovides less control because there is, in general, no limit on how large TR  Ts canbe the initial lookahead on the new connection other than the lookahead valuesalready in place on other existing links.One example of the receiver constrained approach is called a connection transferprotocol. This approach is based on the assumption that logical process LPs cannotsend a message to logical process LPR unless LPs first obtains a handle a referenceto LPR . An important restriction is that logical processes cannot autonomouslymanufacture new handles. Rather, handles can only be obtained by either creating anew LP or receiving a handle from another LP via a message. It is assumed an LPcan only send messages to LPs for which it holds a handle. The parallel simulationexecutive keeps track of which handles are owned by each LP in order to determinethe topology among LPs.To illustrate connection transfer, suppose logical process LPA owns a handle toLPR , indicating LPA can send messages to LPR . Assume LPA is at logical time TA ,and it has a lookahead of LA, and LPA is not blocked waiting for a simulation timeadvance. This implies TR S TA  LA where TR is the current logical time ofLPR . Toestablish a new connection from a third logical process LPs to LPR, LPA can sendLPs a copy of its handle. In order to prevent LPs from sending messages into the patofLPR , this new connection must first be recorded within the simulation executive toensure LPs is taken into account when computing LBTS values for LPR. After thishas been accomplished, a copy of the LPR handle can be transmitted to LPs. LPscannot start using this handle until it reaches simulation time TA  maxO, LA  L s.This constraint guarantees that LPs will not send a message into a past of LPR .A key observation in this transfer protocol is that LPAS existing connection withLPR is used to prevent LPR from advancing too far ahead and LPAS communication with LPs, prevents LPs from lagging too far behind when the newconnection is established. This enables one to avoid causality errors when LPs sendsmessages to LPR . Thus, LPA plays a critical role in this mechanism.84 CONSERVATIVE SYNCHRONIZATION ALGORITHMS 3.9 REPEATABILITY AND SIMULTANEOUS EVENTS 85An alternate approach is to use time windows to establish new connections.Recall that a time window of size Tw prevents any LP from advancing more than Twunits of time ahead of any other LP. This constrains the advance of LPs but allows anLP to advance more than a lookahead amount ahead of other LPs because Tw maybe larger than the lookahead. To prevent an LP from receiving a message in its past,the sender LP is required to use a lookahead of at least Tw when the new connectionis first established. Specifically, if the sender is at time Ts when it establishes the newconnection, the first message it sends on the new connection must have a time stampof at least Ts  Tw. Because no LP can be more than Tw units of time ahead ofanother LP, this guarantees that the new message will not be in the past of thereceiving LP. As the sending LP advances its simulation time past Ts, the lookaheadfor this new connection can be decreased by an equal amount.The windowbased approach described above provides a means for establishingnew connections among existing LPs. When a new LP is created, one must ensurethat the new LP is initialized to have a simulation time no less than TL , the loweredge of the time window. This is easily accomplished by requiring that the logicalprocess that created the new process do so by scheduling an event into its future. Thetime stamp of this create process event defines the initial simulation time of thenew LP.Finally a mechanism is required to ensure that proper destruction of LPs andorconnections. This is relatively straightforward. Destroying a connection is equivalentto setting the lookahead for the connection to infinity. This ensures that the receivingLP will not block on the connection. Destroying an LP can be accomplished bydestroying all connections to and from the LP, thus removing the LP from thesystem.3.9 REPEATABILITY AND SIMULTANEOUS EVENTSIn some cases it is important that repeated executions of the simulation programusing the same external inputs and initial state produce exactly the same results oneach execution. For example, the U.S. Department of Defense sometimes usessimulations to make acquisition decisions, so the General Accounting Office GAOmay reexecute simulations to verifY the results that are produced. Further, repeatability simplifies debugging the simulation program because errors can be reproduced. In addition, if the simulation should produce repeatable results but does not,this may immediately flag a bug in the simulation code or the simulation executive.Recall that the synchronization protocols described thus far attempt to producethe same results as a sequential execution of the same simulation program whereevents are processed in time stamp order. If no two events contained the same timestamp, then the results of the simulation would always be repeatable if thecomputation performed by each event is repeatable. This latter condition is actuallya nontrivial matter. For example, if the simulation program executes on differentCPUs, differences in floating point roundoff error could result in nomepeatableexecutions. Here, it is assumed that individual event computations are repeatable. So,to ensure that executions of the parallel simulation produce exactly the same results,it is sufficient to ensure that events containing the same time stamp at any logicalprocess are processed by that process in the same order from one execution to thenext. Events containing the same time stamp are referred to as simultaneous events.3.9.1 Using Hidden Time Stamp Fields to Order Simultaneous EventsOne approach to ensuring that simultaneous events are processed in the same orderfrom one execution to the next is to extend the time stamp field to include additional,lowerprecision bits that are hidden from the application program. The simulationexecutive can automatically assign values to these bits to, in effect, ensure that notwo events in the system contain the same time stamp. The simulation application isconstrained to process events in increasing time stamp order, where the time stampnow includes these hidden bits. The value placed in these lowerprecision bits mustbe assigned so that the time stamp values including the hidden bits are consistent forcausally dependent events. For example, if the computation for an event EIschedules a new event E2 to contain the same time stamp excluding the hiddenbits as the original, the hidden bits in the time stamp ofEl must be larger than thosein E I .One can append two tiebreaking fields to the application defined time stampcalled the age and id, with the age field given precedence assigned to moresignificant bits over the id field. Events that exist at the beginning of the simulationare assigned an age of 1. If an event with time stamp T and age A schedules anotherevent with the same time stamp ignoring the extra fields, the new event is assignedan age ofA  1. If the new event has a time stamp larger than T, the age of the newevent is 1.The age field ensures that events always schedule other events with higher timestamps, but it does not ensure uniqueness. For example, an event with time stamp T,and age 5 could schedule two new events also with time stamp T. In this case bothevents will have an age of 6. The id field ensures uniqueness. This field is actually atuple with two components S, i. S is an identifier that indicates the logical processscheduling the event. It is assumed that this identifier remains the same from oneexecution to the next. The i field is a counter indicating this is the ith event scheduledby the process.3.9.2 Priority NumbersWhile hidden bits can be used to ensure repeatable orderings of simultaneous events,a critical drawback with this approach is that it assigns the task of orderingsimultaneous events to the simulation executive, which typically has no knowledgeof the semantic meaning of the events. The proper ordering of simultaneous eventsshould normally be controlled by the simulation application. For example, consider asimulation that is used to make acquisition decisions by comparing two competingweapons systems. If two simultaneous events denoting detonations at a target occur,the kill may be credited to the system associated with the first detonation event86 CONSERVATIVE SYNCHRONIZATION ALGORITHMS 3.10 PERFORMANCE OF CONSERVATIVE MECHANISMS 873.10 PERFORMANCE OF CONSERVATIVE MECHANISMSdeparture eventfor previous jobramvarlLP,TTQ Now  current simulation time S  service time for job NWait  number of jobs in queue Busy  true if server busyArrival Eventif not BusyBusy  trueschedule departure event at NowSelseNWait  NWaitlDeparture Eventschedule Arrival event at LP2 at Nowif NWaitONWait  NWaitlschedule departure event at NowSelseBusy  falseTQSLookahead plays a crucial role in the performance of conservative synchronizationalgorithms. Further the simulation application must be written in such a way as tomaximize its lookahead. This often has a profound effect on the way the simulationprogram is developed.To illustrate this, consider the simulation of a queuing network. Here, each stationin the queuing network consists of a single server that services incoming jobs one ata time, and an unlimited capacity queue holding jobs waiting for service see Fig.3.17. For example, a runway in the air traffic example presented earlier can beFigure 3.17 Classical approach to modeling a queuing network server using arrival anddeparture events.3.9.3 ReceiverSpecified OrderingAnother approach to treating simultaneous events is to have the simulation executivedeliver all such events to the simulation application, and then force the application toorder the events in a manner consistent with the objectives of the simulation. Forexample, in the above example, a random number generator might be used todetermine which system is credited with the destruction of the target. To ensurerepeatability, the simulation application needs only to ensure that the algorithm usedto order the simultaneous events is repeatable. It is noteworthy that if this approach isused to order simultaneous events, the simulation executive does not need to deliverthe events to the simulation application in a repeatable fashion, since the applicationwill order the events using its own criteria once the events are passed to it. Theexecutive must only be able to guarantee to the application that it has delivered allsimultaneous events, so that the application can then order and process these events.One slight complication to the scheme described above is that the simulationexecutive cannot guarantee that it has delivered all simultaneous events to a logicalprocess if zero lookahead is allowed. To see this, consider the following scenarioThe simulation executive guarantees some logical process LP that it has passed it allsimultaneous events with time stamp T. LP advances to time T to order and processthese events. LP now generates a new event E at time T this is possible becausezero lookahead is allowed. A message for E is received and processed by anotherprocess LPz. LPz now returns a message to LP with time stamp T. This contradictsthe original claim that LP had received all events with time stamp T One solutionto this dilemma is to require that a logical process temporarily have nonzerolookahead whenever it requests to receive all simultaneous events at a given timestamp. The lookahead may return to zero once the LP has advanced its simulationtime.that is processed. The simulation executive might use a scheme for orderingsimultaneous events that systematically favors one system over the other. Thiscould lead to misleading results if many simultaneous events occur, which might bethe case if a coarse timing model with limited temporal precision is used.One approach to specifying the ordering of simultaneous events is to allow thesimulation application to define a priority number for each event, with lowerprioritynumbered events processed before those with a higherpriority number. In effect thepriority field serves as the lowprecision bits of the time stamp. A potential drawbackwith this approach is that the sending process is responsible for assigning the timestamp of the event. It is often more convenient to have the receiving processprioritize events because, in general, only the receiver knows what other events occurat the same time stamp, and the appropriate priority might depend on the state of thereceiver. If the priority depended on the state of the sender, the sender could alwaysinclude such information as a data field within the event itself, but it is much moredifficult to have the sender assign a priority based on state information within thereceiver.89fork Now  current simulation time S  service time for job Done  time server will become idleArrival Eventif Now  Done  if server busy Done  DoneSschedule arrival event at LP2 at Doneelse  server is idle Done  NowSschedule arrival event at LP2 at DoneFigure 3.18 Optimized queuing network simulator for firstcomefirstserve queues.TTQSTQ3.10 PERFORMANCE OF CONSERVATIVE MECHANISMS3.21. These measurements show the performance of the parallel simulator using thedeadlock detection and recovery algorithm executing on a sharedmemory multiprocessor with five processors, one for each of the logical processes shown in Figure3.19. A closed queuing network is simulated, with the number of jobs circulatingamong the queues left as a parameter.Figure 3.19 Central server queuing model. The fork module routes incoming jobs to one ofits output ports. Here, jobs are equally likely to be routed to either port. The merge modulejoins incoming stream of jobs into a single output stream.CONSERVATIVE SYNCHRONIZATION ALGORITHMS88modeled as a queue, with jobs representing aircraft and the server modeling therunway itself. If a job arrives while the server is busy with another job, the newlyarriving job is placed into the queue where it waits to receive service. Here, assumethat the server processes the jobs in the queue in firstcomefirstserve order, or moreprecisely, the queue is a firstinfirstout buffer.The classical approach to modeling a single queue is to use two types of eventsarrival events to signify the arrival of a new customer at the station and departureevents to represent a customer completing service, and leaving the station, typicallyto advance to another station. In the air traffic simulation presented in Chapter 2, thearrival and landed events are equivalent to the arrival and departure events that areused here, respectively. The sequence of events for a single job arriving at a station,receiving service, and departing from the station is shown in Figure 3.17. An arrivalevent occurs at time T. If the server is idle, the job immediately begins service andschedules a departure event S time units into the future, where S is the amount oftime the job receives service. If the server is busy, the job is added to the queue.After Q units of time, the job ahead of this one in the queue will complete serviceand process a departure event at time T  Q. That departure event schedules a newdeparture event for this job S time units into the future. The departure event alsoschedules an arrival event at the current time the delay to move between stations isassumed to be zero denoting the departing jobs arrival at the next station.One can optimize the simulation program shown in Figure 3.17 by eliminatingthe departure event. An optimized program is shown in Figure 3.18. Exploiting thefact that the station uses a firstcomefirstserve policy, one need only maintain astate variable called Done indicating when the server would have served all jobscurrently in the queue. If a new job arrives at time T and T is greater than Done,then the server must be idle when the job arrives, and the newly arriving jobcompletes service at time NowS. Otherwise, the server is busy until the time storedin the Done variable. The newly arriving job will therefore depart at time DoneS.In either case the departure event can be eliminated because it is not needed todetermine when the job departs. It should be noted that this optimization reliesheavily on the fact that a firstcomefirstserve service discipline is used. Theoptimization would not be possible with other disciplines, such as lastinfirstout orpreemptive scheduling, because the departure time of the job depends on what otherjobs arrive subsequent to this one.The simulation program shown in Figure 3.17 has poor lookahead propertiesbecause LP1 must advance to simulation time T  Q S before it can generate thenew arrival event with time stamp T  QS. The LP has zero lookahead withrespect to scheduling this event. On the other hand, the simulation program shown inFigure 3.18 has good 100kahead because the program attempts to schedule events farinto the simulated future. One way of viewing lookahead is to observe that the arrivalevent at time T  Q  S is invariant to any other events occurring in the intervalT, T  Q S. This allows the event to be generated at time T.These two programs exhibit dramatically different performance. Performancemeasurements of the execution ofeach of these simulation programs in modeling thecentral server queuing network in Figure 3.19 are shown in Figure 3.20 and Figure90 CONSERVATIVE SYNCHRONIZATION ALGORITHMS 3.11 SUMMARY AND CRITIQUE OF CONSERVATIVE MECHANISMS 91Figure 3.20 Number of messages processed between deadlocks for parallel simulation ofcentral server queuing network for the simulation programs shown in Figure 3.17 and Figure3.18. 8x8 BiasedILAR0.9 8x8 UniformILAR0.18 8x8 BimodalILAR0.1....... 4x4 BiasedILARO.9....... 4x4 UniformILAR0.18 4x4 BimodalILARO.I169 ,...eOUo12 ,......C.lTI 6tT.4 8 12Number of ProcessorsFigure 3.22 Performance of the null message algorithm for synthetic workloads withdifferent degrees of lookahead.optimizeddeterministic servicetmeOptimized exponentialservice time.....Classical determini sticservice time.......Classical exponentialservice time11 2 4 8 16 32 64 128 256Number of Jobs in Network10000 rraTI 1000 ff.faloCDc. 1001JlCDg 10 ,CD2Figure 3.21 Speedup relative to a sequential execution of the parallel simulation of thecentral server queuing network.Figure 3.20 gives the number of messages that are processed between deadlocksfor different numbers of jobs. This number should be maximized, since deadlockrepresents an overhead that does not occur in a sequential execution. Shown arecurves for the classical implementation using both arrival and departure events andthe optimized version using only arrival events. The service time is selected eitherdeterministically or from an exponentially distributed random variable. It can be seenthat when there are more than a few jobs circulating through the network, thesimulation application that is optimized to exploit lookahead is far more efficientthan the unoptimized simulation that is to say, deadlocks occur much lessfrequently. The speedup measurements in Figure 3.21 show that the optimizedsimulation executes two to three times faster than the sequential execution forreasonably large job populations, but the unoptimized version executes much moreslowly than the sequential execution.The curves for the optimized simulation in Figure 3.20 illustrate a kind ofavalanche effect. This means that the efficiency of the simulator is poor for smallmessage populations but improves dramatically once the population reaches acertain critical level.It should be highlighted that the simulation used in these experiments was onewhere it was possible to design the application to exploit lookahead. The resultingsimulation program is somewhat fragile in that seemingly modest changes in themodel, such as the addition of job preemption invalidate this optimization.Figure 3.22 shows the speedup obtained for an experiment using the null messagealgorithm in executing a synthetic workload. The workload consists of a collectionof LPs configured in a toroid topology. A fixed number of messages migrates atrandom throughout the network. Each event schedules exactly one new event. Thedistribution used to select the time stamp of the new event was a parameter in theseexperiments. Lookahead is characterized by a value called the Inverse LookaheadRation ILAR which is defined as the minimum of the time stamp incrementdivided by its mean. High ILAR values up to 1.0 correspond to good lookahead,while low values correspond to poor lookahead. As can be seen, dramaticallydifferent performance results are obtained depending on the ILAR value, whichindicates the lookahead in the simulation.3.11 SUMMARY AND CRITIQUE OF CONSERVATIVE MECHANISMSThis chapter introduced the synchronization problem which has been a focal point ofmuch of the work to date concerning the execution of analytic discrete eventsimulation programs on parallel and distributed computers. The central goal of thesynchronization algorithm is to satisfy the local causality constraint, namely toensure that each LP processes events in time stamp order. Once this has beenaccomplished, one can ensure that the concurrent execution of the simulationprogram will produce the same results as a sequential execution. This chapterptimizeddeterministic servicetme........Optimized exponentialservice time.......Classical exponentialservice time.....Classical deterministicservice time4O,rr....r...,1 2 4 8 16 32 64 128 256Number of Jobs in Network31c.lTI3l 2 fc.92 CONSERVATIVE SYNCHRONIZATION ALGORITHMS 3.11 SUMMARY AND CRITIQUE OF CONSERVATIVE MECHANISMS 93focused on conservative synchronization algorithms that prevent the local causalityconstraint from ever being violated.The key observation made in the original ChandyMisraBryant null messagealgorithm Bryant 1977 Chandy and Misra 1978 was that the principal informationeach LP must derive is a lower bound on the time stamp LBTS of messages that itmay later receive. Armed with this information, the LP can then determine whichevents can be safely processed. The failing of these initial algorithms was in onlyutilizing the current simulation time of each LP and lookahead information incomputing LBTS values. Without any additional information the best one was ableto guarantee was the smallest time stamped message produced by an LP was itscurrent time plus lookahead. This information was then conveyed to other LPs in theform of null messages. The resulting cycles of null messages could severely degradeperformance.Newer algorithms circumvented this problem by also including information onthe time stamp of the next unprocessed event in computing LBTS values. Thisallowed the synchronization protocol to immediately advance simulation time to thetime stamp of the next unprocessed event, just as sequential simulators are able todo. Collecting next event information in this way is much more straightforward if thecomputation is stopped at a barrier because a snapshot of the entire simulation canbe easily made, greatly simplifying the task of determining the smallest timestamped event in the system. Synchronous algorithms such as Lubachevskysbounded lag Lubachevsky 1989, Ayanis distance between objects algorithmAyani 1989, and the synchronous algorithm described in Section 3.5.5, which issimilar to Nicols YAWNS protocol Nicol 1993 and Steinmans Time Bucketsprotocol Steinman 1991, all exploit this fact. Implementing a barrier, and makingsure that there are no transient messages in the system when the barrier is realizedthen becomes important, at least on distributed memory machines where transientmessages may arise and be difficult to detect. A barrier mechanism developed byNicol provides a solution to this problem Nicol 1995.Underlying all of these techniques is the requirement that the simulation containgood lookahead properties, or there is little hope of achieving much concurrentexecution, except in specialized cases such as when there are many events with thesame time stamp. Different restrictions may be placed on the behavior of the parallelsimulator in order to make lookahead information more easily known to thesynchronization protocol. Examples include specifying the topology among logicalprocesses, thereby restricting which LPs may send messages to which others, orspecifying distances between LPs. While firstgeneration algorithms utilized restrictions such as no dynamic creation of processes or static network topologies, andrequired messages sent over a link to have nondecreasing time stamps, theselimitations were not fundamental to conservative algorithms and could be overcomeby various techniques. Similarly it was observed that techniques could be developedthat ensure simulation executions could be repeated Mehl 1992.Conservative synchronization algorithms have advanced to a state where they areviable for use in realworld simulation problems. Nevertheless, some fundamentallimitations of these algorithms exist. Perhaps the most obvious drawback ofconservative approaches is that they cannot fully exploit the concurrency that isavailable in the simulation application. If it is possible that event EA might affect EBeither directly or indirectly, conservative approaches must execute EA and EBsequentially. If the simulation is such that EA seldom affects EB , these eventscould have been processed concurrently most of the time. In general, if the worstcase scenario for determining when it is safe to proceed is far from the typicalscenario that arises in practice, the conservative approach will usually be overlypessimistic, and force sequential execution when it is not necessary. In this senseconservative algorithms use a Murphys law approach to executing the simulationany possible scenario that could lead to violation of the local causality constraintmust be prevented.Another way of stating this fact is to observe that except in a handful of specialcases such as feedforward networks without cycles, conservative algorithms rely onlookahead to achieve good performance. If there were no lookahead, the smallesttimestamped event in the simulation could affect every other pending event, forcingsequential execution no matter what conservative protocol is used. Consider a fullyconnected network topology. If the logical process furthest behind in the simulationis at time Ts and that LP has a lookahead of L, then the simulation executive canonly guarantee the safety of those events whose time stamp lies in the intervalTs, Ts L. In effect, the lookahead defines a time window where events with timestamp within this window can be safely processed concurrently. The larger that Lis,the more events there are that can be processed concurrently. Characteristics such aspreemptive behavior diminish the lookahead properties of the simulation. Conservative algorithms struggle to achieve good performance when the simulationapplication has poor lookahead, even if there is a healthy amount of parallelismavailable.A related problem faced by conservative methods concerns the question ofrobustness. Seemingly minor changes to the application may have a catastrophiceffect on performance. For example, adding short, highpriority messages thatinterrupt normal processing in a computer network simulation can destroy thelookahead properties on which the model relied to achieve good performance,leading to severe performance degradations. This is problematic because experimenters often do not have advance knowledge of the full range of experiments thatwill be required, so it behooves them to invest substantial amounts of effort toparallelize the application if an unforeseen addition to the model at some future datecould invalidate all of this work.Perhaps the most serious drawback with conservative simulation protocols is therequirement that the simulation program be designed to maximize its lookahead toachieve acceptable performance. A natural approach to programming an LP is tohave it update state variables as the LP advances, and generate state updatemessages with time stamp equal to the LPs current simulation time for thosevariables of interest to other LPs. This leads to simulations with zero lookahead,which in tum may limit concurrent execution to only those events that have the sametime stamp. To increase lookahead, the modeler must design the simulation programso that it can predict state updates at an earlier simulation time then the messages94 CONSERVATIVE SYNCHRONIZATION ALGORITHMS 3.12 ADDITIONAL READINGS 95can be generated in advance with a suitable lookahead value. Further it is up to themodeler to guarantee that it can generate events sufficiently far in advance so thatlookahead guarantees made to the simulation executive can be maintained.Ideally one would separate the development of the simulation application fromthe mechanisms used in the underlying simulation executive. For example, sequential simulation programs normally do not need to be concerned with the datastructure used to implement the pending event list. Writing the application in a wayto maximize its lookahead often leads to a complex, fragile, code that is difficultto modify and maintain. Thus far, techniques to automatically determine thelookahead in the application or restructure the simulation code to improve lookaheadhave had only limited success.3.12 ADDITIONAL READINGSIndependently Chandy and Misra Chandy and Misra 1978 and Bryant Bryant1977 developed the original null message algorithm that bears their names. Earlywork in defining the synchronization problem and determining conditions fordeadlock are also described in Peacock, Wong et al. 1979. Variations on the nullmessage algorithm, including many aimed at reducing the number of messages thatare generated are described in Misra 1986, Bain and Scott 1988, Su and Seitz1989, Cai and Turner 1990, DeVries 1990, Preiss, Loucks et al. 1991, Yu,Ghosh et al. 1991, and Blanchard, Lake et al. 1994. Other early proposals forsynchronization algorithms are described in Reynolds 1982, Nicol and Reynolds1984, Groselj and Tropper 1988, Jones, Chou et al. 1989, and Zeigler and Kim1996. A categorization of algorithms, both conservative and optimistic, isdiscussed in Reynolds 1988. The scheme to use hidden fields in the time stampto order simultaneous events is taken from Mehl 1992.The deadlock detection and recovery algorithm is described in Chandy and Misra1981, and the DijkstraScholton algorithm for detecting deadlock is described inDijkstra and Scholten 1980. A variation on this algorithm that detects localdeadlocks deadlocks among a subset of the logical processes is described in Liuand Tropper 1990. Limitations of the amount of parallelism that can be extractedby these algorithms in simulations of queueing networks are described in Wagnerand Lazowska 1989.Early experimental work discussing empirical performance evaluations of the nullmessage and deadlock detection and recovery algorithms are described in Reed,Malony et al. 1988 and Fujimoto 1989, demonstrating the importance oflookahead in achieving speedup. Work in detecting and improving the lookaheadproperties of simulation applications, often with good success in speeding up thecomputation are described in Groselj and Tropper 1986, Nicol 1988, Cota andSargent 1990, Lin and Lazowska 1990, and Wagner 1991.The importance of utilizing information concerning the next unprocessed event,in addition to only considering the current simulation time of each LP, has been longrecognized in the field. This is articulated in Chandy and Sherman 1989 where asynchronization algorithm called the conditional event algorithm is described.Algorithms using global synchronizations to determine events that are safe toprocess began to appear in the late 1980s. Lubachevskys bounded lag algorithmLubachevsky 1989 was perhaps the first, with the distance between objectsalgorithm appearing shortly thereafter Ayani 1989. The synchronous protocoldescribed in Section 3.5.5 is similar to the two protocols that appeared in 1990, theYAWNS protocol analyzed in Nicol 1993 and the Time Buckets protocol describedin Steinman 1991. Barrier algorithms and mechanisms to eliminate transientmessages are in the general parallel computation literature. The butterfly barrieralgorithm described here is based on the algorithm described in Nicol 1995.Connection transfer protocols to dynamically change LP connections are describedin Bagradia and Liao 1994 and Blanchard and Lake 1997.CHAPTER 4Time WarpConservative synchronization algorithms avoid violating the local causalityconstraint, whereas optimistic algorithms allow violations to occur but provide amechanism to recover. Jeffersons Time Warp mechanism was the first and remainsthe most wellknown optimistic synchronization algorithm. Although many othershave been proposed, many of the fundamental concepts and mechanisms used bythese algorithms such as rollback, antimessages, and Global Virtual Time GVTfirst appeared in Time Warp. This chapter describes the fundamental ideas introduced in Time Warp and the associated algorithms for its efficient implementation.The term optimistic execution refers to the fact that logical processes processevents, optimistically assuming there are no causality errors. Optimistic executionhas long been used in other computing applications, in some cases prior to it beingproposed for simulation. Two other uses include Distributed database systems. When several clients of a database systemsimultaneously initiate transactions on the database for example, to updatean inventory of spare parts, a concurrency control algorithm is required toensure that the final result is the same as if the transactions were performed insome serial order. One approach to accomplishing this is to have transactionsoptimistically proceed as if no conflicts read and write accesses to the samerecord occur. If a conflict is later detected, one of the transactions is aborted,and restarted later. Database concurrency control is in many ways similar tosynchronization of parallel simulations, with the important distinction that anyordering of transactions in database systems is usually acceptable. Thesesystems need only to ensure that transactions appear to be atomic, namelythat the result after concurrently processing the transactions is the same as ifeach were completed in sequence, one after the other. In parallel simulationscausal relationships in the physical system dictate the order in which eventsmust be completed, so it is definitely not the case that any order will do. Microprocessors. Most modem microprocessors use optimistic processingbecause of an implementation technique called pipelining. With pipelining,the microprocessor begins processing the next instruction specifically, fetching the next instruction from memory before completing the current instruc98 TIME WARPtion. This is problematic if the current instruction is a conditional branch, sinceit is not known which instruction should be fetched next until the currentinstruction has almost completed execution. Many modem microprocessorspredict the result of the branch instruction branch taken or not taken andoptimistically begin executing instructions according to this prediction. Ofcourse, if the prediction proves to be incorrect, the CPU must have some way toback out of the incorrect sequence of instructions that it began to execute.4.2 LOCAL CONTROL MECHANISMD processed event unprocessed eventInput QueueFigure 4.1 Events in Time Warp logical process when a straggler message arrives.99Returning to parallel simulation applications, recall that the central problem facedby a logical process is that it may have one or more messages that it has receivedfrom other processes but cannot be sure that it is safe to process these events becausedoing so might later result in violating the local causality constraint. Optimisticsynchronization protocols process events even though their safety cannot beguaranteed, and they provide some mechanism to back out these computationsshould a causality error out of order event processing be detected.4.1 PRELIMINARIESLike the conservative synchronization algorithms, the simulation computation isagain assumed to be composed of a collection of logical processes that communicates exclusively by exchanging timestamped messages. There are no statei st sd eelciss.siclr1g6 , eventuaJly arrives aithe receiver. A logical process neesend messages in timestamp order, and the communication network neearantee that messages aredelivered in the same order in which they were sent.Further, logical processes maybe created or destroyed during the execution, and there is no need to explicitlyspecifY which LPs communicate with which other LPs.Initially, to simplifY the discussion, it is assumed that the simulation has nonzerolookahead namely an LP at simulation time T can only schedule events with timestamp strictly greater than T. It will be seen later that with some simple precautions,zero lookahead simulations can be allowed.The Time Warp algorithm consists of two distinct pieces that are sometime calledthe local control and the global control mechanisms. The local control mechanism isimplemented within each processor, largely independent of the other processors. Theglobal control mechanism is used to commit operations such as IO that cannot berolled back and to reclaim memory resources it requires a distributed computationinvolving all of the processors in the system.4.2 LOCAL CONTROL MECHANISMThe behavior of each logical process in a Time Warp system can be described inrelation to a sequential simulation. Recall that a sequential simulator contains a datastructure called the event list that includes all events that have been scheduled buthave not yet been processed. A Time Warp logical process TWLP can be viewed inexactly the same way, except for two differences The events in the TWLPs event set may result from messages sent to this LPfrom other LPs. The TWLP does not discard events after processing them but rather keeps theprocessed events in a queue. This is necessary because the TWLP may rollback, in which case previously processed events will have to be reprocessed.A TWLP showing both the processed and unprocessed events is depicted inFigure 4.1. Each event processed by the TWLP is placed at the end of theprocessed part of the event list. Because events are processed in time stamporder, events in this list will be sorted by time stamp. The white events in Figure 4.1with time stamps 12, 21, and 35 have already been processed by the LP, and theblack event with time stamp 41 has not yet been processed. The TWLPs clock is atsimulation time 35 in this snapshot, and is about to advance to 41.As in a sequential simulator, the TWLP will repeatedly remove the smallest timestamped unprocessed event from the event list and process that event. Unlike aconservative simulation executive that must first verifY that an event is safe, TWLPmakes no such check, and blindly optimistically goes ahead and processes the nextevent. This means that the TWLP may later receive a message in its past, that is, amessage with time stamp smaller than the clock value of the TWLP. lO Such latearriving messages are referred to as sp,.,ames.ags. Figure 4.1 shows a stragglermessage arriving at a TWLP, with time stamp18tndicating that a violation of thelocal causality constraint has occurred.Events with a time stamp larger than the straggler were processed incorrectlybecause the state of the LP did not take into account the processing of the stragglerevent. Thus in Figure 4.1 the events with time stamps 21 and 35 were processedincorrectly. Time Warp addresses this problem by rolling back or undoing thecomputations for the time stamp 21 and 35 events, and then reprocessing theseevents the straggler at time 18, and the rolled back events at times 21 and 35 intime stamp order.10 We assume for now that all events have unique time stamps. This issue will be discussed later.100 TIME WARP4.2 LOCAL CONTROL MECHANISM 101The central question that must be answered is how does one undo an eventcomputation Consider what an event computation can do. It may do one or both ofthe following1. Modify one or more state variables.2. Schedule new events, namely send messages to other logical processes.Thus a mechanism is required to undo changes to state variables and to unsendpreviously sent messages. Each of these is described next.4.2.1 Rolling Back State VariablesThere are two widely used techniques to undoing modifications to state variables thatare commonly used in Time Warp systemsIInput QueueState QueueaD processed event unprocessed eventD snapshot of LP stateill5 state of LP after   Y 2 processing EZ 9Figure 4.2 Undoing modifications to state variables. a Copy state method b incrementalstate saving.If most of the state variables are modified in each event, copy state saving is moreefficient because one does not need to save the addresses of individual variables, andmany machines are optimized to efficiently copy large blocks of data. On the otherhand, if only a small portion of the state vector is modified on each eventincremental state saving will be more efficient because it only saves those statvariables that were actually modified. These two techniques are not mutuallyexclusive. Some Time Warp systems use copy state saving for variables that aremodified frequently, and incremental state saving for other variables that aremodified infrequently, with the choice of statesaving technique for each variabletypically left to the application programmer.Whether copy state saving or incremental state saving is used, some amount ofstate information is associated with each event. The collection of state informationfor the events within an LP is referred to as the state queue. The events themselves,Dprocessed eventunprocessed eventDLP state logb1state ofL illafter rollback Y 2z 3Input QueueState Queue1. Copy state saving. The TWLP makes a copy of all of the modifiable statevariables within the LP. Typically a copy is made prior to processing eachevent, but as will be discussed later, the copying could be performed lessfrequently. Figure 4.2a depicts the statesaving and restoration process forthe rollback scenario shown in Figure 4.1. The LPs state in this exampleincludes three variables, X, Y, and Z that hold the values 1, 2, and 3,respectively, after processing E l2 the event with time stamp 12. Event E2lwrites a new value, 4, into X, and E35 writes 5 into X and 9 into Z. As shownin Figure 4.2a, all three state variables are copied into memory associatedwith an event just before that event is processed. When the straggler messagewith time stamp 18 arrives, the LPs state is restored to the state of the LP attime 18, the state which existed prior to processing E2l . The rollback causesthe snapshot associated with E21 to be copied back into the LPs statevariables, effectively undoing the modifications to the state variables madeby E21 and E35 .2. Incremental state saving. A log recording changes to individual state variablesis kept for each event computation. Each record in the log indicates a theaddress of the state variable that was modified, and b the value of the statevariable prior to the modification. To roll back modifications to state variablesperformed by an event, the Time Warp executive scans the events being rolledback in the order of decreasing time stamps. For each event the Time Warpexecutive goes through that events log, last record to first, and copies the valuesaved in the log entry to the corresponding state variable. For example, therollback scenario shown in Figure 4.1 using incremental state saving is shownin Figure 4.2b. The log for E21 contains the record adrX, 1 where adrXindicates the address of variable X, and the log for E35 contains the recordsadrX , 4 and adrZ, 3. As shown in Figure 4.2b, rolling back theseevents will restore 1 Z to 3, 2 X to 4, and 3 X to 1, yielding the originalvalues that existed prior to processing E21 102 TIME WARP4.2 LOCAL CONTROL MECHANISM 103o processed event unprocessed eventD saved stateII antimessageD processed event unprocessed eventD saved state111 antimessage4. send antimessage1. straggler message arrives in the past, causing rollback2. restore state of LP to that prior to processing time stamp 21 event3. roll back events at times 21 and 35BEFOREOutput QueueantimessagesAFTEROutput Queueantimessages1. antimessage arrives, annihilatemessage and antimessageOutput QueueantimessagesBEFOREcanceled positive message see Fig. 4.5. Once the rollback has beenperformed, the message and antimessage pair can be annihilated, just as incase 1. This rollback caused by receiving an antimessage in the TWLPs pastis referred to as a secondary rollback the initial rollback resulting from theoriginal straggler message is called a primary rollback. The secondaryFigure 4.4 Receiving an antimessage, positive message has not yet been processed. TopMessageantimessage pair annihilated bottom final state of queues after annihilation.Figure 4.3 Rollback caused by receiving a straggler message. Top Actions performed toroll back the LP bottom final state of the LP after the rollback.4.2.2 Unsending MessagesAt first glance, undoing a message send seems rather complex. This is because anincorrect message may have already been processed by another LP, resulting in thegeneration of additional incorrect messages that have been processed by still otherLPs, which results in still other incorrect messages, and so on. The incorrectcomputation could have spread throughout the entire simulation, requiring amechanism to undo all of the effects of these computationsPerhaps the most elegant aspect of the Time Warp mechanism is a simplemechanism called antimessages used to undo message sends. The name antimessage comes from an analogy with particle physics concerning matter and antimatter. In physics, when an atom of matter comes in contact with an atom of antimatter, the two annihilate each other and disappear. Similarly, for each message sentby a TWLP, an antimessage is created. The antimessage is logically an identicalcopy of the original positive message, except it contains a flag identifying it as anantimessage. When a message and its corresponding antimessage are both stored inthe same queue, they are both deleted and their storage is reclaimed. The process ofcombining messageantimessage pairs is referred to as message annihilation.To unsend a previously sent message, an LP need only send the matching antimessage to the same TWLP to which the original message was sent. This implies theTWLP must keep a log of what messages it has sent, so it can later send antimessages, if necessary. Each TWLP defines a data structure called the output queuefor this purpose. Whenever a message is sent, the corresponding antimessage is leftin the senders output queue. If an event is rolled back, any antimessages stored inthe output queue for that event are sent.The example shown in Figure 4.1 is continued in Figure 4.3 to show theoperations that take place when a rollback occurs. The straggler message causesE21and E35to be rolled back. The state of the LP is restored using the informationstored in the state queue, and the memory used for this saved state information cannow be reclaimed. E2land E35 are marked as unprocessed events. In this example,E35generated one message, E42 , so the antimessage for E42 is removed from theoutput queue and sent. The final state of the LP after the rollback has been processedis shown in Figure 4.3b.Now consider what happens when a logical process, LPA receives an antimessage. There are three cases to consider1. The corresponding positive message has not yet been processed. In this casethe message and antimessage pair can both be annihilated, and removed fromLPAs input queue, and the storage used for these messages can be reclaimedsee Fig. 4.4.2. The corresponding positive message has already been processed. In this caseLPA is rolled back to the point just prior to processing the abouttobeboth the processed and unprocessed events, are stored in a data structure called theinput queue.Figure 4.5 Receiving an antimessage, positive message has already been processed. TopRollback action prior to processing message, annihilated messageantimessage pair bottomfinal state of queues after annihilation.105.slate dependenceD processed event unprocessed evento saved stateII antimessageo not dependent on E dependent on E..scheduling dependence4.2 LOCAL CONTROL MECHANISM1. antimessage arrives, place in input queueno rollback2. positive message arrive, annihilate message and antimessageOutput QueueantimessagesBEFOREOutput QueueantimessagesLATERconnected by dashed arcs indicating dependencies between events due to accesses tocommon state variables it is assumed successive events within the same LP alwaysdepend on each other in this way. An event E2 is dependent on another event E1 ifthere is a path of arcs using either state dependence andor scheduling dependencearcs from E1 to E2 When an event E is rolled back, all events that depend on E areeither rolled back if they have been processed or canceled via the antimessagemechanism.Assuming nonzero lookahead, scheduling arcs must always move from left toright in the spacetime diagram. Similarly state dependence arcs also move from leftFigure 4.6 Receiving an antimessage when the positive message has not yet been received.Top Place antimessage in input queue no rollback occurs bottom when a positivemessage arrives, annihilate messageantimessage pair.simulation timeFigure 4.7 Spacetime diagram depicting a snapshot of a Time Warp execution anddependencies among events.D processed eventIII unprocessed eventJ saved state antimessage1. antimessage arrives, roll back E42 and E453. annihilate message and antimessageOutput QueueantimessagesAFTEROutput QueueantimessagesBEFORETIME WARProllback may generate additional antimessages, which may in tum causefurther rollbacks and antimessages in other logical processes. Recursivelyapplying this roll back, send antimessage procedure will eventually eraseall incorrect computations resulting from the original, incorrect message send.3. The corresponding positive message has not yet been received by LPA Thiscould occur if the communication network does not guarantee that messagesare received in the same order that they were sent in this case the antimessagecould reach LPA before the original positive message. If this occurs, the antimessage is placed in the input queue. When the corresponding positiv.emessage is received, it will also be placed in the same queue as the antlmessage, so the two will annihilate each other. This scenario is depicted inFigure 4.6. No rollbacks occur in this scenario, even if both the antimessageand positive message are in the past of the receiver.The mechanism described above enables the Time Warp mechanism to recover fromincorrect message sends.It is instructive to view a snapshot of the Time Warp execution as a spacetimegraph such as that shown in Figure 4.7. The boxes represent event computations, andthe X Y coordinates of each box indicate the events time stamp and the TWLP thatprocesses the event, respectively. Solid arcs represent event scheduling messagesending relationships that is, an arc from event 1 to 2 denotes the fact that thecomputation for 1 scheduled event 2 Successive events within an LP are104106 TIME WARPto right. II This implies that the graph represented in the spacetime diagram isacyclic, and traversing the dependence arcs always moves one from left to right inthe graph.One can observe the following properties of the Time Warp execution from thespacetime diagram1. Rollbacks always propagate into the simulated time future. Because rollbacksspread along the scheduling and state dependence arcs in the graph, therollback always spreads from left to right in the graph. Viewed another way, ifa TWLP rolls back to simulated time T, all antimessages sent as a result ofthis roll back must have a time stamp strictly larger than T recall the nonzerolookahead assumption. Thus any rollbacks caused by these antimessages willbe to a simulation time greater than T. Subsequent rollbacks resulting fromantimessages generated by this secondary rollback must similarly roll backother TWLPs to successively larger simulated times. This property isimportant because it shows that one cannot have a domino effect where aninitial roll back causes the entire computation to eventually be rolled back tothe beginning of the simulation.2. At any instant during the execution, the computation associated with thesmallest timestamped message or antimessage in the system that has not yetbeen completed will not be later rolled back. The computation associated withan antimessage is the annihilation of the messageantimessage pair. If thereis more than one computation containing the smallest time stamp, the abovestatement applies to at least one of these computations. Intuitively the smallesttimestamped computation cannot be rolled back because rollbacks propagatefrom left to right in the spacetime graph, and there is no computation to theleft of the leftmost uncompleted computation in the graph. Thus there isnothing that can cause this computation to be rolled back. This property isimportant because it shows that so long as the Time Warp system has ascheduling mechanism that eventually executes the lowest timestampedcomputation, the execution always make forward progress, so no deadlocksituations can occur.4.2.3 Zero Lookahead, Simultaneous Events, and RepeatabilityConsider two simultaneous events events containing the same time stamp within asingle TWLP. If one event has been processed when a second containing the sametime stamp arrives, should the processed event be rolled back and reexecuted Aswill be seen momentarily, if an event rolls back other events containing the sametime stamp and zero lookahead is allowed, the simulation may fail.Suppose that zero lookahead is allowed, and it is designated that a straggler doesroll back other already processed events containing the same time stamp. ConsiderII In the case of events containing the same time stamp simultaneous events, the LP places someorip,na f thp. vnt. which is used to preserve the lefttoright nature of the state dependence arcs.4.2 LOCAL CONTROL MECHANISM 107simulation timeFigure 4.8 Cyclic dependence among three events containing the same time stamp.the scenario shown in Figure 4.8. Event Ex at LPA schedules Ey at LPB , which intum schedules Ez back to LPA. Suppose that all three events contain the same timestmp. Ez will roll back Ex, since they have the same time stamp. Rolling back EWill cancel Ey, hich ill in tum cancel Ez . When Ex is reexecuted, it will agaigenerate Ey, which Will generate Ez , this will again roll back E causing the11 . xcance atlOn of Ey and Ez, and so on. The simulation will repeat this cycleindefinitely.Te unending rollback cycle occurs because there is a cycle in the dependencearcs m the space time diagram. Rolling back Ex when Ez arrives implies that E isstate dependent on Ez . This means there is a cycle from E to E to E xviascheduling dependence arcs see Fig. 4.8, and then from Ez bak to E via aZstatedependence arc. Dependence cycles such as this must be eliminated to avoidunending rollback scenarios such as that described above.One way to address this problem is to simply designate that a straggler messagedoes not roll back other already processed events containing the same time stampTh 12. .I prevents rollback cycles. In effect, Simultaneous events are delivered to theLP m he order that they were received. The disadvantage of this approach is that theexeutlOn may not be repeatable, since the events may arrive in a different orderdnng the next execution. To ensure repeatability, the TWLP must first collect allslmltaneous events, and then order them itself in a repeatable fashion, such as bysortmg on some characteristic of the event.Anther approach is to allow an event to roll back some events containing theae Ime stamp but to ensure that an event never rolls back another event on whichIt IS. either scheduling dependent or indirectly scheduling dependent. An event E. isdlrect, scheduling dependent on another event E if there is a path of r1nlyhedulmg depedec arcs from E to Ej . This approach breaks the cycle in Figure.8 because Ez IS mdlrctly scheduling dependent on Ex, so using this rule, Ezannot roll back Ex ThiS approach can be realized by extending the time stamp of12 I ght rm t be noted that receiving an antimessage at ime T could still cause another message in the sameLP With time stamp T to be rolled back however, because rollback cycles must begin with a stragglermessage that IS regenerated wlthm the rollback cycle, this does not lead to an unending cycle.108 TIME WARP 4.3 GLOBAL CONTROL MECHANISM 109the event with an additional age field, like that described in Chapter 3, where the agewas used to order simultaneous events. Like the time stamp field, the larger the agefield, the later is the event. Thus the age can be viewed as an extension of the timestamp field that provides additional, lower precision digits. Recall that the age isassigned as follows Suppose that an event Ex with time stamp Tx contains an ageAx, and Ex schedules another event Ey with time stamp Ty and age Ay. If Ty  Tx ,then Ay is 1. If Ty  Tx then Ay is Ax  1. Thus, if one considers the tree formedwith simultaneous events as nodes and direct scheduling dependencies as arcs, theage indicates the level of the event in the tree.In Figure 4.8, Ex, Ey, and Ez would be assigned ages 1,2, and 3, respectively.Because Ez has a larger later age than Ex Ez does not cause Ex to be rolled back.Note that there may be two or more events containing both the same time stamp andage for example, if Ex also scheduled another event Ew with time stamp Tx , thenboth Ew and Ey would have the same time stamp and age. An additional field isrequired if unique time stamps are required, for example, to guarantee repeatabilityof the execution. In the TWOS Time Warp Operating System developed at the JetPropulsion Laboratory, the body of the message itself was used as the identifier theonly ties that can occur are when the messages themselves are identical, in whichcase, the order doesnt matter. Another approach, discussed in Chapter 3, is to usethe tuple LPs, I where LPs identifies the logical process sending the message and Iis a sequence number indicating the number of events scheduled by LPs, excludingmessage sends that have been rolled back i.e., canceled by sending the corresponding antimessage.Using the approach with an age field, but no additional fields to specify anordering among events with the same time stamp and age, one can still specify thatan event rolls back a processed simultaneous event with the same or greater age.This allows the application to explicitly order simultaneous events with the sametime stamp and age according to its own criteria. The system will automaticallyguarantee events that are directly or indirectly scheduling dependent on this eventwill not result in unending rollback cycles.To summarize, these techniques enable the Time Warp system to schedule zerolookahead events. Unending rollback cycles can be prevented by either specifyingthat a straggler message does not roll back events containing the same time stamp orby using an age field. By itself, neither of these techniques guarantees repeatableexecutions. Repeatability can be obtained through the use of an additional, unique,identifier field in the time stamp, or by requiring the application to collect and ordersimultaneous events in a repeatable fashion.4.3 GLOBAL CONTROL MECHANISMThe local control mechanism described above is sufficient to ensure that theexecution yields the same results as if the simulation were executed sequentiallyand all events were processed in time stamp order. However, two problems must beaddressed before this can be regarded as a viable mechanism1. The computation consumes more and more memory throughout the executionvia the creation of new events, but it seldom releases memory A mechanismis required to reclaim memory resources used for processed events, antimessages, and state history information that is no longer needed. Reclamationof memory holding history information is referred to as fossil collection.2. Certain operations performed by the simulation computation cannot be rolledback. Specifically, once IO is performed, it cannot be easily undone.Both of these problems can be solved if one can guarantee that certain events are nolonger prone to roll back. For example, if one could guarantee that no roll back willoccur to a simulated time earlier than T, the history information for events with timestamps smaller than T could be reclaimed. Similarly IO operations generated byany event with a time stamp less than T could be performed without fear of theoperation later being rolled back. Thus the solution to both of the problems citedabove is to determine a lower bound on the time stamp of any future rollback. Thislower bound is referred to as Global Virtual Time GVT.From the local control mechanism it becomes immediately apparent that a TWLPonly rolls back as the result of receiving a message, which is either a positivemessage or an antimessage, in the TWLPs past. Observe that positive messages canonly be created by unprocessed or partially processed events. Therefore, if onecould capture a snapshot of the Time Warp system, the minimum time stamp amongall antimessages, positive messages, and unprocessed and partially processed eventsIII the system represents a lower bound on the time stamp of any future rollback.Observe that a positive message in transit to a TWLP holds an unprocessed event.Therefore Global Virtual Time is defined as followsDefinition Global Virtual Time at wallclock time T GVTT during the executionof a Time Warp simulation is defined as the minimum time stamp among allunprocessed and partially processed messages and antimessages in the system atwallclock time T.Figures 4.2, 4.3, and 4.5 illustrate that if the TWLP is rolled back to time T onlyfu h . . . ,e IstOry mformatlOn specIfically state information and antimessages for theevnts that are rolled back is needed. Therefore memory for events with time stampstnctly less than GVT and the antimessages and state information associated withthese events can be reclaimed. Memory associated with events with time stamp equalto the GVT value cannot be reclaimed, however, even if the Time Warp system isdfine.d so that a straggler message with time stamp T does not roll back other eventswIth tle stamp equal to T. This is because GVT could be equal to the time stamp ofan antimessage that has not been processed i.e., annihilated, so such an antimessage could require one to roll back events with time stamp exactly equal to GVTsee Fig. 4.9.110 TIME WARP 4.3 GLOBAL CONTROL MECHANISM 111D processed event unprocessed eventD saved stateantimessageOutput QueueantimessagesFigure 4.9 Example where the information in an event with time stamp equal to GVT isneeded. Here, GVT is 42, and there are two processed events with time stamp 42. In the first,the TWLP processed is canceled by an antimessage with time stamp equal to GVT.4.3.1 Fossil CollectionMost Time Warp systems compute GVT periodically for example, every fewseconds, or when the simulation runs out of memory. There are two commonapproaches to reclaiming memory Batch fossil collection. When fossil collection is performed, the Time Warpexecutive in each processor scans through the event lists of all of the TWLPsmapped to that processor, and reclaims the memory used by events andassociated state information and antimessages. Onthefly fossil collection. When GVT is computed, the system does notimmediately reclaim memory. Instead, processed events are placed into a list ofevents, typically managed as a FIFO queue. When memory for a new event isrequired, the Time Warp executive allocates memory from this list but onlyreallocates the storage for an event if the time stamp of that event is less thanGVT. This approach avoids a possible timeconsuming search through theevent lists of the TWLPs in order to perform fossil collection.4.3.2 Error HandlingUnlike conservative simulation systems, if a simulation program executing on aTime Warp executive produces an error for example, performs a divide by zerooperation, the program cannot simply be aborted. This is because the error maylater be erased by a rollback operation. When an error occurs, it is flagged, and theTWLP is blocked so that it will not process any additional events. If the error is noterased by a rollback, it is committed when GVT advances past the simulation time atwhich the error occurred. The program can then be aborted and the user can benotified of the error.The tricky aspect of error handling is to ensure that the error itself can be erased.Different countermeasures are required for different types of errors. Below weenumerate some possible errors and how they can be handled Program detected errors. These are errors in logic and inconsistencies detectedby the simulation program itself. For instance, it may be that a certain statevariable should never take on a negative value for example, the number ofaircraft waiting to land at an airport. As a precautionary measure the programmight periodically test this variable to make sure that it has not been assigned anegative value, as a check for a programming bug that could cause thesimulation to reach an incorrect state. This is the most straightforward typeof error to address. The Time Warp executive can provide an abort primitivethat marks the logical process as being in an error state so that no more eventsare processed by that TWLP. As described earlier, if the error is erased by arollback, the process can be returned to the normal state. If the error iscommitted, the program is then aborted. Infinite loops. If the program enters an infinite loop, a mechanism is required tobreak out of the loop if a message is received that would cause the eventcontaining the infinite loop to be rolled back. This can be accomplished byusing an interrupt mechanism to handle incoming messages. Alternatively, acall into the Time Warp executive might be placed in each loop of theapplication program that could become an infinite loop. This call mustcheck to see if any messages were received that would roll back the eventthat is now being processed. If such an event is detected, the processing of thecurrent event should be aborted, and the rollback processed. Benign errors. These are errors that do not result in the incorrect modificationof any memory other than checkpointed state variables. Examples includearithmetic errors such as dividing by zero, taking the square root of a negativenumber, or an illegal memory reference for example, to a location within theoperating system that is aborted by the operating systems memory protectionmechanisms. A mechanism is required to pass control to the Time Warpexecutive when such errors occur. Support must be provided by the operatingsystem or the languages runtime system. For example, the Ada languageprovides a mechanism to execute user or in this case, Time Warp executivecode when such an error occurs. Similarly programs written for the Unixoperating system can specify code that is executed when a signal is raised,denoting such exceptions. Once the error has been detected and control ispassed to the Time Warp executive, the error can be handled in the same way asa programdetected error. Destructive errors. Destructive errors are those that result in the modificationof state that is not checkpointed for example, an errant pointer reference orrunning off the end of an array could modify internal data structures within the4.4 COMPUTING GLOBAL VIRTUAL TIMEGVT computation is a close cousin to the lower bound on time stamp LBTScomputation discussed in Chapter 3 for conservative synchronization. GVTcomputes a lower bound on the simulation time of any future rollback. But asnoted earlier, rollback only occurs when an LP receives a message or antimessage inits simulation time past, so the value relevant to each LP is a lower bound on thetime stamp of messagesantimessages that may arrive in the future. This isessentially the same as the LBTS value computed by conservative algorithms.Differences between LBTS and GVT computations stem largely from 1different underlying assumptions concerning topology among logical processesand lookahead, and 2 different performance requirements. Regarding the former,Time Warp systems usually assume that any LP can send messages to any other LPand that there is zero lookahead. Conservative algorithms usually make morestringent assumptions regarding topology and lookahead that they can then exploitto minimize blocking among LPs. An LBTS computation where one assumes a fullyconnected topology and zero lookahead is essentially the same as a GVT computation.With respect to different performance requirements, LBTS in conservativesystems must be computed often, and very rapidly, because processors may beidle and waiting for it to complete. This is typically not the case in optimisticsystems where processors optimistically process events and need not wait for theGVT computation to complete, so long as they do not run out of memory or need tocommit IO operations very quickly. Thus simpler, but perhaps higherlatencyalgorithms may be adequate, or even preferred in optimistic systems. Here, we focuson algorithms developed specifically for computing GVT. These same algorithmscan, in principle, be used to compute LBTS, and conversely, the LBTS algorithms113Messages ... ComputeLocalMinimum  ... value of local minimum. transient message4.4 COMPUTING GLOBAL VIRTUAL TIMEwallclock timediscussed in the previous chapter can be used to compute GVT, provided that oneadheres to their underlying assumptions.From the definition of GVT, it is clear that if one could capture in a snapshot allunprocessed and partially processed events and antimessages in the system atwallclock time T, computing GVTT would be trivial. There are two challengingproblems associated with making such a snapshot. They are referred to as thetransient message problem and the simultaneous reporting problem. Each of these isdescribed next, followed by a discussion of algorithms for efficiently computingGVT.4.4.1 Transient Message ProblemFigure 4.10 Transient message problem. Processors PA and PH compute their localminimums to be 15 and 20, respectively. If the transient message is not considered, GVTwill be incorrectly computed as 15, when it should be 10.Suppose that one could instantaneously freeze all processors in the system, haveeach report its local minimum among the unprocessed events and antimessageswithin that processor, and then compute a global minimum from these values. Thisalgorithm would not compute a correct value for GVT. The reason is that while all ofthe processors are frozen, there may be one or more messages in the network,namely messages that have been sent by one processor but have not yet beenreceived at its final destination. As shown in Figure 4.10, such messages may resultin rollbacks. Thus it is clear that these socalled transient messages must be includedin the GVT computation, or an error results. This problem is referred to as thetransient message problem.There are essentially two approaches to solving the transient message problem1 Have the sender take into account the time stamp of transient messages when itreports its local minimum, or 2 have the receiver to take into account the timestamp of transient messages when they arrive. The latter approach requires one toprovide a mechanism to determine when all relevant transient messages have beenreceived. An approach using message counters will be described later. We firstdiscuss solutions using the former approach.One simple solution to the transient message problem is to use messageacknowledgments. The key idea is to ensure that every transient message isaccounted for by at least one processor when GVT is being computed. It isTime Warp executive. Because the Time Warp executive is usually viewed bythe operating system as part of the application program, such an error does nottypically result in a trap, as would occur if say the program attempted tooverwrite a memory location within the operating system. These are the mostproblematic types of errors because once the state has been erroneouslymodified, it is difficult to recover from such an error. A brute force solutionis to checkpoint all memory that can be modified by the application withoutcausing a trap in the operating system, such as all state used by the Time Warpexecutive. Another approach is to provide explicit runtime checks for sucherrors for example, array bounds checks or adding code to check that memorywrites only modify checkpointed state, effectively converting the error to aprogram detected error. Such checks are only required on references usingmemory addresses that are computed at runtime. Minimizing use of computedaddresses for example, not performing arithmetic on pointers to computeaddresses reduces the likelihood of destructive errors.TIME WARP112114 TIME WARP 4.4 COMPUTING GLOBAL VIRTUAL TIME 115Iacceptable for more than one processor to account for a single message because thiswould not affect the global minimum computation. If an acknowledgment is sent forevery message, the sender of each message is responsible for accounting for themessage in its local minimum until the acknowledgment is received. The receivertakes responsibility for the message as soon as it receives the message. Thishandshake between the sender and receiver ensures that no transient messagesfall between the cracks during the GVT computation.A simple, synchronous GVT algorithm using acknowledgments can now bedescribed. This algorithm uses a central controller to initiate the GVT computation,m bgkonOd GVT to the othe proe, I1. The controller broadcasts a StartGVT message, instructing each processorin the system to initiate a GVT computation.2. Upon receiving the StartGVT message, each processor stops processingevents, and issues a ReceivedStart message to the controller. The processorblocks until receiving another message from the controller.3. When the controller has received a ReceivedStart message from everyprocessor, it broadcasts a ComputeLocalMinimum message.4. Upon receiving the ComputeLocalMinimum message, each processorcomputes the minimum time stamp among a the unprocessed events andantimessages within that processor and b the minimum time stamp of anymessage the processor has sent but has not yet received an acknowledgment.This minimum value is sent to the controller.5. When the controller has received the local minimum from each processor, itcomputes a global minimum and broadcasts this value to each processor.Using this approach, the scenario in Figure 4.1 0 will result in PB reporting 10instead of 15 as its local minimum because it will not have received an acknowledgment for the time stamp 10 message it has sent. Thus the controller will correctlycompute the GVT to be the value 10.4.4.2 Simultaneous Reporting ProblemAn important drawback to the synchronous GVT computation described in theprevious section is the need to block every processor in the system see step 2 in theinterval between receiving the StartGVT message and the ComputeLocalMinimum message. Some processors may be delayed in responding to the StartGVT message because they were processing an event when the message arrived,potentially delaying all processors in the system, since the controller must receive aresponse from all before issuing the ComputeLocalMinimum message. 13 A betterapproach would be to allow processors to continue processing events while the GVT13 This could be circumvented by using an interrupt mechanism, however.computation is in progress, that is, to use an asynchronous computation that does notrequire global synchronization points.One might suggest a simple asynchronous GVT algorithm where the controllersimply broadcasts a ComputeLocalMinimum request to all of the processors, andthen simply collects these values and computes a global minimum. Processors mayprocess events asynchronously while the GVT computation is in progress. Thissimple approach does not work, however, because it introduces a problem known asthe simultaneous reporting problem. Intuitively this problem arises because not allprocessors will report their local minimum at precisely the same instant in wallclocktime. This can result in one or more messages slipping between the cracks that is,one or more unprocessed messages may not be accounted for by either the processorsending or receiving the message.A scenario illustrating the simultaneous reporting problem is depicted in Figure4.11. The controller broadcasts the ComputeLocalMinimum message. ProcessorPA receives this message and reports its local minimum is 35. The ComputeLocalMinimum message that is sent to processor PB is delayed in the communicationsnetwork, however, and does not arrive until some time later. In the mean time, PBsends a message to PA with time stamp 30 and then moves on to begin processinganother event with time stamp 40. At this point P B receives the ComputeLocalMinimum message and reports its local minimum is 40, the time stamp of its nextlocal event. The controller computes an incorrect GVT value of 35 minimum of 35and 40 because it failed to take into account the time stamp 30 message. Thescenario shown in Figure 4.11 could still occur even if message acknowledgmentswere used.The basic problem is that accounting for all unprocessed messages in the systembecomes more complicated if processors are allowed to process events and generatenew ones while the GVT computation is in progress. In Figure 4.11, PA cannot takeinto account the time stamp 30 message in its local minimum computation because ithad not even received the event when its local minimum was computed. PB does nottake this event into account in its local minimum computation because it believesthat the message is PAs responsibility, since the message was generated andacknowledged if message acknowledgments are used well before it computed itsMessages .. ComputeLocalMinimum ... value of local minimum.message containing an eventwallclock timeFigure 4.11 Simultaneous reporting problem. The time stamp 30 message is not accountedfor, resulting in an incorrect GVT value of 35 being computed.local minimum. Thus the time stamp 30 message is not accounted for by eitherprocessor.1. The controller broadcasts a ComputeLocalMinimum message to all processors to initiate the GVT computation.2. Upon receiving the ComputeLocalMinimum message, the processor sendsthe controller a message indicating the minimum time stamp among allunprocessed events within the processor, all unacknowledged message.s andantimessages it has sent, and all marked acknowledgment messages It hasreceived since it last received a new GVT value. The processor now sets a flag,indicating it is infind mode.3. For each message or antimessage received by the processor while it is in findmode, the processor sends a marked acknowledgment message indicating thetime stamp of the message it is acknowledging. An unmarked acknowledgment message is sent for all messages received while not in find mode.4. When the controller receives a local minimum value from every processor inthe system, it computes the minimum of all these values as the new GVT andbroadcasts the new GVT to all processors in the system.1174.4 COMPUTING GLOBAL VIRTUAL TIME5. Upon receiving the new GVT value, each processor changes its status so that itis no longer in find mode.It can be shown that this algorithm computes a new value of GVT that is no largerthan the true GVT value at the instant the controller broadcast the ComputeLocalMinimum message to initiate the GVT computation.4.4.4 Matterns GVT AlgorithmOne drawback with Samadis algorithm is that it requires an acknowledgmentmessage to be sent for each message and antimessage. The underlying communications software may automatically send acknowledgments for messages in orderto implement the reliable message delivery service being used by the Time Warpexecutive however, such acknowledgments are typically not visible to the TimeWarp executive. Instead, a separate applicationlevel acknowledgment message mustbe sent to implement Samadis algorithm.Like Samadis algorithm, Matterns GVT algorithm is also asynchronous. That isit avoids global synchronizations, but it does not require message acknowledgments.The basic idea in the algorithm is to cleanly divide the distributed computation by acut that separates it into a past and future. As shown in Figure 4.12, eachprocessor defines a point in its execution called a cut point, with all actionscomputations, message sends, and message receives before a processors cutpoint in wallclock time referred to as being in that processors past, and all actionsoccurring after the cut point referred to as occurring in that processors future. Theset of cut points across all of the processors defines a cut of the distributedcomputation. A consistent cut is defined as a cut where there is no message thatwas sent in the future of the sending processor and received in the past of thereceiving processor. Graphically, if each message is represented as an arrow, thismeans there is no message arrow extending from the future part of the graph to thepast part. Figure 4.12a shows a consistent cut and Figure 4.l2b shows aninconsistent cut. The latter cut is inconsistent because of the message sent from Pcsfuture to PDs past.A snapshot taken along a consistent cut includes the local state of each processorat its cut point and all transient messages crossing the cutnamely all messages sentin the past part and received in the future part of the computation. A key observationis that the snapshot of the computation taken along a consistent cut can be used tocompute GVT. To see this, consider the execution of the synchronous GVTalgorithm discussed earlier. Specifically, let each processors cut point in Figure4.12a represent the wallclock time at which it received a ComputeLocalMinimum message. Recall that in the synchronous GVT algorithm, this messagecauses the processor to freeze, that is, not perform any new computations or sendor receive any additional messages. The processors then compute GVT based on acut defined at one instant in wallclock time after all the processors have been frozensee Figure 4.12c. It is clear the synchronous GVT algorithm correctly computesGVTT. where T is the wallclock time of the cut points in Figure 4.12c.TIME WARP1164.4.3 Samadis GVT AlgorithmThis algorithm assumes that message acknowledgments are sent o.n every messgeand antimessage sent between processors. As discussed earher, the sendmgprocessor is responsible for accounting for each message it has sent until it receivesthe acknowledgment, thereby solving the transient message problem.The simultaneous reporting problem is solved by having processors tag anyacknowledgment messages that it sends in the period starting from when theprocessor reported its local minimum until it receives the new GVT value. Thisidentifies messages that might slip between the cracks and notifies the sender thatit the sender is responsible for accounting for the message in its local minimumcomputation. Thus, in Figure 4.11, PA will tag the acknowledgment it sends for thetime stamp 30 message it received from PB PB includes in its local minimumcomputation 1 the minimum time stamp among the unprocessed message or antimessage stored within the processor, 2 the minimum time stamp among .themessages it has sent for which it has not yet received an acknowledgment I.e.,transient messages, and 3 the minimum time stamp among the tagged acknowledgment messages the processors received since the last GVT computation. Thus, inFigure 4.11, if the tagged acknowledgment for the time stamp 30 messge reachesP before it reports its local minimum, this message will be included m the localnimum computation by 3 above. If the acknowledgment is received after PBreports its local minimum, this message will still be included by PB in its localminimum computation because the message was a transient unacknowledgedmessage when PB reported its local minimum. Either way, PB will account for themessage.More precisely, Samadis GVT algorithm operates as follows4.4 COMPUTING GLOBAL VIRTUAL TIME 119Figure 4.13 GVT computation using two cuts. GVT is based on the snapshot defined by thesecond cut, C2.redwhite cut pointwallclock timeC2ClPD rcorresponding execution using the synchronous algorithm. For example, in Figure4.l2b, the snapshot defined by the inconsistent cut includes a message in Pos localstate that would not exist in the snapshot obtained by the synchronous GVTalgorithm.Happily, an asynchronous GVT algorithm need not construct a consistent cut. Itcan simply ignore all messages that would make the cut inconsistent. Thesemessages can be ignored because they must have a time stamp larger than theGVT computed using the consistent cut. To see this, observe that the time stamp ofany message or antimessage sent by a processor after its cut point at wallclocktime T must be at least as large as the minimum of 1 the smallest time stamp of anyunprocessed event in the processor at time T and 2 the smallest time stamp of anymessage received by the processor after time T. The computed GVT value must beless than or equal to both of these quantities, so the time stamp of any inconsistentmessage must be larger than the computed GVT value.Thus, to compute GVT, one needs to only identify 1 the smallest timestampedunprocessed event within each processor at its cut point and 2 the smallest timestamp of any transient message crossing the cut from the past to the future. Thedifficult part is determining the set of transient messages without using acknowledgments. This can be accomplished by utilizing two cuts, as shown in Figure 4.13,and computing the GVT along the second cut, C2. The purpose of the first cut is tonotify each processor to begin recording the smallest time stamp of any message itsends these messages may be transient messages that cross the second cut and mustbe included in the GVT computation. The second cut is defined in a way toguarantee that there are no messages generated prior to the first cut that still have notbeen delivered to the destination processor. This guarantees that all transientmessages in the system crossing the second cut must have been sent after the firstcut and thus have been included in the GVT computation.Processors are said to be colored to denote where they are with respect to the twocuts. Initially processors are colored white. After the first cut point is reached, theprocessors color changes to red. After the second cut point, the processor returns tothe white color. Messages that are sent while the processor is white are called whiteIpastfuture cut point idle time in synchronousGYT algorithmbFigure 4.12 Cuts dividing the distributed computation into past and fugure parts. a Aconsistent cut b an inconsistent cut c cut for computation shown in a using thesynchronous GVT algorithm so that all cut points correspond to the same point in wallclocktime.wallclock timeNow suppose that rather than freezing each processor at its cut point, we alloweach processor to compute forward that is to say, we compute GVT asynchronously,as in Figure 4.l2a. Assume that the cut in the asynchronous algorithm is aconsistent one. The set of transient messages in the synchronous snapshot is exactlythe same as the set of transient messages in the asynchronous one because l anymessage in the synchronous snapshot will obviously also appear in the asynchronous one and 2 the asynchronous snapshot cannot include any additional messagesbecause any such message crossing the cut must have been sent by a processor afterits cut point, which would cause the cut to become inconsistent. Similarly it is clearthat the local snapshot taken by each processor in the asynchronous algorithm willbe identical to that in the synchronous algorithm because the consistent nature of thecut prevents any new events appearing in the asynchronous snapshot that did notappear in the synchronous one. Thus an asynchronous algorithm that computes GVTbased on the consistent cut in Figure 4.l2a will compute GVTr, the same valuecomputed by the synchronous algorithm.From the above discussion it is clear that if the asynchronous cut were notconsistent, the snapshot based on that cut would not match that used in the120 TIME WARP 4.4 COMPUTING GLOBAL VIRTUAL TIME 121messages, and messages sent while the processor is red are called red messages. Bydesign, all white messages must be received prior to C2. Thus, all transient messagescrossing C2 must be colored red. The set of messages crossing C2 is a subset of allred messages. Thus the minimum time stamp among all red messages is a lowerbound on the minimum time stamp of all transient messages crossing C2.GVT is computed as the minimum among 1 all red messages, and 2 theminimum time stamp of any unprocessed message in the snapshot defined by C2.This can be done by each processor based on local information. Thus the onlyquestion that remains concerns defining C2 so no white message crosses C2.C1 can be constructed by logically organizing the processors in a ring, and I.sending a control message around the ring. Upon receiving the control message, .each processor changes color from white to red, and passes the control message to the next processor in the ring. Once C1 has been fully constructed, it is guaranteedthat no new white messages are being created. C2 can be constructed one processorat a time by sending the control message around the ring again. However, aprocessor Pi will not forward the message to the next processor in the ring until itcan guarantee that there are no more white messages destined for Pi This can bedetermined as follows1. Compare the number of white messages sent to Pi2. Compute the number of white messages received by Pi3. Wait for these two numbers to be the same i.e., wait until all sent messageshave been received.For 1 each processor keeps a counter indicating the number of white messagessent to Pi These counters are accumulated within the control message as it circulatesamong the processors during the construction of C1. After the control token hasvisited every processor, the sum in the token indicates the total number of whitemessages that were sent to Pi This total value is passed to Pi during the constructionof C2, again via the control token. For 2, Pi keeps a counter indicating the number ofwhite messages it has received from any processor. When this second counter isequal to the number of messages sent to Pi received in the control token, Pi knows ithas received all of the white messages, so it can then pass the token to the nextprocessor in the ring.Counters indicating the number of white messages sent to a processor, and thenumber of white messages received by the processor must be maintained for everyprocessor in the system. Thus a vector of counters is required by each processor.Specifically, processor Pi maintains a local vector counter Vi such that j i J jindicates the number of white messages sent by Pi to Pj j is a negative numberthat denotes the number of white messages received by . When j  L Vj,j J i .s 0, then Pj has received all of the white messages that were sent to it.Each processor maintains the following local variables1. Tmin is defined as the smallest time stamp of any unprocessed message in theprocessor.2. Tred is defined as the smallest time stamp of any red message sent by theprocessor.3. Vi is the vector counter for processor Pi as defined above.4. Color is the current color of the processor, white or red.The GVT algorithm can now be described. Each message includes a flag indicatingits color white or red. On each message send, processor Pi executessend Color, timestamp to P jif Color  whitethen Vdj  Vdj  1else Tred   min T red , timestampWhen a message is received by a processor Pi it executesif Msg.color  whitethen Vi i  Vi i  1The control message contains three fields1. CMsgTmin recording the minimum time stamp value among unprocessedmessages among processors that the control message has visited thus far.2. CMsg Tred recording the minimum time stamp of any red message sent by aprocessor among the processors that the control message has visited thus far.3. CMsgCount is the cumulative vector counters among the processors visitedthus far. CMsgCounti indicates the number of white messages sent to Pithat have not yet been received number of transient messages.When the control message is received by Pi it executes the following procedureif Colorwhite thenT red   00Color  redwait until ViiJ  CMsgCountiJ.sOsend min CMsgTmin , Tmin  , min CMsgT red , Tred  ,Vi  CMsgCountto next processor in ringVi  0The GVT computation can be initiated by a controller. When the controllerreceives the control message after the first round, it first checks if CMsgCountis zero. If it is, there are no transient messages, so the GVT is the smaller ofCMsgTmin and CMsgTred . If not, the controller initiates a second round.4.5 OTHER MECHANISMSThe local and global control mechanisms described above form the heart of the TimeWarp mechanism. A number of additional mechanisms are often included in TimeWarp systems to provide added functionality, such as to support dynamic allocationof memory or to optimize performance. Several such mechanisms are describednext. An invocation of malloe  may be later rolled back. Since rollbacks aretransparent to the application, it is impossible for the TWLP to call free  torelease this memory. Further, assuming that the pointer to this memory wassaved in checkpointed state variables, the rollback would have erased anypointers to the dynamically allocated memory so that memory could bereferenced by the TWLP. Thus a memory leak occurs where memory hasbeen allocated but cannot be referenced or returned to the system. An invocation of free  may be rolled back. The original call to free will have released the memory to the system. This memory may have sincebeen reallocated by malloe  to another logical process. When free  isrolled back, the first process in effect declares that it did not actually releasethis memory, leading to the awkward situation that two different logicalprocesses are now using the same memory at the same time for completely1234.5 OTHER MECHANISMSdifferent purposes This will typically lead to unpredictable, and difficult tolocate, bugs in the program.4.5.2 Infrequent State SaVingThe copy statesaving mechanism discussed earlier makes a copy of all of themodifiable state variables used by the logical process prior to processing each event.Making a copy of the state vector before each event may consume large amounts oftime and memory. Incremental state saving provides an efficient mechanism toreduce these overheads if a relatively small portion of the state is modified by the.........The first problem resulting in a memory leak leads to an inefficient use ofmemory but does not cause incorrect results to be produced by the simulation. Itcould cause the program to prematurely exhaust all available memory resources,resulting in an aborted execution that would not have otherwise occurred. Thisproblem can be solved by having the Time Warp executive note each call tomalloe  and invoke free  whenever a call to malloe  is rolled back. Thiscould be implemented by defining a new procedure called TWmalloe  that iscalled by the application instead of malloe  . TWmalloe  would simply callmalloe , and then note the call to malloe  in the data structure for the eventthat is now being processed. The Time Warp executive would call free  for anysuch calls to malloe  made by each event that it rolls back.The second problem is perhaps more serious in that it can result in an executionerror, or worse, the simulation to produce incorrect results but otherwise appear to becorrect. Further this error may be difficult to reproduce because the same sequenceof rollbacks may not appear on subsequent executions. This problem can be solvedby treating calls to free  the same as calls to IO procedures. Namely thememory is not actually returned to the system until GVT advances beyond the timestamp of the event that called free  . Operationally, this could be implemented bydefining a new procedure called TWfree  that is called by the application ratherthan free  . The Time Warp executive maintains a list of calls to TWfree  thathave been made by each TWLP but have not yet been committed. TWfree  adds anew entry to this list but does not call free  to release the memory. If the eventcalling TWfree  is rolled back, the corresponding call is removed from the list ofuncommitted TWfree  calls. This list is scanned when fossil collection isperformed, and the memory is reclaimed by calling free  for any call toTWfree  with time stamp less than GVT.Finally, like any memory used to hold state variables, dynamically allocatedmemory must be checkpointed. This memory could be copy state saved by havingthe Time Warp executive maintain a list of dynamically allocated memory locationsfor each TWLP and automatically make a copy of this state prior to processing eachevent. Alternatively, incremental state saving can be used. Because the incrementalstatesaving mechanism does not need to distinguish between dynamically allocatedmemory and memory that was allocated before execution began, this mechanismoperates no differently than what was described earlier.TIME WARP1224.5.1 Dynamic Memory AllocationThe discussion thus far has implicitly assumed that all of the memory used for statevariables in the simulation is allocated before execution begins and is not releaseduntil execution completes. Simulation programs often allocate additional memoryfor state variables during the execution of the simulation. For example, a simulationof a factory might require additional memory to hold information concerning newcomponents created within the factory. Sequential simulation programs can obtainadditional memory by calling a memory allocation procedure defined by theoperating or runtime system for the program. For example, C programs may callthe malloe  procedure to allocate additional memory. Similarly an additionalprocedure may be invoked by the program to return memory that is no longer used,for example, the free  procedure for C programs. To facilitate the presentation,the discussion that follows refers specifically to calls to malloe  to allocatememory and free  to release memory, however the mechanisms that arediscussed apply generically to any dynamic memory allocation and release procedure.Some precautions must be taken when using dynamic memory allocation andrelease procedures in Time Warp systems, or for that matter, any parallel simulationsystem using rollback for synchronization. Suppose that the application callsmalloe  and free  directly without informing the Time Warp executive ofcalls to these procedures. Two problems may arise124 TIME WARP 4.5 OTHER MECHANISMS 125WhereD processed event unprocessed eventD saved stateantimessage2. send antimessage1. straggler message causes roilback3. restore state of LP to that prior to processing time stamp t 2 event4. reprocess events with time stamp 12 and 21 coast forwardInput Queueevent listOutput QueueantimessagesFigure 4.14 Rollback scenario using infrequent state saving. Note that the antimessage withtime stamp 24 is not sent for this rollback, but the antimessage with time stamp 38 is sent.an antimessage with time stamp 24 to be sent, which could cause a secondaryrollback to time 24, a time earlier than the original rollback. This could produce anew wave of cancellations and rollbacks with even smaller time stamps, leading to adomino effect that could roll back the computation beyond GVT. This clearly cannotbe allowed. Designating that no antimessages be sent for coastforward events, anddiscarding positive messages generated during the coastforward phase, eliminatesthis problem.How often should one checkpoint if using infrequent state saving The abovediscussion correctly points out that infrequent state saving is double edged. On theone hand, it reduces the number of state save operations that must be performed,favoring very infrequent state saves. On the other hand, rollbacks become more timeconsuming because of the need to recreate the required state, a cost that alsoincreases the less frequently one performs state saving. It can be shown that if oneassumes that the behavior of the Time Warp execution i.e., the frequency ofrollbacks and number ofevents that are rolled back, excluding coast forward eventsdoes not change as the statesaving frequency is varied, then mopt  the number ofevents processed between statesaves, should be set in the rangeevent computation. However, incremental state saving becomes more expensive thancopy state saving ifmost of the TWLPs state is modified by each event,14 since cpystate saving can perform block moves to save and restore state. Copy state savmgdoes not need to store the address of each variable that is modified only a singleaddress must be stored if the state variables are stored in contiguous memorylocations.An alternative approach is to use copy state saving, but save the state of thelogical processes less frequently than prior to every event. For example, a copy statesave operation might be performed every kth event. This approach is referred to asinfrequent state saving. This technique is more attractive than incremental statesaving if most of the state of the logical process is modified by each event.The rollback mechanism defined earlier must be modified to accommodateinfrequent state saving because the state of the logical process at the simulationtime of the rollback may not have been saved. Instead, the process must be rolledback to an earlier simulation time where the state of the LP was saved, and the eventsreprocessed in order to recreate the desired state.For example, Figure 4.14 depicts a rollback scenario where the state is saved afterevery third event. The state of the process prior to processing the time stamp 12event was saved, but the state prior to processing the events with time stamp 21 and35 was not saved. A straggler message containing time stamp 26 arrives. Since thestate of the LP at simulation time 26 the state prior to processing the time stamp 35event was not saved, the state of the LP must be restored to that at simulation time12, prior to processing the time stamp 12 event. The events with time stamps 12 d21 must be reprocessed to reconstruct the state at simulation time 26. Reprocessmgevents in order to reconstruct the desired state vector is referred to as the coastforward phase, and is only required when infrequent state saving is used. The coastforward computation increases the cost of state restoration, thereby increasing thetime to perform a rollback, and it is an additional cost associated with infrequentstate saving. The other computations associated with rollback, namely sending antimessages, processing the straggler, and reprocessing other rolledback eventsbesides those involved in the coastforward computation, must still be performedjust as was the case when a state save operation was performed prior to processingeach event.Coastforward events such as those at time stamps 12 and 21 in Figure 4.14must be treated differently from other events being rolled back. In particular, it isimperative that 1 no antimessages be sent when rolling back coastforward eventsand 2 no positive messages be sent when reprocessing events during the coastforward phase. Thus, in Figure 4.14, the antimessage with time stamp 24 must notbe sent. Suppose that the logical process were to send antimessages for coastforward events. Then this scenario depicts a case where a rollback to time 26 causes14 Empirical measurements report that incremental state saving is slower than copy state saving when morethan approximately 20 of the state variables are modified for certain simulations of telecommunicationnetworks. In general, however, the crossover point, where incremental state saving starts to becomemore expensive, is implementation and application dependent.rlm opt IV126 TIME WARP 4.5 OTHER MECHANISMS 127andand lI. is the number of events processed between rollbacks when state saving isperformed after each event or equivalently, the number of events executed by theprocess divided by the number of rollbacks when state saving is performed after eachevent, Ll is the cost to perform a state save i.e., to copy the state vector, and e is theexpected execution time for an event Lin, Preiss et al. 1993.4.5.3 Specifying What to CheckpointFor any checkpointing frequency, copy state saving can be realized transparent to theapplication program15 once the Time Warp executive has determined which memorylocations need to be saved. The Time Warp executive will automatically have thisinformation if the storage for state variables is allocated within the Time Warpexecutive. Alternatively, if storage for the state variables is allocated within theapplication itself, the application must pass the location of its variables to the TimeWarp executive. This latter approach is usually preferable if one is modifying anexisting simulation program to operate on a Time Warp system because the memoryallocation scheme used within the application can remain the same. Similarly theTime Warp executive must be notified whenever the set of checkpointed memorylocations changes, such as if memory is dynamically allocated or released during theexecution of the program. If copy state saving is used, this can be done transparent tothe application for dynamic memory allocation and release because such mechanisms must be handled within the Time Warp executive, as discussed earlier.Incremental state saving is more difficult to implement transparent to theapplication program. This is because unlike copy state saving, the Time Warpexecutive must checkpoint a variable each time it is modified. But how does the TimeWarp executive know when a variable is modified There are several approaches thatcan be used Manual checkpointing. The application programmer can be required to insert acall to the Time Warp executive prior to each modification of each statevariable. Actually, if a variable is modified many times while processing asingle event, the Time Warp executive needs to be notified only on the firstmodification, though no harm is done except with respect to efficiency bycheckpointing a variable more than once within an event. The call into theTime Warp executive must specify the address of the state variable and thecontents of that variable prior to modifying it. This information is then entered15 This means the application program need not explicitly perform any action for state saving to beperformed, once the Time Warp executive has determined which locations to state save.into the incremental state save log. The primary advantages of this approachare that it simplifies the Time Warp executive and is easy to implement. Theprincipal disadvantages are that introducing such state saving calls can betedious and prone to errors. Compilerpreprocessor checkpointing. A variation on the manual checkpointing approach is to automate the process of inserting the calls to the Time Warpexecutive. This can be accomplished by a preprocessor that scans through theapplication program, inserting calls to the Time Warp checkpointing routine asnecessary, or within the program compiler itself. To minimize the number ofunnecessary checkpointing calls, a control flow analysis of the program can beperformed so that a checkpoint call is only inserted the first time the variable ismodified, but ideally not on subsequent modifications. It may not be possibleto avoid all unnecessary checkpoint calls, however. The principal advantage ofthis approach is that it relieves the application programmer of the responsibilityfor inserting incremental statesaving calls. The principal disadvantage is thecost of developing and maintaining a specialized preprocessor or compiler. Overloading the assignment operator. Many languages, especially objectoriented programming languages, allow the application program to redefineprimitives such as the assignment operator  in many languages. Thisoperator can be redefined to perform an incremental statesaving operationbefore modifying the program variable. It is more difficult to avoid unnecessary checkpoints to variables with this technique. One approach is to maintainsome information with each state variable to indicate if it has already beenmodified by this event. For example, one could store the time stamp of the lastmodification with each variable and check this information prior to checkpointing it. The time required to check this information, however, could bealmost as large as just blindly checkpointing the event, so the principal savingswith this approach is reducing storage required to maintain the log, andreducing the time to restore the state after rollback. The principal advantagesof overloading assignment operators are that it frees the application programmer from checkpointing each call and it is simpler to implement than buildinga compiler, preprocessor, or as discussed next a program for editing theexecutable. The principal disadvantage is this approach can only be applied toprogramming languages that support overloading of the assignment operator.C is perhaps the most wellknown language that supports this capability.Other widely used languages such as C do not provide this facility. Executable editing. Another approach to inserting incremental statesavingcalls is to edit the file containing the executable for the program. Theexecutable contains a representation of the machine instructions for theprogram. Thus this approach is similar to the preprocessor approach, but itoperates on the machinelevel representation of the program rather than thehighlevel language. Many modem reducedinstructionsetcomputers RISCuse a loadstore architecture where the only instructions accessing memoryare load and store instructions. For these CPUs the program editing the128 TIME WARP 4.5 OTHER MECHANISMS 129executable needs only to examine store instructions, and if the store corresponds to a modification of a state variable, machine instructions are inserted tocheckpoint the variable before it is modified. Some technique must be used toidentify those store instructions modifying state variables. For instance, if theTime Warp system does not allow automatic variables stored on the stack to bestate variables, all modifications made by eventprocessing procedures tononstack variables can be checkpointed. The same techniques may be usedfor nonloadstore machine architectures, but a wider variety of machineinstructions must be examined to catch all modifications to state variables.Like the preprocessorcompiler approach, flow analysis can be used toeliminate unnecessary checkpointing operations. The central advantages ofthis approach are that it is less language dependent compared to the preprocessorcompiler and operator overloading approaches and can be applied tolibraries of compiled code that must be checkpointed, but for which the sourcecode is not available. The central disadvantages of this approach are that it ismachine dependent and may not be easily ported to new architectures.denoting the arrival of an aircraft at JFK, and then receive a new message indicatingthe aircraft has been rerouted to Boston because of congestion at JFK. This could bemodeled by having the ORD process retract the arrival event at JFK and schedule anew arrival event at the process modeling Logan Airport in Boston.At first glance, retraction and cancellation of events appear to be one and thesame. However, there is an important difference. Retractions are invoked by theapplication program, so there must be a mechanism to undo an event retraction ifthe event computation that invoked the retraction is rolled back. No such mechanismis necessary for event cancellation, because cancellation is an operation that isrealized within the Time Warp executive.Two approaches to implementing event retraction include Implementing it at the application level, such as in a library, without providingany additional support in the simulation executive. Implementing a retraction primitive within the simulation executive.4.5.4 Event RetractionThe principal advantages and disadvantages of these approaches to incrementalstate saving are summarized in Table 4.1. There is no one approach that is clearlysuperior to the others for all circumstances and situations. The manual and operatoroverloading approaches are perhaps the most commonly used techniques today.Recall that event retraction refers to a mechanism, invoked by the applicationprogram, to unschedule an event that had previously been scheduled. This mightbe used to implement unexpected events such as interrupts. For example, supposethat the air traffic simulation were augmented to model inflight rerouting ofaircraft. The logical process for the ORD airport may have scheduled an event4.5.5 Lazy CancellationWhen a rollback occurs, the Time Warp mechanism described earlier in this chapterwill immediately cancel messages sent by rolledback events by sending an antimessage for each one. It is sometimes the case that when an event is rolled back andEvent retraction can be implemented on top of an existing Time Warpexecutive that does not support retraction by realizing the retraction primitive andscheduling an event with time stamp slightly smaller than the event being retracted.Let E denote the event being retracted, and ER the event that retracts E. Both E andER must be processed by the same logical process. When ER is processed, a variableis set in the state vector of the logical process that indicates that event E should beignored when it is removed from the event list for processing. The central advantageof this approach is that it allows event retraction to be implemented with an existingTime Warp executive that does not support this facility. The disadvantage is that thismechanism may be somewhat less efficient than a retraction mechanism implemented within the simulation executive.The second approach to implementing retraction is to provide a new mechanismto retract previously scheduled events. A straightforward technique is to have theapplication program send an antimessage for each event it is retracting. The TimeWarp annihilation mechanism will guarantee the event is properly canceled, andcomputations depending on that event are rolled back and reexecuted.A retraction operation for an event E can be rolled back by simply resending theoriginal positive message for E. A log ofretractions can be kept by placing a positivecopy of the each event that is retracted in the output queue. When the logical processis rolled back, positive messages stored in the output queue can be sent along withthe other antimessages.Manual insertion is tedious anderror proneCost to develop and maintainpreprocessor or compilerOnly applicable to languagesthat support overloading theassignment operatorNot easily ported to newmachine architecturesPrincipal DisadvantageOperator overloading Easy to implementTABLE 4.1 Approaches to Incremental State SavingManual checkpointing Easy to implement in a TimeWarp executivePreprocessorcompiler PortableTechnique Principal AdvantageExecutable editing Not language specific, supportsstatesaving libraries wheresourcecode is not available130 TIME WARP 4.5 OTHER MECHANISMS 131receiving a straggler message or an antimessage in a processs past, no antimessages are sent. Instead, the process only sends an antimessage if the originalmessage was not again created when the events were reprocessed. Operationally thiscan be implemented by examining each message sent during the reexecution phase,and checking the output queue to see if that message was previously sent. If it was,the new message is discarded and the antimessage remains in the output queue. If itwas not, the message is sent just as is done during normal Time Warp operation.An antimessage is removed from the output queue and sent if no matching positivemessage was generated during the recomputation phase, and the logical processadvances to a simulation time larger than the simulation time when the antimessagewas created. A field in the antimessage called the send time stamp is used to denotethe simulation time of the logical process when the antimessage was created.For example, in the scenario shown in Figure 4.15, the events in the ORD processfor flight 100 will first be processed, resulting in the time stamp 1200 message beingsent to JFK. The ORD process will roll back when it receives the time stamp 700straggler message for flight 200. However, unlike the scenario described earlier, thetime stamp 1000 and 1200 events will not be canceled. This message, and the eventat 800, will be processed. The 900 event denoting the arrival of flight 100 will bereprocessed. It will again regenerate the same event with time stamp 1000, so nonew event is scheduled. Similarly, when the 1000 event is reprocessed, it will alsoregenerate the same 1200 event, so again, no new event is scheduled. Lazycancellation avoids canceling the 1000 event, and more significantly, the 1200event. Had the 1200 event generated by the reexecution been different from thatproduced during the initial processing, or if no event were regenerated at all, theantimessage for the 1200 event would be sent once the ORD logical processadvanced beyond simulation time 1000.The original mechanism presented earlier where antimessages are sent as soon asthe rollback occurs is called aggressive cancellation. Lazy cancellation offers theadvantage that it avoids canceling messages that are recreated when rolledbackevents are reprocessed. This will also eliminate secondary rollbacks that occur inaggressive cancellation when an antimessage is unnecessarily canceled. Theprincipal disadvantages of lazy cancellation are as follows1. Cancellation of incorrect computations is delayed, compared to aggressivecancellation. In general, the sending of antimessages is delayed until somerolledback events are reprocessed. This delay allows the damage i.e.,incorrect computations caused by the incorrect message to spread further thanit would had the antimessage been immediately sent. Thus more computationmay need to be rolled back when the antimessage is finally sent.2. Reprocessing events using lazy cancellation requires some additional overhead computation relative to aggressive cancellation to compare messages thatare sent with antimessages in the output queue. Also the simulation executivemust check if any antimessages must be sent whenever the processssimulation clock is advanced.simulation timesimulation timeb12001200JFK ...aJFK ....1..OROreprocessed, the same message that was produced and subsequently canceledduring the original execution is again recreated when the event is reprocessed. Inthis case it was not really necessary to cancel the original message.For example, let us consider an air traffic simulation like that discussed earlier,with aircraft arrival and departure events. Consider the scenario shown in Figure4.15. An event is received, and processed, indicating that flight 100 arrived at ORDat 900 AM, exchanged passengers and subsequently departed for JFK at 1000 AM,and arrived at JFK at noon. The ORD process will thus send a message to the JFKprocess modeling the arrival at noon. Later in the execution of the simulation, astraggler message is received at ORD with time stamp 700 AM modeling flight 200sarrival. Assume that these are the only two flights that arrive that morning. 16 Flight200 then departs for another airport at 800 AM. Since flight 200 has come and gonebefore flight 100 arrived, it is quite likely the message sent by ORD denoting flight100s arrival at JFK at noon will not be affected by flight 200. 17 However, TimeWarp will still roll back the events for flight 100, cancel the message sent to JFKwith time stamp 1200, reprocess these events, and resend the same message back toJFK. The cancellation of the 1200 message at JFK will cause a rollback if the JFKLP already processed that message. It is clear the cancellation of the 1200 messagewas unnecessary.Lazy cancellation is a technique that avoids canceling messages that are laterrecreated when the events are reprocessed. When a rollback occurs either fromFigure 4.15 Scenario demonstrating lazy cancellation. a Scenario of events after simulating flight 100 b scenario after events for flight 200 are processed. Events for flight 200 donot affect the arrival of flight 100 at JFK at 1200.16 ORD only handling two aircraft in one morning is clearly unheard of, but it greatly simplifies thepresentation of this example.17 This is not necessarily the case. For example, passengers arriving on flight 200 might then depart onflight 100, affecting the subsequent message to JFK if that message included a passenger count. However,the model presented here does not include information such as this in its messages.132 TIME WARP 4.6 SCHEDULING LOGICAL PROCESSES 1333. Lazy cancellation requires some additional memory to hold antimessagesduring the recomputation phase that would have been sent allowing thememory they used to be reclaimed if aggressive cancellation had been used.Depending on the application, performance of Time Warp using lazy cancellationcan be better or worse compared to aggressive cancellation. One can constructscenarios where lazy cancellation outperforms aggressive cancellation by as much asa factor of N when executing on N processors, and one can construct scenarioswhere aggressive cancellation executes N times faster than lazy cancellation. Only alimited amount of empirical data is available comparing aggressive and lazycancellation however, these data suggest that lazy cancellation typically performsmarginally better than aggressive cancellation in queueing network benchmarkprograms. The difference in performance that has been reported is usually somewhatmodest, however. For example, on the order of 10 to 15 relative to aggressivecancellation is typical.The lazy cancellation optimization does point out a noteworthy characteristicconcerning the Time Warp mechanism, and for that matter, most synchronizationprotocols that have been proposed thus far. Time Warp conservatively assumesthat one event depends on a second event if they both are scheduled at the samelogical process, and invokes the rollback mechanism based on this assumption. Iftwo events occur with a single TWLP but do not depend on each other, there is noneed to process them in time stamp order. Because Time Warp does not attempt toanalyze dependencies between events within the same process, which in general is avery difficult problem, it cannot avoid such unnecessary rollbacks.4.5.6 Lazy ReevaluationAnother technique along similar lines as lazy cancellation is the lazy reevaluationor jumpforward optimization. Again, consider the scenario depicted in Figure 4.15.Lazy cancellation avoided canceling and resending the time stamp 1200 message tothe JFK process. However, even with lazy cancellation, the two events at ORD forflight 100 with time stamp 900 and 1000 will have to be reprocessed after therollback. If the reprocessing of these events for flight 100 is identical to that in theoriginal execution i.e., if flight 100s events are completely unaffected by those forflight 200, there is no reason to process them again after the rollback.Lazy reevaluation is an optimization that attempts to exploit this fact. The centralquestion is how can the Time Warp simulation executive determine if the recomputation of an event will be the same as the original computation The answer isif the state of the logical process before reprocessing the event after a rollback is thesame as what it was in the original execution of these events, then the recomputationwill again be the same. This assumes, if course, that each event computation isrepeatable.To implement this technique, the saved state vector indicating the state of thelogical process before each event was processed must not be discarded when anevent is rolled back. During the reexecution phase, before reprocessing an event, theTime Warp executive compares the state of the logical process with the state thatexisted before the event was last processed, which is saved in the state queue. If thetwo are equal and the set of unprocessed events is the same as the previousexecution, the logical process can skip reprocessing the events, and immediatelyjump back to its state prior to when the rollback was processed. For example, inFigure 4.15, assume again that the time stamp 900 and 1000 events have beenprocessed when the straggler at 700 is received. The ORD logical process rollsback, processes the straggler and the time stamp 800 event created by the straggler.The logical process then observes that the current state of the logical process is thesame as the saved state of the process prior to processing the 900 event. The logicalprocess can restore the state of the logical process to that which existed just prior towhen the straggler arrived, mark the 900 and 1000 events as processed, and resumeprocessing events as if the rollback never occurred.Lazy reevaluation is similar to lazy cancellation, except it deals with state vectorsrather than messages. It is useful when straggler events do not affect the state of thelogical process. One situation where this might occur is the query event. Queryevents are intended to read state variables within a logical process and return thosevalues to the scheduler of the event. The main drawback with lazy reevaluation isthe cost of comparing state vectors, and the fact that straggler events typically domodify state variables, so often processes are not able to benefit from this technique.If incremental state saving is used, comparison of state vectors may be cumbersomeand timeconsuming, since one must examine logs to recover a snapshot of the stateof the process.4.6 SCHEDULING LOGICAL PROCESSESA processor will, in general, contain many logical processes. A scheduling policy isrequired to select the logical process that should be allowed to execute next. Twoimportant questions are When should execution change from one logical process toanother And, when execution changes to a new logical process, which one isallowed to execute next While scheduling has been widely studied in generalcon.texts i.e., nonsimulation applications, it merits special attention in the design ofa Time Warp simulation executive because selection of a poor scheduling algorithmcan lead to extremely poor performance..Nearly all existing simulation executives both optimistic and conservative onlyshift execution from one logical process to another between, as opposed to during,processing successive events. Possible approaches concerning when to switchexecution from one logical process to another include the following Process all unpocessed events for a logical process before changing executionto another logIcal process. This has the advantage that it minimizes thefreuency of chan.ging the procss that is executing, which may entail contextSWitch overheads If a processonented simulation paradigm is used, and yieldsbetter locality of memory references, improving the performance of cache134 TIME WARP 4.8 ADDITIONAL READINGS 135memories. IS However, this approach can cause some logical processes toexecute far ahead of others in simulation time, and can lead to very poorperformance. This is particularly true if a process schedules events for itself.This issue will be explored in much greater depth in the next chapter. Thus thisapproach is not well suited for Time Warp executives. Allow execution to change to a different logical process after processing eachevent. This approach is most commonly used in Time Warp executives today,and it provides the greatest control and ensures that the most appropriatelogical process is executed at any instant. Selection of the most appropriatelogical process will be discussed momentarily. Allow execution to change to a different logical process after processing somenumber of events. This approach is a compromise between the two approachesdescribed above. The trigger that causes the Time Warp executive toconsider switching execution to another logical process might be after thecurrent TWLP has processed some number ofevents, after it has been allocateda certain amount of CPU time to execute, or perhaps when the current logicalprocess has advanced in simulation time some threshold ahead of other logicalprocesses.When a scheduling decision is made, which logical process should be allowed toexecute In principal, one would like to execute the logical process that leads to theshortest overall execution time however, this is impossible to predict because onecannot know what event computations will take place in the future. A related goal isto allow the logical process that is least likely to roll back to execute next. From acorrectness standpoint, the scheduling mechanism must be fair in the sense that thesmallest timestamped event in the entire simulation system must eventually beallowed to execute, or Time Warp cannot guarantee forward progress and could entera livelock situation where it continues to process and roll back events, but GVT doesnot advance.A commonly used approach is to always process the event in the processor withthe smallest time stamp next, by the rationale that this event is least likely to berolled back. This smallest time stamp first STF policy has the advantages that it issimple and that it tends to let the logical process farthest behind in simulation timeexecute next. Further, if the Time Warp executive has no information on whatmessages will arrive in the future, the process with the smallest timestamp isintuitively the process one would expect is least likely to be rolled back by messagesthat will arrive in the future. The smallest timestamped event in the entire simulatorwill never be rolled back.18 A cache memory is a highspeed memory that holds recently referenced instructions and data. Virtual1yal1 general purpose microprocessors use some form of cache memory. If the program repeatedly accessesthe same memory locations, the memory locations being referenced are more likely to be in the cache,improving performance.4.7 SUMMARYThis chapter has focused on the Time Warp mechanism, by far the bestknownoptimistic synchronization protocol. Although many optimistic protocols have sincebeen developed, Time Warp is important because it has introduced many new,fundamental concepts and mechanisms that are widely used in other protocols. Forexample, antimessages, state saving, event rollback, Global Virtual Time, and fossilcollection are utilized in most other optimistic synchronization mechanisms.Further, use of rollback requires the introduction of new mechanisms to performother commonly used operations such as IO and dynamic memory allocation.Specifically, the new wrinkle that is added is that there must be the ability to rollback these operations. Thus the techniques discussed here to allow these operationsto be rolled back are equally applicable to other synchronization mechanisms usingrollback.The discussion in this chapter has been focused on what might be termed pureTime Warp simulation executives. As will be discussed in the next chapter, a pureTime Warp system has certain deficiencies that can lead to very poor performancefor some applications, and a variety of other optimistic synchronization protocolshave since been developed to solve these problems.4.8 ADDITIONAL READINGSThe Time Warp algorithm is described in Jefferson 1985. Prior to Time Warp,optimistic synchronization was proposed for database concurrency control Kungand Robinson 1981, and it has long been used in computer architecture, such as inpredicting the outcome of branch instructions Hennessy and Patterson 1996.Conversely, Time Warp has also been proposed as a means to parallelize arbitrarysequential programs Knight 1986 Cleary, Unger et al. 1988 Tinker and Katz 1988Fujimoto 1989 Tinker 1989.Implementation of incremental state saving using operator overloading andexecutable editing are described in Ronngren, Liljenstam et al. 1996, Chandrasekaren and Hill 1996, and West and Panesar 1996, respectively. Measurements ofstatesaving costs in a Time Warp system are reported in Cleary, Gomes et al.1.994. The optimal checkpointing interval for infrequent state saving is derived inLIll, Preiss et al. 1993, and performance evaluations using this technique arereorted in Bellenot 1992, Preiss, MacIntyre et al. 1992 Palaniswamy andWIlse 1993,. Avril and Tropper 1995, and Fleischmann and Wilsey 1995.Adaptive selectln of the checkpointing interval is described in Ronngren and Ayani1994. A hybnd approach that combines copy and incremental state saving isdescribed in Franks, Gomes et al. 1997.Perhaps the first GVT algorithm using message acknowledgments is described inSmadi 1985 where the trasient message and simultaneous reporting problems arediscussed. The GVT algonthm based on distributed snapshots is described inMattern 1993. Distributed snapshots, on which the Mattern algorithm is based,136 TIME WARPsimulation timeCHAPTER 5 memory bufferholding an event memory bufferholding an evento fossil collectedmemory bufferbsimUlation timeGVTaILP 0,  ..ILPA ..0 .. ..IAdvanced Optimistic TechniquesIn a pure Time Warp system there is no limit as to how far some logical processescan advance ahead of others in simulation time. This is problematic because it canlead to very inefficient use of computation and communication resources. First, theamount of memory required to execute the simulation can become unboundedlylarge. To see this, consider a simulation with two logical processes, each processingone event per unit of simulation time. Suppose that LPA requires 1 millisecond ofwallclock time to process each event but LPB requires 10 milliseconds. LPA willadvance ten events in the time LPB takes to advance only one event, so it requiressufficient memory to hold ten message buffers see Figure 5.1 a. After LPB hasprocessed a second event, LPA has processed ten more events see Figure 5.1 b.Only one event can be fossil collected from each LP. It is clear that the memoryrequirements will grow without bound as the simulation progresses.Figure 5.1 Example illustrating unbounded memory requirements of a Time Warp simulation. a Snapshot after 10 milliseconds b snapshot after 20 milliseconds.are discussed in Chandy and Lamport 1985 and Ahuja 1990. Other GVTalgorithms are described in Bellenot 1990 Lin and Lazowska 1990 Concepcionand Kelly 1991, Tomlinson and Garg 1993, Dsouza, Fan et al. 1994, Lin1994, Varghese, Chamberlain et al. 1994, Xiao, Cleary et al. 1995, andFujimoto and Hybinette 1998. The onthefty fossil collection technique isdescribed in Fujimoto and Hybinette 1998.Errorhandling mechanisms for Time Warp were implemented in some of theearliest implementations. For example, JPLs Time Warp Operating System Jefferson, Beckman et al. 1987 included facilities to catch nondestructive errors usingsignal handlers. More recently, the problem is discussed in Nicol and Liu 1997. ,.1.Extensions to Time Warp to realize sharedstate variables and apPlication,f... invoked event retraction are described in Fujimoto 1989, Ghosh and Fujimoto ,1991, Mehl and Hammees 1993, Bruce 1995, and Lomow, Das et al. 1991.Lazy cancellation is described in Gafni 1988, and performance comparisons withaggressive cancellation are presented in Reiher, Fujimoto et al. 1990. Lazy reevaluation is described in West 1988. Use of hidden fields in the time stamp toallow zero lookahead simulations are discussed in Reiher, Wieland et al. 1990 andMehl 1992.Several attempts to develop analytic models to predict Time Warp performancehave been conducted. Early investigations were confined to analyzing two processorsLavenberg, Muntz et al. 1983 Mitra and Mitrani 1984 Felderman and Kleinrock1991. Later work extended this to arbitrary numbers of processors Gupta, Akyildizet al. 1991 and under limited memory constraints, as discussed in the next chapterAkyildiz, Chen et al. 1993. In Lin and Lazowska 1990 it is shown that if statesaving and rollbacks require zero time, Time Warp using aggressive cancellation willachieve execution time equal to the critical path through the computation if incorrectcomputation never rolls back correct computation, and Time Warp using lazycancellation can reduce execution time even further. Lipton and Mizell 1990show that Time Warp can outperform the ChandyMisra null message algorithm byup to a factor of N using N processors, but the null message algorithm can onlyoutperform Time Warp by a constant factor assuming constant rollback costs.Performance bounds for Time Warp and conservative protocols are derived inNicol 1991. Finally, several empirical studies of Time Warp performance for avariety of applications have been performed. Among the earliest are Fujimoto1989, Wieland, Hawley et al. 1989, Morse 1990, and Preiss 1990. Theevaluation described in Fujimoto 1990 introduces a synthetic workload modelcalled PHOLD that is sometimes used to benchmark different systems. Earlyimplementations of Time Warp are described in Jefferson, Beckman et al. 1987and Fujimoto 1989.Hardware support for Time Warp has also been proposed. Support for statesaving is described in Fujimoto, Tsai et al. 1992, and support for reductionnetworks to perform global minimum computations is described in Reynolds,Pancerella et al. 1993.138 ADVANCED OPTIMISTIC TECHNIQUES 5.1 MEMORY UTILIZATION IN TIME WARP 139Figure 5.2 Snapshot of a Time Warp computation indicating memory required to storeevents.t9 Actually memory can also become available through the message annihilation procedure. However,there is no guarantee additional annihilations will occur, so this cannot be relied upon to break thedeadlock situation.wait forever because a GVT advance may be required to reclaim additional memory,but GVT cannot advance because LPB is blocked.i9Solving this problem requires 1 a mechanism to either directly or indirectlycontrol memory utilization and 2 a policy to judiciously invoke this mechanism tomaximize performance without consuming too much memory. After introducingsome additional terminology, we next discuss memory management mechanismsand policies, and a property known as storage optimality.5.1.1 Preliminaries State Vectors and Message Send Time StampsThere are three types of memory used by the Time Warp system that are of concernhere memory used to hold 1 positive messages stored in the input queue, 2antimessages stored in the output queue, and 3 state vectors stored in the statequeue. The first two contain timestamped event messages. For the purposes ofmemory management, it is convenient to view the third, state vectors, also asmessages. Specifically, a state vector can be viewed as a message sent by an LP toitself containing information concerning the state variables within the LP. The timestamp of the state vector is that of the event that reads the state information, which isthe next event that is processed by the LP. Viewing state vectors in this way allows usto discuss memory management in terms of only one type of memory object, timestamped messages.Many memory management protocols assign two time stamps to each message.In addition to the time stamp field discussed previously, a second one is defined thatindicates the virtual time of the logical process when it scheduled the event. This isreferred to as the send time stamp of the event. The time stamp field discussed up to processed evento unprocessed eventGVTsimulation time.000I.......... 000.I............j..  ...O .I..........    0 .II...........j ..................DIILPcLPDLPELPnA second source of inefficiency with such overoptimistic execution is that itcan lead to very long andor frequent rollbacks. It is clear from Figure 5.Ib that ifLPB now sends a message to LPA a long rollback will occur. Long rollbacks implymuch time was wasted performing computations that were later thrown away. Thisisnot so problematic if the processors have no other useful computations to performand would otherwise be idle, but otherwise, it is a serious deficiency. Further,significant computation and communication resources may have to be expended toperform the rollback itself. As will be discussed later, a rollback thrashingbehavior could result where logical processes spend most of their time processingrollbacks rather than performing useful simulation computations. IBuilding upon the Time Warp mechanism, this chapter discusses advanced topics concerning optimistic synchronization. Specifically, memory management mechan isms and other optimistic synchronization protocols that have been proposed sinceTime Warp first appeared are discussed. We conclude this chapter by examining thedesign of an operational Time Warp system we discuss various optimizationsincluded in this design to exploit sharedmemory multiprocessors.5.1 MEMORY UTILIZATION IN TIME WARPConsider the execution of a Time Warp program where there is some fixed amountof memory available on the parallel computer. One might ask, What happens if theTime Warp system runs out of memory In a sequential simulation one typicallyaborts the program, and informs the user that additional memory is required tocomplete the simulation run. Can a similar strategy be used for Time WarpprogramsThis approach is not entirely satisfactory for Time Warp programs. This isbecause the amount of memory required during the parallel execution depends asmuch on the dynamics of the execution as it does on properties of the simulationmodel for example, the number of state variables that are used, which is not underdirect control of the user in a pure Time Warp system. Specifically, logicalprocesses that have advanced ahead of others in simulation time will require morememory to hold past events, antimessages, and the like, than processes that remainclose to GVT. The Time Warp system may have run out of memory because itallowed some LPs to advance too far ahead of others. Simply purchasing additionalmemory will not necessarily solve the problem because the Time Warp executivemay be even more overoptimistic in the next execution, causing the system to againrun out of memory. Further, the amount of memory required will vary from one runto the next, meaning the same simulation program may execute to completion oneday but then abort because of lack of memory resources the next time it is executed.One might suggest blocking an LP when it runs out of memory rather thanaborting the program. Unfortunately, this is not a satisfactory solution either becauseit can lead to deadlock situations. For example, in Figure 5.2, suppose that LPB , thelogical process executing at simulation time equal to GVT, has run out of memory inattempting to schedule a new event. If the LP blocks to wait for more memory, it maya140 ADVANCED OPTIMISTIC TECHNIQUES 5.1 MEMORY UTILIZATION IN TIME WARP 141  message sendhack rolledback event10 20 30 simulated timeFigure 5.3 Example of message sendback mechanism. Event 20 is returned to sender, LPA,causing events 10 and 20 to be rolled back.now is referred to as the receive time stamp or simply time stamp because itindicates the simulation time of the logical process when it receives the message.The send time stamp of an event for example, 20 in Fig. 5.3 is equal to the receivetime stamp of the event that scheduled this event 10 For example, Figure 5.3shows three events, 10, 20, and 30 The send and receive time stamps of themessage containing 20 are 10 and 20, respectively.5.1.2 Memory Management Mechanisms and Message SendbackThere are several mechanisms that can be used to control memory usage or toreclaim memory once the system has run out1. Blocking. The Time Warp system can block certain logical processes toprevent them from advancing forward and allocating additional memory.Blocking prevents future allocations of memory but does not provide ameans for reclaiming memory currently being used by the Time Warpsystem.2. Pruning. The Time Warp system reclaims memory used by state vectorscreated via copy state saving. The net effect after reclaiming these state vectorsis the same as if infrequent state saving had been used to avoid creating thestate vectors in the first place.3. Rollback. A logical process can be rolled back to reclaim memory used forstate vectors in the rolled back events. In addition rollback will typically causeantimessages to be sent, and the subsequent message annihilations androllbacks will release additional memory resources.4. Message sendback. This is a new mechanism to reclaim memory by returninga message back to its original sender.Blocking and rollback have been discussed previously, and pruning is based onprinciples infrequent state saving that have also been discussed in Chapter 4. Themessage sendback mechanism is described next.Message sendback is based on the observation that a logical process can recovermemory used by a message in its input queue by removing the message from thequeue, returning it to its sender, and reclaiming the memory.20 The logical processreceiving the returned message will usually roll back when it receives the returnedmessage to a simulation time prior to when the message was sentl For example,Figure 5.3 illustrates an example of the message sendback mechanism. Here, LPBreturns the message holding event 20 to its original sender, LPA Upon receiving thereturned message, LPA rolls back events 10 and 30 Rolling back these events mayresult in sending antimessages, which may in turn cause secondary rollbacks andreclaim additional memory. After the events have been rolled back LPA will thenreprocess 10 and possibly 30, resending 20 to LPB . This cycle will repeat,creating a type of busy wait loop until sufficient memory is available for thecomputation to proceed forward.Recall that GVT defines a lower bound on the time stamp of any future rollback.Therefore the message sendback mechanism cannot return a message whose sendtime stamp is less than or equal to GVT,22 as this would result in a rollback beyondGVT. Further the definition of GVT itself must be modified to accommodatemessage sendback. This is because there may be a sentback message in transitreferred to as a backward transient message, being returned to its sender while theGVT computation is in progress. If the GVT computation does not take into accountthis message, a rollback beyond GVT could occur through the following sequence ofactions1. A message M is sent back with send time stamp equal to T T  GVT.2. A new GVT computation is initiated and computes a new GVT value greaterthan T M remains in transit while the new GVT value is computed.3. M is received by its original sender, causing a rollback to T. However, GVThas advanced, so a rollback beyond GVT occurs.To circumvent this problem, GVT is redefined as followsDefinition GVT with message sendback Global Virtual Time at wallclock timeT during the execution of a Time Warp simulation GVTT is defined as theminimum among l the receive time stamp of all unprocessed and partiallyprocessed messages and antimessages in the system and 2 the send time stampof all backward transient message at wallclock time T.The algorithms described in the previous chapter can still be used to computeGVT. One need only note messages that are being sent back, and modify theminimum computation to use the send time stamp of any such transient messagerather than the receive time stamp.20 If the message that is sent back has already been processed, the LP must also roll back to prior to thetime stamp of the message. If the message being sent back is actually a state vector recall state vectorsare viewed as messages here, no rollback is required.21 This will normally be the case. An LP may be at a point preceding the generation of the message if itWas rolled back after sending the message.22 It can be shown an infinite loop can occur if messages with send time equal to GVT are returned.142 ADVANCED OPTIMISTIC TECHNIQUES 5.1 MEMORY UTILIZATION IN TIME WARP 143simulated timeTime Warp can be fossil collected ineligible for deletionD eligible for deletionIIIII.....  IGVTTsequential already processed pending, in event listD not yet generatedLPc .LPB  .LPA    ...  .IIIreclaimed by the memory management protocol via message sendback orrollback.Figure 5.4 Snapshot of a Time Warp execution illustrating the different types of events withrespect to memory management. Arcs in this figure indicate which events scheduled whichother events.The memory management protocol must locate and, if necessary, reclaim storageused by future events. The protocol will achieve the storage optimality property if itis able to reclaim all of the storage or to within a constant factor used by the futureevents.The focus of the discussion here is on storage optimal memory managementprotocols for optimistic parallel simulations. Note that existing conservative simulation protocols are not storage optimal, though it is widely believed that memoryusage is not as severe a problem in conservative simulation systems as it is inoptimistic systems.It may be mentioned that storage optimality is much more difficult to achieve ondistributedmemory architectures than on sharedmemory platforms. To see this,consider the simulation application depicted in Figure 5.5 consisting of five logicalprocesses organized as a ring. Assume that each logical process executes on adifferent processor. This application initially contains a single message. When themessage is processed, it creates one new message that it sends to the next process inthe ring, with a certain time stamp increment. Because there is only one event in thepending event list at any time, a sequential simulator will only need two memorybUffers one to hold the current event being processed, and one to hold the new eventthat is scheduled by this event. A Time Warp execution of this program on a sharedmemory multiprocessor using a storage optimal memory management protocol willalso only require two buffers if they are stored in shared memory and can beallocated to any processor. Operationally the Time Warp program will process an1. Past events. Send time stamp receive time stamp  GVT these events canbe fossil collected.2. Present events. send time stamp GVT  receive time stamp these eventsmatch those that would exist in the pending event set in a sequential executionat time T. 233. Future events. GVT  send time stamp receive time stamp these eventshave not yet been created in the sequential simulator at time T, and they can be5.1.3 Storage OptimalityIt is clear that some minimal amount of memory is required to reasonably expect theTime Warp simulation to complete, just as some minimal amount of memory isrequired to execute a sequential simulation. The amount of storage required toexecute a sequential simulation would seem to be a reasonable lower bound on theamount of memory required for a parallel execution. It is clear, however, that even ifone were able to execute the simulation program using this little memory,performance would be poor because there is little latitude for processes to advanceoptimistically ahead of others. Thus one would like a memory management protocolthat is able to guarantee that it can complete the simulation if given only the memoryneeded in a sequential execution but can utilize additional memory to exploitoptimistic execution when it is available. This approach gives rise to storage optimalmemory management protocolsDefinition Storage Optimality A memory management protocol is said to bestorage optimal if it is able to guarantee that it can complete the execution of anysimulation program using no more than K times the memory required to complete asequential execution of the same program for some constant K.Of course, K should be small for the algorithm to be practical. To see how amemory management protocol can achieve storage optimality, consider a Time Warpsimulation where GVT is equal to T. Storage optimality can be achieved if thememory management protocol can reclaim all memory buffers that would not existin a sequential execution of the simulation program that has advanced to simulationtime T. To achieve this, the memory management system must be able to recognizethose events that would exist in the sequential simulation at time T, and those thatwould not. Fortunately this is straightforward. The key observation is that the eventsthat reside in the pending event list in a sequential simulation at time T are those thathave a time stamp greater than or equal to T, and were scheduled prior to simulationtime T.To illustrate this concept, consider the snapshot of the execution of a Time Warpprogram shown in Figure 5.4. There are three types of events, differentiated by theposition of the send and receive time stamps relative to GVT equal to T23 Actually this definition includes all events scheduled at time T, not all of which may be in the pendingevent list at the same time during the sequential simulation.144 ADVANCED OPTIMISTIC TECHNIQUES 5.1 MEMORY UTILIZATION IN TIME WARP 145simulation timeFigure 5.5 Application demonstrating the difficulty of achieving storage optimality indistributed memory architectures.event E I at the LP where it currently resides, then schedule a new event E2 for thenext LP in the ring. Once E I has been processed, GVT can advance, and the storageused to hold E I can be reclaimed. This buffer can now be used to hold the new eventschedule by Eb so only two buffers are required.When the same Time Warp program is executed on a distributedmemoryplatform, however, memory buffers cannot be shared among processors. Thuseach processor must hold two memory buffers to implement this application, or atotal of2N buffers are needed where N is the number of processors in the system.24Thus the execution is not storage optimal, since it requires N times the number ofbuffers required in a sequential execution. In principal, this problem could becircumvented if logical processes are allowed to migrate between processorshowever, process migration is a relatively timeconsuming operation. Similarlyone could implement sharedmemory abstractions on top of a distributed memoryarchitecture however, the high cost of access to memory that physically resides onanother machine currently makes this approach not very attractive from a performance standpoint.Specific memory management protocols are described next. Four protocols areconsidered, each using a different mechanism to control memory utilization. TheCancelback protocol uses the message sendback mechanism. Artificial Rollback usesrollback. Pruneback uses state pruning, and MemoryBased Flow Control usesblocking. Among these, Cancelback and Artificial Rollback are storage optimalthey are designed for sharedmemory machines. Pruneback and memorybased flowcontrol are not storage optimal but are designed for distributedmemory machines.24 One could argue that in principal, each processor could release its memory buffers once it hascompleted processing the event, resulting in only two buffers being allocated in the entire system at anyinstant during the execution. This is not really a storage optimal execution, however, because the parallelcomputer must physically have memory for 2N buffers, even if only two are being used at one time.Other optimistic synchronization protocols are described later in this chapter thatprovide mechanisms to limit the amount of optimism in the Time Warp execution.Although the principal motivation behind these protocols is reducing the amount ofrolledback computation, these protocols indirectly limit memory consumption.5.1.4 CancelbackCancelback uses a global pool of memory to hold buffers that are not in use. Allmemory is allocated from this pool, and memory that has been released, such as viafossil collection or annihilation, is returned to this pool. After each memoryallocation, the processor checks if additional memory or some minimal amountis available. If it is not, the fossil collection procedure is called to reclaim memory. Iffossil collection fails to allocate additional memory, the processor uses the messagesendback protocol to reclaim memory.Any message with a send time stamp greater than GVT is eligible to be sent back.Because message sendback may roll back the sender LP to the send time stamp ofthe returned message, the message containing the largest send time stamp in thesimulation is a good candidate for being sent back. This will tend to roll back LPsthat have advanced ahead of the others. If no buffers with send time stamp greaterthan GVT are found, then there are no future events in the system indicating allevents in the Time Warp execution would also be required in a sequential execution,so the program is aborted due to lack of memory.An example illustrating the Cancelback protocol is depicted in Figure 5.6. Figure5.6a shows a snapshot of the execution when Cancelback is called. The memorybuffer containing event Ee contains the largest send time stamp, so Cancelbackselects this event for storage reclamation. As depicted in Figure 5.6b, this event issent back to LPA causing event Es the event that scheduled Ed to be rolled back.Each Cancelback call is a relatively timeconsuming operation because it includesa GVT computation, fossil collection, and a procedure for locating the eventcontaining the largest time stamp as well as the sendback mechanism itself. Thiscost can be reduced by performing message sendback on several messages in eachCancelback call. Operationally this can be done by applying the message sendbackmechanism to several events, until at least some minimal amount of memory hasbeen reclaimed. This minimal amount of memory that must be reclaimed is calledthe salvage parameter.5.1.5 Artificial RollbackThe artificial rollback protocol is somewhat similar to Cancelback, except it uses arollback mechanism to reclaim memory rather than sendback. Like Cancelback,artificial rollback also uses a salvage parameter that indicates the minimum amountof storage that should be reclaimed when this mechanism is invoked. If an LP runsout of memory and nothing can be reclaimed via fossil collection, the LP with thelargest simulation time clock value is rolled back to the simulation time of the LP146 ADVANCED OPTIMISTIC TECHNIQUES 5.1 MEMORY UTILIZATION IN TIME WARP 147ILPc .....ILPBLPAGVTasimulation time processed evento unprocessed eventLPcLPBI..... .IGYTa processed eventD unprocessed eventsimulation timeECl........ .simulation timebGYT1..... ....... 1 ..................................................................  processed eventI Eel D unprocessed eventt  8deleted eventI Be3ILPcLPBLPA processed eventsimulation timebEe...... .....,I,.. .. .. . T, 0 unprocessed event................ 8deleted eventEsGVTI..... .ILPBLPcFigure 5.6 Cancelback example. a Snapshot of the simulation before Cancelback call bsnapshot after Cancelback operation event Ec is sent back, causing event Es to be rolledback.Figure 5.7 Artificial Rollback example. a Snapshot of system before invoking ArtificialRollback b snapshot after the protocol rolls back events ER1 and ER2 5.1.6 Prunebackwith the second largest clock value. If insufficient memory is reclaimed, then the twoLPs mentioned above are both rolled back to the simulation time of the LP with thethird largest clock. This procedure continues until enough memory has beenreclaimed, or until no additional buffers are eligible for recovery via rollback.Figure 5.7 illustrates an invocation of the Artificial Rollback protocol for thesame situation as in Fig. 5.6. As shown in Figure 5.7a, LPA is the furthest ahead inthe simulation, so it is rolled back to the current simulation time of LPc the secondmost advanced LP. This results in the rollback of events ER1 and ER2  Rolling backthese events results in the cancellation, and storage reclamation of events ECI  Ec2 and Ec3 .Like Cancelback, Artificial Rollback is also storage optimal when implementedon a sharedmemory multiprocessor with a global buffer pool. It is somewhatsimpler to implement than Cancelback because there are no reverse transit messages.Thus there is no possibility of race conditions such as a positive message in reversetransit passing by an antimessage in forward transit and missing each other.As discussed earlier, the use of global, shared memory allows Cancelback andArtificial Rollback to have the storage optimality property. Variations on theseprotocols can be defined that do not use a global memory pool for example, eachprocessor or even each LP can have a separate memory pool but at the cost oflosingthe storage optimality property.The Pruneback protocol does not use rollbacks to reclaim memory. Instead, it usesthe pruning mechanism described earlier where uncommitted state vectors arereclaimed. Pruneback was designed for distributedmemory architectures wherethere is a pool of free memory in each processor that can be used by LPs mapped tothat processor. When a processor exhausts its supply of memory, and no additionalmemory can be obtained via fossil collection, the memory used for uncommittedstate vectors created via the copy state saving mechanism is reclaimed.The specific state vectors that are reclaimed is arbitrary, with the constraint that atleast one copy of the state vector older than GVT must remain as well as the eventsin the input queue with a time stamp larger than the state vector, even if the eventtime stamp is less than GVT in case a rollback to GVT later occurs. The currentstate of the logical process which mayor may not reside in the state queue,depending on the implementation also cannot be reclaimed by the pruningmechanism. From a correctness standpoint, any state vector can be reclaimed. Thechoice of which state vectors to reclaim will affect performance, however. Astraightforward approach might be to reclaim every kth state vector, mimickingthe strategy used in infrequent state saving. Another approach might be to try toreclaim states with small time stamp first, operating under the assumption thatrollbacks to the far past are less likely than rollbacks to the near past.The pruning mechanism has the advantage that unlike rollback, no usercomputations are erased that have to be later repeated. Also, unlike Cancelbackand Artificial Rollback, memory reclamation is performed local to the processorrunning out of memory, avoiding the need for expensive interprocessor communications and synchronizations. The cost associated with pruning state vectors is thesame as with infrequent state saving. If a rollback occurs, one may have to roll backfurther than is really necessary to go back to the last saved state vector, andrecompute forward referred to as coast forward in Chapter 4 to obtain the desiredstate. The other disadvantage of Pruneback relative to Cancelback and ArtificialRollback is that Pruneback can only reclaim state vectors it does not attempt toreclaim storage used by events or antimessages.149re fossilon5.1 MEMORY UTILIZATION IN TIME WARPe departubuffer collecti. buffers in usefree bufferpoolarrival messagsend allocatesADVANCED OPTIMISTIC TECHNIQUES148Figure 5.8 Model for estimating memory requirements for sequential simulation.5.1.7 MemoryBased Flow ControlThe memory management protocols discussed thus far were all based on a recoverymechanism that takes some action to reclaim memory if memory resources becomeexhausted. The final protocol described here uses a blocking mechanism thatattempts to prevent processors from running out of memory. Like the Prunebackprotocol, this protocol is well suited for distributedmemory architectures, and it isnot storage optimal. The basic idea behind this protocol is that the Time Warpsystem predicts the amount of memory the program will require until the next fossilcollection and then only provides this amount of memory to the program. Thisestimate is computed by the Time Warp system, transparent to the application, bymonitoring the programs execution, and it is updated throughout the execution sothat the system can adapt to changes in program behavior. If a logical processrequests memory and none is available, it blocks until additional memory is freed viarollbacks, message cancellations, or fossil collection.The projected amount of memory that will be required is computed by 1estimating the instantaneous amount of memory required by a sequential executionof the program, 2 inflating this amount to determine the amount that will berequired until the next fossil collection, and 3 inflating the amount computed in 2further to allow for optimistic event processing. To estimate l, the allocation ofmessage buffers during a sequential execution is modeled as a queueing networksee Fig. 5.8. Each buffer is viewed as a job that arrives at a server when it isallocated from the free pool. The buffer remains in service until the storage it uses isreclaimed, at which time it departs from the server and is returned to the free pool.The average number of jobs in the server buffers in use at one time indicates theamount of memory required in a sequential execution.In general, the average number of jobs in a server can be computed if one knowsthe rate at which buffers arrive, and the average amount of time each job remains inthe server. Here, these quantities are computed per unit simulation time. Specifically,let1. A denote the average rate that buffers are allocated jobs arrive at the server.Operationally A can be computed as AIT, where A is the number of messagesends occurring over a period of T units of simulation time.2. II J.l denote the amount of simulation time that the buffer remains in use this issimply the average difference between the receive and send times of eachevent, and it can be computed as LIA, where L is the sum of the differences insend and receive time, computed over A message sends.Littles Law is a wellknown result from queueing theory. It says that the averagenumber ofjobs that will be in service at one time is AIJ.l. Intuitively Ajobs arrive perunit simulation time, so over II J.l units of simulation time, AIJ.l new jobs arrive,displacing the AIJ.l jobs in the server at the start of the observation period. Thus theaverage number of buffers required in the sequential execution is LIT. Oneadditional modification is required to estimate the amount of sequential memoryduring the execution of a Time Warp simulation If an event is canceled by sendingan antimessage, this is erased from the measured statistics by assigning the antimessage a negative time stamp increment and negative count.To provide enough memory between successive fossil collections, this memorysize is increased by Art buffers, where T is the expected increment in GVT at thenext fossil collection measured from previous fossil collections. The resultingnumber of buffers, LIT  Art is multiplied by a scaling factor m greater than 1.0that is constantly varied during the execution m is increased decreased from onefossil collection to the next so long as it increases performance.5.1.8 Trading Off Performance and MemoryThe memory management protocols discussed thus far provide a means to executeTime Warp programs with limited amounts of memory. It is clear that if only aminimal amount of memory is provided, performance will be poor. This sectionaddresses the question of how performance varies as the amount of memoryprovided to the Time Warp program varies.Consider the Cancelback and Artificial Rollback protocols. Intuitively, as thememory is increased beyond that required for sequential execution, one wouldexpect performance to increase. As the amount of memory is increased further,performance may actually decrease, particularly for poorly balanced workloads for150 ADVANCED OPTIMISTIC TECHNIQUES 5.2 PERFORMANCE HAZARDS IN TIME WARP 1515.2 PERFORMANCE HAZARDS IN TIME WARP5.2.1 Chasing Down Incorrect ComputationsAt any instant in the execution of Time Warp there will usually be incorrectcomputations that will be later rolled back and intermixed with correct ones whichwill eventually be committed. It is important to realize that while the rollbackantimessage mechanism is canceling incorrect computations, this incorrect computationis spreading throughout the simulation system. It is essential that the Time Warpsystem be able to cancel erroneous computations more rapidly than they propagatethroughout the system. Stated another way, if the disease spreads faster than the cure,the patient diesA scenario illustrating this phenomenon is shown in Figure 5.10. This scenarioconsists of three logical processes, LPA LPB , and LPc, each mapped to a differentprocessor. Initially an incorrect computation in LPc generated an erroneous event inLPB , which in tum generated an erroneous event in LPA  see Figure 5.10a. Theevent in LPc is about to be canceled by an antimessage. Figure 5.10b shows thestate of the system after the cancellation in LPc has occurred, which has alsogenerated a new antimessage which is sent to LPB . While this is going on, LPAschedules a new, erroneous event for LPc. This set of actions cancellation and sendantimessage, and propagate the wrong computation to another processor isrepeated again and again, yielding the snapshots shown in Figure 5.10e and d.As can be seen, the incorrect computation remains a step ahead of the cancellationoperations and is never completely canceled, much like a dog chasing its own tail.Time Warp is more likely to encounter situations such as this when there is littlecomputation required to process incorrect events, and the amount of parallelism inthe application is less than the number of processors. Having little computation perevent hastens the spread of incorrect computations. If the degree of parallelism isdetails of the application program, so better or worse performance could occur forother applications.The previous section focused on mechanisms and policies to ensure that Time Warpmakes effective usage of memory resources. Let us now tum our attention toensuring effective utilization of the Cpu. As was pointed out earlier in this chapter,Time Warp may produce inefficient executions due to long rollbacks if noconstraints are placed on how far some processes can advance ahead of others.Severe performance degradation can result in other situations as well.This section focuses on specific scenarios that can lead to very poor performancein Time Warp executions, and countermeasures that should be taken in implementing Time Warp to minimize the probability that they occur. The next sectiondiscusses a variety of other optimistic synchronization protocols that have beendeveloped to circumvent these potential problems.experimentai 12processorsanalytical 12processors......experimental 8processorsanalytical 8processors......experimental 4processorsanalytical 4processors..bJ.JrI321oo9870. 665Ql4CJexample, the program shown in Fig. 5.1 where some processors advance morerapidly through simulation time than others. This is because limiting the amount ofmemory provides a flow control mechanism that avoids overly optimistic execution,which can lead to long rollbacks and poor performance.Figure 5.9 shows the performance of Time Warp using Cancelback as the amountof memory provided to the execution is varied. Similar results could be expectedusing the Artificial Rollback protocol. These data use a synthetic workloadconsisting of a fixed number of jobs moving among the different logical processes.The sequential execution requires as many buffers as there are jobs, since each jobalways has exactly one event in the pending event list. Performance data are shownfor the cases of four, eight, and twelve processors as the number of buffers beyondsequential execution is varied. Each processor contains one LP. The number ofbuffers needed in the sequential execution are 128, 256, and 384, or 32 buffers perlogical process. Two curves are plotted for a given number of processors. One showsperformance predictions computed from an analytic model for Cancelback. Thesecond shows experiments performed on the Georgia Tech Time Warp GTWsimulation executive, with Cancelback, described later in this chapter. Theseexperiments were performed on a Kendall Square KSR1 multiprocessor. In thisimplementation each memory buffer includes information concerning a single event,a state vector holding copy state saved variables state saving is performed prior tothe processing of each event, and information concerning the messages that weresent while processing the event.It is seen that performance rises rapidly as the number of memory buffers isincreased beyond the minimum. A welldefined knee is formed, beyond whichperformance improves only slightly or not at all. This program does not experiencepoor performance for very large amounts of memory because of the balanced natureof the synthetic workload. Overall, for these experiments, if the Time Warp programprovides from 25 to 75 additional buffers beyond that required for sequentialexecution, performance is about the same as if unlimited memory were provided.The reader should be cautioned, however, that these results are heavily dependent on50 100 150 200 250 300 350Number of Message Buffers BeyondMinimumFigure 5.9 Performance of Time Warp using Cancelback as the amount of memory isvaried.152 ADVANCED OPTIMISTIC TECHNIQUES 5.2 PERFORMANCE HAZARDS IN TIME WARP 153simulation timeStep 1 and the incremental state restoration portion of step 2 suggest that therollback overhead is proportional to the number ofevents being rolled back. Supposethat rolling back a computation T units of simulated time takes twice as long asforward progress by the same amount. Consider the case of two logical processesLPA and LPB that are mapped to different processors, and LPA is 10 units ofsimulation time ahead of LPB  see Figure 5.11a. LPB sends a message to LPAcausing the latter to roll back 10 units of simulation time. While LPA is rolling back10 units of time, LPB advances forward 20 units oftime according to our assumptionthat rollback is more timeconsuming than forward progress see Figure 5.l1b.Later, LPA sends a message back to LPB , causing LPB to roll back 20 units insimulation time. While LPB is rolling back, LPA advances forward 40 units of time,leading to a rollback of this length see Figure 5.l1c. One can see that this is an2. The state of the logical process must be restored this requires copying a statevector from the copy state save queue, and undoing all modifications to statevariables stored in the incremental state saving log.3. Pointers within the input queue must be updated details depend on thespecific data structure used to implement this queue.simulation timebXanceled evento antimessageaLPBLPc5.2.2 Rollback Echoes1. Antimessages must be sent for all of the events that are to be canceled.If the time to perform a rollback is high, very poor performance may result. Rollbackrequires the following computations to be performed04 rollback. messagesimulation timecba........LPB T ..LPA .I I I I I Io 10 20 30 40 50simulation time ImmA .I I I I I Io 10 20 30 40 50simulation timeLPB t   .LP t ..A rnj I I I I o 10 20 30 40 50Figure 5.11 Echo example. a LPB sends a message to LPA, causing LPA to roll back. bWhile LPA rolls back, LPB advances forward. c Sequence repeats, with the length ofrollback expanding at an exponential rate.dsimulation timesimulation timeclow, there will be processors available to spread the erroneous computation as soonas they arrive.This scenario suggests that antimessages must be able to rapidly outrun theincorrect computations that they are intended to cancel. Canceling a message andsending an antimessage must consume less time than processing an event andscheduling a new event in order to avoid the scenario depicted in Figure 5.10.Further, antimessages should be given higher priority than positive messages. Analternative approach is to always give highest priority to the computation associatedwith the smallest time stamp, whether processing positive messages or performingannihilation and rollback.Figure 5.10 Scenario illustrating the dog chasing its tail effect in Time Warp. a Initialsnapshot of system b snapshot after message cancellation and propagation of incorrectcomputation to one more processor c and d successive snapshots showing the cancellationwave failing to catch up to the spread of the incorrect computation.154 ADVANCED OPTIMISTIC TECHNIQUES 5.3 OTHER OPTIMISTIC SYNCHRONIZATION ALGORITHMS 155unstable situation where the length of each rollback and thus the time spent rollingback logical processes is doubling with each round in this scenario. The net rate ofprogress in simulation time as measured by GVT advances decreases as thesimulation proceeds This scenario is referred to as an echo in the parallelsimulation literature.Fortunately, under normal circumstances, rolling back T units of simulation timeshould not be more timeconsuming than forward progress by the same amount.Sending a positive message during forward execution should be less timeconsumingthan sending an antimessage during rollback. The reason is that sending a positivemessage requires allocation of a memory buffer, specifying its contents, and creatingthe antimessage copy, which are steps not required during rollback. Logging statechanges during incremental state saving will similarly be about as timeconsuming,if not less, than restoring state after a rollback. Forward execution also entails othercomputations, such as selecting the event to be processed next, copy state saving foreach event, processing incoming positive and negative messages, and the simulationcomputation itself.On the other hand, infrequent state saving does increase the cost of rollback. Thisis because when infrequent state saving is used, one may have to roll back furtherthan is strictly necessary to go back to the last saved state of the process, and thencompute forward coast forward to regenerate the desired state. The coastforwardstep must be included as part of the rollback cost because forward computationbeyond the simulation time of the causality error cannot proceed until coast forwardis completed. Thus a disadvantage of the infrequent statesaving technique is that itpushes the Time Warp program closer to unstable scenarios such as that described inFigure 5.1. In general, techniques that increase the cost of rollback must be carefullyweighed against the benefit that will be gained. Moreover this discussion highlightsthe fact that implementation overheads can playa key factor in determining theperformance of a parallel simulation system based on the Time Warp protocol.5.3 OTHER OPTIMISTIC SYNCHRONIZATION ALGORITHMSA number of other optimistic synchronization algorithms have been proposed sinceTime Warp first appeared. They were motivated to a large extent by the potentialperformance hazards discussed throughout this chapter. Thus a common themeamong these protocols is a policy to avoid excessive rollbacks and logical processesexecuting too far ahead of others in simulation time. Most algorithms provideparameters to tune the algorithm in order to control the behavior of the algorithmand to maximize performance.Like the discussion of memory management mechanisms, each optimisticsynchronization protocol can be characterized by the mechanism used to controlthe execution, and the policy or the rules governing when the mechanisms isapplied. Control mechanisms that have been widely explored include the following1. Blocking. The progress of one or more logical processes is blocked in order toavoid inappropriate optimistic execution.2. Rollback. Inappropriate optimistic execution is controlled by selectively rolling back one or more logical processes.Three of the four memory management protocols discussed earlier in this chapterare examples of synchronization protocols that can be used to control optimisticexecution. The memorybased flow control mechanism uses a blocking mechanismmemory utilization is used as the metric to control blocking. Cancelback andArtificial Rollback use rollback. The fourth protocol, Pruneback, does not provide amechanism for directly controlling optimistic execution.A second, orthogonal axis by which protocols can be classified is according towhether the control policy is static or adaptive. Static control mechanisms set controlparameters for example, the amount of memory for the memorybased controlmechanisms at the beginning of the execution and do not vary these parametersduring the course of the execution. Adaptive protocols monitor the execution of theprogram and attempt to adaptively change the control parameters during theexecution. The memorybased flow control mechanism described earlier is anexample of an adaptive control mechanism.5.3.1 Moving Time WindowA simple approach to control optimistic execution is to set a bound on how far onelogical process can advance ahead of others in simulation time. LPs that reach thisbound are forced to block until the LPs lagging behind in simulation time haveadvanced. Operationally this can be accomplished by defining a window ofsimulation time extending from GVT to GVT  W, where W is the size of thetime window. Logical processes are not allowed to advance beyond GVT  W. Thistime window advances forward whenever GVT advances.The above idea is the essence of the Moving Time Window MTW parallelsimulation protocol. The original MTW protocol called for a static window size,specified by the modeler, that does not change during the execution of thesimulation. It is clear that adaptive versions of this algorithm are also possiblewhere the runtime system automatically monitors the execution and adjusts thewindow size to maximize performance.The central advantage of time windows is that they provide a simple, easy toimplement mechanism to avoid runaway LPs from advancing far ahead of others.The central disadvantage of this approach is the cost of maintaining the window,which requires either frequent GVT computations or an efficient mechanism forestimating GVT. Another disadvantage of this approach is that the window does notdistinguish correct computations from incorrect ones that is, incorrect computationswithin the window would still be allowed to execute, while correct ones beyond thewindow are not allowed to execute. Further it is not immediately clear how the sizeof the window should be set this is clearly dependent on the application, since the156 ADVANCED OPTIMISTIC TECHNIQUES 5.3 OTHER OPTIMISTIC SYNCHRONIZATION ALGORITHMS 157number of simulation events per unit of simulation time must be considered.Empirical data concerning this approach are mixed, with some studies indicatingsignificant benefit but others reporting modest gain.A variation on the time window approach is to define the window in terms of thenumber of processed, uncommitted events NPUE that may reside in a logicalprocess rather than on simulation time. In the Breathing Time Warp protocol, theuser must specify this NPUE parameter. An LP is blocked when the number ofprocessed events in that LP with time stamp larger than GVT reaches NPUE. The LPbecomes unblocked when GVT is advanced and some of these events becomecommitted.5.3.2 LookaheadBased BlockingThe windowbased approaches define blocking mechanisms that are not related todetailed dependencies between events in the simulation program. Another approachis to base blocking decisions on information obtained by applying a conservativesynchronization protocol. This inevitably implies utilizing lookahead information todetermine which events are safe to process. One can envision starting with aconservative synchronization protocol to determine which events are safe to process,and adding to it optimistic synchronization mechanisms such as rollback and antimessages to allow processing of events that cannot be guaranteed to be safe to beprocessed. This leads to hybrid conservativeoptimistic synchronization algorithms.One such algorithm is the filtered rollbacks approach. This is an extension to theconservative Bounded Lag synchronization algorithm discussed in Section 3.5.Lookahead is used to determine the minimum distance in simulation time betweenLPs. In the original Bounded Lag algorithm, the user provides a lower bound to theminimum amount of simulated time that must elapse for an event in one process toaffect another. Such minimum distances are derived based on application specificinformation such as the minimum time stamp increment encountered by an event asit passes through a logical process. The minimum distance between processes isused as a basis for deciding which events are safe to process. In filtered rollbacks thesimulator is allowed to violate this lower bound, possibly leading to violation of thelocal causality constraint. Such errors are corrected using a Time Warp like rollbackmechanism. By adjusting the distance values, one can vary the optimism of thealgorithm between the conservative bounded lag scheme and the optimistic movingtime window approach.5.3.3 Local RollbackOne can distinguish between two different types of rollback in Time Warp primaryrollbacks where a logical process received a straggler message in its past, andsecondary rollbacks caused by receiving an antimessage that cancels an event thathas already been processed. Simulation protocols that allow primary rollbacks aresometimes called aggressive protocols, while those that send messages that may laterbe canceled thus creating the possibility of secondary rollbacks are said to allowrisk. Time Warp is aggressive and allows risk. It is the combination of these twodifferent types of rollback that causes the cascaded rollback effect where rollbackscan be propagated among many processors. The dog chasing its tail effectdescribed earlier is one possible consequence of the cascaded rollback effect.Cascaded rollbacks can be eliminated without completely discarding the benefitsof optimistic synchronization by allowing primary rollbacks but not secondaryrollbacks. This approach gives rise to aggressive, norisk ANR protocols. It isaccomplished by not allowing messages to be sent until it can be guaranteed thatthey will not be rolled back. Operationally this means that when a logical processsends a message, the processor does not immediately send it but rather stores themessage in a buffer. The message is not sent until GVT advances beyond the sendtime stamp of the message, thereby guaranteeing that the message will not be latercanceled. Because rollbacks do not propagate to other processors, this mechanism issometimes referred to as a local rollback mechanism. The local rollback mechanismavoids the need for antimessages, but statesaving mechanisms are still required toallow recovery from rollbacks due to straggler messages.5.3.4 Breathing Time BucketsThe Breathing Time Buckets BTB algorithm uses a combination of synchronousevent processing with barriers like that in Filtered Rollbacks, and its predecessor,Bounded Lag, time windows, and local rollback. The central idea in BTB is tocompute a quantity called the event horizon that determines the size of the timewindow.BTB can be viewed as an extension of the sequential event list mechanism thatallows optimistic parallel processing in this sense, it is an optimistic version of thesynchronous conservative protocol discussed in Section 3.5.5. The key idea isdetermining the minimum time stamp among events that will be produced in thefuture. In the conservative protocol, lookahead is used to determine the minimumtime stamp of future events. In BTB one optimistically processes events to computethis quantity.Consider a sequential simulation that has advanced to simulation time T. Therewill be some number of events in the pending event list, all with time stamp of atleast T. Let Hr denote the smallest time stamp of any new event that will begenerated after simulation time T. HT is referred to as the event horizon. If theparallel simulation has advanced to time T i.e., GVT is T, all events in the pendingevent list with time stamp less than HT can be processed without the possibility ofbeing rolled back.The BTB algorithm operates in cycles, where in each cycle, the parallelsimulation will perform as follows assume that GVT is at simulation time T atthe beginning ofthe cycle1. Optimistically process events in each processor in time stamp order. Duringthis phase local rollback is used so that new messages generated as the result158 ADVANCED OPTIMISTIC TECHNIQUES 5.3 OTHER OPTIMISTIC SYNCHRONIZATION ALGORITHMS 159I I .. l.I Cl OX C2 .. f .igIlO ....o.....J ....0 ..0....IJ..ODOIof processing an event are not sent but rather kept local in the sender in amessage buffer. Also, as each processor optimistically processes events, itkeeps track of the minimum receive time stamp among new events which werescheduled during this cycle. This quantity HiT is the local minimum inprocessor i it represents the event horizon in processor i based only on localinformation. Each processor will only process events so long as their timestamp is less than the current value ofHT recall that events are processed intime stamp order.2. Compute the global event horizon. HT is computed as the minimum ofH,T over all of the processors.3. Send messages. All messages with a send time stamp less than HT can nowbe sent. It can be guaranteed that these messages will not later be rolled back.4. Advance simulation time. GVT is advanced to HT.LPcLPBLPcLPBTasimulation timeo unprocessed event processed, uncommitted event processed, committed event5.3.5 Wolf CallsFigure 5.12 Example illustrating the execution of the Breathing Time Buckets algorithm.a Initial set ofunprocessed event b result of simulation phase and computation of the eventhorizon c messages generated during the simulation phase are sent, and simulation time isadvanced.The dog chasing its tail effect discussed earlier can be avoided by blockingprocessors when a rollback is detected. In the Wolf Calls protocol, when a processordetects that it has received a straggler message, it broadcasts a special controlmessage that causes the processors to stop advancing until the error has been erasedia timessages and rollbacks. The special control messages warning ofpOSSIble danger are referred to as wolf calls, in reference to the wellknownfable.Broadcasting has the disadvantage that it may unnecessarily block some LPs.This can be alleviated if the LP detecting the error can determine the set ofprocessors to which e error may have spread. In this case the control messageseed only be snt to thIS set ofprocessors. This requires an upper bound on the speedIII wallc10ck tIme that an erroneous computation may spread, as well as an upperbound on the latency of the control messages. Such bounds may be difficult to definesimulation timeHTbTcT simulation timeI.......................................... .I... f ...y ..I ....  ..9oLPcLPALPBThe above steps are repeated until the simulation terminates.An example illustrating the execution of BTB is shown in Figure 5.12. Here,there are three logical processes LPA LPB, and LPc, and each is mapped to adifferent processor. Figure 5.12a shows a snapshot of the system at simulation timeT with the boxes representing unprocessed events within each LP that weregenerated in previous cycles. Each LP processes its events in time stamp order solong as the time stamp of the event is less than the minimum time stamp of any newevent produced in this cycle. For example, as shown in Figure 5.12b, LPAprocesses events AI, A2, and A3, and A2 schedules a new event Y. It does notprocess A4 because A4 has a time stamp larger than Y. Similarly LPB processes itstwo events BI and B2, and then it stops because there are no more events for it toprocess. LPc processes CI but does not process C2 because CI produced a newevent X with a smaller time stamp than C2. The local event horizon for LPA iscomputed as Ty, the time stamp of event Y, while that of LPc is Tx. The local eventhorizon of LPB is 00 because it did not schedule any new events.After the optimistic eventprocessing phase is complete, the global event horizonis computed as the minimum of the three local event horizons, or Tx  see Figure5.12b. Like the GVT algorithms discussed in Chapter 4, some care must be takenhere to ensure that there are no transient messages in the system. Simulation timei.e., GVT is advanced to HT, and the messages generated by the optimisticprocessing are sent to their destination processor. The events AI, A2, BI, B2, andCI are now committed, and memory used by these events log information can nowbe reclaimed via the fossil collection mechanism. In this example, A3 was processedbut not committed one could roll back this event and reprocess it in the next cycle,or preserve this computation. In this example, the message X is sent to LPA withtime stamp less than that of A3, so A3 must be rolled back and reprocessed in thenext cycle. Had A3 scheduled a new event, the message for this new event would nothave been sent to the destination processor because A3 had not been committed, sothe erroneous event can be canceled locally. The final state of the simulation at theend of this cycle is shown in Figure 5.12c.160 ADVANCED OPTiMISTIC TECHNIQUES 5.4 PUTTING iT ALL TOGETHER GEORGIA TECH TIME WARP GTW 161on multiprocessor systems, however, and even with this optimization, the set ofprocessors that might be affected by an error may be significantly larger than the setthat actually is affected by the error.5.3.6 Probabilistic RollbacksBesides rollbacks necessary to ensure correctness of the simulation, rollbacks maybe added to prevent overly optimistic execution. In the probabilistic rollbackapproach each LP periodically draws a binary random number with probability pof coming up heads and probability I  p of coming up tails. If the result is heads,the LP is rolled back to GVT. If it is tails, no such operation is performed. Thefrequency and probability p are userdefined tuning parameters.5.3.7 SpaceTime SimulationSpacetime simulation is an approach based on relaxation techniques similar to thoseused in continuous simulation problems. The goal of a discrete event simulationprogram is to compute the values of state variables across simulated time. Forexample, the simulation of a server in a queueing network simulation can be viewedas determining the number ofjobs that exist in the server at every point in simulatedtime.As was described in Chapter 2, the simulation can be viewed as a twodimensional spacetime graph where one dimension enumerates the state variablesused in the simulation, and the second dimension is simulated time. The simulatormust fill in the spacetime graph, that is, determine the value of the simulators statevariables over simulated time in order to characterize the behavior of the physicalsystem. In spacetime simulation this graph is partitioned into regions, with oneprocess assigned to each region. This process is responsible for filling in the portionof the spacetime graph that is assigned to it. In order to accomplish this task, theprocess must be aware of boundary conditions for its region, and update them inaccordance with activities in its own region. Changes to boundary conditions arepassed among processes in the form of messages. Thus each process repeatedlycomputes its portion of the spacetime graph, transmits changes in the boundaryconditions to processes responsible for neighboring regions, and then waits for newmessages indicating further changes to the boundary conditions. The computationproceeds until no more changes occur, that is, until the computation converges to afixed point. Time Warp can be viewed as a special case of spacetime simulationwhere the spacetime region is defined as rectangles, one per Lp, that span all ofsimulation time for the set of state variables mapped to that LP.5.3.8 SummaryBuilding on the basic mechanisms defined for Time Warp, numerous synchronization protocols have been defined to circumvent the pitfalls that may occur with overoptimistic execution. Most provide control parameters that must be set by the user totune performance. Some provide mechanisms to have the Time Warp systemautomatically set these parameters and, in some cases, change these parametersduring execution to optimize performance. Space does not permit elaboration of allof the protocols that have been proposed. Pointers to additional information on thissubject are provided at the end of this chapter.At present, conventional wisdom within the parallel simulation community isthat1. Pure Time Warp systems with no flow control mechanisms are not wellsuited as a general purpose parallel simulation executive because of thepotential for extremely poor performance in certain situations.2. The problem of overoptimistic execution in Time Warp is solvable. A numberofprotocols have been developed and have been demonstrated to achieve goodperformance.3. There is no one protocol that is clearly better than all others in most cases thatarise in practice. For any given simulation application containing an adequateamount of intrinsic parallelism, several approaches may be used that will beeffective in controlling optimistic execution.5.4 PUTTING IT ALL TOGETHER GEORGIA TECH TIME WARP GTWWe conclude this chapter by examining a particular Time Warp executive calledGeorgia Tech Time Warp GTW. GTW is a parallel simulation executive designedto execute on networked uniprocessor and multiprocessor machines. Here, wespecifically discuss the version that executes on sharedmemory multiprocessors.GTW includes a variety of algorithms and techniques designed specifically for thisclass of parallel computers that have not been discussed thus far.5.4.1 Programmers InterfaceThe GTW executive was designed to provide a modest set of basic simulationprimitives, while allowing more sophisticated mechanisms to be implemented aslibrary routines. For example, GTW supports an eventoriented world view.Mechanisms for more complex world views such as processoriented simulationmust be built on top of the GTW executive.A GTW program consists of a collection of logical processes that communicateby exchanging timestamped messages. The execution of each LP is entirelymessage driven that is, any execution of application code is a direct result ofreceiving a message. LPs cannot spontaneously begin new computations withoutfirst receiving a message. Each LP has three procedures associated with it The IProcprocedure is called at the beginning of the simulation to initialize the LP andgenerate the initial messages, the Proc procedure also called the event handler iscalled to process each event received by the LP, and an optional FProc procedure is162 ADVANCED OPTIMISTIC TECHNIQUES 5.4 PUTTING IT ALL TOGETHER GEORGIA TECH TIME WARP GTW 163called at the end of the simulation, typically to output application specific statistics.These procedures, and the routines that they call, completely specify the behavior ofthe LP. Each LP is identified by a unique integer m.In addition the user must provide a procedure for global initialization of thesimulation. This procedure is passed command line arguments and must specify thenumber of logical processes, the IProc, Proc, and FProc procedures for each Lp, andthe mapping of LPs to processors.LPs may define four different types of state1. A state that is automatically checkpointed by the GTW executive.2. A state that is incrementally checkpointed using GTW directives invoked bythe application.3. Local sometimes called automatic variables defined within the IProc, Proc,and FProc procedures.4. Global variables that are not checkpointed.The fourth category is intended to hold data structures that are not modified duringthe simulation. The state vector of each LP is an arbitrary data structure definedwithin the application program.During initialization typically in the IProc procedure, the application programmust specify the memory locations that are automatically checkpointed. A copy ofthe LPs automatically checkpointed state is made prior to each invocation of itsevent handler, transparent to the application. Incrementally checkpointed variablesmust be individually copied through explicit calls to GTW primitives. A variableneeds only to be checkpointed once in each event, but it must be checkpointed priorto any modification of the variable within the event. Any state that is dynamicallyallocated after the initialization phase of the simulation must be incrementallycheckpointed. Incremental checkpointing by overloading the assignment operator inC is also provided in a layer above the GTW executive.Two procedures are provided for message passing. The TWGetMsg procedureallocates a message buffer by storing a pointer to the buffer in a GTWdefinedvariable. The TWSend procedure sends the message.5.4.2 IO and Dynamic Memory AllocationApplication programs may also schedule events that will not be processed until GVTexceeds the time stamp of the event, guaranteeing that the computation will not belater rolled back. This allows application programs to perform irrevocable operationssuch as IO. Such events are referred to as IO events, although event handlers forIO events may perform arbitrary computations and do not need to actually performany IO operations. A different event handler may be associated with each IOevent. An LP may schedule an IO event for itself or for another LP.The GTW executive provides two types ofIO events. Blocking IO events do notallow optimistic execution ofthe LP beyond the time stamp of the IO event. This isintended for IO operations requiring input from external sources. In this case the LPcannot compute forward until it receives this input, so it is better to simply block theLP rather than optimistically execute it beyond the time of the IO event. The LP istemporarily blocked once a blocking IO event becomes its smallest timestamped,unprocessed event. The LP remains blocked until it is either rolledback the LP willagain block once the rolledback events are reprocessed, if the IO event has notbeen canceled, or until GVT advances to the time stamp of the blocking IO event.Once the event handler for the IO event is called, the LP resumes normal optimisticexecution. The event handler for blocking IO events can access the LPs state vector.Nonblocking IO events do not temporarily block LPs as described above. Theevent handler for these events cannot access the state vector of the LP, since the LPwill typically have advanced beyond the time stamp of the IO event when the IOevent handler is called. All data needed by the IO event handler must be includedwithin the message for the event. Output operations will typically use nonblockingIO events.Mechanisms are provided to attach handlers, called rollback handlers, to eventsthat are executed when the event is rolled back or canceled. This, and IO events, areused to implement dynamic memory allocation. Specifically, dynamic memoryallocation in GTW must be performed by calling a GTW procedure calledTWmalloc . TWmalloc  calls malloc , the memory allocation procedure in the C programming language, and attaches a rollback handler to the eventthat is now being processed. The rollback handler contains a call to the free procedure to release the memory if the event is rolled back or canceled, avoidingmemory leaks as was discussed in Chapter 4. Memory must be released by callingthe TWfree  procedure which schedules a nonblocking IO event. The IOevent handler contains a call to free . This ensures that memory is not releaseduntil it is guaranteed the memory really is no longer needed. If the event callingTWfree  is rolled back or canceled, the IO event will be automaticallycanceled via GTWs rollback mechanism.5.4.3 GTW Data StructuresEach data structure in the GTWexecutive is said to be owned or reside on aspecific processor. In principle, no such specification is required because all memorycan be accessed by any processor in a sharedmemory system. However, the GTWdesign assigns each data structure to a unique owner in some cases the owner maychange during execution in order to ensure that synchronization for example,locks is not used where it is not needed and to maximize locality in memoryreferences. Synchronization and nonlocal memory references are typically muchmore timeconsuming than local memory references on most existing multiprocessorplatforms.Time Warp uses three distinct types of memory objects events stored in the inputqueue, antimessages stored in the output queue, and state history information storedin the state queue. In GTW these are, in effect, combined into a single object type,the event object. The event object includes a copy of the automatically checkpointed164 ADVANCED OPTIMISTIC TECHNIQUES 5.4 PUTIING IT ALL TOGETHER GEORGIA TECH TIME WARP GTW 165portion ofthe state of the LP, and pointers that are used to implement antimessages,as will be described momentarily. Each processor contains a single event queue datastructure that implements the functionality of the input, output, and state queues forthe logical processes mapped to that processor.The event queue data structure is shown in Figure 5.13. Each LP contains a list ofthe processed events for that LP. This list is sorted by receive time stamp and isimplemented using a linear list. Unprocessed events for all LPs mapped to the sameprocessor are stored in a single priority queue data structure. Using a single queuefor all LPs eliminates the need for a separate scheduling queue data structure toenumerate the executable LPs, and this allows both the selection of the next LP toexecute and location of the smallest time stamped unprocessed event in that LP to beimplemented with a single dequeue operation. The singlequeue approach reducesthe overhead associated with normal event processing, and as discussed later,greatly simplifies the GVT computation. A drawback with this approach is thatmigration of an LP to another processor by a dynamic load management mechanismis more difficult because the events for a specific LP are intermixed with events forother LPs in the priority queue holding unprocessed events, requiring one to extractthe events from this data structure. To circumvent this problem, GTW allows aprocessor to be configured with multiple priority queues, with each LP assigned toone queue. The set of LPs mapped to a single priority queue, referred to as a cluster,processor 1r.I d unprocessed events processe eventsII LP,III,IIIIprocessor 3j 1 processed events I II II I LP, , I, I, IFigure 5.13 Event queue data structure in GTWis the atomic unit that is migrated between processors by the dynamic loadmanagement software.In addition to the event queue shown in Figure 5.13, each processor maintainstwo additional queues to hold incoming messages from other processors. Themessage queue MsgQ holds incoming positive messages that are sent to an LPresiding on this processor. Messages are placed into this queue by the TWSendprimitive. The message cancellation queue CanQ is similar to the MsgQ exceptthat it holds messages that have been canceled. When a processor wishes to cancel amessage, it enqueues a pointer to the message being canceled into the CanQ of theprocessor to which the message was originally sent. Logically each messageenqueued in the CanQ can be viewed as an antimessage however, it is a pointerto the message itself rather than an explicit antimessage that is enqueued, as will bedescribed momentarily. Separate queues are used to hold messages generated byother processors as opposed to the processors directly manipulating the datastructures shown in Fig. 5.13 to avoid using timeconsuming synchronizationlocks to access the event queue.Each processor also maintains another priority queue called the IO queue whichholds IO events as well as some nonIO events for LPs mapped to that processor.IO events are scheduled in exactly the same way as ordinary events that is, they areenqueued in the unprocessed event priority qUEUe via the MsgO if the sender andreceiver are on different processors. This simplifies cancellation ofIO events. Justprior to calling an event handler, the GTWexecutive first checks to see if the event isan IO event. IO events are placed in the IO queue, and the call to the eventhandler is deferred until later. If the event is a blocking IO event, the LP is alsomarked as blocked. All events for blocked LPs, both IO and nonIO events, aresimilarly diverted to the IO queue when they are removed from the unprocessedevent queue. If a blocked LP is rolled back, it becomes unblocked, and the LPsevents in the IO queue are returned to the unprocessed event queue. The fossilcollection procedure processes IO events with time stamp less than or equal toGVT, and unblocks blocked LPs.5.4.4 Direct CancellationAs noted earlier, it is important that Time Warp systems provide a mechanism torapidly cancel incorrect computations and prevent the dog chasing its tail effectdescribed in Section 5.2.1. GTW uses a mechanism called direct cancellation tospeed the cancellation of incorrect computations. Rather than using copies ofmessages to implement antimessages, pointers are used. Whenever an eventcomputation schedules sends a new event, a pointer to the new event is leftbehind in the sending events data structure see Fig. 5.13. Because the new event isoften stored on a different processor than the event that scheduled it, this techniquerequires a pointer in a data structure in one processor to point to a data structure inanother processor. This is easily implemented on sharedmemory machine architectures where different processors have a common address space. Avariation on thisapproach is implemented in the distributedmemory version of GTW, where arrays166 ADVANCED OPTIMISTIC TECHNIQUES 5.4 PUTTING IT ALL TOGETHER GEORGIA TECH TIME WARP GTW 167of pointers to events and array indices are used to implement pointers acrossprocessors with distinct address spaces.To support direct cancellation, each event contains two pointer variables, calledCause and NextCause. The events scheduled while processing one event are storedas a linked list referred to as the causality list. The Cause field is a pointer to the firstevent in this list, and NextCause contains a pointer to the next event in the causalitylist. For example, in Figure 5.13 event E scheduled two events, E2 and E3 E2 hassince been processed but E3 has not. If E 1 is rolled back, E2 and E3 would becanceled by enqueueing them into the CanQ data structure in processors 2 and 3,respectively.The principal advantages of direct cancellation are that it eliminates the need forexplicit antimessages and an output queue, thereby economizing on storage, andmore important it eliminates the need to search queues to locate events that must becanceled. Direct cancellation can be used with either the lazy or aggressivecancellation policy.5.4.5 EventProcessing LoopAfter the simulator is initialized, each processor enters a loop that repeatedlyperforms the following stepsL All incoming messages are removed from the MsgQ data structure, and themessages are filed, one at a time, into the event queue data structure. If amessage has a time stamp smaller than the last event processed by the LP, theLP is rolled back.2. All incoming canceled messages are removed from the CanQ data structureand are processed one at a time. Storage used by canceled messages isreturned to the free memory pool. Rollbacks may also occur here, and they arehandled in essentially the same manner as rollbacks caused by stragglerpositive messages, as described above.3. A single unprocessed event is removed from the priority queue holdingunprocessed events for the processor and is processed by calling the LPsevent handler Proc procedure. A smallest time stamp first schedulingalgorithm is used that is, the unprocessed event containing the smallesttime stamp is selected as the next one to be processed.These steps continue until the simulation has been completed. The applicationspecifies the simulation time at which execution completes. Events that arescheduled with a time stamp larger than the end time are discarded. The simulationends where there are no unprocessed events remaining in the system.5.4.6 Buffer ManagementThe principal atomic unit of memory in the GTW executive is a buffer. Each buffercontains the storage for a single event, a copy of the automatically checkpointedstate, pointers for the direct cancellation mechanism and incremental state saving,and miscellaneous status flags and other information.The original implementation of the GTW software version 1.0 used receiverbased free pools. This means that the TWGe tMs g  routine allocates a free bufferfrom the processor receiving the message. The sender then writes the contents of themessage into the buffer and calls TWSend  to enqueue it in the receivingprocessors MsgQ. This approach suffers from two drawbacks. First, locks arerequired to synchronize accesses to the free pool, even if both the sender andreceiver LP are mapped to the same processor. This is because the processors freelist is shared among all processors that send messages to this processor. The seconddrawback is concerned with caching effects, as discussed next.In cachecoherent multiprocessor systems using invalidate protocols, receiverbased free pools do not make the most effective use of the cache. Buffers in the freepool for a processor will likely be resident in the cache for that processor, assumingthat the cache is sufficiently large. This is because in most cases, the buffer was lastaccessed by an event handler executing on that processor. Assume that the senderand receiver for the message reside on different processors. When the sendingprocessor allocates a buffer at the receiver and writes the message into the buffer, aseries of cache misses and invalidations occur as the buffer is moved to thesenders cache. Later, when the receiver dequeues the message buffer and executesthe receivers event handler, a second set of misses occurs, and the buffers contentsare again transferred back to the receivers cache. Thus two rounds of cache missesand invalidations occur with each message send.A better solution is to use senderbased free pools. The sending processorallocates a buffer from its local free pool, writes the message into it, and enqueuesit at the receiver. With this scheme, the free pool is local to each processor, so nolocks are required to control access to it. Also, when the sender allocates the bufferand writes the contents of the message into it, memory references will hit in thecache in the scenario described above. Thus only one round of cache misses andinterprocessor communications occurs when the receiving processor reads themessage buffer.The senderbased pool creates a new problem, however. Each message send, ineffect, transfers the ownership of the buffer from the sending to the receivingprocessor, since message buffers are always reclaimed by the receiver during fossilcollection or cancellation. Memory buffers accumulate in processors that receivemore messages than they send. This leads to an unbalanced distribution of buffers,with free buffer pools in some processors becoming depleted while others have anexcess. To address this problem, each processor is assigned a quota of Nbuf buffersthat it attempts to maintain. After fossil collection, the number of buffers residing inthe processor is checked. If this number exceeds Nbuf  the excess buffers aretransferred to a global free list. On the other hand, if the number of buffers fallsbelow Nbuf  A A is a userdefined parameter, additional buffers are allocated fromthe global pool. Counters associated with each event list allow determination of thenumber of buffers reclaimed on each fossil collection without scanning through thelist of reclaimed buffers.168 ADVANCED OPTIMISTIC TECHNIQUES 5.4 PUTIING IT ALL TOGETHER GEORGIA TECH TIME WARP GTW 1695.4.7 Flow ControlThe flow control mechanism based on adaptively controlling memory allocationdescribed in Section 5.1.7 is used. Specifically, the policy described there is used toestimate the memory requirements of each processor. If an LP attempts to schedule anew event while processing an event E, and no memory buffers are available to holdthe new event, then E is aborted rolled back, and returned to the list of unprocessedevents. The processor then returns to the main scheduling loop and, unless newevents were received, attempts to reprocess the event. This abort and retry cyclerepeats, creating a kind of busy wait loop until either a new event with a smallertime stamp is received or GVT advances and memory is reclaimed.5.4.8 GVT Computation and Fossil CollectionGTW uses onthefly fossil collection see Section 4.3.1 in order to reduce the timerequired to reclaim memory. Each processed event is threaded into both the free listand the processed event list for the LP after the event has been processed, and thetime stamp of the event is checked to make sure it is less than GVT before thememory buffer is reused. A GVT algorithm developed specifically for sharedmemory multiprocessors is used to compute GVT. This is described next.An asynchronous algorithm i.e., no barrier synchronizations is used that isinterleaved with normal event processing. The algorithm requires neither messageacknowledgments nor special GVT messages. All interprocessor communicationis realized using a global flag variable GVTFlag, an array to hold each processorslocal minimum, and a variable to hold the new GVT value.Any processor can initiate a GVT computation by writing the number ofprocessors in the system into GVTFlag. This flag is viewed as being set if itholds a nonzero value. A lock on this variable ensures that at most one processorinitiates a GVT computation.Let TGVT be the instant in wallclock time that GVTFlag is set. As before, GVT isdefined as a lower bound on the time stamp of all unprocessed or partially processedmessages and antimessages in the system at TGVT  Messages are accounted for byrequiring that 1 the sending processor is responsible for messages sent after TGVTand 2 the receiving processor is responsible for messages sent prior to TGVT  Toimplement 1, each processor maintains a local variable called SendMin thatcontains the minimum time stamp of any message sent after GVTFlag is set.GVTFlag is checked after each message or antimessage send, and SendMin isupdated if the flag is set. To implement 2, each processor checks GVTFlag at thebeginning of the main eventprocessing loop and notes whether the flag was set.Then, as part of the normal eventprocessing procedure, the processor receives andprocesses all messages antimessages in MsgQ CanQ and removes the smallesttimestamped event from the unprocessed event queue. If GVTFlag was set at thebeginning of the loop, the time stamp of this unprocessed event is a lower bound onthe time stamp of any event sent to this processor prior to TGVT  The processorcomputes the minimum of this time stamp and SendMin, and writes this value intoits entry of the global array. It then decrements GVTFlag to indicate that it hasreported its local minimum, and resumes normal event processing. The setGVTFlag is now ignored until the new GVT value is received.The last processor to compute its local minimum the processor that decrementsGVTFlag to zero computes the global minimum and writes this new GVT value intoa global variable. Each processor detects the new GVT and updates its local copy ofthis value.The overhead associated with this algorithm is minimal. When GVT is not beingcomputed, GVTFlag must be checked, but this overhead is small because the flag isnot being modified and will normally reside in each processors local cache. Nosynchronization is required. To compute GVT, the principal overheads are updatingGVTFlag and SendMin, and the global minimum computation performed by oneprocessor. Performance measurements indicate that even with frequent GVTcomputations for example, every millisecond the parallel simulator is only slightlyslower than when GVT computations are infrequent.5.4.9 Incremental State SavingIncremental state saving is implemented by defining an array or words for each LPinto which values of state variables are stored prior to modification by an event.Logically this array can be viewed as a single, large array, but it is actuallyimplemented as a linked list of fixed sized arrays. Each state save operation advancesa pointer into the array to the next location, and copies a word from the LPs state intothe array. When rollback occurs, the old contents of state variables are restored, oneafter the other, until the point of rollback is reached.5.4.10 Local Message SendsThe TWSend  routine first checks if the destination LP is mapped to the sameprocessor as the sender. If they are the same, TWSend  simply enqueues themessage in the unprocessed event queue, bypassing MsGQ and thus avoidingsynchronization overheads. Thus local message sends are no more timeconsumingthan scheduling an event in a sequential simulation.5.4.11 Message CopyingThe GTW executive performs no message copying, neither in sending nor receivingmessages. This allows efficient execution of applications using large messages.Software executing above the GTW executive must ensure that the contents of amessage are not modified after the message it sent and that the contents of receivedmessages are not modified by the event handler.5.4.12 Batch Event ProcessingThe scheduling loop always checks MsgQ and CanQ prior to processing each event.Rather than checking these queues before each event, an alternative approach is tocheck these queues prior to processing a batch of B events, thereby amortizing the170 ADVANCED OPTIMISTIC TECHNIQUES 5.5 SUMMARY 171overhead of each queue check over many events. If there are no B events available tobe processed, the queue is checked after processing those that are available.The batchprocessing approach reduces queue management overheads somewhat,but it may lead to more rolled back computation because, in effect, the arrival ofstraggler and antimessages is delayed. Thus it is clear that B should not be set to toolarge a value. Setting the batch size is left to user.5.4.13 Performance MeasurementsGTW has been successfully applied to speeding up simulations of asynchronoustransfer mode ATM networks, wireless networks, and commercial air trafficsystems. It is currently being used extensively to model commercial air trafficboth in the United States and around the world for development of future expansionsof air transportation services. This work involves use of a simulation model calledDPAT Detailed Policy Assessment Tool running on GTW that was developed byMitre Corp. At the time of this writing, work is in progress by Mitre to install GTWin the Federal Aviation Administrations air traffic control center as an online tool tomanage the air traffic space when new conditions develop in the air traffic space forexample, thunderstorms reducing capacity of certain airports.Figure 5.14 shows GTW performance in simulating a wireless personal communication services PCS network. This simulation models a wireless networkproviding communication services to mobile PCS subscribers. The service area ispartitioned into subareas or cells, with each cell containing a receivertransmitterand a fixed number of channels. When a portable cellular telephone moves fromone cell to another, it must stop using one transmitterreceiver, and begin usinganother. Thus a new radio frequency channel must be allocated in the new cell tomaintain the call. If no channels are available, the call is dropped. It is important thatthe network be engineered so that the number of dropped calls remains below acertain level, such as 1 of calls transmitted through the system.Execution Speed 795Kcommitted events per second28K 60K2 4ProcessorsFigure 5.14 GTW performance in simulating a wireless personal communication servicesnetwork.The specific simulation used here consists of 2048 cells each modeled by alogical process, and 50,000 portables. The simulation was executed on a KendallSquare Research KSR2 multiprocessor each KSR processor is approximately 20faster than a Sun Sparc2 workstation, based on measurements of sequentialsimulations. Figure 5.14 shows the average number of events committed by thesimulator per second of wallclock time, referred to as the event rate, for differentnumbers of processors. The size of this model prevented execution on a sequentialcomputer however, based on measurements of smaller models, the event rate isestimated to be between 15,000 and 20,000 events per second. Thus this simulationyields approximately fortyfold speedup on 49 processors. One anomaly in thesemeasurements is the simulation achieves super linear speedup, namely more thantwo times a performance improvement as the number of processors is doubled. Thisis because as the number of processors increases, the amount of cache memoryprovided to the simulation increases, so a larger fraction of the program resides incache memory, leading to a disproportionate improvement in performance.5.5 SUMMARYWhile the previous chapter focused on fundamental mechanisms used in Time Warpsystems, this chapter presented the techniques used to develop efficient optimisticparallel simulation systems. A principal concern with Time Warp, as originallyproposed, is the amount of memory required to execute the simulation. It was seenthat memory management protocols provide a means for Time Warp to executewithin a constant factor of the amount of memory required for a sequential executionwhen executing on a sharedmemory multiprocessor. More generally, an importantproblem that must be addressed in practical Time Warp systems is preventing overoptimistic execution. Controlling memory allocation is one approach to solvingthis problem. Several other synchronization protocols were discussed, most usingfundamental concepts used in Time Warp for example, rollback, antimessages,GVT that provide an effective means to control execution. Finally the implementation of the GTW system, a parallel discrete event simulation executive based onTime Warp, was described. Several techniques exploiting shared memory, such asbuffer management techniques and GVT algorithms, were described.Practical experience with optimistic parallel simulation indicates that runawayoptimism can be controlled, and that efficient parallel simulation executives can bedeveloped using these techniques. An intuitive explanation as to why runawayoptimism tends not to be a severe problem is that erroneous computations can onlybe initiated when one processes a correct event prematurely this premature event,and subsequent erroneous computations, must have time stamps that are larger thanthe correct, straggler computation. Also, the further the incorrect computationspreads, the larger its time stamp becomes, lowering its priority for executionsince preference is normally given to computations containing smaller time stamps.Thus Time Warp systems automatically tend to slow the propagation of errors,allowing the error detection and correction mechanism to correct the mistake before172 ADVANCED OPTIMISTIC TECHNIQUES5.6 COMPARING OPTIMISTIC AND CONSERVATIVE SYNCHRONIZATION 173TABLE 5.1 Comparing conservative and optimistic synchronization5.6 COMPARING OPTIMISTIC AND CONSERVATIVESYNCHRONIZATIONtoo much damage has been done. A potentially more dangerous situation is when theerroneous computation propagates with smaller time stamp increments than thecorrect one. However, even here, a wide variety of solutions exist to control runawayoptimism if it does appear.Unfortunately, the years of research in synchronization protcols do not reveal aclear winner for all applications. The fact remains that the optlma proto.col.for anyparticular situation is problem dependent. One can, however, gv gldelmes onwhich approach is appropriate for which situations. Important dlstmctlOns amongthese approaches are summarized in Table 5.1.Simulation executives based on conservative protocols are generally less complexthan those based on optimistic synchronization. If the simulation has large lookheads, the synchronization mechanism will not have to be inoked very frequently a welldesigned system. This will result in lower runtlme overheads. than moptimistic executives because optimistic protocols must create and fOSSIl collecthistory information. State saving is perhaps the most umbersme among theoptimistic overheads. But, as was seen in Chapter 4, relatlvely ordmary tasks such Ime arp 4 oglcaprocessesTime Warp 16 logicalprocesses....... Deadlock Avoidance64 logical processes Deadlock Avoidance16 logical processes Deadlock Recovery 64log ical processes Deadlock Recovery 6410 ical rocesses641 1oo8,7 1 4 f 3 72 fl16 32 48Message Densitymessages per logical processFigure 5.15 Performance of Time Warp and conservative protocols on an 8 processorsystem.as dynamic memory allocation, runtime errors, and IO that can be implementedusing conventional programming methods in conservative systems require inclusionof special mechanisms in optimistic systems. While relatively straightforwardsolutions to these problems exist, they do add to the complexity of the simulationexecutive. One area where conservative systems do incur additional overheads thatdo not arise in optimistic systems is when the topology among logical processeschanges during the execution. If the simulation protocol uses topology information,such as the distance between processes approach, an additional mechanism isrequired. But, on balance, optimistic systems are generally more complex thanconservative ones.But the Achilless heel for conservative protocols is the need for lookahead toachieve good performance. As illustrated in Chapter 3, conservative protocolsperform poorly if there is little or no lookahead, even if the model contains alarge amount of parallelism. This is illustrated in Figure 5.15 where Time Warpperformance is compared with the conservative null message and deadlock detectionand recovery protocols in simulating a queueing network on eight processors. Thisapplication contains a fixed number ofjobs cycling through the network. Figure 5.15shows speedup as a function of the message density, defined as the number ofjobs inthe queueing network divided by the number of LPs. This particular queueingnetwork includes a small number of highpriority jobs that preempt service of otherjobs, resulting in very poor lookahead. As can be seen, Time Warp is able tosuccessfully extract parallelism from this application, while both conservativealgorithms yield speedup less than one that is, they run slower than a sequentialexecution.Conservative protocols cannot fully exploit the parallelism in the applicationbecause they must guard against a worstcase scenario, which may seldom actuallyoccur in practice. A corollary to this observation is that no conservative protocol canscale unless certain assumptions are made regarding lookahead, except in relativelyspecialized circumstances such as large numbers of events containing exactly thesame time stamp. By scale we mean that if the number ofLPs and the number ofprocessors increase in proportion, the paralleldistributed simulation is able toMore robust less reliant onlookahead greatertransparency of thesynchronizationmechanismRequires additionalmechanisms e.g., statesaving to supportrollbackOptimisticComplex simulationexecutive requires statesaving, fossil collectionspecial mechanisms fordynamic memoryallocation, IO, runtimeerrorsLimited by actualdependencies rather thanpotential dependenciesConservativeStraightforward inclusion infederationsLimited by worstcasescenario requires goodlookahead for concurrentexecution and scalabilityPotentially complex, fragilecode to exploit lookaheadSimple simulationexecutive may needspecial mechanism fordynamic LP topologylower overheads if goodlookaheadApplication developmentLegacy simulatorsParallelismOverheadsProtocol174 ADVANCED OPTIMISTIC TECHNIQUES 5.7 ADDITIONAL READINGS 175maintain approximately the same rate of advance in simulation time per second ofwallclock time. By contrast, parallelism in optimistic protocols is not limited bypotential dependencies between logical processes but rather by the actual dependencies as represented by event interactions. Thus optimistic approaches offergreater potential for scalability in the absence of lookahead.As was noted in Chapter 3, not only must the physical system exhibit goodlookahead characteristics, but the simulation must be programmed in a way thatexposes the lookahead to the underlying simulation executive. This may require arelatively high level of expertise on the part of the simulation programmer. Ifchanges to the model are included that affect the lookahead characteristics, such asthe addition of preemptive behavior, substantial revisions to the model may benecessary. In the worst case, no revision yielding acceptable performance may bepossible.More important, exploiting lookahead can lead to simulation code that iscomplex This leads to software that is difficult to develop and maintain. Constraintson dynamically changing the topology of logical processes can further aggravate thisproblem. If ones goal is to develop a general purpose simulation executive thatprovides robust performance across a wide range of models and does not require themodel developer to be familiar with details of the synchronization mechanism,optimistic synchronization offers greater hope.On the other hand, if one is retrofitting an existing legacy sequential simulationfor paralleldistributed processing for example, by federating it with othersimulators, or even itself, in a distributed simulation environment, conservativesynchronization offers the path of least effort. This is because there is no need to addstatesaving mechanisms to the simulation code or to ensure that dynamic memoryallocation and IO are implemented to account for rollback. In the long run one maybe able to automate many of these tasks for optimistic processing, but tools tosupport such automation do not yet exist.5.7 ADDITIONAL READINGSMemory management in Time Warp dates back to Jeffersons original paper wherethe message sendback mechanism for flow control is discussed Jefferson 1985.Cancelback is discussed in Jefferson 1990, and its performance is evaluatedempirically and analytically in Das and Fujimoto 1997 and Akyildiz, Chen et al.1993, respectively. Lin first defined the storage optimality concept and proposedthe Artificial Rollback protocol Lin and Preiss 1991. The pruning mechanism andPruneback protocol are discussed in Preiss and Loucks 1995. The adaptive,blocking based memory management protocol discussed in Section 5.1.7 isdescribed in Panesar and Fujimoto 1997. An adaptive version of the Cancelbackprotocol is described in Das and Fujimoto 1997.Numerous algorithms for controlling optimistic execution in Time Warp havebeen proposed. The Moving Time Window algorithm is described in Sokol andStucky 1990. Breathing Time Warp is discussed in Steinman 1993. Riskfreeexecution was first proposed in the SRADS protocol Dickens and Reynolds 1990and was also utilized in the Breathing Time Buckets mechanism Steinman 1991.Performance evaluations of this technique are described in Bellenot 1993, andWonnacott and Bruce 1996. Riskfree execution is also used in a realtimesimulation executive called PORTS Ghosh, Panesar et al. 1994 because it offersfavorable properties for predicting execution time in simulations. The Wolf Callsmechanism is described in Madisetti, Walrand et al. 1988, and a variation on thisidea that avoids race conditions in locating LPs to roll back is described in Damani,Wang et al. 1997. Probabilistic rollbacks are described in Madisetti, Hardaker et al.1993. The echo performance hazard was identified in Lubachevsky, Shwartz et al.1991 where a blockingbased optimistic protocol is discussed. Several proposalsfor mixing conservative and optimistic LPs have appeared, including Arvind andSmart 1992, Rajaei, Ayani et al. 1993, and Jha and Bagrodia 1994.Several proposals for adaptive synchronization mechanisms have appeared. InBall and Hoyt 1990, and Ferscha 1995, processes may be blocked based onstatistical estimates of the time stamp of the next event that will arrive from anotherprocessor. In Harnnes and Tripathi 1994, blocking intervals are based on recentsimulation time advances by LPs. A class of protocols based on rapid disseminationof global state information is described in Srinivasan and Reynolds 1995. Spacetime simulation is discussed in Chandy and Sherman 1989, Bagrodia, Liao et al.1991. Other optimistic protocols include Prakash and Subramanian 1992, Turnerand Xu 1992, Deelman and Szymanski 1997, Fabbri and Donatiello 1997, Tay,Teo et al. 1997.The GTWexecutive is described in Das, Fujimoto et al. 1994. GTWs GVTalgorithm, and onthefly fossil collection are described in Fujimoto and Hybinette1997. The buffer management mechanism is described in Fujimoto and Panesar1995. Air traffic simulations using GTWare described in Wieland 1997.CHAPTER 6Time Parallel SimulationAs discussed in Chapter 2, one can view the simulation computation as determiningthe values of a set of state variables across simulation time. The approachesdiscussed thus far accomplish this task by partitioning the state variables definedin the simulation among a set of logical processes. They assign each LP theresponsibility of computing the evolution of its state variables over the duration ofthe simulation. As illustrated in Figure 6.1 a, this approach uses a spatialdecomposition approach where the spacetime diagram is partitioned into a set ofhorizontal strips, with each LP responsible for computing the values of the variablescontained within that strip over simulation time.Another approach is to use a temporal decomposition of the spacetime diagram.Here, as shown in Figure 6.1b, the spacetime diagram is partitioned into a set ofvertical strips, and a logical process is assigned to each strip. Each LP must performa simulation of the entire system for the interval of simulation time covered by itsstrip of the spacetime diagram.Stated another way, the simulation computation constructs a sample path throughthe set of all possible states in which the system can reside across simulation timesee Fig. 6.2. A time parallel simulation partitions the simulation time axis into asequence of nonoverlapping simulation time intervals To, Tj , Tj , T2 , ... , TnITn A logical process assigned to the ith window computes the portion of the samplepath within that window.This socalled time parallel approach to parallel simulation offers severalattractive properties Massive parallelism. The amount of parallelism in the simulation is potentiallyvery large because simulations often extend over long periods of simulationtime. Time parallel simulation algorithms typically run out of processors beforethey run out of parallelism within the simulation computation. Independent logical processes. Once a logical process begins the simulation ofa vertical strip, it can proceed with this simulation independent of other logicalprocesses, thereby avoiding expensive synchronization operations throughoutmuch of the computation. Time parallel simulation algorithms typically onlyrequire coordination among the logical processes prior to beginning thecomputation of each strip. This is in sharp contrast to spaceparallel algorithms177178 TIME PARALLEL SIMULATION 6.1 TIME PARALLEL CACHE SIMULATION USING FIXUP COMPUTATIONS 179Figure 6.1 Spacetime diagram of the simulation computation. a Space parallel approachb time parallel approach.which require continual synchronization among the logical processes throughout the entire computation.computation to recompute the sample path for the ith interval using the finalstate computed by LP i  1 as the initial state for LP. The fixup computationmay be to simply repeat the simulation using the new initial state. This processis repeated until the final state computed by each interval matches the initialstate of the next interval. When a state match occurs across all of the intervals,the simulation is completed. Precomputation ofstate at specific time division points. It may be possible todetermine the state of the simulation at specific points in simulation timewithout performing a detailed simulation ofeverything preceding that time. Forexample, as will be seen later, one may be able to guarantee a buffer willoverflow in simulating a queue based on the rate of traffic entering the queuerelative to the maximum rate of traffic departing. If this is the case, one maydefine the simulation time intervals so that the state of the simulation is knownat the beginning of each time interval, thereby solving the state matchingproblem. Parallel prefix computations. If one can formulate the state of the simulation asa linear recurrence equation, a parallel prefix computation can be used to solvethe equation over simulation time.As will be seen momentarily, the solution to the state matching problem requiresdetailed knowledge of the simulation application and the statistics that are beingcomputed. Thus time parallel simulation techniques do not provide a generalapproach to parallel or distributed simulation but rather define a methodology thatcan be applied to develop parallel simulation algorithms for specific simulationproblems.In the following we describe three time parallel simulation algorithms forsimulating a cache memory system, an asynchronous transfer mode ATM multiplexer, and a GGl queue.6.1 TIME PARALLEL CACHE SIMULATION USING FIXUPCOMPUTATIONSbaThe central problem that must be solved by the time parallel simulation algorithmis to ensure that the states computed at the boundaries of the time intervals match.This is referred to as the statematching problem. Specifically, the state computed atthe end of the ith interval must match the state at the beginning of the ith interval.But how can one compute the initial state of the ith interval without first performingthe simulation computation for all prior intervalsSeveral approaches to solving the state matching problem for specific simulationproblems have been proposed. The three approaches to be discussed here are asfollows Fixup computations. Logical process LP, responsible for the ith time interval,guesses the initial state of the simulation in its time interval and performs asimulation based on this guess. In general, the final state computed for theiI st interval will not match the initial guess, so LP i must perform a fixupFigure 6.2LPBsimu ated timeSample path for a simulation computation.The time parallel simulation approach using fixup computations executes thefollowing steps1. Logical process LP i is assigned an interval of simulation time Ti1, Ti andselects an initial state SOTi 1 for its interval. More generally, TJ denotesthe state of the system at simulation time Ti computed after the jth iterationj1,2, ....2. LP simulates the system over the time interval Ti 1, Ti , computing a finalstate TJ for this interval. Each logical process can execute on a separateprocessor, and no interprocessor communications is required during this step.3. LPi sends a copy of the final state it just computed Ti to LPil180 TIME PARALLEL SIMULATION 6.1 TIME PARALLEL CACHE SIMULATION USING FIXUP COMPUTATIONS 181Figure 6.3 Time parallel simulation using fixup computations. a Computation performedin first round b fixup computation in second round.computed in the previous iteration for each logical process, so the computationcompletes after only two iterations. In general, however, more passes may berequired to construct the complete sample path.Using this approach, the computation for the first interval will always becompleted by the end of the first iteration, the second interval computation willbe completed after the second iteration, and so on. Thus, in the worst case Niterations will be required to complete the sample path for N intervals orapproximately the same time for the sequential execution if overhead computationsrequired in the time parallel execution are negligible. The algorithm will performwell if the final state for each interval computation does not depend on the initialstate. When this is the case as in Fig. 6.3, the final state computed in the firstiteration for each interval computation will be correct despite the fact that anincorrect initial state was used.This time parallel simulation approach can be applied to simulating a cachememory in a computer system using a least recently used LRU replacement policy.A cache is a highspeed memory that holds recently referenced memory locationsdata and instructions. The goal is to store frequently used data and instructions inthe cache that can be accessed very quickly, typically an order of magnitude fasterthan main memory. Because the cache memory has a low access time, it isexpensive, so the computer system may only contain a limited amount of cachememory. If data or instructions are referenced by the CPU that do not reside in thecache, the information must be loaded into the cache, displacing other datainstructions from the cache if there is no unused memory in the cache. Main memory ispartitioned into a collection of fixedsize blocks a typical block size is 64 bytes,and some set of blocks are maintained within the cache. The cache managementhardware includes tables that indicate which blocks are currently stored in thecache.The cache replacement policy is responsible for determining which block todelete from the cache when a new block must be loaded. A commonly used policy isto replace the block that hasnt been referenced in the longest time, based on thepremise that recently referenced blocks are likely to be referenced again in the nearfuture. This approach is referred to as the least recently used LRU replacementpolicy. Due to implementation constraints, cache memories typically subdivide thecache into sets of blocks, and use LRU replacement within each set.Time parallel simulation can be effective in simulating cache memories, particularly those using LRU replacement because the final state of the cache is seldomdependent on the caches initial state for reasonably long strings of memoryreferences. This is because subsequent references will tend to load new blocks intothe cache, eventually displacing the caches original contents.The input to the cache simulation is a sequence of memory references. Eachreference indicates which block of memory is being referenced. The sequence ispartitioned into N subsequences, one for each logical process i.e., each processorparticipating in the simulation.The state of the cache is a list of the blocks that are currently stored in the cache.These blocks are stored in a data structure known as the LRU stack. When a block isfirst roundsecond roundsimulated timesimu ated timeLPnLPnbaLPcLPcLPBLPBLP...LPA4. If SfTi1 does not match the initial state SfITi  l  used in the simulation forthe interval Ti 1, T, then LP i sets its initial state to SfTi 1 and recomputesthe sample path for its interval. If during this recomputation the state of thenew sample path that is being computed matches that of the previous iterationi.e., SJt is identical to SfI t for some simulation time t, the process canstop its computation because the remainder of its sample path will be identicalto that derived from the previous computation.5. Repeat steps 3 and 4 until the initial state used in each interval matches thefinal state computed for the previous interval for all logical processes.This process is illustrated graphically in Figure 6.3. As shown in Figure 6.3a,each logical process is assigned an interval in the simulation time axis, selects someinitial state, and computes the sample path based on this initial state. The logicalprocesses execute independent of each other during this phase. Only LPA computeda correct sample path because it is the only process that used the correct initial statethe initial state for the entire simulation. Each of the logical processes except LPAresets its initial state to the final state computed by the LP assigned the immediatelypreceding time window. In Figure 6.3b, the new sample path converges to that182 TIME PARALLEL SIMULATION 6.2 SIMULATION OF AN ATM MULTIPLEXER USING REGENERATION POINTS 183referenced, the LRU stack is searched to determine if the block already resides in thecache. If it does, a cache hit is said to occur, and the block is removed and placedon top of the stack. Removing a block from the stack causes the blocks above it tomove down one position in the stack, much like removing a tray from the middle of astack of trays in a cafeteria. Thus blocks that have been referenced recently will benear the top of the stack, while those that have not been referenced recently tend tosink toward the bottom of the stack. The block that has not been referenced for thelongest time, that is, the least recently used block, will be at the bottom of the stack.If the referenced block is not in the stack, the block does not reside in the cacheand a miss is said to occur. The block must now be loaded into the cache. To makeroom, the block at the bottom of the stack the LRU block is deleted from the stack,causing all blocks to slide down one position. The newly referenced block is placedon top of the stack.For example, Figure 6.4 shows the execution of a time parallel simulation for asingle set of a cache memory system containing four blocks. The addresses listedacross the top of the figure indicate the blocks that are referenced by successivememory references. A time parallel simulation using three logical processes is used.Each LP initially assumes the cache is empty as its initial state. LPA first referencesblocks I and 2, with each causing a miss. Block I is then referenced again, causing itto be moved to the top of the LRU stack. Blocks 3 and 4 are referenced and loadedinto the stack. When block 6 is referenced, the LRU block block 2 at the bottom ofthe stack is deleted. The time parallel simulation divides the input trace of memoryreferences into three segments, and each LP independently processes its trace,assuming that the cache is initially empty. Figure 6.4a shows the sample pathcomputed by each LP during the first round of the simulation.In the second round of this computation LPA is idle because it correctly computedits portion of the sample path in the first pass. LPB recomputes its sample path usingthe final state computed by LPA as the initial state for its cache. Similarly LPcaddress 12134367 21269336 4231727412134367 266 iLRU 1213436 212993 421727Stack    21 1 43    1 669   4 3 1 1 2    221 4     1 222    2331LPA LPB LPcaaddress 12134367 21269336 42317274idle 26 LRU 7 2 1 2 match 6 4 2 matchStack 6771 36436671 936LPA LPB LPcbFigure 6.4 Example execution of time parallel cache simulation. a Execution during thefirst round using empty caches as the initial state b execution during the second round.recomputes its sample path using LPBs final state after the first iteration. Aftersimulating the fifth memory reference in its trace, LPB observes that the state of thecache is now identical to what it had computed after the fifth memory reference inthe first round. Therefore the remainder of its sample path will be identical to thatcomputed in the first round, so there is no need to recompute it. Similarly LPcsrecomputation becomes identical to that computed in its first round after only fourmemory references, so it can also stop. LPB and LPc need only replace the first fourand three stack states, respectively, in the first round simulation with the new valuescomputed in the second round to reconstruct the entire sample path.In general, the LRU replacement policy guarantees that if the number of blocks inthe set is k, the state of the simulation will be independent of the initial state of thecache after k different blocks have been referenced. Thus, if each subsequence ofmemory references used by the logical processes references at least k differentblocks, the time parallel simulation will require only two steps to compute thecomplete sample path.6.2 SIMULATION OF AN ATM MULTIPLEXER USING REGENERATIONPOINTSThe points where the time axis was partitioned in the cache simulation could bemade arbitrarily, so time intervals were defined with an equal number of memoryreferences in each interval assigned to each logical process in order to balance theworkload among the processors. The time parallel simulation algorithm describednext selects the time division points at places where the state of the system can beeasily determined. Specifically, the simulation time axis is broken at regenerationpoints these are points where the system returns to a known state.Asynchronous transfer mode ATM25 networks are a technology that has beendeveloped to better support integration ofa wide variety ofcommunication servicevoice, data, video, and faxesall within a single telecommunication network. Thesesocalled Broadband Integrated Services Digital Networks BISDN are expected toprovide high bandwidth and reliable communication services in the future. Messagessent into ATM networks are first divided into fixedsize cells that then form theatomic unit of data that is transported through the network.A multiplexer, depicted in Figure 6.5, is a component of a network that combinesconcentrates several streams of incoming traffic here, ATM cells into a singleoutput stream. The incoming lines might represent phone lines to individualcustomers, while the outgoing line represents a highbandwidth trunk line carrytraffic for many customers. The bandwidth of the outgoing line is usually smallerthan the sum of the bandwidths of the incoming lines. This means that cells willaccumulate in the multiplexer if the total incoming traffic flow exceeds the capacityof the output link. For this purpose the multiplexer contains a certain amount of25 An unfortunate acronym. ATM networks are not to be confused with automated teller machines used bybanksp184 TIME PARALLEL SIMULATION 6.2 SIMULATION OF AN ATM MULTIPLEXER USING REGENERATION POINTS 185input 1IItttinput 2 j t111....121....Out1,4 4,23,4 4,3 3,2 j 1,3Figure 6.5 An ATM multiplexer with N inputs. The circuit contains a fixed sized buffer.Data cells are lost if the buffer overflows.buffer memory to hold cells waiting to be sent on the outgoing link. To simplify itsdesign, if the buffer memory overflows i.e., a new cell arrives but the outgoing linkis busy and there is no unused buffer space in which to store the cell, the cell issimply discarded. A goal in designing a multiplexer is to provide sufficient buffermemory to ensure that the expected number of lost cells is below some design goal,typically on the order of only one lost cell per 109 successfully transmitted cells.Since cell losses are rare, very long simulation runs are required to capture astatistically significant number of cell losses.Assume that each of the incoming links can transmit B cells per second, and theoutgoing link is of bandwidth C x B cells per second, where C is an integer. Linkbandwidths are normalized so that the input link has unit bandwidth, and the outputlink has bandwidth C. Here, the unit of time that is used is the amount of timerequired to transmit a single cell over an input link. This quantity is referred to as acell time. C cells may be transmitted by the output link during a single cell time.A traffic generator called a source transmits cells on each incoming link. Aseparate source is attached to each incoming link of the multiplexer. The trafficproduced by each source can be viewed as a sequence of on and off periods.When the source is on, it transmits cells, one per cell time, over the link. When thesource is off, no cells are transmitted. This behavior is depicted in Figure 6.6.The stream of incoming cells to the multiplexer can be characterized as asequence of tuples Ai bi  i  1,2,3, ..., where Ai denotes the number active oron transmitting cells sources, and bi denotes the length of time that exactly thisnumber of sources remain active. For example, in Figure 6.6 the input is characterized by the tuples 1,4, 4,2, 3,4, and so on, meaning initially one source isactive for four units of time, then four are active for two units of time, then three forfour units of time, and so on.The simulation problem is formulated as follows Consider a multiplexer with Ninput links of unit capacity, an output link with capacity C, and a FIFO queuecontaining K buffers, with each buffer able to hold a single cell. Any cells arrivingwhen the queue is full are discarded. The simulation must determine the averageutilization and number of discarded cells for incoming traffic characterized by thesequence of tuples Ai bi , as depicted in Figure 6.6.Let Ti denote the simulation time corresponding to the end of the ith tuple, withTo denoting the beginning of the simulation which is equal to zero. The state ofthesimulation model at any instant of simulation time is simply the number of cellsDoff.oninput 3 Jinput 4 simulation timecell timesFigure 6.6 Onjoffperiods of four sources. The input to the multiplexer is characterized by asequence of tuples Ai Di , where A indicates the number of on sources and D indicates theduration in cell times that this number of sources are active.stored in the queue. Let QT denote the number of cells stored in the queue at timeTi . Let SeT denote the total number of cells serviced transmitted on the outputlink and LT, denote the number of cells lost up to time Ti . The notation used forthis simulation is summarized in Table 6.1. All of these quantities are integers.During each time interval the multiplexer will be in one of two possiblesituations1. Underload Ai s c. The number of active sources is less than or equal to theoutput link capacity that is, the rate that cells are flowing into the multiplexeris less than or equal to the outgoing rate, so the length of the FIFO queue Qiis either decreasing or remains the same. No cell losses can occur during anunderload period.TABLE 6.1 Symbols used in multiplexer simulationN Number of input linksC Capacity of the output linkK Number of buffersA Number of active sources during ith time intervalj Length of ith time intervalT Time marking the end of the ith intervalQT Length of the queue at time T QTo  0SeT Total number of cells transmitted on output at time T STo  0LT Total number of cell lost up to time T LTo  0186 TIME PARALLEL SIMULATION 6.2 SIMULATION OF AN ATM MULTIPLEXER USING REGENERATION POINTS 1872. Overload Ai  C. The rate of cells entering the multiplexer exceeds thecapacity of the outgoing link, so the length of the FIFO queue is increasing.All cell losses occur during overload periods.During each time unit Ai cells are received on the input links and C cells aretransmitted on the output link. During an overload period the queue fills at a rate ofAi  C cells per unit time, unless the queue becomes full. If the queue becomesfull, the queue length remains fixed at K, and the additional cells that could not beplaced into the queue are lost. Conversely, during an underload period the queue isdrained at a rate of C  Ai cells per unit time, unless the queue becomes empty.Thus the length of the queue can be computed asThe number of serviced and lost cells can be computed by accounting for all cellsduring each time period, which is the time represented by a single tuple. Specifically,at the beginning of the ith time period there are QTii cells buffered in themultplexer. During this time period Aibi additional new cells are received by themultIplexer. At the end of the time period, there are QT cells remaining. Thedifference between these two quantities, QTi i   Aibi  QTi, corresponds tocells that were either serviced or lost during the time period.First, let us compute the number of serviced cells. During an underload period nocells are lost. Thus all of the QTi i   Aib  QT cells derived in the previousparagraph represent cells that were serviced. Now consider an overload period.Observe that the output link transmits C cells per unit time so long as the queue isnot empty. Because the queue cannot become empty during an overload period, thenumber of serviced cells increased by Cb i. In other words,difference between these two quantities is the number oflost cells during the ith timeperiod. Thus we haveif Ai  C overload 6.3if Ai  C underload.LT  LTi i   QTi i  Aib  QT  Cbi LTi  i These equations for computing QTi, SeT, and LT enable one to simulate thebehavior of the multiplexer. To perform a time parallel simulation of the multiplexer,the sequence of tuples is partitioned into P subsequences, where P is the number ofprocessors available to perform the simulation, and a subsequence is assigned toeach one. The principal question that must be answered is again the statematchingproblem, or here, What is the initial state the initial length of the queue for eachsubsequence assigned to each processorThis statematching problem can be solved if the length of the queue at certainpoints in simulation time can be computed during a parallel precomputation phase.One could then partition the tuple sequence at points in time where the state of themultiplexer is known. Two key observations are used to determine the state of themultiplexer at specific points in time1. Guaranteed overflow. Consider a tuple defining an overload period. If thelength of the overload period is sufficiently long that even if the queue wereempty at the beginning of the tuples period, the buffer is guaranteed to be fullby the end of the period, then it would be known that the length of the queue isK at the end of the tuple period. A tuple with this property is referred to as aguaranteed overflow tuple.2. Guaranteed underflow. Similarly consider a tuple defining an underloadperiod. In this case the queue is being drained. If the duration of the tupleis sufficiently long that the queue will be empty at the end of the tuples periodeven if the queue were full at the beginning of the period, then the tuple is saidto be a guaranteed underflow tuple, and the queue must be empty at the end ofthe tuples period.More precisely, the conditions for Ai bi  to be a guaranteed overflow underflowtuple are6.16.2if Ai  C overloadif AiC underload.ifAi  C overloadif Ai  C underload.QTi  minQTi i   Qi  Cbi, K maxQTii   C Abi, 0SeT  STi i   Cbi STi i   QTi 1  Aibi  QTThe utilization of the output link is computed as the total number of cells servicedduring the simulation divided the number of cells that could have been servicedduring the length of the simulation. Specifically, if the simulation ends at time TMi.e., the simulation includes M intervals, or M tuples, the output link utilization isSTMCTMNow consider lost cells. No cells are lost during underload. During overload, thenumber of cells either lost or serviced is QTi 1  Aib  QT, as discussedearlier. The number of serviced cells is easily computed because as was observedin computing St , C cells will be serviced per unit time during overload. TheGuaranteed overflowGuaranteed underflowifthenifthenAi  Cbi  K,QT  K.C  Al5i  K,QTi   O.6.4188 TIME PARALLEL SIMULATION 6.3 SIMULATION OF QUEUES USING PARALLEL PREFIX 189The time parallel simulation algorithm for the ATM multiplexer operates as follows.Given a sequence of tuples Ai bi, i  1,2, ... 1. Identify a set of guaranteed overflow or underflow tuples G by applyingconditions 6.4. This can be accomplished by assigning an equal lengthsubsequence to each processor, and by having each processor search from thebeginning of its subsequence for a guaranteed overflow or underflow tuple.The tuples in G define the time division points at which the simulation timeaxis is broken, as shown in Figure 6.2.2. For each processor, if the ith tuple is found to be a guaranteed overflow tuple,set QT to K if the ith tuple is a guaranteed underflow tuple, set QT to O.3. For each processor, compute QT, ST and LTi  using equations 6.1,6.2, and 6.3 defined above for each tuple, starting with the tuple followingthe guaranteed overflow or underflow tuple. Assume that Sand L are initiallyzero for each subsequence that is, each processor only computes the numberof serviced and lost cells for the subsequence assigned to it not a cumulativetotal for the entire simulation. When this has been completed, send the queuelength at the end of the subsequence to the processor assigned the nextsubsequence. Upon receipt of this information, the processor can simulate thetuples assigned to it that preceded the guaranteed overflowlunderflow tuple.4. Compute the total number of serviced and lost cells by summing the valuescomputed for these quantities by each processor.This algorithm relies on being able to identify a guaranteed overflow or underflowtuple in the subsequence assigned to each processor. The algorithm fails if anyprocessor does not locate a guaranteed underflow or overflow tuple. In general, it isimpossible to guarantee such a tuple will be found. In practice, because cell lossesare so rare, there will usually be an abundance of guaranteed underflow tuples. Analternative approach to this problem is to examine short sequences of tuples in orderto identify a sequence that results in a guaranteed underflow overflow, even thoughindividual tuples within the sequence could not be guaranteed to result in anunderflow overflow. There is again no guarantee, however, that such a sequencecan always be identified.The central advantage of this algorithm compared to the approach described inthe previous section for cache memories is no fixup computation is required. Thecentral disadvantages are the need for a precomputation to compute the time divisionpoints, and the possibility the algorithm may fail if such time division points cannotbe identified.6.3 SIMULATION OF QUEUES USING PARALLEL PREFIXA third approach to time parallel simulations utilizes parallel prefix computations todetermine the state of the simulation across simulation time. A prefix computationcomputes the N initial products of N variables Xl X2 ,  ,XN P j XlP2 Xl X2P3  Xl X2 X3where the asterisk  is an associative operator. This set of equations can berewritten more compactly as the linear recurrence Pi  Pi l .Ai, i  1, 2, ... , N,where Po is the identity element. As will be discussed momentarily, prefixcomputations are of interest because efficient algorithms exist for performingthese computations on a parallel computer.The simulation of a GIGII queue26 where the service time does not depend onthe state of the queue can be recast as a prefix computation. Specifically, let r denotethe interarrival time of the ith job at the queue, and Si denote the servie timeassigned to the ith job. These values can be trivially computed in parallel becausethey are independent random numbers. The simulation computation must computethe arrival time of the ith job Ai, and the departure time of the ith job D fori  1,2, ... ,N. The arrival time of the ith job can immediately be rewritten I as alinear recurrences.o it can be immediately solved using a parallel prefix computation. The departuretimes can also be written as a linear recurrence. Specifically, the ith job beginsservice either when it arrives, or when the iI st job departs, which ever is later. ThusD i  maxDii A Si This can be rewritten as the following linear recurrencewere. the atrix multiplication is performed using max as the additive operatorWith Identity 00, and  as the multiplicative operator identity 0. Rewriting thedeparture. time equation in this form puts it into the proper format for a parallel prefixcomputatIOn.Becaue the simulation computation can be specified as a parallel prefixomputatlOn, the only question that remains concerns the parallel prefix computationItself. The parallel prefix for N initial products can be computed in Olog N time.Consider computation of the arrival times Ai Suppose that the N data values rl26 The notation G G I means a general distribution is used to select the interarrival time and service timeof jobs arriving at the queue, and the I denotes the fact that there is one server.190 TIME PARALLEL SIMULATION 6.5 ADDITIONAL READINGS 191add value one positionto the leftadd value two positionsto the leftadd value four positionsto the leftFigure 6.7 Binary tree for performing a parallel prefix computation.rz, ... , rN are assigned to different processors. The ith initil prtial product Ai canbe computed in parallel by defining a binary tree, as shown m FIgure 6.7. In the firststep, each data value ri is added to the data value one position to the left riI In thenext step, a new cumulative sum is formed by adding in the value two elements tothe left then four to the left, eight, and so on. If each processor repeats these steps,all N iitial products will be performed in rlogNl steps, as shown in Figure 6.7.More precisely, the parallel prefix computation is defined as followsFOR jO to rlog Nl  1 DOFOR ALL i E 2 j l, 2 j 2, ... , N DO IN PARALLELNewRi . ri2 j   riENDFOR ALLFOR ALL i E 2 j l, 2 j 2, ... , N DO IN PARALLELr  i   NewR  i ENDFOR ALLENDFORThe FOR ALL statement performs the iterations of the loop in parallel, one iterationper processor. The second FOR ALL loop is used to copy intermediate results bakinto the r array for the next iteration. When the above program completes, r 1 will hold the arrival time for the ith job.In practice, there will usually be many more partial products than processors, soone would aggregate groups of the values onto individual processos and prfocomputations involving data values on the same processor sequetlally. ThIS wIllimprove the efficiency of the parallel algorithm because tere wIll. be more lo.calcomputation between interprocessor communications, whIch are timeconsummgrelative to the time to perform an addition.6.4 SUMMARYTime parallel algorithms are currently not as robust as spae parallel approachesbecause they rely on specific properties of the system bemg modeled, such asspecification of the systems behavior as recurrence equations andor a relativelysimple state descriptor. This approach is currently limited to a handful of importantapplications, such as queuing networks, Petri nets, cache memories, and statisticalmultiplexers. Space parallel simulations offer greater flexibility and wider applicability, but concurrency is limited to the number of logical processes. In some casesboth time and space parallelism can be used together.Time parallel algorithms do provide a form a parallelization that can be exploitedwhen there isnt spatial parallelism available in the application. The three examplesdescribed in this chapter are all examples where there is very little space parallelismin the original simulation problem. The fact that the time parallel algorithms canexploit massive amounts of parallelism for these problems highlights the utility ofthe time parallel approach, provided suitable algorithms can be developed.6.5 ADDITIONAL READINGSTime parallel simulation for tracedriven simulations is described in Heidelbergerand Stone 1990 this is perhaps the first proposal for using this technique.Extensions to this method to simulate caches are described in Nicol, Greenberg etal. 1992. The algorithm using regeneration points to simulate ATM multiplexers isdescribed in Andradottir and Ott 1995 and Fujimoto, Nikolaidis et al. 1995, andextension of this method to simulate cascaded multiplexers is described in Nikolaidis, Fujimoto et al. 1994. Time parallel simulation of queues using parallel prefixalgorithms were first reported in Greenberg, Lubachevsky et al. 1991. Relatedwork in using time parallel simulation to simulate queues and Petri networks aredescribed in Lin and Lazowska 1991, Ammar and Deng 1992, Wang and Abrams1992, and Baccelli and Canales 1993. Other algorithms have been proposed tosimulate telephone switching networks Gaujal, Greenberg et al. 1993 and Markovchains Heidelberger and Nicol 1991. PART IIIDISTRIBUTED VIRTUALENVIRONMENTS DVEsCHAPTER 7DVEs IntroductionWe now shift attention to distributed simulation technologies intended to createcomputergenerated virtual environments into which users, possibly at geographically distant locations, can be embedded. Typical applications for this technology aretraining and entertainment. As discussed in Chapter I, work in this field hasprogressed on a largely separate track from the work described in previous chaptersconcerning parallel discrete event simulation PDES technology. This can be tracedto the fact that distributed virtual environments DVEs have different requirementsthan the analytic simulation applications to which PDES has historically beenapplied. We begin this discussion by contrasting these two technologies. Generalapproaches for building DVEs are then discussed. The remainder of this chapterprovides an overview of Distributed Interactive Simulation DIS and its successor,the High Level Architecture HLA to describe a typical approach to buildingdistributed simulation systems for DYE applications.7.1 GOALSA principal goal in most virtual environments is concerned with achieving asufficiently realistic representation of an actual of imagined system, as perceivedby the participants embedded into the environment. What sufficiently realisticmeans depends on what one is trying to accomplish. In the context of training, thismeans that humans embedded into the environment are able to develop skills thatwould be applicable in actual situations they might later encounter. Thus theenvironment must be sufficiently realistic that the system with which the operatoris working, such as an aircraft in the case of a flight simulator, behaves the way a realaircraft would behave in terms of its response to controls and other effects such assmoke or wind.Fortunately, every aspect of the environment does not have to be absolutelyaccurate in its minutest detail. In reality, a human is often looking for certain cuesthat trigger some action, such as a target coming into view in a flight simulator. Otheraspects of the environment, such as realistic looking trees, may not necessarilycontribute very much to increasing the effectiveness ofthe environment as a training1Q196 DVEs INTRODUCTION 7.3 SERVER VERSUS SERVERLESS ARCHITECTURES 197vehicle, depending on the scenario. This concept that certain parts of the environment must be more accurately reproduced than others is called selective fidelity.An often discussed goal of many DVEs in the context of adversarial exercises forexample, military training or multiuser video games is to achieve what is referredto as a fair fight. This means that the outcome for example, success or failure of auser to accomplish some task depends entirely on the skill of the player rather thanon artifacts of the virtual environment. Success becomes more difficult to achieve ifthe players are using different types of computing equipment. For example, oneplayer should not be able to see a target any sooner than a second player just becausehe is using a highresolution graphics workstation, while the other is using aninexpensive personal computer.Determining how realistic is realistic enough is an important problem thatmust be addressed when constructing a virtual environment, but it is a problem thatis beyond the scope of the discussion here. The important thing to note is that thereis tolerance for error built into the application, typically more so than for analyticsimulation applications. This affects the requirements of the underlying distributedsimulation system.7.2 CONTRASTING DVE AND PDES SYSTEMSDriven primarily by differing requirements, key features that distinguish DVE fromPDES systems are summarized below Paced versus unpaced execution. Unlike PDES systems which are typicallydesigned to achieve asfastaspossible execution of the simulation, advancesof simulation time in DVEs are almost always paced with wallcock time.27This has important ramifications in the development of the simulation modeland the underlying simulation executive. PDES models and the synchronization algorithms described in the previous chapters all operate correctly despitearbitrary message latencies. In general, large unpredictable latencies cannot betolerated in DVEs. Geographical distribution. PDES systems are seldom executed in a geographically distributed fashion because of the difficulty in achieving speedupwhen communication latencies are high. However, it is not unusual for usersand other resources for example, data bases in DVEs to be geographicallydistributed. Thus PDES systems are typically deployed on multiprocessors,while DVEs are more often deployed on networked workstations interconnected through a local area network LAN or wide area network WAN.27 Not all PDES executions are unpaced, however. Execution may be paced if there are physical devices orhuman participants interacting with the simulation. Repeatability. Many analytic simulation applications must produce exactly thesame results if the simulation program is reexecuted with the same inputs.This is often not so essential in DVEs. Synchronization requirements. Because the goal of a DVE is to provide asufficiently realistic environment as perceived by its users, synchronizationrequirements can often be relaxed. For example, if two or more events occur soclose together in time that the humans viewing the environment cannotperceive which occurred first, it may be permissible for the distributedsimulation to process those events in any order without compromising thegoals of the training exercise or entertainment session. This is in contrast toPDES systems where each event is assigned a precise time stamp, and thesimulation executive guarantees that events are always processed in time stamporder.The last observation is important because it suggests that one may not need to usesophisticated synchronization algorithms for much of the DVEs message traffic toensure that events are always processed in time stamp order. Indeed, DYEs that aredeployed today typically do not utilize these algorithms. This aspect will bediscussed in greater detail in Chapter 9.7.3 SERVER VERSUS SERVERLESS ARCHITECTURESAn important decision in the design of a DVE that includes geographicallydistributed participants andor resources concerns the physical distribution of thesimulation computations. Possible approaches include the following Centralized server architecture. As shown in Figure 7.la, interactive usersclients may log into a central computer a server that maintains the sharedstate of the virtual world. Machines at the users site may perform localsimulation computations pertaining to entities owned by the client, andgenerate one or more graphical displays for the user. For example, each localmachine may be executing a flight simulator program in a multiuser game thatperiodically generates messages to the server to indicate the aircrafts currentposition. The server maintains the global state of the simulation for example,the position of each aircraft and is responsible for notifYing each clientsimulator whenever some portion of the virtual world relevant to the client haschanged. For example, the server might reflect a position update message itreceived to all other aircraft simulators that can see the aircraft whoseposition has changed. Such systems often also include computationonlyentities sometime referred to as synthetic forces in. military contexts thatdo not have an interactive user associated with them. An example of acomputationonly entity is a computergenerated enemy aircraft in ahumanversuscomputer dogfight, whose movements are controlled by a198 DVEs INTRODUCTION 7.4 DISTRIBUTED INTERACTIVE SIMULATION 199Figure 7.1 Three DVE architectures with geographically distributed users. a Centralizedcompute server architecture b cluster of workstations server architecture c distributed,serverless architecture.computer program. Such computationonly entities may also be maintainedwithin the compute server. Distributed server architecture. This is similar to the centralized serverarchitecture, except that a multiprocessor or a collection of processors interconnected on a LAN is used to implement the server see Figure 7.1b. Theshared state of the virtual world is now distributed among the processors withinthe compute server and must be maintained by exchanging messages amongthese processors. Computationonly entities are typically executed on theprocessors within the compute server to take advantage of the highperformance interconnection. Changes in the shared state must be transmitted amongthe processors within the compute server as well as end user processors thatrequire updates to state information. Distributed, serverless architecture. Rather than utilize a server, the simulationcomputations are distributed among geographically distributed processors seeFigure 7.1 c. Computationonly entities may now be distributed amonggeographically distributed processors.Distributed Interactive Simulation DIS has been used extensively in building DVEsfor training in the defense community. A principal objective of DIS and subsequently the High Level Architecture effort is to enable interoperability amongseparately developed simulators7.4 DISTRIBUTED INTERACTIVE SIMULATIONThe distributed server and distributed, serverless architectures allow one, inprinciple, to circumvent the scalability problem in the centralized architecture, sincethe number of CPUs can increase in proportion to the number of simulated entities.The clusterofworkstations server architecture offers the advantage of lowlatency,highbandwidth communications among the processors within the server. If themajority of the communications is computertocomputer communications asopposed to usertouser communications, as would be the case if the DVE ispopulated by a large number of computationalonly entities, this is a significantadvantage. However, individual usertouser communications may incur higherlatencies in the serverbased approach because two message transmissions arerequired, one from the user to the server, and then another from the server to thesecond user. Only a single transmission over the WAN is required in the serverlessapproach shown in Figure 7.Ic.A second important consideration in deciding whether to use a serverbased orserverless architecture is reliability and maintainability of the system. In one sense,serverbased systems are less reliable because failure of the server for example, apower failure at the location housing the server will render the entire systemunavailable to any user. But on the other hand, having all of the computers in onephysical location affords the system operators much greater control over the serversphysical environment. System administrators can more easily prevent clumsy usersfrom tripping over cables or spilling coffee on vital components Further, mundanetasks such as making sure all of the machines are configured properly and have theappropriate version of the software are simplified if all of the machines are in asingle room under one system administrator.The remainder of this chapter provides an introduction to DVEs by examiningone class of systems. Specifically, design principles utilized in Distributed Interactive Simulation DIS systems are discussed next. The section that followsconcerning the HighLevel Architecture describes the types of services providedin a distributed simulation system infrastructure supporting DVEs. LAN interconnectc WAN interconnectabThe centralized approach shown in Figure 7.1a is the simplest from animplementation standpoint because global information concerning the state of thesimulation can be maintained on a single computer. The central drawback of thisapproach is, of course, that it does not scale to modeling large numbers of entities.As more entities are added, there will be insufficient compute cycles to service all ofthem.The primary mission of DIS is to define an infrastructure for linking simulations ofvarious types at multiple locations to create realistic, complex, virtual worlds for thesimulation of highly interactive activities DIS Steering Committee 1994.Although DIS, per se, has been supplanted by the HighLevel Architecture effort,HLA in fact builds upon and extends the DIS concept, so principles used in DIS stillremain relevant to systems being deployed in the late 1990s.200 DVEs INTRODUCTION 7.4 DISTRIBUTED INTERACTIVE SIMULATION 201A DIS exercise may include 1 humanintheloop systems such as tank or flightsimulators sometimes referred to as virtual simulators, 2 computation onlyelements such as wargame simulations sometimes referred to as constructivesimulations, and 3 live elements such as instrumented tanks or aircraft. Eachsimulator participating in the exercise is referred to as a DIS node.7.4.1 DIS Design PrinciplesDIS utilizes the following design principles DIS Steering Committee 1994 Autonomous simulation nodes. Autonomy is important because it simplifiesdevelopment developers of one simulator need not be overly concerned withdetails of other simulators in the DVE, it simplifies integration of existinglegacy simulators into a DVE, and it simplifies allowing simulators to join orleave the exercise while it is in progress. A DIS node is responsible formaintaining the state of one or more entities during the simulation execution,as well as some representation of the state of other entities relevant to it, suchas the position of other entities visible to a human operating the controls in atank simulator. The simulator receives inputs from the user and models thebehavior of the entity in response to those inputs. When this behavior causesactions that may be visible to other entities for example, firing the canon, thesimulator generates messages referred to as protocol data units PDUs tonotifY other nodes of this action. Autonomy among nodes is enhanced in DISin two specific ways1. DIS nodes are not responsible for identifYing the recipients of messages. Incontrast to typical PDES systems, it is the responsibility of the underlyingdistributed simulation infrastructure to determine which nodes shouldreceive notification of the event, andor receiving nodes to filtermessages that do not impact entities assigned to that node. A simplesolution used in SIMNET and early DIS systems was to broadcast themessage to all simulators, forcing each node to determine which events arerelevant to the entities it is modeling. The general problem of determiningwhich simulators should receive what messages is referred to as datadistribution management and will be discussed in greater detail in theChapter 8.2. Each simulator in a DIS exercise advances simulation time according to alocal typically hardware clock. Again, in contrast to PDES systems, nocoordination among simulators is used to advance simulation time. With theexception of communications that may be necessary to keep these clockssynchronized discussed in Chapter 9, each node advances simulation timeautonomously from other nodes. Transmission of ground truth information. Each node sends absolute truthconcerning the state of the entities it represents. This state information willusually be a subset of the state variables maintained by the node. Typical stateinformation transmitted to other nodes includes the location and orientation ofthe entity, the direction and velocity that it is moving, the position ofsubcomponents of the entity for example, the direction the gun turret of atank is pointed, and so on. Any degradation of this information, such as due toenvironment effects or sensor limitations, is performed at the receiver. Transmission ofstate change information only. To economize on communications, simulation nodes only transmit changes in behavior. Informationconcerning objects that do not change for example, static terrain does notneed to be transmitted over the network. If a vehicle continues to do the samething for example, travel in a straight line with constant velocity, the rate atwhich state updates are transmitted is reduced. Simulators do transmit keepalive messages referred to as heart beat messages, typically, every fiveseconds, so new simulators entering the exercise will be made aware of theseentities. Periodic updates also improve the robustness of the system, byproviding resistance to lost messages. Use of dead reckoning algorithms to extrapolate entity state information.Each node maintains information concerning other entities, such as those thatare visible to it on the battlefield. This information is updated whenever theentities send new status information via PDUs. In between state updates, allsimulators use common algorithms to extrapolate the current state position ofother entities between state updates, based on previously reported information.This also enables one to reduce the amount of communication that is requiredamong nodes because less frequent updates are required than if no suchextrapolations were performed. Dead reckoning will be described in greaterdetail later in this chapter.The principal components of a typical DIS simulator are depicted in Figure 7.2.This node includes a model of the dynamics of the vehicle manned by this node.Typically a continuous simulation is used to model the motion of the vehicle throughnetworkvisual displaysystemcontrolsand panelsFigure 7.2 Principal components of a virtual simulator in a DIS system reproduced fromMiller and Thorpe 1995.202 DVEs INTRODUCTION,.i7.4 DISTRIBUTED INTERACTIVE SIMULATION 203space. The image generator produces the visual image seen by the operator, using adatabase containing descriptions ofthe terrain visible to the vehicle. The controljdisplay interface and sound generator provide additional inputs and output to theoperator. The other vehicle state table maintains information concerning the stateof other entities relevant to this vehicle, such as other vehicles within range of itssensors. State information concerning this vehicle is sent to other simulators via thenetwork interface, and updates to state information for other vehicles are receivedfrom this interface.7.4.2 DIS PDUsA key ingredient in DIS to support interoperability among simulators is thedefinition of standard PDU types that are communicated among simulators. SeveralDIS PDUs are defined to transmit events of interest to entities in different DISnodes, among which are the following examples Entity state PDUs contain ground truth information indicating the appearanceand location of an entity. Fire PDUs indicate a munition has been fired from an entity. This might causea muzzle flash to be produced on the displays of entities that can view theentity firing the munition. Detonation PDUs indicate the trajectory of a munition has completed. Entitiesreceiving this PDU must assess their damages and produce appropriate audioand visual effects from the detonation.Other simulation events modeled by PDUs include requesting and receivingservice for example, resupplying the entity or repairing damages, generation ofemissions for example, electronic warfare, radio communications, and a variety ofother functions. In addition to simulation events, still other PDUs are used to managethe simulation itself, such as to establish the creation or removal of a new entity.Much of this functionality is subsumed by the High Level Architecture, sodiscussion of this aspect is deferred until later.For example, a typical sequence of operations in a DIS system between two tanksimulators is depicted in Figure 7.3. This scenario is taken from Miller and Thorpe1995. The circled numbers in this figure correspond to the actions listed below1. The first simulator the upper diagram in Fig. 7.3 detects that the operatorhas pressed a trigger to fire the tanks cannon.2. The simulator generates an audio signal to the tanks operator indicating thatthe cannon has fired.3. A muzzel flash is produced and displayed locally to the tanks operator.4. A Fire PDU is broadcast on the network, and received by the secondsimulator.visual displaysystemvisual displaysystemFigure 7.3 Sequence of actions between two simulators in DIS reproduced from Miller andThorpe 1995.5. The muzzle flash for the first tank is displayed to the operator of the secondtank.6. Ballistic flyout calculations at the first simulator are used to display a tracerfor the shell.7. Impact of the shll at the target is displayed locally in the first simulator.8. A Detonation PDU is broadcast over the network, and received at the secondsimulator.9. The impact of the shell is displayed at the second tank.IO. Calculations assessing the damage are performed at the second tank.11. Visible effects of the damage, if any, are broadcast to other simulators via anEntity State PDU7.4.3 Time ConstraintsIt is clear that the latency to transmit a message over the network will play animportant role in determining the realism of the virtual environment. For example,if the Fire PDU is delayed in the network, it could be. received by the secondsimulator after the detonation PDU is received so it could appear that the effect theshell detonating precedes the cause the shell being fired in the second simulator.While this may initially seem to be an unacceptable situation, anomalies such as this204 DVEs INTRODUCTION7.5 DEAD RECKONING 205may be pennissible. If, for instance, the two events occur at approximately the sametime, the operator of the second simulator may not be able to perceive that theyoccurred in an incorrect order. Further, in a training exercise, humans may be able tofilter such anomalies so that even if they are noticeable, they do not compromisethe goals of the training exercise.Human factors studies indicate that people begin noticing latencies when theyexceed 100 milliseconds Bailey 1982 Woodson 1987. In SIMNET, the goal was tokeep latencies below human reaction times, or 250 milliseconds. A similar value300 milliseconds was subsequently adopted in DIS for loosely coupled actionssuch as that shown in Figure 7.3 from trigger pull to display of the muzzle flash atthe remote machine and 100 milliseconds for tightly coupled actions wheredetailed, temporally sensitive interactions occur DIS Steering Committee 1994.Fighter pilots flying in close fonnation is one example of the latter. Eve lowerlatencies may be required in other situations, such as when hardware devlCes areembedded into the virtual environment.7.5 DEAD RECKONINGConsider a DVE consisting of a collection of vehicles moving over some space.Assume that each simulator executes on a separate processor and models a singlevehicle. Each simulator must generate a display for its driver indicating the positionand orientation of the other vehicles within visual range. Assume that the display isupdated at a rate of 60 times per second. Each simulator maintains locally theposition of other vehicles, and every 17 milliseconds 160th of a second generatesa suitable graphical image. In order to keep other vehicles up to date, each simulatoralso broadcasts its current location to the other simulators every 17 milliseconds.Even if each incoming message requires only 0.1 milliseconds to process, a DVEcontaining 100 vehicles will require each simulator to consume almost twothirds ofits CPU cycles just to process incoming messages. Further, in computing environments with limited communication bandwidth for example, users connected to theInternet via phone lines, the amount of infonnation that must be exchanged isclearly prohibitive.A technique called dead reckoning can be used to reduce interprocessor.communication. The basic idea is that rather than send frequent state updates,each simulator estimates the location of remote entities from its last reportedposition, direction, velocity, and the like. For example, if it is .own hat thevehicle is traveling east at 50 feet per second, and its coordinate pOSItIOn at tIme 200seconds into the simulation is 1000, 1000 where each unit corresponds to one foot,then it can be predicted that one second later it will be at position 1050, 1000without transmitting a position update message see Fig. 7.4.The tenn dead reckoning comes from a technique used to detennine theposition of a ship at sea. A ships position can be accurately detennined byexamining the position of stars relative to the horizon or from satellites. However,it is not always possible to do so because of weather conditions or malfunctioninglast reported stateposition 1000,1000traveling east  50 feetsecond1000 ...... ....... ,1000npredictedpositionone second later1000 1050Figure 7.4 Predicting the location of a vehicle base in its last reported position directionand velocity.  ,navigational equipment. In such a case the ships position can be estimated from itslast recorded location and knowledge of its movements since then. This is the sameproblem in a DVE one must extrapolate the position ofother moving elements in theDVE based on recent reports of its location and movements.In som cses, if infonnation is available concerning the movement of the object,ore sophIstIcated means can be used to project its future location. For example, if itIS know that the object is a projectile, its position can be computed using trajectorycalculatIOns. More generally, the idea of replacing communications with localcomputations can be applied whenever a method is available to estimate thefuture state of a remote entity based on previous states.To implement this technique, each simulator maintains a local model of remotevehicles called the dead reckoning model DRM. If the display is updated 60 timesper second, the DRM is interrogated every 17 milliseconds to detennine the currentlocation of each remote entity visible to this entity.Of course, this approach breaks down if the vehicles true motion deviates fromthe DRM used by other simulators. This might occur because the user steers thevehicle to mov in a new direction, or because the DRM gives only an approximatIon to the vehicles actual motion, as is typically the case to economize on DRMomputations. Some mechanism is required to resynch the DRM if it becomes toolllaccurate. This rblem is addressed by having each vehicle simulator generate anpdate messge If It detects the DRM other simulators are using has become toolllacuate WIth respect to its true position. To accomplish this, each simulatormaItallls a local copy of its own dead reckoning model see Fig. 7.5. If the positionpedicted y theD and the true position of the vehicle as computed by the actualhlghfidehty mode.1 differ y more than some threshold, a message is generated toproVide new state mfonnatIon concerning the vehicles true position and movement.The logical operation of this process is illustrated in Figure 7.5. Thus, if a vehiclecontinues to do the same thing, the frequency of update messages that aregenerated is greatly reduced.206 DVEs INTRODUCTION 7.5 DEAD RECKONING 207bExtrapolations concerning the orientation of the remote entity can be derived basedon periodic reports of its angular velocity and acceleration.An example illustrating the use of dead reckoning is shown in Figure 7.6a. Thedark solid line denotes the true position of a remote vehicle in twodimensionalspace, as determined by that vehicles highfidelity model. The thin solid linestangent to the true position represents the local simulators estimation of the remotevehicles location, obtained from its local DRM for the remote vehicle. As discussedearlier, the DRM is sampled at regular intervals in order to generate updates to thesimulators display. Each such sample is represented by a square box in the figure,marked A, B, C, and so on. In this example, the local simulator has the correctposition, velocity, and acceleration of the remote vehicle at time t. The DRMpredicts the vehicles position at points A, B, and C based on the information it had at1. Zerothorder DRM Dt  PtJ. In the absence of any additional information,there is no basis for any other choice.2. Firstorder DRM Dt  PtJ  vtjLlt, where yeti is the velocity vector forthe remote entity reported in the update at time ti This is the model used in theexample in Figure 7.4.3. Second order DRM Dt  PtJ  vtJ1t  atJ1t2 where vtJ is thevelocity vector and atJ is the acceleration vector reported in the state updateat time ti display update true position t, state update..... message estimate oftrue positionmodel produces an estimate Dt for times t. Let ti denote the most recent stateupdate earlier than time t, and 1t denote the quantity t  ti Three DRMs come tomind that differ according to the amount of information concerning the remote entitythat is availableFigure 7.6 Example of dead reckoning scheme. a Basic scheme b time compensationc interpolation to smooth updates.simulator foraircraft 2simulator foraircraft 1DRMaircraft 2DRMaircraft 1DRMaircraft 11IIIII1 over thresholdof timeoutentity stateupdate PDUsample DRM atI frame rateHighFidelityModelaircraft 1In DIS, if the DRM continues to produce sufficiently accurate information,update messages do not stop completely. As discussed earlier, the update rate dropsto some minimal frequency, such as once every five seconds, providing a means ofnotifying new simulators joining the virtual environment of other vehicles already inthe environment.An important advantage of the dead reckoning approach is that it providesresilience to lost messages. If a position update is lost, the simulator will continue touse dead reckoning to extrapolate its position although the accuracy may exceed theprescribed threshold until the next update is received. This allows the simulationapplication to utilize best effort message delivery, as opposed to services guaranteeing reliable delivery by retransmitting lost messages which introduce additionallatency, and impose additional overheads on the network.Dead reckoning was first used for distributed simulations in SIMNET. Local stateupdates were performed every 115th second. Messages were generated at approximately a rate of one per second in DVEs using manned vehicles, and slightly higherfor aircraft Miller and Thorpe 1995. Somewhat lower traffic rates are reported inDIS exercises, as will be discussed in the Chapter 8.7.5.1 Dead Reckoning ModelsLet pet denote the true position of a remote entity at time t. Position updates arerCpivcl 1t tim t, to  that indicate Pt.. Pt,. and so on. The dead reckoningFigure 7.5 Logical operation of dead reckoning in DIS between two flight simulators. Thehighfidelity model for aircraft 2 and its generation of state updates are not shown to simplifythe figure.208 DVEs INTRODUCTION17.6 HIGH LEVEL ARCHITECTURE 209time t. Next, the remote vehicle detects that its DRMs error exceeds the threshold,so a state update message is generated at time t2. This information is used the nexttime the DRM is sampled in the local simulator, resulting in the corrected positionpoint D to be displayed. Continuing this example, the remote vehicles position isnext estimated to be at point E.7.5.2 Time CompensationOne problem with the approach described thus far is that it does not take intoaccount the latency to send the state update messages over the network. Thus theposition reported in the message is out of date by the time the remote processorreceives the message. Ifthe processor updates its display based on the contents of themessage, it will be displaying where the vehicle was L time units ago, where L is thelatency to receive and process the message. The error introduced by not taking intoaccount this latency could be significant if the vehicle is moving rapidly, or latenciesare large. In a wide area network the latency could be hundreds of milliseconds, ormore.To address this problem, one can include a time stamp in the state update messageindicating the simulation time28 at which the update was generated. The receiver canthen compensate for the message latency by using the dead reckoning model toproject the vehicles position when the message is received and processed. This timecompensated position information is then used to update the remote simulatorsdisplay.For example, Figure 7.6b illustrates the use of this technique. A state updatemessage is received soon after display update C. Rather than simply using thisposition information for the next display update as was done in Figure 7.6a, thetime stamp of the update message t2  and the new state information in that messageare used to estimate the true position at the time of update D. That is, the position isestimated using t equal to tD  t2, where tD is the time for display update D. Thisresults in position D in Figure 7.6b being displayed at the next screen update.7.5.3 SmoothingA second problem with the approach discussed so far is that when a state updatemessage is received and the DRM is reset to a new, corrected position, the remotevehicle may suddenly jump in the display to a new position. This is ratherapparent in Figure 7.6a with the abrupt transition of the vehicles location fromposition C to position D in successive frames of the display. It is also apparent,though not as severely in Figure 7.6b. Such jumps will make the environment seemunnatural to its participants. Thus it is usually preferable to smooth the sequenceof state updates that are displayed so that the transition to the corrected positionoccurs gradually over time. Smoothing usually reduces the absolute accuracy of the28 Recall that simulation time is essentially synonymous with wallcock time in virtual environments.displayed position relative to the true position immediately after an update messageis received because one imposes a delay before the new information is fully utilized.But this is often preferable to causing sudden jumps in the display.One approach to smoothing the display is shown in Figure 7.5c. When the newupdate message arrives and is first used by the DRM point D in Fig. 7.5c, theDRM extrapolates the anticipated position of the vehicle some time into the future.In Figure 7.5c the DRM estimates the position of the vehicle at point E, the nexttime the display will be updated, using the correction information that was justobtained. It computes the current position by interpolating between the last displayedposition point C and this predicted, future position point E. It then displays thisinterpolated position point D. When the next display time comes, it will show thevehicle at point E, the same as ifno smoothing had been used see Figure 7.6 band c.More generally, this technique can be extended to extrapolate the position Kdisplay update times into the future, and interpolating K intermediate positions thatwould result before reaching this final position. The example shown in Figure 7.6cuses K  1. A larger value of K may result in a smoother transition to thecorrected position. The disadvantage with this approach is that it increases the timeuntil the simulator reaches its best estimate of the remote vehicles location i.e., thelocation indicated by its DRM. Thus the accuracy of the displayed position,averaged over time, may be reduced. This may be irritating to the operator tryingto fire upon a vehicle when the simulator records a miss, even though the usersweapon was aimed directly at the displayed target In any event, interpolation doessmooth the transition to the updated state information and helps to produce a morenatural looking display.7.6 HIGH LEVEL ARCHITECTUREThe discussion thus far concerning DIS has focused on general design principles andmodeling techniques such as dead reckoning to reduce interprocessor communiction. We now examine the underlying support provided by a distributed simulationexecutive to support DVEs such as DIS. Like DIS, the principal goal of the HighLevel Architecture is to support interoperability and reuse of simulations. UnlikeDIS, HLA provides explicit support for simulations other than training. For example,explicit support for synchronizing analytic simulation models is provided.In the HLA a distributed simulation is called a federation, and each individualsimulator is referred to as afederate. A federate need not be a computer simulationit could be an instrumented physical device for example, the guidance system for amissile or a passive data viewer.We next give an historical perspective on the HLA, followed by discussion oftechnical aspects of HLA federations. We are specifically concerned with theinterface to the distributed simulation executive, called the RunTime InfrastructureRT in the HLA, in order to identify the types of services that are provided tosupport DVEs.210 DVEs INTRODUCTION7.6 HIGH LEVEL ARCHITECTURE 2117.6.1 Historical PerspectiveThe roots for the HLA stem from DIS aimed primarily at training simulations andthe Aggregate Level Simulation protocol ALSP which applied the concept ofsimulation interoperability to wargaming simulations. The HLA development beganin October 1993 when the Defense Advanced Research Projects Agency DARPAawarded three industrial contracts to develop a common architecture that couldencompass the DoD modeling and simulation community. The designs werereceived in January 1995 and, with inputs from the DoD community, were combinedto form an initial architecture proposal. In early 1995 the Defense Modeling andSimulation Office DMSO formed a group called the Architecture ManagementGroup AMG which included representatives from several sizable efforts in theDoD in modeling and simulation. The AMG was given the task of overseeing thedevelopment of the HLA. An initial architecture proposal was given to the AMG inMarch 1995, and the HLA began to take form.The AMG formed several technical working groups to develop specific aspects ofthe HLA. These included definition of the Interface Specification and the ObjectModel Template, described below, as well as specific technical aspects such as thetime management services dealing with synchronization issues such as thosediscussed in Part II of this book, and data distribution management for largescaledistributed simulations. At the same time, a team lead by Mitre Corp. was taskedwith developing a prototype implementation of the RTI. This implementation waslater merged with software developed at MIT Lincoln Laboratories to support largescale federations. Several teams were formed to develop initial federations thatwould be representative of the DoD modeling and simulation community. Specifically, four socalled protofederations prototype federations were formed The platform protofederation including DISstyle training simulations that is,realtime humanintheIoop training simulations. The Joint Training protofederation including asfastaspossible timedrivenand eventdriven wargaming simulation models to be used for commandleveltraining. The analysis protofederation including asfastaspossible eventdrivenwargaming simulations such as those that might be used in acquisitiondecisions. The engineering protofederation including hardwareintheIoop simulationswith hard realtime constraints.Protofederation development was largely focused on adapting existing simulations for use in the HLA to verify the claims that such an infrastruture couldsuccessfully support model reuse.Initial implementations of the RTI and protofederations began to appear in late1995, with the AMG meeting approximately every six weeks to monitor progress ofthe HLA development, and to discuss various technical and administrative issues.Final experimentation was completed in June 1996, and the individual protofederations reported their results back to the AMG in July of that year. Based on theseresults, the AMG recommended the baseline HighLevel Architecture that had beendefined to the Executive Council for Modeling and Simulation EXCIMS in Au st1996. The EXCIMS in turn, recommended the architecture to the undersecretaofdefense Acquisition and Technology for approval and standardization. On September 10, 19.96 the undersecretary of defense designated that the HLA become thetandard hIghlevel technical architecture for all modeling and simulation activitiesm the U.S. Departmnt of Defense. This meant that all simulation programs in theDoD ,ould be reqUIred to pass certain procedures to certify that they were HLAcomplIant, or obtain a waiver from this rule. At time of this writing 1999 efforts toefine an IEEE standard for the HLA are in progress. The discussion that followsIS based on version 1.3 of the HLA that was proposed for standardization by theIEEE.7.6.2 Overview of the HLAAny realworld. entity that is visible to more than one federate is represented in theHA by an object. The HLA does not assume the use of objectoriented programm lguaes, however. Each object instance contains 1 an identity thatdIstmgUIshes It from other objects, 2 attributes that indicate those state variablesand parameers of an object that are accessible to other objects, and 3 associationsbetween objec.ts for example, one object is part of another object.The HLA mcludes a nonruntime and a runtime component. The nonruntimecoponent specifies the object model used by the federation. This includes the set ofobjec.t .types chosen to rpresent the real world, the attributes and associations classdefimtIons of these objects, the level of detail at which the objects represent theworl for example, spatial and temporal resolution, and the key models andalgonthms for example, for dead reckoning that are to be used..The runtie component is the RTI that provides facilities for allowing federatesto mteact WIth each other, as well as a means to control and manage the executions.ee FI. 7.7. IndiVidual. federates may be software simulations combat models,fhgh sImulatrs, etc., lIve components for example, an instrumented tank, orpassve data VIewers. The .RTI can be viewed as a distributed operating system thatprovIde the software enVIronment necessary to interconnect cooperating federates.It povIds several categories of services, as described below. The interfacespecIficaton defines a standard et of services provided by the RTI, and applicationprogram mterfaes APIs for dIfferent programming languages.The state vanables for the federation are stored within the federates rather thanthe RTI. The RTI has no knowledge of the semantics of the informatio th t t ..  Th n a 1 IStransmI.ttmg. IS IS .necesary to make the RTI general purpose, not tied to detailedseatIc of the SImuatIOn model. This means that the RTI cannot includeoptImIatlOs that reqUIre knowledge of the semantics of the simulation. Deadreckonmg IS an example of one such optimization. Techniques such as this that212 DVEs INTRODUCTION,7.6 HIGH LEVEL ARCHITECTURE 213Figure 7.7 Components in an HLA federation.RunTime Infrastructurea distributed operating system providingdistributed simulation servicesFederates Federatessoftware simulators live componentsFederatesdata viewersMore precisely, HLA consist of three components1. The HLA rules that define the underlying design principles used in the HLA.2. An object model template OMT that specifies a common format fordescribing the information of common interest to the simulations in thefederation.3. An interface specification that defines the services provided by the RunTimeInfrastructure RTI for linking together individual federates.Each of these is described next.7.6.3 HLA RulesThe HLA rules summarize the key principles behind the HLA. The rules defined atthe time of this writing are enumerated in Table 7.1. These principles were discussedin the previous section, so we do not elaborate upon them further.require knowledge of the semantics of the simulation must be implemented withinthe federates.Each object attribute has an owner that is responsible for notifying the RTIwhenever the value of the attribute has been modified. An attribute can have at mostone owner at any time. Federates that have indicated interest in the attribute arenotified of such changes via attribute reflections. Reflections are implemented ascalls from the RTI to the federate indicating which attributes have been modified andtheir new values. Different attributes for a single object can be owned by differentfederates. At any instant there can be at most one owner of an attribute however,ownership of the attribute may pass from one federate to another during anexecution. Any number of other federates may subscribe to receive updates toattributes as they are produced by the owner.Federates may interact with each other by modifying object attributes that arethen reflected to other federates that have expressed an interest in that object. Asecond way federates may interact is through HLA interactions. Interactions are usedto model instantaneous events not directly associated with an object attribute. Forexample, weapon exchanges between combating units will be typically modeledusing interactions rather than updates to attributes.Each simulation must define a simulation object model SOM that identifies theobjects used to model realworld entities in the simulation and specifies the publicattributes, the attributes whose ownership may be transferred, and those attributeswhose value may be received and processed. Using the SOMs for the simulationsthat are included in a particular federation, a federation object model FOM mustthen be developed that describes the common object model used by all simulationsin the federation. The FOM specifies all shared information objects, attributes,associations, and interactions for a particular federation. The HLA includes anobject model template OMT to provide a standard, tabular format for specifyingobjects, attributes, and the relationships among them.7.6.4 Object Models and the Object Model TemplateIn any DVE composed of interacting, autonomous simulators, some means isrequired to specify aspects of the virtual world that are visible to more than onesimulation. In the HLA, object models are used for this purpose because theyTABLE 7.1 HLA rules1. Federations shalll have an HLA Federation Object Model FOM documented inaccordance with the HLA Object Model Template.2. In a federation, all simulationassociated object instance representation shall be in thefederates, not in the runtime infrastructure RTI.3. During a federation execution, all exchange ofFOM data among federates shall occur viathe RTI.4. During a federation execution, federates shall interact with the RTI in accordance withthe HLA Interface Specification IFSpec.5. During a federation execution, an attribute of an instance of an object shall be owned byat most one federate at any time.6. Federates shall have an HLA Simulation Object Model SOM, documented inaccordance with the HLA Object Model Template OMT.7. Federates shall be able to update andor reflect any attributes of objects in their SOMsand send andor receive SOM interactions externally as specified in their SOMS.8. Federates shall be able to transfer andor accept ownership of attributes dynamicallyduring a federation execution, as specified in their SOMs.9. Federates shall be able to vary the conditions e.g., thresholds under which they provideupdates of attributes of objects, as specified in their SOMs.10. Federates shall be able to manage local time in a way that will allow them to coordinatedata exchange with other members of a federation.Source Defense Modeling and Simulation Office. 214 DVEs INTRODUCTION 7.6 HIGH LEVEL ARCHITECTURE 215provide a convenient means of describing the real world. Each simulator advertises its capabilities with respect to the object attributes it can update and receivevia its simulation object model SOM. The federation object model FOM specifiesthe common objects used by the constituent simulators participating in a federationexecution. Object models in the HLA are documented using a set of tables that arecollectively referred to as the object model template OMT. The OMT describeswhat is actually meant by an object model in the HLA, so the remainder of thissection focuses on the information specified in the OMT.The OMT specifies the class hierarchy for objects and interactions, and detailsconcerning the type and semantics of object attributes. It is used to specify bothSOMs and FOMs. Class definitions are similar to those used in object orienteddesign methodologies, as discussed in Chapter 2, but differ in several respects. Inparticular, HLA object models only specify object classes and properties of theirattributes. They do not specify the methods or operations for manipulating theclasses. In objectoriented programming the attributes for an instance of an objectare usually encapsulated and stored in one location, while in the HLA attributes for asingle object instance may be distributed across different federates. Moreover HLAobject models are used for defining federations of simulations rather than anapproach to software development.The OMT version 1.3 consists of the following tables The object model identification table provides general information concerningthe FOM or SOM such as its name, purpose, version, and pointofcontactinformation. The object class structure table specifies the class hierarchy of objects in theSOMFOM. The attribute table enumerates the type and characteristics of object attributes. The interaction class structure table defines the class hierarchy defining thetypes of interactions in the SOMFOM. The parameter table defines types and characteristics of interaction parameters. The routing space table specifies the nature and meaning of constructs calledrouting spaces that are used to efficiently distribute data throughout largefederations routing spaces and data distribution management are discussed indetail in Chapter 8. The FOMSOM lexicon is used to describe the meaning ofall terms used in theother tables, such as the meanings of object classes, attributes, interactionclasses, and parameters.The object and interaction class structure tables and their associated attribute andparameter tables form the heart of the object model template. These are described ingreater detail next.Object Class Structure Table The object class structure table enumerates theclasses used in the object model, and specifies classsubclass relationships. Classnames are globally unique character strings. Subclasses can be defined that arespecializations or refinements of the parent class called the superclass. A subclassinherits the attributes and interactions of its superclass. For example, Table 7.2shows a SOM for a hypothetical World War II flight simulator. Two classes aredefined, one for moving vehicles, and one for bases. Three subclasses for the Vehicleclass are defined, to distinguish between air, sea, and land vehicles. The Aircraftsubclass is further refined into fixed wing aircraft and zeppelins, and fixed wingaircraft are refined further to include specific aircraft types an American P51Mustang, a British Spitfire, and a German Messerschmitt MEI09. The aircraftsubclass inherits the position attributes shown in another table, as will be discussedmomentarily from the Vehicle superclass.In the HLA OMT a class can have only a single parent class single inheritance.Multiple inheritance is not supported at present. Thus the class hierarchy forms a setof trees a forest, with the classes at the highest level i.e., the first column in theobject class structure table defining the roots of the trees.During the execution of the simulation each federate must be notified whenobjects visible to that federate change, such as when another aircraft beingdisplayed by a flight simulator turns to move in a new direction. One mechanismprovided in the HLA for this purpose is called declaration mangement whichTABLE 7.2 Example object class structure tableVehicle PS Aircraft PS Fixed Wing PS P51 Mustang SSpitfire PSME109 SZeppelin SNaval vessle S Submarine S Vboat SBattleship S Iowa class SAircraft carrier SLand vehicle S Tank S Panzer SSherman SJeep SBase S Air field PSSea port SArmy post S216 DVEs INTRODUCTION 7.6 HIGH LEVEL ARCHITECTURE 217implements classbased subscription. Specifically, a federate can subscribe toattributes of an object class, indicating it wishes to receive notification wheneverthat attribute of any object instance of that class is modified. For example, if afederate subscribes to the position attribute of the Spitfire class, it will receive amessage for any update to the position attribute of any Spitfire object in thefederation. Note that classbased subscription allows a federate to track changes toall spitfire objects. The federate must discard updates corresponding to spitfires thatare not of interest to it, such as that are too far away to be visible to its pilot. A moresophisticated mechanism by which the RTI automatically eliminates these messageswill be discussed in Chapter 8.A federate may subscribe at any point in the class hierarchy. Attributes of a classare inherited by the subclasses of that class. Subscribing to an attribute ofa class at acertain level in the hierarchy automatically subscribes the federate to that attribute asit is inherited by all the subclasses of that class. For example, in Table 7.2, asubscription to the position attribute of the Fixed Wing class will result insubscription to the position attribute of the P51, Spitfire, or MEI09 subclasses.If subclasses are later added, such as a B17 subclass of the Fixed Wing class toinclude bombers in the exercise, federates subscribed to the Fixed Wing class neednot change their subscription to also receive position updates for this new type ofaircraft.The object class structure table also indicates a federates or federations, in thecase of FOMs ability to publish update the value of attributes of an object class, orto subscribe to and thus receive and react to attribute updates. This is representedby P or S flags with each class.29 Table 7.2, for instance, shows publication andsubscription information for a flight simulator modeling a Spitfire. It is able toupdate attributes of the Spitfire class, and can process updates to attributes of otherclasses that it uses to update the display shown to the Spitfires pilot.Attribute Table Each object class includes a fixed set of attribute types.Attributes specify portions of the object state shared among federates during theexecution. The object model template represents the following characteristics ofattributes corresponding to columns in the attribute table Object class. This indicates the class in the object class structure table, such asVehicle, and can be chosen from any level in the class hierarchy. Attribute name. This specifies the name of the attribute, such as position. Data type. This field specifies the type of the attribute and is not unlike typespecifications in conventional programming languages. For basic data types,this field specifies one of a standard set of base data types, such as integer orfloating point. Alternatively, userdefined data types can be specified. Recordscan be defined including multiple fields, such as the coordinate position andaltitude of an aircraft. In this case individual fields are specified in a complexdatatype table that includes information similar to the attribute table in that29 It is possible to define classes that are neither subscribable nor publishable. A federate may not be ableto publish or subscribe to the object class, but it can publish or subscribe to a subclass of that object class.units, accuracy, and the like, are specified. Enumerated types can also bedefined, in which case another table is used to specify possible values and theirrepresentation. Cardinality. This field is used to record the size of an array or sequence. Units. This specifies the units for numeric values, such as meters or kilograms.This and the resolution and accuracy fields do not apply to complex andenumerated types. Resolution. This indicates the smallest difference between published values ofthe attribute. For example, if altitude is recorded to the nearest 100 meters, thenthe resolution is 100 meters. Accuracy. This indicates the maximum deviation of the attributes value fromthe intended value for example, recall the use of thresholds in dead reckoningschemes. This may be specified as perfect to indicate that there is nodeviation between published values and the intended value. Note that thisparameter does not specify the accuracy of the simulation model, only howclose published values are to the true simulated value which mayor may notbe very accurate to realworld systems. Accuracy condition. This allows one to specify conditions under which theaccuracy specification applies. This may be specified as always to indicate theaccuracy specification always applies. Update type. This indicates the policy used for updating the attribute. This isspecified as static meaning it is not updated, periodic if it is updated atregular intervals, or conditional if it is updated when certain conditions occur,such as when a dead reckoning threshold is exceeded. Update condition. This elaborates on the update type field. For example, ifupdates are periodic, it specifies the rate. Transferableacceptable. This indicates whether the federate is able to transferaccept ownership of the attribute tofrom another federate. Updatereflect. This indicates whether the federate is able to updatereflect theattribute value.Interaction Class Structure Table Interactions are actions taken by onefederate that have an affect on another federates. A class hierarchy similar tothat used for objects is used to document interaction types. For example, in Table 7.3TABLE 7.3 Example interaction class structure tableShoot Aircraft cannon lRBomb IS .Torpedo SAntiaircraft R218 DVEs INTRODUCTION 7.6 HIGH LEVEL ARCHITECTURE 219a shoot interaction is defined. Four subclasses of the interaction are specifiedcorresponding to cannon fire, dropping a bomb, launching a torpedo, and antiaircraft fire.Entries in the interaction class structure table are annotated to indicate whetherthe federate can initiate I, react R, or sense S interactions. A federate can initiatean interaction if it is able to send that interaction class with appropriate parameters.For example, in Table 7.3 the Spitfire simulator can generate cannon fire and dropbombs but cannot fire a torpedo or generate antiaircraft fire. A federate can react tointeractions, if it can receive them, and produce appropriate changes in state forexample, it can generate appropriate attribute updates in response to the interaction.Table 7.3 indicates the Spitfire can react to cannon fire and antiaircraft interactions,such as by producing damage and generating state updates to designate damage ordestruction of the aircraft. The third category, sense, indicates that the object canreceive interactions, but cannot produce suitable reactions to the interaction. In thisexample, the Spitfire can sense torpedo launches and bombs dropped by otheraircraft and update its local display, if necessary, but it cannot and should not createattribute updates as the result of these interactions. Observers in the battlefieldcalled Stealth viewers similarly can receive interactions and display them butcannot generate suitable actions from the interaction. The rules concerning inheritance of classes in interaction class structure tables are essentially the same as thoseof the object class structure table.Parameter Table Just as attributes provide state information for objects, parameters provide state information for interactions. Parameter types are specified in theparameter table. The parameter table includes many fields that are similar to attributetables. Specifically, it includes columns to specify the interaction class, parametername, and the data type, cardinality, units, resolution, accuracy, and accuracycondition for parameters. Their meaning is similar to that specified in the attributetable. The other entries in the attribute table that are not included in the parametertable concern state updates and ownership transfer, which do not apply to interactions.7.6.5 Interface SpecificationIn the HLA there is a clear separation between the functionality of individualfederates and the RTI. For example, all knowledge concerning the semantics andbehavior of the physical system being modeled is within the federate. Further theactual state of the simulation model also resides within the federate. This allows theRTI to be a general software component that is applicable to any federation. The RTIcould be used for general i.e., nondefense distributed virtual environments,although its functionality was derived primarily from requirements originatingfrom the defense modeling and simulation community.The interface specification defines a set of services provided by simulations or bythe RunTime Infrastructure RTI during a federation execution. HLA runtimeservices fall into the following categories Federation management. This includes services to create and delete federationexecutions, to allow simulations to join or resign from existing federations, andto pause, checkpoint, and resume a federation execution. Declaration management. These services provide the means for simulations toestablish their intent to publish object attributes and interactions, and tosubscribe to updates and interactions produced by other simulations. Object management. These services allow simulations to create and deleteobject instances, and to produce and receive individual attribute updates andinteractions. Ownership management. These services enable the transfer of ownership ofobject attributes during the federation execution recall that only one federate isallowed to modify the attributes of an object instance at any time. Time management. These services coordinate the advancement of logical time,and its relationship to wallclock time during the federation execution. Data distribution management. These services control the distribution of stateupdates and interactions among federates, in order to control the distribution ofinformation so that federates receive all of the information relevant to it andideally no other information.7.6.6 Typical Federation ExecutionA typical federation execution begins with the invocation of federation managementservices to initialize the execution. Specifically, the Create Federation Executionservice is used to start the execution, and each federate joins the federation via theJoin Federation Execution service.Each federate will then use the declaration management services to specify whatinformation it can provide to the federation, and what information it is interested inreceiving. The Publish Object Class service is invoked by the federate to indicate itis able to provide new values for the state of objects of a specific class, such as theposition of vehicles modeled by the federate. Conversely, the Subscribe Object ClassAttribute service indicates the federate is to receive all updates for objects of acertain class, such as the altitude attribute of all aircraft objects. Federates may usethe data distribution management services to further qualify these subscriptions,such as to say that the federate is only interested in aircraft flying at an altitudegreater than 1000 feet. The federate may then inform the RTI of specific instances ofobjects stored within it via the Register Object Instance object managementservice.After the execution has been initialized, each federate models the behavior of theentities for which it is responsible. The federate may interact with other federatesthrough object management services. The federate notifies other federates via theRTI of changes in the state of objects under its control vja the Update AttributeValues object management service. This will cause the RTI to generate messages forall other federates that have expressed interest in receiving these updates via theirsubscriptions. Alternatively, the federate may issue interactions with other federates220 DVEs INTRODUCTION 1 7.8 ADDITIONAL READINGS 221that are not associated with modifying the state of an object via the Send Interactionservice. This might be used to exchange weapons fire, for example. As objects areregistered and become visible to federates that have subscribed to their attributevalues, the RTI notifies the federate of the existence of these objects via the DiscoverObject Instance service. Similarly this service is invoked as objects come withinrange. For example, this service might be invoked as an aircraft reaches an altitudeof 1000 feet in the previous example, or as new vehicles come within view of a tanksimulator, causing the tank simulator to display the vehicle to the user, and to trackand update its position.Throughout the execution, analytic simulations invoke the time managementservices to ensure that events are processed in time stamp order and to advancesimulation time. For example, the Next Event Request service requests the nextsmallest timestamped event from the RTI and also causes the federates simulationtime to advance.As mentioned earlier, an object can be updated by at most one federate, called theobjects owner. A handoffprotocol is provided in the RTI to transfer ownership fromone federate to another. For example, a missile launch might be controlled by thearmys simulation, but the trajectory of the missiles flight may be controlled by theair forces simulation. A federate can invoke the Negotiated Attribute OwnershipDivestiture service to begin the process of transferring ownership to anotherfederate. The RTI invokes the Request Attribute Ownership Assumption service inother federates to identify a federate that will take over ownership of the attribute.The federate taking on ownership responds by calling the Attribute OwnershipAcquisition service. The RTI confirms the ownership transfer to both parties byinvoking the Attribute Ownership Divestiture Notification and Attribute OwnershipAcquisition Notification services in the federates giving up, and receiving ownership,respectively.Finally, at the end of the execution, each federate invokes the Resign FederationExecution service. The execution is terminated by invoking the Destroy FederationExecution service.7.7 SUMMARYThis chapter provided a birds eye view of distributed simulations for DVEs. DIS isillustrative of a typical DVE. Although designed for military applications, itintroduces design principles such as autonomy of simulation nodes and deadreckoning that apply to commercial applications, particularly those requiring manysimulation nodes. A noteworthy observation in this discussion is that DVEs mustappear realistic to participants embedded in the simulation environment, sometimesat the expense of absolute accuracy. Smoothing techniques used ill conjunction withdead reckoning algorithms is one example. This is an important distinction betweendistributed simulations for DVEs and for analytic analysis.While the discussion of DIS provides insight into the operation of simulatornodes in a DVE, the discussion of the High Level Architecture provides insight intothe interface between simulators, and between simulators and the runtime infrastructure. In particular, the object model template provides some insight into the kindof information that simulations must agree upon in order to interoperate. The HLAinterface specification provides an overview of the types of services one mightexpect to find in an RTI supporting DVEs. The chapters that follow focus onrealization of RTI services.7.8 ADDITIONAL READINGSAn overview of the SIMNET project by two of its principal architects is presented inMiller and Thorpe 1995. The DIS vision and underlying design principles arepresented in DIS Steering Committee 1994. More detailed discussions arepresented in Goldiez, Smith et al. 1993 and Hofer and Loper 1995, and itsrelationship to modeling and simulation in the Department ofDefense is described inDavis 1995. Specifics concerning the DIS protocols, PDUs, formats, and the like,are described in IEEE standards documents IEEE Std 1278.11995 1995 IEEE Std1278.21995 1995. These references also include a description of the deadreckoning protocol used in DIS. An alternative approach to dead reckoning basedon utilizing past position measurements to project current positions of remoteobjects is described in Singhal and Cheriton 1994.Detailed documentation concerning the High Level Architecture is available fromthe Defense Modeling and Simulation Office DMSO through their Internet website httphla.dmso.mil. This site includes documents concerning the HLA rulesDefense Modeling and Simulation Office 1996, object model template DefenseModeling and Simulation Office 1996, and interface specification Defense Modeling and Simulation Office 1998. An introduction to the HLA is presented inDahmann, Fujimoto et al. 1997.1CHAPTER 8Networking and Data DistributionThe network plays a critical role in distributed simulation systems. Simulatortosimulator message latency can have a large impact on the realism of the virtualenvironment. One hundred to 300 milliseconds are considered adequate for mostapplications, though for some, latencies as low as a few tens of milliseconds may berequired. Aggregate network bandwidth, the amount of data transmitted through thenetwork per unit time, is also important because largescale distributed simulationscan easily generate enormous amounts of data. Thus it is important both to achievemaximal bandwidth and minimal latency in the network itself, as well as to controlthe amount of data that must be transmitted and still provide each participant aconsistent view of the virtual world.8.1 MESSAGEPASSING SERVICESAll networks provide the ability to transmit messages between simulators. Havingsaid that, there are a number of different characteristics that differentiate differenttypes of network services. Some of the more important characteristics for DVEs areenumerated below. Simple DVEs may utilize only one type of communicationservice. Other DVEs, especially ones designed to handle a large number ofparticipants, typically utilize a variety of different services.8.1.1 Reliable DeliveryA reliable message delivery system is one where the service guarantees that eachmessage will be received at the specified destinations, or an exception will beraised for any message that cannot be delivered. Typically this means the networkingsoftware will retransmit messages if it cannot determine for example, via anacknowledgment message that the message has been successfully received. Anunreliable or besteffort delivery service does not provide such a guarantee.For many types of information, such as periodically generated position updatemessages, unreliable communication services are often preferred because theynormally incur less latency and overhead than reliable services. An occasional lostmessage can be tolerated because a new update will be sent before long anyway. On22224 NETWORKING AND DATA DISTRIBUTION18.1 MESSAGEPASSING SERVICES 225the other hand, reliable delivery is preferred in other cases, such as if a lost messagecauses a simulator to lock up, waiting for the lost message to arrive, or to enter aninconsistent state.8.1.2 Message OrderingAn ordered delivery service guarantees that successive messages sent from onesimulator to another will be received in the same order in which they were sent. Thisproperty may or may not be provided with the network delivery service. Forexample, if successive messages are routed along different paths through thenetwork, they may not arrive in the same order that they were sent. On the otherhand, if the network transmits messages along a single path and ensures that amessage cannot jump ahead of other messages in message queues, ordered deliveryis ensured. If the network does not provide ordered delivery and this property isrequired, the processor receiving the messages will have to ensure that they arecorrectly ordered before delivering them to the receiver. This is often accomplishedby attaching a sequence number to each message and making sure the destinationreorders messages according to the sequence number. As discussed later, thiscapability is often realized within the network communication software.More sophisticated message ordering is sometimes important in DVEs, extendingwell beyond the ordering of messages sent between each pair of processors. Certainanomalies such as the effect of some action appearing to happen before the causemay occur unless precautions are taken. This will be discussed in greater detail inChapter 9.8.1.3 ConnectionOriented versus Connectionless CommunicationIn a connectionless also called a datagram service, the sender places the data itwishes to transmit into a message, and passes the message to the networkingsoftware for transmission to the destinations. This is analogous to mailing a letterthrough the postal system. In a connectionoriented service, the sender must firstestablish a connection with the destination before data can be sent. After the datahave all been sent, the connection must be terminated. Connectionoriented servicesare analogous to telephone services.A connectionless service is often preferred for much of the informationtransmitted among simulators, such as for state updates, because of its simplicity.On the other hand, a connectionoriented service is better for streams of information,such as an audio or video channel established between a pair of simulators.8.1.4 Unicast versus Group CommunicationA unicast service sends each message to a single destination. A broadcast servicesends a copy of the message to all possible destinations, while a multicastmechanism sends it to more than one, but not necessarily all, destinations. Multicastcommunication is important when realizing largescale simulations. For example, atypical operation is a player needs to send a state update message to the other playersthat can see it. More will be said about how multicast can be used in DVEs later inthis chapter.8.1.5 Examples DIS and NPSNetThe communication requirements in the DIS standard 1278.2 specify three classes ofcommunication services, based in part on existing networking products1. Besteffort multicast2. Besteffort unicast3. Reliable unicastThese communication services are available in the Internet Protocol suite, discussedlater in this chapter. There, besteffort communication is achieved using theconnectionless User Datagram Protocol UDP and reliable communication isachieved using the Transmission Control Protocol TCP which provides a connectionoriented service.Conspicuously missing from the DIS communication services is reliable multicast. Reliable multicast is considered important in many DVEs, but when the DISprotocols were defined, realization of this service was not sufficiently mature toenable inclusion in the DIS standard. At the time of this writing in the late 1990s,realization of reliable multicast services is still an active area of research in thenetworking community.In DIS besteffort communication is typically used for entity state PDUsESPDUs. Loss of ESPDUs can be tolerated so long as they are not too frequentbecause new updates are generated at regular intervals. These PDUs make up thelions share of communications in typical DIS exercises one demonstration exercisereports 96 of the total DIS traffic was ESPDUs Cheung 1994. Reliablecommunications are often used for nomecurring events where losses are moreproblematic, such as fire and detonation PDUs.A second example illustrating the communication services typically used in largeDVEs is NPSNet. NPSNet is a project at the Naval Post Graduate School aimed atdeveloping largescale virtual environments containing thousands of users. TheNPSNet communications architecture is described in Macedonia, Zyda et al. 1995.It provides four classes of communications1. Light weight interaction. This service provides a besteffort, connectionlessgroup communication facility. State updates, interactions, and controlmessages will typically use this service. Each message must be completelycontained in a maximum transfer unit MTU, sized at 1500 bytes for Ethernetand 296 bytes for 9600 bitssecond pointtopoint links. It is implementedwith a multicast communication mechanism.226 NETWORKING AND DATA DISTRIBUTION8.3 NETWORKING TECHNOLOGIES 2272. Network pointers. These provide references to resources, analogous to links inthe World Wide Web. They are somewhat similar to lightweight interactionsfor example, both use multicast but contain references to objects, whilelightweight interactions contain the objects themselves.3. Heavy weight objects. These provide a reliable, connectionoriented communication service.4. Realtime streams. These provide realtime delivery of continuous streams ofdata, as well as sequencing and synchronization of streams. They are intendedfor transmission of video or audio streams.8.2 NETWORKING REQUIREMENTSThe success of a DVE, especially largescale DVEs containing many participantsoften depends critically on the performance of the underlying network infrastructure.For example, networking requirements for DIS are described in IEEE Std 1278.21995 1995. Specific requirements include the following Low latency. As discussed in the previous chapter, the DIS standard calls forsimulatortosimulator latencies of under 300 milliseconds for most termedloosely coupled interactions, and 100 millisecond latency for tightly coupledinteractions, such as where a user is closely scrutinizing the actions of anotherparticipant in the DVE. Lowlatency variance. The variance of delay between successive messages sentfrom one processor to another is referred to as jitter. Low jitter may be requiredin a DVE to maintain a realistic depiction of the behavior of remote entities.Jitter requirements can be alleviated somewhat by buffering messages at thereceiver to smooth fluctuations in latency. Dead reckoning also provides someresilience to delay variance. The DIS standard specifies a maximum dispersionof arrival times for a sequence of PDUs carrying voice information to be 50milliseconds. Reliable delivery. As discussed previously, besteffort communication issufficient for much of the traffic generated in a DVE, but reliable delivery isimportant for certain, nonperiodic events. The DIS standard calls for 98 ofthe PDUs for tightly coupled interactions and 95 for loosely coupledinteractions to be delivered within the specified latency requirement. Group communication. Group communication services are important formoderate to large DVEs, especially in geographically distributed systemswhere broadcast communication facilities are not readily available. A multicastgroup is the set of destinations that is to receive messages sent to the group,analogous to the set of subscribers to an Internet newsgroup. Large DVEspresent a particularly challenging application for multicast protocols becausethey may require group communication services that support the following1. Large numbers of groups for example, thousands.2. A single simulator to belong to many different groups at one time.3. Frequent changes to group composition that must be realized quickly forexample, within a few milliseconds so that simulators do not missimportant information.Data distribution techniques are described later in this chapter, providing someinsight into the usage of group communication services in largescale DVEs. Security. Certain applications such as defense simulations have securityrequirements for example, to prevent eavesdropping on classified information. Sensitive data must be encrypted at the source and decrypted at thedestination. This of course affects the latency requirements, which mustinclude the time to perform such operations.8.3 NETWORKING TECHNOLOGIESAs discussed in Chapter 1, networks may be broadly categorized as local areanetworks LANs covering a limited physical extent for example, a building or acampus, metropolitan area networks MANs covering a city, and wide areanetworks WANs that can extend across continents. Unlike parallel computingplatforms which typically use proprietary interconnection networks, the distributedcomputers typically used to implement DVEs have standard networking technologiesfor interconnecting machines.In general, LANs provide a much friendlier environment for distributedsimulation systems than WANs MANs are intermediate between these two.LANs often operate at link bandwidths, ranging from many megabits per secondfor example, Ethernet operates at 10 MBits per second up to a gigabit per second ormore, and can achieve applicationtoapplication latencies of a few milliseconds orless. They typically have error rates ranging from 108 to 1012 . By contrast, links inWANs typically range in bandwidth from thousands ofbits per second up to megabitper second, and they often incur latencies on the order ofhundreds of milliseconds ormore. Error rates are on the order of 105 to 107 . Bandwidth is important indistributed virtual environments because the different computers executing thedistributed simulation must perceive a common state of the virtual world, and asubstantial amount of interprocessor communication is required to keep this worldconsistent among the different participants.8.3.1 LAN TechnologiesHistorically LAN interconnects have typically been based on either a shared bus or aring interconnection scheme see Fig. 8.1. A recent phenomenon is the appearanceof switched LANs, such as those based on Ethernet or asynchronous transfer mode228 NETWORKING AND DATA DISTRIBUTION 8.3 NETWORKING TECHNOLOGIES 229Figure 8.1 Typical LAN topologies. a Busbased network b ring network.ATM switching technology. These are increasing in popularity, especially forapplications requiring high bandwidth. Bus and ring topologies date back to the1970s and have been in widespread use since then. Each is discussed next.BusBased Networks In a busbased system each computer connects directlyto the bus via a tap. Actually, as shown in Figure 8.1a, the bus may consist of manybus segments that are interconnected by a circuit, called a repeater, that copies thesignal from one segment of the bus to another, amplifYing the signal to compensatefor attenuation. Each message is broadcast over the bus. Processors connected to thebus must check the address of the message against their local address and read themessage into their local memory if there is a match.A complication arises concerning access to the bus. If more than one processorsimultaneously places data onto the bus, the data will become garbled, and neithermessage will get through. This is not unlike two people trying to talk at the sametime during a conversation. Thus it would be prudent to check that the bus is idlebefore beginning to transmit. However, suppose that while the bus is in use by someprocessor, two other processors decide they want to send a message. Both will waituntil the bus becomes idle, then both will simultaneously begin transmitting. Thiswill result in a collision that is, the data on the bus will be garbled.To address this problem, a Medium Access Control MAC protocol is required. Awellknown example of a MAC protocol is the exponential backoff algorithm usedin Ethernet. Each processor sending a message can detect that a collision hasoccurred by noticing that the data on the bus does not match the data it is sending.When this occurs, the processor stops transmitting and puts a jamming signal on thebus to make sure that all other processors have recognized that there is a collision.The processor then becomes idle for a certain, randomly selected amount of timeranging from 0 to K  1 time periods, and it repeats the process by waiting until thebus becomes free before retransmitting the message. In Ethernet, K is initialized to 2for each message but is doubled with each successive collision in trying to send thatmessage. To prevent excessively long delays because K has become too large, theexponential growth is often stopped when it has reached a maximum value. At thispoint the retransmission interval is based on a random number drawn between 0 andKmax , where Kmax is maximum value assigned to K. This retransmission algorithm isreferred to as the truncated binary exponential backofJ algorithm. The EthernetMAC protocol is an example of what is commonly referred to as a Carrier SenseMultiple Access with Collision Detection CSMAjCD protocol.Rings Here, the processors are organized as a ring using pointtopoint communication links rather than a shared bus between pairs of processors, as shown inFigure 8.1b. Data circulate in one direction on the ring. When a message passesthrough a processor, the messages destination address is compared with that of theprocessor. If there is a match, the processor retains a copy of the message. In somerings the destination processor removes the message from the ring. In others, themessage is allowed to circulate around the ring, in which case the source isresponsible for removing the message. One node of the ring is designated as amonitor that prevents messages from cycling indefinitely around the ring, in case theprocessor responsible for removing the message has failed, as well as to performother housekeeping functions. Some rings include a means to bypass a node, andjoradditional links to protect against link failures which would otherwise cause theentire network to fail.Like busbased systems, a MAC protocol is needed to control access to the ring.Token ring and slotting ring protocols are most commonly used for ring networks.In a token ring, a special message, called the token, circulates around the ring.The token contains a flag indicating whether it is free or busy. A processor can onlytransmit a message after it has received the token, and the tokens flag is set to free.The processor then sets the tokens flag to busy, attaches its message, and sends thetoken and message around the ring. The destination processor removes the messagefrom the network and forwards only the busy token around the ring. The processorthat sent the message will eventually receive the busy token, mark it free, and thenforward the token on the ring to allow another processor to send a message. Themonitor processor is responsible for ensuring that the token is not lost or duplicated.Unlike the CSMAjCD protocol used in Ethernet, one can bound the time to accessthe media using a token ring protocol. The FiberDistributed Data Interface FDDIis an example of a networking technology based on token rings.In a slotted ring protocol, a fixed number of message slots continually cyclearound the ring. Each slot is marked either full or empty. To send a message, thesource processor first breaks the message up into pieces of size equal to the slot size.It then waits for an empty slot for each piece of the message it wants to send. Whenan empty slot occurs, it marks the slot full and inserts its message. Each receiverexamines each slot to see if it contains a message destined for it. If it does, it marksthe slot to indicate the message has been received and forwards the slot down theba230 NETWORKING AND DATA DISTRIBUTION 8.3 NETWORKING TECHNOLOGIES 231Figure 8.2 Switched LAN. a Typical topology b fourinput, fouroutput switch.Historically communications in wide area networks have evolved from two distinctcamps, the telephone industry and the computer data communications industry. Invoice communication using telephones, the sound produced by the speaker isconverted to an electrical signal that is then sampled at regular intervals. Eachsample is transmitted through the network and converted back to sound at thereceiver. Telephone voice traffic requires the network to provide a steady stream ofvoice samples flowing from the speaker to the listener with a minimal amount oflatency. Ideally there will be little or no variance in the time between successivevoice samples recall that this is referred to as jitter, since this could cause unwanteddistortions in the resulting audio signal.To accommodate these requirements, telephoneswitching networks usually use atechnique called circuit switching. Circuit switching is a connectionorientedapproach where a path or circuit is established from the source to the destinationbefore data can be transmitted. The circuit is established while the caller dials thetelephone number. In this respect it is similar to connections in ATM networks.There is an important difference, however. In circuit switching, link bandwidth isallocated to each circuit using the link, regardless of whether or not the circuit hasany data to transmit. Because bandwidth is preallocated when the call is set up, thisregional or area code portion of a telephone number, and the VCI to the localphone number within the region. This allows a twolevel routing scheme whererouting by longdistance switches can be done using only the VPI field withouthaving to examine the VCI field, and routing within the region can be done using theVCI field.A fourinput, fouroutput switch is depicted in Figure 8.2b. The switch containstables that indicate for each input port and channel which output port and channel areused in the next hop of the connection. When a cell arrives, its incoming portnumber, VPI, and CPI are used to look up the outgoing port and channel number.The cell is routed through the internal switch fabric to the output link, and is sent onthat link with the new VPI and VCI to identify the channel that is being used. Theswitch fabric contains a switching circuit, such as a crossbar switch, that allowssimultaneous transmission of cells from different input links to different output links.If cells arriving simultaneously on two different input links need to use the sameoutput link, one is transmitted on that link and the other must be stored in a buffer.The latter is transmitted on the link at a later time. If the queue becomes full, the cellis dropped. If a reliable transmission service is provided, the host processor sendingthe cell must retransmit it.Switched LANs are more expensive than bus or ring networks because of theneed for switching hardware, compared to the cables and taps required in Ethernet.Switched LANs can provide higher bandwidth in applications where the busringbecomes a bottleneck, so they are becoming more widely used where highbandwidth communications are required.8.3.2 WAN Technologiesbswitchfabricaring. When the slot completes a roundtrip and arrives back at the sender, the senderprocessor marks the slot empty and forwards it around the ring.Switched LANs Switched LANs are networks based on Ninput, Noutputswitches. Recently these have seen greater use, especially with the advent ofasynchronous transfer mode ATM technology which has resulted in severalcommercial products. A small switched LAN network topology is shown inFigure 8.2a. Each link in this figure is bidirectional it can carry traffic in bothdirections.ATM networks use a connectionoriented approach where a path through thenetwork i.e., an ATM connection, must first be established before sending data. Atany instant several connections may be mapped to a single physical communicationlink. For example, in Figure 8.2a the connection from host A to host B and theconnection from X to Y both use the link from switch SI to switch S2. The networkmust be able to distinguish between data traveling on these two different connectionsso that S2 can route incoming data to the appropriate destination. This isaccomplished by defining a link as containing a number of virtual channels andby assigning each connection using the link a different channel number. Aconnection can be viewed as a sequence of channels from the source to thedestination processor. For example, in Figure 8.2a, the connection from A to Bmight use channel I to reach switch SI, then channel 3 to reach S2, and channel 4 toreach host B. The virtual circuit from X to Y might use channel 5 on the link from SIto S2, enabling S2 to distinguish between data sent on the X toY circuit from dataon the AtoB circuit.Each data packet, or cell, includes a field that indicates the number of the channelit is using. This field changes from hop to hop as the cell moves through the network.Actually this channel number consists of two fields, a 12bit Virtual Path IndicatorVPI and a 16bit Virtual Channel Indicator VCI. The VPI is analogous to the232 NETWORKING AND DATA DISTRIBUTION 8.3 NETWORKING TECHNOLOGIES 233ensures that the bandwidth will be available during the conversation, enabling thenetwork to provide a predictable latency and low jitter in transmitting information.Operationally bandwidth allocation in circuit switching networks can be realizedusing a technique called time division multiplexing. Consider a multiplexer shown inFigure 8.3a that combines four lowbandwidth communication links into one highspeed link. With time division multiplexing, time on the output link is partitionedinto successive slots, each able to transmit a single piece of data, such as a voicesample see Figure 8.3b. The slots are allocated in succession to each input link,much like dealing cards at a poker table. In this example, the first slot is allocated tolink I, the second to link 2, and so on, and this process repeats when all of themultiplexers input links have received a slot. An input link to the multiplexer canonly transmit data during a time slot allocated to that link. If link I is not carryingany data, its slot is not used. In general, a link that rotates among N slots can carrytraffic for N different circuits at one time. The N  Ist circuit trying to use the link isnot granted permission to do so when it is trying to set up a new call. If an alternatepath through the network cannot be found, the call cannot be established, and theuser is given a busy signal, usually using a tone different from the one given whenthe receivers phone is engaged.Data communications in computer networks such as the Internet typically usepacket switching rather than circuit switching. While voice traffic tends to produce asteady stream of information, data traffic does not. Rather, data traffic tends to bevery bursty. Two computers may exchange little or no information but then suddenlymust transmit megabytes of data, for example, because a user has decided toa123 4 1 234 1bII 131 I 234 1cFigure 8.3 Link allocation techniques. a Multiplexer b time division multiplexing cstatistical multiplexing.download a large file through the network or has decided to send a lengthy electronicmail message. Further, data traffic in these networks usually does not have the strictlatency and jitter requirements of voice traffic. Consider communication requirements for electronic mail. The time division multiplexing approach used in circuitswitching is not well suited for transmitting bursty data. For example, as can be seenfrom Figure 8.3b, if one link has a large data file to transmit, but the other links areidle, only onefourth of the links physical bandwidth can be used to transmit the file,and threefourths of the link bandwidth is unused.A more efficient approach is to allocate the links bandwidth on demand, towhoever needs to use it. This approach, referred to as statistical multiplexing, isshown in Figure 8.3c. Here, successive time slots can be allocated to any link thathas data to send. Each packet contains an identifier like the VPI and VCI fields inATM networks to identify the traffic being sent over the link. Buffers are required tohold data that cannot be immediately sent. This introduces delays, depending ontraffic conditions, resulting in increased jitter, and more difficult to predict latencies.In packet switching, a message is divided into some number of packets of somefixed, maximum length. The packets are then routed through the network, usually asindividual units with no relationship to each other. Different packets for the samemessage may travel over different routes to reach the destination. The message mustthen be reassembled from the individual packets at the destination.To summarize, data communication networks usually use packet switchingbecause it is more efficient for bursty traffic than circuit switching. The price onepays for this is less predictable, and potentially long, latency due to queueing withinthe network. Further the jitter in transmitting a stream of information may be larger.This is the price one pays for better link utilization in packetswitched networks.8.3.3 Quality of ServiceMuch current work in the networking community has been focused on trying toachieve both the predictable, lowlatency properties in circuit switching and theefficient utilization of bursty traffic in packet switching. This is accomplished bysegregating different types of traffic according to their bandwidth and delayrequirements. The basic idea is to provide traffic requiring low latency and jittercertain amounts of guaranteed bandwidth, similar to the guarantees made in circuitswitching. However, bandwidth for example, time slots not used by this traffic canbe utilized by other traffic for example, bursty, delay insensitive traffic such as filetransfers or electronic mail, thereby enabling better link utilization. Much work inthe networking community is currently focused on defining suitable resourceallocation and reservation schemes to meet specific Quality of Service QoSrequirements for different applications using the same networking infrastructure.This is a welcome development for DVEs because a single exercise can creatediverse types of network traffic with different QoS requirements.A network architecture designed to provide QoS guarantees to applicationsincludes several important components Zhang 1993234 NETWORKING AND DATA DISTRIBUTION8.4 COMMUNICATION PROTOCOLS 235 Flow specification. This is a language used to specify characteristics of themessage flows produced by traffic sources and the QoS requirements of theapplication with respect to the flows. Routing. The routing algorithms specify the paths used to transmit messagesthrough the network. This includes paths for pointtopoint traffic as well asrouting trees for multicast traffic. The routing algorithm can have a large effecton the QoS provided to the application a poor routing algorithm will result incongestion in the network that might otherwise have been avoided. Resource reservation. These protocols provide a means to reserve resources inthe network such as bandwidth and message buffers for traffic produced by theapplication. RSVP is one example of such a protocol that propagates resourcereservation requests among nodes in the Internet to reserve resources forunicast and multicast traffic Zhang 1993. Admission control. Since the network only has a finite amount of resources,some reservation requests must be rejected in order to ensure QoS guaranteesfor other traffic flows that have already been accepted can be maintained.Admission control algorithms determine which resource reservation requestsare accepted and which are denied. Packet scheduling. This algorithms determines which packet is transmittedover each communication link next. As discussed in the previous section, thiscan significantly impact the QoS provided to each traffic flow.8.4 COMMUNICATION PROTOCOLSdata. An important advantage of this layered approach is that it allows separation ofdifferent functions in the network, allowing implementations of one function at onelevel to change, without affecting functions at other levels. The series of layeredprotocols in a specific implementation is often referred to as the protocol stack, aterminology that will be adopted here.8.4.1 OSI Protocol StackThe International Standards Organization ISO defined the Open System International abbreviated OSI reference model as a guide for protocol specification. It isintended as a framework into which specific protocol standards can be defined.Seven layers are defined, as depicted in Figure 8.4. Each of these layers is describedbriefly to convey the functionality provided by each layer.Layer 1 Physical Layer The physical layer is concerned with defining theelectrical signals optical signals in the case of fiber optic links, or electromagneticsignals in the case of microwave or radio links and mechanical interface to enablethe transmission of a bit stream across a communication link. Detailed informationsuch as the voltage levels for ones and zeros, timing information, the dimension ofconnectors, and the signal assigned to each wire of the link are specified here.RS232C is an example of a physical layer protocol that is often used to connectterminals to computers.computer I computer 2When dignitaries from two foreign countries come together, a protocol is necessaryfor the two to interact and communicate, but without each offending the otheraccording to their own local customs and procedures. Similarly interconnecting twoor more computers, particularly computers from different manufacturers, requiresagreement along many different levels before communication can take place. Forexample, what cable and connectors should be used What is the unit of datatransmitted a byte, a packet, a variable length message, or. .. Where is thedestination address stored in the message What should happen if data transmittedover a link are lost or damaged by a transmission error The set of rules andconventions that are used to precisely answer questions such as these are referred toas a communication protocol.Networking protocols are usually specified as a sequence of layers in order toseparate different concerns. For example, the error recovery procedure has little to dowith the type of connector that is used, so there is no reason to bundle them together.Each layer provides functionality that is utilized by higher layels. For example, thetype of connector, plus numerous other detailed specifications typically define thelowest level, providing the capability to transmit bits from one side of a communication link to another. Clearly this functionality is necessary to implement an errorrecovery protocol that involves the receiver asking the sender to retransmit corruptedapplication14 application protocol .,applicationpresentation ., presentationsessionsession protocol14.. sessiontransport protocoltransport .. transportnetwork .... network protocol . networkdata link ... d.aIl Eroc. . data linkphysical ... PhyIErc. . physicalIl networkFigure 8.4 Sevenlayer OSI protocol stack.236 NETWORKING AND DATA DISTRIBUTION 8.4 COMMUNICATION PROTOCOLS 237Layer 2 Data Link Layer The data link layer divides the stream of bits providedby the physical layer into frames and performs error checking on individual frames.Error checking is performed by combining all of the bits in the frame for example,adding all of the bytes together, although more sophisticated techniques are oftenused to produce a compact checksum which is appended to the end of the frame.The receiver performs the same operation on the bytes of the frame and comparesthe result with the checksum at the end. If they do not match, the frame is discarded,and a protocol to retransmit the corrupted frame comes into play. For example, thereceiver might send a signal to the sender asking it to retransmit the frame.Flow control across the link is also performed at this level so that a fast senderdoes not inundate a slow receiver with excessive amounts of data. This can beaccomplished, for example, by having the receiver ask the sender to stop or resumetransmitting frames.Layer 3 Network Layer The network layer is responsible for creating a paththrough the network from the sender to the receiver. This requires a routing functionto specifY which outgoing link should be used at each hop as the data move throughthe network. Routing is not as straightforward as it might seem at first because it maybe desirable to take into account loading on the network in order to route messagesto bypass congested links. Also the routing function must consider failed linksandor processors. The network layer is not applicable to LANs based on buses orrings because each message is broadcast to all processors, so no routing function isneeded.Two networklayer protocols that are widely used today are X.25 which is used inmany public networks such as those operated by telephone companies and InternetProtocol IP which is part of the DoD U.S. Department of Defense protocol stack.X.25 is a connectionoriented protocol that establishes a path through the networkand retains the path for subsequent transmissions. IP is connectionless, and routeseach packet independently through the network without first establishing a path.Layer 4 Transport Layer The transport layer provides an endtoend communication service between hosts, hiding details of the underlying network. Largemessages are broken into packets in this layer, and are transmitted through thenetwork and reassembled at the destination. The two most wellknown transportlayer protocols are the Transmission Control Protocol TCP and the User DatagramProtocol UDP. Both are implemented in the DoD protocol stack. TCP provides areliable, connectionoriented, ordered communication service. UDP provides anunreliable, connectionless service. UDP messages may be lost, duplicated, or arriveout of order. It is similar to the IP protocol defined at the network layer.Many distributed simulation applications interface to the network at the transportlevel, bypassing layers 5, 6, and 7, discussed next. This, and the layers above it, arethe most visible layers of the protocol stack for implementers of distributedsimulation systems and applications.Layer 5 Session Layer The session layer is applicable when connectionoriented communications are used. It enhances the transport layer by providingdialogue control to manage whos talking, and synchronization. For example, itall6ws two communicating systems to authenticate each other before beginningcommunication. Other services provide the ability to group a collection of messagesso that they can be released as an atomic action for example, for databasemanagement systems, and reordering messages, perhaps according to priority.In distributed simulation standards such as DIS the session layer includes rulesconcerning what PDUs should be generated and when, and what classes ofcommunication services should be used. This would include, for instance, therules concerning which PDUs must be exchanged during an engagement.Layer 6 Presentation Layer The presentation layer provides a means forparties to communicate without concern for low level details of the representation ofdata. For instance, data types, such as floating point numbers, may be representeddifferently on different machines, so some conversion is necessary if these numbersare transmitted across the network. A wellknown problem is the byteorderingproblem. Different computers may use different conventions concerning whether themost significant byte of a word is stored at the first or last byte address of the wordreferred to as the machine Endianness, so a byteswapping operation is required iftransmitting data between machines using a different ordering convention. Thesetasks are often accomplished by converting the data to a standard format calledeXternal Data Representation XDR before transmission, and converting them tothe desired format at the receiver. A disadvantage of using a standard format for dataon the wire is unnecessary conversions will be performed if the sending andreceiving machines use the same representations, but this representation is differentfrom the standard. The presentation layer also packs and unpacks structured data,such as arrays and records, intofrom messages.Data may be encrypteddecrypted in this layer for security or privacy purposes.Along the same lines, data may be compressed i.e., encoded into a smaller numberof bits by the sender and decompressed at the receiver to reduce the number ofbytesthat must be transmitted through the network. These functions are performed in thepresentation layer.In DIS the format of PDUs and their interpretation are defined in this layer.Formats for compressing data to reduce bandwidth requirements are also specifiedhere.Layer 7 Application Layer This layer defines a collection of protocols forvarious commonly used functions required by end users. Examples include protocolsfor file transfers, electronic mail, remote login, and access to directory servers. DISprotocols defined in this layer include specification of the kind of data exchangedposition, orientation, etc., dead reckoning algorithms, and rules for determiningwhen a hit or miss occurs, and the resulting damage.238 NETWORKING AND DATA DISTRIBUTION 8.4 COMMUNICATION PROTOCOLS 239Commonly used values for n are OCl 51.84 Mbitsjsec, OC3 155.520Mbitsjsec, OC12 622.080 Mbitsjsec, and OC48 2,488.32 Mbitsjsec.Level 3 ATM Adaptation Layer The adaptation layer is responsible foradapting specific types of data for example, video frames, voice samples, dataCLPI bitavcr16 bits8 bits4 bitsLevel 2 ATM Layer The ATM layer deals with cell transport through thenetwork and routing. Each cell is 53 bytes including a 5 byte header and 48 bytepayload data. The 48 byte payload size was a compromise between 32 promotedby Europe and 64 promoted by the United States. The functionality of the ATMlayer can be seen from the fields of the header, shown in Figure 8.5. Two slightlydifferent formats are used. The format in Figure 8.5a is used for cells first enteringthe network at the usernetwork interface or UNI, and the format in Figure 8.5b isfor cells passed between two ATM switches at the networknetwork interface orNNI.In the UNI format the General Flow Control field is intended for use forprioritized congestion control, such as to give voice traffic priority over videotraffic. It is also used to limit the amount of traffic flowing into the network, such asto restrict input of additional traffic during periods of high utilization. The VirtualPath Identifier VPI and Virtual Channel Identifier VCI fields identify the path thecell is to follow in traveling through the network, as was discussed in Section 8.2.1.The Payload Type Identifier PTI is used to distinguish between user data cellsand control cells used in the network. The Cell Loss Priority CLP bit is used toidentify cells that should be dropped first if a buffer overflow occurs. Finally theCyclic Redundancy Checksum CRC is used to detect errors in the header field.The NNI format is the same as the UNI format, except the GFC field iseliminated, and the four bits used by this field are instead used in the VPI field.Level 1 ATM Physical Layer The ATM physical layer protocols providesimilar functionality to the physical layer in the OSI model. Data may be transferredbetween two locations by one of two means1. An ATM adapter board in a computer can put a stream of cells directly ontothe wire or fiber. A continuous stream of cells is sent. If there are no data,empty cells are transmitted. This approach is usually used in switched LANsusing ATM.2. ATM cells may be embedded into another framing protocol. In particular, cellsmay be transmitted via the Synchronous Optical NETwork SONET protocolwhich is widely used in the telephone industry.8.4.2 ATM Protocol StackAs discussed earlier, circuit switching is well suited for voice traffic but is inefficientfor bursty data communications. Conversely, packet switching is more efficient forbursty traffic but suffers from unpredictable latencies and increased jitter, making itless attractive for voice communication. Asynchronous transfer mode ATMtechnology was developed as a means to efficiently handle both types of traffic,as well as a others such as video for teleconferencing and multimedia applications.ATM uses a hybrid approach that features virtual circuits and cell switching.Unlike pure packet switching where packets are routed independently through thenetwork, virtual circuits allow the network to more easily control allocation ofresources to data belonging to a particular connection. This is necessary in order toensure that Quality of Service guarantees are met. Unlike circuit switching wherebandwidth is allocated to each circuit regardless of whether or not it is needed, cellswitching and statistical multiplexing are used that allow more efficient utilization ofnetwork resources when traffic is bursty. Cell switching is also better suited forrealization of multicast protocols which can be of great benefit to distributedsimulation applications.The protocol stack for ATM defines three layers, the ATM physical layer, theATM layer, and the ATM Adaptation layer AAL. Standard transport layer level 4protocols such as TCP can then be built on top of the ATM services.Figure 8.5 Header format for ATM cells. a Format for cells at the usernetwork interfaceb format for cells within the network. A SONET frame containing 810 bytes is transmitted every 125 microseconds tomatch the telephone industrys sample rate of 8000 samples per second. This yields adata rate including overhead of 51.840 Mbits per second. This data rate is calledSTSl for Synchronous Transport Signal level 1 for electrical signals, or OCI forOptical CarrierI for optical signals. Higherrate signals are called STSjOCn,where n indicates the number of OCI circuits that can be realized from the higherbandwidth. For example, an OC12 signal is 622.08 Mbitsjsec 12 times the OCIdata rate and can carry twelve independent OCl circuits, four OC3 circuits or one622.08 Mbitsjsec channel. In the latter case where a single channel is supported, thesignal is called OC12c where the c stands for concatenated or clear channel.VPI12 bitsGFC  General Flow ControlVPI  Virtual Path IdentifierVCI  Virtual Circuit IdentifierVCI16 bitsbCLPI bitPTI  Payload Type IdentifierCLP  Cell Loss PriorityCRC  Cyclic Redundancy Checksum240 NETWORKING AND DATA DISTRIBUTION 8.4 COMMUNICATION PROTOCOLS 241host Atoday. The idea in internetworking is to be able to link together different types ofnetworks to form a network of networks that appears to users as a single network.In this way, users connected to an Ethernet LAN can communicate with usersconnected to a token ring as if they were together on a single network. Byconvention, internetworking with a lower case i refers to the practice of hookingtogether different networking technologies to create a new network, and the Internetupper case I refers to the global network of networks in widespread use today.One approach to internetworking is to use common higherlevel protocols whileallowing different subnetworks to utilize as they must different lower layers in theprotocol stack. This is the approach used in the Internet. Specifically, the Internetuses the Internet Protocol IP developed by the U.S. Department of Defense at level3 in the OSI reference model. IP is a connectionless protocol where the standard unitof data transmitted through the network is a datagram. To connect a new networkingtechnology e.g., ATM to the Internet, one need oniy define how to encapsulate IPdatagrams into that technologys framing protocol cells, in the case of ATM, andspecifY a way of mapping IP network addresses to the addressing scheme used inthat network ATM addresses.Separate subnetworks are connected through IP routers. For example, an Ethernetand ATM network connected by an IP router is shown in Figure 8.6a. If host A onthe Ethernet wishes to send a message to host B on the ATM network, the message isencapsulated into an IP datagram at host A and then placed into an Ethernet frameand sent to the router using Ethernets CSMACD protocol, discussed earlier. Therouter extracts the datagram from the Ethernet and encapsulates it into ATM cellshigherlevel fP routerprotocolsIP IPogicallink ogicailink AALMAC MAC ATMphysical physical physical Ethernet thastBhigherlevelprotocolsIPAALATMphysicalt i ATM rATMswitchEthernet1. AALl supports constant bit rate CBR traffic such as that occurring inuncompressed voice and video. This traffic is usually connection oriented andsensitive to delays.2. AAL2 supports variable bit rate VBR traffic that is connection oriented, andsensitive to delays. Some video and audio traffic belong to this category, suchas video where the number of bits used to represent each video image isreduced using a compression algorithm, resulting in a varying number of bitsfrom one video frame to the next.3. AAL34 supports VBR traffic that is not delay sensitive, and mayor may notbe connection oriented. Computergenerated data for example, file transfersor electronic mail is often of this type. This is called AAL34 rather than justAAL3 for historical reasons. Originally two different protocols were defined,one to support connectionoriented services, and the second for connectionlessservices. It was later discovered that the resulting mechanisms were verysimilar, so these were combined.4. AAL5 also supports delay insensitive VBR traffic but is simpler thanAAL34. This protocol is also called SEAL Simple and Efficient AdaptationLayer it was proposed after it was found that AAL34 was more complexthan necessary to support computergenerated traffic.Clearly much of the traffic generated by distributed simulations for virtualenvironments is delay sensitive. The amount of data transmitted at one time dependson the amount of state information that is changed, which in tum depends on howmuch activity there is in the portion of the virtual environment managed by theprocessor generating the traffic. Since the volume of data often varies from onemessage to the next, much of the traffic generated in a distributed simulation best fitsthe AAL2 category. On the other hand, uncompressed video or voice traffictransmitted during the exercise would fall into the AALl category. Still other trafficsuch as generation of message logs for afteraction review of the simulation is notdelay sensitive and fits best into the AAL34 or AAL5 categories.packets for transmission through the network. Specifically, this layer is responsiblefor dividing the data into cells, sending the cells into the network, and reassemblingthem at the destination. This task is typically done within the ATM interface cardbecause cells arrive at too high a rate for the CPU to process them using interrupts.For example, at OC3 data rates 155.520 Mbitssec, a stream of incoming cellscould generate an interrupt every 3 microseconds.Several different adaptation layer protocols are defined, optimized for differenttypes of traffic.Figure 8.6 Intemetworking example. a Hardware configuration b logical view in termsof the protocol stacks.8.4.3 Internetworking and Internet ProtocolsA wide variety of different networking technologies and protocols have beendeveloped, each with different advantages and disadvantages. Many are in usea b242 NETWORKING AND DATA DISTRIBUTION 8.5 GROUP COMMUNICATION 243that are then sent to host B through the ATM switch. Host B then recovers thedatagram from the cells and passes it up to the user using higherlevel protocols. Alogical view of this operation in terms of the protocol stacks is shown in Figure8.6b.More generally, a message may have to traverse several subnetworks beforereaching its final destination. Thus a principal task for each router is to select thenext router in the path to reaching the final destination. Each router maintains tablesthat indicate which direction to route datagrams, based on its IP address. Routersalso communicate with other routers to try to maintain the best paths through theInternet by routing message around failed components and congested areas.The suite of Internet protocols is the most popular in use today. It is an openstandard governed by the Internet Engineering Task Force IETF. The protocolstack consists of five layers, with several protocols defined within each layer,affording one the ability to pick and choose the protocol that best fits onesapplication. These protocols cover the physical, data link, network, transport, andapplication layers of the OSI reference model, or levels 1 through 4, and level 7.Level 1 Internet Physical Layer The internetworking concept suggests manydifferent physical layer protocols, and indeed, many different physical layer protocols defined today use the Internet protocol stack. Ethernet is perhaps the mostpopular among users connected to a LAN. SLIP Serial Line Internet Protocol isused for serial lines, such as home access to the Internet via personal computers.Token ring protocols are also used, as well as many others.Level 2 Data Link Layer As mentioned earlier, standard protocols are neededfor mapping IP addresses to physical addresses for different types of subnetworks.For example, Address Resolution Protocol ARP maps IP addresses to Ethernetaddresses and Reverse ARP RARP is used to map Ethernet addresses to IPaddresses. These are defined in the data link layer.An IP address is 32 bits or 4 octets, usually represented as 4 decimal numbers,one for each octet ranging from 0 to 255, separated by periods e.g., 194.72.6.57.Four formats are defined. Three formats are used to uniquely specifY a host byindicating a network number and a host within that network. These formats varyaccording to the number of bits allocated to the network number and host ID fields.Format A uses 7 bits for the network number and 24 bits for the host number theother bit in the 32 bit address specifies the address format being used, accommodating organizations with many hosts on one network. Class C addresses use 21bits for the network number and 8 bits for the host, and it is used for organizationsrequiring far fewer hosts. Class B provides an intermediate format with 14 bits forthe network number and 16 bits for the host number. Note that with this scheme, if ahost is connected to two networks, it must have two different IP addresses.The fourth address format is used for multicast addresses for group communications. As discussed later, multicast is a mechanism to transmit copies of a singlemessage to multiple receivers, something of particular benefit in largescaledistributed simulations. A Multicast Backbone MBone has been established onthe Internet that is used for group communications.Level 3 Network Layer Three network layer protocols are defined. In additionto IP, two other protocols called ICMP Internet Control Message Protocol andIGMP Internet Group Multicast Protocol are defined. ICMP is used to send controlmessages between routers and hosts, such as to implement flow control and toindicate errors. IGMP is used for multicasting.IP provides best effort delivery of datagrams to a host. Datagrams may be lost orcorrupted before reaching their destination. No ordering guarantees are madeconcerning delivery that is, datagrams do not need to be received in the sameorder in which they were sent. A Checksum is used in the datagram header, but noneis provided for the data.When IP is passed data to send, it adds a message header that includes thedestinations IP address. If necessary, the datagram is broken up into IP packets, andthe packets are sent separately to the destination. The receiver must collect thepackets and reassemble the datagram before delivering it to the receiver.Level 4 Transport Layer Two transport layer protocols are defined, UDP UserDatagram Protocol and TCP Transport Control Protocol. These are sometimesdenoted TCPlIP and UDPlIP to designate that the protocols operate using IP at thenetwork layer.IP provides delivery to a host computer. Both UDP and TCP provide for deliveryto an application on the host. This is done be defining source and destination portsdesignated by a 16 bit port number within the host to uniquely identifY theapplication. UDP does not provide much else besides application addressing beyondwhat IP provides. Specifically, delivery is unreliable and unordered.On the other hand, TCP does provide a connectionoriented, reliable, ordereddelivery service. It effectively provides a stream of bytes between applications.Reliable delivery means that the data will be delivered or an error will be flagged.Message acknowledgments are used to detect lost data. If an acknowledgment is notreceived after a certain timeout period, it must be retransmitted. Sequence numbersare used to order data.Level 5 Application Layer Several application protocols are defined at the topof the protocol stack. Some wellknown protocols include File Transfer ProtocolFTP and Trivial File Transfer Protocol TFTP for transferring files betweenremote hosts, TELNET for creating a login session on a remote host, Simple MailTransfer Protocol SMTP for exchanging electronic mail, and Domain NameService DNS which provides services for mapping easier to remember symbolichost names such as cc.gatech.edu to IP addresses.8.5 GROUP COMMUNICATIONThe discussion thus far has focused on pointtopoint or unicast communicationwhere there is one sender and one receiver of each message. Of particular interest toDVE applications are group communication facilities. For example, if a player in a244 NETWORKING AND DATA DISTRIBUTION 8.6 DATA DISTRIBUTION 245DVE moves to a new position, each of te other players that can see that themobile player should be sent a message. Here, we describe the communicationfacilities that are often provided to support group communications. Later, wedescribe data distribution mechanisms to utilize these group communication facilities in DVE applications.8.5.1 Groups and Group Communication PrimitivesA group is a set of processes, or here, simulators, such that each receives a copy ofevery message sent to the group. Open groups are groups where the sender need notbe a member of the group to send a message to the group. Closed groups only allowsimulators that are part of the group to send a message to the group.Groups may dynamically expand and contract during the execution of thedistributed simulation. For example, if a vehicle moves to a position that makes itvisible to another player, the latter should be added to the group to which the vehiclesimulator is publishing position updates so that it can receive updates concerning thevehicles position. Ajoin operation is used to add a simulator to a group. Conversely,a leave operation is used to delete someone from a group. Minimally the groupcommunications facilities will provide primitives to create, destroy, join, leave, andsend a message to the group.Separate send and receive primitives may be defined for group communications.Alternatively, the same primitives used for pointtopoint communication may beused, with separate group destination addresses defined to distinguish between pointtopoint and group communication.An addressing scheme is required to uniquely identify the group on join, leave,send, and receive operations. This address may be an internal representation of thegroup address, such as a network multicast address used to implement the group. Inaddition an ASCII name is also often used. Assuming that groups are assignedunique names, a name server process can then be used to convert the ASCII addressto the internal representation of the group address.8.5.2 Transport MechanismsThe actual mechanism used to transport messages to the simulators in the groupdepends on the mechanisms provided by the underlying network. In particular, groupcommunication can be implemented by the following mechanisms Unicast. Separate pointtopoint communications are used to send messages todestinations in the group. Broadcast. If the underlying network provides an efficient broadcast medium,the message can be broadcast to all simulators, and destinations not in thegroup discard the message. Multicast. The network provides a mechanism to only deliver messages tomembers of the multicast group. This is often accomplished by constructing aspanning tree that controls the generation and distribution of copies of themessage among the members of the group. Alternatively, a floodingpruningmechanism may be used where intermediate network nodes retransmit copiesof incoming messages on outgoing links according to certain rules thatguarantee distribution to all destination nodes but economize on bandwidthusage.Broadcast was used in Simnet and many DIS systems. This is adequate for smallsystems, but it does not scale because communication increases by ON2  for Nsimulators. In current systems the principal limitation is the amount of CPU timerequired to receive, examine, and discard messages that are not relevant to the localsimulator.Largescale DVEs being constructed today usually use either multicast communication or unicast to implement group communication. At the time of this writing,unicast is typically used when reliable communication is required because reliablemulticast is still an active research area. Multicast is often used when besteffortcommunication is adequate.8.6 DATA DISTRIBUTIONWhatever communication facility is provided by the underlying network, somestrategy must be developed to efficiently distribute state information among thesimulators. The software in the distributed simulation executive to control thisdistribution of information is referred to as the data distribution DD system. Thetask of the DD software is to provide efficient, convenient to use services to ensurethat each simulator receives all of the messages relevant to it, and ideally, no others.8.6.1 Interface to the Data Distribution SystemWhenever a simulator performs some action that may be of interest to othersimulators, such as moving an entity to a new location, a message is generated.Some means is required to specify which other simulators should receive a copy ofthis message. Specifically, the distributed simulation executive must providemechanisms for the simulators to describe both the information it is producing,and the information it is interested in receiving. Based on these specifications, theexecutive must then determine which simulators should receive what messages.This situation is not unlike that in Internet newsgroups. Users express whatinformation they are interested in receiving by subscribing to specific newsgroups.The information that is being published is described by the newsgroups to which itis sent. For example, a recipe for a new cake would be published to a cookingnewsgroup and not one concerning the weather. The newsgroup names are criticalbecause they provide a common vocabulary for users to characterize both theinformation being published and the information they are interested in receiving.246 NETWORKING AND DATA DISTRIBUTION 8.6 DATA DISTRIBUTION 247The set of newsgroup names defines a name space, which is a commonvocabulary used to describe data and to express interests. Each user provides aninterest expression that specifies a subset of the name space, which is a list ofnewsgroups, that indicates what information the user is interested in receiving. Adescription expression, again a subset of the name space, is associated with eachmessage that describes the contents of the message. Logically the software managingthe newsgroups matches the description expression of each message with the interestexpression ofeach user. If the two overlap i.e., have at least one element of the namespace in common, the message is sent to that user.The newsgroup analogy is useful to specify how information should be disseminated in a DVE. There are three basic concepts Name space. The name space is a set of values used to express interests and todescribe information. The name space is a set of tuples VI V2 , ... , VN whereeach Vi is a value of some basic type, or another tuple. For example, asimulator modeling a radar might indicate it is interested in receiving amessage whenever the location of any aircraft within one mile of the radarchanges. The name space could be defined as a tuple VI V2, where VI is anenumerated type specifying the different types of vehicles in the DVE forexample, truck, aircraft, and ship, and V2 is a tuple specifying the X and Ycoordinate positions of a vehicle. Interest expressions. An interest expression provides the language by which thesimulator specifies what information it is interested in receiving. An interestexpression defines some subset of the name space. For example, the interestexpression for all aircraft within 1 mile of location Xo, Yo is all tuplesaircraft, X, Y such that Xo  X2  Yo  Y2  1. Description expressions. A description expression is associated with eachmessage. It specifies a subset of the name space that characterizes the contentsof the message. For example, a simulator modeling an aircraft moving tolocation 151, 211 may generate a message indicating this is its new position,with description expression aircraft, 151, 211. Each simulator with aninterest expression containing this point in the name space will receive themessage.The name space, interest expressions, and description expressions define the heartof the interface to the DD mechanisms. The DD software must map this interface tothe primitives provided by the communication facilities such as joining, leaving, andsending messages to multicast groups. The challenging aspect of the DD interface isdefining abstractions that are both convenient for the modeler to use, and provide anefficient realization using standard communication primitives. DD interfaces that aresimilar to basic communications primitives lend themselves to straightforwardimplementation but may be difficult for modelers to use. On the other hand,higherlevel mechanisms such as those described above to specify the radarsimulators interests may be difficult to implement, leading to slow andor inefficientmechanisms.In the radar example the name space was defined in terms of the actual data fieldsof the message that is, the position attributes of entities were used to define part ofthe name space. In general, the name space need not be defined directly in terms ofthe messages data fields. Instead, it may be defined to be completely distinct fromthe simulation state. For example, in the HighLevel Architecture, a separateabstraction called routing spaces is used to define the name space. Separating thename space from the data within the message offers the advantage that it separatestwo different concerns, the state information necessary to model the virtualenvironment and the specification of the DD mechanisms in order to ensure efficientdata distribution. Separating these concerns allows the simulator to more easilymanipulate the DD mechanisms in order to ensure efficient realization. For example,if join and leave operations are timeconsuming in the underlying implementation,one might define the interest and description expressions in a way that reduces thefrequency that these operations will have to be invoked. If the name space is separatefrom the entity attributes in the simulation, this may be accomplished with minimalimpact on the simulation model.It is worth noting that although spatial proximity of simulated entities in thevirtual world is an important criterion for data distribution, it is not the only one ofpractical interest. For example, if the virtual environment includes radio transmissions, the frequency to which a transmitter is tuned may be used to derive interestexpressions. Thus, while physical proximity is a useful example to illustrate theseconcepts, it may be inappropriate to design the data distribution system to rely onproperties that are only relevant in the context of physical distances.Finally the interface to the DD system may also include some means for allowinga simulator to discover or forget entities in the DVE. For example, when a radar isswitched on, the simulator modeling the radar must discover those vehicles withinrange. Similarly, when a vehicle moves out of range, it must become invisible i.e.,forgotten by the radar simulator. This is equivalent to providing notifications to thesimulator whenever it becomes eligible or ineligible to receive messages fromanother simulator. This can be realized by associating a set of description expressions characterizing the data that could be produced by that simulator in the future.By matching the interest expressions of a simulator with these possible descriptionexpressions of other simulators one can determine when a simulator is eligible orineligible to receive messages from another simulator. Changes to either the set ofpossible description expressions or a simulators interest expressions could result inentities becoming discovered or forgotten.8.6.2 Example Data Distribution in the High Level ArchitectureTo illustrate these concepts, consider the interface to the data distribution system inversion 1.3 of the High Level Architecture Defense Modeling and SimulationOffice 1998. The HLA provides two mechanisms for controlling the distribution ofdata248 NETWORKING AND DATA DISTRIBUTION 8.6 DATA DISTRIBUTION 2491. Classbased data distribution.2. Valuebased data distribution via routing spaces.The name space consists of the tuple Vj , V2  where VI enumerates all objectclassattribute pairs specified in the federation object model FOM, and V2enumerates the routing spaces that are defined by the federation.ClassBased Data Distribution Recall that in the HLA the federation mustdefine a FOM that specifies the classes of objects of common interest to thefederation. The FOM specifies a hierarchy indicating object classes and theirinterrelationships, and object attributes. For example, a class may be definedcalled vehicle, with separate subclasses called aircraft, tank, and truck. Theseclass definitions are used for data distribution. Specifically, each federate invokesthe Subscribe Object Class Attributes service to indicate it is to receive notificationof updates to attributes of a specific class in the class hierarchy tree. For example, afederate might invoke this service to subscribe to the position attribute of the vehicleclass to be notified whenever a vehicles location is updated.Each attribute defined within a class is automatically inherited by the subclassesderived from that class. For example, the aircraft subclass inherits the positionattribute defined in the vehicle class. Recall that when a federate subscribes to anattribute in the class hierarchy tree, it automatically subscribes to that attributeinherited by the subclasses in the subtree rooted at that class. Thus a subscription tothe position attribute of the vehicle class will cause the federate to be notifiedwhenever the position attribute of any aircraft, tank, or truck object is modified viathe Update Attribute values service.In the classbased data distribution scheme the name space is the set of class,attribute tuples, specified in the FOM. In our simple example, the name spaceconsists of the tuples vehicle, position, aircraft, position, tank, position, andtruck, position. Classbased data distribution can be realized as follows Asubscription to an attribute A within class C becomes an interest expressionenumerating a set of tuples Co, A, CI , A, C2 , A, ... , CN , A where Co  Cand Ci i  1,2, ... , N are the subclasses and subclasses of these subclasses, etc.of C. For example, a subscription to vehicle, position turns into the interestexpression vehicle, position, aircraft, position, tank, position, and truck,position. The description expression for an update to attribute A of an object ofclass C is simply C, A, such as tank, position when the position ofa tank object isupdated. Federates subscribed to vehicle, position and tank, position will benotified of this update because their interest expressions overlap with the descriptionexpression, but those only subscribed to aircraft, position andor truck, positionwill not.This approach to defining interest and description expressions is not unique. Analternative approach that yields the same results is to define a subscription toattribute A of class C as an interest expression containing a single tuple C, A. Anupdate to attribute A of class C carries a description expression Co, A, CI , A,C2 , A, ... , CN , A, where Co  C and Ci i  1,2, ... , N are the classes alongthe path from C up the class hierarchy tree to the class where A was originallydefined, that is, not inherited. Here, a subscription to vehicle, position turns into theinterest expression vehicle, position. An update to the position attribute of a tankobject contains the description expression tank, position, vehicle, position.Again, federates subscribed to vehicle, position and tank, position will benotified of this update, but those only subscribed to aircraft, position andortruck, position will not.Classbased data distribution allows a federate to subscribe to, for example, allaircraft objects. It does not provide the ability to discriminate based on valuescomputed during the federation execution. For example, classbased distributiondoes not allow a federate to specify that it is only interested in aircraft within 10miles of its current location. Routing spaces are provided for this purpose. Thecentral distinction between these two mechanisms is the name space in classbaseddata distribution only includes information that is known prior to the execution ofthe federation, so classbased distribution can only distinguish between staticallydefined characteristics of objects. On the other hand, routing spaces extend the namespace to include information computed during the execution, thereby allowingfederates to discriminate based on dynamic information. For this reason the routingspace mechanism is sometimes referred to as valuebased data distribution.Routing Spaces Routing spaces are an abstraction defined separately fromobjects and attributes, solely for the purpose of data distribution. A routing space is amultidimensional coordinate system. The name space for a single Ndimensionalrouting space is a tuple XI X2 , .. ,XN  with Xmin  Xi  Xmax  where Xmin andXmax are federationdefined values. For example, Figure 8.7 shows a twodimensional routing space with axis values ranging from 0.0 to 1.0. The relationship of therouting space to elements of the virtual environment is left to the federationdesigners. For example, a twodimensional routing space might be used to representthe geographical area covered by the virtual environment however, the datadistribution software is not aware of this interpretation.Interest and description expressions in the HLA define areas,30 called regions, ofa routing space. Specifically, each region is a set of one or more extents, where eachextent is a rectangular Ndimensional area defined within the Ndimensional routingspace. Four extents are shown in Figure 8.7. Each extent is specified as a sequence ofN ranges R I R2 ,  , RN , where range R i is an interval along dimension i of therouting space. For example, the extent labeled S1 in Figure 8.7 is denoted 0.1, 0.5,0.2, 0.5, using the convention R I corresponds to the horizontal axis, and R2corresponds to the vertical axis.A region is the union of the set of points in the routing space covered by itsextents. Interest expressions are referred to as subscription regions, and descriptionexpressions are referred to as update regions. For example, the routing space in30 Areas are line segments in onedimensional routing spaces, volumes for three or more dimensionalspaces.250 NETWORKING AND DATA DISTRIBUTION 8.6 DATA DISTRIBUTION 251of using rectangular regions is it simplifies the implementation of the datadistribution mechanism without requiring the RTI to understand the semantics ofthe simulation models.Further the update region only specifies that the aircraft is somewhere within thatregion, but does not pinpoint its exact location. In particular, the aircraft may lie inthe part of U that does not overlap with 81. In this case the sensor federate willreceive a message, even though the aircraft cannot be detected by the sensor. Again,the federate must filter incoming messages and discard the ones that are not relevant.One could achieve more precise filtering by reducing the size of the updateregion, for example the update region might be defined as a single point that ismodified each time the aircrafts position is updated. However, this introduces otherdifficulties. Successive updates may lie outside the subscription region, although thepath of the aircraft actually passes within range of the sensor. This is illustrated bythe arrow within the update region in Figure 8.7 the head and tail of the arrowindicate successive position updates, and the line segment indicates its path. If theupdate region were a point that is modified with each position update, the sensorfederate would not be notified of the aircrafts position because its update regionnever actually intersects the subscription region. To avoid this problem, the aircraftsupdate region at any instant in the execution should include at least all positions theaircraft could occupy from the most recent to the next position that will be reportedby the aircraft simulator.Definition of subscription regions also involves certain compromises, particularlyif the subscription region changes, as would be the case for a sensor mounted on amoving vehicle. Changing a subscription region can be a time consuming operationinvolving joining and leaving multicast groups, as will be seen momentarily.Defining large subscription regions will result in less frequent region modifications,but will result in the federate receiving more messages that are not relevant to it.Small regions yield more precise filtering but more frequent changes. The regionsize should be set to strike a balance between these two extremes.Finally the HLA interface includes services for notifying when an object becomesdiscovered or forgotten. The latter is called undiscovery in HLA terminology. In thisregard, update regions serve a dual role as defining both the possible descriptionexpression as well as the description expressions themselves. An object instancebecomes discovered by a federate when its update region overlaps with the federatessubscription region and no overlap existed previously. The object is undiscoveredwhen one or both regions are modified, resulting in no overlap when there previouslywas overlap.8.6.3 Implementation IssuesThe data distribution software must map the name space, interest expressions, anddescription expressions to the communications services provided by the underlyingnetwork. Here, we will assume a group communication facility is available. Even ifthe underlying network does not support group communications, this provides aconvenient abstraction that can be built on top of a unicast facility.1.00.50.0 o,I0.00.51.0 .,Figure 8.7 includes one update region U and two subscription regions Sl and S2. Slincludes a single extent, and S2 consists of two extents. The extents defining a singleregion need not overlap.Each federate can qualify a subscription to an object class by associating asubscription region with the subscription, for example to only get updates forvehicles within a certain portion of the routing space. Similarly an update regionmay be associated with each instance of an object. If a federates subscription regionfor an object class overlaps with the update region associated with the instance of theobject being modified, then a message is sent to the federate.For example, suppose the routing space in Figure 8.7 corresponds to thegeographic area i.e., the playbox of a virtual environment that includes movingvehicles. Suppose that the update region U is associated with an aircraft object thatcontains attributes indicating the aircrafts position. The region defined by Uindicates the aircraft is within this portion of the playbox. Suppose that SI and S2are the subscription regions created by two distinct federates F I and F2, eachmodeling a sensor. The extents of these subscription regions are set to encompass allareas that the sensors can reach. If the aircraft moves to a new position within U,thereby updating its position attribute, a message will be sent to FI because itssubscription region Sl overlaps with U, but no message will be sent to F2 whosesubscription region does not overlap with U.This example illustrates that certain compromises must usually be made indefining a DD system. First, the range of the sensors is in general, not a collectionof rectangular areas. In principal, one can approximate any area with many suchrectangles, but this may degrade the performance of the DD system, offsetting theadvantage of using routing spaces at all. Thus federates may receive some messagescorresponding to positions the sensor cannot detect, so the sensor federate mustcheck incoming messages and discard those that are not relevant to it. The advantageFigure 8.7 Update region U and subscription regions SI and S2 in a twodimensionalrouting space...,252 NETWORKING AND DATA DISTRIBUTION 8.6 DATA DISTRIBUTION 253Implementing the data distribution mechanism requires one to do the following1. Map the name space to multicast groups here, we assume each point in thename space maps to a single group.2. Convert each interest expression, which expresses a subset of the name space,to one or more groups according to the mapping defined in l the simulatordeclaring the interest expression must subscribe to these groups.3. Convert each description expression to a set of groups according to themapping defined in l messages should be sent to these groups.To illustrate this approach, consider an implementation of a data distributionsystem for the HLA routing space approach. To simplify the discussion, we onlyconsider a single routing space, and ignore classbased data distribution. The routingspace defines a name space. One can partition the routing space into a checkerboadgrid and assign each grid sector to a different multicast group. For example, IIIFigure 8.8 the routing space is partitioned into 25 grid cells. Federate Fl subscribesto groups 6, 7, 8, 11, 12, and 13 because these groups include portions of the namespace specified by F1s subscription region interest expression S1. Similarly F2subscribes to 11, 12, 16, and 17, corresponding to region S2. When an attribute isupdated that is associated with publication region description expression U, amessage is sent to multicast groups 12 and 13.This approach will guarantee that each federate will receive all information that ithad requested via its interest expressions. However, federates may receive additionalmessagesa message sent with publication region U because U posts a message tomulticast group 12, and F2 is subscribed to that group.2. A federate may receive multiple copies of the same message. For example,federate Fl will receive two copies of the message, one via group 12 and asecond via group 13.These duplicate messages can be eliminated at the receiver by checking that theupdate and subscription regions overlap.Extra messages such as those described above are not unique to this particularexample. In the first case, a simulator receives a message even though its interestexpression does not overlap with the messages description expression. This canhappen whenever the part of the name space covered by a multicast group overlapswith the interest description expression and part of it does not. For example, it canbe seen in Figure 8.8 that part of the name space corresponding to multicast group12 includes portions ofF2 U, but parts do not. More precisely, let Nj be the part ofthe name space covered by the interest expression, let Nd be the part of the namespace covered by the description expression, and Ng the part covered by the multicastgroup. The simulator could receive messages outside of its interest expression if1. the interest expression overlaps with the group Ni n Ng i 0 and2. the description expression overlaps with the group Nd n Ng i 0, but3. the interest expression does not overlap with the description expressionNi nNd  00.0 I.L......L...o....J0.0 1.0Figure 8.8 Example mapping an HLA routing space, subscription regions, and publicationregions to multicast groups.1. A federate may receive messages, even though its subscription region did notoverlap with the publication region. For example, in Figure 8.8 F2 will receivethen duplicate messages will be received.Extra messages result from an imperfect mapping of the name space to multicastgroups where part of the name space covered by a group lies within the interestdescription expression, and part does not. One could eliminate these messagesby requiring interest and description expressions to be defined so that each maps toexactly the name space covered by a single group for example, one might constraineach routing region to cover exactly one grid cell in Fig. 8.8. This does not reallysolve the problem. In practice, the simulator would still receive unwanted messagesbecause its desired interest and declaration expressions most likely do not exactlymatch the group definitions.Finally one remaining aspct concerns the operations that must be performedwhen a simulators interest expressions change for example, as the result of a1. Nj n N g1 i 0 and N d n N g1 i 0, and2. Nj n N g2 i 0 and N d n N g2 i 0,Further a simulator may receive duplicate copies of messages if the intersection ofits interest expression and the description expression used to send the message spansmore than one multicast group. In other words if there exist two multicast groupscovering portions of the the name space N g1 and Ngz, where0.5254 NETWORKING AND DATA DISTRIBUTION 8.6 DATA DISTRIBUTION 255simulator for a sensor moving to a new location. This will, in general, result in thesimulator leaving certain multicast groups, and joining others. The steps required torealize this change are as follows1. IdentitY the multicast groups that cover the portion of the name space specifiedby the new interest expression. Let Gnew be the set of groups that cover thenew interest expression. Let Gold be the set of groups covered by the oldinterest expression.2. Issue a join operation to add the simulator to each group in the setG new  Gold3. Issue a leave operation to remove the simulator from each group in the setGold  G new For example, Figure 8.9 depicts a situation where the subscription region of afederate changes. The new region includes multicast groups 13,21,27,28, and 29which were not included in the previous region, so joins must be issued for thesegroups. Similarly leave operations must be issued for groups 2, 3,4, 10, and 18.8.6.4 Dynamic Group ManagementA challenging task in designing the data distribution mechanism is to define aneffective mapping of the name space to the available multicast groups. Thus far wehave been assuming this mapping was static, namely it does not change during thenew region39 4031 3223 2415 167 8 issue leaveissuejoinD  no operations issuedFigure 8.9 Example illustrating changes to multicast groups when a subscription regionchanges.execution of the distributed simulation. A static approach is simple, and straightforward to implement. The disadvantage of this approach is it can lead to certaininefficiencies, particularly if the number of groups supported by the underlyingcommunication services is limited.In many networks only a limited number of multicast groups are available. Thedistributed simulation executive must make effective use of the groups that areprovided. Approaches such as the static grid scheme discussed above may not makeefficient use of the groups. For example, it is often the case that entities in the virtualenvironment are not uniformly distributed across the playbox but rather are clusteredin certain hot spots. For example, areas of the virtual environment containingmountainous terrain are unlikely to contain many land vehicles. Other areas whereintensive activities are occurring, such as combat in a military DVE, may containmany entities. Thus multicast groups in quiet areas of the virtual environmentwhere few state updates occur will be underutilized, while those in busy areas willsee heavy use.Dynamic group management schemes change the mapping of the name space tothe multicast groups during the execution of the simulation in order to achieve moreefficient usage of the groups. This is not unlike utilizing a personal address book thatcan hold only a limited number ofentries. Just as the personal address book providesfast access relative to a general telephone directory, but can include information foronly a small number of people, multicast groups provide more efficient communications but are in limited supply. The goal of the group management system is toreserve allocation ofgroups to only those portions of the name space that are heavilyutilized, such as portions of the name space carrying a large amount of traffic. Forexample, using the gridbased scheme described earlier in the HLA, one might mapthose cells of the grid containing the most messages to the available multicastgroups, and implement communication in other grid cells using some other i.e.,unicast or broadcast mechanism. If a cell assigned to a group becomes underutilized, the group can be reallocated to another cell with a higher level of activity,just as infrequently used entries in a personal address book can be erased andoverwritten by more frequently referenced addresses. A policy is required todetermine which addresses are kept in the address book i.e., which sections ofthe name space are stored in multicast groups.As with the address book scheme, changing the mapping of the name space togroups does incur a certain amount of overhead computation. Remapping a group toanother grid cell or, more generally, to another portion of the name space requiressimulators that are subscribed to the remapped group to leave, and simulators withinterest expressions overlapping the part of the new name space assigned to thegroup to join. Thus care must be taken to ensure that the overhead of remapping themulticast group does not exceed the benefit.8.6.5 WideArea Viewers and FastMoving EntitiesAnother challenge associatedwith efficient utilization of multicast groups is the socalled widearea viewer problem. A widearea viewer is a simulator that can seelarge portions of the virtual environment, though usually not in great detail. For256 NETWORKING AND DATA DISTRIBUTION 8.8 ADDITIONAL READINGS 257example, aircraft can view much broader geographic areas than ground vehicles buttypically require much less detailed information concerning each portion of itsviewing area than entities with a much narrower field of view. In using gridstructures such as that described earlier for the HLA, widearea viewers will haveinterest expressions spanning large portions of the name space. The viewer will haveto join many multicast groups and receive large quantities of information, much ofwhich will be discarded because the viewer often will not be able to utilize so muchdetailed information.A solution to the widearea viewer problem is to cover the playbox with multiplegrids of different resolutions. For example, in addition to the fivebyfive grid shownin Figure 8.8, another routing space might be established using a coarser, twobytwogrid structure. Less detailed information could be published using descriptionexpressions that refer to the routing space with the coarse structure. For example,position updates might be generated less frequently than on the original routingspace. Interest expressions for widearea viewers would refer to the coarse routingspace in order to obtain less detailed information of the entities.A related problem concerns rapidly moving entities, such as highspeed aircraft.The interest expressions for such entities may change very rapidly. This can placehigh demands on the data distribution and network communication software,especially if the data distribution software is also remapping groups, because joinsand leaves to multicast groups are often timeconsuming operations.Coarse grid schemes also help solve the fastmoving entity problem. Because themulticast groups cover a broader portion of the name space, as the fastmoving entitychanges its interest expression, each change will usually result in less frequent joinand leave operations.Another approach to dealing with fastmoving entities is to use a lookaheadprediction scheme. This is essentially a latency hiding technique where one issuesjoin operations in advance so that they will be completed by the time the entityactually moves into a new area of the playbox. For example, suppose that an entity inFigure 8.8 currently has a subscription region covering grid cell 2 and is movingdirectly east. It can subscribe to the information in grid cell 3 before actually enteringthat cell, in anticipation of its entry in the not too distant future. The subscriptionmust be issued far enough in advance so that it is in place before the entity actuallymoves into cell 3. Once the entity moves into cell 3, it can drop its subscription tocell 2.8.7 SUMMARYThe DVE must ensure a suitable view of the virtual world is maintained for eachparticipant. The network providing communication services plays a critical role inrealization of this function. DVEs, especially largescale DVEs containing manyparticipants, present very challenging requirements with respect to latency, jitter,bandwidth, and reliability. These requirements coupled with the dynamic, rapidlychanging communication interconnectivity among participants distinguish DVEsfrom more traditional networking applications such as data and telephone communications or even more recent multimedia applications such as teleconferencing andvideo on demand.This chapter examined networking technologies and how they affect the design ofDVEs. Techniques such as circuit switching and time division multiplexing are wellsuited for transmitting continuous streams of traffic while guaranteeing fulfillment oflatency and jitter requirements. Packet switching and statistical multiplexing enableefficient transmission of bursty traffic. DVEs often require both of these types oftraffic. Technologies such as ATM, which are designed to efficiently handle bothtraffic types on a single networking, infrastructure may provide the best solution forDVEs.Protocol stacks provide a useful means of organizing the functionality realized inmodem networks. Because DVEs must often rely on commercially availablenetworking technology not designed specifically for DVEs, stacks such as ATMand the Internet Protocol are often used to implement DVEs today. Group communications provide a particularly useful service and are essential for large DVEs.The DVE must include data distribution software above the communication layerto ensure that each participant receives updates to the state of the virtual world thatare relevant to it. Interest expressions are used to specify what information is relevantto each participant, and data description expressions characterize the informationbeing generated. A name space provides the vocabulary for these expressions.Developing an efficient realization of the data distribution system can be achallenging task, trading off efficiency with ease of use. In practice, compromisesmust often be made to define a data distribution system that is both efficient andprovides sufficient expressiveness in defining interest and description expressions.8.8 ADDITIONAL READINGSThere are several excellent textbooks devoted to computer networks. Some examplesare Partridge 1993, Tanenbaum 1996, and Walrand and Varaiya 1996. Severalother books discuss networks in the context of distributed operating systems, such asTanenbaum 1994, Sinha 1996, and Chow and Johnson 1997. Network requirements for DIS are discussed in Pullen and Wood 1995 as well as in IEEE Std1278.21995 1995. The architecture of NPSNet for realizing largescale DVEs isdescribed in Macedonia, Zyda et al. 1995. Surveys of group communications andlocal area networks are presented in Diot, Dabbous et al. 1997 and Abeysundaraand Kamal 1991, respectively.Several data distribution management DD mechanisms are described in Morse1996. Data distribution based on routing spaces was used in the Synthetic Theaterof War STOW program Van Hook, Calvin et al. 1994, a largescale militarytraining exercise resulting in a demonstration conducted in the fall of 1997. Use ofrouting spaces in the HLA is described in Defense Modeling and Simulation Office1996. Another approach to data distribution used in the Joint Precision StrikeDemonstration is to define interest expressions as predicates on entity attributes258 NETWORKING AND DATA DISTRIBUTIONPowell, Mellon et al. 1996. Gridding schemes have long been used for militaryapplications. Examples include ModSAF Russo, Shuette et al. 1995, CCTTMastaglio and Callahan 1995, and NPSNET Macedonia, Zyda et al. 1995.Approaches to ensure proper time synchronization of the data distribution mechanism discussed in Part II of this book are described in Steinman and Wieland1994, Blanchard and Lake 1997, and Tacic and Fujimoto 1997.CHAPTER 9Time Management and Event OrderingThe goal of the data distribution system is to ensure that the simulators perceive acommon view of the virtual environment. Some differences in the views perceivedby different simulators is permissible, so long as they do not compromise theobjectives of the DVE, whether it be effective training or satisfied customers in anentertainment system. The goal of the time management system in the DVE is toensure that participants perceive a common view of temporal aspects of the DVE.For example, it may be important for two participants observing a common set ofevents to perceive them in the same sequence.9.1 THE PROBLEMTo illustrate why a DVE might need a time management system, consider acollection of realtime simulators interconnected via a network. Each simulatorsends messages to the other simulators when it performs some action of interest tothe other simulators. Consider a scenario involving three simulators in a simulatedbattlefield, a tank, a target, and a command post containing observers monitoringengagements see Fig. 9.1. Suppose that the tank fires on the target, causing it to bedestroyed, while the observers watch this engagement. In the distributed simulatorthis engagement unfolds by the tank simulator first generating a message indicatingthat it is firing upon the target. This message is sent to both the target and theobserver simulators. Upon receiving this message, the target simulator models thedetonation of the round, and determines that it has been destroyed. The targetsimulator then generates a new message indicating this fact. The observer simulatorreceives both the original tankfiring message and the targetdestroyed message.However, suppose that the tankfiring message is delayed in the network, causing theobserver simulator to receive the targetdestroyed message first. In other words, theobserver might see the target being destroyed before it sees that the tank has firedupon itThis problem occurs because in the physical world, timing delays are governed bynatural phenomena, such as the speed of light. However, in the simulated world,delays are affected by artificial phenomenon such as the latency a messagerQ260 TIME MANAGEMENT AND EVENT ORDERING 9.2 MESSAGEORDERING SERVICES 261fire eventSimulator AIr..,.tankSimulator B11..P....targetSimulator C observer  Wallclock TimeFigure 9.1 Scenario leading to a temporal anomaly. The anomaly can be eliminated bydelaying delivery of the destroyed event to simulator C.encounters as it is transmitted through the network. Thus timing relationships maybe distorted in the simulated world, causing anomalies such as impossible orderingsof events.It may be noted that this problem is an instance of the synchronization problemthat was studied in detail in Part II of this book, in the context of analytic simulationenvironments. Indeed, the synchronization algorithms described there are directlyapplicable to DVEs. However, it will be seen that other ordering mechanisms may beused to eliminate at least certain classes of temporal anomalies. The remainder ofthis chapter discusses these approaches, as well as the problem of synchronizingwallclock time across different simulators. In the discussions that follow, we assumethat a reliable message delivery mechanism is used in other words, every messagethat is sent is eventually delivered to its destination.It is important to note that temporal anomalies mayor may not represent asignificant problem in a DVE. For example, it could be that the exchange describedin Figure 9.1 happens so rapidly that the tankfiring and targetdestroyed eventsappear to happen simultaneously, in which case it may be perfectly acceptable todisplay them in the incorrect order. Similarly, if these anomalies do not occur veryfrequently, and can be easily ignored by users of the system without compromisingthe goals of the DVE, the computation and communication resources required toeliminate them may not be justified. Indeed, most DVEs today do not provideprotection against temporal anomalies.On the other hand, a more serious problem is temporal anomalies may lead todivergence in different portions of the virtual environment. For example, if twosimulators observe different orderings of the same set of events, they may attempt toevolve the virtual world in inconsistent ways, leading to impossible outcomes orother types of errors. An example illustrating this phenomenon will be describedlater. Thus a reasonable approach might be to use unordered communication servicesin situations where temporal anomalies cannot occur, or are not important, but to usesome type of time management mechanism where ordering is important.9.2 MESSAGEORDERING SERVICESHere, we are concerned with ensuring the distributed simulation executivecorrectly orders messages before delivering them to each simulator. A messageis said to be sent when a simulator invokes a primitive instructing the simulationexecutive to transmit a message, for example, invoking the Update Attribute Valuesservice in the High Level Architecture. A message is received when it is passed tothe distributed simulation executive at the destination, typically by the operatingsystem. The message is said to be delivered when the simulation executive passes themessage to the receiving simulator, such as invoking the Reflect Attribute Valuesservice in the HLA. In general, messages received by the distributed simulationexecutive may not be deliverable until correct ordering can be guaranteed. Thus onedisadvantage of ordering services is that they may introduce additional latency.We next describe three different types of ordering services causal order, causaland totally ordered CATOCS, and time stamp order. The algorithms used toimplement time stamp order are identical to those described in Chapters 3, 4, and 5.In particular, the conservative synchronization algorithms described in Chapter 3 aremost relevant because they do not require each simulator to realize a rollbackcapability.9.2.1 Causal OrderCausal ordering can be used to eliminate certain types of temporal anomalies. If thesimulation executive can determine that there is a dependency between two events, itwill ensure that these events are delivered to each simulator in an ordering consistentwith the dependence relationships.More precisely, causal ordering is based on Lamports happens before relationship Lamport 1978. The execution of each simulator3 is viewed as an orderedsequence of actions. For example, execution of a single machine instruction can beviewed as an action. Two specific actions of particular interest are sending andreceiving a message. The happens before relationship denoted by an arrow,  isdefined by the following rules1. If actions A and B occur in the same simulator, and A appears before B in theordered sequence of actions within that simulator, then A  B read Ahappens before B,2. If A is a message send, and B is receiving the same message, then A  B.3. If A  B, and B  C, then A  C transitivity.For example, Figure 9.2 shows a spacetime graph where time is wallclock timefor three simulators. Each box represents an event, and each arc a messagetransmission. In this figure   2 2  4 and by the transitive property31 More generally, causal ordering is usually defined for each process however, we use the tenus processand simulator synonymously here to simplify the discussion.262 TIME MANAGEMENT AND EVENT ORDERING 9.2 MESSAGEORDERING SERVICES 263When simulator i sends a message M, it increments Ci by one and places a vectorclock time stamp on the message with its new i.e., just incremented clock value.The vector clock time stamp indicates what messages had been delivered to thesender and what other messages the sender has sent, prior to sending M. The timestamp therefore provides a summary of the number of messages sent by eachsimulator that causally precede i.e., happen before M. For example, a vector clocktime stamp 5, 9, 6 on a message M sent by simulator 1 indicates that prior tosending M2. Cj j i i indicates the number of group messages that have been deliveredto simulator i that were sent by j.kTWallcloc ImeSimulator A .......Simulator B ...L.....jjjrSimulator CfL.....11IFigure 9.2 The happens before relationship. Here   4 but  and 3 are concurrentevents.Figure 9.3 Implementation of causal ordering using vector clocks. Vector time stamps areindicated in parentheses, and vctor clocks are indicated in brackets.1. Simulator 1 had sent 4 messages to the group.2. Nine messages from simulator 2 were delivered to simulator 1.3. Six messages from simulator 3 were delivered to simulator 1.These 19 messages 4  9  6 causally precede i.e., happen before M, so for anysimulator belonging to the group, all 19 messages must be delivered before M isdelivered in order to maintain the causal ordering property.For example, Figure 9.3 illustrates the example depicted in Figure 9.1 whencausal ordering is used. Simulators A, E, and C in Figure 9.1 have been renamed 1,2, and 3, respectively, to be consistent with the notation used in this section. Thereare three simulators, so each initializes its vector clock to 0, 0, 0. When simulator 1sends a message to indicate that it has fired, it increments its vector clock to 1, 0, 0and places this vector into the time stamp field of the message.Consider the distributed simulation executive implementing causal ordering forsimulator 2. When the message arrives, the executive must decide whether or not itWallclock Time1,0,01 1,1,02,1,0.....  ......2,0,00,0,0 fire 1,0,00,0,00,0,0Simulator 1tankSimulator 2targetSimulator 3observerI. Ci  the number of messages simulator i has sent to the group, andE  E4 . Intuitively, if there is a lefttoright path from an event Ex to another eventEy in the spacetime graph, then Ex  Ey TW events tha are not related by the relationship i.e., two events where no lefttonght path eXists from one to the otherare referred to as concurrent events. In Figure 9.2, E1 and E3 are concurrent events.If there is a path between two events in the spacetime graph, then this implies thefirst event might affect the second that is, there is a potential dependence betweenthese two events.A causal order delivery service guarantees that if E  E2 , then each simulatorreceiving both messages will have E delivered to it before E2 is delivered.Concurrent events may be delivered to the simulator in any order.It is important to note that the happens before relationship is independent of thesemantic meaning of the events in the simulation. It is possible that E and E4 inFigure 9.2 are independent events that have no relationship to each other so far as thesimulation is concerned. In this case the causal ordering mechanism will infer acausal relationship exists for the purpose of message ordering, even though noneactually exists. Thus causal ordering may be overconservative in ensuring events areproperly ordered.The happens before relationship can be extended to describe relationshipsbetween messages. In particular 1 if a message M is sent by a simulator before thesame simulator sends another message M2 , then M j  M2  2 ifM is delivered toa simulator before that simulator sends another message M 2 , then M  M 2  3 ifM  M2 , and M 2  M3, then M  M 3 . . .Causal ordering will eliminate temporal anomalies such as that shown III Figure9.1. Specifically, because the tankfires event happens before the targetdestroyedevent, the distributed simulation executive will delay delivery of the target destroyedevent until after it has delivered the tank fired event. This is illustrated by the dashedline in Figure 9.1.Causal order can be implemented for messages sent to a multicast group using aconstruct called avector clock. Consider a closed group containing N simulators.Assign each simulator in the group a number 1, 2, ... ,N. Each simulator imaintains a vector of counters C where264 TIME MANAGEMENT AND EVENT ORDERING 9.2 MESSAGEORDERING SERVICES 265attack aircraft 2attack aircraft 2Wallclock TimeSimulatorD .....1.L...t. blue aircraft 2..Simulator C1.t.3.....Iblue aircraft 1SimulatorBlhred aircraft 29.2.2 Causal and Totally OrderedWhile causal ordering prevents certain temporal anomalies, it does not eliminateothers. One problem is that causal order does not guarantee any order concerningconcurrent events. In particular, it is possible for two different simulators to see thesame set of events in a different order. This can lead to other temporal anomalies.To illustrate this problem, consider the scenario depicted in Figure 9.4. Here,there are four flight simulators, two on the red team about to take off from an airportand two on the blue team flying overhead. The blue team has devised a strategywhereby blue aircraft 1 will attack the first red aircraft to take off and blue aircraft 2will attack the second. Suppose that red aircraft I takes off first, followed by redaircraft 2. The simulator for each aircraft generates a message indicating that it hastaken off. Because the two aircraft take off, events are concurrent there is no pathbetween them in the spacetime diagram the message delivery system is free todeliver them in any order to the receiving simulators. In the example depicted inFigure 9.4, blue aircraft 2 correctly sees red aircraft I take off first. However, blueaircraft I sees the incorrect order. Thus, both blue aircraft move to attack aircraft 2, asituation which is impossible with the strategy that they had devised.This problem could be solved if the causal order delivery service were strengthened so that not only are events delivered in causal order but all simulators are alsoguaranteed to receive messages for a common set of events in the same order. Inother words, a total order of all events must be defined that is, some orderingrelationship is specified for every two events. By contrast, Lamports happens beforemany multicast groups large distributed simulations may contain thousands ofgroups and one must ensure causal order among messages sent to different groups.Further, causal ordering does not prevent all temporal anomalies, as will bedescribed next.take off firstSimulator A1....red aircraft 1Figure 9.4 Temporal anomaly resulting from two simulators seeing the same sets of events,but in a different order.1. Ts  CAs  1, and2. Tj Crj for all j f s.can deliver this message. In this case, simulator 2s local clock is 0, 0, 0, and T, thetime stamp on the incoming message, is 1, 0, 0. The fact that Tl  C21  1indicates that all messages generated by simulator 1 in this case there are no suchmessages preceding this one have already been delivered to simulator 2. The factthat T2  C22 and T3  C23 indicates that all of the messages for this groupthat had been delivered to simulator 1 when it sent the fire message have alreadybeen delivered to simulator 2. Thus this message is not causally dependent on anyother messages that had not already been delivered to simulator 2, and the messagemay be delivered. Simulator 2s vector clock can be advanced to 1, 0, 0.Next, simulator 2 generates a new message and advances its clock from 1, 0, 0to 1, 1, 0. When this message arrives, the executive for simulator 3 notices thesender had previously received a message from 1 because T 1  1, but simulator 3had not yet received this message because C31  0. Indeed, this is the messagefor the fire event. Thus the destroyed message cannot be delivered to simulator 3yet. When the message for the fire event with time stamp 1, 0, 0 arrives, this eventis delivered to simulator 3, and 3s clock is advanced to 1, 0, 0. The messagecarrying the destroyed event with time stamp 1, 1, 0 can now be delivered tosimulator 3.In general, if a simulator r has local vector clock Cr , then a message fromsimulator s with vector time stamp T can only be delivered to r ifThe first condition guarantees this is the next message from simulator s.Successive messages from the same simulator are clearly causally dependent, sothey must be delivered in the order in which they were sent. The second conditionguarantees that any messages delivered to the sender before sending this messagehave also been delivered to the receiver. This ensures that messages such as the fireevent which causally precedes the targetdestroyed message have been deliveredbefore the targetdestroyed message is delivered. It may be noted that if Tj Crj, the message can be delivered. This implies that some messages have beendelivered to the receiver that the sender had yet received when it generated themessage. This does not violate the causal order guarantee. For example, Figure 9.3shows a second message generated by the tank simulator with time stamp 2, 0, 0.This message can be delivered to simulator 3 any time after the fire event with timestamp 1, 0, 0 has been delivered. In the scenario depicted in Figure 9.3 it isdelivered after the fire and destroyed messages, when simulator 3s local clockis 1, 1,0. In this case the second component of the messages time stamp 2, 0, 0 isless than the second component of the observers clock 1,1,0.Causal order delivery does ensure that causally related events are received in aproper order. The disadvantage of this approach for distributed simulation applications is the need for potentially large vector time stamps, since there may bethousands of simulators in a distributed simulation exercise. The algorithms forensuring causal ordering also become much more complex when one must consider266 TIME MANAGEMENT AND EVENT ORDERING 9.2 MESSAGEORDERING SERVICES 267comes withinrangefire eventissue ordersWallclock TimeFigure 9.5 Example illustrating an incorrect event ordering.Simulator A,,..IIr.. tankcomes withinSimulator Brange....t vehicle 1Simulator C..rvehicle 2Wallclock TimeFigure 9.6 Example jllustrating hidden dependencies between events.Simulator B, L....h battalion 1Simulator .... conunanderSimulatorD.  enemydiversion reach federates representing the opposing force before messages corresponding to the main attack The problem is that all message ordering in CATOCS isbased only on messages passed between federates. Thus, as noted earlier, semanticrelationships between events are not visible to the distributed simulation executive.Simulator C. Ibattalion 29.2.3 Delta CausalityDelta causality is a variation on causal ordering that was developed for realtimeapplications. It is based on the idea that in applications such as DVEs, informationthat is old for example, because it has been delayed in the network for anexcessively long time is no longer useful. This concept was used in DIS, forexample PDUs that exceed the 100 or 300 millisecond latency constraint can beignored by the receiver.With delta causality, messages are assigned a certain expiration time, defined asthe time the message was sent plus some time value delta. Messages received at thedestination before the expiration time has elapsed are delivered in causal order.Ordering Concurrent Events CATOCS ensures that the simulators observethe same ordering of concurrent events but does not guarantee that they will see thecorrect ordering of these events. Consider the example shown in Figure 9.5. Here,simulator A modeling a tank has orders to pursue the first vehicle it sees. In thisexample, vehicle I comes into range first, but because its message is delayed, thetank receives a message from vehicle 2 first, so it decides to pursue that vehicle. Itshould be noted that pursuing the wrong vehicle may not necessarily compromisethe virtual environment, especially if none of the participants can detect that this hashappened. Of potentially greater concern is if network delays are such that oneplayer systematically receives an advantage or disadvantage in such situations,leading to an unfair fight.Hidden Dependencies Consider a battle in a military campaign that is stagedas a timed sequence of actions. For example, a diversion might be initiated by oneunit at time 400, followed by initiation of the actual attack by another unit at time410 see Fig. 9.6. It is clear that the commanders planning the operation staged itsexecution so that the diversion occurs before the main attack. However, neithercausal ordering nor CATOCS guarantees that messages corresponding to the32 Clearly any mechanism for implementing CATOCS trivially also realizes causal order.relationship only defines a partial ordering because no ordering relationship isspecified for concurrent events. A causal and totally ordered communication serviceCATOCS ensure that 1 ifEI  E2 and a simulator receives messages for both Eand E2, then E I will be delivered before E2 and 2 if two simulators both receivemessages for any two events E and Eb both will receive them in the same order.Note that CATOCS still does not specify what order concurrent events will bedelivered to each simulator, only that all simulators will receive the messages in thesame order.CATOCS will eliminate the anomaly shown in Figure 9.4 because it will ensurethat both blue aircraft observe the red aircraft taking off in the same order. Thus itwill prevent the two blue aircraft from attacking the same opponent.A simple approach to realizing CATOCS32 is to send each message to a centralsequencer, that then sends the message to the group. If each receiver ensures itreceives messages from the sequencer in the order that they were sent, it isguaranteed that all messages are received in the same order. Ensuring that messagesreceived from the sequencer are received in a correct order can be accomplished bymaintaining a counter in the sequencer that is incremented with each message send.The counter is included in each message sent by the sequencer. Each destinationsimulator can only deliver a message with sequence number i when it has deliveredall of the messages with smaller sequence numbers.While causal ordering and CATOCS are sufficient to avoid certain anomalies,neither is sufficient in other situations. Specifically, causal order is not sufficient ifordering relationships among concurrent events are important, or if there arehidden dependencies between events, as elaborated upon below.268 TIME MANAGEMENT AND EVENT ORDERING 9.3 SYNCHRONIZING WALLCLOCK TIME 2699.3 SYNCHRONIZING WALLCLOCK TIMEFigure 9.7 Implementing time stamp order in a DVE with maximum latency M and clockoffset O.Simulator AI.Ii Simulator B,,I II II IEach processor in a distributed computing environment usually maintains a local,hardware clock indicating the time of day wallclock time. This clock may be usedto assign time stamps to events. Thus it is important to ensure that the clocks indifferent processors remain in relatively close synchrony with one another, orconfusion will result concerning event time stamps.There are two forms of the clock synchronization problem. If the distributedsimulation is self contained and does not need to interact with external elements thatare tightly synchronized to true time or UTC, as discussed below, then it isusually sufficient to ensure that the hardware clocks used by the different processorsremain in synchrony with each other. These clocks may drift with respect to UTCwithout noticeable consequences. On the other hand, if the simulation does interactwith devices synchronized to UTC, such as a hardwareintheloop simulation whichWallclock Time in BIIT TtMThe disadvantages of this approach relative to the synchronization mechanismsdeveloped for analytic simulations are 1 the requirement of a guaranteed maximumlatency and clock offset and 2 message delivery may be delayed longer thannecessary. Regarding l, if a bound on latency and offset cannot be determined, onecan dictate that messages exceeding this maximum latency may be delivered out oforder, similar to the delta causality approach. If this is infrequent and simulators canbe guaranteed not to diverge should they receive certain messages late, this is areasonable and practical approach.Regarding 2, this scheme in effect enforces a maximum latency M on everymessage, even if the network could deliver the message with a latency smaller thanM. Conservative synchronization protocols use lookahead to circumvent thisproblem. On the other hand, if M is sufficiently small, this drawback may beacceptable. For example, recall that it has been observed latencies lower than 100to 300 milliseconds are beyond what humans can perceive. This suggests thatif the network can maintain latencies below this level, the system will performsatisfactorily.Causal ordering is not guaranteed for messages received after the expiration time haselapsed.9.2.4 Time Stamp OrderIn time stamp order, each event is assigned a time stamp, and messages are deliveredto each simulator in order of nondecreasing time stamps. Time stamp ordereliminates the temporal anomalies that can occur with causal order and CATOCS.It ensures that beforeandafter relationships are properly modeled by the simulator,and assuming that events containing the same time stamp are handled in the sameorder by all simulators, it ensures that all see the same ordering of events.The fundamental difference between the causal and timestamp ordered services isconcerned with their respective definition of the happens before relationship. In atimestamped order service, an event is said to happen before another event if it has asmaller time stamp than the second, following ones intuitive notion of eventordering in physical systems. Simulators can precisely specify which eventshappen before which other events by assigning appropriate time stamps. Forexample the fire event might be assigned a time stamp of 102, and the destroyevent a time stamp of 103 to specify that the fire event happens before the destroyevent.One approach to realizing time stamp order is to utilize synchronizationalgorithms such as those discussed in Part II of this book. The central disadvantagesof using this approach are l the reliance on lookahead, or optimistic processingwhich requires simulators to implement a rollback capability, and 2 the computation and communication overhead necessary to realize these algorithms.Another approach to realizing the time stamp order delivery service can be used ifthe maximum message latency can be bounded. Assume that each processorcontains a synchronized wallclock time clock, and the maximum message latencyis M. Each processor assigns a time stamp on each message it sends equal to itscurrent wallclock time. Assume for a moment that the clocks are perfectlysynchronized. Each processor can collect incoming messages into a queue. Amessage is eligible for delivery if its time stamp is less than T  M, where T isthe receivers current clock value. This is because it is guaranteed all messages withtime stamp less than T M must have been received by their destination processor,under the maximum latency constraint.In general, clocks will not be perfectly synchronized, especially in lowcostapplications where one cannot afford highly accurate clocks. This approach can beextended to account for this situation if the difference between any two clocks in thesystem can be bounded. In particular, let 0 be the maximum offset differencebetween two clocks. Figure 9.7 depicts a worstcase scenario. Let T be the wallclocktime of the receiver, simulator B, when simulator A sends a message. Simulator Acan place a time stamp as small as T  0 on the message. The message could bereceived by simulator B as late as wallclock time T  M. Thus messages with timestamp less than or equal to the processors current time minus M  0 can be safelydelivered while maintaining time stamp order.270 TIME MANAGEMENT AND EVENT ORDERINGincludes components that produce outputs at specific points in time, then one mustensure that these clocks also remain in synch with UTe. This, of course, also impliesthat the clocks in the distributed simulation must remain in synchrony with eachother.9.3.1 Time and Time SourcesHistorically time has been defined in terms of the duration between periodic,astronomical events. A solar day is defined as the amount of time that elapsesbetween successive times when the sun is at the highest point in the sky. The solarday is divided into 24 hours, which is, in tum, divided into 3600 seconds. Thus asolar second is 186400 of a solar day. Greenwich Mean Time is the mean solar timeat the Greenwich meridian that passes through the Greenwich observatory nearLondon, England.Today, atomic clocks are used to define standard sources of time. Time duration isbased on transitions of the cesium atom. One slight complication is this time sourcedoes not take into account the fact that the earths rotation is gradually slowingdown, so the length of a day as defined by astronomical phenomena is actuallyincreasing slightly as time progresses. To compensate for this, extra leap seconds areperiodically added to the values produced by the atomic clocks, giving rise to aninternational standard for time called Universal Time Coordinated UTC, usuallyspoken Coordinated Universal Time. Without the introduction of these extra ticks,noon would gradually occur earlier and earlier in the day i.e., with the sun at alower point in the sky, creating obvious problems. UTC is widely accepted as theinternational standard with respect to defining the current time.UTC can be obtained from various standards organizations via radio, satellite,and telephone links. For example, in the United States, the National Institute ofStandards and Technology NIST maintains a shortwave radio broadcast thatdisseminates UTC, and the Geostationary Environment Operational SatellitesGEOS also provide UTe. Other services provide telephone access via a modem.Radio broadcasts allow time estimates to be accurate within about 10 milliseconds.Satellite broadcasts offer better accuracy, often within 0.5 millisecond, or less. Theprincipal limitation in achieving perfectly accurate time is uncertainty in thepropagation delay from the time source to the receiver.There are essentially two sources of time that can be used by a computer An internal hardware oscillator, derived from a quartz crystal, that produces anelectrical signal of some known, fixed, frequency. An external time service providing UTC one can purchase devices calledtime providers to receive and interpret UTC signals.The internal hardware clock within a computer usually contains a counter that isdecremented once per clock cycle. When the counter reaches zero, it is reset to somefixed value stored in a hardware register, and an interrupt is generated to the cpu.9.3 SYNCHRONIZING WALLCLOCK TIME 271Each such interrupt is referred to as a clock tick. The constant register is set to hold avalue to produce interrupts at some fixed known frequency, such as every sixtieth ofa second.9.3.2 Clock Synchronization AlgorithmsThe problem with hardware clocks derived from oscillators is that they drift that isin practice, they run slightly faster or slower than UTC. Ideally the hardware clock Cwould increase at a rate dC1dt equal to one, where one second on the hardware clockcorresponds to one second of UTe. In practice, the clock advances at a rate dC1dTwhere I  p s dCjdT s I  p, with p defined as the maximum drift error perunit of UTC. If all clocks operate with this specification, in the worst case, twoclocks may drift apart at a rate of 2p per unit UTe. This could be a problem,particularly if the distributed simulation exercise has a long duration. Thus somemechanism is required to resynchronize the hardware clocks to correct for drift, andif necessary, to maintain synchronization with UTC.The clock offset 5 refers to the maximum difference between two clocks that isallowed. If two clocks drift apart at a rate of 2p per unit UTC time, after 52p UTCtime units they will have drifted 5 time units apart, so resynchronization must bedone at least once every 52p time units.There are three classes of clock synchronization algorithmsI. Centralized, pull time service. Each processor requests the current time from acentral time server, and receives a value in response. These are also sometimescalled passive server algorithms because the time service is only invoked uponrequest.2. Centralized, push time service. Here, a central time server periodically sendsnew time values to the processors without them having to request it.3. Distributed algorithms. No central server is used, but rather the processors usea distributed computation to synchronize their clocks.The centralized approaches are particularly well suited when an external UTCtime service is utilized. In this case the processor with the UTC connection isassumed to have the correct time, and the problem becomes one of distributingestimates of the UTC to other processors. Distributed algorithms tend to be gearedtoward making sure that the clocks in different processors do not drift too far apart.In both cases a principal challenge in distributing time among the processors is thedifficulty of measuring and predicting delays in transmitting messages over thenetwork. In the case where there is no UTC server, an additional problem is definingthe correct time value, since there is no standard reference time source. Averagingtechniques are usually used for this purpose.Centralized, Pull Algorithms The processor requests a time value by sending amessage to the server and then waits for a reply. Assume that the time of the272 TIME MANAGEMENT AND EVENT ORDERING 9.3 SYNCHRONIZING WALLCLOCK TIME 273Requestor Time Serve.....3l,4LI,IWallclock TimeTFigure 9.8 Centralized, pull algorithm.requestors clock is Ts when the message is sent, and the reply is received at time TRsee Fig. 9.8. The problem is if the time value provided by the server is T, and thelatency to transmit this information from the server to the requestor is L, then thetime when the message was received is T L, but L is unknown. Without anyadditional information, one might assume the latency through the network for therequest and response messages are the same.33 Using this assumption, if I is theamount of time required by the server to receive and process the time requestmessage, the requestor can estimate the time at which the message was received isT  TR  Ts  12. The latter term is an estimate of the latency for the responsemessage returned from the time server.To obtain a better estimate of the network latency, one can collect severalmeasurements of TR  Ts, discard those above a certain threshold as being outliersthat are not representative measurements, and then average the remaining measurements Cristian 1989. This will give a better estimate of the actual latency, at thecost of requiring additional message traffic to produce the estimate.Centralized, Push Algorithms Rather than have each processor poll the timeserver to request time values, the server can simply broadcast new time values on aregular basis. Each processor receiving the new time value must compensate byadding an estimate of the latency for the value to travel through the network. Thismust be known a priori because each processor does not have a means for estimatingthe latency, without generating additional messages.If there is no UTC connection providing a standard time reference, a true valueof time must be derived from some other means. Using one processors clock makesthe entire distributed simulation reliant on the accuracy of one machine. A betterapproach is to average the clocks from several machines.The algorithm used in Berkeley Unix uses such an approach. A central timeserver polls each of the other processors to get their current time value. It thenaverages the values that are received and notifies the other processors that shouldresynchronize their clocks to add or remove a certain amount of time.33 Latencies tend to be symmetric in local area networks, but are often asymmetric in wide area networkssuch as the Internet.Distributed Algorithms The centralized approaches suffer from the usualproblems of centralization, namely having a single point of failure and lack ofscalability to large numbers of processors. Distributed time synchronization algorithms alleviate this problem.A simple approach is to have each processor broadcast its time at a certain,agreedupon frequency, much like the way radio stations broadcast the time at thebeginning of each hour. More precisely, each processor broadcasts its time every Rtime units, starting from some agreed upon initial time To. Thus each broadcast ismade at time To  iR for integer values of i. The broadcasts will not actually occursimultaneously because the clocks in different processors are not identical. Afterissuing a broadcast, each processor collects time messages from other processorsover a certain time interval and then computes a new time value by averaging thevalues received from other processors. The highest and lowest m values may bediscarded from this averaging process to protect against unreliable values. One canalso compensate for network latency by using an estimate ofmessage delays, defineda priori or through measurements.Broadcasting time estimates in this way does not scale to a large number ofprocessors. An alternative approach is to limit each processors broadcast to some setof processors. The processors can be organized into a virtual topology, such as amesh. Each processor would then only send its time message to its nearest neighborsin the mesh and perform the averaging on values received from its neighbors.9.3.3 Correcting Clock Synchronization ErrorsOnce the clocks have been synchronized, the processors local clock must becorrected to reflect the newly computed time value. Simply resetting the localclock to the new time value is usually not a very good idea because this could causetime to go backward which could result in problems in the simulation computations.For example, simulators using dead reckoning algorithms to compensate for latencymay not expect a negative time duration. Actually it is undesirable for time to makeabrupt changes forward as well. For example, if the user in a training simulator hasthree seconds to respond before the aircraft goes into an uncontrollable spin, asudden increase in time could prematurely cause the users allowed reaction period toend, not providing adequate opportunity to respond. If one were to simply reset theclock whenever a new value is computed, there would be abrupt changes, or timecould actually move backward.To pre.vent problems such as these, the new time value should be graduallyphased in. For example, if the clock is 10 milliseconds too fast, and a clock tickinterrupt is generated every 30 milliseconds, one could reset the interrupt software toincrement time by 29 milliseconds on each interrupt for 10 successive interrupts.This will avoid abrupt changes and backward movement of time.274 TIME MANAGEMENT AND EVENT ORDERING9.3.4 Network Time ProtocolThe Network Time Potocol NTP is used in the Internet to provide a synchronizedclock service. It is built on top of the user datagram protocol UDP and the InternetProtocol lP. NTP uses a selforganizing, hierarchical treebased, subnetwork ofclock servers. A set ofprimary servers reside at the top level 1 of the hierarchy thatare synchronized with national time servers. Servers at lower levels levels 2, 3, etc.of the tree are called secondary servers. Clock synchronization information flowsdown the tree in order to determine the offset between primary and secondary serverclocks.To determine the offset between clocks, a message is sent from a server at level iof the hierarchy to a peer at the next level, and returned. Four time stamps areassigned to the message see Fig. 9.9. Tj holds the local clock of the server when itsends the message. The time of the peer at this instant is T1  6, where 6 is the offsetbetween the two clocks. The second time stamp T2 represents the time of the peerwhen the message is received, or T1  6  L, where L is the message latency fromserver to peer. The third time stamp T3 is the peers clock when it returns themessage to the server. At this instant, the servers local clock is T3  6, assumingthat the offset has not changed. The final time stamp T4 is the local time of the serverwhen it receives the message, or T3  6  L, where L is the message latency frompeer to server.LetA be equal to T2  T1 i.e., 6  L and B be equal to T3  T4 i.e., L. Thenthe roundtrip delay L L is AB. Further, since L must be greater than zero,A  6  L  6 that is, A provides an upper bound on the clock offset. SimilarlyB  6  L  6 that is, B is a lower bound on the offset. The estimated offset 6e iscomputed to be the midpoint between these two boundsThe width of the interval is the roundtrip latency L  L, so the maximum error inthis estimate for the offset is half the roundtrip latency.Offset and delay estimates are collected from several clock sources. NTP uses aclock selection algorithm that selects a suitable subset of these sources with whichthe peer synchronizes. Clock selection is accomplished using two subalgorithms.ServerPeerWallclock TimeFigure 9.9 Message exchange to determine offset and roundtrip latency in NTP.9.5 ADDITIONAL READINGS 275The first is called the interval intersection algorithm. This algorithm is designed todetect and discard from the set unreliable clocks that are called falsetickers. Theinterval intersection algorithm determines a confidence interval for the offset of eachclock based on the measured offset and various error estimates. The actual offset lieswith very high probability within this confidence interval. The intersection algorithmdetermines a subset containing at least K clocks where it attempts to ensure theconfidence intervals of all K clocks have at least one point in common, and the sizeof the overlapping interval is minimized. Clocks with intervals falling outside thisoverlapping interval are designated as falsetickers and are discarded.The second subalgorithm of the clock selection algorithm is called the clusteringalgorithm. This algorithm is applied to the clocks that survive the interval selectionalgorithm. It performs a computation to identify and reject the outlier among themeasured offsets, taking into account the level of the clock source in the hierarchyand the size of its interval. This process is repeated until either a specified minimumnumber of clocks is reached or the estimated error is determined to be sufficientlysmall. Details of these calculations are somewhat involved and are beyond the scopeof the current discussion, but are described in Mills 1995.Once the clock selection algorithm whittles down the set of available clocks to aset of trusted clocks, another computation called the clock combining algorithmcomputes a weighted average of the remaining clock estimates. The value resultingfrom this computation is used to adjust the frequency of the local oscillator which isused to generate a time reference for producing the time stamps used by the clocksynchronization algorithm.9.4 SUMMARYTime management and eventordering mechanisms are used to reduce or eliminatetemporal anomalies such as the effect appearing to happen before the cause. Causalorder and causal and totally ordered message delivery based on Lamports happensbefore relationship can be used to ensure correct ordering of messages that may bedependent on each other. Delta causality takes into account the timely nature ofinformation in a DVE. Time stamp order eliminates anomalies that cannot beaddressed with causalordering mechanisms.Wallclock time must be synchronized among different machines in DVEs toensure that a consistent notion of time is maintained across the system. Several clocksynchronization algorithms are available for this purpose. In some cases wallclocktime must also be synchronized with international time standards such as UTC.9.5 ADDITIONAL READINGSThe happens before relationship used to define causal ordering is defined in Lamport1978. Several approaches to realizing causal ordering have been proposed. Forexample, see Fidge 1988, Schiper, Eggli et al. 1989, Birman, Schiper et al.276 TIME MANAGEMENT AND EVENT ORDERING1991, Melda1, Sankar et al. 1991, Rayna1, Schiper et al. 1991, Singhal andKshernka1yani 1992, and Schwarz and Mattern 1994. Delta causality is describedin Yavatkar 1992 and Adelstein and Singhal 1995. Messageordering alternativesthat were considered in the HighLevel Architecture are described in Fujimoto andWeatherly 1996. Clock synchronization algorithms are described in Kopetz andOchsenreiter 1987, Srikanth and Toueg 1987, LundeliusWelch and Lynch1988, Cristian 1989, Gusella and Zatti 1989, Ramanathan, Kandlur et al.1990, and Drummond and Babaoglu 1993 a survey is presented in Ramanathan,Shin et al. 1990. The network time protocol is described in Mills 1991.REFERENCESAbeysundara, B. Wand A. E. Kamal 1991. Highspeed local area networks and theirperformance A survey. ACM Computing Surveys 232 221264.Adelstein, F. and M. Singhal 1995. Realtime causal message ordering in multimediasystems. Proceedings of the 15th International Conference on Distributed ComputingSystems, IEEE 3643.Agrawal, D., M. Choy, et aI. 1994. Maya A simulation platform for distributed sharedmemories. Proceedings of the 8th Workshop on Parallel and Distributed Simulation,Society for Computer Simulation 15 1155.Ahuja, M. 1990. Flush primitives for asynchronous distributed systems. InformationProcessing Letters 512.Akyildiz, 1. F., L. Chen, et aI. 1993. The effect of memory capacity on Time Warpperformance. Journal ofParallel and Distributed Computing 184 411422.Ammar, H., and S. Deng 1992. Time Warp simulation using time scale decomposition. ACMTransactions on Modeling and Computer Simulation 22 158177.Andradottir, S., and T. Ott 1995. Timesegmentation parallel simulation of networks ofqueues with loss or communication blocking. ACM Transactions on Modeling andComputer Simulation 54 269305.Arvind, K., and C. Smart 1992. Hierarchical parallel discrete event simulation in compositeElsa. Proceedings ofthe 6th Workshop on Parallel and Disributed Simulation 24 147158.Avril, H., and C. Tropper 1995. Clustered Time Warp and logic simulation. Proceedings ofthe 9th Workshop on Parallel and Distributed Simulation 112119.Ayani, R. 1989. A parallel simulation scheme based on the distance between objects.Proceedings of the SCS Multiconference on Distributed Simulation, B. Unger and R. M.Fujimoto, Society for Computer Simulation 21 113118.Baccelli, F. and M. Canales 1993. Parallel simulation of stochastic petri nets usingrecurrence equations. ACM Transactions on Modeling and Computer Simulation 312041.Bagrodia, R., Y. Chen, et aI. 1996. Parallel simulation of high speed wormhole routingnetworks. Proceedings ofthe 10th Workshop on Parallel and Distributed Simulation, IEEEComputlr Society 4756.Bagrodia, R., WT. Liao, et aI. 1991. A unifYing framework for distributed simulation. ACMTransactions on Modeling and Computer Simulation 14.Bagrodia, R., and WT. Liao 1994, Maisie A language for the design of efficient discreteevent simulations, IEEE Transactions on Software Engineering 204 225238..........278 REFERENCES REFERENCES 279Bailey, M. L., J. v Briner, et a1. 1994. Parallel logic simulation of VLSI systems. ACMComputing Surveys 263 255294.Bailey, M. L., M. A. Pagels, et a1. 1993. How using buses in multicomputer programs affectsconservative parallel simulation. Proceedings of the 7th Workshop on Parallel andDistributed Simulation, SCS Simulation Series 23 93100.Bailey, R. W 1982. Human Performance Engineering A Guide for System Designers.Englewood Cliffs, NJ, Prentice Hall.Bain, W L. and D. S. Scott 1988. An algorithm for time synchronization in distributeddiscrete event simulation. Proceedings of the SCS Multiconference on DistributedSimulation, SCS Simulation Series 19 3033.Ball, D. and S. Hoyt 1990. The adaptive Time Warp concurrency control algorithm.Proceedings of the SCS Multiconference on Distributed Simulation 22 174177.Banks, J., J. S. Carson II, et a1. 1996. DiscreteEvent System Simulation. Upper SaddleRiver, NJ, Prentice Hall.Bellenot, S. 1990. Global virtual time algorithms. Proceedings of the SCS Multiconferenceon Distributed Simulation, Society for Computer Simulation 122127.Bellenot, S. 1992. State skipping performance with the Time Warp operating system.Proceedings of the 6th Workshop on Parallel and Distributed Simulation 24 5364.Bellenot, S. 1993. Performance of a risk free Time Warp operating system. Proceedings ofthe 7th Workshop on Parallel and Distributed Simulation 155158.Biles, WE., D. M. Daniels, et a1. 1985. Statistical considerations in simulation on a networkof microcomputers. Proceedings of the 1985 Winter Simulation Conference 388393.Birman, K., A. Schiper, et a1. 1991. Lightweight causal and atomic group multicast. ACMTransactions on Computer Systems 93 272314.Blanchard, T. D. and T. Lake 1997. A lightweight RTI prototype with optimistic publication.Proceedings of the Spring Simulation Interoperability Workshop.Blanchard, T. D., T. W Lake, et a1. 1994. Cooperative acceleration Robust conservativedistributed discrete event simulation. Proceedings of the 1994 Workshop on Parallel andDistributed Simulation. Edinburgh, Scotland 5864.Briner, J., Jr. 1991. Fast parallel simulation of digital systems. Advances in Parallel andDistributed Simulation, SCS Simulation Series, 23 7177.Bruce, D. 1995. The treatment of state in optimistic systems. Proceedings of the 9thWorkshop on Parallel and Distributed Simulation 4Q49.Bryant, R. E. 1977. Simulation of Packet Communication Architecture Computer Systems.Computer Science Laboratory. Massachusetts, Massachusetts Institute of Technology,Cambridge.Cai, W, and S. J. Turner 1990. An algorithm for distributed discreteevent simulationThecarrier null message approach. Proceedings of the SCS Multiconference on DistributedSimulation, SCS Simulation Series 22 38.Carothers, C., R. M. Fujimoto, et a1. 1995. A case study in simulating PCS networks usingTime Warp. Proceedings of the 9th Workshop on Parallel and Distributed Simulation,IEEE Computer Society 8794.Chai, A., and S. Ghosh 1993. Modeling and distributed simulation of a broadbandISDNnetwork. IEEE Computer 269.Chamberlain, R. D., and M. A. Franklin 1990. Hierarchical discrete event simulation onhypercube architectures. IEEE Micro 104 1020.Chamberlain, R. D., and C. D. Henderson 1994. Evaluating the use of presimulation inVLSI circuit partitioning. Proceedings of the 8th Workshop on Parallel and DistributedSimulation, Society for Computer Simulation 139146.Chandrasekaran, S., and M. Hill 1996. Optimistic simulation of parallel architectures usingprogram executables. Proceedings of the 10th Workshop on Parallel and DistributedSimulation, IEEE Computer Society 143150.Chandrasekaren, S., and M. D. Hill 1996. Optimistic simulation of parallel architecturesusing program executables. Proceedings of the 10th Workshop on Parallel Simulation143150.Chandy, K. M., and L. Lamport 1985. Distributed snapshots Determining global states ofdistributed systems. ACM Transactions on Computer Systems 31 6375.Chandy, K. M., and J. Misra 1978. Distributed simulation A case study in design andverification of distributed programs. IEEE Transactions on Software Engineering 5544Q452.Chandy, K. M., and J. Misra 1981. Asynchronous distributed simulation via a sequence ofparallel computations. Communications of the ACM 244 198205.Chandy, K. M., and R. Sherman 1989. The conditional event approach to distributedsimulation. Proceedings of the SCS Multiconference on Distributed Simulation, B. Ungerand R. M. Fujimoto, Society for Computer Simulation 21 9399.Chandy, K. M., and R. Sherman 1989. Space, time, and simulation. Proceedings ofthe SCSMulticonference on Distributed Simulation, SCS Simulation Series 21 5357.Chen, Y., v Jha, et a1. 1997. A multidimensional study on the feasibility of parallel switchlevel circuit simulation. Proceedings of the 11 th Workshop on Parallel and DistributedSimulation, IEEE Computer Society 4654.Cheung, S. 1994. Analysis of UITSEC 1993 DIS Demonstration Data. Institute forSimulation and Training, Orlando, FL.Chow, R., and T. Johnson 1997. Distributed Operating Systems and Algorithms. Reading,MA Addison Wesley.Chung, M., and Y. Chung 1991. An experimental analysis of simulation clock advancementin parallel logic simulation on an SIMD machine. Advances in Parallel and DistributedSimulation, SCS Simulation Series 23 125132.Cleary, J., F. Gomes, et a1. 1994. Cost of state saving and rollback. Proceedings of the 8thWorkshop on Parallel and Distributed Simulation 94101.Cleary, J., B. Unger, et a1. 1988. A distributed andparallel backtracking algorithm usingvirtual time. Proceedings of the SCS Multiconference on Distributed Simulation, Societyfor Computer Simulation 19 177182.Cleary, J. G., and J. J. Tsai 1997. Performance of a conservative simulator ofATM networks.Proceedings ofthe 11th Workshop on Parallel and Distributed Simulation, IEEE ComputerSociety 142145.Comfort, J. C. 1984. The simulation of a masterslave event set processor. Simulation 423117124.Concepcion, A., and S. Kelly 1991. Computing global virtual time using the multileveltoken passing algorithm. Advances in Parallel and Distributed Simulation. 23 6370.280 REFERENCES REFERENCES 281Costa, A., A. DeGloria, et al. 1994. An evaluation system for distributedtime VHDLsimulation. Proceedings of the 8th Workshop on Parallel and Distributed Simulation,Society for Computer Simulation 147150.Cota, B. A., and R. G. Sargent 1990. A framework for automatic lookahead computation inconservative distributed simulations. Proceedings of the SCS Multiconference on Distributed Simulation, SCS Simulation Series 22 5659.Cristian, F. 1989. Probabilistic clock synchronization. Distributed Computing 3 146158.DSouza, L. M., X. Fan, et al. 1994. pGVT An algorithm for accurate GVT estimation.Proceedings of the 8th Workshop on Parallel and Distributed Simulation 102110.Dahmann, J. S., R. M. Fujimoto, et al. 1997. The Department of Defense High LevelArchitecture. Proceedings of the 1997 Winter Simulation Conference.Damani, o. P., Y.M. Wang, et al. 1997. Optimistic distributed simulation based on transitivedependency tracking. Proceedings of the 11 th Workshop on Parallel and DistributedSimulation 9097.Das, S., R. M. Fujimoto, et al. 1994. GTW A Time Warp system for shared memorymultiprocessors. Proceedings of the 1994 Winter Simulation Conference 13321339.Das, S. R., and R. M. Fujimoto 1997. Adaptive memory management and optimism controlin Time Warp. ACM Transactions on Modeling and Computer Simulation 72 239271.Das, S. R., and R. M. Fujimoto 1997. An empirical evaluation of performance memorytradeoffs in Time Warp. IEEE Transactions on Parallel and Distributed Systems 82 210224.Davis,C. K., S. V Sheppard, et a1. 1988. Automatic development of parallel simulationmodels in Ada. Proceedings of the 1988 Winter Simulation Conference 339343.Davis, P. K. 1995. Distributed interactive simulation in the evolution of DoD warfaremodeling and simulation. Proceedings of the IEEE 838 11381155.Dee1man, E., and B. K. Szymanski 1997. Breadthfirst rollback in spatially explicitsimulations. Proceedings of the 11th Workshop on Parallel and Distributed Simulation,IEEE Computer Society 124131.Defense Modeling and Simulation Office 1996. Data Distribution and Management DesignDocument, VO.2. Washington, DC.Defense Modeling and Simulation Office 1996. High Level Architecture Object ModelTemplate. Washington, DC.Defense Modeling and Simulation Office 1996. High Level Architecture Rules, V1.0.Washington, DC.Defense Modeling and Simulation Office 1998. High Level Architecture Interface Specification, V1.3. Washington, DC.DeVries, R. C. 1990. Reducing null messages in Misras distributed discrete eventsimulation method. IEEE Transactions on Software Engineering 161 8291.Dickens, P., P. Heidelberger, et al. 1996. Parallelized direct execution simulation of messagepassing programs. IEEE Transactions on Parallel and Distributed Systems 710 10901105.Dickens, P. M., and J. Reynolds, P. F. 1990. SRADS with local rollback. Proceedings oftheSCS Multiconference on Distributed Simulation. 22 161164.Dijkstra, E. W, and C. S. Scholten 1980. Termination detection for diffusing computations.Information Processing Letters 111 14.Diot, C., W Dabbous, et al. 1997. Multipoint communication A survey of protocols,functions, and mechanisms. IEEE Journal on Selected Areas in Communications 153277290.DIS Steering Committee 1994. The DIS Vision, A Map to the Future of DistributedSimulation. Institute for Simulation and Training, Orlando, FL.Drummond, R., and o. Babaoglu 1993. Lowcost clock synchronization. DistributedComputing 6 193203.Earnshaw, R. W, and A. Hind 1992. A parallel simulator for performance modeling ofbroadband telecommunication networks. Proceedings of the 1992 Winter SimulationConference 13651373.Ebling, M., M. DiLorento, et al. 1989. An ant foraging model implemented on the timeWarp operating system. Proceedings of the SCS Multiconference on Distributed Simulation, SCS Simulation Series 21 2126.Fabbri, A., and L. Donatiello 1997. SQTW A mechanism for state dependent parallelsimulation Description and experimental study. Proceedings of the 11 th Workshop onParallel and Distributed Simulation 8289.Felderman, R., and L. Kleinrock 1991. Two processor Time Warp analysis Some results ona unifYing approach. Advances in Parallel and Distributed Simulation. 23 310.Ferscha, A. 1995. Probabilistic adaptive direct optimism control in Time Warp. Proceedingsof the 9th Workshop on Parallel and Distributed Simulation 120129.Fidge, C. 1988. Time stamps in messagepassing systems that preserve the partial order.Proceedings of the 11th Australian Computer Science Conference.Fishwick, P. A. 1994. Simulation Model Design and Execution Building Digital Worlds.New York McGrawHill.Fleischmann, J., and P. A. Wilsey 1995. Comparative analysis of periodic state savingtechniques in Time Warp simulators. Proceedings of the 9th Workshop on Parallel andDistributed Simulation 5058.Flynn, M. J. 1995. Computer Architecture Pipelined and Parallel Processor Design.Boston Jones and Bartlett.Franks, S., F. Gomes, et al. 1997. State saving for interactive optimistic simulation.Proceedings of the 11th Workshop on Parallel and Distributed Simulation 7279.Frohlich, N., R. Schlagenhaft, et al. 1997. A new approach for partitioning VLSI circuits ontransistor level. Proceedings ofthe 11th Workshop on Parallel and Distributed Simulation,IEEE Computer Society 6467.Fujimoto, R. M. 1983. Simon A simulator of multicomputer networks, ERL, University ofCalifornia, Berkeley.Fujimoto, R. M. 1989. Performance measurements of distributed simulation strategies.Transactions of the Society for Computer Simulation 62 89132.Fujimoto, R. M. 1989. Time Warp on a shared memory multiprocessor. Transactions oftheSociety for Computer Simulation 63 211239.Fujimoto, R. M. 1989. The virtual time machine. International Symposium on ParallelAlgorithms and Architectures 199208.Fujimoto, R. M. 1990. Performance of Time Warp under synthetic workloads. Proceedingsof the SCS Multiconference on Distributed Simulation 22 2328.282 REFERENCES REFERENCES 283Fujimoto, R. M., and M. Hybinette 1997. Computing global virtual time in shared memorymultiprocessors. ACM Transactions on Modeling and Computer Simulation 74 425446.Fujimoto, R. M., I. Nikolaidis, et al. 1995. Parallel simulation of statistical multiplexers.Journal ofDiscrete Event Dynamic Systems 5 115140.Fujimoto, R. M., and K. Panesar 1995. Buffer management in shared memory Time Warpsystems. Proceedings of the 9th Workshop on Parallel and Distributed Simulation 149156.Fujimoto, R. M., J. J. Tsai, et al. 1992. Design and evaluation of the roll back chip Specialpurpose hardware for Time Warp. IEEE Transactions on Computers 411 6882.Fujimoto, R. M., and R. M. Weatherly 1996. Time management in the DoD High LevelArchitecture. Proceedings of the 10th Workshop on Parallel and Distributed Simulation.Gafni, A. 1988. Rollback mechanisms for optimistic distributed simulation systems.Proceedings of the SCS Multiconference on Distributed Simulation. 19 6167.Gaujal, B., A. G. Greenberg, et al. 1993. A sweep algorithm for massively parallelsimulation of circuitswitched networks. Journal of Parallel and Distributed Computing184 484500.Ghosh, K. and R. M. Fujimoto 1991. Parallel discrete event simulation using spacetimememory. Proceedings of the 1991 International Conference on Parallel Processing. 3201208.Ghosh, K, K Panesar, et al. 1994. PORTS A parallel, optimistic, realtime simulator.Proceedings of the 8th Workshop on Parallel and Distributed Simulation 2431.Glass, K., M. Livingston, et al. 1997. Distributed simulation of spatially explicit ecologicalmodels. Proceedings of the 11 th Workshop on Parallel and Distributed Simulation, IEEEComputer Society 6063.Glynn, P. W. and P. Heidelberger 1991. Analysis of parallel replicated simulations under acompletion time constraint. ACM Transactions on Modeling and Computer Simulation11 323.Goldiez, B., S. Smith, et al. 1993. Simulator Networking Handbook. Institute for Simulationand Training. Orlando, FL.Greenberg, A. G., B. D. Lubachevsky, et al. 1991. Algorithms for unboundedly parallelsimulations. ACM Transactions on Computer Systems 93 201221.Groselj, B., and C. Tropper 1986. Pseudosimulation An algorithm for distributed simulationwith limited memory. International Journal ofParallel Programming 155 413456.Groselj, B., and C. Tropper 1988. The time of next event algorithm. Proceedings ofthe SCSMulticonference on Distributed Simulation, Society for Computer Simulation 19 2529.Gupta, A., I. F. Akyildiz, et al. 1991. Performance analysis of Time Warp with multiplehomogeneous processors IEEE Transactions on Software Engineering 1710 10131027.Gusella, R., and S. Zatti 1989. The accuracy of the clock synchronization achieved byTEMPO in Berkeley UNIX 4.3BSD. IEEE Transactions on Software Engineering 157847853.Hamnes, D.O., and A. Tripathi 1994. Investigations in adaptive distributed simulation.Proceedings of the 8th Workshop on Parallel and Distributed Simulation 2023.Hao, F., K. Wilson, et a1. 1996. Logical process size in parallel ATM simulations.Proceedings of the 1996 Winter Simulation Conference 645652.Heidelberger, P. 1986. Statistical analysis of parallel simulations. Proceedings of the 1986Winter Simulation Conference 290295.Heidelberger, P., and D. Nicol 1991. Simultaneous parallel simulations of continuous timeMarkov chains at multiple parameter settings. Proceedings of the 1991 Winter SimulationConference 602607.Heidele.rge.r, P., and H. Stone 1990. Parallel tracedriven cache simulation by Timepartlhonmg. Proceedings of the 1990 Winter Simulation Conference 734737.Hennessy, J. L., and D. A. Patterson 1996. Computer Architecture A Quantitative Approach.San Mateo, CA Morgan Kaufmann.Hering,. K., nd R..Haupt 1996. Hierarchical strategy of model partitioning for VLSIDeSIgn usmg an Improved mIxture of experts approach. Proceedings ofthe 10th Workshopon Parallel and Distributed Simulation, IEEE Computer Society 106113.Hiller, J. B., and T. C. Hartrum 1997. Conservative synchronization in objectorientedparallel battlefield discrete event simulations. Proceedings of the 11th Workshop onParallel and Distributed Simulation, IEEE Computer Society 1219.Hofer, R. C., and M. L. Loper 1995. DIS today. Proceedings ofthe IEEE 838 11241137.Hwang, K 1993. Advanced Computer Architecture Parallelism, Scalability, and Programmability. New York McGrawHill.IEEE Std 1278.11995 1995. IEEE Standard for Distributed Interactive SimulationApplication Protocols. New York Institute of Electrical and Electronics Engineers.IEEE Std 1278.21995 1995. IEEE Standard for Distributed Interactive SimulationCommunication Services and Profiles. New York Institute of Electrical and ElectronicsEngineers.Jefferson, D. R. 1985. Virtual time. ACM Transactions on Programming Languages andSystems 73 404425.Jefferson, D. R. 1990. Virtual time II Storage management in distributed simulation.Proceedings of the Ninth Annual ACM Symposium on Principles ofDistributed Computing 7589.Jefferson, D. R., B. Beckman, et al. 1987. The Time Warp operating systems. 11thSymposium on Operating Systems Principles. 21 7793.Jha, v., and R. Bagrodia 1994. A unified framework for conservative and optimisticdistributed simulation. Proceedings of the 8th Workshop on Parallel and DistributedSimulation 1219.Jones, D. w., C.C. Chou, et al. 1989. Experience with concurrent simulation. Proceedingsof the 1989 Winter Simulation Conference 756764.Kapp, K L., T. C. Hartrum, et al. 1995. An improved cost function for static partitioning ofparallel circuit simulations using a conservative synchronization protocol. Proceedings ofthe 9th Workshop on Parallel and Distributed Simulation, IEEE Computer Society 7885.Keller, .1 T. Rauber, et a1. 1996. Conservative circuit simulation on sharedmemorymultIprocessors. Proceedings of the 10th Workshop on Parallel and Distributed Simulation, IEEE Computer Society 126134.Kim, H. K., and J. Jean 1996. Concurrency preserving partitioning CCP for parallel logicsimulation. Proceedings of the 10th Workshop on Parallel and Distributed SimulationIEEE Computer Society 98105. 284 REFERENCES REFERENCES 285Knight, T. 1986. An architecture for mostly functional programs. Proceedings of the Lispand Functional Programming Conference. Cambridge, MA ACM.Konas, P. and P.C. Yew 1992. Synchronous parallel discrete event simulation on sharedmemory multiprocessors. Proceedings of the 6th Workshop on Parallel and DistributedSimulation, SCS Simulation Series 24 1221.Konas, P., and P.C. Yew 1994. Improved parallel architectural simulations on sharedmemory multiprocessors. Proceedings of the 8th Workshop on Parallel and DistributedSimulation, Society for Computer Simulation 156159.Kopetz, H., and W Ochsenreiter 1987. Clock synchronization in distributed realtimesystems. IEEE Transactions on Computers 368 933940.Krishnaswamy, D., P. Baneljee, et al. 1997. Asynchronous parallel algorithms for test setpartitioned fault simulation. Proceedings ofthe 11th Workshop on Parallel and DistributedSimulation, IEEE Computer Society 3037.Krishnaswamy, v., and P. Banneljee 1996. Actor based parallel VHDL simulation usingTime Warp. Proceedings of the 10th Workshop on Parallel and Distributed Simulation,IEEE Computer Society 135142.Kumar, P., and S. Harous 1990. An approach towards distributed simulation of timed petrinets. Proceedings of the 1990 Winter Simulation Conference 428435.Kumaran, K., B. Lubachevsky, et al. 1996. Massively parallel simulations of ATM systems.Proceedings ofthe 10th Workshop on Parallel and Distributed Simulation, IEEE ComputerSociety 3946.Kung, H. T., and J. T. Robinson 1981. On optimistic methods of concurrency control. ACMTransactions on Database Systems 62.Lamport, L. 1978. Time, clocks, and the ordering of events in a distributed system.Communications of the ACM 217 558565.Lavenberg, S., R. Muntz, et al. 1983. Performance analysis of a rollback method fordistributed simulation. Performance 83. Amsterdam North Holland, Elsevier Science, pp.117132.Law, A. M., and D. Kelton 1991. Simulation Modelling and Analysis. New York McGrawHill.Lin, YB. 1994. Determining the global progress of parallel simulation with FIFOcommunication property. Information Processing Letters 50 1317.Lin, YB., and E. Lazowska 1990. Determining the global virtual time in distributedsimulation. Proceedings ofthe International Conference on Parallel Processing 201209.Lin, YB., and E. D. Lazowska 1990. Exploiting lookahead in parallel simulation. IEEETransactions on Parallel and Distributed Systems 14 457469.Lin, YB., and E. D. Lazowska 1990. Optimality considerations of Time Warp parallelsimulation. Proceedings of the SCS Multiconference on Distributed Simulation. 22 144149.Lin, YB. and E. D. Lazowska 1991. A timedivision algorithm for parallel simulation. ACMTransactions on Modeling and Computer Simulation 11 7383.Lin, YB., E. D. Lazowska, et al. 1990. Comparing synchronization protocols for parallellogic simulation. Proceedings of the 1990 International Conference on Parallel Processing. 3 223227.Lin, YB., and B. R. Preiss 1991. Optimal memory management for Time Warp parallelsimulation. ACM Transactions on Modeling and Computer Simulation 14.Lin, Y.B., B. R. Preiss, et al. 1993. Selecting the checkpoint interval in Time Warpsimulations. Proceedings of the 7th Workshop on Parallel and Distributed Simulation 310.Lipton, R. J. and D. W Mizell 1990. Time Warp vs. ChandyMisra A worstcasecomparison. Proceedings of the SCS Multiconference on Distributed Simulation. 22137143.Liu, L. Z., and C. Tropper 1990. Local deadlock detection in distributed simulations.Proceedings ofthe SCS Multiconference on Distributed Simulation, SCS Simulation Series22 6469.Lomow, G., S. R. Das, et al. 1991. Mechanisms for user invoked retraction of events in TimeWarp. ACM Transactions on Modeling and Computer Simulation 13 219243.Lubachevsky, B. D. 1989. Efficient distributed eventdriven simulations of multipleloopnetworks. Communications of the ACM 321 111123.Lubachevsky, B. D., A. Shwartz, et al. 1991. An analysis of rollbackbased simulation. ACMTransactions on Modeling and Computer Simulation 12 154193.LundeliusWelch, J., and N. Lynch 1988. A new faulttolerant algorithm for clocksynchronization. Information and Computation 771 136.Macedonia, M., M. Zyda, et al. 1995. Exploiting reality with multicast groups A networkarchitecture for largescale virtual environments. 1995 IEEE Virtual Reality AnnualSymposium 1115.Madisetti, v., J. Walrand, et al. 1988. WOLF A rollback algorithm for optimistic distributedsimulation systems. Proceedings of the 1988 Winter Simulation Conference 296305.Madisetti, V. K., D. A. Hardaker, et al. 1993. The MIMDIX operating system for parallelsimulation and supercomputing. Journal of Parallel and Distributed Computing 184473483.Manjikian, N., and W M. Loucks 1993. High performance parallel logic simulation on anetwork of workstations. Proceedings of the 7th Workshop on Parallel and DistributedSimulation, SCS Simulation Series 23 7684.Mastaglio, T. W, and R. Callahan 1995. A largescale complex environment for teamtraining. IEEE Computer 287 4956.Mattern, F. 1993. Efficient algorithms for distributed snapshots and global virtual timeapproximation. Journal ofParallel and Distributed Computing 184 423434.Mehl, H. 1992. A deterministic tiebreaking scheme for sequential and distributed simulation. Proceedings of the Workshop on Parallel and Distributed Simulation. Society forComputer Simulation 24 199200.Mehl, H., and S. Hammees 1993. Shared variables in distributed simulation. Proceedings ofthe 7th Workshop on Parallel and Distributed Simulation 6875.Meldal, S., S. Sankar, et al. 1991. Exploiting locality in maintaining potential causality.Proceedings of the ACM Symposium on Distributed Computing 231239.Merrifield, B. C., S. B. Richardson, et al. 1990. Quantitative studies in discrete eventsimulation modeling of road traffic. Proceedings of the SCS Multiconference on Distributed Simulation, SCS Simulation Series 22 188193.286 REFERENCES REFERENCES 287Miller, D. C., and 1. A. Thorpe 1995. SIMNET The advent of simulator networking.Proceedings of the IEEE 838 11141123.Mills, D. L. 1991. Internet time synchronization The network time protocol. IEEETransactions on Communications 3910 14821493.Mills, D. L. 1995. Improved algorithms for synchronizing computer network clocks,ACMjIEEE Transactions on Networking 33 245254.Misra, 1. 1986. Distributed discrete event simulation. ACM Computing Surveys 181 3965.Mitra, D., and I. Mitrani 1984. Analysis and optimum performance of two message passingparallel processors synchronized by rollback. Performance 84. Amsterdam NorthHolland, Elsevier Science, pp. 3550.Mitre Corp. 1997. DPAT Detailed policy assessment tool, brochure. Center for AdvancedAviation System Development CAASD. McLean, Virginia.Morse, K. 1990. Parallel distributed simulation in Modsim. Proceedings of the 1990International Conference on Parallel Processing. 3 210217.Morse, K. 1996. Interest management in large scale distributed simulations, University ofCalifornia, Irvine.Morse, K. and 1. Steinman, 1997. Data Distribution Management in the HLA Multidimensional regions and physically correct routing, Proceedings of the Spring SimulationInteroperability Workshop, Orlando, Florida.Mouftah, H. T., and R. P. Sturgeon 1990. Distributed discrete event simulation forcommunication networks. IEEE Journal on Selected Areas in Communications 8917231734.Nandy, B., and W. Loucks 1992. An algorithm for partitioning and mapping conservativeparallel simulations onto multicomputers. Proceedings ofthe 5th Workshop on Parallel andDistributed Simulation, SCS Simulation Series 24 139146.Nicol, D., A. Greenberg, et al. 1992. Massively parallel algorithms for tracedriven cachesimulation. Proceedings ofthe 6th Workshop on Parallel and Distributed Simulation 24 311.Nicol, D. M. 1988. Parallel discreteevent simulation ofFCFS stochastic queueing networks.SIGPLAN Notices 239 124137.Nicol, D. M. 1991. Performance bounds on parallel selfinitiating discreteevent simulations.ACM Transactions on Modeling and Computer Simulation 11 2450.Nicol, D. M. 1993. The cost of conservative synchronization in parallel discrete eventsimulations. Journal of the Association for Computing Machinery 402 304333.Nicol, D. M. 1995. Noncommittal barrier synchronization. Parallel Computing 21 529549.Nicol, D. M., and X. Liu 1997. The dark side of risk. Proceedings of the lith Workshop onParallel and Distributed Simulation 188195.Nicol, D. M., and P. F. 1. Reynolds 1984. Problem oriented protocol design. Proceedings ofthe 1984 Winter Simulation Conference 471474.Nicol, D. M., and S. Roy 1991. Parallel simulation of timed petri nets. Proceedings of the1991 Winter Simulation Conference 574583.Nikolaidis, I., R. M. Fujimoto, et al. 1994. Time parallel simulation of cascaded statisticalmultiplexers. Proceedings of the 1994 ACM SIGMETRICS Conference on Measurementand Modeling ofComputer Systems 231240.Nitzberg, B., and V. Lo 1991. Distributed shared memory A survey of issues andalgorithms. Computer 248 5260.Palaniswamy, A. C., and P. A. Wilsey 1993. An analytical comparison of periodiccheckpointing and incremental state saving. Proceedings of the 7th Workshop on Paralleland Distributed Simulation 127134.Panesar, K., and R. M. Fujimoto 1997. Adaptive flow control in Time Warp. Proceedings ofthe 11th Workshop on Parallel and Distributed Simulation.Partridge, C. 1993. Gigabit Networking. Reading, MA Addison Wesley.Peacock, 1. K., 1. W. Wong, et al. 1979. Distributed simulation using a network ofprocessors.Computer Networks 31 4456.Phillips, C. I. and L. G. Cuthbert 1991. Concurrent discrete event simulation tools. IEEEJournal on Selected Areas in Communications 93 477485.Powell, E. T., L. Mellon, et al. 1996. Joint precision strike demonstration JPSD simulationarchitecture. 14th Workshop on Standards for the Interoperability ofDistributed Simulations. Orlando, FL, pp. 807810.Prakash, A., and R. Subramanian 1992. An efficient optimistic distributed scheme based onconditional knowledge. Proceedings of the 6th Workshop on Parallel and DistributedSimulation. 24 8596.Preiss, B. 1990. Performance of discrete event simulation on a multiprocessor usingoptimistic and conservative synchronization. Proceedings of the 1990 InternationalConference on Parallel Processing. 3 218222.Preiss, B., W. Loucks, et al. 1991. Null message cancellation in conservative distributedsimulation. Advances in Parallel and Distributed Simulation, SCS Simulation Series 233338.Preiss, B., I. MacIntyre, et al. 1992. On the tradeoff between time and space in optimisticparallel discrete event simulation. Proceedings of the 6th Workshop on Parallel andDistributed Simulation. 24 3342.Preiss, B. R., and W. M. Loucks 1995. Memory management techniques for Time Warp on adistributed memory machine. Proceedings ofthe 9th Workshop on Parallel and DistributedSimulation 3039.Pullen, 1. M. and D. C. Wood 1995. Networking technology and DIS. Proceedings of theIEEE 838 11561167.Rajaei, H., R. Ayani, et al. 1993. The local Time Warp approach to parallel simulation.Proceedings of the 7th Workshop on Parallel and Distributed Simulation 119126.Ramanathan, P., D. D. Kandlur, et al. 1990. Hardwareassisted software clock synchronization for homogeneous distributed systems. IEEE Transactions on Computers 394 514524.Ramanathan, P., K. G. Shin, et al. 1990. Fault tolerant clock synchronization in distributedsystems. IEEE Computer 2310 3342.Raynal, M., A. Schiper, et al. 1991. Causal ordering abstraction and a simple way toimplement it. Information Processing Letters 396 343350.288 REFERENCES REFERENCES 289Reed, D. A., and R. M. Fujimoto 1987. Multicomputer Networks MessageBased ParallelProcessing. Cambridge MIT Press.Reed, D. A., A. D. Malony, et al. 1988. Parallel discrete event simulation using sharedmemory. IEEE Transactions on Software Engineering 144 541553.Reiher, P., F. Wieland, et al. 1990. Providing determinism in the Time Warp operatingsystemCosts, benefits, and implications. Proceedings of the Workshop on ExperimentalDistributed Systems. Huntsville, AL IEEE, pp. 113118.Reiher, P. L., R. M. Fujimoto, et al. 1990. Cancellation strategies in optimistic executionsystems. Proceedings of the SCS Multiconference on Distributed Simulation. 22 11212l.Reinhardt, S. K., M. D. Hill, et al. 1993. The Wisconsin wind tunnel Virtual prototyping ofparallel computers. Proceedings of the 1993 SlGMETRICS Conference on Measurementand Modeling of Computer Systems. 21 4860.Reynolds, P., Jr., C. Pancerella, et al. 1993. Design and performance analysis of hardwaresupport for parallel simulations. Journal of Parallel and Distributed Computing 184435453.Reynolds, P. F., Jr. 1982. A shared resource algorithm for distributed simulation. Proceedings of the 9th Annual Symposium on Computer Architecture 10 259266.Reynolds, P. F., Jr. 1988. A spectrum of options for parallel simulation. Proceedings of the1988 Winter Simulation Conference 325332.Rich, D.O., and R. E. Michelsen 1991. An assessment of the ModsimjTWOS parallelsimulation environment. Proceedings ofthe 1991 Winter Simulation Conference 509518.Ronngren, R., and R. Ayani 1994. Adaptive checkpointing in Time Warp. Proceedings ofthe8th Workshop on Parallel and Distributed Simulation 110117.Ronngren, R., M. Liljenstam, et al. 1996. Transparent incremental state saving in Time Warpparallel discrete event simulation. Proceedings of the 10th Workshop on Parallel andDistributed Simulation 7077.Ronngren, R., H. Rajaei, et al. 1994. Parallel simulation of a high speed LAN. Proceedingsof the 8th Workshop on Parallel and Distributed Simulation, Society for ComputerSimulation 132138.Russo, K. L., L. C. Shuette, et al. 1995. Effectiveness of various new bandwidth reductiontechniques in ModSAF. Proceedings of the 13th Workshop on Standards for the1nteroperability ofDistributed Simulations 58759l.Samadi, B. 1985. Distributed simulation, algorithms and performance analysis. ComputerScience Department, PhD Thesis, University of California, Los Angeles.Schiper, A., 1. Eggli, et al. 1989. A new algorithm to implement causal ordering.Proceedings of the International Workshop on Distributed Algorithms. Berlin SpringerVerlag.Schwarz, R., and F. Mattern 1994. Detecting causal relationships in distributed computations In search of the Holy Grail. Distributed Computing 7 149174.Shah, G., U. Ramachandran, et al. 1994. Timepatch A novel technique for the parallelsimulation of multiprocessor caches. Proceedings ofthe 4th Workshop on Scalable SharedMemory Multiprocessors.Singhal, M., and A. Kshemkalyani 1992. An efficient implementation of vector clocks.lriformation Processing Letters 43 4752.Singhal, S. K., and D. R. Cheriton 1994. Using a position historybased protocol fordistributed object visualization. Computer Science Department, Stanford University, PaloAlto.Sinha, P. 1996. Distributed Operating Systems. Piscataway, NJ IEEE Press.Sokol, L. M. and B. K. Stucky 1990. MTW Experimental results for a constrainedoptimistic scheduling paradigm. Proceedings of the SCS Multicoriference on DistributedSimulation. 22 169173.Soule, L. and A. Gupta 1991. An evaluation of the ChandyMisraBryant algorithms fordigital logic simulation. ACM Transactions on Modeling and Computer Simulation 14308347.Sporrer, c., and H. Bauer 1993. Corolla partitioning for distributed logic simulation ofVLSIcircuits. Proceedings of the 7th Workshop on Parallel and Distributed Simulation, SCSSimulation Series 8592.Srikanth, T. K., and S. Toueg 1987. Optimal clock synchronization. Journal ofthe ACM 34626645.Srinivasan, S., and 1. Reynolds, P. F. 1995. NPSI adaptive synchronization algorithms forPDES. Proceedings of the 1995 Winter Simulation Conference 658665.Steinman, 1. 1991. SPEEDES Synchronous parallel environment for emulation and discreteevent simulation. Advances in Parallel and Distributed Simulation, SCS Simulation Series23 95103.Steinman, 1. 1993. Breathing Time Warp. Proceedings of the 7th Workshop on Parallel andDistributed Simulation 109118.Steinman, 1. S., and F. Wieland 1994. Parallel proximity detection and the distribution listalgorithm. Proceedings of the 8th Workshop on Parallel and Distributed Simulation.Edinburgh, Scotland 311.Stone, H. S. 1990. High Performance Computer Architecture. Reading, MA, AddisonWesley.Su, W K. and C. L. Seitz 1989. Variants of the ChandyMisraBryant distributed discreteevent simulation algorithm. Proceedings of the SCS Multiconference on DistributedSimulation, Society for Computer Simulation 21 3843.Sunderam, V. S., and V. 1. Rego 1991. EcliPSe, a system for high performance concurrentsimulation. Software Practices and Experiences 2111 11891219.Tacic, I., and R. M. Fujimoto 1997. Synchronized data distribution management indistributed simulations. Proceedings of the Spring Simulation Interoperability Workshop.Orlando, FL, pp. 303312.Tallieu, F. and F. Verboven 1991. Using Time Warp for computer network simulations ontransputers. Proceedings of the 24th Annual Simulation Symposium, IEEE ComputerSociety 21 112117.Tanenbaum, A. S. 1994. Distributed Operating Systems. Upper Saddle River, NJ PrenticeHall.Tanenbaum, A. S. 1996. Computer Networks, Upper Saddle River, NJ Prentice Hall.Tay, S. C., Y. M. Teo, et al. 1997. Speculative parallel simulation with an adaptive throttlescheme. Proceedings othe lIth Workshop on Parallel and Distributed Simulation 116123.290 REFERENCES REFERENCES 291Thomas, G. S., and J. Zahorjan 1991. Parallel simulation of performance petri netsExtending the domain of parallel simulation. Proceedings of the 1991 Winter SimulationConference 564573.Tinker, P. 1989. Task scheduling for general rollback computing. Proceedings of the 1989International Conference on Parallel Processing 2 180183.Tinker, P., and M. Katz 1988. Parallel execution of sequential scheme with ParaTran.Proceedings of the Lisp and Functional Programming Conference. Snowbird, UT ACM.Tomlinson, A. L, and V. K. Garg 1993. An algorithm for minimally latent global virtual time.Proceedings of the 7th Workshop on Parallel and Distributed Simulation 3542.Turner, S., and M. Xu 1992. Performance evaluation of the bounded Time Warp algorithm.Proceedings ofthe 6th Workshop on Parallel and Distributed Simulation, SCS SimulationSeries 117128.Unger, B. w., E Gomes, et al. 1995. A high fidelity ATM traffic and network simulator.Proceedings of the 1995 Winter Simulation Conference 9961003.Van Hook, D. J., J. O. Calvin, et al. 1994. An approach to DIS scalability. Proceedings ofthe 11th Workshop on Standards for the Interoperability ofDistributed Simulations 347356.Varghese, G., R. Chamberlain, et al. 1994. The pessimism behind optimistic simulation.Proceedings of the 8th Workshop on Parallel and Distributed Simulation 126131.Voss, L. 1993. A Revolution in Simulation Distributed Interaction in the 90s and Beyond.Arlington, VA Pasha Publications.Wagner, D. 1991. Algorithmic optimizations of conservative parallel simulations. Advancesin Parallel and Distributed Simulation, SCS Simulation Series 23 2532.Wagner, D. B., and E. D. Lazowska 1989. Parallel simulation of queueing networksLimitations and Potentials. Proceedings of the 1989 ACM Sigmetrics Conference on theMeasurement and Modeling of Computer Systems and Performance 8917 146155.Walrand, J., and P. Varaiya 1996. High Speed Communication Networks Building theSuperhighway, San Mateo, CA Morgan Kaufmann.Wang, J. J., and M. Abrams 1992. Approximate timeparallel simulation of queueingsystems with losses. Proceedings of the 1992 Winter Simulation Conference 700708.Waters, R. C., and J. W. Barrus 1997. The rise of shared virtual environments. IEEESpectrum 343 2025.West, D. 1988. Optimizing Time Warp Lazy rollback and lazy reevaluation. ComputerScience Department, University of Calgary, Alberta Canada.West, D., and K. Panesar 1996. Automatic incremental state saving. Proceedings ofthe 10thWorkshop on Parallel and Distributed Simulation 7885.Wieland, E 1997. Limits to growth Results from the detailed policy assessment tool.Proceedings of the 16th Annual IEEE Digital Avionics Systems Conference. Irvine, CA.Wieland, E, E. Blair, et al. 1995. Parallel discrete event simulation PDES A case study indesign, development, and performance using SPEEDES. Proceedings ofthe 9th Workshopon Parallel and Distributed Simulation, IEEE Computer Society 1O11O.Wieland, E, L. Hawley, et al. 1989. Distributed combat simulation and Time Warp Themodel and its performance. Proceedings of the SCS Multiconference on DistributedSimulation, SCS Simulation Series 21 1420.Willis, J. C., and D. P. Siewiorek 1992. Optimizing VHDL compilation for parallelsimUlation. IEEE Design and Test of Computers 4253.Wilson, A. L., and R. M. Weatherly 1994. The aggregate level simulation protocol Anevolving system. Proceedings of the 1994 Winter Simulation Conference 781787.Wonnacott, P., and D. Bruce 1996. The APOSTLE simulation language Granularity controland performance data. Proceedings of the 10th Workshop on Parallel and DistributedSimulation 114123.Woodson, W. E. 1987. Human Factors Reference Guide for Electronics and ComputerProfessionals. New York McGrawHill.Xiao, Z., J. Cleary, et al. 1995. A fast asynchronous continuous GVT algorithm for sharedmemory multiprocessor architectures. Proceedings of the 9th Workshop on Parallel andDistributed Simulation.Yavatkar, R. 1992. MCP A protocol for coordination and temporal synchronization inmultimedia collaborative applications. Proceedings of the 12th International Conferenceon Distributed Computing Systems, IEEE 606613.Yu, M.L., S. Ghosh, et al. 1991. A nondeadlocking conservative asynchronous distributeddiscrete event simulation algorithm. Advances in Parallel and Distributed Simulation SCSSimulation Series 23 3946. Zeigler, B. P., and D. Kim 1996. Design of high level modellinghigh performancesimulation environments. Proceedings of the 10th Workshop on Parallel and DistributedSimulation, IEEE Computer Society 154161.Zhang, L., S. Deering, S, et al. 1993. RSVP a new resource ReSerVation Protocol IEEENetwork, 75 818. INDEXAAL, see ATM adaptation layer protocolAda language, 111Adaptation Layer Protocol, see ATM adaptationlayer protocolAdaptive control policy, 155Address Resolution Protocol ARP, 242Addressing schemes, 241, 244Adventure, 1011Aggregate Level Simulation Protocol ALSP, 10,210Aggregate network bandwidth, 223Aggregated simulations, 12Aggressive cancellation, 131132Aggressive Norisk Protocol ANR, 157Aggressive protocols, 156157Air traffic control, see ApplicationsALSp, see Aggregate Level Simulation ProtocolAMG, see Architecture Management GroupAnalytic simulations, 68, 2324, 29, 51, 91,197,220, 269ANR, see Aggressive Norisk ProtocolsAPI, see Application Program InterfacesAntimessages, 97, 102106, 109110, 112114,116117124125,129135,152,156157,159, 163166, 168, 171Application Program Interfaces API, 211Applicationsair traffic control, 36, 9, 14,23, 33, 36, 4042,45, 47, 52, 55, 5759, 75, 80, 82, 8788,128, 130, 170entertainment, 9, 11, 13,23, 195197,259military, 11,24, 85, 220training, 510,1214,23,2829,195197,199,204, 209210, 257, 259, 273Architecture Management Group AMG, 210211ARP, see Protocols, address resolutionArtificial rollback, 144146, 148150, 155, 174Asfastaspossible simulation, 7, 29, 51, 196,210Asynchronous Transfer Mode ATM, 1415,23,170,179,183,188,191,227228,230231,233, 238242, 257ATM adaptation layer AAL, 238, 240ATM Protocol stack, 238ATM, see Asynchronous Transfer ModeATM adaptation layer protocol AAL, 240Atomic clocks, 270Attribute reflections, 212Attribute table, 216Autonomy of simulation nodes, 220Avatar, 7Backward transient messages, 141Bandwidth, II, 183184, 199,204,223,227228,231234, 237238, 245, 256Barrier synchronizations, 6566, 168Baseline definition, 10Batch fossil collection, see Fossil collectionBattlefield simulations, see Applications, militaryBenchmarks, 9, 15, 132, 136BISDN, see Broadband Integrated ServicesDigital NetworksBlocking, 66, 112, 138, 140, 144, 148, 155156,159, 162163, 165, 174175,276Bounded Lag Synchronization Algorithm, 80, 92,95, 156Breathing Time Buckets Algorithm BTB,157158, 175Breathing Time Warp Algorithm, 156, 174Broadband Integrated Services Digital NetworksBISDN, 1415, 183Broadcast service, 224BTB, see Breathing Time Buckets AlgorithmBuffers, 56, 88, 144146, 148150, 154, 157158,162,166168,171,175,179,184,187,231,239Butterfly barriers, 68, 7072, 95294 INDEXINDEX 295Cache memory, see Memory, cacheCancelback, 144150, 155, 174Carrier Sense Multiple Access with CollisionDetection CSMAjCD, 229, 241Cascaded rollback, 157CATOCS, see Causal And Totally OrderedCommunication ServiceCausal And Totally Ordered CommunicationService CATOCS, 261, 266268Causal ordering, 261262, 268Causality errors, 5153, 8283, 97Cause pointer variable, 166Cell Loss Priority CLP, 239ChandyMisraBryant Algorithm, 12,57,60,8182, 92Checkpointing, 112, 125128, 135, 162,219Checksum, 239Circuit switching, 231233, 238, 257Classbased subscription, 216, 248249, 252Clock synchronization wallclock timecentralized, pull algoritlun, 271centralized, push algorithm, 272clock combining algorithm, 275clock selection algoritlun, 274275clustering algorithm, 275Clock ticks, 271, 273Closed groups, 244CLP, see Cell Loss PriorityCoastforward, 124, 154Collisions, 228229Colored messages, 119121Communication latency, see LatencyCommunication protocol, 3, 18,234Computationonly entities, 197Concurrency control algoritlun, 97, 135Concurrent events, 262, 265266Concurrent Theater Level Simulation CTLS,12Conditional information, 8182, 95, 98, 217Connection Transfer Protocol, 83Conservative synchronization, 51, 58, 65, 87,9293 9798, 112, 156, 174, 261Consistency protocols, 19Consistent cut, 117119Continuous simulation, 3031, 160,201Copy state saving, 100101, 123124, 126, 140,147, 150, 153154Cray T3D, 20CRC, see Cyclic Redundancy ChecksumCSMACD, see Carrier Sense Multiple Accesswith Collision DetectionCTLS, see Concurrent Theater Level SimulationCut point, 117119Cyclic Redundancy Checksum CRC, 239DARPA, see Defense Advanced Research ProjectsAgencyData distribution management, 200, 210, 219,257Data Distribution systems DD, 245247, 250,257Datagrams, 224, 241243, 274DD, see Data Distribution systemsDead Reckoning Algorithms, 201, 204, 206,208209,217,220221,237,273Dead Reckoning Model DRM, 205209Deadlock, 5456, 58, 6066, 79, 8990, 94, 106,138139,173Deadlock detection, 60, 6465, 79, 89, 94, 173Deadlock Detection and Recovery Algorithm,6465, 79, 89, 94, 173Declaration management, 215, 219Defense Advanced Research Projects AgencyDARPA, 9, 210Defense Modeling and Simulation Office DMSO,25,210, 221, 247, 257Delta causality, 267, 275Derived types, 4546Description expression, 246249, 251253,256257Detailed Policy Assessment Tool DPAT, 170Diamond Park, 13Diffusing computation, 6061Direct execution, 15, 24Direct Memory Access DMA, 21DIS, see Distributed Interactive SimulationDIS nodes, 200, 202Discrete event simulation, 16, 18,3032,34,36,40, 4648, 52, 160, 171Distance between processes, 64, 75, 156, 173Distance matrices, 76, 78Distributed algorithms, 273Distributed computers, 45, 8, 1719, 2223, 30,36, 38, 46, 51, 91, 227Distributed Interactive Simulation DIS, 10, 1314,22,25,195,199202,204,206,209210,220221, 225226, 237, 245, 257, 267Distributed memory multicomputers, 17, 19, 21Distributed sharedmemory DSM, see MemoryDistributed Virtual Environments DVEs, 8,1011,1314,19,23,25,193,195197,199200, 204206, 209, 213, 215, 218, 220,223227, 233, 243, 245247, 255257,259260, 267, 275DMA, see Direct Memory AccessDMSO, see Defense Modeling and SimulationOfficeDNS, see Domain Name ServiceDoD, see United States Department of DefenseDogchasingitstaileffect, 151152, 157, 159,165Domin Name Service DNS, 243Dommo effect, 106, 125DPAT, see Detailed Policy Assessment ToolDrift error, 269, 271DRM, see Dead Reckoning ModelDSM, see Memory, distributedsharedDungeons and Dragons, 1011DVE, see Distributed Virtual EnvironmentsElectronic mail, 3, 233, 237, 240, 243Encapsulation, 45Endianness, 237Entertainment, see Applications, entertainmentEntity State PDUs ESPDU, 225Error rates, 227ESPDU, see Entity State PDUsEthernet, 23, 225, 227229, 231, 241242Event cancellation, 47, 107, 129132, 146,151152, 165167Event horizon, 157158Event list, 3435, 3839, 4344, 48, 74, 94, 99,110, 129, 142143, 150, 157, 167168Event retraction, 47, 128129, 136Eventdriven simulations, 33, 210Eventoriented simulations, 33,41,4345, 161,210EXCIMS, see Executive Council for Modeling andSimulationExecutive Council for Modeling and SimulationEXCIMS,211Extents routing spaces, 249250eXternal Data Representation XDR, 237Fair fight, 196Falsetickers, 275Fault tolerance, 5FDDI, see FiberDistributed Data InterfaceFederation management services, 219Federation Object Model FOM, 212, 214, 248Federations, 209, 211220, 248252, 253254FiberDistributed Data Interface FDDI, 229FIFO, see FirstInFirstOutFile Transfer Protocol FTP, 243Filtered rollbacks, 156FirstInFirstOut FIFO, 54, 61, 110, 184186Flight simulators, 45, 195, 197,200,211,215216,265Flow analysis, 127128Flow specification, 234FOM, see Federation Object ModelFossil collection, 109110, 123, 135136, 145,147149, 158, 165168, 172, 175FProc procedure, 161162Free procedure, 122123, 163FTp, see File Transfer ProtocolGames, see Applications, entertainmentGAO, see General Accounting OfficeGatelevel logic simulation, 15General Accounting Office GAO, 84Georgia Tech Time Warp GTW, 150, 161171,175GEOS, see Geostationary EnvironmentOperational SatellitesGeostationary Environment Operational SatellitesGEOS,270Global clock, 21, 34Global control mechanisms, 66, 98, 122Global Virtual Time GVT, 74, 97, 109110,112121, 123, 125, 134135, 138139,141142, 144145, 147, 149, 154158, 160,162165,168169,171,175asynchronous algorithm, 115, 119GMT, see Greenwich Mean TimeGreenwich Mean Time GMT, 28, 270GTV, see Georgia Tech Time WarpGVT, see Global Virtual TimeHardwareintheIoop simulation, 7, 210, 269High Level Architecture HLA, 910, 12,25, 195,199,202,209215,218,220221,247249,251252, 255257, 261, 276compliance, 10, 211interactions, 212HLA, see High Level ArchitectureHumanintheIoop, 7, 200, 210IBM, see International Business MachinesICMp, see Internet Control Message ProtocolIETF, see Internet Engineering Task ForceIGMp, see Internet Group Multicast ProtocolILAR, see Lookahead, inverse lookahead ratioInconsistent cut, 117, 119Incremental state saving, 100101, 123124,126128, 133, 135, 153154, 167, 169Infrequent state saving, 124125, 135, 140,147148, 154Input queue, 102, 104, 139140, 147, 153, 163Intel Paragon, 20Interest expression, 246249, 251257Interface Specification, 210, 218, 221International Business Machines IBM, 22International Standards Organization ISO, 235Internet, 3, 5, 8,1011,13,17,23,204,221,225226, 232, 234, 236, 240243, 245, 257,272, 274296 INDEXINDEX 297Internet Control Message Protocol ICMP, 243Internet Engineering Task Force IETF, 242Internet Group Multicast Protocol IGMP, 243Internet Protocol IP, 225, 236, 240243, 257,274Internetworking, 241242Interval intersection algorithm, 275Inverse LookAhead Ratio ILAR, see LookaheadIp, see Internet ProtocolIProc procedure, 161162ISO, see International Standards OrganizationJet Propulsion Laboratory JPL, 9, 108, 136Jitter, 226, 231233, 238, 256257Join operation, 244, 254, 256JPL, see Jet Propulsion LaboratoryLAN, see Networks, local areaLatency, 3, 11, 1718,2324, 112, 159, 196, 199,203204,206,208,223,226227,231233,238, 256257, 259, 261, 267269, 272274Lazy cancellation, 129133, 136Lazy reevaluation, 132133LBTS, see Lower Bound Time StampLeap seconds, 270Least Recently Used replacement policy LRU,181183Leave operation, 244, 247, 254, 256Links, 15, 5354, 5657, 61, 7576, 78, 81, 92,183186,227,229236,241242Livelock, 134Local area networks LAN, see NetworksLocal causality constraint, 5255, 59, 9193,9799, 156Local control mechanism, 98, 108109Local minimum computation, 115116Local rollback, 157Lockstep, 21Logical Processes LP, 3940, 4647,5165,70,7486, 8889, 9194, 97102, 104108,111112,122124,128134,137141,143148,150169,171175,177183,191Logs, 133, 240Lookahea 5760,6466,7476, 7879, 8184,8688,9094,98, 105106, 112, 156157,172174,256,268269inverse lookahead ratio ILAR, 91zero, 58, 64, 86, 88, 93, 98, 106, 108, 112, 136Lookahead prediction scheme, 256Lower Bound Time Stamp LBTS, 7879, 8183,92,112LP, see Logical ProcessesLRU, see Least Recently Used replacement policyMAC, see Medium Access ControlMalloc procedure, 122123, 163MAN, see Networks, metropolitan areaMBone, see Multicast BackboneMedium Access Control MAC, 228229Memory, 1622, 25, 34,46,48, 52, 66, 89, 92,9798, 100, 102, 109112, 122124,126127, 132140, 142149, 150151,154155,158,162163,165168,171174,181184, 228cache, 1920,134,171,179,181182distributed shared DSM, 21leaks, 122123, 163management, 21, 138139, 142144, 148149,154155,171,174memorybased flow control, 144, 148MERL, see Mitsubishi Electric ResearchLaboratoryMessage annihilation, 102, 139140Message sendback, 140141, 143145, 174Metropolitan area network MAN, 23, 227Military applications, see Applications, militaryMIMD, see MultipleInstruction stream, MultipleData streamMIT Lincoln Laboratories, 25, 210Mitre Corporation, 25, 170, 210Mitsubishi Electric Research Laboratory MERL,13Moving Time Window MTW, 155, 174MTW, see Moving Time WindowMUD, see MultiUser DungeonsMulticast Backbone MBone, 242Multicast service, 225MultipleInstruction stream, Multiple DatastreamMIMD, 2122Multiplexer, 179, 183188,232MultiUser Dungeons MUD, 11Name space, 246249, 251257National Institute of Standards and TechnologyNIST,270Naval Post Graduate School, 225NCubejTen, 20Network Time Protocol NTP, 274NetworkNetwork Interface NNI, 239Networkslocal area LAN, 13, 23, 196, 198, 227,230231,236,238,241242,257,272metropolitan area MAN, 23, 227ring, 229, 231wide area WAN, 23, 196, 199,208,227,231,272Newsgroups, 13, 245246NextCause pointer variable, 166NIST, see National Institute of Standards andTechnologyNonUniform Memory Access NUMA, 20NPSNet, 225, 257. See also Naval PostgraduateSchoolNTp, see Network Time ProtocolNull messages, 12, 54, 5658, 60, 6465, 78,8182, 9192, 94, 136, 173NOMA, see NonUniform Memory AccessObject Model Template OMT, 210, 213215,221Objectoriented simulation, 41, 4546, 127,211,214OC1, see Optical Carrier1OMT, see Object Model TemplateOnthefly fossil collection, see Fossil collectionOpen groups, 244Open System International OSI, 235, 238,241242Optical Carrier1 OC1, 238239Optimistic barriers, 73Optimistic synchronization, 51, 74, 97, 135, 138,142, 145, 150151, 154157, 160163,171172,174OSI, see Open System InternationalOutput queue, 102, 129, 131, 139, 163, 166Packet scheduling, 234Packet switching, 232233, 238Parallel Discrete Event Simulation PDES, 45, 9,12, 15, 1819,22,2425,3940,53,59,64,66,74,80,83,85,9798,122,143,154155,157,161,171,177,179,182,191,195197,200Parameter table, 218Partial ordering, 266Payload Type Identifier PTI, 239PDES, see Parallel Discrete Event SimulationPDU, see Protocol Data UnitsPhysical time, 2728Pipelining, 97Pointer variables, 166Pointers, 112, 122, 164167,226Polymorphism, 46Prefix computation, 179, 188190Probabilistic rollback, 160Proc procedure, 38, 41, 44, 46, 161163, 165167,169Processoriented simulation, 4145, 133, 161Protocol Data Units PDU, 200203, 221,225226, 237, 267detonation, 202entity state ESPDU, 225fire, 202Protocol stack, 235236, 238, 241243ProtocolsATM adaptation layer AAL, 240address resolution ARP, 242aggregate level simulation ALSP, 10, 12,210,291aggressive, 131132, 136, 156157, 166aggressive norisk ANR, 157breathing time buckets BTB 157158, 175breathing time warp, 156, 174ChandyjMisrajBryant 12,57,60, 8182,92communication, 3, 18,234connection transfer, 83conservative synchronization, 51, 58, 65, 87,9293, 9798, 112, 156, 174, 261consistency, 19file transfer FTP, 243Internet IP, 225, 236, 240243, 257, 274Internet control message ICMP, 243Internet group multicast IGMP, 243network time NTP, 274optimistic synchronization, 51, 74, 97,135,138,145,151,154,156157,172,174ReSerVation RSVP, 234reverse address resolution RARP, 242serial line internet SLIP, 242simple mail transfer SMTP, 243slotted ring, 229synchronous execution, 10, 70, 76, 82, 84, 92,98, 118119, 132, 138, 155, 160, 171172,269token ring, 229, 242transmission control TCP, 225, 236, 238, 243trivial file transfer TFTP, 243user datagram UDP, 225, 236, 243, 274wolf calls, 159, 175YAWNS, 92, 95Protofederationsanalysis, 210engineering, 210joint training, 210platform, 210Pruneback, 144, 147148, 155, 174Pruning, 140, 144, 147148, 174,245PTI, see Payload Type IdentifierPublication regions, 249Pull processing, 47Push processing, 47QoS, see Quality of ServiceQuality of Service QoS, 233234, 238Query events, 4647, 133Queuing networks, 9, 191298 INDEX INDEX 299Radar, 246247RARP, see Reverse Address Resolution ProtocolRealtime execution, 29Receive time stamp, 140142, 158, 164ReducedInstructionSetComputers RISC, 127Reflect attribute values service, 261Regions routing spaces, 160, 249251Release messages, 67Repeaters, 228Replacement policy, 181, 183Replicated trials, 48ReSerVation Protocol RSVP, 234Resource reservation, 234Retraction, 129Reverse Address Resolution Protocol RARP, 242Ring networks, see NetworksRISC, see ReducedInstructionSetComputersRollback, 73, 97, 99100, 102104, 106113,122127, 129133, 135136, 138, 140148,150157, 159160, 163, 166, 169, 171172,174175,261,268cascaded, 157cycles, 107108echoes, 152handlers, 163local, 157probabilistic, 160secondary, 103, 106, 125, 131, 141, 156157thrashing, 138Routing, 234, 236, 249, 276Routing spaces, 214, 247249, 250, 257RS232C, 235RSVP, see ReSerVation ProtocolRTI, see RunTime InfrastructureRunTime Infrastructure RTI, 10, 209213, 216,218221,251Salvage parameter, 145Scalability, 172, 174, 199,273Scaled realtime execution, 29Scheduling an event, 3435Secondary rollback, 103, 106, 125, 131, 141,156157Secondary servers, 274Selective fidelity, 196Send time stamp, 131, 139142, 145, 157158Sequential discrete event simulation, 34Serial Line Internet Protocol SLIP, 242Server architecture, 197199Serverless architecture, 197Sharedmemory multiprocessor, 17, 19, 21, 161Silicon Graphics, 22SIMD, see Single Instruction stream, MultipleData streamSIMNET, see SIMulator NETworking ProjectSimple Mail Transfer Protocol SMTP, 243Simula,46Simulation executive, 12, 3536, 38, 41, 4445,60,63,66,8086,9394,99, 129, 131133,135,150,171175,196197,209,245,255,261263, 267Simulation Object Model SaM, 212, 214Simulation time, 7, 2729, 33, 39, 43, 75, 106,109, 153, 156, 158, 160, 171Simulation time creep, 82Simulationsaggregated, 12analytic, 610, 12, 14,2324,29, 51, 91,195197,209,220,260,269asfastaspossible, 7, 29, 51, 196,210continuous, 3031, 160,201discrete event, 16, 18,3032,34,36,40,4648,52, 160, 171distributed interactive DIS, 10, 1314,22,25,195, 199202,204,206,209210,220221,225226,237,245,257,267eventoriented, 33, 41, 4345, 161,210gatelevel logic, 15hardwareintheloop, 7, 210, 269objectoriented, 41, 4546, 127,211,214parallel discrete event, 45, 9,12,15,1819,22,2425, 3940, 53, 59, 64, 66, 74, 80, 83,85,9798, 122, 143, 154155, 157, 161,171,177,179,182,191,195197,200processoriented, 4145, 133, 161sequential discrete event, 34space parallel, 191spacetime, 160, 175time parallel, 177179, 181183, 187188, 191timestepped, 3133,41war games, 910, 12SIMulator NETworking Project SIMNET, 910,200, 204, 206, 221, 245Simultaneous events, 53, 8586, 94, 106108Simultaneous reporting problem, 113, 115116,135Single Instruction stream, Multiple Data streamSIMD, 1719,2122SLIP, see Serial Line Internet ProtocolSlotted ring protocols, 229Smallest Time stamp First policy STF, 134Smoothing, 208, 220SMp, see Symmetric MultiProcessorSMTp, see Simple Mail Transfer ProtocolSnapshots, 135, 151152Solar days, 270SaM, see Simulation Object ModelSONET, see Synchronous Optical NETworkSpace parallel simulation, 191Spacetime simulation, 160, 175Spanning tree, 6364, 245Spatial decomposition, 177Speedup, 79, 9091, 94,171,173,196State queue, 101102, 133, 139, 147, 163164State variables, 27, 3036, 3839, 45, 5152, 58,93,98,100101,105,111,122124,126,128,133,136,138139,153,160,169,177,200,211Static control policy, 82, 92,155,201,217,254255Statistical multiplexing, 233, 238, 257Stealth viewers, 218STF, see Smallest Time stamp First policyStorage optimality, 139, 142144, 146, 148, 174Straggler events, 99, 133Straggler messages, 99, 157STSl, see Synchronous Transport Signal Level ISTSOCn, 238Subscription, 212, 216, 219220, 248249, 252,256Subscription regions, 249251, 253Sun Enterprise System, 19,22,171Switched LANs, 230231Symmetric MultiProcessor SMP, 1920Synchronization problem, 51, 91, 94, 260Synchronization protocols, 8, 10, 12,5156,6669, 7173, 7980, 82, 84, 9192, 9495,9798, 115, 122, 132, 135, 138, 155156,160,163,165,169,171172,174175,177178,196197,210,226,237,258,260,268269, 271, 273274, 276Synchronous execution, 10,70,76,82,84,92,98,118119,132,138,155,160,171172,269Synchronous Optical NETwork SONET, 238Synchronous Transport Signal Levell STSI,238Synthetic environments, 9Synthetic forces, 197TCP, see Transmission Control ProtocolTelephone industry, 15, 191,224,231,236,238,255, 257, 270Temporal decomposition, 177Test and Evaluation, 12TFTP, see Trivial File Transfer ProtocolThreading mechanisms, 44Timeatomic clocks, 270clock combining algorithms, 275clock selection algorithms, 274275clock synchronization algorithms, 79, 269, 271,275clock ticks, 271, 273falsetickers, 275Greenwich Mean Time, 28, 270interval intersection algorithms, 275leap seconds, 270physical, 2728simulation, 7, 2729, 33, 39, 43, 75, 106, 109,153, 156, 158, 160, 171solar days, 270universal time coordinated UTC, 269272,275wallclock, 27, 275Time creep, 82Time division multiplexing, 232233, 257Time flow mechanism, 3031Time parallel simulation, 177179, 181183,187188, 191Time providers, 270Time Warp, 12,97102,104,106,108112,117,122123, 125145, 148152, 154, 156157,160161,163,165,171,173174,276Time Warp Logical Process TWLP, 99100,102104,106107,109111,122124,132,134Time Warp Operation System TWOS, 9, 108,288Time windows, 78, 81, 84, 155, 157Timestepped simulations, 3133, 41Token ring protocols, 229, 242Tokens, 120,229,241Training, see Applications, trainingTransient messages, 66, 7073, 77, 92, 95,113121,135,141,158Transmission Control Protocol TCP, 225, 236,238, 243Transport mechanisms, 244Trees, 6164, 6772, 74, 108, 190, 245, 248249,274Trivial File Transfer Protocol TFTP, 243Truncated binary exponential backoff algorithm,229TWfree  procedure, 123TWGetMsg procedure, 162, 167TWLP, see Time Warp Logical ProcesSTWmalloc  procedure, 123TWOS, see Time Warp Operation SystemTWSend procedure, 162, 165, 167, 169UDp, see User Datagram ProtocolUMA, see Uniform Memory AccessUnconditional information, 8182Undiscovery, 251UNI, see UserNetwork InterfaceUnicast service, 224225, 234, 243, 245, 251, 255300 INDEXUniform Memory Access UMA, 20United States Department of Defense DoD, 910,210211,236Universal Time Coordinated UTe, 269272,275Unix, 22, Ill, 272Unreliable delivery service, 223, 236, 243, 273,275Update attribute values service, 219, 248, 261Update regions, 251User Datagram Protocol UDP, 225, 236, 243,274UserNetwork Interface UNI, 239UTC, see Universal Time CoordinatedValuebased data distribution, 248249VCI, see Virtual Channel IndicatorVector clock, 262264Virtual Channel Indicator VCI, 230231, 233,239Virtual circuits, 238Virtual environment, 514, 19,23,25,28, 30, 48,195196,203204,206,208,223,225,240,247, 249250, 255, 259260, 266Virtual Path Indicator VPI, 230231, 233, 239VPI, see Virtual Path IndicatorWallclock time, 27, 275WAN, see Networks, wide areaWar game simulations, 910, 12Weapons systems, 11, 85, 220Wide area network WAN, 23, 196, 199,208,227,231,272Wolf Calls protocol, 159, 175X.25,236XDR, see eXternal Data RepresentationXerox Palo Alto Research Center, 10YAWNS protocol, 92, 95Zero lookahead, see LookaheadA stateoftJ1e.art guide for the implementation ofdistributed simulation technologyThe rapid expansion of the Intemet and commodity parol lei computers hos madepmallel and distributed simulation PADS a hot technology indeed. Applicationsabound not only in the analysis of complex systems such os tronspodotiorl or thenext generation Internet but a Iso in com pute r generoted vi duo I worI d s formilitary ond professionol troining interocllve computer gomes onel the enteltoinment industry.In this book, PADS expert Richard M FUlimoto provides softwme developers withcuttingedge techniques for speeding up the execution of slmulotions ouoss multipleprocessors and dealing with dota distribution over wide mea networks, includingthe Internet. With an emphosis on pmallel ond distributed discrete event simulationtechnologies, Dr Fujimoto compiles and consolidotes resemch results in the fieldsponning the last twenty yems, discussing the use of pmollel ond distributedcomputers in both the modeling and analysis of system behavior ond the creationof distributed viduol envimnments.While other books on PADS concentrate on applicotions, Parallel and DIstrIbuted Simulation Systems clemly shows how to implement the technology. Itexplains in detail the synchronization algorithms needed to properIy realize thesimulations, including an indepth discussion of Time Warp and advoncedoptimistic techniques Finally. the book is richly supplemented with references,tobles and illustrotions, ond exomples of contemporary systems such as theDepmtment of Defenses High Level Architecture HLA which hos become thestondord mchitecture for defense progroms in the United Stotes.RICHARD M. FUJIMOTO, PhD, is Professor of Computer Science ot the Georgia Institute of Technology Widely recognized for his contributions to the development of PADS Dr FUlimoto led the working group responsible for defining timemonogement services for the U S Depmtment of Defense High Level ArchitprlIrpHLA eHodWllEYINTERSCIENCEjol1l1 Wi  50m ,
