Data Mining and Knowledge Discovery, 3, 736 1999c 1999 Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.Discovery of frequent Datalog patternsLUC DEHASPE luc.dehaspecs.kuleuven.ac.beDepartment of Computer Science, Katholieke Universiteit Leuven,Celestijnenlaan 200A, B3001 Heverlee, BelgiumHANNU TOIVONEN hannu.toivonenrni.helsinki.fiRolf Nevanlinna Institute  Department of Computer Science,P.O. Box 4, FIN00014 University of Helsinki, FinlandEditor Saso Dzeroski and Nada LavracAbstract. Discovery of frequent patterns has been studied in a variety of data mining settings. In its simplestform, known from association rule mining, the task is to discover all frequent itemsets, i.e., all combinations ofitems that are found in a sufficient number of examples. The fundamental task of association rule and frequentset discovery has been extended in various directions, allowing more useful patterns to be discovered with specialpurpose algorithms. We present Warmr, a general purpose inductive logic programming algorithm that addressesfrequent query discovery a very general Datalog formulation of the frequent pattern discovery problem.The motivation for this novel approach is twofold. First, exploratory data mining is well supported Warmroffers the flexibility required to experiment with standard and in particular novel settings not supported by specialpurpose algorithms. Also, application prototypes based on Warmr can be used as benchmarks in the comparisonand evaluation of new special purpose algorithms. Second, the unified representation gives insight to the blurredpicture of the frequent pattern discovery domain. Within the Datalog formulation a number of dimensionsappear that relink diverged settings.We demonstrate the frequent query approach and its use on two applications, one in alarm analysis, and one ina chemical toxicology domain.Keywords frequent patterns, inductive logic programming, Datalog queries, association rules, episodes,sequential patterns1. IntroductionDiscovery of recurrent patterns in large data collections has become one of the central topicsin data mining. In tasks where the goal is to uncover structure in the data and where there isno preset target concept, the discovery of relatively simple but frequently occurring patternshas shown good promise.Association rules Agrawal et al, 1993 are a basic example of this kind of setting. Aprototypical application example is in market basket analysis first find out which itemstend to be sold together, the frequent item set discovery phase, and next postprocess thesefrequent patterns into rules about the conditional probability that one set of items will bein the basket given another set already there, the association rule discovery phase. Themotivation for such an application is the potentially high business value of the discoveredpatterns. At the heart of the task is the problem of determining all combinations of itemsthat occur frequently together, where frequent is defined as exceeding a userspecifiedfrequency threshold. The use of a frequency threshold for filtering out noninterestingpatterns is natural for a large number of data mining problems. Patterns that are rare, e.g.,that concern only a couple of customers, are probably not reliable nor useful for the user.8 DEHASPE AND TOIVONENA family of data mining problems can be specified as follows Mannila and Toivonen,1997. Given a database r, a class L of sentences patterns, and a selection predicate q,the task is to find the theory of r with respect to L and q, i.e., the set ThL, r, q  Q L  qr, Q is true. The selection predicate q is used for evaluating whether a sentenceQ  L defines a potentially interesting pattern in r. For the discovery of frequent patterns, q is defined so that qr, Q is true if and only if the frequency of patternQ in databaser exceeds the frequency threshold. Problem settings that fit this description and that areclose to the problem of discovering association rules include the use of item type hierarchies Han and Fu, 1995, Holsheimer et al., 1995, Srikant and Agrawal, 1995, the discovery of episodes in event sequences Mannila and Toivonen, 1996, Mannila et al., 1997, andthe search of sequential patterns from series of transactions Agrawal and Srikant, 1995,Srikant and Agrawal, 1996. For all these cases the pattern language L is different, andspecialized algorithms exist for the tasks.We present a powerful inductive logic programming algorithm, Warmr, for a largesubfamily of this type of tasks. Warmr discovers frequent Datalog queries that succeedwith respect to a sufficient number of examples. In other words, the language L consists ofDatalog queries, and Warmr outputs those that are frequent in a given Datalog orrelational database r. The Datalog formulation is very general, as it allows the use ofvariables and multiple relations in patterns, and it thus significantly extends the expressivepower of patterns that can be found.The flexibility of Warmr is a strong advantage over previous algorithms for the discoveryof frequent patterns. Each discovery task is specified to Warmr in terms of a declarative language bias definition. The language bias declarations determine which Datalogqueries are admissible, i.e., which subset of Datalog constitutes the language L for theparticular task. With different languages and databases Warmr can be adapted to diversetasks, including the settings mentioned above and also more complex novel problems, without requiring changes to the implementation. Warmr thus supports truly explorative datamining pattern types can be modified and experimented with very flexibly with a singletool.This article is organized as follows. We start in Section 2 by describing the data miningtask of discovering frequent Datalog queries. In Section 3 we present Warmr, analgorithm that discovers frequent Datalog queries. We show in Section 4 how Warmrand the described setting can be used to implement and extend some wellknown data miningtasks. Two novel applications of frequent Datalog queries and Warmr are discussedin Section 5, one in telecommunication alarm analysis and one in a chemical toxicologydomain. Finally, in Sections 6 and 7, we touch upon related work, and conclude.2. The frequent query discovery taskWe use Datalog see, e.g., Ullman, 1988 to represent data and patterns. There is astraightforward and welldefined correspondence between Datalog and both relationaldatabases and firstorder clausal logic. The use of Datalog allows us to describe a numberof data mining tasks in the area of frequent pattern discovery in a clear and uniform manner.Relational algebra, the formal framework of relational databases, has the same expressivepower as Datalog without recursion. For instance, the recursive concept ancestor can beFREQUENT DATALOG PATTERNS 9defined in Datalog but not in relational algebra. Datalog, in turn, is a subset of clausallogic and Prolog that is restricted to functionfree definite clauses. In this paper, werestrict ourselves to Datalog to simplify the discussion. It should be stressed however thatthe algorithms described below in practice operate in the more general Prolog setting.We now briefly review the Datalog concepts used in this paper, and describe the datamining task of discovering frequent patterns or, in Datalog terminology, frequent queries.Next, we introduce a formalism to specify a remaining essential parameter of the frequentquery discovery task a search space L of Datalog queries.2.1. Datalog conceptsIn Datalog a term is defined as a constant symbol or a variable. To distinguish betweenthem, we write variables with an initial upper case letter, while using names beginningwith lower case letters for constants. An atom is an mary predicate symbol followed bya bracketed mtuple of terms. A definite clause is a universally quantified formula of theform BA1, . . . , An n  0, where B and the Ai are atoms. This formula can be read asB if A1 and . . . and An. If n  0 a definite clause is also called a fact. Ground clausesare clauses that contain only constants as terms, no variables. A substitution  is a set ofbindings X1a1, . . . , Xmam of variables Xi to terms ai. If we substitute ai for eachoccurrence of Xi in a clause C, we obtain C, the instance of C by substitution . If C isground, it is called a ground instance of formulaC, and  is called a grounding substitution.A deductive Datalog database is a set of definite clauses. Often a distinction is madebetween the extensional database, which contains the predicates defined by means of groundfacts only, and the remaining intensional part.A Datalog and Prolog query is a logical expression of the form  A1, . . . , An.Submitting such a query to a Datalog database corresponds to asking the question doesa grounding substitution exist such that conjunction A1 and . . . and An holds within thedatabase. The resolution based derivation of the answer to a given query with variables X1, . . . , Xm binds these variables to terms a1, . . . , am, such that the querysucceeds if ai is substituted for each Xi. This socalled answering substitution is denotedby X1a1, . . . , Xmam. Due to the nondeterministic nature of the computation of answers, a single query Q may result in many answering substitutions. We will refer byanswerset Q, r to the set of all answering substitutions obtained by submitting query Qto a Datalog database r.As already pointed out, if recursion is not allowed, these concepts correspond directly torelational database terminology. Predicates map to relations, facts to tuples of a relation,and a query such as windowWindow id, Alarmtype, is aAlarmtype, warningcan be written in SQL asselect window.window id, window.alarmtypefrom window, is awhere window.alarmtypeis a.alarmtypeand is a.ancestorwarning10 DEHASPE AND TOIVONENAn overview of strategies to make the relationship between Datalog and relationaldatabases operational can be found in Ullman, 1988.2.2. Frequent query discoveryIn terms of the generic formulation of the frequent pattern discovery problem by Mannilaand Toivonen 1997 see also above, Section 1, we consider the following data miningtask.Definition 1. Assume r is a Datalog database, L is a set of Datalog queries Q thatall contain an atom key, and qr, Q is true if and only if the frequency of query Q  Lwith respect to r given key is at least equal to the frequency threshold specified by the user.The frequent query discovery task is to find the set ThL, r, q, key  of frequent queries.Compared to the original formulation, we have added a fourth parameter, i.e., atom key,to the frequent pattern discovery task. In our framework this extra parameter is essential,as it determines what is counted. As we will shortly clarify in our definition of frequency,each binding of the variables in key uniquely identifies an entity.In previous formulations of the task, the key unambiguously followed from the context.For instance, we count transactions in market basket analysis or windows in episode discovery. In frequent query discovery, with many predicates in the database, the focus ofcounting can be on any attribute and has to be specified by the user. The additional keyparameter allows us to switch, for instance, from counting transactions, to customers, tosupermarkets, to managers, to regions or to whatever relevant attribute in our market basketdatabase.We next define what frequency exactly means in this setting.Definition 2. Assume r is a Datalog database and Q  L is a query that containsatom key. Then the relative frequency of query Q with respect to database r given key isfrq Q, r, key k  answerset  key, r Qk succeeds w.r.t. rk  answerset  key, ri.e., the fraction of substitutions of the key variables with which the query Q is true, or,more intuitively, the fraction of examples in which pattern Q occurs.Example Consider the analysis of alarms from a telecommunication network Klemettinen et al, 1998. The task is to discover episodes Mannila et al., 1997, combinations ofalarms that tend to occur close to each other and that are thus potentially causally related.Given a long sequence of alarms and a window width, one looks at the sequence by slidinga window of the given width on the sequence. A set of alarms that occurs frequently in thewindows is called a parallel episode Mannila et al., 1997.For the episode discovery task, we store in the database r facts of the form windowwid,atype to indicate that window wid contains an alarm of type atype. Notice that in apractical implementation this information should probably be computed in an incrementalmanner at run time.FREQUENT DATALOG PATTERNS 11There are a large number of alarm types, and it is often useful to study episodes on different levels of abstraction. For instance, alarm type 1001 belongs to the class of switchalarms, but also to the class of BSC disturbances and the class of BSC alarms. In general,alarm types form an is a hierarchy with the shape of a lattice. This information can beprovided as background knowledge, in the form of facts is a1001, switch, is a1001,BSC disturbance, is aBSC disturbance, BSC alarm, and so on. The transitive definition of is a can be specified as a clause in the intensional part of the background database.Discovery of frequent parallel episodes with an alarm type hierarchy can now be definedas a special case of frequent query discovery as follows. The database r contains, for eachtime window wid on the input alarm sequence, a fact window idwid and zero or morefacts windowwid, atype. As we want to count windows, we set key to window idWid.The database r further contains, as domain knowledge, facts of the form is aatype, parent atype. The patterns inL consist of the obligatory key atom window idWid togetherwith one or more atoms or atom pairs of the form windowWid, atypei and windowWid,ATypej, is aATypej , atypek.Suppose we want to count the frequency of queryQ   window idWid, windowWid, AType, is aAType, switchi.e., the window contains an alarm type that belongs to the class of switch alarms. Foreach fact window idwidk there is a substitutionk  Widwidk  answerset  window idWid, r .We then have to find the fraction of such substitutions k for whichQk   window idwidk, windowwidk, AType, is aAType, switchsucceeds.This situation is actually identical to the discovery of frequent sets for association rules.For the supermarket basket analysis application, simply replace windows by transactionsand alarm types by item types. The problem of discovering such frequent sets on multiple levels of an item type hierarchy has been considered, e.g., in Han and Fu, 1995,Holsheimer et al., 1995, Srikant and Agrawal, 1995.To conclude the description of the frequent query discovery task, let us once more establish the link with relational database terminology. In SQL syntax the absolute frequencyof Q can be obtained with the following query, inspired by Lindner and Morik, 1995,Blockeel and De Raedt, 1996select countdistinct from select fields that correspond to the variables in keyfrom relations in Qwhere conditions expressed in Q2.3. Declarative language biasThe representation of patterns as Datalog queries requires a formalism to constrain thequery language L to a set of meaningful and useful patterns. With association rules the12 DEHASPE AND TOIVONENdefinition of L is straightforward L is simply 2I , the collection of all subsets of theset I of items. Srikant, Vu, and Agrawal Srikant et al, 1997 describe a technique toimpose and exploit userdefined constraints on combinations of items, but otherwise thedefinition of L has received little attention in the frequent pattern discovery literature. Ininductive logic programming, on the other hand, this issue has been studied extensivelyin the subfield of declarative language bias. This is motivated by huge, often infinite,search spaces, that require a tight specification of patterns worth considering. Severalformalisms have been proposed for adding language bias information in a declarative mannerto the search process for an overview, see Ade et al, 1995, Nedellec et al., 1996. Ourformalism, Wrmode, is an adaptation to Warmr of the Rmode format developed forTilde Blockeel and De Raedt, 1998 which, in turn, is based on the formalism originallydeveloped for Progol Muggleton, 1995.We will demonstrate later how the Wrmode notation can be used to constrainL to someinteresting classes of patterns. The required subset of Wrmode is described below.2.3.1. The Wrmode basics Let us first look at the simple case where L contains novariables, i.e., only ground queries are allowed. Under these circumstances, the Wrmodenotation extends the straightforward L  2I bias to Datalog queries given a set Atomsof ground atoms, the languageL consists of 2Atoms, i.e., of all possible combinations of theatoms. For example, Atoms  pa,b, qc defines L  2Atoms   true  pa,b qc  pa,b,qc.When variables are allowed in L, the power set idea can be extended to a set of literals,as done, for instance, in Weber, 1997 and Dehaspe and De Raedt, 1997. However, thissolution is inconvenient for two reasons. First, we might want to define infinite languages.For instance, in a graph represented as a set of Datalog facts edgeFrom,To we mightwant to allow queries  edgeX1,X2, edgeX2,X3, edgeX3,X4, . . . of arbitrary length.Thereto, an atom inAtoms should be allowed several times in the query and not just once asin 2Atoms. Second, we do not want to control the exact names of the variables in the query,as we do for constants, but rather the sharing of names between variables. For example,query  rectangleWidth,Height, WidthHeight is equivalent to query  rectangleX,Y,XY but not to query  rectangleWidth,Height,XY.In the Wrmode framework, nonground atoms in Atoms are allowed to occur multipletimes in the query, as long as their variables obey the socalled mode constraints. Theseare declared for each variable argument of each atom by means of three modelabels ,,and  the variable is strictly input, i.e. bound before the atom is called the variable is strictly output, i.e. bound by the atom the variable can be both input andor output, i.e. anything goesIn our approach the atoms of the query are evaluated one by one, following an orderingthat is consistent with the mode declarations. The intuition is that the evaluation of someatoms, such as X  Y in the example above, presupposes the binding of certain inputvariables to a constant. On the other hand some atoms, e.g. rectangleWidth,Height, areallowed or required to introduce new output variables bound during evaluation of the atom.FREQUENT DATALOG PATTERNS 13Table 1. Examples of Wrmode definitions and queries disallowed in the corresponding languages.Wrmode definition Atoms example pattern  L example pattern 6 Lp,,q  pX,Y,qZ  pX,Y,qYp,,q  qX,pX,Y,pY,Z  pX,Yp,,q  qX,pX,X,pX,Y  qX,pY,Xp,a,q  pX,a,qX,qZ,pZ,a  pX,YA query is then mode conform if an ordering of atoms exists such that every input variableoccurs in one of the previous atoms, and no output variable does. Some examples of queriesthat are consistent and inconsistent with mode declarations are listed in Table 1. Throughoutthe paper we use the notational convention that the atoms of a query are evaluated from leftto right.2.3.2. Typing in Wrmode Additional constraints on the sharing of variable names canbe imposed via type declarations. The Wrmode convention is to append these to the modedeclarations in Atoms. A query is then type conform if and only if arguments that sharea variable name have identical types or at least one of them is untyped. For example, withmode and type declarations Atoms  ps,t,qt,a, query  pX,Y,qY,a is in L,but not  pX,Y,qX,a since the first arguments of predicates p and q have incompatibletypes. Notice the difference between constants and types in declarations Atoms types arepreceded by a mode label and constants are not.2.3.3. The Wrmode key As we have seen, the frequent query discovery task requiresthe specification of a key atom which is obligatory in all queries. Within Wrmode notationthis is done with key  KeyAtom, where KeyAtom is a mode and type declaration as definedabove. Obviously, the key atom declaration should not contain any  mode labels.For an example of a nonground language, consider key  p, and Atoms  q.These declarations together define L   pX,Y  pX,Y,qX  pX,Y,qY pX,Y,qX,qX  pX,Y,qX,qY . . . . Notice the presence of logically redundantqueries such as  pX,Y,qX,qX.2.3.4. Logical redundancy and Wrmode The Wrmode notation is only meant tocapture applicationspecific constraints on L. These are usually supplemented by a number of general constraints hardwired in the data mining algorithm in which Wrmode isembedded. Logical nonredundancy is such an applicationneutral and algorithmspecificconstraining principle. For instance, the last Wrmode definition in Table 1 allows pX,a, qX, pY,a, qY, qY, which is logically equivalent to the shorter  pX,a,qX.Within Warmr the first pattern would be filtered away in the candidate generation phase,as explained below in Section 3.2.3.14 DEHASPE AND TOIVONEN2.3.5. Wrmode extensions The implementation of Wrmode contains many extrafeatures that provide more expressive power and allow more condensed notations. Onegeneral mechanism is to make the presence of atoms conditional on the presence or absenceof other atoms. For instance, in the case of alarm analysis, the query  window idWid,windowWid,AType is bound to succeed for all windows, assuming all windows contain atleast one alarm. To avoid the evaluation of such uninteresting patterns we could requirethat windowWid,AType only occurs in combination with an atom is aAType,atype. Thedetails of how to achieve this with Wrmode are beyond the scope of this article, and wewill make abstraction of this and similar extensions in the rest of the paper.Example In the previous example we stated that for the parallel episode discovery task,the patterns in L consist of an atom window idWid together with atoms or atom pairs ofthe forms windowWid, atypei and windowWid, ATypej, is aATypej , atypek.For the Wrmode specification of this we first set the key atom by specifyingkey  window idwIn words, we want to count windows, not alarm types or whatever. For the set of atoms thatcan be used in constructing queries we setAtoms   windoww,atype1, . . . , windoww,atypem,windoww,a, is aa,aclass1, . . . , is aa,aclassnfor allm alarm types andn alarm classes. According to these declarations, the first argumentof window has to be an input variable of the same typew as the argument of window id, andthe second argument is either a constant from atype1,. . . ,atypem or an output variabledifferent from the variable in window id. The first argument of is a is an input variable ofthe same type a as the second argument of window. Finally, the second argument of is a isa constant from aclass1,. . . ,aclassn.3. Query discovery with WarmrDesign of algorithms for frequent pattern discovery has turned out to be a popular topicin data mining for a sample of algorithms, see Agrawal et al, 1993, Agrawal et al, 1996,Lu et al, 1995, Savasere et al, 1995, Toivonen, 1996. Almost all algorithms are on somelevel based on the same idea of levelwise search, known in data mining from the Apriorialgorithm Agrawal et al, 1996. We first review the generic levelwise search method and itscentral properties and then introduce the algorithm Warmr Dehaspe and De Raedt, 1997for finding frequent queries. To conclude this section, we recall how this method fits in thetwophased discovery of frequent and confident rules.3.1. The levelwise algorithmThe levelwise algorithm Mannila and Toivonen, 1997 is based on a breadthfirst searchin the lattice spanned by a specialization relation  between patterns, cf. Mitchell, 1982,where p1p2 denotes pattern p1 is more general than pattern p2, or p2 is more specificthan pattern p1.FREQUENT DATALOG PATTERNS 15Algorithm 1  WarmrInputs Database r Wrmode language L and key  threshold minfreqOutputs All queries Q  L with frq Q, r, key  minfreq1. Initialize level d  12. Initialize the set of candidate queries Q1   key3. Initialize the set of infrequent queries I  4. Initialize the set of frequent queries F  5. While Qd not empty6. Find frq Q, r, key of all Q  Qd using WarmrEval7. Move the queries  Qd with frequency below minfreq to I8. Update F  F Qd9. Compute new candidates Qd1 from Qd, F and I using WarmrGen10. Increment d11. Return FThe method looks at a level of the lattice at a time, starting from the most general patterns.The method iterates between candidate generation and candidate evaluation phases incandidate generation, the lattice structure is used for pruning nonfrequent patterns from thenext level in the candidate evaluation phase, frequencies of candidates are computed withrespect to the database. Pruning is based on monotonicity of  with respect to frequencyif a pattern is not frequent then none of its specializations is frequent. So while generatingcandidates for the next level, all the patterns that are specializations of infrequent patternscan be pruned. For instance, in the Apriori algorithm for frequent itemsets, candidatesare generated such that all their subsets i.e., generalizations are frequent.The levelwise approach has two crucial useful properties Assuming all candidates of a level are tested in single database pass, the database isscanned at most d 1 times, where d is the maximum level size of a frequent pattern.This is an important factor when mining large databases. The time complexity is in practice linear in the product of the size of the result timesthe number of examples, assuming matching patterns against the data is fast.3.2. The Warmr algorithmThe inputs of Warmr correspond to the four parameters of the frequent query discoverytask as introduced in Definition 1. Algorithm 1, steps 510, shows Warmrs main loopas an iteration of candidate evaluation in step 6 and candidate generation in step 9. Themanipulation of set I of infrequent queries in steps 3 and 7 is necessary for the generationphase. This is discussed below, together with some other features that distinguish Warmrfrom Apriori.16 DEHASPE AND TOIVONEN3.2.1. Specialization relation The subset specialization relation used in most frequentpattern discovery settings can in some restricted cases also be used for structuring a spaceof Datalog queries, as done in Weber, 1997, Weber, 1998 and Dehaspe and De Raedt,1997. In general however, the subset condition is too strong. For instance, we wouldlike to consider query  pZ,Z,qZ as a specialization of query  pX,Y,pY,X, althoughpX,Y,pY,X is not a subset of pZ,Z,qZ.The most obvious generalpurpose definition of the subsumption relation is based onlogical implication Query1Query2 if and only if Query2  Query1. Logical implication could detect for instance that  pX,Y,pY,X is a generalization of  pZ,Z,qZ.However, due to the high computational cost of the logical implication check, inductivelogic programming algorithms often rely on a stronger variant coined subsumption byPlotkin Plotkin, 1970. Query1 subsumesQuery2 if and only if there exists a possiblyempty substitution of the variables ofQuery2, such that every atom of the resulting queryoccurs in Query1, i.e., Query1  Query2. For instance,  pZ,Z,qZ subsumes pX,Y,pY,X, with   XZ, YZ.3.2.2. Candidate evaluation AlgorithmWarmrEval adapts Definition 2 of frequencyof a single query Qj to the levelwise approach, which matches a set of patterns against oneexample at a time. The example is here represented by k, the substitution for the keyvariables obtained in step 2 by running  key against the database. The algorithm, instep 2.b, applies a fixed substitution k to the subsequent queries Qj drawn from Q, andincrements an associated counter qj in case Qjk succeeds with respect to the database.If we execute the latter evaluation with respect to r, we still need one pass through thedatabase per query, instead of one pass per level. The solution adopted in WarmrEval,step 2.a, is based on the assumption that there exists a relatively small subset rk of r,such that the evaluation of any Qk only involves tuples from rk. Readers familiar withrelational database technology might notice a similar assumption underlies the definitionof a cluster index. In many cases rk is a partition on r. The algorithm then makes asingle pass through the data in the sense that the key values k are retrieved one by one, thesubsequent subdatabases rk are activated once in 2.a, and all queries are evaluated locallywith respect to rk in 2.b. An experimental evaluation of this localisation of informationin a related data mining task can be found in Blockeel et al, 1998a.Consider as an example the alarm analysis database introduced in the previous section.Each subset rk of this database would contain a fact window idwidk and zero or morefacts windowwidk,atype. This subset of the database indeed suffices for solving queriesQk built with predicates window id and window. But what about the facts of the formis aatype, parent atype Clearly these are relevant for many keys widk and involvedin solving queries in many examples k. As a consequence, they cannot be assigned to onerk exclusively. We will discuss efficient solutions in Section 3.2.4 below.3.2.3. Candidate generation To generate candidates, WarmrGen employs at step2 a classical specialization operator under subsumption Plotkin, 1970, Muggleton andDe Raedt, 1994. A specialization operator  maps queries  L onto sets of queries  2L,such that for anyQuery1 and Query2  Query1,Query1 subsumesQuery2. TheFREQUENT DATALOG PATTERNS 17Algorithm 2  WarmrEvalInputs Database r set of queries Q Wrmode keyOutputs The frequencies of queries Q1. For each query Qj  Q, initialize frequency counter qj  02. For each substitution k  answerset key, r, do the followinga Isolate the relevant fraction of the database rk  rb For each query Qj  Q, do the followingIf query Qjk succeeds w.r.t. rk, increment counter qj3. For each query Q Q, return frequency counter qjoperator used in WarmrGen essentially adds one atom to the query at a time, as allowedby Wrmode declarations. For instance, given the Wrmode declarationskey  window idwAtoms   windoww,atype1, . . . , windoww,atypem ,windoww,a, is aa,aclass1, . . . , is aa,aclassnandQj   window idW, windowW,atype1,windowW,AT1WarmrGen builds specializations Qj of Qj by adding an atom from setwindowW,atypei, windowW,AT2, is aAT1,aclassjwith 1  i  m and 1  j  n.Mode and type declarations on variables may cause an atom to be added for the first timeonly deep down the lattice. For instance, in the example above, atom is aAT1,aclassjcould not have been added if windowW,AT1 had not been in Q. This complicates pruningsignificantly. We can no longer require that all generalizations of a candidate are frequent assome of the generalizations, such as  window idW, windowW,atype1,is aAT1,aclass1in the example, might simply not be in L. Instead, WarmrGen at step 2.i requirescandidates not to subsume any infrequent query. In step 2.ii, we also require thatcandidates and frequent queries are mutually inequivalent under subsumption. This waya potentially huge set of redundant solutions is eliminated.3.2.4. Efficiency and complexity considerations Warmr, like any other inductive logicprogramming algorithm, has to cope with the theoretical result that both evaluation of a queryand testing subsumption are NP complete problems. In some practical cases however,as discussed in respectively De Raedt and Dzeroski, 1994 and Kietz and Lubbe, 1994,both problems can be solved efficiently. We now localize these critical operations in theWarmr algorithm and discuss some implemented and possible optimizations.The composition and the loading of rk in WarmrGen step 2.a can be optimizedin two ways. First, if a fixed portion rB reoccurs as a subset of many rks, we can load18 DEHASPE AND TOIVONENAlgorithm 3  WarmrGenInputs Wrmode language L infrequent queries I frequent queries F frequent queries Qd for level dOutputs Candidate queries Qd1 for level d11. Initialize Qd1  2. For each query Qj  Qd, and for each immediate specialization Qj  L of Qj Add Qj to Qd1, unlessi Qj is more specific than some query  I, orii Qj is equivalent to some query  Qd1  F3. Return Qd  1the common rB once, and iteratively load only the specific rk rB . In inductive logicprogramming jargon, rB typically corresponds to background knowledge. For instance, inthe telecommunication domain, background knowledge rB might consist of 1 groundfacts about alarm types, network elements, and network topology, and 2 clausal rules thatcapture general fault and network management principles and so forth. For instance, theis aatype, parent type facts in the example above could be stored in rB . Second, incases where the repeated composition of rk is still too costly, e.g. if many facts have tobe selected from many different predicates, a preprocessing step can be considered whereall the rks are composed once and written to flat files, see Blockeel et al, 1998a for anexperimental evaluation.In some cases, rk is very small compared to r and can be loaded in main memory evenif r cannot. This has the crucial advantage that evaluation of candidates in WarmrEvalstep 2.b can be done more efficiently with respect to a cached fraction of the database. Itis possible however to contrive a database and language such that each rk  r and the evaluation of complex queries with respect to huge databases becomes impractical. Considerfor instance the consequences of adding to the alarm analysis database facts of the formfollowswidi,widj and allowing queries such as  window idW1, followsW1,W2,windowW2,1001 i.e., the window follows a window that contains an alarm type 1001.To solve such queries in an example k we need all the facts windowwidj with j  k.If we further add predicate precedes, rk becomes roughly equal to r. This example maystretch the notion of a window on the data, but it does illustrate that the isolation of rkfrom r and local evaluation of Qj in WarmrEval step 2.b is not guaranteed to beprofitable. This approach does allow however to take advantage of situations where thebulk of the database is immaterial to that evaluation.Some alternative strategies to boost WarmrEval step 2.b will be considered infuture work. For instance, when in query  window idwidk, windowwidk,AType,is aAType,switch, windowwidk,1001 the atom windowwidk,1001 fails, there is no pointin looking for alternative bindings for AType. In Prolog terminology, the cut operatorFREQUENT DATALOG PATTERNS 19should be inserted after is aAType,switch to suppress backtracking. Another possibilityto reduce backtracking would be to reorganize set Q into some treelike structure, similarto Aprioris hash trees, and evaluate the queries collectively against the data.To prune candidatesQj , WarmrGen in steps 2.i and 2.ii scans I,F , andQd1 untila query is found that is subsumed by and subsumes, in the case of 2.ii Qj . As astraightforward optimization, a sorted list of predicates is associated with each query, andthe expensive subsumption test on a couple of queries is only applied after a positive subsettest on the corresponding predicate lists. Planned improvements include the reorganizationof the massively overlapping queries from I, F , and Qd1 into a tree, as above, andverification of subsumption against this structure.One can also alleviate the candidate generation problem by using a declarative languagebias formalism that is equipped with a refinement operator that is optimal in the sense that itgenerates each query at most once, as done, for instance, in Dehaspe and De Raedt, 1996,Dehaspe and De Raedt, 1997, Weber, 1997, Weber, 1998, Wrobel, 1997.3.3. Twophased discovery of frequent and confident rulesFrequent patterns are commonly not considered useful for presentation to the user as such.Their popularity is mainly based on the fact that they can be efficiently postprocessed intorules that exceed given confidence and frequency threshold values. The best known example of this twophased strategy is the discovery of association rules Agrawal et al, 1993,and closely related patterns include episodes Mannila et al., 1997 and sequential patterns Agrawal and Srikant, 1995. For all these patterns, the threshold values offer a naturalway of pruning weak and rare rules.We introduce the notion query extension to refer to the firstorder equivalent of an association rule. In terms of the Datalog concepts introduced in Section 2, a query extensionE isan expression of the formA1, . . . , Ak  Ak1, . . . , An, whereAi are atoms. This formulashould be read as if query  A1,. . . ,Ak succeeds then the extended query  A1,. . . ,Ansucceeds also. The confidence of query extension E can be computed as the ratio of thefrequencies of queries  A1,. . . ,An and  A1,. . . ,Ak. The frequency or support of queryextension E is the frequency of query  A1,. . . ,An.As observed in Agrawal et al, 1993 for association rules, confident and frequent queryextensions can be found effectively in two steps. In the first step one determines the set of allfrequent queries  A1,. . . ,An, and in the second produces query extensionsA1, . . . , Ak Ak1, . . . , An whose confidence exceeds the given threshold. If all frequent queries andtheir frequencies are known as a result of the first step, then this easy second step isguaranteed to output all frequent and confident query extensions.Example Suppose we run Warmr on the alarm analysis database introduced in Section 2.2, with L defined by the Wrmode declarations at the end of Section 2.3, and obtainthe two following queries with associated relative frequencies window idW,windowW,1001 freq 0.4 window idW,windowW,1001,windowW,A,is aA,switch freq 0.3These patterns can be processed without going back to the database into a query extension20 DEHASPE AND TOIVONENwindow idW,windowW,1001 windowW,A,is aA,switchfreq0.3  conf0.75i.e., with 30 frequency and 75 confidenceif there is in the window an alarm of type1001, then the window will also contain an alarm that belongs to the class of switch alarms.4. Cases of frequent query discoveryWe now present more extensive problems in the discovery of frequent patterns in the domainof alarm analysis. These cases are inspired by previous data mining settings, but they containelements that have not been considered before. We show how minor modifications to thelanguage bias define increasingly complex tasks. We then briefly compare different settingsand algorithms for solving them.4.1. More complex parallel episodesConsider the case where each occurrence of an alarm has a number of attributes associatedwith it. In the telecommunications domain such attributes include, in addition to the alarmtype, e.g., the urgency and the sender of the alarm. The alarm database thus consists of anumber of windows, each with a number of alarms, where each alarm has certain individualproperties. Discovery of frequent combinations of alarms where the properties of alarmsare also considered, is an interesting problem. However, mapping such a case to one of theexisting frequent pattern discovery problems does not seem to be possible without loss ofinformation.Assume, for instance, an alarm database with facts win idwid of window identifiers, asbefore, and facts winwid, atype, urgency, sender that denote for each alarm in a windowthe type, the urgency level, and the sending network element of the alarm. A sample of sucha database is shown below. This setting has some similarity with the multipleinstanceproblem known from attributevalue learning Dietterich et al, 1997.win1,1001,notice,367 win2,1001,notice,534win1,1054,warning,367 win2,1005,notice,245. . . . . .Let us now look at two possible strategies for transforming this setting to simple sets ofbinary indicators, such as alarm types or items in the more simple examples consideredin Section 2. First, one could blow up the number of indicators and introduce an itematype urgency sender for all combinations that occur, e.g., the first fact shown abovewould be written as win1,1001 notice 367. A first objection to this solution is that,especially with a high number of manyvalued properties, this transformation will result in an exponential number of infrequent items. Moreover, even if this transformationis practicable, it would disallow the discovery of frequent combinations of the originalproperties.As as second attempt we could add the individual properties as extra item types, as is donewith item hierarchies and in Klemettinen et al, 1998. This, indeed, allows the discoveryof patterns such asFREQUENT DATALOG PATTERNS 21 win idWid,winWid,1001,winWid,warning,winWid,534i.e., an alarm of type 1001, a warning, and an alarm from 534. However, we lose thefacility to discover something about combinations of properties, such as a warning of type1001 from 534 properties warning and 534 are tested independently and cannot be linkedto the same alarm.To summarize the problem, we would like alarms and their properties to occur both inisolation, and in any combination, e.g. win idW,winW,1001,notice,367,winW,1054,warning,S,winW,1034,U, Ta notice of type 1001 from 367, a warning of type 1054, and an alarm of type 1034.With Warmr such patterns could be discovered by choosing the language bias essentiallyas followskey  win idwAtoms   winw, atype, urgency, sender,eqatype, 1001, eqatype, 1002, . . . ,equrgency, notice, equrgency, warning, . . . ,eqsender, 245, eqsender, 265, . . . where eq is an equality test. The corresponding language contains the pattern above informat win idWid, winWid,A1,U1,S1, eqA1,1001, eqU1,notice, eqS1,367,winWid,A2,U2,S2, eqA2,1054, eqU2,warning,winWid,A3,U3,S3, eqA3,1034This bias could be easily extended to handle the case where a window as a whole mayhave properties in addition to the properties of alarms. For instance, whether the windowfalls on office time or not is a useful bit of information. One can then look for combinationsof alarm and window properties, such as win idWid,winWid,1001,warning,S,office hoursWid,yesoffice time windows containing a warning of type 1001. To find such patterns withWarmr, we only have to addAtoms  Atoms  office hoursw,yes, office hoursw,noto the language bias. Such an extension could be very interesting in the supermarket basketanalysis domain, where the properties of a transaction may contain information about thecontext of shopping, about the customer, or aggregate information about the basket, such asthe time or the location of shopping, or the total value or the number of items in the basket.Frequent patterns that could be discovered include, e.g., baskets containing cigarettes andpaid in cash and senior customers buying something promoted.Allowing the presence of arbitrary relations between alarm properties or arbitrary background knowledge makes the setting even more interesting. For example, we can slightlymodify one of the patterns shown above22 DEHASPE AND TOIVONEN winW,1001,notice,367,winW,1054,warning,S,winW,1034,U,Sa notice 1001 from 367, and a warning 1054 and an alarm 1034 from the same sender.Notice the shared S variablesuch patterns where the sender is shared can be found ifWarmrs bias is extended withAtoms  Atoms  winw, atype, urgency, senderAll the facts mentioned above are relevant for a single window wink only and can be storedin mutually exclusive rk subdatabases to improve query evaluation cf. Section 3.2.2.Common background knowledge rB can be used to specify, e.g, neighborhood relationsin the network of telecommunication equipment. The database and language then look asfollowsr  r  neighbor245,265,. . . Atoms  Atoms  neighborsender,senderAn example of a query admitted by this extension is win idW,winW,1001,notice,S1,winW,1054,warning,S2,neighborS1,S2 This formulation is close to the one of general episodes given in Mannila and Toivonen,1996. They allow binary relations on alarms, such as for instance the neighbor relationabove, or an order relation based on a time stamp associated with the alarms. Their settinghas not been implemented in a full scale before.4.2. Dimensions of the frequent pattern discovery taskDifferent frequent pattern discovery tasks can be characterized in terms of their supportfor a fairly small number of features. In Table 2 we present an overview of differenttasks. Since most of the work has been presented in the context of association rules, weuse primarily terms from association rules and market basket analysis and secondarily thealarm vocabulary.Itemsets IS stands for the discovery of frequent sets of items, as it is done for thediscovery of association rules in the very basic setting Agrawal et al, 1993 or parallelepisodes of Section 2, without a class hierarchy on alarm types Mannila et al., 1997.Item hierarchies IH is the basic setting extended with a hierarchy on the items Hanand Fu, 1995, Holsheimer et al, 1995, Srikant and Agrawal, 1995 or the case described asparallel episodes in the examples of Section 2. Sequential patterns SP refers to the casein basket analysis where a number of transactions are observed for each customer, and patterns relating items in different transactions are searched for Agrawal and Srikant, 1995,Srikant and Agrawal, 1996. Parallel episodes with alarm properties PE stands for themore complex parallel episodes described in this section items in a transaction have individual properties individual occurrences of alarms have properties. General episodesGE is the setting where items have properties and patterns contain unary and binary relations on items within a transaction Mannila and Toivonen, 1996. Finally, Datalogpatterns DP stands for the possibilities of frequent Datalog queries.FREQUENT DATALOG PATTERNS 23Table 2. Dimensions of frequent pattern types. Legend IS  itemsets, IH  itemsets with item hierarchies, SP sequential patterns, PE  parallel episodes with alarm properties, GE  general episodes, DP  full Datalogpatterns.IS IH SP PE GE DPMany items per transaction      Item type properties     Many ordered transactions per example  Item instance and transaction properties   Binary item properties besides order  Arbitrary Datalog queries Table 3. Dimensions of pattern discovery algorithms. Legend IS  itemsets, IH  itemsets with item hierarchies,SP  sequential patterns, PE  parallel episodes with alarm properties, GE  general episodes, DP  Warmr.IS IH SP PE GE DPLevelwise search      Bindings can be stored     All backtracking suppressed  Subset relation between item types only Incremental candidate evaluation  The table lists six of the properties where the tasks differ. These properties are directlyreflected by the existence of different types of atoms in the language L. A cell contains aplus if the pattern type can deal or can easily be extended to deal with the given feature.Note that the table is coarse for instance, item type properties means a concept hierarchyfor most of the cases, and only some can handle other properties associated with item types.According to Table 2, the most obvious gaps to fill are to either extend sequential patternsto include item and transaction attributes and binary properties, or to extend episodes to thecase where there is another level of containment between items and examples e.g., sets ofalarms are sent as transactions, which then occur in windows. Finally, recall that episodeshave not been implemented before in the extent described here.4.3. Dimensions of the pattern discovery algorithmsWe conclude this section with a summary of those dimensions that characterize and relatethe different pattern discovery algorithms. Table 3 shares column labels with Table2 herea plus in a cell means that a specialized algorithm for the column can exploit the featuremarked on the row.All algorithms can use the levelwise search method. In all settings except Warmr, theuse of variables is strongly limited, e.g., only to the window or transaction variable. As aneffect, the management of variable bindings is very efficient and often the bindings can evenbe stored for later use with other patterns. The use of variables also affects the efficiencyof the recognition of patterns. In some settings, the search can be organized so that thereis essentially no backtracking within patterns. Some algorithms exploit the fact that theirqueries can be mapped to simple cases, in particular to testing the subset relation, which is24 DEHASPE AND TOIVONENefficient when compared to subsumption in the general case. This has an effect both oncandidate generation and testing. Episode algorithms can take additional advantage of thefact that the sliding of the window can be handled in an incremental manner.The relevantthough not very surprisingobservation here is that Table 3 is roughlycomplementary with Table 2 columns with many plusses in one table tend have few plussesin the other. Thus, the combination of these two tables provides a fairly balanced picture ofthe obvious tradeoff between expressivity and efficiency in the context of frequent patternmining. It also demonstrates there is no dichotomy item sets vs. queries Apriori vs.Warmr, but rather a gradual and complex change in the tradeoff between expressivityand efficiency, with a number of intermediate problems that have received considerableattention.Warmr is a generic algorithm that does not take advantage of any special properties ofparticular problem instances. Therefore, for any specific setting, a more efficient algorithmcan probably be devised. Warmr is useful, in particular, for exploring different settings,both existing and novel ones.Finally, the two tables provide a blueprint for a single integrated system that uses Table 2to determine the minimal level of expressivity required and Table 3 to fire the maximallyefficient algorithm available within that setting. In such a system, Warmr would be thecatchall method.5. ExperimentsIn this section we present experimental results with Warmr in the task of frequent querydiscovery. First, we round off the running example on alarm analysis. The actual inputsand a sample of the outputs are discussed in more detail. We then move on to a case whereusefulness of the patterns discovered by Warmr has been confirmed by expert evaluationDehaspe et al, 1998. The task there is to identify substructures of chemical compoundsthat have a potential for inducing cancer in human beings.5.1. Alarm analysisThe experimental data originates from a fault management database of a mobile communication network. The problem of discovering recurrent combinations of alarms fromsuch databases has been considered in Goodman and Latin, 1991, Hatonen et al., 1996,Klemettinen et al, 1998, Mannila et al., 1997. Closely related data mining problems havebeen considered, e.g., in Bettini et al, 1996, Dousson et al, 1993, Morris et al, 1995, Oatesand Cohen, 1996, Sasisekharan et al, 1996, Srikant and Agrawal, 1996, Padmanabhan andTuzhilin, 1996, Wang et al, 1994.The dataset consists of a sequence of 46662 alarms emitted by the network elementssuch as base stations and transmission devices during a period of one month. The timegranularity of the data is one second. The average frequency of alarms is approximately1500 alarmsday, or 1 alarmminute, but since alarms tend to occur in bursts, the busiestsecond contains 50 alarms.There are 180 different alarm types, which can be further classified into 10 overlappingclasses. Each instance of an alarm has one of 4 urgency levels. The alarms in the datasetFREQUENT DATALOG PATTERNS 25have been received from 2012 network management objects of 9 different types. Theseobjects represent units of different granularities, and they form a containment hierarchy.This hierarchy gives essential information about the nature of the relationships of the objects.The discovery task we consider is to find those combinations of alarms that are frequent. This problem is the one considered in episode discovery, but here, to the best of ourknowledge, we implement a much more expressive variation than has been done before.We consider alarms with different combinations of properties, and we also consider caseswhere the alarms are connected, e.g., in the object hierarchy or in some other way. Wecannot see any way of transforming this task to episode or sequential pattern discovery taskwithout losing information.5.1.1. Database and background knowledge Following are some of the most importantpredicates used to represent the alarm data. The most obvious ones, such as alarm typealarm,alarmtype, relate each alarm instance to an alarm type, an occurrence time, several alarmclasses, etc. A background predicate precedesalarm1, alarm2 allows temporal order testsbetween alarms alarm1 and alarm2. Some new clauses are defined in the backgroundknowledge based on the occurrence time, to add potentially useful information such asofficehouralarm.In a similar manner, the database contains clauses senderalarm, object that indicate thesender of each alarm background knowledge includes predicates such as ancestorobject,ancestor, siblingobject1, object2, and same object typeobject1, object2.In the experiments we considered windows of width 120 seconds that start from an alarm.To represent and define windowing, we specified in the background knowledge a factwin idwid and a fact startwid,alarm to identify the window and the alarm it startsfrom. Finally, we added a clause that derives in windowwid, interval, alarm if alarmoccurs within time interval from the start of window wid.5.1.2. Language bias The Wrmode language bias used in the experiments looks essentially as followskey  win idwAtoms in windoww,120,a,startw,a,sendera,o, precedesa,a,officehoura, weekenda, weekdaya,mon, . . . , weekdaya,fri,urgencya,1,. . . ,urgencya,4, same urgencya,a,alarm typea,1001,. . . ,alarm typea,2270, same alarm typea,a,alarm classa,sw,. . . ,alarm classa,tr, same alarm classa,a,sender elema,95,. . . ,sender elema,314, same sender elema,a,object typeo,bcf,. . . ,object typeo,brx, same object typeo,o,ancestoro,o,. . . ,siblingo,o 5.1.3. Results A specific task we considered was to describe the windows followingalarms of a specific alarm type. Problems reported by this particular alarm are difficult totrack here the goal is to discover patterns of alarms from related objects that might help in26 DEHASPE AND TOIVONENexplaining the important alarms. We present one of the patterns, in the more informativequery extension formatwin idW,startW,A,in windowW,120,B,alarm classB,bsc messagein windowW,120,C,alarm classC,trans,same urgencyA,C,same urgencyC,BFreq0.15  Conf0.77i.e., with 15 frequency and 77 confidence if a window starts with an alarm A andcontains an alarmB of class bscmessage then it will also contain an alarmC of class transsuch that all alarms referred to will have the same urgency.In a more general setting, where the window could start on alarms of any type, thefollowing pattern was discoveredwin idW,in windowW,120,A,senderA,O,object typeO,bcf,ancestorO,P,object typeP,bsc,in windowW,120,B,alarm classB,bst messageprecedesA,B,urgencyA,2,alarm classA,bst messageFreq0.27  Conf0.68i.e., with 68 confidence, and 27 frequency if there is in the window an alarm sent byan object of type bcf, and an ancestor of that object is of type bsc, and there is also an alarmof class bst message, then the first alarm precedes the second one and has urgency 2 andclass bst message.Observe that the natural language paraphrase is actually not exactly equivalent to the rule.A query extension X  Y should be read as if query  X succeeds then query  X,Ysucceeds cf. Section 3.3. Within a window, the condition part might be met for a numberof substitutions of alarms A,B, whereas only some of them may meet the conclusion part.A more exact English formulation would have the form if the condition part holds withsome substitution of the variables, then the condition and conclusion parts hold togetherwith possibly some other substitution. The simpler formulation if X succeeds then Ysucceeds is true if the intersection of condition and conclusion variables is empty, whichis obviously the case with traditional association rules, or if that intersection only containsvariables that can be bound in at most one way.There are a large number of factors that affect the sending of alarms, such as the objectrelationships, network configuration and also the surrounding environment. Representingthese factors and taking them into account in the pattern discovery is much easier withWarmr than with the previous episode formalisms. Network management experts havealso found the pattern types discovered with Warmr more informative and useful.5.2. Predictive toxicologyThe goal of this second experiment is to discover frequent substructures of chemical compounds in relation to their possible carcinogenicity. A few raw statistics confirm this taskis of clear scientific and medical interest in western countries cancer is the second mostcommon cause of death, one third of the population will get cancer, and one fourth of thepopulation will die of cancer. An estimated 80 of these cancers are linked to environmental factors such as exposal to carcinogenic chemicals. At the same time, only fractionof chemicals are tested for carcinogenesis. This contradiction can be explained by the factFREQUENT DATALOG PATTERNS 27that current methods are expensive and time consuming, hence the interest in cheaper andfaster computer based methods.The National Toxicology Program NTP of the U.S. National Institute for EnvironmentalHealth Sciences aims at safety testing of new chemicals 500  1000 every year andidentification of hazardous chemicals in use nearly 100,000. Given a compound, theyperform a range of tests that vary in expense, speed and accuracy to estimate the carcinogeniceffect of the compound on humans. At the extreme cheap, fast, and relatively inaccurate endare biological tests that use bacteria. At the other end are long about two years, expensive,and relatively reliable standardized bioassays on thousands of rodents. The urgent need forpredictive toxicology models that identify hazardous chemical exposures more rapidly andat lower cost than current procedures is the driving force behind the Predictive ToxicologyEvaluation PTE project Bristol et al, 1996.Within PTE they have collected and published a database of about 300 classified NTPchemical carcinogenesis bioassays, and a collection of 30 chemicals whose tests are tobe completed by the end of 1998. The prediction of rodent chemical carcinogenesisof these compound was launched at IJCAI97 as a research challenge for artificial intelligence Srinisavan et al, 1997. Rather than competing with expert chemists in classifying chemicals to carcinogenic or otherwise, our goal was to discover frequent patterns that would aid chemists  and data miners seeking predictive theories  to identify useful substructures for carcinogenicity research, and so contribute to the scientificinsight. This can be contrasted with previous machine learning research in this application, which has mainly concentrated on predicting the toxicity of unknown chemicalsSrinisavan et al, 1997, Kramer et al, 1997. We believe that a repository of frequent substructures and their frequencies would be valuable for chemical machine learning research.For example, once we know all frequent substructures, we can make stronger claims aboutthe nonexistence of high quality single rules than can usually be done with classifyingapproaches based on heuristic search.The results of this experiment have been previously published in Dehaspe et al, 1998.Related problems in structure discovery in molecular biology have been considered, e.g., inWang et al, 1997, Kramer et al, 1997, King et al, 1996, King and Srinivasan, 1996. Substructure discovery and the utilization of background knowledge have been discussed inDjoko et al, 1995. Closely related data mining problems have recently arisen also inschema discovery in semistructured data Wang and Liu, 1997.5.2.1. Database and background knowledge The database for the carcinogenesis problem was taken from httpwww.comlab.ox.ac.ukouclgroupsmachlearnPTE. The set wehave used contains 337 compounds, 182 54 of which have been classified as carcinogenic and the remaining 155 46 otherwise.Each compound is basically described as a set of atoms and their bond connectivities, asproposed in King et al, 1996. The atoms of a compound are represented as Datalogfacts such as atomd1,d1 25,h,1,0.327 stating that compound d1 contains atom d1 25 ofelement h and type 1 with partial charge 0.327. For convenience, we have defined additionalview predicates atomel, atomty, and atomch e.g., atomeld1,d1 25,h. Bonds betweenatoms are defined with facts such as bondd1,d1 24,d1 25,1, meaning that in compoundd1 there is a bond between atoms d1 24 and d1 25, and the bond is of type 1. There are28 DEHASPE AND TOIVONENroughly 18500 of these atombond facts to represent the basic structure of the compounds.Notice we can define a partition rk on these facts such that evaluation of candidatesQi is done relatively efficiently with respect to a single compound cidk at a time cf.Section 3.2.2. For each member rk of the partition, i.e., each compound, we have added afact comp idcidk.In addition, background knowledge contains around 7000 facts and some short Datalogprograms to define mutagenic compounds, genotoxicity properties of compounds, genericstructural groups such as alcohols, connections between such chemical groups, tests toverify whether an atom is part of a chemical group, and a family of structural alerts calledAshby alerts Ashby and Tennant, 1991.We randomly split the set of 337 compounds into 23 for the discovery of frequentsubstructures, and 13 for the validation of derived query extensions about carcinogenicity.5.2.2. Language bias The most extensive set of language bias Wrmode specificationsused in the experiments iskey  comp idcAtoms atomelc,a,c,. . . ,atomelc,a,h, atomtyc,a,1,. . . ,atomtyc,a,75,bondc,a,a,btype,eqbtype,1,. . . ,eqbtype,7,carcinogenicc,non carcinogenicc,amesc,has propertyc,salmonella,p,. . . ,has propertyc,chromex,n,alcoholc,struct,methylc,struct,. . . ,six ringc,struct,ashby alertc,cyanide,struct,. . . ,ashby alertc,methanol,struct,connectedc,struct,struct,occurs inc,a,struct5.2.3. Results In order to investigate the usefulness of different types of informationin the biochemical database, Warmrs language bias was varied to produce three sets offrequent patterns. Experiment 1 only atom element, atom type, and bond information. At level 6,Warmr generates substructure atomelC,A1,c, bondC,A1,A2,BT, atomelC,A2,c, atomtyC,A2,10,atomelC,A3,h, bondC,A2,A3,BT Freq0.57i.e., a carbon atom bound to a carbon atom of type 10 bound to a hydrogen atom,where the two bonds are of the same bond type. Experiment 2 everything except the atombond information. An example of a substructure discovered at level 4 is six ringC,S1, alcoholC,S2, ashby alertC,di10,S3, connectedS1,S3Freq0.05i.e., an alcohol and a six ring connected to a structure with Ashby alert di10.FREQUENT DATALOG PATTERNS 29 Experiment 3 the full database, except the Ashby alerts. At level 5, Warmr producessubstructure six ringC,S, atomelC,A1,h, atomelC,A2,c, bondC,A1,A2,X, occursinA2,S Freq0.70i.e., a hydrogen atom bound to a carbon atom in a six ring.5.2.4. Query extensions As described in Section 3.3, our repository of frequent substructures can be exploited directly, i.e., without going back to the database, to producequery extensions about carcinogenicity. For instance, we can combine cytogen caC,n, sulfideC,S Freq0.07and non carcinogenicC, cytogen caC,n, sulfideC,S Freq0.06to generate the query extensioncytogen caC,n, sulfideC,S non carcinogenicC Freq0.06  Conf0.86To rank these rules we have applied a simple binomial test that verifies how surprising theconfidence of query extension substructureCnon carcinogenicC is, i.e., how muchit deviates from the confidence of truenon carcinogenicC. All rules with significancebelow 3   were discarded, with  an estimation of the standard deviation. For instance,the significance level of the above rule is 3.16  . The 215 rules that passed this test werefurther annotated with their significance level on the 13 validation set, and finally combinedwith human domain expertise provided by Ross Donald King Dehaspe et al, 1998. Themain findings are summarized below.5.2.5. Discussion In Experiment 1, only using atombond information, no substructuredescribed with less than 7 logical atoms is found to be related to carcinogenicity. This placesa lower limit on the complexity of rules that are based exclusively on chemical structure.For Experiments 2 and 3, validation on an independent test set showed that the rulesidentified as interesting in the training set were clearly useful in prediction the estimatedaccuracies of the rules from the training data were optimistically biased, as expected.The rules found in Experiments 2 and 3 are dominated by biological tests for carcinogenicity. It is very interesting that these tests appear broadly independent of each other, sothat if a chemical is identified as a possible carcinogen by several of these tests, it is possibleto predict with high probability that it is a carcinogen unfortunately, such compounds arerare.Inspection of the rules from Experiment 2 revealed that the Ashby alerts were not used byany rules. We believe this reflects the difficulty humans and machine have in discoveringgeneral chemical substructures associated with carcinogenicity. However, it is possible thatthe intuitive alerts used by Ashby were incorrectly interpreted and encoded in Prologby King and Srinivasan, 1996.30 DEHASPE AND TOIVONENInspection of the rules from Experiment 3 revealed no interesting substantial chemicalsubstructures atoms connected by bonds in the rules found.Two particularly interesting rules that combine biological tests with chemical attributeswere found. It is difficult to compare these directly with existing knowledge, becausemost work on identifying structural alerts has been based on alerts for carcinogenicity, while both rules identify alerts for noncarcinogenicity. It is reasonable to searchfor noncarcinogenicity alerts as there can be specific chemical mechanisms for this,e.g. cytochromes specifically neutralize harmful chemicals. The rule  cytogen caC,n,ringsulfide,A,B for identifying noncarcinogenic compounds is interesting. The combination of conditions in the rule seems to be crucial the cytogen and sulfide tests in isolationseem to do worse. Within rule  atomchC,A,X, X 0.215, salmonellaC,n the additionof the chemical test makes the biological test more accurate at the expense of less coverage.As the rule refers to charge this rule may be connected to transport across cell membranes.It is interesting and significant that no atombond substructures described with less than7 conditions were found to be related to carcinogenicity. This result is not inconsistentwith the results obtained by King and Srinivasan, 1996 and Srinivasan et al, 1997 usingProgol because most of the substructures there involve partial charges, and the rest donot meet the coverage requirements in Experiment 1.Although the lack of significant atombond substructures found in Experiment 1 is disappointing, it is perhaps not too surprising. The causation of chemical carcinogenesis ishighly complex with many separate mechanisms involved. Therefore predicting carcinogenicity differs from standard drug design problems, where there is normally only a singlewell defined mechanism. We consider that it is probable that the current database is notyet large enough to provide the necessary statistical evidence required to easily identifychemical mechanisms. Biological tests avoid this problem because they detect multiplemolecular mechanisms e.g., the Ames test for mutagenesis detects many different wayschemicals can interact with DNA and cause mutations biological tests also detect whetherthe compound can cross cell membranes and not be destroyed before reaching DNA.The ultimate goal of the work in predictive toxicology is to produce a program thatcan predict carcinogenicity in humans from just input chemical structure. Such a systemwould allow chemicals to be quickly and cheaply tested without harm to any animals.This goal is still distant. Our results suggest that an intermediate goal for data miningin this predictive toxicology problem is to identify the combinations of biological testsand chemical substructures that provides the most costeffective tests for testing chemicalcarcinogenesis.6. Related workWe restrict the discussion of related work to research not explicitly addressed elsewherein the paper. For an overview of inductive logic programming work in the context ofknowledge discovery in databases, we refer to Dzeroski, 1996.FREQUENT DATALOG PATTERNS 316.1. Logical paradigm learning from interpretationsThe definition of frequent query discovery and the relatively efficient candidate evaluation inWarmr is rooted in the learning from interpretations paradigm, introduced by De Raedt andDzeroski De Raedt and Dzeroski, 1994 and related to other inductive logic programmingsettings in De Raedt, 1996. Indeed, subset rk of database r see Section 3.2.2 can beformalized in firstorder logic as a Herbrand interpretation. Every rk in which a querysucceeds is then a Herbrand model of that query.The learning from interpretations paradigm has proven to be particularly suitable forthe design of upgrades to popular attributevalue learning techniques. In that respect,Apriori  Warmr is only one of the more recent additions to a sequence of similar upgrades De Raedt et al, 1998 ExploraKlosgen, 1996Claudien De Raedtand Dehaspe, 1997, CN2 Clark and Niblett, 1989ICL De Raedt and Van Laer, 1995,C4.5 Quinlan, 1986Tilde Blockeel and De Raedt, 1998, Blockeel et al, 1998a, hierarchical clustering Langley, 1996Blockeel et al, 1998b, and reinforcement learningDzeroski et al, 1998.6.2. Clausal discoveryWarmr is the first algorithm that addresses the frequent query discovery task, but the queryextensions derived from its output see Section 3.3 can also be obtained via algorithms thatsearch for good quality rules directly. The discovery of clauses is handled for instance byKnowledge Miner Shen et al, 1996, Claudien De Raedt and Dehaspe, 1997, Midos Wrobel, 1997, RDT Kietz and Wrobel, 1992, Mobal Lindner and Morik, 1995,Laurel Weber, 1997, Weber, 1998, and Progol in learning from positives only modeMuggleton, 1996. We first clarify the link between query extensions and clauses and nextrelate Warmr to clausal discovery algorithms.The frequency of a clause H B, where head H is a disjunction and body B a conjunctionof atoms, can be defined in our framework as frc H  B, r, key k  answerset  key, r   Bk succeeds and  B,Hk fails w.r.t. rk  answerset  key, ri.e., the fraction of substitutions of the key variables with which the body of the clause istrue while there is no way to make the body true and the head false, or, the fraction ofexamples in which the clause nontrivially holds cf. global coverage in Claudien. Theconfidence of a clause H  B can then be defined as the frequency of the whole clausefrc H  B, r, key divided by the frequency of the body frq  B, r, key cf. globalaccuracy in Claudien. By definition of frq and frc the following properties holdfrq  B, r, key  frq  B,H, r, key  frc H  B, r, key  1confidenceB  H  confidenceH  B  1These properties allow us to translate query extensions B  H to clauses H  B andback. For a more detailed account of the relation between clauses and query extensions,we refer to Dehaspe, 1998.32 DEHASPE AND TOIVONENThe key differences between Warmr and clausal discovery algorithms have to do withthe logical expressions in the respective search spaces queries vs. clauses. The efficientlevelwise method for traversing the search space is not directly applicable to clausal discovery. For levelwise search, a quality criterion is required that is monotone with respectto the specialization relation, cf. Mannila and Toivonen, 1997. In a space of queries,frequency is such a monotone quality criterion. In a space of clauses, neither confidencenor frequency qualifies directly, and pruning is often based on the frequency of clausebodies, which is again monotone w.r.t. generality. Moreover, clausal discovery enginesoften have an anytime character and typically incorporate heuristics to direct the searchimmediately to regions where highly confident and frequent rules can be expected. In thatsense clausal discovery engines are complementary to Warmr, which performs a moreexhaustive breadth first search for frequent queries, and only in a postprocessing step candiscover query extensions that meet both the frequency and the confidence standards.7. ConclusionsWe have presented a general Datalog formulation of the frequent pattern discovery problem given a set L of Datalog queries, find out which queries succeed frequently in agiven database. We outlined Wrmode, a declarative formalism for specifying the languagebias, i.e., the search space L of admissible or potentially interesting Datalog queries. Wealso gave an algorithm, Warmr, for solving such tasks.We have demonstrated the use of Warmr and Wrmode in practical tasks in the domainsof telecommunication alarm analysis and chemical toxicology. We have given examples ofhow to use the presented methods in useful novel settings.Warmr, which is available for academic purposes upon request, is a flexible tool thatcan be used by both users and developers as an explorative data mining tool pattern typescan be modified in a flexible way, and thus a number of settings can be easily experimentedwith without changes in the implementation.Possible directions for future research on frequent query discovery include at least thefollowing. First, an efficient general method could be developed for query reorganisationto minimize backtracking during query evaluation cf. Section 3.2.2 and facilitate pruningduring query generation cf. Section 3.2.3. Second, in the spirit of Tables 2 and Table 3, auserfriendly generic system could be developed that automatically selects the most efficientalgorithm available. This could be done on the basis of an analysis of the user inputs, i.e.the database and the language bias. Fourth, Table 2 uncovers a number of gaps thatcould be filled with some useful specialized algorithms. Fifth, many optimizations andtechniques for mining and postprocessing frequent patterns and association rules have beenproposed. Some of these, such as the sampling techniques described in Toivonen, 1996,could probably be plugged into Warmr.AcknowledgmentsLuc Dehaspe is supported by ESPRIT Long Term Research Project No 20237, ILP2. HannuToivonen is supported by the Academy of Finland. This paper was conceived while HannuFREQUENT DATALOG PATTERNS 33Toivonen was visiting the Department of Computer Science, Katholieke Universiteit Leuven.The authors are grateful to Luc De Raedt and Heikki Mannila for comments on the paperand for many fundamental ideas and discussions, to Hendrik Blockeel, Bart Demoen, andWim Van Laer for their share in the implementation of Warmr, and to Hendrik Blockeel,Wim Van Laer, and Helene Legras for proofreading. Data for the telecommunication alarmsequence analysis experiments is provided by Juha Leino from Nokia Telecommunications.The PTE data were made available by Ashwin Srinivasan and Ross King. Ross King alsoprovided the evaluation of patterns in the PTE experiment.Profound comments from anonymous reviewers and Saso Dzerodski are gratefully acknowledged.ReferencesAde, H., De Raedt, L. and Bruynooghe, M. 1995. Declarative Bias for SpecifictoGeneral ILP Systems. MachineLearning 20119  154.Agrawal, R. and Srikant, R. 1995. Mining sequential patterns. Proceedings of the Eleventh InternationalConference on Data Engineering ICDE95, pp. 3  14.Agrawal, R., Imielinski, T. and Swami, A. 1993. Mining association rules between sets of items in large databases.Proceedings of ACM SIGMOD Conference on Management of Data SIGMOD93. ACM, Washington, D.C.,pp. 207  216.Agrawal, R. Mannila, H., Srikant, R., Toivonen, H. and Verkamo, A. I. 1996. Fast discovery of association rules.Advances in Knowledge Discovery and Data Mining. AAAI Press, Menlo Park, CA, pp. 307  328.Ashby, J. and Tennant, R. W. 1991. Definitive relationships among chemical structure, carcinogenicity andmutagenicity for 301 chemicals tested by the U.S. NTP. Mutation Research, 257229306.Bettini, C., Wang, X. S. and Jajodia, S. 1996. Testing complex temporal relationships involving multiplegranularities and its application to data mining. Proceedings of the Fifteenth ACM SIGACTSIGMODSIGARTSymposium on Principles of Database Systems PODS96, pp. 68  78.Blockeel, H. and De Raedt, L. 1996. Relational knowledge discovery in databases. Proceedings of the 6thInternational Workshop on Inductive Logic Programming. Lecture Notes in Artificial Intelligence, SpringerVerlag, 1314, pp. 199212.Blockeel, H. and De Raedt, L. 1998. Topdown induction of first order logical decision trees. ArtificialIntelligence, 101285297.Blockeel, H., De Raedt, L., Jacobs, N. and Demoen, B. 1998a. Scaling up ILP by learning from interpretations.This volume.Blockeel, H., De Raedt, L. and Ramon, J. 1998b. Topdown induction of clustering trees. In Proceedings of the15th International Conference on Machine Learning, 5563. Morgan Kaufmann.Bristol, D., Wachsman, J. and Greenwell, A. 1996. The NIEHS predictivetoxicology evaluation project.Environmental Health Perspectives Supplement 310011010.Clark, P. and Niblett, T. 1989. The CN2 algorithm. Machine Learning 34261284.De Raedt, L. and Dehaspe, L. 1997. Clausal discovery. Machine Learning 2699146.De Raedt, L. and Dzeroski, S. 1994. First order jkclausal theories are PAClearnable. Artificial Intelligence70375392.De Raedt, L. and Van Laer, W. 1995. Inductive constraint logic. In Jantke, K. P. Shinohara, T. and Zeugmann,T., eds., Proceedings of the 6th International Workshop on Algorithmic Learning Theory, volume 997 of LectureNotes in Artificial Intelligence, 8094. SpringerVerlag.De Raedt, L., Blockeel, H., Dehaspe, L. and Van Laer, W. 1998. Three companions for first order data mining.In Dzeroski, S., and Lavrac, N., eds., Inductive Logic Programming for Knowledge Discovery in Databases,Lecture Notes in Artificial Intelligence. SpringerVerlag. To appear.De Raedt, L. 1996. Induction in logic. In Michalski, R., and J., W., eds., Proceedings of the 3rd InternationalWorkshop on Multistrategy Learning, 2938.34 DEHASPE AND TOIVONENDehaspe, L. and De Raedt, L. 1996. DLAB A declarative language bias formalism. In Proceedings of theInternational Symposium on Methodologies for Intelligent Systems ISMIS96, volume 1079 of Lecture Notesin Artificial Intelligence, 613622. SpringerVerlag.Dehaspe, L. and De Raedt, L. 1997. Mining association rules in multiple relations. In Proceedings ofthe 7th International Workshop on Inductive Logic Programming, volume 1297 of Lecture Notes in ArtificialIntelligence, 125132. SpringerVerlag.Dehaspe, L., Toivonen, H. and King, R. 1998. Finding frequent substructures in chemical compounds. InProceedings of the Fourth International Conference on Knowledge Discovery and Data Mining KDD98,30  36. AAAI Press.Dehaspe, L. 1998. Frequent pattern discovery in firstorder logic. Ph.D. Dissertation, K.U.Leuven.Dietterich, T. G., Lathrop, R. H. and LozanoPerez, T. 1997. Solving the multipleinstance problem withaxisparallel rectangles. Artificial Intelligence 89123171.Djoko, S., Cook, D. J. and Holder, L. B. 1995. Analyzing the benefits of domain knowledge in substructurediscovery. In Proceedings of the First International Conference on Knowledge Discovery and Data MiningKDD95, 75  80.Dousson, C., Gaborit, P. and Ghallab, M. 1993. Situation recognition Representation and algorithms. InProceedings of the Thirteenth International Joint Conference on Artificial Intelligence IJCAI93, 166  172.Dzeroski, S., De Raedt, L. and Blockeel, H. 1998. Relational reinforcement learning. In Proceedings of the 15thInternational Conference on Machine Learning. Morgan Kaufmann.Dzeroski, S. 1996. Inductive logic programming and knowledge discovery in databases. In Fayyad, U. PiatetskyShapiro, G. Smyth, P. and Uthurusamy, R., eds., Advances in Knowledge Discovery and Data Mining. MITPress. 118152.Goodman, R. M. and Latin, H. 1991. Automated knowledge acquisition from network management databases. InKrishnan, I., and Zimmer, W., eds., Integrated Network Management, II. Amsterdam, The Netherlands ElsevierScience Publishers B.V NorthHolland. 541  549.Han, J. and Fu, Y. 1995. Discovery of multiplelevel association rules from large databases. In Proceedings ofthe 21st International Conference on Very Large Data Bases VLDB95, 420  431.Hatonen, K. Klemettinen, M. Mannila, H. Ronkainen, P. and Toivonen, H. 1996. Knowledge discovery fromtelecommunication network alarm databases. In Proceedings of the 12th International Conference on DataEngineering ICDE96, 115  122. New Orleans, Louisiana IEEE Computer Society Press.Holsheimer, M., Kersten, M., Mannila, H. and Toivonen, H. 1995. A perspective on databases and data mining.In Proceedings of the First International Conference on Knowledge Discovery and Data Mining KDD95,150  155. Montreal, Canada AAAI Press.Kietz, J. and Lubbe, M. 1994. An efficient subsumption algorithm for inductive logic programming. InProceedings of the 11th International Conference on Machine Learning. Morgan Kaufmann.Kietz, J.U. and Wrobel, S. 1992. Controlling the complexity of learning in logic through syntactic and taskoriented models. In Muggleton, S., ed., Inductive logic programming. Academic Press. 335359.King, R. and Srinivasan, A. 1996. Prediction of rodent carcinogenicity bioassays from molecular structure usinginductive logic programming. Environmental Health Perspectives 104510311040.King, R., Muggleton, S., Srinivasan, A. and Sternberg, M. 1996. Structureactivity relationships derived bymachine learning The use of atoms and their bond connectivities to predict mutagenicity by inductive logicprogramming. Proceedings of the National Academy of Sciences 93438442.Klemettinen, M., Mannila, H. and Toivonen, H. 1998. Rule discovery in telecommunication alarm data. Journalof Network and Systems Management.Klosgen, W. 1996. Explora A multipattern and multistrategy discovery assistant. In Fayyad, U. PiatetskyShapiro, G. Smyth, P. and Uthurusamy, R., eds., Advances in Knowledge Discovery and Data Mining. MITPress.Kramer, S., Pfahringer, B. and Helma, C. 1997. Mining for causes of cancer machine learning experiments atvarious levels of detail. In Proceedings of the Third International Conference on Knowledge Discovery andData Mining KDD97, 223  226.Langley, P. 1996. Elements of Machine Learning. San Mateo, CA Morgan Kaufmann.Lindner, G. and Morik, K. 1995. Coupling a relational learning algorithm with a database system. In Kodratoff,Y. Nakhaeizadeh, G. and Taylor, G., eds., Proceedings of the MLnet Familiarization Workshop on Statistics,Machine Learning and Knowledge Discovery in Databases.Lu, H., Setiono, R., and Liu, H. 1995. Neurorule A connectionist approach to data mining. In Proceedings ofthe 21st International Conference on Very Large Data Bases VLDB95, 478  489.FREQUENT DATALOG PATTERNS 35Mannila, H. and Toivonen, H. 1996. Discovering generalized episodes using minimal occurrences. In Proceedingsof the Second International Conference on Knowledge Discovery and Data Mining KDD96, 146  151.Portland, Oregon AAAI Press.Mannila, H. and Toivonen, H. 1997. Levelwise search and borders of theories in knowledge discovery. DataMining and Knowledge Discovery 13241  258.Mannila, H., Toivonen, H. and Verkamo, A. I. 1997. Discovery of frequent episodes in event sequences. DataMining and Knowledge Discovery 13259  289.Mitchell, T. 1982. Generalization as search. Artificial Intelligence 18203226.Morris, R. A., Khatib, L. and Ligozat, G. 1995. Generating scenarios from specifications of repeating events. InSecond International Workshop on Temporal Representation and Reasoning TIME95.Muggleton, S. and De Raedt, L. 1994. Inductive logic programming  Theory and methods. Journal of LogicProgramming 19,20629679.Muggleton, S. 1995. Inverse entailment and Progol. New Generation Computing 13.Muggleton, S. 1996. Learning from positive data. In Muggleton, S., ed., Proceedings of the 6th InternationalWorkshop on Inductive Logic Programming, 225244. Stockholm University, Royal Institute of Technology.Nedellec, C., Ade, H., Bergadano, F. and Tausend, B. 1996. Declarative bias in ILP. In De Raedt, L., ed.,Advances in Inductive Logic Programming, volume 32 of Frontiers in Artificial Intelligence and Applications.IOS Press. 82103.Oates, T. and Cohen, P. R. 1996. Searching for structure in multiple streams of data. In Proceedings of theThirteenth International Conference on Machine Learning ICML96, 346  354. San Francisco, CA MorganKaufmann.Padmanabhan, B. and Tuzhilin, A. 1996. Pattern discovery in temporal databases A temporal logic approach.In Proceedings of the Second International Conference on Knowledge Discovery and Data Mining KDD96,351354.Plotkin, G. 1970. A note on inductive generalization. In Machine Intelligence, volume 5. Edinburgh UniversityPress. 153163.Quinlan, J. 1986. Induction of decision trees. Machine Learning 181106.Sasisekharan, R., Seshadri, V. and Weiss, S. M. 1996. Data mining and forecasting in largescale telecommunication networks. IEEE Expert, Intelligent Systems  Their Applications 11137  43.Savasere, A., Omiecinski, E. and Navathe, S. 1995. An efficient algorithm for mining association rules inlarge databases. In Proceedings of the 21st International Conference on Very Large Data Bases VLDB95,432  444.Shen, W., Ong, K., Mitbander, B. and Zaniolo, C. 1996. Metaqueries for data mining. In Fayyad, U. PiatetskyShapiro, G. Smyth, P. and Uthurusamy, R., eds., Advances in Knowledge Discovery and Data Mining. MITPress. 375398.Srikant, R. and Agrawal, R. 1995. Mining generalized association rules. In Dayal, U., Gray, P. M. D. and Nishio,S., eds., Proceedings of the 21st International Conference on Very Large Data Bases VLDB95, 407  419.Zurich, Switzerland Morgan Kaufmann.Srikant, R. and Agrawal, R. 1996. Mining sequential patterns Generalizations and performance improvements. In Advances in Database Technology5th International Conference on Extending Database TechnologyEDBT96, 3  17.Srikant, R., Vu, Q. and Agrawal, R. 1997. Mining association rules with item constraints. In Heckerman, D.,Mannila, H., Pregibon, D., and Uthurusamy, R., eds., Proceedings of the Third International Conference onKnowledge Discovery and Data Mining KDD97, 67  73. AAAI Press.Srinisavan, A., King, R. D., Muggleton, S. H. and Sternberg, M. J. E. 1997. The predictive toxicology evaluationchallenge. In Proceedings of the Fifteenth International Joint Conference on Artificial Intelligence IJCAI97.Morgan Kaufmann.Srinivasan, A., King, R., Muggleton, S., and Sternberg, M. 1997. Carcinogenesis predictions using ILP. InProceedings of the 7th International Workshop on Inductive Logic Programming, Lecture Notes in ArtificialIntelligence, 273287. SpringerVerlag.Toivonen, H. 1996. Sampling large databases for association rules. In Proceedings of the 22nd InternationalConference on Very Large Data Bases VLDB96, 134  145. Mumbay, India Morgan Kaufmann.Ullman, J. D. 1988. Principles of Database and KnowledgeBase Systems, volume I. Rockville, MD ComputerScience Press.Wang, K. and Liu, H. 1997. Schema discovery for semistructured data. In Proceedings of the Third InternationalConference on Knowledge Discovery and Data Mining KDD97, 271  274.36 DEHASPE AND TOIVONENWang, J. T.L., Chirn, G.W., Marr, T. G., Shapiro, B., Shasha, D. and Zhang, K. 1994. Combinatorial patterndiscovery for scientific data Some preliminary results. In Snodgrass, R., and Winslett, M., eds., Proceedingsof ACM SIGMOD Conference on Management of Data SIGMOD94, 115  125. Minneapolis, MI ACM.Wang, X., Wang, J. T. L., Shasha, D., Shapiro, B., Dikshitulu, S., Rigoutsos, I. and Zhang, K. 1997. Automateddiscovery of active motifs in three dimensional molecules. In Proceedings of the Third International Conferenceon Knowledge Discovery and Data Mining KDD97, 89  95.Weber, I. 1997. Discovery of firstorder regularities in a relational database using offline candidate determination.Proceedings of the 7th International Workshop on Inductive Logic Programming. Lecture Notes in ArtificialIntelligence, SpringerVerlag, 1297, pp. 288295.Weber, I. 1998. A declarative language bias for levelwise search of firstorder regularities. Proc. Fachgruppentreffen Maschinelles Lernen FGML98. Techn. Univ. Berlin, Technischer Bericht 9811. httpwww.informatik.unistuttgart.deifiisPers onenIrenefgml98.ps.gz.Wrobel, S. 1997. An algorithm for multirelational discovery of subgroups. Proceedings of the First EuropeanSymposium on Principles of Data Mining and Knowledge Discovery PKDD 97. SpringerVerlag, pp. 78 87.Luc Dehaspe received a Masters degree in Philology and a Masters degree in Computer Science from the Universityof Leuven. He obtained his Ph.D. in Computer Science from that same university in December 1998, with a thesison frequent pattern discovery in a firstorder logic framework. He is currently a member of the data mining andinductive logic programming research group at the Department of Computer Science. His research mainly focuseson the development and application of data mining tools that use firstorder logic as the language to represent dataand patterns.Hannu Toivonen leads a data analysis and mining team at Rolf Nevanlinna Institute, University of Helsinki. Healso holds an assistant professorship at the Department of Computer Science, where he earned his Ph.D. in 1996with a thesis on the discovery of frequent patterns. Prior to joining the university Hannu Toivonnen was a researchengineer at Nokia Research Center and at Nokia Telecommunications. His current research interests include, inaddition to data mining, the use of Markov chain Monte Carlo methods for data analysis.
