Proceedings of the 2006 Winter Simulation ConferenceL. F. Perrone, F. P. Wieland, J. Liu, B. G. Lawson, D. M. Nicol, and R. M. Fujimoto, eds.SPLITTING FOR RAREEVENT SIMULATIONPierre LEcuyerValerie DemersDepartement dInformatique et de Recherche OperationnelleUniversite de Montreal, C.P. 6128, Succ. CentreVilleMontreal Quebec, H3C 3J7, CANADABruno TuffinIRISAINRIA, Campus Universitaire de Beaulieu35042 Rennes Cedex, FRANCEABSTRACTSplitting and importance sampling are the two primary techniques to make important rare events happen more frequently in a simulation, and obtain an unbiased estimatorwith much smaller variance than the standard Monte Carloestimator. Importance sampling has been discussed andstudied in several articles presented at the Winter Simulation Conference in the past. A smaller number of WSCarticles have examined splitting. In this paper, we reviewthe splitting technique and discuss some of its strengths andlimitations from the practical viewpoint. We also introduceimprovements in the implementation of the multilevel splitting technique. This is done in a setting where we want toestimate the probability of reaching B before reaching or returning to A when starting from a fixed state x0 6 B, whereA and B are two disjoint subsets of the state space and Bis very rarely attained. This problem has several practicalapplications.1 SETTINGWe consider a discretetime Markov chain X j, j  0 withstate space X . Let A and B be two disjoint subsets of Xand let x0  X B be the initial state. The chain starts instate X0  x0, leaves the set A if x0  A, and then eventually reaches B or A. If x0  A, time 0 is when the chainfirst exits from A. Let A  inf j  0  X j  A, the firsttime when the chain hits A or returns to A after leaving it,and B  inf j  0  X j  B, the first time when the chainreaches the set B. The goal is to estimate   PB  A, theprobability that the chain reaches B before A. This particular form of rareevent problem, where  is small, occurs inmany practical situations Shahabuddin 1994, Heidelberger1995.The standard Monte Carlo method estimates  by running n independent copies of the chain up to the stoppingtime   minA,B, and counting the proportion of runsfor which the event B  A occurs. The resulting estimator n has relative errorREn Varn121 12n 12n,which increases to infinity when   0. This naive estimatoris thus highly unreliable when  is small.An alternative unbiased estimator of  , say n, is said tohave bounded relative error if lim0 REn  . Thisimplies thatlim0logE2n log 2. 1When the latter weaker condition holds, the estimator nis said to be asymptotically efficient Heidelberger 1995,Bucklew 2004. To take into account the computing costof the estimator, it is common practice to consider theefficiency of an estimator n of  , defined as Effn 1VarnCn where Cn is the expected time to compute n. Efficiency improvement means finding an unbiased estimator with larger efficiency than the one previously available. The estimator n has bounded worknormalized relative error, or relative efficiency boundedaway from zero, if lim0 2Effn  0. It is worknormalized asymptotically efficient a weaker condition iflim0 logCnE2n log  2. A sufficient conditionfor this is that 1 holds and lim0 logCn log  0.Splitting and importance sampling are the two major approaches to deal with rareevent simulation. Importancesampling increases the probability of the rare event bychanging the probability laws that drive the evolution of thesystem. It then multiplies the estimator by an appropriatelikelihood ratio to recover the correct expectation i.e., sothat the estimator remains unbiased for  in the above setting. The main difficulty in general is to find a good wayto change the probability laws. For the details, we refer thereader to Glynn and Iglehart 1989, Heidelberger 1995,Bucklew 2004, and many other references given there.In the splitting method, the probability laws remain unchanged, but an artificial drift toward the rare event is created by terminating with some probability the trajectoriesthat seem to go away from it and by splitting cloning thoseLEcuyer, Demers, and Tuffinthat are going in the right direction. In general, an unbiased estimator is recovered by multiplying the original estimator by an appropriate factor in some settings, this factor is 1. The method can be traced back to Kahn and Harris 1951 and has been studied sometimes under differentnames by several authors, including Booth and Hendricks1984, VillenAltamirano and VillenAltamirano 1994,Melas 1997, Garvels and Kroese 1998, Glasserman et al.1998, Glasserman et al. 1999, Fox 1999, Garvels2000, Del Moral 2004, Cerou, LeGland, Del Moral, andLezaud 2005, VillenAltamirano and VillenAltamirano2006, and other references cited there.The splitting methodology was invented to improve theefficiency of simulations of particle transport in nuclearphysics it is used to estimate the intensity of radiationthat penetrates a shield of absorbing material, for exampleHammersley and Handscomb 1964, Spanier and Gelbard1969, Booth and Hendricks 1984, Booth 1985, Booth andPederson 1992, Pederson, Forster, and Booth 1997. Thisremains its primary area of application. It is also used toestimate delay time distributions and losses in ATM andTCPIP telecommunication networks Akin and Townsend2001, Gorg and Fuss 1999. In a recent reallife application, splitting is used to estimate the probability that two airplanes get closer than a nominal separation distance, or evenhit each other, in a stochastic dynamical model of air trafficwhere aircrafts are responsible for selfseparation with eachother Blom et al. 2005.In Section 2, we review the theory and practice of splitting in a setting where we want to estimate   PB  A.We start with multilevel splitting and then discuss more general alternatives. For multilevel splitting, we propose newvariants, more efficient than the standard implementations.Numerical illustrations are given in Section 3. LEcuyer,Demers, and Tuffin 2006 contains an expanded versionof the present overview article. It also studies the combination of splitting and importance sampling with two typesof randomized quasiMonte Carlo methods the classicalone e.g., Owen 1998, LEcuyer and Lemieux 2000 andthe arrayRQMC method for Markov chains proposed byLEcuyer, Lecot, and Tuffin 2005.2 SPLITTING2.1 Multilevel SplittingWe define the splitting algorithm via an importance functionh  X  R that assigns a importance value to each state ofthe chain Garvels, Kroese, and Van Ommeren 2002. Weassume that A  x  X  hx  0 and B  x  X hx   for some constant   0. In the multilevel splittingmethod, we partition the interval 0,  in m subintervalswith boundaries 0 0  1     m  . For k 1, . . . ,m,let Tk  inf j  0  hX j  k, let Dk  Tk  A denotethe event that hX j reaches level k before reaching level0, and define the conditional probabilities pk  PDk Dk1for k  1, and p1  PD1. Since Dm  Dm1     D1,we have  PDm mk1pk.The intuitive idea of multilevel splitting is to estimate eachprobability pk separately, by starting a large number ofchains in states that are generated from the distribution ofXTk1 conditional on the event Dk1. This conditional distribution, denoted by Gk1, is called the firsttime entrancedistribution at threshold k1, for k  1, . . . ,m 1 G0 isdegenerate at x0. Conceptually, the estimation is done insuccessive stages, as follows.In the first stage, we start N0 independent chains fromthe initial state x0 and simulate each of them until timeminA, T1. Let R1 be the number of those chains for whichD1 occurs. Then p1  R1N0 is an obvious unbiased estimator of p1. The empirical distribution G1 of these R1 entrancestates XT1 can be viewed as an estimate of the conditionaldistribution G1.In stage k, for k  2, ideally we would like to generate Nk1 states independently from the entrance distributionGk1. Or even better, to generate a stratified sample fromGk1. But we usually cannot do that, because Gk1 is unknown. Instead, we pick Nk1 states out of the Rk1 that areavailable by cloning if necessary, simulate independentlyfrom these states up to time minA, Tk, and estimate pk bypk  RkNk1 where Rk is the number of chains for whichDk occurs. The initial state of each of the Nk1 chains atthe beginning of stage k has distribution Gk1. Thus, foreach of these chains, the event Dk has probability pk and theentrance state at the next level if Dk occurs has distributionGk.Even though the pks are not independent, we canprove by induction on k that the product p1    pm R1N0R2N1   RmNm1 is an unbiased estimatorof  Garvels 2000, page 17 If we assume thatEp1    pk1  p1    pk1, thenEp1    pk  Ep1    pk1Epk  N0,R1, . . . ,Nk1 Ep1    pk1Nk1pkNk1 p1    pk.Combining this with the fact that Ep1  p1, the result follows.2.2 Fixed Splitting vs Fixed EffortThere are many ways of doing the splitting Garvels 2000.For example, we may clone each of the Rk chains thatreached level k in ck copies, for a fixed positive integer ck.Then, each Nk  ckRk is random. This is fixed splitting. Ifwe want the expected number of splits of each chain to beLEcuyer, Demers, and Tuffinck, where ck  bckc and 0   1, then we assume thatthe actual number of splits is bckc1 with probability  andbckc with probability 1 .In the fixed effort method, we fix each Nk a priori andmake just the right amount of splitting to reach this targetvalue. This can be achieved by random assignment drawthe Nk starting states at random, with replacement, from theRk available states. This is equivalent to sampling Nk statesfrom the empirical distribution Gk of these Rk states. In afixed assignment, on the other hand, we split each of the Rkstates approximately the same number of times as follows.Let ck  bNkRkc and dk  Nk mod Rk. Select dk of the Rkstates at random, without replacement. Each selected state issplit ck 1 times and the other states are split ck times. Thefixed assignment gives a smaller variance than the randomassignment because it corresponds to stratification over theempirical distribution Gk at level k.These variants are all unbiased, but they differ in termsof variance. Garvels and Kroese 1998 conclude from theiranalysis and empirical experiments that fixed effort performsbetter, mainly because it reduces the variance of the numberof chains that are simulated at each stage. It turns out thatwith optimal splitting factors, this is not always true see thenext subsection.The fixed effort implementation with random assignmentfits the framework of interacting particle systems studiedby Del Moral 2004 to approximate FeynmanKac distributions. In this type of system, particles that did not reachthe threshold are killed and replaced by clones of randomlyselected particles among those that have succeeded. This redistributes the effort on most promising particles while keeping the total number constant. Cerou, LeGland, Del Moral,and Lezaud 2005 derive limit theorems for the corresponding estimators.2.3 Variance Analysis for a Simplified SettingWe outline a very crude variance analysis in an idealizedfixedeffort setting whereN0  N1     Nm1  nand where the pis are independent binomial random variables with parameters n and p  1m. Then, for m  1, wehave Garvels 2000, LEcuyer, Demers, and Tuffin 2006Varp1    pmmi1Ep2i  2p2 p1 pnm p2mmp2m11 pnmm1p2m21 p22n2     p1 pmnm.If we assume thatn m11 pp, 2the first term mp2m11 pn  m21mn dominates inthe last expression. The standard Monte Carlo variance, onthe other hand, is 1 n  n. To illustrate the hugepotential variance reduction, suppose   1020, m  20,p  110, and n  1000. Then the MC variance is 1023whereas mp2m11 pn  1.81041. This oversimplified setting is not realistic, because the pi are generally notindependent and it is difficult to have pi  1m for all i, butit gives an idea of the order of magnitude of potential variance reduction.The amount of work or CPU time, or number of stepssimulated at each stage is proportional to n, so the totalwork is proportional to nm. Most of this work is to simulate the n chains down to level 0 at each stage. Thus,the efficiency of the splitting estimator under the simplifiedsetting is approximately proportional to n21mnm2 21mm2 when 2 holds. By differentiating with respect to m, we find that this expression is maximized bytaking m   ln2 we neglect the fact that m mustbe an integer. This gives pm    e2m, so p e2. Garvels and Kroese 1998 have obtained this result.The squared relative error in this case is approximately21mmn2  e2mn  e2 ln2n and the relative efficiency is proportional to 221mm2  em2 e2 ln2, again under the condition 2.When   0 for fixed p, we have m  , so 2 doesnot hold. Then, the relative error increases toward infinityand the relative efficiency converges to zero, at a logarithmicrate in both cases. This agrees with Garvels 2000, page 20.With n  p1    pm, the limit in 1 islim0logp2  p1 pnmlog lim0 logp2  p1 pn log p 2.Thus, this splitting estimator is not quite asymptotically efficient, but almost when n is very large.Consider now a fixedsplitting setting, assuming thatN0 n, pk  p  1m for all k, and that the constant splittingfactor at each stage is c  1p i.e., Nk  Rkp. Then,Nk, k  1 is a branching process and the estimator becomesp1    pm R1N0R2N1   RmNm1Rmpm1n.From standard branching process theory Harris 1963, wehave thatVarp1    pm  m1 pp2m1n.LEcuyer, Demers, and TuffinIf p is fixed andm, then the squared relative errorm1pnp is unbounded here as well. However, the limit in 1becomeslim0logm1 p2np 2log lim02m log p log1m1 pnpm log p 2,so the splitting estimator is asymptotically efficientGlasserman et al. 1999. This implies that fixed splittingis asymptotically better in this case.Glasserman et al. 1999 study the fixed splitting framework with splitting factor ck  c, for a countablestate spaceMarkov chain. They assume that the probability transitionmatrix Pk for the firstentrance state at level k given thefirstentrance state at level k 1 converges to a matrix Pwith spectral radius   1. This implies that pk   whenk. Then they use branching process theory to prove thatthe multilevel splitting estimator in their setting is worknormalized asymptotically efficient if and only if c  1 .Glasserman et al. 1998 show that the condition c  1is not sufficient for asymptotic efficiency and provide additional necessary conditions in a general multidimensionalsetting. Their results highlight the crucial importance ofchoosing a good importance function h.Even though fixed splitting is asymptotically better under ideal conditions, its efficiency is extremely sensitive tothe choice of splitting factors. If the splitting factors aretoo high, the number of chains and the amount of work explodes, whereas if they are too low, the variance is very largebecause very few chains reach B. Since the optimal splittingfactors are unknown in reallife applications, the more robust fixedeffort approach is usually preferable.2.4 ImplementationThe fixedeffort approach has the disadvantage of requiring more memory than fixed splitting, because it must usea breadthfirst implementation at each stage k all the chainsmust be simulated until they reach either A or level k beforewe know the splitting factor at that level. The states of allthe chains that reach k must be saved this may require toomuch memory when the Nks are large. With fixed splitting,we can adopt a depthfirst strategy, where each chain is simulated entirely until it hits  or A, then its most recent clonescreated at the highest level that it has reached are simulatedentirely, then those at the next highest level, and so on. Thisprocedure is applied recursively. At most one state per levelneed to be memorized with this approach. This is feasiblebecause the amount of splitting at each level is fixed a priori.As a second issue, an important part of the work in multilevel splitting is due to the fact that all the chains consideredin stage k from level k1 and which do not reach k mustbe simulated until they get down to A. When k1 is large,this can take significant time. Because of this, the expectedamount of work increases with the number of thresholds.One heuristic that reduces this work in exchange for a smallbias truncates the chains that reach level k downward after they have reached k1, where   2 is a fixed integerlarge enough so that a chain starting at level k has a verysmall probability of getting back up to k. We discuss unbiased alternatives in Section 2.7.2.5 The RESTART AlgorithmThe RESTART method VillenAltamirano andVillenAltamirano 1994, VillenAltamirano andVillenAltamirano 2006 is a variant of splitting whereany chain is split by a fixed factor when it hits a levelupward, and one of the copies is tagged as the original forthat level. When any of those copies hits that same leveldownward, if it is the original it just continues its path, otherwise it is killed immediately. This rule applies recursively,and the method is implemented in a depthfirst fashion,as follows whenever there is a split, all the nonoriginalcopies are simulated completely, one after the other, thensimulation continues for the original chain. Unbiasednessis proved by Garvels 2000 and VillenAltamirano andVillenAltamirano 2002. The reason for killing most ofthe paths that go downward is to reduce the work. Thenumber of paths that are simulated down to A never exceedsN0. On the other hand, the number of chains that reach agiven level is more variable with this method than with thefixedeffort and fixedassignment multilevel splitting algorithm described previously. As a result, the final estimatorof  has a larger variance Garvels 2000. Another source ofadditional variance is that the resplits tend to share a longercommon history and to be more positively correlated. Thissource of variance can be important when the probabilityof reaching B from a given level varies significantly withthe entrance state at that level Garvels 2000. In terms ofoverall efficiency, none of the two methods is universallybetter RESTART wins in some cases and splitting winsin other cases. VillenAltamirano and VillenAltamirano2002 provide a detailed variance analysis of RESTART.2.6 Choice of the Importance Function and OptimalParametersKey issues in multilevel splitting are the choices of the importance function h, levels k, and splitting factors. To discuss this, we introduce some more notation. Let Xk  Xbe the support of the entrance distribution Gk, i.e., the statesin which the chain can possibly be when hitting level k forthe first time. Let x  PB  A    j, X j  x, theprobability of reaching B before A if the chain is currentlyin state x, and pkx  PDk  Dk1,XTk1  x, the probability of reaching level k before hitting A if the chain hasLEcuyer, Demers, and Tuffinjust entered level k 1 in state x, for x  Xk1. Note thatpk xXk1 pkxdGk1x and   x0.Onedimensional case Selecting the levels. If theMarkov chain has a onedimensional state space X  R,x is increasing in x, and if A  ,0 and B  , forsome constant , then we could simply choose hx  x orany strictly increasing function. In this case, the kth level isattained when the state reaches the value k. This value neednot be reached exactly in general, the chain can jump directly from a smaller value to a value larger than k, perhapseven larger than k1. So even in the onedimensional case,the entrance state x at a given level is not unique in generaland the probability pkx of reaching the next level dependson this random entrance state. It remains to choose thelevels k.We saw earlier that in a fixed effort setting and under simplifying assumptions, it is optimal to have pk  p  e2 forall k. This gives m  ln2 levels. To obtain equal pks,it is typically necessary to take unequal distances betweenthe successive levels k, i.e., k k1 must depend on k.Suppose now that we use fixed splitting with ck  1pk e2 for each k. If we assume crudely that each chain is splitby a factor of e2 at each stage, the total number of copies ofa single initial chain that have a chance to reach B ise2m2  e ln2  e21. 3Since each one reaches Bwith probability  , this crude argument indicates that the expected number of chains that reachB is approximately equal to p  e2 times the initial number of chains at stage 0, exactly as in the fixedeffort case.However, the variance generally differs.For RESTART, VillenAltamirano and VillenAltamirano1994 concluded from a crude analysis that pk  e2 wasapproximately optimal. However, their more careful analysis in VillenAltamirano and VillenAltamirano 2002 indicates that the pks should be as small as possible. Since thesplitting factor at each level must be an integer, they recommend pk  12 and a splitting factor of ck  2.Cerou and Guyader 2005 determine the thresholds adaptively for the splitting with fixed effort in dimension 1. Theyfirst simulate n chains trajectories until these chains reachA or B. Then they sort the chains according to the maximum value of the importance function h that each chainhas reached. The k trajectories with the largest values arekept, while the n k others are resimulated, starting fromthe state at which the highest value of the importance function was obtained for the n kth largest one. They proceed like this until n k trajectories have reached B. Theirestimator is proved to be consistent, but is biased.Multidimensional case Defining the importance function. In the case of a multidimensional state space, thechoice of h is much more difficult. Note that h and the ksjointly determine the probabilities pkx and pk. Based onlarge deviation theory, Glasserman et al. 1998 shows thatthe levels need to be chosen in a way consistent with themost likely path to a rare set. Garvels, Kroese, and VanOmmeren 2002 show by induction on k that for any fixedp1, . . . , pm, h should be defined so that pkx  pk independent of x for all x  Xk1 and all k. This rule minimizesthe residual variance of the estimator from stage k onward.With an h that satisfies this condition, the optimal levels andsplitting factors are the same as in the onedimensional casem 12 ln levels, pk  e2 and ENk  N0 for each k.A simple choice of h and ks that satisfies these conditionsishx  hx def x and k  e2mk  e2k.Garvels, Kroese, and Van Ommeren 2002 gave the following equivalent alternative choice k  k for each k andhx  hx deflnx2 mlnx2for all x  X . However, these levels are optimal only ifwe assume that the chain can reach k only on the set x x  e2mk, an optimistic assumption that rarely holdsin practice, especially in the multidimensional case.Garvels, Kroese, and Van Ommeren 2002 also showhow to get a first estimate of x beforehand, in simple situations where the Markov chain has a finite state space, bysimulating the chain backward in time. They construct anapproximation of h from this estimate and then use it intheir splitting algorithm. They apply their method to a tandem queue with two or three nodes and obtain good results.However, this method appears to have limited applicabilityfor large and complicated models.Booth and Hendricks 1984 propose adaptive methodsthat learn the importance function as follows. In their setting, the state space is partitioned in a finite number of regions and the importance function h is assumed to be constant in each region. This importance function is used to determine the expected splitting factors and Russian rouletteprobabilities see Section 2.8 when a chain jumps from oneregion to another. They estimate the average value of xin region j by the fraction of chains that reach B among thosethat have entered region j. These estimates are taken as importance functions in further simulations used to improvethe estimates, and so on.Constructing the functions h or h essentially requiresthe knowledge of the probability x for all x. But if weknew these probabilities, there would be no need for simulation This is very similar and related to the issue ofconstructing the optimal change of measure in importancesampling Glasserman et al. 1998. In general, finding anoptimal h, or an h for which pkx is independent of x, canLEcuyer, Demers, and Tuffinbe extremely difficult or even impossible. When pkx depends on x, selecting the thresholds so that pk  e2 is notnecessarily optimal. More importantly, with a bad choice ofh, splitting may increase the variance, as illustrated by thenext example.Example 1 This example was used by Parekh and Walrand1989, Glasserman et al. 1998, Glasserman et al. 1999,and Garvels 2000, among others. Consider an open tandem Jackson network with two queues, arrival rate 1, andservice rate  j at queue j for j  1,2. Let X j  X1, j, X2, jdenote the number of customers at each of the two queuesimmediately after the jth event arrival or end of service.We have A  0,0 and B  x1,x2  x2   for somelarge integer . A naive choice of importance function herewould be hx1,x2  x2. This seems natural at first sightbecause the set B is defined in terms of x2 only. With thischoice, the entrance distribution at level k turns out to beconcentrated on pairs x1,x2 with small values of x1. Tosee why, suppose that x2  k  0 for some integer k andthat we are in state x1,x2 1 where x1  0 is small. Thepossible transitions are to states x11,x21, x1,x22,and x11,x2, with probabilities proportional to 1, 2, and1, respectively. But the chains that go to state x1 1,x2are cloned whereas the other ones are not, and this tends toincrease the population of chains with a small x1.Suppose now that 1  2 the first queue is the bottleneck. In this case, the most likely paths to overflow arethose where the first queue builds up to a large level andthen the second queue builds up from the transfer of customers from the first queue Heidelberger 1995. The importance function hx1,x2  x2 does not favor these typesof paths it rather favors the paths where x1 remains smalland these paths have a very high likelihood of returning to0,0 before overflow. As a result, splitting with this h maygive an even larger variance than no splitting at all. For thisparticular example, h increases in both x1 and x2 Garvels,Kroese, and Van Ommeren 2002.2.7 Unbiased TruncationWe pointed out earlier that a large fraction of the work inmultilevel splitting is to simulate the chains down to levelzero at each stage. Truncating the chains whenever they fallbelow some level k in stage k reduces the work but introduces a bias. A large  may give negligible bias, but alsoa small work reduction. In what follows, we describe unbiased truncation techniques based on the Russian rouletteprinciple Kahn and Harris 1951, Hammersley and Handscomb 1964.Probabilistic truncation. The idea here is to kill thechains at random, with some probability, independently ofeach other. The survivors act as representatives of the killedchains. For stage k, we select real numbers rk,2, . . . ,rk,k1in 1,. The first time a chain reaches level k j fromabove during that stage, for j  2, it is killed with probability 1 1rk, j. If it survives, its weight is multiplied byrk, j. This is a version of Russian roulette. When a chainof weight w  1 reaches level k, it is cloned into bwc additional copies with probability   wbwc and bw 1cadditional copies with probability 1 . Each copy is givenweight 1. Now, the number of representatives retained at anygiven stage is random. Note that we may have rk, j  1 forsome values of j.Periodic truncation. To reduce the variability of the number of selected representatives at each level k j, we maydecide to retain every rk, jth chain that downcrosses thatlevel and multiply its weight by rk, j e.g., if rk, j  3, wekeep the third, sixth, ninth, etc. This would generally give a biased estimator, because the probability that achain is killed would then depend on its sample path upto the time when it crosses the level for instance, the firstchain that downcrosses the level would always be killed ifrk, j  1. As simple trick to remove that bias is to modifythe method as follows generate a random integer Dk, j uniformly in 1, . . . ,rk, j, retain the i rk, j Dk, jth chain thatdowncrosses level k j for i  0,1,2, . . . , and kill the otherones. We assume that the random variables Dk,2, . . . ,Dk,k1are independent. Then, any chain that downcrosses the levelhas the same probability 11rk, j of being killed, independently of its trajectory above that level. This is true for anypositive integer rk, j. Moreover, the proportion of chains thatsurvive has less variance than for the probabilistic truncationthe killing indicators are no longer independent across thechains. The chains that reach k are cloned in proportion totheir weight, exactly as in the probabilistic truncation.Tagbased truncation. In the periodic truncation method,the level at which a chain is killed is determined only whenthe chain reaches that level. An alternative is to fix all theselevels right at the beginning of the stage. We first select positive integers rk,2, . . . ,rk,k1. Then each chain is tagged tothe level k j with probability qk, j  rk, j1rk,2   rk, jfor j  2, . . . ,k 1, and to level 0 with probability 1qk,k1   qk,2  1rk,2   rk,k1. Thus, all the chainshave the same probability of receiving any given level andthe probability of receiving level zero is positive. If thetags are assigned randomly and independently across thechains, then this method is equivalent to probabilistic truncation. But if the integers rk,2, . . . ,rk,k1 are chosen so thattheir product divides or equals Nk, the number of chains atthe beginning of stage k, then the tags can also be assignedso that the proportion of chains tagged to level k j is exactly qk, j, while the probability of receiving a given tag isthe same for all chains. The reader can verify that the following scheme gives one way of achieving this Put the NkLEcuyer, Demers, and Tuffinchains in a list in any order, generate a random integer Duniformly in 0, . . . ,Nk1, and assign the tag k ji,Dto the ith chain in the list, for all i, where ji,D is thesmallest integer j in 2, . . . ,k such that rk,2   rk, j does notdivides D i mod Nk when D i mod Nk  0, we putji,D  k. . After the tags are assigned, the chains canbe simulated one by one for that stage. Whenever a chaindowncrosses for the first time in this stage a level k jhigher than its tag, its weight is multiplied by rk, j. If itdowncrosses the level of its tag, it is killed immediately.The chains that reach k are cloned in proportion to theirweight, as before.Unbiasedness. LEcuyer, Demers, and Tuffin 2006 showthat all the above truncation methods are unbiased by proving the next proposition and then showing that each truncation method satisfies the assumptions of the proposition.Proposition 1 Suppose there are real numbersrk,2, . . . ,rk,k1 in 1, such that for j  2, . . . ,k  1,each chain has a probability 1 1rk, j of being killed atits first downcrossing of level k j, independently of itssample path up to that moment, and its weight is multipliedby rk, j if it survives. Then the truncated estimator remainsunbiased.Getting rid of the weights. In the unbiased truncationmethods discussed so far, the surviving chains have different weights. The variance of these weights may contributesignificantly to the variance of the final estimator. For example, if k is large, the event that a chain reaches k from k1after going down to 1 is usually a rare event, and when itoccurs the corresponding chain has a large weight, so thismay have a nonnegligible impact on the variance. Thiscan be addressed by resplitting the chains within the stagewhen they upcross some levels, instead of increasing theirweights at downcrossings. We explain how the probabilisticand tagbased truncation methods can be modified to incorporate this idea. In these methods, the weights of all chainsare always 1, and whenever a chain downcrosses k j notonly the first time, for j  2, it can get killed.Probabilistic truncation and resplitting within eachstage. The probabilistic truncation method can be modified as follows. During stage k, whenever a chain reaches alevel k j from below, it is split in rk, j identical copies thatstart evolving independently from that point onward if rk, jis not an integer, we split the chain in brk, j 1c copies withprobability   rk, jbrk, jc and in brk, jc copies with probability 1  . Whenever a chain downcrosses k j notonly the first time, for j  2, it is killed with probability11rk, j. All chains always have weight 1.Tagbased truncation with resplits. This method isequivalent to applying RESTART separately within eachstage of the multistage splitting algorithm. It modifiesthe tagbased truncation as follows Whenever a chain upcrosses level k j for j  2, it is split in rk, j copies. Oneof these rk, j copies is identified as the original and keeps itscurrent tag, while the other rk, j1 copies are tagged to thelevel k j where the split occurs. As before, a chain is killedwhen it downcrosses the level of its tag.Unbiasedness. LEcuyer, Demers, and Tuffin 2006prove the following proposition and show that the two truncation methods with resplits that we just described satisfy itsassumptions.Proposition 2 Suppose there are positive real numbersrk,2, . . . ,rk,k1 such that for j  2, . . . ,k 1, each chain iskilled with probability 1 1rk, j whenever it downcrosseslevel k j, independently of its sample path up to the timewhen it reached that level, and that this chain is split intoC chains when it upcrosses that same level, where C is arandom variable with mean rk, j, independent of the historyso far. Then the estimator with probabilistic truncation andresplits without weights is unbiased for  .Effectiveness and Implementation. The resplit versionsof the truncation methods are expected to give a smallervariance but require more work. So there is no universalwinner if we think of maximizing the efficiency. One disadvantage of the resplit versions is that the number of chainsalive at any given time during stage k has more variance andmay exceed Nk1. In a worstcase situation, a chain may godown and up many times across several levels without beingkilled, giving rise to a flurry of siblings along the way. Fortunately, this type of bad behavior has an extremely smallprobability and poses no problem when the splitting parameters are well chosen. In all our experiments, the numberof chains alive simultaneously during any given stage k hasrarely exceeded Nk1. If we want to insist that the numberof chains never exceeds Nk1, we can use weights insteadof splitting, but just for the splits that would have made thenumber of chains too large. We may want to do that if thechains are stored in an array of size n  Nk1 and we do notwant their number to exceed n. This type of implementationis needed when we combine splitting with the arrayRQMCmethod LEcuyer, Demers, and Tuffin 2006.We have a lot of freedom for the choice of the truncation and resplit parameters rk, j. We can select differentsets of values at the different stages of the multilevel splitting algorithm. It appears sensible to take rk, j  1pk j Nk j1Rk j, the actual splitting factor used at level k j ofthe splitting algorithm, for j  2. In our experiments, thishas always worked well.LEcuyer, Demers, and Tuffin2.8 Getting Rid of the LevelsIn some versions of the splitting and Russian roulette technique, there are no levels or thresholds, but only an importance function some authors call it branching function.For instance, Ermakov and Melas 1995 and Melas 1997study a general setting where a chain can be split or killedat any transition. If the transition is from x to y and if  hyhx 1, then the chain is split in a random number C of copies where EC   , whereas if   1 it iskilled with probability 1 this is Russian roulette. Incase of a split, the C1 new copies are started from state xand new transitions are generated independently for thosechains. Their method is developed to estimate the averagecost per unit of time in a regenerative process, where a statedependent cost is incurred at each step. In the simulation,each cost incurred in a given state x is divided by hx. Wemay view 1hx as the weight of the chain at that point.At the end of a regenerative cycle, the total weighted costaccumulated by the chain over its cycle is the observationassociated with this cycle. The expected cost per cycle isestimated by averaging the observations over all simulatedcycles. The expected length of a cycle is estimated in thesame way, just replacing costs by lengths. The authors showthat their method is consistent and propose an adaptive algorithm that estimates the optimal h.This method can be applied to a finitehorizon simulationas well. In our setting, it suffices to replace the regenerationtime by the time when the chain reaches A or B, and forgetabout the length of the cycle. When a chain reaches B, itcontributes its weight 1hXB to the estimator. For a verycrude analysis, suppose we take hx  x and that thereis a split in two every time the function h doubles its value.Here, hyhx  yx, so a chain that reaches the setBwould have split in two approximately log2  times. Thisgives a potential of 2 log2   1 copies that can possiblyreach B for each initial chain at level 0, the same number asfor the multilevel splitting and RESTART see Equation 3.This argument suggests that an optimal h in this case shouldbe proportional to x.In general, splitting and Russian roulette can be implemented by maintaining a weight for each chain. Initially,each chain has weight 1. Whenever a chain of weight w issplit in C copies, the weight of all the copies is set to eitherwC or wEC. Booth 1985 shows that using wEC isusually better. When Russian roulette is applied, the chain iskilled with some probability   1 if it survives, its weightis multiplied by 11. The values of C and  at eachstep can be deterministic or random, and may depend onthe past history of the chain. Whenever a cost is incurred, itmust be multiplied by the weight of the chain. Unbiasednessfor this general setting is proved under mild conditions byBooth and Pederson 1992, for example.2.9 Weight WindowsParticle transport simulations in nuclear physics often combine splitting and Russian roulette with importance sampling. Then, the weight of each chain must be multipliedby the likelihood ratio accumulated so far. The weight is redefined as this product. In the context of rare events, it isfrequently the case that the final weight of a chain is occasionally large and usually very small. This gives rise to alarge variance and a highlyskewed distribution, for whichvariance estimation is difficult.To reduce the variance of the weights, Booth 1982 introduced the idea of weight windows, which we define as follows see also Booth and Hendricks 1984 and Fox 1999.Define the weighted importance of a chain as the product ofits weight w and the value of the importance function hxat its current state. Select three real numbers 0  amin a amax. Whenever the weighted importance   whx ofa chain falls below amin, we apply Russian roulette, killingthe chain with probability 1a. If the chain survives, itsweight is set to ahx. If the weighted importance  risesabove amax, we split the chain in c  damaxe copies andgive weight wc to each copy. The estimator of   PB A is the sum of weights of all the chains that reach the setB before reaching A. The importance function hx  xshould be approximately optimal in this case. The basic motivation is simple if the weight window is reasonably narrow, all the chains that reach B would have approximatelythe same weight, so the only significant source of variancewould be the number of chains that reach B Booth and Hendricks 1984. If we take a  amin  amax2   , then thisnumber has expectation n approximately, where n is theinitial number of chains.In the original proposal of Booth 1982 and Booth andHendricks 1984, the windows are on the weights, not onthe weighted importance. The state space is partitioned in afinite number of regions say, up to 100 regions, the importance function is assumed constant in each region, and eachregion has a different weight window, inversely proportionalto the value of the importance function in that region. Suchweight windows are used extensively in the Los Alamos particle transport simulation programs. Our formulation is essentially equivalent, except that we do not assume a finitepartition of the state space.Fox 1999, Chapter 10 discusses the use of weight windows for splitting and Russian roulette, but does not mentionthe use of an importance function. Weight windows withoutan importance function could be fine when a good change ofmeasure importance sampling is already applied to drivethe system toward the set B. Then, the role of splitting andRussian roulette is only to equalize the contributions of thechains that reach B and kill most of those whose anticipatedcontribution is deemed negligible, to save work. This type ofsplitting, based only on weights and without an importanceLEcuyer, Demers, and Tuffinfunction, gives no special encouragement to the chains thatgo toward B. If we use it alone, the event B  A willremain a rare event.If there is no importance sampling, the multilevel splitting techniques described earlier except those with truncation and no resplits, in Section 2.7 have the advantage ofnot requiring explicit random weights. All the chains thatreach level k have the same weight when they reach thatlevel for the first time. So there is no need for weight windows in that context.3 EXAMPLESExample 2 We return to Example 1, an open tandem Jackson queueing network with two queues. The choice of h iscrucial for this example, especially if 1  2 Glassermanet al. 1998. Here we look at a case where 1  2. Weconsider the following choices of hh1x1,x2  x2 4h2x1,x2  x2 min0,x2  x1 2 5h3x1,x2  x2 minx1,  x21 1 x2.6The function h1 is a naive choice based on the idea that theset B is defined in terms of x2 only. The second choice, h2,counts  minus half the minimal number of steps requiredto reach B from the current state. To reach B, we need atleast min0,x2  x1   arrivals at the first queue andx2 transfers to the second queue. The third choice, h3, isadapted from VillenAltamirano 2006, who recommendshx1,x2  x2  x1 when 1  2. This h was modified asfollows. We define h3x  x1x2 when x1x2  1 andh3x   when x2  . In between, i.e., in the area wherex11 x2 , we interpolate linearly in x2 for any fixedx1. This gives h3.We did a numerical experiment with 1  4, 2  2, and  30, with our three choices of h. For each h and each truncation method discussed earlier, we computed the varianceper chain, Vn  nVarn, where n is the expected numberof chains at each level, and the worknormalized varianceper chain, Wn  SnVarn, where Sn is the expected totalnumber of simulated steps of the n Markov chains. If Sn isseen as the computing cost of the estimator, then 1Wn isthe usual measure of efficiency. For fixed splitting withouttruncation and resplits, Vn andWn do not depend on n.Here we briefly summarize the detailed results given inLEcuyer, Demers, and Tuffin 2006. We have Vn   1.3 109 with standard Monte Carlo no splitting andVn  1.11016 with the multilevel splitting with h2, usingfixed effort and no truncation. This is a huge variance reduction. With h1, Vn and Wn were significantly higher than forh2 and h3, whereas h3 was just a bit better than h2. The truncation and resplit methods improved the efficiency roughlyby a factor of 3. There is slightly more variance reductionwith the variants that use resplits than with those that do notresplit, but also slightly more work, and the efficiency remains about the same.Example 3 We consider an OrnsteinUhlenbeck stochasticprocess Rt, t  0, which obeys the stochastic differentialequationdRt  abRtdtdW twhere a 0, b, and   0 are constants, and W t, t  0 isa standard Brownian motion Taylor and Karlin 1998. Thisis the Vasicek model for the evolution of shortterm interestrates Vasicek 1977. In that context, b can be viewed asa longterm interest rate level toward which the process isattracted with strength abRt.Suppose the process is observed at times t j  j forj  0,1, . . . and let X j  Rt j. Let A  ,b, B  ,for some constant , and x0  b. We want to estimate theprobability that the process exceeds level  at one of the observation times before it returns below b, when started fromR0  x0. Here we take b  0.Suppose we take the importance function h equal to theidentity. The thresholds k should be placed closer to eachother as k increases, because the attraction toward b  0 becomes stronger. Preliminary empirical experiments suggestthe following rule, which makes the pks approximately independent of k set tentatively k  km for k  1, . . . ,m,let k be the largest k for which k  2, and reset k kkk for k  1, . . . ,k  1. The latter makes the firstthresholds approximately equidistant.Because of the time discretization, the entrance distribution Gk has positive support over the entire interval k,.This means that a chain can cross an arbitrary number ofthresholds in a single jump. The simulation starts from afixed state only at the first level.We made some experiments with a 0.1, b 0,   0.3,x0  0.1,   0.1,   4, and m  14 levels. With these parameters, we haveVn    1.6108 with standard MonteCarlo no splitting andVn  1.01014 with the multilevelsplitting without truncation, with either the fixed splitting orfixed effort approach. The truncation and resplit methodsimprove the worknormalized varianceWn roughly by a factor of 3, as in the previous example. The work is reduced bya factor of 4.3 without the resplits and by a factor of 3.5 withthe resplits, but the variance is increased roughly by a factorof 1.4 without the resplits and 1.2 with the resplits.The benefits of splitting and of truncation increase with. For   6, for example, we have Vn    4.2 1018with standard Monte Carlo and Vn  5.0 1033 with themultilevel splitting without truncation, with m  30 thisgives pks of approximately the same size as with   4 andm  14. In this case, the truncation and resplit methods reduce the worknormalized variance approximately by a factor of 8 to 10. Fixed effort and fixed splitting also have comparable efficiencies when no truncation is used.LEcuyer, Demers, and TuffinExample 4 There are situations where the splitting methodis not appropriate whereas importance sampling can bemade very effective. Consider for example a highlyreliableMarkovian multicomponent system Shahabuddin 1994 forwhich the failure of a few components e.g., 2 or 3 may besufficient for the entire system to fail, and where all the components have a very small failure rate and a high repair rate.If we want to apply splitting, the thresholds must be definedin terms of the vector of failed components the state of thesystem. But whenever there are failed components, the nextevent is a repair with a very high probability. So regardlessof how we determine the thresholds, the probabilities pk ofreaching the next threshold from the current one are alwaysvery small. For this reason, the splitting method cannot bemade efficient in this case. On the other hand, there are effective importance sampling methods for this type of modelShahabuddin 1994, Cancela, Rubino, and Tuffin 2002.4 CONCLUSIONSplitting is a valuable but seemingly underexploited variance reduction technique for rareevent simulation. It certainly deserves further study. In multidimensional settings,finding out an appropriate importance function h can bea difficult task and seems to the the main bottleneck foran effective application of the method. Providing furtherhints in this direction, and developing adaptive techniquesto learn good importance functions, would be of significantinterest. Unfortunately, splitting can hardly be applied toproblems where rarity comes from the occurrence of a lowprobability transition that cannot be decomposed in severalhigherprobability transitions.ACKNOWLEDGMENTSThis research has been supported by NSERCCanada grantNo. ODGP0110050 and a Canada Research Chair to thefirst author, an NSERCCanada scholarship to the secondauthor, and EuroNGI Network of Excellence, INRIAs cooperative research initiative RARE and SurePath ACI Security Project to the third author, and an FQRNTINRIATravel Grant to the first and third authors.REFERENCESAkin, O., and J. K. Townsend. 2001. Efficient simulation ofTCPIP networks characterized by nonrare events usingDPRbased splitting. In Proceedings of IEEE Globecom,17341740.Blom, H. A. P., G. J. Bakker, J. Krystul, M. H. C. Everdij,B. K. Obbink, and M. B. Klompstra. 2005. SequentialMonte Carlo simulation of collision risk in free flight airtraffic. Technical report, Project HYBRIDGE IST200132460.Booth, T. E. 1982. Automatic importance estimation in forward Monte Carlo calculations. Transactions of the American Nuclear Society 41308309.Booth, T. E. 1985. Monte Carlo variance comparison forexpectedvalue versus sampled splitting. Nuclear Scienceand Engineering 89305309.Booth, T. E., and J. S. Hendricks. 1984. Importance estimation in forward Monte Carlo estimation. Nuclear TechnologyFusion 590100.Booth, T. E., and S. P. Pederson. 1992. Unbiased combinations of nonanalog Monte Carlo techniques and fairgames. Nuclear Science and Engineering 110254261.Bucklew, J. A. 2004. Introduction to rare event simulation.New York SpringerVerlag.Cancela, H., G. Rubino, and B. Tuffin. 2002. MTTF estimation by Monte Carlo methods using Markov models.Monte Carlo Methods and Applications 8 4 312341.Cerou, F., and A. Guyader. 2005, October. Adaptive multilevel splitting for rare event analysis. Technical Report5710, INRIA.Cerou, F., F. LeGland, P. Del Moral, and P. Lezaud. 2005.Limit theorems for the multilevel splitting algorithm inthe simulation of rare events. In Proceedings of the 2005Winter Simulation Conference, ed. F. B. A. M. E. Kuhl,N. M. Steiger and J. A. Joines, 682691.Del Moral, P. 2004. FeynmanKac formulae. genealogicaland interacting particle systems with applications. Probability and its Applications. New York Springer.Ermakov, S. M., and V. B. Melas. 1995.Design and analysisof simulation experiments. Dordrecht, The NetherlandsKluwer Academic.Fox, B. L. 1999. Strategies for quasiMonte Carlo. Boston,MA Kluwer Academic.Garvels, M. J. J. 2000. The splitting method in rare eventsimulation. Ph. D. thesis, Faculty of mathematical Science, University of Twente, The Netherlands.Garvels, M. J. J., and D. P. Kroese. 1998. A comparison ofRESTART implementations. In Proceedings of the 1998Winter Simulation Conference, 601609 IEEE Press.Garvels, M. J. J., D. P. Kroese, and J.K. C. W. Van Ommeren. 2002. On the importance function in splittingsimulation. European Transactions on Telecommunications 13 4 363371.Glasserman, P., P. Heidelberger, and P. Shahabuddin. 1999.Asymptotically optimal importance sampling and stratification for pricing path dependent options. MathematicalFinance 9 2 117152.Glasserman, P., P. Heidelberger, P. Shahabuddin, and T. Zajic. 1998. A large deviations perspective on the efficiencyof multilevel splitting. IEEE Transactions on AutomaticControl AC43 12 16661679.Glasserman, P., P. Heidelberger, P. Shahabuddin, and T. Zajic. 1999. Multilevel splitting for estimating rare eventprobabilities. Operations Research 47 4 585600.LEcuyer, Demers, and TuffinGlynn, P. W., and D. L. Iglehart. 1989. Importancesampling for stochastic simulations. Management Science 3513671392.Gorg, C., and O. Fuss. 1999. Simulating rare event details ofatm delay time distributions with restartlre. In Proceedings of the IEE International Teletrafic Congress, ITC16,777786 Elsevier.Hammersley, J. M., and D. C. Handscomb. 1964. Montecarlo methods. London Methuen.Harris, T. 1963. The theory of branching processes. NewYork SpringerVerlag.Heidelberger, P. 1995. Fast simulation of rare events inqueueing and reliability models. ACM Transactions onModeling and Computer Simulation 5 1 4385.Kahn, H., and T. E. Harris. 1951. Estimation of particletransmission by random sampling. National Bureau ofStandards Applied Mathematical Series 122730.LEcuyer, P., V. Demers, and B. Tuffin. 2006. Rareevents,splitting, and quasiMonte Carlo. submitted.LEcuyer, P., C. Lecot, and B. Tuffin. 2005. A randomizedquasiMonte Carlo simulation method for Markov chains.submitted.LEcuyer, P., and C. Lemieux. 2000. Variance reduction vialattice rules.Management Science 46 9 12141235.Melas, V. B. 1997. On the efficiency of the splitting androulette approach for sensitivity analysis. In Proceedingsof the 1997 Winter Simulation Conference, 269274. Piscataway, NJ IEEE Press.Owen, A. B. 1998. Latin supercube sampling for very highdimensional simulations. ACM Transactions on Modelingand Computer Simulation 8 1 71102.Parekh, S., and J. Walrand. 1989. A quick simulation methodfor excessive backlogs in networks of queues. IEEETransactions on Automatic Control AC345456.Pederson, S. P., R. A. Forster, and T. E. Booth. 1997. Confidence intervals for Monte Carlo transport simulation. Nuclear Science and Engineering 1275477.Shahabuddin, P. 1994. Importance sampling for the simulation of highly reliable markovian systems. ManagementScience 40 3 333352.Spanier, J., and E. M. Gelbard. 1969. Monte Carlo principles and neutron transport problems. Reading, Massachusetts AddisonWesley.Taylor, H. M., and S. Karlin. 1998. An introduction to stochastic modeling. third ed. San Diego Academic Press.Vasicek, O. 1977. An equilibrium characterization of theterm structure. Journal of Financial Economics 5177188.VillenAltamirano, J. 2006. Rare event RESTART simulation of twostage networks. manuscript.VillenAltamirano, M., and J. VillenAltamirano. 1994.RESTART A straightforward method for fast simulationof rare events. In Proceedings of the 1994 Winter Simulation Conference, 282289 IEEE Press.VillenAltamirano, M., and J. VillenAltamirano. 2002.Analysis of RESTART simulation Theoretical basis andsensitivity study. European Transactions on Telecommunications 13 4 373386.VillenAltamirano, M., and J. VillenAltamirano. 2006. Onthe efficiency of RESTART for multidimensional systems. manuscript.AUTHOR BIOGRAPHIESPIERRE LECUYER is Professor in the DepartementdInformatique et de Recherche Operationnelle, at the Universite de Montreal, Canada. He holds the Canada Research Chair in Stochastic Simulation and Optimization.His main research interests are random number generation, quasiMonte Carlo methods, efficiency improvementvia variance reduction, sensitivity analysis and optimizationof discreteevent stochastic systems, and discreteevent simulation in general. He is currently AssociateArea Editorfor ACM TOMACS, ACM TOMS, and Statistical Computing. He obtained the prestigious E. W. R. Steacie fellowshipin 199597 and a Killam fellowship in 200103. His recentresearch articles are available online from his web pagehttpwww.iro.umontreal.calecuyer.VALERIE DEMERS is a PhD Student in mathematics at the Universite de Montreal. Her main researchareas are randomized quasiMonte Carlo methods forMarkov chains, rareevent simulation, and variance reduction methods in general. Her email address isdemersvIRO.UMontreal.CA.BRUNO TUFFIN received his PhD degree in appliedmathematics from the University of Rennes 1 France in1997. Since then, he has been with INRIA in Rennes.He spent 8 months as a postdoc at Duke University in1999. His research interests include developing MonteCarlo and quasiMonte Carlo simulation techniques forthe performance evaluation of telecommunication systems, and developing new Internetpricing schemes. Hisweb page is www.irisa.frarmorlesmembresTuffinTuffin en.htm.
