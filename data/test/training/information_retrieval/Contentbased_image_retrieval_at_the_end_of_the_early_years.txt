ContentBased Image Retrievalat the End of the Early YearsArnold W.M. Smeulders, Senior Member, IEEE, Marcel Worring, Simone Santini, Member, IEEE,Amarnath Gupta, Member, IEEE, and Ramesh Jain, Fellow, IEEEAbstractThe paper presents a review of 200 references in contentbased image retrieval. The paper starts with discussing theworking conditions of contentbased retrieval patterns of use, types of pictures, the role of semantics, and the sensory gap.Subsequent sections discuss computational steps for image retrieval systems. Step one of the review is image processing for retrievalsorted by color, texture, and local geometry. Features for retrieval are discussed next, sorted by accumulative and global features,salient points, object and shape features, signs, and structural combinations thereof. Similarity of pictures and objects in pictures isreviewed for each of the feature types, in close connection to the types and means of feedback the user of the systems is capable ofgiving by interaction. We briefly discuss aspects of system engineering databases, system architecture, and evaluation. In theconcluding section, we present our view on the driving force of the field, the heritage from computer vision, the influence on computervision, the role of similarity and of interaction, the need for databases, the problem of evaluation, and the role of the semantic gap.Index TermsReview, content based, retrieval, semantic gap, sensory gap, narrow domain, broad domain, weak segmentation,accumulative features, salient features, signs, structural features, similarity, semantic interpretation, query space, display space,interactive session, indexing, architecture, evaluation, image databases.1 INTRODUCTIONTHERE is something about Munchs The Scream orConstables Wivenoe Park that no words can convey.It has to be seen. The same holds for of a picture of theKalahari Desert, a dividing cell, or the facial expression ofan actor playing King Lear. It is beyond words. Try toimagine an editor taking in pictures without seeing them ora radiologist deciding on a verbal description. Pictures haveto be seen and searched as pictures by objects, by style, bypurpose.Research in contentbased image retrieval today is alively discipline, expanding in breadth. As happens duringthe maturation process of many a discipline, after earlysuccesses in a few applications, research is now concentrating on deeper problems, challenging the hard problems atthe crossroads of the discipline from which it was borncomputer vision, databases, and information retrieval.At the current stage of contentbased image retrievalresearch, it is interesting to look back toward the beginningand see which of the original ideas have blossomed, whichhavent, and which were made obsolete by the changinglandscape of computing. In February 1992, the US NationalScience Foundation USNSF organized a workshop inRedwood, California, to identify major research areas thatshould be addressed by researchers for visual informationmanagement systems that would be useful in scientific,industrial, medical, environmental, educational, entertainment, and other applications 81. In hindsight, theworkshop did an excellent job of identifying unsolvedproblems that researchers should have undertaken. Inparticular, the workshop correctly stated that VisualInformation Management Systems should not be considered as an application of the existing state of the art incomputer vision and databases to manage and processimages and that computer vision researchers shouldidentify features required for interactive image understanding,rather than their disciplines current emphasis on automatictechniques emphasis added. As possible applicationfields, the workshop considered mainly Grand Challengeproblems, such as weather forecasting, biological modeling,medical images, satellite images, and so on. Undoubtedly,the participants saw enough to justify the use of the largecomputational and storage capacity necessary for visualdatabases. This in 1992. The workshop was preceded bymany years by the Conference on Database Applications ofPictorial Applications, held in Florence in 1979, probablyone of the first conferences of that kind 13. In theintroduction, it was said that This has facilitated theadvancement of integrated databases ... on the one hand,of and graphical and image processing in brief pictorialapplications on the other. Then, the author proceeds tocomplain that Developments in these two fields havetraditionally been unrelated, an observation still verymuch valid today.IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 22, NO. 12, DECEMBER 2000 1349. A.W.M. Smeulders and M. Worring are with Intelligent SensoryInformation Systems, University of Amsterdam, faculty WINS Kruislaan403, 1098 SJ Amsterdam, The Netherlands.Email smeulders, worringwins.uva.nl.. S. Santini and A. Gupta are with the Department of Electrical andComputer Science Engineering and the San Diego Super Computer Center,University of California, San Diego, 9500 Gilman Dr., La Jolla, CA 920390407. Email ssantiniece.ucsd.edu.. J. Ramesh is with Praja, Inc., 10455B Pacific Center Court, San Diego,CA 92121. Email jainpraja.com.Manuscript received 23 July 1999 revised 12 May 2000 accepted 12 Sept.2000.Recommended for acceptance by K. Bowyer.For information on obtaining reprints of this article, please send email totpamicomputer.org, and reference IEEECS Log Number 110298.016288280010.00  2000 IEEEJust after the USNSF workshop, the Mosaic Internetbrowser was released, spawning the Web revolution thatvery quickly changed all cards. In the same era, a host ofnew digital vision sensors became available. The number ofimages that the average user could reach increaseddramatically in just a few years. Instantly, indexing toolsfor the Web or digital archives became urgent.In this paper, we present a view of what we like to callthe early years of contentbased image retrieval. Whilecontent basedimage retrieval papers published prior to1990 are rare, often obsolete, and of little direct impacttoday, the number of papers published since 1997 is justbreathtaking. So much, in fact, that compiling a comprehensive review of the state of the art already exceeds thepossibility of a paper like this one. A selection wasnecessary and with it came the need to establish someselection criteria. In addition to the obvious one completeness of a paper, importance to the field, we have alsoconsidered accessibility for the reader. That is to say, wehave preferred, whenever possible, to include journalpapers over conference papers. We also felt that the fieldis too young and mobile to make a precise historic accountand we have made no attempt in that direction.We adopt patterns of use and patterns of computation asthe leading principles of our review. We follow the data asthey flow through the computational process and consideralternative processes with the same position in the flowFig. 2. In the data flow diagrams, we use the conventionsindicated in Fig. 1. We concentrate on computationalmethods to arrive at a toolbased overview rather than asystembased overview. The choice implies that referencesdescribing complete systems are split, where parts of themethod will appear in several sections of the paper. For asystembased review, see 141.We restrict ourselves to still pictures and leave videodatabases as a separate topic. Video retrieval could beconsidered a broader topic than image retrieval as video isbuilt from single images. From another perspective, videoretrieval could be considered simpler than image retrievalsince video reveals its objects more easily as the pointscorresponding to one object move together. In still pictures,the authors narrative expression of intention is in frameselection, illumination, and composition. In addition, videohas a linear timeline, as important to the narrative structureof video as it is in text. We leave video retrieval for anotherplace, for example, 1, 16.The paper is organized as indicated in Fig. 2. First wediscuss the scope of the contentbased retrieval in Section 2.In that section, the characteristics of the domain and sourcesof knowledge are being discussed. Then, description ofcontent is analyzed in two steps. First, in Section 3, imageprocessing methods by color, texture, and local shape arediscussed. They serve as a preprocessing step to thepartitioning of the data array and the computation offeatures, as discussed in Section 4. In Section 5, we discussthe interpretation of a single image and the similarity1350 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 22, NO. 12, DECEMBER 2000Fig. 1. Data flow and symbol conventions as used in this paper. Differentstyles of arrows indicate different data structures.Fig. 2. Basic algorithmic components of query by pictorial example captured in a dataflow scheme while using the conventions of Fig. 1.between a pair of images. Query definition, display, andinteraction are the topic of Section 6. The paper concludes atthe level of systems indexing, system architecture, andevaluation of performance. Each chapter is concluded by adiscussion on the state of the art.2 SCOPEIn the literature, a wide variety of contentbased retrievalmethods and systems may be found. In this section, wediscuss patterns in applications, the repertoire of images,the influence of the scene and the role of domainknowledge, and the semantic gap between image featuresand the user.2.1 Applications of ContentBased RetrievalIn 31, we see three broad categories of user aims whenusing the system, see Fig. 3.. There is a broad class of methods and systems aimedat browsing through a large set of images fromunspecified sources. Users of search by association atthe start have no specific aim other than findinteresting things. Search by association oftenimplies iterative refinement of the search, thesimilarity or the examples with which the searchwas started. Systems in this category typically arehighly interactive, where the specification may bysketch 30 or by example images. The oldest realisticexample of such a system is probably 88. The resultof the search can be manipulated interactively byrelevance feedback 68, 51. To support the questfor relevant results, other sources than images arealso employed, see for example, 168, 21.. Another class of users aims the search at a specificimage. The search may be for a precise copy of theimage in mind, as in searching art catalogues, e.g., 48.Target search may also be for another image of thesame object of which the user has an image. This istarget search by example. Target search may also beapplied when the user has a specific image in mindand the target is interactively specified as similar to agroup of given examples, for instance 31. Thesesystems are suited to search for stamps, art, industrialcomponents, and catalogues, in general.. The third class of applications, category search, aimsat retrieving an arbitrary image representative of aspecific class. It may be the case that the user has anexample and the search is for other elements of thesame class. Categories may be derived from labels oremerge from the database 170, 186. In categorysearch, the user may have available a group ofimages and the search is for additional images of thesame class 28. A typical application of categorysearch is catalogues of varieties. In 74, 79, systemsare designed for classifying trademarks. Systems inthis category are usually interactive with a domainspecific definition of similarity.These three types of use are not the whole story 42. Astudy 121 of journalists identified five typical patterns ofuse searches for one specific image, general browsing tomake an interactive choice, searches for a picture to go witha broad story, searches to illustrate a document, andsearches for fillins only on the esthetic value of the picture.An attempts to formulate a general categorization of userrequests for still and moving images are found in 6. Thisand similar studies reveal that the range of queries is widerthan just retrieving images based on the presence orabsence of objects of simple visual characteristics.2.2 The Image Domain and the Sensory GapIn the repertoire of images under considerationthe imagedomain Ithere is a gradual distinction between narrowand broad domains 160. At one end of the spectrum, wehave the narrow domainA narrow domain has a limited and predictable variability in allrelevant aspects of its appearance.In a narrow domain, one finds a limited variability of thecontent of the images. Usually, the recording circumstancesare also similar over the whole domain. In the narrowdomain of lithographs, for instance, the recording is underwhite light with frontal view and no occlusion. Also, whenthe objects appearance has limited variability, the semanticdescription of the image is generally welldefined and, byand large, unique. Another example of a narrow domain isa set of frontal views of faces recorded against a clearbackground. Although each face is unique and has largevariability in the visual details, there are obvious geometrical, physical, and colorrelated constraints governing thedomain. The domain would be wider had the faces beenphotographed from a crowd or from an outdoor scene. Inthat case, variations in illumination, clutter in the scene,occlusion, and viewpoint will have a major impact on theanalysis.On the other end of the spectrum, we have the broaddomainA broad domain has an unlimited and unpredictable variability inits appearance even for the same semantic meaning.In broad domains, images are polysemic and theirsemantics are described only partially. It might be the casethat there are conspicuous objects in the scene for which theobject class is unknown or even that the interpretation of thescene is not unique. A broad class of images can be found inlarge photo stocks 168 or other photo archives 42. TheSMEULDERS ET AL. CONTENTBASED IMAGE RETRIEVAL AT THE END OF THE EARLY YEARS 1351Fig. 3. Three patterns in the purpose of contentbased retrieval systems.broadest class available to date is the set of images availableon the Internet.Many problems of practical interest have an imagedomain in between these extreme ends of the spectrum, seeFig. 4. The notions of broad and narrow domains are helpfulin characterizing patterns of use, in selecting features, andin designing systems. In a broad image domain, the gapbetween the feature description and the semantic interpretation is generally wide. For narrow, specialized imagedomains, the gap between features and their semanticinterpretation is usually smaller, so domainspecific modelsmay help. For faces, many geometric models have beensuggested, as well as statistical models 127. Thesecomputational models are not available for broad imagedomains as the required number of computational variableswould be enormous.For broad image domains in particular, one has to resortto generally valid principles. Is the illumination of thedomain white or colored Does it assume defined and fullyvisible objects or may the scene contain clutter andoccluded objects Is it a 2Drecording of a 2Dscene or a2Drecording of a 3Dscene The given characteristics ofillumination, presence or absence of occlusion, clutter, anddifferences in camera viewpoint determine demands on theretrieval methods.The sensory gap is the gap between the object in the world and theinformation in a computational description derived from arecording of that scene.The sensory gap makes the description of objects an illposed problem It yields uncertainty in what is knownabout the state of the object. The sensory gap is particularlypoignant when a precise knowledge of the recordingconditions is missing. The 2Drecords of different3Dobjects can be identical. Without further knowledge,one has to decide that they might represent the same object.Also, a 2Drecording of a 3Dscene contains informationaccidental for that scene and that sensing but one does notknow what part of the information is scene related. Theuncertainty due to the sensory gap not only holds for theviewpoint, but also for occlusion where essential partstelling two objects apart may be out of sight, clutter, andillumination.Comparing alternative interpretations can attenuate thesensory gap. Contentbased image retrieval systems mayprovide support in this disambiguation through eliminationamong several potential explanations, much the same as innatural language processing.2.3 Domain KnowledgeIn visual search, explicit representation of the knowledge ofthe domain is important to alleviate the sensory gap.Among the sources of general knowledge, we mention. Laws of syntactic literal equality and similaritydefine the relation between image pixels or imagefeatures regardless of its physical or perceptualcauses. For instance, considering two images similarbecause they both exhibit some selected shades ofblue in their upper parts is productive in separatingoutdoor scenes from other images. It is syntacticsimilarity because the method doesnt make areference to the reasons by which this similarityexists in this case, the scattering in the sky or to theperceptual reasons by which these two images willappear similar to an observer. By the same token, theRGB color space is effective in literal similarity as itis effective in art 65 while it does not represent theprocess of physical color formation or the process ofcolor perception.. Laws describing the human perception of equalityand similarity are important because they defineequality on the same basis as the user experiences it.In color, the CIELab and Munsellspaces weredesigned to conform to the human perception ofcolor similarity. If the appreciation of a humanobserver of an object is based on the perception ofcertain conspicuous items in the image 177, it isnatural to direct the computation of broad domainfeatures to these points and regions 157, 138.Similarly, a biologically plausible architecture 76 ofcentersurround processing units is likely to selectregions which humans would also focus on first.. Physical laws describing equality and difference ofimages under differences in sensing and objectsurface properties. The physics of illumination,surface reflection, and image formation have ageneral effect on images. The general laws of physicsmay be employed for large classes of objects. Acommon example is the law for uniform lightreflection off matte objects. These laws are exploitedto design color features expressing equality regardless of the pose and viewpoint.. Geometric and topological rules describe equalityand differences of patterns in space. When twoobjects are geometrically equal, the physical properties of their surfaces or the physical conditions of thesensing may be different. As an example ofgeometric laws used in retrieval, for all images withdepth, local details near the horizon will appearsmaller. Also, the horizon is geometrically defined as1352 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 22, NO. 12, DECEMBER 2000Fig. 4. Quick reference to narrow versus broad domains.a virtual line containing the focal points. Anotherexample of geometric laws is the expression ofspatial 22 or topological relationships 172 between objects.. Categorybased rules encode the characteristicscommon to class z of the space of all notions Z. Ifz is the class of all teapots, the characteristics includethe presence of a spout. Categories are almostexclusively used in a narrow domains. The domainknowledge may take the form of further constraintsto the literal image qualities, additional physical orgeometrical laws, or domainspecific manmadecustoms. When the domain is engineering drawings,detailed geometric knowledge will steer the detection of symbols. In medieval art, color and therelative position of objects have a symbolic meaning30, generating a set of constraints useful in thesearch. Each application domain has a private set ofconstraints.. Finally, manmade customs or manrelated patternsintroduce rules of culturebased equality and difference. Under culture, we also assume language. In thesearch for indoor pictures, one may check for manystraight lines and perpendicular corners as a firstselection criterion. Utensils have a deterministic sizeto allow grip. Fashion determines colors 95.These laws are ordered as indicated in Fig. 5.2.4 Use and User, the Semantic GapWe opine that most of the disappointments with earlyretrieval systems come from the lack of recognizing theexistence of the semantic gap and its consequences forsystem setup.The semantic gap is the lack of coincidence between theinformation that one can extract from the visual data and theinterpretation that the same data have for a user in a givensituation.A linguistic description is almost always contextual,whereas an image may live by itself. A linguistic description of an image is a daunting, probably impossible task146. A user looks for images containing certain objects orconveying a certain message. Image descriptions, on theother hand, rely on datadriven features and the two may bedisconnected. Association of a complete semantic system toimage data would entail at least solving the general objectrecognition problem from a single image. Since thisproblem is yet unsolved, research is focused on differentmethods to associate higher level semantics to datadrivenobservables.As indicated in Fig. 2, the most immediate means ofsemantic characterization entail annotation by keywords orcaptions. This reduces contentbased access to informationretrieval 135. Common objections to the practice oflabeling are cost and coverage. On the cost side, labelingthousands of images is a cumbersome and expensive job tothe degree that the deployment of the economic balancebehind the database is likely to decrease. To solve theproblem, systems in 21, 142 use a program that exploresthe Internet, collecting images and inserting them in apredefined taxonomy on the basis of the text surroundingthem. A similar approach for digital libraries is taken by24. On the coverage side, labeling is seldom complete,context sensitive, and, in any case, there is a significantfraction of requests whose semantics cant be captured bylabeling alone 6, 64. Both methods will cover thesemantic gap only in isolated cases.2.5 Discussion on ScopeThe pivotal point in contentbased retrieval is that the userseeks semantic similarity, but the database can only providesimilarity by data processing. This is what we called thesemantic gap. At the same time, the sensory gap betweenthe properties in an image and the properties of the objectplays a limiting role in retrieving the content of the image.We discussed applications of contentbased retrieval inthree broad types target search, category search, and searchby association. Target search connects with the tradition ofpattern matching in computer vision. New challenges incontentbased retrieval are the huge amount of objects tosearch among, the incomplete query specification, theSMEULDERS ET AL. CONTENTBASED IMAGE RETRIEVAL AT THE END OF THE EARLY YEARS 1353Fig. 5. When searching for a chair, we may be satisfied with any object under that name, that is, we search for mandefined equality. When wesearch for all one leg chairs, we add an additional constraint to the general class and restrict the equality class. The same holds when searching for ared chair, adding a condition independent of the geometrical constraint. When we search for a chair perceptually equivalent to a given chair, at leastphysical and geometrical equality must hold. Finally, when we search for exactly the same image of that chair, literal equality is requested, stillignoring the variations due to noise, of course.incomplete image description, and the variability of sensingconditions and object states. Category search builds on theobject recognition and statistical pattern recognition methods in computer vision. New challenges in contentbasedretrieval compared to the achievements of object recognition are the interactive manipulation of results, the usuallyvery large number of object classes, and the absence of anexplicit training phase for feature and classifier tuning.Search by association is further removed from most ofthe computer vision tradition. It is hampered most by thesemantic gap. As long as the gap is there, use of contentbased retrieval for browsing will not be within the grasp ofthe general public as humans are accustomed to rely on theimmediate semantic imprint the moment they see an image.The aim of contentbased retrieval systems must be toprovide maximum support in bridging the semantic gapbetween the simplicity of available visual features and therichness of the user semantics.We analyze characteristics of the image domain, thedomain knowledge, and the types of use as the primefactors determining the functionality of a system. Animportant distinction is that between broad and narrowdomains. The broader the domain, the more browsing orsearch by association can be the right solution. Thenarrower the domain, the more likely an application ofdomain knowledge will succeed. The challenge for imagesearch engines on a broad domain is to tailor the engine to thenarrow domain the user has in mind via specification, examples,and interaction.3 DESCRIPTION OF CONTENT IMAGE PROCESSINGIt is important to establish that contentbased retrieval doesnot rely on describing the content of the image in itsentirety. It may be sufficient that a retrieval system presentssimilar images, similar in some userdefined sense. Thedescription of content should serve that goal primarily.We consider the description of content in two steps. First,we discuss imageprocessing operations that transpose theimage data into another spatial data array, see Fig. 6. Wedivide the methods over local color, the local texture, orlocal geometry. They may be characterized in general byfx  g  ix 1where ix is the image, element of image space I , g is anoperator on images, and the resulting image field is givenby fx. Computational parameters of g may include thesize of the neighborhood around x to compute fx or ahomogeneity criterion when the size of the patch tocompute fx depends on the actual data, as in 163,126, for example.So, the purpose of image processing in image retrievalmust be to enhance aspects in the image data relevant to thequery and to reduce the remaining aspects.One such goal can be met by using invariance as a tool todeal with accidental distortions in the information introduced by the sensory gap. From the above discussion on thesensory gap, it is clear that invariant features may carrymore objectspecific information than other features as theyare insensitive to the accidental conditions of the sensing.The aim of invariant descriptions is to identify objects, nomatter from how and where they are observed, at the loss ofsome of the information content. If two objects ti or twoappearances of the same object are equivalent under agroup of transformations W , they are in an equivalenceclass 18t1 W t2  9w 2W  t2  w  t1 2A property f of t is invariant under W if and only if ftremains the same regardless the unwanted conditionexpressed by W ,t1 W t2  ft1  ft2  3The degree of invariance, that is, the dimensionality ofthe group W , should be tailored to the recording circumstances. In general, a feature with a very wide class ofinvariance loses the power to discriminate among essentialdifferences. The size of the class of images consideredequivalent grows with the dimensionality of W . In the end,the invariance may be so wide that no discriminationamong objects is retained. The aim is to select the tightestset of invariants suited for the expected set of nonconstantconditions. What is needed in image search is a specification of the minimal invariant conditions in the specificationof the query discussed in 159. The minimal set of invariantconditions can only be specified by the user as it is part ofhis or hers intention. The oldest work on invariance incomputer vision has been done in object recognition, asreported, among others, in 117. Invariant description inimage retrieval is relatively new, but quickly gainingground for a good introduction, see 18, 32. An alternativeto invariant features1 is to represent the viewing conditionsseparately from the objects in the scene. This way noinformation is lost in the reduction to invariant features,while the information is only rearranged. It should be left tothe later stages to decide what is important.3.1 Color Image ProcessingColor has been an active area of research in image retrieval,more than in any other branch of computer vision. Colormakes the image ix take values in a color vector space.The interest in color may be ascribed to the superiordiscriminating potentiality of a threedimensional domaincompared to the single dimensional domain of graylevelimages.1354 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 22, NO. 12, DECEMBER 20001. As pointed out by one of the referees of this paper.Fig. 6. The data flow diagrams for image processing modules, see Fig. 1for conventions.Two aspects of color return in many of the contributions.One is that the recorded color varies considerably with theorientation of the surface, the viewpoint of the camera, theposition of the illumination, the spectrum of the illuminant,and the way the light interacts with the object. Thisvariability should be dealt with in one way or another.Second, the human perception of color is an intricate topicwhere many attempts have been made to capture perceptual similarity.Only when there is no variation in the recording or in theperception is the RGB color representation a good choicesince that representation was designed to match the inputchannel of the eye. RGBrepresentations are in widespreaduse. They describe the image in its literal color properties.An image expressed as Rx Gx Bx indices will beomitted from now on makes most sense when recording inthe absence of variance, as is the case, e.g., for art paintings64, the color composition of photographs 48, andtrademarks 79, 39, where twodimensional images arerecorded in frontal view under standard conditions.A significant improvement over the RGBcolor space atleast for retrieval applications comes from the use ofopponent color representations 169, which uses theopponent color axes RG 2BRGRGB. Thisrepresentation has the advantage of isolating the brightnessinformation on the third axis. With this solution, the firsttwo chromaticity axes can be downsampled as humans aremore sensitive to brightness than they are to chroma. Theyare invariant to changes in illumination intensity andshadows.Others approaches use the Munsell or the Labspacesbecause of their relative perceptual uniformity. The Labrepresentation is designed so that the Euclidean distancebetween two colors representations models the humanperception of color differences. Care should be taken whendigitizing the nonlinear conversion to Labspace 115.The HSVrepresentation is often selected for its invariantproperties. The hue is invariant under the orientation of theobject with respect to the illumination and camera directionand hence more suited for object retrieval.A wide variety of tight photometric color invariants forobject retrieval were derived in 57 from an analysis of theSchafer model of object reflection. They derive for mattepatches under white light the invariant color spaceRGRG BRBR GBGB only dependent on sensor and surface albedo. For a shinysurface and white illumination, they derive the invariantrepresentation asjRGjjRGj  jBRj  jGBjand two more permutations. The color models are robustagainst major viewpoint distortion.Color constancy is the capability of humans to perceivethe same apparent color in the presence of variations inillumination which change the physical spectrum of theperceived light. In computer vision, color constancy wasfirst considered in 49. For flat, matte, and uniformlyilluminated objects, the paper forms the canonical gamutdefined as the convex set of physically feasible normalizedRGB, i.e., rgbresponses. The reference then maps allobserved rgbresponses in the image into the canonicalone. The map explaining all observations determines thecolor constancy. In 47, this is improved to include specularreflection, shape, and varying illumination. By computingthe blueratio vector rb  gb  1, only color orientation is usedand intensity is ruled out. In this 2Dspace, the colorconstancy map can again be selected from a canonicalgamut of colors and surfaces. In 56, the ratiosRx1Gx2Rx2Gx1Gx1Bx2Gx2Bx1Bx1Rx2Bx2Rx1 offer more stability to surface geometry variations. Colorconstancy was applied to retrieval in 54 by using anillumination invariant color representation. The authorsindex the ratio of neighboring colors. Color constantindexing leads to some loss in discriminating power amongobjects, but yields illumination independent retrievalinstead. The scheme was improved in 158 by usingalgebraic invariants.Rather than invariant descriptions, another approach tocope with the inequalities in observation due to surfacereflection is to search for clusters in a color histogram of theimage. In the RGBhistogram, clusters of pixels reflected offan object form elongated streaks. Hence, in 126, anonparametric cluster algorithm in RGBspace is used toidentify which pixels in the image originate from oneuniformly colored object.3.2 Image Processing for Local ShapeUnder the name local shape, we collect all properties thatcapture conspicuous geometric details in the image. Weprefer the name local shape over differential geometricalproperties to express the result rather than the method. Theresult of local shape evaluation is a dense image data fielddifferent from object shape discussed in Section 4.Local shape characteristics derived from directional colorderivativesin the paper referred to as texture propertieshave been used in 115 to derive perceptuallyconspicuous details in highly textured patches of diversematerials. A wide, rather unstructured variety of imagedetectors can be found in 165.Scale space theory was devised as the complete andunique primary step in preattentive vision, capturing allconspicuous information 187. It provides the theoreticalbasis for the detection of conspicuous details on any scale.In 105, a series of Gabor filters of different directions andscale have been used to enhance image properties 137.Conspicuous shape geometric invariants are presented in136. A method employing local shape and intensityinformation for viewpoint and occlusion invariant objectretrieval is given in 148. The method relies on votingamong a complete family of differential geometric invariants. Also, 178 searches for differential affineinvariantdescriptors. From surface reflection, in 5, the local sign ofthe Gaussian curvature is computed while making noassumptions on the albedo or the model of diffusereflectance.SMEULDERS ET AL. CONTENTBASED IMAGE RETRIEVAL AT THE END OF THE EARLY YEARS 1355Combining shape and color both in invariant fashion is apowerful combination, as described by 56, where thecolors inside and outside affine curvature maximums incolor edges are stored to identify objects.3.3 Image Texture ProcessingIn computer vision, texture is defined as all what is left aftercolor and local shape have been considered or it is definedby such terms as structure and randomness. Many commontextures are composed of small textons usually too great innumber to be perceived as isolated objects. The elementscan be placed more or less regularly or randomly. They canbe almost identical or subject to large variations in theirappearance and pose. In the context of image retrieval,research is mostly directed toward statistical or generativemethods for the characterization of patches.Basic texture properties include the Markovian analysis,dating back to Haralick in 1973, and generalized versionsthereof 91, 58. In retrieval, the property is computed in asliding mask for localization 99, 59.Another important texture analysis technique usesmultiscale autoregressive MRSARmodels, which considertexture as the outcome of a deterministic dynamic systemsubject to state and observation noise 174, 106. Othermodels exploit statistical regularities in the texture field 9.Wavelets 34 have received wide attention. They haveoften been considered for their locality and their compression efficiency. Many wavelet transforms are generated bygroups of dilations or dilations and rotations that have beensaid to have some semantic correspondent. The lowestlevels of the wavelet transforms 34, 26 have been appliedto texture representation 92, 162 sometimes in conjunction with Markovian analysis 25. Other transforms havealso been explored, most notably fractals 44. A solidcomparative study on texture classification from mostlytransformbased properties can be found in 133.Texture search proved useful in satellite images 98 andimages of documents 33. Textures also served as a supportfeature for segmentationbased recognition 102, but thetexture properties discussed so far offer little semanticreferent. They are therefore illsuited for retrieval applications in which the user wants to use verbal descriptions of theimage. Therefore, in retrieval research, in 101, the Woldfeatures of periodicity, directionality, and randomness areused, which agree reasonably well with linguisticdescriptions of textures as implemented in 128.3.4 Discussion on Image ProcessingImage processing in contentbased retrieval should primarily be engaged in enhancing the image information thequery poses, not in describing the content of the image in itsentirety.To enhance the image information, retrieval has set thespotlights on color, as color has a high discriminatingpower among objects in a scene, much higher than graylevels. The purpose of most image color processing is toreduce the influence of the accidental conditions of thescene and sensing i.e., the sensory gap. Progress has beenmade in tailored color space representation for welldescribed classes of variant conditions. Also, the applicationof geometric description derived from scale space theorywill reveal viewpoint and scene independent salient pointsets, thus opening the way to similarity of images on a fewmost informative regions or points.In this section, we have made a separation between color,local geometry, and texture. At this point, it is safe toconclude that the division is artificial. Wavelets say something about the local shape as well as the texture and so domany scale space and local filter strategies. For the purposesof contentbased retrieval, an integrated view on color,texture, and local geometry is urgently needed as only anintegrated view on local properties can provide the meansto distinguish among hundreds of thousands differentimages. A recent advancement in that direction is thefusion of illumination and scale invariant color and textureinformation into a consistent set of localized properties 66.Also, in 20, homogeneous regions are represented ascollections of ellipsoids of uniform color or texture, butinvariant texture properties deserve more attention, 173and 185. Further research is needed in the design ofcomplete sets of image properties with welldescribedvariant conditions which they are capable of handling.Invariance is just one side of the coin, where discriminatingpower is the other. In contentbased image retrieval, thefirst steps are taken to establish the discriminating power ofinvariant properties 55. This is essential as the balancebetween stability against variations and retained discriminatory power determines the effectiveness of a property.4 DESCRIPTION OF CONTENT FEATURESIn the first section, we discuss the ultimate form of spatialdata by grouping the data into object silhouettes, clusters ofpoints or pointsets. In subsequent sections, we leave thespatial domain to condense the pictorial information intofeature values.4.1 Grouping DataIn contentbased image retrieval, the image is often dividedin parts before features are computed from each part, seeFig. 7. Partitionings of the image aim at obtaining moreselective features by selecting pixels in a tradeoff againsthaving more information in features when no subdivisionof the image is used at all. We distinguish the followingpartitionings. When searching for an object, it would be mostadvantageous to do a complete object segmentationfirst Strong segmentation is a division of the image1356 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 22, NO. 12, DECEMBER 2000Fig. 7. Symbolic representation of different ways to group image data.data into regions in such a way that region Tcontains the pixels of the silhouette of object O in thereal world and nothing else, specified by T  O.It should be noted immediately that object segmentation forbroad domains of general images is not likely to succeed,with a possible exception for sophisticated techniques invery narrow domains.. The difficulty of achieving strong segmentation maybe circumvented by weak segmentation wheregrouping is based on datadriven properties Weaksegmentation is a grouping of the image data inconspicuous regions T internally homogenous according to some criterion, hopefully with T  O.The criterion is satisfied if region T is within the bounds ofobject O, but there is no guarantee that the region covers allof the objects area. When the image contains two nearlyidentical objects close to each other, the weak segmentationalgorithm may falsely observe just one patch. Fortunately,in contentbased retrieval, this type of error is rarelyobstructive for the goal. In 125, the homogeneity criterionis implemented by requesting that colors be spatiallycoherent vectors in a region. Color is the criterion in 50,126. In 20, 112, the homogeneity criterion is based oncolor and texture. The limit case of weak segmentation is aset of isolated points 148, 57. No homogeneity criterion isneeded then, but the effectiveness of the isolated points reston the quality of their selection. When occlusion is presentin the image, weak segmentation is the best one can hopefor. Weak segmentation is used in many retrieval systems,either as a purpose of its own or as a preprocessing stage fordatadriven modelbased object segmentation.. When the object has a nearly fixed shape, like atraffic light or an eye, we call it a sign Localizingsigns is finding an object with a fixed shape andsemantic meaning, with T  xcenter.Signs are helpful in contentbased retrieval as they deliveran immediate and unique semantic interpretation.. The weakest form of grouping is partitioning Apartitioning is a division of the data array regardlessof the data, symbolized by T 6 O.The area T may be the entire image or a conventionalpartitioning as the central part of the image against theupper, right, left, and lower parts 67. The feasibility offixed partitioning comes from the fact that images arecreated in accordance with certain canons or normativerules, such as placing the horizon about 23 up in thepicture or keeping the main subject in the central area. Thisrule is often violated, but this violation in itself has semanticsignificance. Another possibility of partitioning is to dividethe image in tiles of equal size and summarize the dominantfeature values in each tile 130.Each of these four approaches to partitioning leads to apreferred type of features, as summarized in Fig. 8 andillustrated in Fig. 9, where feature hierarchies are used tomake a combination on all types.4.2 Global and Accumulating FeaturesIn the computational process, features are calculated next.The general class of accumulating features aggregate thespatial information of a partitioning irrespective of theimage data. A special type of accumulative features are theglobal features which are calculated from the entire image.Accumulating features are symbolized byFj XTjh  fx 4where  represents an aggregations operation the sum inthis case, but it may be a more complex operator. Fj isthe set of accumulative features or a set of accumulativefeatures ranked in a histogram. Fj is part of feature spaceF . Tj is the partitioning over which the value of Fj iscomputed, see Fig. 9 for an illustration. In the case ofglobal features, Tjvoid represents the image, otherwise, TjSMEULDERS ET AL. CONTENTBASED IMAGE RETRIEVAL AT THE END OF THE EARLY YEARS 1357Fig. 8. The different types of features using the data flow conventions of Fig. 1.represents a fixed tiling of the image. The operator h mayhold relative weights, for example, to compute transformcoefficients.A simple but very effective approach to accumulatingfeatures is to use the histogram, that is, the set of featuresFm ordered by histogram index m. The original idea touse histograms for retrieval comes from Swain and Ballard169, who realized that the power to identify an objectusing color is much larger than that of a grayvalued image.As a histogram loses all information about the location of anobject in the image, 169, 41 project the histogram backinto the image to locate it by searching for best matches. Ahistogram may be effective for retrieval as long as there is auniqueness in the color pattern held against the pattern inthe rest of the entire data set. In addition, the histogramshows an obvious robustness to translation of the object androtation about the viewing axis. Swain and Ballard alsoargue that color histograms change slowly with change inviewpoint and scale and with occlusion.All of this is in favor of the use of histograms. When verylarge data sets are at stake, plain histogram comparison willsaturate the discrimination. For a 64bin histogram, experiments show that, for reasonable conditions, the discriminating power among images is limited to 25,000 images 167.To keep up performance, in 125, a joint histogram is used,providing discrimination among 250,000 images in theirdatabase, rendering 80 percent recall among the best 10 fortwo shots from the same scene using simple features. Otherjoint histograms add local texture or local shape 61,directed edges 78, and local higher order structures 48.Another alternative is to add a dimension representing thelocal distance. This is the correlogram 73, defined as a threedimensional histogram where the colors of any pair are alongthe first and second dimension and the spatial distancebetween them along the third. The autocorrelogram definingthe distances between pixels of identical colors is found on thediagonal of the correlogram. A more general version is thegeometric histogram 134, with the normal histogram, thecorrelogram, and several alternatives as special cases. Thisalso includes the histogram of the triangular pixel values,reported to outperform all of the above as it contains moreinformation.To avoid an explosion of dimensions of the histogram,one could also prefer to reconsider the quality of theinformation along each of the dimensions. In Section 3, wehave considered invariant representations suited to enrichthe information on the axes of the histogram as it rules outthe accidental influence of sensing and scene conditions.A different view on accumulative features is to demandthat all information or all relevant information in theimage is preserved in the feature values. When the bitcontent of the features is less than the original image, thisboils down to compression transforms. Many compressiontransforms are known, but the quest is for transformssimultaneously suited as retrieval features. As properquerying for similarity is based on a suitable distancefunction between images, the transform has to be appliedon a metric space. The components of the transform have tocorrespond to semantically meaningful characteristics of theimage. Finally, the transform should admit indexing incompressed form yielding a big computational advantageover having the image be untransformed first. Schneier andAbdelMottaleb 149 is just one of many where the cosinebased JPEGcoding scheme is used for image retrieval. TheJPEGtransform fulfills the first and third requirement, butfails on a lack of semantics. In the MPEGstandard, thepossibility of including semantic descriptors in the compression transform is introduced 29. For an overview offeature indexes in the compressed domain, see 103. In 92,a wavelet packet was applied to texture images and, foreach packet, entropy and energy measures were determined and collected in a feature vector. In 75, vectorquantization was applied in the space of coefficients toreduce its dimensionality. This approach was extended toincorporate the metric of the color space in 146. In 77, awavelet transform was applied independently to the threechannels of a color image and only the sign of the mostsignificant coefficients is retained. In a recent paper 3, a1358 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 22, NO. 12, DECEMBER 2000Fig. 9. Illustration of the various feature types as discussed in the paper.scheme is offered for a broad spectrum of invariantdescriptors suitable for application on Fourier, wavelets,and splines and for geometry and color alike.Another type of complete feature sets capturing allinformation in the image is to use moments. Their invariantcombinations of moments 72 and 89 have been successfully employed in retrieval of objects in 48, especiallywhen the image contains just the object.4.3 Salient FeaturesAnother way to avoid the brittleness of strong segmentationis to opt for weak segmentation. This leads to a grouping ofthe data into homogeneous regions. From the mergedregions, a selection is made on their salience. The mostconspicuous regions are stored. The limit case of a weaksegmentation is the detection of conspicuous points, see Fig. 9.Salient features may be covered by the generic equationFjx Tjh  fx 5where  stands for a local selection operation and operator hmaximizes the saliency of the processed image field fx.The area Tj over which the value of Fj is searched for isusually the whole image, although there would be noobjection to concentrating on the center or top part of theimage in search for specific events.As the information of the image is condensed into just alimited number of feature values, the information should beselected with precision for greatest saliency and provenrobustness. That is why saliency in 100 is defined as thespecial points which survive longest when gradually blurringthe image in scale space. Also, in 138, lifetime is animportant selection criterion for salient points in addition towiggliness, spatial width, and phase congruency. To enhancethe quality of salient descriptions, in 178, invariant andsalient features of local patches have been considered. In eachcase, the image is summarized in a list of conspicuous points.In 148, salient and invariant transitions in gray value imagesare recorded. Similarly, in 57, 55, photometric invariance isthe leading principle in summarizing the image in salienttransitions in the image. Salient feature calculations lead tosets of regions or points with known location and featurevalues capturing their salience.In 20, first, the most conspicuous homogeneous regionsin the image are derived and mapped into feature space.Then, expectationmaximization 37 is used to determinethe parameters of a mixture of Gaussians to model thedistribution of points into the feature space. The means andcovariance matrices of these Gaussians, projected on theimage plane, are represented as ellipsoids characterized bytheir center x, their area, eccentricity, and direction. Theaverage values of the color and texture descriptions insidethe ellipse are also stored.4.4 SignsWhen one of the possible interpretations of an image is sopreponderant that it can be considered the meaning of theimage, the image holds a sign, characterized by theprobability P on interpretation zPzx  P zjhz  fx 6with symbols as in 5. The analysis leads to a localization ofa sign with its probability. Typical signs are an icon, acharacter, a traffic light, or a trademark. In the case of maps,the interpretation of map symbols and their spatial relationships provides access to the content of the map 144. Othersystems based on signs are designed with specific application domains in mind, like OCR from an image 200, facesto detect from the image 197, medical images 90, 17,textile 95, art 65, or detecting the constituent componentsof silhouettes of plants based on a visual lexicon in 180.For signs, a strong semantic interpretation is withingrasp and the undisputed semantic interpretation bringsclarity in interpreting the image. That is the attractiveness ofusing signs, in spite of the fact that the analysis tends tobecome applicationoriented.4.5 Shape and Object FeaturesThe theoretically best way to enhance objectspecificinformation contained in images is by segmenting theobject in the image. But, as discussed above, the brittlenessof segmentation algorithms prevents the use of automaticsegmentation in broad domains. In fact, in many cases, it isnot necessary to know exactly where an object is in theimage as long as one can identify the presence of the objectby its unique characteristics. When the domain is narrow, atailored segmentation algorithm may be needed more and,fortunately, also be better feasible. When segmentation isapplied, we havetjx  sj  fx 7where fx is the data field resulting from the processingabove equal to the image ix when g is the identityoperator, sj is the segmentation operator for object j, andtjx indicates the object area Tj. For shape, Fj is a possiblyordered set of features from F for jFj XTjh  tjx 8where  represents an aggregation operation and h is thefunctional computing shape in this case. Object internalfeatures are computed similar to 4.The object internal features are largely identical to theaccumulative features, now computed over the object area.They need no further discussion here.An abundant comparison of shape for retrieval can befound in 109, evaluating many features on a 500elementtrademark data set. Straightforward features of generalapplicability include Fourier features and moment invariants of the object this time, sets of consecutive boundarysegments, or encoding of contour shapes 43.For retrieval, we need a shape representation that allowsa robust measurement of distances in the presence ofconsiderable deformations. Many sophisticated modelswidely used in computer vision often prove too brittle forimage retrieval. On the other hand, the interactive use ofretrieval makes some mismatch acceptable and, therefore,precision can be traded for robustness and computationalefficiency.SMEULDERS ET AL. CONTENTBASED IMAGE RETRIEVAL AT THE END OF THE EARLY YEARS 1359More sophisticated methods include elastic matchingand multiresolution representation of shapes. In elasticdeformation of image portions 36, 122 or modalmatching techniques 150, image patches are deformed tominimize a cost functional that depends on a weighed sumof the mismatch of the two patches and on the deformationenergy. The complexity of the optimization problemdepends on the number of points on the contour. Hence,the optimization is computationally expensive and this, inspite of the greater precision of these methods, has limitedtheir diffusion in image databases.Multiscale models of contours have been studied as arepresentation for image databases in 116. Contours wereextracted from images and progressively smoothed bydividing them into regions of constant sign of the secondderivative and progressively reducing the number of suchregions. At the final step, every contour is reduced to anellipsoid which could be characterized by some of thefeatures in 48. A different view on multiresolution shape isoffered in 94, where the contour is sampled by a polygonand then simplified by removing points from the contouruntil a polygon survives selecting them on perceptualgrounds. When computational efficiency is at stake, anapproach for the description of the object boundaries isfound in 201, where an ordered set of critical points on theboundary are found from curvature extremes. Such sets ofselected and ordered contour points are stored in 108relative to the basis spanned by an arbitrary pair of thepoints. All point pairs are used as a basis to make theredundant representation geometrically invariant, a technique similar to 192 for unordered point sets.For retrieval of objects in 2Dimages of the 3Dworlds, aviewpoint invariant description of the contour is important.A good review of global shape invariants is given in 140.4.6 Description of Structure and LayOutWhen feature calculations are available for different entitiesin the image, they may be stored with a relationshipbetween them, see Fig. 9 for an illustration. Such astructural feature set may contain feature values plusspatial relationships, a hierarchically ordered set of featurevalues, or relationships between point sets or object sets.The process is symbolized byHjk XTjkh  fx 9where Tjk indicates the kth part of the jth object and Hjk isan ordered spatial relationship describing object j in kelements. Structural and layout feature descriptions arecaptured in a graph, hierarchy, or any other ordered set offeature values and their relationships.To that end, in 107, 50, layout descriptions of anobject are discussed in the form of a graph of relationsbetween blobs. A similar layout description of an image interms of a graph representing the spatial relations betweenthe objects of interest was used in 129 for the descriptionof medical images. In 53, a graph is formed of topologicalrelationships of homogenous RGBregions. When selectedfeatures and the topological relationships are viewpointinvariant, the description is viewpoint invariant, but theselection of the RGBrepresentation as used in the paperwill only suit that purpose to a limited degree. The systemsin 70, 163 study spatial relationships between regions,each characterized by locations, size, and features. In thelater system, matching is based on the 2Dstring representation founded by Chang and Hau 22. For a narrow domain,in 129, 132, the relevant elements of a medical Xrayimage are characterized separately and joined together in agraph that encodes their spatial relations.Starting from a shape description, the authors in 94decompose an object into its main components, making thematching between images of the same object easier.Automatic identification of salient regions in the image,based on nonparametric clustering followed by decomposition of the shapes found into limbs, is explored in 52.4.7 Discussion on FeaturesAlso in the description of the image by features, it should bekept in mind that for retrieval a total understanding of theimage is rarely needed. Strong segmentation of the sceneand complete feature descriptions may not be necessary atall to achieve a similarity ranking. Of course, the deeper onegoes into the semantics of the pictures, the deeper theunderstanding of the picture will also have to be, but thecritical point in the advancement of contentbased retrievalis the semantic meaning of the image that is rarely selfevident.The theoretically best approach to a semantic interpretation of an image remains the use of a strong segmentation ofthe scene. Automatic strong segmentation is, however, hardto achieve, if not impossible for general domains. Thebrittleness of strong segmentation is a mostly unsurpassableobstacle when describing the content of images by describing the content of its objects. Especially for broad domainsand for sensory conditions where clutter and occlusion areto be expected, automatic strong segmentation is hard, if notimpossible. In that case, segmentation is to be done by handwhen retrieval relies on it.Narrow domains such as trademark validation, theidentification of textiles, and the recognition of fish dependon the shape of the object, assessing similarity on the basis ofthe silhouettes. The finetocoarse decompositions are attractive in their discriminating power and computationalefficiency. Again, a major bottleneck is the highly accuratesegmentation of the object as well as a frontal viewpoint ofthe object. In selected narrow domains, this may be achievedby recording the object against a clear background.General contentbased retrieval systems have dealt withsegmentation brittleness in a few ways. First, a weakerversion of segmentation has been introduced in contentbased retrieval. In weak segmentation, the result is ahomogeneous region by some criterion, but not necessarilycovering the complete object silhouette. It results in a fuzzy,blobby description of objects, rather than a precisesegmentation. Salient features of the weak segments capturethe essential information of the object in a nutshell. Theextreme form of the weak segmentation is the selection of asalient point set as the ultimately efficient data reduction inthe representation of an object, very much like the focusofattention algorithms for an earlier age. Only points on the1360 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 22, NO. 12, DECEMBER 2000interior of the object can be used for identifying the objectand conspicuous points at the borders of objects have to beignored. Little work has been done on how to make theselection. Weak segmentation and salient features are atypical innovation of contentbased retrieval. It is expectedthat salience will receive much attention in the furtherexpansion of the field, especially when computationalconsiderations gain in importance.The alternative is to do no segmentation at all. Contentbased retrieval has gained from the use of accumulativefeatures, computed on the global image or partitioningsthereof, disregarding the content, the most notable beingthe histogram. Where most attention has gone to colorhistograms, histograms of local geometric properties andtexture are following. To compensate for the complete lossof spatial information, the geometric histogram wasrecently defined with an additional dimension for thespatial layout of pixel properties. As it is a superset of thehistogram, an improved discriminability for large data setsis anticipated. When accumulative features are calculatedfrom the central part of a photograph may be very effectivein telling them apart by topic, but the center does notalways reveals the purpose. Likewise, features calculatedfrom the top part of a picture may be effective in tellingindoor scenes from outdoor scenes, but again this holds to alimited degree. A danger of accumulative features is theirinability to discriminate among different entities andsemantic meanings in the image. More work on semanticdriven groupings will increase the power of accumulativedescriptors to capture the content of the image.Structural descriptions match well with weak segmentation, salient regions, and weak semantics. One has to becertain that the structure is within one object and not anaccidental combination of patches which have no meaningin the object world. The same brittleness of strongsegmentation lurks here. We expect a sharp increase inthe research of local, partial, or fuzzy structural descriptorsfor the purpose of contentbased retrieval, especially ofbroad domains.5 INTERPRETATION AND SIMILARITYWhen the information from images is captured in a featureset, there are two possibilities for endowing them withmeaning One derives an unilateral interpretation from thefeature set, while the other one compares the feature setwith the elements in a given data set on the basis of asimilarity function, see Fig. 10.5.1 Semantic InterpretationIn contentbased retrieval, it is useful to push the semanticinterpretation of features derived from the image as far asone can.Semantic features aim at encoding interpretations of the imagewhich may be relevant to the application.Of course, such interpretations are a subset of the possibleinterpretations of an image. To that end, consider a featurevector F derived from an image i. For given semanticinterpretations z from the set of all interpretations Z, alearning phase leads to conditional probabilitiesP  P zjF 10A strong semantic feature with interpretation zj wouldgenerate a P zjF  z zj. If the feature carries nosemantics, it would generate a distribution P zjF  P zindependent of the value of the feature. In practice, manyfeature types will generate a probability distribution that isneither a pulse nor independent of the feature value. Thismeans that the feature value skews the interpretation ofthe image, but does not determine it completely.Under the umbrella weak semantics, we collect theapproaches that try to combine features in some semantically meaningful interpretation. Weak semantics aims atencoding, in a simple and approximate way, a subset of thepossible interpretations of an image that are of interest in agiven application. As an example, the system in 30 usescolor features derived from Ittens color theory to encodethe semantics associated to color contrast and harmony inart application.SMEULDERS ET AL. CONTENTBASED IMAGE RETRIEVAL AT THE END OF THE EARLY YEARS 1361Fig. 10. Data flow diagram of similarity and interpretation.In the MAVIS2system 84, data are considered at foursemantic levels, embodied in four layers called the rawmedia, the selection, the selection expression, and conceptual layers. Each layer encodes information at anincreasingly symbolic level. Agents are trained to createlinks between features, feature signatures at the selectionlayer, interrelated signatures at the selection expressionlayer, and concept expressed as textual labels at theconceptual layer. In addition to the vertical connections, thetwo top layers have intralayer connections that measure thesimilarity between concepts at that semantic level andcontribute to the determination of the similarity betweenelements at the lower semantic level.5.2 Similarity between FeaturesA different road to assigning a meaning to an observedfeature set, is to compare a pair of observations by asimilarity function. While searching for a query image iqxamong the elements of the data set of images, idx,knowledge of the domain will be expressed by formulatinga similarity measure Sqd between the images q and d on thebasis of some feature set. The similarity measure dependson the type of features, see Fig. 10.The similarity of two feature vectors F, accumulative orobject features alike, is given bySqd  sFqFd 11At its best use, the similarity measure can be manipulated torepresent different semantic contents images are thengrouped by similarity in such a way that close images aresimilar with respect to use and purpose. There is surprisingly little work dedicated to characterizing similaritymeasures. A few ideas, however, have emerged. A commonassumption is that the similarity between two featurevectors F can be expressed assFqFd  g  dFqFd 12where g is a positive, monotonically nonincreasing functionand d is a distance function on F . This assumption isconsistent with a class of psychological models of humansimilarity perception 154, 147 and requires that thefeature space be metric. If the feature space is a vectorspace, d often is a simple Euclidean distance, although thereis indication that more complex distance measures might benecessary 147. This similarity model was wellsuited forearly query by example systems in which images wereordered by similarity with one example.A different view sees similarity as an essentiallyprobabilistic concept. This view is rooted in the psychological literature 8 and, in the context of contentbasedretrieval, it has been proposed, for example, in 114. Ageneral form of such a similarity measure would besFqFd  fP Fq  Fd 13where  means that the two features describe images of thesame class and Fq  Fq   Fq are the real stimulus and noise due to sensory and measurement conditions.Measuring the distance between histograms has been anactive line of research since the early years of contentbasedretrieval, where histograms can be seen as a set of orderedfeaturessFqFd  g  dFqFd 14In contentbased retrieval, histograms have mostly beenused in conjunction with color features, but there is nothingagainst being used in texture or local geometric properties.Swain and Ballard 169 proposed the use of the intersectiondistance dFqFd Pnj1 minFqj Fdj , where Fq and Fdare two histograms containing n bins each. They alsoproved that if all images have the same number of pixels,i.e.,Pj Fj is the same for all images, then this distance hasthe same ordinal properties as the L1 distance. In 62, adifferent approach is followed. The distance between twohistograms is defined in vector form asdFqFd Fq  FdtFq  Fdqwhere the matrix  expresses the similarity between the jthand the kth bins. This has the advantage of considering thesimilarity between values in the feature space, i.e., ofincorporating the metric of the feature space into thesimilarity measure.Other commonly used distance functions for colorhistograms include the Minkowski distancesdrFqFd Xnj1jFqj  Fdj jr 1rThese measures do not take into account the similaritybetween different, but related bins of a histogram. In 166,it is observed that this may lead to false negatives. Thepaper proposes the use of cumulative histograms of theform Fqm Pmk0 Fqk after ordering the bins by parameterj. Comparisons between cumulative rather than plainhistograms show that the former tend to be more forgivingfor changes in the bin assignment due to noise. Analternative, also explored in the paper, is to describe thehistogram by the first three statistical moments, where 3 isan empirical finding. In 166, the histogram was applied tocolor images by representing colors in the HSV system andcomputing the moments of the channel separately, resultingin nine parameters, three moments for each of the threecolor channels. A recent distance measure for colorhistograms is found in 4, where a hue histogram and asaturation histogram are formed separately with theadvantages of saving on memory and the possibility ofexcluding colors from a query. The reference comparescolors on the basis of the angular distance in RGBspace.The natural measure to compare ordered sets ofaccumulative features is nonparametric test statistics. Theycan be applied to the distributions of the coefficients oftransforms to determine the likelihood that two samplesderive from the same distribution 35, 131. They can alsobe applied to compare the equality of two histograms andall variations thereof.1362 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 22, NO. 12, DECEMBER 20005.3 Similarity of Object SilhouettesIn 183, a good review is given of methods to compareshapes directly after segmentation into a set of objectpoints txSqd  stqx tdx 15without an intermediate description in terms of shapefeatures.For shape comparison, the authors make a distinctionbetween transforms, moments, deformation matching, scalespace matching, and dissimilarity measurement. Difficultiesfor shape matching based on global transforms are theinexplicability of the result and the brittleness for smalldeviations. Moments, specifically their invariant combinations, have been frequently used in retrieval 89. Matchinga query and an object in the data file can be done along theordered set of eigen shapes 150 or with elastic matching36, 11. Scale space matching is based on progressivelysimplifying the contour by smoothing 116. By comparingthe signature of annihilated zero crossings of the curvature,two shapes are matched in a scale and rotation invariantfashion. A discrete analogue can be found in 94, wherepoints are removed from the digitized contour on the basisof perceptually motivated rules. Results on a 2,000elementdatabase are reported to perform better than most of themethods listed above.When based on a metric, dissimilarity measures willrender an ordered range of deviations suited for apredictable interpretation. In 183, an analysis is given forthe Hausdorff and related metrics between two shapes onrobustness and computational complexity. The directedHausdorff metric is defined as the maximum distancebetween a point on query object q and its closest counterparton d. The partial Hausdorff metric, defined as the kthmaximum rather than the absolute maximum, is used in63 for affine invariant retrieval.5.4 Similarity of Structural FeaturesThe result of a structural description is a hierarchicallyordered set of feature values H, see Fig. 9. In this section, weconsider the similarity ofSqd  sHqHd 16between the two structural or layout descriptions.Many different techniques have been reported for thesimilarity of feature structures. In 191, 74, a Bayesianframework is developed for the matching of relationalattributed graphs by discrete relaxation. This is applied toline patterns from aerial photographs.A metric for the comparison of two topological arrangements of named parts, applied to medical images, is definedin 172. The distance is derived from the number of editsteps needed to nullify the difference in the Voronoidiagrams of two images.In 23, 2Dstrings describing spatial relationships between objects are discussed and, much later, reviewed in198. From such topological relationships of image regions,in 71, a 2Dindexing is built in trees of symbol strings, eachrepresenting the projection of a region on the coordinateaxis. The distance between Hq and Hd is the weighednumber of editing operations required to transform the onetree to the other. In 153, a graph is formed from the imageon the basis of symmetry as it appears from the medial axis.Similarity is assessed in two stages via graphbasedmatching followed by energydeformation matching.In 53, hierarchically ordered trees are compared for thepurpose of retrieval by rewriting them into strings. Adistancebased similarity measure establishes the similarityscores between corresponding leaves in the trees. At thelevel of trees, the total similarity score of correspondingbranches is taken as the measure for subtreesimilarity.From a small size experiment, it is concluded thathierarchically ordered feature sets are more efficient thanplain feature sets, with projected computational shortcutsfor larger data sets.In 163, images are transformed into homogeneousregions for retrieval based on color layout. The regionsare scanned, typically five equally spaced vertical scans,and converted into a string of symbols taken from avisual dictionary. The strings are summarized intoregionrelative histograms, Fi j, indicating how manytimes a symbol precedes another symbol in one of thescans. During querying, the similarity of q to d is givenbyPi FqFd1Pj FqFd, that is, the elementbyelementcorrespondence of the region ordered histograms.5.5 Similarity of Salient FeaturesSalient features are used to capture the information in theimage in a limited number of salient points. Similaritybetween images can then be checked in several differentways.In the first place, the color, texture, or local shapecharacteristics may be used to identify the salient points ofthe data as identical to the salient points of the query.Sqd  g  dFqFd 17where Fq and Fd are feature vectors of salient propertiesand g is an optional monotone function. A measure ofsimilarity between the feature values measured of theblobs resulting from weak segmentation consists of aMahalanobis distance between the feature vector composed of the color, texture, position, area, eccentricity,and direction of the two ellipses 20. If the features ofthe ellipse are collected in a vector F, the distancebetween q and d is given bydqd  Fq  FdT1Fq  Fdh i12where  is a diagonal weights matrix set by the user.The similarity between two blobs is defined asSqd  expdqd2.In the second place, one can store all salient points fromone image in a histogram on the basis of a few characteristics, such as color on the inside versus color on theoutside. The similarity is then based on the groupwisepresence of enough similar points 57.Sqd  g  dFqFd 18SMEULDERS ET AL. CONTENTBASED IMAGE RETRIEVAL AT THE END OF THE EARLY YEARS 1363where Fq and Fd are histograms merely indicating thepresence of salient points. The metric dFqFd is nowaiming at measuring the presence of the same set of salientpoints. Comparing sparsely occupied histograms has longbeen used in text retrieval, where vector space modeling143 implies the registration in a Ndimensionalhistogram F with as many dimensions as there are differentwords in the dictionary, typically 10,000. In a binary vectorspace, each dimension is expressing whether that word ispresent or absent in the text. A text is a point in this highdimensional space. Differences between the text d in thedata file and the query q boil down to the intersectiondistance discussed above distancedFqFd X1iNi FdiFqiover all dimensions. The same strategy is used whencomparing salient point features derived from differentimages. The intersection is appropriate when both q and dmay be partially occluded in the image or cluttered with thebackground. When q is neither cluttered or occluded but dmay still be, the intersection should be replaced by theoperation. The model has been used in image retrieval in158, while keeping access to their location in the image bybackprojection 169. Following the development of thevector space model in text retrieval, a weight per dimensionmay favor the appearance of some salient features over another. See also 69 for a comparison with correlograms.A third alternative for similarity of salient points is toconcentrate only on the spatial relationships among thesalient points sets Pq and PdSqd  g  dPqPd 19In pointbypointbased methods for shape comparison,shape similarity is studied in 83, where maximumcurvature points on the contour and the length betweenthem are used to characterize the object. To avoid theextensive computations, one can compute the algebraicinvariants of point sets, known as the crossratio. Due totheir invariant character, these measures tend to have only alimited discriminating power among different objects. Amore recent version for the similarity of nameless pointsetsis found in geometric hashing 192, where each tripletspans a base for the remaining points of the object. Anunknown object is compared on each triplet to see whetherenough similarly located points are found. Geometrichashing, though attractive in its concept, is too computationally expensive to be used on the very large data sets ofimage retrieval due to the anonymity of the points.Similarity of two points sets Pq and Pd given in a rowwise matrix is discussed in 188. A distance is given forsimilarity invariance by,D2PqPd  1 kPqPTd k2  2 detPqPTd kPdk2kPqk2and, for affine transformations,D2PqPd  2 trPd Pd Pq Pqwhere Pd and Pq are the pseudoinverse of Pd and Pq,respectively.5.6 Similarity at the Semantic LevelIn 70, knowledgebased type abstraction hierarchies areused to access image data based on context and a userprofile, generated automatically from cluster analysis of thedatabase. Also in 24, the aim is to create a very largeconceptspace inspired by the thesaurusbased search fromthe information retrieval community. In 115, a linguisticdescription of texture patch visual qualities is given andordered in a hierarchy of perceptual importance on thebasis of extensive psychological experimentation.A more general concept of similarity is needed forrelevance feedback, in which similarity with respect to anensemble of images is required. To that end, in 45, morecomplex relationships are presented between similarity anddistance functions defining a weighted measure of twosimpler similaritiesSs S1 S2  w1 expdS1 s  w2 expdS2 sThe purpose of the bireferential measure is to find allregions that are similar to two specified query points, anidea that generalizes to similarity queries given multipleexamples. The approach can be extended with the definition of a complete algebra of similarity measures withsuitable composition operators 45, 38. It is then possibleto define operators corresponding to the disjunction,conjunction, and negation of similarity measures, muchlike traditional databases. The algebra is useful for the userto manipulate the similarity directly as a means to expresscharacteristics in specific feature values.5.7 Learning an InterpretationAs data sets grow large and the available processing powermatches that growth, the opportunity arises to learn fromexperience. Rather than designing, implementing, andtesting an algorithm to detect the visual characteristics foreach different semantic term, it becomes possible to learnthe semantics of objects from their appearance.For a review on statistical pattern recognition, see 80. In182, a variety of techniques is discussed treating retrievalas a classification problem.One approach is principal component analysis over astack of images taken from the same class z of objects. Thiscan be done in feature space 118 or at the level of the entireimage, for example, faces in 113. The analysis yields a setof eigenface images, capturing the common characteristics of a face without the need of a geometric model.Effective ways to learn from partially labeled data haverecently been introduced in 194, 139, both using theprinciple of transduction 181. This saves the effort oflabeling the entire data set, unfeasible and unreliable as itgrows big.In 186, preliminary work is reported towards automaticdetection of categories on totally unlabeled data sets. Theyrepresent objects as probabilistic constellations of features.Recurring salient rigid parts are selected automatically bymaximization of the expectation.1364 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 22, NO. 12, DECEMBER 2000In 176, a very large number of precomputed features isconsidered, of which a small subset is selected by boosting80 to learn the image class.An interesting technique to bridge the gap betweentextual and pictorial descriptions to exploit information atthe level of documents is borrowed from informationretrieval, called latent semantic indexing 151, 199. First,a corpus is formed of documents in this case, images with acaption from which features are computed. Then, bysingular value decomposition, the dictionary covering thecaptions is correlated with the features derived from thepictures. The search is for hidden correlations of featuresand captions.5.8 Discussion on Interpretation and SimilarityWhenever the image itself permits an obvious interpretation, the ideal contentbased system should employ thatinformation. A strong semantic interpretation occurs whena sign can be positively identified in the image. This israrely the case due to the large variety of signs in a broadclass of images and the enormity of the task to define areliable detection algorithm for each of them. Weaksemantics rely on inexact categorization induced bysimilarity measures, preferably online by interaction. Thecategorization may agree with semantic concepts of theuser, but the agreement is, in general, imperfect. Therefore,the use of weak semantics is usually paired with the abilityto gear the semantics of the user to his or her needs byinterpretation. Tunable semantics is likely to receive moreattention in the future, especially when data sets grow big.Similarity is an interpretation of the image based on thedifference with another image. For each of the feature types,a different similarity measure is needed. For similaritybetween feature sets, special attention has gone to establishing similarity among histograms due to their computationalefficiency and retrieval effectiveness.Similarity of shape has received considerable attention inthe context of objectbased retrieval. Generally, global shapematching schemes break down when there is occlusion orclutter in the scene. Most global shape comparison methodsimplicitly require a frontal viewpoint against a clear enoughbackground to achieve a sufficiently precise segmentation.With the recent inclusion of perceptually robust points inthe shape of objects, an important step forward has beenmade.Similarity of hierarchically ordered descriptions deservesconsiderable attention as it is one mechanism to circumventthe problems with segmentation while maintaining some ofthe semantically meaningful relationships in the image. Partof the difficulty here is to provide matching of partialdisturbances in the hierarchical order and the influence ofsensorrelated variances in the description.Learning computational models for semantics is aninteresting and relatively new approach. It gains attentionquickly as the data sets and the machine power grow big.Learning opens up the possibility to an interpretation of theimage without designing and testing a detector for eachnew notion. One such approach is appearancebasedlearning of the common characteristics of stacks of imagesfrom the same class. Appearancebased learning is suitedfor narrow domains. For success of the learning approach,there is a tradeoff between standardizing the objects in thedata set and the size of the data set. The more standardizedthe data are the less data will be needed, but, on the otherhand, the less broadly applicable the result will be.Interesting approaches to derive semantic classes fromcaptions or a partially labeled or unlabeled data set havebeen presented recently, see above.6 INTERACTIONWe turn our attention to the interacting user. Interaction ofusers with a data set has been studied most thoroughly incategorical information retrieval 123. The techniquesreported there need rethinking when used for imageretrieval as the meaning of an image, due to the semanticgap, can only be defined in context. Image retrieval requiresactive participation of the user to a much higher degreethan required by categorized querying. In contentbasedimage retrieval, interaction is a complex interplay betweenthe user, the images, and their semantic interpretations.6.1 Query Space Definition and InitializationTo structure the description of methods, we first definequery space. The first component of query space is theselection of images IQ from the large image archive I .Typically, the choice is based on factual descriptions like thename of the archive, the owner, date of creation, or Web siteaddress. Any standard retrieval technique can be used forthe selection. The second component is a selection of thefeatures FQ  F derived from the images in IQ. In practice,the user is not always capable of selecting the features mostfit to reach the goal. For example, how should a general userdecide between shape description using moments or Fourier coefficients Under all circumstances, however, the usershould be capable of indicating the class of features relevantfor the task, like shape, texture, or both. In addition tofeature class, 57 has the user indicate the requestedinvariance. The user can, for example, specify an interest infeatures robust against varying viewpoint, while theexpected illumination is specified as white light in all cases.The appropriate features can then be automatically selectedby the system. As concerns the third component of queryspace, the user should also select a similarity function, SQ.To adapt to different data sets and goals, SQ should be aparameterized function. Commonly, the parameters areweights for the different features. The fourth component ofquery space is a set of labels ZQ  Z to capture goaldependent semantics. Given the above, we define anabstract query spaceThe query space Q is the goal dependent 4tuplefIQFQSQZQg.To start a query session, an instantiationQ  fIQ FQ SQ ZQgof the abstract query space is created. When no knowledgeabout past or anticipated use of the system is available, theinitial query space Q0 should not be biased toward specificSMEULDERS ET AL. CONTENTBASED IMAGE RETRIEVAL AT THE END OF THE EARLY YEARS 1365images or make some image pairs a priori more similar thanothers. The active set of images IQ is therefore equal to all ofIQ. Furthermore, the features of FQ are normalized based onthe distribution of the feature values over IQ e.g., 48, 142.To make SQ unbiased over FQ, the parameters should betuned, arriving at a natural distance measure. Such a measurecan be obtained by normalization of the similarity betweenindividual features to a fixed range 184, 142. For theinstantiation of a semantic label, the semantic gap preventsattachment to an image with full certainty. Therefore, in theideal case, the instantiation ZQ of ZQ assigns, for each i 2 IQand each z 2 ZQ, a probability Piz, rather than a strict label.The query space forms the basis for specifying queries,display of query results, and for interaction.6.2 Query SpecificationFor specifying a query q in Q, many different interactionmethodologies have been proposed. A query falls in one oftwo major categories exact query, where the query answerset Aq equals the images in IQ, satisfying a set of givencriteria, and an approximate query, where Aq is a ranking ofthe images in IQ with respect to the query, based on SQ.Within each of the two categories, three subclasses can bedefined depending on whether the query relates to thespatial content of the image, to the global image information, or to groups of images. An overview of the initialization and specification of queries is shown in Fig. 11.For exact queries, the three subclasses are based ondifferent predicates the result should satisfy. Exact query by spatial predicate is based on the locationof silhouettes, homogeneous regions, or signs. Queryon silhouette location is applicable in narrowdomains only. Typically, the user queries using aninterpretation z 2 ZQ. To answer the query, thesystem then selects an appropriate algorithm forsegmenting the image and extracting the domaindependent features. In 156, the user interactivelyindicates semantically salient regions to provide astarting point. The user also provides sufficientcontext to derive a measure for the probability of z.Implicit spatial relations between regions sketchedby the user in 163 yield a pictorial predicate. Othersystems let the user explicitly define the predicate onrelations between homogeneous regions 20. In bothcases, to be added to the query result, the homogenous regions as extracted from the image mustcomply with the predicate. A Web search system inwhich the user places icons representing categorieslike human, sky, and water in the requested spatialorder is presented in 97. In 144, users posespatialpredicate queries on geographical signslocated in maps based on their absolute or relativepositions.. Exact query by image predicate is a specification ofpredicates on global image descriptions, often in theform of range predicates. Due to the semantic gap,range predicates on features are seldom used in adirect way. In 120, ranges on color values are predefined in predicates like MostlyBlue and SomeYellow. Learning from user annotations of apartitioning of the image allows for feature rangequeries like amount of sky  50 percent andamount of sand  30 percent 130.. Exact query by group predicate is a query using anelement z 2 ZQ, where ZQ is a set of categories thatpartitions IQ. Both in 21 and 179, the user querieson a hierarchical taxonomy of categories. Thedifference is that the categories are based oncontextual information in 21 while they are interpretations of the content in 179.1366 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 22, NO. 12, DECEMBER 2000Fig. 11. Example queries for each of the six different query types and possible results from the Corel image database.In the approximate types of query specifications, the userspecifies a single feature vector or one particular spatialconfiguration in FQ, where it is anticipated that no imagewill satisfy the query exactly.. Approximate query by spatial example results in animage or spatial structure corresponding to literalimage values and their spatial relationships. Pictorialspecification of a spatial example requires a featurespace such that feature values can be selected orsketched by the user. Lowlevel feature selectors usecolor pickers or selections from shape and textureexamples 48, 61. Kato et al. 88 were the first tolet users create a sketch of the global imagecomposition which was then matched to the edgesin IQ. Sketched outlines of objects in 93 are firstnormalized to remove irrelevant detail from thequery object before matching it to objects segmentedfrom the image. When specification is by parameterized template 36, 150, each image in IQ isprocessed to find the best match with edges of theimages. The segmentation result is improved if theuser may annotate the template with salient detailslike color corners and specific textures. Preidentification of all salient details in images in IQ can thenbe employed to speed up the search process 161.When weak segmentation of the query image and allimages in IQ is performed, the user can specify thequery by indicating example regions 20, 163.. Approximate query by image example feeds the system acomplete array of pixels and queries for the mostsimilar images, in effect asking for the knearestneighbors in feature space. Most of the currentsystems have relied upon this form of querying 48,61. The general approach is to use an SQ based onglobal image features. Query by example queries aresubclassified 184 into query by external imageexample, if the query image is not in the database,versus query by internal image example. The differencein external and internal example is minor for theuser, but affects the computational support as, forinternal examples, all relations between images canbe precomputed. Query by image example is suitedfor applications where the target is an image of thesame object or set of objects under different viewingconditions 57. In other cases, the use of one imagecannot provide sufficient context for the query toselect one of its many interpretations 146.. Approximate image query by group example is specification through a selection of images which ensembledefines the goal. The rationale is to put the image inits proper semantic context to make one of thepossible interpretations z 2 ZQ preponderant. Oneoption is that the user selects m  1 images from apalette of images presented to find images bestmatching the common characteristics of them images 31. An mquery set is capable of definingthe target more precisely. At the same time, themquery set defines relevant feature value variationsand nullifies irrelevant variations in the query.Group properties are amplified further by addingnegative examples. This is achieved in 10 byconstructing a query q best describing positive andnegative examples indicated by the user. When, foreach group in the database, a small set ofrepresentative images can be found, they are storedin a visual dictionary from which the user creates thequery 146.Of course, the above queries can always becombined into more complex queries. For example,both 20, 163 compare the similarity of regionsusing features. In addition, they encode spatialrelations between the regions in predicates. Evenwith such complex queries, a single query q is rarelysufficient to make Aq the user desired answer set.For most image queries, the user must engage inactive interaction with the system on the basis of thequery results as displayed.6.3 Query Space DisplayThere are several ways to display the query result to theuser. In addition, system feedback can be given to help theuser in understanding the result. We defineThe visualization operator V maps the query space Q into thedisplay space D having perceived dimension d.Note that d is the intrinsic dimensionality of the query resultor d is induced by the projection function in V if the queryresult is of too high a dimension to visualize directly. Inboth cases, d is not necessarily equal to the two dimensionsof the screen, so an additional projection operator might berequired to map D onto the screen.When the query is exact, the result of the query is a set ofimages fulfilling the predicate. As an image either fulfillsthe predicate or not, there is no intrinsic order in the queryresult and d  0 is sufficient.For approximate queries, the images in IQ are given asimilarity ranking based on SQ with respect to the query. Inmany systems, the role of V is limited to bounding thenumber of images displayed, which are then displayed in a2D rectangular grid 48, 21. Note, however, that weshould have d  1. If the user refines its query using queryby example, the images displayed do not have to be theimages closest to the query. In 184, images are selectedthat together provide a representative overview of thewhole active database. An alternative display model displays the image set minimizing the expected number oftotal iterations 31.The space spanned by the features in FQ is a highdimensional space. When images are described by featurevectors, every image has an associated position in thisspace. In 146, 175, 68, the operator V maps the highdimensional feature space onto a display space with d  3.Images are placed in such a way that distances betweenimages in D reflect SQ. A simulated fisheye lens is used toinduce perception of depth in 175. In the reference, the setof images to display depends on how well the userselections conform to selections made in the communityof users. To improve the users comprehension of theinformation space, 68 provides the user with a dynamicview on FQ through continuous variation of the activefeature set. The display in 86 combines exact andSMEULDERS ET AL. CONTENTBASED IMAGE RETRIEVAL AT THE END OF THE EARLY YEARS 1367approximate query results. First, the images in IQ areorganized in 2Dlayers according to labels in ZQ. Then, ineach layer, images are positioned based on SQ.In exact queries based on accumulative features, backprojection 169, 41 can be used to give system feedback,indicating which parts of the image fulfill the criteria. Forexample, in 130, each tile in the partition of the imageshows the semantic label, like sky, building, or grass, the tilereceived. For approximate queries, in addition to mere rankordering, in 20, system feedback is given by highlightingthe subparts of the images contributing most to the rankingresult.6.4 Interacting with Query SpaceIn early systems, the process of query specification anddisplay is iterated, where, in each step, the user revises thequery. Updating the query often is still appropriate for exactqueries. For approximate queries, however, the interactivesession should be considered in its entirety. During thesession, the system updates the query space, attempting tolearn the goals from the users feedback. As for the user, theresult is what is visualized in display space, we defineAn interactive query session is a sequence of query spacesfQ0 Q1  Qn1 Qng with Anq  V Qn.In a truly successful session, Anq is the users search goal.The interaction process is schematically indicated in Fig. 12.The interaction of the user yields a relevance feedback RFiin every iteration i of the session. The transition from Qi toQi1 materializes the feedback of the user. For target search,category search, or associative search, various ways of userfeedback have been considered. All are balancing betweenobtaining as much information from the user as possibleand keeping the burden on the user minimal. The simplestform of feedback is to indicate which images are relevant31. In 28, 110, the user in addition explicitly indicatesnonrelevant images. The system in 142 considers fivelevels of significance, which gives more information to thesystem, but makes the process more difficult for the user.When d  2, the user can manipulate the projecteddistances between images, putting away nonrelevantimages and bringing relevant images closer to each other146. The user can also explicitly bring in semanticinformation by annotating individual images, groups ofimages, or regions inside images 111 with a semantic label.In general, user feedback leads to an update of queryspacefIiQ F iQ SiQ ZiQg RFi fIi1Q  F i1Q  Si1Q  Zi1Q g 20Different ways of updating Q are open. In 184, theimages displayed correspond to a partitioning of IQ. Byselecting an image, one of the sets in the partition is selectedand the set IQ is reduced. Thus, the user zooms in on a targetor a category.The methods follows the patternIiQ RFiIi1Q  21In current systems, the feature vectors in FQ corresponding to images in IQ are assumed fixed. This has greatadvantages in terms of efficiency. When features areparameterized, however, feedback from the user could leadto optimization of the parameters. For example, in parameterized detection of objects based on salient contourdetails, the user can manipulate the segmentation result tohave the system select a more appropriate salient detailbased on the image evidence 161. The general pattern isFiQ RFiF i1Q  22For associative search, users typically interact to learn thesystem the right associations. Hence, the system updates thesimilarity functionSiQ RFiSi1Q  23In 28, 142, SQ is parameterized by a weight vector onthe distances between individual features. The weights in28 are updated by comparing the variance of a feature inthe set of positive examples to the variance in the union ofpositive and negative examples. If the variance for thepositive examples is significantly smaller, it is likely that thefeature is important to the user. The system in 142 firstupdates the weight of different feature classes. The rankingof images according to the overall similarity function iscompared to the rankings corresponding to each individualfeature class. Both positive and negative examples are usedto compute the weight of the feature, computed as theinverse of the variance over the positive examples. Thefeedback RFi in 146 leads to an update of the userdesireddistances between pairs of images in IQ. The parameters ofthe continuous similarity function should be updated tomatch the new distances. A regularization term is introduced, limiting the deviation from the initial naturaldistance function.The final set of methods follow the patternZiQ RFiZi1Q  24The system in 111 precomputes a hierarchical groupingof partitionings or images for that matter based on thesimilarity for each individual feature. The feedback fromthe user is employed to create compound groupings1368 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 22, NO. 12, DECEMBER 2000Fig. 12. The framework for interaction in contentbased image retrieval.corresponding to a user given z 2 ZQ. The compoundgroupings are such that they include all of the positive andnone of the negative examples. Unlabeled images in thecompound group receive label z. The update of probabilities P is based on different partitionings of IQ. For categoryand target search, a system may refine the likelihood of aparticular interpretation, updating the label based onfeature values or on similarity values. The method in110 falls in this class. It considers category search, where ZQis relevant, nonrelevant. In the limit case for only onerelevant image, the method boils down to target search. Allimages indicated by the user as relevant or nonrelevant incurrent or previous iterations are collected and a Parzenestimator is constructed incrementally to find optimalseparation between the two classes. The generic pattern,which uses similarity in updating probabilities, is the formused in 31 for target search with ZQ  target. In thereference, an elaborate Bayesian framework is derived tocompute the likelihood of any image in the database beingthe target, given the history of actions RFi. In each iteration,the user selects examples from the set of images displayed.Image pairs are formed by taking one selected and onedisplayed, but nonselected, image. The probability of beingthe target for an image in IQ is increased or decreaseddepending on the similarity to the selected and thenonselected example in the pair.6.5 Discussion on InteractionAny information the user can provide in the search processshould be employed to provide the rich context required inestablishing the meaning of a picture. The interactionshould form an integral component in any modern imageretrieval system, rather than a last resort when theautomatic methods fail. Already, at the start, interactioncan play an important role. Most of the current systemsperform query space initialization irrespective of whether atarget search, a category search, or an associative search isrequested. But, the fact of the matter is that the set ofappropriate features and the similarity function depend onthe user goal. Asking the user for the required invarianceyields a solution for a specific form of target search. Forcategory search and associative search, the userdriveninitialization of query space is still an open issue.For image retrieval, we have identified six query classes,formed by the Cartesian product of the result type exact,approximate and the level of granularity of the descriptions spatial content, image, image groups. The queriesbased on spatial content require segmentation of the image.For large data sets, such queries are only feasible whensome form of weak segmentation can be applied to allimages or when signs are selected from a predefinedlegend. A balance has to be found between flexibility on theuser side and scalability on the system side. Query by imageexample has been researched most thoroughly, but a singleimage is only suited when another image of the sameobjects is the aim of the search. In other cases, there issimply not sufficient context. Queries based on groups, aswell as techniques for prior identification of groups in datasets, are promising lines of research. Such groupbasedapproaches have the potential to partially bridge thesemantic gap while leaving room for efficient solutions.The query result has an inherent display dimensionwhich is often ignored. Most methods simply displayimages in a 2D grid. Enhancing the visualization of thequery result is, however, a valuable tool in helping the usernavigating query space. As apparent from the query spaceframework, there is an abundance of information availablefor display. New visualization tools are urged to allow foruser and goaldependent choices on what to display.Through manipulation of the visualized result, the usergives feedback to the system. The interaction patterns asenumerated here reveal that, in current methods, feedbackleads to an update of just one of the components of queryspace. There is no inherent reason why this should be thecase. In fact, joint updates could indeed be effective andwell worth researching. For example, the pattern whichupdates category membership based on a dynamic similarity function would combine the advantages of browsingwith category and target search.One final word about the impact of interactivity on thearchitecture of the system. The interacting user brings aboutmany new challenges for the response time of the system.Contentbased image retrieval is only scalable to large datasets when the database is able to anticipate what interactivequeries will be made. A frequent assumption is that theimage set, the features, and the similarity function areknown in advance. In a truly interactive session, theassumptions are no longer valid. A change from static todynamic indexing is required.7 SYSTEM ASPECTS7.1 Storage and IndexingWe have been concerned with the content of the imageeventually leading to a feature vector F or a hierarchicallyordered set H containing the information of the image.Repetition over all images in the data set yields a file offeature vectors, the data file. In the previous section, wediscussed the request as translated into the query imagevector, Fq, to be compared with the elements Fd of data fileon the basis of the similarity function.Scientifically and practically, the most interesting applications of retrieval are on large data sets, where there isstatistically sound coverage of the image spectrum andlearning general laws from the data sets makes sense. Forlarge image sets, computational performance cannot beignored as an issue. When storing the feature vectors in astandard, linear file with one record to each feature vector,we are bound to scan through all feature vectors. In thatcase, we have to perform N fetches of a record plussubsequent calculations to find the data vector most similarto the query feature vector. The response time of the systemis ON and, so, it is the number of operations wheninserting a new element in the database and updating themutual distance matrix among the elements. Linear scanning the feature vector file puts interactive response timesout of reach, most certainly for data sets of 10,000 imagesand more.In addition to the number of images, the dimension ofthe image vector can also be considerable. In 176, over10,000 features are computed from the image eachSMEULDERS ET AL. CONTENTBASED IMAGE RETRIEVAL AT THE END OF THE EARLY YEARS 1369describing a local pattern. In the example of a wavelethistogram for texturebased retrieval 162, an image hasa ninedimensional vector for each pixel compressed to a512bin histogram to a total of 5122 histograms of 512bins per image. The shape indexing technique 153represents an image vector by a hierarchically orderedset of six types of nodes and three types of links, eachencoding a number of image descriptors. Indexing in highdimensional spaces is difficult by the curse of dimensionality, a phenomenon by which indexing techniquesbecome inefficient as the dimensionality of the featurespace grows. The performance of Rtrees degrades by afactor of 12 as the number of dimensions increases from 5to 10 190.We focus on three classes of indexing methods that are inuse on large image databases, substantiated by performancefigures space partitioning, data partitioning, and distancebasedtechniques. In spacepartitioning index techniques, thefeature space is organized like a tree as discussed in 15.A node in this tree stands for a region in this space. Whenthe number of points in a region exceeds a prescribedamount, the region is split into subregions which becomethe children of the node containing the original region. Thebest known index in this class is the k d tree. A k d treeis a data structure that generalizes binary trees ink dimensions, hence the name. It splits an overfilled nodeof the tree along its k dimensions, an splits the data pointsin the node along the median of the data values 7. In theimplementation of the reference, there is an approximationbound . In 152, a 15fold decrease in response time isreported using the k d tree for a 20nearest neighborsquery over N  500,000 images with M  78 dimensions infeature space and   01. A further improvement is thek d Btree as a multidimensional generalization ofstandard Btree with splitting capacity of the k d tree. Itis a balanced data structure with equal depth from the root,with OlogN performance 196.Data partitioning index techniques associate, with eachpoint in feature space, a region that represents theneighborhood of that vector. An Rtree is such a datapartitioning structure to index hyperrectangular regions inMdimensional space. The leaf nodes of an Rtree representthe minimum bounding rectangles of sets of feature vectors.An internal node is a rectangle encompassing the rectanglesof all its children. An Rtree is a variant which does notallow the minimum bounding rectangles in a node tooverlap. In Rtree, the minimum bounding rectangles mayoverlap. The ordinary Rtree, family has not been verysuccessful for vectors with dimension somewhere over 10.The VAMsplit Rtree splits along the dimension ofmaximum variance 190, hence the name, and was shownto have a better performance than standard Rtree. Sincethe splitting criterion is dependent on the spread of the datarather than the number of data in a bucket, it proves to bevery effective, even for clustered data and for correlateddimensions. A VAMsplit Rtree can be constructed twoorders of magnitude faster than an Rtree, taking less thana second for a 20nearestneighbors query for M  11 on anN  100,000 image database. The SStree 189 and itsfurther improvement, the SRtree 87, use the intersectionof the minimum bounding hypersphere and minimumbounding hyperrectangle as the bounding region of a dataelement. As the dimension grows, it combines, for onebounding region, the advantage of the small volume of thehyperrectangle with the small diameter of the hypersphere.It has been shown that the SRtree is efficient in low andhigh feature vector sizes.Distancebased index structures are examplebasedspacepartitioning techniques, and, hence, very suited forquery by example when feature space is metric. Theprimary idea is to pick an example point and divide therest of the feature space into M 0 groups in concentric ringsaround the example. This results in a distancebased index,the vantage point tree first proposed in 195. In 14, theVPtree was generalized for high dimensional featurevectors. The MVPtree is a static data structure that usesmultiple example vantage points at each node. The firstvantage point is used to create M 0 partitions. And in eachpartition, a second vantage point creates M 0 more divisions.MVPtrees, with M 0  3, Mmaxpernode  13, using the L2metric found to perform fewer distance calculations duringvector comparison for a nearestneighbor search thancompeting techniques. The Mtree proposed in 27 is amore robust and scalable indexing strategy that uses thetriangleinequality of metric spaces, but, at the same time,retains the data partitioning properties of spatial accessmethods such as the Rtree and the SStree. Mtrees aresimilar to MVPtrees in the sense that they both split thespace based on spherical volumes around some referencevectors. However, Mtrees are locally adapting, balancedindexes. Hence, they are less affected by the distribution ofvectors in feature space.Although the Mtree cannot guarantee worst caseperformance, it has been shown to work well in practice,with distance computation costs of OlogM in most cases.It also has the advantage of dealing directly with featuresthat can be represented in a metric space but not in a vectorspace, unlike techniques like FastMap 46 or multidimensional scaling which approximate the feature space with avector space that maintains approximately the same metric.It is regrettable that not too much work across the divisionbetween the disciplines of vision and databases has beendone yet, with a few exceptions in commercial systems 48,61 and research 46, 85, 171. Recent work from the visionside is found in 170, where the database organizes itself fornarrow domains in clustering hierarchies of the mostexpressive features, and in 2, where clusters of featurevalues are sought in a graphtheoretical representation.7.2 System ArchitecturesMany systems like Photobook 128, PictoSeek 57, and116 are rooted in computer vision. In such systems, thedata and features are typically stored in files addressed byname. From an architectural point of view, this approach islikely to run into data integrity and performance problemswhen trying to scale up to a large database and a largenumber of users.The large number of elements is clearly an issue in 21,40, 168 and any other of the numerous Web searchengines, where the emphasis is on filling the database usingthe World Wide Web as a logical repository. Architectural1370 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 22, NO. 12, DECEMBER 2000issues focus on modules for searching the Web. In sucharchitectures, a clear distinction can be made between offline indexing and online readout for retrieval. The separation simplifies database integrity greatly. Hence, withsimple, static index structures one can obtain a goodperformance during retrieval.One step beyond the readonly databases is the use of astandard database management system extended for imageretrieval. Groundbreaking examples are QBIC 48 andVirage 61. In 120, an extended relational database systemwas used. Reducing image retrieval as a plugin module inan existing database solves the integrity problem for imagecontent and allows dynamic updates. It also providesnatural integration with features derived from othersources. Standard databases maintain a narrow dataexchange channel between the search engine and the dataand, hence, performance is rather poor. In the references,visualization and knowledge management is not beingaddressed as part of the integrated system.For more complete systems, detailed architectures are tobe considered. One of the early contributions to do so can befound in the COREsystem 193, which has been the basisfor many different applications. The architecture is centeredaround a general Data Base Management System on top ofwhich modules for analysis, indexing, training, andretrieval have been resolved in the database parlance. Insuch an approach, the database structure dominates knowledge management, feature calculation, and visualizationtools, which severely hampers ease of expression in theseareas.The generic architecture described in 60 is based on adetailed model of the various information types. Eachinformation type holds its data in a separate repositoryrather than the unified database, specifically separatingdatadriven and semantic, information bearing features.The advantage is that processing units, one for each newdata type, can be put together quickly. The drawback is thatthe distinction between informationbearing features anddatadriven features varies with the level of knowledge inthe system and, hence, will be obstructing knowledgebasedanalysis.The Infoscopes architecture 82 follows a similardistinction of features. It adds a new dimension to thearchitecture of contentbased systems by making explicitthe knowledge for the various parts of the system. ThePiction system 155 proposes an architecture for collaborative use of image information and related textual information, while making knowledge explicit.The systems discussed so far put their main emphasis ondata and, later, on knowledge processing. In contrast,systems reviewed in 141 and, particularly, the MARSsystem 142, are based on the information retrievalparadigm. The interaction with the user is consideredcrucial for a successful system. They propose an architecture for future systems which has a sequential processingstructure from features to user interaction. This architectureignores the role of data organization and explicit domainknowledge.Integration of database research and visualization 96has brought about techniques for visualizing the structureand content of an image database. Contentbased similaritybetween images is not exploited here. The systems in 68,184 have pursued integration furthest by using contentbased similarity, interaction, and visualization, as well asdatabase techniques for retrieving relevant images. Hence,they use techniques from three of the research fieldsidentified earlier. The El Nin o database system 146proposes an architecture for the integration of several,possibly remote, engines through a mediator. The interaction is viewed as an outside source of knowledge infusionsemantics are expected to emerge in the course of theinteraction. The addition of a knowledge component wouldlead to truly integrated contentbased image retrievalsystems.7.3 System EvaluationThe evaluation of image retrieval is a difficult yet essentialtopic for the successful deployment of systems and theirusefulness in practical applications. The initial impetus forthe evaluation of image databases comes from the neighboring discipline of information retrieval, in which userbased evaluation techniques have reached a considerabledegree of sophistication 143. The main tool that imageretrieval research borrowed from information retrieval arethe precision and recall measures. Suppose a data set D anda query q are given. Through the use of human subjects, thedata set can be divided into two sets the set of imagesrelevant for the query q, Rq and its complement, the set ofirrelevant images Rq. Suppose that the query q is given toa data set and that it returns a set of images Aq as theanswer. The precision of the answer is the fraction of thereturned images that is indeed relevant for the queryp  jAq RqjjAqj  25while the recall is the fraction of relevant images that isreturned by the queryr  jAq RqjjRqj 26While precision and recall are a useful tool in information retrieval, in the context of image databases, they are notsufficient for two reasons. First, the selection of a relevantset in an image database is much more problematic than ina text database because of the more problematic definitionof the meaning of an image. In the case of a document, thehuman judgment of whether a certain document is relevantto a certain query is relatively stable and there is a strongconnection between this judgment and the statisticalcharacteristics of the presence of certain elementary unitswords. In the case of an image, relevance is much lessstable because of the larger number of interpretation, of aimage separated from a linguistic context. Moreover, noanalysis in terms of semiotically stable constitutive elementscan be done therefore, the correlation between imagerelevance and lowlevel feature is much more vague.The second reason is that image databases usually do notreturn an undifferentiated set of relevant results, but aranked list of results or some more complex configurationthat shows the relation between the results of the query.SMEULDERS ET AL. CONTENTBASED IMAGE RETRIEVAL AT THE END OF THE EARLY YEARS 1371Although a query result is, in principle, an ordering of thewhole database, the size of the answer set Aq is usuallyreduced to k most relevant images. When the number ofrelevant images is greater than k, recall is meaningless as ameasure of the quality of the database.In spite of these drawbacks, precision and recall or othermeasures derived from them are useful measurements inspecial circumstances. In particular, when the imagedatabase relies on the strong semantics provided by labelor other textual description, precision and recall can beusefully employed 164.In 119, the problem was considered of measuring theperformance of a database without using the notion ofrelevant set. They assumed that an ideal database wouldtake a query q and provide an ideal ordering of thedatabase Z  z1 . . .  zk. The ideal database will alsoprovide a relevance measure for each image, SIj 2 0 1.The database under test, on the other end, will order theimage, given the same query q, as ZQ  z1  . . .  zk , where1 . . .  k is a permutation of 1 . . .  k. The displacementof the image Ij between the two orderings is given byjj jj, and all the displacements can be added andweighted by the relevance of the respective imagesobtaining the weighed displacement XjSIjjj jj 27The weighed displacement gives a way of comparing theoutputs of two databases, but this leaves open the problemof obtaining the ideal ordering Z. In most cases, suchordering is obtained by performing experiments withhuman subjects.With respect to experimental practices that use humansubjects, a distinction can be made between the evaluationof a complete system and that of parts of a system 145. Inthe first case, the system can be evaluated in the context of awelldefined activity by measuring the increased effectiveness resulting from the introduction of the database. Wellknown techniques from social sciences can be used for theexperimental design 19 and for the statistical analysis ofthe data 104. In the second case, human subjects should beused to obtain the ground truth. Such an approach isfollowed, for instance, in 12, 124.7.4 Discussion on System AspectsAs concerns system architecture, we maintain that a fullgrown contentbased retrieval system will result from theintegration of a sensory and feature calculating part, adomain knowledge and interpretation module, an interaction and user interface module, and a storage and indexingmodule. For the system architectures discussed above, weconclude that most systems have an innovative emphasisunderstandably limited to one or two of these components.We feel there is a need for a framework for contentbasedimage retrieval providing a more balanced view of the fourconstituent components. The framework would be based onexplicit communication protocols to permit a disciplinespecific parlance within each of the modules, see Fig. 13.Such a framework follows the lines of objectorientedmodular design, task differentiation, class abstractions,data hiding, and a communication protocol as CORBA.8 CONCLUDING REMARKSAt the end of this review, we would like to present our viewon a few trends1. The driving force. Contentbased image retrieval camearound quickly. Suddenly it was there, like the neweconomy. And it moves fast. In our review, most ofthe journal contributions are from the last five years.We are aware of the fact that much of what we have1372 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 22, NO. 12, DECEMBER 2000Fig. 13. The proposed framework for contentbased image retrieval.said here will be outdated soon, and hopefully so,but we hope we laid down patterns in computationand interaction which may last a little longer.The impetus behind contentbased image retrieval is given by the wide availability of digitalsensors, the Internet, and the falling price of storagedevices. Given the magnitude of these drivingforces, it is to us that contentbased retrieval willcontinue to grow in every direction new audiences,new purposes, new styles of use, new modes ofinteraction, larger data sets, and new methods tosolve the problems.What is needed most to bring the early years ofcontentbased retrieval to an end is more precisefoundations. For some of the reviewed papers, it wasnot clear what problem they were trying to solve orwhether the proposed method would perform betterthan an alternative. A classification of usagetypes,aims, and purposes would be very helpful here,including hard criteria for distinguishing amongdomain types. In spite of the difficulties intrinsic tothe early years, it is now clear that contentbasedretrieval is not just old wine in new sacks. It willdemand its own view of things as it is our belief thatcontentbased retrieval in the end will not be part ofthe field of computer vision alone. The manmachineinterface, domain knowledge, and database technology each will have their impact on the product.2. The heritage of computer vision. An important obstacleto overcome before contentbased image retrievalcould take off was to realize that image retrievaldoes not entail solving the general image understanding problem. It may be sufficient that aretrieval system present similar images, similar insome userdefined sense. Strong segmentation of thescene and complete feature descriptions may not benecessary at all to achieve the similarity ranking. Ofcourse, the deeper one goes into the semantics of thepictures, the deeper the understanding of the picturewill have to be, but that could very well be based oncategorizing pictures rather than on a preciseunderstanding.We discussed applications of contentbased retrieval in three broad types target search, categorysearch, and search by association. These user aimsare rooted in the research tradition of the field.Target search builds on pattern matching andobjectrecognition. New challenges in contentbasedretrieval are the huge amount of objects amongwhich to search, the incompleteness of the queryspecification and of the image descriptions, and thevariability of sensing conditions and object states.Category search builds on object recognitionand statistical pattern recognition problems. Newchallenges in contentbased retrieval compared tothe achievements of object recognition are theinteractive manipulation of results, the usuallyvery large number of classes, and the absence ofan explicit training phase for feature selection andclassification tuning.In the search by association, the goal is unspecified at the start of the session. Here, the heritage ofcomputer vision is limited to feature sets andsimilarity functions. The association process isessentially iterative, interactive, and explicative.Therefore, association search is hampered most bythe semantic gap. All display and relevance feedback has to be understood by the user, so theemphasis must be on developing features transparent to the user.3. The influence on computer vision. In reverse, contentbased image retrieval offers a different look attraditional computer vision problems.In the first place, contentbased retrieval hasbrought large data sets. Where the number of testimages in a typical journal paper was well under ahundred until very recently, a stateoftheart paperin contentbased retrieval reports experiments onthousands of images. Of course, the purpose isdifferent for computer vision and contentbasedretrieval. It is much easier to compose a generaldata set of arbitrary images rather than the specificones needed in a computer vision application, butthe stage has been set for more robustness. For onething, to process a thousand images at leastdemands software and computational method berobust.In the second place, contentbased retrieval hasrun into the absence of a general method for strongsegmentation. Especially for broad domains and forsensory conditions where clutter and occlusion areto be expected, strong segmentation into objects ishard, if not impossible. Contentbased retrievalsystems have dealt with the segmentation bottleneckin a few creative ways. First, a weaker version ofsegmentation has been introduced in contentbasedretrieval. In weak segmentation, the result is ahomogeneous region by some criterion, but notnecessarily covering the complete object silhouette.Weak segmentation leads to the calculation of salientfeatures capturing the essential information of theobject in a nutshell. The extreme form of the weaksegmentation is the selection of a salient point set asthe ultimately efficient data reduction in the representation of an object, very much like the focusofattention algorithms for an earlier age. Weaksegmentation and salient features are a typicalinnovation of contentbased retrieval. It is expectedthat salience will receive much attention in thefurther expansion of the field, especially whencomputational considerations will gain in importance. The alternative to work around strongsegmentation is to do no segmentation at all. Globalfeatures, such as wavelets and histograms, havebeen very effective. When the image is recordedwith a photographic purpose, it is likely that thecenter of the image means something different thanthe surrounding parts of the image, so using thatdivision of the picture could be of help too. Using nosegmentation at all is likely to run dry on semanticsSMEULDERS ET AL. CONTENTBASED IMAGE RETRIEVAL AT THE END OF THE EARLY YEARS 1373at the point where characteristics of the target are notspecific enough in large databases to discriminateagainst the features of all other images.In the third place, contentbased retrieval hasrevitalized interest in color image processing. This isdue to superior identification of trivalued intensitiesin identifying an object, as well as to the importanceof color in the perception of images. As contentbased is useroriented, color cannot be left out. Thepurpose of most image color processing here is toreduce the influence of accidental conditions of thescene and the sensing i.e., the sensory gap bycomputing sensing and scene invariant representations. Progress has been made in tailored color spacerepresentation for welldescribed classes of variantconditions. Also, the application of local geometricaldescriptions derived from scale space theory willreveal viewpoint and scene independent salientpoint sets, thus opening the way to similarity ofimages on a small number of most informativeregions or points.Finally, attention for invariance has been revitalized as well with many new features and similaritymeasures. For contentbased retrieval, invariance isjust one side of the coin, where discriminating poweris the other. Little work has been reported so far toestablish the remaining discriminating power ofproperties. This is essential as the balance betweenstability against variations and retained discriminating power determines the effectiveness of a property.4. Similarity and learning. Similarity is an interpretationof the image based on the difference between twoelements or groups of elements. For each of thefeature types, a different similarity measure isneeded. For similarity between feature sets, specialattention has gone to establishing similarity betweenhistograms due to their computational efficiency andretrieval effectiveness. Where most attention hasgone to color histograms, it is expected thathistograms of local geometric properties and texturewill follow. Being such a unique computationalconcept, the histogram is receiving considerableattention from the database community for upgrading the performance on very large data sets. This isadvantageous in the applicability of applying retrieval on very broad domains. To compensate for thecomplete loss of spatial information, new ways wererecently explored as described above.Similarity of hierarchically ordered descriptionsdeserves considerable attention as it is one mechanismto circumvent the problems with segmentation whilemaintaining some of the semantically meaningfulrelationships in the image. Part of the difficulty here isto provide matching of partial disturbances in thehierarchical order and the influence of sensorrelatedvariances in the description.We make a pledge for the importance of humanbased similarity rather than general similarity. Also,the connection between image semantics, imagedata, and query context will have to be made clearerin the future. Similarityinduced semantics and theassociated techniques for similarity adaptation e.g.,relevance feedback are a first important step, butmore sophisticated techniques, possibly drawingfrom machine learning, are necessary.Learning is quickly gaining attention as a meansto build explicit models for each semantic term.Learning is made possible today by the availabilityof large data sets and powerful machines and allowsone to form categories from captions, from partiallylabeled sets, or even from unlabeled sets. Learning islikely to be successful for large, labeled data sets onnarrow domains first, which may be relaxed tobroader domains and less standardized conditionsas the available data sets will grow even more.Obviously, learning from labeled data sets is likelyto be more successful than unsupervised learningfirst. New computational techniques, however,where only part of the data is labeled or the data islabeled by a caption rather than categories, opennew possibilities. It is our view that, in order to bringsemantics to the user, learning is inevitable.5. Interaction. We consider the emphasis on interactionin image retrieval as one of the major departuresfrom the computer vision tradition, as was alreadycited in the 1992 workshop 81. Interaction was firstpicked up by frontrunners, such as NEC laboratoriesin Japan and the MIT Media Lab, to name a few.Now, interaction and feedback have moved into thefocus of attention. Putting the user in control andvisualization of the content has always been aleading principle in information retrieval research.It is expected that more and more techniques fromtraditional information retrieval will be employed orreinvented in contentbased image retrieval. Textretrieval and image retrieval share the need forvisualizing the information content in a meaningfulway as well as the need to accept a semanticresponse of the user rather than just providingaccess to the raw data.User interaction in image retrieval has, however,some different characteristics from text retrieval.There is no sensory gap and the semantic gap fromkeywords to full text in text retrieval is of a differentnature. No translation is needed from keywords topictorial elements. In addition to the standard querytypes, six essentially different image based typeshave been identified in this paper. Each require theirown user interface tools and interaction patterns.Due to the semantic gap, visualization of the queryspace in image retrieval is of great importance forthe user to navigate the complex query space. While,currently, two or threedimensional display spacesare mostly employed in query by association, targetsearch and category search are likely to follow. In allcases, an influx of computer graphics and virtualreality is foreseen in the near future.As there is no interactivity if the response time isfrequently over a second, the interacting user poseshigh demands on the computational support. Indexing a data set for interactive use is a major challengeas the system cannot completely anticipate the usersactions. Still, in the course of the interaction, the1374 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 22, NO. 12, DECEMBER 2000whole query space, i.e., the active image set, thefeatures, the similarity, and the interpretations, canall change dynamically.6. The need for databases. When data sets grow in sizeand when larger data sets define more interestingproblems, both scientifically as well as for the public,the computational aspects can no longer be ignored.The connection between contentbased imageretrieval and database research is likely to increasein the future. Already, the most promising efforts areinterdisciplinary, but, so far, problems like thedefinition of suitable query languages, efficientsearch in high dimensional feature space, search inthe presence of changing similarity measures arelargely unsolved.It is regrettable that little work cutting across thecomputer vision and database disciplines has beendone so far, with a few notable exceptions. Whentruly large data sets come into view, with hundredsof thousands of images, databases can no longer beignored as an essential component of a contentbased retrieval system. In addition, when interactiveperformance is essential, storage and indexing mustbe organized in advance. Such large data sets willhave an effect on the choice of features as theexpressive power, computational cost, and hierarchical accessibility determine their effectiveness. Forvery large data sets, a view on content integratedwith computation and indexing cannot be ignored.When speaking about indexing in computervision, the emphasis is still on what to index,whereas the emphasis from the database side is onhow to index. The difference has become smallerrecently, but we believe most work is still to be done.Furthermore, in dealing with large feature vectorsizes, the expansion of query definitions and queryexpansions in a useful manner for a variety of useraims is still mostly unanswered.For efficiency, more work on complete sets offeature calculations from compressed images isneeded.7. The problem of evaluation. It is clear that the evaluationof system performance is essential to sort out thegood and the notsogood methods. Up to this pointin time, a fair comparison of methods under similarcircumstances has been virtually absent. This is dueto the infancy of contentbased retrieval, but also toobjective difficulties. Where interaction is a necessary component in most systems, it is difficult toseparate out the influence of the data set in theperformance. Also, it may be the case that somequeries may match the expressive power of thesystem, whereas others, similar at first glance, maybe much harder. Searching for a sunset may boildown to searching for a large orange disc at aboutthe center of the image. Searching for a lamp, whichmay seem similar to the general audience, is a muchharder problem as there is a whole variety of designsbehind a lamp. The success of the system heavilydepends on the toolset of the system relative to thequery. In addition, it is logical that a large data set iscomposed of several smaller data sets to get asufficiently big size. Then, the difficulty is theinternal coherence of the large data set with respectto the coherence of its constituents. When a data setis composed of smaller data sets holding interiordecorations, prairie landscapes, ships, and pigeons,it is clear that the essential difficulty of retrieval iswithin each set rather than among them and theessential size of the data set is still one quarter. Thereis no easy answer here other than the composition ofgenerally agreed upon data sets or the use of veryvery large data sets. In all cases, the vitality of thecontentbased approach calls for a significant growthof the attention to evaluation in the future.A reference standard against which new algorithms could be evaluated has helped the field of textrecognition enormously, see httptrec.nist.gov. Acomprehensive and publicly available collection ofimages, sorted by class and retrieval purposes,together with a protocol to standardize experimentalpractices, will be instrumental in the next phase ofcontentbased retrieval. We hope that a program forsuch a repository will be initiated under the auspicesof a funding agency.At any rate, evaluation will likely play anincreasingly significant role. Image databases, withtheir strong interactional component, present verydifferent problems from the present ones which willrequire borrowing concepts from the psychologicaland social sciences.8. The semantic gap and other sources. A critical point inthe advancement of contentbased retrieval is thesemantic gap, where the meaning of an image israrely selfevident.Use of contentbased retrieval for browsing willnot be within the grasp of the general public ashumans are accustomed to relying on the immediatesemantic imprint the moment they see an image andthey expect a computer to do the same. The aim ofcontentbased retrieval systems must be to providemaximum support in bridging the semantic gapbetween the simplicity of available visual featuresand the richness of the user semantics.One way to resolve the semantic gap comes fromsources outside the image by integrating othersources of information about the image in the query.Information about an image can come from anumber of different sources the image content,labels attached to the image, images embedded in atext, and so on. We still have very primitive ways ofintegrating this information in order to optimizeaccess to images. Among these, the integration ofnatural language processing and computer visiondeserves attention.ACKNOWLEDGMENTSThe authors are grateful for the generous suggestions by thethree anonymous referees. In addition, they acknowledgethe discussions with E. Pauwels and M. Kersten from theCWI, J. M. Geusebroek and T. Gevers from the University ofAmsterdam, and A. Tam from Victoria University. Thework was supported in part by the ICES MIAproject.SMEULDERS ET AL. CONTENTBASED IMAGE RETRIEVAL AT THE END OF THE EARLY YEARS 1375REFERENCES1 P. Aigrain, H. Zhang, and D. Petkovic, ContentBased Representation and Retrieval of Visual Media A State of the Art Review,Multimedia Tools and Applications, vol. 3, pp. 179202, 1996.2 S. Aksoy and R. Haralick, GraphTheoretic Clustering for ImageGrouping and Retrieval, Proc. Computer Vision and PatternRecognition, pp. 6368, 1999.3 R. Alferez and YF. Wang, Geometric and Illumination Invariantsfor Object Recognition, IEEE Trans. Pattern Analysis and MachineIntelligence, vol. 21, no. 6, pp. 505536, June 1999.4 D. Androutsos, K.N. Plataniotis, and A.N. Venetsanopoulos, ANovel VectorBased Approach to Color Image Retrieval Using aVector AngularBased Distance Measure, Image Understanding,vol. 75, nos. 12, pp. 4658, 1999.5 E. Angelopoulou and L.B. Wolff, Sign of Gaussian Curvaturefrom Curve Orientation in Photometric Space, IEEE Trans. PatternAnalysis and Machine Intelligence, vol. 20, no. 10, pp. 1,0561,066,Oct.. 1998.6 L. Armitage and P. Enser, Analysis of User Need in ImageArchives, J. Information Science, vol. 23, no. 4, pp. 287299, 1997.7 S. Arya, D.M. Mount, N.S. Netanyahu, R. Silverman, and A.Y. Wu,An Optimal Algorithm for Approximate Nearest NeighborhoodSearching, Proc. Symp. Discrete Algorithms, pp. 573582, 1994.8 F.G. Ashby and N.A. Perrin, Toward a Unified Theory ofSimilarity and Recognition, Psychological Rev., vol. 95, no. 1,pp. 124150, 1988.9 D. Ashlock and J. Davidson, Texture Synthesis with TandemGenetic Algorithms Using Nonparametric Partially OrderedMarkov Models, Proc. Congress on Evolutionary Computation,pp. 1,1571,163, 1999.10 J. Assfalg, A. del Bimbo, and P. Pala, Using Multiple Examplesfor Content Based Retrieval, Proc. Intl Conf. Multimedia and Expo,2000.11 R. Basri, L. Costa, D. Geiger, and D. Jacobs, Determining theSimilarity of Deformable Shapes, Vision Research, vol. 38, nos. 1516, pp. 2,3652,385, 1998.12 S. Berretti, A. del Bimbo, and E. Vicario, Modeling SpatialRelationships between Color Sets, Proc. Workshop ContentBasedAccess of Image and Video Libraries, 1998.13 Database Techniques for Pictorial Applications, Lecture Notes inComputer Science, A. Blaser, ed., vol. 81, Springer Verlag GmbH,1979.14 T. Bozkaya and M. Ozsoyoglu, DistanceBased Indexing forHighDimensional Metric Spaces, Proc. SIGMOD Intl Conf.Management of Data, pp. 357368, 1997.15 L. Brown and L. Gruenwald, TreeBased Indexes for ImageData, J. Visual Comm. and Image Representation, vol. 9, no. 4,pp. 300313, 1998.16 R. Brunelli, O. Mich, and C.M. Modena, A Survey on theAutomatic Indexing of Video Data, J. Visual Comm. and ImageRepresentation, vol. 10, pp. 78112, 1999.17 G. Bucci, S. Cagnoni, and R. De Dominicis,  Integrating ContentBased Retrieval in a Medical Image Reference Database,Computerized Medical Imaging and Graphics, vol. 20, no. 4, pp. 231241, 1996.18 H. Burkhardt and S. Siggelkow, Invariant Features for Discriminating between Equivalence Classes, Nonlinear ModelBased ImageVideo Processing and Analysis, John Wiley and Sons, 2000.19 D. Campbell and J. Stanley, Experimental and QuasiExperimentalDesigns for Research. Rand McNally College Publishing, 1963.20 C. Carson, S. Belongie, H. Greenspan, and J. Malik, RegionBasedImage Querying, Proc. Intl Workshop ContentBased Access ofImage and Video libraries, 1997.21 S.F. Chang, J.R. Smith, M. Beigi, and A. Benitez, VisualInformation Retrieval from Large Distributed Online Repositories, Comm. ACM, vol. 40, no. 12, pp. 6371, 1997.22 S.K. Chang and A.D. Hsu, ImageInformation SystemsWhereDo We Go from Here IEEE Trans. Knowledge and Data Eng., vol. 4,no. 5, pp. 431442, Oct. 1992.23 S.K. Chang, Q.Y. Shi, and C.W. Yan, Iconic Indexing by 2DStrings, IEEE Trans. Pattern Analysis and Machine Intelligence,vol. 9, pp. 413428, 1987.24 H. Chen, B. Schatz, T. Ng, J. Martinez, A. Kirchhoff, and C. Lim,A Parallel Computing Approach to Creating EngineeringConcept Spaces for Semantic Retrieval The Illinois Digital LibraryInitiative Project, IEEE Trans. Pattern Analysis and MachineIntelligence, vol. 18, no. 8, pp. 771782, Aug. 1996.25 H. Choi and R. Baraniuk, Multiscale Texture Segmentation UsingWaveletDomain Hidden Markov Models, Proc. 32nd AsilomarConf. Signals, Systems, and Computers, vol. 2, pp. 1,6921,697, 1998.26 C.K. Chui, L. Montefusco, and L. Puccio, Wavelets Theory,Algorithms, and Applications. Academic Press, 1994.27 P. Ciaccia, M. Patella, and P. Zezula, MTree An Efficient AccessMethod for Similarity Search in Metric Spaces, Proc. Very LargeData Bases Conf., 1997.28 G. Ciocca and R. Schettini, Using a Relevance FeedbackMechanism to Improve ContentBased Image Retrieval, Proc.Visual 99 Information and Information Systems, pp. 107114, 1999.29 P. Correira and F. Pereira, The Role of Analysis in ContentBasedVideo Coding and Indexing, Signal Processing, vol. 66, no. 2,pp. 125142, 1998.30 J.M. Corridoni, A. del Bimbo, and P. Pala, Image Retrieval byColor Semantics, Multimedia Systems, vol. 7, pp. 175183, 1999.31 I.J. Cox, M.L. Miller, T.P. Minka, and T.V. Papathomas, TheBayesian Image Retrieval System, PicHunter Theory, Implementation, and Pychophysical Experiments, IEEE Trans. ImageProcessing, vol. 9, no. 1, pp. 2037, 2000.32 G. Csurka and O. Faugeras, Algebraic and Geometrical Tools toCompute Projective and Permutation Invariants, IEEE Trans.Pattern Analysis and Machine Intelligence, vol. 21, no. 1, pp. 5865,Jan. 1999.33 J.F. Cullen, J.J. Hull, and P.E. Hart, Document Image DatabaseRetrieval and Browsing Using Texture Analysis, Proc. Fourth IntlConf. Document Analysis and Recognition, pp. 718721, 1997.34 I. Daubechies, Ten Lectures on Wavelets. Philadelphia SIAM, 1992.35 J. de Bonet and P. Viola, Texture Recognition Using a NonParametric MultiScale Statistical Model, Proc. Computer Visionand Pattern Recognition, 1998.36 A. del Bimbo and P. Pala, Visual Image Retrieval by ElasticMatching of User Sketches, IEEE Trans. Pattern Analysis andMachine Intelligence, vol. 19, no. 2, pp. 121132, Feb. 1997.37 A. Dempster, N. Laird, and D. Rubin, Maximum Likelihood fromIncomplete Data via the EM Algorithm, J. Royal Statistical Soc.,vol. 39, no. 1, pp. 138, 1977.38 D. Dubois and H. Prade, A Review of Fuzzy Set AggregationConnectives, Information Sciences, vol. 36, pp. 85121, 1985.39 J.P. Eakins, J.M. Boardman, and M.E. Graham, SimilarityRetrieval of Trademark Images, IEEE Multimedia, vol. 5, no. 2,pp. 5363, Apr.June 1998.40 B. Eberman, B. Fidler, R. Ianucci, C. Joerg, L. Kontothanassis, D.E.Kovalcin, P. Moreno, M.J. Swain, and J.M. van Thong, IndexingMultimedia for the Internet, Proc. Visual 99 Information andInformation Systems, pp. 195202, 1999.41 F. Ennesser and G. Medioni, Finding Waldo, or Focus of AttentionUsing Local Color Information, IEEE Trans. Pattern Analysis andMachine Intelligence, vol. 17, no. 8, pp. 805809, Aug. 1995.42 P.G.B. Enser, Pictorial Information Retrieval, J. Documentation,vol. 51, no. 2, pp. 126170, 1995.43 C. Esperanca and H. Samet, A Differential Code for ShapeRepresentation in Image Database Applications, Proc. Intl Conf.Image Processing, 1997.44 L.M. Kaplan et al., Fast Texture Database Retrieval UsingExtended Fractal Features, Storage and Retrieval for Image andVideo Databases, VI, vol. 3,312, pp. 162173, SPIE Press, 1998.45 R. Fagin, Combining Fuzzy Information from Multiple Systems,J. Computer Systems Science, vol. 58, no. 1, pp. 8399, 1999.46 C. Faloutsos and K.I. Lin, Fastmap A Fast Algorithm forIndexing, DataMining and Visualization of Traditional andMultimedia Datasets, Proc. SIGMOD, Intl Conf. Management ofData, pp. 163174, 1995.47 G.D. Finlayson, Color in Perspective, IEEE Trans. PatternAnalysis and Machine Intelligence, vol. 18, no. 10, pp. 1,0341,038,Oct. 1996.48 M. Flickner, H. Sawhney, W. Niblack, J. Ashley, Q. Huang, B. Dom,M. Gorkani, J. Hafner, D. Lee, D. Petkovic, D. Steele, and P. Yanker,Query by Image and Video Content The QBIC System, IEEEComputer, 1995.49 D.A. Forsyth, A Novel Algorithm for Color Constancy, Intl J.Computer Vision, vol. 5, no. 1, pp. 536, 1990.50 D.A. Forsyth and M.M. Fleck, Automatic Detection of HumanNudes, Intl J. Computer Vision, vol. 32, no. 1, pp. 6377, 1999.51 G. Frederix, G. Caenen, and E.J. Pauwels, PARISS Panoramic,Adaptive and Reconfigurable Interface for Similarity Search,Proc. Intl Conf. Image Processing, 2000.1376 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 22, NO. 12, DECEMBER 200052 G. Frederix and E.J. Pauwels, Automatic Interpretation Based onRobust Segmentation and Shape Extraction, Proc. Visual 99Information and Information Systems, pp. 769776, 1999.53 C.S. Fuh, S.W. Cho, and K. Essig, Hierarchical Color ImageRegion Segmentation for ContentBased Image Retrieval System,IEEE Trans. Image Processing, vol. 9, no. 1 pp. 156163, 2000.54 B.V. Funt and G.D. Finlayson, Color Constant Color Indexing,IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 17, no. 5,pp. 522529, May 1995.55 J.M. Geusebroek, A.W.M. Smeulders, and R. van den Boomgaard,Measurement of Color Invariants, Proc. Computer Vision andPattern Recognition, 2000.56 T. Gevers and A.W.M. Smeulders, ContentBased Image Retrieval by ViewpointInvariant Image Indexing, Image and VisionComputing, vol. 17, no. 7, pp. 475488, 1999.57 T. Gevers and A.W.M. Smeulders, Pictoseek Combining Colorand Shape Invariant Features for Image Retrieval, IEEE Trans.Image Processing, vol. 9, no. 1, pp. 102119, 2000.58 G.L. Gimelfarb and A.K. Jain, On Retrieving Textured Imagesfrom an Image Database, Pattern Recognition, vol. 29, no. 9,pp. 1,4611,483, 1996.59 C.C. Gottlieb and H.E. Kreyszig, Texture Descriptors Based onCoOccurrences Matrices, Computer Vision, Graphics, and ImageProcessing, vol. 51, 1990.60 W.I. Grosky, MultiMedia Information Systems. IEEE Multimedia, vol. 1, no. 1, Mar. 1994.61 A. Gupta and R. Jain, Visual Information Retrieval, Comm.ACM, vol. 40, no. 5, pp. 7179, 1997.62 J. Hafner, H.S. Sawhney, W. Equitz, M. Flickner, and W. Niblack,Efficient Color Histogram Indexing for Quadratic Form DistanceFunctions, IEEE Trans. Pattern Analysis and Machine Intelligence,vol. 17, no. 7, pp. 729736, July 1995.63 M. Hagendoorn and R.C. Veltkamp, Reliable and EfficientPattern Matching Using an Affine Invariant Metric, Intl J.Computer Vision, vol. 35, no. 3, pp. 203225, 1999.64 S. Hastings, Query Categories in a Study of Intellectual Access toDigitized Art Images, Proc. 58th Ann. Meeting Am. Soc. InformationScience, 1995.65 H. Hatano, Image Processing and Database System in theNational Museum of Western Art, Intl J. Special Libraries, vol.30, no. 3, pp. 259267, 1996.66 G. Healey and D. Slater, Computing IlluminationInvariantDescriptors of Spatially Filtered Color Image Regions, IEEETrans. Image Processing, vol. 6, no. 7 pp. 1,0021,013, 1997.67 K. Hirata and T. Kato, Rough SketchBased Image InformationRetrieval, NEC Research and Development, vol. 34, no. 2, pp. 263273, 1992.68 A. Hiroike, Y. Musha, A. Sugimoto, and Y. Mori, Visualization ofInformation Spaces to Retrieve and Browse Image Data, Proc.Visual 99 Information and Information Systems, pp. 155162, 1999.69 N.R. Howe and D.P. Huttenlocher, Integrating Color, Texture,and Geometry for Image Retrieval, Proc. Computer Vision andPattern Recognition, pp. 239247, 2000.70 C.C. Hsu, W.W. Chu, and R.K. Taira, A KnowledgeBasedApproach for Retrieving Images by Content, IEEE Trans. Knowledge and Data Eng., vol. 8, no. 4, pp. 522532, 1996.71 F.J. Hsu, S.Y. Lee, and B.S. Lin, Similairty Retrieval by 2D CTreesMatching in Image Databases, J. Visual Comm. and ImageRepresentation, vol. 9, no. 1, pp. 87100, 1998.72 M.K. Hu, Pattern Recognition by Moment Invariants, Proc. IRETrans. Information Theory, vol. 8, pp. 179187, 1962.73 J. Huang, S.R. Kumar, M. Mitra, W.J. Zhu, and R. Zabih, SpatialColor Indexing and Applications, Intl J. Computer Vision, vol. 35,no. 3, pp. 245268, 1999.74 B. Huet and E.R. Hancock, Line Pattern Retrieval UsingRelational Histograms, IEEE Trans. Pattern Analysis and MachineIntelligence, vol. 21, no. 12, pp. 1,3631,371, Dec. 1999.75 F. Idris and S. Panchanathan, Image Indexing Using WaveletVector Quantization, Proc. Digital Image Storage and ArchivingSystems, vol. 2,606, pp. 269275, 1995.76 L. Itti, C. Koch, and E. Niebur, A Model for SaliencyBased VisualAttention for Rapid Scene Analysis, IEEE Trans. Pattern Analysisand Machine Intelligence, vol. 20, no. 11, pp. 1,2541,259, Nov. 1998.77 C.E. Jacobs and A. Finkelstein, S.H. Salesin, Fast MultiresolutionImage Querying, Proc. SIGGRAPH, 1995.78 A.K. Jain and A. Vailaya, Image Retrieval Using Color andShape, Pattern Recognition, vol. 29, no. 8, pp. 1,2331,244, 1996.79 A.K. Jain and A. Vailaya, ShapeBased Retrieval A Case Studywith Trademark Image Databases, Pattern Recognition, vol. 31,no. 9, pp. 1,3691,390, 1998.80 A.K. Jain, R.P.W. Duin, and J. Mao, Statistical Pattern Recognition A Review, IEEE Trans. Pattern Analysis and MachineIntelligence, vol. 22, no. 1, pp. 437, Jan. 2000.81 Proc. US NSF Workshop Visual Information Management Systems,R. Jain, ed., 1992.82 R. Jain, InfoScopes Multimedia Information Systems, Multimedia Systems and Techniques, Kluwer Academic Publishers, 1996.83 L. Jia and L. Kitchen, ObjectBased Image Similarity Computation Using Inductive Learning of ContourSegment Relations,IEEE Trans. Iamge Processing, vol. 9, no. 1, pp. 8087, 2000.84 D.W. Joyce, P.H. Lewis, R.H. Tansley, M.R. Dobie, and W. Hall,Semiotics and Agents for Integrating and Navigating throughMultimedia Representations, Proc. Storage and Retrieval for MediaDatabases, vol. 3972, pp. 120131, 2000.85 D. Judd, P. McKinley, and A.K. Jain, LargeScale Parallel DataClustering, IEEE Trans. Pattern Analysis and Machine Intelligence,vol. 20, no. 8, pp. 871876, Aug. 1998.86 T. Kakimoto and Y. Kambayashi, Browsing Functions in ThreeDimensional Space for Digital Libraries, Intl J. Digital Libraries,vol. 2, pp. 6878, 1999.87 N. Katayama and S. Satoh, The SRTree An Index Structure forHighDimensional Nearest Neighbor Queries, Proc. SIGMOD,Intl Conf. Management of Data, pp. 369380, 1997.88 T. Kato, T. Kurita, N. Otsu, and K. Hirata, A Sketch RetrievalMethod for Full Color Image DatabaseQuery by VisualExample, Proc. ICPR, Computer Vision and Applications, pp. 530533, 1992.89 A. Kontanzad and Y.H. Hong, Invariant Image Recognition byZernike Moments, IEEE Trans. Pattern Analysis and MachineIntelligence, vol. 12, no. 5, pp. 489497, May 1990.90 F. Korn, N. Sidiropoulos, C. Faloutsos, E. Siegel, and Z.Protopapas, Efficient and Effective Nearest Neighbor Search ina Medical Image Database of Tumor Shapes, Image Descriptionand Retrieval, pp. 1754, 1998.91 S. Krishnamachari and R. Chellappa, Multiresolution GaussMarkov Random Field Models for Texture Segmentation, IEEETrans. Image Processing, vol. 6, no. 2, 1997.92 A. Laine and J. Fan, Texture Classification by Wavelet PacketSignature, IEEE Trans. Pattern Analysis and Machine Intelligence,vol. 15, no. 11, pp. 1,1861,191, Nov. 1993.93 L.J. Latecki and R. Lakamper, ContourBased Shape Similarity,Proc. Visual 99 Information and Information Systems, pp. 617624,1999.94 L.J. Latecki and R. Lakamper, Convexity Rule for ShapeDecomposition Based on Discrete Contour Evolution, ImageUnderstanding, vol. 73, no. 3, pp. 441454, 1999.95 T.K. Lau and I. King, Montage An Image Database for theFashion, Textile, and Clothing Industry in Hong Kong, Proc.Asian Conf. Computer Vision, pp. 575582, 1998.96 M. Leissler, M. Hemmje, and E.J. Neuhold, Supporting ImageRetrieval by Database Driven Interactive 3D InformationVisualization, Visual Information and Information Systems, pp. 114,Springer Verlag, 1999.97 M.S. Lew and N. Sebe, Visual Websearching Using IconicQueries, Proc. Computer Vision and Pattern Recognition, pp. 788789, 2000.98 C.S. Li and V. Castelli, Deriving Texture Feature Set for ContentBased Retrieval of Satellite Image Database, Proc. Intl Conf. ImageProcessing, 1997.99 H.C. Lin, L.L. Wang, and S.N. Yang, Color Image Retrieval Basedon Hidden Markov Models, IEEE Trans. Image Processing, vol. 6,no. 2, pp. 332339, 1997.100 T. Lindeberg and J.O. Eklundh, Scale Space Primal SketchConstruction and Experiments, Image Vision Computing, vol. 10,pp. 318, 1992.101 F. Liu and R.W. Picard, Periodicity, Directionality, and Randomness Wold Features for Image Modeling and Retrieval, IEEETrans. Pattern Analysis and Machine Intelligence, vol. 18, no. 7,pp. 517549, July 1996.102 W.Y. Ma and B.S. Manjunath, Edge Flow A Framework ofBoundary Detection and Image Segmentation, Proc. ComputerVision and Pattern Recognition, pp. 744749, 1997.SMEULDERS ET AL. CONTENTBASED IMAGE RETRIEVAL AT THE END OF THE EARLY YEARS 1377103 M.K. Mandal, F. Idris, and S. Panchanathan, Image and VideoIndexing in the Compressed Domain A Critical Review, Imageand Vision Computing, 2000.104 J. Mandel, The Statistical Analysis of Experimental Data.Interscience Publishers, 1964. Republished in 1984, Mineola,N.Y. Dover.105 B.S. Manjunath and W.Y. Ma, Texture Features for Browsing andRetrieval of Image Data, IEEE Trans. Pattern Analysis and MachineIntelligence, vol. 18, no. 8, pp. 837842, Aug. 1996106 J. Mao and A.K. Jain, Texture Classification and SegmentationUsing Multiresolution Simultaneous Autoregressive Models,Pattern Recognition, vol. 25, no. 2, 1992.107 J. Matas, R. Marik, and J. Kittler, On Representation andMatching of MultiColored Objects, Proc. Fifth Intl Conf.Computer Vision, pp. 726732, 1995.108 R. Mehrotra and J.E. Gary, SimilarShape Retrieval in ShapeData Management, Computer, vol. 28, no. 9, pp. 5762, Sept.1995.109 B.M. Mehtre, M.S. Kankanhalli, and W.F. Lee, Shape Measuresfor Content Based Image Retrieval A Comparison, InformationProcesing Management, vol. 33, no. 3, pp. 319337, 1997.110 C. Meilhac and C. Nastar, Relevance Feedback and CategorySearch in Image Databases, Proc. Intl Conf. Multimedia Computingand Systems, pp. 512517, 1999.111 T.P. Minka and R.W. Picard, Interactive Learning with a Societyof Models, Pattern Recognition, vol. 30, no. 4, pp. 565582, 1997.112 M. Mirmehdi and M. Petrou, Segmentation of Color Texture,IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 22, no. 2,pp. 142159, Feb. 2000.113 B. Moghaddam and A. Pentland, Probabilistic Visual Learningfor Object Representation, IEEE Trans. Pattern Analysis andMachine Intelligence, vol. 19, no. 7, pp. 696710, July 1997.114 B. Moghaddam, W. Wahid, and A. Pentland, Beyond EigenfacesProbabilistic Matching for Face Recognition, Proc. Third Intl Conf.Automatic Face and Gesture Reognition, 1998.115 A. Mojsilovic, J. Kovacevic, J. Hu, R.J. Safranek, and S.K.Ganapathy, Matching and Retrieval Based on the Vocabularyand Grammar of Color Patterns, IEEE Trans. Image Processing,vol. 9, no. 1, pp. 3854, 2000.116 F. Mokhtarian, SilhouetteBased Isolated Object Recognitionthrough Curvature ScaleSpace, IEEE Trans. Pattern Analysis andMachine Intelligence, vol. 17, no. 5, pp. 539544, May 1995.117 Applications of Invariance in Computer Vision, Lecture Notes inComputer Science, J.L. Mundy, A. Zissermann, D. Forsyth, eds.,vol. 825, Springer Verlag, 1994118 H. Murase and S.K. Nayar, Visual Learning and Recognition of3D Objects from Appearance, Intl J. Computer Vision, vol. 14,no. 1, pp. 524, 1995.119 D.A. Narasimhalu, M.S. Kankanhalli, and J. Wu, BenchmarkingMultimedia Databases, Multimedia Tools and Applications, vol. 4,no. 3, pp. 333355, 1997.120 V.E. Ogle, CHABOTRetrieval from a Relational Database ofImages, Computer, vol. 28, no. 9, pp. 4048, Sept. 1995.121 S. Ornager, Image Retrieval Theoretical and Empirical UserStudies on Accessing Information in Images, Proc. 60th Am. Soc.Information Science Ann. Meeting, vol. 34, pp. 202211, 1997.122 P. Pala and S. Santini, Image Retrieval by Shape and Texture,Pattern Recognition, vol. 32, no. 3, pp. 517527, 1999.123 M.L. Pao and M. Lee, Concepts of Information Retrieval. LibrariesUnlimited, 1989.124 T.V. Papathomas, E. Conway, I.J. Cox, J. Ghosn, M.L. Miller, T.P.Minka, and P.N. Yianilos, Psychophysical Studies of the Performance of an Image Database Retrieval System, Proc. Symp.Electronic Imaging Conf. Human Vision and Electronic Imaging III,1998.125 G. Pass and R. Zabith, Comparing Images Using Joint Histograms, Multimedia Systems, vol. 7, pp. 234240, 1999.126 E.J. Pauwels and G. Frederix, Nonparametric Clustering forImage Segmentation and Grouping, Image Understanding, vol. 75,no. 1, pp. 7385, 2000.127 A. Pentland and T. Choudhury, Face Recognition for SmartEnvironments, Computer, vol. 33, no. 2, pp. 50, Feb. 2000.128 A. Pentland, R.W. Picard, and S. Sclaroff, Photobook ContentBased Manipulation of Image Databases, Intl J. Computer Vision,vol. 18, no. 3, pp. 233254, 1996.129 E. Petrakis and C. Faloutsos, Similarity Searching in MedicalImage Databases, IEEE Trans. Knowledge and Data Eng., vol. 9,no. 3, pp. 435447, June 1997.130 R.W. Picard and T.P. Minka, Vision Texture for Annotation,Multimedia Systems, vol. 3, pp. 314, 1995.131 J. Puzicha, T. Hoffman, and J.M. Buhmann, NonParametricSimilarity Measures for Unsupervised Texture Segmentation andImage Retrieval, Proc. Computer Vision and Pattern Recognition,1997.132 W. Qian, M. Kallergi, L.P. Clarke, H.D. Li, D. Venugopal, D.S.Song, and L.P. Clark, TreeStructured Wavelet TransformSegmentation of Microcalcifications in Digital Mammography,J. Medical Physiology, vol. 22, no. 8, pp. 1,2471,254, 1995.133 T. Randen and J.H. Husoy, Filtering for Texture Classification AComparative Study, IEEE Trans. Pattern Analysis and MachineIntelligence, vol. 21, no. 4, pp. 291310, Apr. 1999.134 A. Rao, R.K. Srihari, and Z. Zhang, Geometric Histogram ADistribution of Geometric Configurations of Color Subsets,Internet Imaging, vol. 3,964, pp. 91101, 2000.135 E. Riloff and L. Hollaar, Text Databases and InformationRetrieval, ACM Computing Surveys, vol. 28, no. 1, pp. 133135,1996.136 E. Rivlin and I. Weiss, Local Invariants for Recognition, IEEETrans. Pattern Analysis and Machine Intelligence, vol. 17, no. 3,pp. 226238, Mar. 1995.137 R. RodriguezSanchez, J.A. Garcia, J. FdezValdivia, and X.R.FdezVidal, The RGFF Representational Model A System for theAutomatically Learned Partitioning of Visual Pattern in DigitalImages, IEEE Trans. Pattern Analysis and Machine Intelligence,vol. 21, no. 10, pp. 1,0441,073, Oct. 1999.138 P.L. Rosin, Edges Saliency Measures and Automatic Thresholding, Machine Vision Applications, vol. 9, no. 7, pp.139159, 1997.139 D. Roth, M.H. Yang, and N. Ahuja, Learning to RecognizeObjects, Proc. Computer Vision and Pattern Recognition, pp. 724731, 2000.140 I. Rothe, H. Suesse, and K. Voss, The Method of Normalization ofDetermine Invariants, IEEE Trans. Pattern Analysis and MachineIntelligence, vol. 18, no. 4, pp. 366376, Apr. 1996.141 Y. Rui, T.S. Huang, and S.F. Chang, Image Retrieval CurrentTechniques, Promising Directions, and Open Issues, J. VisualComm. and Image Representation, vol. 10, no. 1, pp. 3962, 1999.142 Y. Rui, T.S. Huang, M. Ortega, and S. Mehrotra, RelevanceFeedback A Power Tool for Interactive ContentBased ImageRetrieval, IEEE Trans. Circuits and Systems for Video Technology,1998.143 G. Salton, Automatic Text Processing. AddisonWesley, 1988.144 H. Samet and A. Soffer, MARCO MAp Retrieval by Content,IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 18, no. 8,pp. 783798, Aug. 1996.145 S. Santini, Evaluation Vademecum for Visual InformationSystems, Storage and Retrieval for Image and Video Databases VIII,vol. 3,972, 2000.146 S. Santini, A. Gupta, and R. Jain, User Interfaces for EmergentSemantics in Image Databases, Proc. Eighth IFIP Working Conf.Database Semantics DS8, 1999.147 S. Santini and R. Jain, Similarity Measures, IEEE Trans. PatternAnalysis and Machine Intelligence, vol. 21, no. 9, pp. 871883, Sept.1999.148 C. Schmid and R. Mohr, Local Grayvalue Invariants for ImageRetrieval, IEEE Trans. Pattern Analysis and Machine Intelligence,vol. 19, no. 5, pp. 530535, May 1997.149 M. Schneier and M. AbdelMottaleb, Exploiting the JPEGCompression Scheme for Image Retrieval, IEEE Trans. PatternAnalysis and Machine Intelligence, vol. 18, no. 8, 849 853, Aug. 1996.150 S. Sclaroff, Deformable Prototypes for Encoding Shape Categories in Image Databases, Pattern Recognition, vol. 30, no. 4,pp. 627641, 1997.151 S. Sclaroff, M. La Cascia, and S. Sethi, Using Textual and VisualCues for ContentBased Image Retrieval from the World WideWeb, Image Understanding, vol. 75, no. 2, pp. 8698, 1999.152 S. Sclaroff, L. Taycher, and M. La Cascia, Imagerover A ContentBase Image Browser for the World Wide Web, Proc. WorkshopContentBased Access to Image and Video Libraries, pp. 1,0001,006,1997.153 D. Sharvit, J. Chan, H. Tek, and B.B. Kimia, SymmetryBasedIndexing of Image Databases, J. Visual Comm. and ImageRepresentation, vol. 9, no. 4, pp. 366380, 1998.1378 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 22, NO. 12, DECEMBER 2000154 R.N. Shepard, Toward a Universal Law of Generalization forPhysical Science, Science, vol. 237, pp. 1,3171,323, 1987.155 R.H. Shrihari, Automatic Indexing and ContentBased Retrievalof Captioned Images, Computer, vol. 28, no. 9, Sept. 1995.156 C.R. Shyu, C.E. Brodley, A.C. Kak, and A. Kosaka, ASSERT APhysician in the Loop ContentBased Retrieval System for HCRTImage Databases, Image Understanding, vol. 75, nos. 12, pp. 111132, 1999.157 K. Siddiqi and B.B. Kimia, Parts of Visual Form ComputationalAspects, IEEE Trans. Pattern Analysis and Machine Intelligence,vol. 17, no. 3, pp. 239251, Mar. 1995.158 D. Slater and G. Healey, The IlluminationInvariant Recognitionof 3D Objects Using Local Color Invariants, IEEE Trans. PatternAnalysis and Machine Intelligence, vol. 18, no. 2, pp. 206210, Feb.1996.159 A.W.M. Smeulders, T. Gevers, J.M. Geusebroek, and M. Worring,Invariance in ContentBased Retrieval, Proc. Intl Conf. Multimedia and Expo, 2000.160 A.W.M. Smeulders, M.L. Kersten, and T. Gevers, Crossing theDivide between Computer Vision and Data Bases in Search ofImage Databases, Proc. Fourth Working Conf. Visual DatabaseSystems, pp. 223239, 1998.161 A.W.M. Smeulders, S.D. Olabariagga, R. van den Boomgaard, andM. Worring, Interactive Segmentation, Proc. Visual 97 Information Systems, pp. 512, 1997.162 J.R. Smith and S.F. Chang, Automated Binary Feature Sets forImage Retrieval, Proc. Intl Conf. Acoustics, Speech, and SignalProcessing, 1996.163 J.R. Smith and S.F. Chang, Integrated Sspatial and Feature ImageQuery, Multimedia Systems, vol. 7, no. 2, pp. 129140, 1999.164 J.R. Smith and C.S. Li, Image Retrieval Evaluation, Proc.Workshop ContentBased Access of Image and Video Libraries, 1998.165 S.M. Smith and J.M. Brady, SUSANA New Approach to LowLevel Image Processing, Intl J. Computer Vision, vol. 23, no. 1,pp. 4578, 1997.166 M. Stricker and M. Orengo, Similarity of Color Images, Storageand Retrieval of Image and Video Databases III, vol. 2,420, pp. 381392,1995.167 M. Stricker and M. Swain, The Capacity of Color HistogramIndexing, Proc. Computer Vision and Pattern Recognition, pp. 704708, 1994.168 M.J. Swain, Searching for Multimedia on the World Wide Web,Proc. Intl Conf. Multimedia Computing and Systems, pp. 3337, 1999.169 M.J. Swain and B.H. Ballard, Color Indexing, Intl J. ComputerVision, vol. 7, no. 1, pp. 1132, 1991.170 D.J. Swets and J. Weng, Hierarchical Discriminant Analysis forImage Retrieval, IEEE Trans. Pattern Analysis and MachineIntelligence, vol. 21, no. 5, pp. 386401, May 1999.171 T.F. SyedaMahmood, Location Hashing An Efficient Methodfor Locating Object Queries in Image Databases, Storage andRetrieval in Image and Video Databases, vol. 3,656, pp. 366378, 1999.172 H.D. Tagare, F.M. Vos, C.C. Jaffe, and J.S. Duncan, ArrangementA Spatial Relation between Parts for Evaluating Similarityof Tomographic Section, IEEE Trans. Pattern Analysis and MachineIntelligence, vol. 17, no. 9, pp. 880893, Sept. 1995.173 T. Tan, Rotation Invariant Texture Features and Their Use inAutomatic Script Identification, IEEE Trans. Pattern Analysis andMachine Intelligence, vol. 20, no. 7, pp. 751756, July 1998.174 P.M. Tardif and A. Zaccarin, Multiscale Autoregressive ImageRespresentation for Texture Segmentation, Image Processing VIII,vol. 3,026, pp. 327337, 1997.175 J. Tatemura, Browsing Images Based on Social and ContentSimilarity, Proc. Intl Conf. Multimedia and Expo, 2000.176 K. Tieu and P. Viola, Boosting Image Retrieval, Proc. ComputerVision and Pattern Recognition, pp. pp. 228235, 2000.177 A. Treisman, P. Cavanagh, B. Fisher, V.S. Ramachandran, and R.von der Heydt, Form Perception and AttentionStriate Cortexand Beyond, Visual Perception The Neurophysiological Foundation,pp. 273316, 1990.178 T. Tuytelaars and L. van Gool, ContentBased Image RetrievalBased on Local Affinely Invariant Regions, Proc. Visual 99Information and Information Systems, pp. 493500, 1999.179 A. Vailaya, M. Figueiredo, A. Jain, and H. Zhang, ContentBasedHierarchical Classification of Vacation Images, Proc. Intl Conf.Multimedia Computing and Systems, 1999.180 G.W.A.M. van der Heijden and M. Worring, Domain Concept toFeature Mapping for a Plant Variety Image Database, ImageDatabases and Multimedia Search, vol. 8, pp. 301308, 1997.181 V.N. Vapnik, The Nature of Statistical Learning Theory. SpringerVerlag, 1995.182 N. Vasconcelos and A. Lippman, A Probabilistic Architecture forContentBased Image Retrieval, Proc. Computer Vision and PatternRecognition, pp. pp. 216221, 2000.183 R.C. Veltkamp and M. Hagendoorn, StateoftheArt in ShapeMatching, Multimedia Search State of the Art, SpringerVerlag,2000.184 J. Vendrig, M. Worring, and A.W.M. Smeulders, Filter ImageBrowsing Exploiting Interaction in Retrieval, Proc. Visual 99Information and Information Systems, 1999.185 L.Z. Wang and G. Healey, Using Zernike Moments for theIllumination and Geometry Invariant Classification of MultiSpectral Texture, IEEE Trans. Image Processing, vol. 7, no. 2,pp. 196203, 1991.186 M. Weber, M. Welling, and P. Perona, Towards AutomaticDiscovery of Object Categories, Proc. Computer Vision and PatternRecognition, pp. 101108, 2000.187 J. Weickert, S. Ishikawa, and A. Imiya, Linear Scale Space HasFirst Been Proposed in Japan, J. Math., Imaging and Vision, vol. 10,pp. 237252, 1999.188 M. Werman and D. Weinshall, Similarity and Affine InvariantDistances between 2D Point Sets, IEEE Trans. Pattern Analysis andMachine Intelligence, vol. 17, no. 8, pp. 810814, Aug. 1995.189 D. White and R. Jain, Similarity Indexing with the SSTree, Proc.12th Intl Conf. Data Eng., 1996.190 D.A. White and R. Jain, Algorithms and Strategies for SimilarityRetrieval, Storage and Retrieval in Image, and Video Databases,vol. 2,060, pp. 6272, 1996.191 R.C. Wilson and E.R. Hancock, Structural Matching by DiscreteRelaxation, IEEE Trans. Pattern Analysis and Machine Intelligence,vol. 19, no. 6, pp. 634648, June 1997.192 H.J. Wolfson and I. Rigoutsos, Geometric Hashing An Overview, IEEE Trans. Computational Science Eng., vol. 4, no. 4, pp. 1021, 1997.193 J.K. Wu, A.D. Narasimhalu, B.M. Mehtre, C.P. Lam, and Y.J. Gao,CORE A Content Based Retrieval System for MultimediaInformation Systems, Multimedia Systems, vol. 3, pp. 2541, 1995.194 Y. Wu, Q. Tian, and T.S. Huang, DiscriminantEM Algorithmwith Application to Image Retrieval, Proc. Computer Vision andPattern Recognition, pp. 222227, 2000.195 P.N. Yanilos, Data Structures and Aalgorithms for NearestNeighbor Search in General Metric Spaces, Proc. Third Ann.Symp. Discrete Algorithms, pp. 516523, 1993.196 N. Yazdani, M. Ozsoyoglu, and G. Ozsoyoglu, A Framework forFeatureBased Indexing for Spatial Databases, Proc. Seventh IntlWorking Conf. Scientific and Statistical Database Management, pp. 259269, 1994.197 P.C. Yuen, G.C. Feng, and D.Q. Tai, Human Face Image RetrievalSystem for Large Database, Proc. 14th Intl Conf. PatternRecognition, vol. 2, pp. 1,5851,588, 1998.198 Q.L. Zhang, S.K. Chang, and S.S.T. Yau, A Unified Approach toIconic Indexing, Retrieval and Maintenance of Spatial Relationships in Image Databases, J. Visual Comm. and Image Representation, vol. 7, no. 4, pp. 307324, 1996.199 R. Zhao and W. Grosky, From Features to Semantics SomePreliminary Results, Proc. Intl Conf. Multimedia and Expo, 2000.200 Y. Zhong, K. Karu, and A.K. Jain, Locating Text in ComplexColor Images, Pattern Recognition, vol. 28, no. 10, pp. 1,5231,535,1995.201 P. Zhu and P.M. Chirlian, On Critical Point Detection of DigitalShapes, IEEE Trans. Pattern Analysis and Machine Intelligence,vol. 17, no. 8, pp. 737748, Aug. 1995.SMEULDERS ET AL. CONTENTBASED IMAGE RETRIEVAL AT THE END OF THE EARLY YEARS 1379Arnold W.M. Smeulders has been in imageprocessing since 1975 when he received theMSc degree in physics. He received thePhD degree in medical image analysis in 1982.He is a full professor of multimedia and thedirector of the Informatics Institute of theUniversity of Amsterdam. He heads the 25 person Intelligent Sensory Information Systemsgroup ISIS working on the theory of computervision, image retrieval, and industrial vision. Hehas published extensively on vision and recognition. His current topicsare image retrieval, color, intelligent interaction, the role of engineeringin vision, and the relation between language and vision. He is member ofthe board of the IAPR and cochair of TC12 on Multimedia. Previously, heserved as an associate editor of the IEEE Transactions on PatternAnalysis and Machine Intelligence and currently serves as an associateeditor of Cytometry and BioImaging. He is a senior member of the IEEE.Marcel Worring received a degree in computerscience honors from the Free University ofAmsterdam. His PhD was on digital imageanalysis and was obtained from the Universityof Amsterdam. He is an assistant professor inthe Intelligent Sensory Information Systemsgroup of the University of Amsterdam. Hiscurrent interests are in multimedia informationanalysis, in particular, user interaction andbrowsing, video, and document analysis. Hehas been a visiting researcher in the Departmentof Diagnostic Imaging at Yale University 1992 and at the VisualComputing Lab at the University of California, San Diego 1998.Simone Santini M 98 received the Laurea degree from the Universityof Florence, Italy, in 1990, the MSc and the PhD degrees from theUniversity of California, San Diego UCSD in 1996 and 1998,respectively. In 1990, he was a visiting scientist at the ArtificialIntelligence Laboratory at the University of Michigan, Ann Arbor, and,in 1993, he was a visiting scientist at the IBM Almaden ResearchCenter. Currently, he is a project scientist in the Department of Electricaland Computer Engineering, UCSD, and a researcher at Praja, Inc. Hiscurrent research interests are interactive image and video databases,behavior identification and event detection in multisensor stream, querylanguages for eventbased multimedia databases, and evaluation ofinteractive database systems. He is a member of the IEEE.Amarnath Gupta received the BTech degreein mechanical engineering from the IndianInstitute of Technology, Kharagpur, in 1984,the MS degree in biomedical engineering fromthe University of Texas at Arlington in 1987,and the PhD degree engineering in computerscience from Jadavpur University, India, in1994. He is an assistant research scientist atthe Center for Computational Science andEngineering, University of California, San DiegoUCSD. Before joining UCSD, he was a scientist at Virage, Inc.,where he worked on multimedia information systems. His currentresearch interests are in multimedia and spatiotemporal informationsystems, heterogeneous information integration, and scientific databases. He is a member of the IEEE.Ramesh Jain F 92 received the BE degreefrom Nagpur University in 1969 and thePhD degree from the Indian Institute of Technology, Kharagpur in 1975. He was a professor ofelectrical and computer engineering, and computer science and engineering at the Universityof California, San Diego. Before joining UCSD,he was a professor of electrical engineering andcomputer science, and the founding Director ofthe Artificial Intelligence Laboratory at the University of Michigan, Ann Arbor. He also has beenaffiliated with Stanford University, IBM Almaden Research Labs,General Motors Research Labs, Wayne State University, the Universityof Texas at Austin, the University of Hamburg, West Germany, and theIndian Institute of Technology, Kharagpur, India. His current researchinterests are in multimedia information systems, image databases,machine vision, and intelligent systems. He is a fellow of the IEEE,AAAI, and Society of PhotoOptical Instrumentation Engineers, and amember of ACM, Pattern Recognition Society, Manufacturing Engineers. He has been involved in the organization of several professionalconferences and workshops, and served on the editorial boards of manyjournals. He was the editorinchief of IEEE Multimedia and is on theeditorial boards of Machine Vision and Applications, Pattern Recognition, and Image and Vision Computing.1380 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 22, NO. 12, DECEMBER 2000
