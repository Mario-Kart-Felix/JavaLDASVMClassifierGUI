Data Clustering A ReviewA.K. JAINMichigan State UniversityM.N. MURTYIndian Institute of ScienceANDP.J. FLYNNThe Ohio State UniversityClustering is the unsupervised classification of patterns observations, data items,or feature vectors into groups clusters. The clustering problem has beenaddressed in many contexts and by researchers in many disciplines this reflects itsbroad appeal and usefulness as one of the steps in exploratory data analysis.However, clustering is a difficult problem combinatorially, and differences inassumptions and contexts in different communities has made the transfer of usefulgeneric concepts and methodologies slow to occur. This paper presents an overviewof pattern clustering methods from a statistical pattern recognition perspective,with a goal of providing useful advice and references to fundamental conceptsaccessible to the broad community of clustering practitioners. We present ataxonomy of clustering techniques, and identify crosscutting themes and recentadvances. We also describe some important applications of clustering algorithmssuch as image segmentation, object recognition, and information retrieval.Categories and Subject Descriptors I.5.1 Pattern Recognition Models I.5.3Pattern Recognition Clustering I.5.4 Pattern Recognition ApplicationsComputer vision H.3.3 Information Storage and Retrieval InformationSearch and RetrievalClustering I.2.6 Artificial IntelligenceLearningKnowledge acquisitionGeneral Terms AlgorithmsAdditional Key Words and Phrases Cluster analysis, clustering applications,exploratory data analysis, incremental clustering, similarity indices, unsupervisedlearningSection 6.1 is based on the chapter Image Segmentation Using Clustering by A.K. Jain and P.J.Flynn, Advances in Image Understanding A Festschrift for Azriel Rosenfeld K. Bowyer and N. Ahuja,Eds., 1996 IEEE Computer Society Press, and is used by permission of the IEEE Computer Society.Authors addresses A. Jain, Department of Computer Science, Michigan State University, A714 WellsHall, East Lansing, MI 48824 M. Murty, Department of Computer Science and Automation, IndianInstitute of Science, Bangalore, 560 012, India P. Flynn, Department of Electrical Engineering, TheOhio State University, Columbus, OH 43210.Permission to make digital  hard copy of part or all of this work for personal or classroom use is grantedwithout fee provided that the copies are not made or distributed for profit or commercial advantage, thecopyright notice, the title of the publication, and its date appear, and notice is given that copying is bypermission of the ACM, Inc. To copy otherwise, to republish, to post on servers, or to redistribute tolists, requires prior specific permission and  or a fee. 2000 ACM 036003009909000001 5.00ACM Computing Surveys, Vol. 31, No. 3, September 19991. INTRODUCTION1.1 MotivationData analysis underlies many computing applications, either in a designphase or as part of their online operations. Data analysis procedures can bedichotomized as either exploratory orconfirmatory, based on the availabilityof appropriate models for the datasource, but a key element in both typesof procedures whether for hypothesisformation or decisionmaking is thegrouping, or classification of measurements based on either i goodnessoffitto a postulated model, or ii naturalgroupings clustering revealed throughanalysis. Cluster analysis is the organization of a collection of patterns usually represented as a vector of measurements, or a point in a multidimensionalspace into clusters based on similarity.Intuitively, patterns within a valid cluster are more similar to each other thanthey are to a pattern belonging to adifferent cluster. An example of clustering is depicted in Figure 1. The inputpatterns are shown in Figure 1a, andthe desired clusters are shown in Figure1b. Here, points belonging to the samecluster are given the same label. Thevariety of techniques for representingdata, measuring proximity similaritybetween data elements, and groupingdata elements has produced a rich andoften confusing assortment of clusteringmethods.It is important to understand the difference between clustering unsupervised classification and discriminantanalysis supervised classification. Insupervised classification, we are provided with a collection of labeled preclassified patterns the problem is tolabel a newly encountered, yet unlabeled, pattern. Typically, the given labeled training patterns are used tolearn the descriptions of classes whichin turn are used to label a new pattern.In the case of clustering, the problem isto group a given collection of unlabeledpatterns into meaningful clusters. In asense, labels are associated with clusters also, but these category labels aredata driven that is, they are obtainedsolely from the data.Clustering is useful in several exploratory patternanalysis, grouping, decisionmaking, and machinelearning situations, including data mining,document retrieval, image segmentation, and pattern classification. However, in many such problems, there islittle prior information e.g., statisticalmodels available about the data, andthe decisionmaker must make as fewassumptions about the data as possible.It is under these restrictions that clustering methodology is particularly appropriate for the exploration of interrelationships among the data points tomake an assessment perhaps preliminary of their structure.The term clustering is used in several research communities to describeCONTENTS1. Introduction1.1 Motivation1.2 Components of a Clustering Task1.3 The Users Dilemma and the Role of Expertise1.4 History1.5 Outline2. Definitions and Notation3. Pattern Representation, Feature Selection andExtraction4. Similarity Measures5. Clustering Techniques5.1 Hierarchical Clustering Algorithms5.2 Partitional Algorithms5.3 MixtureResolving and ModeSeekingAlgorithms5.4 Nearest Neighbor Clustering5.5 Fuzzy Clustering5.6 Representation of Clusters5.7 Artificial Neural Networks for Clustering5.8 Evolutionary Approaches for Clustering5.9 SearchBased Approaches5.10 A Comparison of Techniques5.11 Incorporating Domain Constraints inClustering5.12 Clustering Large Data Sets6. Applications6.1 Image Segmentation Using Clustering6.2 Object and Character Recognition6.3 Information Retrieval6.4 Data Mining7. SummaryData Clustering  265ACM Computing Surveys, Vol. 31, No. 3, September 1999methods for grouping of unlabeled data.These communities have different terminologies and assumptions for thecomponents of the clustering processand the contexts in which clustering isused. Thus, we face a dilemma regarding the scope of this survey. The production of a truly comprehensive surveywould be a monumental task given thesheer mass of literature in this area.The accessibility of the survey mightalso be questionable given the need toreconcile very different vocabulariesand assumptions regarding clusteringin the various communities.The goal of this paper is to survey thecore concepts and techniques in thelarge subset of cluster analysis with itsroots in statistics and decision theory.Where appropriate, references will bemade to key concepts and techniquesarising from clustering methodology inthe machinelearning and other communities.The audience for this paper includespractitioners in the pattern recognitionand image analysis communities whoshould view it as a summarization ofcurrent practice, practitioners in themachinelearning communities whoshould view it as a snapshot of a closelyrelated field with a rich history of wellunderstood techniques, and thebroader audience of scientific professionals who should view it as an accessible introduction to a mature field thatis making important contributions tocomputing application areas.1.2 Components of a Clustering TaskTypical pattern clustering activity involves the following steps Jain andDubes 19881 pattern representation optionallyincluding feature extraction andorselection,2 definition of a pattern proximitymeasure appropriate to the data domain,3 clustering or grouping,4 data abstraction if needed, and5 assessment of output if needed.Figure 2 depicts a typical sequencing ofthe first three of these steps, includinga feedback path where the groupingprocess output could affect subsequentfeature extraction and similarity computations.Pattern representation refers to thenumber of classes, the number of available patterns, and the number, type,and scale of the features available to theclustering algorithm. Some of this information may not be controllable by theX XY Ya bx xxxx 1 11x   x112   2x   x 2   2x  x  xxxxxxxxxxxxxxxxx3 3  334444444444444 44xxxxxxxx 66677776x x xxxxx4 5 55555Figure 1. Data clustering.266  A. Jain et al.ACM Computing Surveys, Vol. 31, No. 3, September 1999practitioner. Feature selection is theprocess of identifying the most effectivesubset of the original features to use inclustering. Feature extraction is the useof one or more transformations of theinput features to produce new salientfeatures. Either or both of these techniques can be used to obtain an appropriate set of features to use in clustering.Pattern proximity is usually measuredby a distance function defined on pairsof patterns. A variety of distance measures are in use in the various communities Anderberg 1973 Jain and Dubes1988 Diday and Simon 1976. A simpledistance measure like Euclidean distance can often be used to reflect dissimilarity between two patterns,whereas other similarity measures canbe used to characterize the conceptualsimilarity between patterns Michalskiand Stepp 1983. Distance measures arediscussed in Section 4.The grouping step can be performedin a number of ways. The output clustering or clusterings can be hard apartition of the data into groups orfuzzy where each pattern has a variable degree of membership in each ofthe output clusters. Hierarchical clustering algorithms produce a nested series of partitions based on a criterion formerging or splitting clusters based onsimilarity. Partitional clustering algorithms identify the partition that optimizes usually locally a clustering criterion. Additional techniques for thegrouping operation include probabilisticBrailovski 1991 and graphtheoreticZahn 1971 clustering methods. Thevariety of techniques for cluster formation is described in Section 5.Data abstraction is the process of extracting a simple and compact representation of a data set. Here, simplicity iseither from the perspective of automaticanalysis so that a machine can performfurther processing efficiently or it ishumanoriented so that the representation obtained is easy to comprehend andintuitively appealing. In the clusteringcontext, a typical data abstraction is acompact description of each cluster,usually in terms of cluster prototypes orrepresentative patterns such as the centroid Diday and Simon 1976.How is the output of a clustering algorithm evaluated What characterizes agood clustering result and a poor oneAll clustering algorithms will, whenpresented with data, produce clusters regardless of whether the data containclusters or not. If the data does containclusters, some clustering algorithmsmay obtain better clusters than others.The assessment of a clustering procedures output, then, has several facets.One is actually an assessment of thedata domain rather than the clusteringalgorithm itself data which do notcontain clusters should not be processedby a clustering algorithm. The study ofcluster tendency, wherein the input dataare examined to see if there is any meritto a cluster analysis prior to one beingperformed, is a relatively inactive research area, and will not be consideredfurther in this survey. The interestedreader is referred to Dubes 1987 andCheng 1995 for information.Cluster validity analysis, by contrast,is the assessment of a clustering procedures output. Often this analysis uses aspecific criterion of optimality however,these criteria are usually arrived atFeatureSelectionExtractionPatternGroupingClustersInterpatternSimilarityRepresentationsPatternsfeedback loopFigure 2. Stages in clustering.Data Clustering  267ACM Computing Surveys, Vol. 31, No. 3, September 1999subjectively. Hence, little in the way ofgold standards exist in clustering except in wellprescribed subdomains. Validity assessments are objective Dubes1993 and are performed to determinewhether the output is meaningful. Aclustering structure is valid if it cannotreasonably have occurred by chance oras an artifact of a clustering algorithm.When statistical approaches to clustering are used, validation is accomplishedby carefully applying statistical methods and testing hypotheses. There arethree types of validation studies. Anexternal assessment of validity compares the recovered structure to an apriori structure. An internal examination of validity tries to determine if thestructure is intrinsically appropriate forthe data. A relative test compares twostructures and measures their relativemerit. Indices used for this comparisonare discussed in detail in Jain andDubes 1988 and Dubes 1993, and arenot discussed further in this paper.1.3 The Users Dilemma and the Role ofExpertiseThe availability of such a vast collectionof clustering algorithms in the literature can easily confound a user attempting to select an algorithm suitable forthe problem at hand. In Dubes and Jain1976, a set of admissibility criteriadefined by Fisher and Van Ness 1971are used to compare clustering algorithms. These admissibility criteria arebased on 1 the manner in which clusters are formed, 2 the structure of thedata, and 3 sensitivity of the clustering technique to changes that do notaffect the structure of the data. However, there is no critical analysis of clustering algorithms dealing with the important questions such asHow should the data be normalizedWhich similarity measure is appropriate to use in a given situationHow should domain knowledge be utilized in a particular clustering problemHow can a vary large data set say, amillion patterns be clustered efficientlyThese issues have motivated this survey, and its aim is to provide a perspective on the state of the art in clusteringmethodology and algorithms. With sucha perspective, an informed practitionershould be able to confidently assess thetradeoffs of different techniques, andultimately make a competent decisionon a technique or suite of techniques toemploy in a particular application.There is no clustering technique thatis universally applicable in uncoveringthe variety of structures present in multidimensional data sets. For example,consider the twodimensional data setshown in Figure 1a. Not all clusteringtechniques can uncover all the clusterspresent here with equal facility, becauseclustering algorithms often contain implicit assumptions about cluster shapeor multiplecluster configurations basedon the similarity measures and grouping criteria used.Humans perform competitively withautomatic clustering procedures in twodimensions, but most real problems involve clustering in higher dimensions. Itis difficult for humans to obtain an intuitive interpretation of data embedded ina highdimensional space. In addition,data hardly follow the ideal structurese.g., hyperspherical, linear shown inFigure 1. This explains the large number of clustering algorithms which continue to appear in the literature eachnew clustering algorithm performsslightly better than the existing ones ona specific distribution of patterns.It is essential for the user of a clustering algorithm to not only have a thorough understanding of the particulartechnique being utilized, but also toknow the details of the data gatheringprocess and to have some domain expertise the more information the user hasabout the data at hand, the more likelythe user would be able to succeed inassessing its true class structure Jainand Dubes 1988. This domain informa268  A. Jain et al.ACM Computing Surveys, Vol. 31, No. 3, September 1999tion can also be used to improve thequality of feature extraction, similaritycomputation, grouping, and cluster representation Murty and Jain 1995.Appropriate constraints on the datasource can be incorporated into a clustering procedure. One example of this ismixture resolving Titterington et al.1985, wherein it is assumed that thedata are drawn from a mixture of anunknown number of densities often assumed to be multivariate Gaussian.The clustering problem here is to identify the number of mixture componentsand the parameters of each component.The concept of density clustering and amethodology for decomposition of feature spaces Bajcsy 1997 have alsobeen incorporated into traditional clustering methodology, yielding a technique for extracting overlapping clusters.1.4 HistoryEven though there is an increasing interest in the use of clustering methodsin pattern recognition Anderberg1973, image processing Jain andFlynn 1996 and information retrievalRasmussen 1992 Salton 1991, clustering has a rich history in other disciplines Jain and Dubes 1988 such asbiology, psychiatry, psychology, archaeology, geology, geography, and marketing. Other terms more or less synonymous with clustering includeunsupervised learning Jain and Dubes1988, numerical taxonomy Sneath andSokal 1973, vector quantization Oehlerand Gray 1995, and learning by observation Michalski and Stepp 1983. Thefield of spatial analysis of point patterns Ripley 1988 is also related tocluster analysis. The importance andinterdisciplinary nature of clustering isevident through its vast literature.A number of books on clustering havebeen published Jain and Dubes 1988Anderberg 1973 Hartigan 1975 Spath1980 Duran and Odell 1974 Everitt1993 Backer 1995, in addition to someuseful and influential review papers. Asurvey of the state of the art in clustering circa 1978 was reported in Dubesand Jain 1980. A comparison of various clustering algorithms for constructing the minimal spanning tree and theshort spanning path was given in Lee1981. Cluster analysis was also surveyed in Jain et al. 1986. A review ofimage segmentation by clustering wasreported in Jain and Flynn 1996. Comparisons of various combinatorial optimization schemes, based on experiments, have been reported in Mishraand Raghavan 1994 and AlSultan andKhan 1996.1.5 OutlineThis paper is organized as follows. Section 2 presents definitions of terms to beused throughout the paper. Section 3summarizes pattern representation,feature extraction, and feature selection. Various approaches to the computation of proximity between patternsare discussed in Section 4. Section 5presents a taxonomy of clustering approaches, describes the major techniques in use, and discusses emergingtechniques for clustering incorporatingnonnumeric constraints and the clustering of large sets of patterns. Section6 discusses applications of clusteringmethods to image analysis and datamining problems. Finally, Section 7 presents some concluding remarks.2. DEFINITIONS AND NOTATIONThe following terms and notation areused throughout this paper.A pattern or feature vector, observation, or datum x is a single data itemused by the clustering algorithm. Ittypically consists of a vector of d measurements x 5 x1, . . . xd.The individual scalar components xiof a pattern x are called features orattributes.Data Clustering  269ACM Computing Surveys, Vol. 31, No. 3, September 1999d is the dimensionality of the patternor of the pattern space.A pattern set is denoted  5x1, . . . xn. The ith pattern in  isdenoted xi 5 xi,1, . . . xi,d. In manycases a pattern set to be clustered isviewed as an n 3 d pattern matrix.A class, in the abstract, refers to astate of nature that governs the pattern generation process in some cases.More concretely, a class can be viewedas a source of patterns whose distribution in feature space is governed bya probability density specific to theclass. Clustering techniques attemptto group patterns so that the classesthereby obtained reflect the differentpattern generation processes represented in the pattern set.Hard clustering techniques assign aclass label li to each patterns xi, identifying its class. The set of all labelsfor a pattern set  is  5l1, . . . ln, with li  1,   , k,where k is the number of clusters.Fuzzy clustering procedures assign toeach input pattern xi a fractional degree of membership fij in each outputcluster j.A distance measure a specializationof a proximity measure is a metricor quasimetric on the feature spaceused to quantify the similarity of patterns.3. PATTERN REPRESENTATION, FEATURESELECTION AND EXTRACTIONThere are no theoretical guidelines thatsuggest the appropriate patterns andfeatures to use in a specific situation.Indeed, the pattern generation processis often not directly controllable theusers role in the pattern representationprocess is to gather facts and conjectures about the data, optionally performfeature selection and extraction, and design the subsequent elements of theclustering system. Because of the difficulties surrounding pattern representation, it is conveniently assumed that thepattern representation is available priorto clustering. Nonetheless, a careful investigation of the available features andany available transformations evensimple ones can yield significantly improved clustering results. A good pattern representation can often yield asimple and easily understood clusteringa poor pattern representation may yielda complex clustering whose true structure is difficult or impossible to discern.Figure 3 shows a simple example. Thepoints in this 2D feature space are arranged in a curvilinear cluster of approximately constant distance from theorigin. If one chooses Cartesian coordinates to represent the patterns, manyclustering algorithms would be likely tofragment the cluster into two or moreclusters, since it is not compact. If, however, one uses a polar coordinate representation for the clusters, the radiuscoordinate exhibits tight clustering anda onecluster solution is likely to beeasily obtained.A pattern can measure either a physical object e.g., a chair or an abstractnotion e.g., a style of writing. As notedabove, patterns are represented conventionally as multidimensional vectors,where each dimension is a single feature Duda and Hart 1973. These features can be either quantitative or qualitative. For example, if weight and colorare the two features used, then20, black is the representation of ablack object with 20 units of weight.The features can be subdivided into thefollowing types Gowda and Diday19921 Quantitative features e.g.a continuous values e.g., weightb discrete values e.g., the numberof computersc interval values e.g., the duration of an event.2 Qualitative featuresa nominal or unordered e.g., color270  A. Jain et al.ACM Computing Surveys, Vol. 31, No. 3, September 1999b ordinal e.g., military rank orqualitative evaluations of temperature cool or hot orsound intensity quiet orloud.Quantitative features can be measuredon a ratio scale with a meaningful reference value, such as temperature, oron nominal or ordinal scales.One can also use structured featuresMichalski and Stepp 1983 which arerepresented as trees, where the parentnode represents a generalization of itschild nodes. For example, a parent nodevehicle may be a generalization ofchildren labeled cars, buses,trucks, and motorcycles. Further,the node cars could be a generalization of cars of the type Toyota, Ford,Benz, etc. A generalized representation of patterns, called symbolic objectswas proposed in Diday 1988. Symbolicobjects are defined by a logical conjunction of events. These events link valuesand features in which the features cantake one or more values and all theobjects need not be defined on the sameset of features.It is often valuable to isolate only themost descriptive and discriminatory features in the input set, and utilize thosefeatures exclusively in subsequent analysis. Feature selection techniques identify a subset of the existing features forsubsequent use, while feature extraction techniques compute new featuresfrom the original set. In either case, thegoal is to improve classification performance andor computational efficiency.Feature selection is a wellexploredtopic in statistical pattern recognitionDuda and Hart 1973 however, in aclustering context i.e., lacking class labels for patterns, the feature selectionprocess is of necessity ad hoc, and mightinvolve a trialanderror process wherevarious subsets of features are selected,the resulting patterns clustered, andthe output evaluated using a validityindex. In contrast, some of the popularfeature extraction processes e.g., principal components analysis Fukunaga1990 do not depend on labeled dataand can be used directly. Reduction ofthe number of features has an additional benefit, namely the ability to produce output that can be visually inspected by a human.4. SIMILARITY MEASURESSince similarity is fundamental to thedefinition of a cluster, a measure of thesimilarity between two patterns drawnfrom the same feature space is essentialto most clustering procedures. Becauseof the variety of feature types andscales, the distance measure or measures must be chosen carefully. It ismost common to calculate the dissimilarity between two patterns using a distance measure defined on the featurespace. We will focus on the wellknowndistance measures used for patternswhose features are all continuous.The most popular metric for continuous features is the Euclidean distanced2xi, xj 5  Ok51dxi, k 2 xj, k21 25 ixi 2 xji2,which is a special case p52 of theMinkowski metric........ ... ... ......... ... ... ......... ... ... ......... ... ... ......... ... ... ......... ... ... ......... ... ... ......... ... ... ......... ... ... .............................................. ............................................. .............................. .............................. ...............Figure 3. A curvilinear cluster whose pointsare approximately equidistant from the origin.Different pattern representations coordinatesystems would cause clustering algorithms toyield different results for this data see text.Data Clustering  271ACM Computing Surveys, Vol. 31, No. 3, September 1999dpxi, xj 5  Ok51dxi, k 2 xj, kp1p5 ixi 2 xjip.The Euclidean distance has an intuitiveappeal as it is commonly used to evaluate the proximity of objects in two orthreedimensional space. It works wellwhen a data set has compact or isolated clusters Mao and Jain 1996.The drawback to direct use of theMinkowski metrics is the tendency ofthe largestscaled feature to dominatethe others. Solutions to this probleminclude normalization of the continuousfeatures to a common range or variance or other weighting schemes. Linear correlation among features can alsodistort distance measures this distortion can be alleviated by applying awhitening transformation to the data orby using the squared Mahalanobis distancedMxi, xj 5 xi 2 xjS21xi 2 xjT,where the patterns xi and xj are assumed to be row vectors, and S is thesample covariance matrix of the patterns or the known covariance matrix ofthe pattern generation process dM z , zassigns different weights to differentfeatures based on their variances andpairwise linear correlations. Here, it isimplicitly assumed that class conditional densities are unimodal and characterized by multidimensional spread,i.e., that the densities are multivariateGaussian. The regularized Mahalanobisdistance was used in Mao and Jain1996 to extract hyperellipsoidal clusters. Recently, several researchersHuttenlocher et al. 1993 Dubuissonand Jain 1994 have used the Hausdorffdistance in a point set matching context.Some clustering algorithms work on amatrix of proximity values instead of onthe original pattern set. It is useful insuch situations to precompute all thenn 2 1  2 pairwise distance valuesfor the n patterns and store them in asymmetric matrix.Computation of distances betweenpatterns with some or all features beingnoncontinuous is problematic, since thedifferent types of features are not comparable and as an extreme examplethe notion of proximity is effectively binaryvalued for nominalscaled features. Nonetheless, practitioners especially those in machine learning, wheremixedtype patterns are common havedeveloped proximity measures for heterogeneous type patterns. A recent example is Wilson and Martinez 1997,which proposes a combination of a modified Minkowski metric for continuousfeatures and a distance based on countspopulation for nominal attributes. Avariety of other metrics have been reported in Diday and Simon 1976 andIchino and Yaguchi 1994 for computing the similarity between patterns represented using quantitative as well asqualitative features.Patterns can also be represented using string or tree structures Knuth1973. Strings are used in syntacticclustering Fu and Lu 1977. Severalmeasures of similarity between stringsare described in BaezaYates 1992. Agood summary of similarity measuresbetween trees is given by Zhang 1995.A comparison of syntactic and statistical approaches for pattern recognitionusing several criteria was presented inTanaka 1995 and the conclusion wasthat syntactic methods are inferior inevery aspect. Therefore, we do not consider syntactic methods further in thispaper.There are some distance measures reported in the literature Gowda andKrishna 1977 Jarvis and Patrick 1973that take into account the effect of surrounding or neighboring points. Thesesurrounding points are called context inMichalski and Stepp 1983. The similarity between two points xi and xj,given this context, is given by272  A. Jain et al.ACM Computing Surveys, Vol. 31, No. 3, September 1999sxi, xj 5 fxi, xj, ,where  is the context the set of surrounding points. One metric definedusing context is the mutual neighbordistance MND, proposed in Gowda andKrishna 1977, which is given byMNDxi, xj 5 NNxi, xj 1 NNxj, xi,where NNxi, xj is the neighbor number of xj with respect to xi. Figures 4and 5 give an example. In Figure 4, thenearest neighbor of A is B, and Bsnearest neighbor is A. So, NNA, B 5NNB, A 5 1 and the MND betweenA and B is 2. However, NNB, C 5 1but NNC, B 5 2, and thereforeMNDB, C 5 3. Figure 5 was obtained from Figure 4 by adding three newpoints D, E, and F. Now MNDB, C5 3 as before, but MNDA, B 5 5.The MND between A and B has increased by introducing additionalpoints, even though A and B have notmoved. The MND is not a metric it doesnot satisfy the triangle inequalityZhang 1995. In spite of this, MND hasbeen successfully applied in severalclustering applications Gowda and Diday 1992. This observation supportsthe viewpoint that the dissimilaritydoes not need to be a metric.Watanabes theorem of the ugly duckling Watanabe 1985 statesInsofar as we use a finite set ofpredicates that are capable of distinguishing any two objects considered, the number of predicatesshared by any two such objects isconstant, independent of thechoice of objects.This implies that it is possible tomake any two arbitrary patternsequally similar by encoding them with asufficiently large number of features. Asa consequence, any two arbitrary patterns are equally similar, unless we usesome additional domain information.For example, in the case of conceptualclustering Michalski and Stepp 1983,the similarity between xi and xj is defined assxi, xj 5 fxi, xj, , ,where  is a set of predefined concepts.This notion is illustrated with the helpof Figure 6. Here, the Euclidean distance between points A and B is lessthan that between B and C. However, Band C can be viewed as more similarthan A and B because B and C belong tothe same concept ellipse and A belongsto a different concept rectangle. Theconceptual similarity measure is themost general similarity measure. WeABCXX12Figure 4. A and B are more similar than Aand C.ABCXX12DF EFigure 5. After a change in context, B and Care more similar than B and A.Data Clustering  273ACM Computing Surveys, Vol. 31, No. 3, September 1999discuss several pragmatic issues associated with its use in Section 5.5. CLUSTERING TECHNIQUESDifferent approaches to clustering datacan be described with the help of thehierarchy shown in Figure 7 other taxonometric representations of clusteringmethodology are possible ours is basedon the discussion in Jain and Dubes1988. At the top level, there is a distinction between hierarchical and partitional approaches hierarchical methodsproduce a nested series of partitions,while partitional methods produce onlyone.The taxonomy shown in Figure 7must be supplemented by a discussionof crosscutting issues that may inprinciple affect all of the different approaches regardless of their placementin the taxonomy.Agglomerative vs. divisive This aspect relates to algorithmic structureand operation. An agglomerative approach begins with each pattern in adistinct singleton cluster, and successively merges clusters together until a stopping criterion is satisfied. Adivisive method begins with all patterns in a single cluster and performssplitting until a stopping criterion ismet.Monothetic vs. polythetic This aspectrelates to the sequential or simultaneous use of features in the clusteringprocess. Most algorithms are polythetic that is, all features enter into thecomputation of distances betweenpatterns, and decisions are based onthose distances. A simple monotheticalgorithm reported in Anderberg1973 considers features sequentiallyto divide the given collection of patterns. This is illustrated in Figure 8.Here, the collection is divided intotwo groups using feature x1 the vertical broken line V is the separatingline. Each of these clusters is furtherdivided independently using featurex2, as depicted by the broken lines H1and H2. The major problem with thisalgorithm is that it generates 2d clusters where d is the dimensionality ofthe patterns. For large values of dd . 100 is typical in information retrieval applications Salton 1991,the number of clusters generated bythis algorithm is so large that thedata set is divided into uninterestingly small and fragmented clusters.Hard vs. fuzzy A hard clustering algorithm allocates each pattern to asingle cluster during its operation andin its output. A fuzzy clusteringmethod assigns degrees of membership in several clusters to each inputpattern. A fuzzy clustering can beconverted to a hard clustering by assigning each pattern to the clusterwith the largest measure of membership.Deterministic vs. stochastic This issue is most relevant to partitionalapproaches designed to optimize asquared error function. This optimization can be accomplished using traditional techniques or through a random search of the state spaceconsisting of all possible labelings.Incremental vs. nonincrementalThis issue arises when the pattern setx   x   x   x   x   x   x   x   x   x   x   x   x   xx   x   x   x   x   x   x   x   x   x   x   x   x   xxxxxxxxxxxxxx x x x xxxxxxxxxxxxxxxxxxABCFigure 6. Conceptual similarity between points .274  A. Jain et al.ACM Computing Surveys, Vol. 31, No. 3, September 1999to be clustered is large, and constraints on execution time or memoryspace affect the architecture of thealgorithm. The early history of clustering methodology does not containmany examples of clustering algorithms designed to work with largedata sets, but the advent of data mining has fostered the development ofclustering algorithms that minimizethe number of scans through the pattern set, reduce the number of patterns examined during execution, orreduce the size of data structuresused in the algorithms operations.A cogent observation in Jain andDubes 1988 is that the specification ofan algorithm for clustering usuallyleaves considerable flexibilty in implementation.5.1 Hierarchical Clustering AlgorithmsThe operation of a hierarchical clustering algorithm is illustrated using thetwodimensional data set in Figure 9.This figure depicts seven patterns labeled A, B, C, D, E, F, and G in threeclusters. A hierarchical algorithm yieldsa dendrogram representing the nestedgrouping of patterns and similarity levels at which groupings change. A dendrogram corresponding to the sevenpoints in Figure 9 obtained from thesinglelink algorithm Jain and Dubes1988 is shown in Figure 10. The dendrogram can be broken at different levels to yield different clusterings of thedata.Most hierarchical clustering algorithms are variants of the singlelinkSneath and Sokal 1973, completelinkKing 1967, and minimumvarianceWard 1963 Murtagh 1984 algorithms.Of these, the singlelink and completelink algorithms are most popular. Thesetwo algorithms differ in the way theycharacterize the similarity between apair of clusters. In the singlelinkmethod, the distance between two clusClusteringPartitionalSingleLinkCompleteLinkHierarchicalSquareErrorGraphTheoreticMixtureResolvingModeSeekingkmeansMaximizationExpectationFigure 7. A taxonomy of clustering approaches.X1 11111111111 1 112222222 2 23 333 3 33 3333333333 334 44444 4 4444444422222222211111111 11VHH21X21Figure 8. Monothetic partitional clustering.Data Clustering  275ACM Computing Surveys, Vol. 31, No. 3, September 1999ters is the minimum of the distancesbetween all pairs of patterns drawnfrom the two clusters one pattern fromthe first cluster, the other from the second. In the completelink algorithm,the distance between two clusters is themaximum of all pairwise distances between patterns in the two clusters. Ineither case, two clusters are merged toform a larger cluster based on minimumdistance criteria. The completelink algorithm produces tightly bound or compact clusters BaezaYates 1992. Thesinglelink algorithm, by contrast, suffers from a chaining effect Nagy 1968.It has a tendency to produce clustersthat are straggly or elongated. Thereare two clusters in Figures 12 and 13separated by a bridge of noisy patterns. The singlelink algorithm produces the clusters shown in Figure 12,whereas the completelink algorithm obtains the clustering shown in Figure 13.The clusters obtained by the completelink algorithm are more compact thanthose obtained by the singlelink algorithm the cluster labeled 1 obtainedusing the singlelink algorithm is elongated because of the noisy patterns labeled . The singlelink algorithm ismore versatile than the completelinkalgorithm, otherwise. For example, thesinglelink algorithm can extract theconcentric clusters shown in Figure 11,but the completelink algorithm cannot.However, from a pragmatic viewpoint, ithas been observed that the completelink algorithm produces more useful hierarchies in many applications than thesinglelink algorithm Jain and Dubes1988.Agglomerative SingleLink Clustering Algorithm1 Place each pattern in its own cluster. Construct a list of interpatterndistances for all distinct unorderedpairs of patterns, and sort this listin ascending order.2 Step through the sorted list of distances, forming for each distinct dissimilarity value dk a graph on thepatterns where pairs of patternscloser than dk are connected by agraph edge. If all the patterns aremembers of a connected graph, stop.Otherwise, repeat this step.XABC D EF GCluster1Cluster2Cluster3X12Figure 9. Points falling in three clusters.A B C D E F GSimilarityFigure 10. The dendrogram obtained usingthe singlelink algorithm.XY111111111222222Figure 11. Two concentric clusters.276  A. Jain et al.ACM Computing Surveys, Vol. 31, No. 3, September 19993 The output of the algorithm is anested hierarchy of graphs whichcan be cut at a desired dissimilaritylevel forming a partition clusteringidentified by simply connected components in the corresponding graph.Agglomerative CompleteLink Clustering Algorithm1 Place each pattern in its own cluster. Construct a list of interpatterndistances for all distinct unorderedpairs of patterns, and sort this listin ascending order.2 Step through the sorted list of distances, forming for each distinct dissimilarity value dk a graph on thepatterns where pairs of patternscloser than dk are connected by agraph edge. If all the patterns aremembers of a completely connectedgraph, stop.3 The output of the algorithm is anested hierarchy of graphs whichcan be cut at a desired dissimilaritylevel forming a partition clusteringidentified by completely connectedcomponents in the correspondinggraph.Hierarchical algorithms are more versatile than partitional algorithms. Forexample, the singlelink clustering algorithm works well on data sets containing nonisotropic clusters includingwellseparated, chainlike, and concentric clusters, whereas a typical partitional algorithm such as the kmeansalgorithm works well only on data setshaving isotropic clusters Nagy 1968.On the other hand, the time and spacecomplexities Day 1992 of the partitional algorithms are typically lowerthan those of the hierarchical algorithms. It is possible to develop hybridalgorithms Murty and Krishna 1980that exploit the good features of bothcategories.Hierarchical Agglomerative Clustering Algorithm1 Compute the proximity matrix containing the distance between eachpair of patterns. Treat each patternas a cluster.2 Find the most similar pair of clusters using the proximity matrix.Merge these two clusters into onecluster. Update the proximity matrix to reflect this merge operation.3 If all patterns are in one cluster,stop. Otherwise, go to step 2.Based on the way the proximity matrixis updated in step 2, a variety of agglomerative algorithms can be designed.Hierarchical divisive algorithms startwith a single cluster of all the givenobjects and keep splitting the clustersbased on some criterion to obtain a partition of singleton clusters.1 111111 111111 11222222222 22X111111 2222222        1X2Figure 12. A singlelink clustering of a patternset containing two classes 1 and 2 connected bya chain of noisy patterns .1 111111 111111 11222222222 22X111111 2222222        1X2Figure 13. A completelink clustering of a pattern set containing two classes 1 and 2 connected by a chain of noisy patterns .Data Clustering  277ACM Computing Surveys, Vol. 31, No. 3, September 19995.2 Partitional AlgorithmsA partitional clustering algorithm obtains a single partition of the data instead of a clustering structure, such asthe dendrogram produced by a hierarchical technique. Partitional methodshave advantages in applications involving large data sets for which the construction of a dendrogram is computationally prohibitive. A problemaccompanying the use of a partitionalalgorithm is the choice of the number ofdesired output clusters. A seminal paper Dubes 1987 provides guidance onthis key design decision. The partitionaltechniques usually produce clusters byoptimizing a criterion function definedeither locally on a subset of the patterns or globally defined over all of thepatterns. Combinatorial search of theset of possible labelings for an optimumvalue of a criterion is clearly computationally prohibitive. In practice, therefore, the algorithm is typically run multiple times with different startingstates, and the best configuration obtained from all of the runs is used as theoutput clustering.5.2.1 Squared Error Algorithms.The most intuitive and frequently usedcriterion function in partitional clustering techniques is the squared error criterion, which tends to work well withisolated and compact clusters. Thesquared error for a clustering  of apattern set  containing K clusters ise2,  5 Oj51K Oi51njixi j 2 cji2,where xi j is the ith pattern belonging tothe jth cluster and cj is the centroid ofthe jth cluster.The kmeans is the simplest and mostcommonly used algorithm employing asquared error criterion McQueen 1967.It starts with a random initial partitionand keeps reassigning the patterns toclusters based on the similarity betweenthe pattern and the cluster centers untila convergence criterion is met e.g.,there is no reassignment of any patternfrom one cluster to another, or thesquared error ceases to decrease significantly after some number of iterations.The kmeans algorithm is popular because it is easy to implement, and itstime complexity is On, where n is thenumber of patterns. A major problemwith this algorithm is that it is sensitiveto the selection of the initial partitionand may converge to a local minimum ofthe criterion function value if the initialpartition is not properly chosen. Figure14 shows seven twodimensional patterns. If we start with patterns A, B,and C as the initial means aroundwhich the three clusters are built, thenwe end up with the partition A, B,C, D, E, F, G shown by ellipses. Thesquared error criterion value is muchlarger for this partition than for thebest partition A, B, C, D, E, F, Gshown by rectangles, which yields theglobal minimum value of the squarederror criterion function for a clusteringcontaining three clusters. The correctthreecluster solution is obtained bychoosing, for example, A, D, and F asthe initial cluster means.Squared Error Clustering Method1 Select an initial partition of the patterns with a fixed number of clusters and cluster centers.2 Assign each pattern to its closestcluster center and compute the newcluster centers as the centroids ofthe clusters. Repeat this step untilconvergence is achieved, i.e., untilthe cluster membership is stable.3 Merge and split clusters based onsome heuristic information, optionally repeating step 2.kMeans Clustering Algorithm1 Choose k cluster centers to coincidewith k randomlychosen patterns ork randomly defined points insidethe hypervolume containing the pattern set.278  A. Jain et al.ACM Computing Surveys, Vol. 31, No. 3, September 19992 Assign each pattern to the closestcluster center.3 Recompute the cluster centers usingthe current cluster memberships.4 If a convergence criterion is not met,go to step 2. Typical convergencecriteria are no or minimal reassignment of patterns to new clustercenters, or minimal decrease insquared error.Several variants Anderberg 1973 ofthe kmeans algorithm have been reported in the literature. Some of themattempt to select a good initial partitionso that the algorithm is more likely tofind the global minimum value.Another variation is to permit splitting and merging of the resulting clusters. Typically, a cluster is split whenits variance is above a prespecifiedthreshold, and two clusters are mergedwhen the distance between their centroids is below another prespecifiedthreshold. Using this variant, it is possible to obtain the optimal partitionstarting from any arbitrary initial partition, provided proper threshold valuesare specified. The wellknown ISODATA Ball and Hall 1965 algorithmemploys this technique of merging andsplitting clusters. If ISODATA is giventhe ellipse partitioning shown in Figure 14 as an initial partitioning, it willproduce the optimal threecluster partitioning. ISODATA will first merge theclusters A and B,C into one clusterbecause the distance between their centroids is small and then split the clusterD,E,F,G, which has a large variance,into two clusters D,E and F,G.Another variation of the kmeans algorithm involves selecting a differentcriterion function altogether. The dynamic clustering algorithm which permits representations other than thecentroid for each cluster was proposedin Diday 1973, and Symon 1977 anddescribes a dynamic clustering approach obtained by formulating theclustering problem in the framework ofmaximumlikelihood estimation. Theregularized Mahalanobis distance wasused in Mao and Jain 1996 to obtainhyperellipsoidal clusters.5.2.2 GraphTheoretic Clustering.The bestknown graphtheoretic divisiveclustering algorithm is based on construction of the minimal spanning treeMST of the data Zahn 1971, and thendeleting the MST edges with the largestlengths to generate clusters. Figure 15depicts the MST obtained from ninetwodimensional points. By breakingthe link labeled CD with a length of 6units the edge with the maximum Euclidean length, two clusters A, B, Cand D, E, F, G, H, I are obtained. Thesecond cluster can be further dividedinto two clusters by breaking the edgeEF, which has a length of 4.5 units.The hierarchical approaches are alsorelated to graphtheoretic clustering.Singlelink clusters are subgraphs ofthe minimum spanning tree of the dataGower and Ross 1969 which are alsothe connected components Gotlieb andKumar 1968. Completelink clustersare maximal complete subgraphs, andare related to the node colorability ofgraphs Backer and Hubert 1976. Themaximal complete subgraph was considered the strictest definition of a clusterin Augustson and Minker 1970 andRaghavan and Yu 1981. A graphoriented approach for nonhierarchicalstructures and overlapping clusters isFigure 14. The kmeans algorithm is sensitiveto the initial partition.Data Clustering  279ACM Computing Surveys, Vol. 31, No. 3, September 1999presented in Ozawa 1985. The Delaunay graph DG is obtained by connecting all the pairs of points that areVoronoi neighbors. The DG contains allthe neighborhood information containedin the MST and the relative neighborhood graph RNG Toussaint 1980.5.3 MixtureResolving and ModeSeekingAlgorithmsThe mixture resolving approach to cluster analysis has been addressed in anumber of ways. The underlying assumption is that the patterns to be clustered are drawn from one of severaldistributions, and the goal is to identifythe parameters of each and perhapstheir number. Most of the work in thisarea has assumed that the individualcomponents of the mixture density areGaussian, and in this case the parameters of the individual Gaussians are tobe estimated by the procedure. Traditional approaches to this problem involve obtaining iteratively a maximumlikelihood estimate of the parametervectors of the component densities Jainand Dubes 1988.More recently, the Expectation Maximization EM algorithm a generalpurpose maximum likelihood algorithmDempster et al. 1977 for missingdataproblems has been applied to the problem of parameter estimation. A recentbook Mitchell 1997 provides an accessible description of the technique. In theEM framework, the parameters of thecomponent densities are unknown, asare the mixing parameters, and theseare estimated from the patterns. TheEM procedure begins with an initialestimate of the parameter vector anditeratively rescores the patterns againstthe mixture density produced by theparameter vector. The rescored patternsare then used to update the parameterestimates. In a clustering context, thescores of the patterns which essentiallymeasure their likelihood of being drawnfrom particular components of the mixture can be viewed as hints at the classof the pattern. Those patterns, placedby their scores in a particular component, would therefore be viewed as belonging to the same cluster.Nonparametric techniques for densitybased clustering have also been developed Jain and Dubes 1988. Inspiredby the Parzen window approach to nonparametric density estimation, the corresponding clustering proceduresearches for bins with large counts in amultidimensional histogram of the input pattern set. Other approaches include the application of another partitional or hierarchical clusteringalgorithm using a distance measurebased on a nonparametric density estimate.5.4 Nearest Neighbor ClusteringSince proximity plays a key role in ourintuitive notion of a cluster, nearestneighbor distances can serve as the basis of clustering procedures. An iterative procedure was proposed in Lu andFu 1978 it assigns each unlabeledpattern to the cluster of its nearest labeled neighbor pattern, provided thedistance to that labeled neighbor is below a threshold. The process continuesuntil all patterns are labeled or no additional labelings occur. The mutualneighborhood value described earlier inthe context of distance computation canalso be used to grow clusters from nearneighbors.XBAC DEFG HIedge with the maximum length226 2.34.52 2 2X21Figure 15. Using the minimal spanning tree toform clusters.280  A. Jain et al.ACM Computing Surveys, Vol. 31, No. 3, September 19995.5 Fuzzy ClusteringTraditional clustering approaches generate partitions in a partition, eachpattern belongs to one and only onecluster. Hence, the clusters in a hardclustering are disjoint. Fuzzy clusteringextends this notion to associate eachpattern with every cluster using a membership function Zadeh 1965. The output of such algorithms is a clustering,but not a partition. We give a highlevelpartitional fuzzy clustering algorithmbelow.Fuzzy Clustering Algorithm1 Select an initial fuzzy partition ofthe N objects into K clusters byselecting the N 3 K membershipmatrix U. An element uij of thismatrix represents the grade of membership of object xi in cluster cj.Typically, uij  0,1.2 Using U, find the value of a fuzzycriterion function, e.g., a weightedsquared error criterion function, associated with the corresponding partition. One possible fuzzy criterionfunction isE2, U 5 Oi51N Ok51Kuijixi 2 cki2,where ck 5 i51Nuikxi is the kth fuzzycluster center.Reassign patterns to clusters to reduce this criterion function valueand recompute U.3 Repeat step 2 until entries in U donot change significantly.In fuzzy clustering, each cluster is afuzzy set of all the patterns. Figure 16illustrates the idea. The rectangles enclose two hard clusters in the dataH1 5 1,2,3,4,5 and H2 5 6,7,8,9.A fuzzy clustering algorithm might produce the two fuzzy clusters F1 and F2depicted by ellipses. The patterns willhave membership values in 0,1 foreach cluster. For example, fuzzy clusterF1 could be compactly described as1,0.9, 2,0.8, 3,0.7, 4,0.6, 5,0.55,6,0.2, 7,0.2, 8,0.0, 9,0.0and F2 could be described as1,0.0, 2,0.0, 3,0.0, 4,0.1, 5,0.15,6,0.4, 7,0.35, 8,1.0, 9,0.9The ordered pairs i, m i in each clusterrepresent the ith pattern and its membership value to the cluster m i. Largermembership values indicate higher confidence in the assignment of the patternto the cluster. A hard clustering can beobtained from a fuzzy partition bythresholding the membership value.Fuzzy set theory was initially appliedto clustering in Ruspini 1969. Thebook by Bezdek 1981 is a good sourcefor material on fuzzy clustering. Themost popular fuzzy clustering algorithmis the fuzzy cmeans FCM algorithm.Even though it is better than the hardkmeans algorithm at avoiding localminima, FCM can still converge to localminima of the squared error criterion.The design of membership functions isthe most important problem in fuzzyclustering different choices includeXY123 45 6789H 2HFF11 2Figure 16. Fuzzy clusters.Data Clustering  281ACM Computing Surveys, Vol. 31, No. 3, September 1999those based on similarity decompositionand centroids of clusters. A generalization of the FCM algorithm was proposedby Bezdek 1981 through a family ofobjective functions. A fuzzy cshell algorithm and an adaptive variant for detecting circular and elliptical boundaries was presented in Dave 1992.5.6 Representation of ClustersIn applications where the number ofclasses or clusters in a data set must bediscovered, a partition of the data set isthe end product. Here, a partition givesan idea about the separability of thedata points into clusters and whether itis meaningful to employ a supervisedclassifier that assumes a given numberof classes in the data set. However, inmany other applications that involvedecision making, the resulting clustershave to be represented or described in acompact form to achieve data abstraction. Even though the construction of acluster representation is an importantstep in decision making, it has not beenexamined closely by researchers. Thenotion of cluster representation was introduced in Duran and Odell 1974 andwas subsequently studied in Diday andSimon 1976 and Michalski et al.1981. They suggested the followingrepresentation schemes1 Represent a cluster of points bytheir centroid or by a set of distantpoints in the cluster. Figure 17 depicts these two ideas.2 Represent clusters using nodes in aclassification tree. This is illustrated in Figure 18.3 Represent clusters by using conjunctive logical expressions. For example,the expression X1 . 3X2 , 2 inFigure 18 stands for the logical statement X1 is greater than 3 and X2 isless than 2.Use of the centroid to represent acluster is the most popular scheme. Itworks well when the clusters are compact or isotropic. However, when theclusters are elongated or nonisotropic,then this scheme fails to represent themproperly. In such a case, the use of acollection of boundary points in a cluster captures its shape well. The numberof points used to represent a clustershould increase as the complexity of itsshape increases. The two different representations illustrated in Figure 18 areequivalent. Every path in a classification tree from the root node to a leafnode corresponds to a conjunctive statement. An important limitation of thetypical use of the simple conjunctiveconcept representations is that they candescribe only rectangular or isotropicclusters in the feature space.Data abstraction is useful in decisionmaking because of the following1 It gives a simple and intuitive description of clusters which is easyfor human comprehension. In bothconceptual clustering MichalskiX XBy Three Distant PointsBy The Centroid  1X2 X21Figure 17. Representation of a cluster by points.282  A. Jain et al.ACM Computing Surveys, Vol. 31, No. 3, September 1999and Stepp 1983 and symbolic clustering Gowda and Diday 1992 thisrepresentation is obtained withoutusing an additional step. These algorithms generate the clusters aswell as their descriptions. A set offuzzy rules can be obtained fromfuzzy clusters of a data set. Theserules can be used to build fuzzy classifiers and fuzzy controllers.2 It helps in achieving data compression that can be exploited further bya computer Murty and Krishna1980. Figure 19a shows samplesbelonging to two chainlike clusterslabeled 1 and 2. A partitional clustering like the kmeans algorithmcannot separate these two structures properly. The singlelink algorithm works well on this data, but iscomputationally expensive. So a hybrid approach may be used to exploit the desirable properties of boththese algorithms. We obtain 8 subclusters of the data using the computationally efficient kmeans algorithm. Each of these subclusters canbe represented by their centroids asshown in Figure 19a. Now the singlelink algorithm can be applied onthese centroids alone to clusterthem into 2 groups. The resultinggroups are shown in Figure 19b.Here, a data reduction is achievedby representing the subclusters bytheir centroids.3 It increases the efficiency of the decision making task. In a clusterbased document retrieval techniqueSalton 1991, a large collection ofdocuments is clustered and each ofthe clusters is represented using itscentroid. In order to retrieve documents relevant to a query, the queryis matched with the cluster centroids rather than with all the documents. This helps in retrieving relevant documents efficiently. Also inseveral applications involving largedata sets, clustering is used to perform indexing, which helps in efficient decision making Dorai andJain 1995.5.7 Artificial Neural Networks forClusteringArtificial neural networks ANNsHertz et al. 1991 are motivated bybiological neural networks. ANNs havebeen used extensively over the pastthree decades for both classification andclustering Sethi and Jain 1991 Jainand Mao 1994. Some of the features ofthe ANNs that are important in patternclustering areX0 1 2 3 4 5012345          22222223333333333311 111111111 111 111111111X  3             X 31              2               3Using Nodes in a Classification TreeUsing Conjunctive StatementsX211 X 3 2 X 3X 2 3X 3X 2111X  2          X  22 21 2 1 2Figure 18. Representation of clusters by a classification tree or by conjunctive statements.Data Clustering  283ACM Computing Surveys, Vol. 31, No. 3, September 19991 ANNs process numerical vectors andso require patterns to be representedusing quantitative features only.2 ANNs are inherently parallel anddistributed processing architectures.3 ANNs may learn their interconnection weights adaptively Jain andMao 1996 Oja 1982. More specifically, they can act as pattern normalizers and feature selectors byappropriate selection of weights.Competitive or winnertakeallneural networks Jain and Mao 1996are often used to cluster input data. Incompetitive learning, similar patternsare grouped by the network and represented by a single unit neuron. Thisgrouping is done automatically based ondata correlations. Wellknown examplesof ANNs used for clustering include Kohonens learning vector quantizationLVQ and selforganizing map SOMKohonen 1984, and adaptive resonance theory models Carpenter andGrossberg 1990. The architectures ofthese ANNs are simple they are singlelayered. Patterns are presented at theinput and are associated with the output nodes. The weights between the input nodes and the output nodes areiteratively changed this is called learning until a termination criterion is satisfied. Competitive learning has beenfound to exist in biological neural networks. However, the learning or weightupdate procedures are quite similar tothose in some classical clustering approaches. For example, the relationshipbetween the kmeans algorithm andLVQ is addressed in Pal et al. 1993.The learning algorithm in ART modelsis similar to the leader clustering algorithm Moor 1988.The SOM gives an intuitively appealing twodimensional map of the multidimensional data set, and it has beensuccessfully used for vector quantization and speech recognition Kohonen1984. However, like its sequentialcounterpart, the SOM generates a suboptimal partition if the initial weightsare not chosen properly. Further, itsconvergence is controlled by various parameters such as the learning rate anda neighborhood of the winning node inwhich learning takes place. It is possible that a particular input pattern canfire different output units at differentiterations this brings up the stabilityissue of learning systems. The system issaid to be stable if no pattern in thetraining data changes its category aftera finite number of learning iterations.This problem is closely associated withthe problem of plasticity, which is theability of the algorithm to adapt to newdata. For stability, the learning rateshould be decreased to zero as iterationsprogress and this affects the plasticity.The ART models are supposed to bestable and plastic Carpenter andGrossberg 1990. However, ART netsare orderdependent that is, differentpartitions are obtained for different orders in which the data is presented tothe net. Also, the size and number ofclusters generated by an ART net depend on the value chosen for the vigilance threshold, which is used to decidewhether a pattern is to be assigned toone of the existing clusters or start anew cluster. Further, both SOM andART are suitable for detecting only hyperspherical clusters Hertz et al. 1991.A twolayer network that employs regularized Mahalanobis distance to extracthyperellipsoidal clusters was proposedin Mao and Jain 1994. All these ANNsuse a fixed number of output nodes1111 2222 111122X X1 1 1111122111111111111111 1 111111122 222222222222222 2 2222 22222 222221111 111111a b1 1X2 X2Figure 19. Data compression by clustering.284  A. Jain et al.ACM Computing Surveys, Vol. 31, No. 3, September 1999which limit the number of clusters thatcan be produced.5.8 Evolutionary Approaches forClusteringEvolutionary approaches, motivated bynatural evolution, make use of evolutionary operators and a population ofsolutions to obtain the globally optimalpartition of the data. Candidate solutions to the clustering problem are encoded as chromosomes. The most commonly used evolutionary operators areselection, recombination, and mutation.Each transforms one or more inputchromosomes into one or more outputchromosomes. A fitness function evaluated on a chromosome determines achromosomes likelihood of survivinginto the next generation. We give belowa highlevel description of an evolutionary algorithm applied to clustering.An Evolutionary Algorithm forClustering1 Choose a random population of solutions. Each solution here corresponds to a valid kpartition of thedata. Associate a fitness value witheach solution. Typically, fitness isinversely proportional to thesquared error value. A solution witha small squared error will have alarger fitness value.2 Use the evolutionary operators selection, recombination and mutationto generate the next population ofsolutions. Evaluate the fitness values of these solutions.3 Repeat step 2 until some termination condition is satisfied.The bestknown evolutionary techniques are genetic algorithms GAsHolland 1975 Goldberg 1989, evolution strategies ESs Schwefel 1981,and evolutionary programming EPFogel et al. 1965. Out of these threeapproaches, GAs have been most frequently used in clustering. Typically,solutions are binary strings in GAs. InGAs, a selection operator propagates solutions from the current generation tothe next generation based on their fitness. Selection employs a probabilisticscheme so that solutions with higherfitness have a higher probability of getting reproduced.There are a variety of recombinationoperators in use crossover is the mostpopular. Crossover takes as input a pairof chromosomes called parents andoutputs a new pair of chromosomescalled children or offspring as depictedin Figure 20. In Figure 20, a singlepoint crossover operation is depicted. Itexchanges the segments of the parentsacross a crossover point. For example,in Figure 20, the parents are the binarystrings 10110101 and 11001110. Thesegments in the two parents after thecrossover point between the fourth andfifth locations are exchanged to produce the child chromosomes. Mutationtakes as input a chromosome and outputs a chromosome by complementingthe bit value at a randomly selectedlocation in the input chromosome. Forexample, the string 11111110 is generated by applying the mutation operatorto the second bit location in the string10111110 starting at the left. Bothcrossover and mutation are applied withsome prespecified probabilities whichdepend on the fitness values.GAs represent points in the searchspace as binary strings, and rely on theparent1parent2child1child21      0       1      1      0       1     0       11       0       1      1      1      1       1      01       1      0       0      0      1      0     11       1       0      0      1       1      1      0crossover pointFigure 20. Crossover operation.Data Clustering  285ACM Computing Surveys, Vol. 31, No. 3, September 1999crossover operator to explore the searchspace. Mutation is used in GAs for thesake of completeness, that is, to makesure that no part of the search space isleft unexplored. ESs and EP differ fromthe GAs in solution representation andtype of the mutation operator used EPdoes not use a recombination operator,but only selection and mutation. Each ofthese three approaches have been usedto solve the clustering problem by viewing it as a minimization of the squarederror criterion. Some of the theoreticalissues such as the convergence of theseapproaches were studied in Fogel andFogel 1994.GAs perform a globalized search forsolutions whereas most other clusteringprocedures perform a localized search.In a localized search, the solution obtained at the next iteration of the procedure is in the vicinity of the currentsolution. In this sense, the kmeans algorithm, fuzzy clustering algorithms,ANNs used for clustering, various annealing schemes see below, and tabusearch are all localized search techniques. In the case of GAs, the crossoverand mutation operators can producenew solutions that are completely different from the current ones. We illustrate this fact in Figure 21. Let us assume that the scalar X is coded using a5bit binary representation, and let S1and S2 be two points in the onedimensional search space. The decimal valuesof S1 and S2 are 8 and 31, respectively.Their binary representations are S1 501000 and S2 5 11111. Let us applythe singlepoint crossover to thesestrings, with the crossover site fallingbetween the second and third most significant bits as shown below.0100011111This will produce a new pair of points orchromosomes S3 and S4 as shown inFigure 21. Here, S3 5 01111 andS4 5 11000. The corresponding decimal values are 15 and 24, respectively.Similarly, by mutating the most significant bit in the binary string 01111 decimal 15, the binary string 11111 decimal 31 is generated. These jumps, orgaps between points in successive generations, are much larger than thoseproduced by other approaches.Perhaps the earliest paper on the useof GAs for clustering is by Raghavanand Birchand 1979, where a GA wasused to minimize the squared error of aclustering. Here, each point or chromosome represents a partition of N objectsinto K clusters and is represented by aKary string of length N. For example,consider six patternsA, B, C, D, E,and Fand the string 101001. This sixbit binary K 5 2 string corresponds toplacing the six patterns into two clusters. This string represents a twopartition, where one cluster has the first,third, and sixth patterns and the secondcluster has the remaining patterns. Inother words, the two clusters areA,C,F and B,D,E the sixbit binarystring 010110 represents the same clustering of the six patterns. When thereare K clusters, there are K differentchromosomes corresponding to eachKpartition of the data. This increasesthe effective search space size by a factor of K. Further, if crossover is appliedon two good chromosomes, the resultingfXXSS S S1 23 4X XXXFigure 21. GAs perform globalized search.286  A. Jain et al.ACM Computing Surveys, Vol. 31, No. 3, September 1999offspring may be inferior in this representation. For example, let A,B,C andD,E,F be the clusters in the optimal2partition of the six patterns considered above. The corresponding chromosomes are 111000 and 000111. By applying singlepoint crossover at thelocation between the third and fourthbit positions on these two strings, weget 111111 and 000000 as offspring andboth correspond to an inferior partition.These problems have motivated researchers to design better representation schemes and crossover operators.In Bhuyan et al. 1991, an improvedrepresentation scheme is proposedwhere an additional separator symbol isused along with the pattern labels torepresent a partition. Let the separatorsymbol be represented by . Then thechromosome ACFBDE corresponds to a2partition A,C,F and B,D,E. Usingthis representation permits them tomap the clustering problem into a permutation problem such as the travelingsalesman problem, which can be solvedby using the permutation crossover operators Goldberg 1989. This solutionalso suffers from permutation redundancy. There are 72 equivalent chromosomes permutations corresponding tothe same partition of the data into thetwo clusters A,C,F and B,D,E.More recently, Jones and Beltramo1991 investigated the use of edgebased crossover Whitley et al. 1989 tosolve the clustering problem. Here, allpatterns in a cluster are assumed toform a complete graph by connectingthem with edges. Offspring are generated from the parents so that they inherit the edges from their parents. It isobserved that this crossover operatortakes OK6 1 N time for N patternsand K clusters ruling out its applicability on practical data sets having morethan 10 clusters. In a hybrid approachproposed in Babu and Murty 1993, theGA is used only to find good initialcluster centers and the kmeans algorithm is applied to find the final partition. This hybrid approach performedbetter than the GA.A major problem with GAs is theirsensitivity to the selection of variousparameters such as population size,crossover and mutation probabilities,etc. Grefenstette Grefenstette 1986has studied this problem and suggestedguidelines for selecting these control parameters. However, these guidelinesmay not yield good results on specificproblems like pattern clustering. It wasreported in Jones and Beltramo 1991that hybrid genetic algorithms incorporating problemspecific heuristics aregood for clustering. A similar claim ismade in Davis 1991 about the applicability of GAs to other practical problems. Another issue with GAs is theselection of an appropriate representation which is low in order and short indefining length.It is possible to view the clusteringproblem as an optimization problemthat locates the optimal centroids of theclusters directly rather than finding anoptimal partition using a GA. This viewpermits the use of ESs and EP, becausecentroids can be coded easily in boththese approaches, as they support thedirect representation of a solution as arealvalued vector. In Babu and Murty1994, ESs were used on both hard andfuzzy clustering problems and EP hasbeen used to evolve fuzzy minmax clusters Fogel and Simpson 1993. It hasbeen observed that they perform betterthan their classical counterparts, thekmeans algorithm and the fuzzycmeans algorithm. However, all ofthese approaches suffer as do GAs andANNs from sensitivity to control parameter selection. For each specificproblem, one has to tune the parametervalues to suit the application.5.9 SearchBased ApproachesSearch techniques used to obtain theoptimum value of the criterion functionare divided into deterministic and stochastic search techniques. DeterminisData Clustering  287ACM Computing Surveys, Vol. 31, No. 3, September 1999tic search techniques guarantee an optimal partition by performing exhaustiveenumeration. On the other hand, thestochastic search techniques generate anearoptimal partition reasonablyquickly, and guarantee convergence tooptimal partition asymptotically.Among the techniques considered so far,evolutionary approaches are stochasticand the remainder are deterministic.Other deterministic approaches to clustering include the branchandboundtechnique adopted in Koontz et al.1975 and Cheng 1995 for generatingoptimal partitions. This approach generates the optimal partition of the dataat the cost of excessive computationalrequirements. In Rose et al. 1993, adeterministic annealing approach wasproposed for clustering. This approachemploys an annealing technique inwhich the error surface is smoothed, butconvergence to the global optimum isnot guaranteed. The use of deterministic annealing in proximitymode clustering where the patterns are specified interms of pairwise proximities ratherthan multidimensional points was explored in Hofmann and Buhmann1997 later work applied the deterministic annealing approach to texture segmentation Hofmann and Buhmann1998.The deterministic approaches are typically greedy descent approaches,whereas the stochastic approaches permit perturbations to the solutions innonlocally optimal directions also withnonzero probabilities. The stochasticsearch techniques are either sequentialor parallel, while evolutionary approaches are inherently parallel. Thesimulated annealing approach SAKirkpatrick et al. 1983 is a sequentialstochastic search technique, whose applicability to clustering is discussed inKlein and Dubes 1989. Simulated annealing procedures are designed toavoid or recover from solutions whichcorrespond to local optima of the objective functions. This is accomplished byaccepting with some probability a newsolution for the next iteration of lowerquality as measured by the criterionfunction. The probability of acceptanceis governed by a critical parametercalled the temperature by analogy withannealing in metals, which is typicallyspecified in terms of a starting firstiteration and final temperature value.Selim and AlSultan 1991 studied theeffects of control parameters on the performance of the algorithm, and BaezaYates 1992 used SA to obtain nearoptimal partition of the data. SA isstatistically guaranteed to find the global optimal solution Aarts and Korst1989. A highlevel outline of a SAbased algorithm for clustering is givenbelow.Clustering Based on SimulatedAnnealing1 Randomly select an initial partitionand P0, and compute the squarederror value, EP0. Select values forthe control parameters, initial andfinal temperatures T0 and Tf.2 Select a neighbor P1 of P0 and compute its squared error value, EP1. IfEP1 is larger than EP0, then assignP1 to P0 with a temperaturedependent probability. Else assign P1 toP0. Repeat this step for a fixed number of iterations.3 Reduce the value of T0, i.e. T0 5cT0, where c is a predeterminedconstant. If T0 is greater than Tf,then go to step 2. Else stop.The SA algorithm can be slow inreaching the optimal solution, becauseoptimal results require the temperatureto be decreased very slowly from iteration to iteration.Tabu search Glover 1986, like SA, isa method designed to cross boundariesof feasibility or local optimality and tosystematically impose and release constraints to permit exploration of otherwise forbidden regions. Tabu searchwas used to solve the clustering problem in AlSultan 1995.288  A. Jain et al.ACM Computing Surveys, Vol. 31, No. 3, September 19995.10 A Comparison of TechniquesIn this section we have examined various deterministic and stochastic searchtechniques to approach the clusteringproblem as an optimization problem. Amajority of these methods use thesquared error criterion function. Hence,the partitions generated by these approaches are not as versatile as thosegenerated by hierarchical algorithms.The clusters generated are typically hyperspherical in shape. Evolutionary approaches are globalized search techniques, whereas the rest of theapproaches are localized search technique. ANNs and GAs are inherentlyparallel, so they can be implementedusing parallel hardware to improvetheir speed. Evolutionary approachesare populationbased that is, theysearch using more than one solution ata time, and the rest are based on usinga single solution at a time. ANNs, GAs,SA, and Tabu search TS are all sensitive to the selection of various learningcontrol parameters. In theory, all four ofthese methods are weak methods Rich1983 in that they do not use explicitdomain knowledge. An important feature of the evolutionary approaches isthat they can find the optimal solutioneven when the criterion function is discontinuous.An empirical study of the performance of the following heuristics forclustering was presented in Mishra andRaghavan 1994 SA, GA, TS, randomized branchandbound RBA Mishraand Raghavan 1994, and hybrid searchHS strategies Ismail and Kamel 1989were evaluated. The conclusion wasthat GA performs well in the case ofonedimensional data, while its performance on high dimensional data sets isnot impressive. The performance of SAis not attractive because it is very slow.RBA and TS performed best. HS is goodfor high dimensional data. However,none of the methods was found to besuperior to others by a significant margin. An empirical study of kmeans, SA,TS, and GA was presented in AlSultanand Khan 1996. TS, GA and SA werejudged comparable in terms of solutionquality, and all were better thankmeans. However, the kmeans methodis the most efficient in terms of execution time other schemes took more timeby a factor of 500 to 2500 to partition adata set of size 60 into 5 clusters. Further, GA encountered the best solutionfaster than TS and SA SA took moretime than TS to encounter the best solution. However, GA took the maximumtime for convergence, that is, to obtain apopulation of only the best solutions,followed by TS and SA. An importantobservation is that in both Mishra andRaghavan 1994 and AlSultan andKhan 1996 the sizes of the data setsconsidered are small that is, fewer than200 patterns.A twolayer network was employed inMao and Jain 1996, with the firstlayer including a number of principalcomponent analysis subnets, and thesecond layer using a competitive net.This network performs partitional clustering using the regularized Mahalanobis distance. This net was trained usinga set of 1000 randomly selected pixelsfrom a large image and then used toclassify every pixel in the image. Babuet al. 1997 proposed a stochastic connectionist approach SCA and compared its performance on standard datasets with both the SA and kmeans algorithms. It was observed that SCA issuperior to both SA and kmeans interms of solution quality. Evolutionaryapproaches are good only when the datasize is less than 1000 and for low dimensional data.In summary, only the kmeans algorithm and its ANN equivalent, the Kohonen net Mao and Jain 1996 havebeen applied on large data sets otherapproaches have been tested, typically,on small data sets. This is because obtaining suitable learningcontrol parameters for ANNs, GAs, TS, and SA isdifficult and their execution times arevery high for large data sets. However,it has been shown Selim and IsmailData Clustering  289ACM Computing Surveys, Vol. 31, No. 3, September 19991984 that the kmeans method converges to a locally optimal solution. Thisbehavior is linked with the initial seedselection in the kmeans algorithm. Soif a good initial partition can be obtained quickly using any of the othertechniques, then kmeans would workwell even on problems with large datasets. Even though various methods discussed in this section are comparativelyweak, it was revealed through experimental studies that combining domainknowledge would improve their performance. For example, ANNs work betterin classifying images represented usingextracted features than with raw images, and hybrid classifiers work betterthan ANNs Mohiuddin and Mao 1994.Similarly, using domain knowledge tohybridize a GA improves its performance Jones and Beltramo 1991. So itmay be useful in general to use domainknowledge along with approaches likeGA, SA, ANN, and TS. However, theseapproaches specifically, the criteriafunctions used in them have a tendencyto generate a partition of hyperspherical clusters, and this could be a limitation. For example, in clusterbased document retrieval, it was observed thatthe hierarchical algorithms performedbetter than the partitional algorithmsRasmussen 1992.5.11 Incorporating Domain Constraints inClusteringAs a task, clustering is subjective innature. The same data set may need tobe partitioned differently for differentpurposes. For example, consider awhale, an elephant, and a tuna fishWatanabe 1985. Whales and elephantsform a cluster of mammals. However, ifthe user is interested in partitioningthem based on the concept of living inwater, then whale and tuna fish areclustered together. Typically, this subjectivity is incorporated into the clustering criterion by incorporating domainknowledge in one or more phases ofclustering.Every clustering algorithm uses sometype of knowledge either implicitly orexplicitly. Implicit knowledge plays arole in 1 selecting a pattern representation scheme e.g., using ones priorexperience to select and encode features, 2 choosing a similarity measuree.g., using the Mahalanobis distanceinstead of the Euclidean distance to obtain hyperellipsoidal clusters, and 3selecting a grouping scheme e.g., specifying the kmeans algorithm when it isknown that clusters are hyperspherical. Domain knowledge is used implicitly in ANNs, GAs, TS, and SA to selectthe controllearning parameter valuesthat affect the performance of these algorithms.It is also possible to use explicitlyavailable domain knowledge to constrain or guide the clustering process.Such specialized clustering algorithmshave been used in several applications.Domain concepts can play several rolesin the clustering process, and a varietyof choices are available to the practitioner. At one extreme, the available domain concepts might easily serve as anadditional feature or several, and theremainder of the procedure might beotherwise unaffected. At the other extreme, domain concepts might be usedto confirm or veto a decision arrived atindependently by a traditional clustering algorithm, or used to affect the computation of distance in a clustering algorithm employing proximity. Theincorporation of domain knowledge intoclustering consists mainly of ad hoc approaches with little in common accordingly, our discussion of the idea willconsist mainly of motivational materialand a brief survey of past work. Machine learning research and pattern recognition research intersect in this topical area, and the interested reader isreferred to the prominent journals inmachine learning e.g., Machine Learning, J. of AI Research, or Artificial Intelligence for a fuller treatment of thistopic.290  A. Jain et al.ACM Computing Surveys, Vol. 31, No. 3, September 1999As documented in Cheng and Fu1985, rules in an expert system maybe clustered to reduce the size of theknowledge base. This modification ofclustering was also explored in the domains of universities, congressional voting records, and terrorist events by Lebowitz 1987.5.11.1 Similarity Computation. Conceptual knowledge was used explicitlyin the similarity computation phase inMichalski and Stepp 1983. It was assumed that the pattern representationswere available and the dynamic clustering algorithm Diday 1973 was used togroup patterns. The clusters formedwere described using conjunctive statements in predicate logic. It was statedin Stepp and Michalski 1986 andMichalski and Stepp 1983 that thegroupings obtained by the conceptualclustering are superior to those obtained by the numerical methods forclustering. A critical analysis of thatwork appears in Dale 1985, and it wasobserved that monothetic divisive clustering algorithms generate clusters thatcan be described by conjunctive statements. For example, consider Figure 8.Four clusters in this figure, obtainedusing a monothetic algorithm, can bedescribed by using conjunctive conceptsas shown belowCluster 1 X  a  Y  bCluster 2 X  a  Y . bCluster 3 X . a  Y . cCluster 4 X . a  Y  cwhere  is the Boolean conjunctionand operator, and a, b, and c areconstants.5.11.2 Pattern Representation. It wasshown in Srivastava and Murty 1990that by using knowledge in the patternrepresentation phase, as is implicitlydone in numerical taxonomy approaches, it is possible to obtain thesame partitions as those generated byconceptual clustering. In this sense,conceptual clustering and numericaltaxonomy are not diametrically opposite, but are equivalent. In the case ofconceptual clustering, domain knowledge is explicitly used in interpatternsimilarity computation, whereas in numerical taxonomy it is implicitly assumed that pattern representations areobtained using the domain knowledge.5.11.3 Cluster Descriptions. Typically, in knowledgebased clustering,both the clusters and their descriptionsor characterizations are generatedFisher and Langley 1985. There aresome exceptions, for instance,, Gowdaand Diday 1992, where only clusteringis performed and no descriptions aregenerated explicitly. In conceptual clustering, a cluster of objects is describedby a conjunctive logical expressionMichalski and Stepp 1983. Eventhough a conjunctive statement is one ofthe most common descriptive formsused by humans, it is a limited form. InShekar et al. 1987, functional knowledge of objects was used to generatemore intuitively appealing cluster descriptions that employ the Boolean implication operator. A system that represents clusters probabilistically wasdescribed in Fisher 1987 these descriptions are more general than conjunctive concepts, and are wellsuited tohierarchical classification domains e.g.,the animal species hierarchy. A conceptual clustering system in which clustering is done first is described in Fisherand Langley 1985. These clusters arethen described using probabilities. Asimilar scheme was described in Murtyand Jain 1995, but the descriptionsare logical expressions that employ bothconjunction and disjunction.An important characteristic of conceptual clustering is that it is possible togroup objects represented by both qualitative and quantitative features if theclustering leads to a conjunctive concept. For example, the concept cricketball might be represented asData Clustering  291ACM Computing Surveys, Vol. 31, No. 3, September 1999color 5 red  shape 5 sphere make 5 leather radius 5 1.4 inches,where radius is a quantitative featureand the rest are all qualitative features.This description is used to describe acluster of cricket balls. In Stepp andMichalski 1986, a graph the goal dependency network was used to groupstructured objects. In Shekar et al.1987 functional knowledge was usedto group manmade objects. Functionalknowledge was represented usingandor trees Rich 1983. For example,the function cooking shown in Figure 22can be decomposed into functions likeholding and heating the material in aliquid medium. Each manmade objecthas a primary function for which it isproduced. Further, based on its features, it may serve additional functions.For example, a book is meant for reading, but if it is heavy then it can also beused as a paper weight. In Sutton et al.1993, object functions were used toconstruct generic recognition systems.5.11.4 Pragmatic Issues. Any implementation of a system that explicitlyincorporates domain concepts into aclustering technique has to address thefollowing important pragmatic issues1 Representation, availability andcompleteness of domain concepts.2 Construction of inferences using theknowledge.3 Accommodation of changing or dynamic knowledge.In some domains, complete knowledgeis available explicitly. For example, theACM Computing Reviews classificationtree used in Murty and Jain 1995 iscomplete and is explicitly available foruse. In several domains, knowledge isincomplete and is not available explicitly. Typically, machine learning techniques are used to automatically extractknowledge, which is a difficult and challenging problem. The most prominentlyused learning method is learning fromexamples Quinlan 1990. This is aninductive learning scheme used to acquire knowledge from examples of eachof the classes in different domains. Evenif the knowledge is available explicitly,it is difficult to find out whether it iscomplete and sound. Further, it is extremely difficult to verify soundnessand completeness of knowledge extracted from practical data sets, because such knowledge cannot be represented in propositional logic. It ispossible that both the data and knowledge keep changing with time. For example, in a library, new books might getadded and some old books might bedeleted from the collection with time.Also, the classification system knowledge employed by the library is updated periodically.A major problem with knowledgebased clustering is that it has not beenapplied to large data sets or in domainswith large knowledge bases. Typically,the number of objects grouped was lessthan 1000, and number of rules used asa part of the knowledge was less than100. The most difficult problem is to usea very large knowledge base for clustering objects in several practical problemsincluding data mining, image segmentation, and document retrieval.5.12 Clustering Large Data SetsThere are several applications where itis necessary to cluster a large collectionof patterns. The definition of large hasvaried and will continue to do so withchanges in technology e.g., memory andprocessing time. In the 1960s, largecookingheating liquid holdingelectric        ...                             water ... metallic ... Figure 22. Functional knowledge.292  A. Jain et al.ACM Computing Surveys, Vol. 31, No. 3, September 1999meant several thousand patterns Ross1968 now, there are applicationswhere millions of patterns of high dimensionality have to be clustered. Forexample, to segment an image of size500 3 500 pixels, the number of pixelsto be clustered is 250,000. In documentretrieval and information filtering, millions of patterns with a dimensionalityof more than 100 have to be clustered toachieve data abstraction. A majority ofthe approaches and algorithms proposed in the literature cannot handlesuch large data sets. Approaches basedon genetic algorithms, tabu search andsimulated annealing are optimizationtechniques and are restricted to reasonably small data sets. Implementationsof conceptual clustering optimize somecriterion functions and are typicallycomputationally expensive.The convergent kmeans algorithmand its ANN equivalent, the Kohonennet, have been used to cluster largedata sets Mao and Jain 1996. The reasons behind the popularity of thekmeans algorithm are1 Its time complexity is Onkl,where n is the number of patterns,k is the number of clusters, and l isthe number of iterations taken bythe algorithm to converge. Typically, k and l are fixed in advanceand so the algorithm has linear timecomplexity in the size of the data setDay 1992.2 Its space complexity is Ok 1 n. Itrequires additional space to storethe data matrix. It is possible tostore the data matrix in a secondarymemory and access each patternbased on need. However, thisscheme requires a huge access timebecause of the iterative nature ofthe algorithm, and as a consequenceprocessing time increases enormously.3 It is orderindependent for a giveninitial seed set of cluster centers, itgenerates the same partition of thedata irrespective of the order inwhich the patterns are presented tothe algorithm.However, the kmeans algorithm is sensitive to initial seed selection and evenin the best case, it can produce onlyhyperspherical clusters.Hierarchical algorithms are more versatile. But they have the following disadvantages1 The time complexity of hierarchicalagglomerative algorithms is On2log n Kurita 1991. It is possibleto obtain singlelink clusters usingan MST of the data, which can beconstructed in On log2 n time fortwodimensional data Choudhuryand Murty 1990.2 The space complexity of agglomerative algorithms is On2. This is because a similarity matrix of sizen 3 n has to be stored. To clusterevery pixel in a 100 3 100 image,approximately 200 megabytes ofstorage would be required assuningsingleprecision storage of similarities. It is possible to compute theentries of this matrix based on needinstead of storing them this wouldincrease the algorithms time complexity Anderberg 1973.Table I lists the time and space complexities of several wellknown algorithms. Here, n is the number of patterns to be clustered, k is the number ofclusters, and l is the number of iterations.Table I. Complexity of Clustering AlgorithmsClustering Algorithm TimeComplexitySpaceComplexityleader Okn Okkmeans Onkl OkISODATA Onkl Okshortest spanning path On2 Onsingleline On2 log n On2completeline On2 log n On2Data Clustering  293ACM Computing Surveys, Vol. 31, No. 3, September 1999A possible solution to the problem ofclustering large data sets while onlymarginally sacrificing the versatility ofclusters is to implement more efficientvariants of clustering algorithms. A hybrid approach was used in Ross 1968,where a set of reference points is chosenas in the kmeans algorithm, and eachof the remaining data points is assignedto one or more reference points or clusters. Minimal spanning trees MST areobtained for each group of points separately. These MSTs are merged to forman approximate global MST. This approach computes similarities betweenonly a fraction of all possible pairs ofpoints. It was shown that the number ofsimilarities computed for 10,000 patterns using this approach is the same asthe total number of pairs of points in acollection of 2,000 points. Bentley andFriedman 1978 contains an algorithmthat can compute an approximate MSTin On log n time. A scheme to generate an approximate dendrogram incrementally in On log n time was presented in Zupan 1982, whileVenkateswarlu and Raju 1992 proposed an algorithm to speed up the ISODATA clustering algorithm. A study ofthe approximate singlelinkage clusteranalysis of large data sets was reportedin Eddy et al. 1994. In that work, anapproximate MST was used to form singlelink clusters of a data set of size40,000.The emerging discipline of data mining discussed as an application in Section 6 has spurred the development ofnew algorithms for clustering large datasets. Two algorithms of note are theCLARANS algorithm developed by Ngand Han 1994 and the BIRCH algorithm proposed by Zhang et al. 1996.CLARANS Clustering Large Applications based on RANdom Search identifies candidate cluster centroids throughanalysis of repeated random samplesfrom the original data. Because of theuse of random sampling, the time complexity is On for a pattern set of nelements. The BIRCH algorithm Balanced Iterative Reducing and Clustering stores summary information aboutcandidate clusters in a dynamic treedata structure. This tree hierarchicallyorganizes the clusterings represented atthe leaf nodes. The tree can be rebuiltwhen a threshold specifying cluster sizeis updated manually, or when memoryconstraints force a change in thisthreshold. This algorithm, like CLARANS, has a time complexity linear inthe number of patterns.The algorithms discussed above workon large data sets, where it is possibleto accommodate the entire pattern setin the main memory. However, thereare applications where the entire dataset cannot be stored in the main memory because of its size. There are currently three possible approaches tosolve this problem.1 The pattern set can be stored in asecondary memory and subsets ofthis data clustered independently,followed by a merging step to yield aclustering of the entire pattern set.We call this approach the divide andconquer approach.2 An incremental clustering algorithmcan be employed. Here, the entiredata matrix is stored in a secondarymemory and data items are transferred to the main memory one at atime for clustering. Only the clusterrepresentations are stored in themain memory to alleviate the spacelimitations.3 A parallel implementation of a clustering algorithm may be used. Wediscuss these approaches in the nextthree subsections.5.12.1 Divide and Conquer Approach.Here, we store the entire pattern matrixof size n 3 d in a secondary storagespace e.g., a disk file. We divide thisdata into p blocks, where an optimumvalue of p can be chosen based on theclustering algorithm used Murty andKrishna 1980. Let us assume that wehave n  p patterns in each of the blocks.294  A. Jain et al.ACM Computing Surveys, Vol. 31, No. 3, September 1999We transfer each of these blocks to themain memory and cluster it into k clusters using a standard algorithm. One ormore representative samples from eachof these clusters are stored separatelywe have pk of these representative patterns if we choose one representativeper cluster. These pk representativesare further clustered into k clusters andthe cluster labels of these representative patterns are used to relabel theoriginal pattern matrix. We depict thistwolevel algorithm in Figure 23. It ispossible to extend this algorithm to anynumber of levels more levels are required if the data set is very large andthe main memory size is very smallMurty and Krishna 1980. If the singlelink algorithm is used to obtain 5 clusters, then there is a substantial savingsin the number of computations asshown in Table II for optimally chosen pwhen the number of clusters is fixed at5. However, this algorithm works wellonly when the points in each block arereasonably homogeneous which is oftensatisfied by image data.A twolevel strategy for clustering adata set containing 2,000 patterns wasdescribed in Stahl 1986. In the firstlevel, the data set is loosely clusteredinto a large number of clusters usingthe leader algorithm. Representativesfrom these clusters, one per cluster, arethe input to the second level clustering,which is obtained using Wards hierarchical method.5.12.2 Incremental Clustering. Incremental clustering is based on theassumption that it is possible to consider patterns one at a time and assignthem to existing clusters. Here, a newdata item is assigned to a cluster without affecting the existing clusters significantly. A high level description of atypical incremental clustering algorithm is given below.An Incremental Clustering Algorithm1 Assign the first data item to a cluster.2 Consider the next data item. Eitherassign this item to one of the existing clusters or assign it to a newcluster. This assignment is donebased on some criterion, e.g. the distance between the new item and theexisting cluster centroids.3 Repeat step 2 till all the data itemsare clustered.The major advantage with the incremental clustering algorithms is that itis not necessary to store the entire pattern matrix in the memory. So, thespace requirements of incremental algorithms are very small. Typically, theyare noniterative. So their time requirements are also small. There are severalincremental clustering algorithms1 The leader clustering algorithmHartigan 1975 is the simplest interms of time complexity which isOnk. It has gained popularity because of its neural network implementation, the ART network Carpenter and Grossberg 1990. It isvery easy to implement as it requires only Ok space.xxxxx x xxxxx xxxxxxxx x xxxxxxxxx xxxxxxx xxxxx xxxxxxxxxxx. . .1              2                                               pn ppkFigure 23. Divide and conquer approach toclustering.Table II. Number of Distance Computations nfor the SingleLink Clustering Algorithm and aTwoLevel Divide and Conquer Algorithmn Singlelink p Twolevel100 4,950 1200500 124,750 2 10,750100 499,500 4 31,50010,000 49,995,000 10 1,013,750Data Clustering  295ACM Computing Surveys, Vol. 31, No. 3, September 19992 The shortest spanning path SSPalgorithm Slagle et al. 1975 wasoriginally proposed for data reorganization and was successfully usedin automatic auditing of recordsLee et al. 1978. Here, SSP algorithm was used to cluster 2000 patterns using 18 features. These clusters are used to estimate missingfeature values in data items and toidentify erroneous feature values.3 The cobweb system Fisher 1987 isan incremental conceptual clustering algorithm. It has been successfully used in engineering applications Fisher et al. 1993.4 An incremental clustering algorithmfor dynamic information processingwas presented in Can 1993. Themotivation behind this work is that,in dynamic databases, items mightget added and deleted over time.These changes should be reflected inthe partition generated without significantly affecting the current clusters. This algorithm was used tocluster incrementally an INSPECdatabase of 12,684 documents corresponding to computer science andelectrical engineering.Orderindependence is an importantproperty of clustering algorithms. Analgorithm is orderindependent if it generates the same partition for any orderin which the data is presented. Otherwise, it is orderdependent. Most of theincremental algorithms presented aboveare orderdependent. We illustrate thisorderdependent property in Figure 24where there are 6 twodimensional objects labeled 1 to 6. If we present thesepatterns to the leader algorithm in theorder 2,1,3,5,4,6 then the two clustersobtained are shown by ellipses. If theorder is 1,2,6,4,5,3, then we get a twopartition as shown by the triangles. TheSSP algorithm, cobweb, and the algorithm in Can 1993 are all orderdependent.5.12.3 Parallel Implementation. Recent work Judd et al. 1996 demonstrates that a combination of algorithmic enhancements to a clusteringalgorithm and distribution of the computations over a network of workstations can allow an entire 512 3 512image to be clustered in a few minutes.Depending on the clustering algorithmin use, parallelization of the code andreplication of data for efficiency mayyield large benefits. However, a globalshared data structure, namely the cluster membership table, remains andmust be managed centrally or replicatedand synchronized periodically. Thepresence or absence of robust, efficientparallel clustering techniques will determine the success or failure of clusteranalysis in largescale data mining applications in the future.6. APPLICATIONSClustering algorithms have been usedin a large variety of applications Jainand Dubes 1988 Rasmussen 1992Oehler and Gray 1995 Fisher et al.1993. In this section, we describe several applications where clustering hasbeen employed as an essential step.These areas are 1 image segmentation, 2 object and character recognition, 3 document retrieval, and 4data mining.6.1 Image Segmentation Using ClusteringImage segmentation is a fundamentalcomponent in many computer visionYX13 462 5Figure 24. The leader algorithm is orderdependent.296  A. Jain et al.ACM Computing Surveys, Vol. 31, No. 3, September 1999applications, and can be addressed as aclustering problem Rosenfeld and Kak1982. The segmentation of the imagespresented to an image analysis systemis critically dependent on the scene tobe sensed, the imaging geometry, configuration, and sensor used to transducethe scene into a digital image, and ultimately the desired output goal of thesystem.The applicability of clustering methodology to the image segmentationproblem was recognized over three decades ago, and the paradigms underlying the initial pioneering efforts are stillin use today. A recurring theme is todefine feature vectors at every imagelocation pixel composed of both functions of image intensity and functions ofthe pixel location itself. This basic idea,depicted in Figure 25, has been successfully used for intensity images with orwithout texture, range depth imagesand multispectral images.6.1.1 Segmentation. An image segmentation is typically defined as an exhaustive partitioning of an input imageinto regions, each of which is consideredto be homogeneous with respect to someimage property of interest e.g., intensity, color, or texture Jain et al. 1995.If 5 xij, i 5 1. . . Nr, j 5 1. . . Ncis the input image with Nr rows and Nccolumns and measurement value xij atpixel i, j, then the segmentation canbe expressed as 6 5 S1, . . . Sk, withthe lth segmentSl 5 il1, jl1, . . . ilNl, jlNlconsisting of a connected subset of thepixel coordinates. No two segmentsshare any pixel locations Si  Sj 5 i  j, and the union of all segmentscovers the entire image  i51k Si 51. . . Nr 3 1. . . Nc. Jain andDubes 1988, after Fu and Mui 1981identified three techniques for producing segmentations from input imageryregionbased, edgebased, or clusterbased.Consider the use of simple gray levelthresholding to segment a highcontrastintensity image. Figure 26a shows agrayscale image of a textbooks bar codescanned on a flatbed scanner. Part bshows the results of a simple thresholding operation designed to separate thedark and light regions in the bar codearea. Binarization steps like this areoften performed in character recognition systems. Thresholding in effectclusters the image pixels into twogroups based on the onedimensionalintensity measurement Rosenfeld 1969xxx123Figure 25. Feature representation for clustering. Image measurements and positions are transformedto features. Clusters in feature space correspond to image segments.Data Clustering  297ACM Computing Surveys, Vol. 31, No. 3, September 1999Dunn et al. 1974. A postprocessing stepseparates the classes into connected regions. While simple gray level thresholding is adequate in some carefullycontrolled image acquisition environments and much research has been devoted to appropriate methods forthresholding Weszka 1978 Trier andJain 1995, complex images requiremore elaborate segmentation techniques.Many segmenters use measurementswhich are both spectral e.g., the multispectral scanner used in remote sensing and spatial based on the pixelslocation in the image plane. The measurement at each pixel hence corresponds directly to our concept of a pattern.6.1.2 Image Segmentation Via Clustering. The application of local featureclustering to segment grayscale imageswas documented in Schachter et al.1979. This paper emphasized the appropriate selection of features at eachpixel rather than the clustering methodology, and proposed the use of imageplane coordinates spatial informationas additional features to be employed inclusteringbased segmentation. The goalof clustering was to obtain a sequence ofhyperellipsoidal clusters starting withcluster centers positioned at maximumdensity locations in the pattern space,and growing clusters about these centers until a x2 test for goodness of fitwas violated. A variety of features were0 50 100 150 200 250 300x.datca bFigure 26. Binarization via thresholding. a Original grayscale image. b Graylevel histogram. cResults of thresholding.298  A. Jain et al.ACM Computing Surveys, Vol. 31, No. 3, September 1999discussed and applied to both grayscaleand color imagery.An agglomerative clustering algorithm was applied in Silverman andCooper 1988 to the problem of unsupervised learning of clusters of coefficient vectors for two image models thatcorrespond to image segments. The firstimage model is polynomial for the observed image measurements the assumption here is that the image is acollection of several adjoining graphsurfaces, each a polynomial function ofthe image plane coordinates, which aresampled on the raster grid to producethe observed image. The algorithm proceeds by obtaining vectors of coefficientsof leastsquares fits to the data in Mdisjoint image windows. An agglomerative clustering algorithm merges ateach step the two clusters that have aminimum global betweencluster Mahalanobis distance. The same framework was applied to segmentation oftextured images, but for such imagesthe polynomial model was inappropriate, and a parameterized Markov Random Field model was assumed instead.Wu and Leahy 1993 describe theapplication of the principles of networkflow to unsupervised classification,yielding a novel hierarchical algorithmfor clustering. In essence, the techniqueviews the unlabeled patterns as nodesin a graph, where the weight of an edgei.e., its capacity is a measure of similarity between the corresponding nodes.Clusters are identified by removingedges from the graph to produce connected disjoint subgraphs. In image segmentation, pixels which are 4neighborsor 8neighbors in the image plane shareedges in the constructed adjacencygraph, and the weight of a graph edge isbased on the strength of a hypothesizedimage edge between the pixels involvedthis strength is calculated using simplederivative masks. Hence, this segmenter works by finding closed contoursin the image, and is best labeled edgebased rather than regionbased.In Vinod et al. 1994, two neuralnetworks are designed to perform pattern clustering when combined. A twolayer network operates on a multidimensional histogram of the data toidentify prototypes which are used toclassify the input patterns into clusters.These prototypes are fed to the classification network, another twolayer network operating on the histogram of theinput data, but are trained to have differing weights from the prototype selection network. In both networks, the histogram of the image is used to weightthe contributions of patterns neighboring the one under consideration to thelocation of prototypes or the ultimateclassification as such, it is likely to bemore robust when compared to techniques which assume an underlyingparametric density function for the pattern classes. This architecture wastested on grayscale and color segmentation problems.Jolion et al. 1991 describe a processfor extracting clusters sequentially fromthe input pattern set by identifying hyperellipsoidal regions bounded by lociof constant Mahalanobis distancewhich contain a specified fraction of theunclassified points in the set. The extracted regions are compared againstthe bestfitting multivariate Gaussiandensity through a KolmogorovSmirnovtest, and the fit quality is used as afigure of merit for selecting the bestregion at each iteration. The processcontinues until a stopping criterion issatisfied. This procedure was applied tothe problems of threshold selection formultithreshold segmentation of intensity imagery and segmentation of rangeimagery.Clustering techniques have also beensuccessfully used for the segmentationof range images, which are a popularsource of input data for threedimensional object recognition systems Jainand Flynn 1993. Range sensors typically return raster images with themeasured value at each pixel being thecoordinates of a 3D location in space.These 3D positions can be understoodData Clustering  299ACM Computing Surveys, Vol. 31, No. 3, September 1999as the locations where rays emergingfrom the image plane locations in a bundle intersect the objects in front of thesensor.The local feature clustering concept isparticularly attractive for range imagesegmentation since unlike intensitymeasurements the measurements ateach pixel have the same units lengththis would make ad hoc transformationsor normalizations of the image featuresunnecessary if their goal is to imposeequal scaling on those features. However, range image segmenters often addadditional measurements to the featurespace, removing this advantage.A range image segmentation systemdescribed in Hoffman and Jain 1987employs squared error clustering in asixdimensional feature space as asource of an initial segmentationwhich is refined typically by mergingsegments into the output segmentation. The technique was enhanced inFlynn and Jain 1991 and used in arecent systematic comparison of rangeimage segmenters Hoover et al. 1996as such, it is probably one of the longestlived range segmenters which hasperformed well on a large variety ofrange images.This segmenter works as follows. Ateach pixel i, j in the input range image, the corresponding 3D measurementis denoted xij, yij, zij, where typicallyxij is a linear function of j the columnnumber and yij is a linear function of ithe row number. A k 3 k neighborhood of i, j is used to estimate the 3Dsurface normal nij 5 nijx , nijy , nijz  ati, j, typically by finding the leastsquares planar fit to the 3D points inthe neighborhood. The feature vector forthe pixel at i, j is the sixdimensionalmeasurement xij, yij, zij, nijx , nijy , nijz ,and a candidate segmentation is foundby clustering these feature vectors. Forpractical reasons, not every pixels feature vector is used in the clusteringprocedure typically 1000 feature vectors are chosen by subsampling.The CLUSTER algorithm Jain andDubes 1988 was used to obtain segment labels for each pixel. CLUSTER isan enhancement of the kmeans algorithm it has the ability to identify several clusterings of a data set, each witha different number of clusters. Hoffmanand Jain 1987 also experimented withother clustering techniques e.g., completelink, singlelink, graphtheoretic,and other squared error algorithms andfound CLUSTER to provide the bestcombination of performance and accuracy. An additional advantage of CLUSTER is that it produces a sequence ofoutput clusterings i.e., a 2cluster solution up through a Kmaxcluster solutionwhere Kmax is specified by the user andis typically 20 or so each clustering inthis sequence yields a clustering statistic which combines betweencluster separation and withincluster scatter. Theclustering that optimizes this statisticis chosen as the best one. Each pixel inthe range image is assigned the segment label of the nearest cluster center.This minimum distance classificationstep is not guaranteed to produce segments which are connected in the imageplane therefore, a connected components labeling algorithm allocates newlabels for disjoint regions that wereplaced in the same cluster. Subsequentoperations include surface type tests,merging of adjacent patches using a testfor the presence of crease or jump edgesbetween adjacent segments, and surfaceparameter estimation.Figure 27 shows this processing applied to a range image. Part a of thefigure shows the input range imagepart b shows the distribution of surfacenormals. In part c, the initial segmentation returned by CLUSTER and modified to guarantee connected segments isshown. Part d shows the final segmentation produced by merging adjacentpatches which do not have a significantcrease edge between them. The finalclusters reasonably represent distinctsurfaces present in this complex object.300  A. Jain et al.ACM Computing Surveys, Vol. 31, No. 3, September 1999The analysis of textured images hasbeen of interest to researchers for several years. Texture segmentation techniques have been developed using a variety of texture models and imageoperations. In Nguyen and Cohen1993, texture image segmentation wasaddressed by modeling the image as ahierarchy of two Markov RandomFields, obtaining some simple statisticsfrom each image block to form a featurevector, and clustering these blocks using a fuzzy Kmeans clustering method.The clustering procedure here is modified to jointly estimate the number ofclusters as well as the fuzzy membership of each feature vector to the various clusters.A system for segmenting texture images was described in Jain and Farrokhnia 1991 there, Gabor filterswere used to obtain a set of 28 orientation and scaleselective features thatcharacterize the texture in the neighborhood of each pixel. These 28 featuresare reduced to a smaller numberthrough a feature selection procedure,and the resulting features are preprocessed and then clustered using theCLUSTER program. An index statistica bc dFigure 27. Range image segmentation using clustering. a Input range image. b Surface normalsfor selected image pixels. c Initial segmentation 19 cluster solution returned by CLUSTER using1000 sixdimensional samples from the image as a pattern set. d Final segmentation 8 segmentsproduced by postprocessing.Data Clustering  301ACM Computing Surveys, Vol. 31, No. 3, September 1999Dubes 1987 is used to select the bestclustering. Minimum distance classification is used to label each of the original image pixels. This technique wastested on several texture mosaics including the natural Brodatz texturesand synthetic images. Figure 28ashows an input texture mosaic consisting of four of the popular Brodatz textures Brodatz 1966. Part b shows thesegmentation produced when the Gaborfilter features are augmented to containspatial information pixel coordinates.This Gabor filter based technique hasproven very powerful and has been extended to the automatic segmentation oftext in documents Jain and Bhattacharjee 1992 and segmentation of objects in complex backgrounds Jain etal. 1997.Clustering can be used as a preprocessing stage to identify pattern classesfor subsequent supervised classification. Taxt and Lundervold 1994 andLundervold et al. 1996 describe a partitional clustering algorithm and a manual labeling technique to identify material classes e.g., cerebrospinal fluid,white matter, striated muscle, tumor inregistered images of a human head obtained at five different magnetic resonance imaging channels yielding a fivedimensional feature vector at eachpixel. A number of clusterings wereobtained and combined with domainknowledge human expertise to identifythe different classes. Decision rules forsupervised classification were based onthese obtained classes. Figure 29ashows one channel of an input multispectral image part b shows the 9cluster result.The kmeans algorithm was appliedto the segmentation of LANDSAT imagery in Solberg et al. 1996. Initial cluster centers were chosen interactively bya trained operator, and correspond tolanduse classes such as urban areas,soil vegetationfree areas, forest,grassland, and water. Figure 30ashows the input image rendered asgrayscale part b shows the result of theclustering procedure.6.1.3 Summary. In this section, theapplication of clustering methodology toimage segmentation problems has beenmotivated and surveyed. The historicalrecord shows that clustering is a powerful tool for obtaining classifications ofimage pixels. Key issues in the design ofany clusteringbased segmenter are thea bFigure 28. Texture image segmentation results. a Fourclass texture mosaic. b Fourclustersolution produced by CLUSTER with pixel coordinates included in the feature set.302  A. Jain et al.ACM Computing Surveys, Vol. 31, No. 3, September 1999choice of pixel measurements featuresand dimensionality of the feature vectori.e., should the feature vector containintensities, pixel positions, model parameters, filter outputs, a measure ofsimilarity which is appropriate for theselected features and the application domain, the identification of a clusteringalgorithm, the development of strategies for feature and data reduction toavoid the curse of dimensionality andthe computational burden of classifyinglarge numbers of patterns andor features, and the identification of necessary pre and postprocessing techniques e.g., image smoothing andminimum distance classification. Theuse of clustering for segmentation datesback to the 1960s, and new variationscontinue to emerge in the literature.Challenges to the more successful use ofclustering include the high computational complexity of many clustering algorithms and their incorporation ofa bFigure 29. Multispectral medical image segmentation. a A single channel of the input image. b9cluster segmentation.a bFigure 30. LANDSAT image segmentation. a Original image ESAEURIMAGESattelitbild. bClustered scene.Data Clustering  303ACM Computing Surveys, Vol. 31, No. 3, September 1999strong assumptions often multivariateGaussian about the multidimensionalshape of clusters to be obtained. Theability of new clustering procedures tohandle concepts and semantics in classification in addition to numerical measurements will be important for certainapplications Michalski and Stepp 1983Murty and Jain 1995.6.2 Object and Character Recognition6.2.1 Object Recognition. The use ofclustering to group views of 3D objectsfor the purposes of object recognition inrange data was described in Dorai andJain 1995. The term view refers to arange image of an unoccluded objectobtained from any arbitrary viewpoint.The system under consideration employed a viewpoint dependent or viewcentered approach to the object recognition problem each object to berecognized was represented in terms ofa library of range images of that object.There are many possible views of a 3Dobject and one goal of that work was toavoid matching an unknown input viewagainst each image of each object. Acommon theme in the object recognitionliterature is indexing, wherein the unknown view is used to select a subset ofviews of a subset of the objects in thedatabase for further comparison, andrejects all other views of objects. One ofthe approaches to indexing employs thenotion of view classes a view class is theset of qualitatively similar views of anobject. In that work, the view classeswere identified by clustering the rest ofthis subsection outlines the technique.Object views were grouped intoclasses based on the similarity of shapespectral features. Each input image ofan object viewed in isolation yields afeature vector which characterizes thatview. The feature vector contains thefirst ten central moments of a normalized shape spectral distribution, H h,of an object view. The shape spectrum ofan object view is obtained from its rangedata by constructing a histogram ofshape index values which are related tosurface curvature values and accumulating all the object pixels that fall intoeach bin. By normalizing the spectrumwith respect to the total object area, thescale size differences that may existbetween different objects are removed.The first moment m1 is computed as theweighted mean of H hm1 5 OhhH h. 1The other central moments, mp, 2  p 10 are defined asmp 5 Ohh 2 m1 pH h. 2Then, the feature vector is denoted asR 5 m1, m2,   , m10, with therange of each of these moments being21,1.Let 2 5 O1, O2,   , On be a collection of n 3D objects whose views arepresent in the model database, D. Theith view of the jth object, Oji in thedatabase is represented by Lji, Rji,where Lji is the object label and Rji is thefeature vector. Given a set of objectrepresentations 5 i 5 L1i , R1i ,   ,Lmi , Rmi  that describes m views of theith object, the goal is to derive a partition of the views, 3 i 5 C1i ,C2i ,   , Ckii . Each cluster in 3 i contains those views of the ith object thathave been adjudged similar based onthe dissimilarity between the corresponding moment features of the shapespectra of the views. The measure ofdissimilarity, between Rji and Rki , is defined asRji, Rki  5 Ol5110Rjli 2 Rkli 2. 36.2.2 Clustering Views. A databasecontaining 3,200 range images of 10 different sculpted objects with 320 viewsper object is used Dorai and Jain 1995.304  A. Jain et al.ACM Computing Surveys, Vol. 31, No. 3, September 1999The range images from 320 possibleviewpoints determined by the tessellation of the viewsphere using the icosahedron of the objects were synthesized.Figure 31 shows a subset of the collection of views of Cobra used in the experiment.The shape spectrum of each view iscomputed and then its feature vector isdetermined. The views of each objectare clustered, based on the dissimilaritymeasure  between their moment vectors using the completelink hierarchical clustering scheme Jain and Dubes1988. The hierarchical grouping obtained with 320 views of the Cobra object is shown in Figure 32. The viewgrouping hierarchies of the other nineobjects are similar to the dendrogram inFigure 32. This dendrogram is cut at adissimilarity level of 0.1 or less to obtain compact and wellseparated clusters. The clusterings obtained in thismanner demonstrate that the views ofeach object fall into several distinguishable clusters. The centroid of each ofthese clusters was determined by computing the mean of the moment vectorsof the views falling into the cluster.Dorai and Jain 1995 demonstratedthat this clusteringbased view groupingprocedure facilitates object matchingFigure 31. A subset of views of Cobra chosen from a set of 320 views.Data Clustering  305ACM Computing Surveys, Vol. 31, No. 3, September 1999in terms of classification accuracy andthe number of matches necessary forcorrect classification of test views. Object views are grouped into compact andhomogeneous view clusters, thus demonstrating the power of the clusterbased scheme for view organization andefficient object matching.6.2.3 Character Recognition. Clustering was employed in Connell andJain 1998 to identify lexemes in handwritten text for the purposes of writerindependent handwriting recognition.The success of a handwriting recognition system is vitally dependent on itsacceptance by potential users. Writerdependent systems provide a higherlevel of recognition accuracy than writerindependent systems, but require alarge amount of training data. A writerindependent system, on the other hand,must be able to recognize a wide varietyof writing styles in order to satisfy anindividual user. As the variability of thewriting styles that must be captured bya system increases, it becomes more andmore difficult to discriminate betweendifferent classes due to the amount ofoverlap in the feature space. One solution to this problem is to separate thedata from these disparate writing stylesfor each class into different subclasses,known as lexemes. These lexemes represent portions of the data which are moreeasily separated from the data of classesother than that to which the lexemebelongs.In this system, handwriting is captured by digitizing the x, y position ofthe pen and the state of the pen point0.00.050.100.150.200.25Figure 32. Hierarchical grouping of 320 views of a cobra sculpture.306  A. Jain et al.ACM Computing Surveys, Vol. 31, No. 3, September 1999up or down at a constant samplingrate. Following some resampling, normalization, and smoothing, each strokeof the pen is represented as a variablelength string of points. A metric basedon elastic template matching and dynamic programming is defined to allowthe distance between two strokes to becalculated.Using the distances calculated in thismanner, a proximity matrix is constructed for each class of digits i.e., 0through 9. Each matrix measures theintraclass distances for a particulardigit class. Digits in a particular classare clustered in an attempt to find asmall number of prototypes. Clusteringis done using the CLUSTER programdescribed above Jain and Dubes 1988,in which the feature vector for a digit isits N proximities to the digits of thesame class. CLUSTER attempts to produce the best clustering for each valueof K over some range, where K is thenumber of clusters into which the datais to be partitioned. As expected, themean squared error MSE decreasesmonotonically as a function of K. Theoptimal value of K is chosen by identifying a knee in the plot of MSE vs. K.When representing a cluster of digitsby a single prototype, the best onlinerecognition results were obtained by using the digit that is closest to that clusters center. Using this scheme, a correct recognition rate of 99.33 wasobtained.6.3 Information RetrievalInformation retrieval IR is concernedwith automatic storage and retrieval ofdocuments Rasmussen 1992. Manyuniversity libraries use IR systems toprovide access to books, journals, andother documents. Libraries use the Library of Congress Classification LCCscheme for efficient storage and retrieval of books. The LCC scheme consists of classes labeled A to Z LC Classification Outline 1990 which are usedto characterize books belonging to different subjects. For example, label Qcorresponds to books in the area of science, and the subclass QA is assigned tomathematics. Labels QA76 to QA76.8are used for classifying books related tocomputers and other areas of computerscience.There are several problems associatedwith the classification of books usingthe LCC scheme. Some of these arelisted below1 When a user is searching for booksin a library which deal with a topicof interest to him, the LCC numberalone may not be able to retrieve allthe relevant books. This is becausethe classification number assignedto the books or the subject categories that are typically entered in thedatabase do not contain sufficientinformation regarding all the topicscovered in a book. To illustrate thispoint, let us consider the book Algorithms for Clustering Data by Jainand Dubes 1988. Its LCC numberis QA 278.J35. In this LCC number, QA 278 corresponds to the topiccluster analysis, J corresponds tothe first authors name and 35 is theserial number assigned by the Library of Congress. The subject categories for this book provided by thepublisher which are typically entered in a database to facilitatesearch are cluster analysis, dataprocessing and algorithms. There isa chapter in this book Jain andDubes 1988 that deals with computer vision, image processing, andimage segmentation. So a user looking for literature on computer visionand, in particular, image segmentation will not be able to access thisbook by searching the database withthe help of either the LCC numberor the subject categories provided inthe database. The LCC number forcomputer vision books is TA 1632LC Classification 1990 which isvery different from the number QA278.J35 assigned to this book.Data Clustering  307ACM Computing Surveys, Vol. 31, No. 3, September 19992 There is an inherent problem in assigning LCC numbers to books in arapidly developing area. For example, let us consider the area of neural networks. Initially, category QPin LCC scheme was used to labelbooks and conference proceedings inthis area. For example, Proceedingsof the International Joint Conferenceon Neural Networks IJCNN91 wasassigned the number QP 363.3. Butmost of the recent books on neuralnetworks are given a number usingthe category label QA Proceedingsof the IJCNN92 IJCNN92 is assigned the number QA 76.87. Multiple labels for books dealing withthe same topic will force them to beplaced on different stacks in a library. Hence, there is a need to update the classification labels fromtime to time in an emerging discipline.3 Assigning a number to a new book isa difficult problem. A book may dealwith topics corresponding to two ormore LCC numbers, and therefore,assigning a unique number to sucha book is difficult.Murty and Jain 1995 describe aknowledgebased clustering scheme togroup representations of books, whichare obtained using the ACM CR Association for Computing Machinery Computing Reviews classification treeACM CR Classifications 1994. Thistree is used by the authors contributingto various ACM publications to providekeywords in the form of ACM CR category labels. This tree consists of 11nodes at the first level. These nodes arelabeled A to K. Each node in this treehas a label that is a string of one ormore symbols. These symbols are alphanumeric characters. For example, I515is the label of a fourthlevel node in thetree.6.3.1 Pattern Representation. Eachbook is represented as a generalized listSangal 1991 of these strings using theACM CR classification tree. For thesake of brevity in representation, thefourthlevel nodes in the ACM CR classification tree are labeled using numerals 1 to 9 and characters A to Z. Forexample, the children nodes of I.5.1models are labeled I.5.1.1 to I.5.1.6.Here, I.5.1.1 corresponds to the nodelabeled deterministic, and I.5.1.6 standsfor the node labeled structural. In asimilar fashion, all the fourthlevelnodes in the tree can be labeled as necessary. From now on, the dots in between successive symbols will be omitted to simplify the representation. Forexample, I.5.1.1 will be denoted as I511.We illustrate this process of representation with the help of the book by Jainand Dubes 1988. There are five chapters in this book. For simplicity of processing, we consider only the information in the chapter contents. There is asingle entry in the table of contents forchapter 1, Introduction, and so we donot extract any keywords from this.Chapter 2, labeled Data Representation, has section titles that correspondto the labels of the nodes in the ACMCR classification tree ACM CR Classifications 1994 which are given below1a I522 feature evaluation and selection,2b I532 similarity measures, and3c I515 statistical.Based on the above analysis, Chapter 2 ofJain and Dubes 1988 can be characterized by the weighted disjunctionI522  I532  I5151,4. The weights1,4 denote that it is one of the four chapters which plays a role in the representation of the book. Based on the table ofcontents, we can use one or more of thestrings I522, I532, and I515 to representChapter 2. In a similar manner, we canrepresent other chapters in this book asweighted disjunctions based on the table ofcontents and the ACM CR classificationtree. The representation of the entire book,the conjunction of all these chapter representations, is given by I522  I532 I5151,4  I515  I5312,4 I541  I46  I4341,4.308  A. Jain et al.ACM Computing Surveys, Vol. 31, No. 3, September 1999Currently, these representations aregenerated manually by scanning the table of contents of books in computerscience area as ACM CR classificationtree provides knowledge of computerscience books only. The details of thecollection of books used in this study areavailable in Murty and Jain 1995.6.3.2 Similarity Measure. The similarity between two books is based on thesimilarity between the correspondingstrings. Two of the wellknown distancefunctions between a pair of strings areBaezaYates 1992 the Hamming distance and the edit distance. Neither ofthese two distance functions can bemeaningfully used in this application.The following example illustrates thepoint. Consider three strings I242, I233,and H242. These strings are labelspredicate logic for knowledge representation, logic programming, and distributed database systems of three fourthlevel nodes in the ACM CRclassification tree. Nodes I242 and I233are the grandchildren of the node labeled I2 artificial intelligence andH242 is a grandchild of the node labeledH2 database management. So, the distance between I242 and I233 should besmaller than that between I242 andH242. However, Hamming distance andedit distance BaezaYates 1992 bothhave a value 2 between I242 and I233and a value of 1 between I242 andH242. This limitation motivates the definition of a new similarity measure thatcorrectly captures the similarity between the above strings. The similaritybetween two strings is defined as theratio of the length of the largest common prefix Murty and Jain 1995 between the two strings to the length ofthe first string. For example, the similarity between strings I522 and I51 is0.5. The proposed similarity measure isnot symmetric because the similaritybetween I51 and I522 is 0.67. The minimum and maximum values of this similarity measure are 0.0 and 1.0, respectively. The knowledge of therelationship between nodes in the ACMCR classification tree is captured by therepresentation in the form of strings.For example, node labeled pattern recognition is represented by the string I5,whereas the string I53 corresponds tothe node labeled clustering. The similarity between these two nodes I5 and I53is 1.0. A symmetric measure of similarity Murty and Jain 1995 is used toconstruct a similarity matrix of size 100x 100 corresponding to 100 books usedin experiments.6.3.3 An Algorithm for ClusteringBooks. The clustering problem can bestated as follows. Given a collection of books, we need to obtain a set  ofclusters. A proximity dendrogram Jainand Dubes 1988, using the completelink agglomerative clustering algorithmfor the collection of 100 books is shownin Figure 33. Seven clusters are obtained by choosing a threshold t valueof 0.12. It is well known that differentvalues for t might give different clusterings. This threshold value is chosen because the gap in the dendrogram between the levels at which six and sevenclusters are formed is the largest. Anexamination of the subject areas of thebooks Murty and Jain 1995 in theseclusters revealed that the clusters obtained are indeed meaningful. Each ofthese clusters are represented using alist of string s and frequency sf pairs,where sf is the number of books in thecluster in which s is present. For example, cluster c1 contains 43 books belonging to pattern recognition, neural networks, artificial intelligence, andcomputer vision a part of its representation 5C1 is given below.5C1 5 B718,1, C12,1, D0,2,D311,1, D312,2, D321,1,D322,1, D329,1, . . . I46,3,I461,2, I462,1, I463, 3,. . . J26,1, J6,1,J61,7, J71,1Data Clustering  309ACM Computing Surveys, Vol. 31, No. 3, September 1999These clusters of books and the corresponding cluster descriptions can beused as follows If a user is searchingfor books, say, on image segmentationI46, then we select cluster C1 becauseits representation alone contains thestring I46. Books B2 Neurocomputingand B18 Sensory Neural Networks Lateral Inhibition are both members of cluster C1 even though their LCC numbersare quite different B2 is QA76.5.H4442,B18 is QP363.3.N33.Four additional books labeled B101,B102, B103, and B104 have been used tostudy the problem of assigning classification numbers to new books. The LCCnumbers of these books are B101Q335.T39, B102 QA76.73.P356C57,B103 QA76.5.B76C.2, and B104QA76.9D5W44. These books are assigned to clusters based on nearestneighbor classification. The nearestneighbor of B101, a book on artificialintelligence, is B23 and so B101 is assigned to cluster C1. It is observed thatthe assignment of these four books tothe respective clusters is meaningful,demonstrating that knowledgebasedclustering is useful in solving problemsassociated with document retrieval.6.4 Data MiningIn recent years we have seen ever increasing volumes of collected data of allsorts. With so much data available, it isnecessary to develop algorithms whichcan extract meaningful informationfrom the vast stores. Searching for useful nuggets of information among hugeamounts of data has become known asthe field of data mining.Data mining can be applied to relational, transaction, and spatial databases, as well as large stores of unstructured data such as the World Wide Web.There are many data mining systems inuse today, and applications include theU.S. Treasury detecting money laundering, National Basketball Associationcoaches detecting trends and patterns ofplay for individual players and teams,and categorizing patterns of children inthe foster care system Hedberg 1996.Several journals have had recent specialissues on data mining Cohen 1996,Cross 1996, Wah 1996.6.4.1 Data Mining Approaches.Data mining, like clustering, is an exploratory activity, so clustering methodsare well suited for data mining. Clustering is often an important initial step ofseveral in the data mining processFayyad 1996. Some of the data miningapproaches which use clustering are database segmentation, predictive modeling, and visualization of large databases.Segmentation. Clustering methodsare used in data mining to segmentdatabases into homogeneous groups.This can serve purposes of data compression working with the clustersrather than individual items, or toidentify characteristics of subpopulations which can be targeted for specificpurposes e.g., marketing aimed at senior citizens.A continuous kmeans clustering algorithm Faber 1994 has been used tocluster pixels in Landsat images Faberet al. 1994. Each pixel originally has 7values from different satellite bands,including infrared. These 7 values aredifficult for humans to assimilate andanalyze without assistance. Pixels withthe 7 feature values are clustered into256 groups, then each pixel is assignedthe value of the cluster centroid. Theimage can then be displayed with thespatial information intact. Human viewers can look at a single picture andidentify a region of interest e.g., highway or forest and label it as a concept.The system then identifies other pixelsin the same cluster as an instance ofthat concept.Predictive Modeling. Statistical methods of data analysis usually involve hypothesis testing of a model the analystalready has in mind. Data mining canaid the user in discovering potential310  A. Jain et al.ACM Computing Surveys, Vol. 31, No. 3, September 1999hypotheses prior to using statisticaltools. Predictive modeling uses clustering to group items, then infers rules tocharacterize the groups and suggestmodels. For example, magazine subscribers can be clustered based on anumber of factors age, sex, income,etc., then the resulting groups characterized in an attempt to find a modelwhich will distinguish those subscribersthat will renew their subscriptions fromthose that will not Simoudis 1996.Visualization. Clusters in large databases can be used for visualization, inorder to aid human analysts in identifying groups and subgroups that havesimilar characteristics. WinViz Lee andOng 1996 is a data mining visualization1234 56 78910111213 141516171819202122232425262728293031323334 353637 3839 4041424344 454647484950515253545556575859 6061626364656667 68 69 70717273747576777879 808182 83848586 878889909192939495969798991000.00.20.40.60.81.0Figure 33. A dendrogram corresponding to 100 books.Data Clustering  311ACM Computing Surveys, Vol. 31, No. 3, September 1999tool in which derived clusters can beexported as new attributes which canthen be characterized by the system.For example, breakfast cereals are clustered according to calories, protein, fat,sodium, fiber, carbohydrate, sugar, potassium, and vitamin content per serving. Upon seeing the resulting clusters,the user can export the clusters to WinViz as attributes. The system showsthat one of the clusters is characterizedby high potassium content, and the human analyst recognizes the individualsin the cluster as belonging to the brancereal family, leading to a generalization that bran cereals are high in potassium.6.4.2 Mining Large Unstructured Databases. Data mining has often beenperformed on transaction and relationaldatabases which have welldefinedfields which can be used as features, butthere has been recent research on largeunstructured databases such as theWorld Wide Web Etzioni 1996.Examples of recent attempts to classify Web documents using words orfunctions of words as features includeMaarek and Shaul 1996 and Chekuriet al. 1999. However, relatively smallsets of labeled training samples andvery large dimensionality limit the ultimate success of automatic Web document categorization based on words asfeatures.Rather than grouping documents in aword feature space, Wulfekuhler andPunch 1997 cluster the words from asmall collection of World Wide Web documents in the document space. Thesample data set consisted of 85 documents from the manufacturing domainin 4 different userdefined categorieslabor, legal, government, and design.These 85 documents contained 5190 distinct word stems after common wordsthe, and, of were removed. Since thewords are certainly not uncorrelated,they should fall into clusters wherewords used in a consistent way acrossthe document set have similar values offrequency in each document.Kmeans clustering was used to groupthe 5190 words into 10 groups. Onesurprising result was that an average of92 of the words fell into a single cluster, which could then be discarded fordata mining purposes. The smallestclusters contained terms which to a human seem semantically related. The 7smallest clusters from a typical run areshown in Figure 34.Terms which are used in ordinarycontexts, or unique terms which do notoccur often across the training document set will tend to cluster into theFigure 34. The seven smallest clusters found in the document set. These are stemmed words.312  A. Jain et al.ACM Computing Surveys, Vol. 31, No. 3, September 1999large 4000 member group. This takescare of spelling errors, proper nameswhich are infrequent, and terms whichare used in the same manner throughout the entire document set. Terms usedin specific contexts such as file in thecontext of filing a patent, rather than acomputer file will appear in the documents consistently with other terms appropriate to that context patent, inventand thus will tend to cluster together.Among the groups of words, unique contexts stand out from the crowd.After discarding the largest cluster,the smaller set of features can be usedto construct queries for seeking outother relevant documents on the Webusing standard Web searching toolse.g., Lycos, Alta Vista, Open Text.Searching the Web with terms takenfrom the word clusters allows discoveryof finer grained topics e.g., family medical leave within the broadly definedcategories e.g., labor.6.4.3 Data Mining in Geological Databases. Database mining is a criticalresource in oil exploration and production. It is common knowledge in the oilindustry that the typical cost of drillinga new offshore well is in the range of3040 million, but the chance of thatsite being an economic success is 1 in10. More informed and systematic drilling decisions can significantly reduceoverall production costs.Advances in drilling technology anddata collection methods have led to oilcompanies and their ancillaries collecting large amounts of geophysicalgeological data from production wells and exploration sites, and then organizingthem into large databases. Data miningtechniques has recently been used toderive precise analytic relations between observed phenomena and parameters. These relations can then be usedto quantify oil and gas reserves.In qualitative terms, good recoverablereserves have high hydrocarbon saturation that are trapped by highly poroussediments reservoir porosity and surrounded by hard bulk rocks that prevent the hydrocarbon from leakingaway. A large volume of porous sediments is crucial to finding good recoverable reserves, therefore developing reliable and accurate methods forestimation of sediment porosities fromthe collected data is key to estimatinghydrocarbon potential.The general rule of thumb experts usefor porosity computation is that it is aquasiexponential function of depthPorosity 5 K z e2Fx1, x2, ..., xmzDepth. 4A number of factors such as rock types,structure, and cementation as parameters of function F confound this relationship. This necessitates the definition of proper contexts, in which toattempt discovery of porosity formulas.Geological contexts are expressed interms of geological phenomena, such asgeometry, lithology, compaction, andsubsidence, associated with a region. Itis well known that geological contextchanges from basin to basin differentgeographical areas in the world andalso from region to region within a basin Allen and Allen 1990 Biswas 1995.Furthermore, the underlying features ofcontexts may vary greatly. Simplemodel matching techniques, which workin engineering domains where behavioris constrained by manmade systemsand wellestablished laws of physics,may not apply in the hydrocarbon exploration domain. To address this, dataclustering was used to identify the relevant contexts, and then equation discovery was carried out within each context.The goal was to derive the subset x1,x2, ..., xm from a larger set of geologicalfeatures, and the functional relationship F that best defined the porosityfunction in a region.The overall methodology illustratedin Figure 35, consists of two primarysteps i Context definition using unsupervised clustering techniques, and iiEquation discovery by regression analysis Li and Biswas 1995. Real exploration data collected from a region in theData Clustering  313ACM Computing Surveys, Vol. 31, No. 3, September 1999Alaska basin was analyzed using themethodology developed. The data objects patterns are described in terms of37 geological features, such as porosity,permeability, grain size, density, andsorting, amount of different mineralfragments e.g., quartz, chert, feldsparpresent, nature of the rock fragments,pore characteristics, and cementation.All these feature values are numericmeasurements made on samples obtained from welllogs during exploratorydrilling processes.The kmeans clustering algorithmwas used to identify a set of homogeneous primitive geological structuresg1, g2, ..., gm. These primitives werethen mapped onto the unit code versusstratigraphic unit map. Figure 36 depicts a partial mapping for a set of wellsand four primitive structures. The nextstep in the discovery process identifiedsections of wells regions that were madeup of the same sequence of geologicalprimitives. Every sequence defined acontext Ci. From the partial mapping ofFigure 36, the context C1 5 g2  g1 g2  g3 was identified in two well regions the 300 and 600 series. After thecontexts were defined, data points belonging to each context were groupedtogether for equation derivation. Thederivation procedure employed multipleregression analysis Sen and Srivastava1990.This method was applied to a data setof about 2600 objects corresponding tosample measurements collected fromwells is the Alaskan Basin. Thekmeans clustered this data set intoseven groups. As an illustration, we selected a set of 138 objects representing acontext for further analysis. The features that best defined this cluster wereselected, and experts surmised that thecontext represented a low porosity region, which was modeled using the regression procedure.7. SUMMARYThere are several applications wheredecision making and exploratory pattern analysis have to be performed onlarge data sets. For example, in document retrieval, a set of relevant documents has to be found among severalmillions of documents of dimensionalityof more than 1000. It is possible tohandle these problems if some usefulabstraction of the data is obtained andis used in decision making, rather thandirectly using the entire data set. Bydata abstraction, we mean a simple andcompact representation of the data.This simplicity helps the machine inefficient processing or a human in comprehending the structure in data easily.Clustering algorithms are ideally suitedfor achieving data abstraction.In this paper, we have examined various steps in clustering 1 pattern representation, 2 similarity computation,3 grouping process, and 4 cluster representation. Also, we have discussedFigure 35. Description of the knowledgebased scientific discovery process.314  A. Jain et al.ACM Computing Surveys, Vol. 31, No. 3, September 1999statistical, fuzzy, neural, evolutionary,and knowledgebased approaches toclustering. We have described four applications of clustering 1 image segmentation, 2 object recognition, 3document retrieval, and 4 data mining.Clustering is a process of groupingdata items based on a measure of similarity. Clustering is a subjective process the same set of data items oftenneeds to be partitioned differently fordifferent applications. This subjectivitymakes the process of clustering difficult.This is because a single algorithm orapproach is not adequate to solve everyclustering problem. A possible solutionlies in reflecting this subjectivity in theform of knowledge. This knowledge isused either implicitly or explicitly inone or more phases of clustering.Knowledgebased clustering algorithmsuse domain knowledge explicitly.The most challenging step in clustering is feature extraction or pattern representation. Pattern recognition researchers conveniently avoid this stepby assuming that the pattern representations are available as input to theclustering algorithm. In small size datasets, pattern representations can be obtained based on previous experience ofthe user with the problem. However, inthe case of large data sets, it is difficultfor the user to keep track of the importance of each feature in clustering. Asolution is to make as many measurements on the patterns as possible anduse them in pattern representation. Butit is not possible to use a large collectionof measurements directly in clusteringbecause of computational costs. So several feature extractionselection approaches have been designed to obtainlinear or nonlinear combinations ofthese measurements which can be usedto represent patterns. Most of theschemes proposed for feature extractionselection are typically iterative innature and cannot be used on large datasets due to prohibitive computationalcosts.The second step in clustering is similarity computation. A variety ofschemes have been used to computesimilarity between two patterns. TheyArea  CodeStratigraphic Unit100 200 300 400 500 600 700319031803170200031603150314031303120311031003000Figure 36. Area code versus stratigraphic unit map for part of the studied region.Data Clustering  315ACM Computing Surveys, Vol. 31, No. 3, September 1999use knowledge either implicitly or explicitly. Most of the knowledgebasedclustering algorithms use explicitknowledge in similarity computation.However, if patterns are not represented using proper features, then it isnot possible to get a meaningful partition irrespective of the quality andquantity of knowledge used in similarity computation. There is no universallyacceptable scheme for computing similarity between patterns represented using a mixture of both qualitative andquantitative features. Dissimilarity between a pair of patterns is representedusing a distance measure that may ormay not be a metric.The next step in clustering is thegrouping step. There are broadly twogrouping schemes hierarchical and partitional schemes. The hierarchicalschemes are more versatile, and thepartitional schemes are less expensive.The partitional algorithms aim at maximizing the squared error criterion function. Motivated by the failure of thesquared error partitional clustering algorithms in finding the optimal solutionto this problem, a large collection ofapproaches have been proposed andused to obtain the global optimal solution to this problem. However, theseschemes are computationally prohibitive on large data sets. ANNbased clustering schemes are neural implementations of the clustering algorithms, andthey share the undesired properties ofthese algorithms. However, ANNs havethe capability to automatically normalize the data and extract features. Animportant observation is that even if ascheme can find the optimal solution tothe squared error partitioning problem,it may still fall short of the requirements because of the possible nonisotropic nature of the clusters.In some applications, for example indocument retrieval, it may be useful tohave a clustering that is not a partition.This means clusters are overlapping.Fuzzy clustering and functional clustering are ideally suited for this purpose.Also, fuzzy clustering algorithms canhandle mixed data types. However, amajor problem with fuzzy clustering isthat it is difficult to obtain the membership values. A general approach maynot work because of the subjective nature of clustering. It is required to represent clusters obtained in a suitableform to help the decision maker. Knowledgebased clustering schemes generateintuitively appealing descriptions ofclusters. They can be used even whenthe patterns are represented using acombination of qualitative and quantitative features, provided that knowledge linking a concept and the mixedfeatures are available. However, implementations of the conceptual clusteringschemes are computationally expensiveand are not suitable for grouping largedata sets.The kmeans algorithm and its neuralimplementation, the Kohonen net, aremost successfully used on large datasets. This is because kmeans algorithmis simple to implement and computationally attractive because of its lineartime complexity. However, it is not feasible to use even this linear time algorithm on large data sets. Incrementalalgorithms like leader and its neuralimplementation, the ART network, canbe used to cluster large data sets. Butthey tend to be orderdependent. Divideand conquer is a heuristic that has beenrightly exploited by computer algorithmdesigners to reduce computational costs.However, it should be judiciously usedin clustering to achieve meaningful results.In summary, clustering is an interesting, useful, and challenging problem. Ithas great potential in applications likeobject recognition, image segmentation,and information filtering and retrieval.However, it is possible to exploit thispotential only after making several design choices carefully.ACKNOWLEDGMENTSThe authors wish to acknowledge thegenerosity of several colleagues who316  A. Jain et al.ACM Computing Surveys, Vol. 31, No. 3, September 1999read manuscript drafts, made suggestions, and provided summaries ofemerging application areas which wehave incorporated into this paper. Gautam Biswas and Cen Li of VanderbiltUniversity provided the material onknowledge discovery in geological databases. Ana Fred of Instituto SuperiorTcnico in Lisbon, Portugal providedmaterial on cluster analysis in the syntactic domain. William Punch and Marilyn Wulfekuhler of Michigan State University provided material on theapplication of cluster analysis to datamining problems. Scott Connell of Michigan State provided material describinghis work on character recognition. Chitra Dorai of IBM T.J. Watson ResearchCenter provided material on the use ofclustering in 3D object recognition.Jianchang Mao of IBM Almaden Research Center, Peter Bajcsy of the University of Illinois, and Zoran Obradovicof Washington State University alsoprovided many helpful comments. Mariode Figueirido performed a meticulousreading of the manuscript and providedmany helpful suggestions.This work was supported by the National Science Foundation under grantINT9321584.REFERENCESAARTS, E. AND KORST, J. 1989. Simulated Annealing and Boltzmann Machines A Stochastic Approach to Combinatorial Optimizationand Neural Computing. WileyInterscienceseries in discrete mathematics and optimization. John Wiley and Sons, Inc., New York,NY.ACM, 1994. ACM CR Classifications. ACMComputing Surveys 35, 516.ALSULTAN, K. S. 1995. A tabu search approachto clustering problems. Pattern Recogn. 28,14431451.ALSULTAN, K. S. AND KHAN, M. M. 1996.Computational experience on four algorithmsfor the hard clustering problem. PatternRecogn. Lett. 17, 3, 295308.ALLEN, P. A. AND ALLEN, J. R. 1990. BasinAnalysis Principles and Applications. Blackwell Scientific Publications, Inc.,Cambridge, MA.ALTA VISTA, 1999. httpaltavista.digital.com.AMADASUN, M. AND KING, R. A. 1988. Lowlevelsegmentation of multispectral images via agglomerative clustering of uniform neighbourhoods. Pattern Recogn. 21, 3 1988,261268.ANDERBERG, M. R. 1973. Cluster Analysis forApplications. Academic Press, Inc., NewYork, NY.AUGUSTSON, J. G. AND MINKER, J. 1970. Ananalysis of some graph theoretical clusteringtechniques. J. ACM 17, 4 Oct. 1970, 571588.BABU, G. P. AND MURTY, M. N. 1993. A nearoptimal initial seed value selection inKmeans algorithm using a genetic algorithm.Pattern Recogn. Lett. 14, 10 Oct. 1993, 763769.BABU, G. P. AND MURTY, M. N. 1994. Clusteringwith evolution strategies. Pattern Recogn.27, 321329.BABU, G. P., MURTY, M. N., AND KEERTHI, S.S. 2000. Stochastic connectionist approachfor pattern clustering To appear. IEEETrans. Syst. Man Cybern..BACKER, F. B. AND HUBERT, L. J. 1976. A graphtheoretic approach to goodnessoffit in completelink hierarchical clustering. J. Am.Stat. Assoc. 71, 870878.BACKER, E. 1995. ComputerAssisted Reasoningin Cluster Analysis. Prentice Hall International UK Ltd., Hertfordshire, UK.BAEZAYATES, R. A. 1992. Introduction to datastructures and algorithms related to information retrieval. In Information RetrievalData Structures and Algorithms, W. B.Frakes and R. BaezaYates, Eds. PrenticeHall, Inc., Upper Saddle River, NJ, 1327.BAJCSY, P. 1997. Hierarchical segmentationand clustering using similarity analysis. Ph.D. Dissertation. Department ofComputer Science, University of Illinois atUrbanaChampaign, Urbana, IL.BALL, G. H. AND HALL, D. J. 1965. ISODATA, anovel method of data analysis and classification. Tech. Rep.. Stanford University,Stanford, CA.BENTLEY, J. L. AND FRIEDMAN, J. H. 1978. Fastalgorithms for constructing minimal spanningtrees in coordinate spaces. IEEE Trans.Comput. C27, 6 June, 97105.BEZDEK, J. C. 1981. Pattern Recognition WithFuzzy Objective Function Algorithms. Plenum Press, New York, NY.BHUYAN, J. N., RAGHAVAN, V. V., AND VENKATESH,K. E. 1991. Genetic algorithm for clustering with an ordered representation. In Proceedings of the Fourth International Conference on Genetic Algorithms, 408415.BISWAS, G., WEINBERG, J., AND LI, C. 1995. AConceptual Clustering Method for KnowledgeDiscovery in Databases. Editions Technip.BRAILOVSKY, V. L. 1991. A probabilistic approach to clustering. Pattern Recogn. Lett.12, 4 Apr. 1991, 193198.Data Clustering  317ACM Computing Surveys, Vol. 31, No. 3, September 1999BRODATZ, P. 1966. Textures A Photographic Album for Artists and Designers. Dover Publications, Inc., Mineola, NY.CAN, F. 1993. Incremental clustering for dynamic information processing. ACM Trans.Inf. Syst. 11, 2 Apr. 1993, 143164.CARPENTER, G. AND GROSSBERG, S. 1990. ART3Hierarchical search using chemical transmitters in selforganizing pattern recognition architectures. Neural Networks 3, 129152.CHEKURI, C., GOLDWASSER, M. H., RAGHAVAN, P.,AND UPFAL, E. 1997. Web search using automatic classification. In Proceedings of theSixth International Conference on the WorldWide Web Santa Clara, CA, Apr., httptheory.stanford.edupeoplewasspublicationsWeb SearchWeb Search.html.CHENG, C. H. 1995. A branchandbound clustering algorithm. IEEE Trans. Syst. ManCybern. 25, 895898.CHENG, Y. AND FU, K. S. 1985. Conceptual clustering in knowledge organization. IEEETrans. Pattern Anal. Mach. Intell. 7, 592598.CHENG, Y. 1995. Mean shift, mode seeking, andclustering. IEEE Trans. Pattern Anal. Mach.Intell. 17, 7 July, 790799.CHIEN, Y. T. 1978. Interactive Pattern Recognition. Marcel Dekker, Inc., New York, NY.CHOUDHURY, S. AND MURTY, M. N. 1990. A divisive scheme for constructing minimal spanning trees in coordinate space. PatternRecogn. Lett. 11, 6 Jun. 1990, 385389.1996. Special issue on data mining. Commun.ACM 39, 11.COLEMAN, G. B. AND ANDREWS, H.C. 1979. Image segmentation by clustering. Proc. IEEE 67, 5, 773785.CONNELL, S. AND JAIN, A. K. 1998. Learningprototypes for online handwritten digits. InProceedings of the 14th International Conference on Pattern Recognition Brisbane, Australia, Aug., 182184.CROSS, S. E., Ed. 1996. Special issue on datamining. IEEE Expert 11, 5 Oct..DALE, M. B. 1985. On the comparison of conceptual clustering and numerical taxonomy. IEEE Trans. Pattern Anal. Mach. Intell.7, 241244.DAVE, R. N. 1992. Generalized fuzzy Cshellsclustering and detection of circular and elliptic boundaries. Pattern Recogn. 25, 713722.DAVIS, T., Ed. 1991. The Handbook of GeneticAlgorithms. Van Nostrand Reinhold Co.,New York, NY.DAY, W. H. E. 1992. Complexity theory An introduction for practitioners of classification. In Clustering and Classification, P.Arabie and L. Hubert, Eds. World ScientificPublishing Co., Inc., River Edge, NJ.DEMPSTER, A. P., LAIRD, N. M., AND RUBIN, D.B. 1977. Maximum likelihood from incomplete data via the EM algorithm. J. RoyalStat. Soc. B. 39, 1, 138.DIDAY, E. 1973. The dynamic cluster method innonhierarchical clustering. J. Comput. Inf.Sci. 2, 6188.DIDAY, E. AND SIMON, J. C. 1976. Clusteringanalysis. In Digital Pattern Recognition, K.S. Fu, Ed. SpringerVerlag, Secaucus, NJ,4794.DIDAY, E. 1988. The symbolic approach in clustering. In Classification and Related Methods, H. H. Bock, Ed. NorthHolland Publishing Co., Amsterdam, The Netherlands.DORAI, C. AND JAIN, A. K. 1995. Shape spectrabased view grouping for freeform objects. InProceedings of the International Conference onImage Processing ICIP95, 240243.DUBES, R. C. AND JAIN, A. K. 1976. Clusteringtechniques The users dilemma. PatternRecogn. 8, 247260.DUBES, R. C. AND JAIN, A. K. 1980. Clusteringmethodology in exploratory data analysis. InAdvances in Computers, M. C. Yovits,, Ed.Academic Press, Inc., New York, NY, 113125.DUBES, R. C. 1987. How many clusters arebestan experiment. Pattern Recogn. 20, 6Nov. 1, 1987, 645663.DUBES, R. C. 1993. Cluster analysis and relatedissues. In Handbook of Pattern Recognition Computer Vision, C. H. Chen, L. F. Pau,and P. S. P. Wang, Eds. World ScientificPublishing Co., Inc., River Edge, NJ, 332.DUBUISSON, M. P. AND JAIN, A. K. 1994. A modified Hausdorff distance for object matching. In Proceedings of the International Conference on Pattern Recognition ICPR94, 566568.DUDA, R. O. AND HART, P. E. 1973. PatternClassification and Scene Analysis. JohnWiley and Sons, Inc., New York, NY.DUNN, S., JANOS, L., AND ROSENFELD, A. 1983.Bimean clustering. Pattern Recogn. Lett. 1,169173.DURAN, B. S. AND ODELL, P. L. 1974. ClusterAnalysis A Survey. SpringerVerlag, NewYork, NY.EDDY, W. F., MOCKUS, A., AND OUE, S. 1996.Approximate single linkage cluster analysis oflarge data sets in highdimensional spaces.Comput. Stat. Data Anal. 23, 1, 2943.ETZIONI, O. 1996. The WorldWide Web quagmire or gold mine Commun. ACM 39, 11,6568.EVERITT, B. S. 1993. Cluster Analysis. EdwardArnold, Ltd., London, UK.FABER, V. 1994. Clustering and the continuouskmeans algorithm. Los Alamos Science 22,138144.FABER, V., HOCHBERG, J. C., KELLY, P. M., THOMAS,T. R., AND WHITE, J. M. 1994. Concept extraction A datamining technique. LosAlamos Science 22, 122149.FAYYAD, U. M. 1996. Data mining and knowledge discovery Making sense out of data.IEEE Expert 11, 5 Oct., 2025.318  A. Jain et al.ACM Computing Surveys, Vol. 31, No. 3, September 1999FISHER, D. AND LANGLEY, P. 1986. Conceptualclustering and its relation to numerical taxonomy. In Artificial Intelligence and Statistics, A W. Gale, Ed. AddisonWesley Longman Publ. Co., Inc., Reading, MA, 77116.FISHER, D. 1987. Knowledge acquisition via incremental conceptual clustering. Mach.Learn. 2, 139172.FISHER, D., XU, L., CARNES, R., RICH, Y., FENVES, S.J., CHEN, J., SHIAVI, R., BISWAS, G., AND WEINBERG, J. 1993. Applying AI clustering toengineering tasks. IEEE Expert 8, 5160.FISHER, L. AND VAN NESS, J. W. 1971.Admissible clustering procedures. Biometrika58, 91104.FLYNN, P. J. AND JAIN, A. K. 1991. BONSAI 3Dobject recognition using constrained search.IEEE Trans. Pattern Anal. Mach. Intell. 13,10 Oct. 1991, 10661075.FOGEL, D. B. AND SIMPSON, P. K. 1993. Evolvingfuzzy clusters. In Proceedings of the International Conference on Neural Networks SanFrancisco, CA, 18291834.FOGEL, D. B. AND FOGEL, L. J., Eds. 1994. Special issue on evolutionary computation.IEEE Trans. Neural Netw. Jan..FOGEL, L. J., OWENS, A. J., AND WALSH, M.J. 1965. Artificial Intelligence ThroughSimulated Evolution. John Wiley and Sons,Inc., New York, NY.FRAKES, W. B. AND BAEZAYATES, R., Eds.1992. Information Retrieval Data Structures and Algorithms. PrenticeHall, Inc.,Upper Saddle River, NJ.FRED, A. L. N. AND LEITAO, J. M. N. 1996. Aminimum code length technique for clusteringof syntactic patterns. In Proceedings of theInternational Conference on Pattern Recognition Vienna, Austria, 680684.FRED, A. L. N. 1996. Clustering of sequencesusing a minimum grammar complexity criterion. In Grammatical Inference LearningSyntax from Sentences, L. Miclet and C.Higuera, Eds. SpringerVerlag, Secaucus,NJ, 107116.FU, K. S. AND LU, S. Y. 1977. A clustering procedure for syntactic patterns. IEEE Trans.Syst. Man Cybern. 7, 734742.FU, K. S. AND MUI, J. K. 1981. A survey onimage segmentation. Pattern Recogn. 13,316.FUKUNAGA, K. 1990. Introduction to StatisticalPattern Recognition. 2nd ed. AcademicPress Prof., Inc., San Diego, CA.GLOVER, F. 1986. Future paths for integer programming and links to artificial intelligence.Comput. Oper. Res. 13, 5 May 1986, 533549.GOLDBERG, D. E. 1989. Genetic Algorithms inSearch, Optimization and Machine Learning.AddisonWesley Publishing Co., Inc., Redwood City, CA.GORDON, A. D. AND HENDERSON, J. T. 1977.Algorithm for Euclidean sum of squares.Biometrics 33, 355362.GOTLIEB, G. C. AND KUMAR, S. 1968. Semanticclustering of index terms. J. ACM 15, 493513.GOWDA, K. C. 1984. A feature reduction andunsupervised classification algorithm for multispectral data. Pattern Recogn. 17, 6, 667676.GOWDA, K. C. AND KRISHNA, G. 1977.Agglomerative clustering using the concept ofmutual nearest neighborhood. PatternRecogn. 10, 105112.GOWDA, K. C. AND DIDAY, E. 1992. Symbolicclustering using a new dissimilarity measure. IEEE Trans. Syst. Man Cybern. 22,368378.GOWER, J. C. AND ROSS, G. J. S. 1969. Minimumspanning rees and singlelinkage clusteranalysis. Appl. Stat. 18, 5464.GREFENSTETTE, J 1986. Optimization of controlparameters for genetic algorithms. IEEETrans. Syst. Man Cybern. SMC16, 1 Jan.Feb. 1986, 122128.HARALICK, R. M. AND KELLY, G. L. 1969.Pattern recognition with measurement spaceand spatial clustering for multiple images.Proc. IEEE 57, 4, 654665.HARTIGAN, J. A. 1975. Clustering Algorithms.John Wiley and Sons, Inc., New York, NY.HEDBERG, S. 1996. Searching for the motherlode Tales of the first data miners. IEEEExpert 11, 5 Oct., 47.HERTZ, J., KROGH, A., AND PALMER, R. G. 1991.Introduction to the Theory of Neural Computation. Santa Fe Institute Studies in the Sciences of Complexity lecture notes. AddisonWesley Longman Publ. Co., Inc., Reading,MA.HOFFMAN, R. AND JAIN, A. K. 1987.Segmentation and classification of range images. IEEE Trans. Pattern Anal. Mach. Intell. PAMI9, 5 Sept. 1987, 608620.HOFMANN, T. AND BUHMANN, J. 1997. Pairwisedata clustering by deterministic annealing.IEEE Trans. Pattern Anal. Mach. Intell. 19, 1Jan., 114.HOFMANN, T., PUZICHA, J., AND BUCHMANN, J.M. 1998. Unsupervised texture segmentation in a deterministic annealing framework.IEEE Trans. Pattern Anal. Mach. Intell. 20, 8,803818.HOLLAND, J. H. 1975. Adaption in Natural andArtificial Systems. University of MichiganPress, Ann Arbor, MI.HOOVER, A., JEANBAPTISTE, G., JIANG, X., FLYNN,P. J., BUNKE, H., GOLDGOF, D. B., BOWYER, K.,EGGERT, D. W., FITZGIBBON, A., AND FISHER, R.B. 1996. An experimental comparison ofrange image segmentation algorithms. IEEETrans. Pattern Anal. Mach. Intell. 18, 7, 673689.Data Clustering  319ACM Computing Surveys, Vol. 31, No. 3, September 1999HUTTENLOCHER, D. P., KLANDERMAN, G. A., ANDRUCKLIDGE, W. J. 1993. Comparing imagesusing the Hausdorff distance. IEEE Trans.Pattern Anal. Mach. Intell. 15, 9, 850863.ICHINO, M. AND YAGUCHI, H. 1994. GeneralizedMinkowski metrics for mixed featuretypedata analysis. IEEE Trans. Syst. Man Cybern. 24, 698708.1991. Proceedings of the International Joint Conference on Neural Networks. IJCNN91.1992. Proceedings of the International Joint Conference on Neural Networks.ISMAIL, M. A. AND KAMEL, M. S. 1989.Multidimensional data clustering utilizinghybrid search strategies. Pattern Recogn. 22,1 Jan. 1989, 7589.JAIN, A. K. AND DUBES, R. C. 1988. Algorithmsfor Clustering Data. PrenticeHall advancedreference series. PrenticeHall, Inc., UpperSaddle River, NJ.JAIN, A. K. AND FARROKHNIA, F. 1991.Unsupervised texture segmentation using Gabor filters. Pattern Recogn. 24, 12 Dec.1991, 11671186.JAIN, A. K. AND BHATTACHARJEE, S. 1992. Textsegmentation using Gabor filters for automatic document processing. Mach. VisionAppl. 5, 3 Summer 1992, 169184.JAIN, A. J. AND FLYNN, P. J., Eds. 1993. ThreeDimensional Object Recognition Systems.Elsevier Science Inc., New York, NY.JAIN, A. K. AND MAO, J. 1994. Neural networksand pattern recognition. In ComputationalIntelligence Imitating Life, J. M. Zurada, R.J. Marks, and C. J. Robinson, Eds. 194212.JAIN, A. K. AND FLYNN, P. J. 1996. Image segmentation using clustering. In Advances inImage Understanding A Festschrift for AzrielRosenfeld, N. Ahuja and K. Bowyer, Eds,IEEE Press, Piscataway, NJ, 6583.JAIN, A. K. AND MAO, J. 1996. Artificial neuralnetworks A tutorial. IEEE Computer 29Mar., 3144.JAIN, A. K., RATHA, N. K., AND LAKSHMANAN, S.1997. Object detection using Gabor filters.Pattern Recogn. 30, 2, 295309.JAIN, N. C., INDRAYAN, A., AND GOEL, L. R.1986. Monte Carlo comparison of six hierarchical clustering methods on random data.Pattern Recogn. 19, 1 Jan.Feb. 1986, 9599.JAIN, R., KASTURI, R., AND SCHUNCK, B. G.1995. Machine Vision. McGrawHill seriesin computer science. McGrawHill, Inc., NewYork, NY.JARVIS, R. A. AND PATRICK, E. A. 1973.Clustering using a similarity method based onshared near neighbors. IEEE Trans. Comput. C22, 8 Aug., 10251034.JOLION, J.M., MEER, P., AND BATAOUCHE, S.1991. Robust clustering with applications incomputer vision. IEEE Trans. Pattern Anal.Mach. Intell. 13, 8 Aug. 1991, 791802.JONES, D. AND BELTRAMO, M. A. 1991. Solvingpartitioning problems with genetic algorithms.In Proceedings of the Fourth InternationalConference on Genetic Algorithms, 442449.JUDD, D., MCKINLEY, P., AND JAIN, A. K.1996. Largescale parallel data clustering.In Proceedings of the International Conferenceon Pattern Recognition Vienna, Austria, 488493.KING, B. 1967. Stepwise clustering procedures. J. Am. Stat. Assoc. 69, 86101.KIRKPATRICK, S., GELATT, C. D., JR., AND VECCHI,M. P. 1983. Optimization by simulated annealing. Science 220, 4598 May, 671680.KLEIN, R. W. AND DUBES, R. C. 1989.Experiments in projection and clustering bysimulated annealing. Pattern Recogn. 22,213220.KNUTH, D. 1973. The Art of Computer Programming. AddisonWesley, Reading, MA.KOONTZ, W. L. G., FUKUNAGA, K., AND NARENDRA,P. M. 1975. A branch and bound clusteringalgorithm. IEEE Trans. Comput. 23, 908914.KOHONEN, T. 1989. SelfOrganization and Associative Memory. 3rd ed. Springer information sciences series. SpringerVerlag, NewYork, NY.KRAAIJVELD, M., MAO, J., AND JAIN, A. K. 1995.A nonlinear projection method based on Kohonens topology preserving maps. IEEETrans. Neural Netw. 6, 548559.KRISHNAPURAM, R., FRIGUI, H., AND NASRAOUI, O.1995. Fuzzy and probabilistic shell clustering algorithms and their application to boundary detection and surface approximation.IEEE Trans. Fuzzy Systems 3, 2960.KURITA, T. 1991. An efficient agglomerativeclustering algorithm using a heap. PatternRecogn. 24, 3 1991, 205209.LIBRARY OF CONGRESS, 1990. LC classificationoutline. Library of Congress, Washington,DC.LEBOWITZ, M. 1987. Experiments with incremental concept formation. Mach. Learn. 2,103138.LEE, H.Y. AND ONG, H.L. 1996. Visualizationsupport for data mining. IEEE Expert 11, 5Oct., 6975.LEE, R. C. T., SLAGLE, J. R., AND MONG, C. T.1978. Towards automatic auditing ofrecords. IEEE Trans. Softw. Eng. 4, 441448.LEE, R. C. T. 1981. Cluster analysis and itsapplications. In Advances in InformationSystems Science, J. T. Tou, Ed. PlenumPress, New York, NY.LI, C. AND BISWAS, G. 1995. Knowledgebasedscientific discovery in geological databases.In Proceedings of the First International Conference on Knowledge Discovery and DataMining Montreal, Canada, Aug. 2021,204209.320  A. Jain et al.ACM Computing Surveys, Vol. 31, No. 3, September 1999LU, S. Y. AND FU, K. S. 1978. A sentencetosentence clustering procedure for patternanalysis. IEEE Trans. Syst. Man Cybern. 8,381389.LUNDERVOLD, A., FENSTAD, A. M., ERSLAND, L., ANDTAXT, T. 1996. Brain tissue volumes frommultispectral 3D MRI A comparative study offour classifiers. In Proceedings of the Conference of the Society on Magnetic Resonance,MAAREK, Y. S. AND BEN SHAUL, I. Z. 1996.Automatically organizing bookmarks per contents. In Proceedings of the Fifth International Conference on the World Wide WebParis, May, httpwww5conf.inria.frfichhtmlpapersessions.html.MCQUEEN, J. 1967. Some methods for classification and analysis of multivariate observations. In Proceedings of the Fifth BerkeleySymposium on Mathematical Statistics andProbability, 281297.MAO, J. AND JAIN, A. K. 1992. Texture classification and segmentation using multiresolution simultaneous autoregressive models.Pattern Recogn. 25, 2 Feb. 1992, 173188.MAO, J. AND JAIN, A. K. 1995. Artificial neuralnetworks for feature extraction and multivariate data projection. IEEE Trans. NeuralNetw. 6, 296317.MAO, J. AND JAIN, A. K. 1996. A selforganizingnetwork for hyperellipsoidal clustering HEC.IEEE Trans. Neural Netw. 7, 1629.MEVINS, A. J. 1995. A branch and bound incremental conceptual clusterer. Mach. Learn.18, 522.MICHALSKI, R., STEPP, R. E., AND DIDAY, E.1981. A recent advance in data analysisClustering objects into classes characterizedby conjunctive concepts. In Progress in Pattern Recognition, Vol. 1, L. Kanal and A.Rosenfeld, Eds. NorthHolland PublishingCo., Amsterdam, The Netherlands.MICHALSKI, R., STEPP, R. E., AND DIDAY,E. 1983. Automated construction of classifications conceptual clustering versus numerical taxonomy. IEEE Trans. Pattern Anal.Mach. Intell. PAMI5, 5 Sept., 396409.MISHRA, S. K. AND RAGHAVAN, V. V. 1994. Anempirical study of the performance of heuristic methods for clustering. In Pattern Recognition in Practice, E. S. Gelsema and L. N.Kanal, Eds. 425436.MITCHELL,T. 1997. MachineLearning. McGrawHill, Inc., New York, NY.MOHIUDDIN, K. M. AND MAO, J. 1994. A comparative study of different classifiers for handprinted character recognition. In PatternRecognition in Practice, E. S. Gelsema and L.N. Kanal, Eds. 437448.MOOR, B. K. 1988. ART 1 and Pattern Clustering. In 1988 Connectionist Summer School,Morgan Kaufmann, San Mateo, CA, 174185.MURTAGH, F. 1984. A survey of recent advancesin hierarchical clustering algorithms whichuse cluster centers. Comput. J. 26, 354359.MURTY, M. N. AND KRISHNA, G. 1980. A computationally efficient technique for data clustering. Pattern Recogn. 12, 153158.MURTY, M. N. AND JAIN, A. K. 1995. Knowledgebased clustering scheme for collection management and retrieval of library books. Pattern Recogn. 28, 949964.NAGY, G. 1968. State of the art in pattern recognition. Proc. IEEE 56, 836862.NG, R. AND HAN, J. 1994. Very large data bases.In Proceedings of the 20th International Conference on Very Large Data Bases VLDB94,Santiago, Chile, Sept., VLDB Endowment,Berkeley, CA, 144155.NGUYEN, H. H. AND COHEN, P. 1993. Gibbs random fields, fuzzy clustering, and the unsupervised segmentation of textured images. CVGIP Graph. Models Image Process. 55, 1 Jan.1993, 119.OEHLER, K. L. AND GRAY, R. M. 1995.Combining image compression and classification using vector quantization. IEEE Trans.Pattern Anal. Mach. Intell. 17, 461473.OJA, E. 1982. A simplified neuron model as aprincipal component analyzer. Bull. Math.Bio. 15, 267273.OZAWA, K. 1985. A stratificational overlappingcluster scheme. Pattern Recogn. 18, 279286.OPEN TEXT, 1999. httpindex.opentext.net.KAMGARPARSI, B., GUALTIERI, J. A., DEVANEY, J. A.,AND KAMGARPARSI, K. 1990. Clustering withneural networks. Biol. Cybern. 63, 201208.LYCOS, 1999. httpwww.lycos.com.PAL, N. R., BEZDEK, J. C., AND TSAO, E. C.K.1993. Generalized clustering networks andKohonens selforganizing scheme. IEEETrans. Neural Netw. 4, 549557.QUINLAN, J. R. 1990. Decision trees and decision making. IEEE Trans. Syst. Man Cybern. 20, 339346.RAGHAVAN, V. V. AND BIRCHAND, K. 1979. Aclustering strategy based on a formalism ofthe reproductive process in a natural system.In Proceedings of the Second InternationalConference on Information Storage and Retrieval, 1022.RAGHAVAN, V. V. AND YU, C. T. 1981. A comparison of the stability characteristics of somegraph theoretic clustering methods. IEEETrans. Pattern Anal. Mach. Intell. 3, 393402.RASMUSSEN, E. 1992. Clustering algorithms.In Information Retrieval Data Structures andAlgorithms, W. B. Frakes and R. BaezaYates,Eds. PrenticeHall, Inc., Upper SaddleRiver, NJ, 419442.RICH, E. 1983. Artificial Intelligence. McGrawHill, Inc., New York, NY.RIPLEY, B. D., Ed. 1989. Statistical Inferencefor Spatial Processes. Cambridge UniversityPress, New York, NY.ROSE, K., GUREWITZ, E., AND FOX, G. C. 1993.Deterministic annealing approach to constrained clustering. IEEE Trans. PatternAnal. Mach. Intell. 15, 785794.Data Clustering  321ACM Computing Surveys, Vol. 31, No. 3, September 1999ROSENFELD, A. AND KAK, A. C. 1982. Digital Picture Processing. 2nd ed. Academic Press,Inc., New York, NY.ROSENFELD, A., SCHNEIDER, V. B., AND HUANG, M.K. 1969. An application of cluster detectionto text and picture processing. IEEE Trans.Inf. Theor. 15, 6, 672681.ROSS, G. J. S. 1968. Classification techniquesfor large sets of data. In Numerical Taxonomy, A. J. Cole, Ed. Academic Press, Inc.,New York, NY.RUSPINI, E. H. 1969. A new approach to clustering. Inf. Control 15, 2232.SALTON, G. 1991. Developments in automatictext retrieval. Science 253, 974980.SAMAL, A. AND IYENGAR, P. A. 1992. Automaticrecognition and analysis of human faces andfacial expressions A survey. Pattern Recogn.25, 1 Jan. 1992, 6577.SAMMON, J. W. JR. 1969. A nonlinear mappingfor data structure analysis. IEEE Trans.Comput. 18, 401409.SANGAL, R. 1991. Programming Paradigms inLISP. McGrawHill, Inc., New York, NY.SCHACHTER, B. J., DAVIS, L. S., AND ROSENFELD,A. 1979. Some experiments in image segmentation by clustering of local feature values. Pattern Recogn. 11, 1928.SCHWEFEL, H. P. 1981. Numerical Optimizationof Computer Models. John Wiley and Sons,Inc., New York, NY.SELIM, S. Z. AND ISMAIL, M. A. 1984. Kmeanstype algorithms A generalized convergencetheorem and characterization of local optimality. IEEE Trans. Pattern Anal. Mach. Intell. 6, 8187.SELIM, S. Z. AND ALSULTAN, K. 1991. A simulated annealing algorithm for the clusteringproblem. Pattern Recogn. 24, 10 1991,10031008.SEN, A. AND SRIVASTAVA, M. 1990. RegressionAnalysis. SpringerVerlag, New York, NY.SETHI, I. AND JAIN, A. K., Eds. 1991. ArtificialNeural Networks and Pattern RecognitionOld and New Connections. Elsevier ScienceInc., New York, NY.SHEKAR, B., MURTY, N. M., AND KRISHNA, G.1987. A knowledgebased clustering scheme.Pattern Recogn. Lett. 5, 4 Apr. 1, 1987, 253259.SILVERMAN, J. F. AND COOPER, D. B. 1988.Bayesian clustering for unsupervised estimation of surface and texture models.IEEE Trans. Pattern Anal. Mach. Intell. 10, 4July 1988, 482495.SIMOUDIS, E. 1996. Reality check for data mining. IEEE Expert 11, 5 Oct., 2633.SLAGLE, J. R., CHANG, C. L., AND HELLER, S. R.1975. A clustering and datareorganizing algorithm. IEEE Trans. Syst. Man Cybern. 5,125128.SNEATH, P. H. A. AND SOKAL, R. R. 1973.Numerical Taxonomy. Freeman, London,UK.SPATH, H. 1980. Cluster Analysis Algorithmsfor Data Reduction and Classification. EllisHorwood, Upper Saddle River, NJ.SOLBERG, A., TAXT, T., AND JAIN, A. 1996. AMarkov random field model for classificationof multisource satellite imagery. IEEETrans. Geoscience and Remote Sensing 34, 1,100113.SRIVASTAVA, A. AND MURTY, M. N 1990. A comparison between conceptual clustering andconventional clustering. Pattern Recogn. 23,9 1990, 975981.STAHL, H. 1986. Cluster analysis of large datasets. In Classification as a Tool of Research,W. Gaul and M. Schader, Eds. ElsevierNorthHolland, Inc., New York, NY, 423430.STEPP, R. E. AND MICHALSKI, R. S. 1986.Conceptual clustering of structured objects Agoaloriented approach. Artif. Intell. 28, 1Feb. 1986, 4369.SUTTON, M., STARK, L., AND BOWYER, K.1993. Functionbased generic recognition formultiple object categories. In ThreeDimensional Object Recognition Systems, A. Jainand P. J. Flynn, Eds. Elsevier Science Inc.,New York, NY.SYMON, M. J. 1977. Clustering criterion andmultivariate normal mixture. Biometrics77, 3543.TANAKA, E. 1995. Theoretical aspects of syntactic pattern recognition. Pattern Recogn. 28,10531061.TAXT, T. AND LUNDERVOLD, A. 1994. Multispectral analysis of the brain using magneticresonance imaging. IEEE Trans. MedicalImaging 13, 3, 470481.TITTERINGTON, D. M., SMITH, A. F. M., AND MAKOV,U. E. 1985. Statistical Analysis of FiniteMixture Distributions. John Wiley and Sons,Inc., New York, NY.TOUSSAINT, G. T. 1980. The relative neighborhood graph of a finite planar set. PatternRecogn. 12, 261268.TRIER, O. D. AND JAIN, A. K. 1995. Goaldirected evaluation of binarization methods.IEEE Trans. Pattern Anal. Mach. Intell. 17,11911201.UCHIYAMA, T. AND ARBIB, M. A. 1994. Color imagesegmentation using competitive learning.IEEE Trans. Pattern Anal. Mach. Intell. 16, 12Dec. 1994, 11971206.URQUHART, R. B. 1982. Graph theoretical clustering based on limited neighborhoodsets. Pattern Recogn. 15, 173187.VENKATESWARLU, N. B. AND RAJU, P. S. V. S. K.1992. Fast ISODATA clustering algorithms.Pattern Recogn. 25, 3 Mar. 1992, 335342.VINOD, V. V., CHAUDHURY, S., MUKHERJEE, J., ANDGHOSE, S. 1994. A connectionist approachfor clustering with applications in imageanalysis. IEEE Trans. Syst. Man Cybern. 24,365384.322  A. Jain et al.ACM Computing Surveys, Vol. 31, No. 3, September 1999WAH, B. W., Ed. 1996. Special section on mining of databases. IEEE Trans. Knowl. DataEng. Dec..WARD, J. H. JR. 1963. Hierarchical grouping tooptimize an objective function. J. Am. Stat.Assoc. 58, 236244.WATANABE, S. 1985. Pattern Recognition Human and Mechanical. John Wiley and Sons,Inc., New York, NY.WESZKA, J. 1978. A survey of threshold selection techniques. Pattern Recogn. 7, 259265.WHITLEY, D., STARKWEATHER, T., AND FUQUAY,D. 1989. Scheduling problems and traveling salesman the genetic edge recombination. In Proceedings of the Third International Conference on Genetic AlgorithmsGeorge Mason University, June 47, J. D.Schaffer, Ed. Morgan Kaufmann PublishersInc., San Francisco, CA, 133140.WILSON, D. R. AND MARTINEZ, T. R. 1997.Improved heterogeneous distance functions. J. Artif. Intell. Res. 6, 134.WU, Z. AND LEAHY, R. 1993. An optimal graphtheoretic approach to data clustering Theoryand its application to image segmentation.IEEE Trans. Pattern Anal. Mach. Intell. 15,11011113.WULFEKUHLER, M. AND PUNCH, W. 1997. Findingsalient features for personal web page categories.In Proceedings of the Sixth International Conference on the World Wide Web Santa Clara,CA, Apr., httptheory.stanford.edupeoplewasspublicationsWeb SearchWeb Search.html.ZADEH, L. A. 1965. Fuzzy sets. Inf. Control 8,338353.ZAHN, C. T. 1971. Graphtheoretical methodsfor detecting and describing gestalt clusters.IEEE Trans. Comput. C20 Apr., 6886.ZHANG, K. 1995. Algorithms for the constrainedediting distance between ordered labeledtrees and related problems. Pattern Recogn.28, 463474.ZHANG, J. AND MICHALSKI, R. S. 1995. An integration of rule induction and exemplarbasedlearning for graded concepts. Mach. Learn.21, 3 Dec. 1995, 235267.ZHANG, T., RAMAKRISHNAN, R., AND LIVNY, M.1996. BIRCH An efficient data clusteringmethod for very large databases. SIGMODRec. 25, 2, 103114.ZUPAN, J. 1982. Clustering of Large DataSets. Research Studies Press Ltd., Taunton,UK.Received March 1997 revised October 1998 accepted January 1999Data Clustering  323ACM Computing Surveys, Vol. 31, No. 3, September 1999
