Evaluating Collaborative FilteringRecommender SystemsJONATHAN L. HERLOCKEROregon State UniversityandJOSEPH A. KONSTAN, LOREN G. TERVEEN, and JOHN T. RIEDLUniversity of MinnesotaRecommender systems have been evaluated in many, often incomparable, ways. In this article,we review the key decisions in evaluating collaborative filtering recommender systems the usertasks being evaluated, the types of analysis and datasets being used, the ways in which predictionquality is measured, the evaluation of prediction attributes other than quality, and the userbasedevaluation of the system as a whole. In addition to reviewing the evaluation strategies used by priorresearchers, we present empirical results from the analysis of various accuracy metrics on one content domain where all the tested metrics collapsed roughly into three equivalence classes. Metricswithin each equivalency class were strongly correlated, while metrics from different equivalencyclasses were uncorrelated.Categories and Subject Descriptors H.3.4 Information Storage and Retrieval Systems andSoftwareperformance Evaluation efficiency and effectivenessGeneral Terms Experimentation, Measurement, PerformanceAdditional Key Words and Phrases Collaborative filtering, recommender systems, metrics,evaluation1. INTRODUCTIONRecommender systems use the opinions of a community of users to help individuals in that community more effectively identify content of interest froma potentially overwhelming set of choices Resnick and Varian 1997. One ofThis research was supported by the National Science Foundation NSF under grants DGE 9554517, IIS 9613960, IIS 9734442, IIS 9978717, IIS 0102229, and IIS 0133994, and by NetPerceptions, Inc.Authors addresses J. L. Herlocker, School of Electrical Engineering and Computer Science,Oregon State University, 102 Dearborn Hall, Corvallis, OR 97331 email herlockcs.orst.edu J. A.Konstan, L. G. Terveen, and J. T. Riedl, Department of Computer Science and Engineering, University of Minnesota, 4192 EECS Building, 200 Union Street SE, Minneapolis, MN 55455 emailkonstan, terveen, riedlcs.umn.edu.Permission to make digital or hard copies of part or all of this work for personal or classroom use isgranted without fee provided that copies are not made or distributed for profit or direct commercialadvantage and that copies show this notice on the first page or initial screen of a display alongwith the full citation. Copyrights for components of this work owned by others than ACM must behonored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers,to redistribute to lists, or to use any component of this work in other works requires prior specificpermission andor a fee. Permissions may be requested from Publications Dept., ACM, Inc., 1515Broadway, New York, NY 10036 USA, fax 1 212 8690481, or permissionsacm.org.C 2004 ACM 104681880401000005 5.00ACM Transactions on Information Systems, Vol. 22, No. 1, January 2004, Pages 553.6  J. L. Herlocker et al.the most successful technologies for recommender systems, called collaborative filtering, has been developed and improved over the past decade to thepoint where a wide variety of algorithms exist for generating recommendations. Each algorithmic approach has adherents who claim it to be superior forsome purpose. Clearly identifying the best algorithm for a given purpose hasproven challenging, in part because researchers disagree on which attributesshould be measured, and on which metrics should be used for each attribute. Researchers who survey the literature will find over a dozen quantitative metricsand additional qualitative evaluation techniques.Evaluating recommender systems and their algorithms is inherently difficult for several reasons. First, different algorithms may be better or worse ondifferent data sets. Many collaborative filtering algorithms have been designedspecifically for data sets where there are many more users than items e.g., theMovieLens data set has 65,000 users and 5,000 movies. Such algorithms maybe entirely inappropriate in a domain where there are many more items thanusers e.g., a research paper recommender with thousands of users but tens orhundreds of thousands of articles to recommend. Similar differences exist forratings density, ratings scale, and other properties of data sets.The second reason that evaluation is difficult is that the goals for whichan evaluation is performed may differ. Much early evaluation work focusedspecifically on the accuracy of collaborative filtering algorithms in predicting withheld ratings. Even early researchers recognized, however, that whenrecommenders are used to support decisions, it can be more valuable to measurehow often the system leads its users to wrong choices. Shardanand and Maes1995 measured reversalslarge errors between the predicted and actualrating we have used the signalprocessing measure of the Receiver OperatingCharacteristic curve Swets 1963 to measure a recommenders potential as afilter Konstan et al. 1997. Other work has speculated that there are propertiesdifferent from accuracy that have a larger effect on user satisfaction and performance. A range of research and systems have looked at measures including thedegree to which the recommendations cover the entire set of items Mobasheret al. 2001, the degree to which recommendations made are nonobvious McNeeet al. 2002, and the ability of recommenders to explain their recommendationsto users Sinha and Swearingen 2002. A few researchers have argued thatthese issues are all details, and that the bottomline measure of recommendersystem success should be user satisfaction. Commercial systems measure usersatisfaction by the number of products purchased and not returned, whilenoncommercial systems may just ask users how satisfied they are.Finally, there is a significant challenge in deciding what combination of measures to use in comparative evaluation. We have noticed a trend recentlymanyresearchers find that their newest algorithms yield a mean absolute error of0.73 on a fivepoint rating scale on movie rating datasets. Though the new algorithms often appear to do better than the older algorithms they are comparedto, we find that when each algorithm is tuned to its optimum, they all producesimilar measures of quality. Weand othershave speculated that we may bereaching some magic barrier where natural variability may prevent us fromgetting much more accurate. In support of this, Hill et al. 1995 have shownACM Transactions on Information Systems, Vol. 22, No. 1, January 2004.Evaluating Collaborative Filtering Recommender Systems  7that users provide inconsistent ratings when asked to rate the same movie atdifferent times. They suggest that an algorithm cannot be more accurate thanthe variance in a users ratings for the same item.Even when accuracy differences are measurable, they are usually tiny. On afivepoint rating scale, are users sensitive to a change in mean absolute errorof 0.01 These observations suggest that algorithmic improvements in collaborative filtering systems may come from different directions than just continuedimprovements in mean absolute error. Perhaps the best algorithms should bemeasured in accordance with how well they can communicate their reasoningto users, or with how little data they can yield accurate recommendations. Ifthis is true, new metrics will be needed to evaluate these new algorithms.This article presents six specific contributions towards evaluation of recommender systems.1 We introduce a set of recommender tasks that categorize the user goals fora particular recommender system.2 We discuss the selection of appropriate datasets for evaluation. We explorewhen evaluation can be completed offline using existing datasets and whenit requires online experimentation. We briefly discuss synthetic data setsand more extensively review the properties of datasets that should be considered in selecting them for evaluation.3 We survey evaluation metrics that have been used to evaluation recommender systems in the past, conceptually analyzing their strengths andweaknesses.4 We report on experimental results comparing the outcomes of a set of different accuracy evaluation metrics on one data set. We show that the metricscollapse roughly into three equivalence classes.5 By evaluating a wide set of metrics on a dataset, we show that for somedatasets, while many different metrics are strongly correlated, there areclasses of metrics that are uncorrelated.6 We review a wide range of nonaccuracy metrics, including measures of thedegree to which recommendations cover the set of items, the novelty andserendipity of recommendations, and user satisfaction and behavior in therecommender system.Throughout our discussion, we separate out our review of what has beendone before in the literature from the introduction of new tasks and methods.We expect that the primary audience of this article will be collaborative filtering researchers who are looking to evaluate new algorithms against previousresearch and collaborative filtering practitioners who are evaluating algorithmsbefore deploying them in recommender systems.There are certain aspects of recommender systems that we have specificallyleft out of the scope of this paper. In particular, we have decided to avoid the largearea of marketinginspired evaluation. There is extensive work on evaluatingmarketing campaigns based on such measures as offer acceptance and saleslift Rogers 2001. While recommenders are widely used in this area, we cannot add much to existing coverage of this topic. We also do not address generalACM Transactions on Information Systems, Vol. 22, No. 1, January 2004.8  J. L. Herlocker et al.usability evaluation of the interfaces. That topic is well covered in the researchand practitioner literature e.g., Helander 1988 and Nielsen 1994 We havechosen not to discuss computation performance of recommender algorithms.Such performance is certainly important, and in the future we expect there tobe work on the quality of timelimited and memorylimited recommendations.This area is just emerging, however see for example Miller et al.s recent workon recommendation on handheld devices Miller et al. 2003, and there is notyet enough research to survey and synthesize. Finally, we do not address theemerging question of the robustness and transparency of recommender algorithms. We recognize that recommender system robustness to manipulation byattacks and transparency that discloses manipulation by system operators isimportant, but substantially more work needs to occur in this area before therewill be accepted metrics for evaluating such robustness and transparency.The remainder of the article is arranged as followsSection 2. We identify the key user tasks from which evaluation methodshave been determined and suggest new tasks that have not been evaluatedextensively.Section 3. A discussion regarding the factors that can affect selection of adata set on which to perform evaluation.Section 4. An investigation of metrics that have been used in evaluating theaccuracy of collaborative filtering predictions and recommendations. Accuracy has been by far the most commonly published evaluation method forcollaborative filtering systems. This section also includes the results from anempirical study of the correlations between metrics.Section 5. A discussion of metrics that evaluate dimensions other than accuracy. In addition to covering the dimensions and methods that have beenused in the literature, we introduce new dimensions on which we believeevaluation should be done.Section 6. Final conclusions, including a list of areas were we feel future workis particularly warranted.Sections 25 are ordered to discuss the steps of evaluation in roughly the orderthat we would expect an evaluator to take. Thus, Section 2 describes the selection of appropriate user tasks, Section 3 discusses the selection of a dataset,and Sections 4 and 5 discuss the alternative metrics that may be applied to thedataset chosen. We begin with the discussion of user tasksthe user task setsthe entire context for evaluation.2. USER TASKS FOR RECOMMENDER SYSTEMSTo properly evaluate a recommender system, it is important to understand thegoals and tasks for which it is being used. In this article, we focus on endusergoals and tasks as opposed to goals of marketers and other system stakeholders. We derive these tasks from the research literature and from deployed systems. For each task, we discuss its implications for evaluation. While the tasksweve identified are important ones, based on our experience in recommendersystems research and from our review of published research, we recognize thatACM Transactions on Information Systems, Vol. 22, No. 1, January 2004.Evaluating Collaborative Filtering Recommender Systems  9the list is necessarily incomplete. As researchers and developers move into newrecommendation domains, we expect they will find it useful to supplement thislist andor modify these tasks with domainspecific ones. Our goal is primarilyto identify domainindependent task descriptions to help distinguish amongdifferent evaluation measures.We have identified two user tasks that have been discussed at length withinthe collaborative filtering literatureAnnotation in Context. The original recommendation scenario was filteringthrough structured discussion postings to decide which ones were worth reading. Tapestry Goldberg et al. 1992 and GroupLens Resnick et al. 1994 bothapplied this to already structured message databases. This task required retaining the order and context of messages, and accordingly used predictions toannotate messages in context. In some cases the worst messages were filteredout. This same scenario, which uses a recommender in an existing context, hasalso been used by web recommenders that overlay prediction information ontop of existing links Wexelblat and Maes 1999. Users use the displayed predictions to decide which messages to read or which links to follow, and thereforethe most important factor to evaluate is how successfully the predictions helpusers distinguish between desired and undesired content. A major factor is thewhether the recommender can generate predictions for the items that the useris viewing.Find Good Items. Soon after Tapestry and GroupLens, several systemswere developed with a more direct focus on actual recommendation. RingoShardanand and Maes 1995 and the Bellcore Video Recommender Hill et al.1995 both provided interfaces that would suggest specific items to their users,providing users with a ranked list of the recommended items, along with predictions for how much the users would like them. This is the core recommendationtask and it recurs in a wide variety of research and commercial systems. Inmany commercial systems, the best bet recommendations are shown, but thepredicted rating values are not.While these two tasks can be identified quite generally across many differentdomains, there are likely to be many specializations of the above tasks withineach domain. We introduce some of the characteristics of domains that influencethose specializations in Section 3.3.While the Annotation in Context and the Find Good Items are overwhelmingly the most commonly evaluated tasks in the literature, there are otherimportant generic tasks that are not well described in the research literature.Below we describe several other user tasks that we have encountered in our interviews with users and our discussions with recommender system designers.We mention these tasks because we believe that they should be evaluated, butbecause they have not been addressed in the recommender systems literature,we do not discuss them further.Find All Good Items. Most recommender tasks focus on finding some gooditems. This is not surprising the problem that led to recommender systemswas one of overload, and most users seem willing to live with overlooking someACM Transactions on Information Systems, Vol. 22, No. 1, January 2004.10  J. L. Herlocker et al.good items in order to screen out many bad ones. Our discussions with firmsin the legal databases industry, however, led in the opposite direction. Lawyerssearching for precedents feel it is very important not to overlook a single possiblecase. Indeed, they are willing to invest large amounts of time and their clientsmoney searching for that case. To use recommenders in their practice, theyfirst need to be assured that the false negative rate can be made sufficientlylow. As with annotation in context, coverage becomes particularly important inthis task.Recommend Sequence. We first noticed this task when using the personalized radio web site Launch launch.yahoo.com which streams music based on avariety of recommender algorithms. Launch has several interesting factors, including the desirability of recommending already rated items, though not toooften. What intrigued us, though, is the challenge of moving from recommending one song at a time to recommending a sequence that is pleasing as a whole.This same task can apply to recommending research papers to learn about afield read this introduction, then that survey, . . . . While data mining researchhas explored product purchase timing and sequences, we are not aware of anyrecommender applications or research that directly address this task.Just Browsing. Recommenders are usually evaluated based on how wellthey help the user make a consumption decision. In talking with users of ourMovieLens system, of Amazon.com, and of several other sites, we discoveredthat many of them use the site even when they have no purchase imminent.They find it pleasant to browse. Whether one models this activity as learningor simply as entertainment, it seems that a substantial use of recommendersis simply using them without an ulterior motive. For those cases, the accuracyof algorithms may be less important than the interface, the ease of use, and thelevel and nature of information provided.Find Credible Recommender. This is another task gleaned from discussionswith users. It is not surprising that users do not automatically trust a recommender. Many of them play around for a while to see if the recommendermatches their tastes well. Weve heard many complaints from users who arelooking up their favorite or least favorite movies on MovieLensthey dontdo this to learn about the movie, but to check up on us. Some users even gofurther. Especially on commercial sites, they try changing their profiles to seehow the recommended items change. They explore the recommendations to tryto find any hints of bias. A recommender optimized to produce useful recommendations e.g., recommendations for items that the user does not alreadyknow about may fail to appear trustworthy because it does not recommendmovies the user is sure to enjoy but probably already knows about. We are notaware of any research on how to make a recommender appear credible, thoughthere is more general research on making websites evoke trust Bailey et al.2001.Most evaluations of recommender systems focus on the recommendationshowever if users dont rate items, then collaborative filtering recommender systems cant provide recommendations. Thus, evaluating if and why users wouldACM Transactions on Information Systems, Vol. 22, No. 1, January 2004.Evaluating Collaborative Filtering Recommender Systems  11contribute ratings may be important to communicate that a recommender system is likely to be successful. We will briefly introduce several different ratingtasks.Improve Profile. the rating task that most recommender systems have assumed. Users contribute ratings because they believe that they are improvingtheir profile and thus improving the quality of the recommendations that theywill receive.Express Self. Some users may not care about the recommendationswhatis important to them is that they be allowed to contribute their ratings. Manyusers simply want a forum for expressing their opinions. We conducted interviews with power users of MovieLens that had rated over 1000 movies someover 2000 movies. What we learned was that these users were not rating toimprove their recommendations. They were rating because it felt good. We particularly see this effect on sites like Amazon.com, where users can post reviewsratings of items sold by Amazon. For users with this task, issues may include the level of anonymity which can be good or bad, depending on the user,the feeling of contribution, and the ease of making the contribution. Whilerecommender algorithms themselves may not evoke selfexpression, encouraging selfexpression may provide more data which can improve the quality ofrecommendations.Help Others. Some users are happy to contribute ratings in recommendersystems because they believe that the community benefits from their contribution. In many cases, they are also entering ratings in order to express themselves see previous task. However, the two do not always go together.Influence Others. An unfortunate fact that we and other implementers ofwebbased recommender systems have encountered is that there are users ofrecommender systems whose goal is to explicitly influence others into viewing orpurchasing particular items. For example, advocates of particular movie genresor movie studios will frequently rate movies high on the MovieLens web siteright before the movie is released to try and push others to go and see the movie.This task is particularly interesting, because we may want to evaluate how wellthe system prevents this task.While we have briefly mentioned tasks involved in contributing ratings, wewill not discuss them in depth in this paper, and rather focus on the tasksrelated to recommendation.We must once again say that the list of tasks in this section is not comprehensive. Rather, we have used our experience in the field to filter out the taskcategories that a have been most significant in the previously published work,and b that we feel are significant, but have not been considered sufficiently.In the field of HumanComputer Interaction, it has been strongly arguedthat the evaluation process should begin with an understanding of the usertasks that the system will serve. When we evaluate recommender systems fromthe perspective of benefit to the user, we should also start by identifying themost important task for which the recommender will be used. In this section,we have provided descriptions of the most significant tasks that have beenACM Transactions on Information Systems, Vol. 22, No. 1, January 2004.12  J. L. Herlocker et al.identified. Evaluators should consider carefully which of the tasks describedmay be appropriate for their environment.Once the proper tasks have been identified, the evaluator must select adataset to which evaluation methods can be applied, a process that will mostlikely be constrained by the user tasks identified.3. SELECTING DATA SETS FOR EVALUATIONSeveral key decisions regarding data sets underlie successful evaluation of arecommender system algorithm. Can the evaluation be carried out offline on anexisting data set or does it require live user tests If a data set is not currentlyavailable, can evaluation be performed on simulated data What propertiesshould the dataset have in order to best model the tasks for which the recommender is being evaluated A few examples help clarify these decisionsWhen designing a recommender algorithm designed to recommend word processing commands e.g., Linton et al. 1998, one can expect users to have experienced 510 or more of the candidates. Accordingly, it would be unwiseto select recommender algorithms based on evaluation results from movie orecommerce datasets where ratings sparsity is much worse.When evaluating a recommender algorithm in the context of the Find GoodItems task where novel items are desired, it may be inappropriate to useonly offline evaluation. Since the recommender algorithm is generating recommendations for items that the user does not already know about, it isprobable that the data set will not provide enough information to evaluatethe quality of the items being recommended. If an item was truly unknownto the user, then it is probable that there is no rating for that user in thedatabase. If we perform a live user evaluation, ratings can be gained on thespot for each item recommended.When evaluating a recommender in a new domain where there is significantresearch on the structure of user preferences, but no data sets, it may be appropriate to first evaluate algorithms against synthetic data sets to identifythe promising ones for further study.We will examine in the following subsections each of the decisions that weposed in the first paragraph of this section, and then discuss the past andcurrent trends in research with respect to collaborative filtering data sets.3.1 Live User Experiments vs. Offline AnalysesEvaluations can be completed using offline analysis, a variety of live user experimental methods, or a combination of the two. Much of the work in algorithmevaluation has focused on offline analysis of predictive accuracy. In such anevaluation, the algorithm is used to predict certain withheld values from adataset, and the results are analyzed using one or more of the metrics discussed in the following section. Such evaluations have the advantage that it isquick and economical to conduct large evaluations, often on several differentdatasets or algorithms at once. Once a dataset is available, conducting such anexperiment simply requires running the algorithm on the appropriate subset ofACM Transactions on Information Systems, Vol. 22, No. 1, January 2004.Evaluating Collaborative Filtering Recommender Systems  13that data. When the dataset includes timestamps, it is even possible to replaya series of ratings and recommendations offline. Each time a rating was made,the researcher first computes the prediction for that item based on all priordata then, after evaluating the accuracy of that prediction, the actual ratingis entered so the next item can be evaluated.Offline analyses have two important weaknesses. First, the natural sparsityof ratings data sets limits the set of items that can be evaluated. We cannotevaluate the appropriateness of a recommended item for a user if we do nothave a rating from that user for that item in the dataset. Second, they arelimited to objective evaluation of prediction results. No offline analysis candetermine whether users will prefer a particular system, either because of itspredictions or because of other less objective criteria such as the aesthetics ofthe user interface.An alternative approach is to conduct a live user experiment. Such experiments may be controlled e.g., with random assignment of subjects to differentconditions, or they may be field studies where a particular system is madeavailable to a community of users that is then observed to ascertain the effectsof the system. As we discuss later in Section 5.5, live user experiments canevaluate user performance, satisfaction, participation, and other measures.3.2 Synthesized vs. Natural Data SetsAnother choice that researchers face is whether to use an existing datasetthat may imperfectly match the properties of the target domain and task, orto instead synthesize a dataset specifically to match those properties. In ourown early work designing recommender algorithms for Usenet News Konstanet al. 1997 Miller et al. 1997, we experimented with a variety of synthesizeddatasets. We modeled news articles as having a fixed number of propertiesand users as having preferences for those properties. Our data set generator could cluster users together, spread them evenly, or present other distributions. While these simulated data sets gave us an easy way to test algorithms for obvious flaws, they in no way accurately modeled the nature of realusers and real data. In their research on horting as an approach for collaborative filtering, Aggarwal et al. 1999 used a similar technique, noting howeverthat such synthetic data is unfair to other algorithms because it fits theirapproach too well, and that this is a placeholder until they can deploy theirtrial.Synthesized data sets may be required in some limited cases, but only as earlysteps while gathering data sets or constructing complete systems. Drawingcomparative conclusions from synthetic datasets is risky, because the data mayfit one of the algorithms better than the others.On the other hand, there is new opportunity now to explore more advancedtechniques for modeling user interest and generating synthetic data from thosemodels, now that there exists data on which to evaluate the synthetically generated data and tune the models. Such research could also lead to the development of more accurate recommender algorithms with clearly defined theoreticalproperties.ACM Transactions on Information Systems, Vol. 22, No. 1, January 2004.14  J. L. Herlocker et al.3.3 Properties of Data SetsThe final question we address in this section on data sets is what propertiesshould the dataset have in order to best model the tasks for which the recommender is being evaluated We find it useful to divide data set propertiesinto three categories Domain features reflect the nature of the content beingrecommended, rather than any particular system. Inherent features reflect thenature of the specific recommender system from which data was drawn andpossibly from its data collection practices. Sample features reflect distributionproperties of the data, and often can be manipulated by selecting the appropriate subset of a larger data set. We discuss each of these three categories here,identifying specific features within each category.Domain Features of interest includea the content topic being recommendedrated and the associated context inwhich ratingrecommendation takes placeb the user tasks supported by the recommenderc the novelty need and the quality needd the costbenefit ratio of falsetrue positivesnegativese the granularity of true user preferences.Most commonly, recommender systems have been built for entertainmentcontent domains movies, music, etc., though some testbeds exist for filteringdocument collections Usenet news, for example. Within a particular topic,there may be many contexts. Movie recommenders may operate on the web, ormay operate entirely within a video rental store or as part of a settop box ordigital video recorder.In our experience, one of the most important generic domain features to consider lies in the tradeoff between desire for novelty and desire for high quality.In certain domains, the user goal is dominated by finding recommendations forthings she doesnt already know about. McNee et al. 2002 evaluated recommenders for research papers and found that users were generally happy witha set of recommendations if there was a single item in the set that appearedto be useful and that the user wasnt already familiar with. In some ways, thismatches the conventional wisdom about supermarket recommendersit wouldbe almost always correct, but useless, to recommend bananas, bread, milk, andeggs. The recommendations might be correct, but they dont change the shoppers behavior. Opposite the desire for novelty is the desire for high quality. Intuitively, this end of the tradeoff reflects the users desire to rely heavily uponthe recommendation for a consumption decision, rather than simply as onedecisionsupport factor among many. At the extreme, the availability of highconfidence recommendations could enable automatic purchase decisions such aspersonalized book or musicofthemonth clubs. Evaluations of recommendersfor this task must evaluate the success of highconfidence recommendations,and perhaps consider the opportunity costs of excessively low confidence.Another important domain feature is the costbenefit ratio faced by usersin the domain from which items are being recommended. In the video recommender domain, the cost of false positives is low 3 and two to three hours ofACM Transactions on Information Systems, Vol. 22, No. 1, January 2004.Evaluating Collaborative Filtering Recommender Systems  15your evening, the cost of false negatives is almost zero, and the benefit of recommendations is huge an enormous quantity of movies have been released overthe years, and browsing in the video store can be quite stressfulparticularlyfor families. This analysis explains to a large extent why video recommendershave been so successful. Other domains with similar domain features, such asbooks of fiction, are likely to have datasets similar to the video domain and results demonstrated on video data may likely translate somewhat well to thoseother domains although books of fiction are likely to have different samplefeaturessee below. See Konstan et al. 1997 for a slightly more detailed discussion of costbenefit tradeoff analysis in collaborative filtering recommendersystems.Another subtle but important domain feature is the granularity of true userpreferences. How many different levels of true user preference exist With binary preferences, users only care to distinguish between good and bad itemsI dont necessarily need the best movie, only a movie I will enjoy. In such acase, distinguishing among good items is not important, nor is distinguishingamong bad items. Note that the granularity of user preference may be differentthan the range and granularity of the ratings which is an inherent feature ofdata sets. Users may rank movies on a 110 scale, but then only really care ifrecommendations are good I had a good time watching the movie or bad I wasbored out of my mind.Overall, it would probably be a mistake to evaluate an algorithm on datawith significantly different domain features. In particular, it is very importantthat the tasks your algorithm is designed to support are similar to the taskssupported by the system from which the data was collected. If the user tasksare mismatched, then there are likely to be many other feature mismatches.For example, the MovieLens system supported primarily the Find Good Itemsuser task. As the result, the user was always shown the best bets and thusthere are many more ratings for good items than bad items the user had toexplicitly request to rate a poor item in most cases. So MovieLens data isless likely to have many ratings for less popular items. It would probably beinappropriate to use this data to evaluate a new algorithm whose goal was tosupport Annotation In Context. Of course, if an algorithm is being proposedfor general use, it is best to select data sets that span a variety of topics andcontexts.Inherent features include several features about ratingsa whether ratings are explicit, implicit, or bothb the scale on which items are ratedc the dimensions of rating andd the presence or absence of a timestamp on ratings.Explicit ratings are entered by a user directly i.e., Please rate this on ascale of 15, while implicit ratings are inferred from other user behavior. Forexample, a music recommender may use implicit data such as play lists ormusic listened to, or it may use explicit scores for songs or artists, or a combination of both. The ratings scale is the range and granularity of ratings. TheACM Transactions on Information Systems, Vol. 22, No. 1, January 2004.16  J. L. Herlocker et al.simplest scale is unaryliked items are marked, all others are unknown. Unaryis common in commerce applications, where all that is known is whether theuser purchased an item or not. We call the data unary instead of binary becausea lack of purchase of item X does not necessarily mean that the user would notlike X. Binary items include a separate designation for disliked. Systems thatoperate on explicit ratings often support 5point, 7point, or 100point scales.While most recommenders have had only a single rating dimension describedby Miller et al. 1997 as what predictions should we have displayed for thisitem, both research and commercial systems are exploring systems whereusers can enter several ratings for a single item. Zagats restaurant guides, forexample, traditionally use food, service, and decor as three independent dimensions. Movie recommenders may separate story, acting, and special effects. Datasets with multiple dimensions are still difficult to find, but we expect severalto become available in the future. Timestamps are a property of the data collection, and are particularly important in areas where user tastes are expectedto change or where user reactions to items likely depend on their history ofinteraction with other items.Other inherent features concern the data collection practicese whether the recommendations displayed to the user were recorded andf the availability of user demographic information or item contentinformation.Unfortunately, few datasets recorded the recommendations that were displayed, making it difficult to retrospectively isolate, for example, ratings thatcould not have been biased by previously displayed predictions. Some logs maykeep timestamped queries, which could be used to reconstruct recommendations if the algorithm is known and fully deterministic. The availability of demographic data varies with the specific system, and with the specific data collected.The EachMovie and MovieLens datasets both collected limited demographics.Researchers speculate, however, that a large percentage of the demographicanswers may be false based on user suspicion of marketing questions. Wewould expect greater reliability for demographic data that users believe actuallyserves a constructive purpose in the recommender either for recommendationor for related purposes. A film recommender that uses zip code to narrow thetheater search, such as Miller et al.s 2003 MovieLens Unplugged, seems morelikely to provide meaningful data.Finally, we considerg the biases involved in data collection.Most data sets have biases based on the mechanism by which users have theopportunity to rate items. For example, Jester Goldberg et al. 2001 asked allusers to rate the same initial jokes, creating a set of dense ratings for thosejokes which would not otherwise occur. MovieLens has experimented with different methods to select items to have the firsttime user rate before using therecommender system Rashid et al. 2002, and in the process demonstrated thateach method leads to a different bias in initial ratings.ACM Transactions on Information Systems, Vol. 22, No. 1, January 2004.Evaluating Collaborative Filtering Recommender Systems  17Sample features include many of the statistical properties that are commonlyconsidered in evaluating a data seta the density of the ratings set overall, sometimes measured as the averagepercentage of items that have been rated per user since many datasets haveuneven popularity distributions, density may be artificially manipulated byincluding or excluding itemsb the number or density of ratings from the users for whom recommendationsare being made, which represents the experience of the user in the system atthe time of recommendation ratings from users with significant experiencecan be withheld to simulate the condition when they were new users andc the general size and distribution properties of the data setsome data setshave more items than users, though most data sets have many more usersthan items.Each of these sample features can have substantial effect on the successof different algorithms, and can reflect specific policies of the recommender.Density both individual and overall reflects both the overall size of the recommenders item space and the degree to which users have explored it. One policydecision that significantly affects density is the level of rating required to participate in the community. Systems that either require an extensive level ofstartup rating or require recurring ratings to maintain membership or statuslevels will generally have greater density than lowcommitment recommendersin the same domain. Density also interacts with the type of ratingimplicit ratings are likely to lead to greater density, since less effort is needed by the user.Finally, system that allow automated software agents to participate may havea significantly higher density than other systems, even if the underlying itemspace is similar see, e.g., Good et al. 1999. Because software agents are notlimited in attention, they can rate much more extensively than humans.Two particular distribution properties are known to be highly important.The relationship between the numbers of users and numbers of items can determine whether it is easier to build correlations among users or among itemsthis choice can lead to different relative performance among algorithms. Theratings distribution of items and users also may affect both algorithm and evaluation metric choice. Systems where there is an exponential popularity curvesome items have exponentially more ratings than others may be able to findagreement among people or items in the dense subregion and use that agreement to recommend in the sparse space. Jester, mentioned above, does thisdirectly by creating a highly dense region of jokes rated by all users. Systemswith a more even ratings distribution may be more challenged to cope withsparsity unless they incorporate dimensionality reduction techniques.To complete the discussion of domain features, inherent features, and samplefeatures, it is important to note that there are significant interactions betweenthese categories of features. For example, the type of task supported by a recommender system a domain feature will significantly affect the distribution ofratings collected a sample feature. However, each of these features representsa dimension which may be useful in explaining differences in evaluation results.ACM Transactions on Information Systems, Vol. 22, No. 1, January 2004.18  J. L. Herlocker et al.Evaluation of a recommender algorithm on a data set with features thatconflict with the end goal of the recommender algorithm could still be useful.By explicitly identifying the features that conflict, we can reason about whetherthose conflicts will unreasonably bias the evaluation results.3.4 Past and Current Trends in DatasetsThe most widely used common dataset was the EachMovie Dataset httpresearch.compaq.comSRCeachmovie. This extensive dataset has over2.8 million ratings from over 70,000 users, and it includes information suchas timestamps and basic demographic data for some of the users. In additionto seeding our MovieLens system httpwww.movielens.org, the EachMovieDataset was used in dozens of machine learning and algorithmic researchprojects to study new and potentially better ways to predict user ratings.Examples include Cannys 2002 factor analysis algorithm, Domingos andRichardsons 2003 algorithm for computing network value, and Pennock et als2000 work on recommending through personality diagnosis algorithms.Extracts 100,000 ratings and 1 million ratings of the MovieLens datasethave also been released for research use these extracts have been used byseveral researchers, including Schein et al. 2001 in their investigation of coldstart recommendations, Sarwar et al. 2001 in their evaluation of itembasedalgorithms, Reddy et al. 2002 in their community extraction research, andMui et al. 2001 in their work on collaborative sanctioning.More recently, several researchers have been using the Jester dataset, whichwas collected from the Jester joke recommendation website Goldberg et al.2001. Statistically, the Jester dataset has different characteristics than theMovieLens and Eachmovie data. First of all, there is a set of training itemsjokes that are rated by every single user, providing complete data on that subset of items. Second, in the Jester user interface, the user clicks on a unlabeledscale bar to rate a joke, so the ratings are much less discrete and may sufferfrom different kinds of biases since it is hard for the user to intentionally createa ranking among their rated items.The majority of publications related to collaborative filtering recommenderalgorithms have used one of the three data sets described above. A few otherdata sets have been used, but most of them are not publicly available for verification. The lack of variety in publicly available collaborative filtering datasets particularly with significant numbers of ratings remains one of the mostsignificant challenges in the field. Most researchers do not have the resourcesto build productionquality systems that are capable of collecting enough datato validate research hypotheses, and thus are often forced to constrain theirresearch to hypotheses that can be explored using the few existing datasets.With the maturation of collaborative filtering recommender technology, morelive systems have been built that incorporate recommender algorithms. As a result, we have recently seen an increased number of studies that have used livesystems. Herlockers explanation experiments Herlocker et al. 2000 exploredthe use of 23 different graphical displays to explain why each recommendation was given. Schafers MetaLens Schafer et al. 2002 was built to incorporateACM Transactions on Information Systems, Vol. 22, No. 1, January 2004.Evaluating Collaborative Filtering Recommender Systems  19MovieLens and other systems into a new interface his evaluation focused entirely on the interface and user experience. Other recent work has combineddifferent evaluations. Our work on value of information Rashid et al. 2002leads users through different signup processes, and then evaluates both thequality of resulting predictions and the subjective user experience.In the near future, we expect to see a lot more results from live experiments,as recommender algorithms make their way into more production systems. Wealso hope that new datasets will be released with data from new domains, causing new explosions in collaborative filtering recommender algorithm researchsimilar to what happened with the release of the EachMovie data.4. ACCURACY METRICSEstablishing the user tasks to be supported by a system, and selecting a dataset on which performance enables empirical experimentationscientifically repeatable evaluations of recommender system utility. A majority of the publishedempirical evaluations of recommender systems to date has focused on the evaluation of a recommender systems accuracy. We assume that if a user couldexamine all items available, they could place those items in a ordering of preference. An accuracy metric empirically measures how close a recommendersystems predicted ranking of items for a user differs from the users true ranking of preference. Accuracy measures may also measure how well a system canpredict an exact rating value for a specific item.Researchers who want to quantitatively compare the accuracy of differentrecommender systems must first select one or more metrics. In selecting a metric, researchers face a range of questions. Will a given metric measure the effectiveness of a system with respect to the user tasks for which it was designedAre results with the chosen metric comparable to other published researchwork in the field Are the assumptions that a metric is based on true Will ametric be sensitive enough to detect real differences that exist How large adifference does there have to be in the value of a metric for a statistically significant difference to exist Complete answers to these questions have not yetbeen substantially addressed in the published literature.The challenge of selecting an appropriate metric is compounded by the largediversity of published metrics that have been used to quantitatively evaluatethe accuracy of recommender systems. This lack of standardization is damaging to the progress of knowledge related to collaborative filtering recommendersystems. With no standardized metrics within the field, researchers have continued to introduce new metrics when they evaluate their systems. With alarge diversity of evaluation metrics in use, it becomes difficult to compare results from one publication to the results in another publication. As a result, itbecomes hard to integrate these diverse publications into a coherent body ofknowledge regarding the quality of recommender system algorithms.To address these challenges, we examine in the advantages and disadvantages of past metrics with respect to the user tasks and data set features thathave been introduced in Sections 2 and 3. We follow up the conceptual discussion of advantages and disadvantages with empirical results comparing theACM Transactions on Information Systems, Vol. 22, No. 1, January 2004.20  J. L. Herlocker et al.performance of different metrics when applied to results from one class of algorithm in one domain. The empirical results demonstrate that some conceptualdifferences among accuracy evaluation metrics can be more significant thanothers.4.1 Evaluation of Previously Used MetricsRecommender system accuracy has been evaluated in the research literaturesince 1994 Resnick et al. 1994. Many of the published evaluations of recommender systems used different metrics. We will examine some of the mostpopular metrics used in those publications, identifying the strengths and theweaknesses of the metrics. We broadly classify recommendation accuracy metrics into three classes predictive accuracy metrics, classification accuracy metrics, and rank accuracy metrics.4.1.1 Predictive Accuracy Metrics. Predictive accuracy metrics measurehow close the recommender systems predicted ratings are to the true userratings. Predictive accuracy metrics are particularly important for evaluatingtasks in which the predicting rating will be displayed to the user such as Annotation in Context. For example, the MovieLens movie recommender Dahlenet al. 1998 predicts the number of stars that a user will give each movie anddisplays that prediction to the user. Predictive accuracy metrics will evaluatehow close MovieLens predictions are to the users true number of stars givento each movie. Even if a recommender system was able to correctly rank ausers movie recommendations, the system could fail if the predicted ratings itdisplays to the user are incorrect.1 Because the predicted rating values createan ordering across the items, predictive accuracy can also be used to measurethe ability of a recommender system to rank items with respect to user preference. On the other hand, evaluators who wish to measure predictive accuracyare necessarily limited to a metric that computes the difference between thepredicted rating and true rating such as mean absolute error.Mean Absolute Error and Related Metrics. Mean absolute error often referred to as MAE measures the average absolute deviation between a predictedrating and the users true rating. Mean absolute error Eq. 1 has been used toevaluate recommender systems in several cases Breese et al. 1998, Herlockeret al. 1999, Shardanand and Maes 1995.E Ni1 pi  riN1Mean absolute error may be less appropriate for tasks such as Find Good Itemswhere a ranked result is returned to the user, who then only views items at thetop of the ranking. For these tasks, users may only care about errors in itemsthat are ranked high, or that should be ranked high. It may be unimportanthow accurate predictions are for items that the system correctly knows the userwill have no interest in. Mean absolute error may be less appropriate when the1This is a primary reason that many implementations of recommender systems in a commercialsetting only display a recommendeditems list and do not display predicted values.ACM Transactions on Information Systems, Vol. 22, No. 1, January 2004.Evaluating Collaborative Filtering Recommender Systems  21granularity of true preference a domain feature is small, since errors will onlyaffect the task if they result in erroneously classifying a good item as a bad oneor vice versa for example, if 3.5 stars is the cutoff between good and bad, thena onestar error that predicts a 4 as 5 or a 3 as 2 makes no difference to theuser.Beyond measuring the accuracy of the predictions at every rank, there aretwo other advantages to mean absolute error. First, the mechanics of the computation are simple and easy to understand. Second, mean absolute error haswell studied statistical properties that provide for testing the significance of adifference between the mean absolute errors of two systems.Three measures related to mean absolute error are mean squared error, rootmean squared error, and normalized mean absolute error. The first two variations square the error before summing it. The result is more emphasis on largeerrors. For example, an error of one point increases the sum of error by one, butan error of two points increases the sum by four. The third related measure,normalized mean absolute error Goldberg et al. 2001, is mean absolute errornormalized with respect to the range of rating values, in theory allowing comparison between prediction runs on different datasets although the utility ofthis has not yet been investigated.In addition to mean absolute error across all predicted ratings, Shardanandand Maes 1995 measured separately mean absolute error over items to whichusers gave extreme ratings. They partitioned their items into two groups, basedon user rating a scale of 1 to 7. Items rated below three or greater than five wereconsidered extremes. The intuition was that users would be much more awareof a recommender systems performance on items that they felt strongly about.From Shardanand and Maes results, the mean absolute error of the extremesprovides a different ranking of algorithms than the normal mean absolute error.Measuring the mean absolute error of the extremes can be valuable. However,unless users are concerned only with how their extremes are predicted, it shouldnot be used in isolation.4.1.2 Classification Accuracy Metrics. Classification metrics measure thefrequency with which a recommender system makes correct or incorrect decisions about whether an item is good. Classification metrics are thus appropriatefor tasks such as Find Good Items when users have true binary preferences.When applied to nonsynthesized data in offline experiments, classificationaccuracy metrics may be challenged by data sparsity. The problem occurs whenthe collaborative filtering system being evaluated is generating a list of toprecommended items. When the quality of the list is evaluated, recommendationsmay be encountered that have not been rated. How those items are treated inthe evaluation can lead to certain biases.One approach to evaluation using sparse data sets is to ignore recommendations for items for which there are no ratings. The recommendation list is firstprocessed to remove all unrated items. The recommendation task has been altered to predict the top recommended items that have been rated. In taskswhere the user only observes the top few recommendations, this could lead toinaccurate evaluations of recommendation systems with respect to the usersACM Transactions on Information Systems, Vol. 22, No. 1, January 2004.22  J. L. Herlocker et al.Table I. Table Showing the Categorization of Itemsin the Document Set with Respect to a GivenInformation NeedSelected Not Selected TotalRelevant Nrs Nrn NrIrrelevant Nis Nin NiTotal Ns Nn Ntask. The problem is that the quality of the items that the user would actuallysee may never be measured.In an example of how this could be significant, consider the following situation that could occur when using the nearest neighbor algorithm described inHerlocker et al. 2002 when only one user in the dataset has rated an eclecticitem I, then the prediction for item I for all users will be equal to the ratinggiven by that user. If a user gave item I a perfect rating of 5, then the algorithmwill predict a perfect 5 for all other users. Thus, item I will immediately beplaced at the top of the recommendation list for all users, in spite of the lack ofconfirming data. However, since no other user has rated this item, the recommendation for item I will be ignored by the evaluation metric, which thus willentirely miss the flaw in the algorithm.Another approach to evaluation of sparse data sets is to assume defaultratings, often slightly negative, for recommended items that have not beenrated Breese et al. 1998. The downside of this approach is that the defaultrating may be very different from the true rating unobserved for an item.A third approach that we have seen in the literature is to compute how manyof the highly rated items are found in the recommendation list generated bythe recommender system. In essence, we are measuring how well the systemcan identify items that the user was already aware of. This evaluation approachmay result in collaborative filtering algorithms that are biased towards obvious,nonnovel recommendations or perhaps algorithms that are over fittedfittingthe known data perfectly, but new data poorly. In Section 5 of this article, wediscuss metrics for evaluating novelty of recommendations.Classification accuracy metrics do not attempt to directly measure the abilityof an algorithm to accurately predict ratings. Deviations from actual ratingsare tolerated, as long as they do not lead to classification errors. The particularmetrics that we discuss are Precision and Recall and related metrics and ROC.We also briefly discuss some ad hoc metrics.Precision and Recall and Related MeasuresPrecision and recall are the most popular metrics for evaluating information retrieval systems. In 1968, Cleverdon proposed them as the key metricsCleverdon and Kean 1968, and they have held ever since. For the evaluationof recommender systems, they have been used by Billsus and Pazzani 1998,Basu et al. 1998, and Sarwar et al. 2000a, 2000b.Precision and recall are computed from a 2  2 table, such as the one shownin Table I. The item set must be separated into two classesrelevant or notrelevant. That is, if the rating scale is not already binary, we need to transformACM Transactions on Information Systems, Vol. 22, No. 1, January 2004.Evaluating Collaborative Filtering Recommender Systems  23it into a binary scale. For example, the MovieLens dataset Dahlen et al. 1998has a rating scale of 15 and is commonly transformed into a binary scale byconverting every rating of 4 or 5 to relevant and all ratings of 13 to notrelevant. For precision and recall, we also need to separate the item set intothe set that was returned to the user selectedrecommended, and the set thatwas not. We assume that the user will consider all items that are retrieved.Precision is defined as the ratio of relevant items selected to number of itemsselected, shown in Eq. 2P  NrsNs. 2Precision represents the probability that a selected item is relevant. Recall,shown in Eq. 3, is defined as the ratio of relevant items selected to totalnumber of relevant items available. Recall represents the probability that arelevant item will be selectedR  NrsNr. 3Precision and recall depend on the separation of relevant and nonrelevantitems. The definition of relevance and the proper way to compute it has been asignificant source of argument within the field of information retrieval Harter1996. Most information retrieval evaluation has focused on an objective version of relevance, where relevance is defined with respect to a query, and isindependent of the user. Teams of experts can compare documents to queriesand determine which documents are relevant to which queries. However, objective relevance makes no sense in recommender systems. Recommender systemsrecommend items based on the likelihood that they will meet a specific userstaste or interest. That user is the only person who can determine if an itemmeets his taste requirements. Thus, relevance is more inherently subjective inrecommender systems than in traditional document retrieval.In addition to user tastes being different, user rating scales may also be different. One user may consider a rating of 3 on a 5point scale to be relevant, whileanother may consider it irrelevant. For this reason, much research using multipoint scales such as in Hill et al. 1995, Resnick et al. 1994, and Shardanandand Maes 1995 has focused on other metrics besides PrecisionRecall. Oneinteresting approach that has been taken to identify the proper threshold is toassume that a top percentile of items rated by a user are relevant Basu et al.1998.Recall, in its purest sense, is almost always impractical to measure in arecommender system. In the pure sense, measuring recall requires knowingwhether each item is relevant for a movie recommender, this would involveasking many users to view all 5000 movies to measure how successfully we recommend each one to each user. IR evaluations have been able to estimate recallby pooling relevance ratings across many users, but this approach dependson the assumption that all users agree on which items are relevant, which isinconsistent with the purpose of recommender systems.Several approximations to recall have been developed and used to evaluate recommender systems. Sarwar et al. 2000a evaluate their algorithms byACM Transactions on Information Systems, Vol. 22, No. 1, January 2004.24  J. L. Herlocker et al.taking a dataset of user ratings which they divide into a training set and a testset. They train the recommender algorithm on the training set, and then predictthe top N items that the user is likely to find valuable, where N is some fixedvalue. They then compute recall as the percentage of known relevant itemsfrom the test set that appear in the top N predicted items. Since the numberof items that each user rates is much smaller than the number of items in theentire dataset see the discussion on data sparsity at the beginning of this section, the number of relevant items in the test set may be a small fraction of thenumber of relevant items in the entire dataset. While this metric can be useful,it has underlying biases that researchers must be aware of. In particular, thevalue of this metric depends heavily on the percentage of relevant items thateach user has rated. If a user has rated only a small percentage of relevantitems, a recommender with high true recall may yield a low value for measured recall, since the recommender may have recommended unrated relevantitems. Accordingly, this metric should only be used in a comparative fashion onthe same dataset it should not be interpreted as an absolute measure.We have also seen precision measured in the same fashion Sarwar et al.2000a with relevant items being selected from a small pool of rated itemsand predicted items being selected from a much larger set of items. Similarly, this approximation to precision suffers from the same biases as the recallapproximation.Perhaps a more appropriate way to approximate precision and recall wouldbe to predict the top N items for which we have ratings. That is, we take ausers ratings, split them into a training set and a test set, train the algorithmon the training set, then predict the top N items from that users test set. If weassume that the distribution of relevant items and nonrelevant items withinthe users test set is the same as the true distribution for the user across allitems, then the precision and recall will be much closer approximations of thetrue precision and recall. This approach is taken in Basu et al. 1998.In information retrieval, precision and recall can be linked to probabilitiesthat directly affect the user. If an algorithm has a measured precision of 70,then the user can expect that, on average, 7 out of every 10 documents returnedto the user will be relevant. Users can more intuitively comprehend the meaningof a 10 difference in precision than they can a 0.5point difference in meanabsolute error.One of the primary challenges to using precision and recall to compare different algorithms is that precision and recall must be considered together toevaluate completely the performance of an algorithm. It has been observed thatprecision and recall are inversely related Cleverdon and Kean 1968 and aredependent on the length of the result list returned to the user. When moreitems are returned, then the recall increases and precision decreases. Therefore, if the information system doesnt always return a fixed number of items,we must provide a vector of precisionrecall pairs to fully describe the performance of the system. While such an analysis may provide a detailed picture ofthe performance of a system, it makes comparison of systems complicated, tedious, and variable between different observers. Furthermore, researchers mayACM Transactions on Information Systems, Vol. 22, No. 1, January 2004.Evaluating Collaborative Filtering Recommender Systems  25carefully choose at which levels of recall or search length they report precisionand recall to match the strengths in their system.Several approaches have been taken to combine precision and recall into asingle metric. One approach is the F1 metric Eq. 4, which combines precisionand recall into a single number The F1 has been used to evaluate recommendersystems in Sarwar et al. 2000a, 2000b. An alternate approach taken by theTREC community is to compute the average precision across several differentlevels of recall or the average precision at the rank of each relevant documentHarman 1995. The latter approach was taken in all but the initial TRECconference. This approach is commonly referred to as Mean Average Precisionor MAP. F1 and mean average precision may be appropriate if the underlying precision and recall measures on which it is based are determined to beappropriateF1  2PRP  R . 4Precision alone at a single search length or a single recall level can be appropriate if the user does not need a complete list of all potentially relevantitems, such as in the Find Good Items task. If the task is to find all relevantitems in an area, then recall becomes important as well. However, the searchlength at which precision is measured should be appropriate for the user taskand content domain.As with all classification metrics, precision and recall are less appropriatefor domains with nonbinary granularity of true preference. For those tasks, atany point in the ranking, we want the current item to be more relevant thanall items lower in the ranking. Since precision and recall only measure binaryrelevance, they cannot measure the quality of the ordering among items thatare selected as relevant.ROC Curves, Swets A Measure, and Related MetricsROC curvebased metrics provide a theoretically grounded alternative toprecision and recall. There are two different popularly held definitions for theacronym ROC. Swets 1963, 1969 introduced the ROC metric to the informationretrieval community under the name relative operating characteristic. Morepopular however, is the name receiver operating characteristic, which evolvedfrom the use of ROC curves in signal detection theory Hanley and McNeil 1982.Regardless, in both cases, ROC refers to the same underlying metric.The ROC model attempts to measure the extent to which an informationfiltering system can successfully distinguish between signal relevance andnoise. The ROC model assumes that the information system will assign a predicted level of relevance to every potential item. Given this assumption, we cansee that there will be two distributions, shown in Figure 1. The distributionon the left represents the probability that the system will predict a given levelof relevance the xaxis for an item that is in reality not relevant to the information need. The distribution on the right indicates the same probabilitydistribution for items that are relevant. Intuitively, we can see that the furtherACM Transactions on Information Systems, Vol. 22, No. 1, January 2004.26  J. L. Herlocker et al.Fig. 1. A possible representation of the density functions for relevant and irrelevant items.apart these two distributions are, the better the system is at differentiatingrelevant items from nonrelevant items.With systems that return a ranked list, the user will generally view the recommended items starting at the top of the list and work down until the information need is met, a certain time limit is reached, or a predetermined numberof results are examined. In any case, the ROC model assumes that there isa filter tuning value zc, such that all items that the system ranks above thecutoff are viewed by the user, and those below the cutoff are not viewed by theuser. This cutoff defines the search length. As shown in Figure 1, at each valueof zc, there will be a different value of recall percentage of good items returned,or the area under the relevant probability distribution to the right of zc andfallout percentage of bad items returned, or the area under the nonrelevantprobability distribution to the right of zc. The ROC curve represents a plot ofrecall versus fallout, where the points on the curve correspond to each value ofzc. An example of an ROC curve is shown in Figure 2.A common algorithm for creating an ROC curve goes as follows1 Determine how you will identify if an item is relevant or nonrelevant.2 Generate a predicted ranking of items.3 For each predicted item, in decreasing order of predicted relevance startingthe graph at the origina If the predicted item is indeed relevant, draw the curve one step vertically.b If the predicted item is not relevant, draw the curve one step horizontally to the right.c If the predicted item has not been rated i.e., relevance is not known,then the item is simply discarded and does not affect the curve negatively or positively.An example of an ROC curve constructed in this manner is shown inFigure 2.A perfect predictive system will generate an ROC curve that goes straightupward until 100 of relevant items have been encountered, then straight rightACM Transactions on Information Systems, Vol. 22, No. 1, January 2004.Evaluating Collaborative Filtering Recommender Systems  27Fig. 2. An example of an ROC curve. The pvalues shown on the curve represent different prediction cutoffs. For example, if we chose to select all items with predictions of 4 or higher, then weexperience approximately 45 of all relevant items and 20 of all nonrelevant items.for the remaining items. A random predictor is expected to produce a straightline from the origin to the upper right corner.2ROC curves are useful for tuning the signalnoise tradeoff in informationsystems. For example, by looking at an ROC curve, you might discover thatyour information filter performs well for an initial burst of signal at the top ofthe rankings, and then produces only small increases of signal for moderateincreases in noise from then on.Similar to Precision and Recall measures, ROC curves make an assumption of binary relevance. Items recommended are either successful recommendations relevant or unsuccessful recommendation nonrelevant. One consequence of this assumption is that the ordering among relevant items has noconsequence on the ROC metricif all relevant items appear before all nonrelevant items in the recommendation list, you will have a perfect ROC curve.Comparing multiple systems using ROC curves becomes tedious and subjective, just as with precision and recall. However, a single summary performance2Schein et al. 2002 present an alternate method of computing an ROCa Customer ROC CROC.A CROC measurement applied to a perfect recommender may not produce a perfect ROC graph asdescribed. The reasoning is that some recommender systems may display more recommendationsthen there exist relevant items to the recommender, and that these additional recommendationsshould be counted as falsepositives.ACM Transactions on Information Systems, Vol. 22, No. 1, January 2004.28  J. L. Herlocker et al.number can be obtained from an ROC curve. The area underneath an ROCcurve, also known as Swets A measure, can be used as a single metric of the systems ability to discriminate between good and bad items, independent of thesearch length. According to Hanley and McNeil 1982, the area underneath theROC curve is equivalent to the probability that the system will be able to choosecorrectly between two items, one randomly selected from the set of bad items,and one randomly selected from the set of good items. Intuitively, the area underneath the ROC curve captures the recall of the system at many differentlevels of fallout. It is also possible to measure the statistical significance of thedifference between two areas Hanley and McNeil 1982 Le and Lindren 1995.The ROC area metric has the disadvantage that equally distant swaps inthe rankings will have the same affect on ROC area regardless of whether theyoccur near the top of the ranking or near the end of the ranking. For example,if a good item is ranked 15 instead of 10, it will have roughly the same affect onthe ROC area as if a good item is ranked 200 instead of 195. This disadvantagecould be significant for tasks such as Find Good Items where the first situationis likely to have a greater negative affect on the user. This disadvantage issomewhat minimized by the fact that relevance is binary and exchanges withina relevance class have no affect if items ranked 1015 are all relevant, anexchange between 10 and 15 will have no affect at all. On the other hand,for tasks such as Find All Good Items, the discussed disadvantage may not besignificant.Hanley and McNeil 1982 present a method by which one can determinethe number of data points necessary to ensure that a comparison between twoareas has good statistical power defined as a high probability of identifyinga difference if one exists. Hanleys data suggests that many data points maybe required to have a high level of statistical power. The number of requireddata points for significance becomes especially large when the two areas beingcompared are very close in value. Thus, to confidently compare the results ofdifferent algorithms using ROC area, the potential result set for each user mustalso be large.The advantages of ROC area metric are that it a provides a single numberrepresenting the overall performance of an information filtering system, bis developed from solid statistical decision theory designed for measuring theperformance of tasks such as those that a recommender system performs, andc covers the performance of the system over all different recommendation listlengths.To summarize the disadvantages of the ROC area metric a a large set of potentially relevant items is needed for each query b for some tasks, such as FindGood Items users are only interested in performance at one setting, not all possible settings c equally distant swaps in rankings have the same effect no matter where in the ranking they occur and d it may need a large number of datapoints to ensure good statistical power for differentiating between two areas.The ROC area measure is most appropriate when there is a clear binaryrelevance relationship and the task is similar to Find Good Items, where theuser wants to see as many of the relevant answers as possible within certainresource limitations.ACM Transactions on Information Systems, Vol. 22, No. 1, January 2004.Evaluating Collaborative Filtering Recommender Systems  29Ad Hoc Classification Accuracy MeasuresAd hoc measures of classification accuracy have attempted to identify errorrates and, in particular, large errors. Error rate can be measured in a mannerderived from Precision and Recall. Specifically, the error rate for a system is thenumber of incorrect recommendations it makes divided by the total number ofrecommendations. If a system recommends only a few items, it is possible tomeasure error rate experimentally. Jester, for example, which presents jokes tousers, can evaluate the error rate based on the immediate feedback users giveto each joke Goldberg et al. 2001. More commonly, the error rate computationis limited to the subset of recommended items for which a rating is availablethis approach introduces the bias that users commonly avoid consuming andtherefore rating items that dont interest them, and therefore this approximateerror rate is likely to be lower than the true error rate.Another ad hoc technique specifically identifies large errors. Sarwar et al.1998 measured reversals when studying agentboosted recommendations. Errors of three or more points on a fivepoint scale were considered significantenough to potentially undermine user confidence, and therefore were talliedseparately. Such a measurement mixes aspects of classification and predictionaccuracy, but has not been generally used by later researchers. It might beparticularly appropriate for the Evaluate Recommender task.4.1.3 Rank Accuracy Metrics. Rank accuracy metrics measure the abilityof a recommendation algorithm to produce a recommended ordering of itemsthat matches how the user would have ordered the same items. Unlike classification metrics, ranking metrics are more appropriate to evaluate algorithmsthat will be used to present ranked recommendation lists to the user, in domainswhere the users preferences in recommendations are nonbinary.Rank accuracy metrics may be overly sensitive for domains where the userjust wants an item that is good enough binary preferences since the userwont be concerned about the ordering of items beyond the binary classification.For example, even if the top ten items ranked by the system were relevant, arank accuracy metric might give a nonperfect value because the best item isactually ranked 10th. By the same token, rank accuracy metrics can distinguishbetween the best alternatives and just good alternatives and may be moreappropriate for domains where that distinction is important. In such domainsit is possible for all the top recommended items to be relevant, but still not bethe best items.Ranking metrics do not attempt to measure the ability of an algorithm toaccurately predict the rating for a single itemthey are not predictive accuracy metrics and are not appropriate for evaluating the Annotation in Contexttask. If a recommender system will be displaying predicted rating values, itis important to additionally evaluate the system using a predictive accuracymetric as described above. We examine several correlation metrics, a halflifeutility metric, and the NDPM metric.4.1.4 PredictionRating Correlation. Two variables are correlated if thevariance in one variable can be explained by the variance in the second. ThreeACM Transactions on Information Systems, Vol. 22, No. 1, January 2004.30  J. L. Herlocker et al.of the most well known correlation measures are Pearsons productmomentcorrelation, Spearmans , and Kendalls Tau.Pearson correlation measures the extent to which there is a linear relationship between two variables. It is defined as the covariance of the zscores, shownin Eq. 5.c x  x y  yn  stdevx stdev y . 5Rank correlations, such as Spearmans  Eq. 6 and Kendalls Tau, measurethe extent to which two different rankings agree independent of the actualvalues of the variables. Spearmans  is computed in the same manner as thePearson productmoment correlation, except that the xs and ys are transformed into ranks, and the correlations are computed on the ranks. u  uv  vn  stdevu stdevv . 6Kendalls Tau represents a different approach to computing the correlation ofthe rankings that is independent of the variable values. One approximationto Kendalls Tau is shown in Eq. 7. C stands for the number of concondantpairspairs of items that the system predicts in the proper ranked order. Dstands for the number of discordant pairspairs that the system predicts inthe wrong order. TR is number of pairs of items in the true ordering the rankingdetermined by the users ratings that have tied ranks i.e., the same ratingwhile TP is the number of pairs of items in the predicted ordering that havetied ranks the same prediction valueTau  C  DsqrtC  D  TRC  D  TP . 7In spite of their simplicity, the above correlation metrics have not been usedextensively in the measurement of recommender systems or information retrieval systems. Pearson correlation was used by Hill et al. 1995 to evaluatethe performance of their recommender system.The advantages of correlation metrics are a they compare a nonbinarysystem ranking to a nonbinary user ranking, b they are well understood bythe scientific community, and c they provide a single measurement score forthe entire system.There may be weaknesses in the way in which the badness of an interchange is calculated with different correlation metrics. For example, KendallsTau metric applies equal weight to any interchange of equal distance, no matterwhere it occurs similar to the ROC area metric. Therefore, an interchange between recommendations 1 and 2 will be just as bad as an interchange betweenrecommendations 1000 and 1001. However, if the user is much more likely toconsider the first five, and will probably never examine items ranked in thethousands, the interchange between 1 and 2 should have a more substantialnegative impact on the outcome of the metric.The Spearman correlation metric does not handle weak partial orderingswell. Weak orderings occur whenever there are at least two items in the rankingACM Transactions on Information Systems, Vol. 22, No. 1, January 2004.Evaluating Collaborative Filtering Recommender Systems  31such that neither item is preferred over the other. If a ranking is not a weakordering then it is called a complete ordering. If the users ranking based ontheir ratings is a weak ordering and the system ranking is a complete ordering,then the Spearman correlation will be penalized for every pair of items whichthe user has rated the same, but the system ranks at different levels. This isnot ideal, since the user shouldnt care how the system orders items that theuser has rated at the same level. Kendalls Tau metric also suffers the sameproblem, although to a lesser extent than the Spearman correlation.Halflife Utility MetricBreese et al. 1998, presented a new evaluation metric for recommendersystems that is designed for tasks where the user is presented with a rankedlist of results, and is unlikely to browse very deeply into the ranked list. Anotherdescription of this metric can be found in Heckerman et al. 2000. The task forwhich the metric is designed is an Internet webpage recommender. They claimthat most Internet users will not browse very deeply into results returned bysearch engines.This halflife utility metric attempts to evaluate the utility of a ranked listto the user. The utility is defined as the difference between the users rating foran item and the default rating for an item. The default rating is generally aneutral or slightly negative rating. The likelihood that a user will view eachsuccessive item is described with an exponential decay function, where thestrength of the decay is described by a halflife parameter. The expected utilityRa is shown in Eq. 8. ra, j represents the rating of user a on item j of theranked list, d is the default rating, and  is the halflife. The halflife is therank of the item on the list such that there is a 50 chance that the user willview that item. Breese et al. 1998 used a halflife of 5 for his experiments, butnoted that using a halflife of 10 caused little additional sensitivity of resultsRa jmaxra, j  d , 02 j11. 8The overall score for a dataset across all users R is shown in Eq. 9. Rmaxais the maximum achievable utility if the system ranked the items in the exactorder that the user ranked themR  100a Raa Rmaxa. 9The halflife utility metric is best for tasks domains where there is an exponential drop in true utility one could consider utility from a costbenefit ratioanalysis as the search length increases, assuming that the halflife  and default vote d are chosen appropriately in the utility metric. The utility metricapplies most of the weight to early items, with every successive rank havingexponentially less weight in the measure. To obtain high values of the metric,the first predicted rankings must consist of items rated highly by the user. Thedownside is that if the true function describing the likelihood of accessing eachrank is significantly different from the exponential used in the metric then themeasured results may not be indicative of actual performance. For example, ifACM Transactions on Information Systems, Vol. 22, No. 1, January 2004.32  J. L. Herlocker et al.the user almost always searches 20 items into the ranked recommendation list,then the true likelihood function is a step function which is constant for thefirst 20 items and 0 afterwards.The halflife utility metric may be overly sensitive in domains with binaryuser preferences where the user only requires that top recommendations begood enough or for user tasks such as Find All Good Items where the userwants to see all good items.There are other disadvantages to the halflife utility metric. First, weakorderings created by the system will result in different possible scores for thesame system ranking. Suppose the system outputs a recommendation list, withthree items sharing the top rank. If the user rated those three items differently,then depending on what order the recommender outputs those items, the metriccould have very different values if the ratings were significantly different.Second, due to the application of the max function in the metric Eq. 8,all items that are rated less than the default vote contribute equally to thescore. Therefore, an item occurring at system rank 2 that is rated just slightlyless than the default rating which usually indicates ambivalence will havethe same effect on the utility as an item that has the worst possible rating.The occurrence of extremely wrong predictions in the high levels of a systemranking can undermine the confidence of the user in the system. Metrics thatpenalize such mistakes more severely are preferred.To summarize, the halflife utility metric is the only one that we have examined that takes into account nonuniform utility. Thus, it could be appropriate for evaluation of the Find Good Items tasks in domains such nonuniformutility is believed to exist. On the other hand, it has many disadvantages, inparticular when considering standardization across different researchers. Different researchers could use significantly different values of alpha or the defaultvotemaking it hard to compare results across researchers and making it easyto manipulate results. Furthermore, the halflife parameter is unlikely to bethe same for all users different users needdesire different numbers of results.The NDPM MeasureNDPM was used to evaluate the accuracy of the FAB recommender systemBalabanovc and Shoham 1997. It was originally proposed by Yao 1995. Yaodeveloped NDPM theoretically, using an approach from decision and measurement theory. NDPM stands for normalized distancebased performance measure. NDPM Eq. 10 can be used to compare two different weakly orderedrankingsNDPM  2C  C ..2Ci. 10Cis the number of contradictory preference relations between the systemranking and the user ranking. A contradictory preference relation happenswhen the system says that item 1 will be preferred to item 2, and the userranking says the opposite. Cu is the number of compatible preference relations, where the user rates item 1 higher than item 2, but the system rankingACM Transactions on Information Systems, Vol. 22, No. 1, January 2004.Evaluating Collaborative Filtering Recommender Systems  33has item 1 and item 2 at equal preference levels. Ci is the total number ofpreferred relationships in the users ranking i.e., pairs of items rated by theuser for which one is rated higher than the other. This metric is comparableamong different datasets it is normalized, because the numerator representsthe distance, and the denominator represents the worst possible distance.NDPM is similar in form to the Spearman and Kendalls Tau rank correlations, but provides a more accurate interpretation of the effect of tied userranks. However, it does suffer from the same interchange weakness as the rankcorrelation metrics interchanges at the top of the ranking have the same weightas interchanges at the bottom of the ranking.Because NDPM does not penalize the system for system orderings when theuser ranks are tied, NDPM may be more appropriate than the correlation metrics for domains where the user is interested in items that are goodenough.User ratings could be transformed to binary ratings if they were not already,and NDPM could be used to compare the results to the system ranking.As NDPM only evaluates ordering and not prediction value, it is not appropriate for evaluating tasks where the goal is to predict an actual rating value.4.1.5 An Empirical Comparison of Evaluation Metrics. After conceptuallyanalyzing the advantages and disadvantages, a natural question is which ofthese advantages and disadvantages have a significant effect on the outcome ofa metric In an effort to quantify the differences between the abovementionedevaluation metrics, we computed a set of different evaluation metrics on a set ofresults from different variants of a collaborative filtering prediction algorithmand examined the extent to which the different evaluation metrics agreed ordisagreed.We examined the predictions generated by variants of a classic nearestneighbor collaborative filtering algorithm formed by perturbing many differentkey parameters. We used this data for examination of evaluation metrics. Therewere 432 different variants of the algorithm tested, resulting in the same number of sets of predictions. The parameters of the algorithms that were variedto produce the different results included size of neighborhood, similarity measure used to compute closeness of neighbors, threshold over which other userswere considered neighbors, and type of normalization used on the ratings. seeHerlocker et al. 2002 for more information on the algorithmFor each of these result sets, we computed mean absolute error, Pearson correlation, Spearman rank correlation, area underneath an ROC4 and ROC53curve, the halflife utility metric, mean average precision at relevant documents and the NDPM metric. For several of the metrics, there are two differentvariants overall and peruser. The difference between these two variants isthe manner in which averaging was performed. In the overall case, predictions for all the users were pooled together into a single file and then sorted.Likewise, the ratings for those items were pooled into a single file and sorted.A ranking metric was then applied once to compare those two files. In the3ROC4 refers to an ROC curve where ratings of 4 and above are considered signal and 3 and beloware considered noise.ACM Transactions on Information Systems, Vol. 22, No. 1, January 2004.34  J. L. Herlocker et al.peruser case, predictions were computed for each user, and the ranking metricwas computed for each user. Then the ranking metric was averaged over allusers.The experiment was performed with data taken from the MovieLens webbased movie recommender www.movielens.org. The data were sampled fromthe data collected over a sevenmonth period from September 19th, 1997through April 22nd, 1998. The data consisted of 100,000 movie ratings from943 users on 1682 items. Each user sampled had rated at least 20 items. Foreach of the users, 10 rated items are withheld from the training. After training the system with all the other ratings, predictions are generated for the 10withheld items. Then the predictions were compared to the users ratings, andthe list ranked by predictions was compared to the list ranked by user ratings.The data are freely available from www.grouplens.org, and we encourage researchers using other families of collaborative filtering algorithms to replicatethis work using the same data set for comparability.This analysis is performed on a single family of algorithms on a singledataset, so the results should not be considered comprehensive. However, webelieve that the results show evidence of relationships between the metrics thatshould be investigated further. Our goal is not to provide a deep investigationof the empirical results, which would constitute a entire article by itself.Figure 3 is a scatter plot matrix showing an overview of all the results. Eachcell of the matrix represents a comparison between two of the metrics.4 Eachpoint of the scatter plot represents a different variant of the recommenderalgorithm. In the following paragraphs and figures, we will look more closelyat subsets of the results.In analyzing the data in Figure 3, we notice that there is very strong linear agreement among different subsets of the metrics tested. One subset withstrong agreement includes the peruser correlation metrics and mean averageprecision. This subset is shown expanded in Figure 4. For the data and algorithms tested, Figure 4 suggests that rank correlations do not provide substantially different outcomes from each other or from Pearson correlation. Figure 4also indicates that mean average precision is highly correlated with the peruser correlation metrics.Figure 5 shows a different, mutually exclusive subset of the evaluation metrics that includes the peruser Halflife Utility metric as well as the peruserROC4 and ROC5 area metrics. We can see that these three metrics are stronglycorrelated, even more so that the previous subset of metrics.The final subset that we shall examine contains all the metrics that arecomputed overall as opposed to the metrics depicted in Figures 4 and 5, whichare peruser. Figure 6 shows that the metrics computed overall mean absoluteerror,5 Pearson Correlation, and ROC4 have mostly linear relationships.4Note that because we are displaying the complete matrix, each comparison pair appears twice.However, by having the complete matrix, we can easily scan one metrics interactions with all othermetrics in a single row or column.5Note that Mean Absolute Error could produces the same result whether it is averaged peruser oroverall. However, it strongly correlates with the overall metrics and not the peruser metrics.ACM Transactions on Information Systems, Vol. 22, No. 1, January 2004.Evaluating Collaborative Filtering Recommender Systems  35Fig. 3. Overview of the comparative evaluation of several different evaluation metrics on 432different variants of a nearest neighborbased collaborative filtering algorithm.Fig. 4. Comparison among results provided by all the peruser correlation metrics and the meanaverage precision per user metric. These metrics have strong linear relationships with each other.ACM Transactions on Information Systems, Vol. 22, No. 1, January 2004.36  J. L. Herlocker et al.Fig. 5. Comparison between results provided by the ROC4 Area metric, the ROC5 Area metric,and the Halflife Utility metric. The graphs depict strong linear correlations.Fig. 6. Comparison between metrics that are averaged overall rather than peruser. Note thelinear relationship between the different metrics.ACM Transactions on Information Systems, Vol. 22, No. 1, January 2004.Evaluating Collaborative Filtering Recommender Systems  37Fig. 7. A comparison of representative metrics from the three subsets that were depicted inFigures 46. Within each of the subsets, the metrics strongly agree, but this figure shows thatmetrics from different subsets do not correlate well.To bring the analysis back to the entire set of metrics, we have chosenone representative from each of the subsets that were depicted in Figures 46. Figure 7 shows a comparison between these representatives. Pearson peruser represents the subset of peruser metrics and Mean Average Precisionthat are depicted in Figure 4. ROC4 per user represents the ROC4, ROC5, and Halflife Utility metrics, all averaged peruser. Mean Absolute Errorrepresents the subset of overall metrics depicted in Figure 6. We can seethat while there is strong agreement within each subset of algorithms asseen in Figures 46, there is little agreement between algorithms from different subsets. Algorithms averaged peruser do not seem to agree with algorithms averaged overall. The ROC4, ROC5, and Halflife Utility metricsaveraged per user do not agree with the other metrics that are averaged peruser.Several interesting observations can be taken from that data in Figures 37.Metrics that are computed per user and then averaged provide differentrankings of algorithms that metrics that are computed overall.There doesnt appear to be a substantial difference between the Pearsoncorrelation metrics and rank correlation metrics, although a good number ofoutliers exist.Mean Average Precision provides roughly the same ordering of algorithmsas the correlation metrics that are computed per user and averaged.ACM Transactions on Information Systems, Vol. 22, No. 1, January 2004.38  J. L. Herlocker et al.The ROC area metrics ROC4 and ROC5 when computed overall performvery similar to the other overall metrics, such as Mean Absolute Error andPearson Overall. However, when they are averaged per user, they providedifferent rankings of algorithms than other peruser metrics, with the exception of the Halflife Utility metricThe Halflife utility metric, which is averaged per user provides differentrankings of algorithms than the peruser correlation metrics and mean average precision, yet produces rankings similar to the ROC area metrics whencomputed per user.In support of these observations, Schein et al. 2002 have also observedthat overall metrics and peruser metric can provide conflicting results. Theyobserved differences between an overall ROC which they call Global ROC anda peruser ROC which they call a Customer ROC.One should hold in mind that these empirical results, while based on numerous data points, all represent perturbations of the same base algorithm.The range in rank scores do not vary that much. Future work could extendthe comparison of these evaluation metrics across significantly different recommendation algorithms.4.2 Accuracy MetricsSummaryWe have examined a variety of accuracy evaluation metrics that have beenused before to evaluate collaborative filtering systems. We have examined themboth conceptually and empirically. The conceptual analysis suggests that certain evaluation metrics are more appropriate for certain tasks. Based on thisanalysis, there appears to be a potential for inaccurate measurement of certaintasks if the wrong metric is used. Our empirical analysis of one class of collaborative filtering algorithm demonstrates that many of the argued conceptualmismatches between metrics and tasks do not manifest themselves when evaluating the performance of predictive algorithms on movie rating data. On theother hand, we were able to demonstrate that different outcomes in evaluationcan be obtained by carefully choosing evaluation metrics from different classesthat we identified.The empirical analysis that we have performed represents only a sampleone class of algorithm and one dataset. A TREClike environment for collaborative filtering algorithms, with different tracks and datasets for differenttasks would provide algorithmic results from many different algorithms andsystems. These results would provide valuable data for the further verificationof the properties of metrics discussed in this section.A final note is that a similar but very brief analysis has been performed onmetrics for evaluating text retrieval systems. Voorhees and Harman 1999 report the strength of correlations computed between different evaluation metricsused in the TREC7 analysis. Instead of showing scatterplots relating metrics,they computed correlation values between different metrics. Their results focused primarily on different variants of precisionrecall that we do not discusshere. As the domain features of the document retrieval context are significantlyACM Transactions on Information Systems, Vol. 22, No. 1, January 2004.Evaluating Collaborative Filtering Recommender Systems  39different from the recommender systems context, we do not attempt to incorporate their results here.5. BEYOND ACCURACYThere is an emerging understanding that good recommendation accuracy alonedoes not give users of recommender systems an effective and satisfying experience. Recommender systems must provide not just accuracy, but also usefulness.For instance, a recommender might achieve high accuracy by only computingpredictions for easytopredict itemsbut those are the very items for whichusers are least likely to need predictions. Further, a system that always recommends very popular items can promise that users will like most of the itemsrecommendedbut a simple popularity metric could do the same thing. Recommending only very unpopular items and promising that users wont likethem is even less useful.By recalling that performance of a recommender system must be evaluatedwith respect to specific user tasks and the domain context, we can deepen theargument for moving beyond accuracy. For example, consider the Find GoodItems task in a domain where the user wants to select a single item whose valueexceeds a thresholdand suppose that the system follows a typical strategy ofoffering a relatively small, ordered set of recommendations. In this case, it maybe best for the system to try to generate a few highly useful recommendations,even at the risk of being off the mark with the others. If the supporting information about the items is good enough, then the user will be able to identifythe best recommendation quickly. Turpin and Hershs 2001 study of searchengines provides support for this position. Their subjects were divided into twosets, with half using a simple, baseline search engine, and the others using astate of the art engine. While the latter returned significantly more accurate results, subjects in both cases were about as successful at completing their taskse.g., finding the answer to a question such as Identify a set of Roman ruinsin presentday France. Turpin and Hersh believed that this showed that thedifference between say 3 and 5 relevant documents in a list of 10 documentswas not really material to the user nor did it matter much if the relevantdocuments were right at the top of the list or a bit further down. Subjects wereable to scan through the titles and brief synopses and quickly locate a relevantdocument.This section considers measures of recommender system usefulness thatmove beyond accuracy to include suitability of the recommendations to users.Suitability includes coverage, which measures the percentage of a dataset thatthe recommender system is able to provide predictions for confidence metrics that can help users make more effective decisions the learning rate,which measures how quickly an algorithm can produce good recommendations and noveltyserendipity, which measure whether a recommendationis a novel possibility for a user. Finally, we explore measures of recommender system utility based on user satisfaction with and performance on asystem.ACM Transactions on Information Systems, Vol. 22, No. 1, January 2004.40  J. L. Herlocker et al.5.1 CoverageThe coverage of a recommender system is a measure of the domain ofitems in the system over which the system can form predictions or makerecommendations. Systems with lower coverage may be less valuable to users,since they will be limited in the decisions they are able to help with. Coverage is particularly important for the Find All Good Items task, since systemsthat cannot evaluate many of the items in the domain cannot find all of thegood items in that domain. Coverage is also very important for the AnnotateIn Context task, as no annotation is possible for items where no prediction isavailable. Coverage can be most directly defined on predictions by asking Whatpercentage of items can this recommender form predictions for This type ofcoverage is often called prediction coverage. A different sort of coverage metriccan be formed for recommendations, more along the lines of What percentageof available items does this recommender ever recommend to users For anecommerce site, the latter form of coverage measures how much of the merchants catalog of items are recommended by the recommender for this reasonwell call it catalog coverage.Coverage has been measured by a number of researchers in the past Goodet al. 1999, Herlocker et al. 1999, Sarwar et al. 1998. The most common measure for coverage has been the number of items for which predictions can beformed as a percentage of the total number of items. The easiest way to measurecoverage of this type is to select a random sample of useritem pairs, ask fora prediction for each pair, and measure the percentage for which a predictionwas provided. Much as precision and recall must be measured simultaneously,coverage must be measured in combination with accuracy, so recommendersare not tempted to raise coverage by making bogus predictions for everyitem.An alternative way of computing coverage considers only coverage over itemsin which a user may have some interest. Coverage of this type is not usuallymeasured over all items, but only over those items a user is known to haveexamined. For instance, when the predictive accuracy is computed by hidinga selection of ratings and having the recommender compute a prediction forthose ratings, the coverage can be measured as the percentage of covered itemsfor which a prediction can be formed. The advantage of this metric is it maycorrespond better to user needs, since it is not important whether a systemcan recommend items a user has no interest in. For instance, if a user hasno interest in particle physics, it is not a disadvantage that a particular recommender system for research papers cannot form predictions for her aboutparticle physics.Catalog coverage, expressed as the percentage of the items in the catalogthat are ever recommended to users, has been measured less often. Catalogcoverage is usually measured on a set of recommendations formed at a singlepoint in time. For instance, it might be measured by taking the union of the top10 recommendations for each user in the population. Similarly to all coveragemetrics, this metric distorts if it is not considered in combination with accuracy.For instance, if there is an item in the catalog that is uninteresting to all users,ACM Transactions on Information Systems, Vol. 22, No. 1, January 2004.Evaluating Collaborative Filtering Recommender Systems  41a good algorithm should never recommend it, leading to lower coveragebuthigher accuracy.We know of no perfect, general coverage metric. Such a metric would havethe following characteristics 1 It would measure both prediction coverageand catalog coverage 2 For prediction coverage it would more heavily weightitems for which the user is likely to want predictions 3 There would be away to combine the coverage measure with accuracy measures to yield an overall practical accuracy measure for the recommender system. Recommendersystems researchers must continue to work to develop coverage metrics withthese properties. In the meantime, we should continue to use the best availablemetrics, and it is crucial that we continue to report the coverage of our recommender systems. Best practices are to report the raw percentage of items forwhich predictions can be formed, and to also report catalog coverage for recommender algorithms. Where practical, these metrics should be augmented withmeasures that more heavily weight likely items. These metrics should be considered experimental, but will eventually lead to more useful coverage metrics.Comparing recommenders along these dimensions will ensure that new recommenders are not achieving accuracy by cherrypicking easytorecommenditems, but are providing a wide range of useful recommendations to users.5.2 Learning RateCollaborative filtering recommender systems incorporate learning algorithmsthat operate on statistical models. As a result, their performance varies basedon the amount of learning data available. As the quantity of learning dataincreases, the quality of the predictions or recommendations should increase.Different recommendation algorithms can reach acceptable quality of recommendations at different rates. Some algorithms may only need a few datapoints to start generating acceptable recommendations, while others may needextensive data points. Three different learning rates have been considered inrecommender systems overall learning rate, per item learning rate, and peruser learning rate. The overall learning rate is recommendation quality as afunction of the overall number of ratings in the system or the overall number ofusers in the system. The peritem learning rate is the quality of predictions foran item as a function of the number of ratings available for that item. Similarlythe peruser learning rate is the quality of the recommendations for a user asa function of the number of ratings that user has contributed.The issue of evaluating the learning rates in recommender systems has notbeen extensively covered in the literature, although researchers such as Scheinet al. 2001 have looked at evaluating the performance of recommender systems in coldstart situations. Coldstart situations commonly referred to asthe startup problem refer to situations where there are only a few ratings onwhich to base recommendations. Learning rates are nonlinear and asymptoticquality cant improve forever, and thus it is challenging to represent themcompactly. The most common method for comparing the learning rates of different algorithms is to graph the quality versus the number of ratings qualityis usually accuracy.ACM Transactions on Information Systems, Vol. 22, No. 1, January 2004.42  J. L. Herlocker et al.The lack of evaluation of learning rates is due largely to the size of the Eachmovie, MovieLens, and Jester datasets, all of which have a substantial numberof ratings. As recommender systems spread into the more datasparse domains,algorithm learning rates will become a much more significant evaluation factor.5.3 Novelty and SerendipitySome recommender systems produce recommendations that are highly accurate and have reasonable coverageand yet that are useless for practical purposes. For instance, a shopping cart recommender for a grocery store mightsuggest bananas to any shopper who has not yet selected them. Statistically,this recommendation is highly accurate almost everyone buys bananas. However, everyone who comes to a grocery store to shop has bought bananas in thepast, and knows whether or not they want to purchase more. Further, grocerystore managers already know that bananas are popular, and have already organized their store so people cannot avoid going past the bananas. Thus, mostof the time the shopper has already made a concrete decision not to purchasebananas on this trip, and will therefore ignore a recommendation for bananas.Much more valuable would be a recommendation for the new frozen food thecustomer has never heard ofbut would love. A similar situation occurs ina music store around very well known items, like the Beatles famous WhiteAlbum. Every music aficionado knows about the White Albumand most already own it. Those who do not own it already have likely made a consciousdecision not to own it. A recommendation to purchase it is therefore unlikelyto lead to a sale. In fact, the White Album is an even worse recommendationthan bananas, since most people only buy one copy of any given album. Muchmore valuable would be a recommendation for an obscure garage band thatmakes music that this customer would love, but will never hear about througha review or television ad.Bananas in a grocery store, and the White Album in a music store, are examples of recommendations that fail the obviousness test. Obvious recommendations have two disadvantages first, customers who are interested in thoseproducts have already purchased them and second, managers in stores do notneed recommender systems to tell them which products are popular overall.They have already invested in organizing their store so those items are easilyaccessible to customers.Obvious recommendations do have value for new users. Swearingen andSinha 2001 found that users liked receiving some recommendations of itemsthat they already were familiar with. This seems strange since such recommendations do not give users any new information. However, what they doaccomplish is to increase user confidence in the system which is very importantfor the Find Credible Recommender task. Additionally, users were more likelyto say they would buy familiar items than novel ones. This contrasts with thesituation when users were asked about downloading material for free e.g., asis the case for many technical papers on the Web or for many mp3 music files.Here, users tended to prefer more novel recommendations. The general lessonto take away is that a system may want to try to estimate the probability thata user will be familiar with an item. For some tasks and perhaps early in theACM Transactions on Information Systems, Vol. 22, No. 1, January 2004.Evaluating Collaborative Filtering Recommender Systems  43course of users experience with the system, a greater number of familiar itemsshould be recommended for others, fewer or none should be included.We need new dimensions for analyzing recommender systems that considerthe nonobviousness of the recommendation. One such dimension is novelty,which has been addressed before in information retrieval literature see BaezaYates and RibieroNeto 1999 for a brief discussion of novelty in informationretrieval. Another related dimension is serendipity. A serendipitous recommendation helps the user find a surprisingly interesting item he might nothave otherwise discovered. To provide a clear example of the difference between novelty and serendipity, consider a recommendation system that simplyrecommends movies that were directed by the users favorite director. If thesystem recommends a movie that the user wasnt aware of, the movie will benovel, but probably not serendipitous. The user would have likely discoveredthat movie on their own. On the other hand, a recommender that recommendsa movie by a new director is more likely to provide serendipitous recommendations. Recommendations that are serendipitous are by definition also novel.The distinction between novelty and serendipity is important when evaluating collaborative filtering recommender algorithms, because the potential forserendipitous recommendations is one facet of collaborative filtering that traditional contentbased information filtering systems do not have. It is importantto note that the term, serendipity, is sometimes incorrectly used in the literaturewhen novelty is actually being discussed.Several researchers have studied novelty and serendipity in the context ofcollaborative filtering systems Sarwar et al. 2001. They have modified theiralgorithms to capture serendipity by preferring to recommend items that aremore preferred by a given user than by the population as a whole. A simplemodification is to create a list of obvious recommendations, and remove theobvious ones from each recommendation list before presenting it to users. A disadvantage of this approach is that the list of obvious items might be differentfor each user, since each person has had different experiences in the past. Analternative would combine what is known about the users tastes with what isknown about the communitys tastes. For instance, consider a hypothetical recommender that can produce a list of the probabilities for each item in the systemthat a given user will like the item. A naive recommender would recommendthe top 10 items in the listbut many of these items would be obvious to thecustomer. An alternative would be to divide each probability by the probabilitythat an average member of the community would like the item, and resort bythe ratio. Intuitively, each ratio represents the amount that the given user willlike the product more than most other users. Very popular items will be recommended only if they are likely to be exceptionally interesting to the presentuser. Less popular items will often be recommended, if they are particularlyinteresting to the present user. This approach will dramatically change the setof recommendations made to each user, and can help users uncover surprisingitems that they like.Designing metrics to measure serendipity is difficult, because serendipity is ameasure of the degree to which the recommendations are presenting items thatare both attractive to users and surprising to them. In fact, the usual methodsACM Transactions on Information Systems, Vol. 22, No. 1, January 2004.44  J. L. Herlocker et al.for measuring quality are directly antithetical to serendipity. Using the itemsusers have bought in the past as indicators of their interest, and covering itemsone by one to see if the algorithm can rediscover them, rewards algorithms thatmake the most obvious recommendations.A good serendipity metric would look at the way the recommendations arebroadening the users interests over time. To what extent are they for typesof things she has never purchased before How happy is she with the itemsrecommended Does she return a higher percentage of them than other itemsGood novelty metrics would look more generally at how well a recommendersystem made the user aware of previously unknown items. To what extent doesthe user accept the new recommendations We know of no systematic attempt tomeasure all of these facets of novelty and serendipity, and consider developinggood metrics for novelty and serendipity an important open problem.5.4 ConfidenceUsers of recommender systems often face a challenge in deciding how to interpret the recommendations along two often conflicting dimensions. The firstdimension is the strength of the recommendation how much does the recommender system think this user will like this item. The second dimension is theconfidence of the recommendation how sure is the recommender system that itsrecommendation is accurate. Many operators of recommender systems conflatethese dimensions inaccurately they assume that a user is more likely to like anitem predicted five stars on a five star scale than an item predicted four stars onthe same scale. That assumption is often false very high predictions are oftenmade on the basis of small amounts of data, with the prediction regressing tothe mean over time as more data arrives. Of course, just because a predictionis lower does not mean it is made based on more dataAnother, broader, take on the importance of confidence derives from considering recommender systems as part of a decisionsupport system. The goal of therecommendation is to help users make the best possible decision about what tobuy or use given their interests and goals. Different shortterm goals can leadto different preferences for types of recommendations. For instance, a user selecting a research paper about agent programming might prefer a safe reliablepaper that gives a complete overview of the area, or a risky, thoughtprovokingpaper to stimulate new ideas. The same user might prefer the overview paperif she is looking for a paper to reference in a grant proposal, or the thoughtprovoking paper if she is looking for a paper to read with her graduate students.How can the recommender system help her understand which recommendationwill fit her current needsTo help users make effective decisions based on the recommendations, recommender systems must help users navigate along both the strength and confidence dimension simultaneously. Many different approaches have been usedin practice. Ecommerce systems often refuse to present recommendations thatare based on datasets judged too small.6 They want recommendations their6Ecommerce managers will say that the first rule for a recommender system is Dont make melook stupidACM Transactions on Information Systems, Vol. 22, No. 1, January 2004.Evaluating Collaborative Filtering Recommender Systems  45customers can rely on. The Movie Critic system provided explicit confidence visualization with each recommendation a target with an arrow in it. The closerthe arrow was to the center, the more confident the recommendation. Herlockeret al. 2000 explored a wide range of different confidence displays, to studywhich ones influenced users to make the right decision. The study found thatthe choice of confidence display made a significant difference in users decisionmaking. The best confidence displays were much better than no display. Theworst displays actually worsened decisionmaking versus simply not displayingconfidence at all.Measuring the quality of confidence in a system is difficult, since confidenceis itself a complex multidimensional phenomenon that does not lend itselfto simple onedimensional metrics. However, recommenders that do not include some measure of confidence are likely to lead to poorer decisionmakingby users than systems that do include confidence. If the confidence displayshows users a quantitative or qualitative probability of how accurate the recommendation will be, the confidence can be tested against actual recommendations made to users. How much more accurate are the recommendationsmade with high confidence than those made with lower confidence If the confidence display is directly supporting decisions, measuring the quality of thedecisions made may be the best way to measure confidence. How much betteris decisionmaking when users are shown a measure of confidence than whennot5.5 User EvaluationThe metrics that we have discussed so far involve measuring variables that webelieve will affect the utility of a recommender system to the user and affect thereaction of the user to the system. In this section, we face the question of howto directly evaluate user reaction to a recommender system. The full spaceof user evaluation is considerably more complex than the space of the previously discussed metrics, so rather than examining specific metrics in detail, webroadly review the user evaluation space and past work in user evaluation ofrecommender systems. In order to better understand the space of user evaluation methods, we begin by proposing a set of evaluation dimensions. We usethese dimensions to organize our discussion of prior work, showing the typesof knowledge that can be gained through use of different methods. We close bysummarizing what we consider the best current practices for user evaluationsof recommender systems.Dimensions for User EvaluationExplicit ask vs. implicit observe. A basic distinction is between evaluations that explicitly ask users about their reactions to a system and thosethat implicitly observe user behavior. The first type of evaluation typicallyemploys survey and interview methods. The second type usually consists oflogging user behavior, then subjecting it to various sorts of analyses.Laboratory studies vs. field studies. Lab studies allow focused investigationof specific issues they are good for testing welldefined hypotheses underACM Transactions on Information Systems, Vol. 22, No. 1, January 2004.46  J. L. Herlocker et al.controlled conditions. Field studies can reveal what users actually do in theirown real contexts, showing common uses and usage patterns, problems andunmet needs, and issues that investigators may not have thought of to consider in a lab study. In particular, tasks such as Evaluate Recommender andExpress Self may require field studies because user behavior may be highlycontextsensitive.Outcome vs. process. For any task, appropriate metrics must be developedthat define what counts as a successful outcome Newman 1997. From asystems perspective, accuracy may be the fundamental metric. From a userperspective, however, metrics must be defined relative to their particulartasks. For most tasks such as Find Good Items a successful outcome requiresusers to act on the systems recommendations, and actually purchase a book,rent a movie, or download a paper. However, to simply measure whether agoal is achieved is not sufficient. Systems may differ greatly in how efficientlyusers may complete their tasks. Such process factors as amount of time andeffort required to complete basic tasks also must be measured to ensure thatthe cost of a successful outcome does not outweigh the benefit.Shortterm vs. longterm. Some issues may not become apparent in a shortterm study, particularly a lab study. For example, recall that Turpin andHersh found that subjects were able to perform information retrieval tasksjust as successfully with a less accurate search engine. However, if subjectscontinually had to read more summaries and sift through more offtopic information, perhaps they would grow dissatisfied, get discouraged, and eventually stop using the system.We consider several studies to illustrate the use of these methods. Studiesby Cosley et al. 2003, Swearingen and Sinha 2001, Herlocker et al. 2000,and McDonald 2001 occupy roughly the same portion of the evaluation space.They are shortterm lab studies that explicitly gather information from users.They all do some study of both task and process, although this dimension wasnot an explicit part of their analysis. Amento et al. 1999, 2003 also did ashortterm lab study, but it gathered both implicit and explicit user informationand explicitly measured both task outcomes and process. Finally, Dahlen et al.1998 did a lab study that used offline analysis to replay the history of userinteractions, defining and measuring implicit metrics of user participation overthe long term.Most recommender systems include predicted user ratings with the itemsthey recommend. Cosley et al. 2003 conducted a lab study to investigate howthese predicted ratings influence the actual ratings that users enter. They presented subjects with sets of movies they had rated in the past in some cases,the predictions were identical to the subjects past ratings, in some cases, theywere higher, and in some they were lower. Cosley et al. found that the predictedratings did influence user ratings, causing a small, but significant bias in thedirection of the prediction. They also found that presenting inaccurate predictions reduced user satisfaction with the system. Thus, the methods used in thisstudy yielded some evidence that users are sensitive to the predictive accuracyof the recommendations they receive.ACM Transactions on Information Systems, Vol. 22, No. 1, January 2004.Evaluating Collaborative Filtering Recommender Systems  47Swearingen and Sinha 2002, 2001 carried out a study to investigate theperceived usefulness and usability of recommender systems. Subjects usedeither three movie recommenders or three book recommenders. They beganby rating enough items for the system to be able to compute recommendations for them. They then looked through the recommendations, rating eachone as useful andor new, until they found at least one item they judgedworth trying. Finally, they completed surveys and were interviewed by theexperimenters.The methods used in this experiment let the researchers uncover issues otherthan prediction accuracy that affected user satisfaction. For example, usersmust develop trust in a recommender system, and recommendations of familiar items supports this process. Explanations of why an item was recommendedalso helped users gain confidence in a systems recommendations. Users alsoface the problem of evaluating a systems recommendationsfor example, amovie title alone is insufficient to convince someone to go see it. Thus, theavailability and quality of supporting information a system providedfor example, synopses, reviews, videos or sound sampleswas a significant factor inpredicting how useful users rated the system. A final point shows that userreactions may have multiple aspectssatisfaction alone may be insufficient.For example, subjects liked Amazon more than MediaUnbound and were morewilling to purchase from Amazon. However, MediaUnbound was rated as moreuseful, most likely to be used again, and as best understanding user tastes.Further analysis showed that Amazons greater use of familiar recommendations may be the cause of this difference. The general point, however, is that forsome purposes, users prefer one system, and for other purposes, the other.Herlocker et al. 2000 carried out an indepth exploration of explanationsfor recommender systems. After developing a conceptual model of the factorsthat could be used to generate an explanation, they empirically tested a number of different explanation types. They used traditional usability evaluationmethods, discovering that users preferred explanations based on how a usersneighbors rated an item, the past performance of the recommender system, similarity of an item to other items the user rated highly, and the appearance of afavorite actor or actress. Specifically, they led users to increase their estimateof the probability that they would see a recommended movie.McDonald 2001 conducted a controlled study of his Expertise Recommendersystem. This system was developed within a particular organizational context,and could suggest experts who were likely to be able to solve a particular problem and who were socially close to the person seeking help. The notable featureof McDonalds study was that subjects were given a rich scenario for evaluatingrecommendations, which specified a general topic area and a specific problem.In other words, the study was explicit in attempting to situate users in a taskcontext that would lead them to evaluate the recommendations within thatcontext.Amento et al. 1999, 2003 evaluated their topic management system, whichlets users explore, evaluate, and organize collections of websites. They comparedtheir system to a Yahoostyle interface. They gathered independent expert ratings to serve as a site quality metric and used these ratings to define the taskACM Transactions on Information Systems, Vol. 22, No. 1, January 2004.48  J. L. Herlocker et al.outcome metric as the number of highquality sites subjects selected using eachsystem. In addition, they measured task time and various effort metrics, suchas the number of sites subjects browsed. Thus, a key feature of their methodswas they were able to measure outcomes and process together they found thatusers of their system achieved superior results with less time and effort. Appropriate metrics will vary between tasks and domains however, often effort canbe conceived in terms of the number of queries issued or the number of itemsfor which detailed information is examined during the process of evaluatingrecommendations.Dahlen et al. 1998 studied the value of jumpstarting a recommender system by including dead datathat is, ratings from users of another, inactivesystem. Their experimental procedure involved replaying the history of bothsystems, letting them evaluate the early experience of users in terms of theirparticipation. This was the only course of action open, since it was impossible tosurvey users of the previous system. They found that early users of the jumpstarted system participated more extensivelythey used the system more often, at shorter intervals, and over a long period of time, and they also enteredmore ratings. We believe that, in general, user contribution to and participation in recommender systems in the long term is quite important and relativelyunderappreciated. And we believe that the metrics used in the jumpstartingstudy can be applied quite broadly.To summarize our observations on user evaluation, we emphasize that accurate recommendations alone do not guarantee users of recommender systemsan effective and satisfying experience. Instead, systems are useful to the extent that they help users complete their tasks. A fundamental point proceedsfrom this basis to do an effective user evaluation of a recommender system,researchers must clearly define the tasks the system is intended to support.7Observations of actual use if available and interviews with actual or prospective users are appropriate techniques, since it often is the case that systemsend up being used differently than the designers anticipated. Once tasks aredefined, they can be used in several ways. First, they can be used to tailor algorithm performance. Second, clearly defined tasks can increase the effectivenessof lab studies. Subjects can be assigned a set of tasks to complete, and variousalgorithms or interfaces can be tested to find out which ones lead to the besttask outcomes.We also recommend that evaluations combine explicit and implicit data collection whenever possible. This is important because user preferences and performance may diverge users may prefer one system to another, even when theirperformance is the same on both, or vice versa. One advantage of gathering dataabout both performance and preferences is that the two can be correlated. Thisis analogous to work on correlating implicit and explicit ratings Claypool et al.2001, Morita and Shinoda 1994 Having done this, future evaluations that cangather only one of these types of data can have some estimate of what the othertype of data would show.7Whittaker et al. 2000 elaborate on this theme to develop a research agenda for HumanComputerInteraction centered around the notion of reference tasks.ACM Transactions on Information Systems, Vol. 22, No. 1, January 2004.Evaluating Collaborative Filtering Recommender Systems  496. CONCLUSIONEffective and meaningful evaluation of recommender systems is challenging.To date, there has been no published attempt to synthesize what is knownabout the evaluation of recommender systems, nor to systematically understand the implications of evaluating recommender systems for different tasksand different contexts. In this article, we have attempted to overview the factors that have been considered in evaluations as well as introduced new factorsthat we believe should be considered in evaluation. In addition, we have introduced empirical results on accuracy metrics that provide some initial insightinto how results from different evaluation metrics might vary. Our hope is thatthis article will increase the awareness of potential biases in reported evaluations, increase the diversity of evaluation dimensions examined where it isnecessary, and encourage the development of more standardized methods ofevaluation.6.1 Future WorkWhile there are many open research problems in recommender systems, wefind four evaluationrelated problems to be particularly worthy of attention.User Sensitivity to Algorithm Accuracy. We know from recent work byCosley et al. 2003 that user satisfaction is decreased when a significant level oferror is introduced into a recommender system. The level of error introduced inthat study, however, was many times larger than the differences between thebest algorithms. Key questions deserving attention include a For differentmetrics, what is the level of change needed before users notice or user behavior changes b To which metrics are users most sensitive c How does usersensitivity to accuracy depend on other factors such as the interface d Howdo factors such as coverage and serendipity affect user satisfaction If thesequestions are answered, it may be possible to build a predictive model of usersatisfaction that would permit more extensive offline evaluation.Algorithmic Consistency Across Domains. While a few studies have lookedat multiple datasets, no researchers have systematically compared a set ofalgorithms across a variety of different domains to understand the extent towhich different domains are better served by different classes of algorithms.If such research did not find differences, it would simplify the evaluation ofalgorithmssystem designers could select a dataset with the desired propertieswithout needing domainspecific testing.Comprehensive Quality Measures. Most metrics to date focus on accuracy,and ignore issues such as serendipity and coverage. There are wellknown techniques by which algorithms can tradeoff reduced serendipity and coverage forimproved accuracy such as only recommending items for which there are manyratings. Since users value all three attributes in many applications, these algorithms may be more accurate, but less useful. We need comprehensive qualitymeasures that combine accuracy with other serendipity and coverage, so algorithm designers can make sensible tradeoffs to serve users better.ACM Transactions on Information Systems, Vol. 22, No. 1, January 2004.50  J. L. Herlocker et al.Discovering the Inherent Variability in Recommender Datasets. We speculate above that algorithms trying to make better predictions on movie datasetsmay have reached the optimal level of error given human variability. Such variability can be explored using testretest situations and analyses of taste changeover time. If we can find effective ways to analyze datasets and learn the inherent variability, we can discover sooner when researchers have mined as muchdata as possible from a dataset, and thus when they should shift their attentionfrom accuracy to other attributes of recommender systems.ACKNOWLEDGMENTSWe would like to express our appreciation to the present and past membersof the GroupLens Research Group, our colleagues at ATT Research, and ourcurrent students. Wed also like to thank our many colleagues in the recommender systems community with whom weve had fruitful discussions over theyears.REFERENCESAGGARWAL, C. C., WOLF, J. L., WU, K.L., AND YU, P. S. 1999. Horting hatches an egg A newgraphtheoretic approach to collaborative filtering. In Proceedings of ACM SIGKDD InternationalConference on Knowledge Discovery  Data Mining. ACM, New York.AMENTO, B., TERVEEN, L., HILL, W., HIX, D., AND SCHULMAN, R. 2003. Experiments in social datamining The TopicShop System. ACM Trans. ComputerHuman Interact. 10, 1 Mar., 5485.AMENTO, B., TERVEEN, L., HIX, D., AND JU, P. 1999. An empirical evaluation of user interfaces fortopic management of web sites. In Proceedings of the 1999 Conference on Human Factors inComputing Systems CHI 99. ACM, New York, 552559.BAEZAYATES, R. AND RIBIERONETO, B. 1999. Modern Information Retrieval. AddisonWesleyLongman, Boston, Mass.BAILEY, B. P., GURAK, L. J., AND KONSTAN, J. A. 2001. An examination of trust production incomputermediated exchange. In Proceedings of the 7th Conference on Human Factors and theWeb July.BALABANOVIC, M. AND SHOHAM, Y. 1997. Fab Contentbased, collaborative recommendation.Commun. ACM 40, 6672.BASU, C., HIRSH, H., COHEN, W. W. 1998. Recommendation as classification using social andcontentbased information in recommendation. In Proceedings of the 15th National Conferenceon Artificial Intelligence AAAI98. C. Rich, and J. Mostow, Eds. AAAI Press, Menlo Park, Calif.,714720.BILLSUS, D. AND PAZZANI, M. J. 1998. Learning collaborative information filters. In Proceedings ofthe 15th National Conference on Artificial Intelligence AAAI98. C. Rich, and J. Mostow, Eds.AAAI Press, Menlo Park, Calif., 4653.BREESE, J. S., HECKERMAN, D., AND KADIE, C. 1998. Empirical analysis of predictive algorithmsfor collaborative filtering. In Proceedings of the 14th Conference on Uncertainty in Artificial Intelligence UAI98. G. F. Cooper, and S. Moral, Eds. MorganKaufmann, San Francisco, Calif.,4352.CANNY, J. 2002. Collaborative filtering with privacy via factor analysis. In Proceedings of the25th Annual International ACM SIGIR Conference on Research and Development in Informationretrieval. ACM, New York, 238245.CLAYPOOL, M., BROWN, D., LE, P., AND WASEDA, M. 2001. Inferring user Interest. IEEE InternetComput. 5, 3239.CLEVERDON, C. AND KEAN, M. 1968. Factors Determining the Performance of Indexing Systems.Aslib Cranfield Research Project, Cranfield, England.COSLEY, D., LAM, S. K., ALBERT, I., KONSTAN, J. A., AND RIEDL, J. 2003. Is seeing believing Howrecommender interfaces affect users opinions. CHI Lett. 5.ACM Transactions on Information Systems, Vol. 22, No. 1, January 2004.Evaluating Collaborative Filtering Recommender Systems  51DAHLEN, B. J., KONSTAN, J. A., HERLOCKER, J. L., GOOD, N., BORCHERS, A., AND RIEDL, J. 1998. Jumpstarting movielens User benefits of starting a collaborative filtering system with dead data. TR98017. University of Minnesota.DOMINGOS, P. AND RICHARDSON, M. 2003. Mining the network value of customers. In Proceedingsof the 7th International Conference on Knowledge Discovery and Data Mining. ACM, New York,5766.GOLDBERG, D., NICHOLS, D., OKI, B. M., AND TERRY, D. 1992. Using collaborative filtering to weavean information tapestry. Commun. ACM 35, 6170.GOLDBERG, K., ROEDER, T., GUPTRA, D., AND PERKINS, C. 2001. Eigentaste A constanttime collaborative filtering algorithm. Inf. Retr. 4, 133151.GOOD, N., SCHAFER, J. B., KONSTAN, J. A., BORCHERS, A., SARWAR, B. M., HERLOCKER, J. L., AND RIEDL,J. 1999. Combining collaborative filtering with personal agents for better recommendations.In Proceedings of the 16th National Conference on Artificial Intelligence AAAI99, J. Hendler,and D. Subramanian, Eds. AAAI Press, Menlo Park, Calif., 439446.HANLEY, J. A. AND MCNEIL, B. J. 1982. The meaning and use of the area under a receiver operatingcharacteristic ROC curve. Radiology 143, 2936.HARMAN, D. 1995. The TREC conferences. HypertextInformation RetrievalMultimedia Synergieeffekte Elektronisher Informationssysteme. In Proceedings of HIM 95.HARTER, S. P. 1996. Variations in relevance assessments and the measurement of retrieval effectiveness. J. ASIS 47, 3749.HECKERMAN, D., CHICKERING, D. M., MEEK, C., ROUNTHWAITE, R., AND KADIE, C. 2000. Dependencynetworks for inference, collaborative filtering, and data visualization. J. Mach. Learn. Res. 1,4975.HELANDER, M. 1988. Handbook of HumanComputer Interaction. North Holland, Amsterdam.HERLOCKER, J. L., KONSTAN, J. A., BORCHERS, A., AND RIEDL, J. 1999. An algorithmic frameworkfor performing collaborative filtering. In Proceedings of the 22nd International Conference onResearch and Development in Information Retrieval SIGIR 99 Aug. M. A. Hearst, F. F. Gey,and R. Tong, Eds. ACM, New York. 230237.HERLOCKER, J. L., KONSTAN, J. A., AND RIEDL, J. 2000. Explaining collaborative filtering recommendations. In Proceedings of the 2000 Conference on Computer Supported Cooperative Work,241250.HERLOCKER, J. L., KONSTAN, J. A., AND RIEDL, J. 2002. An empirical analysis of design choices inneighborhoodbased collaborative filtering algorithms. Inf. Retr. 5, 287310.HILL, W., STEAD, L., ROSENSTEIN, M., AND FURNAS, G. W. 1995. Recommending and evaluatingchoices in a virtual community of use. In Proceedings of ACM CHI95 Conference on HumanFactors in Computing Systems. ACM, New York, 194201.KONSTAN, J. A., MILLER, B. N., MALTZ, D., HERLOCKER, J. L., GORDON, L. R., AND RIEDL, J. 1997. GroupLens Applying collaborative filtering to usenet news. Commun. ACM 40, 7787.LE, C. T., LINDREN, B. R. 1995. Construction and comparison of two receiver operating characteristics curves derived from the same samples. Biom. J. 37, 869877.LINTON, F., CHARRON, A., AND JOY, D. 1998. OWL A recommender system for organziationwidelearning. In Proceedings of the 1998 Workshop on Recommender Systems 6569.MCDONALD, D. W. 2001. Evaluating Expertise Recommendations. In Proceedings of the ACM 2001International Conference on Supporting Group Work GROUP01. ACM, New York.MCNEE, S., ALBERT, I., COSLEY, D., GOPALKRISHNAN, P., RASHID, A. M., KONSTAN, J. A., AND RIEDL, J. 2002.On the recommending of citations for research papers. In Proceedings of ACM CSCW 2002. ACM,New York.MILLER, B. N., ALBERT, I., LAM, S. K., KONSTAN, J. A., AND RIEDL, J. 2003. MovieLens unpluggedExperiences with a recommender systems on four mobile devices. In Proceedings of the 2003Conference on Intelligent User Interfaces.MILLER, B. N., RIEDL, J., AND KONSTAN, J. A. 1997. Experiences with GroupLens Making Usenetuseful again. In Proceedings of the 1997 USENIX Technical Conference.MOBASHER, B., DAI, H., LUO, T., AND NAKAGAWA, M. 2001. Effective personalization based on association rule discovery from web usage data. In Proceedings of the 3rd ACM Workshop on WebInformation and Data Management WIDM01, held in conjunction with the International Conference on Information and Knowledge Management CIKM 2001. ACM, New York.ACM Transactions on Information Systems, Vol. 22, No. 1, January 2004.52  J. L. Herlocker et al.MORITA, M. AND SHINODA, Y. 1994. Information filtering based on user behavior analysis and bestmatch text retrieval. In Proceedings of SIGIR 94, ACM, New York. 272281.MUI, L., ANG, C., AND MOHTASHEMI, M. 2001. A Probabilistic Model for Collaborative Sanctioning.Technical Memorandum 617. MIT LCS.NEWMAN, W. 1997. Better or just different On the benefits of designing interactive systems interms of critical parameters. In Proceedings of the Designing Interactive Systems DIS97. ACM,New York, 239246.NIELSEN, J. 1994. Usability Engineering. Academic Press, San Diego, Calif.PENNOCK, D. M., HORVITZ, E., LAWRENCE, S., AND GILES, C. L. 2000. Collaborative filtering by personality diagnosis A hybrid memory and modelbased approach. In Proceedings of the 16thAnnual Conference on Uncertainty in Artificial Intelligence UAI2000. Morgan Kaufmann, SanFrancisco, Calif., 473480.RASHID, A. M., ALBERT, I., COSLEY, D., LAM, S. K., MCNEE, S., KONSTAN, J. A., AND RIEDL, J. 2002.Getting to know you Learning new user preferences in recommender systems. In Proceedings ofthe 2002 Conference on Intelligent User Interfaces IUI 2002. 127134.REDDY, P. K., KITSUREGAWA, P., SREEKANTH, P., AND RAO, S. S. 2002. A graph based approach toextract a neighborhood customer community for collaborative filtering. In Databases in Networked Information Systems, Second International Workshop. Lecture Notes in Computer ScienceSpringerVerlag, New York, 188200.RESNICK, P., IACOVOU, N., SUCHAK, M., BERGSTROM, P., AND RIEDL, J. 1994. GroupLens An open architecture for collaborative filtering of netnews. In Proceedings of the 1994 Conference on ComputerSupported Collaborative Work. R. Furuta and C. Neuwirth, Eds. ACM, New York. 175186.RESNICK, P. AND VARIAN, H. R. 1997. Recommender systems. Commun. ACM 40, 5658.ROGERS, S. C. 2001. Marketing Strategies, Tactics, and Techniques  A handbook for practitioners.Quorum Books, Westport, Conn.SARWAR, B. M., KARYPIS, G., KONSTAN, J. A., AND RIEDL, J. 2000a. Analysis of recommendationalgorithms for Ecommerce. In Proceedings of the 2nd ACM Conference on Electronic CommerceEC00. ACM, New York. 285295.SARWAR, B. M., KARYPIS, G., KONSTAN, J. A., AND RIEDL, J. 2000b. Application of dimensionalityreduction in recommender systemA case study. In Proceedings of the ACM WebKDD 2000 WebMining for ECommerce Workshop.SARWAR, B. M., KARYPIS, G., KONSTAN, J. A., AND RIEDL, J. 2001. Itembased collaborative filteringrecommendation algorithms. In Proceedings of the 10th International World Wide Web ConferenceWWW10.SARWAR, B. M., KONSTAN, J. A., BORCHERS, A., HERLOCKER, J. L., MILLER, B. N., AND RIEDL, J. 1998.Using filtering agents to improve prediction quality in the grouplens research collaborative filtering system. In Proceedings of the ACM 1998 Conference on Computer Supported CooperativeWork CSCW 98, ACM, New York.SCHAFER, J. B., KONSTAN, J. A., AND RIEDL, J. 2002. Metarecommendation systemsUsercontrolledintegration of diverse recommendations. In Proceedings of the 11th International Conference onInformation and Knowledge Management, Nov. 2002, 4351.SCHEIN, A. I., POPESCUL, A., UNGAR, L. H., AND PENNOCK, D. M. 2001. Generate models for coldstart recommendations. Proceedings of the 2001 ACM SIGIR Workshop on Recommender Systems.ACM, New York.SCHEIN, A. I., POPESCUL, A., UNGAR, L. H., AND PENNOCK, D. M. 2002. Methods and metrics forcoldstart collaborative filtering. In Proceedings of the 25th Annual international ACM SIGIRConference on Research and Development in Information Retrieval Aug.. ACM, New York.SHARDANAND, U. AND MAES, P. 1995. Social information filtering Algorithms for automating wordof mouth. In Proceedings of ACM CHI95 Conference on Human Factors in Computing Systems.ACM, New York. 210217.SINHA, R. AND SWEARINGEN, K. 2002. The role of transparency in recommender systems. In CHI2002 Conference Companion.SWEARINGEN, K. AND SINHA, R. 2001. Beyond algorithms An HCI perspective on recommendersystems. In Proceedings of the SIGIR 2001 Workshop on Recommender Systems.SWETS, J. A. 1963. Information retrieval systems. Science 141, 245250.SWETS, J. A. 1969. Effectiveness of information retrieval methods. Amer. Doc. 20, 7289.ACM Transactions on Information Systems, Vol. 22, No. 1, January 2004.Evaluating Collaborative Filtering Recommender Systems  53TURPIN, A. AND HERSH, W. 2001. Why batch and user evaluations do not give the same results.In Proceedings of the 24th Annual ACM SIGIR Conference on Research and Development inInformation Retrieval. ACM, New York, 1724.VOORHEES, E. M. AND HARMAN, D. K. 1999. Overview of the seventh Text REtrieval ConferenceTREC7. In NIST Special Publication 500242 July, E. M. Voorhees, and D. K. Harman, Eds.NIST, 124.WEXELBLAT, A. AND MAES, P. 1999. Footprints Historyrich tools for information foraging. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. M. G. Williams,and M. W. Altom, Eds. ACM, New York, 270277.WHITTAKER, S., TERVEEN, L. G., AND NARDI, B. 2000. Lets stop pushing the envelope and startaddressing it A reference task agenda for HCI. HumanComputer Interact. 15, 23 Sept., 75106.YAO, Y. Y. 1995. Measuring retrieval effectiveness based on user preference of documents.J. ASIS. 46, 133145.Received January 2003 revised June 2003 accepted August 2003ACM Transactions on Information Systems, Vol. 22, No. 1, January 2004.
