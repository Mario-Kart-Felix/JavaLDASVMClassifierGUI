Forschungszentrum L3SResearch CenterEmerging Applicationsof Link Analysis for RankingDer Fakultat fur Elektrotechnik und Informatik derGottfried Wilhelm Leibniz Universitat Hannoverzur Erlangung des akademischen GradesDoktorIngenieur vorgelegte DissertationVon Dipl.Ing. Paul  Alexandru Chiritageboren am 07.07.1980 in Bukarest, Rumanien.Hannover, Deutschland, 2007.Emerging Applications of Link Analysis for RankingiiAbstractThe booming growth of digitally available information has thoroughly increased the popularityof search engine technology over the past years. At the same time, upon interacting with thisoverwhelming quantity of data, people usually inspect only the very few most relevant items fortheir task. It is thus very important to utilize high quality ranking measures which efficientlyidentify these items under the various information retrieval activities we pursue.In this thesis we provide a twofold contribution to the Information Retrieval field. First, weidentify those application areas in which a user oriented ranking is missing, though extremelynecessary in order to facilitate a qualitative access to relevant resources. Second, for eachof these areas we propose appropriate ranking algorithms which exploit their underlying socialcharacteristics, either at the macroscopic, or at the microscopic level. We achieve this by utilizinglink analysis techniques, which build on top of the graph based representation of relationsbetween resources in order to rank them or simply to identify social patterns relative to theinvestigated data set.We start by arguing that Ranking Desktop Items is very effective in improving resource accesswithin Personal Information Repositories. Thus, we propose to move link analysis methods downto the PC Desktop by exploiting usage analysis statistics, and show the resulted importanceordering to be highly beneficial for the particular scenario of Desktop Search.We then apply the same technique for Spam Detection. We connect people across email socialnetworks based on their email exchanges and induce a reputation metric which nicely isolatesmalicious members of a community. Similarly, we model several higher level artificial constructswhich could negatively manipulate generic link analysis ranking algorithms, and indicate howto remove them in the case of Web page ranking.Finally, we exploit manually created large scale information repositories in order to PersonalizeWeb Search. We investigate two different types of such repositories, namely globally editedones and individually edited ones. For the former category we project link analysis onto publictaxonomies such as the Open Directory and define appropriate similarity measures which orderthe search output in accordance to each users preferences. For the latter one, we propose toexpand Web queries by utilizing both text and link analysis on top of Personal InformationRepositories. Extensive experiments analyzing both approaches show them to yield significantimprovements over regular Google search.iiiEmerging Applications of Link Analysis for RankingivZusammenfassungDer starke Zuwachs von elektronisch verfugbaren Daten haben stark zur Popularitat von Suchmaschinen beigetragen. Allerdings sind die Nutzer von Suchmaschinen typischerweise nur an den wenigen Dokumenten interessiert,die im Bezug auf ihre Arbeit die hochste Relevanz besitzen. Es ist also sehr wichtig hochwertige Rankingmethodenzu entwickeln, die effizient diese relevanten Dokumente fur die verschiedenen Aktivitaten zur Informationssucheidentifizieren, die solche Nutzer entwickeln.Diese Arbeit enthalt zwei Beitrage zu dem Bereich Information Retrieval. Erstens identifizieren wir die Anwendungsbereiche, in den ein nutzerorientiertes Ranking derzeit nicht vorhanden ist, obwohl es extrem notwendig ist,um einen hochqualitativen Zugang zu den fur einen Nutzer relevanten Ressourcen zu ermoglichen. Zweitens entwickeln wir fur jeden von diesen Anwendungsbereichen die entsprechenden Rankingalgorithmen, die auf sozialenCharakteristika aufbauen und diese ausnutzen, entweder auf einem makroskopischen oder einem mikroskopischen Niveau. Dies wird durch Link Analysis Techniken erreicht, die auf der graphbasierten Darstellung derVerknupfungen zwischen Objekten bauen, um sie zu ordnen oder einfach um Muster im Bezug auf deren sozialeEigenschaften zu erkennen.Wir fangen an und argumentieren, dass das Ranken von Objekten auf dem Desktop sehr effektiv den Zugang zuallen Ressourcen auf dem Desktop verbessern kann. Dafur schlagen wir vor, die Link Analysis Methoden auchauf dem Desktop zu nutzen unter Verwendung von Statistiken ber das Nutzerverhalten. Wir zeigen, dass ein aufdiese Weise entwickeltes Ranking sehr vorteilhaft fur das Anwendungsszenario einer DesktopSuchmaschine ist.Anschlieend setzen wir dieselben grundlegenden Ideen fur die Erkennung von Spam Emails ein. Dazu verbindenwir Menschen in sozialen Netzwerken, basierend auf dem Austausch von Emails zwischen diesen, und leitendaraus eine Reputationsmetrik ab, die boswillige Mitglieder jeder Community isoliert. Auf eine ahnliche Weisemodellieren wir mehrere kunstliche Linkstrukturen auf einer hoheren Abstraktionsebene, die Link AnalysisAlgorithmen im allgemeinen negativ beeinflussen konnen. Wir geben auch an, wie man solche Linkstrukturenim Anwendungsszenario Ranken von Webseiten entfernen kann.Der letzte Teil dieser Arbeit nutzt manuell erstellte Informationsrepositorien, um die Web Suche zu personalisieren. Wir untersuchen zwei verschiedene Arten von solchen Repositorien, solche die global bearbeitet werdenkonnen und solche die individuell bearbeitet werden konnen. Im ersten Fall wenden wir Link Analysis Techniken auf offentliche Webverzeichnissen an, wie zum Beispiel das Open Directory, und definieren geeignete Ahnlichkeitsmetriken, die die Suchergebnisse nach den Praferenzen des Nutzers anordnen. Fur individuell bearbeitbare Repositorien, schlagen wir eine Methode zur Erweiterung von Suchanfragen vor, die sowohl auf der Analysevon Text, als auch auf Link Analysis Methoden in Zusammenhang mit Personal Information Repositoriesberuhen. Ausfuhrliche Experimente, die beide Vorgehensweisen auswerten, zeigen in beiden Fallen wesentlicheVerbesserungen im Vergleich zu einer herkommlichen Suche mit Google.vEmerging Applications of Link Analysis for RankingviForewordThe algorithms presented in this thesis have been published within several Information Systems conferences, as follows.The usage analysis based Desktop ranking ideas were split across two interestareas 1 Semantic Web, when we aimed for specific user actions, modeled usuallyusing ontologies, 61, 62, 63, and 2 Information Retrieval, when all activitieswere logged and analyzed from a statistical point of view 66 Beagle Semantically Enhanced Searching and Ranking on the Desktop.By Paul  Alexandru Chirita, Stefania Ghita, Wolfgang Nejdl, Raluca Paiu.In Proceedings of the 3rd European Semantic Web Conference ESWC,Budva, Montenegro, 2006 63. ActivityBased Metadata for Semantic Desktop Search. By Paul  Alexandru Chirita, Stefania Ghita, Rita Gavriloaie, Wolfgang Nejdl, Raluca Paiu.In Proceedings of the 2nd European Semantic Web Conference ESWC,Heraklion, Greece, 2005 61. Semantically Enhanced Searching and Ranking on the Desktop. By Paul Alexandru Chirita, Stefania Ghita, Wolfgang Nejdl, Raluca Paiu. In Proceedings of the Semantic Desktop Workshop held at the 3rd InternationalSemantic Web Conference, Galway, Ireland, 2005 62. Analyzing User Behavior to Rank Desktop Items. By Paul  AlexandruChirita, Wolfgang Nejdl. In Proceedings of the 13th International Symposium on String Processing and Information Retrieval SPIRE, Glasgow,United Kingdom, 2006 66.The other two chapters have been focused exclusively on Information Retrievaltechniques. The work on spam detection was presented in less, but more importantconferences, after major parts of the research had been already completed 58, 44,28viiEmerging Applications of Link Analysis for Ranking MailRank Using Ranking for Spam Detection. By Paul  Alexandru Chirita,Jrg Diederich, Wolfgang Nejdl. In Proceedings of the 14th ACM International CIKM Conference on Information and Knowledge Management, Bremen, Germany, 2005 58. Site Level Noise Removal for Search Engines. By Andre Carvalho, Paul Alexandru Chirita, Edleno Silva de Moura, Pavel Calado, Wolfgang Nejdl. In Proceedings of the 15th International World Wide Web ConferenceWWW, Edinburgh, United Kingdom, 2006 44. An Analysis of Factors used in Search Engine Ranking. By Albert Bifet,Carlos Castillo, Paul  Alexandru Chirita, Ingmar Weber. In Proceedingsof the Adversarial Information Retrieval Workshop held at the 14th International World Wide Web Conference, Chiba, Japan, 2006 28.The most important chapter addresses the topic Web search personalization, andis built on top of the following publications 67, 60, 59, 56 Using ODP Metadata to Personalize Search. By Paul  Alexandru Chirita,Wolfgang Nejdl, Raluca Paiu, Christian Kohlschtter. In Proceedings of the28th ACM International SIGIR Conference on Research and Developmentin Information Retrieval, Salvador, Brazil, 2005 67. Summarizing Local Context to Personalize Global Web Search. By Paul Alexandru Chirita, Claudiu Firan, Wolfgang Nejdl. In Proceedings of the15th ACM International CIKM Conference on Information and KnowledgeManagement, Arlington, United States, 2006 60. PTAG Large Scale Automatic Generation of Personalized AnnotationTAGs for the Web. By Paul  Alexandru Chirita, Stefania Costache,Siegfried Handschuh, Wolfgang Nejdl. In Proceedings of the 16th International World Wide Web Conference WWW, Banff, Canada, 2007 56. Pushing Task Relevant Web Links down to the Desktop. By Paul  AlexandruChirita, Claudiu Firan, Wolfgang Nejdl. In Proceedings of the 8th ACMWorkshop on Web Information and Data Management WIDM held at the15th ACM International CIKM Conference on Information and KnowledgeManagement, Arlington, United States, 2006 59.During the Ph.D. work, I have also published a number of exercise papers, inwhich I mostly intended to capture the opinion of the research community uponeither one of the three above mentioned topics, or a fourth application of linkanalysis for ranking, namely PeerToPeer ranking. However, in order to keep thequality of the thesis at a high level, I decided to regard these articles as relatedwork, and discuss them only briefly within the appropriate background sections.Here is a complete list with all of themviiiPaul  Alexandru Chirita The Beagle Toolbox Towards an Extendable Desktop Search Architecture. By Ingo Brunkhorst, PaulAlexandru Chirita, Stefania Costache,Julien Gaugaz, Ekaterini Ioannou, Tereza Iofciu, Enrico Minack, WolfgangNejdl, Raluca Paiu. In Proceedings of the 2nd Semantic Desktop Workshopheld at the 5th International Semantic Web Conference, Athens, UnitedStates, 2006 37. Desktop Context Detection Using Implicit Feedback. By Paul  AlexandruChirita, Julien Gaugaz, Stefania Costache, Wolfgang Nejdl. In Proceedingsof the Workshop on Personal Information Management held at the 29thACM International SIGIR Conf. on Research and Development in Information Retrieval, Seattle, United States, 2006. 55. Building a Desktop Search Testbed poster. Sergey Chernov, PavelSerdyukov, Paul  Alexandru Chirita, Gianluca Demartini, and WolfgangNejdl. In Proceedings of the 29th European Conference on InformationRetrieval ECIR, Rome, Italy, 2007 52. Preventing Shilling Attacks in Online Recommender Systems. By Paul Alexandru Chirita, Wolfgang Nejdl, Cristian Zamfir. In Proceedings of the7th ACM Workshop on Web Information and Data Management WIDMheld at the 14th ACM International CIKM Conference on Information andKnowledge Management, Bremen, Germany, 2005 70. Efficient Parallel Computation of PageRank. By Christian Kohlschtter, Paul Alexandru Chirita, Wolfgang Nejdl. In Proceedings of the 28th EuropeanConference on Information Retrieval ECIR, London, United Kingdom,2006 145. PROS A Personalized Ranking Platform for Web Search. By Paul  Alexandru Chirita, Daniel Olmedilla, Wolfgang Nejdl. In Proceedings of the 3rdInternational Conference on Adaptive Hypermedia and Adaptive WebbasedSystems AHA, Eindhoven, Netherlands, 2004 73. Finding Related Hubs on the Link Structure of the WWW. By Paul  Alexandru Chirita, Daniel Olmedilla, Wolfgang Nejdl. In Proceedings of the 3rdIEEE  WIC  ACM International Conference on Web Intelligence WI,Beijing, China, 2004 72. Finding Related Hubs and Authorities poster. By Paul  AlexandruChirita, Daniel Olmedilla, Wolfgang Nejdl. In Proceedings of the 1st IEEELatinAmerican Web LAWeb Congress, Santiago, Chile 71. Using Link Analysis to Identify Aspects in Faceted Web Search. By ChristianKohlschtter, Paul  Alexandru Chirita, Wolfgang Nejdl. In Proc. of theFaceted Search Workshop held at the 29th Intl. ACM SIGIR Conf. on Res.and Development in Information Retrieval, Seattle, U.S.A., 2006 144.ixEmerging Applications of Link Analysis for Ranking Search Strategies for Scientific Collaboration Networks. By Paul  Alexandru Chirita, Andrei Damian, Wolfgang Nejdl, Wolf Siberski. In Proceedingsof the 2nd P2P Information Retrieval Workshop held at the 14th ACM International CIKM Conference on Information and Knowledge Management,Bremen, Germany, 2005 57. Designing PublishSubscribe Networks using SuperPeers. By Paul  Alexandru Chirita, Stratos Idreos, Manolis Koubarakis, Wolfgang Nejdl. In S.Staab and H. Stuckenschmidt eds. Semantic Web and PeertoPeer,Springer Verlag, 2004 64. PublishSubscribe for RDFBased P2P Networks. By Paul  AlexandruChirita, Stratos Idreos, Manolis Koubarakis, Wolfgang Nejdl. In Proceedings of the 1st European Semantic Web Symposium ESWS, Heraklion,Greece, 2004 65. Personalized Reputation Management in P2P Networks. By Paul  Alexandru Chirita, Wolfgang Nejdl, Mario Schlosser, Oana Scurtu. In Proceedingsof the Trust, Security and Reputation Workshop held at the 3rd International Semantic Web Conference, Hiroshima, Japan, 2004 68. Knowing Where to Search Personalized Search Strategies for Peers in P2PNetworks. By Paul  Alexandru Chirita, Wolfgang Nejdl, Oana Scurtu. InProceedings of the 1st P2P Information Retrieval Workshop held at the27th ACM International SIGIR Conference on Research and Developmentin Information Retrieval, Sheffield, United Kingdom, 2004 69.xAcknowledgementsFirst, I would like to thank Prof. Dr. Wolfgang Nejdl. Not because every Ph.D.student starts with acknowledging his supervisors work, but because in my case,his attentive guidance and continuous counseling during my masters thesis weredecisive in even making me consider pursuing a Ph.D. It was an excellent choice.Wolfgang taught me how to organize my thoughts, how to approach and solve aproblem, how to market my results. Born in the same day as my father, he wentbeyond such usual support He helped both me and my wife to have a growingcareer, to accelerate our immigration process, to enjoy our lives.I would also like to thank Prof. Dr. RicardoBaeza Yates for his kindness, for hisuseful research commments, and for hosting me at Yahoo Research in 2006.I am grateful to Prof. Dr. Klaus Jobmann and Prof. Dr. Heribert Vollmer foragreeing to be part of my dissertation committee.A few other professors shaped my way through this point. Prof. Dr. ValentinCristea believed in me and supported me continuously, both before and after myarrival in Hannover. Prof. Dr. Dan Iordache and Prof. Dr. Octavian Stanasilatutored my first research projects and recommended me for the great scholarshipat Ecole Polytechnique in Paris.In the beginning of my Ph.D. I had the luck of collaborating with Prof. Dr.Manolis Koubarakis, a great researcher, whose style had a visible influence uponmy writing. Prof. Dr. Edleno de Moura, Prof. Dr. Pavel Calado, and AndreCarvalho, my hospitable hosts at the Federal University of Amazonas in Brazil,provided me with an extremely helpful experience with reallife search engines.My good friend Carlos Castillo paved my way within the Information Retrievalcommunity, introducing me to the best researchers and believing in my skills.I had the pleasure of working within a very high quality group, many of mycolleagues and friends assisting me with indispensable research comments andxiEmerging Applications of Link Analysis for Rankingsuggestions, as well as with a nice and friedly atmosphere. In particular, I wouldlike to thank to all those younger Ph.D. and master students who reckoned myknowledge and chose to work with me. It would have been much harder withoutthem.Last, but definitely not least, I am forever grateful to my family. To my wife, forstanding by me along this Ph.D., for enduring the distance when one of us had tobe away and for supporting me in my initiatives. To my mother, for her excellentguidance through my development and education, but also for her extreme care.To my grandfather, who would have liked to see this, for shaping my intellectualskills since early childhood. To my father, for being there whenever I needed him.To my aunt Doina and uncle Bebe, my second pair of parents, for always caringfor me. To Angi, Sica, and grandpa, for their great love.xiiContents1 Introduction 12 General Background 72.1 Ranking in the Web . . . . . . . . . . . . . . . . . . . . . . . . . 72.1.1 Brief Introduction to Search Engines . . . . . . . . . . . . 72.1.2 Link Analysis Ranking . . . . . . . . . . . . . . . . . . . . 82.1.3 Other Features used for Web Ranking . . . . . . . . . . . . 162.2 Using Ranking in IR Applications . . . . . . . . . . . . . . . . . . 173 Ranking for Enhancing Desktop Search 193.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 193.2 Specific Background . . . . . . . . . . . . . . . . . . . . . . . . . 213.2.1 Ranking Algorithms for the PC Desktop . . . . . . . . . . 213.2.2 General Systems for PIM . . . . . . . . . . . . . . . . . . . 223.2.3 Specific Applications aimed at Desktop Search Only . . . . 243.3 Ranking by Tracking Specific User Actions . . . . . . . . . . . . . 243.3.1 Activity Contexts at the Desktop Level . . . . . . . . . . . 253.3.2 A Context Oriented Architecture for Desktop Search . . . 303.3.3 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . 333.4 Ranking by Tracking All User Actions . . . . . . . . . . . . . . . 373.4.1 Generic Usage Analysis Based Ranking . . . . . . . . . . . 37xiiiEmerging Applications of Link Analysis for Ranking3.4.2 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . 403.5 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 514 Ranking for Spam Detection 534.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 534.2 Specific Background . . . . . . . . . . . . . . . . . . . . . . . . . 554.2.1 Email AntiSpam Approaches . . . . . . . . . . . . . . . . 564.2.2 Trust and Reputation in Social Networks . . . . . . . . . . 584.2.3 Spam Detection in the World Wide Web . . . . . . . . . . 584.3 Ranking for Email Spam Detection . . . . . . . . . . . . . . . . . 624.3.1 The MailRank Algorithm . . . . . . . . . . . . . . . . . . 644.3.2 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . 704.3.3 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . 764.4 Ranking for Web Spam Detection . . . . . . . . . . . . . . . . . . 774.4.1 Site Level Spam Detection . . . . . . . . . . . . . . . . . . 794.4.2 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . 854.4.3 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . 945 Ranking for Web Search Personalization 975.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 975.2 Specific Background . . . . . . . . . . . . . . . . . . . . . . . . . 985.2.1 Personalized Search . . . . . . . . . . . . . . . . . . . . . . 995.2.2 Automatic Query Expansion . . . . . . . . . . . . . . . . . 1035.3 Taxonomy Based Personalized Web Search . . . . . . . . . . . . . 1045.3.1 Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . 1055.3.2 Estimating Topic Similarity . . . . . . . . . . . . . . . . . 1065.3.3 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . 1095.4 Taxonomy Based Automatic User Profiling . . . . . . . . . . . . . 1135.4.1 Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . 1145.4.2 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . 1165.5 Desktop Based Personalized Web Search . . . . . . . . . . . . . . 118xivPaul  Alexandru Chirita5.5.1 Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . 1205.5.2 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . 1265.6 Introducing Adaptivity . . . . . . . . . . . . . . . . . . . . . . . . 1325.6.1 Adaptivity Factors . . . . . . . . . . . . . . . . . . . . . . 1325.6.2 Desktop Based Adaptive Personalized Search . . . . . . . . 1345.6.3 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . 1365.7 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1376 Conclusions and Open Directions 141Bibliography 146xvEmerging Applications of Link Analysis for RankingxviChapter 1IntroductionThe amount of information available in everyday life has grown exponentiallyduring the previous century. From a very difficult access to knowledge in theearly 1900, mankind reached a high state of development within just one hundredyears, being now able to locate tons of data from all over the planet.It all started around 1830, when Carl Friedrich Gauss electrified for the firsttime binary information in his telegraphy experiments. Though this momentis considered the birth of digital media note that manual binary counting hasbeen in practice at least since 2000 BC from the Babylonian Empire, it tookuntil the Second World War to exploit this extremely valuable discovery at itsentire capabilities, when computers began to be utilized on a somewhat largerscale of applications. In the same time, the amount of available storage solutionshas grown as well. In 1928 Fritz Pfleumer invented the magnetic tape, a visibleprogress towards storing digital media, which was however fastly replaced around1973 by floppy disks invented in 1969 by David Noble from IBM, which at theirturn were replaced by the much larger CDs invented in 1985 by Kees Imminkand Toshitada Doi from Philips and Sony and DVDs introduced in 19961998by a large consortium of top media companies. Thus, more and more data hasbecome digitalized. Almost all reports, presentations, media such as images ormovies, books, etc. are now also produced in a digital format, making informationdistribution a much easier task. The already existing nondigital media such asprinted books or articles is now slowly being digitalized as well, in order to providea much faster access to it. Finally, along with the invention of the World WideWeb 19901991, it became trivial to even send such data almost anywhere onEarth within seconds. In fact, nowadays, for some people, if a document does not1Chapter 1. Introduction.exist in the Web, then it does not exist at all.Clearly this strong scientific progress has brought huge benefits, which were probably fiction just one hundred years ago. Yet which challenges does it bring tous There are plenty, but two of them have proved themselves to be really important First, as so much information is now being available in digital format,when creating a data collection on some specific topic one needs to collect a largeenough set of documents so as to cover most if not all the interests or subtopicsavailable for that subject. In some local environments such as enterprises, thistask may not seem difficult. However, as soon as we think of larger corpora, suchas topicoriented news, or especially the entire World Wide Web, then gatheringall documents therein becomes suddenly impossible. Second, once this usuallyoverwhelming amount of data has been accumulated, one would need to locatethose documents which are helpful in solving some given task. This is how theneed for search appeared. Searching by itself deals with identifying the relevantdocuments within the corpora, given a specific user query. Yet this is still notsufficient. In large collections such as the Web, there will be millions of suchdocuments. Therefore, an additional technique is necessary Ranking. Ranking isthe process of positioning items e.g., documents, individuals, groups, businesses,etc. on an ordinal scale in relation to others. This way one does not need tobrowse through the entire relevant output, but rather only look at the alreadyidentified best items.The software that gathers and searches digital data is known under the broad nameof Search Engine and the science seeking to design better algorithms for search engines is called Information Retrieval. IR is a broad interdisciplinary field, drawingon many other disciplines. It stands at the junction of many established researchareas and draws upon cognitive psychology, information architecture, informationdesign, human information behavior, linguistics, semiotics, information science,computer science, librarianship and statistics. The invention of the World WideWeb and the subsequent development of Web Search Engines has made IR anextremely popular research field, especially since a lot of new and highly interesting problems appeared together with this vast amount of data How to crawl alarge amount of Web pages in an efficient way How to rank search results Howto personalize the search experience How to suggest better queries to assist theuser in search How to cluster search results And so on.This thesis is about Ranking in Large Scale Information Systems. Researchfor efficient ranking algorithms is necessary for quite a lot of such application environments. Many examples can be given The World Wide Web, EnterpriseNetworks, Digital Libraries, Social Networks, PeerToPeer Networks, PersonalInformation Repositories, Email Inboxes, etc. For all these, current ranking algo2Paul  Alexandru Chiritarithms are still rather poor or even inexistent, although in the same time they aremore and more necessary, due to the extreme increase in the amount data storedand searched for each particular scenario.All the algorithms we propose focus on context oriented ranking. For the caseof excessively large media, such as the World Wide Web, Enterprise or PeerToPeer networks, this comes in the form of personalization. In these environmentsthere is so much data that no matter which query is issued to the search engine, nomatter which generic information organization algorithm is developed, thousandsif not millions of matching items will be found1. As ranking alone is not sufficientto solve this problem, and as nobody has time to look into this many relevantresults, an additional dimension is introduced, namely the specific preferences ofeach subject utilizing the system. For the case of emerging large media, such asPC Desktops or even Email Inboxes, though different, the situation is becomingmore and more similar. First, Desktop search would commonly return severalhundreds of results nowadays, which is again too much to browse through. In thesame time many people receive dozens of emails per day, many of which are spam.Thus, even in this relatively small environment, some sort of item ranking measurewould be useful in order to prioritize which incoming emails to read sooner, later,or not to read at all. Second, as the content addressed by these media is highlypersonal and heterogeneous across users, personalization is implicitly included ineach application, i.e., each subject receives a different ranking, relative to her owndata.Besides being no longer manageable with current search mechanisms, the abovementioned information systems also exhibit another interesting commonalityThey are all Social Systems. This might be obvious for macrosystems involving multiple users, as for example the Web, or social networks developed on topof email exchanges within a community. Yet the very same characteristics can beidentified when performing this analysis at the microsystem level of single usersThe glue around all these two perspectives is the Power Law 196, also known asthe rich get richer law, which says that very few resources i.e., persons, Webpages, etc. are highly important across a collection, while all others are almostnot important at all. For example, at the macroscopic level, the distribution ofindegree of Web pages follows a power law 34, just as the distribution of humansocial acquaintances 225. As argued above, the same occurs at the microscopiclevel of single users. The distribution of time spent reading personal files on theDesktop follows again a power law 66, and so does the frequency of English wordsone would use in her personal documents 222.In this thesis we provide a twofold contribution to the Information Retrieval field.1Exceptions can be found, of course.3Chapter 1. Introduction.First, we identify those application areas in which a context oriented rankingis missing, though extremely necessary in order to facilitate a qualitative accessto relevant resources. Second, for each of these areas we propose appropriateranking algorithms which exploit their underlying social characteristics, eitherat the macroscopic, or the microscopic level. We achieve this by utilizing linkanalysis techniques, which build on top of the graph based representation of linksbetween resources in order to rank them, or simply to identify social patternsrelative to the investigated data set.The thesis is organized around the applications of link analysis for ranking whichwe tackled, as follows Chapter 2 introduces us into the realm of ranking.We concentrate mostly on link analysis ranking algorithms, and especially ona description of PageRank 172 and HITS 143. Once these have been presented,we also briefly describe the various link analysis ranking algorithms that followedthem, as well as some of the other evidences used by search engines when rankingWeb pages. In the end we give an overview of other information systems in needof ranking facilities, and motivate why they were left out from our investigation.The three subsequent chapters describe each our specific contributions withinthe three emerging application areas we investigated. They all start with anintroduction to the area, followed by a comparison with the related work specific tothat particular domain. Subsequently, each proposed algorithm is first presentedand explained, and then empirically evaluated. We conclude each chapter with adiscussion on the pluses and minuses of each proposed algorithm, as well as onthe possible further steps in the area.In Chapter 3, we start from the motivation that current Desktop ranking algorithms are using only pure textual information retrieval techniques, which aremore than 25 years old and which can no longer cope with the current information flow. We thus argue that Desktop search output should be ranked using aproper, specialized algorithm, based on specific indicators for this environment.As a solution, we propose two new approaches, both built on top of Desktop usageanalysis First, Section 3.3 investigates a technique in which only some specificDesktop activity contexts are studied and considered to confer ranking value tothe resources associated to them. Then, we generalize this approach in Section3.4 by considering all user accesses to Destktop items as relevant for ranking.Chapter 4 dives into another central application of digital information management, Spam Detection. We tackle two environments which both suffer fromthe spam problem extensively Email and Web. The initial half of the chapterSection 4.3 presents our social network based reputation scheme for email addresses, together with a set of experiments proving its efficiency in combatingthe spam issue. These algorithms make a smooth transition from the local in4Paul  Alexandru Chiritaformation environment i.e., personal files, including emails, as stored on the PCDesktop towards global milieus such as the Internet represented here by thesocial networks constructed via the exchanges of emails. The second half of thechapter Section 4.4 moves us even further towards global data management andintroduces our site level approach to Web hyperlink spam detection.Chapter 5 proposes several new approaches to better create and exploit userprofiles for personalized Web search. We start with a discussion on how large scaletaxonomies could be employed for both these goals, i.e., tailoring the Web searchoutput according to users interests and analyzing previous user actions in orderto define a good profile. Though this technique yields very good results, it stillrequires sharing a small amount of personal information with the search engine.Therefore, in the second part of the chapter we concentrate on generating fullysecure user profiles for the same task of Web search personalization. We extractpreferences from users Personal Information Repository and design an algorithmwhich achieves very good performance without sharing any private informationwith the search provider.Chapter 6 concludes the thesis with an enumeration of the contributions webrought to the Information Retrieval research community, while also discussingpossible future research directions and open challenges associated to these topics.5Chapter 1. Introduction.6Chapter 2General BackgroundRanking has been widely investigated in search engine literature, both for the taskof Information Retrieval per se, as well as for other closely related tasks. Thischapter will therefore be split in two parts First, we will present the inner detailsof search engine ranking, putting the focus on link analysis methods, as they arethe foremost important approach for Web IR, and we argue, for quite several otherenvironments as well. This part is especially important, as it will introduce themethods which we will build upon throughout the thesis. Once these have beenintroduced, in the second part of the chapter we will move towards discussingsome of the already existing applications of ranking for other purposes than Websearch, and we will motivate their exclusion from our study.2.1 Ranking in the Web2.1.1 Brief Introduction to Search EnginesTypical search engines consist of two major modules A crawler and a searcher.Crawlers are assigned with the preprocessing task of gathering the search datainto a local index see Figure 2.1. They perform the following operations URL Listing  Maintain a list of already visited URLs, as well as of the URLswhich are to be visited in the future. URL Retrieving  Fetch from the Web new URLs received as input from theListing Module.7Chapter 2. General Background. URL Processing  Process each visited URL in order to extract 1 its outgoing hyperlinks, which are then transferred to the URL Listing Module forfurther processing i.e., filtering the already visited ones and planning thenew ones for future retrieval, and 2 its indexable content, usually textualdata, which is sent towards the Format  Store Module. Data Format and Store Arrange the indexable data into a special format,compress it, and store it into the local index.Figure 2.1 Crawler Architecture. Figure 2.2 Searcher Architecture.Once a reasonably large amount of Web pages has been collected, the user canstart searching it. This is performed through a simple interface in which querykeywords are entered see Figure 2.2. Afterwards, the Searching and RankingModule first inspects the index to gather the hits, i.e., the pages containing theuser selected keywords, and then orders these hits according to different rankingcriteria and displays them to the user as such.The remainder of this section will mostly present a detailed look into link analysisranking, and especially into PageRank and HITS, as they represent the foundationalgorithms of this technique on the World Wide Web. In the end, we will brieflyintroduce some other measures used by search engines to order their output.2.1.2 Link Analysis RankingLink analysis has been first utilized in the 1950s in a slightly different scenarioCitation Analysis see for example the work of Katz 137 or Garfield 103, 104.Just like in the Web, the need for citation analysis came because of the fastlygrowing amount of information, in this case coming from scientific conferencesand journals. Thus, a publication venue evaluation technique was necessary inorder to ease the identification of qualitative articles 105. A set of interestingfeatures were discovered, as for example the fact that few journals receive most8Paul  Alexandru Chiritacitations, while many journals receive few or even no citations at all 106. Whenplotted, this turned into something which was yet to become a quite famousdistribution A powerlaw.Power law models have been analyzed and rigorously defined within the sameperiod, the first representative effort in this direction being that of Herbert Simon196. When defined over positive integers the most common case, powerlawdistributions are characterized by having the probability of value i proportionalto 1ik, with k being a small positive real number. Nowadays, not only in theWeb, but also in much wider contexts like social interactions or usage of naturallanguage, the power law seems to dominate most of the discovered phenomena.Examples are plenty, from the distribution of in and outdegree of Web pages34, 149, 6, 19, to that of the frequency of words in English 160, 222, of socialacquaintances 225, or even of oligonucleotide sequences within DNA 166. Mostimportant for us is the fact that the identification of power laws was very beneficialfor Web searching and ranking 33, 43, 47, 150, 200. We will thus now proceed toreviewing the most popular link analysis ranking algorithms, many of which yieldhighly qualitative results especially due to the powerlaw nature of their input.PageRankPreliminaries. Link analysis algorithms are founded on the representation of theWeb as a graph. Hereafter we will refer to this graph as G  V,E, where V isthe set of all Web pages and E is the set of directed edges  p, q . E contains anedge  p, q  iff a page p links to page q. Ip represents the set of pages pointingto p inneighbors and Op the set of pages pointed to by p outneighbors. Wedenote the pth component of v as vp. Also, we will typeset vectors in boldand scalars e.g., vp in normal font. Finally, let A be the normalized adjacencymatrix corresponding to G with, Aij 1Oj if page j links to page i and Aij  0otherwise.Description. PageRank 172, 33 computes Web page scores by exploiting thegraph inferred from the link structure of the Web. Its underlying motivation isthat pages with many backlinks are more important than pages with only a fewbacklinks. As this simple definition would allow a malicious user to easily increasethe importance of her page simply by creating lots of pages pointing to it, thealgorithm uses the following recursive description A page has high rank if thesum of the ranks of its backlinks is high. Stated another way, the vector PR ofpage ranks is the eigenvector corresponding to the dominant eigenvalue of A.Given a Web page p, the PageRank formula is9Chapter 2. General Background.PRp  c qIpPRqOq 1 c  Ep  c qIpPRqOq1 cV 2.1Theoretically, A describes the transition probabilities associated to a Markovchain. It is known that a finite homogeneous Markov chain which is irreducible1and aperiodic2 has a unique stationary probability distribution , depicting theend probabilities for the Markov chain to reach each specific state. The chainconverges to this distribution no matter which is the initial probability distribution0. However, some Web content such as PDF articles have no outlinks, andthus their corresponding column in A would consist only of 0 entries, makingthe Markov chain associated to the Web graph a reducible one. These pagesare actually absorbing states of the Markov chain, eventually sucking all thePageRank into them. It is also trivial to show that the above mentioned Markovchain is a periodic one. These are both reasons for introducing a dumping factorc  1 usually set to 0.85 32, which in fact also nicely models an intuitivedescription of PageRank A random Web surfer will follow an outgoing link fromthe current page with probability c and will get bored and select a different pagewith probability 1c. Note that this model does not include the BACK button155, but even so, it was proved in practice to yield very good results. Finally,one could use other nonuniform distributions for E if biasing or personalizationis desired onto a given set of target pages see also Equation 2.1, whose right handside expression is derived assuming an uniform bias on all input pages.Convergence and Stability Properties. The properties of PR are easilyexplained by the eigenvalues of A. Let us start from the wellknown PerronFrobenius theorem, as well as a propositionTheorem 1 For any strictly positive matrix A  0 there exist 0  0 and x0  0such that1. A  x0  0  x02. if  6 0 is any other eigenvalue of A, then   03. 0 has geometric and algebraic multiplicity 1.Proposition 1 Let A  0 be a strictly positive matrix with row and column sumsri j aij, and cj i aij. Then, the PerronFrobenius eigenvalue 0 is1A Markov chain is irreducible if any state can be reached from any other state with positiveprobability.2A Markov chain is aperiodic if for any state i the greatest common divisor of its possiblerecurrence times is 1.10Paul  Alexandru Chiritalimited byminiri  0  maxiri, and minjcj  0  maxjcj. 2.2It is straightforward to see that for any stochastic matrix, all ri are 1, and thus0  1. However, an even more interesting eigenvalue is the second highest one.It is known that the asymptotic rate of convergence of the power method i.e.,Equation 2.1 is governed by the degree of separation between the dominant andthe closest subdominant eigenvalues 155. For our case, it has been shown thatthe closest subdominant eigenvalue is exactly c 119, 153. Consequently, if weused c  0.85 and aimed for an accuracy of 104, we would need to run thecomputation process for 43 iterations, since 432  0.8543  104.Another interesting property of PageRank is its score stability 157 on the class ofall directed graphs. Intuitively, if a small modification e.g., one link is deleted oradded to some page p occurs in the hyperlink structure of its underlying graph,then the difference between the new score PRp and the old one PRp has anupper bound close to 0. PageRank is also monotonic, i.e., adding a link towards apage can only increase its score, and therefore its rank 54. However, Lempel andMoran 159 showed that all these properties do not imply rank stability  Eventhough a small change in the input Web graph results in a small score difference,it might be the case that the corresponding page is strongly promoted or demotedacross the overall rankings.Finally, we note that the distribution of the resulting PageRank values follows apowerlaw only for some particular values of the damping factor 21, and thus acareful selection of c is usually very important for the success of the applicationexploiting the ranking algorithm.Dangling Nodes. As we have seen, pages with no outlinks also known asdangling nodes are quite problematic for PageRank. Even with the dumpingfactor in place, they may still have a negative influence upon the rankings. Thus,several solutions have been proposed. In the original paper 172, the authorssuggested to remove them and calculate the PageRank only on the Web graphwithout dangling pages. Similarly, Kamvar et al. 133 proposed that after havingcalculated PageRank without the dangling nodes, to add them back in for severaladditional iterations. This approach was also suggested in 32, 120, where theauthors remarked that this seems preferable to keeping them in the calculation.Note that the process of removing dangling nodes may itself produce new danglingnodes, and should therefore be repeated iteratively until no dangling nodes remain.It is interesting to note that this removal procedure seems to terminate ratherquickly when applied to the Web. Finally, Eiron et al. 92 investigated the11Chapter 2. General Background.possibility to jump to a randomly selected page with probability 1 from everydangling node, approach which seems to be the best choice so far.Implementation Optimizations. There is a large amount of work on optimizing the computation of PageRank, mainly due to the extremely large size of itsinput data, the Web graph. Arasu et al. 12 suggested for example using theGaussSeidel method instead of the power iteration. Kamvar et al. 134 appliedAitkens 2 quadratic extrapolation method obtaining an increase in speed of upto 300. The same group also found small speed improvements by evaluatingconvergence over individual elements of PageRank 193 in order to proceed withthe computation only for the nonconverged nodes. Chien et al. 54 investigatedthe possibilities to update PageRank given some small changes in the Web graph.Chen et al. 51 proposed several IO effective implementation approaches for thepower iteration over large graphs. Finally, Langville and Meyer 154 proposed tomove the dangling nodes towards the bottom of the PageRank matrix and showedthis technique to further improve its computation time. Obviously most of theseworks focus on time optimizations. However, we note that other aspects have beentackled as well, such as improving the space complexity of the algorithm 117, orcalculating its output under missing data 3, or enhancing the ranking procedureto allow for both horizontal i.e., topology based and vertical i.e., topical basedprocessing 82.Another strongly investigated research area is the parallelization of PageRank.Existing approaches to PageRank parallelization can be divided into two classesExact Computations and Approximations. For the former ones, the Web graph isinitially partitioned into blocks grouped randomly e.g., P2P PageRank 189,lexicographically sorted by page e.g., Open System PageRank 195, or balancedaccording to the number of links e.g., PETSc PageRank 111. Then, standarditerative methods such as Jacobi or Krylov subspace 111 are performed over thesepieces in parallel, until convergence. The partitions must periodically exchangeinformation Depending on the strategy this can expose suboptimal convergencespeed because of the Jacobi method and result in heavy interpartition IO. Infact, as the Jacobi method performs rather slow in parallel, we modified the GaussSeidel algorithm to work in a distributed environment and found the best speedimprovements so far see Kohlschutter, Chirita and Nejdl 145.When approximating PageRank, the idea is that it might be sufficient to get arank vector which is comparable, but not equal to PageRank. Instead of rankingpages, higherlevel formations are used, such as the interlinkage between hosts,domains, server network addresses or directories, which is orders of magnitudesfaster. The inner structure of these formations i.e., the page level ranking canthen be computed in an independently parallel manner offline, by combining12Paul  Alexandru Chiritathe local rank of each page with the global rank of the higher level entity it belongsto, as in BlockRank 133, SiteRank 218, 1, the UModel 36, ServerRank 213or HostRank  DirRank 92.Derived Algorithms. There are a lot of publications proposing either alternatives to PageRank, or small modifications to it. Some of the latter ones havealready been mentioned above e.g., 92, as they are intended to achieve bettercomputation performance. We will thus focus here only on those papers exhibitingbigger differences when compared to PageRank, either in terms of methodologyor of the end purpose of the computation. BaezaYates and Davis 16 improvePageRank quality by giving different weights to links as a function of the tag inwhich they were inserted, of the length of the anchor text, and of the relativeposition of the link in the page. Feng et al. 96 calculate a ranking exclusivelyover the Web sites, rather than the more granular pages. They start in a similarmanner to BlockRank 133, computing PageRank within each site separately, butthen this information is utilized to derive a stochastic coupling between Web sites,which is later applied in a regular power iteration at the Web site level. Moreover,other optimized Web ranking algorithms include the work of Upstill et al. 207,who argue that ranking by the indegree of pages is usually enough to approximate their quality, as well as the work of Abiteboul et al. 2, who attempt tocompute page reputations PageRank approximations directly at crawling time.In this latter paper, the iterations of the power method are achieved through therediscovery of new links towards already visited pages, as well as through crawlupdates. Several crawling strategies are identified, but the greedy approach seemsto be closest to the powerlaw model of the Web. The interesting aspect of thiswork is that page scores are updated as the Web is crawled over and over again,thus implicitly coping with its volatility.Another PageRank related research direction it to bias its scores towards the topicassociated to each user query. The most popular work is that of Haveliwala 118,who builds a topicoriented PageRank, starting by computing offline a set of 16PageRank vectors biased3 on each of the 16 main topics of the Open DirectoryProject4 ODP. Then, the similarity between a user query and each of these topicsis computed, and the 16 vectors are combined using appropriate weights. Rafieiand Mendelzon 179 include the topics covered by Web pages into the reputationalgorithm, each topic being represented by a set of terms. Given a topic, thepages covering it i.e., containing its descriptive words are identified and used inthe ranks calculation. The approach is not feasible, due to the practically infiniteamount of existing topics. Nie et al. 170 had the better idea of distributing3Biasing is obtained by setting higher values for the targeted pages within the E vector.4httpdmoz.org13Chapter 2. General Background.the PageRank of a page across the 16 ODP topics it contains. This way, theimportance flows according to the text content of the source and target pages,and all 16 topic oriented rankings are generated at once5.Finally, there exist also some works moving a bit beyond PageRank. Baeza etal. 14 for example investigated various new damping functions for link analysisranking, showing them to result in rather similar quality to PageRank, while beingcomputationally much faster. Tomlin 205 proposed to use the richer networkflow model instead of the now common Markov chain approach to computingPageRank. Last, but not least, we proposed HubRank see Chirita, Olmedillaand Nejdl 72, 73, which biases PageRank onto hubs, thus nicely combiningthe authoritative computation of PageRank with a hub measure for Web pages.Moreover, we showed this approach to yield better qualitative results than regularPageRank over a set of toy experiments.HITSDescription. Kleinbergs HITS 143 was the first efficient link analysis rankingalgorithm for the Web. It builds upon the idea of computing two scores for eachpage in a Web community, namely a hub score and an authority score. Generally, ahub is a page pointing to many other authoritative pages, whereas at the oppositeend, authorities are pages containing valuable information pointed to by manyhubs. The algorithm starts from a setR of pages with high PageRank, as returnedby a search engine. This set is first extended into Rext with all the pages pointingto, as well as pointed by pages from R. Then, given that each of these pages hasbeen assigned an initial authority score a0i and a hub score h0i , HITS refines theirscores using the following iteration procedureaki p,qEhk1p  hki p,qEak1q 2.3For convergence purposes, after each iteration the values within a and h needto be normalized to sum to 1 as otherwise they would continuously increase.Written in matrix form, if L is the adjacency matrix of the graph underlying thepages from Rext, the same equations becomeak  L  LT  ak1 hk  LT  L  hk1 2.45Of course, this approach limits the application to computing topical oriented rankings foronly a small number of topics.14Paul  Alexandru ChiritaJust as with PageRank, it is straightforward to notice that a converges to thedominant eigenvector of L  LT , and h converges to the dominant eigenvector ofLT  L. In fact, Ding et al. 83 have discovered another interesting property ofthese vectors There is a direct relationship between HITS authority matrix LLTand the cocitation matrices used in bibliometrics similarly, the hub matrix LT Lis related to coreference matrices.Since HITS is usually constructed around the TopK usually 200 pages returnedas output to some user query, it is usually computed over less than 20,000 pages,and is thus very fast. Moreover, one needs to compute only one of the above mentioned eigenvectors For example, given the authority vector i.e., the eigenvectorof L  LT , then the corresponding hub vector is given by h  L  a.A series of limitations made HITS less successful than PageRank. First of all,its results are topic drifted, i.e., they are focused around the main topic of theinput graph. This problem was solved by Bharat and Henzinger 27 througha weighting of the authority and hub scores according to the relevance of eachpage to the initial user query. Gibson et al. 110 exploited this problem in orderto infer the topics residing within different subsets of the Web graph. A secondproblem is HITS susceptibility to spamming. It is fairly easy to construct a verygood hub, and subsequently to push the score of a target authority page. Also,HITS is neither rank stable, nor score stable 157, 159.Derived Algorithms. HITS was also studied extensively and many improvedvariants of it have been proposed. However, since our work is only partiallyrelated to HITS, we will review here just some of its relevant followup algorithms.Randomized HITS 169 is a twolevel reputation ranking approach, combining therandom surfer model from PageRank with the concepts of hubs and authoritiesfrom HITS in order to achieve a rank stable algorithm for calculating hub andauthority scores over a given graph. Similarly, SALSA 158 adopts two Markovchains for traversing the Web graph, one converging to the weighted indegreeof each page, for authority scores, and the other converging to its weighted outdegree, for hub scores. The algorithm is thus no longer dependent on TightlyKnit Communities of pages as HITS is, but is still vulnerable to many forms ofspam. Other variants have been proposed by Tsaparas 206 and Borodin et al.29, though their work is more important due to the theoretical analysis therein,rather than the qualitative improvements of the algorithms proposed. Finally, wealso note that a lot of research on the HITS algorithm has been performed withinthe IBM CLEVER project 171.15Chapter 2. General Background.Other Link Analysis Ranking Algorithms and BeyondThough there are many other approaches to ranking pages in the World WideWeb, we would like to briefly discuss here only the most important one of themMachine Learning. As Web ranking becomes more and more complicated, withincreasingly more input features being necessary, Machine Learning seems to gainmomentum.The most important approach in this category is RANKNET 39, 182, apparentlythe ranking mechanism used by the Microsoft Live search engine. Its underlyingidea is to use machine learning to combine a large amount of features of Web pages,starting from the already common link based ones, and up to visiting statisticsfor different pages, as well as textual evidences. The authors show this techniqueto yield better ranking results than PageRank.Another technique which has been investigated for the Web ranking purposeis Latent Semantic Analysis 80. Cohn and Chang 75 use it to propose aprobabilistic model to estimate the authority of documents in the Web. Unlike theeigenvector based solutions, they apply Hoffmans Probabilistic LSI model 123over a Document x Citation matrix and obtain better document quality estimates.However, their performance is dependent on the chosen starting set and may getstuck in local optima with poor overall results. Also, it is not clear how fast thisalgorithm would compute on Web size input data.2.1.3 Other Features used for Web RankingThis thesis builds upon the link analysis methods presented in the previous section. Nevertheless, we have seen that such information is not sufficient. First andforemost, without text analysis, it would be nearly impossible to accurately identify the documents best matching a user query. The most employed technique forthis matter is the Vector Space Model 188, according to which both queries andWeb pages are represented as bags of words, weighted by their Term Frequencymultiplied by Inverse Document Frequency. Many extensions are possible in theWeb environment, very important being the differentiation of terms based on themarkup used around them i.e., title, bold, etc., and the inclusion of anchor textin the actual Web page.A second source of ranking data is represented by the query logs. In fact, in thepre link analysis era, Yahoo used to rank its output utilizing the number of clicksobtained by each URL when displayed as response to some input query. While itsimportance has now decreased, the click rate is still a highly relevant factor in Websearch engines. Moreover, it can give a lot more information besides the actual16Paul  Alexandru Chiritaimportance of documents. One could for example locate terms that frequentlycooccur with some query, and thus automatically bias and improve the qualityof the results list. Or, for ambiguous search requests, one could determine thepopularity of each query interpretation and tailor the search output accordinglye.g., the Java programming language is by far more popular than the coffee,or the island in the Pacific.Third, we have the session specific features. These are usually based on miningthe IP addresses of users. The most important one is the geographic location, as itis clear that different cultures imply different perspectives on a qualitative searchresult for most queries. Then, there is the time of day. Major global subjectinterests change over the day, ranging for example from news in the morning,to shopping in the evening. The same applies to the analysis of daily interestsover the year. Finally, as people feel uncomfortable with sharing their searchhistory, several simple personalization techniques have been developed, such asanonymously identifying each person using a cookie stored on her machine.A lot of other sources of quality rating exist. It is believed that the majorcompanies utilize over 500 such features. Some examples not covered by theabove mentioned categories include the fact of being listed in hand crafted Webtaxonomies such as the Open Directory, the age of each page, the amount andfrequency of changes operated on it, etc.2.2 Using Ranking in IR ApplicationsIn this thesis we tackle the major emerging applications of link analysis for ranking. Nevertheless, there exist a few other utilizations of PageRank and alike. Thissection briefly surveys these additional interest areas, together with a discussionof their future success potential.Social Network Ranking. As PageRank is strongly exploiting the social natureof humans, some authors proposed to develop social reputation metrics based onvotes for and sometimes also against other individuals 124. We have also pursued this goal for ranking people within PeerToPeer environments see Chiritaet al. 69, or Kamvar et al. 135. However, the conclusions drawn from theseanalyses indicate that PageRank provides only a minor improvement for regularPeerToPeer tasks, such as known item search or generic keyword based search57, 65, 68, 69.Web Characterization. There has been quite a lot of research on Web characterization in the past, yet only few studies included a PageRank analysis as well.Panduragan et al. 174 were among the first to show that the importance of Web17Chapter 2. General Background.pages i.e., their PageRank follows a powerlaw distribution. At the other end,Arasu 12 investigated several PageRank computational optimizations which exploit the Web structure discovered in previous characterizational studies. All inall, this fields importance has now decreased, as the wide span of Web analytical methods discovered so far seems to be sufficient for the current algorithmicnecessities.Ranking Concepts over the Semantic Web. Swoogle crawls the socalledSemantic Web seeking for any existing ontological instances. The located items arethen ranked using a variant of PageRank 84 built on top of the links between theidentified objects. The small size of the search engine and the current insuccess ofthe Semantic Web on a global scale make questionable the success of this otherwiseinteresting application.Text Summarization and Classification. Based on the assumption that themacro social dynamics caught by PageRank could be in fact also present at themicro level of singular subjects, it was believed that the similarity between the sentences and  or documents authored by the same person also follows a powerlaw.More specifically, if we build links between our previously authored sentences documents and weight them according to the level of textual similarity in terms ofoverlapping words between the connected nodes, then we obtain a graph shapedby a powerlaw degree distribution. Consequently, the most representative sentences can be used to summarize their underlying documents 93, 94, 81, or togroup these documents into categories 100, 13. This technique performs fairlywell, yet still below the more powerful Natural Language Processing algorithms.We also applied such a micro social analysis onto another, more promising application area Personal Information Management. More details are given in thenext section.18Chapter 3Ranking for Enhancing DesktopSearch3.1 IntroductionThe capacity of our harddisk drives has increased tremendously over the pastdecade, and so has the number of files we usually store on our computer. Usingthis space, it is quite common to have over 100,000 indexable items within ourPersonal Information Repository PIR. It is no wonder that sometimes we cannotfind a document anymore, even when we know we saved it somewhere. Ironically,in some of these cases nowadays, the document we are looking for can be foundfaster on the World Wide Web than on our personal computer. In view of thesetrends, resource searching and organization in personal repositories has receivedmore and more attention during the past years. Thus, several projects have startedto explore search and Personal Information Management PIM on the Desktop,including Stuff Ive Seen 86, Haystack 178, or our Beagle 63.Web search has become more efficient than PC search due to the boom of Websearch engines and to powerful ranking algorithms like the PageRank algorithmintroduced by Google1. The recent arrival of Desktop search applications, whichindex all data on a PC, promises to increase search efficiency on the Desktopnote that we use the terms Desktop and PIR interchangeably, referring to thepersonal collection of indexable files, emails, Web cache documents, messengerhistory, etc.. However, even with these tools, searching through our relatively1httpwww.google.com19Chapter 3. Ranking for Enhancing Desktop Search.small set of personal documents is currently inferior to searching the rathervast set of documents on the Web. This happens because these Desktop searchapplications cannot rely on PageRank like ranking mechanisms, and they also fallshort of utilizing Desktop specific characteristics, especially context information.Indeed, Desktop search engines are now comparable to first generation Web searchengines, which provided fulltext indexing, but only relied on textual informationretrieval algorithms to rank their results.Most of the prior work in Personal Information Management has focused on developing complex, yet user friendly, systems for organizing and refinding informationusing visualization paradigms, rather than ranking ones. This was in pursue ofthe hypothesis that all Desktop documents are equally important, and thus noranking is necessary. In a more recent formulation, this was denoted search refinding. In this thesis we advocate the contrary We argue that some personaldocuments are actually much more important than others, and that users wouldsearch for these documents much more often than for any other ones.We therefore have to enhance simple indexing and searching of data on our Desktop with more sophisticated ranking techniques. Otherwise, the user has no otherchoice, but to look at the entire result sets for her queries  usually a tedious task.The main problem with ranking on the Desktop comes from the lack of links between documents, the foundation of current ranking algorithms in addition toTFxIDF metrics. A semantically enhanced Desktop offers the missing ingredients By gathering semantic information from user activities, from the contextsthe user works in2, we build the necessary links between documents.Within this chapter we propose to enhance and contextualize Desktop search byanalyzing users local resource organization structures, as well as her Desktopactivity patterns. We investigate and evaluate in detail the possibilities to translate this information into a Desktop linkage structure, and we propose severalalgorithms that exploit these newly created links in order to efficiently rank Desktop items. Where applicable, we also utilize the same information in order togenerate resource specific metadata, which is then employed to enhance Desktopsearch recall. We empirically show that all our algorithms lead to ranking resultssignificantly better than TFxIDF when used in combination with it, thus making metadata and especially access based links a very valuable source of input toDesktop search ranking algorithms.The chapter is organized as follows We start in the next section with a reviewof the previous specific attempts to enhance Desktop search. In Section 3.3 we2Studies have shown that people tend to associate things to certain contexts 129, and thisinformation should be utilized during search. So far, however, neither has this information beencollected, nor have there been attempts to use it.20Paul  Alexandru Chiritadescribe and empirically evaluate our first ranking algorithm, which ranks personalitems by analyzing exclusively several predefined user actions. We generalize thisapproach by considering all resource accesses within the algorithm from Section3.4. In the end, we conclude the chapter with a discussion on the pluses andminuses of each technique, as well as on the possible next steps.3.2 Specific BackgroundThough ranking plays an important role on the Web, there is almost no approach specifically aiming at ranking Desktop search results. Even though thereexist quite a few systems organizing personal information sources and improving information access in these environments, few of the papers describing themconcentrate on search algorithms. In this section we will first describe severalsuch systems and discuss their approaches to Desktop search. Then, we will concentrate our attention towards some of the other existing personal informationmanagement systems, whose purpose was to provide means for organizing thelocal information, rather than searching it. Finally, we will briefly review thecurrent industrial approaches for Information Retrieval at the PC Desktop level.3.2.1 Ranking Algorithms for the PC DesktopVery few works fall into this category. A very recent one, Connections 198, isprobably the only system specifically targeted at enhancing Desktop search quality. Similar to us and to Haystack 4, they also attempt to connect related Desktopitems, yet they exploit these links using rather complex measures combining BFSand link analysis techniques, which results in rather large search response delays,without a clear increase in output quality.The ranking paradigm addressed to generic personal data collections has alsobeen researched in the context of the Semantic Web. AlemanMeza et al. 7for example analyzed the importance of semantically capturing users interests inorder to develop a ranking technique for the large number of possible semanticassociations between the entities of interest for a specific query. They defined anontology for describing the user interest and used this information to computeweights for the links among the semantic entities. The approach is orthogonalto ours, as we build links only by exploiting fast usage analysis information,instead of using various complex algorithms to connect at runtime the Desktopentities relevant for every specific user query. Another interesting technique forranking the results for a query on a semantic data set takes into consideration the21Chapter 3. Ranking for Enhancing Desktop Search.inferencing processes that led to each result 201. In this approach, the relevanceof the returned results for a query is computed based upon the specificity of therelations links used when extracting information from the knowledge base. Thecalculation of the relevance is however a problemsensitive decision, and thereforetask oriented strategies must be developed for this computation.Finally, as current Information Retrieval literature falls short of providing valuableranking mechanisms for the PC Desktop, most tools for this environment recurto traditional textual retrieval models, such as the Vector Space Model 17. Theonly ordering criterion specific for personal information is to sort items by theirrecency, i.e., by the time difference between the current moment and their lastaccess stamp. Clearly, this is a nave technique, which gives valuable results onlyin particular cases.3.2.2 General Systems for PIMSeveral systems have been constructed in order to facilitate refinding of variousstored resources on the Desktop. Stuff Ive Seen 86 for example provides aunified index of the data that a person has seen on her computer, regardless of itstype. Contextual cues such as time, author, thumbnails and previews can be usedto search for and present information, but no Desktop specific ranking scheme isinvestigated. Similarly, MyLifeBits 109 targets storing locally all digital media ofeach person, including documents, images, sounds and videos. They organize thesedata into collections and, like us, connect related resources with links. However,they do not investigate building Desktop ranking algorithms that exploit theselinks, but rather use them to provide contextual information.The Fenfire project 95 proposes a solution to interlink any kind of information onones Desktop. That might be the birthday with the persons name and the articlesshe wrote, or any other kind of information. The idea is to make the translationfrom the current file structure to a structure that allows people to organize theirdata closer to the reality and to their needs, in which making comments andannotations would be possible for any file. Nevertheless, the purpose of thisprocess has again no relation to personal information retrieval, i.e., searching andranking on the Desktop.Haystack 178 pursues similar goals as Fenfire. One important focus is on workingwith the information itself, not with the programs it is usually associated with.For example only one application should be enough to see both a document, andthe email address of the person who wrote it. Therefore, a user could build herown links to Semantic Web objects practically any data, which could then beviewed as thumbnails, Web pages, taxonomies, etc. The underlying idea of the22Paul  Alexandru Chiritaproject was to emphasize the relationship between a particular individual andher corpus 4. On the one hand, this is quite similar to our approach in thesense that it automatically creates connections between documents with similarcontent and it exploits activity analysis to extend the Desktop search results set.On the other hand, just like the previous articles, it does not investigate thepossibilities to rank these results, once they have been obtained. Its followups127, 136 further explore the efficient organization of Desktop resources. Theyuse an RDF database to store metadata about the various personal items, as wellas about any connections between different Desktop data. Finally, Magnet 197was designed as an additional component of Haystack with the goal to supportnave user navigation through structured information via a domainindependentsearch framework and user interface.A smaller yet similar system, Semex 85, automatically generates associationsbetween items on the Desktop in order to provide a meaningful context based localbrowsing. Interestingly, they also support onthefly integration of associationsstemming from both personal and public data. Again, no ranking is discussed intheir prototype whitepapers.Lifestreams 102, 98 is an older Desktop organization system based on a timeordered stream of documents meant to replace conventional files and directories.All its aspects, including query results, consist of substreams of the main Desktopusage stream, thus being rather different from the systems nowadays.Hull and Hart 126 modified conventional PC peripherics e.g., printers to automatically store every processed document, thus providing search through anypreviously accessed document. They also use only traditional ranking techniques,such as ordering by date or TFxIDF. Though it does not describe the architectureof a system, the work of Ringel et al. 183 is also quite relevant for Desktop searchapplications They suggested using timeline visualizations augmented with publicand personal landmark events in order to display query results over an index ofpersonal content.Gnowsis 191, 192 and IRIS 53 create a personal map across various typesof personal information objects. They allow users to annotate the files they accessed, as well as to manually establish links between them. This way, a semanticmetadata repository is populated and provided as a basis to other, semanticallyenhanced Desktop applications, including search.Finally, this work was developed in the context of the Beagle system 61, 63, 62.There, we first produce a collection of metadata associated to each personalresource, as well as a linkage structure over the PIR, using similar methods asGnowsis and IRIS. However, we also apply the results of this research for a specifictask, namely developing Desktop searching and ranking algorithms. More details23Chapter 3. Ranking for Enhancing Desktop Search.about how links are collected and exploited in Beagle can be found in Sections3.3 and 3.4.3.2.3 Specific Applications aimed at Desktop Search OnlyDesktop search applications are not new to the industry. Only the high interestin this area is new. For example, applications such as Enfish Personal3 havebeen available since 1998, usually under a commercial license. As the amountof searchable Desktop data has reached very high values and will most probablyalso amplify in the future, the major search engines have recently given morefocus to this area than the academia. Thus, several Desktop search distributionshave been released for free e.g., Google Desktop Search4, MSN Desktop Search5,etc.. Moreover, some providers have even integrated their Desktop search toolinto the operating system, such as Apple6. The open source community has alsomanifested its interest in the area, the most prominent approaches being GnomeBeagle7 now also integrated into SuSE and KDE KAT8, developed within theMandriva community. Many other commercial Desktop search applications existe.g., Copernic, Yahoo Desktop Search, X1, Scopeware Vision, PC Data Finder,etc., but as our main focus is to devise a Desktop ranking algorithm, rather thanan entire search application, we will not dive into the particularities of each ofthese tools.Most of the above mentioned applications target a very exhaustive list of indexedfile types, including any metadata associated to them. They also update theirindex on the fly, thus inherently tracking any kind of user activity. However,all of them seem to only employ dates, or TFxIDF as measures to rank searchresults, without exploiting the usage information they have available. Therefore,they inherently miss the contextual information often resulting or inferable fromexplicit user actions or additional background knowledge.3.3 Ranking by Tracking Specific User ActionsThe vast majority of search activities at the PC Desktop level are navigational,in the sense that the user is trying to locate one or more items that were almost3httpwww.enfish.com4httpDesktop.google.com5httptoolbar.msn.com6httpwww.apple.commacosxfeaturesspotlight7httpwww.gnome.orgprojectsbeagle8httpkat.mandriva.com24Paul  Alexandru Chiritasurely stored somewhere in the personal information repository in the past. Weargue however that the current abundance of Desktop data makes such a locationimpossible without any efficient ranking mechanism. On my Desktop for example,the query Google ranking algorithm i.e., a three word query, strongly abovethe average query length of 1.7 terms for Desktop search 86 would return about500 results More, this is definitely a clear query, with a very specific search goalin mind. This yields two motivations for bringing ranking into Desktop search1 One would surely not be willing to browse through all the 500 results untilshe finds the right document 2 The fact of having so many outputs for such aclear query indicates that good query formulation is no longer sufficient in findingitems within the personal information repository.This chapter will propose a preliminary solution for Desktop ranking. We willfirst isolate several common user activity contexts and show how to exploit themfor locating previously stored information. Then, we will put these usage areastogether into a semantic ranking architecture meant to enhance the precisionof Desktop search algorithms by personalizing on each users regular file accesspatterns. The empirical results depicted in the last part of the chapter will proveour approach to yield visible improvements when seeking for items in the PIR.3.3.1 Activity Contexts at the Desktop LevelIn this section we overview the Desktop activity contexts we considered. Wedescribe the metadata information that can be extracted from personal resources,as well as the implicit connections residing between them. The former enhancerecall by including the relevant document in the results set even when the userquery is poorly formulated, and the latter induce a PIR ranking which implicitlystrongly enhances Desktop search precision.Exploiting the Email ContextScenario. Alice is interested in distributed page ranking, as her advisor askedher to write a report to summarize the state of the art in this research area.She remembers that during the last month she has discussed with a colleagueabout a distributed PageRank algorithm, and also that the colleague sent her thearticle via email. Though the article does not mention distributed PageRank,but instead talks about distributed trust networks, it is basically equivalent todistributed PageRank as her colleague remarked in this email. Obviously sheshould be able to find the article, if she could exploit this additional information.25Chapter 3. Ranking for Enhancing Desktop Search.Context and Metadata. There are several aspects relevant to our email context.Sender and receiver fields of the email are clearly relevant pieces of information.Basic properties for this context are also the date when an email was sent, orthe date it was accessed, the subject of the email and its body. The status of theemail can be described as seen  unseen or read  unread. We also have a propertyof the type reply to, which gives thread information and is useful to determinesocial network information in general, for example which people discussed whichtopic, etc. The has attachment property describes a 1n relation between the mailand its one or more attachments. The to and from properties connect to ClassMailAddress which connects to Class Person. A Person is usually associatedto more than one MailAddress instances. For attachments we also keep theirconnection to the email they were saved from, because when we search for anattachment we want to use all attributes originally connected to the email it wasattached to see the motivating scenario above. The stored as attribute is theinverse relation of the Filestored from property, which we describe later. Notethat all these metadata should be generated automatically, while the user works,according to the schema depicted in Figure 3.1.Figure 3.1 Email context.Exploiting the File Hierarchy ContextScenario. Alex spent his holiday in Hannover, Germany, taking a lot of digitalpictures. He usually saves his pictures from a trip into a folder named after thecity or the region he visits. However, he has no time to rename all images, and thustheir file names are the ones used by his camera for example DSC00728.JPG.When he forgets the directory name, no ordinary search can retrieve his pictures,as the only word he remembers, Germany, does neither appear in the filenames, nor in the directory structure. It would certainly be useful if an enhanced26Paul  Alexandru ChiritaDesktop search with a query like pictures Germany would assist in retrievinghis Hannover pictures.Context and Metadata. Obviously, our context metadata for files include thebasic file properties like last date of access and creation, as well as the file owner.File types can be inferred automatically, and provide useful information as wellin our case, the file is of type JPEG image data. Additionally, a file might bea visited Web page which we stored on our computer or an attachment saved froman email. This stored from property is of great importance because it representsinformation that current file systems miss, the provenance of information. Wealso keep track of the whole file path, including directory structure. Finally, weextend the strings used in the path name using WordNet 167, a lexical referencesystem which contains English nouns, verbs, adjectives and adverbs organized intosynonym sets, each representing one underlying lexical concept. Different relationslink the synonym sets. In our case, we use the following additional relationshipsto enrich the context of the stored file Hypernym Designates a class of more general instances. X is a hypernymof Y if Y is a kind of X. Holonym Designates the superset of an object. A is a holonym of B if B isa part of A. Synonyms  A set of words that are interchangeable in some context. X isa synonym of Y if Y can substitute X in a certain context without alteringits meaning.The complete file context ontology is also depicted in Figure 3.2.Figure 3.2 File context.27Chapter 3. Ranking for Enhancing Desktop Search.Exploiting the Web Cache ContextScenario. Even though Web search engines are providing surprisingly goodresults, they still need to be improved to take user context and user actions intoaccount. Consider for example Michael, who is looking for the Yahoo internshipsWeb page, which he has previously visited, coming from the Yahoo home page.If he does not remember the right set of keywords to directly jump to this page, itwould certainly be nice if enhanced Desktop search, based on his previous surfingbehavior, would support him by returning the Yahoo home page, as well asproviding the list of links from this page he clicked on during his last visit.Context and Metadata. The central class in this scenario is VisitedWebPage.Upon visiting a Web page, the user is more interested in the links she has usedon that page, rather than every possible link which can be followed from there.Thus, the metadata contains only the hyperlinks accessed for each stored Webpage 1 departed to shows the hyperlinks the user clicked on the current Webpage, and 2 arrived from represents the pages the user came from. Also here,we have added properties related to the time of access and place of storage in thehard disk cache. For more specific scenarios, we can further define subclasses ofthis base class, which include scenario specific attributes, for example recordingthe browsing behavior in CiteSeer, which we will discuss in the next section.Figure 3.3 Web page context.Exploiting the Browsed Publications ContextScenario. This is an example of a more specific scenario. Suppose that Alicebrowses through CiteSeer for papers on a specific topic, following reference linksto and from appropriate papers, and downloads the most important documentsonto her computer. Now as soon as they are stored in one of her directories, hercarefully selected documents are just another bunch of files without any relationships. They have completely lost all information present in CiteSeer, in this casewhich paper references or is referenced by other papers, and which papers Alicedeemed important enough not only to look at but also to download. In our systemwe preserve this information and make it available as explicit metadata.28Paul  Alexandru ChiritaContext and Metadata. As discussed, stored files on todays computers donot tell us whether they were saved from a Web page or from an email, notto mention the URL of the Web page, outgoing or ingoing visited links andmore specific information inferable using this basic knowledge and a model ofthe Web page context browsed, as discussed in our scenario. We therefore createa subclass of VisitedWebPage called Publication, and add suitable properties asdescribed in Figure 3.4. The Publication class represents a cached CiteSeer Webpage. It records the CiteSeer links traversed from that page using the referencesproperty, as well as the CiteSeer documents which the user visited before, usingthe referenced by property. It is easy to notice that these pages represent asubset of the metadata captured by the departed to and arrived from relations.PDF file and PS file are subclasses of File and describe the format used to save thepublication into the PIR. They are connected to Publication with subpropertiesof stored as, namely stored as pdf and stored as ps.Figure 3.4 Browsed publications context.Other Contexts Enhancing Desktop SearchResources on the PC Desktop offer us a lot of additional sources of information.For example many multimedia files contain embedded metadata, such as the artistand title of a song saved as an mp3 file, or the date and time when a picturewas taken. Some resources could be embedded into others, thus linking variousDesktop contexts e.g., a picture might be inserted in a text document describinga city. Similarly, our chat discussions with work partners from other locationsmight contain some valuable information as well, including related pictures ordocuments transmitted via the messaging application. All in all, the more contextsincluded in the Desktop search architecture, the broader will be its resourcecoverage and output quality.29Chapter 3. Ranking for Enhancing Desktop Search.3.3.2 A Context Oriented Architecture for Desktop SearchWe will now present our 3layer architecture for generating and exploiting thecontextual metadata enhancing Desktop resources. At the bottom level, we havethe physical resources currently available on the PC Desktop. Even though theycan all eventually be reduced to files, it is important to differentiate between thembased on content and usage context. Thus, we distinguish structured documents,emails, offline Web pages, general files9 and file hierarchies. As discussed in theprevious section, while all of them do provide a basis for Desktop search, theyalso miss a lot of contextual information, such as the author of an email, or thebrowsing path followed on a specific Web site. We generate and store this additional search input using metadata annotations, which are placed on the secondconceptual layer of our architecture. Finally, the uppermost layer implements aranking mechanism over all resources on the lower levels. An importance scoreis computed for each Desktop item, supporting an enhanced ordering of resultswithin Desktop search applications. The complete architecture is depicted in Figure 3.5. In the next subsections we describe both its higher level layers followinga bottomup approach.Figure 3.5 Contextual Desktop Ranking Architecture.The Semantic Information Layer Bringing the Contexts Together. People organize their lives according to preferences often based on their activities.Consequently, Desktop resources are also organized according to performed activities and personal profiles. Since most of the information related to these activitiesis lost within our current Desktop applications, the goal of the semantic information layer is to record and represent this data, as well as to store it as annotationsassociated to each resource. Figure 3.6 depicts an overview of the ontology thatdefines appropriate annotation metadata for the contexts we are focusing on.9Text files or files whose textual content can be retrieved.30Paul  Alexandru ChiritaFigure 3.6 Contextual Ontology for the Desktop.The main characteristic of our extended Desktop search architecture is metadatageneration and indexing onthefly, triggered by modification events generatedupon occurrence of file system changes. This relies on notification functionalitiesprovided by the OS kernel. Events are generated whenever a new file is copied tohard disk or stored by the Web browser, when a file is deleted or modified, whena new email is read, etc. Depending on the type and context of the file  event,various appropriate software modules generate each specific type of metadata andexport it into the Desktop search engine index.The Contextual Ranking Layer. As the amount of Desktop items has beenincreasing significantly over the past years, Desktop search applications will returnmore and more hits to our queries. Contextual metadata, which provide additionalinformation about each resource, result in even more search results. A measure oflocal resource importance is therefore necessary. In the following paragraphs wepropose such a ranking mechanism which exploits the popularity of each itemand the connections between users activity contexts.Basic Ranking. Given the fact that rank computation on the Desktop would not bepossible without the contextual information, which provides semantic links amongresources, annotation ontologies should describe all the aspects and relationshipsinfluencing the ranking. The identity of the authors for example influences ouropinion on documents, and thus author should be represented explicitly as aclass in our publication ontology.Then, we have to specify how these aspects influence importance. ObjectRank18 has introduced the notion of authority transfer schema graphs, which ex31Chapter 3. Ranking for Enhancing Desktop Search.tend schemas similar to the ontologies previously described, by adding weightsand edges in order to express how importance propagates among the entities andresources inside the ontology. They extend our context ontologies with the information we need to compute ranks for all instances of the classes defined in thecontext ontologies.Figure 3.7 Contextual Authority Transfer Schema.Figure 3.7 depicts our context ontology enriched with the appropriate authoritytransfer annotations. For example the authority of an email is influenced by thesender of the email, its attachment, the number of times that email was accessed,the date when it was sent and the email to which it was replied. Consequently, ifan email is important, its sender might be an important person, its attachment animportant one, just as the date when the email was sent and the previous emailin the thread hierarchy. Given the fact that semantically based schema graphsare bidirectional, we split every edge in two parts, one for each direction. This isbecause contextual authority in particular flows in both directions if we know thata person is important, we also want to have all emails we receive from this person32Paul  Alexandru Chiritaranked higher. The final ObjectRank value for each resource is then calculatedutilizing the regular PageRank formula, as applied onto the graph resulted frominstantiating our ontology.Personalization and Beyond. For the computation of authority transfer, we canalso use PageRanks biasing vector in order to include additional external rankingsources into the algorithm, such as for example Google page scores, CiteSeercitation counts, or social network reputation values.These Desktop specific rankings can already be seen as personalized, since theyare specific to the data within each users Personal Information Repository. However, the same PageRank personalization vector can be used to further bias therankings onto some given contexts deemed more interesting by the user. At alower granularity level, different authority transfer weights will express differentpreferences of the user, translating again into personalized rankings. The important requirement for doing this successfully is that we include in each usersontology all concepts influencing her ranking function. For example if we consider a publication important because it was written by an important author, wehave to represent that in the ontology. Similarly, if the importance of our digitalphotographies is heavily influenced by the event or the location where they weretaken, then both of them must be included as classes in the context ontology.3.3.3 ExperimentsExperimental SetupWe evaluated our algorithms by conducting a small scale user study. Colleaguesof ours provided a set of their locally indexed publications, some of which theyreceived as attachments to emails thus containing rich contextual metadata associated to them from the specific email fields. Then, each subject defined herown queries, related to their activities, and performed search over the above mentioned reduced images of their Desktops. In total, 30 queries were issued. Theaverage query length was 2.17 keywords, which is slightly more than the averageof 1.7 keywords reported in other larger scale studies see for example 86. Generally, the more specific the test queries are, the more difficult it is to improveover basic textual information retrieval measures such as TFxIDF. Thus, havingan average query length a bit higher than usual can only increase the quality ofour conclusions.Our entire system is built as an extension to the open source Beagle Desktop searchengine. For comparison purposes, we sent each of the above mentioned queries tothree systems 1 the original Beagle, whose output is selected and sorted using33Chapter 3. Ranking for Enhancing Desktop Search.solely TFxIDF, 2 an intermediate version of our system, Beagle, enhancedonly with activity based metadata using the same TFxIDF measure for orderingits output, but giving more importance to those results having also metadataassociated to them, and 3 the current Beagle, containing enhancements forboth metadata support and Desktop ranking. We combined the regular textualranking with the link analysis based one using the following formulaRa  Ra  TFxIDF a, 3.1where Ra is the Desktop rank of resource a, and TFxIDF a is automaticallycomputed by Beagle. Thus, a search hit will have high score if it has both a highrank and a high TFxIDF score. Finally, for every query and every system, eachuser rated the top 5 output results using grades from 0 to 1, as follows 0 for anirrelevant result, 0.5 for a relevant one, and 1 for highly relevant one.MethodologyWe used the ratings of our subjects to compute average precision and recallvalues at each output rank 17. In general, precision measures the ability ofan information retrieval system to return only relevant results. It is defined asPrecision Number of Relevant Returned ResultsNumber of Returned Results3.2Recall is its complement It measures the ability of a system to return all relevantdocuments, and is computed using the formula belowRecall Number of Relevant Returned ResultsTotal Number of Relevant Results Available in the Entire System3.3Both measures can be calculated at any rank r, i.e., considering only the top rresults output by the application. For example, even if the system has returned2000 hits for some user query, when calculating precision at the top3 results, weconsider only these three as returned results. This is necessary for large scaleenvironments, such the World Wide Web, and more recently, the PC Desktop,because it impossible to check the relevance of all output results  even in theDesktop environment, it is not uncommon to obtain several hundreds of searchresults to a given query. Restricting the calculation of precision and recall tovarious ranks is also useful in order to investigate the quality of the system atdifferent levels. Usually, in a healthy information retrieval system, as the rank level34Paul  Alexandru ChiritaFigure 3.8 Average Precision Results.is increased, recall is also increasing the denominator remains the same, whilethe numerator has the possibility to increase, whereas precision is decreasingbecause most of the relevant results should be at the very top of the list.Another important aspect is calculating the total number of available relevantresults. For search engines, including Desktop ones, an approximation must beused, as the datasets they cope with are too large. Here we consider this amountto be equal to the total number of unique relevant results returned by the threesystems we investigated. For every query, each system returned 5 results, 15 intotal. Thus, the minimum possible total number of relevant results is 0 and themaximum is 15. Similarly, the maximum number of relevant results a system canreturn is 5 since it only outputs 5 results, indicating that the recall will notnecessarily be 1 when restricting the computation to rank 5. This version of recallis called relative recall 17.Results and DiscussionAs the main purpose of our experimental analysis was to produce an estimateof each systems performance, we averaged the precision values at each rankfrom one to five for all 30 queries submitted by our experts. Note that thisgave us a weighted precision, as we considered both relevant i.e., scored 0.5and highly relevant results i.e., scored 1. The results obtained are depicted inFigure 3.8. We first notice that the current Beagle Desktop Search is rather poor,containing more qualitative results towards rank 4 to 5, rather than at the top ofthe result list. This is in fact explainable, since Beagle only uses TFxIDF to rank35Chapter 3. Ranking for Enhancing Desktop Search.Figure 3.9 Average Relative Recall Results.its results, thus missing any kind of global importance measure for the Desktopresources. On the contrary, our first prototype, consisting of Beagle enhancedwith metadata annotations, already performs very well. An important reason forits high improvement is that metadata are mostly generated for those resourceswith high importance to the user, whereas the other automatically installed filese.g., help files are not associated with metadata, and thus ranked lower. Finally,the precision values are even higher for our second prototype, which adds ourDesktop ranking algorithm to the metadata extended version of Beagle. Clearlyranking pushes our resources of interest more towards the top of the list, yieldingeven higher Desktop search output quality.In the second part of the evaluation, we drew similar conclusions with respectto the average recall values depicted in Figure 3.9 The recall of Beagle isvery low, whereas that of our prototypes is almost three times better owingto the additional information available as metadata. The difference betweenour two prototypes is relatively small, which is correct, since recall analyzes theamount of good results returned, and both our systems yield relevant results.We thus conclude that enhancing Beagle with metadata annotations significantlyincreases its recall as metadata usually represents additional, highly relevanttext associated to each Desktop file, whereas adding Desktop ranking furthercontributes with a visible improvement in terms of precision.36Paul  Alexandru Chirita3.4 Ranking by Tracking All User ActionsIn the previous section we proposed the first link analysis based algorithm whichranks resources within Personal Information Repositories. It was based on a setof heuristics defining several strict user activity patterns to generate relationshipsbetween Desktop items. Unfortunately, though they indeed connect highly relateddocuments, these heuristics yield a rather sparse Desktop linkage matrix, leavingout many important resources from the computation. This section will proposeand empirically investigate the opposite approach All user activities will betaken into consideration when building the personal link structure. We will firstintroduce this new general heuristic in more detail, and then we will analyzeboth some Desktop specific behavioral patterns and their effect upon the definedranking algorithm.3.4.1 Generic Usage Analysis Based RankingExploiting Resource Accesses to Generate Ranks. Our previous algorithmcreated links between Desktop resources only when a very specific Desktop usageactivity was encountered e.g., the attachment of an email was saved as a file, ora Web page was stored locally, etc.. We now address the problem from the otherperspective and suppose that in almost all cases when two items are touched in asequence several times, there will also be a relation between them, irrespective ofthe underlying user activity. This basically generalizes the algorithm introducedin Section 3.3.We thus propose to add a link between two items a and b whenever item b istouched after a for the T th time, with T being a threshold set by the user. Highervalues for T mean an increased accuracy of the ranking algorithm, at the costof having a score associated to less resources. Theoretically, there is only a verylow probability to have any two items a and b touched in a sequence even once.However, since context switching occurs quite often nowadays, we also investigatedhigher values for T , but experimental results showed them to perform worse thanT  1. This is in fact correct, since two files are accessed consequently more oftenbecause they are indeed related, than due to a switch of context.After a short period of time a reputation metric can be computed over the graphresulted from this usage analysis process. There exist several applicable metrics,the most common one being again PageRank. On the one hand, it has theadvantage of propagating the inferred semantic similarities connections, i.e.,if there is a link between resources a and b, as well as an additional link betweenresources b and c, then with a relatively high probability we should also have a37Chapter 3. Ranking for Enhancing Desktop Search.connection between a and c. On the other hand, PageRank also implies a smalladditional computational overhead, which is not necessary for a simpler, yet morenave metric, inlink count. According to this latter approach, the files accessedmore often get a higher ranking. However, our experiments from Section 3.4.2 willshow that although it does indeed yield a clear improvement over simple TFxIDF,file access counting is also significantly less effective than PageRank.Another aspect that needs to be analyzed is the type of links residing on the PCDesktop. Since we are now dealing with a generic analysis, we use directed linksfor each sequence a b, as when file b is relevant for file a, it does not necessarilymean that the reversed is true as well. Imagine for example that b is a generalreport we are regularly appending, whereas a is the article we are writing. Clearlyb is more relevant for a, than a is for b. This yields the following algorithmAlgorithm 3.4.1.1. Ranking Desktop Items.Preprocessing1 Let A be an empty link structure.2 Repeat for ever3 If File a is accessed at time ta, File b at time tb AND ta  tb  ,4 Then Add the link a b to A.Ranking1 Let A be an additional, empty link structure.2 For each resource i3 For each resource j linked to i4 If Linksi j  T  in A5 Then Add one link i j to A.6 Run PageRank using A as underlying link structure.As it was not clear how many times two resources should be accessed in a sequencein order to infer a semantic connection between them, we studied several valuesfor the T threshold, namely one, two and three. Additionally, we also explored thepossibilities to directly use the original matrix A with PageRank, thus implicitlygiving more weight to links that occurred more frequently recall that in A eachlink is repeated as many times as it occurred during regular Desktop activity.Finally, in order to address a broad scope of possible ranking algorithms, wealso experimented with more trivial reputation measures, namely 1 frequency ofaccesses and 2 total access time.38Paul  Alexandru ChiritaExploiting Resource Naming and Content to Generate Ranks. There exists a plethora of other generic cues for inferring links between personal resources10.For example the files stored within the same directory have to some extent something in common, especially for filers, i.e., users that organize their personal datainto carefully selected hierarchies 165, 20, 156. Similarly, files having the samefile name ignoring the path are in many times semantically related. In this casehowever, each name should not consist exclusively of stopwords. More, for thissecond additional heuristic we had to utilize an extended stopword list, which alsoincludes several very common file name words, such as index, or readme. Intotal, we appended 48 such words to the original list. We also note that boththese above mentioned approaches favor lower sets If all files within such a sete.g., all files residing in the same directory are linked to each other, then the stationary probability of the Markov chain associated to this Desktop linkage graphis higher for the files residing in a smaller set. This is in fact correct, since forexample a directory storing 10 items has most probably been created manually,thus containing files that are to some extent related, whereas a directory storing1,000 items has in most of the situations been generated automatically.A third broad source of linkage information is file type. There is clearly a connection between the resources sharing the same type, even though it is a very smallone. Unfortunately, each such category will nowadays be filled with up to severalthousands of items e.g., JPG images, thus making this heuristic difficult to integrate into the ranking scheme. A more reliable approach is to use text similarityto generate links between very similar Desktop resources. Likewise, if the sameentity appears in several Desktop resources e.g., Hannover appears both as thename of a folder with pictures and as the subject of an email, then we argue thatsome kind of a semantic connection exists between the two resources. Finally, wenote that users should be allowed to manually create links as well, possibly havinga much higher weight associated to these special links.Practical Issues. Several special cases might arise when applying usage analysisfor Desktop search. First, the log file capturing usage history should persistover system updates in order to preserve the rich linkage information. In ourexperiments, we collected only about 80 KB of log data over 2 months. Second,more important, what if the user looks for a file she stored 5 years ago, when shehad no Desktop search application installed We propose several solutions to this1. The nave approach is to simply enable ranking based exclusively on TFxIDF. However, much better results can be obtained by incorporating contextual information within the ranking scheme.10Note that these additional heuristics follow the general approach taken in this section, i.e.,they are applicable to all personal files, rather than a set of narrowly specified ones.39Chapter 3. Ranking for Enhancing Desktop Search.2. We therefore propose a more complex query term weighting scheme, suchas BM25 132. Teevan et al. 204 have recently proposed an applicationof this metric to personalize Web search based on Desktop content. Inour approach, their method must be adapted to personalize Desktop searchbased on a specific subset of PIR, represented for example by the files witha specific path or date range.3. If the user remembers the approximate moment in time when she accessedthe sought item, then this date represents a useful additional context basedvertical ranking measure. For example, if the user remembers having usedthe target file around year 1998, the additional importance measure is represented by the normalized positive time difference between mid1998 andthe date of each output result.4. If no contextual information is available, we propose to infer it through arelevance feedback process, in which the user first searches the Desktop usingTFxIDF exclusively, and then selects one or several relatively relevantresults, which are then used to extract a context e.g., date or to proposeexpansions to the user query.Comparison to the Web model. Clearly, unlike in the Web, most of theDesktop search queries are navigational users just want to locate somethingthey know their stored before. So, are some Desktop files more important thanothers, or are they all approximately equally important We argue that, as inthe Web, some Desktop resources are much more important than others, and thususers will most of the time be seeking only for these highly important items.For example, one year after some project was closed, a log file inspected bythe researcher 400 times during an experiment will definitely be less importantthan the project report which was probably accessed only 100 times. Therefore,contextual information, though very important, is not sufficient in effectivelylocating Desktop items. More complex importance measures are thus neededin order to exploit users activity patterns, her local Desktop organization, etc.either within a set of targeted scenarios, as in Section 3.3, or in a generalizedapproach, as described in this section.3.4.2 ExperimentsExperimental SetupWe evaluated the utility of our algorithms within three different environments ourlaboratory with researchers in different computer science areas and education,a partner laboratory with slightly different computer science interests, and the40Paul  Alexandru Chiritaarchitecture department of our university. The last location was especially chosento give us an insight from persons with very different activities and requirements.In total, 11 persons installed our logging tool and worked normally on theirDesktops for 2 months11. Then, during the subsequent 3 weeks, they performedseveral Desktop searches related to their regular activities12, and graded each top10 result of each algorithm with a score ranging from 1 to 5, 1 defining a very poorresult with respect to their Desktop data and expectations, and 5 a very good one.This is in fact a Weighted P10 17, i.e., precision at the first 10 results. Forevery query, we shuffled the top ten URIs output by each of our algorithms, suchthat the users were neither aware of their actual place in the rank list, nor of thealgorithms that produced them. On average, for every issued query the subjectshad to evaluate about 30 Desktop documents i.e., the reunion of the outputs ofall approaches we investigated. In total, 84 queries had been issued and about2,500 documents were evaluated.For the link based ranking algorithms recall that for the sake of completeness wehave also evaluated some time based ranking heuristics we set the parameter to four times the average break time of the user. We have also attempted to setit to one hour, and eight times the average break time of the user, but manualinspection showed these values to yield less accurate usage sessions. Althoughmuch more complex techniques for computing usage session times do exist e.g.,exploiting mouse clicks or movements, scrollbar activities, keyboard activities,document printing, etc. 74, 211, we think this heuristic suffices for provingour hypothesis, i.e., usage analysis based ranking improves over simple textualretrieval approaches.In the following we will first present an analysis of this experiment focused onidentifying some general usage patterns and on investigating the behavior of ourranking algorithms. Afterwards we will proceed to the qualitative analysis of thesearch output produced by each approach.Analyzing Usage PatternsOur ultimate goal is to infer links from Desktop resources that have been accessedin a sequence. Yet this is not a straightforward task. Several persons mighthave quite different usage behavior strategies, thus making it very difficult to11The logger was implemented using a hook that caught all manual file open  create  savesystem calls.12The only requirement we made here was to perform at least 5 queries, but almost everysubject provided more. In all cases, we collected the average rating per algorithm for eachperson.41Chapter 3. Ranking for Enhancing Desktop Search. 0 0.5 1 1.5 2 0  1e006  2e006  3e006  4e006  5e006  6e006  7e006File access  No activitySeconds since experiment startcdesktoprankstatsdistribution.txtFigure 3.10 Two months distribution of manual accesses to desktop items.distinguish usage sessions from each other. Moreover, this problem could evenoccur with the same person, at two different moments in time. We thus firstanalyzed the file access patterns of seven of our subjects. All of them manifestedrather similar characteristics on the long term. We depict in Figure 3.10 one useractivity captured over a period of two months. Notice that on such a long term thedaily accesses are rather easy to distinguish Each bar represents one file access.When several accesses occur at small intervals, their associated bars are mergedinto a thicker one. Also, longer breaks have been generated during weekends, andthe three very large pauses represent vacation periods. But what happens withthe activity performed during a single day A general example is presented inFigure 3.11. There are two access intensive working sessions in the morning, andonly one session in the afternoon. In general, we distinguished two broad typesof desktop activity Working e.g., reading an article, writing a program, etc.,which usually results in a relatively small file access frequency, and Browsing e.g.,reading emails, surfing the Web, etc., when much more resources are opened ina short amount of time, in many cases only for reading. We believe these resultscould be used in a separate stream of research in order to find more accuratedefinitions for the parameter  which delimits user sessions from each other.Having identified the file access patterns, we then investigated the distributions offile access frequency and total file access time, as they represent a good indicatorof how the final ranking distributions will look like. The former distribution hasalso been investigated by Dumais et al. 86, obtaining similar results. However,they only looked at the resources that have been accessed at least once, whereas we42Paul  Alexandru Chirita 0 0.5 1 1.5 2 55000  60000  65000  70000  75000  80000  85000  90000File access  No activitySeconds from experiment startcdesktoprankstatsdayd1.txtFigure 3.11 One day distribution of manual accesses to desktop items.considered all Desktop items in our analysis. This helped us obtain an additionalinteresting result, namely that only about 2 of the Desktop indexable items areactually manually accessed by the user. This is most probably because of variousprogram documentations especially when in HTML format, locally stored mailing list archives, etc. We think this finding further supports the idea of exploitingusage information in ranking desktop search results, as current textual measuresi.e., TFxIDF many times output high scores for such automatically deployeddocuments that have never been touched by the user. We depict a sample visitfrequency distribution in Figure 3.12. For all our testers, this distribution followeda power law i.e., fx  c  1x with very low values for the  exponent, ranging from 0.26 to 0.38. Similarly, we depict the distribution of total time spentaccessing each file i.e., reading, writing, etc. in Figure 3.13. This distributioncan be tailored by a power law with an exponential cutoff, as in the followingformulafx  c  1x exzc 3.4The additional inverse exponential term is only used to ensure a faster decreasingvalue of f , zc being a parameter. Again, we obtained very low values for , residingaround 0.18.Ranking analysisWe then analyzed how our algorithms perform, in order to further tune theirparameters and to investigate whether the nonusage analysis heuristics do indeed43Chapter 3. Ranking for Enhancing Desktop Search. 1 10 1  10  100  1000  10000Number of Visits  1File cdesktoprankfinalplotsvisitfrequency1inv.txtFigure 3.12 Frequency distribution of number of manual accesses to desktopitems.make a difference in the overall rankings. We thus defined and analyzed thefollowing 17 algorithms T1 Algorithm 3.4.1.1 with T  1. T1Dir T1 enriched with additional links created as complete subgraphswith the files residing in every Desktop directory i.e., all the files in adirectory point to each other. T1DirFnames T1Dir with further additional links created as completesubgraphs with the resources having the same file name i.e., all items withthe same file name point to each other, provided that the file name does notconsist exclusively of stopwords. T1Fnames T1 enriched with the links between resources with identicalfile names as in the previous algorithm13. This was necessary to inspect thespecific contribution of directories and file names respectively to the overallranking scheme. T1x3Dir Same as T1Dir, but with the links inferred from usage analysisbeing three times more important than those inferred from the directorystructure. T1x3DirFnames Same as above, but also including the links provided byidentical file names.13For emails, this corresponded to having the same subject, eventually with Re or Fwdinserted in the beginning.44Paul  Alexandru Chirita 1 10 100 1000 1  10  100  1000Total Visit Time Seconds  1File cdesktoprankfinalplotsvisittime.txt6000  x0.12  expx32Figure 3.13 Distribution of time spent while manually accessing desktop items. T1x3Fnames Same as T1x3Dir, but using the file name heuristic instead of the directory one. T2 Algorithm 3.4.1.1 with T  2. T3 Algorithm 3.4.1.1 with T  3. VisitFreq Ranking by access frequency. 1HourGap Ranking by total amount of time spent on accessing eachresource, with sessions delimited by one hour of inactivity. 4xAvgGap Ranking by total access time, with sessions delimited by aperiod of inactivity longer than four times the average break time of theuser. 8xAvgGap Same as above, but with sessions bounded by a period ofinactivity longer than eight times the average break time of the user. Weighted Algorithm 3.4.1.1 directly using the matrix A, instead of A,i.e., with links weighted by the number of times they occurred. WeightedDir Algorithm Weighted enriched with links between the filesstored within the same directory. WeightedDirFnames The previous algorithm with a link structure extended with connections between files with identical names. WeightedFnames Same as above, but without the links generated byexploiting the Desktop directory structure.45Chapter 3. Ranking for Enhancing Desktop Search. 1e005 0.0001 1  10  100  1000Threshold 1 ScorePage cdesktoprankfinalplotsranks.txtFigure 3.14 Distribution of scores for the T1 algorithm.Since inlink count is almost identical to file access count frequency, we onlyexperimented with the latter measure. The only difference between these twomeasures is that inlink count will result in lower page scores when a thresholdhigher than one is used to filterout the links see also Algorithm 3.4.1.1.We analyzed two aspects at this stage First, it was important to inspect the finaldistribution of rankings, as this indicates how Desktop search output looks likewhen using these algorithms. In all cases the resource rankings exhibits a distribution very well shaped by a power law Figure 3.14 plots the output rankingsfor algorithm T1, and Figure 3.15 depicts the output when both directory andfile name heuristics were added in this latter case we notice a strong exponentialcutoff towards the end, for the files that benefited less from the link enhancementtechniques.The second aspect to analyze was whether there is a difference between theseheuristics. For this purpose we used a variant of Kendalls  measure of similaritybetween two ranking vectors 139, which resulted in a similarity score fallingwithin 1,1.Three of our testers one from each location were specifically asked to extensivelyuse our tool. When they reached 40 queries each, we applied the Kendall measure on their complete output, as returned by each algorithm. The results areillustrated in Table 3.1. After analyzing them, we drew the following conclusions The heuristics to link the resources residing within the same directory, orthe resources with identical file names did result in a rather different query46Paul  Alexandru Chirita 1e006 1e005 0.0001 0.001 1  10  100  1000  10000  100000T1DirFnames ScorePage cdesktoprankfinalplotsranksDirFnames.txtFigure 3.15 Distribution of scores for the T1DirFnames algorithm.output. The approaches T1x3Dir, T1x3DirFnames and T1x3Fnames did notyield a significant difference in the results. The output of T2 and T3 was very similar, indicating that a thresholdhigher than 2 is not necessary for Algorithm 3.4.1.1. 4xAvgGap and 8xAvgGap performed very similar to each other. Weighted output was very close to T1. Finally, when Weighted was combined with directory or file name information, we obtained almost identical outcomes as when we used T1 withthese heuristics.As a rule of thumb, we considered similar all algorithm pairs with a Kendall  scoreabove 0.5, and therefore removed one of them from the search quality evaluation.Exceptions were Weighted and VisitFreq both very similar to T1 in orderto have at least one representative of their underlying heuristics as well.Search quality analysisAfter the previous analysis, we kept 8 algorithms for precision evaluation T1,T1Dir, T1DirFnames, T1Fnames, T2, VisitFreq, 4xAvgGap andWeighted. Even though they do not incorporate any textual information, westill started with ranking Desktop search results only according to these measures,in order to see the impact of usage analysis on Desktop ranking. The average47Chapter 3. Ranking for Enhancing Desktop Search.Algorithm T1T1DirT1DirFnamesT1FnamesT1x3DirT1x3DirFnamesT1x3FnamesT2T3VisitFreq1HourGap4xAvgGap8xAvgGapWeightedWeightedDirWeightedDirFnamesWeightedFnamesThreshold 1 1T1Dir 0.2 1T1DirFnames0.2 0.5 1T1Fnames 0.2 0.2 0.4 1T1x3Dir 0.3 0.9 0.5 0.2 1T1x3DirFnames0.2 0.5 0.8 0.4 0.5 1T1x3Fnames 0.2 0.2 0.4 0.9 0.2 0.4 1Threshold 2 0.2 0 0.2 0 0 0.2 0 1Threshold 3 0 0.1 0.3 0.1 0.1 0.3 0.1 0.6 1VisitFreq 0.7 0.2 0.2 0.3 0.3 0.2 0.3 0.3 0.1 11HourGap 0.5 0.2 0.1 0.2 0.1 0.1 0.2 0.2 0 0.4 14xAvgGap 0.4 0.3 0.2 0.2 0.3 0.2 0.2 0.2 0 0.4 0.3 18xAvgGap 0.5 0.3 0.2 0.2 0.3 0.2 0.2 0.2 0 0.5 0.5 0.7 1Weighted 0.8 0.2 0.2 0.2 0.3 0.2 0.2 0.2 0 0.6 0.5 0.5 0.5 1WeightedDir 0.2 0.9 0.5 0.2 0.9 0.5 0.2 0 0.1 0.2 0.1 0.3 0.3 0.2 1WeightedDirFnames0.2 0.5 0.9 0.3 0.5 0.8 0.4 0.2 0.3 0.2 0.1 0.2 0.2 0.2 0.5 1WeightedFnames0.3 0.2 0.4 0.8 0.3 0.4 0.8 0 0.1 0.3 0.3 0.3 0.3 0.3 0.2 0.4 1Table 3.1 Kendall similarity for the Desktop ranking algorithms average over120 queries from 3 users.results are summarized in the second column of Table 3.2. As we can see, allmeasures performed worse than TFxIDF we used Lucene14 together with animplementation of Porters stemmer to select the query hits, as well as to computethe TFxIDF values15, but only at a small difference. This indicates that users doissue a good amount of their Desktop queries on aspects related to their relativelyrecent, or even current work. Also, as the T2 algorithm does not improve overT1, it is therefore sufficient to use Algorithm 3.4.1.1 with a threshold T  1 inorder to effectively catch the important Desktop documents. This is explainable,14httplucene.apache.org15Note that even though our Desktop search system, Beagle, is also based on a Linux specific.Net implementation of Lucene, we decided to move this evaluation outside of its context, inorder to allow for a broader range of subjects, i.e., running Java on both Linux and Windows.48Paul  Alexandru Chiritasince a threshold T  2 would only downgrade files that were accessed only once,which have a relatively low score anyway compared to the other more frequentlytouched resources.Finally we investigated how our algorithms perform within a realistic Desktopsearch scenario, i.e., combined with term frequency information. We used thefollowing formulaScorefile  NormalizedScorefile  NormalizedV SMScorefile, queryThe VSM score is computed using the Vector Space Model and both scores arenormalized to fall within 0,1 for a given query16. The resulted average gradingsare presented in the third column of Table 3.2. We notice that in this approach, allmeasures outperform TFxIDF in terms of weighted precision at the top 10 results,and most of them do that at a statistically significant difference see column 4 ofTable 3.2 for the p values with respect to each metric.The usage analysis based PageRank T1 is clearly improving over regular TFxIDF ranking. As for the additional heuristics evaluated, connecting items withsimilar file name or residing in the same directory, they yielded a significant improvement only when both of them have been used. This is because when usedby themselves, these heuristics tend to bias the results away from the usage analysis information, which is the most important by far. When used together, theyadd links in a more uniform manner, thus including the information deliveredby each additional heuristic, while also keeping the main bias on usage analysis. Finally, the simpler usage analysis metrics we investigated e.g., ranking byfrequency or by total access time did indeed improve over TFxIDF as well, butwith a lower impact than the Algorithm 3.4.1.1 enriched with directory and filename information. We conclude that with TFxIDF in place, usage analysis significantly improves Desktop search output rankings and it can be further enhancedby linking resources from the same directory and with identical file names.The final results are also illustrated in Figure 3.16, in order to make the improvement provided by our algorithms also visible at a graphical level. The horizontalline residing at level 3.09 represents the performance of TFxIDF the red barsdepict the average grading of the algorithms combining TFxIDF with our approaches, and the blue ones depict the average grading obtained when using onlyour usage analysis algorithms to order Desktop search output.Observation. We have showed our algorithms to provide significant improvements over regular TFxIDF search. Yet since they tend bias the results very16In order to avoid obtaining many null scores when using access frequency or total accesstime recall that many items have never been touched by the user, in these scenarios we alsoadded a 1N score to all items before normalizing, with N being the total amount of Desktopitems.49Chapter 3. Ranking for Enhancing Desktop Search.Algorithm Weighted P10 Weighted P10 Signif. for CombinedUsg. An. Combined versus TFxIDFT1  TFxIDF 3.04 3.34 p  0.003T1Dir  TFxIDF 3.02 3.36 p  0.001T1DirFnames  TFxIDF 2.99 3.42 p 0.001T1Fnames  TFxIDF 2.97 3.26 p  0.064T2  TFxIDF 2.85 3.13 p  0.311VisitFreq  TFxIDF 2.98 3.23 p  0.1414xAvgGap  TFxIDF 2.94 3.09 p  0.494Weighted  TFxIDF 3.07 3.30 p  0.012TFxIDF 3.09 3.09Table 3.2 Average grading for the usage analysis algorithms with and withouta combination with TFxIDF, together with tests on the statistical significance ofthe improvement the latter ones bring over regular TFxIDF.Figure 3.16 Average grading for the usage analysis algorithms.much towards the previously accessed files, wouldnt we get similar performancesimply by boosting these visited files within the Desktop search output i.e., without using any link analysis approach The answer is no and can be probed bycarefully analyzing the results from Table 3.2. The VisitFreq algorithm is in factan implementation of this nave heuristic it orders the search results based on thefrequency with which they were visited if this is 0, then TFxIDF is used to sortthe output. And as we can see from Table 3.2, VisitFreq performs significantlyworse compared to our complete link analysis approach, T1DirFnames 3.23 versus 3.42, a difference which is also statistically significant with p  0.043, as wellas to T1, which only considers the creation of access based links 3.23 versus 3.34,50Paul  Alexandru Chiritawith p  0.098. Therefore, a more complex measure of importance for Desktop items such as T1DirFnames is indeed necessary in order to also incorporatethe order in which resources have been touched, as well as any other potentialDesktop linkage heuristics.3.5 DiscussionCurrently there are quite several personal information systems managing PC Desktop resources. However, all of them have focused on seeking solutions to findpreviously stored items in a faster way. In this chapter we argued that Personal Information Repositories have grown considerably in size, and thus theyare emerging as a potentially problematic environment in terms of Data Management. More specifically, we argued that all previous search based solutions tolocate information at the PC Desktop level are insufficient for the current scalability requirements. These approaches already yield several hundreds of queryresults, which cannot be successfully ordered by using textual retrieval measuresexclusively. To solve this problem we proposed to introduce link analysis rankingfor Desktop items and investigated in detail two major approaches to achieve thisgoal First, we exploited contextual analysis of specific user actions to build thePIR link structure and compute local reputation values for personal items.This approach does indeed an excellent job at ranking Desktop search output. However, each manually defined activity context brings only a relativelysmall amount of links into the local graph connecting users resources. Thus,a lot of design and programming work is needed in order to achieve bothquality and coverage. In the second part we generalized this idea by including all user actions intothe ranking algorithm. This approach was clearly much simpler, while alsoproviding a visibly larger coverage of the personal items. Unfortunately, thequalitative improvements it brought were smaller than those of the previoustechnique, even though they were still improving over regular Desktop searchat a statistically significant difference. This small quality decrease is becauseof the noisy connections it induces into the local link structure. As it is ageneral heuristic, it also captures false links, such as those generated due tofrequent switches between Desktop activity contexts.All in all, as we found out that people usually access only about 2 of their indexable items, we conclude that both our usage analysis based ranking algorithmsare very suitable for a better Personal Information Management. In fact, our51Chapter 3. Ranking for Enhancing Desktop Search.user based experiments showed both techniques to significantly increase Desktopsearch quality.Two issues still remain open First, it would be interesting to define some betterfully automatic graph trimming heuristics for our second approach, in order tokeep the ranking coverage at a sufficiently high level, while also achieving anexcellent output quality. Second, though less important, one might investigateother ranking extensions which include also non access based heuristics, thusaddressing more local resources, even when these have never been opened by theuser.52Chapter 4Ranking for Spam Detection4.1 IntroductionSpamming is the abuse of using electronic messaging systems to send unsolicitedmessages. Though its most widely recognized form is Email spam, the term isalso applied to similar abuses in other media Instant Messaging spam, Usenetnewsgroup spam, Web search engine spam, Weblogs spam, and Mobile phonemessaging spam.In this chapter we tackle the two most important forms of spamming, namelyEmail and Web spam. Email Spam is a very profitable business, as it has no associated operating cost beyond the actual management of the recipient addresses.Thus, malicious merchants would send millions of Emails either promoting someunderground products for example Viagra medicines, or attempting to lure theaddressee into fake businesses quite spread is the model of the Nigerian businessmen who inherited a large amount of money, yet is in need of an account toship them to, etc. Moreover, senders of these Emails are difficult to identify andto hold responsible for their mailings. Consequently, spammers i.e., creators ofspam are numerous, and the volume of unsolicited mail has become extremelyhigh. Unlike legitimate commercial Email, spam is generally sent without the explicit permission of the recipients, and frequently contains various tricks to bypassEmail filtering. Even though current retail Email services offer the possibility toreport any incurred spam in order to constantly update their antispam filters, thisis and will remain a continuous fight at least up to a certain moment, in whicheach party is devising new techniques, either to deceive or to fight its opponent.53Chapter 4. Ranking for Spam Detection.A similar phenomenon can be observed for search engines as well, as they areindexing more and more content every day. If we also remember that uponsearching this huge quantity of data, people usually view only the very few topanswers returned for each query, it becomes highly important to provide thesesearch results with the best quality possible. Alas, currently this is not an easytask. Spamdexing a portmanteau of spamming and Web indexing refers to thepractice on the World Wide Web of maliciously modifying HTML pages in orderto increase their chances of being placed high on search engine ranking lists, eitherbecause their text has been altered to contain some targeted words very frequently,or because many artificial Web hyperlinks have been added towards them, etc.Though more costly and more difficult to accomplish than its Email surrogate,Web spamming is also bringing enhanced profits, especially for sites dedicated toactivities such as online gambling or porn, as long as they manage to rank highfor a certain number of common Web search queries. This is because high rankingmany times also implies high traffic, which consequently usually converts into ahigh revenue for the specific Web site.This chapter proposes to exploit link analysis ranking solutions for both Email andWeb spam detection. While this approach has been previously explored for thelatter domain, it is rather new for the former one. We thus propose to link peopleacross the social network created upon their exchanges of Emails Once a personsends an Email to a recipient, a vote is automatically cast towards that destinationindividual. The amount of Emails one receives, as well as the importance of theirsenders, contribute to achieving a high rank within the Email social network. Sincespammers generally only send Emails i.e., cast votes in our system, their overallreputation scores will turn out to be rather low, being at least partially separatedfrom the rest of the network. Moreover, using this Email ranking approach, onewould be able to order its incoming Emails according to the global reputationof their sources. Abstracting from the application environment, it is easy toobserve that this is in fact the same model as the one currently employed by theWeb search engines. There, social interactions are described by the process ofcreating hyperlinks between pages, which are then regarded as votes within thereputation algorithms. Unfortunately, as this model has already been exploitedfor several years already within the Web environment, spammers have alreadyinvented new and increasingly more complex link reinforcement structures toboost their reputation scores. Thus, simple PageRanklike schemes are no longersufficient for detecting malicious users in the World Wide Web. In the second partof this chapter we propose an improved technique to isolate spammers We analyzethe relationships between social groups in order to discover entities involved inirregular linkage structures. Once these have been identified, we simply removethem from the reputation algorithm. We test this technique onto a large collection54Paul  Alexandru Chiritaof Web pages, and show that link databases cleaned of such noisy informationyield significantly better Web search results. Finally, while the social networkbased model we propose for ordering Email addresses is rather primitive, similarto the ones used in the Web about 8 years ago, we believe that if adopted, itsdevelopment would be somewhat similar to that of the Web, and consequentlyall recent findings from the Web link analysis research would be fairly simple toadapt and apply for Emails as well.The chapter is organized as follows First, we give an overview of the existingspam detection and social network reputation metrics in Section 4.2. Then, wefirst discuss why and how these metrics could be applied to counter Email spamin Section 4.3. These being introduced, we transit in Section 4.4 towards somehigher level link spam detection approaches. We deploy these solutions onto thecurrently more complex Web environment and analyze them using the very samesocial reputation model i.e., PageRank. In the end of each of these two sectionswe conclude with a discussion of possible future research directions, as focused onthe investigated medium, Email or Web.4.2 Specific BackgroundRanking has generally not been explicitly used to enforce spam detection in theliterature. Especially for the Email application, most techniques focused on content and communication protocol extensions, rather than social network basedreputation mechanisms. In this chapter we propose ranking as a viable solutionfor detecting malicious users in social environments. It has been already provedthat social reputation mechanisms are good at distilling out qualitative subjectswithin global communities 172, 135. In the same time they are also particularly stable and resistant in front of various attacks. Considering the fact that allmedia attacked by spammers are actually social media i.e., Email, Web, InstantMessaging, etc., we argue that utilizing social ranking algorithms could be verybenefic in filtering the members of their underlying communities.This section starts with a generic discussion about Email antispam approaches.Afterwards, we give an overview of general reputation algorithms for social networks, as they are built on top of basic solutions to isolate malicious users. Finally,in the last part we present how these ranking algorithms have been employed, aswell as enhanced for spam detection in the World Wide Web.55Chapter 4. Ranking for Spam Detection.4.2.1 Email AntiSpam ApproachesBecause of the high importance of the Email spam problem, many attemptsto counter spam have been started in the past, including some law initiatives.Technical antispam approaches comprise one or several of the following basicsolutions 175 Contentbased approaches Headerbased approaches Protocolbased approaches Approaches based on sender authentication Approaches based on social networksContentbased approaches 113 analyze the subject of an Email or the Emailbody for certain keywords statically provided or dynamically learned using aBayesian filter or patterns that are typical for spam Emails e.g., URLs with numeric IP addresses in the Email body. The advantage of contentbased schemesis their ability to filter quite a high number of spam messages. For example, SpamAssassin can recognize 97 of the spam if an appropriately trained Bayesianfilter is used together with the available static rules 128. However, in contrastto the social network reputation models, content based approaches need to continuously adapt their set of static rules, as otherwise their high spam recognitionrate will decrease.Headerbased approaches examine the headers of Email messages to detectspam. Whitelist schemes collect all Email addresses of known nonspammers in awhitelist to decrease the number of false positives from contentbased schemes. Incontrast, blacklist schemes store the IP addresses Email addresses can be forgedeasily of all known spammers and refuse to accept Emails from them. A manualcreation of such lists is typically highly accurate but puts quite a high burden onthe user to maintain it. PGP key servers could be considered a manually createdglobal whitelist. An automatic creation can be realized, for instance based onprevious results of a contentbased filter as is done with the socalled autowhitelistsin SpamAssassin. Both blacklists and whitelists are rather difficult to maintain,especially when faced with attacks from spammers who want to get their Emailaddresses on the list whitelist or off the list blacklist. They are related toour Email spam detection approach in the sense of creating lists with trusted malicious users. However, our algorithm is fully automatic, thus being also fairlyeasy to maintain.Protocolbased approaches propose changes to the utilized Email protocol.Challengeresponse schemes 175 require a manual effort to send the first Email56Paul  Alexandru Chiritato a particular recipient. For example, the sender has to go to a certain Web pageand activate the Email manually, which might involve answering a simple questionsuch as solving a simple mathematical equation. Afterwards, the sender will beadded to the recipients whitelist such that further Emails can be sent without theactivation procedure. The activation task is considered too complex for spammers,who usually try to send millions of spam Emails at once. An automatic scheme isused in the greylisting approach1, where the receiving Email server requires eachunknown sending Email server to resend the Email again later. Nevertheless, thesetechniques seem to impose a too high burden on honest users as well, since theyhave not been adopted on a wide scale. Also, as they build upon user interactionprocedures, they are orthogonal to our spam detection solutions.To prevent spammers from forging their identity and allow for tracking them,several approaches for sender authentication 108 have been proposed. Theybasically add another entry to the DNS server, which announces the designatedEmail servers for a particular domain. A server can use a reverse lookup toverify if a received Email actually came from one of these Email servers. Senderauthentication is a requirement for whitelist approaches including ours, sinceotherwise spammers could just use wellknown Email addresses in the Fromline. Though it is already implemented by large Email providers e.g., AOL,Yahoo, it also requires further mechanisms, such as a blacklist or a whitelist.Without them, spammers could simply set up their own domains and DNS servers,thus easily circumventing the system.Recent approaches have started to exploit social interactions for spam detection.Such social network based approaches construct a graph, whose verticesrepresent Email addresses. A directed edge is added between two nodes A and B,if A has sent an Email to B. Boykin and Roychowdhury 31 classify Emailaddresses based on the clustering coefficient of the graph subcomponent Forspammers, this coefficient is very low because they typically do not exchangeEmails with each other. In contrast, the clustering coefficient of the subgraphrepresenting the actual social network of a nonspammer colleagues, friends, etc.is rather high. The scheme can classify 53 of the Emails correctly as ham orspam, leaving the remainder for further examination by other approaches. Thisis similar to the algorithm we present in Section 4.3 in the sense that it uses linkanalysis for spam detection. However, we also exploit the powerlaw distributionof social contacts, thus being able to obtain a much more accurate classificationratio, as well as to order Email addresses by the social reputation of their senders.Finally, closest to our work is the paper of Golbeck and Hendler, who propose atypical spreading activation scheme to rank Email addresses, based on exchanges1httpprojects.puremagic.comgreylisting57Chapter 4. Ranking for Spam Detection.of reputation values 112. They still achieve a spam detection ratio below ours.More important, the reallife applicability of their scheme is uncertain, as itsattack resilience has not been verified at all.4.2.2 Trust and Reputation in Social NetworksTrust and reputation algorithms have become increasingly popular to rank a setof items, such as people social reputation or Web pages Web reputation, forexample, when selling products in online auctions. Their main advantage is thatmost of them are designed for high attack resilience.Social reputation schemes have been designed mostly for use over PeerToPeernetworks 69, 68, 57. However, they provide an useful insight into using linkanalysis ranking to construct reputation systems, as well as into identifying different attack scenarios. From this perspective, they are also very similar to ouralgorithms. The only significant difference is that they have been adapted forand deployed onto different application environments. Ziegler and Lausen 224present a general categorization of trust metrics, as well as a fixedpoint personalized trust algorithm inspired by spreading activation models. It can be viewedas an application of PageRank onto a subgraph of the social network in order tocompute user reputation scores. Richardson 181 builds a Web of trust askingeach person to maintain trust values on a small number of other users. The algorithm presented is also based on a power iteration, but designed for an applicationwithin the context of the Semantic Web, composed of logical assertions. Finally,EigenTrust 135 is a pure fixedpoint PageRanklike distributed computation ofreputation values for PeerToPeer environments. This algorithm is also used inthe MailTrust approach 146, an Email reputation metric highly similar to BasicMailRank, investigated into the physics research community slightly after our firstreport became public. MailTrust is still different from our approach in the sensethat it does not investigate any user oriented Email filtering or importance ordering. Moreover, it builds upon a straightforward application of PageRank, whichis much more sensible to malicious attacks than MailRank note that in contrast,we bias PageRank onto the highly reputable members of each social community,thus making the gap between trustful and malicious users significantly larger.4.2.3 Spam Detection in the World Wide WebDetecting Spam on the Web. The more money are circulated within anenvironment, the more interest will spammers have to get a share of it. Naturally,this is also valid for the World Wide Web. This makes finding good solutions58Paul  Alexandru Chiritafor the Web spam detection problem not only important, but also difficult, asthey need to deal with adversaries that continuously try to deceive search enginealgorithms. As the Web search output is usually ordered using a combination ofvarious techniques available to assess the quality of each result e.g., PageRank,HITS, TFxIDF, etc., spammers have devised specific schemes to circumvent eachof these measures. Consequently, the search engines responded with detection orneutralization techniques, usually built on top of basic Web reputation algorithmssimilar to those outlined in the previous section. This caused the spammers to seeknew rank boosting methods, and so on. Since our work is focused on identifyingspam using the link structure describing various social media, we will presentin this section only the most recent antispam techniques for linkbased Webranking algorithms. Whenever possible, we will compare these approaches withthe algorithms we propose and describe in Section 4.4.Ranking Based Approaches. The currently known types of artificial linkstructures which could boost the rank of one or more Web pages have beeninvestigated by Gyogyi et al. 114. They manually built toyscale link farmsnetworks of pages densely connected to each other or alliances of farms andcalculated their impact upon the final rankings. We used their results to designsome of our spam fighting algorithms.The seminal article of Bharat and Henzinger 27 has indirectly addressed theproblem of spam neutralization on the Web. Though inherently different from ourapproaches, the work provides a valuable insight into Web link spam The authorsdiscovered the existence of mutually reinforcing relationships and proposed toassign each edge i.e., hyperlink an authority weight of 1k if there are k pagesfrom one Web site pointing a single document from another site, as well as a hubweight of 1l if a page from the first site is pointing to l documents residing all onthe second site. Authors use this information to change HITS 143, the hub andauthority ranking algorithm which we also described in Section 2.1.2. We believetheir solution could be used to complement our spam detection approaches, bothfor Web pages  sites and for Emails, in which corporation wide domains can beseen as sites. Later, Li et al. 161 also proposed an improved HITS algorithmto avoid its vulnerability to smallinlargeout situations, where one page has onlya few inlinks but many outlinks. Nevertheless, their work focused only on thisspecific problem, thus not tackling spam detection per se.Another important work is SALSA 158, where the TightlyKnit TKC Community Effect was first discussed. The organization of pages into such a denselylinked graph usually results in increasing their scores. The authors proposed a newlink analysis algorithm which adopts two Markov chains for traversing the Webgraph, one converging to the weighted indegree of each page, for authority scores,59Chapter 4. Ranking for Spam Detection.and the other converging to its weighted outdegree, for hub scores. The approachresembles popularity ranking, which was also investigated by Chakrabarti 46 andBorodin et al. 29. However, it does not incorporate any iterative reinforcementand is still vulnerable to some forms of the TKC effect 184.Zhang et al. 223 discovered that colluding users amplify their PageRank scorewith a value proportional to Out1c, where c is the PageRank dampening factor2. Thus, they propose to calculate PageRank with a different c for each pagep, automatically generated as a function of the correlation coefficient between 1cand PageRankp under different values for c. Their work is extended by BaezaYates et al. 15, who study how the PageRank increases under various collusioni.e., nepotistic topologies and prove this increase to be bounded by a value depending on the original PageRank of the colluding set and on the dampeningfactor.BadRank3 216 is one of the techniques supposed to be used by search enginesagainst link farms. It is an inverse PageRank, in which a page gets a high scoreif it points to many pages with high BadRank, as in the formula belowBRp  c qInpBRqOutq 1 c  IBp 4.1The exact expression of IBp is not known, but it represents the initial BadRankvalue of page p as assigned by spam filters, etc. The algorithm is complementaryto our approaches and the idea of propagating the badness score of a page couldbe implemented as an extension on top of the algorithms presented in Section 4.4.TrustRank 116 proposes a rather similar approach, but focused on the goodpages In the first step, a set of high quality pages is selected and assigned ahigh trust then, a biased version of PageRank is used to propagate these trustvalues along outlinks throughout the entire Web. The algorithm is orthogonal toour approaches Instead of seeking for good pages, we attempt to automaticallyidentify and penalize malicious nodes for Email spam and links for Web spam.SpamRank 25 resembles an opposite TrustRank First, each page receivesa penalty score proportional to the irregularity of the distribution of PageRankscores for its inlinking pages then, Personalized PageRank is used to propagatethe penalties in the graph. The advantage over TrustRank is that good pagescannot be marked as spam, and comes at a cost of higher time complexity. OurWeb spam detection approach is similar with respect to penalizing bad pages,but we build our set of malicious candidates much faster, by identifying abnormal2Recall the definition of PageRank from Equation 2.1 PRp  c qIpPRqOq 1cV  .3httpen.efactory.deepr0.shtml60Paul  Alexandru Chiritalink structures, instead of analyzing the distribution of PageRank scores for theinlinking pages of each page.Wu and Davison 216 first mark a set of pages as bad, if the domains of n of theiroutlinks match the domains of n of their inlinks i.e., they count the number ofdomains that link to and are linked by that page. Then, they extend this setwith all pages pointing to at least m pages in the former set, and remove all linksbetween pages marked as bad. In the end, new rankings are computed using thecleaned transition probability matrix. Their algorithm is not applicable for ourEmail approach, as it is quite common for a group of persons to exchange Emailswithout forming a malicious collective. However, it is complementary to our Webspam detection scheme, as it operates at the lower level of Web pages, instead ofsites. In 217, the same authors build bipartite graphs of documents and theircomplete hyperlinks4 in order to find link farms of pages sharing both anchortext and link targets i.e., possibly automatically created duplicate links. Again,the algorithm makes a good complement for our Web scenario.Finally, Becchetti et al. 23 compute Web page attributes by applying rank propagation and probabilistic link counting over the Web graph. They are thus able toestimate the number of supporters of each node in a graph. More interesting, theyshow how to truncate this value to only consider neighbors at a distance higherthan d, which consequently enables the computation of PageRank without anylink cycles of length smaller than d, most of which are usually artificially created,especially with very small values for d. The performance of this approach wasthen compared with many other Web spam classifiers in 22, reaching at most80.4 accuracy when the available indicators were combined.Other Approaches. While most Web spam detection research has concentrateddirectly on the link analysis algorithms used within current search engines, anothersignificant stream of activity was dedicated to designing innovative third partysolutions to detect such unwanted hyperlinks. These are specifically tailored to thecharacteristics of the Web content, and thus applicable only in this environment.Kumar et al. 149 used bipartite graphs to identify Web communities and markedas nepotistic those communities having several fans i.e., pages contributing to thecore of the bipartite graph with their outlinks residing on the same site. Robertsand Rosenthal 184 analyzed the number of Web clusters pointing to each targetpage in order to decrease the influence of TKCs. They proposed several methodsto approximate these clusters, but they evaluated their approach only minimally.A rather different technique was employed in 10, where the authors presenteda decisionrule classifier employing 16 connectivity features e.g., average level ofpage in the site tree, etc. to detect Web site functionality. They claimed to have4Hyperlinks having the anchor text attached to them.61Chapter 4. Ranking for Spam Detection.successfully used it to identify link spam rings as well, but no details are givenabout the importance of each feature for accomplishing this task.Chakrabarti 45 proposed a finer grained model of the Web, in which pages arerepresented by their Document Object Models, with the resulted DOM trees beinginterconnected by regular hyperlinks. The method is able to counter nepotisticclique attacks, but needs more input data than our Web algorithms, which arebased exclusively on link analysis. Also, we are able to identify a lager group oflink anomaly types.Fetterly et al. 99 used statistical measures to identify potential spam pages. Mostof the features they analyzed can be modeled by well known distributions, thusplacing outliers in the position of potential spammers. After a manual inspection,the vast majority of them seemed to be spammers indeed. A related techniqueto detect spam pages is based on machine learning algorithms Davison 79 usedthem on several features of URLs e.g., similar titles, domains, etc. in order toidentify nepotistic links on the Web.Finally, note that there exist also several types of noisy hyperlinks, which arenot necessarily spam. The most common one is due to mirror hosts and can beeliminated using algorithms such as those proposed by Broder et al. 35 or Bharatet al. 26. Also, navigational links are intended to facilitate browsing, rather thanexpressing votes of trust. One work indirectly related to this type of links is 91,where the authors defined Web documents as a cohesive presentation of thoughton a unifying subject and proposed using these entities for information retrieval,instead of the regular Web pages. Their work is however orthogonal to ours, asthey seek to identify the correct Web entities, whereas we propose solutions toremove spam items i.e., links and nodes from search engine databases.4.3 Ranking for Email Spam DetectionWe now turn our attention to the first and most common form of spam Emailspam. While scientific collaboration without Email is almost unthinkable, thetremendous increase of spam over the past years 108 has rendered Email communication without spam filtering almost impossible. Currently, spam Emailsalready outnumber nonspam ones, socalled ham Emails. Existing spam filterssuch as SpamAssassin5, SpamBouncer6, or Mozilla Junk Mail Control7 still exhibita number of problems, which can be classified in two main categories5httpspamassassin.apache.org6httpwww.spambouncer.org7httpwww.mozilla.orgstart1.5extrausingjunkcontrol.html62Paul  Alexandru Chirita1. Maintenance, for both the initialization and the adaptation of the filterduring operation, since all spam filters rely on a certain amount of inputdata to be maintained Contentbased filters require keywords and rules forspam recognition, blacklists have to be populated with IP addresses fromknown spammers, and Bayesian filters need a training set of spam  hammessages. This input data has to be created when the filter is used first thecoldstart problem, and it also has to be adapted continuously to counterthe attacks of spammers 113, 215.2. Residual error rates, since current spam filters cannot eliminate the spamproblem completely. First, a nonnegligible number of spam Emails stillreaches the end user, socalled false negatives. Second, some ham messagesare discarded because the antispam system considers them as spam. Suchfalse positives are especially annoying if the sender of the Email is fromthe recipients social community and thus already known to the user, or atleast known by somebody else the user knows directly. Therefore, there isa high probability that an Email received from somebody within the socialnetwork of the receiver is a ham message. This implies that a social networkformed by Email communication can be used as a strong foundation forspam detection.Even if there existed a perfect antispam system, an additional problem wouldarise for highvolume Email users, some of which simply get too many ham Emails.In these cases, an automated support for Email ranking would be highly desirable.Reputation algorithms are useful in this scenario, because they provide a ratingfor each Email address, which can subsequently be used to sort incoming Emails.Such ratings can be gained in two ways, globally or personally. The main ideaof a global scheme is that people share their personal ratings such that a singleglobal reputation can be inferred for each Email address. The implementation ofsuch a scheme can, for example, be based on network reputation algorithms 112see also Section 4.2.2, or on collaborative filtering techniques 180. In case ofa personalized scheme, the output ratings are typically different for each Emailuser and depend on her personal social network. Such a scheme is reasonable sincesome people with a presumably high global reputation e.g., Linus Torvalds mightnot be very important in the personal context of a user, compared to other personse.g., the project manager.This section proposes MailRank, a new approach to ranking and classifying Emailsexploiting the social network derived from each users communication circle 31.We introduce two MailRank variants, both applying a poweriteration algorithmon the Email network graph Basic MailRank results in a global reputation foreach known Email address, and Personalized MailRank computes personalized63Chapter 4. Ranking for Spam Detection.values reflecting the point of view of each user. After having discussed the particularities of each approach, the second part of the section analyzes the performanceof MailRank under several scenarios, including sparse networks, and shows its resilience against spammer attacks.4.3.1 The MailRank AlgorithmBootstrapping the Email NetworkAs for all reputation algorithms, MailRank needs to start from collecting as manypersonal votes as possible in order to compute relevant ratings. Generally, thisinput gathering process should require few or no manual user interactions in orderto achieve a high acceptance of the system. Also, the maintenance should requirelittle or no effort at all, thus having the rating of each Email address computedautomatically. To achieve these goals, we use already existing data inferred fromthe communication dynamics, i.e., who has exchanged Emails with whom. Wedistinguish three information sources as best serving our purposes1. Email Address Books. If A has the addresses B1, B2, ..., Bn in its AddressBook, then A can be considered to trust them all, or to vote for them.2. The To Fields of outgoing Emails i.e., To, Cc and Bcc. If Asends Emails to B, then it can be regarded as trusting B, or voting for B.This input data is typically very clean since it is manually selected, whilebeing more accurate than data from address books, which might compriseold or outdated information.3. Autowhitelists created by antispam tools e.g., SpamAssassin contain alist of all Email addresses from which Emails have been received recently,plus one score for each Email address which determines if mainly spam orham Emails have been received from the associated Email address. AllEmail addresses with a high score can be regarded as being trusted.Figure 4.1 depicts an example Email network graph. Node U1 represents the Emailaddress of U1, node U2 the Email address of U2, and so on. U1 has sent Emails toU2, U4, and U3 U2 has sent Emails to U1 and U4, etc. These communication actsare interpreted as trust votes, e.g., from U1 towards U2, U4 and U3, and depicted inthe figure using arrows. Building upon such an Email network graph, we can usea power iteration algorithm to compute a reputation for each Email address. Thiscan subsequently be used for at least two purposes, namely 1 Classificationinto spam and ham Emails, and 2 building a ranking among the remaining hamEmails. Note that it is not necessary for all Email users to participate in MailRank64Paul  Alexandru Chiritain order to benefit from it For example, U3 does not specify any vote, but stillreceives a vote from U1, thus consequently achieving a reputation score.Figure 4.1 Sample Email networkThe following subsections provide more information about how the Email reputation scores are generated, first in a global, and then in a personalized perspective.In the end, we briefly sketch the architecture of the system we built to implementMailRank.Basic MailRankThe main goal of MailRank is to assign a score to each Email address known tothe system and to use this score 1 to decide whether each Email is coming froma spammer or not, and 2 to build up a ranking among the filtered nonspamEmails. Its basic version comprises two main steps1. Determine a set of Email addresses with a very high reputation in the socialnetwork.2. Run PageRank 172 on the Email network graph, biased on the above set.It is highly important for the biasing set not to include any spammer. Biasing isa very efficient way to counter malicious collectives trying to attack the system116, 135. It can be accomplished in three ways manually, automatically, orsemiautomatically. The manual approach guarantees that no spammers are inthe biasing set and provides 100 effectiveness. An automatic selection avoids thecosts for the manual selection, but is also errorprone. Finally, a semiautomaticselection starts with the automatic method to generate a biasing set, which isthen verified manually to be free of spammers. In our system we take the fullyautomatic approach, as follows We first determine the size p of the biasing setby adding the scores of the R nodes with the highest rank such that the resultingsum is equal to 20 of all scores in the system. We additionally limit p to the65Chapter 4. Ranking for Spam Detection.minimum between R and 0.25 of the total number of Email addresses8. In thisway we limit the biasing set to the few most reputable members of the socialnetwork, which make the top extremity of the powerlaw distribution of Emailcommunication links 88, 125.Algorithm 4.3.1.1. The Basic MailRank Algorithm.Client SideEach vote sent to the MailRank server comprisesAddru  The hashed version of the Email address of the voter u. Hashingis necessary in order to ensure privacy for the users participatingin the social ranking system.TrustVotesu  Hashed version of all Email addressesu votes for i.e., she has sent an Email to.Server Side1 Combine all received data into a global Email network graph. LetT be the Markov chain transition probability matrix, computed asFor each known Email address iIf i is a registered address, i.e., user i has submitted her votesFor each trust vote from i to jTji  1NumOfVotesiElse For each known address jTji  1N , where N is the number of known addresses.2 Determine the biasing set B i.e., the most popular Email addr.2a Manual selection or2b Automatic selection or2c Semiautomatic selection3 Let T   c  T  1 c  E, with c  0.85 andEi   1B N1, if i  B, or Ei  0N1, otherwise4 Initialize the vector of scores x  1N N1, and the error  5 While   ,  being the precision thresholdx  T   x  x  x6 Output x, the global MailRank vector.7 Classify each Email address in the MailRank network intospammer  nonspammer based on the threshold T .8Both values, the 20 and the 0.25 have been determined in extensive tuning simulations.66Paul  Alexandru ChiritaThe final vector of MailRank scores can be used to tag an incoming Email as 1nonspammer, if the score of the sender address is larger than a threshold T , 2spammer, if that score is smaller than T , or 3 unknown, if the Email address isnot yet known to the system9. Each user can adjust T according to her preferredfiltering level. If T  0, the algorithm is effectively used to compute the transitiveclosure of the Email network graph starting from the biasing set. This is sufficientto detect all those spammers for which no user reachable from the biasing set hasissued a vote. With T  0, it becomes possible to detect spammers even if somenonspammers vote for spammers e.g., because the computer of a nonspammeris infected by a virus. However, in this case some nonspammers with a very lowrank are at risk of being counted as spammers as well.The Basic MailRank algorithm is summarized in Algorithm 4.3.1.1.MailRank with PersonalizationAs shown in Section 4.3.2, Basic MailRank performs very well in spam detection,while being highly resistant against spammer attacks. However, it still has thelimitation of being too general with respect to user ranking. More specifically, itdoes not address that Users generally communicate with persons ranked average with respect tothe overall rankings. Users prefer to have their acquaintances ranked higher than other unknownusers, even if these latter ones achieve a higher overall reputation from thenetwork. There should be a clear difference between a users communication partners.Personalizing on each users acquaintances tackles these aspects. Its main effectis boosting the weight of users votes, while decreasing this influence for all theother votes. Thus, the direct communication partners will achieve much higherranks, even though initially they were not among the highest ones. Moreover, dueto the rank propagation, their votes will have a high influence as well.Now that we have captured the user requirements mentioned, we should also focusour attention on a final design issue of our system scalability. Simply biasingMailRank on users acquaintances will not scale well, because it must be computedfor each preference set, that is for every registered user. Jeh and Widom 131 have9To allow new, unknown users to participate in MailRank, an automatically generatedEmail could be sent to the unknown user encouraging her to join MailRank challengeresponsescheme, thus bringing her into the nonspammer area of reputation scores.67Chapter 4. Ranking for Spam Detection.proposed an approach to calculate Personalized PageRank vectors, which can alsobe adapted to our scenario, and which can be used with millions of subscribers. Toachieve scalability, the resulting personalized vectors are divided in two parts onecommon to all users, precomputed and stored offline called partial vectors,and one which captures the specifics of each preference set, generated at runtimecalled hubs skeleton. We will have to define a restricted set of users on whichrankings can be biased though we shall call this set hub set, and note it withH10. There is one partial vector and one hub skeleton for each user from H.Once an additional regular user registers, her personalized ranking vector will begenerated by reading the already precomputed partial vectors corresponding toher preference set step 1, by calculating their hubs skeleton step 2, and finallyby tying these two parts together step 3. Both the algorithm from step 1 calledSelective Expansion and the one from step 2 named Repeated Squaringcan be mathematically reduced to biased PageRank. The latter decreases thecomputation error much faster along the iterations and is thus more efficient, butworks only with the output of the former one as input. In the final phase, thetwo subvectors resulted from the previous steps are combined into a global one.The algorithm is depicted in the following lines. To make it clearer, we have alsocollected the most important definitions it relies on in Table 4.1.Term DescriptionSet V The set of all users.Hub Set H A subset of users.Preference Set P Set of users on which to personalize.Preference Vector p Preference set with weights.Personalized PageRank Vector PPV Importance distribution induced by a preference vector.Basis Vector ru PPV for a preference vector with a single nonzero entry at u.Hub Vector ru Basis vector for a hub user u  H.Partial Vector ru  rHu Used with the hubs skeleton to construct a hub vector.Hubs Skeleton ruH Used with partial vectors to construct a hub vector.Table 4.1 Terms specific to Personalized MailRank.10Note that an improved version of this algorithm has been proposed recently by Sarlos et al.190, thus eliminating the limitation on the size of the biasing set.68Paul  Alexandru ChiritaAlgorithm 4.3.2.2. Personalized MailRank.0 Initializations Let u be a user from H, for which we compute the partialvector and the hubs skeleton. Also, let Du be the approximation of the basisvector corresponding to user u, and Eu the error of its computation.Initialize D0u withD0uq c  0.15 , q  H0 , otherwiseInitialize E0u withE0uq 1 , q  H0 , otherwise1 Selective Expansion Compute the partial vectors usingQ0u  V and Qku  V H, for k  0, in the formulas belowDk1u  Dku qQku c  Ekuq  xqEk1u  Eku qQkuEkuqxqqQku1cOqOqi1 Ekuq  xOiqUnder this choice, Dku  c  Eku will converge to ru  rHu ,the partial vector corresponding to u.2 Repeated squaring Having the results from the first step as input,one can now compute the hubs skeleton ruH. This is represented bythe final Du vectors, calculated using Qku  H intoD2ku  Dku qQkuEkuq DkqE2ku  EkuqQkuEkuq  xq qQkuEkuq  EkqAs this step refers to hubusers only, the computation of D2ku and E2kushould consider only the components regarding users from H,as it significantly decreases the computation time.3 Let p  1u1  . . . zuz be a preferenced vector,where ui are from H and i is between 1 and z, and letrph zi1 iruih c  xpih, h  Hwhich can be computed from the hubs skeleton.The PPV v for p can then be constructed asv zi1irui  rHui1chH rph0 rph ru  rHu  c  xh69Chapter 4. Ranking for Spam Detection.MailRank System ArchitectureMailRank is composed of a server, which collects all user votes and delivers ascore for any known Email address, and an Email proxy on the client side, whichinteracts with the MailRank server.The MailRank Server collects the input data i.e., the votes from all users torun the MailRank algorithm. Votes are assigned with a lifetime for 1 Identifyingand deleting Email addresses which have not been used for a long time, and 2Detecting spammers which behave good for some time to get a high rank andstart to send spam Emails afterwards.The MailRank Proxy resides between users Email client and her regular localEmail server. It performs two tasks When receiving an outgoing Email, it firstextracts the users votes from the available input data e.g., by listening to ongoingEmail activities or by analyzing existing sentmail folders. Then, it sends thevotes to the MailRank server and forwards the Email to the local Email server.To increase efficiency, only those votes that have not been submitted yet or thatwould expire otherwise are sent. Also, for privacy reasons, votes are encodedusing hashed versions of Email addresses. Upon receiving an Email, the proxyqueries the MailRank server about the ranking of the sender address if not cachedlocally and classifies  ranks the Email accordingly.Note that one could also make use of secure signing schemes to enable analyzingboth outgoing and incoming Emails for extracting votes11. This helps not onlyto bootstrap the system initially, but also introduces the votes of spammers intoMailRank. Such votes have a very positive aspect, since they increase the score forthe spam recipients i.e., nonspammers. Thus, spammers face more difficultiesto attack the system in order to increase their own rank.4.3.2 ExperimentsExperimental SetupRealworld data about Email networks is almost unavailable because of privacyreasons. Yet some small studies do exist, using data gathered from the log filesof a student Email server 88, or of a comany wide server 125, etc. In all cases,the analyzed Email network graph exhibits a powerlaw distribution of ingoingexponent 1.49 and outgoing exponent 1.81 links.11Analyzing incoming votes raises more security issues since we need to ensure that the senderdid indeed vote for the recipient, i.e., the Email is not faked. This can be achieved by extendingcurrent sender authentication solutions.70Paul  Alexandru ChiritaTo be able to vary certain parameters such as the number of spammers, weevaluated MailRank12 using an extensive set of simulations, based on a powerlawmodel of an Email network, following the characteristics presented in the abovementioned literature studies. Additionally, we used an exponential cutoff at bothtails to ensure that a node has at least five and at most 1500 links to other nodes,which reflects the nature of true social contacts 125. If not noted otherwise, thegraph consisted of 100,000 nonspammers13 and the threshold T was set to 0. Ina scenario without virus infections, this is sufficient to detect spammers and toensure that nonspammers are not falsely classified. Furthermore, we repeatedall simulations for at least three times with different randomly generated Emailnetworks to determine average values. Our experiments focused on three aspectsEffectiveness in case of very sparse MailRank networks i.e., only few nodes submitvotes, the others only receive votes, exploitation of spam characteristics, andattacks on MailRank.Very Sparse MailRank NetworksIn sparse MailRank networks, a certain amount of Email addresses only receivevotes, but do not provide any because their owners do not participate in MailRank.In this case, some nonspammers in the graph could be regarded as spammers,since they achieve a very low score.To simulate sparse MailRank networks, we created a full graph as described aboveand subsequently deleted the votes of a certain set of Email addresses. We usedseveral removal models All Votes can be deleted from all nodes. Bottom99.9 Nodes from the top 0.1 are protected from vote deletion. Avg Nodes having more than the average number of outgoing links areprotected from vote deletion.The first model is rather theoretical, as we expect the highlyconnected nonspammers to register with the system first14. Therefore, we protected the votes ofthe top nodes in the other two methods from being deleted15. Figure 4.2 depicts12As personalization brings a significant improvement only in creating userspecific rankingsof Email addresses i.e., it produces only minimal improvements for spam detection, we usedonly Basic MailRank within the analysis.13We also simulated using 10,000 and 1,000,000 nonspammers and obtained very similarresults.14Such behavior was also observed in reallife systems, e.g., in the Gnutella P2P networkhttpwww.gnutella.com.15The 100 from Bottom99.9 and avg actually refer to 100 of the nonprotected nodes.71Chapter 4. Ranking for Spam Detection. 0 10 20 30 40 50 60 70 80 90 100 0  20  40  60  80  100Nonspammers considered as spammers Nodes with deleted outlinks Number of Nonspammers 100000Bottom99.9RandomAvgFigure 4.2 Very sparse MailRank networks.the average percentage of nonspammers regarded as spammers, depending on thepercentage of nodes with deleted votes. Nonspammers registered to the systemwill be classified as spammers only when very few, nonreputable MailRank userssend them Emails. As studies have shown that people usually exchange Emailswith at least five partners, such a scenario is rather theoretical. However, as thepowerlaw distribution of Email communication is expected only after the systemhas run for a while, we intentionally allowed such temporary anomalies in thegraph. Even though for high deletion rates 7090 they resulted in some nonspammers being classified as spammers, MailRank still performed well, especiallyin the more realistic avg scenario the bigger error observed in the theoreticalRandom scenario was expected, since random removal may result in the deletionof highrank nodes contributing many links to the social network.Exploitation of Spam CharacteristicsIf we monitor current spammer activities i.e., sending Emails to nonspammers,the Emails from spammers towards nonspammers can be introduced into thesystem as well. This way, spammers actually contribute to improve the spamdetection capabilities of MailRank The more new spammer Email addresses andEmails are introduced into the MailRank network, the higher they increase thescore of the receiving nonspammers. This can be seen in a set of simulationswith 20,000 nonspammer addresses and a varying number of spammers up to100,000, as depicted in Figure 4.3, where the rank of the top 0.25 nonspammers72Paul  Alexandru Chirita 5000 10000 15000 20000 25000 30000 0  10  20  30  40  50  60  70  80  90  100Cumulative rank of the top 0.25 nonspammersNumber of spammers 1000Number of Nonspammers 20000Figure 4.3 Rank increase of nonspammer addresses.increases linearly with the number of spammer addresses included in the MailRankgraph.Attacking MailRankIn order to be able to attack MailRank, spammers must receive votes from otherMailRank users to increase their rank. As long as nobody votes for spammers,they will achieve a minimal score and will thus be easily detected. This leavesonly two ways of attacks formation of malicious collectives and virus infections.Malicious collectives. The goal of a malicious collective is to aggregate enoughscore into one node to push it into the biasing set. If no manually selected biasingset can be used to prevent this, one of the already many techniques to identifyWeb link farms could be employed see for example 216, 44. Furthermore, werequire MailRank users willing to submit their votes to manually register theirEmail addresses. This impedes spammers to automatically register millions ofEmail addresses in MailRank and also increases the cost of forming a maliciouscollective. To actually determine the cost of such a manual registration, we havesimulated a set of malicious users as shown in Figure 4.4. The resulting position ofnode 1, the node that should be pushed into the biasing set, is depicted in Figure4.5 for an Email network of 20,000 nonspammers, malicious collectives of 1000nodes each, and an increasing number of collectives on the xaxis. When there arefew largescale spammer collectives, the system could be relatively easy attacked.73Chapter 4. Ranking for Spam Detection.1 0N2 3Figure 4.4 Malicious collective nodes 2N vote for node 1 to increase the rankof node 1 and node 1 itself votes for node 0, the Email address that is finally usedfor sending spam Emails. 0 50 100 150 200 250 300 350 400 450 500 0  100  200  300  400  500  600  700Rank Number of collectives Using malicious collectives to push spammer into biasing setRank of the highest spammerSize of the biasing setFigure 4.5 Automatic creation of the biasing set.However, as users must manually register to the system, forming a collective ofsufficient size is practically infeasible. Moreover, in a real scenario there will bemore than one malicious collective, in which case pushing a node into the biasingset is almost impossible As shown in Figure 4.5, it becomes more difficult for amalicious collective to push one node into the biasing set, the more collectives existin the network. This is because the spammers registered to the system implicitlyvote for the nonspammers upon sending them spam Emails. This way, the rankof the best spammer increases, i.e., it achieves a lower reputation throughout thenetwork, and thus it has lower chances of being accepted into the biasing set.Virus infections. Another possible attack is to make nonspammers vote forspammers. To counter incidental votes for spammers e.g., because of a misconfigured vacation daemon, an additional confirmation process could be required if74Paul  Alexandru Chirita 0 5000 10000 15000 20000 25000 0  10  20  30  40  50  60  70  80  90  100Position on rank list   of nonspammers of email addresses voting for spammersEvaluation of virus attack 20000 nonspammer, 10000 spammerHighest position of spammerNumber of nonspammers with rank  20000Figure 4.6 Simulation results Virus attack.a vote for one particular Email address would move that address from spammerto nonspammer. However, spammers could still pay nonspammers to send spamon their behalf. Such an attack can be successful initially, but the rank of thenonspammer addresses will decrease after some time to those of spammers, dueto the limited life time of votes. Finally, one could use virus  worm technologyto infect nonspammers and make them vote for spammers. We simulated suchan attack according to Newmans studies 168, which showed that when the 10most connected members of a social network are not immunized e.g., using antivirus applications worms would spread too fast. The results are shown in Figure4.6 with a varying amount of nonspammers voting for 50 of all spammers. Ifup to about 25 of the nonspammers are infected and vote for spammers, thereis still a significant difference between the ranks of nonspammers and spammers,and no spammer manages to get a higher rank than the nonspammers. If morethan 25 nonspammers are infected, the spammer with the highest rank startsto move up in the rank list the upper line from Figure 4.6 descends towards rank1. Along with this, there will be no clear separation between spammers andnonspammers, and two threshold values must be employed one MailRank scoreT1 above which all users are considered nonspammers and another one T2  T1beneath which all are considered spammers, the members having a score withinT1, T2 being classified as unknown.75Chapter 4. Ranking for Spam Detection.4.3.3 DiscussionEmail spam detection remains a serious problem for PC users. Many approacheshave been developed, yet each time spammers came up with new methods tobypass Email filtering. In this section we proposed MailRank, a new Email rankingand classification scheme, which intelligently exploits the social communicationnetwork created via Email interactions. On the resulting Email network graph,input values are collected from the sentmail folder of all participants, as well asfrom other sources, and a poweriteration algorithm is used to rank trustworthysenders and to detect spammers. MailRank brings the following advantages uponprevious spam filtering techniques Shorter individual coldstart phase. If a MailRank user does not knowan Email address X, MailRank can provide a rank for X as long as atleast another MailRank user has provided information about it. Thus, thesocalled coldstart phase, i.e., the time a system has to learn until itbecomes functional, is reduced While most successful antispam approachese.g., Bayesian filters have to be trained for each single user in case of anindividual filter or a group of users for example, in case of a companywide filter, MailRank requires only a single global cold start phase whenthe system is bootstrapped. In this sense it is similar to globally managedwhitelists, but it requires less administrative efforts to manage the list andit can additionally provide information about how good an Email address is,and not only a classification into good or bad. High attack resilience. MailRank is based on a power iteration algorithm,which is typically resistant against attacks. Partial participation. Building on the powerlaw nature of Email networks, MailRank can compute a rank for a high number of Email addresseseven if only a subset of Email users actively participates in the system. Stable results. Social networks are typically rather stable, so the computedratings of the Email addresses will usually also change slowly over time.Hence, spammers need to behave well for quite some time to achieve a highrank. Though this cannot resolve the spam problem entirely in the worstcase, a spammer could, for example, buy Email addresses from people whohave behaved well for some time, it will increase the cost for using newEmail addresses. Can reduce load on Email servers. Email servers do not have to processthe Email body to detect spam. This significantly reduces the computational power for spam detection compared to, for example, contentbasedapproaches or collaborative filters 146.76Paul  Alexandru Chirita Personalization. In contrast to spam classification approaches that distinguish only between spam and nonspam, ranking also enables personalization features. This is important since there are certain Email addressese.g., newsletters, which some people consider to be spammers while others do not. To deal with such cases, a MailRank user can herself decideabout the score threshold below which all Email addresses are consideredspammers. Moreover, she could use two thresholds to determine spammers,nonspammers, and unclear classifications. Furthermore, she might wantto give more importance to her relatives or to her manager, than to otherunrelated persons with a globally high reputation. Scalable computation. Power iteration algorithms have been shown tobe computationally feasible even when personalized over very large graphs190. Can also counter other forms of spam. When receiving spam phonecalls SPIT16, for example, it is impossible to analyze the content of the callbefore accepting  rejecting it. At best only the caller identifier is available,which is similar to the sender Email address. MailRank can be used toanalyze the caller ID and decide whether a caller is a spammer or not.Our experiments showed MailRank to perform well in the presence of very sparsenetworks Even in case of a low participation rate, it can effectively distinguishbetween spammer Email addresses and nonspammer ones, even for those usersnot participating actively in the system. MailRank proved itself to be also veryresistant against spammer attacks and, in fact, has the property that when morespammer Email addresses are introduced into the system, the spam detectionperformance increases.In the future, one could move the implementation from a centralized system toa distributed one in order to allow for more scalability and to avoid bottlenecks.From the algorithmic perspective, some of the already existing Web antispamapproaches could be built on top of MailRank, so as to ensure an increased attackresistance of the system.4.4 Ranking for Web Spam DetectionAlthough Email spam is already widely present in our lives, another industry isemerging at an even faster pace Web Search Engine Optimization, shortly SEO115. Given the increasing importance of search engines in modern society, manyonline organizations currently attempt to artificially increase their rank, since16Spam over Internet Telephony, httpwww.infoworld.comarticle040907HNspamspit 1.html77Chapter 4. Ranking for Spam Detection.a higher rank implies more users visiting their Web pages, which subsequentlyimplies an increased profit. This results in a strong negative impact upon theoutput of our everyday Web searches, making the high quality pages harder tofind and the low quality ones more accessible.Search engines adopt several different sources of evidence to rank the Web pagesmatching a user query, such as textual content, title of Web pages, anchor textinformation, or the link structure of the Web 28. Each of them is generallyattacked differently by spammers. As in the entire thesis, in this section we focusagain on link analysis, and thus tackle the latter measure, which is in fact one ofthe most useful sources of evidence adopted. To extract information from the linkstructure, search engines use algorithms that assess the quality or popularity ofWeb pages by analyzing the linkage relationships among them. The success ofthis strategy relies on the assumption that a link to a Web page represents a votefrom a user that sustains the quality of that targeted page.In spite of the success of link analysis algorithms, many artificial hyperlink structures lead these algorithms to provide wrong conclusions about the quality ofWeb pages. This phenomenon happens because links that cannot be interpretedas votes for quality sometimes negatively affect the search engine ranking results.Such links are called nepotistic links or spam links, i.e., links intentionally created to artificially boost the rank of some given set of pages, usually referred toas spam pages 216.In this section we propose a sitelevel approach for detecting generic spam links onthe Web. Previous algorithms have focused on identifying spam only by analyzingpage level relationships, which clearly misses some of the higher level information,generated between a group of sites. We investigate three main types of site levelrelationships mutual reinforcement in which many links are exchanged betweentwo sites, abnormal support where most of one sites links are pointing to thesame target site, and link alliances in which several sites create complex linkstructures that boost the PageRank score of their pages. When the relation between such sets of sites is considered suspicious, we assume that the links betweenthem are nepotistic and penalize them accordingly. Finally, it is important to notethat this new approach is complementary to the existing page level approaches,and both strategies should be adopted simultaneously for identifying spam linksin a search engine database.We will now proceed with presenting our three approaches to site level spamdetection. Then, we will continue with an extensive evaluation of these algorithms,followed by a discussion about their strengths and weaknesses, as well as aboutpossible extesions which could be built on top of them.78Paul  Alexandru Chirita4.4.1 Site Level Spam DetectionWe argue here that many spam links can be easily detected when the relationshipsbetween sites, instead of pages, are analyzed. Even though the current pagecentered approaches for detecting spam still hold and will also be needed in thefuture, they may not be the best solution to deal with many practical situations.For example, a company having two branches with different sites could easilyestablish many links between its two sites in order to boost their PageRank. Thesewould be regarded as true votes by the current ranking approaches, even thoughthey connect two entities having the same owner. Even when this would occuraccidentally in which case we are dealing with noisy links instead see alsoSection 4.2.3 for more details on noisy links, such relationships are still artificialand should not be included in the search engine ranking computation. Worse,automatically generated complex site level link spam structures may be missedby the current page level approaches. Therefore, we propose detecting spam ata site level rather than at a page level, investigating the above mentioned threetypes of artificial site level relationships. The following sections detail a separateanalysis on each of these constructs.Site Level Mutual ReinforcementOur first site level approach to detect spam links on Web collections is based onthe study of how connected are pairs of sites. Our assumption in this approachis that when two sites are strongly connected, they artificially boost their resultsin link analysis algorithms. We name this phenomenon as a site level mutualreinforcement. Mutual reinforcement relations have been tackled as early as 1998by Bharat and Henzinger 27. However, all approaches proposed so far arecentered around the Web page as a unit item. We therefore study the mutualreinforcement problem at the site level, because a considerable amount of spamlinks between these type of Web sites cannot be detected using approaches workingat the page level. We thus consider all links between strongly connected sites asspam, including links between individual pages that are not suspicious per se. Thisis because these links artificially boost the popularity rank of the pages belongingto the pair of suspicious Web sites. Let us now discuss the two different algorithmswe propose for detecting mutual site reinforcement relationships.BiDirectional Mutual Site Reinforcement BMSR. This algorithm takesinto account the number of link exchanges between pages from the two studiedsites. We say that two pages p1 and p2 have a link exchange if there is a linkfrom p1 to p2 and a link from p2 to p1. Our first method tries to identify sitepairs that have an abnormal amount of link exchanges between their pages. In79Chapter 4. Ranking for Spam Detection.Figure 4.7 Example of site level link exchanges.these cases, we consider the pair as suspicious and all the links between its sitesare considered spam. The threshold to consider a pair of sites suspicious is setthrough experiments.UniDirectional Mutual Site Reinforcement UMSR. As sites are largestructures, we also investigate the possibility of relaxing the notion of link exchange into link density, i.e., counting all links between two sites, disregardingtheir orientation. This ensures capturing sites attempting to boost the ranks oftheir pages without necessarily constructing link exchanges. One of the manypossibilities to achieve this goal is depicted on the upper side of Figure 4.7, usingcycles. The connection between two sites may create a set of cycles on the Webgraph containing pages from both of them. It is known that such cycle structuresboost the popularity of Web pages 114, and since many cycles can arise fromstrongly connected sites, such alliances between sites create anomalies in the finalPageRank.And yet this measure might be too drastic For instance, many pages of a sitemight have a link to Yahoo Search just because they think this is a good service.Since all links between the two sites are counted, it does not matter if Yahoodoes not link back to the above mentioned site. We therefore propose a morecomprehensive measure, which returns the minimum between the amount of linksfrom a site s to some site s, and the amount of links coming back from s to s.We call this mutual link density.On the example from Figure 4.7, there are 3 link exchanges between sites s and sand the link density is 9 link exchanges are also counted. In order to calculatethese values, one needs to iterate over all pages, and for each page to incrementthe site level statistics every time a link exchange is found see Algorithm 4.4.2.1below, lines 58, for BMSR, or simply every time a link is encountered Algorithm4.4.2.1, lines 56, and 9, for UMSR. Note that Algorithm 4.4.2.1 computes thelink density as a measure of UMSR. In order to obtain the mutual link density,one would have to calculate UMSRs, s and UMSRs, s separately, and thenreturn their minimum as a result.80Paul  Alexandru ChiritaAlgorithm 4.4.2.1. Detecting Link Exchanges at Site Level.1 Let BMSRs, s and UMSRs, s denote the amount of link exchangesand the link density between sites s and s respectively.2 For each site s3 For each site s 6 s4 BMSRs, s  UMSRs, s  05 For each page p  V , p residing on site s6 For each page q  Outp, q from site s 6 s7 If p  Outq8 Then BMSRs, s  BMSRs, s  19 UMSRs, s  UMSRs, s  UMSRs, s  1Computing Page Ranks. Let us now see how we could use these measures toimprove PageRank quality. An approach is depicted in Algorithm 4.4.2.2, whichremoves all links between all pairs of sites s, s, if the BMSR or UMSR valuesbetween them are above a certain threshold. In our experiments, we used 10,20, 50, 100, 250 and 300 for link density 250 being best, yet still with poorperformance, and 2, 3 and 4 for link exchanges with 2 having better results,indicating that most sites exchange incorrect votes, or links, with only a fewpartners, like a company with its branches.Algorithm 4.4.2.2. Removing SiteLevel Link Exchanges.1 For each site s2 For each site s3 If MSRs, s  MSR4 Then Remove all links between s and s5 Compute regular PageRank.Site Level Abnormal SupportAnother type of situation we consider is the site level abnormal supportSLAbS.It occurs when a single site is responsible for a high percentage of the totalamount of links pointing to another site. This situation can easily arise withina Web collection. For instance, and unfortunately, once the spammers have readthe previous section, they could start to seek for new schemes that circumvent81Chapter 4. Ranking for Spam Detection.Figure 4.8 Example of site chains.the algorithms we presented. A relatively simple approach they could take is tocreate chains of sites supporting each other through a limited number of linkssee Figure 4.8 for an example. This is because their space of available choicesis diminishing Using too many links would make them detectable by our sitelevel mutual reinforcement algorithms above, while using other structures thanchains e.g., hierarchy of sites would visibly make their success more costly. Wetherefore propose the following axiomAxiom 1 The total amount of links to a site i.e., the sum of links to its pagesshould not be strongly influenced by the links it receives from some other site.In other words, for any site s there should not be a site s 6 s, whose numberof links towards s is above a certain percentage of the total number of links sreceives overall. In our experiments we tested with thresholds ranging from 0.5up to 20 of the total number of links to s and the best results were achievedat 2. Whenever such a pair of sites s, s is found, all links between them aremarked as spam. Note that links from s to s are also taken as spam becausewe consider the relation between them suspicious. After this trimming process isover, we remove the detected spam links and the regular PageRank is run overthe cleaned link database. The approach is summarized in Algorithm 4.4.2.3.Algorithm 4.4.2.3. Removing SiteLevel Abnormal SupportSLAbS.1 For each site s2 Let t be the total number of links to pages of s3 For each site s that links to s4 Let ts,s be the number of links from s to s, and Let supp  ts,st5 If supp  AS6 Then Remove all links between s and s7 Compute regular PageRank.82Paul  Alexandru ChiritaSite Level Link AlliancesAnother hypothesis we considered is that the popularity of a site cannot besupported only by a group of strongly connected sites. The intuition behind thisidea is that a Web site is as popular as diverse and independent are the sites thatlink to it. In fact, as we will see from the experiments section, our algorithm whichdetects and considers this concept of independence when computing PageRankgives a strong improvement in the overall quality of the final rankings.Further, continuing the scenario discussed in the previous Section, suppose spammers do have enough resources available to build complex hierarchies of sites thatsupport an end target site, as illustrated in Figure 4.9. These hierarchies havepreviously been named Link Spam Alliances by Gyogyi and GarciaMolina 114,but they did not present any solution to counteract them. Such structures wouldgenerate sites linked by a strongly connected community, thus contradicting ourgeneral hypothesis about the relation between diversity of sites that link to a siteand its actual popularity.Before discussing our approach, we should note that we do not address page levellink alliances, i.e., hierarchies of pages meant to support an end target page, allpages residing on the same site, or on very few sites. These types of structurescould be easily annihilated for example by using different weights for intrasiteand intersite links, or by implementing the approach presented by Bharat andHenzinger in 27, where every inlink of some page p is assigned the weight 1k ifthere are k pages pointing to p for link alliances distributed over several sites.The more complicated situation is to find link alliances intentional or not overseveral sites, as the one depicted in Figure 4.9 boxes represent sites. Ourintuition is that these alliances would still have to consist of highly interconnectedpages. More specifically, if a page p has inlinks from pages i1, i2, . . ., iI , andthese latter pages are highly connected, then they are suspect of being part ofFigure 4.9 Example of link alliance spanning over several sites.83Chapter 4. Ranking for Spam Detection.a structure which could deceive popularity ranking algorithms. We evaluate thedegree of susceptivity using the following algorithmAlgorithm 4.4.2.4. Computing SiteLevel Link Alliance Susceptivity.1 For each page p2 Let Tot count the number of outlinks of all pages q  Inp3 Let TotIn count the number of outlinks of all pages q  Inp,such that they point to some other page from Inp4 For each page q  Inp5 For each page t  Outq6 Tot  Tot 17 If t  Inp8 Then TotIn  TotIn 19 Susceptivityp  TotInTot.Once the susceptivity levels are computed, we downgrade the inlinks of everypage p with 1 Susceptivityp, uniformly distributing the remaining votes toall pages. This latter step is necessary in order to ensure the convergence of theMarkov chain associated to the Web graph, i.e., to ensure the sum of transitionprobabilities from each state st remains 1. The entire approach is also presentedin Algorithm 4.4.2.5.Algorithm 4.4.2.5. Penalizing SiteLevel Link Alliances.1 Let PRi  1V ,i  1, 2, ..., V 2 Repeat until convergence3 For each page p4 PRp  1 Susceptivityp  c qInp PRqOutq  1cV 5 Residual  Susceptivityp  c qInp PRqOutq6 For each page p7 PRp  PRp  ResidualV 84Paul  Alexandru Chirita4.4.2 ExperimentsExperimental SetupWe evaluated our Web spam detection techniques on the link database of theTodoBR search engine17 now Google Brazil. This database consisted of acollection of 12,020,513 pages extracted from the Brazilian Web, connected by139,402,245 links. As it represents a considerably connected snapshot of theBrazilian Web community, which is probably as diverse in content and link structure as the entire Web, we think it makes a realistic testbed for our experiments.In order to evaluate the impact of our algorithms within practical situations, weextracted test queries from the TodoBR log, which is composed of 11,246,351queries previously submitted to the search engine. We divided these selectedqueries in two groups1. Bookmark queries, in which a specific Web page is sought2. Topic queries, in which people are looking for information on a given topic,instead of some page.Each query set was further divided in two subsets, as follows Popular queries  Here, we selected the top most popular bookmark  topicqueries found in the TodoBR log. These queries usually search for wellknown Web sites and are useful to check what happens to these most commonly searched pages after the spam detection methods have been applied. Randomly selected queries  In this scenario we selected the queries randomly.These queries tend to search for less popular sites and show the impact ofour techniques on pages that are probably not highly ranked by PageRank.Then, 14 undergraduate and graduate computer science students within differentareas evaluated the selected queries under various experimental settings. All ofthem were familiar with the Brazilian Web pages and sites, in order to ensuremore reliability to our experiments.The bookmark query sets contained each 50 queries, extracted using the abovementioned techniques. All bookmark query results were evaluated using MRRMean Reciprocal Ranking, which is the metric adopted for bookmark queries onthe TREC Conference18 and is computed by the following equationMRRQS qiQS1PosRelAnsqiQS4.217httpwww.todobr.com.br18httptrec.nist.gov85Chapter 4. Ranking for Spam Detection.where QS is the set of queries we experiment on, and PosRelAnsqi is theposition of the first relevant answer in the rankings output for query qi. MRR is themost common metric for evaluating the quality of results in bookmark queries. Asit can be seen, its formula prioritizes methods that obtain results closer to the topof the ranking, adopting an exponential reduction in the scores i.e., higher scoresare better, as the position of the first relevant answer in the ranking increases.Also, MRR is very good at assessing the real life performance of the searchengine, since the most important URLs are those placed at the top of the searchoutput. However, MRR is not sensible to pages having huge drops in positione.g., from place 15 to place 40. Therefore, we also adopted another measure,mean position, denoted MEANPOS in the tables to follow, which computesthe average position of the first relevant answer in the output provided for eachquery. This metric results in a linear increase in the scores higher is worse asthe position of the relevant answer increases.For topic queries, we used two sets of 30 queries also selected from the TodoBRlog as described previously. These different queries evaluate the impact of ourspam detection algorithms when searching for some given topics. In this case, weevaluated the results using the same pooling method as within the Web Collectionof TREC 121. We thus constructed query pools containing the first top 20answers for each query and algorithm. Then, we assessed our output in termsof various precision based metrics For each algorithm, we evaluated the MeanAverage Precision MAP, the precision at the first 5 positions of the resultedranking P5, as well as the precision at the top 10 output rankings P10. Inall cases the relevant results were divided in two categories, 1 relevant and 2highly relevant. Also, we processed all queries according to the user specifications,as extracted from the TodoBR log phrases, Boolean conjunctive or Booleandisjunctive. The set of documents achieved for each query was then rankedaccording to the PageRank algorithm, with and without each of our link removaltechniques applied. Finally, all our results were tested for statistical significanceusing Ttests i.e., we tested whether the improvement over PageRank withoutany links removed is statistically significant.In all forthcoming tables, we will label the algorithms we evaluated as follows ALL LINKS No spam detection. UMSR Unidirectional Mutual Site Reinforcement. BMSR Bidirectional Mutual Site Reinforcement. SLAbS Site Level Abnormal Support. SLLA Site Level Link Alliances. Combinations of the above, in which every method is applied independentlyto remove UMSR, BMSR, SLAbS or downgrade SLLA links.86Paul  Alexandru ChiritaMethod ThresholdUMSR 250BMSR 2SLAbS 2Table 4.2 Best thresholds found for each algorithm using MRR as the tuningcriterion.Algorithm specific aspects. Another important setup detail is to divide thecollection in Web sites, as the concept of Web site is rather imprecise. In ourimplementation, we adopted the host name part of the URLs as the keys foridentifying individual Web sites. This is a simple, yet very effective heuristic toidentify sites, as pages with different host names usually belong to different sites,while those with identical host names usually belong to the same site.As UMSR, BMSR and SLAbS all use thresholds to determine whether linksbetween pairs of sites are spam or not, it is important to tune such thresholds inorder to adjust the algorithms to the collection in which they are applied. For theexperiments we performed, we adopted the MRR results achieved for bookmarkqueries as the main parameter to select the best threshold. This metric wasadopted because the link information tends to have a greater impact on bookmarkqueries than on topic queries. Further, MRR can be calculated automatically,reducing the cost for tuning. The best parameters for each method depend on thedatabase, the amount of spam and the requirements of the search engine wherethey will be applied.Table 4.2 presents the best thresholds we found for each algorithm using the MRRas the tuning criteria. These parameters were adopted in all the experimentspresented.ResultsBookmark Queries. We evaluated the bookmark queries in terms of MeanReciprocal Rank MRR and Mean Position MEANPOS of the first relevantURL output by the search engine. Table 4.3 shows the MRR scores for eachalgorithm with popular bookmark queries. The best result was achieved whencombining all the spam detection methods proposed, with an improvement of26.98 in MRR when compared to PageRank. The last column shows the Ttestresults, which indicate the statistical significance of the difference in results19 for19Recall that statistical significance is not computed on the average result itself, but on eachevaluation evidence i.e., it also considers the agreement between subjects when assessing the87Chapter 4. Ranking for Spam Detection.Method MRR Gain  Signific., pvalueALL LINKS 0.3781  UMSR 0.3768 0.53 No, 0.34BMSR 0.4139 9.48 Highly, 0.008SLAbS 0.4141 9.5 Yes, 0.04SLLA 0.4241 12.14 Yes, 0.03BMSRSLAbS 0.4213 11.40 Yes, 0.02SLLABMSR 0.4394 16.20 Highly, 0.01SLLASLAbS 0.4544 20.17 Highly, 0.003SLLABMSRSLAbS 0.4802 26.98 Highly, 0.001Table 4.3 Mean Reciprocal Rank higher is better for popular bookmark queries.Method MEANPOS Gain  SignificanceALL LINKS 6.35  UMSR 6.25 1.57 No, 0.34BMSR 5.37 18.25 Yes, 0.04SLAbS 5.84 8.72 No, 0.26SLLA 5 27.06 Highly, 0.003BMSRSLAbS 5.63 12.89 Minimal, 0.12SLLABMSR 4.84 31.17 Highly, 0.01SLLASLAbS 4.68 35.86 Highly, 0.002SLLABMSRSLAbS 4.62 37.29 Yes, 0.04Table 4.4 Mean position of the first relevant result obtained for popular bookmarkqueries.each database when compared to the ALL LINKS version i.e., PageRank on theoriginal link database. The only method that had a negative impact on MRR wasthe UMSR, which indicates that many unidirectional relations between sites arerather useful for the ranking i.e., not artificial. This was also the only algorithmfor which the Ttest indicated a nonsignificant difference in the results pvalueslower than 0.25 are taken as marginally significant, lower than 0.05 are taken assignificant, and lower than 0.01 as highly significant.Table 4.4 presents the Mean Position of the first relevant result MEANPOSachieved for popular bookmark queries under each of the algorithms we proposed.The best combination remains SLLABMSRSLAbS, with a gain of 37.00.Thus, we conclude that for popular bookmark queries the combination of allresults. Thus, smaller average differences could result in a very significant result, if the differencebetween the two algorithms remains relatively constant for each subject.88Paul  Alexandru ChiritaMethod MRR Gain  Signific., pvalueALL LINKS 0.3200  UMSR 0.3018 5.68 Highly, 0.01BMSR 0.3195 0.17 No, 0.45SLAbS 0.3288 2.73 No, 0.31SLLA 0.3610 12.81 Yes, 0.04BMSRSLAbS 0.3263 2.19 No, 0.36SLLABMSR 0.3632 13.47 Yes, 0.03SLLASLAbS 0.3865 20.78 Yes, 0.017SLLABMSRSLAbS 0.3870 20.92 Yes, 0.016Table 4.5 Mean Reciprocal Rank higher is better for randomly selected bookmark queries.methods is the best spam detection solution. Also, individually, Site Level LinkAlliance SLLA produced the highest increase in PageRank quality.After having evaluated our techniques on popular bookmark queries, we testedtheir performance over the randomly selected ones. The MRR results for this scenario are displayed in Table 4.5. Again, the best outcome was achieved when combining all the spam detection methods proposed, with an improvement of 20.92in MRR when compared to PageRank. Note that an improvement is harder toachieve under this setting, since the Web pages searched in these queries are notnecessarily popular, and thus many of them may have just a few ingoing linksand consequently a low PageRank score. Therefore, as removing links at the sitelevel might also have the side effect of a further decrease of their PageRank score,they could become even more difficult to find. This is why both site level mutualreinforcement algorithms BMSR and UMSR resulted in a negative impact inthe results, indicating that some site level mutual reinforcement might not necessarily be a result of spam at least the unidirectional type of reinforcement.Similar results have been observed when computing the Mean Position of thefirst relevant result, instead of the MRR see Table 4.6. Individually, SLLA isstill the best algorithm, whereas the best technique overall is again the combinedSLLABMSRSLAbS.Topic Queries. As mentioned earlier in this section, we evaluated the topicqueries using precision at the top 5 results P5 and at the top 10 resultsP10, as well as the mean average precision MAP. We first turn our attentionto the experiment in which the output URLs assessed both as relevant and highlyrelevant are considered as good results. Table 4.7 presents the evaluation forthe most popular 30 topic queries under this scenario. All results were tested89Chapter 4. Ranking for Spam Detection.Method MEANPOS Gain  SignificanceALL LINKS 8.38  UMSR 8.61 2.71 Highly, 0.01BMSR 8.28 1.28 No, 0.27SLAbS 8.23 1.80 Minimal, 0.24SLLA 7.42 12.89 Minimal, 0.11BMSRSLAbS 8.02 4.09 No, 0.36SLLABMSR 7.27 15.21 Minimal, 0.07SLLASLAbS 7.12 17.61 Highly, 0.01SLLABMSRSLAbS 7 19.76 Highly, 0.005Table 4.6 Average mean position of the first relevant result for randomly selectedbookmark queries.for significance, and in both P5 and P10 no method manifested a significantgain or loss. Even so, in both P5 and P10 we see that BMSR has a slightgain over UMSR. SLLA exhibited the greatest gain in P5, but the results wererelatively similar for all algorithms in P10. As for MAP, most of the resultsexcept for SLAbS, BMSR, and their combination had significant gain on MAP,when compared with the original link database. Finally, SLAbS performance wasrather poor. However, this behavior of SLAbS was recorded only with this kind ofqueries, where it is also explainable Some very popular sites might indeed get anabnormal support from several of their fans some would consider this as spam, butour testers apparently preferred to have the ranks of these sites boosted towardsthe top. The best individual method was SLLA and the best combination wasSLLA with BMSR, which was better than the combination of all three methodsdue to the negative influence of SLAbS.The same experiment was then performed for the 30 randomly selected topicqueries. Its results are depicted in Table 4.8. Here, SLLA remains a very effectiveindividual algorithm, but SLAbS shows even better results. This indicates thatan abnormal support for less popular sites usually appears as a result of spam.More, due to this special behavior of our algorithms, under this setting the maincontributor to the combined measures was SLAbS, thus yielding the best MAPscore for BMSRSLAbS.Before concluding this analysis, we also measured the quality of our methodsunder the same setting, but considering only the highly relevant output URLs asgood results recall that our subjects evaluated each URL as irrelevant, relevantand highly relevant for each query. For the popular topic queries Table 4.9, theperformance of the individual methods was similar to the scenario that considered90Paul  Alexandru ChiritaMethod P5 P10 MAP Signif. for MAPALL LINKS 0.255 0.270 0.198 UMSR 0.255 0.282 0.207 Highly, 0.0031BMSR 0.260 0.285 0.198 No, 0.3258SLAbS 0.226 0.262 0.185 Minimal, 0.0926SLLA 0.275 0.270 0.227 Highly, 0.0030BMSRSLAbS 0.226 0.276 0.200 No, 0.3556SLLASLAbS 0.245 0.255 0.216 Yes, 0.0429SLLABMSR 0.270 0.273 0.231 Highly, 0.0031SLLABMSRSLAbS 0.245 0.259 0.223 Yes, 0.0129Table 4.7 Precision at the first 5 results, at the first 10 results, and Mean AveragePrecision considering all the relevance judgments for popular topic queries.Method P5 P10 MAP Signif. for MAPALL LINKS 0.412 0.433 0.311 UMSR 0.442 0.442 0.333 Highly, 0.0030BMSR 0.400 0.445 0.314 No, 0.3357SLAbS 0.436 0.458 0.340 Yes, 0.0112SLLA 0.461 0.455 0.327 Yes, 0.0125BMSRSLAbS 0.448 0.470 0.358 Highly, 0.0012SLLABMSR 0.485 0.448 0.326 Highly, 0.0006SLLASLAbS 0.461 0.461 0.354 Minimal, 0.0618SLLABMSRSLAbS 0.461 0.467 0.346 Highly, 0.0002Table 4.8 Precision at the first 5 results, at the first 10 results, and Mean AveragePrecision considering all the relevance judgments for random topic queries.both relevant and highly relevant results, with the main difference being that hereSLAbS gains about 12 over the original database, instead of losing. This isbecause the sites previously discovered due to spam or noisy links i.e., thosebeing very popular, but also abnormally supported by some fans were consideredonly relevant by our testers, and thus not included in this more strict experiment.Finally, for the randomly selected queries Table 4.10, SLAbS again showedthe best individual performance just as in the sibling experiment consideringboth kinds of relevance judgments, with the overall top scores being achieved forSLLABMSRSLAbS and BMSRSLAbS.Conclusion. In order to make our results more clear, we also plotted theirrelative gain over regular PageRank i.e., without spam detection. Figure 4.10depicts this gain in percentage for bookmark queries and Figure 4.11 depicts it91Chapter 4. Ranking for Spam Detection.Method P5 P10 MAP Signif. for MAPALL LINKS 0.152 0.141 0.112 UMSR 0.152 0.147 0.131 Highly, 0.0002BMSR 0.152 0.150 0.127 Highly, 0.0022SLAbS 0.152 0.147 0.126 Yes, 0.0172SLLA 0.162 0.153 0.163 Highly, 0.00003BMSRSLAbS 0.152 0.156 0.128 Highly, 0.0016SLLASLAbS 0.157 0.147 0.175 Highly, 0.00002SLLABMSR 0.157 0.153 0.168 Highly, 0.00005SLLABMSRSLAbS 0.157 0.150 0.179 Highly, 0.00001Table 4.9 Precision at the first 5 results, at the first 10 results, and Mean AveragePrecision considering only the highly relevant results selected by our subjects forpopular topic queries.for topic queries. We first note that UMSR yielded negative results in threeof the four experiments with bookmark queries, which makes it less preferableto its sibling BMSR, even though it performed better than the latter one withtopical queries. Also, we observe that SLAbS performed quite well under bothbroad experimental settings, but SLLA is clearly the best single approach forbookmark queries. Finally, all combined measures performed very well, withSLLABMSRSLAbS being the best one.Practical IssuesAmount of removed links. Even though the amount of removed links doesnot necessarily represent the performance increase of each algorithm, it is stillinteresting to see how much did they trim the original link structure. We thuspresent these values in Table 4.11 recall that SLLA does not remove any links,but only downgrades them. We observe that BMSR has removed a relatively lowamount of links at least when compared to the other methods, which indicatesthat SLLASLAbS could be preferred in practical implementations when fastercomputations of the algorithm are desired, at the cost of a minimally lower outputquality.Scalability. Algorithms dealing with large datasets as the Web need to have avery low complexity in order to be applied in a real environment. We argue thatall the algorithms we proposed in this section have a computational cost growthlinear in the number of pages.Both Mutual Reinforcement detection algorithms behave in a similar way,92Paul  Alexandru ChiritaMethod P5 P10 MAP Signif. for MAPALL LINKS 0.170 0.179 0.187 UMSR 0.176 0.191 0.196 Yes, 0.0457BMSR 0.170 0.185 0.195 Minimal, 0.0520SLAbS 0.182 0.191 0.201 Yes, 0.0200SLLA 0.164 0.185 0.194 No, 0.2581BMSRSLAbS 0.188 0.197 0.207 Highly, 0.0068SLLABMSR 0.182 0.194 0.205 Highly, 0.0090SLLASLAbS 0.182 0.206 0.203 Yes, 0.0180SLLABMSRSLAbS 0.200 0.212 0.208 Highly, 0.0012Table 4.10 Precision at the first 5 results, at the first 10 results, and Mean AveragePrecision considering only the highly relevant results selected by our subjects forrandom topic queries.Method Links Detected  of Total LinksUMSR 9371422 7.16BMSR 1262707 0.96SLAbS 21205419 16.20UMSRBMSR 9507985 7.26BMSRSLAbS 21802313 16.66Table 4.11 Amount of links removed by each of our algorithms.with UMSR being slightly less expensive than BMSR. The former one needsa simple pass over all links and thus has the complexity OE. If M AveragepV Outp, and if the inlinks information is present in the search enginedatabase, but with the inlinks in a random order, then the complexity of BMSRis OV  M2, with M2 being the cost of sequential searching. Furthermore, ifthe inlinks are sorted, then the complexity falls to OV  M  logM.SLAbS is very similar to UMSR. For each page p we update the statistics aboutits ingoing links. Thus, if P  AveragepV Inp, then the computationalcomplexity of SLAbS is OV   P .SLLA is based on the inlinkers of a page p that are not from the same site asp. Thus, the algorithm needs to calculate the amount of links from pages fromInp that point to other pages within Inp. If the outlinks or the inlinks arealready sorted, the complexity of this approach is OV  M2  logM. Otherwise,the complexity is OV  M3, since a sequential search is needed.Finally, we note that all algorithms we proposed in this section do a pagebypageprocessing, thus being trivially parallelizable.93Chapter 4. Ranking for Spam Detection.Figure 4.10 Relative gain in  of each algorithm in MRR and Mean Positionfor bookmark queries.4.4.3 DiscussionThere is no doubt that Search Engine Optimization is a really popular job rightnow. Just to give an example, as of writing this thesis, only in New York city therewere several thousands SEO job openings20. This effect occurred because of tworeasons First, an efficient manipulation of the search engine rankings can bringmillions of dollars into the target company Second, even though such incorrecttechniques are visibly decreasing the quality of our everyday search experience,they have not been explicitly moved outside the law. Thus, everybody can do it,as long as he is not detected  rank penalized by the search engine itself.In this section we made another step in this continuous battle of keeping seachquality at very high standards. We introduced a novel approach to remove artificial linkage patterns from search engine databases. More specifically, we proposedto utilize site level link analysis to detect such malicious constructs. We designedand evaluated algorithms tackling three types of inappropriate site level relationships 1 mutual reinforcement, 2 abnormal support and 3 link alliances.Our experiments have showed a quality improvement of 26.98 in Mean Reciprocal Rank for popular bookmark queries, 20.92 for randomly selected bookmarkqueries, and up to 59.16 in Mean Average Precision for topic queries.Another important contribution we brought relates to the generality of our methods Even though our main interest was spam detection, the algorithms we20Information aggregated from several job sites.94Paul  Alexandru ChiritaFigure 4.11 Relative gain in  in MAP for all algorithms for topic queries,considering only highly relevant results as relevant High, and considering bothrelevant and highly relevant answers as relevant All.presented in this section can in fact detect much more types of artificial links.Thus, they are able to identify also intensively interlinked company branches, orsite replicas, etc., all of which have not necessarily been intentionally deployedwith the intent of manipulating search engine rankings. All in all, about 16.7of the links from our collection were marked as noisy i.e., nonvotes, and quitea few of them could not necessarily be considered as nepotistic.While most of the Web spam detection algorithms we presented in this sectionremove the identified malicious links completely, in the future one could investigateusing different weights for various types of links, according to the relation theyrepresent i.e., intersite or intrasite relation, as well as to their probabilityof representing a vote of importance. Finally, it would also be beneficial todesign more complex eventually automatic approaches to tune up the parameterthresholds.95Chapter 4. Ranking for Spam Detection.96Chapter 5Ranking for Web SearchPersonalization5.1 IntroductionWe have seen in the previous two chapters that link analysis ranking can do agood work in many application areas for which it was previously either completelyunexplored, such as Desktop search and Email spam detection, or partially explored, such as Web spam detection. This chapter comes to propose link analysissolutions for a more intensively investigated area, Personalized Web Search.The booming popularity of Web search engines has determined simple keywordsearch to become the only widely accepted user interface for seeking information over the Web. Yet keyword queries are inherently ambiguous. The querycanon book for example covers several different areas of interest religion, digital photography, literature, and music1. Interestingly, this is one of the examplesin which the first ten Google results do not contain any URL on the last topic.Clearly, search engine output should be filtered to better align the results with theusers interests. Personalization algorithms accomplish this goal by 1 learning defining users interests and 2 delivering search results customized accordinglypages about digital cameras for the photographer, religious books for the clergyman, and documents on music theory for the performing artist. A recent study1In music, a canon is a composition in which two or more voices sing the same melodic linebut start at different moments in time.97Chapter 5. Ranking for Web Search Personalization.presented by SearchEngineWatch 203 indicated that more than 80 of the userswould prefer to receive such personalized search results.In this chapter we propose to exploit manually created large scale informationrepositories to personalize Web search, i.e., to return search results which arerelevant to the user profile and are of good quality. There are two different typesof such repositories 1 Globally edited ones and 2 individually edited ones. Forthe former category we build on top of the metadata accumulated within publictaxonomies such as the Open Directory. We thus project link analysis rankinginto a taxonomical space and define appropriate similarity measures to order theWeb search output in accordance to each users preferences. Furthermore, wealso describe a new algorithm that learns user profiles based on projecting searchstatistics on top of the same taxonomical space, thus being either faster, or lessobtrusive than its predecessor approaches.For the category of individually created repositories we propose to expand usersWeb queries by utilizing both text and link analysis on top of Personal InformationRepositories2. Query expansion assists the user in formulating a better query byappending additional keywords to the initial search request in order to encapsulateher interests therein, as well as to focus the search output accordingly. Thetechnique has already been shown to perform very well when used over largedata sets and especially with short input queries 147, 40, i.e., exactly under thecharacteristics of the Web search scenario. Therefore, we automatically extractadditional keywords related both to the users search request and to her interestsas captured by her PIR, thus implicitly personalizing the search output.The chapter is organized as follows We proceed with a discussion of the background literature specific for this area. Further, we discuss how to exploit globalrepositories in order to personalize search in Section 5.3, as well as to automatically infer user profiles in Section 5.4. The use of individually created collectionsis then presented in Section 5.5. The chapter ends with a flexibility analysis relevant for our both personalization approaches, followed by some conclusions andan outline of possible future directions.5.2 Specific BackgroundPersonalization can bring extensive improvements over regular search quality. Andyet, even though many authors attempted to design a widely accepted personalized search algorithm, no one has succeeded so far. This section reviews most2Recall that the PIR is defined as the personal collection of textual documents, emails, cachedWeb pages, etc. stored on a local machine.98Paul  Alexandru Chiritatechniques that have been proposed along the time and compares them to the twoapproaches we introduce in this chapter. Moreover, since our second algorithmbuilds on top of another existing domain, namely automatic query expansion, wealso survey this research area in more detail within the second half of the section.5.2.1 Personalized SearchIntroduction. Search personalization consists of two highly interacting components 1 user profiling, and 2 output ranking. Generally, the latter onedepends on the characteristics of the former A good profile definition methodis needed before designing the ranking algorithm. Thus, user profiles representa central point of all approaches to personalize search. In one way or another,one must know the profile of the person searching the Web in order to deliverthe best results. We distinguish two possibilities for classifying the algorithms forpersonalized search, both centered around the user profile The first relates tothe way it is constructed, whereas the second relates to the way it is used. Mostworks available so far are focused on the former approach Chan 48 for exampleinvestigated the types of information available to pursue it, such as the time spentvisiting a Web page, or the frequency of visits, whether it was bookmarked, etc.Since many authors attempted to exploit this kind of information over the pastfew years, Kelly and Teevan 138 have recently put together a comprehensivebibliography capturing and categorizing these techniques. In this section we willpresent a different literature survey, focused on the latter perspective, i.e., theway profiles are used to achieve personalization, rather than on the way they arebuilt. We distinguish three broad approaches 1 integrating the personalizationaspect directly into PageRank, 2 filtering each query output to contain onlyURLs relevant to the user profile, and 3 using the personalized search algorithmas an additional search engine measure of importance together with PageRank,TFxIDF, etc.. Let us now inspect each of them in detail.PageRankbased Methods. The most efficient personalized search algorithmwill probably be the one which has the personalization aspect already included inthe initial rankings. Unfortunately, this seems very difficult to accomplish. Initialsteps in this direction have been already described by Page et al. 172, whoproposed a slight modification of the PageRank algorithm to redirect the randomsurfer towards some preferred pages. However, it is clearly impossible to computeone PageRank vector for each user profile, i.e., for each set of pages privilegedby the random surfer. The same is valid when the random surfer is uniformlychoosing pages to jump to and some preferred domains or links get a higherweight during the computation 5, 72.99Chapter 5. Ranking for Web Search Personalization.Qiu and Cho 176 used Machine Learning to classify users preferences into one ormore top level ODP topics, and then applied TopicSensitive PageRank 118 seealso Section 2.1.2 for its complete description weighted according to the inferreduser interests.Jeh and Widom 131 proposed an algorithm that avoids the huge resources neededfor storing one Personalized PageRank Vector PPV per user. They started froma set of hubs H3, each user having to select her preferred pages from it. PPVscan then be expressed as a linear combination of PPVs for preference vectors witha single nonzero entry corresponding to each of the pages from the preference setcalled basis vectors. Furthermore, basis vectors are decomposed into partialvectors encoding the part unique to each page, computed at runtime and thehubs skeleton capturing the interrelationships among basis vectors, stored offline. The advantage of this approach is that for a hub set of N pages, one cancompute 2N Personalized PageRank vectors without running the algorithm again.These rankings are generated offline, independently of the user query, which doesnot impose any additional response time on the search engine. The disadvantagesare the necessity for the users to select their preference set only from within a givengroup of pages4 common to all users, as well as the relatively high computationtime for large scale graphs. The latter problem has been solved through severalsubsequent studies 101, 190, their most recent solution using rounding and countmin sketching in order to fastly obtain accurate enough approximations of thePersonalized PageRank scores.Output Filtering Methods. As partially or completely query independenttechniques still exhibit a number of limitations, query oriented approaches havebeen investigated as an alternative. One of them is to sort out the irrelevant orlikely irrelevant results, usually in a process separated from the actual rankingmechanism. Liu et al. 163 restrict searches to a set of categories defined in theODP via Google Directory. Their main contribution consists of investigatingvarious techniques to exploit users browsing behavior for learning profiles as bagsof words associated to each topical category. In comparison to our approaches,they use relatively time consuming algorithms e.g., Linear Least Squares Fitand obtain a slightly worse precision. A different scheme is presented by Pahleviand Kitagawa 173, where for each query the user first selects her topics ofinterest, and then a classifier is used to either mark the results as nonrelevant,or associate them to one of the specified categories. Similarly, in 140 users start3Recall that hubs were defined here as pages with high PageRank, differently from the morepopular definition of Kleinberg 143.4We have done some work in the direction of improving the quality of this set of pages seeChirita et al. 73, but users are still restricted to select their preferred pages from a subset ofH if H  CNN,FOXNews we cannot bias on MSNBC for example.100Paul  Alexandru Chiritaby building a concept tree and then select one of these concepts to search for.The output is constructed by generating a set of queries describing users selectedconcepts and combining their results. Both these latter approaches are verydifficult to accomplish on a Web scale, either because classification delays searchengine response time too much, or because users are not willing to define concepthierarchies every time they search. Finally, when utilized in conjunction withspecific user profiles, automatic query expansion represents a different kind ofoutput filtering, its main underlying idea being to focus the search output ontothe additional keywords appended to the query. We refer the reader to the nextsubsection for a detailed discussion on the currently existing query expansiontechniques.Reranking Methods. A third approach to achieve personalization relies onbuilding an independent simpler personalized ranking algorithm, whose outputis then combined with that of PageRank. Sugiyama et al. 202 analyze userssurfing behavior and generate user profiles as features terms of the pages theyvisited. Then, upon issuing a new query, the results are ranked based on thesimilarity between each URL and the user profile. Similarly, Gauch et al. 107build profiles by exploiting the same surfing information i.e., page content andlength, time spent on each URL, etc., as well as by spidering the URLs savedin the personal Web cache and classifying them into topics of interest in a morerecent work 199, they log this latter information using a Google wrapper instead.Both approaches are similar to our taxonomy based personalized search algorithm,but we construct user profiles only by analyzing the user queries submitted to thesearch engine, rather than the entire browsing behavior, thus being less intrusiveand allowing these data to be collected directly on the server side  e.g., via searchaccounts such as Yahoo My Web, while having a faster ranking scheme.Besides the algorithm depicted in Section 5.5, there exists only one other approachattempted to enhance Web search using Desktop data. Teevan et al. 204 modified the query term weights from the BM25 weighting scheme 132 to incorporateuser interests as captured by their Desktop indexes. The method is orthogonalto our work, since we apply query expansion, a personalization tool much morepowerful than term weighting.Commercial Approaches. On the one hand, industry has long claimed thatpersonalization distinguishes itself as one of the future technologies for Web search.On the other hand, none of the techniques initiated by the search engines inthis direction has managed to succeed in being widely accepted by the public.This indicates that more work needs to be performed before identifying the bestpersonalization approach.Most major search engines offer personalization services as beta services. Google101Chapter 5. Ranking for Web Search Personalization.used to ask users about Open Directory topics of interest in order to achievepersonalization5, possibly by implementing an extension of Haveliwalas TopicSensitive PageRank 118. This prototype is currently no longer available. Itsreplacement is Google Search History6, which has the additional advantage oflearning the user profile based on her previous queries and clicks. Yahoo andAsk offer quite similar services respectively via their MyWeb 27 and MyJeeves8applications.A different approach is taken by Eurekster9, which offers personalization based onuser communities, i.e., by exploiting the interests of users with interests closelyrelated to those of the target user. Other major search engines might applythis kind of community based query log mining as well, including for exampleWindows Live Search10. Some of the metasearch engines also claim to haveaccess to personalization data see for example IBoogie11 when aggregating theresults over different search providers.JetEye12 was one of the first search engines to provide user specific customizationof the core search engine, by deciding what should be tapped for the future,what should be excluded, etc. Other implementations of this approach exist, forexample within Google Coop13, Rollyo14, or LookSmarts Furl15, etc. The newaspect therein is that personalization is also included in the index collection byadding specific Websites to it, comments, keywords, or quality ratings.A lot of other personalization search engines exist, such as Memoory16, or A917.Also, other companies attempting Web search personalization include Kaltix orOutride, both bought by Google in 2003 and 2001 respectively.SnakeT18 97 includes some nave form of personalization, according to whichusers are able to select some clusters of interest, once the output of their querieshas been grouped into categories.5httplabs.google.compersonalized6httpwww.google.compsearchhlen7httpmyWeb2.search.yahoo.com8httpmyjeeves.ask.com9httpsearch.eurekster.com10httpsearch.live.com11httpwww.iboogie.com12httpwww.jeteye.com13httpwww.google.comcoop14httpwww.rollyo.com15httpwww.furl.net16httpwww.searchenginejournal.comp117917httpwww.a9.com18httpwww.snaket.com102Paul  Alexandru Chirita5.2.2 Automatic Query ExpansionAutomatic query expansion aims at deriving a better formulation of the user queryin order to enhance retrieval. It is based on exploiting various social or collectionspecific characteristics in order to generate additional terms, which are appendedto the original input keywords before identifying the matching documents returnedas output. In this section we survey some of the representative query expansionworks grouped according to the source employed to generate additional terms1 Relevance feedback, 2 Collection based cooccurrence statistics, and 3Thesaurus information. Some other approaches are also addressed in the endof the section.Relevance Feedback Techniques. The main underlying idea of RelevanceFeedback RF is that useful information can be extracted from the relevantdocuments returned for the initial query. First approaches were manual andtherefore personalized 185 in the sense that the user was the one choosing therelevant results, and then various methods were applied to extract new terms,related to the query and the selected documents. Efthimiadis 90 presented acomprehensive literature review and proposed several simple methods to extractsuch new keywords based on term frequency, document frequency, etc. We usedsome of these as inspiration for our Desktop specific expansion techniques. Changand Hsu 49 asked users to choose relevant clusters, instead of documents, thusreducing the amount of user interaction necessary. Yet RF has been shown tobe effectively automatized by simply considering the top ranked documents asrelevant 219 this technique is known as Pseudo RF. Lam and Jones 152 usedsummarization to extract informative sentences from the topranked documents,and appended them to the user query. We have adapted this approach for ourDesktop scenario. Also, Carpineto et al. 42 maximized the divergence betweenthe language model defined by the top retrieved documents and that defined bythe entire collection. Finally, Yu et al. 221 selected the expansion terms onlyfrom visionbased segments of Web pages in order to cope with the multiple topicsresiding therein.Cooccurrence Based Techniques. Another source of additional query termsis the searched collection itself. Terms highly cooccurring with the originallyissued keywords have been shown to increase precision when appended to thequery 141. Many statistical measures have been developed to best assess termrelationship levels, either based on analyzing the entire documents 177, lexicalaffinity relationships 40 i.e., pairs of closely related words which contain exactlyone of the initial query terms, etc. We have investigated three such approaches inorder to identify query relevant keywords from the rich, yet rather complex Personal Information Repository. In a more recent investigation, Wang and Tanaka103Chapter 5. Ranking for Web Search Personalization.212 first employed a topical based clustering of all terms, and then selected thecandidate expansion words using conditional entropy.Thesaurus Based Techniques. A broadly explored technique is to expand theuser query with new terms, whose meaning is closely related to the original inputkeywords. Such relationships are usually extracted from large scale thesauri, asWordNet 167, in which various sets of synonyms, hypernyms, hyponyms, etc.are predefined. Just as for the term cooccurrence methods, initial experimentswith this approach were controversial, either reporting improvements, or evenreductions in the output quality see for example the work of Voorhees 209 andthe references therein. Recently, as the experimental collections grew larger,and as the employed algorithms became more complex, better results have beenobtained. Shah and Croft 194, as well as Kim et al. 142, applied variousfiltering techniques over the proposed additional keywords, either by estimatingquery clarity 77, or by using a root sense tagging approach. We also use WordNetbased expansion terms. However, we extend this process with an analysis of theDesktop level relationship between the original query and the proposed additionalkeywords.Other Techniques. There exist several other attempts to extract better termsfor query expansion, two of them being specifically tailored for the World WideWeb environment Cui et al. 78 generated word correlations utilizing a newprobability for query terms to appear in each document, computed over the searchengine logs. Kraft and Zien 147 pointed out that anchor text is very similar tothe user queries, and thus exploited it to acquire additional keywords. Bothapproaches are orthogonal to our Desktop focused work, as we use a different andricher source of expansion terms.5.3 Taxonomy Based Personalized Web SearchWe presented in Section 5.2.1 the most popular approaches to personalizing Websearch. Even though they are the best so far, they all have some importantdrawbacks. PageRank based methods either need too many resources to computeand store all Personalized PageRank Vectors, or are limited to a very restrictedset of pages to personalize on. Existing reranking methods usually employ textclassifiers both for learning user profiles and for evaluating query results i.e.,URLs, thus being inherently slow. Finally, output filtering schemes must performan additional results trimming step, which is also delaying response time to someextent. It is therefore still worth searching for a simpler and faster algorithm withat least similar personalization granularity as the current ones.104Paul  Alexandru ChiritaIn this section we propose to use community wide manually entered cataloguemetadata, such as the ones collected within the Open Directory, which expresstopical categorizations of Web pages. This kind of metadata was one of thefirst available on the Web in significant quantities for example within YahooDirectory, providing hierarchically structured access to highquality content onthe Web. We thus build upon the categorization done in the context of the ODP,as it is one of the largest efforts to manually annotate Web pages. Over 74,000editors are busy keeping the ODP directory reasonably uptodate, deliveringaccess to over 5 million Web pages in its catalogue. However, its advanced searchoffers a rudimentary personalization feature by restricting search to the entriesof just one of the 16 main categories. Google Directory which is also built ontop of ODP provides a related feature, by offering to restrict search to a specificcategory or subcategory. Clearly these services yield worse results than searchingGoogle itself 67. Can we improve them, taking user profiles into account in amore sophisticated way, and how will these enhanced personalized results compareto the ordinary Google results To what extent will they be helpful Morespecifically, will they improve Google for all kinds of queries including very exactqueries having, e.g., five words or more, only for some queries, or not at allIn the following section, we analyze and propose answers to these questions, andthen we evaluate them experimentally in Section 5.3.3.5.3.1 AlgorithmOur search personalization algorithm exploits the annotations accumulated ingeneric largescale taxonomies such as the Open Directory. Even though weconcentrate our forthcoming discussion on ODP, any similar taxonomy can beused. We define user profiles taking a simple approach each user has to selectseveral topics from ODP, which best fit her interests. For example, a user profilecould look like thisArtsArchitectureExperimentalArtsArchitectureFamous NamesArtsPhotographyTechniques and StylesAt runtime, the output given by a search service from MSN, Yahoo, Google,etc. is resorted using a calculated link distance from the user profile to eachoutput URL. This translates into a minimal additional overhead to the searchengine, bounded by the time needed to include the above mentioned distancesinto the overall ranking scheme. The execution is depicted in Algorithm 5.3.1.1.105Chapter 5. Ranking for Web Search Personalization.Algorithm 5.3.1.1. Personalized Search.Input Profu  Profile for user u, given as a vector of topicsQ  Query to be answered by the algorithm.Output Resu Vector of URLs, sorted after user us preferences1 Send Q to a search engine S e.g., Google2 Resu  Vector of URLs, as returned by S3 For i  1 to SizeResuDisti  DistanceResui, P rofu4 Sort Resu using Dist as comparatorWe additionally need a function to estimate the distance between a URL and auser profile. Let us inspect this issue in the following discussion.5.3.2 Estimating Topic SimilarityWhen performing search on Open Directory, each URL comes with an associatedODP topic. Similarly, many of the URLs output by Google are connected toone or more topics within the Google Directory almost 50 of Top100, as weobserved in our experiments described in Chirita et al. 67. In both cases, foreach output URL we are dealing with two sets of nodes from the topic tree 1Those representing the user profile set A, and 2 those associated with theURL set B. The link distance between these sets can then be defined asthe minimum link distance between all pairs of nodes given by the Cartesianproduct AB. There are quite a few possibilities to define the distance betweentwo nodes, depending on the perspective we take on ODP as a tree, as an ontology,as a graph, etc. In the following, we will present the most representative metricswe found suitable for our algorithm, following an increasing level of complexity.Nave Distance. The simplest solution is minimum treedistance, which, giventwo nodes a and b, returns the sum of the minimum number of tree links betweena and the subsumer the deepest node common to both a and b plus the minimumnumber of tree links between b and the subsumer i.e., the shortest path between aand b. On the example from Figure 5.1, the distance between ArtsArchitectureand ArtsDesignInterior DesignEventsCompetitions is 5, and the subsumeris Arts.If we also consider the intertopic links from the Open Directory, the simplestdistance becomes the shortest path between a and b. For example, if there is a link106Paul  Alexandru ChiritaFigure 5.1 Example tree structure of topics from ODPbetween Interior Design and Architecture in Figure 5.1, then the distance betweenCompetitions and Architecture is 3. This solution implies loading either the entiretopic graph or all the intertopic links into memory. Its utility is somewhatquestionable the existence of a link between Architecture and Interior Designdoes not always imply that a famous architect one level below in the tree isvery close to the area of interior design. We therefore chose to consider only theintratopic links directly connected to a or b when computing the shortest pathbetween them similar to 164.Tree Similarity. The main drawback of the previous metric comes from thefact that it ignores the depth of the subsumer lowest common ancestor. Thebigger this depth is, the more related are the nodes i.e., the concepts representedby them. This problem is solved using metrics from Information Theory 76,such as the semantic similarity between two topics t1 and t2 in a taxonomy 164.This is defined as the ratio between their common meaning and their individualmeanings, as followsTs t1, t2 2  logPrt0t1, t2logPrt1  logPrt25.1where t0t1, t2 is the subsumer of t1 and t2, and Prt represents the prior probability that any page is classified under topic t computed for example as the ratiobetween the number of pages stored in the subtree rooted at node t and the overallamount of pages.Concept Similarity. The ODP is manually annotated by human editors and itsometimes contains more URLs within a narrow, but popular concept, compared107Chapter 5. Ranking for Web Search Personalization.to a wider, but less popular one. This can yield inconsistent results with oursecond metric, which motivated us to search for a more elaborate approach. Liet at. 162 investigated ten intuitive strategies for measuring semantic similaritybetween words using hierarchical semantic knowledge bases such as WordNet 167.Each of them was evaluated empirically on a group of testers, the best one havinga 0.9015 correlation between human judgment and the following formulaSa, b  el  eh  eheh  eh5.2The parameters are as follows  and  were defined as 0.2 and 0.6 respectively, h isthe treedepth of the subsumer, and l is the semantic path length between the twowords. If we have several words attached to each concept and subconcept, thenl is 0 if the two words are in the same concept, 1 if they are in different concepts,but the two concepts have at least one common word, or the tree shortest path ifthe words are in different concepts which do not contain common words.Although this measure is very good for words, it is still not perfect when we applyit to the Open Directory topical tree, because it does not make a difference betweenthe distance from a the profile node to the subsumer, and the distance from b theoutput URL to the subsumer. Consider for example node a to be TopGamesand b to be TopComputersHardwareComponentsProcessorsx86. A teenagerinterested in computer games level 2 in the ODP tree could be very satisfiedreceiving a page about new processors level 6 in the tree, which might increaseher gaming quality. On the other hand, the opposite scenario profile on level 6 andoutput URL on level 2 does not hold any more, at least not to the same extenta processor manufacturer will generally be less interested in the games existingon the market. This leads to our following extension of the above formulaS a, b  1   el1    el2  eh  eheh  eh5.3with l1 being the shortest path from the profile to the subsumer, l2 the shortestpath from the URL to the subsumer, and  a parameter in 0, 1.Graph Similarity. An even more complete similarity function can be found in164. It estimates the similarity between two topics based on several sources ofinformation, using the following formulaGs t1, t2  maxk2 minWkt1 ,Wkt2  logPrtklog Prt1tk  Prtk  log Prt2tk  Prtk5.4As in Information Theory, the probability Prtk represents the prior probabilitythat any document is classified under topic tk, while Prtitk represents the posterior probability that any document will be classified under topic ti, given that108Paul  Alexandru Chiritait is classified under tk. Finally, Wij can be interpreted as a fuzzy membership19value of topic tj in the subtree routed at ti.Even though this formula seems to promise good results, computing it is veryresource demanding The authors reported the use of over 5,000 CPU hours ona supercomputer facility to calculate the similarity values for a large subset ofODP. Since we do not have such a facility available, and since computing thismeasure online i.e., at search time is quite time consuming as well, we decidednot to experiment with this measure.Combining the Similarity Function with the Google Page Scores. If weuse Google to do the search and then sort the URLs according to the GoogleDirectory taxonomy, some high quality pages might be missed i.e., those whichare top ranked, but which are not in the directory. In order to integrate those,the above formulas can be combined with the existing Google score associated toeach page. We propose the following approachS a, b    S a, b  1  GooglePageScoreb 5.5with  being another parameter in 0, 1 allowing us to keep the final score S a, binside 0, 1 for normalized page rank scores. If a page is not in the directory,we take S a, b to be 0.Observation. Human judgment is a nonlinear process over information sources162, and therefore it is very difficult if not impossible to propose a metric whichis in perfect correlation to it. However, we think that the thorough experimentalanalysis presented in the next section will provide a good approximation of theutility of these metrics.5.3.3 ExperimentsExperimental SetupTo evaluate the benefits of our personalization algorithm, we interviewed 24 of ourcolleagues researchers in different computer science areas, psychology, educationand design, asking each of them to define a user profile according to the OpenDirectory topics see Section 5.3.1 for an example profile, as well as to choose sixqueries of the following types One singleword specific query, which they thought to have one or maximumtwo meanings20 for example PageRank.19It is fuzzy because it also incorporates the related and symbolic links from ODP.20Of course, that did not necessarily mean that the query had no other meaning.109Chapter 5. Ranking for Web Search Personalization. One singleword relatively ambiguous query, which they thought to have twoor three meanings for example latex. One singleword ambiguous query, which they thought to have at least threemeanings, preferably more for example network. Three queries of the same types as above, but with multiple keywordsi.e., at least two, preferably more. Some examples could include wwwEdinburgh 2006 for the ambiguous query, pocket pc software for the semiambiguous one, and topcoder programming competitions for the specificquery. The average query length for these three multiword query typesturned out to be 2.34 in the case of the ambiguous queries, 2.50 for thesemiambiguous ones, and 2.92 for the specific queries.We compared the test results using the following nine approaches211. Google Search, as returned by the Google API22.2. Google with Nave Distance, using our algorithm from Section 5.3.1 toreorder the Top100 URLs returned by the Google API, and having as inputthe Google Directory topics returned by the API for each resulting URL.3. Google with Tree Distance, under the same setting as above, but usingthe tree distance instead.4.  6. Google with Concept Similarity, using three different values forthe  parameter 0.6, 0.7, and 0.823.7.  9. Combined PageRank with Concept Similarity, using three valuesfor the  parameter 0.3, 0.5, and 0.724.21Note that we actually performed two experiments. The first one, presented in Chirita et al.67, brought only the conclusion that Google with Nave Distance is better than Google, whichin turn is better than ODP search. The second one is much more comprehensive, and thereforedescribed in this section in detail.22httpapi.google.com23The parameters for both this algorithm and the subsequent one were first selected by asking 4persons to performWeb searches utilizing all 9 parameter values from 0.1 to 0.9 at equal intervals.Their results were not analyzed in detail, but used only to choose suitable parameters for theactual evaluation process. This helped us to observe that one should weigh the distance betweenthe profile and the subsumer more than the distance between each URL and the subsumer. Weselected the best three values for , namely   0.6, 0.7, 0.8, in order to find the exact ratiobetween these weights.24Here we investigated whether PageRank is more important, or the concept similarity measure. As within the training phase the results were overall inconclusive, with some queriesperforming much better than others over various settings of the parameters, we chose to analyze  0.3, 0.5, 0.7, as a representative set of values for all possibilities i.e., visibly more biason the taxonomy, visibly more bias on the search engine, or equal. The only characteristicwe observed from the initial tuning phase was that  should be inversely proportional to thedifficulty of the query, having values ranging from 0.1 to 0.9 indeed, for some queries, the best110Paul  Alexandru ChiritaQuery Cat.   0.6   0.7   0.8 Fvalue 214Oneword 3.05 3.05 3.01 F2,69,  0.04Multiword 3.01 3.04 3.01 F2,69,  0.02Table 5.1 Weights analysis for the concept similarity measure.Query Cat.   0.3   0.5   0.7 FvalueOneword 2.95 3.26 3.50 F2,69,99  5.44Multiword 3.53 3.23 3.07 F2,69,95  3.53Table 5.2 Weights analysis for the combined ranking measure.For each algorithm, each tester received the Top5 URLs with respect to eachtype of query, 30 URLs in total. All test data was shuffled, such that testers wereneither aware of the algorithm, nor of the ranking of each assessed URL. We thenasked the subjects to rate each URL from 1 to 5, 1 defining a very poor resultwith respect to their profile and expectations e.g., topic of the result, content,etc. and 5 a very good one25. For each subset of 5 URLs we took the averagegrade as a measure of importance attributed to that  algorithm, query type pair.ResultsHaving this dataset available, we performed an extensive analysis over it. Firstof all, we investigated the optimal values for the constants we defined. Theresults are summarized in Table 5.1 for the  parameter and in Table 5.2 for. We decided to evaluate separately the singleword and the multiword queries,since we expected them to perform differently. For , we found that all threevalues yielded similar results, the difference between their averages being far fromstatistically significant26. The outcome was much more interesting for  Notonly were the results statistically significant, but they were also dependent on thequery complexity. For simple queries it is better to use a large  in order to givemore weight to the Concept Similarity measure, whereas for more complex queries,PageRank should be made predominant through a small . The confidence levelwas 99 for the simple queries experiment and 95 for the one involving complexqueries.rating was obtained with   0.1 or 0.9 respectively.25This is practically a weighted P5, precision at the top 5 results.26To evaluate the statistical significance of our experiments we used Oneway and TwowayAnalysis of Variance ANOVA 30.111Chapter 5. Ranking for Web Search Personalization.Algorithm Ambig. Q. Semiambig. Q. Specific Q. Avg.  Algo.Google 1.93 2.26 3.57 2.59Nave Distance 2.64, p  0.01 2.75, p  0.04 3.34 2.91, p  0.01Concept Sim. 2.67, p  0.01 2.95, p  0.01 3.52 3.05, p  0.01Combined 3.14, p 0.01 3.27, p 0.01 4.09, p 0.01 3.50, p 0.01Avg.  Q. Type 2.60 2.80 3.63Table 5.3 Survey results for the oneword queries.We then picked the best choices for the above mentioned parameters   0.7,which yielded the best results, though at a marginal difference   0.7 for simplequeries, which was significantly better than the other two investigated values withp 0.01 and   0.3 for complex queries, also performing significantly best withp  0.01 and measured the performance of each distance  similarity metric27.The overall results are depicted in Table 5.3 for singleword simple queries andin Table 5.5 for the multiword complex ones, together with their pvalues ascompared to regular Google search.In the simple queries scenario, all our proposed algorithms outperformed GoogleThe Nave Distance received an average rating of 2.91 out of 5, with p  0.02,Concept Similarity received 3.05 p  0.01, and the Combined Measure reached3.50 p  0.01, whereas Google averaged only 2.59. For the specific queries,Google managed to slightly surpass the ODPbased metrics i.e, Nave Distanceand Concept Similarity, probably because for some of these queries ODP containsless than 5 URLs matching both the query and the topics expressed in the userprofile. Even in this particular case, however, the Combined Metric yielded thebest results 4.09 versus 3.57 for Google, a statistically significant difference withp  0.01. As we expected, the more specific the query type was, the bigger itsaverage rating ranging from 2.60 for ambiguous queries up to 3.63 for the specificones. The difference between ambiguous queries and semiambiguous ones wasnot statistically significant p  0.13, indicating that our subjects had difficultiesseparating these query types. However, there was a clear difference between theratings for the semiambiguous queries and for the clear ones p  0.01. Thus,the distinction between ambiguity and clarity was easier to make. Overall, allthese results were statistically significant at a 99 confidence level details of theassociated ANOVA analysis are depicted in Table 5.4. We noticed no interactionbetween the query type and the algorithm last row of Table 5.4, which indicatesthat the average per algorithm is independent of the query type and viceversa.27Since the Tree Distance performed very close to the Nave Distance, we decided not toinclude it in this analysis.112Paul  Alexandru ChiritaSrc. of Var. SS Deg. of Free. FvalueAlgorithm 57.668 3 F3,276,99  9.893Query Type 30.990 2 F2,276,99  27.614Interaction 7.363 6 F6,276,  1.175Table 5.4 Statistical significance analysis for the oneword queries experiments.Algorithm Ambig. Q. Semiambig. Q. Specific Q. Avg.  Alg.Google 2.32 3.19 4.15 3.22Nave Distance 2.17 2.95 3.60 2.90Concept Sim. 2.40, p  0.31 3.09 3.62 3.04Combined 3.08, p 0.01 3.75, p  0.01 3.76 3.53, p 0.01Avg.  Q. Type 2.49 3.24 3.78Table 5.5 Survey results for the multiword queries.The results were tighter under the multiword queries scenario Table 5.5 Googleaverage scores were somewhat above those of the Nave Distance and ConceptSimilarity, the best metric being, as in the previous experiment, the CombinedMeasure and that at a statistically significant difference with p 0.01. For specific queries, Google performed best, but was strongly surpassed by the CombinedMeasure when using ambiguous and semiambiguous query keywords p  0.01 inboth latter cases. The overall averages per algorithm were differentiated enoughto achieve a 99 overall confidence level see Table 5.6 for the ANOVA details,with a minimal interaction between the results associated to the query type andthose associated to the algorithm type.5.4 Taxonomy Based Automatic User ProfilingThe algorithm we presented in the previous section builds on top of manuallyentered user profiles. This interaction step clearly adds some burden upon theusers of our system and demands for automatized profiling techniques. In fact,this is a common problem for many online personalized applications, which iswhy researchers have attempted to automatically learn such user profiles usuallyby exploiting various aspects of browsing behavior see for example 187. Inmost cases this implied the analysis of either very personal or very many usagedata. In this section we advance one step further We learn user profiles onlyfrom past search engine queries, which is still a bit intrusive about 15 of ourprospective testers refused to participate in the experiments, arguing that their113Chapter 5. Ranking for Web Search Personalization.Src. of Var. SS Deg. of Free. FvalueAlgorithm 16.011 3 F3,276,99  4.827Query Type 80.547 2 F2,276,99  36.430Interaction 9.450 6 F6,276,75  1.424Table 5.6 Statistical significance analysis for the multiword queries experiments.queries are private, but clearly less intrusive than all previous approaches. Wealso need very little data to converge to a profile expressing users interests wellenough.We start with the following observation Since current personalized search algorithms either need extensive information to learn the user profile 202, 107, orsimply have it entered manually, could we use ODP to provide a more accurate,less intrusive and less resource consuming method to learn user profiles Thissection will provide an algorithm to answer this question positively.5.4.1 AlgorithmOverview. Learning user profiles also known as Preference Elicitation 50inherently needs some kind of personal user input data, such as search enginequeries, bookmarks, time or frequency of visits to a set of Web pages, etc. Weminimize the amount of these data by exploiting only queries sent to a searchengine. For each query, we add to the user profile the weighted set of distinctODP topics associated to its output URLs, as follows Every time a topic appearsin the result set, a weight of 1 is added if a URL has no topic associated to it,we try to infer its topic by analyzing the topics associated to its related pagesusing the Related Pages feature of the Google API, to its home page, or to itsparent directory. Since these latter topics have been automatically deducted, theirweight is represented by a parameter smaller than 1. After a few days of surfing,the profile can be generated as the set of topics with the highest N weights28.Before testing the algorithm, we evaluated the feasibility of our solution to derivethe topics associated to a URL missing from ODP. We selected 134 queries eitherused by previous researchers e.g., 118, or listed as most the frequent queriesby Google29 or Lycos30. Two experts rated the similarity between the topics28Even the user profiles generated after one day of surfing do provide useful information abouteach users interests. However, they also contain several false topical interest predictions, eitherbecause many of the queries issued that day cover several topics, or because the user issued someerroneous queries e.g., with typos. Therefore, in general, several days of surfing are necessary.29httpwww.google.compresszeitgeist.html30http50.lycos.com114Paul  Alexandru Chiritaassociated to each URL and those associated to its related pages, to its homepage, and to its parent directory. The grades were between 0 and 1, zeromeaning a totally unrelated topic and one representing exactly the same topic.The average values we obtained were   0.64 for the related URLs and   0.46for the home pages we dropped the analysis of the parent directory, because inalmost all cases it did not output any topic. We then used these two values as aweight for the topics heuristically associated to URLs recall that we use a weightof 1 if a URL returned for some user query is contained in the ODP.Another unknown parameter was the amount of output URLs whose associatedtopics we should investigate  add to the user profile for each query. We experimented with the top 10, 20, 30, 50, 80, and 100 URLs. Optimal results wereobtained when exploring the top 30 URLs per query more URLs usually returnedonly additional weights, but no more topics, whereas less than 30 URLs sometimesresulted in missing relevant topics.The complete algorithm for learning user profiles is depicted in Algorithm 5.4.1.1.Algorithm 5.4.1.1. Learning User Profiles.1 Let P topics, weights be the user profile.2 For each new query q sent to the search engine3 For each output URL u4 Let T topics, 1 be the set of topics associated to u.5 If T 6 6 P  P  T  Continue7 Let TRtopics,  be the set of topics associated to Relatedu.8 If TR 6 9 P  P  TR Continue10 Let THtopics,  be the set of topics associated to HomePageu.11 If TH 6 12 P  P  TH Continue13 Sort P decreasingly, after weights.14 Return the top N topics as the user profile.New topics. User interests have been shown to change over time 151, and ouralgorithm can clearly keep track of this aspect in the long term. But how to copewith the very first day one searches on a totally different topic A nave solutionwould be to simply temporarily disable the service exploiting the user profile i.e.,personalized search, news, etc.. A more elegant one is to divide the profile into apermanent profile PP and a temporary profile PT , which itself can also be divided115Chapter 5. Ranking for Web Search Personalization.into the profile built from all todays searches PS and the profile built only fromthe current search session PC if any. Then, the profile weights can be computedusing the following formula 202P t, w  0.6  PP t, w  0.4  PT t, w  0.6  PP t, w  0.4  0.15  PSt, w  0.85  PCt, wNoise. Even such carefully selected profiles are not perfect, though. Not allpreviously sent queries are actually relevant to the users interest Some of themrepresent a onetime interest, some others relate to a topic no longer addressedby the subject, and so on. However, we argue that these kinds of queries usuallyrepresent only a small percentage of the overall search history, and thus their effectover the generated profile is negligible, at least in the long term. Nevertheless, ifdesired, several methods might be applied in order to filter out noisy queries. Forexample, one could cluster users searched expressions by exploiting their TopKoutput URLs, and then identify and eliminate the outliers.5.4.2 ExperimentsExperimental SetupThe evaluation of our algorithm was performed using Google Search History31. Agroup of 16 persons created a Google account and used it for a period rangingfrom 7 to 21 days. After this, they were asked to send us a file containing theirqueries pasted from their Google account, which we then used together with theGoogle API to generate their profiles. Since it was not clear how specific theseprofiles should be, we investigated the following approaches Level 2 Profiles were generated to contain only topics from the secondlevel of the ODP taxonomy or above. More specific topics i.e., from lowerlevels were trimmed  considered as level 2 topics, while the more generaltopics were left as such. Level 3, Level 4, and Level 5 Same as above, but with the profilesrestricted not to contain topics on levels lower than 3, 4, and 5 respectively. Combined Level 24 Same as Level 4, but with the weight of each topicmultiplied by its depth in the ODP tree. The intuition here is that lowerlevel topics are more specific, and should thus be given more importance.31httpwww.google.comsearchhistory116Paul  Alexandru ChiritaFigure 5.2 Precision at the top 30 topics computed with complete query output. Combined Level 25 Same as Combined Level 24 but down to level5.Even though most persons are not interested in more than 10 topics at a time,we generated profiles consisting of the most important 30 topics, in order to fullyevaluate the capabilities of our algorithms. Every user had to rate each topic aseither irrelevant, or relevant for her interests, the overall results being put togetherin graphs of average precision and average relative recall32 17, whose input valueswere averaged over all our testers.ResultsOur results for average precision are depicted in Figure 5.2, and those for averagerelative recall in Figure 5.3. Level 5 is outperforming all other algorithms interms of precision, especially within the Top20 output topics. Its precision atthe Top10 topics P10 is as high as 0.94. At the other end, although it hasa slightly better recall than the other algorithms, Level 2 is clearly not usefulfor this task because of its quite poor precision. The combined measures yieldedonly an average performance, indicating that people are not only interested invery specific topics which received an increased weight with these two metrics,but also in broader, more general topics.32For relative recall, we assumed that the correct results reside within the output n  30items.117Chapter 5. Ranking for Web Search Personalization.Figure 5.3 Relative Recall at the top 30 topics computed with complete queryoutput.Further, we wanted to find out how many surfing days were necessary to havethe system automatically generate a good enough profile. We experimented withLevel 5 and Level 2, the latter one because it seemed to perform very well onthe best five topics in the previous experiment. As they are quite similar, we onlydiscuss here our results for Level 5, the better algorithm Only four days seemto be enough to obtain a precision at 10 above 80 the average number of URLsclicked per day was 8.75, the more stable output being achieved after six days ofusing the search engine. The complete average precision results are depicted inFigure 5.4.5.5 Desktop Based Personalized Web SearchThough very effective, the personalized search algorithm described in Section 5.3.1still exhibits two drawbacks First, there is the reduced privacy level. Even thoughwe proposed a lessintrusive profiling technique in Section 5.4, some personalinformation is nonetheless necessary, i.e., past user queries. Second, there is thelimited coverage, as only thouse URLs classified in the ODP are an intrinsic partof the personalization process.In this section we overcome these last limitations by exploiting users PersonalInformation Repository, a selfedited data collection. Several advantages arisewhen moving Web personalization down to the Desktop level. First, as all profile118Paul  Alexandru ChiritaFigure 5.4 Precision per day, using the Level 5 algorithm.information is now stored and exploited locally, on the personal machine, completeprivacy can be achieved. Search engines should not be able to know about apersons interests, i.e., they should not be able to connect a specific person withthe queries she issued, or worse, with the output URLs she clicked within thesearch interface33 see also Volokh 208 for a discussion on the privacy issuesrelated to personalized Web search. Second, this enhanced profile can be usedwith any search algorithm, thus being no longer restricted to ODP. Last, but notleast, such a comprehensive description of user interests brings inherently also anincreased search quality. After all, the local Desktop is a very rich repository ofinformation, accurately describing most, if not all interests of the user.The algorithms presented in this section build on top of Web search query expansion. They exploit users Personal Information Repository to automaticallyextract additional keywords related both to the query itself and to users interests, thus implicitly personalizing the search output. The challenge they bringis to provide an efficient utilization of the user profile. Desktop data comes in avery unstructured way, covering documents which are highly diverse in format,content, and even language characteristics. Though projecting this informationonto a large taxonomy such as ODP is fairly easy, it is not the optimal solution,as the resulting personalized search algorithm would still suffer from the limitedcoverage problem. We thus focus on exploiting the PIR as it is, in combination33Generally, search engines can map queries at least to IP addresses, for example by usingcookies and mining the query logs. However, by moving the user profile entirely down to theDesktop level we can at least ensure such information is not explicitly associated to a particularuser and stored on the search engine side.119Chapter 5. Ranking for Web Search Personalization.with the above mentioned technique of query expansion. We first investigate theanalysis of local query context on the Desktop in the first half of Section 5.5.1. Wepropose several keyword, expression, and summary based techniques for determining relevant expansion terms from those personal documents matching the Webquery best. Then, in the second half of Section 5.5.1 we move our analysis to theglobal Desktop collection and investigate query expansion based on cooccurrencemetrics, as well as on filtering a generic thesaurus based algorithm. The experiments presented in Section 5.5.2 show some of these approaches to perform verywell, especially on ambiguous queries, producing NDCG 130 improvements of upto 51.28.5.5.1 AlgorithmsThis section presents the five generic approaches for analyzing users Desktop datain order to provide expansion terms for Web search. In the proposed algorithms wegradually increase the amount of personal information utilized. Thus, in the firstpart we investigate three local analysis techniques focused only on those Desktopdocuments matching users query best. We append to the Web query the mostrelevant terms, compounds, and sentence summaries from these documents. In thesecond part of the section we move towards a global Desktop analysis, proposingto investigate term cooccurrences, as well as thesauri, in the expansion process.Expanding with Local Desktop AnalysisLocal Desktop Analysis is related to enhancing Pseudo Relevance Feedback togenerate query expansion keywords from the PIR best hits for users Web query,rather than from the top ranked Web search results. We distinguish three granularity levels for this process and we investigate each of them separately.Term and Document Frequency. As the simplest possible measures, TF andDF have the advantage of being very fast to compute. Previous experimentswith small data sets have showed them to yield very good results 38, 90. Wethus independently associate a score with each term, based on each of the twostatistics. The TF based one is obtained by multiplying the actual frequency of aterm with a position score descending as the term first appears closer to the endof the document. This is necessary especially for longer documents, because moreinformative terms tend to appear towards their beginning 38. The complete TFbased keyword extraction formula is as followsTermScore 1212 nrWords posnrWords log 1  TF  5.6120Paul  Alexandru Chiritawhere nrWords is the total number of terms in the document and pos is theposition of the first appearance of the term TF represents the frequency of eachterm in the Desktop document matching users Web query.The identification of suitable expansion terms is even simpler when using DFGiven the set of TopK relevant Desktop documents, generate their snippets asfocused on the original search request. This query orientation is necessary, sincethe DF scores are computed at the level of the entire PIR and would produce toonoisy suggestions otherwise. Once the set of candidate terms has been identified,the selection proceeds by ordering them according to the DF scores they areassociated with. Ties are resolved using the corresponding TF scores 90.Note that a hybrid TFxIDF approach is not necessarily efficient, since one Desktopterm might have a high DF on the Desktop, while being quite rare in the Web.For example, the term PageRank would be quite frequent on the Desktop of anInformation Retrieval scientist, thus achieving a low score with TFxIDF. However,as it is rather rare in the Web, it would make a good resolution of the query towardsthe correct topic.Lexical Compounds. Anick and Tipirneni 11 defined the lexical dispersionhypothesis, according to which an expressions lexical dispersion i.e., the numberof different compounds it appears in within a document or group of documentscan be used to automatically identify key concepts over the input document set.Although several possible compound expressions are available, it has been shownthat simple approaches based on noun analysis are almost as good as highlycomplex partofspeech pattern identification algorithms 8. We thus inspectthe matching Desktop documents for all their lexical compounds of the followingform adjective noun All such compounds could be easily generated offline, at indexing time, for all thedocuments in the local repository. Moreover, once identified, they can be furthersorted depending on their dispersion within each document in order to facilitatefast retrieval of the most frequent compounds at runtime.Sentence Selection. This technique builds upon sentence oriented documentsummarization First, the set of relevant Desktop documents is identified then, asummary containing their most important sentences is generated as output. Sentence selection is the most comprehensive local analysis approach, as it producesthe most detailed expansions i.e., sentences. Its downside is that, unlike withthe first two algorithms, its output cannot be stored efficiently, and consequentlyit cannot be computed offline. We generate sentence based summaries by ranking121Chapter 5. Ranking for Web Search Personalization.the document sentences according to their salience score, as follows 152SentenceScore SW 2TW PS TQ2NQThe first term is the ratio between the square amount of significant words withinthe sentence and the total number of words therein. A word is significant in adocument if its frequency is above a threshold as followsTF  ms 7 0.1  25NS , if NS  257 , if NS  25, 407  0.1  NS  40 , if NS  40with NS being the total number of sentences in the document see 152for details. The second term is a position score set to AvgNS SentenceIndexAvg2NS for the first ten sentences, and to 0 otherwise,AvgNS being the average number of sentences over all Desktop items. Thisway, short documents such as emails are not affected, which is correct, since theyusually do not contain a summary in the very beginning. However, as longerdocuments usually do include overall descriptive sentences in the beginning 89,these sentences are more likely to be relevant. The final term biases the summarytowards the query. It is the ratio between the square number of query termspresent in the sentence and the total number of terms from the query. It is basedon the belief that the more query terms contained in a sentence, the more likelywill that sentence convey information highly related to the query.Expanding with Global Desktop AnalysisIn contrast to the previously presented approach, global analysis relies on information from across the entire personal Desktop to infer the new relevant queryterms. In this section we propose two such techniques, namely term cooccurrencestatistics, and filtering the output of an external thesaurus.Term Cooccurrence Statistics. For each term, we can easily compute offline those terms cooccurring with it most frequently in a given collection i.e.,PIR in our case, and then exploit this information at runtime in order to inferkeywords highly correlated with the user query. Our generic cooccurrence basedquery expansion algorithm is as follows122Paul  Alexandru ChiritaAlgorithm 5.5.1.1. Cooccurrence based keyword similarity search.Offline computation1 Filter potential keywords k with DF  10, . . . , 20 N .2 For each keyword ki3 For each keyword kj4 Compute SCki,kj , the similarity coefficient of ki, kj.Online computation1 Let S be the set of keywords, potentially similar to an input expression E.2 For each keyword k of E3 S  S  TSCk, where TSCk contains theTopK terms most similar to k.4 For each term t of S5a Let Scoret kE0.01  SCt,k5b Let Scoret DesktopHitsEt6 Select TopK terms of S with the highest scores.The offline computation needs an initial trimming phase step 1 for optimizationpurposes. In addition, we also restricted the algorithm to computing cooccurrencelevels across nouns only, as they contain by far the largest amount of conceptualinformation, and as this approach reduces the size of the cooccurrence matrixconsiderably. During the runtime phase, having the terms most correlated witheach particular query keyword already identified, one more operation is necessary,namely calculating the correlation of every output term with the entire query.Two approaches are possible 1 using a product of the correlation between theterm and all keywords in the original expression step 5a, or 2 simply countingthe number of documents in which the proposed term cooccurs with the entireuser query step 5b. Small scale tuning experiments performed before the actualempirical analysis indicated the former approach yields a slightly better outcome.Finally, we considered the following Similarity Coefficients 141 Cosine Similarity, defined asCS DFx,yDFx DFy5.7 Mutual Information, defined asMI  logN DFx,yDFx DFy5.8123Chapter 5. Ranking for Web Search Personalization. Likelihood Ratio, defined in the paragraphs below.DFx is the Document Frequency of term x, and DFx,y is the number of documentscontaining both x and y. To further increase the quality of the generated scoreswe limited the latter indicator to cooccurrences within a window of W terms. Weset W to be the same as the maximum amount of expansion keywords desired.Dunnings Likelihood Ratio  87 is a cooccurrence based metric similar to 2.It starts by attempting to reject the null hypothesis, according to which two termsA and B would appear in text independently from each other. This means thatP AB  P AB  P A, where P AB is the probability that term A is notfollowed by term B. Consequently, the test for independence of A and B can beperformed by looking if the distribution of A given that B is present is the sameas the distribution of A given that B is not present. Of course, in reality we knowthese terms are not independent in text, and we only use the statistical metricsto highlight terms which are frequently appearing together. We compare the twobinomial processes by using likelihood ratios of their associated hypotheses. First,let us define the likelihood ratio for one hypothesis max0 H kmax H k5.9where  is a point in the parameter space , 0 is the particular hypothesis beingtested, and k is a point in the space of observations K. If we assume that twobinomial distributions have the same underlying parameter, i.e., p1, p2  p1 p2, we can write maxp Hp, p k1, k2, n1, n2maxp1,p2 Hp1, p2 k1, k2, n1, n25.10where Hp1, p2 k1, k2, n1, n2  pk11  1 p1n1k1 n1k1 pk22  1 p2n2k2 n2k2.Since the maxima are obtained with p1 k1n1, and p2 k2n2for the denominator,and p  k1k2n1n2for the numerator, we have maxp Lp, k1, n1Lp, k2, n2maxp1,p2 Lp1, k1, n1Lp2, k2, n25.11where Lp, k, n  pk  1  pnk. Taking the logarithm of the likelihood, weobtain2  log   2  logLp1, k1, n1  logLp2, k2, n2logLp, k1, n1 logLp, k2, n2where logLp, k, n  k  log p  n  k  log 1 p. Finally, if we write O11 P A B, O12  P A B, O21  P A B, and O22  P A B, then the124Paul  Alexandru Chiritacooccurrence likelihood of terms A and B becomes2  log   2  O11  log p1 O12  log 1 p1 O21  log p2 O22  log 1 p2O11 O21  log p O12 O22  log 1 pwhere p1 k1n1 O11O11O12, p2 k2n2 O21O21O22, and p  k1k2n1n2.Thesaurus Based Expansion. Large scale thesauri encapsulate global knowledge about term relationships. Thus, for this technique we first identify the setof terms closely related to each query keyword, and then we calculate the Desktop cooccurrence level of each of these possible expansion terms with the entireinitial search request. In the end, those suggestions with the highest frequenciesare kept. The algorithm is as followsAlgorithm 3.1.2.2. Filtered thesaurus based query expansion.1 For each keyword k of an input query Q2 Select the following sets of related terms using WordNet2a Syn All Synonyms2b Sub All subconcepts residing one level below k2c Super All superconcepts residing one level above k.3 For each set Si of the above mentioned sets4 For each term t of Si5 Search the PIR with Qt, i.e.,the original query, as expanded with t.6 Let H be the number of hits of the above searchi.e., the cooccurence level of t with Q.7 Return TopK terms as ordered by their H values.We observe three types of term relationships steps 2a2c 1 synonyms, 2 subconcepts, namely hyponyms i.e., subclasses and meronyms i.e., subparts,and 3 superconcepts, namely hypernyms i.e., superclasses and holonymsi.e., superparts. As they represent quite different types of association, weinvestigated them separately. We limited the output expansion set step 7to contain only terms appearing at least T times on the Desktop, in orderto avoid noisy suggestions, with T  min NDocsPerTopic ,MinDocs. We setDocsPerTopic  2, 500, and MinDocs  5, the latter one coping with the caseof small PIRs.125Chapter 5. Ranking for Web Search Personalization.5.5.2 ExperimentsExperimental SetupWe evaluated the quality of our personalization algorithms with 18 subjects Ph.D.and PostDoc. students in different areas of computer science and education.First, they installed our Lucene based search engine to index all their locally storedcontent Files within user selected paths, Emails, and Web Cache. Without loss ofgenerality, we focused the experiments on singleuser machines. Implementing thesame algorithms for multiuser computers is a trivial task. Then, our evaluatorswere asked to choose four queries related to their everyday activities as follows One very frequent AltaVista query, as extracted from the top 2 queriesmost issued to the search engine within a 7.2 million entries log from October2001. In order to connect such a query to each users interests, we added anoffline preprocessing phase. We first generated the most frequent 144,000AltaVista requests i.e., 2, and then randomly selected 50 queries with atleast 10 and at most 50 hits on each subjects Desktop. This was necessaryin order to ensure that we do not personalize a query which is either notinteresting for the user, or of too general interest. In the end, each evaluatorhad to choose the first of these queries that matched her interests at leastpartially. One randomly selected AltaVista query, filtered using the same procedureas above. One selfselected specific query, which they thought to have only one meaning. One selfselected ambiguous query, which they thought to have at least threemeanings.The average query lengths were 2.0 and 2.3 terms for the log based queries, as wellas 2.9 and 1.8 for the selfselected ones. Even though our algorithms are mainlyintended to enhance search when using ambiguous query keywords, we chose toinvestigate their performance on a wide span of query types, in order to see howthey perform in all situations. The log based queries evaluate real life requests34, incontrast to the selfselected ones, which target rather the identification of top andbottom performances. We should note that the former ones were somewhat fartheraway from each subjects interest, thus being also more difficult to personalize34Note that at the time when we developed the taxonomy based personalized search algorithmfrom Section 5.3, we had no real life query log available to test with. Moreover, automaticallymatching between the query log and users interests would have been anyway nearly impossible,since no other user specific information is available for that scenario.126Paul  Alexandru Chiritaon. To gain an insight into the relationship between each query type and userinterests, we asked each person to rate the query itself with a score of 1 to 5,having the following interpretations 1 never heard of it, 2 do not know it,but heard of it, 3 know it partially, 4 know it well, 5 major interest. Theobtained grades were 3.11 for the top AltaVista queries, 3.72 for the randomlyselected ones, 4.45 for the selfselected specific ones, and 4.39 for the selfselectedambiguous ones. Finally, in order to simplify the experimentation, all Desktoplevel parts of our algorithms were performed with Lucene using its predefinedsearching and ranking functions. However, implementing this task using the linkanalysis ranking algorithms from Chapter 3 would be trivial.For each of the four test queries, we then collected the Top5 URLs generatedby 20 versions of the algorithms presented in Section 5.5.1. These results werethen shuffled into one set containing usually between 70 and 90 URLs. Thus, eachsubject had to assess about 325 documents for all four queries, being neither awareof the algorithm, nor of the ranking of each assessed URL. Overall, 72 queries wereissued and over 6,000 URLs were evaluated during the experiment. For each ofthese URLs, the testers had to give a rating ranging from 0 to 2, dividing therelevant results in two categories, 1 relevant and 2 highly relevant. Finally, wedecided to assess the quality of each ranking using an innovative method, namelythe normalized version of Discounted Cumulative Gain DCG 130. DCG isa rich measure, as it gives more weight to highly ranked documents, while alsoincorporating different relevance levels by giving them different gain valuesDCGi G1 , if i  1DCGi 1 Gi log i , otherwise.We used Gi  1 for the relevant results, and Gi  2 for the highly relevantones. As queries having more relevant output documents will have a higher DCG,we also normalized its value to a score between 0 the worst possible DCG giventhe ratings and 1 the best possible DCG given the ratings to facilitate averagingover queries. All results were tested for statistical significance using Ttests, i.e.,we tested whether the improvement over the Google API output35 is statisticallysignificant.Algorithmic specific aspects. As our goal is to generate expansion terms forWeb queries, it is important to tune the number of such proposed keywords. Aninitial investigation with a separate group of 4 people showed different valuesto produce the best results across different queries and algorithms. The maininfluencing factor was by far the query ambiguity level, and we therefore set the35Whenever necessary, we also tested for significance the difference between pairs of thealgorithms we proposed.127Chapter 5. Ranking for Web Search Personalization.expansion length to four terms for all proposed techniques, leaving a differentiationat the algorithm level for a future experiment which we will present later, inSection 5.6.In order to optimize the runtime computation speed, we chose to limit the numberof output keywords per Desktop document to the number of expansion keywordsdesired i.e., four. For all algorithms we also investigated bigger limitations. Thisallowed us to observe that the Lexical Compounds method would perform betterif only at most one compound per document were selected. We therefore choseto experiment with this optimized approach as well. For all other techniques,considering less than four terms per document did not seem to yield any additionalqualitative gain.In the forthcoming tables, we label the algorithms we evaluated as follows0. Google The actual Google query output, as returned by the Google API1. TF, DF Term and Document Frequency2. LC, LCO Regular and Optimized by considering only one top compoundper document Lexical Compounds3. SS Sentence Selection4. TCCS, TCMI, TCLR Term Cooccurrence Statistics using respectively Cosine Similarity, Mutual Information, and Likelihood Ratio as similarity coefficients5. WNSYN, WNSUB, WNSUP WordNet based expansion with synonyms, subconcepts, and superconcepts, respectively.Except for the thesaurus based expansion, in all cases we also investigated theperformance of our algorithms when exploiting only the Web browser cache torepresent users personal information. This is motivated by the fact that otherpersonal documents such as for example emails are known to have a somewhatdifferent language than that residing on the World Wide Web 204. We differentiate between these two techniques by adding the A label suffix to specify thatthe entire Desktop was used as PIR, and W where only the Web browser cachewas employed.ResultsLog Queries. We evaluated all variants of our algorithms using NDCG. For thecase of queries extracted from the search engine log, the best performance wasachieved with TFA, LCOA, and TCLRA. The improvements they broughtwere up to 5.2 for top queries with p  0.14 and up to 13.8 for randomly128Paul  Alexandru Chiritaselected queries with p  0.01, and thus statistically significant, both beingobtained with LCOA. A summary of all results is presented in Table 5.7.Both TFA and LCOA yielded very good results, indicating that simple keywordand especially expression oriented approaches might be sufficient for the Desktopbased query expansion task. LCOA was much better than LCA, amelioratingits quality with up to 25.8 in the case of randomly selected log queries, improvement which was also significant with p  0.04. Thus, a selection of compoundsspanning over several Desktop documents is more informative about users interests behind a query than the general approach, in which there is no restriction onthe number of compounds produced from every personal item.The more complex Desktop oriented approaches, namely sentence selection and allterm cooccurrence based algorithms, showed a rather average performance, withno visible improvements being noticed except for TCLRA. Also, the thesaurusbased expansion usually produced very few suggestions, possibly because of themany technical queries employed by our subjects. We observed however thatexpanding with subconcepts is very good for everyday life terms e.g., car,whereas the use of superconcepts is valuable for compounds having at leastone term with low technicality e.g., document clustering. As expected, thesynonym based expansion performed generally well, though in some very technicalcases it yielded rather general suggestions, which were not filtered out by theDesktop data and thus worsened the query quality.Two general approaches performed rather poorly. First, DF, even with a snippetbased query orientation, still produced quite some frequent and nonresolutiveDesktop terms, thus deteriorating retrieval. Second and more important, the useof Web browser cache data to describe user interests, had visibly worse resultsthan Google in all 8 cases. As this was quite unexpected, we interviewed oursubjects for clarification. Several reasons have been identified some of them wereusing their credit cards quite often for online transactions and used to clean theircache frequently in order to protect themselves from malicious intruders severalwere running multiple operating systems and used the other one for browsingothers were surfing the Web very rarely, and two of them were recent hires. Also,the Web cache data seemed more noisy, as it also included text from one timeinterest pages, popups, etc. Though we believe these characteristics would lesslikely show up in other environments, using only the Web browser cache does notseem to be sufficient for extracting good personalized expansion terms for Websearch.Finally, we noticed Google to be very optimized for a set of top frequent queries,making improvements harder for this category of search tasks. Also, even thoughwe investigated quite several approaches to filter out personalized queries from129Chapter 5. Ranking for Web Search Personalization.Algorithm NDCG Signific. NDCG Signific.Top vs. Google Random vs. GoogleGoogle 0.42  0.40 TFA 0.43 p  0.32 0.43 p  0.04TFW 0.13  0.15 DFA 0.17  0.23 DFW 0.12  0.10 LCA 0.39  0.36 LCOA 0.44 p  0.14 0.45 p  0.01LCW 0.13  0.13 LCOW 0.16  0.12 SSA 0.33  0.36 SSW 0.11  0.19 TCCSA 0.37  0.35 TCMIA 0.40  0.36 TCLRA 0.41  0.42 p  0.06TCCSW 0.19  0.14 TCMIW 0.18  0.16 TCLRW 0.17  0.17 WNSYN 0.42  0.38 WNSUB 0.28  0.33 WNSUP 0.26  0.26 Table 5.7 Normalized Discounted Cumulative Gain at the first 5 results whensearching for an AltaVista top left and random right query.the search engine log, we should recall that our users were in almost all cases onlypartially familiar with the topic of the query to evaluate. Thus, our improvementsin the range of 1015 obtained with TFA and LCOA both statisticallysignificant show that such Desktop enhanced query expansion is useful even whensearching for queries covering users interests only marginally.Selfselected Queries. We also evaluated our algorithms with queries closer tothe interests of our subjects. We split them in two categories, clear and ambiguousrequests. While our algorithms did not manage to enhance Google for the clearsearch tasks, they did produce strong improvements of up to 52.9 which wereof course also highly statistically significant with p  0.01 when utilized withambiguous queries. In fact, almost all our algorithms resulted in statisticallysignificant improvements over Google for this query type. A summary of allresults is presented in Table 5.8.130Paul  Alexandru ChiritaAlgorithm NDCG Signific. NDCG Signific.Clear vs. Google Ambiguous vs. GoogleGoogle 0.71  0.39 TFA 0.66  0.52 p 0.01TFW 0.21  0.14 DFA 0.37  0.31 DFW 0.17  0.11 LCA 0.65  0.54 p 0.01LCOA 0.69  0.59 p 0.01LCW 0.16  0.15 LCOW 0.18  0.14 SSA 0.56  0.52 p 0.01SSW 0.20  0.11 TCCSA 0.60  0.50 p  0.01TCMIA 0.60  0.47 p  0.02TCLRA 0.56  0.47 p  0.03TCCSW 0.17  0.14 TCMIW 0.13  0.13 TCLRW 0.22  0.18 WNSYN 0.70  0.36 WNSUB 0.46  0.32 WNSUP 0.51  0.29 Table 5.8 Normalized Discounted Cumulative Gain at the first 5 results whensearching for a user selected clear left and ambiguous right query.In general, the relative differences between our algorithms were similar to thoseobserved for the log based queries. As in the previous analysis, the simple Desktopbased Term Frequency and Lexical Compounds metrics performed best. Nevertheless, a very good outcome was also obtained for Desktop based sentence selectionand all term cooccurrence metrics, indicating that there is still room for Websearch improvement in the case of ambiguous queries. There were no visible differences between the behavior of the three different approaches to cooccurrencecalculation. Finally, for the case of clear queries, we noticed that fewer expansionterms than four might be less noisy and thus helpful in further improving over thealready high quality of the Google output. We thus pursued this idea with theadaptive algorithms presented in the following section.131Chapter 5. Ranking for Web Search Personalization.5.6 Introducing AdaptivityBoth Web search personalization algorithms proposed in the previous sectionsof this chapter yielded very good results overall. However, neither of them wasconstant in surpassing the output quality of Google. In particular, the industrialsearch engine seems to be optimized to handle very common queries, as well asvery specific ones. This is why we argue that personalized search algorithmsshould be flexible, allowing for a combination of regular and user oriented results,both weighted automatically according to the strengths of either approach, tothe various aspects of each query, and to the particularities of the person usingit. In this section we first discuss the factors influencing the behavior of searchalgorithms which might be used as input for the adaptivity process. We thenshow how one of them, namely query clarity, can be applied on top of our Desktopspecific query expansion technique. We conclude with an empirical analysis whichconfirms the additional quality increase brought by the adaptivity feature.5.6.1 Adaptivity FactorsSeveral indicators could assist the algorithm to automatically tune the amountof personalization injected into the search output. We will start discussing adaptation by analyzing the query clarity level. Then, we will briefly introduce anapproach to model the generic query formulation process in order to additionallytailor the search algorithm automatically, and in the end we will discuss someother possible factors that might be of use for this task.Query Clarity. The interest for analyzing query difficulty has increased onlyrecently, and there are not many papers addressing this topic. However, it hasbeen long known that query disambiguation algorithms have a high potential ofimproving retrieval effectiveness for low recall searches with very short queries36see for the example the work of Krovetz and Croft 148, which is exactly ourtargeted scenario. Also, the success of Information Retrieval systems clearly variesacross different topics initial work analyzing this phenomenon has been done inthe context of the TREC Robust Track 210. We thus propose to use an estimatenumber expressing the calculated level of query clarity in order to automaticallytweak the amount of personalization fed into the algorithm. The only relatedwork is the paper of Amati et al. 9, who decide on a yes or no basis whether toapply query expansion or not within a search application.36Note that many query disambiguation approaches proposed for other kinds of search scenarios e.g., collection specific searches with long queries have not been too successful.132Paul  Alexandru ChiritaSeveral approaches have been proposed to quantify query ambiguity. They are asfollows The Query Length is expressed simply by the number of words in the userquery. The solution is not only nave, but rather inefficient, as reported byHe and Ounis 122. The Query Scope relates to the IDF of the entire query, as inC1  log DocumentsInCollectionHitsQuery 5.12This metric performs well when used with document collections covering asingle topic, but poor otherwise 77, 122. The Query Clarity 77 seems to be the best, as well as the most appliedtechnique so far. It measures the divergence between the language modelassociated to the user query and the language model associated to thecollection. In a simplified version i.e., without smoothing over the termswhich are not present in the query, it can be expressed as followsC2 wQueryPmlwQuery  logPmlwQueryPcollw5.13where PmlwQuery is the probability of the word w within the submittedquery, and Pcollw is the probability of w within the entire collection ofdocuments.A number of other solutions exist see for example 220, 41, but we think theyare too computationally expensive for the huge amount of data that needs to beprocessed when used on the World Wide Web. We thus decided to investigate themeasures C1 and C2.Query Formulation Process. Interactive query expansion has a high potentialfor enhancing the search experience 186. We believe that modeling its underlyingprocess would be very helpful in producing qualitative adaptive Web search algorithms. For example, when the user is adding a new term to her previously issuedquery, she is basically reformulating her original search request. Thus, the newlyadded terms are more likely to convey information about her underlying searchintention. For a general, non personalized retrieval engine, this could correspondto giving more weight to these new query keywords. Within our Desktop basedpersonalized scenario, the generated expansions could similarly be biased towardsthese terms. Nevertheless, modeling the query reformulation process remains anopen challenge. Besides, it is not clear whether regular users are indeed capableof adding topic resolutive terms when they are reformulating their search requests133Chapter 5. Ranking for Web Search Personalization.and it is not straightforward to accurately separate real query reformulations fromsimple changes in users search goal.Other Features. The general idea of adapting the retrieval process to variousaspects of the query, of the user himself, and even of the employed algorithm hasreceived only little attention in the scientific literature. Only some approacheshave been investigated, usually in an indirect way. There exist studies of severalindicators, such as query behaviors at different times of day, or of the topicsspanned by the queries of various classes of users, or of the preponderant locationsand languages employed for searching the Web, etc. e.g., 24. However, theygenerally do not discuss how these features can be actually incorporated in thesearch process itself. Moreover, they have almost never been related to the taskof Web personalization, even though they are more important for this latter case,as the personalized search algorithms should be flexible and able to optimallyconfigure themselves for each search request in an automatic way.5.6.2 Desktop Based Adaptive Personalized SearchAs the query clarity indicators were the only ones sufficiently developed in order tobe used within search algorithms, we decided to integrate them into our Desktopspecific query expansion technique, so as to evaluate the feasibility of using anadaptive personalized search system. We settled onto building upon C1 and C2,as discussed in the previous section, and started by analyzing their performanceover a large set of queries. The resulting clarity predictions were split into threecategories Small Scope  Clear Query C1  0, 12, C2  4,. Medium Scope  SemiAmbiguous Query C1  12, 17, C2  2.5, 4. Large Scope  Ambiguous Query C1  17,, C2  0, 2.5.Surprisingly, the same intervals were quite well delimited on both the Web andpersonal repositories and the two measures C1 and C2 produced rather similarpredictions. In order to limit the size of the experimental process, we chose toanalyze only the results produced when employing C1 for the PIR and C2 for theWeb, as this was the best combination by a small margin, as observed from aninitial manual analysis of the output produced by each metric.As an algorithmic basis we employed LCOA, i.e., personalized query expansionusing optimized lexical compounds within the entire user PIR, as it was clearlythe winning method in the previous analysis. However, an investigation of theexpansion terms it generated showed it to slightly overfit the results for clearqueries. We therefore utilized a substitute for this particular case. Two candidates134Paul  Alexandru ChiritaDesktop Scope Web Clarity No. of Terms AlgorithmLarge Ambiguous 4 LCOALarge SemiAmbig. 3 LCOALarge Clear 2 LCOAMedium Ambiguous 3 LCOAMedium SemiAmbig. 2 LCOAMedium Clear 1 TFA  WNSYNSmall Ambiguous 2 TFA  WNSYNSmall SemiAmbig. 1 TFA  WNSYNSmall Clear 0 Table 5.9 Adaptive Personalized Query Expansion.were considered 1 TFA, i.e., term frequency with all Desktop data, as it wasthe second best approach overall, and 2 WNSYN, i.e., WordNet synonyms, aswe observed that its first and second expansion terms were often very good.Given the algorithms and the clarity measures, we implemented the adaptivityprocedure by tailoring the amount of expansion terms added to the original query,as a function of its ambiguity in the Web, as well as within the Personal Information Repository of the user. Note that the ambiguity level is related to thenumber of documents covering a certain query. Thus, to some extent, it has different meanings on the Web and within PIRs. While a query deemed ambiguouson a large collection such as the Web will very likely indeed have a large number ofmeanings, this may not be the case for the Desktop. Take for example the queryPageRank. If the user is a link analysis expert, many of her documents mightmatch this term, and thus the query would be classified as ambiguous. However,when analyzed against the Web, this is definitely a clear query. Consequently,we employed more additional query terms when the query was more ambiguousin the Web, but also on the Desktop. Put another way, queries deemed clear onthe Desktop were inherently not well covered within users Personal InformationRepository, and thus had fewer keywords appended to them. The actual numberof expansion terms we utilized for each combination of scope and clarity levels isdepicted in Table 5.9.Note that we also requested to have at least 20 hits of the original query on thelocal Desktop, in order to cope with too shallow PIRs, either because the machinewas new, or because the topic did not represent a common interest of the user.Whenever this constraint was not satisfied, the query was simply left unexpanded.135Chapter 5. Ranking for Web Search Personalization.5.6.3 ExperimentsExperimental SetupFor the empirical analysis of our adaptive search algorithms we used exactly thesame experimental setup as for our previous Desktop personalized query expansiontechniques, with two logbased queries and two selfselected ones all different frombefore, evaluated with NDCG over the Top5 results output by each algorithm.The approaches included into this final study were as follows0. Google The actual Google query output, as returned by the Google API1. TFA Term Frequency, over the entire PIR2. LCOA Optimized by considering only one top compound per document Lexical Compounds, also using all available user data3. WNSYN WordNet based expansion with synonyms4. ALCOTF Adaptive personalized search with TFA for clear Desktopqueries, and LCOA otherwise5. ALCOWN Same as above, but with WNSYN used instead of TFA.ResultsThe overall results were at least similar, or better than Google for all kinds oflog queries see Table 5.10. In the case of top frequent queries, both adaptivealgorithms, ALCOTF and ALCOWN, improve Google with 10.8 and 7.9respectively, both differences being also statistically significant with p  0.01.They also achieve an improvement of up to 6.62 over the best performing staticalgorithm, LCOA the pvalue in this case being 0.07. For randomly selectedqueries, even though ALCOTF manages to yield significantly better results thanGoogle p  0.04, both adaptive approaches fall behind the static algorithms.The major reason for this seems to be the unstable dependency of the number ofexpansion terms, as a function of query clarity.The analysis of the selfselected queries shows that adaptivity can bring evenfurther improvements into the Web search personalization process see Table5.11. For ambiguous queries, the scores given to Google search are enhanced by40.6 through ALCOTF and by 35.2 through ALCOWN, both stronglysignificant with p  0.01. Allowing for flexibility in the number of expansionkeywords brings another 8.9 improvement over the static personalization ofLCOA p  0.05. Even in the case of clear queries, the adaptive algorithmsperform better though only with a very small margin, improving over Googlewith 0.4 and 1.0 respectively see Figure 5.5.136Paul  Alexandru ChiritaAlgorithm NDCG Signific. NDCG Signific.Top vs. Google Random vs. GoogleGoogle 0.51  0.45 TFA 0.51  0.48 p  0.04LCOA 0.53 p  0.09 0.52 p  0.01WNSYN 0.51  0.45 ALCOTF 0.56 p  0.01 0.49 p  0.04ALCOWN 0.55 p  0.01 0.44 Table 5.10 Normalized Discounted Cumulative Gain at the first 5 results whenusing our adaptive personalized search algorithms on an AltaVista top left andrandom right query.Algorithm NDCG Signific. NDCG Signific.Clear vs. Google Ambiguous vs. GoogleGoogle 0.81  0.46 TFA 0.76  0.54 p  0.03LCOA 0.77  0.59 p 0.01WNSYN 0.79  0.44 ALCOTF 0.81  0.64 p 0.01ALCOWN 0.81  0.63 p 0.01Table 5.11 Normalized Discounted Cumulative Gain at the first 5 results whenusing our adaptive personalized search algorithms on a user selected clear leftand ambiguous right query.All results are depicted graphically in Figure 5.5. We notice that ALCOTFis the overall best algorithm, performing better than Google for all types ofqueries, either extracted from the search engine log, or selfselected. Therefore, theexperiments presented in this section confirm clearly that adaptivity is a necessaryfurther step to take.5.7 DiscussionThe billions of pages available on the World Wide Web, together with the continuous attempts of spammers to artificially promote low quality content, havedetermined information finding to become a more and more difficult task in thisenvironment. It is only the extensive work performed within the search engineindustry that kept search quality at reasonably good levels. Personalization comes137Chapter 5. Ranking for Web Search Personalization.Figure 5.5 Relative NDCG gain in  for each algorithm overall, as well asseparated per query category.to back this effort by tailoring the search results according to each users interests,thus implicitly bringing more order into the output and demoting spam pages.In this chapter we proposed a series of algorithms meant to overcome the limitations of current Web search personalization approaches. We first showed how togeneralize personalized search in catalogues such as ODP and Google Directorybeyond the currently available search restricted to specific categories, yieldingsimple yet very effective personalization algorithms. The precision achieved byour approach significantly surpassed the precision offered by Google in severalsets of extensive experiments. The big plus of this initial algorithm is its veryfast, straightforward computation, backed by the very good output quality. Theminuses are its lack of automatic user profiling and its reduced coverage, which islimited to the contents of the input taxonomy.We then tackled the first challenge brought by our ODP enabled approach anddescribed a new algorithm that automatically learns user profiles in online systems,based on an analysis of search statistics. This was either faster, or less obtrusivethan any previous approach. Nevertheless, it still required some small amount ofuser specific information.In the third part of the chapter we introduced an algorithm meant to overcome allproblems posed by the previously existing approaches, including the above men138Paul  Alexandru Chiritationed ones. More specifically, we proposed five techniques for determining queryexpansion terms by analyzing the Personal Information Repository of each user.Each of them produced additional keywords by mining Desktop data at increasinggranularity levels, ranging from term and expression level analysis up to globalcooccurrence statistics and external thesauri. Just as before, we provided a thorough empirical analysis of several variants of our approaches, under four differentscenarios. Some of these were showed to perform very well, especially on ambiguous queries. All in all, this last algorithm had no privacy implications, completeWeb coverage, and automatic profiling with no user interaction necessary, all atthe cost of a minimal additional overhead put onto the search process.To make our results even better we proposed to make the entire personalizationprocess adaptive to the features of each query, a strong focus being put on claritylevel. Within another separate set of experiments, we showed these adaptivealgorithms to provide further improvements over our previously identified bestapproach. In fact, with adaptivity in place, our technique was better than Googleunder all tested search scenarios.A series of fine tuning studies might still be interesting to add. For example,there is no investigation upon the dependency between various query features andthe optimal number of expansion terms. Similarly, there is still no techniqueto define the best weighting of terms within the query, either within regularsearch, or especially within personalized algorithms. Last, but not least, newadaptivity metrics based for example on the query reformulation patterns wouldmost probably allow for additional quality increases.139Chapter 5. Ranking for Web Search Personalization.140Chapter 6Conclusions and Open DirectionsAs data storage capacities become larger every day, the need for effective information organization algorithms grows as well. In this thesis we identified andanalyzed in detail those applications with the most imperative demand for sucha structuring, namely Desktop Search, Spam Detection, and Web Search. Weproposed link analysis ranking as a solution for enhancing data access, buildingupon the underlying social characteristics of each application context, either atthe macroscopic, or the microscopic level. This section first summarizes our majorresearch contributions with respect to the three above mentioned domains, andthen discusses some issues which remained open for future investigations.Summary of ContributionsThe main focus of our work was to deploy link analysis ranking solutions forinformation management within all areas in which practical data organizationapproaches were either completely inexistent, or only partially developed.We started in Chapter 2 with a comprehensive overview of link analysis rankingalgorithms, as it represents the foundation of our approaches. First, we placed theranking module into a generic search engine architecture, thus showing how ourtechniques could be deployed into practical applications. Then, we presented adetailed discussion of all major aspects of PageRank, the supporting link analysisalgorithm used within our approaches. The main aspects we covered included convergence, stability, treatment dangling nodes, implementation and optimizationmechanisms, as well as possible future extensions. Consequently, we presented141Chapter 6. Conclusions and Open Directions.a similar overview for HITS, another important milestone algorithm within thehistory of link analysis. In the final part of the chapter we introduced the readerto some other nonlink features used for ranking Web pages. Also, before movingto the core of the thesis, we pin pointed the other applications in need of data organization mechanisms and motivated the selection of the above mentioned threeones.In Chapter 3 we argued that all previous search based solutions to locate information at the PC Desktop level are insufficient for the scalability requirementsimposed by current storage devices. We therefore introduced a totally new Desktop Ranking technique which builds upon link analysis in order to rank itemswithin Personal Information Repositories. Two approaches were taken when implementing the idea. First, we exploited contextual analysis of specific user actionsin order to build a link structure over the PIR and to compute local reputationvalues for personal items. Second, we generalized the algorithm by including alluser actions into the ranking algorithm, solution which was clearly much simpler,while also providing a visibly larger coverage of the personal items. Both techniques were thoroughly investigated empirically, and in both cases the Desktopsearch output quality was strongly increased, improving over the regular TFxIDFranking used by current applications at a statistically significant difference.Chapter 4 discussed Spam Detection as a bridging application between personalDesktops, social networks and the Internet. We tackled the two forms of spamwhich are by far most spread Email spam and Web spam. First, we proposedMailRank, a link analysis based approach to Email ranking and classificationwhich intelligently exploits the social communication network created via Emailexchanges. The algorithm collects its input values from the sentmail folder ofall participants, and then applies a poweriteration technique in order to ranktrustworthy senders and to detect spammers. An experimental analysis showedMailRank to yield very accurate antispam decisions, stable in the presence ofsparse networks or of various malicious attacks, while also bringing an additional,new research benefit Ordering personal Emails according to the social reputation of their sender. In the second part of the chapter we applied the very samebackground to design a novel approach to remove artificial patterns from Web hyperlink structures. We proposed to utilize link analysis at the level of Web sites,in contrast to the current approaches, built at the page level. We designed andevaluated algorithms tackling three types of inappropriate site level relationships1 Mutual reinforcements between Web sites, 2 Abnormal support from onesite towards another, and 3 Link alliances across multiple sites. Our experiments on top of the link database of the TodoBR search engine showed a qualityimprovement of up to about 60 in Mean Average Precision.142Paul  Alexandru ChiritaOur most important contributions are those tackling the Web search application,described in Chapter 5. Even though an extensive Web ranking research doesexist already, the search output quality is still average. The chapter proposed aseries of algorithms meant to overcome existing Web search limitations throughenhanced user personalization. First, we showed how to provide fast personalization based on large scale Web taxonomies by introducing an additional criterionfor Web page ranking, namely the link distance between a user profile definedwith taxonomical themes and the sets of topics covered by each URL returned inWeb search. The precision achieved by this technique significantly surpassed theprecision offered by Google search, reaching up to 63 in quality improvement.We then described a new algorithm that automatically learns user profiles notonly for our taxonomy based approach, but for any online system, exploiting simple statistics built on top of the taxonomy topics associated to the output of eachusers Web queries. The precision of this mechanism was 94 when computedat the Top10 user interests, a value above 80 being reached after only 4 daysof regular Web searching. The second half of the chapter introduced a furtherenhanced personalized search framework which brought three major additionalimprovements over our taxonomy based approach 1 No privacy implications,2 Complete Web coverage unrestricted to the size of the taxonomy, and 3Automatic profiling with no user interaction necessary. More specifically, we proposed five techniques for determining Web query expansion terms by analyzing thePersonal Information Repository of each user. Each of them produced additionalkeywords by mining Desktop data at increasing granularity levels, ranging fromterm and expression level analysis up to global cooccurrence statistics and external thesauri. Some of these techniques performed very well, especially on queriesdeemed ambiguous by our testers. Finally, to bring our results even closer to theuser needs, we investigated the design of a search process adaptive to the featuresof each query, especially to its clarity level. This last approach provided additionalimprovements over all our previous algorithms and yielded better quality outputthan Google under any tested scenario.The headline results of this thesis are 1 The initiation of a new Desktop researchstream oriented on Usage Analysis also considered by TREC1 for addition as maintrack in 2008, 2 An increased awareness of the importance of both Desktop dataand large scale taxonomies for Web Search Personalization, 3 The general ideaof exploiting social network information for Email spam detection, as well as 4Quite several algorithm and system design considerations for link analysis enabledapplications.1Text REtrieval Conference, httptrec.nist.gov.143Chapter 6. Conclusions and Open Directions.Open DirectionsResearch on a topic is almost never complete. New ideas generate new problems,which in turn generate new ideas, and so on. Especially as some of our contributions opened new research paths, there exist a few areas we find interestingfor future investigation. Let us discuss them separated onto the applications wetackled in the thesis.Desktop Search. This is the most rife domain with respect to possible additionalsteps. First, it remains unclear how to accurately detect personal items which areaccessed together and also belong to the same activity context. Many approachescould be envisioned, for example by automatically trimming the Desktop linksgenerated by our current generalized technique, or by developing enhanced linkingheuristics, or simply by developing an improved ranking algorithm. Second, onemight also investigate other ranking extensions which include non access basedheuristics as well, thus addressing more local resources, even when these havenever been opened by the user. Third, since Desktop Usage Analysis has notbeen investigated in the past, it could thus be exploited for quite a lot of otherideas. For instance one could attempt to combine textual evidences with usageanalysis in order to achieve a more exact clustering or context detection overPersonal Information Repositories, or just over Email inboxes, and so on.Spam Detection. Though being very accurate, our Email reputation approachto spam detection has not entirely solved the problem. If broadly accepted by thecommunity, link analysis could be further investigated to provide more complex,secure algorithms, optimized for neutralizing malicious users, in a similar fashionto the Web approaches. Also, as wide usage would imply less scalability, MailRankor its enhanced followup Email antispam algorithm should be moved towards adistributed infrastructure, thus implicitly allowing for more possible points offailure as well. In general, we believe that the power of social networks has notyet been fully exploited within the current spam detection systems, and thus muchmore is yet to come. With respect to Web spam detection, there are two broadproblems which we left unsolved. First, it might be beneficial to assess the spamlevel of each link, and weight the links accordingly, rather than removing themalicious candidates entirely. Second, in a more general sense, it is still unclearhow to optimally combine the already several complementary Web spam detectionapproaches, such as the page level and the site level algorithms. Quite severalsolutions are possible, for example to apply them in a sequence, or altogether,weighted according to the predictions of some machine learning algorithm, etc.Web Search Personalization. While we strived to bring Web Search Personalization as close to perfection as possible, there are of course some issues in need144Paul  Alexandru Chiritaof further investigation. Our Desktop based query expansion framework wouldclearly benefit from an investigation upon the dependency between various queryfeatures and the optimal number of expansion terms. Similarly, there is still notechnique to define the best weighting of terms within the query, either withinregular search, or especially within personalized algorithms. Last, and most challenging, we believe it would be highly beneficial to model the query reformulationprocess of each user. Providing such an approach to learn search behaviors wouldin the future allow to personalize the personalization algorithm, thus bringingan ultimate adaptivity into search. Yet many unknowns exist there Which arethe metrics that best indicate users behavior Which is the importance of eachof them How should they be combinedWe still find ourselves in an early phase of the digital information era. This iswhy we believe a lot of ranking research is still to come, either built on top of linkor text analysis, or on top of more complex statistical metrics, etc. Besides thealready existing wide interest for ranking in the Web, a lot more collateral Information Management problems will appear, such as information organization andalignment within a company Intranet, information structuring within specializedrepositories e.g., a server dedicated to company reports, and so on. For all ofthem, ranking will help, either by playing a minor or even a major role in theultimate solution to each problem.145Chapter 6. Conclusions and Open Directions.146Bibliography1 K. Aberer and J. Wu. A framework for decentralized ranking in web information retrieval. In The Fifth Asia Pacific Web Conference, APWeb,2003.2 S. Abiteboul, M. Preda, and G. Cobena. Adaptive online page importancecomputation. In Proc. of the 12th Intl. Conf. on World Wide Web, 2003.3 S. Acharyya and J. Ghosh. Outlink estimation for pagerank computationunder missing data. In Proc. of the 13th Intl. World Wide Web Conf., 2004.4 E. Adar, D. Kargar, and L. A. Stein. Haystack peruser informationenvironments. In Proc. of the 8th Intl. CIKM Conf. on Information andKnowledge Management, 1999.5 M. S. Aktas, M. A. Nacar, and F. Menczer. Personalizing pagerank based ondomain profiles. In Proc. of the KDD Workshop on Web Mining and UsageAnalysis held in conjunction with the 10th ACM International SIGKDDConference, 2004.6 R. Albert, H. Jeong, and A.L. Barabasi. Diameter of the World Wide Web.Nature, 401130131, September 1999.7 B. AlemanMeza, C. Halaschek, I. B. Arpinar, and A. Sheth. Contextawaresemantic association ranking. In Semantic Web and Databases Workshop,2003.8 J. Allan and H. Raghavan. Using partofspeech patterns to reduce queryambiguity. In Proc. of the 25th Intl. ACM SIGIR Conf. on Research anddevelopment in information retrieval, 2002.147Chapter BIBLIOGRAPHY.9 G. Amati, C. Carpineto, and G. Romano. Query difficulty, robustness, andselective application of query expansion. In Proc. of the ECIR EuropeanConf. on Information Retrieval, 2004.10 E. Amitay, D. Carmel, A. Darlow, R. Lempel, and A. Soffer. The connectivity sonar detecting site functionality by structural patterns. In Proceedingsof the 14th ACM Conference on Hypertext and Hypermedia, 2003.11 P. G. Anick and S. Tipirneni. The paraphrase search assistant Terminological feedback for iterative information seeking. In Proc. of the 22nd Intl.ACM SIGIR Conf. on Research and Development in Information Retrieval,1999.12 A. Arasu, J. Novak, A. Tomkins, and J. Tomlin. Pagerank computation andthe structure of the web Experiments and algorithms. In Proc. of the 11thIntl. World Wide Web Conf., 2002.13 G. Attardi, A. Gull, and F. Sebastiani. Automatic web page categorizationby link and context analysis. In Proc. of the THAI European Symp. onTelematics, Hypermedia and Artificial Intelligence, 1999.14 R. BaezaYates, P. Boldi, and C. Castillo. Generalizing pagerank Dampingfunctions for linkbased ranking algorithms. In Proc. of the 29th Intl. ACMSIGIR Conf. on Research and Development in Information Retrieval, 2006.15 R. BaezaYates, C. Castillo, and V. Lopez. Pagerank increase under different collusion topologies. In First International Workshop on AdversarialInformation Retrieval on the Web, 2005.16 R. BaezaYates and E. Davis. Web page ranking using link attributes. InProc. of the 13th Intl. World Wide Web Conf., 2004.17 R. BaezaYates and B. RibeiroNeto. Modern Information Retrieval. ACMPress  AddisonWesley, 1999.18 A. Balmin, V. Hristidis, and Y. Papakonstantinou. Objectrank Authoritybased keyword search in databases. In Proc. of the Intl. VLDB Conf. onVery Large Databases, 2004.19 A.L. Barabasi and R. Albert. Emergence of scaling in random networks.Science, 286509512, 1999.20 D. Barreau and B. Nardi. Finding and reminding File organization fromthe desktop. ACM SIGCHI Bulletin, 2733943, 1995.148Paul  Alexandru Chirita21 L. Becchetti and C. Castillo. The distribution of pagerank follows a powerlaw only for particular values of the damping factor. In Proc. of the 15thIntl. Conf. on World Wide Web, 2006.22 L. Becchetti, C. Castillo, D. Donato, S. Leonardi, and R. BaezaYates. Linkbased characterization and detection of web spam. In Prod. of the 2nd Intl.AIRWeb Workshop on Adversarial Information Retrieval on the Web, 2006.23 L. Becchetti, C. Castillo, D. Donato, S. Leonardi, and R. BaezaYates. Usingrank propagation and probabilistic counting for linkbased spam detection.In Proc. of the WebKDD Workshop on Web Mining and Web Usage Analysis, 2006.24 S. M. Beitzel, E. C. Jensen, A. Chowdhury, D. Grossman, and O. Frieder.Hourly analysis of a very large topically categorized web query log. InProc. of the 27th Intl. ACM SIGIR Conf. on Research and Development inInformation Retrieval, 2004.25 A. A. Benczur, K. Csalogany, T. Sarlos, and M. Uher. Spamrank  fullyautomatic link spam detection. In First International Workshop on Adversarial Information Retrieval on the Web, 2005.26 K. Bharat, A. Z. Broder, J. Dean, and M. R. Henzinger. A comparison oftechniques to find mirrored hosts on the WWW. Journal of the AmericanSociety of Information Science, 511211141122, 2000.27 K. Bharat and M. R. Henzinger. Improved algorithms for topic distillationin a hyperlinked environment. In Proc. of the 21st Intl. SIGIR Conf. onResearch and Development in Information Retrieval, 1998.28 A. Bifet, C. Castillo, P. A. Chirita, and I. Weber. An analysis of factorsused in search engine ranking. In Proc. of the 1st Workshop on AdversarialInformation Retrieval held at the 14th Intl. World Wide Web Conf., 2005.29 A. Borodin, G. O. Roberts, J. S. Rosenthal, and P. Tsaparas. Findingauthorities and hubs from link structures on the world wide web. In Proc.of the 10th Intl. Conf. on World Wide Web, 2001.30 J. Bortz. Statistics for Social Scientists. Springer, 1993.31 P. Boykin and V. Roychowdhury. Leveraging social networks to fight spam.IEEE Computer, 3846168, 2005.149Chapter BIBLIOGRAPHY.32 S. Brin, R. Motwani, L. Page, and T. Winograd. What can you do with aweb in your pocket Data Engineering Bulletin, 2123747, 1998.33 S. Brin and L. Page. The anatomy of a largescale hypertextual Web searchengine. Computer Networks and ISDN Systems, 3017107117, 1998.34 A. Broder, R. Kumar, F. Maghoul, P. Raghavan, S. Rajagopalan, R. Stata,A. Tomkins, and J. Wiener. Graph structure in the web. In Proc. of the 9thIntl. World Wide Web Conf., 2000.35 A. Z. Broder, S. C. Glassman, M. S. Manasse, and G. Zweig. Syntacticclustering of the web. Comput. Netw. ISDN Syst., 2981311571166, 1997.36 A. Z. Broder, R. Lempel, F. Maghoul, and J. Pedersen. Efficient pagerankapproximation via graph aggregation. In Proc. of the 13th Intl. World WideWeb Conf., 2004.37 I. Brunkhorst, P.A. Chirita, S. Costache, J. Gaugaz, E. Ioannou, T. Iofciu,E. Minack, W. Nejdl, and R. Paiu. The beagle toolbox Towards anextendable desktop search architecture. In Proc. of the Semantic DesktopWorkshop held at the 5th Intl. Semantic Web Conf., 2006.38 J. Budzik and K. Hammond. Watson Anticipating and contextualizinginformation needs. In Proceedings of the Sixtysecond Annual Meeting ofthe American Society for Information Science, 1999.39 C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton,and G. Hullender. Learning to rank using gradient descent. In Proc. 22ndInternational Conf. on Machine Learning, 2005.40 D. Carmel, E. Farchi, Y. Petruschka, and A. Soffer. Automatic query wefinement using lexical affinities with maximal information gain. In Proc. of the25th Intl. ACM SIGIR Conf. on Research and development in informationretrieval, 2002.41 D. Carmel, E. YomTov, A. Darlow, and D. Pelleg. What makes a querydifficult In Proc. of the 29th Intl. ACM SIGIR Conf. on Research anddevelopment in information retrieval, 2006.42 C. Carpineto, R. de Mori, G. Romano, and B. Bigi. An informationtheoreticapproach to automatic query expansion. ACM Transactions on InformationSystems, 191127, 2001.150Paul  Alexandru Chirita43 J. Carriere and R. Kazman. Webquery Searching and visualizing the Webthrough connectivity. In Proceedings of the 6th International World WideWeb Conference, 1997.44 A. Carvalho, P. A. Chirita, E. S. de Moura, P. Calado, and W. Nejdl. Sitelevel noise removal for search engines. In Proc. of the 15th Intl. World WideWeb Conf., 2006.45 S. Chakrabarti. Integrating the document object model with hyperlinks forenhanced topic distillation and information extraction. In Proc. of the 10thIntl. Conf. on World Wide Web, 2001.46 S. Chakrabarti. Mining the Web Discovering Knowledge from HypertextData. Morgan Kaufmann, 2003.47 S. Chakrabarti, B. Dom, D. Gibson, J. Kleinberg, P. Raghavan, and S. Rajagopalan. Automatic resource list compilation by analyzing hyperlink structure and associated text. In Proceedings of the 7th International World WideWeb Conference, 1998.48 P. Chan. Constructing web user profiles A noninvasive learning approach.Web Usage Analysis and User Profiling, Springer LNAI 1836, 2000.49 C.H. Chang and C.C. Hsu. Integrating query expansion and conceptualrelevance feedback for personalized web information retrieval. In Proc. ofthe 7th Intl. Conf. on World Wide Web, 1998.50 L. Chen and P. Pu. Survey of preference elicitation methods. Technicalreport, IC200467  EPFL, 2004.51 Y.Y. Chen, Q. Gan, and T. Suel. Ioefficient techniques for computingpagerank, 2002.52 S. Chernov, P. Serdyukov, P.A. Chirita, G. Demartini, and W. Nejdl. 2.building a desktop search testbed. In Proc. of the 29th European Conferenceon Information Retrieval, 2006.53 A. Cheyer, J. Park, and R. Giuli. Iris Integrate. relate. infer. share. In Proc.of the 1st Workshop on the Semantic Desktopheld at the Intl. Semantic WebConf., 2005.54 S. Chien, C. Dwork, R. Kumar, D. Simon, and D. Sivakumar. Link evolution Analysis and algorithms. Internet Mathematics, 13277304, 2004.151Chapter BIBLIOGRAPHY.55 P. A. Chirita, S. Costache, J. Gaugaz, and W. Nejdl. Desktop contextdetection using implicit feedback. In Proc. of the Personal InformationManagement Workshop held at the 29th Intl. ACM SIGIR Conf. on Researchand Development in Information Retrieval, 2006.56 P. A. Chirita, S. Costache, S. Handschuh, and W. Nejdl. Ptag Large scaleautomatic generation of personalized annotation tags for the web. In Proc.of the 16th Intl. World Wide Web Conf., 2007.57 P. A. Chirita, A. Damian, W. Nejdl, and W. Siberski. Search strategies forscientific collaboration networks. In Proc. of the P2P Information RetrievalWorkshop held at the 14th ACM Intl. CIKM Conf. on Information andKnowledge Management, 2005.58 P. A. Chirita, J. Diederich, and W. Nejdl. Mailrank Using ranking forspam detection. In Proc. of the 14th Intl. CIKM Conf. on Information andKnowledge Management, 2005.59 P. A. Chirita, C. S. Firan, and W. Nejdl. Pushing task relevant web linksdown to the desktop. In Proc. of the 8th ACM Intl. Workshop on WebInformation and Data Management held at the 15th Intl. ACM CIKM Conf.on Information and Knowledge Management, 2006.60 P. A. Chirita, C. S. Firan, and W. Nejdl. Summarizing local context topersonalize global web search. In Proc. of the 15th Intl. CIKM Conf. onInformation and Knowledge Management, 2006.61 P. A. Chirita, R. Gavriloaie, S. Ghita, W. Nejdl, and R. Paiu. Activitybased metadata for semantic desktop search. In Proc. of the 2nd EuropeanSemantic Web Conference, Heraklion, Greece, 2005.62 P. A. Chirita, S. Ghita, W. Nejdl, and R. Paiu. Semantically enhancedsearching and ranking on the desktop. In Proc. of the Semantic DesktopWorkshop held at the 4th Intl. Semantic Web Conf., 2005.63 P. A. Chirita, S. Ghita, W. Nejdl, and R. Paiu. Beagle Semanticallyenhanced searching and ranking on the desktop. In Proc. of the 3rd EuropeanSemantic Web Conference, 2006.64 P. A. Chirita, S. Idreos, M. Koubarakis, and W. Nejdl. Designing semanticpublishsubscribe networks using superpeers. In Semantic Web and PeerToPeer book, Jan 2004.152Paul  Alexandru Chirita65 P. A. Chirita, S. Idreos, M. Koubarakis, and W. Nejdl. Publishsubscribefor RDFbased P2P networks. In Proc. of the 3rd European Semantic WebConference, 2004.66 P. A. Chirita and W. Nejdl. Analyzing user behavior to rank desktopitems. In Proc. of the 13th Intl. Symp. on String Processing and InformationRetrieval SPIRE, 2006.67 P. A. Chirita, W. Nejdl, R. Paiu, and C. Kohlschutter. Using odp metadatato personalize search. In Proc. of the 28th Intl. ACM SIGIR Conf. onResearch and Development in Information Retrieval, 2005.68 P. A. Chirita, W. Nejdl, M. Schlosser, and O. Scurtu. Personalized reputation management in p2p networks. In Proc. of the Workshop on Trust,Security and Reputation held at the 3rd Intl. Semantic Web Conf., 2004.69 P. A. Chirita, W. Nejdl, and O. Scurtu. Knowing where to search Personalized search strategies for peers in p2p networks. In Proc. of the P2PInformation Retrieval Workshop held at the 27th Intl. ACM SIGIR Conf.,2004.70 P. A. Chirita, W. Nejdl, and C. Zamfir. Preventing shilling attacks in onlinerecommender systems. In Proc. of the 7th ACM Intl. Workshop on WebInformation and Data Management held at the 14th Intl. ACM CIKM Conf.on Information and Knowledge Management, 2006.71 P. A. Chirita, D. Olmedilla, and W. Nejdl. Finding related hubs and authorities. In IEEE, editor, In Proc. of the First Latin American Web CongressLAWEB, Santiago, Chile, Nov 2003.72 P. A. Chirita, D. Olmedilla, and W. Nejdl. Finding related pages on thelink structure of the www. In Proc. of the 3rd IEEEWICACM Intl. Conf.on Web Intelligence, 2004.73 P. A. Chirita, D. Olmedilla, and W. Nejdl. Pros A personalized ranking platform for web search. In Proc. of the 3rd Intl. Conf. on AdaptiveHypermedia and Adaptive WebBased Systems, 2004.74 M. Claypool, D. Brown, P. Le, and M. Waseda. Inferring user interest.IEEE Internet Computing, 56, 2001.75 D. Cohn and H. Chang. Learning to probabilistically identify authoritativedocuments. In Proc. 17th International Conf. on Machine Learning, 2000.153Chapter BIBLIOGRAPHY.76 T. Cover and J. Thomas. Elements of Information Theory. Wiley, 1991.77 S. CronenTownsend, Y. Zhou, and W. B. Croft. Predicting query performance. In Proc. of the 25th Intl. ACM SIGIR Conf. on Research anddevelopment in information retrieval, 2002.78 H. Cui, J.R. Wen, J.Y. Nie, and W.Y. Ma. Probabilistic query expansionusing query logs. In Proc. of the 11th Intl. Conf. on World Wide Web, 2002.79 B. Davison. Recognizing nepotistic links on the web. In Proceedings of theAAAI2000 Workshop on Artificial Intelligence for Web Search, 2000.80 S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W. Furnas, and R. A.Harshman. Indexing by latent semantic analysis. Journal of the AmericanSociety of Information Science, 416391407, 1990.81 J.Y. Delort, B. BouchonMeunier, and M. Rifqi. Enhanced web documentsummarization using hyperlinks. In Proc. of the 14th ACM HYPERTEXTConf. on Hypertext and Hypermedia, 2003.82 M. Diligenti, M. Gori, and M. Maggini. A unified probabilistic frameworkfor web page scoring systems. IEEE Transactions on Knowledge and DataEngineering, 161416, 2004.83 C. Ding, X. He, P. Husbands, H. Zha, and H. D. Simon. Pagerank, hits anda unified framework for link analysis. In Proc. of the 25th Intl. SIGIR Conf.on Research and Development in Information Retrieval, 2002.84 L. Ding, T. Finin, A. Joshi, R. Pan, R. S. Cost, Y. Peng, P. Reddivari,V. C. Doshi, and J. Sachs. Swoogle A search and metadata engine for thesemantic web. In Proc. of the 13th ACM CIKM Conf. on Information andKnowledge Management, 2004.85 X. L. Dong and A. Halevy. A platform for personal information managementand integration. In Proc. of the 2nd Conf. on Innovative Data SystemsResearch CIDR, 2005.86 S. Dumais, E. Cutrell, J. Cadiz, G. Jancke, R. Sarin, and D. C. Robbins.Stuff ive seen A system for personal information retrieval and reuse. InProc. of the Intl. SIGIR Conf. on Research and Development in InformationRetrieval, 2003.87 T. Dunning. Accurate methods for the statistics of surprise and coincidence.Computational Linguistics, 196174, 1993.154Paul  Alexandru Chirita88 H. Ebel, L. I. Mielsch, and S. Bornholdt. Scalefree topology of emailnetworks. Physical Review E 66, 2002.89 H. P. Edmundson. New methods in automatic extracting. Journal of theACM, 162264285, 1969.90 E. N. Efthimiadis. User choices A new yardstick for the evaluation ofranking algorithms for interactive query expansion. Information Processingand Management, 314605620, 1995.91 N. Eiron and K. S. McCurley. Untangling compound documents on the web.In Proc. of the 14th ACM Conference on Hypertext and Hypermedia, 2003.92 N. Eiron, K. S. McCurley, and J. A. Tomlin. Ranking the web frontier. InProceedings of 13th International World Wide Web Conference, 2004.93 G. Erkan and D. Radev. Lexpagerank prestige in multidocument textsummarization. In Proc. of the 4th Intl. Conf. on Empirical Methods inNatural Language Processing, 2004.94 G. Erkan and D. R. Radev. Lexrank Graphbased lexical centrality assalience in text summarization. Journal of Artificial Intelligence Research,22, 2004.95 B. Fallenstein. Fentwine A navigational rdf browser and editor. In Proceedings of 1st Workshop on Friend of a Friend, Social Networking and theSemantic Web, 2004.96 G. Feng, T.Y. Liu, Y. Wang, Y.Bao, Z. Ma, X.D. Zhang, and W.Y. Ma.Aggregaterank Bringing order to web sites. In In Proc. of the 29th Intl.ACM SIGIR Conf. on Research and Development in Information Retrieval,2006.97 P. Ferragina and A. Gulli. A personalized search engine based on websnippet hierarchical clustering. In Proc. of the 14th Intl. Conf. on WorldWide Web, 2005.98 S. Fertig, E. Freeman, and D. Gelernter. Lifestreams An alternative tothe desktop metaphor. In Proc. of the ACM Conf. on Human Factors inComputing Systems, 1996.99 D. Fetterly, M. Manasse, and M. Najork. Spam, damn spam, and statistics using statistical analysis to locate spam web pages. In WebDB 04Proceedings of the 7th International Workshop on the Web and Databases,2004.155Chapter BIBLIOGRAPHY.100 M. Fisher and R. M. Everson. When are links useful experiments in textclassification. In Proc. of the European Conf. on Information Retrieval,2003.101 D. Fogaras and B. Racz. Scaling link based similarity search. In Proc. ofthe 14th Intl. World Wide Web Conf., 2005.102 E. Freeman and S. Fertig. Lifestreams Organizing your electronic life. InProc. of the AAAI Symposium on AI Applications in Knowledge Navigationand Retrieval, 1995.103 E. Garfield. Citation indexes for science. Science, 122108111, 1955.104 E. Garfield. Science citation index A new dimension in indexing. Science,144649654, 1964.105 E. Garfield. Citation analysis as a tool in journal evaluation. Science,178471479, 1972.106 E. Garfield. Citation Indexing Its Theory and Application in Science,Technology and Humanities. ISI Press, 1983.107 S. Gauch, J. Chaffee, and A. Pretschner. Ontologybased personalized searchand browsing. Web Intelli. and Agent Sys., 134219234, 2003.108 D. Geer. Will new standards help curb spam IEEE Computer, pages1416, Feb. 2004.109 J. Gemmell, G. Bell, R. Lueder, S. Drucker, and C. Wong. Mylifebitsfulfilling the memex vision. In Proc. of the ACM Conference on Multimedia,2002.110 D. Gibson, J. Kleinberg, and P. Raghavan. Inferring web communities fromlink topology. In Proceedings of the 9th ACM Conference on Hypertext andHypermedia, 1998.111 D. Gleich, L. Zhukov, and P. Berkhin. Fast parallel PageRank A linearsystem approach. Technical report, Yahoo Research Labs, 2004.112 J. Golbeck and J. Hendler. Reputation Network Analysis for Email Filtering.In Proc. of the Conference on Email and AntiSpam CEAS, MountainView, CA, USA, July 2004.156Paul  Alexandru Chirita113 A. Gray and M. Haahr. Personalised, Collaborative Spam Filtering. In Proc.of the Conference on Email and AntiSpam CEAS, Mountain View, CA,USA, July 2004.114 Z. Gyongyi and H. GarciaMolina. Link spam alliances. In Proc. of the 31stIntl. VLDB Conf. on Very Large Data Bases, 2005.115 Z. Gyongyi and H. GarciaMolina. Web spam taxonomy. In Proceedings ofthe Adversarial Information Retrieval held the 14th Intl. World Wide WebConference, 2005.116 Z. Gyongyi, H. GarciaMolina, and J. Pendersen. Combating web spamwith trustrank. In Proceedings of the 30th International VLDB Conference,2004.117 T. Haveliwala. Efficient encodings for document ranking vectors. In Proc.of the Intl. Conf. on Internet Computing, 2003.118 T. Haveliwala. Topicsensitive pagerank. In In Proceedings of the EleventhInternational World Wide Web Conference, Honolulu, Hawaii, May 2002.119 T. Haveliwala and S. Kamvar. The second eigenvalue of the google matrix.Technical report, Stanford University, 2003.120 T. H. Haveliwala. Efficient computation of PageRank. Technical report,Stanford University, 1999.121 D. Hawking, E. Voorhees, N. Craswell, and P. Bailey. Overview of the trec8web track. In Eighth Text REtrieval Conference, 1999.122 B. He and I. Ounis. Inferring query performance using preretrieval predictors. In Proc. of the 11th Intl. SPIRE Conf. on String Processing andInformation Retrieval, 2004.123 T. Hoffmann. Probabilistic latent semantic indexing. In Proc. of the 22ndIntl. ACM SIGIR Conf. on Research and Development in Information Retrieval, 1999.124 A. Hotho, R. Jaschke, C. Schmitz, and G. Stumme. Information retrieval infolksonomies Search and ranking. In Proc. of the European Semantic WebConference, 2006.125 B. A. Huberman and L. A. Adamic. Information dynamics in the networkedworld. Complex Networks, Lecture Notes in Physics, 2003.157Chapter BIBLIOGRAPHY.126 J. Hull and P. Hart. Toward zeroeffort personal doc. management. IEEEComputer, 3433035, 2001.127 D. Huynh, D. Karger, and D. Quan. Haystack A platform for creating,organizing and visualizing information using rdf. In Proc. of the Sem. WebWorkshop held at 11th World Wide Web Conf., 2002.128 Isode. Benchmark and comparison of spamassassin and mswitch antispam.Technical report, Isode, Apr. 2004.129 T. J., A. C., A. M. S., and K. D. R. The perfect search engine is notenough A study of orienteering behavior in directed search. In In Proc. ofCHI, 2004.130 K. Jarvelin and J. Keklinen. Ir evaluation methods for retrieving highly relevant documents. In Proc. of the 23th Intl. ACM SIGIR Conf. on Researchand development in information retrieval, 2000.131 G. Jeh and J. Widom. Scaling personalized web search. In Proc. of the 12thIntl. World Wide Web Conference, 2003.132 K. S. Jones, S. Walker, and S. Robertson. Probabilistic model of informationretrieval Development and status. Technical report, Cambridge University,1998.133 S. Kamvar, T. Haveliwala, C. Manning, and G. Golub. Exploiting the blockstructure of the web for computing pagerank. Technical report, StanfordUniversity, 2003.134 S. D. Kamvar, T. H. Haveliwala, C. D. Manning, and G. H. Golub. Extrapolation methods for accelerating PageRank computations. In Proc. of the12th Intl. Conf. on the World Wide Web, 2003.135 S. D. Kamvar, M. T. Schlosser, and H. GarciaMolina. The eigentrustalgorithm for reputation management in p2p networks. In Proc. of the12th Intl. Conf. on World Wide Web, 2003.136 D. R. Karger, K. Bakshi, D. Huynh, D. Quan, and V. Sinha. HaystackA customizable generalpurpose information management tool for end usersof semistructured data. In Proc. of the 1st Intl. Conf. on Innovative DataSyst., 2003.137 L. Katz. A new status index derived from sociometric analysis. Psychometrika, 183943, 1953.158Paul  Alexandru Chirita138 D. Kelly and J. Teevan. Implicit feedback for inferring user preference abibliography. SIGIR Forum, 3721828, 2003.139 M. Kendall. Rank Correlation Methods. Hafner Publishing, 1955.140 L. Kerschberg, W. Kim, and A. Scime. A personalizable agent for semantictaxonomybased web search. In First International Workshop on RadicalAgent Concepts, volume 2564 of LNCS. Springer, 2002.141 M.C. Kim and K.S. Choi. A comparison of collocationbased similaritymeasures in query expansion. Information Processing and Management,3511930, 1999.142 S.B. Kim, H.C. Seo, and H.C. Rim. Information retrieval using wordsenses root sense tagging approach. In Proc. of the 27th Intl. ACM SIGIRConf. on Research and development in information retrieval, 2004.143 J. M. Kleinberg. Authoritative sources in a hyperlinked environment. Journal of the ACM, 465604632, 1999.144 C. Kohlschtter, P. A. Chirita, and W. Nejdl. 9. using link analysis to identifyaspects in faceted web search. In Proc. of the Workshop on Faceted Searchheld at the 29th Intl. ACM SIGIR Conf. on Research and Development inInformation Retrieval, 2006.145 C. Kohlschtter, P. A. Chirita, and W. Nejdl. Efficient parallel computationof pagerank. In Proc. of the 28th European Conference on InformationRetrieval, 2006.146 J. Kong, P. Boykin, B. Rezaei, N. Sarshar, and V. Roychowdhury. Let yourCyberAlter Ego Share Information and Manage Spam. Technical report,University of California, USA, 2005. Preprint.147 R. Kraft and J. Zien. Mining anchor text for query refinement. In Proc. ofthe 13th Intl. Conf. on World Wide Web, 2004.148 R. Krovetz and W. B. Croft. Lexical ambiguity and information retrieval.ACM Trans. Inf. Syst., 102, 1992.149 R. Kumar, P. Raghavan, S. Rajagopalan, and A. Tomkins. Trawling the webfor emerging cybercommunities. In Proc. of the 8th Intl. Conf. on WorldWide Web, 1999.159Chapter BIBLIOGRAPHY.150 S. R. Kumar, P. Raghavan, S. Rajagopalan, D. Sivakumar, A. Tomkins,and E. Upfal. The Web as a graph. In Proceedings of the Symposium onPrinciples of Database Systems, 2000.151 W. Lam, S. Mukhopadhyay, J. Mostafa, and M. Palakal. Detection ofshifts in user interests for personalized information filtering. In Proc. of the19th Intl. ACM SIGIR Conf. on Research and Development in InformationRetrieval, 1996.152 A. M. LamAdesina and G. J. F. Jones. Applying summarization techniquesfor term selection in relevance feedback. In Proc. of the 24th Intl. ACMSIGIR Conf. on Research and Development in Information Retrieval, 2001.153 A. N. Langville and C. D. Meyer. Deeper inside PageRank. Internet Mathematics to appear, 2004.154 A. N. Langville and C. D. Meyer. A reordering for the PageRank problem.Technical report, NCSU, 2004.155 A. N. Langville and C. D. Meyer. A survey of eigenvector methods of webinformation retrieval. The SIAM Review, 471135161, 2005.156 M. Lansdale. The psychology of personal information management. AppliedErgonomics, 1915566, March 1988.157 H. C. Lee and A. Borodin. Perturbation of the hyperlinked environment. InProceedings of COCOON, LNCS 2696, 2003.158 R. Lempel and S. Moran. The stochastic approach for linkstructure analysis SALSA and the TKC effect. Computer Networks Amsterdam, Netherlands 1999, 3316387401, 2000.159 R. Lempel and S. Moran. Rankstability and ranksimilarity of linkbasedweb ranking algorithms in authorityconnected graphs. Inf. Retr., 82245264, 2005.160 M. Levene, T. Fenner, G. Loizou, and R. Wheeldon. A stochastic model forthe evolution of the web. Computer Networks, 39277287, 2002.161 L. Li, Y. Shang, and W. Zhang. Improvement of hitsbased algorithms onweb documents. In Proc. of the 11th Intl. Conf. on World Wide Web, 2002.162 Y. Li, Z. A. Bandar, and D. McLean. An approach for measuring semanticsimilarity between words using multiple information sources. IEEE Trans.on Knowledge and Data Eng., 154871882, 2003.160Paul  Alexandru Chirita163 F. Liu, C. Yu, and W. Meng. Personalized web search for improving retrievaleffectiveness. IEEE Trans. on Knowledge and Data Eng., 1612840, 2004.164 A. G. Maguitman, F. Menczer, H. Roinestad, and A. Vespignani. Algorithmic detection of semantic similarity. In Proc. of the 14th Intl. Conf. onWorld Wide Web, 2005.165 T. Malone. How do people organize their desks implications for the designof office information systems. ACM Transactions on Office InformationSystems, 1199112, 1983.166 C. Martindale and A. K. Konopka. Oligonucleotide frequencies in dna followa yule distribution. Computer and Chemistry, 2013538, 1996.167 G. Miller. Wordnet An electronic lexical database. Communications of theACM, 38113941, 1995.168 M. E. J. Newman, S. Forrest, and J. Balthrop. Email networks and thespread of computer viruses. Physical Review E 66, 2002.169 A. Y. Ng, A. X. Zheng, and M. I. Jordan. Stable algorithms for link analysis.In Proc. 24th Annual Intl. ACM SIGIR Conference, 2001.170 L. Nie, B. Davison, and X. Qi. Topical link analysis for web search. In InProc. of the 29th Intl. ACM SIGIR Conf. on Research and Development inInformation Retrieval, 2006.171 M. of the CLEVER Project. Hypersearching the web. Technical report,IBM, 2003.172 L. Page, S. Brin, R. Motwani, and T. Winograd. The PageRank citationranking Bringing order to the web. Technical report, Stanford DigitalLibrary Technologies Project, 1998.173 S. M. Pahlevi and H. Kitagawa. Taxonomybased adaptive web searchmethod. In Intl. Symp. on Information Technology, 2002.174 G. Pandurangan, P. Raghavan, and E. Upfal. Using pagerank to characterizeweb structure. In Proc. of the 8th Intl. COCOON Conf. on Computing andCombinatorics, 2002.175 M. Perone. An overview of spam blocking techniques. Technical report,Barracuda Networks, 2004.161Chapter BIBLIOGRAPHY.176 F. Qiu and J. Cho. Automatic indentification of user interest for personalizedsearch. In Proc. of the 15th Intl. World Wide Web Conf., 2006.177 Y. Qiu and H.P. Frei. Concept based query expansion. In Proc. of the16th Intl. ACM SIGIR Conf. on Research and Development in InformationRetrieval, 1993.178 D. Quan and D. Karger. How to make a semantic web browser. In Proc. ofthe 13th Intl. World Wide Web Conf., 2004.179 D. Rafiei and A. O. Mendelzon. What is this page known for Computingweb page reputations. In Proceedings of the 9th International World WideWeb Conference, 2000.180 P. Resnick and H. Varian. Recommender Systems. Commun. ACM,4035658, 1997.181 M. Richardson, R. Agrawal, and P. Domingos. Trust management for thesemantic web. In Proceedings of the 2nd International Semantic Web Conference, 2003.182 M. Richardson, A. Prakash, and E. Brill. Beyond pagerank Machine learning for static ranking. In Proc. of the 15th Intl. World Wide Web Conf.,2006.183 M. Ringel, E. Cutrell, S. Dumais, and E. Horvitz. Milestones in timeThe value of landmarks in retrieving information from personal stores. InINTERACT, 2003.184 G. Roberts and J. Rosenthal. Downweighting tightly knit communities inworld wide web rankings. Advances and Applications in Statistics ADAS,3199216, 2003.185 J. Rocchio. Relevance feedback in information retrieval. The Smart RetrievalSystem Experiments in Automatic Document Processing, pages 313323,1971.186 I. Ruthven. Reexamining the potential effectiveness of interactive queryexpansion. In Proc. of the 26th Intl. ACM SIGIR Conf. on Research anddevelopment in informaion retrieval, 2003.187 H. Sakagami and T. Kamba. Learning personal preferences on online newspaper articles from user behaviors. In Proceedings of the 6th InternationalWorld Wide Web Conference, 1997.162Paul  Alexandru Chirita188 G. Salton and M. J. McGill. Introduction to Modern Information Retrieval.McGrawHill, Inc., 1986.189 K. Sankaralingam, S. Sethumadhavan, and J. C. Browne. Distributed pagerank for p2p systems. In Proc. of the 12th IEEE Intl. Symp. on High Performance Distributed Computing HPDC, 2003.190 T. Sarlos, A. A. Benczur, K. Csalogany, D. Fogaras, and B. Racz. Torandomize or not to randomize Space optimal summaries for hyperlinkanalysis. In Proc. of the 15th Intl. World Wide Web Conf., 2006.191 L. Sauermann. Using semantic web technologies to build a semantic desktop.Masters thesis, TU Vienna, 2003.192 L. Sauermann and S. Schwarz. Gnowsis adapter framework Treating structured data sources as virtual rdf graphs. In Proc. of the 4th Intl. SemanticWeb Conf., 2005.193 T. H. H. Sepandar D. Kamvar and G. H. Golub. Adaptive methods for thecomputation of PageRank. Technical report, Stanford University, 2003.194 C. Shah and W. B. Croft. Evaluating high accuracy retrieval techniques. InProc. of the 27th Intl. ACM SIGIR Conf. on Research and development ininformation retrieval, 2004.195 S. Shi, J. Yu, G. Yang, and D. Wang. Distributed page ranking in structured p2p networks. In Proceedings of the 2003 International Conference onParallel Processing, 2003.196 H. A. Simon. On a class of stew distribution functions. Biometrika, 42425440, 1955.197 V. Sinha and D. R. Karger. Magnet supporting navigation in semistructured data environments. In Proc. of the 2005 ACM SIGMOD Intl. Conf.on Management of Data, 2005.198 C. Soules and G. Ganger. Connections using context to enhance file search.In SOSP, 2005.199 M. Speretta and S. Gauch. Personalizing search based on user search histories. In Proc. of the 13th Intl. ACM CIKM Conf. on Information andKnowledge Management, 2004.200 E. Spertus. Parasite Mining structural information on the web. In Proc.of the 6th Intl. World Wide Web Conf., 1997.163Chapter BIBLIOGRAPHY.201 N. Stojanovic, R. Studer, and L. Stojanovic. An approach for the rankingof query results in the semantic web. In ISWC, 2003.202 K. Sugiyama, K. Hatano, and M. Yoshikawa. Adaptive web search based onuser profile constructed without any effort from users. In Proc. of the 13thIntl. World Wide Web Conf., 2004.203 D. Sullivan. The older you are, the more you want personalized search,2004. httpsearchenginewatch.comsearchdayarticle.php3385131.204 J. Teevan, S. Dumais, and E. Horvitz. Personalizing search via automatedanalysis of interests and activities. In Proc. of the 28th Intl. ACM SIGIRConf. on Research and Development in Information Retrieval, 2005.205 J. A. Tomlin. A new paradigm for ranking pages in the world wide web. InProceedings of 12th International World Wide Web Conference, 2003.206 P. Tsaparas. Using nonlinear dynamical systems for web searching andranking. In Proceedings of the PODS Conference on Principles of DatabaseSystems, 2004.207 T. Upstill, N. Craswell, and D. Hawking. Predicting fame and fortunePagerank or indegree In Proceedings of the ADCS Australasian DocumentComputing Symposium, 2003.208 E. Volokh. Personalization and privacy. Commun. ACM, 438, 2000.209 E. M. Voorhees. Query expansion using lexicalsemantic relations. In Proc.of the 17th Intl. ACM SIGIR Conf. on Research and development in information retrieval, 1994.210 E. M. Voorhees. The trec robust retrieval track. SIGIR Forum, 391, 2005.211 O. D. W. and K. J. Modeling information content using observable behavior.In Proceedings of the 64th Annual Meeting of the American Society forInformation Science and Technology, 2001.212 S.C. Wang and Y. Tanaka. Topicoriented query expansion for web search.In Proc. of the 15th Intl. Conf. on World Wide Web, 2006.213 Y. Wang and D. J. DeWitt. Computing PageRank in a distributed internetsearch system. In Proceedings of the 30th VLDB Conference, 2004.214 J. B. Winer. Statistical principles in experimental design. McGraw Hill,1962.164Paul  Alexandru Chirita215 G. Wittel and S. Wu. On Attacking Statistical Spam Filters. In Proc. of theConference on Email and AntiSpam CEAS, Mountain View, CA, USA,July 2004.216 B. Wu and B. Davison. Identifying link farm spam pages. In Proceedings ofthe 14th World Wide Web Conference, 2005.217 B. Wu and B. Davison. Undue influence Eliminating the impact of linkplagiarism on web search rankings. Technical report, LeHigh University,2005.218 J. Wu and K. Aberer. Using SiteRank for decentralized computation of webdocument ranking, 2003.219 J. Xu and W. B. Croft. Query expansion using local and global documentanalysis. In Proc. of the 19th Intl. ACM SIGIR Conf. on Research andDevelopment in Information Retrieval, 1996.220 E. YomTov, S. Fine, D. Carmel, and A. Darlow. Learning to estimatequery difficulty including applications to missing content detection anddistributed information retrieval. In Proc. of the 28th Intl. ACM SIGIRConf. on Research and development in information retrieval, 2005.221 S. Yu, D. Cai, J.R. Wen, and W.Y. Ma. Improving pseudorelevancefeedback in web information retrieval using web page segmentation. InProc. of the 12th Intl. Conf. on World Wide Web, 2003.222 G. U. Yule. Statistical Study of Literary Vocabulary. Cambridge UniversityPress, 1944.223 H. Zhang, A. Goel, R. Govindan, K. Mason, and B. van Roy. Improvingeigenvectorbased reputation systems against collusions. In Proceedings ofthe 3rd Workshop on Web Graph Algorithms, 2004.224 C. Ziegler and G. Lausen. Spreading activation models for trust propagation.In Proceedings of the IEEE International Conference on eTechnology, eCommerce, and eService, 2004.225 G. K. Zipf. Human Behavior and the Principle of Least Effort. AddisonWesley, 1949.165
