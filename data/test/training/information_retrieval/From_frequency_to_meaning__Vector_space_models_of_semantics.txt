Journal of Artificial Intelligence Research 37 2010 141188 Submitted 1009 published 0210From Frequency to MeaningVector Space Models of SemanticsPeter D. Turney peter.turneynrccnrc.gc.caNational Research Council CanadaOttawa, Ontario, Canada, K1A 0R6Patrick Pantel mepatrickpantel.comYahoo LabsSunnyvale, CA, 94089, USAAbstractComputers understand very little of the meaning of human language. This profoundlylimits our ability to give instructions to computers, the ability of computers to explaintheir actions to us, and the ability of computers to analyse and process text. Vector spacemodels VSMs of semantics are beginning to address these limits. This paper surveys theuse of VSMs for semantic processing of text. We organize the literature on VSMs accordingto the structure of the matrix in a VSM. There are currently three broad classes of VSMs,based on termdocument, wordcontext, and pairpattern matrices, yielding three classesof applications. We survey a broad range of applications in these three categories and wetake a detailed look at a specific open source project in each category. Our goal in thissurvey is to show the breadth of applications of VSMs for semantics, to provide a newperspective on VSMs for those who are already familiar with the area, and to providepointers into the literature for those who are less familiar with the field.1. IntroductionOne of the biggest obstacles to making full use of the power of computers is that theycurrently understand very little of the meaning of human language. Recent progress insearch engine technology is only scratching the surface of human language, and yet theimpact on society and the economy is already immense. This hints at the transformativeimpact that deeper semantic technologies will have. Vector space models VSMs, surveyedin this paper, are likely to be a part of these new semantic technologies.In this paper, we use the term semantics in a general sense, as the meaning of a word, aphrase, a sentence, or any text in human language, and the study of such meaning. We arenot concerned with narrower senses of semantics, such as the semantic web or approaches tosemantics based on formal logic. We present a survey of VSMs and their relation with thedistributional hypothesis as an approach to representing some aspects of natural languagesemantics.The VSM was developed for the SMART information retrieval system Salton, 1971by Gerard Salton and his colleagues Salton, Wong,  Yang, 1975. SMART pioneeredmany of the concepts that are used in modern search engines Manning, Raghavan, Schutze, 2008. The idea of the VSM is to represent each document in a collection as apoint in a space a vector in a vector space. Points that are close together in this spaceare semantically similar and points that are far apart are semantically distant. The usersc2010 AI Access Foundation and National Research Council Canada. Reprinted with permission.Turney  Pantelquery is represented as a point in the same space as the documents the query is a pseudodocument. The documents are sorted in order of increasing distance decreasing semanticsimilarity from the query and then presented to the user.The success of the VSM for information retrieval has inspired researchers to extend theVSM to other semantic tasks in natural language processing, with impressive results. Forinstance, Rapp 2003 used a vectorbased representation of word meaning to achieve ascore of 92.5 on multiplechoice synonym questions from the Test of English as a ForeignLanguage TOEFL, whereas the average human score was 64.5.1 Turney 2006 used avectorbased representation of semantic relations to attain a score of 56 on multiplechoiceanalogy questions from the SAT college entrance test, compared to an average human scoreof 57.2In this survey, we have organized past work with VSMs according to the type of matrixinvolved termdocument, wordcontext, and pairpattern. We believe that the choice ofa particular matrix type is more fundamental than other choices, such as the particularlinguistic processing or mathematical processing. Although these three matrix types covermost of the work, there is no reason to believe that these three types exhaust the possibilities.We expect future work will introduce new types of matrices and higherorder tensors.31.1 Motivation for Vector Space Models of SemanticsVSMs have several attractive properties. VSMs extract knowledge automatically from agiven corpus, thus they require much less labour than other approaches to semantics, suchas handcoded knowledge bases and ontologies. For example, the main resource used inRapps 2003 VSM system for measuring word similarity is the British National CorpusBNC,4 whereas the main resource used in nonVSM systems for measuring word similarityHirst  StOnge, 1998 Leacock  Chodrow, 1998 Jarmasz  Szpakowicz, 2003 is alexicon, such as WordNet5 or Rogets Thesaurus. Gathering a corpus for a new languageis generally much easier than building a lexicon, and building a lexicon often involves alsogathering a corpus, such as SemCor for WordNet Miller, Leacock, Tengi,  Bunker, 1993.VSMs perform well on tasks that involve measuring the similarity of meaning betweenwords, phrases, and documents. Most search engines use VSMs to measure the similaritybetween a query and a document Manning et al., 2008. The leading algorithms for measuring semantic relatedness use VSMs Pantel  Lin, 2002a Rapp, 2003 Turney, Littman,Bigham,  Shnayder, 2003. The leading algorithms for measuring the similarity of semantic relations also use VSMs Lin  Pantel, 2001 Turney, 2006 Nakov  Hearst, 2008.Section 2.4 discusses the differences between these types of similarity.We find VSMs especially interesting due to their relation with the distributional hypothesis and related hypotheses see Section 2.7. The distributional hypothesis is that1. Regarding the average score of 64.5 on the TOEFL questions, Landauer and Dumais 1997 notethat, Although we do not know how such a performance would compare, for example, with U.S. schoolchildren of a particular age, we have been told that the average score is adequate for admission to manyuniversities.2. This is the average score for highschool students in their senior year, applying to US universities. Formore discussion of this score, see Section 6.3 in Turneys 2006 paper.3. A vector is a firstorder tensor and a matrix is a secondorder tensor. See Section 2.5.4. See httpwww.natcorp.ox.ac.uk.5. See httpwordnet.princeton.edu.142From Frequency to Meaningwords that occur in similar contexts tend to have similar meanings Wittgenstein, 1953Harris, 1954 Weaver, 1955 Firth, 1957 Deerwester, Dumais, Landauer, Furnas,  Harshman, 1990. Efforts to apply this abstract hypothesis to concrete algorithms for measuringthe similarity of meaning often lead to vectors, matrices, and higherorder tensors. Thisintimate connection between the distributional hypothesis and VSMs is a strong motivationfor taking a close look at VSMs.Not all uses of vectors and matrices count as vector space models. For the purposes ofthis survey, we take it as a defining property of VSMs that the values of the elements in aVSM must be derived from event frequencies, such as the number of times that a given wordappears in a given context see Section 2.6. For example, often a lexicon or a knowledgebase may be viewed as a graph, and a graph may be represented using an adjacency matrix,but this does not imply that a lexicon is a VSM, because, in general, the values of theelements in an adjacency matrix are not derived from event frequencies. This emphasison event frequencies brings unity to the variety of VSMs and explicitly connects them tothe distributional hypothesis furthermore, it avoids triviality by excluding many possiblematrix representations.1.2 Vectors in AI and Cognitive ScienceVectors are common in AI and cognitive science they were common before the VSM wasintroduced by Salton et al. 1975. The novelty of the VSM was to use frequencies in acorpus of text as a clue for discovering semantic information.In machine learning, a typical problem is to learn to classify or cluster a set of itemsi.e., examples, cases, individuals, entities represented as feature vectors Mitchell, 1997Witten  Frank, 2005. In general, the features are not derived from event frequencies,although this is possible see Section 4.6. For example, a machine learning algorithm canbe applied to classifying or clustering documents Sebastiani, 2002.Collaborative filtering and recommender systems also use vectors Resnick, Iacovou,Suchak, Bergstrom,  Riedl, 1994 Breese, Heckerman,  Kadie, 1998 Linden, Smith, York, 2003. In a typical recommender system, we have a personitem matrix, in whichthe rows correspond to people customers, consumers, the columns correspond to itemsproducts, purchases, and the value of an element is the rating poor, fair, excellent thatthe person has given to the item. Many of the mathematical techniques that work wellwith termdocument matrices see Section 4 also work well with personitem matrices, butratings are not derived from event frequencies.In cognitive science, prototype theory often makes use of vectors. The basic idea ofprototype theory is that some members of a category are more central than others Rosch Lloyd, 1978 Lakoff, 1987. For example, robin is a central prototypical member ofthe category bird, whereas penguin is more peripheral. Concepts have varying degrees ofmembership in categories graded categorization. A natural way to formalize this is torepresent concepts as vectors and categories as sets of vectors Nosofsky, 1986 Smith, Osherson, Rips,  Keane, 1988. However, these vectors are usually based on numerical scoresthat are elicited by questioning human subjects they are not based on event frequencies.Another area of psychology that makes extensive use of vectors is psychometrics, whichstudies the measurement of psychological abilities and traits. The usual instrument for143Turney  Pantelmeasurement is a test or questionnaire, such as a personality test. The results of a testare typically represented as a subjectitem matrix, in which the rows represent the subjectspeople in an experiment and the columns represent the items questions in the testquestionnaire. The value of an element in the matrix is the answer that the correspondingsubject gave for the corresponding item. Many techniques for vector analysis, such as factoranalysis Spearman, 1904, were pioneered in psychometrics.In cognitive science, Latent Semantic Analysis LSA Deerwester et al., 1990 Landauer  Dumais, 1997, Hyperspace Analogue to Language HAL Lund, Burgess, Atchley, 1995 Lund  Burgess, 1996, and related research Landauer, McNamara, Dennis,  Kintsch, 2007 is entirely within the scope of VSMs, as defined above, since thisresearch uses vector space models in which the values of the elements are derived fromevent frequencies, such as the number of times that a given word appears in a given context. Cognitive scientists have argued that there are empirical and theoretical reasons forbelieving that VSMs, such as LSA and HAL, are plausible models of some aspects of human cognition Landauer et al., 2007. In AI, computational linguistics, and informationretrieval, such plausibility is not essential, but it may be seen as a sign that VSMs are apromising area for further research.1.3 Motivation for This SurveyThis paper is a survey of vector space models of semantics. There is currently no comprehensive, uptodate survey of this field. As we show in the survey, vector space modelsare a highly successful approach to semantics, with a wide range of potential and actualapplications. There has been much recent growth in research in this area.This paper should be of interest to all AI researchers who work with natural language,especially researchers who are interested in semantics. The survey will serve as a generalintroduction to this area and it will provide a framework  a unified perspective  fororganizing the diverse literature on the topic. It should encourage new research in the area,by pointing out open problems and areas for further exploration.This survey makes the following contributionsNew framework We provide a new framework for organizing the literature termdocument, wordcontext, and pairpattern matrices see Section 2. This framework showsthe importance of the structure of the matrix the choice of rows and columns in determining the potential applications and may inspire researchers to explore new structuresdifferent kinds of rows and columns, or higherorder tensors instead of matrices.New developments We draw attention to pairpattern matrices. The use of pairpattern matrices is relatively new and deserves more study. These matrices address somecriticisms that have been directed at wordcontext matrices, regarding lack of sensitivityto word order.Breadth of approaches and applications There is no existing survey that showsthe breadth of potential and actual applications of VSMs for semantics. Existing summariesomit pairpattern matrices Landauer et al., 2007.Focus on NLP and CL Our focus in this survey is on systems that perform practicaltasks in natural language processing and computational linguistics. Existing overviews focuson cognitive psychology Landauer et al., 2007.144From Frequency to MeaningSuccess stories We draw attention to the fact that VSMs are arguably the mostsuccessful approach to semantics, so far.1.4 Intended ReadershipOur goal in writing this paper has been to survey the state of the art in vector space modelsof semantics, to introduce the topic to those who are new to the area, and to give a newperspective to those who are already familiar with the area.We assume our reader has a basic understanding of vectors, matrices, and linear algebra,such as one might acquire from an introductory undergraduate course in linear algebra, orfrom a text book Golub  Van Loan, 1996. The basic concepts of vectors and matricesare more important here than the mathematical details. Widdows 2004 gives a gentleintroduction to vectors from the perspective of semantics.We also assume our reader has some familiarity with computational linguistics or information retrieval. Manning et al. 2008 provide a good introduction to information retrieval.For computational linguistics, we recommend Manning and Schutzes 1999 text.If our reader is familiar with linear algebra and computational linguistics, this surveyshould present no barriers to understanding. Beyond this background, it is not necessaryto be familiar with VSMs as they are used in information retrieval, natural language processing, and computational linguistics. However, if the reader would like to do some furtherbackground reading, we recommend Landauer et al.s 2007 collection.1.5 Highlights and OutlineThis article is structured as follows. Section 2 explains our framework for organizing theliterature on VSMs according to the type of matrix involved termdocument, wordcontext,and pairpattern. In this section, we present an overview of VSMs, without getting intothe details of how a matrix can be generated from a corpus of raw text.After the highlevel framework is in place, Sections 3 and 4 examine the steps involvedin generating a matrix. Section 3 discusses linguistic processing and Section 4 reviewsmathematical processing. This is the order in which a corpus would be processed in mostVSM systems first linguistic processing, then mathematical processing.When VSMs are used for semantics, the input to the model is usually plain text. SomeVSMs work directly with the raw text, but most first apply some linguistic processing to thetext, such as stemming, partofspeech tagging, word sense tagging, or parsing. Section 3looks at some of these linguistic tools for semantic VSMs.In a simple VSM, such as a simple termdocument VSM, the value of an element in adocument vector is the number of times that the corresponding word occurs in the givendocument, but most VSMs apply some mathematical processing to the raw frequency values.Section 4 presents the main mathematical operations weighting the elements, smoothingthe matrix, and comparing the vectors. This section also describes optimization strategiesfor comparing the vectors, such as distributed sparse matrix multiplication and randomizedtechniques.By the end of Section 4, the reader will have a general view of the concepts involved invector space models of semantics. We then take a detailed look at three VSM systems inSection 5. As a representative of termdocument VSMs, we present the Lucene information145Turney  Pantelretrieval library.6 For wordcontext VSMs, we explore the Semantic Vectors package, whichbuilds on Lucene.7 As the representative of pairpattern VSMs, we review the LatentRelational Analysis module in the SSpace package, which also builds on Lucene.8 Thesource code for all three of these systems is available under open source licensing.We turn to a broad survey of applications for semantic VSMs in Section 6. This section also serves as a short historical view of research with semantic VSMs, beginning withinformation retrieval in Section 6.1. Our purpose here is to give the reader an idea of thebreadth of applications for VSMs and also to provide pointers into the literature, if thereader wishes to examine any of these applications in detail.In a termdocument matrix, rows correspond to terms and columns correspond to documents Section 6.1. A document provides a context for understanding the term. If wegeneralize the idea of documents to chunks of text of arbitrary size phrases, sentences,paragraphs, chapters, books, collections, the result is the wordcontext matrix, which includes the termdocument matrix as a special case. Section 6.2 discusses applications forwordcontext matrices. Section 6.3 considers pairpattern matrices, in which the rows correspond to pairs of terms and the columns correspond to the patterns in which the pairsoccur.In Section 7, we discuss alternatives to VSMs for semantics. Section 8 considers thefuture of VSMs, raising some questions about their power and their limitations. We concludein Section 9.2. Vector Space Models of SemanticsThe theme that unites the various forms of VSMs that we discuss in this paper can bestated as the statistical semantics hypothesis statistical patterns of human word usage canbe used to figure out what people mean.9 This general hypothesis underlies several morespecific hypotheses, such as the bag of words hypothesis, the distributional hypothesis, theextended distributional hypothesis, and the latent relation hypothesis, discussed below.2.1 Similarity of Documents The TermDocument MatrixIn this paper, we use the following notational conventions Matrices are denoted by boldcapital letters, A. Vectors are denoted by bold lowercase letters, b. Scalars are representedby lowercase italic letters, c.If we have a large collection of documents, and hence a large number of documentvectors, it is convenient to organize the vectors into a matrix. The row vectors of the matrixcorrespond to terms usually terms are words, but we will discuss some other possibilities6. See httplucene.apache.orgjavadocs.7. See httpcode.google.compsemanticvectors.8. See httpcode.google.compairheadresearchwikiLatentRelationalAnalysis.9. This phrase was taken from the Faculty Profile of George Furnas at the University of Michigan,httpwww.si.umich.edupeoplefacultydetail.htmsid41. The full quote is, Statistical Semantics Studies of how the statistical patterns of human word usage can be used to figure out what peoplemean, at least to a level sufficient for information access. The term statistical semantics appeared inthe work of Furnas, Landauer, Gomez, and Dumais 1983, but it was not defined there.146From Frequency to Meaningand the column vectors correspond to documents web pages, for example. This kind ofmatrix is called a termdocument matrix.In mathematics, a bag also called a multiset is like a set, except that duplicates areallowed. For example, a, a, b, c, c, c is a bag containing a, b, and c. Order does not matterin bags and sets the bags a, a, b, c, c, c and c, a, c, b, a, c are equivalent. We can representthe bag a, a, b, c, c, c with the vector x  2, 1, 3, by stipulating that the first element ofx is the frequency of a in the bag, the second element is the frequency of b in the bag, andthe third element is the frequency of c. A set of bags can be represented as a matrix X, inwhich each column xj corresponds to a bag, each row xi corresponds to a unique member,and an element xij is the frequency of the ith member in the jth bag.In a termdocument matrix, a document vector represents the corresponding documentas a bag of words. In information retrieval, the bag of words hypothesis is that we canestimate the relevance of documents to a query by representing the documents and thequery as bags of words. That is, the frequencies of words in a document tend to indicatethe relevance of the document to a query. The bag of words hypothesis is the basis forapplying the VSM to information retrieval Salton et al., 1975. The hypothesis expressesthe belief that a column vector in a termdocument matrix captures to some degree anaspect of the meaning of the corresponding document what the document is about.Let X be a termdocument matrix. Suppose our document collection contains n documents and m unique terms. The matrix X will then have m rows one row for each uniqueterm in the vocabulary and n columns one column for each document. Let wi be the ithterm in the vocabulary and let dj be the jth document in the collection. The ith row inX is the row vector xi and the jth column in X is the column vector xj . The row vectorxi contains n elements, one element for each document, and the column vector xj containsm elements, one element for each term. Suppose X is a simple matrix of frequencies. Theelement xij in X is the frequency of the ith term wi in the jth document dj .In general, the value of most of the elements in X will be zero the matrix is sparse,since most documents will use only a small fraction of the whole vocabulary. If we randomlychoose a term wi and a document dj , its likely that wi does not occur anywhere in dj , andtherefore xij equals 0.The pattern of numbers in xi is a kind of signature of the ith term wi likewise, thepattern of numbers in xj is a signature of the jth document dj . That is, the pattern ofnumbers tells us, to some degree, what the term or document is about.The vector xj may seem to be a rather crude representation of the document dj . It tellsus how frequently the words appear in the document, but the sequential order of the wordsis lost. The vector does not attempt to capture the structure in the phrases, sentences,paragraphs, and chapters of the document. However, in spite of this crudeness, searchengines work surprisingly well vectors seem to capture an important aspect of semantics.The VSM of Salton et al. 1975 was arguably the first practical, useful algorithm forextracting semantic information from word usage. An intuitive justification for the termdocument matrix is that the topic of a document will probabilistically influence the authorschoice of words when writing the document.10 If two documents have similar topics, thenthe two corresponding column vectors will tend to have similar patterns of numbers.10. Newer generative models, such as Latent Dirichlet Allocation LDA Blei, Ng,  Jordan, 2003, directlymodel this intuition. See Sections 4.3 and 7.147Turney  Pantel2.2 Similarity of Words The WordContext MatrixSalton et al. 1975 focused on measuring document similarity, treating a query to a searchengine as a pseudodocument. The relevance of a document to a query is given by thesimilarity of their vectors. Deerwester et al. 1990 observed that we can shift the focus tomeasuring word similarity, instead of document similarity, by looking at row vectors in thetermdocument matrix, instead of column vectors.Deerwester et al. 1990 were inspired by the termdocument matrix of Salton et al. 1975,but a document is not necessarily the optimal length of text for measuring word similarity.In general, we may have a wordcontext matrix, in which the context is given by words,phrases, sentences, paragraphs, chapters, documents, or more exotic possibilities, such assequences of characters or patterns.The distributional hypothesis in linguistics is that words that occur in similar contextstend to have similar meanings Harris, 1954. This hypothesis is the justification for applying the VSM to measuring word similarity. A word may be represented by a vectorin which the elements are derived from the occurrences of the word in various contexts,such as windows of words Lund  Burgess, 1996, grammatical dependencies Lin, 1998Pado  Lapata, 2007, and richer contexts consisting of dependency links and selectionalpreferences on the argument positions Erk  Pado, 2008 see Sahlgrens 2006 thesis fora comprehensive study of various contexts. Similar row vectors in the wordcontext matrixindicate similar word meanings.The idea that word usage can reveal semantics was implicit in some of the things thatWittgenstein 1953 said about languagegames and family resemblance. Wittgenstein wasprimarily interested in the physical activities that form the context of word usage e.g., theword brick, spoken in the context of the physical activity of building a house, but the maincontext for a word is often other words.11Weaver 1955 argued that word sense disambiguation for machine translation shouldbe based on the cooccurrence frequency of the context words near a given target word theword that we want to disambiguate. Firth 1957, p. 11 said, You shall know a wordby the company it keeps. Deerwester et al. 1990 showed how the intuitions of Wittgenstein 1953, Harris 1954, Weaver, and Firth could be used in a practical algorithm.2.3 Similarity of Relations The PairPattern MatrixIn a pairpattern matrix, row vectors correspond to pairs of words, such as mason  stoneand carpenter  wood, and column vectors correspond to the patterns in which the pairs cooccur, such as X cuts Y  and X works with Y . Lin and Pantel 2001 introduced thepairpattern matrix for the purpose of measuring the semantic similarity of patterns thatis, the similarity of column vectors. Given a pattern such as X solves Y , their algorithmwas able to find similar patterns, such as Y is solved by X, Y is resolved in X, andX resolves Y .Lin and Pantel 2001 proposed the extended distributional hypothesis, that patternsthat cooccur with similar pairs tend to have similar meanings. The patterns X solves Y 11. Wittgensteins intuition might be better captured by a matrix that combines words with other modalities,such as images Monay  GaticaPerez, 2003. If the values of the elements are derived from eventfrequencies, we would include this as a VSM approach to semantics.148From Frequency to Meaningand Y is solved by X tend to cooccur with similar X  Y pairs, which suggests that thesepatterns have similar meanings. Pattern similarity can be used to infer that one sentenceis a paraphrase of another Lin  Pantel, 2001.Turney et al. 2003 introduced the use of the pairpattern matrix for measuring thesemantic similarity of relations between word pairs that is, the similarity of row vectors.For example, the pairs mason  stone, carpenter  wood, potter  clay, and glassblower  glassshare the semantic relation artisan  material. In each case, the first member of the pair isan artisan who makes artifacts from the material that is the second member of the pair.The pairs tend to cooccur in similar patterns, such as the X used the Y to and the Xshaped the Y into.The latent relation hypothesis is that pairs of words that cooccur in similar patternstend to have similar semantic relations Turney, 2008a. Word pairs with similar rowvectors in a pairpattern matrix tend to have similar semantic relations. This is the inverseof the extended distributional hypothesis, that patterns with similar column vectors in thepairpattern matrix tend to have similar meanings.2.4 SimilaritiesPairpattern matrices are suited to measuring the similarity of semantic relations betweenpairs of words that is, relational similarity. In contrast, wordcontext matrices are suitedto measuring attributional similarity. The distinction between attributional and relationalsimilarity has been explored in depth by Gentner 1983.The attributional similarity between two words a and b, simaa, b  , depends on thedegree of correspondence between the properties of a and b. The more correspondence thereis, the greater their attributional similarity. The relational similarity between two pairs ofwords a  b and c d, simra  b, c d  , depends on the degree of correspondence betweenthe relations of a b and c d. The more correspondence there is, the greater their relationalsimilarity. For example, dog and wolf have a relatively high degree of attributional similarity, whereas dog  bark and cat  meow have a relatively high degree of relational similarityTurney, 2006.It is tempting to suppose that relational similarity can be reduced to attributionalsimilarity. For example, mason and carpenter are similar words and stone and wood aresimilar words therefore, perhaps it follows that mason  stone and carpenter  wood havesimilar relations. Perhaps simra b, c d can be reduced to simaa, csimab, d. However,mason, carpenter, potter, and glassblower are similar words they are all artisans, as arewood, clay, stone, and glass they are all materials used by artisans, but we cannot inferfrom this that mason  glass and carpenter  clay have similar relations. Turney 2006, 2008apresented experimental evidence that relational similarity does not reduce to attributionalsimilarity.The term semantic relatedness in computational linguistics Budanitsky  Hirst, 2001corresponds to attributional similarity in cognitive science Gentner, 1983. Two wordsare semantically related if they have any kind of semantic relation Budanitsky  Hirst,2001 they are semantically related to the degree that they share attributes Turney, 2006.Examples are synonyms bank and trust company, meronyms car and wheel, antonymshot and cold, and words that are functionally related or frequently associated pencil and149Turney  Pantelpaper. We might not usually think that antonyms are similar, but antonyms have a highdegree of attributional similarity hot and cold are kinds of temperature, black and whiteare kinds of colour, loud and quiet are kinds of sound. We prefer the term attributionalsimilarity to the term semantic relatedness, because attributional similarity emphasizes thecontrast with relational similarity, whereas semantic relatedness could be confused withrelational similarity.In computational linguistics, the term semantic similarity is applied to words that sharea hypernym car and bicycle are semantically similar, because they share the hypernymvehicle Resnik, 1995. Semantic similarity is a specific type of attributional similarity. Weprefer the term taxonomical similarity to the term semantic similarity, because the termsemantic similarity is misleading. Intuitively, both attributional and relational similarityinvolve meaning, so both deserve to be called semantic similarity.Words are semantically associated if they tend to cooccur frequently e.g., bee andhoney Chiarello, Burgess, Richards,  Pollock, 1990. Words may be taxonomically similar and semantically associated doctor and nurse, taxonomically similar but not semantically associated horse and platypus, semantically associated but not taxonomically similarcradle and baby, or neither semantically associated nor taxonomically similar calculus andcandy.Schutze and Pedersen 1993 defined two ways that words can be distributed in a corpus of text If two words tend to be neighbours of each other, then they are syntagmaticassociates. If two words have similar neighbours, then they are paradigmatic parallels. Syntagmatic associates are often different parts of speech, whereas paradigmatic parallels areusually the same part of speech. Syntagmatic associates tend to be semantically associated bee and honey are often neighbours paradigmatic parallels tend to be taxonomicallysimilar doctor and nurse have similar neighbours.2.5 Other Semantic VSMsThe possibilities are not exhausted by termdocument, wordcontext, and pairpatternmatrices. We might want to consider triplepattern matrices, for measuring the semanticsimilarity between word triples. Whereas a pairpattern matrix might have a row mason stone and a column X works with Y , a triplepattern matrix could have a row mason stone  masonry and a column X uses Y to build Z. However, ntuples of words growincreasingly rare as n increases. For example, phrases that contain mason, stone, andmasonry together are less frequent than phrases that contain mason and stone together. Atriplepattern matrix will be much more sparse than a pairpattern matrix ceteris paribus.The quantity of text that we need, in order to have enough numbers to make our matricesuseful, grows rapidly as n increases. It may be better to break ntuples into pairs. Forexample, a b c could be decomposed into a b, a c, and b c Turney, 2008a. The similarityof two triples, a b c and d e f , could be estimated by the similarity of their correspondingpairs. A relatively dense pairpattern matrix could serve as a surrogate for a relativelysparse triplepattern matrix.We may also go beyond matrices. The generalization of a matrix is a tensor Kolda Bader, 2009 Acar  Yener, 2009. A scalar a single number is zerothorder tensor, avector is firstorder tensor, and a matrix is a secondorder tensor. A tensor of order three or150From Frequency to Meaninghigher is called a higherorder tensor. Chew, Bader, Kolda, and Abdelali 2007 use a termdocumentlanguage thirdorder tensor for multilingual information retrieval. Turney 2007uses a wordwordpattern tensor to measure similarity of words. Van de Cruys 2009 usesa verbsubjectobject tensor to learn selectional preferences of verbs.In Turneys 2007 tensor, for example, rows correspond to words from the TOEFLmultiplechoice synonym questions, columns correspond to words from Basic English Ogden, 1930,12 and tubes correspond to patterns that join rows and columns hence we havea wordwordpattern thirdorder tensor. A given word from the TOEFL questions is represented by the corresponding wordpattern matrix slice in the tensor. The elements inthis slice correspond to all the patterns that relate the given TOEFL word to any wordin Basic English. The similarity of two TOEFL words is calculated by comparing the twocorresponding matrix slices. The algorithm achieves 83.8 on the TOEFL questions.2.6 Types and TokensA token is a single instance of a symbol, whereas a type is a general class of tokens Manninget al., 2008. Consider the following example from Samuel BeckettEver tried. Ever failed.No matter. Try again.Fail again. Fail better.There are two tokens of the type Ever, two tokens of the type again, and two tokens ofthe type Fail. Lets say that each line in this example is a document, so we have threedocuments of two sentences each. We can represent this example with a tokendocumentmatrix or a typedocument matrix. The tokendocument matrix has twelve rows, one foreach token, and three columns, one for each line Figure 1. The typedocument matrixhas nine rows, one for each type, and three columns Figure 2.A row vector for a token has binary values an element is 1 if the given token appears inthe given document and 0 otherwise. A row vector for a type has integer values an elementis the frequency of the given type in the given document. These vectors are related, in thata type vector is the sum of the corresponding token vectors. For example, the row vectorfor the type Ever is the sum of the two token vectors for the two tokens of Ever.In applications dealing with polysemy, one approach uses vectors that represent wordtokens Schutze, 1998 Agirre  Edmonds, 2006 and another uses vectors that representword types Pantel  Lin, 2002a. Typical word sense disambiguation WSD algorithmsdeal with word tokens instances of words in specific contexts rather than word types.We mention both approaches to polysemy in Section 6, due to their similarity and closerelationship, although a defining characteristic of the VSM is that it is concerned withfrequencies see Section 1.1, and frequency is a property of types, not tokens.12. Basic English is a highly reduced subset of English, designed to be easy for people to learn. The wordsof Basic English are listed at httpogden.basicenglish.org.151Turney  PantelEver tried. No matter. Fail again.Ever failed. Try again. Fail better.Ever 1 0 0tried 1 0 0Ever 1 0 0failed 1 0 0No 0 1 0matter 0 1 0Try 0 1 0again 0 1 0Fail 0 0 1again 0 0 1Fail 0 0 1better 0 0 1Figure 1 The tokendocument matrix. Rows are tokens and columns are documents.Ever tried. No matter. Fail again.Ever failed. Try again. Fail better.Ever 2 0 0tried 1 0 0failed 1 0 0No 0 1 0matter 0 1 0Try 0 1 0again 0 1 1Fail 0 0 2better 0 0 1Figure 2 The typedocument matrix. Rows are types and columns are documents.152From Frequency to Meaning2.7 HypothesesWe have mentioned five hypotheses in this section. Here we repeat these hypotheses andthen interpret them in terms of vectors. For each hypothesis, we cite work that explicitlystates something like the hypothesis or implicitly assumes something like the hypothesis.Statistical semantics hypothesis Statistical patterns of human word usage can beused to figure out what people mean Weaver, 1955 Furnas et al., 1983.  If units of texthave similar vectors in a text frequency matrix,13 then they tend to have similar meanings.We take this to be a general hypothesis that subsumes the four more specific hypothesesthat follow.Bag of words hypothesis The frequencies of words in a document tend to indicatethe relevance of the document to a query Salton et al., 1975.  If documents and pseudodocuments queries have similar column vectors in a termdocument matrix, then theytend to have similar meanings.Distributional hypothesis Words that occur in similar contexts tend to have similarmeanings Harris, 1954 Firth, 1957 Deerwester et al., 1990.  If words have similar rowvectors in a wordcontext matrix, then they tend to have similar meanings.Extended distributional hypothesis Patterns that cooccur with similar pairs tendto have similar meanings Lin  Pantel, 2001.  If patterns have similar column vectorsin a pairpattern matrix, then they tend to express similar semantic relations.Latent relation hypothesis Pairs of words that cooccur in similar patterns tendto have similar semantic relations Turney et al., 2003.  If word pairs have similar rowvectors in a pairpattern matrix, then they tend to have similar semantic relations.We have not yet explained what it means to say that vectors are similar. We discussthis in Section 4.4.3. Linguistic Processing for Vector Space ModelsWe will assume that our raw data is a large corpus of natural language text. Before wegenerate a termdocument, wordcontext, or pairpattern matrix, it can be useful to applysome linguistic processing to the raw text. The types of processing that are used can begrouped into three classes. First, we need to tokenize the raw text that is, we need to decidewhat constitutes a term and how to extract terms from raw text. Second, we may want tonormalize the raw text, to convert superficially different strings of characters to the sameform e.g., car, Car, cars, and Cars could all be normalized to car. Third, we may wantto annotate the raw text, to mark identical strings of characters as being different e.g., flyas a verb could be annotated as flyVB and fly as a noun could be annotated as flyNN.Grefenstette 1994 presents a good study of linguistic processing for wordcontextVSMs. He uses a similar threestep decomposition of linguistic processing tokenization,surface syntactic analysis, and syntactic attribute extraction.13. By text frequency matrix, we mean any matrix or higherorder tensor in which the values of the elementsare derived from the frequencies of pieces of text in the context of other pieces of text in some collectionof text. A text frequency matrix is intended to be a general structure, which includes termdocument,wordcontext, and pairpattern matrices as special cases.153Turney  Pantel3.1 TokenizationTokenization of English seems simple at first glance words are separated by spaces. Thisassumption is approximately true for English, and it may work sufficiently well for a basicVSM, but a more advanced VSM requires a more sophisticated approach to tokenization.An accurate English tokenizer must know how to handle punctuation e.g., dont, Janes,andor, hyphenation e.g., stateoftheart versus state of the art, and recognize multiwordterms e.g., Barack Obama and ice hockey Manning et al., 2008. We may also wish toignore stop words, highfrequency words with relatively low information content, such asfunction words e.g., of, the, and and pronouns e.g., them, who, that. A popular list ofstop words is the set of 571 common words included in the source code for the SMARTsystem Salton, 1971.14In some languages e.g., Chinese, words are not separated by spaces. A basic VSMcan break the text into character unigrams or bigrams. A more sophisticated approachis to match the input text against entries in a lexicon, but the matching often does notdetermine a unique tokenization Sproat  Emerson, 2003. Furthermore, native speakersoften disagree about the correct segmentation. Highly accurate tokenization is a challengingtask for most human languages.3.2 NormalizationThe motivation for normalization is the observation that many different strings of characters often convey essentially identical meanings. Given that we want to get at the meaningthat underlies the words, it seems reasonable to normalize superficial variations by converting them to the same form. The most common types of normalization are case foldingconverting all words to lower case and stemming reducing inflected words to their stemor root form.Case folding is easy in English, but can be problematic in some languages. In French,accents are optional for uppercase, and it may be difficult to restore missing accents whenconverting the words to lowercase. Some words cannot be distinguished without accents forexample, PECHE could be either peche meaning fishing or peach or peche meaning sin.Even in English, case folding can cause problems, because case sometimes has semanticsignificance. For example, SMART is an information retrieval system, whereas smart is acommon adjective Bush may be a surname, whereas bush is a kind of plant.Morphology is the study of the internal structure of words. Often a word is composedof a stem root with added affixes inflections, such as plural forms and past tenses e.g.,trapped is composed of the stem trap and the affix ed. Stemming, a kind of morphologicalanalysis, is the process of reducing inflected words to their stems. In English, affixes aresimpler and more regular than in many other languages, and stemming algorithms basedon heuristics rules of thumb work relatively well Lovins, 1968 Porter, 1980 Minnen,Carroll,  Pearce, 2001. In an agglutinative language e.g., Inuktitut, many concepts arecombined into a single word, using various prefixes, infixes, and suffixes, and morphologicalanalysis is complicated. A single word in an agglutinative language may correspond to asentence of half a dozen words in English Johnson  Martin, 2003.14. The source code is available at ftpftp.cs.cornell.edupubsmart.154From Frequency to MeaningThe performance of an information retrieval system is often measured by precision andrecall Manning et al., 2008. The precision of a system is an estimate of the conditionalprobability that a document is truly relevant to a query, if the system says it is relevant.The recall of a system is an estimate of the conditional probability that the system will saythat a document is relevant to a query, if it truly is relevant.In general, normalization increases recall and reduces precision Kraaij  Pohlmann,1996. This is natural, given the nature of normalization. When we remove superficialvariations that we believe are irrelevant to meaning, we make it easier to recognize similarities we find more similar things, and so recall increases. But sometimes these superficialvariations have semantic significance ignoring the variations causes errors, and so precisiondecreases. Normalization can also have a positive effect on precision in cases where varianttokens are infrequent and smoothing the variations gives more reliable statistics.If we have a small corpus, we may not be able to afford to be overly selective, and it maybe best to aggressively normalize the text, to increase recall. If we have a very large corpus,precision may be more important, and we might not want any normalization. Hull 1996gives a good analysis of normalization for information retrieval.3.3 AnnotationAnnotation is the inverse of normalization. Just as different strings of characters may havethe same meaning, it also happens that identical strings of characters may have differentmeanings, depending on the context. Common forms of annotation include partofspeechtagging marking words according to their parts of speech, word sense tagging markingambiguous words according to their intended meanings, and parsing analyzing the grammatical structure of sentences and marking the words in the sentences according to theirgrammatical roles Manning  Schutze, 1999.Since annotation is the inverse of normalization, we expect it to decrease recall andincrease precision. For example, by tagging program as a noun or a verb, we may beable to selectively search for documents that are about the act of computer programmingverb instead of documents that discuss particular computer programs noun hence wecan increase precision. However, a document about computer programs noun may havesomething useful to say about the act of computer programming verb, even if the documentnever uses the verb form of program hence we may decrease recall.Large gains in IR performance have recently been reported as a result of query annotation with syntactic and semantic information. Syntactic annotation includes querysegmentation Tan  Peng, 2008 and part of speech tagging Barr, Jones,  Regelson,2008. Examples of semantic annotation are disambiguating abbreviations in queries Wei,Peng,  Dumoulin, 2008 and finding query keyword associations Lavrenko  Croft, 2001Cao, Nie,  Bai, 2005.Annotation is also useful for measuring the semantic similarity of words and conceptswordcontext matrices. For example, Pantel and Lin 2002a presented an algorithmthat can discover word senses by clustering row vectors in a wordcontext matrix, usingcontextual information derived from parsing.155Turney  Pantel4. Mathematical Processing for Vector Space ModelsAfter the text has been tokenized and optionally normalized and annotated, the first stepis to generate a matrix of frequencies. Second, we may want to adjust the weights of theelements in the matrix, because common words will have high frequencies, yet they are lessinformative than rare words. Third, we may want to smooth the matrix, to reduce theamount of random noise and to fill in some of the zero elements in a sparse matrix. Fourth,there are many different ways to measure the similarity of two vectors.Lowe 2001 gives a good summary of mathematical processing for wordcontext VSMs.He decomposes VSM construction into a similar fourstep process calculate the frequencies,transform the raw frequency counts, smooth the space dimensionality reduction, thencalculate the similarities.4.1 Building the Frequency MatrixAn element in a frequency matrix corresponds to an event a certain item term, word,word pair occurred in a certain situation document, context, pattern a certain numberof times frequency. At an abstract level, building a frequency matrix is a simple matterof counting events. In practice, it can be complicated when the corpus is large.A typical approach to building a frequency matrix involves two steps. First, scan sequentially through the corpus, recording events and their frequencies in a hash table, adatabase, or a search engine index. Second, use the resulting data structure to generate thefrequency matrix, with a sparse matrix representation Gilbert, Moler,  Schreiber, 1992.4.2 Weighting the ElementsThe idea of weighting is to give more weight to surprising events and less weight to expectedevents. The hypothesis is that surprising events, if shared by two vectors, are more discriminative of the similarity between the vectors than less surprising events. For example,in measuring the semantic similarity between the words mouse and rat, the contexts dissectand exterminate are more discriminative of their similarity than the contexts have and like.In information theory, a surprising event has higher information content than an expectedevent Shannon, 1948. The most popular way to formalize this idea for termdocumentmatrices is the tfidf term frequency  inverse document frequency family of weightingfunctions Sparck Jones, 1972. An element gets a high weight when the corresponding termis frequent in the corresponding document i.e., tf is high, but the term is rare in otherdocuments in the corpus i.e., df is low, and thus idf is high. Salton and Buckley 1988defined a large family of tfidf weighting functions and evaluated them on information retrieval tasks, demonstrating that tfidf weighting can yield significant improvements overraw frequency.Another kind of weighting, often combined with tfidf weighting, is length normalizationSinghal, Salton, Mitra,  Buckley, 1996. In information retrieval, if document lengthis ignored, search engines tend to have a bias in favour of longer documents. Lengthnormalization corrects for this bias.Term weighting may also be used to correct for correlated terms. For example, theterms hostage and hostages tend to be correlated, yet we may not want to normalize them156From Frequency to Meaningto the same term as in Section 3.2, because they have slightly different meanings. Asan alternative to normalizing them, we may reduce their weights when they cooccur in adocument Church, 1995.Feature selection may be viewed as a form of weighting, in which some terms get aweight of zero and hence can be removed from the matrix. Forman 2003 provides a goodstudy of feature selection methods for text classification.An alternative to tfidf is Pointwise Mutual Information PMI Church  Hanks, 1989Turney, 2001, which works well for both wordcontext matrices Pantel  Lin, 2002aand termdocument matrices Pantel  Lin, 2002b. A variation of PMI is Positive PMIPPMI, in which all PMI values that are less than zero are replaced with zero Niwa Nitta, 1994. Bullinaria and Levy 2007 demonstrated that PPMI performs better than awide variety of other weighting approaches when measuring semantic similarity with wordcontext matrices. Turney 2008a applied PPMI to pairpattern matrices. We will give theformal definition of PPMI here, as an example of an effective weighting function.Let F be a wordcontext frequency matrix with nr rows and nc columns. The ith rowin F is the row vector fi and the jth column in F is the column vector fj . The row ficorresponds to a word wi and the column fj corresponds to a context cj . The value of theelement fij is the number of times that wi occurs in the context cj . Let X be the matrixthat results when PPMI is applied to F. The new matrix X has the same number of rowsand columns as the raw frequency matrix F. The value of an element xij in X is definedas followspij fijnri1ncj1 fij1pi ncj1 fijnri1ncj1 fij2pj nri1 fijnri1ncj1 fij3pmiij  logpijpipj4xij pmiij if pmiij  00 otherwise5In this definition, pij is the estimated probability that the word wi occurs in the contextcj , pi is the estimated probability of the word wi, and pj is the estimated probability ofthe context cj . If wi and cj are statistically independent, then pipj  pij by the definitionof independence, and thus pmiij is zero since log1  0. The product pipj is what wewould expect for pij if wi occurs in cj by pure random chance. On the other hand, if thereis an interesting semantic relation between wi and cj , then we should expect pij to be largerthan it would be if wi and cj were indepedent hence we should find that pij  pipj , andthus pmiij is positive. This follows from the distributional hypothesis see Section 2. Ifthe word wi is unrelated to the context cj , we may find that pmiij is negative. PPMI isdesigned to give a high value to xij when there is an interesting semantic relation between157Turney  Pantelwi and cj  otherwise, xij should have a value of zero, indicating that the occurrence of wiin cj is uninformative.A wellknown problem of PMI is that it is biased towards infrequent events. Considerthe case where wi and cj are statistically dependent i.e., they have maximum association.Then pij  pi  pj . Hence 4 becomes log 1pi and PMI increases as the probability ofword wi decreases. Several discounting factors have been proposed to alleviate this problem.An example follows Pantel  Lin, 2002aij fijfij  1min nrk1 fkj ,nck1 fikmin nrk1 fkj ,nck1 fik  16newpmiij  ij  pmiij 7Another way to deal with infrequent events is Laplace smoothing of the probabilityestimates, pij , pi, and pj Turney  Littman, 2003. A constant positive value is added tothe raw frequencies before calculating the probabilities each fij is replaced with fij k, forsome k  0. The larger the constant, the greater the smoothing effect. Laplace smoothingpushes the pmiij values towards zero. The magnitude of the push the difference betweenpmiij with and without Laplace smoothing depends on the raw frequency fij . If thefrequency is large, the push is small if the frequency is small, the push is large. ThusLaplace smoothing reduces the bias of PMI towards infrequent events.4.3 Smoothing the MatrixThe simplest way to improve information retrieval performance is to limit the number ofvector components. Keeping only components representing the most frequently occurringcontent words is such a way however, common words, such as the and have, carry littlesemantic discrimination power. Simple component smoothing heuristics, based on the properties of the weighting schemes presented in Section 4.2, have been shown to both maintainsemantic discrimination power and improve the performance of similarity computations.Computing the similarity between all pairs of vectors, described in Section 4.4, is acomputationally intensive task. However, only vectors that share a nonzero coordinatemust be compared i.e., two vectors that do not share a coordinate are dissimilar. Veryfrequent context words, such as the word the, unfortunately result in most vectors matchinga nonzero coordinate. Such words are precisely the contexts that have little semanticdiscrimination power. Consider the pointwise mutual information weighting described inSection 4.2. Highly weighted dimensions cooccur frequently with only very few words andare by definition highly discriminating contexts i.e., they have very high association withthe words with which they cooccur. By keeping only the contextword dimensions witha PMI above a conservative threshold and setting the others to zero, Lin 1998 showedthat the number of comparisons needed to compare vectors greatly decreases while losinglittle precision in the similarity score between the top200 most similar words of every word.While smoothing the matrix, one computes a reverse index on the nonzero coordinates.Then, to compare the similarity between a words context vector and all other words contextvectors, only those vectors found to match a nonzero component in the reverse index mustbe compared. Section 4.5 proposes further optimizations along these lines.158From Frequency to MeaningDeerwester et al. 1990 found an elegant way to improve similarity measurements with amathematical operation on the termdocument matrix, X, based on linear algebra. The operation is truncated Singular Value Decomposition SVD, also called thin SVD. Deerwesteret al. briefly mentioned that truncated SVD can be applied to both document similarityand word similarity, but their focus was document similarity. Landauer and Dumais 1997applied truncated SVD to word similarity, achieving humanlevel scores on multiplechoicesynonym questions from the Test of English as a Foreign Language TOEFL. TruncatedSVD applied to document similarity is called Latent Semantic Indexing LSI, but it iscalled Latent Semantic Analysis LSA when applied to word similarity.There are several ways of thinking about how truncated SVD works. We will firstpresent the math behind truncated SVD and then describe four ways of looking at itlatent meaning, noise reduction, highorder cooccurrence, and sparsity reduction.SVD decomposes X into the product of three matrices UVT, where U and V are incolumn orthonormal form i.e., the columns are orthogonal and have unit length, UTU VTV  I and  is a diagonal matrix of singular values Golub  Van Loan, 1996. If Xis of rank r, then  is also of rank r. Let k, where k  r, be the diagonal matrix formedfrom the top k singular values, and let Uk and Vk be the matrices produced by selecting thecorresponding columns from U and V. The matrix UkkVTk is the matrix of rank k thatbest approximates the original matrix X, in the sense that it minimizes the approximationerrors. That is, X  UkkVTk minimizes XXF over all matrices X of rank k, where . . . F denotes the Frobenius norm Golub  Van Loan, 1996.Latent meaning Deerwester et al. 1990 and Landauer and Dumais 1997 describetruncated SVD as a method for discovering latent meaning. Suppose we have a wordcontext matrix X. The truncated SVD, X  UkkVTk , creates a lowdimensional linearmapping between row space words and column space contexts. This lowdimensionalmapping captures the latent hidden meaning in the words and the contexts. Limiting thenumber of latent dimensions k  r forces a greater correspondence between words andcontexts. This forced correspondence between words and contexts improves the similaritymeasurement.Noise reduction Rapp 2003 describes truncated SVD as a noise reduction technique.We may think of the matrix X  UkkVTk as a smoothed version of the original matrix X.The matrix Uk maps the row space the space spanned by the rows of X into a smallerkdimensional space and the matrix Vk maps the column space the space spanned bythe columns of X into the same kdimensional space. The diagonal matrix k specifiesthe weights in this reduced kdimensional space. The singular values in  are ranked indescending order of the amount of variation in X that they fit. If we think of the matrixX as being composed of a mixture of signal and noise, with more signal than noise, thenUkkVTk mostly captures the variation in X that is due to the signal, whereas the remainingvectors in UVT are mostly fitting the variation in X that is due to the noise.Highorder cooccurrence Landauer and Dumais 1997 also describe truncatedSVD as a method for discovering highorder cooccurrence. Direct cooccurrence firstorder cooccurrence is when two words appear in identical contexts. Indirect cooccurrencehighorder cooccurrence is when two words appear in similar contexts. Similarity ofcontexts may be defined recursively in terms of lowerorder cooccurrence. Lemaire andDenhiere 2006 demonstrate that truncated SVD can discover highorder cooccurrence.159Turney  PantelSparsity reduction In general, the matrix X is very sparse mostly zeroes, but thetruncated SVD, X  UkkVTk , is dense. Sparsity may be viewed as a problem of insufficientdata with more text, the matrix X would have fewer zeroes, and the VSM would performbetter on the chosen task. From this perspective, truncated SVD is a way of simulating themissing text, compensating for the lack of data Vozalis  Margaritis, 2003.These different ways of viewing truncated SVD are compatible with each other it ispossible for all of these perspectives to be correct. Future work is likely to provide moreviews of SVD and perhaps a unifying view.A good C implementation of SVD for large sparse matrices is Rohdes SVDLIBC.15Another approach is Brands 2006 incremental truncated SVD algorithm.16 Yet anotherapproach is Gorrells 2006 Hebbian algorithm for incremental truncated SVD. Brandsand Gorrells algorithms both introduce interesting new ways of handling missing values,instead of treating them as zero values.For higherorder tensors, there are operations that are analogous to truncated SVD,such as parallel factor analysis PARAFAC Harshman, 1970, canonical decompositionCANDECOMP Carroll  Chang, 1970 equivalent to PARAFAC but discovered independently, and Tucker decomposition Tucker, 1966. For an overview of tensor decompositions, see the surveys of Kolda and Bader 2009 or Acar and Yener 2009. Turney 2007gives an empirical evaluation of how well four different Tucker decomposition algorithmsscale up for large sparse thirdorder tensors. A lowRAM algorithm, Multislice Projection,for large sparse tensors is presented and evaluated.17Since the work of Deerwester et al. 1990, subsequent research has discovered manyalternative matrix smoothing processes, such as Nonnegative Matrix Factorization NMFLee  Seung, 1999, Probabilistic Latent Semantic Indexing PLSI Hofmann, 1999, Iterative Scaling IS Ando, 2000, Kernel Principal Components Analysis KPCA Scholkopf,Smola,  Muller, 1997, Latent Dirichlet Allocation LDA Blei et al., 2003, and DiscreteComponent Analysis DCA Buntine  Jakulin, 2006.The four perspectives on truncated SVD, presented above, apply equally well to all ofthese more recent matrix smoothing algorithms. These newer smoothing algorithms tendto be more computationally intensive than truncated SVD, but they attempt to modelword frequencies better than SVD. Truncated SVD implicitly assumes that the elementsin X have a Gaussian distribution. Minimizing the the Frobenius norm XXF willminimize the noise, if the noise has a Gaussian distribution. However, it is known thatword frequencies do not have Gaussian distributions. More recent algorithms are based onmore realistic models of the distribution for word frequencies.184.4 Comparing the VectorsThe most popular way to measure the similarity of two frequency vectors raw or weightedis to take their cosine. Let x and y be two vectors, each with n elements.15. SVDLIBC is available at httptedlab.mit.edudrsvdlibc.16. MATLAB source code is available at httpweb.mit.eduwingatedwwwresources.html.17. MATLAB source code is available at httpwww.apperceptual.commultislice.18. In our experience, pmiij appears to be approximately Gaussian, which may explain why PMI works wellwith truncated SVD, but then PPMI is puzzling, because it is less Gaussian than PMI, yet it apparentlyyields better semantic models than PMI.160From Frequency to Meaningx  x1, x2, . . . , xn 8y  y1, y2, . . . , yn 9The cosine of the angle  between x and y can be calculated as followscosx,y ni1 xi  yini1 x2i ni1 y2i10x  yx  x  y  y11xx yy12In other words, the cosine of the angle between two vectors is the inner product of thevectors, after they have been normalized to unit length. If x and y are frequency vectorsfor words, a frequent word will have a long vector and a rare word will have a short vector,yet the words might be synonyms. Cosine captures the idea that the length of the vectorsis irrelevant the important thing is the angle between the vectors.The cosine ranges from 1 when the vectors point in opposite directions  is 180degrees to 1 when they point in the same direction  is 0 degrees. When the vectorsare orthogonal  is 90 degrees, the cosine is zero. With raw frequency vectors, whichnecessarily cannot have negative elements, the cosine cannot be negative, but weightingand smoothing often introduce negative elements. PPMI weighting does not yield negativeelements, but truncated SVD can generate negative elements, even when the input matrixhas no negative values.A measure of distance between vectors can easily be converted to a measure of similarityby inversion 13 or subtraction 14.simx,y  1distx,y 13simx,y  1 distx,y 14Many similarity measures have been proposed in both IR Jones  Furnas, 1987 andlexical semantics circles Lin, 1998 Dagan, Lee,  Pereira, 1999 Lee, 1999 Weeds, Weir, McCarthy, 2004. It is commonly said in IR that, properly normalized, the differencein retrieval performance using different measures is insignificant van Rijsbergen, 1979.Often the vectors are normalized in some way e.g., unit length or unit probability beforeapplying any similarity measure.Popular geometric measures of vector distance include Euclidean distance and Manhattan distance. Distance measures from information theory include Hellinger, Bhattacharya,and KullbackLeibler. Bullinaria and Levy 2007 compared these five distance measuresand the cosine similarity measure on four different tasks involving word similarity. Overall,the best measure was cosine. Other popular measures are the Dice and Jaccard coefficientsManning et al., 2008.161Turney  PantelLee 1999 proposed that, for finding word similarities, measures that focused more onoverlapping coordinates and less on the importance of negative features i.e., coordinateswhere one word has a nonzero value and the other has a zero value appear to performbetter. In Lees experiments, the Jaccard, JensenShannon, and L1 measures seemed toperform best. Weeds et al. 2004 studied the linguistic and statistical properties of thesimilar words returned by various similarity measures and found that the measures can begrouped into three classes1. highfrequency sensitive measures cosine, JensenShannon, skew, recall,2. lowfrequency sensitive measures precision, and3. similarfrequency sensitive methods Jaccard, JaccardMI, Lin, harmonic mean.Given a word w0, if we use a highfrequency sensitive measure to score other words wiaccording to their similarity with w0, higher frequency words will tend to get higher scoresthan lower frequency words. If we use a lowfrequency sensitive measure, there will be abias towards lower frequency words. Similarfrequency sensitive methods prefer a word withat has approximately the same frequency as w0. In one experiment on determining thecompositionality of collocations, highfrequency sensitive measures outperformed the otherclasses Weeds et al., 2004. We believe that determining the most appropriate similaritymeasure is inherently dependent on the similarity task, the sparsity of the statistics, thefrequency distribution of the elements being compared, and the smoothing method appliedto the matrix.4.5 Efficient ComparisonsComputing the similarity between all rows or columns in a large matrix is a nontrivialproblem, with a worst case cubic running time On2rnc, where nr is the number of rows andnc is the number of columns i.e., the dimensionality of the feature space. Optimizationsand parallelization are often necessary.4.5.1 SparseMatrix MultiplicationOne optimization strategy is a generalized sparsematrix multiplication approach Sarawagi Kirpal, 2004, which is based on the observation that a scalar product of two vectorsdepends only on the coordinates for which both vectors have nonzero values. Further, weobserve that most commonly used similarity measures for vectors x and y, such as cosine,overlap, and Dice, can be decomposed into three values one depending only on the nonzerovalues of x, another depending only on the nonzero values of y, and the third depending onthe nonzero coordinates shared both by x and y. More formally, commonly used similarityscores, simx,y, can be expressed as followssimx,y  f0 ni1f1xi, yi, f2x, f3y 15For example, the cosine measure, cosx,y, defined in 10, can be expressed in this modelas follows162From Frequency to Meaningcosx,y  f0 ni1f1xi, yi, f2x, f3y 16f0a, b, c ab  c17f1a, b  a  b 18f2a  f3a ni1a2i 19Let X be a matrix for which we want to compute the pairwise similarity, simx,y,between all rows or all columns x and y. Efficient computation of the similarity matrix Scan be achieved by leveraging the fact that simx,y is determined solely by the nonzerocoordinates shared by x and y i.e., f10, xi  f1xi, 0  0 for any xi and that most ofthe vectors are very sparse. In this case, calculating f1xi, yi is only required when bothvectors have a shared nonzero coordinate, significantly reducing the cost of computation.Determining which vectors share a nonzero coodinate can easily be achieved by first buildingan inverted index for the coordinates. During indexing, we can also precompute f2x andf3y without changing the algorithm complexity. Then, for each vector x we retrieve inconstant time, from the index, each vector y that shares a nonzero coordinate with x andwe apply f1xi, yi on the shared coordinates i. The computational cost of this algorithmisiN2i where Ni is the number of vectors that have a nonzero ith coordinate. Its worstcase time complexity is Oncv where n is the number of vectors to be compared, c is themaximum number of nonzero coordinates of any vector, and v is the number of vectorsthat have a nonzero ith coordinate where i is the coordinate which is nonzero for the mostvectors. In other words, the algorithm is efficient only when the density of the coordinatesis low. In our own experiments of computing the semantic similarity between all pairs ofwords in a large web crawl, we observed near linear average running time complexity in n.The computational cost can be reduced further by leveraging the element weightingtechniques described in Section 4.2. By setting to zero all coordinates that have a lowPPMI, PMI or tfidf score, the coordinate density is dramatically reduced at the cost oflosing little discriminative power. In this vein, Bayardo, Ma, and Srikant 2007 describeda strategy that omits the coordinates with the highest number of nonzero values. Theiralgorithm gives a significant advantage only when we are interested in finding solely thesimilarity between highly similar vectors.4.5.2 Distributed Implementation using MapReduceThe algorithm described in Section 4.5.1 assumes that the matrix X can fit into memory,which for large X may be impossible. Also, as each element of X is processed independently, running parallel processes for nonintersecting subsets of X makes the processingfaster. Elsayed, Lin, and Oard 2008 proposed a MapReduce implementation deployed using Hadoop, an opensource software package implementing the MapReduce framework anddistributed file system.19 Hadoop has been shown to scale to several thousands of machines,allowing users to write simple code, and to seamlessly manage the sophisticated parallel execution of the code. Dean and Ghemawat 2008 provide a good primer on MapReduceprogramming.19. Hadoop is available for download at httplucene.apache.orghadoop.163Turney  PantelThe MapReduce models Map step is used to start m  n Map tasks in parallel, eachcaching one mth part of X as an inverted index and streaming one nth part of X throughit. The actual inputs are read by the tasks directly from HDFS Hadoop DistributedFile System. The value of m is determined by the amount of memory dedicated for theinverted index, and n should be determined by trading off the fact that, as n increases,more parallelism can be obtained at the increased cost of building the same inverted indexn times.The similarity algorithm from Section 4.5.1 runs in each task of the Map step of aMapReduce job. The Reduce step groups the output by the rows or columns of X.4.5.3 Randomized AlgorithmsOther optimization strategies use randomized techniques to approximate various similarity measures. The aim of randomized algorithms is to improve computational efficiencymemory and time by projecting highdimensional vectors into a lowdimensional subspace.Truncated SVD performs such a projection, but SVD can be computationally intensive.20The insight of randomized techniques is that highdimensional vectors can be randomly projected into a lowdimensional subspace with relatively little impact on the final similarityscores. Significant reductions in computational cost have been reported with little average error to computing the true similarity scores, especially in applications such as wordsimilarity where we are interested in only the topk most similar vectors to each vectorRavichandran, Pantel,  Hovy, 2005 Gorman  Curran, 2006.Random Indexing, an approximation technique based on Sparse Distributed MemoryKanerva, 1993, computes the pairwise similarity between all rows or vectors of a matrixwith complexity Onrnc1, where 1 is a fixed constant representing the length of the indexvectors assigned to each column. The value of 1 controls the tradeoff of accuracy versusefficiency. The elements of each index vector are mostly zeros with a small number ofrandomly assigned 1s and 1s. The cosine measure between two rows r1 and r2 is thenapproximated by computing the cosine between two fingerprint vectors, fingerprintr1and fingerprintr2, where fingerprintr is computed by summing the index vectors ofeach nonunique coordinate of r. Random Indexing was shown to perform as well as LSAon a word synonym selection task Karlgren  Sahlgren, 2001.Locality sensitive hashing LSH Broder, 1997 is another technique that approximatesthe similarity matrix with complexity On2r2, where 2 is a constant number of randomprojections, which controls the accuracy versus efficiency tradeoff.21 LSH is a general class oftechniques for defining functions that map vectors rows or columns into short signatures orfingerprints, such that two similar vectors are likely to have similar fingerprints. Definitionsof LSH functions include the Minwise independent function, which preserves the Jaccardsimilarity between vectors Broder, 1997, and functions that preserve the cosine similaritybetween vectors Charikar, 2002. On a word similarity task, Ravichandran et al. 2005showed that, on average, over 80 of the top10 similar words of random words are foundin the top10 results using Charikars functions, and that the average cosine error is 0.01620. However, there are efficient forms of SVD Brand, 2006 Gorrell, 2006.21. LSH stems from work by Rabin 1981, who proposed the use of hash functions from random irreduciblepolynomials to create short fingerprints of collections of documents. Such techniques are useful for manytasks, such as removing duplicate documents deduping in a web crawl.164From Frequency to Meaningusing 2  10,000 random projections. Gorman and Curran 2006 provide a detailedcomparison of Random Indexing and LSH on a distributional similarity task. On the BNCcorpus, LSH outperformed Random Indexing however, on a larger corpora combining BNC,the Reuters Corpus, and most of the English news holdings of the LDC in 2003, RandomIndexing outperformed LSH in both efficiency and accuracy.4.6 Machine LearningIf the intended application for a VSM is clustering or classification, a similarity measuresuch as cosine Section 4.4 may be used. For classification, a nearestneighbour algorithmcan use cosine as a measure of nearness Dasarathy, 1991. For clustering, a similaritybased clustering algorithm can use cosine as a measure of similarity Jain, Murty,  Flynn,1999. However, there are many machine learning algorithms that can work directly withthe vectors in a VSM, without requiring an external similarity measure, such as cosine.In effect, such machine learning algorithms implicitly use their own internal approaches tomeasuring similarity.Any machine learning algorithm that works with realvalued vectors can use vectorsfrom a VSM Witten  Frank, 2005. Linguistic processing Section 3 and mathematicalprocessing Section 4 may still be necessary, but the machine learning algorithm can handlevector comparison Sections 4.4 and 4.5.In addition to unsupervised clustering and supervised classification machine learning, vectors from a VSM may also be used in semisupervised learning Ando  Zhang,2005 Collobert  Weston, 2008. In general, there is nothing unique to VSMs that wouldcompel a choice of one machine learning algorithm over another, aside from the algorithmsperformance on the given task. Therefore we refer our readers to the machine learningliterature Witten  Frank, 2005, since we have no advice that is specific to VSMs.5. Three Open Source VSM SystemsTo illustrate the three types of VSMs discussed in Section 2, this section presents three opensource systems, one for each VSM type. We have chosen to present open source systems sothat interested readers can obtain the source code to find out more about the systems andto apply the systems in their own projects. All three systems are written in Java and aredesigned for portability and ease of use.5.1 The TermDocument Matrix LuceneLucene22 is an open source fullfeatured text search engine library supported by the ApacheSoftware Foundation. It is arguably the most ubiquitous implementation of a termdocumentmatrix, powering many search engines such as at CNET, SourceForge, Wikipedia, Disney,AOL and Comcast. Lucene offers efficient storage, indexing, as well as retrieval and rankingfunctionalities. Although it is primarily used as a termdocument matrix, it generalizes toother VSMs.22. Apache Lucene is available for download at httplucene.apache.org.165Turney  PantelContent, such as webpages, PDF documents, images, and video, are programmaticallydecomposed into fields and stored in a database. The database implements the termdocument matrix, where content corresponds to documents and fields correspond to terms.Fields are stored in the database and indices are computed on the field values. Luceneuses fields as a generalization of content terms, allowing any other string or literal to indexdocuments. For example, a webpage could be indexed by all the terms it contains, and alsoby the anchor texts pointing to it, its host name, and the semantic classes in which it isclassified e.g., spam, product review, news, etc.. The webpage can then be retrieved bysearch terms matching any of these fields.Columns in the termdocument matrix consist of all the fields of a particular instanceof content e.g., a webpage. The rows consist of all instances of content in the index.Various statistics such as frequency and tfidf are stored in the matrix. The developerdefines the fields in a schema and identifies those to be indexed by Lucene. The developeralso optionally defines a content ranking function for each indexed field.Once the index is built, Lucene offers functionalities for retrieving content. Users canissue many query types such as phrase queries, wildcard queries, proximity queries, rangequeries e.g., date range queries, and fieldrestricted queries. Results can be sorted by anyfield and index updates can occur simultaneously during searching. Lucenes index can bedirectly loaded into a Tomcat webserver and it offers APIs for common programming languages. Solr,23 a separate Apache Software Foundation project, is an open source enterprisewebserver for searching a Lucene index and presenting search results. It is a fullfeaturedwebserver providing functionalities such as XMLHTTP and JSON APIs, hit highlighting,faceted search, caching, and replication.A simple recipe for creating a web search service, using Nutch, Lucene and Solr, consistsof crawling a set of URLs using Nutch, creating a termdocument matrix index usingLucene, and serving search results using Solr. Nutch,24 the Apache Software Foundationopen source web search software, offers functionality for crawling the web from a seed setof URLs, for building a linkgraph of the web crawl, and for parsing web documents suchas HTML pages. A good set of seed URLs for Nutch can be downloaded freely from theOpen Directory Project.25 Crawled pages are HTMLparsed, and they are then indexedby Lucene. The resulting indexed collection is then queried and served through a Solrinstallation with Tomcat.For more information on Lucene, we recommend Gospodnetic and Hatchers 2004book. Konchady 2008 explains how to integrate Lucene with LingPipe and GATE forsophisticated semantic processing.2623. Apache Solr is available for download at httplucene.apache.orgsolr.24. Apache Nutch is available for download at httplucene.apache.orgnutch.25. See httpwww.dmoz.org.26. Information about LingPipe is available at httpaliasi.comlingpipe. The GATE General Architecture for Text Engineering home page is at httpgate.ac.uk.166From Frequency to Meaning5.2 The WordContext Matrix Semantic VectorsSemantic Vectors27 is an open source project implementing the random projection approachto measuring word similarity see Section 4.5.3. The package uses Lucene to create a termdocument matrix, and it then creates vectors from Lucenes termdocument matrix, usingrandom projection for dimensionality reduction. The random projection vectors can beused, for example, to measure the semantic similarity of two words or to find the wordsthat are most similar to a given word.The idea of random projection is to take highdimensional vectors and randomly projectthem into a relatively lowdimensional space Sahlgren, 2005. This can be viewed as akind of smoothing operation Section 4.3, but the developers of the Semantic Vectorspackage emphasize the simplicity and efficiency of random projection Section 4.5, ratherthan its smoothing ability. They argue that other matrix smoothing algorithms mightsmooth better, but none of them perform as well as random indexing, in terms of thecomputational complexity of building a smooth matrix and incrementally updating thematrix when new data arrives Widdows  Ferraro, 2008. Their aim is to encourageresearch and development with semantic vectors by creating a simple and efficient opensource package.The Semantic Vectors package is designed to be convenient to use, portable, and easyto extend and modify. The design of the software incorporates lessons learned from theearlier Stanford Infomap project.28 Although the default is to generate random projectionvectors, the system has a modular design that allows other kinds of wordcontext matricesto be used instead of random projection matrices.The package supports two basic functions building a wordcontext matrix and searchingthrough the vectors in the matrix. In addition to generating word vectors, the buildingoperation can generate document vectors by calculating weighted sums of the word vectorsfor the words in each document. The searching operation can be used to search for similarwords or to search for documents that are similar to a query. A query can be a single word orseveral words can be combined, using various mathematical operations on the correspondingvectors. The mathematical operations include vector negation and disjunction, based onquantum logic Widdows, 2004. Widdows and Ferraro 2008 provide a good summary ofthe Semantic Vectors software.5.3 The PairPattern Matrix Latent Relational Analysis in SSpaceLatent Relational Analysis29 LRA is an open source project implementing the pairpatternmatrix. It is a component of the SSpace package, a library of tools for building andcomparing different semantic spaces.LRA takes as input a textual corpus and a set of word pairs. A pairpattern matrix isbuilt by deriving lexical patterns that link together the word pairs in the corpus. For example, consider the word pair Korea, Japan and the following retrieved matching sentences27. Semantic Vectors is a software package for measuring word similarity, available under the Simplified BSDLicense at httpcode.google.compsemanticvectors.28. See httpinfomapnlp.sourceforge.net.29. Latent Relational Analysis is part of the SSpace package and is distributed under the GNU GeneralPublic License version 2. It is available at httpcode.google.compairheadresearch. At the time ofwriting, the LRA module was under development.167Turney  Pantel Korea looks to new Japan prime ministers effect on KoreaJapan relations. What channel is the Korea vs. Japan football gameFrom these two sentences, LRA extracts two patterns X looks to new Y  and X vs. Y .These patterns become two columns in the pairpattern matrix, and the word pair Korea,Japan becomes a row. Pattern frequencies are counted and then smoothed using SVD seeSection 4.3.In order to mitigate the sparseness of occurrences of word pairs, a thesaurus such asWordNet is used to expand the seed word pairs to alternatives. For example the pairKorea, Japan may be expanded to include South Korea, Japan, Republic of Korea,Japan, Korea, Nippon, South Korea, Nippon, and Republic of Korea, Nippon.LRA uses Lucene see Section 5.1 as its backend to store the matrix, index it, and serveits contents. For a detailed description of the LRA algorithm, we suggest Turneys 2006paper.6. ApplicationsIn this section, we will survey some of the semantic applications for VSMs. We will aim forbreadth, rather than depth readers who want more depth should consult the references. Ourgoal is to give the reader an impression of the scope and flexibility of VSMs for semantics.The following applications are grouped according to the type of matrix involved termdocument, wordcontext, or pairpattern. Note that this section is not exhaustive thereare many more references and applications than we have space to list here.6.1 TermDocument MatricesTermdocument matrices are most suited to measuring the semantic similarity of documentsand queries see Section 2.1. The usual measure of similarity is the cosine of column vectorsin a weighted termdocument matrix. There are a variety of applications for measures ofdocument similarity.Document retrieval The termdocument matrix was first developed for documentretrieval Salton et al., 1975, and there is now a large body of literature on the VSMfor document retrieval Manning et al., 2008, including several journals and conferencesdevoted to the topic. The core idea is, given a query, rank the documents in order ofdecreasing cosine of the angles between the query vector and the document vectors Saltonet al., 1975. One variation on the theme is crosslingual document retrieval, where aquery in one language is used to retrieve a document in another language Landauer Littman, 1990. An important technical advance was the discovery that smoothing thetermdocument matrix by truncated SVD can improve precision and recall Deerwesteret al., 1990, although few commercial systems use smoothing, due to the computationalexpense when the document collection is large and dynamic. Random indexing Sahlgren,2005 or incremental SVD Brand, 2006 may help to address these scaling issues. Anotherimportant development in document retrieval has been the addition of collaborative filtering,in the form of PageRank Brin  Page, 1998.Document clustering Given a measure of document similarity, we can cluster thedocuments into groups, such that similarity tends to be high within a group, but low across168From Frequency to Meaninggroups Manning et al., 2008. The clusters may be partitional flat Cutting, Karger,Pedersen,  Tukey, 1992 Pantel  Lin, 2002b or they may have a hierarchical structuregroups of groups Zhao  Karypis, 2002 they may be nonoverlapping hard Croft,1977 or overlapping soft Zamir  Etzioni, 1999. Clustering algorithms also differ in howclusters are compared and abstracted. With singlelink clustering, the similarity betweentwo clusters is the maximum of the similarities between their members. Completelinkclustering uses the minimum of the similarities and averagelink clustering uses the averageof the similarities Manning et al., 2008.Document classification Given a training set of documents with class labels and atesting set of unlabeled documents, the task of document classification is to learn from thetraining set how to assign labels to the testing set Manning et al., 2008. The labels maybe the topics of the documents Sebastiani, 2002, the sentiment of the documents e.g.,positive versus negative product reviews Pang, Lee,  Vaithyanathan, 2002 Kim, Pantel,Chklovski,  Pennacchiotti, 2006, spam versus nonspam Sahami, Dumais, Heckerman, Horvitz, 1998 Pantel  Lin, 1998, or any other labels that might be inferred from the wordsin the documents. When we classify documents, we are implying that the documents in aclass are similar in some way thus document classification implies some notion of documentsimilarity, and most machine learning approaches to document classification involve a termdocument matrix Sebastiani, 2002. A measure of document similarity, such as cosine, canbe directly applied to document classification by using a nearestneighbour algorithm Yang,1999.Essay grading Student essays may be automatically graded by comparing them toone or more highquality reference essays on the given essay topic Wolfe, Schreiner, Rehder,Laham, Foltz, Kintsch,  Landauer, 1998 Foltz, Laham,  Landauer, 1999. The studentessays and the reference essays can be compared by their cosines in a termdocument matrix.The grade that is assigned to a student essay is proportional to its similarity to one of thereference essays a student essay that is highly similar to a reference essay gets a high grade.Document segmentation The task of document segmentation is to partition a document into sections, where each section focuses on a different subtopic of the documentHearst, 1997 Choi, 2000. We may treat the document as a series of blocks, where a blockis a sentence or a paragraph. The problem is to detect a topic shift from one block to thenext. Hearst 1997 and Choi 2000 both use the cosine between columns in a wordblockfrequency matrix to measure the semantic similarity of blocks. A topic shift is signaled bya drop in the cosine between consecutive blocks. The wordblock matrix can be viewed asa small termdocument matrix, where the corpus is a single document and the documentsare blocks.Question answering Given a simple question, the task in Question Answering QAis to find a short answer to the question by searching in a large corpus. A typical question is, How many calories are there in a Big Mac Most algorithms for QA have fourcomponents, question analysis, document retrieval, passage retrieval, and answer extractionTellex, Katz, Lin, Fern,  Marton, 2003 Dang, Lin,  Kelly, 2006. Vectorbased similarity measurements are often used for both document retrieval and passage retrieval Tellexet al., 2003.Call routing Chucarroll and Carpenter 1999 present a vectorbased system forautomatically routing telephone calls, based on the callers spoken answer to the question,169Turney  PantelHow may I direct your call If the callers answer is ambiguous, the system automaticallygenerates a question for the caller, derived from the VSM, that prompts the caller for furtherinformation.6.2 WordContext MatricesWordcontext matrices are most suited to measuring the semantic similarity of words seeSection 2.2. For example, we can measure the similarity of two words by the cosine of theangle between their corresponding row vectors in a wordcontext matrix. There are manyapplications for measures of word similarity.Word similarity Deerwester et al. 1990 discovered that we can measure word similarity by comparing row vectors in a termdocument matrix. Landauer and Dumais 1997evaluated this approach with 80 multiplechoice synonym questions from the Test of English as a Foreign Language TOEFL, achieving humanlevel performance 64.4 correctfor the wordcontext matrix and 64.5 for the average nonEnglish US college applicant.The documents used by Landauer and Dumais had an average length of 151 words, whichseems short for a document, but long for the context of a word. Other researchers soonswitched to much shorter lengths, which is why we prefer to call these wordcontext matrices, instead of termdocument matrices. Lund and Burgess 1996 used a context windowof ten words. Schutze 1998 used a fiftyword window 25 words, centered on the targetword. Rapp 2003 achieved 92.5 correct on the 80 TOEFL questions, using a fourwordcontext window 2 words, centered on the target word, after removing stop words. TheTOEFL results suggest that performance improves as the context window shrinks. It seemsthat the immediate context of a word is much more important than the distant context fordetermining the meaning of the word.Word clustering Pereira, Tishby, and Lee 1993 applied soft hierarchical clusteringto rowvectors in a wordcontext matrix. In one experiment, the words were nouns and thecontexts were verbs for which the given nouns were direct objects. In another experiment,the words were verbs and the contexts were nouns that were direct objects of the givenverbs. Schutzes 1998 seminal word sense discrimination model used hard flat clusteringfor rowvectors in a wordcontext matrix, where the context was given by a window of 25words, centered on the target word. Pantel and Lin 2002a applied soft flat clustering toa wordcontext matrix, where the context was based on parsed text. These algorithms areable to discover different senses of polysemous words, generating different clusters for eachsense. In effect, the different clusters correspond to the different concepts that underlie thewords.Word classification Turney and Littman 2003 used a wordcontext matrix to classify words as positive honest, intrepid or negative disturbing, superfluous. They used theGeneral Inquirer GI lexicon Stone, Dunphy, Smith,  Ogilvie, 1966 to evaluate theiralgorithms. The GI lexicon includes 11,788 words, labeled with 182 categories related toopinion, affect, and attitude.30 Turney and Littman hypothesize that all 182 categories canbe discriminated with a wordcontext matrix.Automatic thesaurus generation WordNet is a popular tool for research in naturallanguage processing Fellbaum, 1998, but creating and maintaing such lexical resources30. The GI lexicon is available at httpwww.wjh.harvard.eduinquirerspreadsheet guide.htm.170From Frequency to Meaningis labour intensive, so it is natural to wonder whether the process can be automated tosome degree.31 This task can seen as an instance of word clustering when the thesaurusis generated from scratch or classification when an existing thesaurus is automaticallyextended, but it is worthwhile to consider the task of automatic thesaurus generationseparately from clustering and classification, due to the specific requirements of a thesaurus,such as the particular kind of similarity that is appropriate for a thesaurus see Section 2.4.Several researchers have used wordcontext matrices specifically for the task of assisting orautomating thesaurus generation Crouch, 1988 Grefenstette, 1994 Ruge, 1997 Pantel Lin, 2002a Curran  Moens, 2002.Word sense disambiguation A typical Word Sense Disambiguation WSD systemAgirre  Edmonds, 2006 Pedersen, 2006 uses a feature vector representation in whicheach vector corresponds to a token of a word, not a type see Section 2.6. However, Leacock,Towell, and Voorhees 1993 used a wordcontext frequency matrix for WSD, in which eachvector corresponds to a type annotated with a sense tag. Yuret and Yatbaz 2009 applieda wordcontext frequency matrix to unsupervised WSD, achieving results comparable tothe performance of supervised WSD systems.Contextsensitive spelling correction People frequently confuse certain sets ofwords, such as there, theyre, and their. These confusions cannot be detected by a simple dictionarybased spelling checker they require contextsensitive spelling correction. Awordcontext frequency matrix may be used to correct these kinds of spelling errors Jones Martin, 1997.Semantic role labeling The task of semantic role labeling is to label parts of a sentence according to the roles they play in the sentence, usually in terms of their connectionto the main verb of the sentence. Erk 2007 presented a system in which a wordcontextfrequency matrix was used to improve the performance of semantic role labeling. Pennacchiotti, Cao, Basili, Croce, and Roth 2008 show that wordcontext matrices can reliablypredict the semantic frame to which an unknown lexical unit refers, with good levels ofaccuracy. Such lexical unit induction is important in semantic role labeling, to narrow thecandidate set of roles of any observed lexical unit.Query expansion Queries submitted to search engines such as Google and Yahoooften do not directly match the terms in the most relevant documents. To alleviate thisproblem, the process of query expansion is used for generating new search terms that areconsistent with the intent of the original query. VSMs form the basis of query semanticsmodels Cao, Jiang, Pei, He, Liao, Chen,  Li, 2008. Some methods represent queriesby using session contexts, such as query cooccurrences in user sessions Huang, Chien, Oyang, 2003 Jones, Rey, Madani,  Greiner, 2006, and others use click contexts, such asthe urls that were clicked on as a result of a query Wen, Nie,  Zhang, 2001.Textual advertising In payperclick advertising models, prevalent in search enginessuch as Google and Yahoo, users pay for keywords, called bidterms, which are then used todisplay their ads when relevant queries are issued by users. The scarcity of data makes admatching difficult and, in response, several techniques for bidterm expansion using VSMshave been proposed. The wordcontext matrix consists of rows of bidterms and the columns31. WordNet is available at httpwordnet.princeton.edu.171Turney  Pantelcontexts consist of advertiser identifiers Gleich  Zhukov, 2004 or cobidded bidtermssecond order cooccurrences Chang, Pantel, Popescu,  Gabrilovich, 2009.Information extraction The field of information extraction IE includes namedentity recognition NER recognizing that a chunk of text is the name of an entity, such asa person or a place, relation extraction, event extraction, and fact extraction. Pasca etal. 2006 demonstrate that a wordcontext frequency matrix can facilitate fact extraction.Vyas and Pantel 2009 propose a semisupervised model using a wordcontext matrix forbuilding and iteratively refining arbitrary classes of named entities.6.3 PairPattern MatricesPairpattern matrices are most suited to measuring the semantic similarity of word pairsand patterns see Section 2.3. For example, we can measure the similarity of two wordpairs by the cosine of the angle between their corresponding row vectors in a pairpatternmatrix. There are many applications for measures of relational similarity.Relational similarity Just as we can measure attributional similarity by the cosineof the angle between row vectors in a wordcontext matrix, we can measure relationalsimilarity by the cosine of the angle between rows in a pairpattern matrix. This approachto measuring relational similarity was introduced by Turney et al. 2003 and examinedin more detail by Turney and Littman 2005. Turney 2006 evaluated this approachto relational similarity with 374 multiplechoice analogy questions from the SAT collegeentrance test, achieving humanlevel performance 56 correct for the pairpattern matrixand 57 correct for the average US college applicant. This is the highest performance sofar for an algorithm. The best algorithm based on attributional similarity has an accuracyof only 35 Turney, 2006. The best nonVSM algorithm achieves 43 Veale, 2004.Pattern similarity Instead of measuring the similarity between row vectors in a pairpattern matrix, we can measure the similarity between columns that is, we can measurepattern similarity. Lin and Pantel 2001 constructed a pairpattern matrix in which thepatterns were derived from parsed text. Pattern similarity can be used to infer that onephrase is a paraphrase of another phrase, which is useful for natural language generation,text summarization, information retrieval, and question answering.Relational clustering Bicici and Yuret 2006 clustered word pairs by representingthem as row vectors in a pairpattern matrix. Davidov and Rappoport 2008 first clusteredcontexts patterns and then identified representative pairs for each context cluster. Theyused the representative pairs to automatically generate multiplechoice analogy questions,in the style of SAT analogy questions.Relational classification Chklovski and Pantel 2004 used a pairpattern matrixto classify pairs of verbs into semantic classes. For example, taint  poison is classified asstrength poisoning is stronger than tainting and assess  review is classified as enablementassessing is enabled by reviewing. Turney 2005 used a pairpattern matrix to classifynoun compounds into semantic classes. For example, flu virus is classified as cause thevirus causes the flu, home town is classified as location the home is located in the town,and weather report is classified as topic the topic of the report is the weather.Relational search Cafarella, Banko, and Etzioni 2006 described relational searchas the task of searching for entities that satisfy given semantic relations. An example of172From Frequency to Meaninga query for a relational search engine is list all X such that X causes cancer. In thisexample, the relation, cause, and one of the terms in the relation, cancer, are given by theuser, and the task of the search engine is to find terms that satisfy the users query. Theorganizers of Task 4 in SemEval 2007 Girju, Nakov, Nastase, Szpakowicz, Turney,  Yuret,2007 envisioned a twostep approach to relational search first a conventional search enginewould look for candidate answers, then a relational classification system would filter outincorrect answers. The first step was manually simulated by the Task 4 organizers and thegoal of Task 4 was to design systems for the second step. This task attracted 14 teams whosubmitted 15 systems. Nakov and Hearst 2007 achieved good results using a pairpatternmatrix.Automatic thesaurus generation We discussed automatic thesaurus generation inSection 6.2, with wordcontext matrices, but arguably relational similarity is more relevantthan attributional similarity for thesaurus generation. For example, most of the information in WordNet is in the relations between the words rather than in the words individually.Snow, Jurafsky, and Ng 2006 used a pairpattern matrix to build a hypernymhyponymtaxonomy, whereas Pennacchiotti and Pantel 2006 built a meronymy and causation taxonomy. Turney 2008b showed how a pairpattern matrix can distinguish synonyms fromantonyms, synonyms from nonsynonyms, and taxonomically similar words hair and furfrom words that are merely semantically associated cradle and baby.Analogical mapping Proportional analogies have the form a b c d, which means ais to b as c is to d. For example, mason  stone  carpenter  wood means mason is to stoneas carpenter is to wood. The 374 multiplechoice analogy questions from the SAT collegeentrance test mentioned above all involve proportional analogies. With a pairpatternmatrix, we can solve proportional analogies by selecting the choice that maximizes relationalsimilarity e.g., simrmason stone, carpenter wood has a high value. However, we oftenencounter analogies that involve more than four terms. The wellknown analogy betweenthe solar system and the RutherfordBohr model of the atom contains at least fourteenterms. For the solar system, we have planet, attracts, revolves, sun, gravity, solar system,and mass. For the atom, we have revolves, atom, attracts, electromagnetism, nucleus,charge, and electron. Turney 2008a demonstrated that we can handle these more complex,systematic analogies by decomposing them into sets of proportional analogies.7. Alternative Approaches to SemanticsThe applications that we list in Section 6 do not necessarily require a VSM approach. Foreach application, there are many other possible approaches. In this section, we brieflyconsider a few of the main alternatives.Underlying the applications for termdocument matrices Section 6.1 is the task ofmeasuring the semantic similarity of documents and queries. The main alternatives toVSMs for this task are probabilistic models, such as the traditional probabilistic retrievalmodels in information retrieval van Rijsbergen, 1979 BaezaYates  RibeiroNeto, 1999and the more recent statistical language models inspired by information theory Liu Croft, 2005. The idea of statistical language models for information retrieval is to measurethe similarity between a query and a document by creating a probabilistic language model173Turney  Pantelof the given document and then measuring the probability of the given query according tothe language model.With progress in information retrieval, the distinction between the VSM approach andthe probabilistic approach is becoming blurred, as each approach borrows ideas from theother. Language models typically involve multiplying probabilities, but we can view this asadding logs of probabilities, which makes some language models look similar to VSMs.The applications for wordcontext matrices Section 6.2 share the task of measuring thesemantic similarity of words. The main alternatives to VSMs for measuring word similarityare approaches that use lexicons, such as WordNet Resnik, 1995 Jiang  Conrath, 1997Hirst  StOnge, 1998 Leacock  Chodrow, 1998 Budanitsky  Hirst, 2001. The idea isto view the lexicon as a graph, in which nodes correspond to word senses and edges representrelations between words, such as hypernymy and hyponymy. The similarity between twowords is then proportional to the length of the path in the graph that joins the two words.Several approaches to measuring the semantic similarity of words combine a VSM witha lexicon Turney et al., 2003 Pantel, 2005 Patwardhan  Pedersen, 2006 Mohammad Hirst, 2006. Humans use both dictionary definitions and observations of word usage, so itis natural to expect the best performance from algorithms that use both distributional andlexical information.Pairpattern matrices Section 6.3 have in common the task of measuring the semanticsimilarity of relations. As with wordcontext matrices, the main alternatives are approachesthat use lexicons Rosario  Hearst, 2001 Rosario, Hearst,  Fillmore, 2002 Nastase Szpakowicz, 2003 Veale, 2003, 2004. The idea is to reduce relational similarity toattributional similarity, simra  b, c  d  simaa, c  simab, d, and then use a lexiconto measure attributional similarity. As we discuss in Section 2.4, this reduction does notwork in general. However, the reduction is often a good approximation, and there is someevidence that a hybrid approach, combining a VSM with a lexicon, can be beneficial Turneyet al., 2003 Nastase, SayyadShirabad, Sokolova,  Szpakowicz, 2006.8. The Future of Vector Space Models of SemanticsSeveral authors have criticized VSMs French  Labiouse, 2002 Pado  Lapata, 2003Morris  Hirst, 2004 Budanitsky  Hirst, 2006. Most of the criticism stems from thefact that termdocument and wordcontext matrices typically ignore word order. In LSA,for instance, a phrase is commonly represented by the sum of the vectors for the individualwords in the phrase hence the phrases house boat and boat house will be represented bythe same vector, although they have different meanings. In English, word order expressesrelational information. Both house boat and boat house have a ToolPurpose relation, buthouse boat means ToolPurposeboat, house a boat that serves as a house, whereas boathouse means ToolPurposehouse, boat a house for sheltering and storing boats.Landauer 2002 estimates that 80 of the meaning of English text comes from wordchoice and the remaining 20 comes from word order. However, VSMs are not inherentlylimited to 80 of the meaning of text. Mitchell and Lapata 2008 propose compositionmodels sensitive to word order. For example, to make a simple additive model becomesyntaxaware, they allow for different weightings of the contributions of the vector components. Constituents that are more important to the composition therefore can participate174From Frequency to Meaningmore actively. Clark and Pulman 2007 assigned distributional meaning to sentences using the Hilbert space tensor product. Widdows and Ferraro 2008, inspired by quantummechanics, explores several operators for modeling composition of meaning. Pairpatternmatrices are sensitive to the order of the words in a pair Turney, 2006. Thus there areseveral ways to handle word order in VSMs.This raises the question, what are the limits of VSMs for semantics Can all semanticsbe represented with VSMs There is much that we do not yet know how to represent withVSMs. For example, Widdows 2004 and van Rijsbergen 2004 show how disjunction,conjunction, and negation can be represented with vectors, but we do not yet know how torepresent arbitrary statements in firstorder predicate calculus. However, it seems possiblethat future work may discover answers to these limitations.In this survey, we have assumed that VSMs are composed of elements with values thatare derived from event frequencies. This ties VSMs to some form of distributional hypothesissee Sections 1.1 and 2.7 therefore the limits of VSMs depend on the limits of the familyof distributional hypotheses. Are statistical patterns of word usage sufficient to figure outwhat people mean This is arguably the major open question of VSMs, and the answer willdetermine the future of VSMs. We do not have a strong argument one way or the other,but we believe that the continuing progress with VSMs suggests we are far from reachingtheir limits.9. ConclusionsWhen we want information or help from a person, we use words to make a request ordescribe a problem, and the person replies with words. Unfortunately, computers do notunderstand human language, so we are forced to use artificial languages and unnatural userinterfaces. In science fiction, we dream of computers that understand human language, thatcan listen to us and talk with us. To achieve the full potential of computers, we must enablethem to understand the semantics of natural language. VSMs are likely to be part of thesolution to the problem of computing semantics.Many researchers who have struggled with the problem of semantics have come to theconclusion that the meaning of words is closely connected to the statistics of word usageSection 2.7. When we try to make this intuition precise, we soon find we are working withvectors of values derived from event frequencies that is, we are dealing with VSMs.In this survey, we have organized past work with VSMs according to the structure ofthe matrix termdocument, wordcontext, or pairpattern. We believe that the structureof the matrix is the most important factor in determining the types of applications thatare possible. The linguistic processing Section 3 and mathematical processing Section 4play smaller but important roles.Our goal in this survey has been to show the breadth and power of VSMs, to introduceVSMs to those who less familiar with them, and to provide a new perspective on VSMs tothose who are already familiar with them. We hope that our emphasis on the structure ofthe matrix will inspire new research. There is no reason to believe that the three matrixtypes we present here exhaust the possibilities. We expect new matrix types and new tensorswill open up more applications for VSMs. It seems possible to us that all of the semanticsof human language might one day be captured in some kind of VSM.175Turney  PantelAcknowledgmentsThanks to Annie Zaenen for prompting this paper. Thanks to Saif Mohammad and MarianaSoffer for their comments. Thanks to Arkady Borkovsky and Eric Crestan for developingthe distributed sparsematrix multiplication algorithm, and to Marco Pennacchiotti for hisinvaluable comments. Thanks to the anonymous reviewers of JAIR for their very helpfulcomments and suggestions.ReferencesAcar, E.,  Yener, B. 2009. Unsupervised multiway data analysis A literature survey.IEEE Transactions on Knowledge and Data Engineering, 21 1, 620.Agirre, E.,  Edmonds, P. G. 2006. Word Sense Disambiguation Algorithms and Applications. Springer.Ando, R. K. 2000. Latent semantic space Iterative scaling improves precision of interdocument similarity measurement. In Proceedings of the 23rd Annual ACM SIGIRConference on Research and Development in Information Retrieval SIGIR2000, pp.216223.Ando, R. K.,  Zhang, T. 2005. A framework for learning predictive structures frommultiple tasks and unlabeled data. Journal of Machine Learning Research, 6, 18171853.BaezaYates, R.,  RibeiroNeto, B. 1999. Modern Information Retrieval. Addison Wesley.Barr, C., Jones, R.,  Regelson, M. 2008. The linguistic structure of English websearch queries. In Conference on Empirical Methods in Natural Language ProcessingEMNLP.Bayardo, R. J., Ma, Y.,  Srikant, R. 2007. Scaling up all pairs similarity search. InProceedings of the 16th international conference on World Wide Web WWW 07,pp. 131140, New York, NY. ACM.Bicici, E.,  Yuret, D. 2006. Clustering word pairs to answer analogy questions. InProceedings of the Fifteenth Turkish Symposium on Artificial Intelligence and NeuralNetworks TAINN 2006, Akyaka, Mugla, Turkey.Blei, D. M., Ng, A. Y.,  Jordan, M. I. 2003. Latent Dirichlet Allocation. Journal ofMachine Learning Research, 3, 9931022.Brand, M. 2006. Fast lowrank modifications of the thin singular value decomposition.Linear Algebra and Its Applications, 415 1, 2030.Breese, J., Heckerman, D.,  Kadie, C. 1998. Empirical analysis of predictive algorithmsfor collaborative filtering. In Proceedings of the 14th Conference on Uncertainty inArtificial Intelligence, pp. 4352. Morgan Kaufmann.Brin, S.,  Page, L. 1998. The anatomy of a largescale hypertextual Web search engine.In Proceedings of the Seventh World Wide Web Conference WWW7, pp. 107117.Broder, A. 1997. On the resemblance and containment of documents. In In Compressionand Complexity of Sequences SEQUENCES97, pp. 2129. IEEE Computer Society.176From Frequency to MeaningBudanitsky, A.,  Hirst, G. 2001. Semantic distance in WordNet An experimental,applicationoriented evaluation of five measures. In Proceedings of the Workshopon WordNet and Other Lexical Resources, Second Meeting of the North AmericanChapter of the Association for Computational Linguistics NAACL2001, pp. 2924,Pittsburgh, PA.Budanitsky, A.,  Hirst, G. 2006. Evaluating wordnetbased measures of semantic distance. Computational Linguistics, 32 1, 1347.Bullinaria, J.,  Levy, J. 2007. Extracting semantic representations from word cooccurrence statistics A computational study. Behavior Research Methods, 39 3,510526.Buntine, W.,  Jakulin, A. 2006. Discrete component analysis. In Subspace, LatentStructure and Feature Selection Statistical and Optimization Perspectives Workshopat SLSFS 2005, pp. 133, Bohinj, Slovenia. Springer.Cafarella, M. J., Banko, M.,  Etzioni, O. 2006. Relational web search. Tech. rep., University of Washington, Department of Computer Science and Engineering. TechnicalReport 20060402.Cao, G., Nie, J.Y.,  Bai, J. 2005. Integrating word relationships into language models.In Proceedings of the 28th Annual International ACM SIGIR Conference on Researchand Development in Information Retrieval SIGIR 05, pp. 298305, New York, NY.ACM.Cao, H., Jiang, D., Pei, J., He, Q., Liao, Z., Chen, E.,  Li, H. 2008. Contextaware querysuggestion by mining clickthrough and session data. In Proceeding of the 14th ACMSIGKDD International Conference on Knowledge Discovery and Data Mining KDD08, pp. 875883. ACM.Carroll, J. D.,  Chang, J.J. 1970. Analysis of individual differences in multidimensionalscaling via an nway generalization of EckartYoung decomposition. Psychometrika,35 3, 283319.Chang, W., Pantel, P., Popescu, A.M.,  Gabrilovich, E. 2009. Towards intentdrivenbidterm suggestion. In Proceedings of WWW09 Short Paper, Madrid, Spain.Charikar, M. S. 2002. Similarity estimation techniques from rounding algorithms. In Proceedings of the thiryfourth annual ACM symposium on Theory of computing STOC02, pp. 380388. ACM.Chew, P., Bader, B., Kolda, T.,  Abdelali, A. 2007. Crosslanguage information retrieval using PARAFAC2. In Proceedings of the 13th ACM SIGKDD InternationalConference on Knowledge Discovery and Data Mining KDD07, pp. 143152. ACMPress.Chiarello, C., Burgess, C., Richards, L.,  Pollock, A. 1990. Semantic and associativepriming in the cerebral hemispheres Some words do, some words dont . . . sometimes,some places. Brain and Language, 38, 75104.Chklovski, T.,  Pantel, P. 2004. VerbOcean Mining the web for finegrained semanticverb relations. In Proceedings of Experimental Methods in Natural Language Processing 2004 EMNLP04, pp. 3340, Barcelona, Spain.177Turney  PantelChoi, F. Y. Y. 2000. Advances in domain independent linear text segmentation. InProceedings of the 1st Meeting of the North American Chapter of the Association forComputational Linguistics, pp. 2633.Chucarroll, J.,  Carpenter, B. 1999. Vectorbased natural language call routing. Computational Linguistics, 25 3, 361388.Church, K. 1995. One term or two. In Proceedings of the 18th Annual InternationalACM SIGIR Conference on Research and Development in Information Retrieval, pp.310318.Church, K.,  Hanks, P. 1989. Word association norms, mutual information, and lexicography. In Proceedings of the 27th Annual Conference of the Association of Computational Linguistics, pp. 7683, Vancouver, British Columbia.Clark, S.,  Pulman, S. 2007. Combining symbolic and distributional models of meaning.In Proceedings of AAAI Spring Symposium on Quantum Interaction, pp. 5255.Collobert, R.,  Weston, J. 2008. A unified architecture for natural language processingDeep neural networks with multitask learning. In Proceedings of the 25th InternationalConference on Machine Learning ICML08, pp. 160167.Croft, W. B. 1977. Clustering large files of documents using the singlelink method.Journal of the American Society for Information Science, 28 6, 341344.Crouch, C. J. 1988. A clusterbased approach to thesaurus construction. In Proceedingsof the 11th Annual International ACM SIGIR Conference, pp. 309320, Grenoble,France.Curran, J. R.,  Moens, M. 2002. Improvements in automatic thesaurus extraction. InUnsupervised Lexical Acquisition Proceedings of the Workshop of the ACL SpecialInterest Group on the Lexicon SIGLEX, pp. 5966, Philadelphia, PA.Cutting, D. R., Karger, D. R., Pedersen, J. O.,  Tukey, J. W. 1992. Scattergather aclusterbased approach to browsing large document collections. In Proceedings of the15th Annual International ACM SIGIR Conference, pp. 318329.Dagan, I., Lee, L.,  Pereira, F. C. N. 1999. Similaritybased models of word cooccurrenceprobabilities. Machine Learning, 34 13, 4369.Dang, H. T., Lin, J.,  Kelly, D. 2006. Overview of the TREC 2006 question answeringtrack. In Proceedings of the Fifteenth Text REtrieval Conference TREC 2006.Dasarathy, B. 1991. Nearest Neighbor NN Norms NN Pattern Classification Techniques.IEEE Computer Society Press.Davidov, D.,  Rappoport, A. 2008. Unsupervised discovery of generic relationships usingpattern clusters and its evaluation by automatically generated SAT analogy questions.In Proceedings of the 46th Annual Meeting of the ACL and HLT ACLHLT08, pp.692700, Columbus, Ohio.Dean, J.,  Ghemawat, S. 2008. MapReduce Simplified data processing on large clusters.Communications of the ACM, 51 1, 107113.178From Frequency to MeaningDeerwester, S. C., Dumais, S. T., Landauer, T. K., Furnas, G. W.,  Harshman, R. A.1990. Indexing by latent semantic analysis. Journal of the American Society forInformation Science JASIS, 41 6, 391407.Elsayed, T., Lin, J.,  Oard, D. 2008. Pairwise document similarity in large collectionswith mapreduce. In Proceedings of Association for Computational Linguistics andHuman Language Technology Conference 2008 ACL08 HLT, Short Papers, pp.265268, Columbus, Ohio. Association for Computational Linguistics.Erk, K. 2007. A simple, similaritybased model for selectional preferences. In Proceedingsof the 45th Annual Meeting of the Association of Computational Linguistics, pp. 216223,, Prague, Czech Republic.Erk, K.,  Pado, S. 2008. A structured vector space model for word meaning in context.In Proceedings of the 2008 Conference on Empirical Methods in Natural LanguageProcessing EMNLP08, pp. 897906, Honolulu, HI.Fellbaum, C. Ed.. 1998. WordNet An Electronic Lexical Database. MIT Press.Firth, J. R. 1957. A synopsis of linguistic theory 19301955. In Studies in LinguisticAnalysis, pp. 132. Blackwell, Oxford.Foltz, P. W., Laham, D.,  Landauer, T. K. 1999. The intelligent essay assessor Applications to educational technology. Interactive Multimedia Electronic Journal ofComputerEnhanced Learning, 1 2.Forman, G. 2003. An extensive empirical study of feature selection metrics for text classification. Journal of Machine Learning Research, 3, 12891305.French, R. M.,  Labiouse, C. 2002. Four problems with extracting human semantics fromlarge text corpora. In Proceedings of the 24th Annual Conference of the CognitiveScience Society.Furnas, G. W., Landauer, T. K., Gomez, L. M.,  Dumais, S. T. 1983. Statistical semantics Analysis of the potential performance of keyword information systems. BellSystem Technical Journal, 62 6, 17531806.Gentner, D. 1983. Structuremapping A theoretical framework for analogy. CognitiveScience, 7 2, 155170.Gilbert, J. R., Moler, C.,  Schreiber, R. 1992. Sparse matrices in MATLAB Design andimplementation. SIAM Journal on Matrix Analysis and Applications, 13 1, 333356.Girju, R., Nakov, P., Nastase, V., Szpakowicz, S., Turney, P.,  Yuret, D. 2007. Semeval2007 task 04 Classification of semantic relations between nominals. In Proceedingsof the Fourth International Workshop on Semantic Evaluations SemEval 2007, pp.1318, Prague, Czech Republic.Gleich, D.,  Zhukov, L. 2004. SVD based term suggestion and ranking system. InProceedings of the Fourth IEEE International Conference on Data Mining ICDM04, pp. 391394. IEEE Computer Society.Golub, G. H.,  Van Loan, C. F. 1996. Matrix Computations Third edition. JohnsHopkins University Press, Baltimore, MD.179Turney  PantelGorman, J.,  Curran, J. R. 2006. Scaling distributional similarity to large corpora. InProceedings of the 21st International Conference on Computational Linguistics andthe 44th annual meeting of the Association for Computational Linguistics ACL 2006,pp. 361368. Association for Computational Linguistics.Gorrell, G. 2006. Generalized Hebbian algorithm for incremental singular value decomposition in natural language processing. In Proceedings of the 11th Conference of theEuropean Chapter of the Association for Computational Linguistics EACL06, pp.97104.Gospodnetic, O.,  Hatcher, E. 2004. Lucene in Action. Manning Publications.Grefenstette, G. 1994. Explorations in Automatic Thesaurus Discovery. Kluwer.Harris, Z. 1954. Distributional structure. Word, 10 23, 146162.Harshman, R. 1970. Foundations of the parafac procedure Models and conditions for anexplanatory multimodal factor analysis. UCLA Working Papers in Phonetics, 16.Hearst, M. 1997. Texttiling Segmenting text into multiparagraph subtopic passages.Computational Linguistics, 23 1, 3364.Hirst, G.,  StOnge, D. 1998. Lexical chains as representations of context for the detectionand correction of malapropisms. In Fellbaum, C. Ed., WordNet An ElectronicLexical Database, pp. 305332. MIT Press.Hofmann, T. 1999. Probabilistic Latent Semantic Indexing. In Proceedings of the 22ndAnnual ACM Conference on Research and Development in Information Retrieval SIGIR 99, pp. 5057, Berkeley, California.Huang, C.K., Chien, L.F.,  Oyang, Y.J. 2003. Relevant term suggestion in interactiveweb search based on contextual information in query session logs. Journal of theAmerican Society for Information Science and Technology, 54 7, 638649.Hull, D. 1996. Stemming algorithms A case study for detailed evaluation. Journal of theAmerican Society for Information Science, 47 1, 7084.Jain, A., Murty, N.,  Flynn, P. 1999. Data clustering A review. ACM ComputingSurveys, 31 3, 264323.Jarmasz, M.,  Szpakowicz, S. 2003. Rogets thesaurus and semantic similarity. InProceedings of the International Conference on Recent Advances in Natural LanguageProcessing RANLP03, pp. 212219, Borovets, Bulgaria.Jiang, J. J.,  Conrath, D. W. 1997. Semantic similarity based on corpus statisticsand lexical taxonomy. In Proceedings of the International Conference on Research inComputational Linguistics ROCLING X, pp. 1933, Tapei, Taiwan.Johnson, H.,  Martin, J. 2003. Unsupervised learning of morphology for English andInuktitut. In Proceedings of HLTNAACL 2003, pp. 4345.Jones, M. P.,  Martin, J. H. 1997. Contextual spelling correction using latent semantic analysis. In Proceedings of the Fifth Conference on Applied Natural LanguageProcessing, pp. 166173, Washington, DC.180From Frequency to MeaningJones, R., Rey, B., Madani, O.,  Greiner, W. 2006. Generating query substitutions. InProceedings of the 15th international conference on World Wide Web WWW 06,pp. 387396, New York, NY. ACM.Jones, W. P.,  Furnas, G. W. 1987. Pictures of relevance A geometric analysis ofsimilarity measures. Journal of the American Society for Information Science, 38 6,420442.Kanerva, P. 1993. Sparse distributed memory and related models. In Hassoun, M. H.Ed., Associative neural memories, pp. 5076. Oxford University Press, New York,NY.Karlgren, J.,  Sahlgren, M. 2001. From words to understanding. In Uesaka, Y., Kanerva,P.,  Asoh, H. Eds., Foundations of RealWorld Intelligence, pp. 294308. CSLIPublications.Kim, S.M., Pantel, P., Chklovski, T.,  Pennacchiotti, M. 2006. Automatically assessingreview helpfulness. In Proceedings of the 2006 Conference on Empirical Methods inNatural Language Processing, pp. 423430.Kolda, T.,  Bader, B. 2009. Tensor decompositions and applications. SIAM Review,51 3, 455500.Konchady, M. 2008. Building Search Applications Lucene, LingPipe, and Gate. MustruPublishing.Kraaij, W.,  Pohlmann, R. 1996. Viewing stemming as recall enhancement. In Proceedings of the 19th Annual International ACM SIGIR Conference, pp. 4048.Lakoff, G. 1987. Women, Fire, and Dangerous Things. University Of Chicago Press,Chicago, IL.Landauer, T. K. 2002. On the computational basis of learning and cognition Argumentsfrom LSA. In Ross, B. H. Ed., The Psychology of Learning and Motivation Advancesin Research and Theory, Vol. 41, pp. 4384. Academic Press.Landauer, T. K.,  Dumais, S. T. 1997. A solution to Platos problem The latent semantic analysis theory of the acquisition, induction, and representation of knowledge.Psychological Review, 104 2, 211240.Landauer, T. K.,  Littman, M. L. 1990. Fully automatic crosslanguage documentretrieval using latent semantic indexing. In Proceedings of the Sixth Annual Conferenceof the UW Centre for the New Oxford English Dictionary and Text Research, pp. 3138, Waterloo, Ontario.Landauer, T. K., McNamara, D. S., Dennis, S.,  Kintsch, W. 2007. Handbook of LatentSemantic Analysis. Lawrence Erlbaum, Mahwah, NJ.Lavrenko, V.,  Croft, W. B. 2001. Relevance based language models. In Proceedings ofthe 24th Annual International ACM SIGIR Conference on Research and Developmentin Information Retrieval SIGIR 01, pp. 120127, New York, NY. ACM.Leacock, C.,  Chodrow, M. 1998. Combining local context and WordNet similarity forword sense identification. In Fellbaum, C. Ed., WordNet An Electronic LexicalDatabase. MIT Press.181Turney  PantelLeacock, C., Towell, G.,  Voorhees, E. 1993. Corpusbased statistical sense resolution.In Proceedings of the ARPA Workshop on Human Language Technology, pp. 260265.Lee, D. D.,  Seung, H. S. 1999. Learning the parts of objects by nonnegative matrixfactorization. Nature, 401, 788791.Lee, L. 1999. Measures of distributional similarity. In Proceedings of the 37th AnnualMeeting of the Association for Computational Linguistics, pp. 2532.Lemaire, B.,  Denhiere, G. 2006. Effects of highorder cooccurrences on word semanticsimilarity. Current Psychology Letters Behaviour, Brain  Cognition, 18 1.Lin, D. 1998. Automatic retrieval and clustering of similar words. In roceedings of the17th international conference on Computational linguistics, pp. 768774. Associationfor Computational Linguistics.Lin, D.,  Pantel, P. 2001. DIRT  discovery of inference rules from text. In Proceedingsof ACM SIGKDD Conference on Knowledge Discovery and Data Mining 2001, pp.323328.Linden, G., Smith, B.,  York, J. 2003. Amazon.com recommendations Itemtoitemcollaborative filtering. IEEE Internet Computing, 7680.Liu, X.,  Croft, W. B. 2005. Statistical language modeling for information retrieval.Annual Review of Information Science and Technology, 39, 328.Lovins, J. B. 1968. Development of a stemming algorithm. Mechanical Translation andComputational Linguistics, 11, 2231.Lowe, W. 2001. Towards a theory of semantic space. In Proceedings of the TwentyfirstAnnual Conference of the Cognitive Science Society, pp. 576581.Lund, K.,  Burgess, C. 1996. Producing highdimensional semantic spaces from lexicalcooccurrence. Behavior Research Methods, Instruments, and Computers, 28 2, 203208.Lund, K., Burgess, C.,  Atchley, R. A. 1995. Semantic and associative priming in highdimensional semantic space. In Proceedings of the 17th Annual Conference of theCognitive Science Society, pp. 660665.Manning, C.,  Schutze, H. 1999. Foundations of Statistical Natural Language Processing.MIT Press, Cambridge, MA.Manning, C. D., Raghavan, P.,  Schutze, H. 2008. Introduction to Information Retrieval.Cambridge University Press, Cambridge, UK.Miller, G., Leacock, C., Tengi, R.,  Bunker, R. 1993. A semantic concordance. InProceedings of the 3rd DARPA Workshop on Human Language Technology, pp. 303308.Minnen, G., Carroll, J.,  Pearce, D. 2001. Applied morphological processing of English.Natural Language Engineering, 7 3, 207223.Mitchell, J.,  Lapata, M. 2008. Vectorbased models of semantic composition. In Proceedings of ACL08 HLT, pp. 236244, Columbus, Ohio. Association for ComputationalLinguistics.182From Frequency to MeaningMitchell, T. 1997. Machine Learning. McGrawHill, Columbus, OH.Mohammad, S.,  Hirst, G. 2006. Distributional measures of conceptdistance A taskoriented evaluation. In Proceedings of the Conference on Empirical Methods in NaturalLanguage Processing EMNLP2006, pp. 3543.Monay, F.,  GaticaPerez, D. 2003. On image autoannotation with latent space models.In Proceedings of the Eleventh ACM International Conference on Multimedia, pp.275278.Morris, J.,  Hirst, G. 2004. Nonclassical lexical semantic relations. In Workshop onComputational Lexical Semantics, HLTNAACL04, Boston, MA.Nakov, P.,  Hearst, M. 2007. UCB System description for SemEval Task 4. In Proceedings of the Fourth International Workshop on Semantic Evaluations SemEval 2007,pp. 366369, Prague, Czech Republic.Nakov, P.,  Hearst, M. 2008. Solving relational similarity problems using theweb as acorpus. In Proceedings of ACL08 HLT, pp. 452460, Columbus, Ohio.Nastase, V., SayyadShirabad, J., Sokolova, M.,  Szpakowicz, S. 2006. Learning nounmodifier semantic relations with corpusbased and WordNetbased features. In Proceedings of the 21st National Conference on Artificial Intelligence AAAI06, pp.781786.Nastase, V.,  Szpakowicz, S. 2003. Exploring nounmodifier semantic relations. InFifth International Workshop on Computational Semantics IWCS5, pp. 285301,Tilburg, The Netherlands.Niwa, Y.,  Nitta, Y. 1994. Cooccurrence vectors from corpora vs. distance vectors fromdictionaries. In Proceedings of the 15th International Conference On ComputationalLinguistics, pp. 304309, Kyoto, Japan.Nosofsky, R. 1986. Attention, similarity, and the identificationcategorization relationship.Journal of Experimental Psychology General, 115 1, 3957.Ogden, C. K. 1930. Basic English A General Introduction with Rules and Grammar.Kegan Paul, Trench, Trubner and Co.Pado, S.,  Lapata, M. 2003. Constructing semantic space models from parsed corpora.In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pp. 128135, Sapporo, Japan.Pado, S.,  Lapata, M. 2007. Dependencybased construction of semantic space models.Computational Linguistics, 33 2, 161199.Pang, B., Lee, L.,  Vaithyanathan, S. 2002. Thumbs up sentiment classification usingmachine learning techniques. In Proceedings of the Conference on Empirical Methodsin Natural Language Processing EMNLP, pp. 7986, Philadelphia, PA.Pantel, P. 2005. Inducing ontological cooccurrence vectors. In Proceedings of Associationfor Computational Linguistics ACL05, pp. 125132.Pantel, P.,  Lin, D. 1998. Spamcop A spam classification and organization program. InLearning for Text Categorization Papers from the AAAI 1998 Workshop, pp. 9598.183Turney  PantelPantel, P.,  Lin, D. 2002a. Discovering word senses from text. In Proceedings of theEighth ACM SIGKDD International Conference on Knowledge Discovery and DataMining, pp. 613619, Edmonton, Canada.Pantel, P.,  Lin, D. 2002b. Document clustering with committees. In Proceedings of the25th Annual International ACM SIGIR Conference, pp. 199206.Pasca, M., Lin, D., Bigham, J., Lifchits, A.,  Jain, A. 2006. Names and similarities onthe Web Fact extraction in the fast lane. In Proceedings of the 21st InternationalConference on Computational Linguistics and 44th Annual Meeting of the ACL, pp.809816, Sydney, Australia.Patwardhan, S.,  Pedersen, T. 2006. Using wordnetbased context vectors to estimatethe semantic relatedness of concepts. In Proceedings of the Workshop on MakingSense of Sense at the 11th Conference of the European Chapter of the Association forComputational Linguistics EACL2006, pp. 18.Pedersen, T. 2006. Unsupervised corpusbased methods for WSD. In Word Sense Disambiguation Algorithms and Applications, pp. 133166. Springer.Pennacchiotti, M., Cao, D. D., Basili, R., Croce, D.,  Roth, M. 2008. Automatic inductionof FrameNet lexical units. In Proceedings of the 2008 Conference on Empirical Methodsin Natural Language Processing EMNLP08, pp. 457465, Honolulu, Hawaii.Pennacchiotti, M.,  Pantel, P. 2006. Ontologizing semantic relations. In Proceedings ofthe 21st International Conference on Computational Linguistics and the 44th annualmeeting of the Association for Computational Linguistics, pp. 793800. Associationfor Computational Linguistics.Pereira, F., Tishby, N.,  Lee, L. 1993. Distributional clustering of English words. InProceedings of the 31st Annual Meeting on Association for Computational Linguistics,pp. 183190.Porter, M. 1980. An algorithm for suffix stripping. Program, 14 3, 130137.Rabin, M. O. 1981. Fingerprinting by random polynomials. Tech. rep., Center for researchin Computing technology, Harvard University. Technical Report TR1581.Rapp, R. 2003. Word sense discovery based on sense descriptor dissimilarity. In Proceedings of the Ninth Machine Translation Summit, pp. 315322.Ravichandran, D., Pantel, P.,  Hovy, E. 2005. Randomized algorithms and nlp usinglocality sensitive hash function for high speed noun clustering. In Proceedings of the43rd Annual Meeting of the Association for Computational Linguistics ACL 05, pp.622629, Morristown, NJ. Association for Computational Linguistics.Resnick, P., Iacovou, N., Suchak, M., Bergstrom, P.,  Riedl, J. 1994. Grouplens An openarchitecture for collaborative filtering of netnews. In Proceedings of the ACM 1994Conference on Computer Supported Cooperative Work, pp. 175186. ACM Press.Resnik, P. 1995. Using information content to evaluate semantic similarity in a taxonomy.In Proceedings of the 14th International Joint Conference on Artificial IntelligenceIJCAI95, pp. 448453, San Mateo, CA. Morgan Kaufmann.184From Frequency to MeaningRosario, B.,  Hearst, M. 2001. Classifying the semantic relations in nouncompoundsvia a domainspecific lexical hierarchy. In Proceedings of the 2001 Conference onEmpirical Methods in Natural Language Processing EMNLP01, pp. 8290.Rosario, B., Hearst, M.,  Fillmore, C. 2002. The descent of hierarchy, and selection inrelational semantics. In Proceedings of the 40th Annual Meeting of the Associationfor Computational Linguistics ACL02, pp. 247254.Rosch, E.,  Lloyd, B. 1978. Cognition and Categorization. Lawrence Erlbaum, Hillsdale,NJ.Ruge, G. 1997. Automatic detection of thesaurus relations for information retrieval applications. In Freksa, C., Jantzen, M.,  Valk, R. Eds., Foundations of ComputerScience, pp. 499506. Springer.Sahami, M., Dumais, S., Heckerman, D.,  Horvitz, E. 1998. A Bayesian approach tofiltering junk email. In Proceedings of the AAAI98 Workshop on Learning for TextCategorization.Sahlgren, M. 2005. An introduction to random indexing. In Proceedings of the Methodsand Applications of Semantic Indexing Workshop at the 7th International Conferenceon Terminology and Knowledge Engineering TKE, Copenhagen, Denmark.Sahlgren, M. 2006. The WordSpace Model Using distributional analysis to represent syntagmatic and paradigmatic relations between words in highdimensional vector spaces.Ph.D. thesis, Department of Linguistics, Stockholm University.Salton, G. 1971. The SMART retrieval system Experiments in automatic document processing. PrenticeHall, Upper Saddle River, NJ.Salton, G.,  Buckley, C. 1988. Termweighting approaches in automatic text retrieval.Information Processing and Management, 24 5, 513523.Salton, G., Wong, A.,  Yang, C.S. 1975. A vector space model for automatic indexing.Communications of the ACM, 18 11, 613620.Sarawagi, S.,  Kirpal, A. 2004. Efficient set joins on similarity predicates. In Proceedings of the 2004 ACM SIGMOD International Conference on Management of DataSIGMOD 04, pp. 743754, New York, NY. ACM.Scholkopf, B., Smola, A. J.,  Muller, K.R. 1997. Kernel principal component analysis. InProceedings of the International Conference on Artificial Neural Networks ICANN1997, pp. 583588, Berlin.Schutze, H. 1998. Automatic word sense discrimination. Computational Linguistics, 24 1,97124.Schutze, H.,  Pedersen, J. 1993. A vector model for syntagmatic and paradigmaticrelatedness. In Making Sense of Words Proceedings of the Conference, pp. 104113,Oxford, England.Sebastiani, F. 2002. Machine learning in automated text categorization. ACM ComputingSurveys CSUR, 34 1, 147.Shannon, C. 1948. A mathematical theory of communication. Bell System TechnicalJournal, 27, 379423, 623656.185Turney  PantelSinghal, A., Salton, G., Mitra, M.,  Buckley, C. 1996. Document length normalization.Information Processing and Management, 32 5, 619633.Smith, E., Osherson, D., Rips, L.,  Keane, M. 1988. Combining prototypes A selectivemodification model. Cognitive Science, 12 4, 485527.Snow, R., Jurafsky, D.,  Ng, A. Y. 2006. Semantic taxonomy induction from heterogenous evidence. In Proceedings of the 21st International Conference on ComputationalLinguistics and the 44th annual meeting of the ACL, pp. 801808.Sparck Jones, K. 1972. A statistical interpretation of term specificity and its applicationin retrieval. Journal of Documentation, 28 1, 1121.Spearman, C. 1904. General intelligence, objectively determined and measured. American Journal of Psychology, 15, 201293.Sproat, R.,  Emerson, T. 2003. The first international Chinese word segmentation bakeoff. In Proceedings of the Second SIGHAN Workshop on Chinese Language Processing,pp. 133143, Sapporo, Japan.Stone, P. J., Dunphy, D. C., Smith, M. S.,  Ogilvie, D. M. 1966. The General InquirerA Computer Approach to Content Analysis. MIT Press, Cambridge, MA.Tan, B.,  Peng, F. 2008. Unsupervised query segmentation using generative languagemodels and Wikipedia. In Proceeding of the 17th international conference on WorldWide Web WWW 08, pp. 347356, New York, NY. ACM.Tellex, S., Katz, B., Lin, J., Fern, A.,  Marton, G. 2003. Quantitative evaluation ofpassage retrieval algorithms for question answering. In Proceedings of the 26th AnnualInternational ACM SIGIR Conference on Research and Development in InformationRetrieval SIGIR, pp. 4147.Tucker, L. R. 1966. Some mathematical notes on threemode factor analysis. Psychometrika, 31 3, 279311.Turney, P. D. 2001. Mining the Web for synonyms PMIIR versus LSA on TOEFL. InProceedings of the Twelfth European Conference on Machine Learning ECML01,pp. 491502, Freiburg, Germany.Turney, P. D. 2005. Measuring semantic similarity by latent relational analysis. In Proceedings of the Nineteenth International Joint Conference on Artificial IntelligenceIJCAI05, pp. 11361141, Edinburgh, Scotland.Turney, P. D. 2006. Similarity of semantic relations. Computational Linguistics, 32 3,379416.Turney, P. D. 2007. Empirical evaluation of four tensor decomposition algorithms. Tech.rep., Institute for Information Technology, National Research Council of Canada.Technical Report ERB1152.Turney, P. D. 2008a. The latent relation mapping engine Algorithm and experiments.Journal of Artificial Intelligence Research, 33, 615655.Turney, P. D. 2008b. A uniform approach to analogies, synonyms, antonyms, and associations. In Proceedings of the 22nd International Conference on ComputationalLinguistics Coling 2008, pp. 905912, Manchester, UK.186From Frequency to MeaningTurney, P. D.,  Littman, M. L. 2003. Measuring praise and criticism Inference ofsemantic orientation from association. ACM Transactions on Information Systems,21 4, 315346.Turney, P. D.,  Littman, M. L. 2005. Corpusbased learning of analogies and semanticrelations. Machine Learning, 60 13, 251278.Turney, P. D., Littman, M. L., Bigham, J.,  Shnayder, V. 2003. Combining independentmodules to solve multiplechoice synonym and analogy problems. In Proceedings ofthe International Conference on Recent Advances in Natural Language ProcessingRANLP03, pp. 482489, Borovets, Bulgaria.Van de Cruys, T. 2009. A nonnegative tensor factorization model for selectional preferenceinduction. In Proceedings of the Workshop on Geometric Models for Natural LanguageSemantics GEMS09, pp. 8390, Athens, Greece.van Rijsbergen, C. J. 2004. The Geometry of Information Retrieval. Cambridge UniversityPress, Cambridge, UK.van Rijsbergen, C. J. 1979. Information Retrieval. Butterworths.Veale, T. 2003. The analogical thesaurus. In Proceedings of the 15th Innovative Applications of Artificial Intelligence Conference IAAI 2003, pp. 137142, Acapulco,Mexico.Veale, T. 2004. WordNet sits the SAT A knowledgebased approach to lexical analogy. InProceedings of the 16th European Conference on Artificial Intelligence ECAI 2004,pp. 606612, Valencia, Spain.Vozalis, E.,  Margaritis, K. 2003. Analysis of recommender systems algorithms. InProceedings of the 6th Hellenic European Conference on Computer Mathematics andits Applications HERCMA2003, Athens, Greece.Vyas, V.,  Pantel, P. 2009. Semiautomatic entity set refinement. In Proceedings ofNAACL09, Boulder, CO.Weaver, W. 1955. Translation. In Locke, W.,  Booth, D. Eds., Machine Translationof Languages Fourteen Essays. MIT Press, Cambridge, MA.Weeds, J., Weir, D.,  McCarthy, D. 2004. Characterising measures of lexical distributional similarity. In Proceedings of the 20th International Conference on Computational Linguistics COLING 04, pp. 10151021. Association for ComputationalLinguistics.Wei, X., Peng, F.,  Dumoulin, B. 2008. Analyzing web text association to disambiguateabbreviation in queries. In Proceedings of the 31st Annual International ACM SIGIRConference on Research and Development in Information Retrieval SIGIR 08, pp.751752, New York, NY. ACM.Wen, J.R., Nie, J.Y.,  Zhang, H.J. 2001. Clustering user queries of a search engine. InProceedings of the 10th International Conference on World Wide Web WWW 01,pp. 162168, New York, NY. ACM.Widdows, D. 2004. Geometry and Meaning. Center for the Study of Language andInformation, Stanford, CA.187Turney  PantelWiddows, D.,  Ferraro, K. 2008. Semantic vectors A scalable open source package andonline technology management application. In Proceedings of the Sixth InternationalConference on Language Resources and Evaluation LREC 2008, pp. 11831190.Witten, I. H.,  Frank, E. 2005. Data Mining Practical Machine Learning Tools andTechniques with Java Implementations. Morgan Kaufmann, San Francisco.Wittgenstein, L. 1953. Philosophical Investigations. Blackwell. Translated by G.E.M.Anscombe.Wolfe, M. B. W., Schreiner, M. E., Rehder, B., Laham, D., Foltz, P. W., Kintsch, W., Landauer, T. K. 1998. Learning from text Matching readers and texts by latentsemantic analysis. Discourse Processes, 25, 309336.Yang, Y. 1999. An evaluation of statistical approaches to text categorization. InformationRetrieval, 1 1, 6990.Yuret, D.,  Yatbaz, M. A. 2009. The noisy channel model for unsupervised word sensedisambiguation. Computational Linguistics. Under review.Zamir, O.,  Etzioni, O. 1999. Grouper a dynamic clustering interface to Web searchresults. Computer Networks The International Journal of Computer and Telecommunications Networking, 31 11, 13611374.Zhao, Y.,  Karypis, G. 2002. Evaluation of hierarchical clustering algorithms for document datasets. In Proceedings of the Eleventh International Conference on Information and Knowledge Management, pp. 515524, McLean, Virginia.188
