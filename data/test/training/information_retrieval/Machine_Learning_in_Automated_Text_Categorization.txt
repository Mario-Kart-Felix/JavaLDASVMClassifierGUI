arXivcs0110053v1  cs.IR  26 Oct 2001Machine Learning in Automated Text CategorizationFabrizio SebastianiConsiglio Nazionale delle Ricerche, ItalyThe automated categorization or classification of texts into predefined categories has witnessed abooming interest in the last ten years, due to the increased availability of documents in digital formand the ensuing need to organize them. In the research community the dominant approach to thisproblem is based on machine learning techniques a general inductive process automatically buildsa classifier by learning, from a set of preclassified documents, the characteristics of the categories.The advantages of this approach over the knowledge engineering approach consisting in themanual definition of a classifier by domain experts are a very good effectiveness, considerablesavings in terms of expert manpower, and straightforward portability to different domains. Thissurvey discusses the main approaches to text categorization that fall within the machine learningparadigm. We will discuss in detail issues pertaining to three different problems, namely documentrepresentation, classifier construction, and classifier evaluation.Categories and Subject Descriptors H.3.1 Information storage and retrieval Content analysis and indexingIndexing methods H.3.3 Information storage and retrieval Information search and retrievalInformation filtering H.3.3 Information storage and retrievalSystems and softwarePerformance evaluation efficiency and effectiveness I.2.3 ArtificialIntelligence LearningInductionGeneral Terms Algorithms, Experimentation, TheoryAdditional Key Words and Phrases Machine learning, text categorization, text classification1. INTRODUCTIONIn the last ten years contentbased document management tasks collectively knownas information retrieval  IR have gained a prominent status in the informationsystems field, due to the increased availability of documents in digital form andthe ensuing need to access them in flexible ways. Text categorization TC  akatext classification, or topic spotting, the activity of labelling natural language textswith thematic categories from a predefined set, is one such task. TC dates backto the early 60s, but only in the early 90s it became a major subfield of theinformation systems discipline, thanks to increased applicative interest and to theavailability of more powerful hardware. TC is now being applied in many contexts,ranging from document indexing based on a controlled vocabulary, to documentfiltering, automated metadata generation, word sense disambiguation, populationof hierarchical catalogues of Web resources, and in general any application requiringdocument organization or selective and adaptive document dispatching.Until the late 80s the most popular approach to TC, at least in the operational i.e. realworld applications community, was a knowledge engineering KEone, consisting in manually defining a set of rules encoding expert knowledge onAddress Istituto di Elaborazione dellInformazione, Consiglio Nazionale delle Ricerche, Via G.Moruzzi, 1, 56124 Pisa Italy. Email fabrizioiei.pi.cnr.it2  F. Sebastianihow to classify documents under the given categories. In the 90s this approach hasincreasingly lost popularity especially in the research community in favour of themachine learning ML paradigm, according to which a general inductive processautomatically builds an automatic text classifier by learning, from a set of preclassified documents, the characteristics of the categories of interest. The advantagesof this approach are an accuracy comparable to that achieved by human experts,and a considerable savings in terms of expert manpower, since no intervention fromeither knowledge engineers or domain experts is needed for the construction of theclassifier or for its porting to a different set of categories. It is the ML approach toTC that this paper concentrates on.Currentday TC is thus a discipline at the crossroads of ML and IR, and as such itshares a number of characteristics with other tasks such as informationknowledgeextraction from texts and text mining Knight 1999 Pazienza 1997. There is stillconsiderable debate on where the exact border between these disciplines lies, and theterminology is still evolving. Text mining is increasingly being used to denote allthe tasks that, by analyzing large quantities of text and detecting usage patterns, tryto extract probably useful although only probably correct information. Accordingto this view, TC is an instance of text mining. TC enjoys quite a rich literature now,but this is still fairly scattered1. Although two international journals have devotedspecial issues to this topic Joachims and Sebastiani 2001 Lewis and Hayes 1994,there are no systematic treatments of the subject there are neither textbooks norjournals entirely devoted to TC yet, and Manning and Schutze 1999, Chapter16is the only chapterlength treatment of the subject. As a note, we should warn thereader that the term automatic text classification has sometimes been used inthe literature to mean things quite different from the ones discussed here. Asidefrom i the automatic assignment of documents to a predefined set of categories,which is the main topic of this paper, the term has also been used to mean ii theautomatic identification of such a set of categories e.g. Borko and Bernick 1963,or iii the automatic identification of such a set of categories and the grouping ofdocuments under them e.g. Merkl 1998, a task usually called text clustering, oriv any activity of placing text items into groups, a task that has thus both TCand text clustering as particular instances Manning and Schutze 1999.This paper is organized as follows. In Section 2 we formally define TC and itsvarious subcases, and in Section 3 we review its most important applications. Section 4 describes the main ideas underlying the ML approach to classification. Ourdiscussion of text classification starts in Section 5 by introducing text indexing, i.e.the transformation of textual documents into a form that can be interpreted by aclassifierbuilding algorithm and by the classifier eventually built by it. Section 6tackles the inductive construction of a text classifier from a training set of preclassified documents. Section 7 discusses the evaluation of text classifiers. Section8 concludes, discussing open issues and possible avenues of further research for TC.2. TEXT CATEGORIZATION1A fully searchable bibliography on TC created and maintained by this author is available athttpliinwww.ira.uka.debibliographyAiautomated.text.categorization.htmlMachine Learning in Automated Text Categorization  32.1 A definition of text categorizationText categorization is the task of assigning a Boolean value to each pair dj , ci DC, where D is a domain of documents and C  c1, . . . , cC is a set of predefinedcategories. A value of T assigned to dj , ci indicates a decision to file dj under ci,while a value of F indicates a decision not to file dj under ci. More formally, the taskis to approximate the unknown target function   DC  T, F that describeshow documents ought to be classified by means of a function   D  C  T, Fcalled the classifier aka rule, or hypothesis, or model such that  and  coincideas much as possible. How to precisely define and measure this coincidence calledeffectiveness will be discussed in Section 7.1. From now on we will assume thatThe categories are just symbolic labels, and no additional knowledge of a procedural or declarative nature of their meaning is available.No exogenous knowledge i.e. data provided for classification purposes by an external source is available therefore, classification must be accomplished on thebasis of endogenous knowledge only i.e. knowledge extracted from the documents. In particular, this means that metadata such as e.g. publication date,document type, publication source, etc. is not assumed to be available.The TC methods we will discuss are thus completely general, and do not depend onthe availability of specialpurpose resources that might be unavailable or costly todevelop. Of course, these assumptions need not be verified in operational settings,where it is legitimate to use any source of information that might be available ordeemed worth developing Daz Esteban et al. 1998 Junker and Abecker 1997.Relying only on endogenous knowledge means classifying a document based solelyon its semantics, and given that the semantics of a document is a subjective notion,it follows that the membership of a document in a category pretty much as therelevance of a document to an information need in IR Saracevic 1975 cannot bedecided deterministically. This is exemplified by the phenomenon of interindexerinconsistency Cleverdon 1984 when two human experts decide whether to classifydocument dj under category ci, they may disagree, and this in fact happens withrelatively high frequency. A news article on Clinton attending Dizzy Gillespiesfuneral could be filed under Politics, or under Jazz, or under both, or even underneither, depending on the subjective judgment of the expert.2.2 Singlelabel vs. multilabel text categorizationDifferent constraints may be enforced on the TC task, depending on the application.For instance we might need that, for a given integer k, exactly k or  k, or  kelements of C be assigned to each dj  D. The case in which exactly 1 categorymust be assigned to each dj  D is often called the singlelabel aka nonoverlappingcategories case, while the case in which any number of categories from 0 to Cmay be assigned to the same dj  D is dubbed the multilabel aka overlappingcategories case. A special case of singlelabel TC is binary TC, in which eachdj  D must be assigned either to category ci or to its complement ci.From a theoretical point of view, the binary case hence, the singlelabel case toois more general than the multilabel, since an algorithm for binary classification canalso be used for multilabel classification one needs only transform the problem4  F. Sebastianiof multilabel classification under c1, . . . , cC into C independent problems ofbinary classification under ci, ci, for i  1, . . . , C. However, this requires thatcategories are stochastically independent of each other, i.e. that for any c, c thevalue of dj , c does not depend on the value of dj , c and viceversa thisis usually assumed to be the case applications in which this is not the case arediscussed in Section 3.5. The converse is not true an algorithm for multilabelclassification cannot be used for either binary or singlelabel classification. In fact,given a document dj to classify, i the classifier might attribute k  1 categories todj , and it might not be obvious how to choose a most appropriate category fromthem or ii the classifier might attribute to dj no category at all, and it might notbe obvious how to choose a least inappropriate category from C.In the rest of the paper, unless explicitly mentioned, we will deal with the binarycase. There are various reasons for thisThe binary case is important in itself because important TC applications, including filtering see Section 3.3, consist of binary classification problems e.g.deciding whether dj is about Jazz or not. In TC, most binary classification problems feature unevenly populated categories e.g. much fewer documents are aboutJazz than are not and unevenly characterized categories e.g. what is about Jazzcan be characterized much better than what is not.Solving the binary case also means solving the multilabel case, which is alsorepresentative of important TC applications, including automated indexing forBoolean systems see Section 3.1.Most of the TC literature is couched in terms of the binary case.Most techniques for binary classification are just special cases of existing techniques for the singlelabel case, and are simpler to illustrate than these latter.This ultimately means that we will view classification under C  c1, . . . , cC asconsisting of C independent problems of classifying the documents in D under agiven category ci, for i  1, . . . , C. A classifier for ci is then a function i  D T, F that approximates an unknown target function i  D  T, F.2.3 Categorypivoted vs. documentpivoted text categorizationThere are two different ways of using a text classifier. Given dj  D, we might wantto find all the ci  C under which it should be filed documentpivoted categorization DPC alternatively, given ci  C, we might want to find all the dj  D that shouldbe filed under it categorypivoted categorization  CPC. This distinction is morepragmatic than conceptual, but is important since the sets C and D might not beavailable in their entirety right from the start. It is also relevant to the choice ofthe classifierbuilding method, as some of these methods see e.g. Section 6.9 allowthe construction of classifiers with a definite slant towards one or the other style.DPC is thus suitable when documents become available at different moments intime, e.g. in filtering email. CPC is instead suitable when i a new category cC1may be added to an existing set C  c1, . . . , cC after a number of documents havealready been classified under C, and ii these documents need to be reconsideredfor classification under cC1 e.g. Larkey 1999. DPC is used more often thanCPC, as the former situation is more common than the latter.Machine Learning in Automated Text Categorization  5Although some specific techniques apply to one style and not to the other e.g. theproportional thresholding method discussed in Section 6.1 applies only to CPC,this is more the exception than the rule most of the techniques we will discussallow the construction of classifiers capable of working in either mode.2.4 Hard categorization vs. ranking categorizationWhile a complete automation of the TC task requires a T or F decision for eachpair dj , ci, a partial automation of this process might have different requirements.For instance, given dj  D a system might simply rank the categories in C c1, . . . , cC according to their estimated appropriateness to dj , without takingany hard decision on any of them. Such a ranked list would be of great help to ahuman expert in charge of taking the final categorization decision, since she couldthus restrict the choice to the category or categories at the top of the list, ratherthan having to examine the entire set. Alternatively, given ci  C a system mightsimply rank the documents in D according to their estimated appropriateness to cisymmetrically, for classification under ci a human expert would just examine thetopranked documents instead than the entire document set. These two modalitiesare sometimes called categoryranking TC and documentranking TC Yang 1999,respectively, and are the obvious counterparts of DPC and CPC.Semiautomated, interactive classification systems Larkey and Croft 1996 areuseful especially in critical applications in which the effectiveness of a fully automated system may be expected to be significantly lower than that of a humanexpert. This may be the case when the quality of the training data see Section4 is low, or when the training documents cannot be trusted to be a representativesample of the unseen documents that are to come, so that the results of a completelyautomatic classifier could not be trusted completely.In the rest of the paper, unless explicitly mentioned, we will deal with hardclassification however, many of the algorithms we will discuss naturally lend themselves to ranking TC too more details on this in Section 6.1.3. APPLICATIONS OF TEXT CATEGORIZATIONTC goes back to Marons 1961 seminal work on probabilistic text classification.Since then, it has been used for a number of different applications, of which we herebriefly review the most important ones. Note that the borders between the differentclasses of applications listed here are fuzzy and somehow artificial, and some of thesemay be considered special cases of others. Other applications we do not explicitlydiscuss are speech categorization by means of a combination of speech recognitionand TC Myers et al. 2000 Schapire and Singer 2000, multimedia document categorization through the analysis of textual captions Sable and Hatzivassiloglou 2000,author identification for literary texts of unknown or disputed authorship Forsyth1999, language identification for texts of unknown language Cavnar and Trenkle1994, automated identification of text genre Kessler et al. 1997, and automatedessay grading Larkey 1998.3.1 Automatic indexing for Boolean information retrieval systemsThe application that has spawned most of the early research in the field Borko andBernick 1963 Field 1975 Gray and Harley 1971 Heaps 1973 Maron 1961, is that6  F. Sebastianiof automatic document indexing for IR systems relying on a controlled dictionary,the most prominent example of which is that of Boolean systems. In these lattereach document is assigned one or more keywords or keyphrases describing its content, where these keywords and keyphrases belong to a finite set called controlleddictionary, often consisting of a thematic hierarchical thesaurus e.g. the NASA thesaurus for the aerospace discipline, or the MESH thesaurus for medicine. Usually,this assignment is done by trained human indexers, and is thus a costly activity.If the entries in the controlled vocabulary are viewed as categories, text indexingis an instance of TC, and may thus be addressed by the automatic techniques described in this paper. Recalling Section 2.2, note that this application may typicallyrequire that k1  x  k2 keywords are assigned to each document, for given k1, k2.Documentpivoted TC is probably the best option, so that new documents may beclassified as they become available. Various text classifiers explicitly conceived fordocument indexing have been described in the literature see e.g. Fuhr and Knorz1984 Robertson and Harding 1984 Tzeras and Hartmann 1993.Automatic indexing with controlled dictionaries is closely related to automatedmetadata generation. In digital libraries one is usually interested in tagging documents by metadata that describe them under a variety of aspects e.g. creationdate, document type or format, availability, etc.. Some of these metadata arethematic, i.e. their role is to describe the semantics of the document by means ofbibliographic codes, keywords or keyphrases. The generation of these metadatamay thus be viewed as a problem of document indexing with controlled dictionary,and thus tackled by means of TC techniques.3.2 Document organizationIndexing with a controlled vocabulary is an instance of the general problem of document base organization. In general, many other issues pertaining to documentorganization and filing, be it for purposes of personal organization or structuring ofa corporate document base, may be addressed by TC techniques. For instance, atthe offices of a newspaper incoming classified ads must be, prior to publication,categorized under categories such as Personals, Cars for Sale, Real Estate,etc. Newspapers dealing with a high volume of classified ads would benefit froman automatic system that chooses the most suitable category for a given ad. Otherpossible applications are the organization of patents into categories for making theirsearch easier Larkey 1999, the automatic filing of newspaper articles under the appropriate sections e.g. Politics, Home News, Lifestyles, etc., or the automaticgrouping of conference papers into sessions.3.3 Text filteringText filtering is the activity of classifying a stream of incoming documents dispatched in an asynchronous way by an information producer to an informationconsumer Belkin and Croft 1992. A typical case is a newsfeed, where the producer is a news agency and the consumer is a newspaper Hayes et al. 1990. Inthis case the filtering system should block the delivery of the documents the consumer is likely not interested in e.g. all news not concerning sports, in the caseof a sports newspaper. Filtering can be seen as a case of singlelabel TC, i.e. theclassification of incoming documents in two disjoint categories, the relevant and theMachine Learning in Automated Text Categorization  7irrelevant. Additionally, a filtering system may also further classify the documentsdeemed relevant to the consumer into thematic categories in the example above,all articles about sports should be further classified according e.g. to which sportthey deal with, so as to allow journalists specialized in individual sports to accessonly documents of prospective interest for them. Similarly, an email filter mightbe trained to discard junk mail Androutsopoulos et al. 2000 Drucker et al. 1999and further classify nonjunk mail into topical categories of interest to the user.A filtering system may be installed at the producer end, in which case it mustroute the documents to the interested consumers only, or at the consumer end, inwhich case it must block the delivery of documents deemed uninteresting to theconsumer. In the former case the system builds and updates a profile for eachconsumer Liddy et al. 1994, while in the latter case which is the more common,and to which we will refer in the rest of this section a single profile is needed.A profile may be initially specified by the user, thereby resembling a standingIR query, and is updated by the system by using feedback information providedeither implicitly or explicitly by the user on the relevance or nonrelevance of thedelivered messages. In the TREC community Lewis 1995c this is called adaptivefiltering, while the case in which no userspecified profile is available is called eitherrouting or batch filtering, depending on whether documents have to be ranked indecreasing order of estimated relevance or just acceptedrejected. Batch filteringthus coincides with singlelabel TC under C  2 categories since this latter isa completely general TC task some authors Hull 1994 Hull et al. 1996 Schapireet al. 1998 Schutze et al. 1995, somewhat confusingly, use the term filtering inplace of the more appropriate term categorization.In information science document filtering has a tradition dating back to the60s, when, addressed by systems of various degrees of automation and dealingwith the multiconsumer case discussed above, it was called selective disseminationof information or current awareness see e.g. Korfhage 1997, Chapter 6. Theexplosion in the availability of digital information has boosted the importance ofsuch systems, which are nowadays being used in contexts such as the creation ofpersonalized Web newspapers, junk email blocking, and Usenet news selection.Information filtering by ML techniques is widely discussed in the literature seee.g. Amati and Crestani 1999 Iyer et al. 2000 Kim et al. 2000 Tauritz et al. 2000Yu and Lam 1998.3.4 Word sense disambiguationWord sense disambiguation WSD is the activity of finding, given the occurrencein a text of an ambiguous i.e. polysemous or homonymous word, the sense of thisparticular word occurrence. For instance, bank may have at least two differentsenses in English, as in the Bank of England a financial institution or the bankof river Thames a hydraulic engineering artifact. It is thus a WSD task todecide which of the above senses the occurrence of bank in Last week I borrowedsome money from the bank has. WSD is very important for many applications,including natural language processing, and indexing documents by word sensesrather than by words for IR purposes. WSD may be seen as a TC task see e.gGale et al. 1993 Escudero et al. 2000 once we view word occurrence contexts asdocuments and word senses as categories. Quite obviously, this is a singlelabel TC8  F. Sebastianicase, and one in which documentpivoted TC is usually the right choice.WSD is just an example of the more general issue of resolving natural language ambiguities, one of the most important problems in computational linguistics. Other examples, which may all be tackled by means of TC techniques alongthe lines discussed for WSD, are contextsensitive spelling correction, prepositionalphrase attachment, part of speech tagging, and word choice selection in machinetranslation see Roth 1998 for an introduction.3.5 Hierarchical categorization of Web pagesTC has recently aroused a lot of interest also for its possible application to automatically classifying Web pages, or sites, under the hierarchical catalogues hostedby popular Internet portals. When Web documents are catalogued in this way,rather than issuing a query to a generalpurpose Web search engine a searcher mayfind it easier to first navigate in the hierarchy of categories and then restrict hersearch to a particular category of interest.Classifying Web pages automatically has obvious advantages, since the manualcategorization of a large enough subset of the Web is infeasible. Unlike in theprevious applications, it is typically the case that each category must be populatedby a set of k1  x  k2 documents. CPC should be chosen so as to allow newcategories to be added and obsolete ones to be deleted.With respect to previously discussed TC applications, automatic Web page categorization has two essential peculiarities1 The hypertextual nature of the documents  links are a rich source of information,as they may be understood as stating the relevance of the linked page to thelinking page. Techniques exploiting this intuition in a TC context have beenpresented in Attardi et al. 1998 Chakrabarti et al. 1998b Furnkranz 1999Govert et al. 1999 Oh et al. 2000 and experimentally compared in Yang et al.2001.2 The hierarchical structure of the category set  this may be used e.g. by decomposing the classification problem into a number of smaller classificationproblems, each corresponding to a branching decision at an internal node.Techniques exploiting this intuition in a TC context have been presented inDumais and Chen 2000 Chakrabarti et al. 1998a Koller and Sahami 1997McCallum et al. 1998 Ruiz and Srinivasan 1999 Weigend et al. 1999.4. THE MACHINE LEARNING APPROACH TO TEXT CATEGORIZATIONIn the 80s the most popular approach at least in operational settings for thecreation of automatic document classifiers consisted in manually building, by meansof knowledge engineering KE techniques, an expert system capable of taking TCdecisions. Such an expert system would typically consist of a set of manually definedlogical rules, one per category, of typeif DNF formula then categoryA DNF disjunctive normal form formula is a disjunction of conjunctive clausesthe document is classified under category iff it satisfies the formula, i.e. iff itsatisfies at least one of the clauses. The most famous example of this approach isMachine Learning in Automated Text Categorization  9if wheat  farm orwheat  commodity orbushels  export orwheat  tonnes orwheat  winter   soft then Wheat else  WheatFig. 1. Rulebased classifier for the Wheat category keywords are indicated in italic, categoriesare indicated in Small Caps from Apte et al. 1994.the Construe system Hayes et al. 1990, built by Carnegie Group for the Reutersnews agency. A sample rule of the type used in Construe is illustrated in Figure1.The drawback of this approach is the knowledge acquisition bottleneck wellknownfrom the expert systems literature. That is, the rules must be manually defined bya knowledge engineer with the aid of a domain expert in this case, an expert in themembership of documents in the chosen set of categories if the set of categoriesis updated, then these two professionals must intervene again, and if the classifieris ported to a completely different domain i.e. set of categories a different domainexpert needs to intervene and the work has to be repeated from scratch.On the other hand, it was originally suggested that this approach can give verygood effectiveness results Hayes et al. 1990 reported a .90 breakeven resultsee Section 7 on a subset of the Reuters test collection, a figure that outperformseven the best classifiers built in the late 90s by stateoftheart ML techniques.However, no other classifier has been tested on the same dataset as Construe,and it is not clear whether this was a randomly chosen or a favourable subset ofthe entire Reuters collection. As argued in Yang 1999, the results above do notallow us to state that these effectiveness results may be obtained in general.Since the early 90s, the ML approach to TC has gained popularity and has eventually become the dominant one, at least in the research community see Mitchell1996 for a comprehensive introduction to ML. In this approach a general inductive process also called the learner automatically builds a classifier for a categoryci by observing the characteristics of a set of documents manually classified under ci or ci by a domain expert from these characteristics, the inductive processgleans the characteristics that a new unseen document should have in order to beclassified under ci. In ML terminology, the classification problem is an activity ofsupervised learning, since the learning process is supervised by the knowledge ofthe categories and of the training instances that belong to them2.The advantages of the ML approach over the KE approach are evident. Theengineering effort goes towards the construction not of a classifier, but of an automatic builder of classifiers the learner. This means that if a learner is as it oftenis available offtheshelf, all that is needed is the inductive, automatic constructionof a classifier from a set of manually classified documents. The same happens ifa classifier already exists and the original set of categories is updated, or if theclassifier is ported to a completely different domain.2Within the area of contentbased document management tasks, an example of an unsupervisedlearning activity is document clustering see Section 1.10  F. SebastianiIn the ML approach the preclassified documents are then the key resource. In themost favourable case they are already available this typicaly happens for organizations which have previously carried out the same categorization activity manuallyand decide to automate the process. The less favourable case is when no manuallyclassified documents are available this typicaly happens for organizations whichstart a categorization activity and opt for an automated modality straightaway.The ML approach is more convenient than the KE approach also in this lattercase. In fact, it is easier to manually classify a set of documents than to build andtune a set of rules, since it is easier to characterize a concept extensionally i.e. toselect instances of it than intensionally i.e. to describe the concept in words, orto describe a procedure for recognizing its instances.Classifiers built by means of ML techniques nowadays achieve impressive levelsof effectiveness see Section 7, making automatic classification a qualitatively andnot only economically viable alternative to manual classification.4.1 Training set, test set, and validation setThe ML approach relies on the availability of an initial corpus   d1, . . . , d D of documents preclassified under C  c1, . . . , cC. That is, the values of thetotal function   D  C  T, F are known for every pair dj , ci    C. Adocument dj is a positive example of ci if dj , ci  T , a negative example of ci ifdj , ci  F .In research settings and in most operational settings too, once a classifier  hasbeen built it is desirable to evaluate its effectiveness. In this case, prior to classifierconstruction the initial corpus is split in two sets, not necessarily of equal sizea trainingandvalidation set TV  d1, . . . , dTV . The classifier  for categories C  c1, . . . , cC is inductively built by observing the characteristics ofthese documentsa test set Te  dTV 1, . . . , d, used for testing the effectiveness of the classifiers. Each dj  Te is fed to the classifier, and the classifier decisions dj , ciare compared with the expert decisions dj , ci. A measure of classification effectiveness is based on how often the dj , ci values match the dj , ci values.The documents in Te cannot participate in any way in the inductive construction ofthe classifiers if this condition were not satisfied the experimental results obtainedwould likely be unrealistically good, and the evaluation would thus have no scientificcharacter Mitchell 1996, page 129. In an operational setting, after evaluation hasbeen performed one would typically retrain the classifier on the entire initial corpus,in order to boost effectiveness. In this case the results of the previous evaluationwould be a pessimistic estimate of the real performance, since the final classifierhas been trained on more data than the classifier evaluated.This is called the trainandtest approach. An alternative is the kfold crossvalidation approach see e.g. Mitchell 1996, page 146, in which k different classifiers 1, . . . ,k are built by partitioning the initial corpus into k disjoint setsTe1, . . . , T ek and then iteratively applying the trainandtest approach on pairsTVi    Tei, T ei. The final effectiveness figure is obtained by individuallycomputing the effectiveness of 1, . . . ,k, and then averaging the individual reMachine Learning in Automated Text Categorization  11sults in some way.In both approaches it is often the case that the internal parameters of the classifiers must be tuned, by testing which values of the parameters yield the best effectiveness. In order to make this optimization possible, in the trainandtest approachthe set d1, . . . , dTV  is further split into a training set Tr  d1, . . . , dTr, fromwhich the classifier is built, and a validation set V a  dTr1, . . . , dTV  sometimes called a holdout set, on which the repeated tests of the classifier aimedat parameter optimization are performed the obvious variant may be used in thekfold crossvalidation case. Note that, for the same reason why we do not test aclassifier on the documents it has been trained on, we do not test it on the documents it has been optimized on test set and validation set must be kept separate3.Given a corpus , one may define the generality gci of a category ci as thepercentage of documents that belong to ci, i.e.gci dj    dj , ci  T The training set generality gTrci, validation set generality gV aci, and test setgenerality gTeci of ci may be defined in the obvious way.4.2 Information retrieval techniques and text categorizationText categorization heavily relies on the basic machinery of IR. The reason is thatTC is a contentbased document management task, and as such it shares manycharacteristics with other IR tasks such as text search.IR techniques are used in three phases of the text classifier life cycle1 IRstyle indexing is always performed on the documents of the initial corpusand on those to be classified during the operational phase2 IRstyle techniques such as documentrequest matching, query reformulation,. . .  are often used in the inductive construction of the classifiers3 IRstyle evaluation of the effectiveness of the classifiers is performed.The various approaches to classification differ mostly for how they tackle 2, although in a few cases nonstandard approaches to 1 and 3 are also used. Indexing, induction and evaluation are the themes of Sections 5, 6 and 7, respectively.5. DOCUMENT INDEXING AND DIMENSIONALITY REDUCTION5.1 Document indexingTexts cannot be directly interpreted by a classifier or by a classifierbuilding algorithm. Because of this, an indexing procedure that maps a text dj into a compactrepresentation of its content needs to be uniformly applied to training, validationand test documents. The choice of a representation for text depends on what oneregards as the meaningful units of text the problem of lexical semantics and themeaningful natural language rules for the combination of these units the problem3From now on, we will take the freedom to use the expression test document to denote anydocument not in the training set and validation set. This includes thus any document submittedto the classifier in the operational phase.12  F. Sebastianiof compositional semantics. Similarly to what happens in IR, in TC this latterproblem is usually disregarded4, and a text dj is usually represented as a vector ofterm weights dj  w1j , . . . , wT j, where T is the set of terms sometimes calledfeatures that occur at least once in at least one document of Tr, and 0  wkj  1represents, loosely speaking, how much term tk contributes to the semantics ofdocument dj . Differences among approaches are accounted for by1 different ways to understand what a term is2 different ways to compute term weights.A typical choice for 1 is to identify terms with words. This is often called either theset of words or the bag of words approach to document representation, dependingon whether weights are binary or not.In a number of experiments Apte et al. 1994 Dumais et al. 1998 Lewis 1992ait has been found that representations more sophisticated than this do not yieldsignificantly better effectiveness, thereby confirming similar results from IR Saltonand Buckley 1988. In particular, some authors have used phrases, rather thanindividual words, as indexing terms Fuhr et al. 1991 Schutze et al. 1995 Tzerasand Hartmann 1993, but the experimental results found to date have not beenuniformly encouraging, irrespectively of whether the notion of phrase is motivatedsyntactically, i.e. the phrase is such according to a grammar of the language seee.g. Lewis 1992astatistically, i.e. the phrase is not grammatically such, but is composed of asetsequence of words whose patterns of contiguous occurrence in the collectionare statistically significant see e.g. Caropreso et al. 2001.Lewis 1992a argues that the likely reason for the discouraging results is that,although indexing languages based on phrases have superior semantic qualities,they have inferior statistical qualities with respect to wordonly indexing languagesa phraseonly indexing language has more terms, more synonymous or nearlysynonymous terms, lower consistency of assignment since synonymous terms arenot assigned to the same documents, and lower document frequency for termsLewis 1992a, page 40. Although his remarks are about syntactically motivatedphrases, they also apply to statistically motivated ones, although perhaps to asmaller degree. A combination of the two approaches is probably the best way togo Tzeras and Hartmann 1993 obtained significant improvements by using nounphrases obtained through a combination of syntactic and statistical criteria, wherea crude syntactic method was complemented by a statistical filter only thosesyntactic phrases that occurred at least three times in the positive examples ofa category ci were retained. It is likely that the final word on the usefulness ofphrase indexing in TC has still to be told, and investigations in this direction arestill being actively pursued Caropreso et al. 2001 Mladenic and Grobelnik 1998.As for issue 2, weights usually range between 0 and 1 an exception is Lewiset al. 1996, and for ease of exposition we will assume they always do. As a specialcase, binary weights may be used 1 denoting presence and 0 absence of the term4An exception to this is represented by learning approaches based on Hidden Markov Models Denoyer et al. 2001 Frasconi et al. 2001.Machine Learning in Automated Text Categorization  13in the document whether binary or nonbinary weights are used depends on theclassifier learning algorithm used. In the case of nonbinary indexing, for determining the weight wkj of term tk in document dj any IRstyle indexing techniquethat represents a document as a vector of weighted terms may be used. Most ofthe times, the standard tfidf function is used see e.g. Salton and Buckley 1988,defined astfidftk, dj  tk, dj  logTrTrtk1where tk, dj denotes the number of times tk occurs in dj , and Trtk denotesthe document frequency of term tk, i.e. the number of documents in Tr in whichtk occurs. This function embodies the intuitions that i the more often a termoccurs in a document, the more it is representative of its content, and ii the moredocuments a term occurs in, the less discriminating it is5. Note that this formulaas most other indexing formulae weights the importance of a term to a documentin terms of occurrence considerations only, thereby deeming of null importancethe order in which the terms occur in the document and the syntactic role theyplay. In other words, the semantics of a document is reduced to the collectivelexical semantics of the terms that occur in it, thereby disregarding the issue ofcompositional semantics an exception are the representation techniques used forFoil Cohen 1995a and Sleeping Experts Cohen and Singer 1999.In order for the weights to fall in the 0,1 interval and for the documents to berepresented by vectors of equal length, the weights resulting from tfidf are oftennormalized by cosine normalization, given bywkj tfidftk, djT s1tfidfts, dj22Although normalized tfidf is the most popular one, other indexing functions havealso been used, including probabilistic techniques Govert et al. 1999 or techniquesfor indexing structured documents Larkey and Croft 1996. Functions differentfrom tfidf are especially needed when Tr is not available in its entirety from thestart and Trtk cannot thus be computed, as e.g. in adaptive filtering in thiscase approximations of tfidf are usually employed Dagan et al. 1997, Section 4.3.Before indexing, the removal of function words i.e. topicneutral words such asarticles, prepositions, conjunctions, etc. is almost always performed exceptionsinclude Lewis et al. 1996 Nigam et al. 2000 Riloff 19956. Concerning stemmingi.e. grouping words that share the same morphological root, its suitability to TCis controversial. Although, similarly to unsupervised term clustering see Section5.5.1 of which it is an instance, stemming has sometimes been reported to hurteffectiveness e.g. Baker and McCallum 1998, the recent tendency is to adopt it,5There exist many variants of tfidf , that differ from each other in terms of logarithms, normalization or other correction factors. Formula 1 is just one of the possible instances of this class seeSalton and Buckley 1988 Singhal et al. 1996 for variations on this theme.6One application of TC in which it would be inappropriate to remove function words is authoridentification for documents of disputed paternity. In fact, as noted in Manning and Schutze1999, page 589, it is often the little words that give an author away for example, the relativefrequencies of words like because or though.14  F. Sebastianias it reduces both the dimensionality of the term space see Section 5.3 and thestochastic dependence between terms see Section 6.2.Depending on the application, either the full text of the document or selectedparts of it are indexed. While the former option is the rule, exceptions exist. Forinstance, in a patent categorization application Larkey 1999 indexes only the title,the abstract, the first 20 lines of the summary, and the section containing the claimsof novelty of the described invention. This approach is made possible by the factthat documents describing patents are structured. Similarly, when a document titleis available, one can pay extra importance to the words it contains Apte et al. 1994Cohen and Singer 1999 Weiss et al. 1999. When documents are flat, identifyingthe most relevant part of a document is instead a nonobvious task.5.2 The Darmstadt Indexing ApproachThe AIRX system Fuhr et al. 1991 occupies a special place in the literature onindexing for TC. This system is the final result of the AIR project, one of the mostimportant efforts in the history of TC spanning a duration of more than ten yearsKnorz 1982 Tzeras and Hartmann 1993, it has produced a system operativelyemployed since 1985 in the classification of corpora of scientific literature of O105documents and O104 categories, and has had important theoretical spinoffs inthe field of probabilistic indexing Fuhr 1989 Fuhr and Buckley 19917.The approach to indexing taken in AIRX is known as the Darmstadt IndexingApproach DIA Fuhr 1985. Here, indexing is used in the sense of Section3.1, i.e. as using terms from a controlled vocabulary, and is thus a synonym ofTC the DIA was later extended to indexing with free terms Fuhr and Buckley1991. The idea that underlies the DIA is the use of a much wider set of featuresthan described in Section 5.1. All other approaches mentioned in this paper viewterms as the dimensions of the learning space, where terms may be single words,stems, phrases, or see Sections 5.5.1 and 5.5.2 combinations of any of these. Incontrast, the DIA considers properties of terms, documents, categories, or pairwiserelationships among these as basic dimensions of the learning space. Examples ofthese areproperties of a term tk e.g. the idf of tkproperties of the relationship between a term tk and a document dj  e.g. the tfof tk in dj  or the location e.g. in the title, or in the abstract of tk within dj properties of a document dj  e.g. the length of dj properties of a category ci e.g. the training set generality of ci.For each possible documentcategory pair, the values of these features are collectedin a socalled relevance description vector rddj , ci. The size of this vector is determined by the number of properties considered, and is thus independent of specificterms, categories or documents for multivalued features, appropriate aggregation7The AIRX system, its applications including the AIRPHYS system Biebricher et al. 1988,an application of AIRX to indexing physics literature, and its experiments, have also been richlydocumented in a series of papers and doctoral theses written in German. The interested readermay consult Fuhr et al. 1991 for a detailed bibliography.Machine Learning in Automated Text Categorization  15functions are applied in order to yield a single value to be included in rddj , ci inthis way an abstraction from specific terms, categories or documents is achieved.The main advantage of this approach is the possibility to consider additionalfeatures that can hardly be accounted for in the usual termbased approaches, e.g.the location of a term within a document, or the certainty with which a phrase wasidentified in a document. The termcategory relationship is described by estimates,derived from the training set, of the probability P citk that a document belongs tocategory ci, given that it contains term tk the DIA association factor8. Relevancedescription vectors rddj , ci are then the final representations that are used for theclassification of document dj under category ci.The essential ideas of the DIA  transforming the classification space by meansof abstraction and using a more detailed text representation than the standardbagofwords approach  have not been taken up by other researchers so far. Fornew TC applications dealing with structured documents or categorization of Webpages, these ideas may become of increasing importance.5.3 Dimensionality reductionUnlike in text retrieval, in TC the high dimensionality of the term space i.e. thelarge value of T  may be problematic. In fact, while typical algorithms used in textretrieval such as cosine matching can scale to high values of T , the same doesnot hold of many sophisticated learning algorithms used for classifier induction e.g.the LLSF algorithm of Yang and Chute 1994. Because of this, before classifierinduction one often applies a pass of dimensionality reduction DR, whose effectis to reduce the size of the vector space from T  to T   T  the set T  is calledthe reduced term set.DR is also beneficial since it tends to reduce overfitting, i.e. the phenomenonby which a classifier is tuned also to the contingent characteristics of the trainingdata rather than just the constitutive characteristics of the categories. Classifierswhich overfit the training data are good at reclassifying the data they have beentrained on, but much worse at classifying previously unseen data. Experimentshave shown that in order to avoid overfitting a number of training examples roughlyproportional to the number of terms used is needed Fuhr and Buckley 1991, page235 have suggested that 50100 training examples per term may be needed in TCtasks. This means that if DR is performed, overfitting may be avoided even if asmaller amount of training examples is used. However, in removing terms the riskis to remove potentially useful information on the meaning of the documents. It isthen clear that, in order to obtain optimal costeffectiveness, the reduction processmust be performed with care. Various DR methods have been proposed, either fromthe information theory or from the linear algebra literature, and their relative meritshave been tested by experimentally evaluating the variation in effectiveness that agiven classifier undergoes after application of the function to the term space.There are two distinct ways of viewing DR, depending on whether the task isperformed locally i.e. for each individual category or globally8Association factors are called adhesion coefficients in many early papers on TC see e.g. Field1975 Robertson and Harding 1984.16  F. Sebastianilocal DR for each category ci, a set Ti of terms, with Ti   T , is chosen forclassification under ci see e.g. Apte et al. 1994 Lewis and Ringuette 1994 Liand Jain 1998 Ng et al. 1997 Sable and Hatzivassiloglou 2000 Schutze et al.1995 Wiener et al. 1995. This means that different subsets of dj are used whenworking with the different categories. Typical values are 10  T i   50.global DR a set T  of terms, with T   T , is chosen for the classificationunder all categories C  c1, . . . , cC see e.g. Caropreso et al. 2001 Mladenic1998 Yang 1999 Yang and Pedersen 1997.This distinction usually does not impact on the choice of DR technique, since mostsuch techniques can be used and have been used for local and global DR alikesupervised DR techniques  see Section 5.5.1  are exceptions to this rule. Inthe rest of this section we will assume that the global approach is used, althougheverything we will say also applies to the local approach.A second, orthogonal distinction may be drawn in terms of the nature of theresulting termsDR by term selection T  is a subset of T DR by term extraction the terms in T  are not of the same type of the terms inT e.g. if the terms in T are words, the terms in T  may not be words at all,but are obtained by combinations or transformations of the original ones.Unlike in the previous distinction, these two ways of doing DR are tackled by verydifferent techniques we will address them separately in the next two sections.5.4 Dimensionality reduction by term selectionGiven a predetermined integer r, techniques for term selection also called termspace reduction  TSR attempt to select, from the original set T , the set T  ofterms with T   T  that, when used for document indexing, yields the highesteffectiveness. Yang and Pedersen 1997 have shown that TSR may even result ina moderate  5 increase in effectiveness, depending on the classifier, on theaggressivity T T  of the reduction, and on the TSR technique used.Moulinier et al. 1996 have used a socalled wrapper approach, i.e. one in which T is identified by means of the same learning method which will be used for buildingthe classifier John et al. 1994. Starting from an initial term set, a new termset is generated by either adding or removing a term. When a new term set isgenerated, a classifier based on it is built and then tested on a validation set. Theterm set that results in the best effectiveness is chosen. This approach has theadvantage of being tuned to the learning algorithm being used moreover, if localDR is performed, different numbers of terms for different categories may be chosen,depending on whether a category is or is not easily separable from the others.However, the sheer size of the space of different term sets makes its cost prohibitivefor standard TC applications.A computationally easier alternative is the filtering approach John et al. 1994,i.e. keeping the T   T  terms that receive the highest score according to afunction that measures the importance of the term for the TC task. We willexplore this solution in the rest of this section.Machine Learning in Automated Text Categorization  175.4.1 Document frequency. A simple and effective global TSR function is the document frequency Trtk of a term tk, i.e. only the terms that occur in the highestnumber of documents are retained. In a series of experiments Yang and Pedersen1997 have shown that with Trtk it is possible to reduce the dimensionality bya factor of 10 with no loss in effectiveness a reduction by a factor of 100 bringingabout just a small loss.This seems to indicate that the terms occurring most frequently in the collectionare the most valuable for TC. As such, this would seem to contradict a wellknownlaw of IR, according to which the terms with lowtomedium document frequencyare the most informative ones Salton and Buckley 1988. But these two results donot contradict each other, since it is wellknown see e.g. Salton et al. 1975 thatthe large majority of the words occurring in a corpus have a very low documentfrequency this means that by reducing the term set by a factor of 10 using documentfrequency, only such words are removed, while the words from lowtomedium tohigh document frequency are preserved. Of course, stop words need to be removedin advance, lest only topicneutral words are retained Mladenic 1998.Finally, note that a slightly more empirical form of TSR by document frequencyis adopted by many authors, who remove all terms occurring in at most x trainingdocuments popular values for x range from 1 to 3, either as the only form of DRMaron 1961 Ittner et al. 1995 or before applying another more sophisticated formDumais et al. 1998 Li and Jain 1998. A variant of this policy is removing all termsthat occur at most x times in the training set e.g. Dagan et al. 1997 Joachims1997, with popular values for x ranging from 1 e.g. Baker and McCallum 1998to 5 e.g. Apte et al. 1994 Cohen 1995a.5.4.2 Other informationtheoretic term selection functions. Other more sophisticated informationtheoretic functions have been used in the literature, amongwhich the DIA association factor Fuhr et al. 1991, chisquare Caropreso et al.2001 Galavotti et al. 2000 Schutze et al. 1995 Sebastiani et al. 2000 Yang andPedersen 1997 Yang and Liu 1999, NGL coefficient Ng et al. 1997 Ruiz and Srinivasan 1999, information gain Caropreso et al. 2001 Larkey 1998 Lewis 1992aLewis and Ringuette 1994 Mladenic 1998 Moulinier and Ganascia 1996 Yang andPedersen 1997 Yang and Liu 1999, mutual information Dumais et al. 1998 Lamet al. 1997 Larkey and Croft 1996 Lewis and Ringuette 1994 Li and Jain 1998Moulinier et al. 1996 Ruiz and Srinivasan 1999 Taira and Haruno 1999 Yang andPedersen 1997, odds ratio Caropreso et al. 2001 Mladenic 1998 Ruiz and Srinivasan 1999, relevancy score Wiener et al. 1995, and GSS coefficient Galavottiet al. 2000. The mathematical definitions of these measures are summarized forconvenience in Table 19. Here, probabilities are interpreted on an event space ofdocuments e.g. P tk, ci denotes the probability that, for a random document x,term tk does not occur in x and x belongs to category ci, and are estimated by9For better uniformity Table 1 views all the TSR functions in terms of subjective probability. Insome cases such as tk , ci and 2tk , ci this is slightly artificial, since these two functions arenot usually viewed in probabilistic terms. The formulae refer to the local i.e. categoryspecificforms of the functions, which again is slightly artificial in some cases e.g. tk, ci. Note thatthe NGL and GSS coefficients are here named after their authors, since they had originally beengiven names that might generate some confusion if used here.18  F. SebastianiFunction Denoted by Mathematical formDocument frequency tk , ci P tkciDIA association factor ztk , ci P citkInformation gain IGtk , cicci,cittk,tkP t, c  logP t, cP t  P cMutual information MItk , ci logP tk , ciP tk  P ciChisquare 2tk , ciTr  P tk, ci  P tk , ci P tk, ci  P tk, ci2P tk  P tk  P ci  P ciNGL coefficient NGLtk , ciTr  P tk, ci  P tk, ci P tk , ci  P tk, ciP tk  P tk  P ci  P ciRelevancy score RStk , ci logP tkci  dP tkci  dOdds Ratio ORtk , ciP tk ci  1 P tkci1 P tk ci  P tk ciGSS coefficient GSStk, ci P tk, ci  P tk, ci P tk , ci  P tk, ciTable 1. Main functions used for term space reduction purposes. Information gain is also knownas expected mutual information it is used under this name by Lewis 1992a, page 44 and Larkey1998. In the RStk , ci formula d is a constant damping factor.counting occurrences in the training set. All functions are specified locally to aspecific category ci in order to assess the value of a term tk in a global, categoryindependent sense, either the sum fsumtk Ci1 ftk, ci, or the weighted average fwavgtk Ci1 P ciftk, ci, or the maximum fmaxtk  maxCi1 ftk, ciof their categoryspecific values ftk, ci are usually computed.These functions try to capture the intuition that the best terms for ci are theones distributed most differently in the sets of positive and negative examples ofci. However, interpretations of this principle vary across different functions. Forinstance, in the experimental sciences 2 is used to measure how the results of anobservation differ i.e. are independent from the results expected according to aninitial hypothesis lower values indicate lower dependence. In DR we measure howindependent tk and ci are. The terms tk with the lowest value for 2tk, ci arethus the most independent from ci since we are interested in the terms which arenot, we select the terms for which 2tk, ci is highest.While each TSR function has its own rationale, the ultimate word on its valueis the effectiveness it brings about. Various experimental comparisons of TSRfunctions have thus been carried out Caropreso et al. 2001 Galavotti et al. 2000Mladenic 1998 Yang and Pedersen 1997. In these experiments most functionslisted in Table 1 with the possible exception of MI have improved on the resultsof document frequency. For instance, Yang and Pedersen 1997 have shown that,with various classifiers and various initial corpora, sophisticated techniques suchMachine Learning in Automated Text Categorization  19as IGsumtk, ci or 2maxtk, ci can reduce the dimensionality of the term spaceby a factor of 100 with no loss or even with a small increase of effectiveness.Collectively, the experiments reported in the abovementioned papers seem to indicate that ORsum, NGLsum, GSSmax  2max, IGsum  wavg, 2wavg MImax, MIwavg, where  means performs better than. However, it shouldbe noted that these results are just indicative, and that more general statementson the relative merits of these functions could be made only as a result of comparative experiments performed in thoroughly controlled conditions and on a varietyof different situations e.g. different classifiers, different initial corpora, . . . .5.5 Dimensionality reduction by term extractionGiven a predetermined T   T , term extraction attempts to generate, fromthe original set T , a set T  of synthetic terms that maximize effectiveness. Therationale for using synthetic rather than naturally occurring terms is that, due tothe pervasive problems of polysemy, homonymy and synonymy, the original termsmay not be optimal dimensions for document content representation. Methodsfor term extraction try to solve these problems by creating artificial terms thatdo not suffer from them. Any term extraction method consists in i a methodfor extracting the new terms from the old ones, and ii a method for convertingthe original document representations into new representations based on the newlysynthesized dimensions. Two term extraction methods have been experimented inTC, namely term clustering and latent semantic indexing.5.5.1 Term clustering. Term clustering tries to group words with a high degreeof pairwise semantic relatedness, so that the groups or their centroids, or a representative of them may be used instead of the terms as dimensions of the vectorspace. Term clustering is different from term selection, since the former tends toaddress terms synonymous or nearsynonymous with other terms, while the lattertargets noninformative terms10.Lewis 1992a was the first to investigate the use of term clustering in TC. Themethod he employed, called reciprocal nearest neighbour clustering, consists in creating clusters of two terms that are one the most similar to the other according to some measure of similarity. His results were inferior to those obtained bysingleword indexing, possibly due to a disappointing performance by the clusteringmethod as Lewis 1992a, page 48 says, The relationships captured in the clustersare mostly accidental, rather than the systematic relationships that were hopedfor.Li and Jain 1998 view semantic relatedness between words in terms of theircooccurrence and coabsence within training documents. By using this techniquein the context of a hierarchical clustering algorithm they witnessed only a marginaleffectiveness improvement however, the small size of their experiment see Section6.11 hardly allows any definitive conclusion to be reached.Both Lewis 1992a Li and Jain 1998 are examples of unsupervised clustering,since clustering is not affected by the category labels attached to the documents.Baker and McCallum 1998 provide instead an example of supervised clustering, as10Some term selection methods, such as wrapper methods, also address the problem of redundancy.20  F. Sebastianithe distributional clustering method they employ clusters together those terms thattend to indicate the presence of the same category, or group of categories. Theirexperiments, carried out in the context of a Nave Bayes classifier see Section6.2, showed only a 2 effectiveness loss with an aggressivity of 1000, and evenshowed some effectiveness improvement with less aggressive levels of reduction.Later experiments by Slonim and Tishby 2001 have confirmed the potential ofsupervised clustering methods for term extraction.5.5.2 Latent semantic indexing. Latent semantic indexing LSI  Deerwesteret al. 1990 is a DR technique developed in IR in order to address the problems deriving from the use of synonymous, nearsynonymous and polysemous wordsas dimensions of document representations. This technique compresses documentvectors into vectors of a lowerdimensional space whose dimensions are obtainedas combinations of the original dimensions by looking at their patterns of cooccurrence. In practice, LSI infers the dependence among the original terms froma corpus and wires this dependence into the newly obtained, independent dimensions. The function mapping original vectors into new vectors is obtained byapplying a singular value decomposition to the matrix formed by the original document vectors. In TC this technique is applied by deriving the mapping functionfrom the training set and then applying it to training and test documents alike.One characteristic of LSI is that the newly obtained dimensions are not, unlike interm selection and term clustering, intuitively interpretable. However, they workwell in bringing out the latent semantic structure of the vocabulary used in thecorpus. For instance, Schutze et al. 1995, page 235 discuss the classification undercategory Demographic shifts in the U.S. with economic impact of a document that was indeed a positive test instance for the category, and that contained,among others, the quite revealing sentence The nation grew to 249.6 millionpeople in the 1980s as more Americans left the industrial and agricultural heartlands for the South and West. The classifier decision was incorrect when local DR had been performed by 2based term selection retaining thetop original 200 terms, but was correct when the same task was tackled by meansof LSI. This well exemplifies how LSI works the above sentence does not containany of the 200 terms most relevant to the category selected by 2, but quite possibly the words contained in it have concurred to produce one or more of the LSIhigherorder terms that generate the document space of the category. As Schutzeet al. 1995, page 230 put it, if there is a great number of terms which all contribute a small amount of critical information, then the combination of evidence isa major problem for a termbased classifier. A drawback of LSI, though, is that ifsome original term is particularly good in itself at discriminating a category, thatdiscrimination power may be lost in the new vector space.Wiener et al. 1995 use LSI in two alternative ways i for local DR, thus creatingseveral categoryspecific LSI representations, and ii for global DR, thus creating asingle LSI representation for the entire category set. Their experiments showed theformer approach to perform better than the latter, and both approaches to performbetter than simple TSR based on Relevancy Score see Table 1.Schutze et al. 1995 experimentally compared LSIbased term extraction with2based TSR using three different classifier learning techniques namely, linearMachine Learning in Automated Text Categorization  21discriminant analysis, logistic regression and neural networks. Their experimentsshowed LSI to be far more effective than 2 for the first two techniques, while bothmethods performed equally well for the neural network classifier.For other TC works that use LSI or similar term extraction techniques see e.g.Hull 1994 Li and Jain 1998 Schutze 1998 Weigend et al. 1999 Yang 1995.6. INDUCTIVE CONSTRUCTION OF TEXT CLASSIFIERSThe inductive construction of text classifiers has been tackled in a variety of ways.Here we will deal only with the methods that have been most popular in TC, butwe will also briefly mention the existence of alternative, less standard approaches.We start by discussing the general form that a text classifier has. Let us recallfrom Section 2.4 that there are two alternative ways of viewing classification hardfully automated classification and ranking semiautomated classification.The inductive construction of a ranking classifier for category ci  C usuallyconsists in the definition of a function CSVi  D  0, 1 that, given a documentdj , returns a categorization status value for it, i.e. a number between 0 and 1 that,roughly speaking, represents the evidence for the fact that dj  ci. Documentsare then ranked according to their CSVi value. This works for documentrankingTC categoryranking TC is usually tackled by ranking, for a given documentdj , its CSVi scores for the different categories in C  c1, . . . , cC.The CSVi function takes up different meanings according to the learning methodused for instance, in the Nave Bayes approach of Section 6.2 CSVidj is definedin terms of a probability, whereas in the Rocchio approach discussed in Section6.7 CSVidj is a measure of vector closeness in T dimensional space.The construction of a hard classifier may follow two alternative paths. Theformer consists in the definition of a function CSVi  D  T, F. The latterconsists instead in the definition of a function CSVi  D  0, 1, analogous to theone used for ranking classification, followed by the definition of a threshold i suchthat CSVidj  i is interpreted as T while CSVidj  i is interpreted as F11.The definition of thresholds will be the topic of Section 6.1. In Sections 6.2 to6.12 we will instead concentrate on the definition of CSVi, discussing a number ofapproaches that have been applied in the TC literature. In general we will assumewe are dealing with hard classification it will be evident from the context how andwhether the approaches can be adapted to ranking classification. The presentationof the algorithms will be mostly qualitative rather than quantitative, i.e. will focuson the methods for classifier learning rather than on the effectiveness and efficiencyof the classifiers built by means of them this will instead be the focus of Section 7.6.1 Determining thresholdsThere are various policies for determining the threshold i, also depending on theconstraints imposed by the application. The most important distinction is whetherthe threshold is derived analytically or experimentally.The former method is possible only in the presence of a theoretical result thatindicates how to compute the threshold that maximizes the expected value of the11Alternative methods are possible, such as training a classifier for which some standard, predefined value such as 0 is the threshold. For ease of exposition we will not discuss them.22  F. Sebastianieffectiveness function Lewis 1995a. This is typical of classifiers that output probability estimates of the membership of dj in ci see Section 6.2 and whose effectiveness is computed by decisiontheoretic measures such as utility see Section 7.1.3we thus defer the discussion of this policy which is called probability thresholdingin Lewis 1995a to Section 7.1.3.When such a theoretical result is not available one has to revert to the lattermethod, which consists in testing different values for i on a validation set andchoosing the value which maximizes effectiveness. We call this policy CSV thresholding Cohen and Singer 1999 Schapire et al. 1998 Wiener et al. 1995 it is alsocalled Scut in Yang 1999. Different is are typically chosen for the different cis.A second, popular experimental policy is proportional thresholding Iwayama andTokunaga 1995 Larkey 1998 Lewis 1992a Lewis and Ringuette 1994 Wiener et al.1995, also called Pcut in Yang 1999. This policy consists in choosing the value ofi for which gV aci is closest to gTrci, and embodies the principle that the samepercentage of documents of both training and test set should be classified under ci.For obvious reasons, this policy does not lend itself to documentpivoted TC.Sometimes, depending on the application, a fixed thresholding policy aka kperdoc thresholding Lewis 1992a or Rcut Yang 1999 is applied, whereby it isstipulated that a fixed number k of categories, equal for all dj s, are to be assignedto each document dj . This is often used, for instance, in applications of TC toautomated document indexing Field 1975 Lam et al. 1999. Strictly speaking,however, this is not a thresholding policy in the sense defined at the beginning ofSection 6, as it might happen that d is classified under ci, d is not, and CSVid CSVid. Quite clearly, this policy is mostly at home with documentpivoted TC.However, it suffers from a certain coarseness, as the fact that k is equal for alldocuments nor could this be otherwise allows no finetuning.In his experiments Lewis 1992a found the proportional policy to be superior to probability thresholding when microaveraged effectiveness was tested butslightly inferior with macroaveraging see Section 7.1.1. Yang 1999 found insteadCSV thresholding to be superior to proportional thresholding possibly due to hercategoryspecific optimization on a validation set, and found fixed thresholding tobe consistently inferior to the other two policies. The fact that these latter resultshave been obtained across different classifiers no doubt reinforce them.In general, aside from the considerations above, the choice of the thresholdingpolicy may also be influenced by the application for instance, in applying a textclassifier to document indexing for Boolean systems a fixed thresholding policymight be chosen, while a proportional or CSV thresholding method might be chosenfor Web page classification under hierarchical catalogues.6.2 Probabilistic classifiersProbabilistic classifiers see Lewis 1998 for a thorough discussion view CSVidjin terms of P cidj, i.e. the probability that a document represented by a vectordj  w1j , . . . , wT j of binary or weighted terms belongs to ci, and compute thisprobability by an application of Bayes theorem, given byP cidj P ciP dj ciP dj3Machine Learning in Automated Text Categorization  23In 3 the event space is the space of documents P dj is thus the probabilitythat a randomly picked document has vector dj as its representation, and P ci theprobability that a randomly picked document belongs to ci.The estimation of P dj ci in 3 is problematic, since the number of possiblevectors dj is too high the same holds for P dj, but for reasons that will beclear shortly this will not concern us. In order to alleviate this problem it iscommon to make the assumption that any two coordinates of the document vectorare, when viewed as random variables, statistically independent of each other thisindependence assumption is encoded by the equationP dj ci T k1P wkj ci 4Probabilistic classifiers that use this assumption are called Nave Bayes classifiers,and account for most of the probabilistic approaches to TC in the literature seee.g. Joachims 1998 Koller and Sahami 1997 Larkey and Croft 1996 Lewis 1992aLewis and Gale 1994 Li and Jain 1998 Robertson and Harding 1984. The navecharacter of the classifier is due to the fact that usually this assumption is, quiteobviously, not verified in practice.One of the bestknown Nave Bayes approaches is the binary independence classifier Robertson and Sparck Jones 1976, which results from using binaryvaluedvector representations for documents. In this case, if we write pki as short forP wkx  1ci, the P wkj ci factors of 4 may be written asP wkj ci  pwkjki 1 pki1wkj  pki1 pkiwkj 1  pki 5We may further observe that in TC the document space is partitioned into twocategories12, ci and its complement ci, such that P cidj  1  P cidj. If weplug in 4 and 5 into 3 and take logs we obtainlogP cidj  logP ci  6T k1wkj logpki1 pkiT k1log1 pki logP djlog1 P cidj  log1 P ci  7T k1wkj logpki1 pkiT k1log1 pki logP djwhere we write pki as short for P wkx  1ci. We may convert 6 and 7 into asingle equation by subtracting componentwise 7 from 6, thus obtaininglogP cidj1 P cidj logP ci1 P ciT k1wkj logpki1 pkipki1 pkiT k1log1 pki1 pki812Cooper 1995 has pointed out that in this case the full independence assumption of 4 is notactually made in the Nave Bayes classifier the assumption needed here is instead the weakerlinked dependence assumption, which may be written asP  dj ciP  dj ciT k1P wkj ciP wkj ci.24  F. SebastianiNote thatP ci dj1P ci djis an increasing monotonic function of P cidj, and may thusbe used directly as CSVidj. Note also that logP ci1P ciandT k1 log1pki1pkiare constant for all documents, and may thus be disregarded13. Defining a classifier for category ci thus basically requires estimating the 2T  parameters p1i, p1i, . . . , pT i, pT ifrom the training data, which may be done in the obvious way. Note that in generalthe classification of a given document does not require to compute a sum of T  factors, as the presence ofT k1 wkj logpki1pkipki1pkiwould imply in fact, all the factorsfor which wkj  0 may be disregarded, and this accounts for the vast majority ofthem, since document vectors are usually very sparse.The method we have illustrated is just one of the many variants of the NaveBayes approach, the common denominator of which is 4. A recent paper byLewis 1998 is an excellent roadmap on the various directions that research onNave Bayes classifiers has taken among these are the ones aimingto relax the constraint that document vectors should be binaryvalued. This looksnatural, given that weighted indexing techniques see e.g. Fuhr 1989 Salton andBuckley 1988 accounting for the importance of tk for dj play a key role in IR.to introduce document length normalization. The value of logP ci dj1P ci djtends tobe more extreme i.e. very high or very low for long documents i.e. documentssuch that wkj  1 for many values of k, irrespectively of their semantic relatedness to ci, thus calling for length normalization. Taking length into account iseasy in nonprobabilistic approaches to classification see e.g. Section 6.7, butis problematic in probabilistic ones see Lewis 1998, Section 5. One possibleanswer is to switch from an interpretation of Nave Bayes in which documents areevents to one in which terms are events Baker and McCallum 1998 McCallumet al. 1998 Chakrabarti et al. 1998a Guthrie et al. 1994. This accounts fordocument length naturally but, as noted in Lewis 1998, has the drawback thatdifferent occurrences of the same word within the same document are viewed asindependent, an assumption even more implausible than 4.to relax the independence assumption. This may be the hardest route to follow,since this produces classifiers of higher computational cost and characterized byharder parameter estimation problems Koller and Sahami 1997. Earlier effortsin this direction within probabilistic text search e.g. van Rijsbergen 1977 havenot shown the performance improvements that were hoped for. Recently, thefact that the binary independence assumption seldom harms effectiveness hasalso been given some theoretical justification Domingos and Pazzani 1997.The quotation of text search in the last paragraph is not casual. Unlike other typesof classifiers, the literature on probabilistic classifiers is inextricably intertwinedwith that on probabilistic search systems see Crestani et al. 1998 for a review,since these latter attempt to determine the probability that a document falls in the13This is not true, however, if the fixed thresholding method of Section 6.1 is adopted. In fact,for a fixed document dj the first and third factor in the formula above are different for differentcategories, and may therefore influence the choice of the categories under which to file dj .Machine Learning in Automated Text Categorization  25Fig. 2. A decision tree equivalent to the DNF rule of Figure 1. Edges are labelled by terms andleaves are labelled by categories underlining denotes negation.category denoted by the query, and since they are the only search systems that takerelevance feedback, a notion essentially involving supervised learning, as central.6.3 Decision tree classifiersProbabilistic methods are quantitative i.e. numeric in nature, and as such havesometimes been criticized since, effective as they may be, are not easily interpretableby humans. A class of algorithms that do not suffer from this problem are symbolici.e. nonnumeric algorithms, among which inductive rule learners which we willdiscuss in Section 6.4 and decision tree learners are the most important examples.A decision tree DT text classifier see e.g. Mitchell 1996, Chapter 3 is atree in which internal nodes are labelled by terms, branches departing from themare labelled by tests on the weight that the term has in the test document, andleafs are labelled by categories. Such a classifier categorizes a test document dj byrecursively testing for the weights that the terms labeling the internal nodes havein vector dj , until a leaf node is reached the label of this node is then assigned todj . Most such classifiers use binary document representations, and thus consist ofbinary trees. An example DT is illustrated in Figure 2.There are a number of standard packages for DT learning, and most DT approaches to TC have made use of one such package. Among the most popular onesare ID3 used in Fuhr et al. 1991, C4.5 used in Cohen and Hirsh 1998 Cohenand Singer 1999 Joachims 1998 Lewis and Catlett 1994 and C5 used in Li andJain 1998. TC efforts based on experimental DT packages include Dumais et al.1998 Lewis and Ringuette 1994 Weiss et al. 1999.A possible method for learning a DT for category ci consists in a divide andconquer strategy of i checking whether all the training examples have the same26  F. Sebastianilabel either ci or ci ii if not, selecting a term tk, partitioning Tr into classesof documents that have the same value for tk, and placing each such class in aseparate subtree. The process is recursively repeated on the subtrees until each leafof the tree so generated contains training examples assigned to the same category ci,which is then chosen as the label for the leaf. The key step is the choice of the termtk on which to operate the partition, a choice which is generally made according toan information gain or entropy criterion. However, such a fully grown tree maybe prone to overfitting, as some branches may be too specific to the training data.Most DT learning methods thus include a method for growing the tree and one forpruning it, i.e. for removing the overly specific branches. Variations on this basicschema for DT learning abound Mitchell 1996, Section 3.DT text classifiers have been used either as the main classification tool Fuhr et al.1991 Lewis and Catlett 1994 Lewis and Ringuette 1994, or as baseline classifiersCohen and Singer 1999 Joachims 1998, or as members of classifier committees Liand Jain 1998 Schapire and Singer 2000 Weiss et al. 1999.6.4 Decision rule classifiersA classifier for category ci built by an inductive rule learning method consists ofa DNF rule, i.e. of a conditional rule with a premise in disjunctive normal formDNF, of the type illustrated in Figure 114. The literals i.e. possibly negatedkeywords in the premise denote the presence nonnegated keyword or absencenegated keyword of the keyword in the test document dj , while the clause headdenotes the decision to classify dj under ci. DNF rules are similar to DTs in thatthey can encode any Boolean function. However, an advantage of DNF rule learnersis that they tend to generate more compact classifiers than DT learners.Rule learning methods usually attempt to select from all the possible coveringrules i.e. rules that correctly classify all the training examples the best oneaccording to some minimality criterion. While DTs are typically built by a topdown, divideandconquer strategy, DNF rules are often built in a bottomupfashion. Initially, every training example dj is viewed as a clause 1, . . . , n  i,where 1, . . . , n are the terms contained in dj and i equals ci or ci according towhether dj is a positive or negative example of ci. This set of clauses is already aDNF classifier for ci, but obviously scores high in terms of overfitting. The learnerapplies then a process of generalization in which the rule is simplified through aseries of modifications e.g. removing premises from clauses, or merging clausesthat maximize its compactness while at the same time not affecting the coveringproperty of the classifier. At the end of this process, a pruning phase similar inspirit to that employed in DTs is applied, where the ability to correctly classify allthe training examples is traded for more generality.DNF rule learners vary widely in terms of the methods, heuristics and criteriaemployed for generalization and pruning. Among the DNF rule learners that havebeen applied to TC are Charade Moulinier and Ganascia 1996, DLESC Li andYamanishi 1999, Ripper Cohen 1995a Cohen and Hirsh 1998 Cohen and Singer14Many inductive rule learning algorithms build decision lists i.e. arbitrarily nested ifthenelseclauses instead of DNF rules since the former may always be rewritten as the latter we willdisregard the issue.Machine Learning in Automated Text Categorization  271999, Scar Moulinier et al. 1996, and Swap1 Apte et al. 1994.While the methods above use rules of propositional logic PL, research has alsobeen carried out using rules of first order logic FOL, obtainable through the use ofinductive logic programming methods. Cohen 1995a has extensively compared PLand FOL learning in TC for instance, comparing the PL learner Ripper with itsFOL version Flipper, and has found that the additional representational powerof FOL brings about only modest benefits.6.5 Regression methodsVarious TC efforts have used regression models see e.g. Fuhr and Pfeifer 1994Ittner et al. 1995 Lewis and Gale 1994 Schutze et al. 1995. Regression denotes theapproximation of a realvalued instead than binary, as in the case of classificationfunction  by means of a function  that fits the training data Mitchell 1996,page 236. Here we will describe one such model, the Linear Least Squares FitLLSF applied to TC by Yang and Chute 1994. In LLSF, each document dj hastwo vectors associated to it an input vector Idj of T  weighted terms, and anoutput vector Odj of C weights representing the categories the weights for thislatter vector are binary for training documents, and are nonbinary CSV s for testdocuments. Classification may thus be seen as the task of determining an outputvector Odj for test document dj , given its input vector Idj hence, building aclassifier boils down to computing a C T  matrix M such that MIdj  Odj.LLSF computes the matrix from the training data by computing a linear leastsquares fit that minimizes the error on the training set according to the formulaM  argminM MIOF , where argminM x stands as usual for the M for whichx is minimum, V FdefCi1T j1 v2ij represents the socalled Frobenius normof a C T  matrix, I is the T  Tr matrix whose columns are the input vectorsof the training documents, and O is the C  Tr matrix whose columns are theoutput vectors of the training documents. The M matrix is usually computed byperforming a singular value decomposition on the training set, and its generic entrymik represents the degree of association between category ci and term tk.The experiments of Yang and Chute 1994 Yang and Liu 1999 indicate thatLLSF is one of the most effective text classifiers known to date. One of its disadvantages, though, is that the cost of computing the M matrix is much higher thanthat of many other competitors in the TC arena.6.6 Online methodsA linear classifier for category ci is a vector ci  w1i, . . . , wT i belonging tothe same T dimensional space in which documents are also represented, and suchthat CSVidj corresponds to the dot productT k1 wkiwkj ofdj and ci. Note thatwhen both classifier and document weights are cosinenormalized see 2, the dotproduct between the two vectors corresponds to their cosine similarity, i.e.Sci, dj  cos T k1 wki  wjkT k1 w2ki T k1 w2kjwhich represents the cosine of the angle  that separates the two vectors. This is28  F. Sebastianithe similarity measure between query and document computed by standard vectorspace IR engines, which means in turn that once a linear classifier has been built,classification can be performed by invoking such an engine. Practically all searchengines have a dot product flavour to them, and can therefore be adapted to doingTC with a linear classifier.Methods for learning linear classifiers are often partitioned in two broad classes,batch methods and online methods.Batch methods build a classifier by analysing the training set all at once. Withinthe TC literature, one example of a batch method is linear discriminant analysis,a model of the stochastic dependence between terms that relies on the covariancematrices of the categories Hull 1994 Schutze et al. 1995. However, the foremostexample of a batch method is the Rocchio method because of its importance inthe TC literature this will be discussed separately in Section 6.7. In this section wewill instead concentrate on online classifiers.Online aka incremental methods build a classifier soon after examining thefirst training document, and incrementally refine it as they examine new ones. Thismay be an advantage in the applications in which Tr is not available in its entiretyfrom the start, or in which the meaning of the category may change in time,as e.g. in adaptive filtering. This is also apt to applications e.g. semiautomatedclassification, adaptive filtering in which we may expect the user of a classifier toprovide feedback on how test documents have been classified, as in this case furthertraining may be performed during the operating phase by exploiting user feedback.A simple online method is the perceptron algorithm, first applied to TC inSchutze et al. 1995 Wiener et al. 1995 and subsequently used in Dagan et al.1997 Ng et al. 1997. In this algorithm, the classifier for ci is first initialized bysetting all weights wki to the same positive value. When a training example djrepresented by a vector dj of binary weights is examined, the classifier built sofar classifies it. If the result of the classification is correct nothing is done, while ifit is wrong the weights of the classifier are modified if dj was a positive example ofci then the weights wki of active terms i.e. the terms tk such that wkj  1 arepromoted by increasing them by a fixed quantity   0 called learning rate,while if dj was a negative example of ci then the same weights are demoted bydecreasing them by . Note that when the classifier has reached a reasonable levelof effectiveness, the fact that a weight wki is very low means that tk has negativelycontributed to the classification process so far, and may thus be discarded from therepresentation. We may then see the perceptron algorithm as all other incrementallearning methods as allowing for a sort of onthefly term space reduction Daganet al. 1997, Section 4.4. The perceptron classifier has shown a good effectivenessin all the experiments quoted above.The perceptron is an additive weightupdating algorithm. A multiplicative variant of it is Positive Winnow Dagan et al. 1997, which differs from perceptronbecause two different constants 1  1 and 0  2  1 are used for promoting anddemoting weights, respectively, and because promotion and demotion are achievedby multiplying, instead of adding, by 1 and 2. Balanced Winnow Daganet al. 1997 is a further variant of Positive Winnow, in which the classifier consists of two weights wki and wki for each term tk the final weight wki used incomputing the dot product is the difference wki wki. Following the misclassificaMachine Learning in Automated Text Categorization  29tion of a positive instance, active terms have their wki weight promoted and theirwki weight demoted, whereas in the case of a negative instance it is wki that getsdemoted while wki gets promoted for the rest, promotions and demotions are as inPositive Winnow. Balanced Winnow allows negative wki weights, while inthe perceptron and in Positive Winnow the wki weights are always positive. Inexperiments conducted by Dagan et al. 1997, Positive Winnow showed a bettereffectiveness than perceptron but was in turn outperformed by Dagan et al.s ownversion of Balanced Winnow.Other online methods for building text classifiers are WidrowHoff, a refinement of it called Exponentiated Gradient both applied for the first time to TCin Lewis et al. 1996 and Sleeping Experts Cohen and Singer 1999, a versionof Balanced Winnow. While the first is an additive weightupdating algorithm,the second and third are multiplicative. Key differences with the previously described algorithms are that these three algorithms i update the classifier not onlyafter misclassifying a training example, but also after classifying it correctly, andii update the weights corresponding to all terms instead of just active ones.Linear classifiers lend themselves to both categorypivoted and documentpivotedTC. For the former the classifier ci is used, in a standard search engine, as a queryagainst the set of test documents, while for the latter the vector dj representingthe test document is used as a query against the set of classifiers c1, . . . ,cC.6.7 The Rocchio methodSome linear classifiers consist of an explicit profile or prototypical document of thecategory. This has obvious advantages in terms of interpretability, as such a profileis more readily understandable by a human than, say, a neural network classifier.Learning a linear classifier is often preceded by local TSR in this case, a profileof ci is a weighted list of the terms whose presence or absence is most useful fordiscriminating ci.The Rocchio method is used for inducing linear, profilestyle classifiers. It relieson an adaptation to TC of the wellknown Rocchios formula for relevance feedbackin the vectorspace model, and it is perhaps the only TC method rooted in the IRtradition rather than in the ML one. This adaptation was first proposed by Hull1994, and has been used by many authors since then, either as an object of researchin its own right Ittner et al. 1995 Joachims 1997 Sable and Hatzivassiloglou 2000Schapire et al. 1998 Singhal et al. 1997, or as a baseline classifier Cohen andSinger 1999 Galavotti et al. 2000 Joachims 1998 Lewis et al. 1996 Schapire andSinger 2000 Schutze et al. 1995, or as a member of a classifier committee Larkeyand Croft 1996 see Section 6.11.Rocchios method computes a classifier ci  w1i, . . . , wT i for category ci bymeans of the formulawki   djPOSiwkjPOSi  djNEGiwkjNEGiwhere wkj is the weight of tk in document dj , POSi  dj  Tr  dj , ci  T  andNEGi  dj  Tr  dj , ci  F. In this formula,  and  are control parametersthat allow setting the relative importance of positive and negative examples. Forinstance, if  is set to 1 and  to 0 as e.g. in Dumais et al. 1998 Hull 199430  F. SebastianiFig. 3. A comparison between the TC behaviour of a the Rocchio classifier, and b the kNNclassifier. Small crosses and circles denote positive and negative training instances, respectively.The big circles denote the influence area of the classifier. Note that, for ease of illustration,document similarities are here viewed in terms of Euclidean distance rather than, as more common,in terms of dot product or cosine.Joachims 1998 Schutze et al. 1995, the profile of ci is the centroid of its positivetraining examples. A classifier built by means of the Rocchio method rewardsthe closeness of a test document to the centroid of the positive training examples,and its distance from the centroid of the negative training examples. The role ofnegative examples is usually deemphasized, by setting  to a high value and  toa low one e.g. Cohen and Singer 1999, Ittner et al. 1995, and Joachims 1997use   16 and   4.This method is quite easy to implement, and is also quite efficient, since learninga classifier basically comes down to averaging weights. In terms of effectiveness,instead, a drawback is that if the documents in the category tend to occur indisjoint clusters e.g. a set of newspaper articles lebelled with the Sports categoryand dealing with either boxing or rockclimbing, such a classifier may miss most ofthem, as the centroid of these documents may fall outside all of these clusters seeFigure 3a. More generally, a classifier built by the Rocchio method, as all linearclassifiers, has the disadvantage that it divides the space of documents linearly.This situation is graphically depicted in Figure 3a, where documents are classifiedwithin ci if and only if they fall within the circle. Note that even most of thepositive training examples would not be classified correctly by the classifier.6.7.1 Enhancements to the basic Rocchio framework. One issue in the application of the Rocchio formula to profile extraction is whether the set NEGi shouldbe considered in its entirety, or whether a wellchosen sample of it, such as theset NPOSi of nearpositives defined as the most positive amongst the negativetraining examples, should be selected from it, yieldingwki   djPOSiwkjPOSi  djNPOSiwkjNPOSiMachine Learning in Automated Text Categorization  31ThedjNPOSiwkjNPOSifactor is more significant thandjNEGiwkjNEGi, sincenearpositives are the most difficult documents to tell apart from the positives. Using nearpositives corresponds to the query zoning method proposed for IR bySinghal et al. 1997. This method originates from the observation that when theoriginal Rocchio formula is used for relevance feedback in IR, nearpositives tend tobe used rather than generic negatives, as the documents on which user judgmentsare available are among the ones that had scored highest in the previous ranking.Early applications of the Rocchio formula to TC e.g. Hull 1994 Ittner et al. 1995generally did not make a distinction between nearpositives and generic negatives.In order to select the nearpositives Schapire et al. 1998 issue a query, consisting ofthe centroid of the positive training examples, against a document base consistingof the negative training examples the topranked ones are the most similar to thiscentroid, and are then the nearpositives. Wiener et al. 1995 instead equate thenearpositives of ci to the positive examples of the sibling categories of ci, as inthe application they work on TC with hierarchical category sets the notion of asibling category of ci is welldefined. A similar policy is also adopted in Ng et al.1997 Ruiz and Srinivasan 1999 Weigend et al. 1999.By using query zoning plus other enhancements TSR, statistical phrases, anda method called dynamic feedback optimization, Schapire et al. 1998 have foundthat a Rocchio classifier can achieve an effectiveness comparable to that of a stateoftheart ML method such as boosting see Section 6.11.1 while being 60 timesquicker to train. These recent results will no doubt bring about a renewed interestfor the Rocchio classifier, previously considered an underperformer Cohen andSinger 1999 Joachims 1998 Lewis et al. 1996 Schutze et al. 1995 Yang 1999.6.8 Neural networksA neural network NN text classifier is a network of units, where the input unitsrepresent terms, the output units represent the category or categories of interest,and the weights on the edges connecting units represent dependence relations. Forclassifying a test document dj , its term weights wkj are loaded into the input unitsthe activation of these units is propagated forward through the network, and thevalue of the output units determines the categorization decisions. A typicalway of training NNs is backpropagation, whereby the term weights of a trainingdocument are loaded into the input units, and if a misclassification occurs the erroris backpropagated so as to change the parameters of the network and eliminateor minimize the error.The simplest type of NN classifier is the perceptron Dagan et al. 1997 Ng et al.1997, which is a linear classifier and as such has been extensively discussed inSection 6.6. Other types of linear NN classifiers implementing a form of logisticregression have also been proposed and tested by Schutze et al. 1995 and Wieneret al. 1995, yielding very good effectiveness.A nonlinear NN Lam and Lee 1999 Ruiz and Srinivasan 1999 Schutze et al.1995 Weigend et al. 1999 Wiener et al. 1995 Yang and Liu 1999 is instead anetwork with one or more additional layers of units, which in TC usually representhigherorder interactions between terms that the network is able to learn. Whencomparative experiments relating nonlinear NNs to their linear counterparts havebeen performed, the former have yielded either no improvement Schutze et al.32  F. Sebastiani1995 or very small improvements Wiener et al. 1995 over the latter.6.9 Examplebased classifiersExamplebased classifiers do not build an explicit, declarative representation of thecategory ci, but rely on the category labels attached to the training documentssimilar to the test document. These methods have thus been called lazy learners,since they defer the decision on how to generalize beyond the training data untileach new query instance is encountered Mitchell 1996, pag 244.The first application of examplebased methods aka memorybased reasoningmethods to TC is due to Creecy, Masand and colleagues Creecy et al. 1992Masand et al. 1992 examples include Joachims 1998 Lam et al. 1999 Larkey1998 Larkey 1999 Li and Jain 1998 Yang and Pedersen 1997 Yang and Liu 1999.Our presentation of the examplebased approach will be based on the kNN for knearest neighbours algorithm used by Yang 1994. For deciding whether dj  ci,kNN looks at whether the k training documents most similar to dj also are inci if the answer is positive for a large enough proportion of them, a positive decision is taken, and a negative decision is taken otherwise. Actually, Yangs is adistanceweighted version of kNN see e.g. Mitchell 1996, Section 8.2.1, since thefact that a most similar document is in ci is weighted by its similarity with the testdocument. Classifying dj by means of kNN thus comes down to computingCSVidj dz TrkdjRSV dj , dz  dz , ci 9where Trkdj is the set of the k documents dz which maximize RSV dj , dz and 1 if   T0 if   FThe thresholding methods of Section 6.1 can then be used to convert the realvaluedCSVis into binary categorization decisions. In 9, RSV dj , dz represents somemeasure or semantic relatedness between a test document dj and a training document dz  any matching function, be it probabilistic as used in Larkey and Croft1996 or vectorbased as used in Yang 1994, from a ranked IR system may beused for this purpose. The construction of a kNN classifier also involves determining experimentally, on a validation set a threshold k that indicates how many topranked training documents have to be considered for computing CSVidj. Larkeyand Croft 1996 use k  20, while Yang 1994, 1999 has found 30  k  45 to yieldthe best effectiveness. Anyhow, various experiments have shown that increasing thevalue of k does not significantly degrade the performance.Note that kNN, unlike linear classifiers, does not divide the document spacelinearly, hence does not suffer from the problem discussed at the end of Section6.7. This is graphically depicted in Figure 3b, where the more local character ofkNN with respect to Rocchio can be appreciated.This method is naturally geared towards documentpivoted TC, since ranking thetraining documents for their similarity with the test document can be done oncefor all categories. For categorypivoted TC one would need to store the documentranks for each test document, which is obviously clumsy DPC is thus de facto theonly reasonable way to use kNN.Machine Learning in Automated Text Categorization  33A number of different experiments see Section 7.3 have shown kNN to be quiteeffective. However, its most important drawback is its inefficiency at classificationtime while e.g. with a linear classifier only a dot product needs to be computedto classify a test document, kNN requires the entire training set to be rankedfor similarity with the test document, which is much more expensive. This is adrawback of lazy learning methods, since they do not have a true training phaseand thus defer all the computation to classification time.6.9.1 Other examplebased techniques. Various examplebased techniques havebeen used in the TC literature. For example, Cohen and Hirsh 1998 implementan examplebased classifier by extending standard relational DBMS technology withsimilaritybased soft joins. In their Whirl system they use the scoring functionCSVidj  1dz Trkdj1RSV dj , dzdz,cias an alternative to 9, obtaining a small but statistically significant improvementover a version of Whirl using 9. In their experiments this technique outperformeda number of other classifiers, such as a C4.5 decision tree classifier and the RipperCNF rulebased classifier.A variant of the basic kNN approach is proposed by Galavotti et al. 2000, whoreinterpret 9 by redefining  as 1 if   T1 if   FThe difference from the original kNN approach is that if a training documentdz similar to the test document dj does not belong to ci, this information is notdiscarded but weights negatively in the decision to classify dj under ci.A combination of profile and examplebased methods is presented in Lam andHo 1998. In this work a kNN system is fed generalized instances GIs in place oftraining documents. This approach may be seen as the result ofclustering the training set, thus obtaining a set of clusters Ki  ki1, . . . , kiKibuilding a profile Gkiz generalized instance from the documents belongingto cluster kiz by means of some algorithm for learning linear classifiers e.g.Rocchio, WidrowHoffapplying kNN with profiles in place of training documents, i.e. computingCSVidjdefkizKiRSV dj , Gkiz dj  kiz dj , ci  T dj  kizdj  kizTrkizKiRSV dj , Gkiz dj  kiz dj , ci  T Tr10wheredjkiz dj,ciTdjkizrepresents the degree to which Gkiz is a positiveinstance of ci, anddjkizTr represents its weight within the entire process.This exploits the superior effectiveness see Figure 3 of kNN over linear classifierswhile at the same time avoiding the sensitivity of kNN to the presence of outliers34  F. SebastianiFig. 4. Learning support vector classifiers. The small crosses and circles represent positive andnegative training examples, respectively, whereas lines represent decision surfaces. Decision surfacei indicated by the thicker line is, among those shown, the best possible one, as it is the middleelement of the widest set of parallel decision surfaces i.e. its minimum distance to any trainingexample is maximum. Small boxes indicate the support vectors.i.e. positive instances of ci that lie out of the region where most other positiveinstances of ci are located in the training set.6.10 Building classifiers by support vector machinesThe support vector machine SVM method has been introduced in TC by Joachims1998, 1999 and subsequently used in Drucker et al. 1999 Dumais et al. 1998 Dumais and Chen 2000 Klinkenberg and Joachims 2000 Taira and Haruno 1999Yang and Liu 1999. In geometrical terms, it may be seen as the attempt to find,among all the surfaces 1, 2, . . . in T dimensional space that separate the positive from the negative training examples decision surfaces, the i that separatesthe positives from the negatives by the widest possible margin, i.e. such that theseparation property is invariant with respect to the widest possible traslation of i.This idea is best understood in the case in which the positives and the negativesare linearly separable, in which case the decision surfaces are T 1hyperplanes.In the 2dimensional case of Figure 4, various lines may be chosen as decisionsurfaces. The SVM method chooses the middle element from the widest set ofparallel lines, i.e. from the set in which the maximum distance between two elementsin the set is highest. It is noteworthy that this best decision surface is determinedby only a small set of training examples, called the support vectors.The method described is applicable also to the case in which the positives and thenegatives are not linearly separable. Yang and Liu 1999 experimentally comparedthe linear case namely, when the assumption is made that the categories are linearlyseparable with the nonlinear case on a standard benchmark, and obtained slightlybetter results in the former case.As argued by Joachims 1998, SVMs offer two important advantages for TCMachine Learning in Automated Text Categorization  35term selection is often not needed, as SVMs tend to be fairly robust to overfittingand can scale up to considerable dimensionalitiesno human and machine effort in parameter tuning on a validation set is needed, asthere is a theoretically motivated, default choice of parameter settings, whichhas also been shown to provide the best effectiveness.Dumais et al. 1998 have applied a novel algorithm for training SVMs which bringsabout training speeds comparable to computationally easy learners such as Rocchio.6.11 Classifier committeesClassifier committees aka ensembles are based on the idea that, given a task thatrequires expert knowledge to perform, k experts may be better than one if theirindividual judgments are appropriately combined. In TC, the idea is to apply kdifferent classifiers 1, . . . ,k to the same task of deciding whether dj  ci, and thencombine their outcome appropriately. A classifier committee is then characterizedby i a choice of k classifiers, and ii a choice of a combination function.Concerning issue i, it is known from the ML literature that, in order to guarantee good effectiveness, the classifiers forming the committee should be as independent as possible Tumer and Ghosh 1996. The classifiers may differ for the indexingapproach used, or for the inductive method, or both. Within TC, the avenue whichhas been explored most is the latter to our knowledge the only example of theformer is Scott and Matwin 1999.Concerning issue ii, various rules have been tested. The simplest one ismajorityvoting MV, whereby the binary outputs of the k classifiers are pooled together,and the classification decision that reaches the majority of k12 votes is taken kobviously needs to be an odd number Li and Jain 1998 Liere and Tadepalli 1997.This method is particularly suited to the case in which the committee includesclassifiers characterized by a binary decision function CSVi  D  T, F. Asecond rule is weighted linear combination WLC, whereby a weighted sum of theCSVis produced by the k classifiers yields the final CSVi. The weights wj reflectthe expected relative effectiveness of classifiers j , and are usually optimized ona validation set Larkey and Croft 1996. Another policy is dynamic classifierselection DCS, whereby among committee 1, . . . , k the classifier t mosteffective on the l validation examples most similar to dj is selected, and its judgmentadopted by the committee Li and Jain 1998. A still different policy, somehowintermediate between WLC and DCS, is adaptive classifier combination ACC,whereby the judgments of all the classifiers in the committee are summed together,but their individual contribution is weighted by their effectiveness on the l validationexamples most similar to dj Li and Jain 1998.Classifier committees have had mixed results in TC so far. Larkey and Croft1996 have used combinations of Rocchio, Nave Bayes and kNN, all together orin pairwise combinations, using a WLC rule. In their experiments the combinationof any two classifiers outperformed the best individual classifier kNN, and thecombination of the three classifiers improved an all three pairwise combinations.These results would seem to give strong support to the idea that classifier committees can somehow profit from the complementary strengths of their individualmembers. However, the small size of the test set used 187 documents suggests36  F. Sebastianithat more experimentation is needed before conclusions can be drawn.Li and Jain 1998 have tested a committee formed of various combinations ofa Nave Bayes classifier, an examplebased classifier, a decision tree classifier, anda classifier built by means of their own subspace method the combination rulesthey have worked with are MV, DCS and ACC. Only in the case of a committeeformed by Nave Bayes and the subspace classifier combined by means of ACCthe committee has outperformed, and by a narrow margin, the best individualclassifier for every attempted classifier combination ACC gave better results thanMV and DCS. This seems discouraging, especially in the light of the fact thatthe committee approach is computationally expensive its cost trivially amountsto the sum of the costs of the individual classifiers plus the cost incurred for thecomputation of the combination rule. Again, it has to be remarked that the smallsize of their experiment two test sets of less than 700 documents each were useddoes not allow to draw definitive conclusions on the approaches adopted.6.11.1 Boosting. The boosting method Schapire et al. 1998 Schapire and Singer2000 occupies a special place in the classifier committees literature, since the kclassifiers 1, . . . ,k forming the committee are obtained by the same learningmethod here called the weak learner. The key intuition of boosting is that thek classifiers should be trained not in a conceptually parallel and independent way,as in the committees described above, but sequentially. In this way, in trainingclassifier i one may take into account how classifiers 1, . . . ,i1 perform onthe training examples, and concentrate on getting right those examples on which1, . . . ,i1 have performed worst.Specifically, for learning classifier t each dj , ci pair is given an importanceweight htij where h1ij is set to be equal for all dj , ci pairs15, which represents howhard to get a correct decision for this pair was for classifiers 1, . . . ,t1. Theseweights are exploited in learning t, which will be specially tuned to correctly solvethe pairs with higher weight. Classifier t is then applied to the training documents,and as a result weights htij are updated to ht1ij  in this update operation, pairscorrectly classified by t will have their weight decreased, while pairs misclassifiedby t will have their weight increased. After all the k classifiers have been built, aweighted linear combination rule is applied to yield the final committee.In the BoosTexter system Schapire and Singer 2000, two different boostingalgorithms are tested, using a onelevel decision tree weak learner. The formeralgorithm AdaBoost.MH, simply called AdaBoost in Schapire et al. 1998 isexplicitly geared towards the maximization of microaveraged effectiveness, whereasthe latter AdaBoost.MR is aimed at minimizing ranking loss i.e. at getting acorrect category ranking for each individual document. In experiments conductedover three different test collections, Schapire et al. 1998 have shown AdaBoostto outperform Sleeping Experts, a classifier that had proven quite effective inthe experiments of Cohen and Singer 1999. Further experiments by Schapire andSinger 2000 showed AdaBoost to outperform, aside from Sleeping Experts, aNave Bayes classifier, a standard nonenhanced Rocchio classifier, and Joachims15Schapire et al. 1998 also show that a simple modification of this policy allows optimization ofthe classifier based on utility see Section 7.1.3.Machine Learning in Automated Text Categorization  371997 PrTFIDF classifier.A boosting algorithm based on a committee of classifier subcommittees thatimproves on the effectiveness and especially the efficiency of AdaBoost.MHis presented in Sebastiani et al. 2000. An approach similar to boosting is alsoemployed by Weiss et al. 1999, who experiment with committees of decision treeseach having an average of 16 leaves hence much more complex than the simple2leaves trees used in Schapire and Singer 2000, eventually combined by usingthe simple MV rule as a combination rule similarly to boosting, a mechanism foremphasising documents that have been misclassified by previous decision trees isused. Boostingbased approaches have also been employed in Escudero et al. 2000Iyer et al. 2000 Kim et al. 2000 Li and Jain 1998 Myers et al. 2000.6.12 Other methodsAlthough in the previous sections we have tried to give an overview as completeas possible of the learning approaches proposed in the TC literature, it would behardly possible to be exhaustive. Some of the learning approaches adopted donot fall squarely under one or the other class of algorithms, or have remainedsomehow isolated attempts. Among these, the most noteworthy are the ones basedon Bayesian inference networks Dumais et al. 1998 Lam et al. 1997 Tzeras andHartmann 1993, genetic algorithms Clack et al. 1997 Masand 1994, andmaximumentropy modelling Manning and Schutze 1999.7. EVALUATION OF TEXT CLASSIFIERSAs for text search systems, the evaluation of document classifiers is typically conducted experimentally, rather than analytically. The reason is that, in order to evaluate a system analytically e.g. proving that the system is correct and completewe would need a formal specification of the problem that the system is trying tosolve e.g. with respect to what correctness and completeness are defined, and thecentral notion of TC namely, that of membership of a document in a category is,due to its subjective character, inherently nonformalisable.The experimental evaluation of a classifier usually measures its effectivenessrather than its efficiency, i.e. its ability to take the right classification decisions.7.1 Measures of text categorization effectiveness7.1.1 Precision and recall. Classification effectiveness is usually measured in termsof the classic IR notions of precision  and recall , adapted to the case ofTC. Precision wrt ci i is defined as the conditional probability P dx, ci T dx, ci  T , i.e. as the probability that if a random document dx is classified under ci, this decision is correct. Analogously, recall wrt ci i is defined asP dx, ci  T  dx, ci  T , i.e. as the probability that, if a random documentdx ought to be classified under ci, this decision is taken. These categoryrelativevalues may be averaged, in a way to be discussed shortly, to obtain  and , i.e.values global to the entire category set. Borrowing terminology from logic,  maybe viewed as the degree of soundness of the classifier wrt C, while  may beviewed as its degree of completeness wrt C. As defined here, i and i are to beunderstood as subjective probabilities, i.e. as measuring the expectation of the userthat the system will behave correctly when classifying an unseen document under38  F. SebastianiCategory expert judgmentsci YES NOclassifier YES TPi FPijudgments NO FNi TNiTable 2. The contingency table for category ci.Category set expert judgmentsC  c1, . . . , cC YES NOclassifier YES TP Ci1TPi FP Ci1FPijudgments NO FN Ci1FNi TN Ci1TNiTable 3. The global contingency table.ci. These probabilities may be estimated in terms of the contingency table for cion a given test set see Table 2. Here, FPi false positives wrt ci, aka errors ofcommission is the number of test documents incorrectly classified under ci TNitrue negatives wrt ci, TPi true positives wrt ci and FNi false negatives wrt ci,aka errors of omission are defined accordingly. Estimates indicated by carets ofprecision wrt ci and recall wrt ci may thus be obtained asi TPiTPi  FPii TPiTPi  FNiFor obtaining estimates of  and , two different methods may be adoptedmicroaveraging  and  are obtained by summing over all individual decisions TPTP  FPCi1 TPiCi1TPi  FPi TPTP  FNCi1 TPiCi1TPi  FNiwhere  indicates microaveraging. The global contingency table Table 3 isthus obtained by summing over categoryspecific contingency tables.macroaveraging  precision and recall are first evaluated locally for each category, and then globally by averaging over the results of the different categoriesM Ci1 iCM Ci1 iCwhere M indicates macroaveraging.These two methods may give quite different results, especially if the different categories have very different generality. For instance, the ability of a classifier tobehave well also on categories with low generality i.e. categories with few positive training instances will be emphasized by macroaveraging and much less so byMachine Learning in Automated Text Categorization  39microaveraging. Whether one or the other should be used obviously depends onthe application requirements. From now on, we will assume that microaveraging isused everything we will say in the rest of Section 7 may be adapted to the case ofmacroaveraging in the obvious way.7.1.2 Other measures of effectiveness. Measures alternative to  and  and commonly used in the ML literature, such as accuracy estimated as A  TPTNTPTNFPFN and error estimated as E  FPFNTPTNFPFN  1 A, are not widely used in TC.The reason is that, as Yang 1999 points out, the large value that their denominator typically has in TC makes them much more insensitive to variations in thenumber of correct decisions TP  TN than  and . Besides, if A is the adoptedevaluation measure, in the frequent case of a very low average generality the trivialrejector i.e. the classifier  such that dj , ci  F for all dj and ci tends tooutperform all nontrivial classifiers see also Cohen 1995a, Section 2.3. If A isadopted, parameter tuning on a validation set may thus result in parameter choicesthat make the classifier behave very much like the trivial rejector.A nonstandard effectiveness measure is proposed by Sable and Hatzivassiloglou2000, Section 7, who suggest to base  and  not on absolute values of successand failure i.e. 1 if dj , ci  dj , ci and 0 if dj , ci 6 dj , ci, but on valuesof relative success i.e. CSVidj if dj , ci  T and 1  CSVidj if dj , ci F . This means that for a correct resp. wrong decision the classifier is rewardedresp. penalized proportionally to its confidence in the decision. This proposedmeasure does not reward the choice of a good thresholding policy, and is thus unfitfor autonomous hard classification systems. However, it might be appropriatefor interactive ranking classifiers of the type used in Larkey 1999, where theconfidence that the classifier has in its own decision influences category rankingand, as a consequence, the overall usefulness of the system.7.1.3 Measures alternative to effectiveness. In general, criteria different from effectiveness are seldom used in classifier evaluation. For instance, efficiency, although very important for applicative purposes, is seldom used as the sole yardstick,due to the volatility of the parameters on which the evaluation rests. However,efficiency may be useful for choosing among classifiers with similar effectiveness.An interesting evaluation has been carried out by Dumais et al. 1998, who havecompared five different learning methods along three different dimensions, namelyeffectiveness, training efficiency i.e. the average time it takes to build a classifierfor category ci from a training set Tr, and classification efficiency i.e. the averagetime it takes to classify a new document dj under category ci.An important alternative to effectiveness is utility, a class of measures from decision theory that extend effectiveness by economic criteria such as gain or loss.Utility is based on a utility matrix such as that of Table 4, where the numeric values uTP , uFP , uFN and uTN represent the gain brought about by a true positive,false positive, false negative and true negative, respectively both uTP and uTN aregreater than both uFP and uFN . Standard effectiveness is a special case of utility, i.e. the one in which uTP  uTN  uFP  uFN . Less trivial cases are those inwhich uTP 6 uTN andor uFP 6 uFN  this is the case e.g. in spam filtering, wherefailing to discard a piece of junk mail FP is a less serious mistake than discarding40  F. SebastianiCategory set expert judgmentsC  c1, . . . , cC YES NOclassifier YES uTP uFPjudgments NO uFN uTNTable 4. The utility matrix.a legitimate message FN Androutsopoulos et al. 2000. If the classifier outputsprobability estimates of the membership of dj in ci, then decision theory providesanalytical methods to determine thresholds i, thus avoiding the need to determinethem experimentally as discussed in Section 6.1. Specifically, as Lewis 1995areminds, the expected value of utility is maximized wheni uFP  uTNuFN  uTP   uFP  uTNwhich, in the case of standard effectiveness, is equal to 12 .The use of utility in TC is discussed in detail by Lewis 1995a. Other workswhere utility is employed are Amati and Crestani 1999 Cohen and Singer 1999Hull et al. 1996 Lewis and Catlett 1994 Schapire et al. 1998. Utility has becomepopular within the text filtering community, and the TREC filtering track evaluations have been using it since long Lewis 1995c. The values of the utility matrixare extremely applicationdependent. This means that if utility is used instead ofpure effectiveness, there is a further element of difficulty in the crosscomparisonof classification systems see Section 7.3, since for two classifiers to be experimentally comparable also the two utility matrices must be the same.Other effectiveness measures different from the ones discussed here have occasionally been used in the literature these include adjacent score Larkey 1998,coverage Schapire and Singer 2000, oneerror Schapire and Singer 2000, Pearsonproductmoment correlation Larkey 1998, recall at n Larkey and Croft 1996, topcandidate Larkey and Croft 1996, top n Larkey and Croft 1996. We will notattempt to discuss them in detail. However, their use shows that, although the TCcommunity is making consistent efforts at standardising experimentation protocols,we are still far from universal agreement on evaluation issues and, as a consequence,from understanding precisely the relative merits of the various methods.7.1.4 Combined effectiveness measures. Neither precision nor recall make sensein isolation of each other. In fact the classifier  such that dj , ci  T for alldj and ci the trivial acceptor has   1. When the CSVi function has values in0, 1 one only needs to set every threshold i to 0 to obtain the trivial acceptor. Inthis case  would usually be very low more precisely, equal to the average test setgeneralityCi1gT eciC 16. Conversely, it is wellknown from everyday IR practicethat higher levels of  may be obtained at the price of low values of .16From this one might be tempted to infer, by symmetry, that the trivial rejector always has  1. This is false, as  is undefined the denominator is zero for the trivial rejector see Table5. In fact, it is clear from its definition   TPTPFP that  depends only on how the positivesTP FP  are split between true positives TP and the false positives FP , and does not dependMachine Learning in Automated Text Categorization  41Precision Recall Cprecision CrecallTPTP  FPTPTP  FNTNFP  TNTNTN  FNTrivial Rejector TPFP0 undefined0FN 0TNTN 1TNTN  FNTrivial Acceptor FNTN0TPTP  FPTPTP 10FP 0 undefinedTrivial Yes Collection FPTN0TPTP 1TPTP  FNundefined0FN 0Trivial No Collection TPFN00FP 0 undefinedTNFP  TNTNTN 1Table 5. Trivial cases in TC.In practice, by tuning i a function CSVi  D  T, F is tuned to be, in thewords of Riloff and Lehnert 1994, more liberal i.e. improving i to the detrimentof i or more conservative improving i to the detriment of i17. A classifiershould thus be evaluated by means of a measure which combines  and 18. Varioussuch measures have been proposed, among which the most frequent are1 11point average precision threshold i is repeatedly tuned so as to allow ito take up values of 0.0, .1, . . . , .9, 1.0 i is computed for these 11 differentvalues of i, and averaged over the 11 resulting values. This is analogous to thestandard evaluation methodology for ranked IR systems, and may be useda with categories in place of IR queries. This is most frequently used fordocumentranking classifiers see e.g Schutze et al. 1995 Yang 1994 Yang1999 Yang and Pedersen 1997b with test documents in place of IR queries and categories in place of documents. This is most frequently used for categoryranking classifiers seee.g. Lam et al. 1999 Larkey and Croft 1996 Schapire and Singer 2000Wiener et al. 1995. In this case if macroaveraging is used it needs to beredefined on a perdocument, rather than percategory basis.This measure does not make sense for binaryvalued CSVi functions, since inthis case i may not be varied at will.at all on the cardinality of the positives. There is a breakup of symmetry between  and here because, from the point of view of classifier judgment positives vs. negatives this is thedichotomy of interest in trivial acceptor vs. trivial rejector the symmetric of   TPTPFN is not  TPTPFP but cprecision c  TNFPTN, the contrapositive of . In fact, while 1 andc0 for the trivial acceptor, c1 and 0 for the trivial rejector.17While i can always be increased at will by lowering i, usually at the cost of decreasing i,i can usually be increased at will by raising i, always at the cost of decreasing i. This kind oftuning is only possible for CSVi functions with values in 0, 1 for binaryvalued CSVi functionstuning is not always possible, or is anyway more difficult see e.g. Weiss et al. 1999, page 66.18An exception is singlelabel TC, in which  and  are not independent of each other if adocument dj has been classified under a wrong category cs thus decreasing s this also meansthat it has not been classified under the right category ct thus decreasing t. In this case either or  can be used as a measure of effectiveness.42  F. Sebastiani2 the breakeven point, i.e. the value at which  equals  e.g. Apte et al. 1994Cohen and Singer 1999 Dagan et al. 1997 Joachims 1998 Joachims 1999Lewis 1992a Lewis and Ringuette 1994 Moulinier and Ganascia 1996 Nget al. 1997 Yang 1999. This is obtained by a process analogous to the oneused for 11point average precision a plot of  as a function of  is computedby repeatedly varying the thresholds i breakeven is the value of  or  forwhich the plot intersects the    line. This idea relies on the fact that bydecreasing the is from 1 to 0,  always increases monotonically from 0 to 1and  usually decreases monotonically from a value near 1 to 1CCi1 gTeci.If for no values of the is  and  are exactly equal, the is are set to thevalue for which  and  are closest, and an interpolated breakeven is computedas the average of the values of  and 19.3 the F function van Rijsbergen 1979, Chapter 7, for some 0     e.g.Cohen 1995a Cohen and Singer 1999 Lewis and Gale 1994 Lewis 1995aMoulinier et al. 1996 Ruiz and Srinivasan 1999, whereF 2  12  Here  may be seen as the relative degree of importance attributed to  and .If   0 then F coincides with , whereas if    then F coincides with. Usually, a value   1 is used, which attributes equal importance to  and. As shown in Moulinier et al. 1996 Yang 1999, the breakeven of a classifier is always less or equal than its F1 value.Once an effectiveness measure is chosen, a classifier can be tuned e.g. thresholdsand other parameters can be set so that the resulting effectiveness is the bestachievable by that classifier. Tuning a parameter p be it a threshold or otheris normally done experimentally. This means performing repeated experiments onthe validation set with the values of the other parameters pk fixed at a defaultvalue, in the case of a yettobetuned parameter pk, or at the chosen value, if theparameter pk has already been tuned and with different values for parameter p.The value that has yielded the best effectiveness is chosen for p.7.2 Benchmarks for text categorizationStandard benchmark collections that can be used as initial corpora for TC arepublically available for experimental purposes. The most widely used is the Reuterscollection, consisting of a set of newswire stories classified under categories relatedto economics. The Reuters collection accounts for most of the experimental work inTC so far. Unfortunately, this does not always translate into reliable comparative19Breakeven, first proposed by Lewis 1992a, 1992b, has been recently criticized. Lewis himselfsee his message of 11 Sep 1997 104901 to the DDLBETA text categorization mailing list  quotedwith permission of the author points out that breakeven is not a good effectiveness measure, sincei there may be no parameter setting that yields the breakeven in this case the final breakevenvalue, obtained by interpolation, is artificial ii to have  equal  is not necessarily desirable,and it is not clear that a system that achieves high breakeven can be tuned to score high on othereffectiveness measures. Yang 1999 also notes that when for no value of the parameters  and are close enough, interpolated breakeven may not be a reliable indicator of effectiveness.Machine Learning in Automated Text Categorization  43results, in the sense that many of these experiments have been carried out in subtlydifferent conditions.In general, different sets of experiments may be used for crossclassifier comparison only if the experiments have been performed1 on exactly the same collection i.e. same documents and same categories2 with the same split between training set and test set3 with the same evaluation measure and, whenever this measure depends on someparameters e.g. the utility matrix chosen, with the same parameter values.Unfortunately, a lot of experimentation, both on Reuters and on other collections,has not been performed with these caveat in mind by testing three different classifiers on five popular versions of Reuters, Yang 1999 has shown that a lack ofcompliance with these three conditions may make the experimental results hardlycomparable among each other. Table 6 lists the results of all experiments knownto us performed on five major versions of the Reuters benchmark Reuters22173ModLewis column 1, Reuters22173 ModApte column 2, Reuters22173ModWiener column 3, Reuters21578 ModApte column 4 and Reuters2157810 ModApte column 520. Only experiments that have computed eithera breakeven or F1 have been listed, since other less popular effectiveness measuresdo not readily compare with these.Note that only results belonging to the same column are directly comparable.In particular, Yang 1999 showed that experiments carried out on Reuters22173ModLewis column 1 are not directly comparable with those using the otherthree versions, since the former strangely includes a significant percentage 58of unlabelled test documents which, being negative examples of all categories,tend to depress effectiveness. Also, experiments performed on Reuters2157810ModApte column 5 are not comparable with the others, since this collectionis the restriction of Reuters21578 ModApte to the 10 categories with the highestgenerality, and is thus an obviously easier collection.Other test collections that have been frequently used arethe OHSUMED collection, set up by Hersh et al. 1994 and used in Joachims1998 Lam and Ho 1998 Lam et al. 1999 Lewis et al. 1996 Ruiz and Srinivasan 1999 Yang and Pedersen 199721. The documents are titles or titleplusabstracts from medical journals OHSUMED is actually a subset of the Medlinedocument base the categories are the postable terms of the MESH thesaurus.the 20 Newsgroups collection, set up by Lang 1995 and used in Baker andMcCallum 1998 Joachims 1997 McCallum and Nigam 1998 McCallum et al.1998 Nigam et al. 2000 Schapire and Singer 2000. The documents are messagesposted to Usenet newsgroups, and the categories are the newsgroups themselves.20 The Reuters21578 collection may be freely downloaded for experimentation purposes fromhttpwww.research.att.comlewisreuters21578.html and is now considered the standardvariant of Reuters. We do not cover experiments performed on variants of Reuters different from thefive listed because the small number of authors that have used the same variant makes the reportedresults difficult to interpret. This includes experiments performed on the original Reuters22173ModHayes Hayes et al. 1990 and Reuters21578 ModLewis Cohen and Singer 1999.21The OHSUMED collection may be freely downloaded for experimentation purposes fromftpmedir.ohsu.edupubohsumed44  F. Sebastiani1 2 3 4 5 of documents 21,450 14,347 13,272 12,902 12,902 of training documents 14,704 10,667 9,610 9,603 9,603 of test documents 6,746 3,680 3,662 3,299 3,299 of categories 135 93 92 90 10System Type Results reported byWord nonlearning Yang 1999 .150 .310 .290probabilistic Dumais et al. 1998 .752 .815probabilistic Joachims 1998 .720probabilistic Lam et al. 1997 .443 MF1PropBayes probabilistic Lewis 1992a .650Bim probabilistic Li and Yamanishi 1999 .747probabilistic Li and Yamanishi 1999 .773Nb probabilistic Yang and Liu 1999 .795decision trees Dumais et al. 1998 .884C4.5 decision trees Joachims 1998 .794Ind decision trees Lewis and Ringuette 1994 .670Swap1 decision rules Apte et al. 1994 .805Ripper decision rules Cohen and Singer 1999 .683 .811 .820SleepingExperts decision rules Cohen and Singer 1999 .753 .759 .827DlEsc decision rules Li and Yamanishi 1999 .820Charade decision rules Moulinier and Ganascia 1996 .738Charade decision rules Moulinier et al. 1996 .783 F1Llsf regression Yang 1999 .855 .810Llsf regression Yang and Liu 1999 .849BalancedWinnow online linear Dagan et al. 1997 .747 M .833 MWidrowHoff online linear Lam and Ho 1998 .822Rocchio batch linear Cohen and Singer 1999 .660 .748 .776FindSim batch linear Dumais et al. 1998 .617 .646Rocchio batch linear Joachims 1998 .799Rocchio batch linear Lam and Ho 1998 .781Rocchio batch linear Li and Yamanishi 1999 .625Classi neural network Ng et al. 1997 .802Nnet neural network Yang and Liu 1999 .838neural network Wiener et al. 1995 .820GisW examplebased Lam and Ho 1998 .860kNN examplebased Joachims 1998 .823kNN examplebased Lam and Ho 1998 .820kNN examplebased Yang 1999 .690 .852 .820kNN examplebased Yang and Liu 1999 .856SVM Dumais et al. 1998 .870 .920SvmLight SVM Joachims 1998 .864SvmLight SVM Li and Yamanishi 1999 .841SvmLight SVM Yang and Liu 1999 .859AdaBoost.MH committee Schapire and Singer 2000 .860committee Weiss et al. 1999 .878Bayesian net Dumais et al. 1998 .800 .850Bayesian net Lam et al. 1997 .542 MF1Table 6. Comparative results among different classifiers obtained on five different version ofReuters. Unless otherwise noted, entries indicate the microaveraged breakeven point withinparentheses, M indicates macroaveraging and F1 indicates use of the F1 measure. Boldface indicates the best performer on the collection.the AP collection, used in Cohen 1995a Cohen 1995b Cohen and Singer 1999Lewis and Catlett 1994 Lewis and Gale 1994 Lewis et al. 1996 Schapire andSinger 2000 Schapire et al. 1998.We will not cover the experiments performed on these collections for the samereasons as those illustrated in Footnote 20, i.e. because in no case a significantenough number of authors have used the same collection in the same experimentalconditions, thus making comparisons difficult.7.3 Which text classifier is bestThe published experimental results, and especially those listed in Table 6, allow usto attempt some considerations on the comparative performance of the TC methodsdiscussed. However, we have to bear in mind that comparisons are reliable onlywhen based on experiments performed by the same author under carefully controlled conditions. They are instead more problematic when they involve differentexperiments performed by different authors. In this case various background conMachine Learning in Automated Text Categorization  45ditions, often extraneous to the learning algorithm itself, may influence the results.These may include, among others, different choices in preprocessing stemming,etc., indexing, dimensionality reduction, classifier parameter values, etc., but alsodifferent standards of compliance with safe scientific practice such as tuning parameters on the test set rather than on a separate validation set, which often arenot discussed in the published papers.Two different methods may thus be applied for comparing classifiers Yang 1999direct comparison classifiers  and  may be compared when they have beentested on the same collection , usually by the same researchers and with thesame background conditions. This is the more reliable method.indirect comparison classifiers  and  may be compared when1 they have been tested on collections  and , respectively, typically bydifferent researchers and hence with possibly different background conditions2 one or more baseline classifiers 1, . . . ,m have been tested on both and  by the direct comparison method.Test 2 gives an indication on the relative hardness of  and  using this andthe results from Test 1 we may obtain an indication on the relative effectivenessof  and . For the reasons discussed above, this method is less reliable.A number of interesting conclusions can be drawn from Table 6 by using these twomethods. Concerning the relative hardness of the five collections, if by   we indicate that  is a harder collection that , there seems to be enoughevidence that Reuters22173 ModLewis Reuters22173 ModWiener  Reuters22173 ModApte  Reuters21578 ModApte  Reuters2157810 ModApte.These facts are unsurprising in particular, the first and the last inequalities are adirect consequence of the peculiar characteristics of Reuters22173 ModLewis andReuters2157810 ModApte discussed in Section 7.2.Concerning the relative performance of the classifiers, remembering the considerations above we may attempt a few conclusionsBoostingbased classifier committees, support vector machines, examplebasedmethods, and regression methods deliver topnotch performance. There seems tobe no sufficient evidence to decidedly opt for either method efficiency considerations or applicationdependent issues might play a role in breaking the tie.Neural networks and online linear classifiers work very well, although slightlyworse than the previously mentioned methods.Batch linear classifiers Rocchio and probabilistic Nave Bayes classifiers lookthe worst of the learningbased classifiers. For Rocchio, these results confirmearlier results by Schutze et al. 1995, who had found three classifiers based onlinear discriminant analysis, linear regression, and neural networks, to performabout 15 better than Rocchio. However, recent results by Schapire et al. 1998rank Rocchio along the best performers once nearpositives are used in training.The data in Table 6 are hardly sufficient to say anything about decision trees.However, the work by Dumais et al. 1998 in which a decision tree classifier wasshown to perform nearly as well as their top performing system a SVM classifierwill probably renew the interest in decision trees, an interest that had dwindled46  F. Sebastianiafter the unimpressive results reported in earlier literature Cohen and Singer1999 Joachims 1998 Lewis and Catlett 1994 Lewis and Ringuette 1994.By far the lowest performance is displayed by Word, a classifier implementedby Yang 1999 and not including any learning component22.Concerning Word and nolearning classifiers, for completeness we should recallthat one of the highest effectiveness values reported in the literature for the Reuterscollection a .90 breakeven belongs to Construe, a manually constructed classifier. However, this classifier has never been tested on the standard variants ofReuters mentioned in Table 6, and it is not clear Yang 1999 whether the smalltest set of Reuters22173 ModHayes on which the .90 breakeven value was obtained was chosen randomly, as safe scientific practice would demand. Therefore,the fact that this figure is indicative of the performance of Construe, and of themanual approach it represents, has been convincingly questioned Yang 1999.It is important to bear in mind that the considerations above are not absolutestatements if there may be any on the comparative effectiveness of these TCmethods. One of the reasons is that a particular applicative context may exhibitvery different characteristics from the ones to be found in Reuters, and differentclassifiers may respond differently to these characteristics. An experimental studyby Joachims 1998 involving support vector machines, kNN, decision trees, Rocchio and Nave Bayes, showed all these classifiers to have similar effectiveness oncategories with  300 positive training examples each. The fact that this experiment involved the methods which have scored best support vector machines, kNNand worst Rocchio and Nave Bayes according to Table 6 shows that applicativecontexts different from Reuters may well invalidate conclusions drawn on this latter.Finally, a note is worth about statistical significance testing. Few authors havegone to the trouble of validating their results by means of such tests. These testsare useful for verifying how strongly the experimental results support the claim thata given system  is better than another system , or for verifying how much adifference in the experimental setup affects the measured effectiveness of a system. Hull 1994 and Schutze et al. 1995 have been among the first to work in thisdirection, validating their results by means of the Anova test and the Friedmantest the former is aimed at determining the significance of the difference in effectiveness between two methods in terms of the ratio between this difference and theeffectiveness variability across categories, while the latter conducts a similar test byusing instead the rank positions of each method within a category. Yang and Liu1999 define a full suite of significance tests, some of which apply to microaveragedand some to macroaveraged effectiveness. They apply them systematically to thecomparison between five different classifiers, and are thus able to infer finegrainedconclusions about their relative effectiveness. For other examples of significancetesting in TC see Cohen 1995a Cohen 1995b Cohen and Hirsh 1998 Joachims1997 Koller and Sahami 1997 Lewis et al. 1996 Wiener et al. 1995.22Word is based on the comparison between documents and category names, each treated as avector of weighted terms in the vector space model. Word was implemented by Yang with theonly purpose of determining the difference in effectiveness that adding a learning component toa classifier brings about. Word is actually called STR in Yang 1994 Yang and Chute 1994.Another nolearning classifier is proposed in Wong et al. 1996.Machine Learning in Automated Text Categorization  478. CONCLUSIONAutomated TC is now a major research area within the information systems discipline, thanks to a number of factorsIts domains of application are numerous and important, and given the proliferation of documents in digital form they are bound to increase dramatically inboth number and importance.It is indispensable in many applications in which the sheer number of the documents to be classified and the short response time required by the applicationmake the manual alternative implausible.It can improve the productivity of human classifiers in applications in which noclassification decision can be taken without a final human judgment Larkey andCroft 1996, by providing tools that quickly suggest plausible decisions.It has reached effectiveness levels comparable to those of trained professionals.The effectiveness of manual TC is not 100 anyway Cleverdon 1984 and, moreimportantly, it is unlikely to be improved substantially by the progress of research.The levels of effectiveness of automated TC are instead growing at a steady pace,and even if they will likely reach a plateau well below the 100 level, this plateauwill probably be higher that the effectiveness levels of manual TC.One of the reasons why from the early 90s the effectiveness of text classifiers hasdramatically improved, is the arrival in the TC arena of ML methods that arebacked by strong theoretical motivations. Examples of these are multiplicativeweight updating e.g. the Winnow family, WidrowHoff, etc., adaptive resampling e.g. boosting and support vector machines, which provide a sharp contrastwith relatively unsophisticated and weak methods such as Rocchio. In TC, MLresearchers have found a challenging application, since datasets consisting of hundreds of thousands of documents and characterized by tens of thousands of termsare widely available. This means that TC is a good benchmark for checking whethera given learning technique can scale up to substantial sizes. In turn, this probablymeans that the active involvement of the ML community in TC is bound to grow.The success story of automated TC is also going to encourage an extension of itsmethods and techniques to neighbouring fields of application. Techniques typicalof automated TC have already been extended successfully to the categorization ofdocuments expressed in slightly different media for instancevery noisy text resulting from optical character recognition Ittner et al. 1995Junker and Hoch 1998. In their experiments Ittner et al. 1995 have found that,by employing noisy texts also in the training phase i.e. texts affected by the samesource of noise that is also at work in the test documents, effectiveness levelscomparable to those obtainable in the case of standard text can be achieved.speech transcripts Myers et al. 2000 Schapire and Singer 2000. For instance,Schapire and Singer 2000 classify answers given to a phone operators requestHow may I help you, so as to be able to route the call to a specialized operator according to call type.Concerning other more radically different media, the situation is not as bright however, see Lim 1999 for an interesting attempt at image categorization based on a48  F. Sebastianitextual metaphor. The reason for this is that capturing real semantic content ofnontextual media by automatic indexing is still an open problem. While there aresystems that attempt to detect content e.g. in images by recognising shapes, colourdistributions and texture, the general problem of image semantics is still unsolved.The main reason is that natural language, the language of the text medium, admits far fewer variations than the languages employed by the other media. Forinstance, while the concept of a house can be triggered by relatively few naturallanguage expressions such as house, houses, home, housing, inhabiting, etc., itcan be triggered by far more images the images of all the different houses thatexist, of all possible colours and shapes, viewed from all possible perspectives, fromall possible distances, etc. If we had solved the multimedia indexing problem ina satisfactory way, the general methodology that we have discussed in this paperfor text would also apply to automated multimedia categorization, and there arereasons to believe that the effectiveness levels could be as high. This only adds tothe common sentiment that more research in automated contentbased indexing formultimedia documents is needed.AcknowledgementsThis paper owes a lot to the suggestions and constructive criticism of Norbert Fuhrand David Lewis. Thanks also to Umberto Straccia for comments on an earlierdraft and to Alessandro Sperduti for many fruitful discussions.REFERENCESAmati, G. and Crestani, F. 1999. Probabilistic learning for selective dissemination ofinformation. Information Processing and Management 35, 5, 633654.Androutsopoulos, I., Koutsias, J., Chandrinos, K. V., and Spyropoulos, C. D. 2000.An experimental comparison of naive Bayesian and keywordbased antispam filtering withpersonal email messages. In Proceedings of SIGIR00, 23rd ACM International Conferenceon Research and Development in Information Retrieval Athens, GR, 2000, pp. 160167.Apte, C., Damerau, F. J., and Weiss, S. M. 1994. Automated learning of decision rulesfor text categorization. ACM Transactions on Information Systems 12, 3, 233251.Attardi, G., Di Marco, S., and Salvi, D. 1998. Categorization by context. Journal ofUniversal Computer Science 4, 9, 719736.Baker, L. D. and McCallum, A. K. 1998. Distributional clustering of words for textclassification. In Proceedings of SIGIR98, 21st ACM International Conference on Researchand Development in Information Retrieval Melbourne, AU, 1998, pp. 96103.Belkin, N. J. and Croft, W. B. 1992. Information filtering and information retrieval twosides of the same coin Communications of the ACM 35, 12, 2938.Biebricher, P., Fuhr, N., Knorz, G., Lustig, G., and Schwantner, M. 1988. Theautomatic indexing system AIRPHYS. From research to application. In Proceedings ofSIGIR88, 11th ACM International Conference on Research and Development in Information Retrieval Grenoble, FR, 1988, pp. 333342. Also reprinted in Sparck Jones andWillett 1997, pp. 513517.Borko, H. and Bernick, M. 1963. Automatic document classification. Journal of theAssociation for Computing Machinery 10, 2, 151161.Caropreso, M. F., Matwin, S., and Sebastiani, F. 2001. A learnerindependent evaluation of the usefulness of statistical phrases for automated text categorization. In A. G.Chin Ed., Text Databases and Document Management Theory and Practice. Hershey, USIdea Group Publishing. Forthcoming.Machine Learning in Automated Text Categorization  49Cavnar, W. B. and Trenkle, J. M. 1994. Ngrambased text categorization. In Proceedings of SDAIR94, 3rd Annual Symposium on Document Analysis and InformationRetrieval Las Vegas, US, 1994, pp. 161175.Chakrabarti, S., Dom, B. E., Agrawal, R., and Raghavan, P. 1998a. Scalable featureselection, classification and signature generation for organizing large text databases intohierarchical topic taxonomies. Journal of Very Large Data Bases 7, 3, 163178.Chakrabarti, S., Dom, B. E., and Indyk, P. 1998b. Enhanced hypertext categorizationusing hyperlinks. In Proceedings of SIGMOD98, ACM International Conference on Management of Data Seattle, US, 1998, pp. 307318.Clack, C., Farringdon, J., Lidwell, P., and Yu, T. 1997. Autonomous document classification for business. In Proceedings of the 1st International Conference on AutonomousAgents Marina del Rey, US, 1997, pp. 201208.Cleverdon, C. 1984. Optimizing convenient online access to bibliographic databases. Information Services and Use 4, 1, 3747. Also reprinted in Willett 1988, pp. 3241.Cohen, W. W. 1995a. Learning to classify English text with ILP methods. In L. De RaedtEd., Advances in inductive logic programming , pp. 124143. Amsterdam, NL IOS Press.Cohen, W. W. 1995b. Text categorization and relational learning. In Proceedings of ICML95, 12th International Conference on Machine Learning Lake Tahoe, US, 1995, pp. 124132.Cohen, W. W. and Hirsh, H. 1998. Joins that generalize text classification using Whirl.In Proceedings of KDD98, 4th International Conference on Knowledge Discovery andData Mining New York, US, 1998, pp. 169173.Cohen, W. W. and Singer, Y. 1999. Contextsensitive learning methods for text categorization. ACM Transactions on Information Systems 17, 2, 141173.Cooper, W. S. 1995. Some inconsistencies and misnomers in probabilistic information retrieval. ACM Transactions on Information Systems 13, 1, 100111.Creecy, R. M., Masand, B. M., Smith, S. J., and Waltz, D. L. 1992. Trading MIPS andmemory for knowledge engineering classifying census returns on the Connection Machine.Communications of the ACM 35, 8, 4863.Crestani, F., Lalmas, M., van Rijsbergen, C. J., and Campbell, I. 1998. Is thisdocument relevant . . . probably. A survey of probabilistic models in information retrieval.ACM Computing Surveys 30, 4, 528552.Dagan, I., Karov, Y., and Roth, D. 1997. Mistakedriven learning in text categorization.In Proceedings of EMNLP97, 2nd Conference on Empirical Methods in Natural LanguageProcessing Providence, US, 1997, pp. 5563.Deerwester, S., Dumais, S. T., Furnas, G. W., Landauer, T. K., and Harshman, R.1990. Indexing by latent semantic indexing. Journal of the American Society for Information Science 41, 6, 391407.Denoyer, L., Zaragoza, H., and Gallinari, P. 2001. HMMbased passage models fordocument classification and ranking. In Proceedings of ECIR01, 23rd European Colloquiumon Information Retrieval Research Darmstadt, DE, 2001.Daz Esteban, A., de Buenaga Rodrguez, M., Urena Lopez, L. A., and Garca Vega, M.1998. Integrating linguistic resources in an uniform way for text classification tasks. InProceedings of LREC98, 1st International Conference on Language Resources and Evaluation Grenada, ES, 1998, pp. 11971204.Domingos, P. and Pazzani, M. J. 1997. On the the optimality of the simple Bayesianclassifier under zeroone loss. Machine Learning 29, 23, 103130.Drucker, H., Vapnik, V., and Wu, D. 1999. Automatic text categorization and its applications to text retrieval. IEEE Transactions on Neural Networks 10, 5, 10481054.Dumais, S. T. and Chen, H. 2000. Hierarchical classification of Web content. In Proceedings of SIGIR00, 23rd ACM International Conference on Research and Development inInformation Retrieval Athens, GR, 2000, pp. 256263.Dumais, S. T., Platt, J., Heckerman, D., and Sahami, M. 1998. Inductive learningalgorithms and representations for text categorization. In Proceedings of CIKM98, 7th50  F. SebastianiACM International Conference on Information and Knowledge Management Bethesda,US, 1998, pp. 148155.Escudero, G., Marquez, L., and Rigau, G. 2000. Boosting applied to word sense disambiguation. In Proceedings of ECML00, 11th European Conference on Machine LearningBarcelona, ES, 2000, pp. 129141.Field, B. 1975. Towards automatic indexing automatic assignment of controlledlanguageindexing and classification from free indexing. Journal of Documentation 31, 4, 246265.Forsyth, R. S. 1999. New directions in text categorization. In A. Gammerman Ed., Causalmodels and intelligent data management , pp. 151185. Heidelberg, DE Springer.Frasconi, P., Soda, G., and Vullo, A. 2001. Text categorization for multipage documents A hybrid naive Bayes HMM approach. Journal of Intelligent Information Systems.Forthcoming.Fuhr, N. 1985. A probabilistic model of dictionarybased automatic indexing. In Proceedings of RIAO85, 1st International Conference Recherche dInformation Assistee par Ordinateur Grenoble, FR, 1985, pp. 207216.Fuhr, N. 1989. Models for retrieval with probabilistic indexing. Information Processingand Management 25, 1, 5572.Fuhr, N. and Buckley, C. 1991. A probabilistic learning approach for document indexing.ACM Transactions on Information Systems 9, 3, 223248.Fuhr, N., Hartmann, S., Knorz, G., Lustig, G., Schwantner, M., and Tzeras, K. 1991.AIRX  a rulebased multistage indexing system for large subject fields. In Proceedingsof RIAO91, 3rd International Conference Recherche dInformation Assistee par Ordinateur Barcelona, ES, 1991, pp. 606623.Fuhr, N. and Knorz, G. 1984. Retrieval test evaluation of a rulebased automated indexingAIRPHYS. In Proceedings of SIGIR84, 7th ACM International Conference on Researchand Development in Information Retrieval Cambridge, UK, 1984, pp. 391408.Fuhr, N. and Pfeifer, U. 1994. Probabilistic information retrieval as combination of abstraction inductive learning and probabilistic assumptions. ACM Transactions on Information Systems 12, 1, 92115.Furnkranz, J. 1999. Exploiting structural information for text classification on the WWW.In Proceedings of IDA99, 3rd Symposium on Intelligent Data Analysis Amsterdam, NL,1999, pp. 487497.Galavotti, L., Sebastiani, F., and Simi, M. 2000. Experiments on the use of featureselection and negative evidence in automated text categorization. In Proceedings of ECDL00, 4th European Conference on Research and Advanced Technology for Digital LibrariesLisbon, PT, 2000, pp. 5968.Gale, W. A., Church, K. W., and Yarowsky, D. 1993. A method for disambiguatingword senses in a large corpus. Computers and the Humanities 26, 5, 415439.Govert, N., Lalmas, M., and Fuhr, N. 1999. A probabilistic descriptionoriented approach for categorising Web documents. In Proceedings of CIKM99, 8th ACM International Conference on Information and Knowledge Management Kansas City, US, 1999,pp. 475482.Gray, W. A. and Harley, A. J. 1971. Computerassisted indexing. Information Storageand Retrieval 7, 4, 167174.Guthrie, L., Walker, E., and Guthrie, J. A. 1994. Document classification by machine theory and practice. In Proceedings of COLING94, 15th International Conferenceon Computational Linguistics Kyoto, JP, 1994, pp. 10591063.Hayes, P. J., Andersen, P. M., Nirenburg, I. B., and Schmandt, L. M. 1990. Tcs ashell for contentbased text categorization. In Proceedings of CAIA90, 6th IEEE Conference on Artificial Intelligence Applications Santa Barbara, US, 1990, pp. 320326.Heaps, H. 1973. A theory of relevance for automatic document classification. Informationand Control 22, 3, 268278.Hersh, W., Buckley, C., Leone, T., and Hickman, D. 1994. Ohsumed an interactiveretrieval evaluation and new large text collection for research. In Proceedings of SIGIRMachine Learning in Automated Text Categorization  5194, 17th ACM International Conference on Research and Development in InformationRetrieval Dublin, IE, 1994, pp. 192201.Hull, D. A. 1994. Improving text retrieval for the routing problem using latent semanticindexing. In Proceedings of SIGIR94, 17th ACM International Conference on Researchand Development in Information Retrieval Dublin, IE, 1994, pp. 282289.Hull, D. A., Pedersen, J. O., and Schutze, H. 1996. Method combination for documentfiltering. In Proceedings of SIGIR96, 19th ACM International Conference on Researchand Development in Information Retrieval Zurich, CH, 1996, pp. 279288.Ittner, D. J., Lewis, D. D., and Ahn, D. D. 1995. Text categorization of low qualityimages. In Proceedings of SDAIR95, 4th Annual Symposium on Document Analysis andInformation Retrieval Las Vegas, US, 1995, pp. 301315.Iwayama, M. and Tokunaga, T. 1995. Clusterbased text categorization a comparisonof category search strategies. In Proceedings of SIGIR95, 18th ACM International Conference on Research and Development in Information Retrieval Seattle, US, 1995, pp.273281.Iyer, R. D., Lewis, D. D., Schapire, R. E., Singer, Y., and Singhal, A. 2000. Boostingfor document routing. In Proceedings of CIKM00, 9th ACM International Conference onInformation and Knowledge Management McLean, US, 2000, pp. 7077.Joachims, T. 1997. A probabilistic analysis of the Rocchio algorithm with TFIDF for textcategorization. In Proceedings of ICML97, 14th International Conference on MachineLearning Nashville, US, 1997, pp. 143151.Joachims, T. 1998. Text categorization with support vector machines learning with manyrelevant features. In Proceedings of ECML98, 10th European Conference on MachineLearning Chemnitz, DE, 1998, pp. 137142.Joachims, T. 1999. Transductive inference for text classification using support vector machines. In Proceedings of ICML99, 16th International Conference on Machine LearningBled, SL, 1999, pp. 200209.Joachims, T. and Sebastiani, F. 2001. Guest editors introduction to the special issue onautomated text categorization. Journal of Intelligent Information Systems. Forthcoming.John, G. H., Kohavi, R., and Pfleger, K. 1994. Irrelevant features and the subset selection problem. In Proceedings of ICML94, 11th International Conference on MachineLearning New Brunswick, US, 1994, pp. 121129.Junker, M. and Abecker, A. 1997. Exploiting thesaurus knowledge in rule induction fortext classification. In Proceedings of RANLP97, 2nd International Conference on RecentAdvances in Natural Language Processing Tzigov Chark, BL, 1997, pp. 202207.Junker, M. and Hoch, R. 1998. An experimental evaluation of OCR text representationsfor learning document classifiers. International Journal on Document Analysis and Recognition 1, 2, 116122.Kessler, B., Nunberg, G., and Schutze, H. 1997. Automatic detection of text genre.In Proceedings of ACL97, 35th Annual Meeting of the Association for ComputationalLinguistics Madrid, ES, 1997, pp. 3238.Kim, Y.H., Hahn, S.Y., and Zhang, B.T. 2000. Text filtering by boosting naive Bayesclassifiers. In Proceedings of SIGIR00, 23rd ACM International Conference on Researchand Development in Information Retrieval Athens, GR, 2000, pp. 16875.Klinkenberg, R. and Joachims, T. 2000. Detecting concept drift with support vectormachines. In Proceedings of ICML00, 17th International Conference on Machine LearningStanford, US, 2000, pp. 487494.Knight, K. 1999. Mining online text. Communications of the ACM 42, 11, 5861.Knorz, G. 1982. A decision theory approach to optimal automated indexing. In Proceedings of SIGIR82, 5th ACM International Conference on Research and Development inInformation Retrieval Berlin, DE, 1982, pp. 174193.Koller, D. and Sahami, M. 1997. Hierarchically classifying documents using very fewwords. In Proceedings of ICML97, 14th International Conference on Machine LearningNashville, US, 1997, pp. 170178.52  F. SebastianiKorfhage, R. R. 1997. Information storage and retrieval. Wiley Computer Publishing,New York, US.Lam, S. L. and Lee, D. L. 1999. Feature reduction for neural network based text categorization. In Proceedings of DASFAA99, 6th IEEE International Conference on DatabaseAdvanced Systems for Advanced Application Hsinchu, TW, 1999, pp. 195202.Lam, W. and Ho, C. Y. 1998. Using a generalized instance set for automatic text categorization. In Proceedings of SIGIR98, 21st ACM International Conference on Researchand Development in Information Retrieval Melbourne, AU, 1998, pp. 8189.Lam, W., Low, K. F., and Ho, C. Y. 1997. Using a Bayesian network induction approachfor text categorization. In Proceedings of IJCAI97, 15th International Joint Conferenceon Artificial Intelligence Nagoya, JP, 1997, pp. 745750.Lam, W., Ruiz, M. E., and Srinivasan, P. 1999. Automatic text categorization and its applications to text retrieval. IEEE Transactions on Knowledge and Data Engineering 11, 6,865879.Lang, K. 1995. NewsWeeder learning to filter netnews. In Proceedings of ICML95, 12thInternational Conference on Machine Learning Lake Tahoe, US, 1995, pp. 331339.Larkey, L. S. 1998. Automatic essay grading using text categorization techniques. In Proceedings of SIGIR98, 21st ACM International Conference on Research and Developmentin Information Retrieval Melbourne, AU, 1998, pp. 9095.Larkey, L. S. 1999. A patent search and classification system. In Proceedings of DL99,4th ACM Conference on Digital Libraries Berkeley, US, 1999, pp. 179187.Larkey, L. S. and Croft, W. B. 1996. Combining classifiers in text categorization. In Proceedings of SIGIR96, 19th ACM International Conference on Research and Developmentin Information Retrieval Zurich, CH, 1996, pp. 289297.Lewis, D. D. 1992a. An evaluation of phrasal and clustered representations on a text categorization task. In Proceedings of SIGIR92, 15th ACM International Conference on Researchand Development in Information Retrieval Kobenhavn, DK, 1992, pp. 3750.Lewis, D. D. 1992b. Representation and learning in information retrieval. Ph. D. thesis,Department of Computer Science, University of Massachusetts, Amherst, US.Lewis, D. D. 1995a. Evaluating and optmizing autonomous text classification systems. InProceedings of SIGIR95, 18th ACM International Conference on Research and Development in Information Retrieval Seattle, US, 1995, pp. 246254.Lewis, D. D. 1995b. A sequential algorithm for training text classifiers corrigendum andadditional data. SIGIR Forum 29, 2, 1319.Lewis, D. D. 1995c. The TREC4 filtering track description and analysis. In Proceedingsof TREC4, 4th Text Retrieval Conference Gaithersburg, US, 1995, pp. 165180.Lewis, D. D. 1998. Naive Bayes at forty The independence assumption in informationretrieval. In Proceedings of ECML98, 10th European Conference on Machine LearningChemnitz, DE, 1998, pp. 415.Lewis, D. D. and Catlett, J. 1994. Heterogeneous uncertainty sampling for supervisedlearning. In Proceedings of ICML94, 11th International Conference on Machine LearningNew Brunswick, US, 1994, pp. 148156.Lewis, D. D. and Gale, W. A. 1994. A sequential algorithm for training text classifiers.In Proceedings of SIGIR94, 17th ACM International Conference on Research and Development in Information Retrieval Dublin, IE, 1994, pp. 312. See also Lewis 1995b.Lewis, D. D. and Hayes, P. J. 1994. Guest editorial for the special issue on text categorization. ACM Transactions on Information Systems 12, 3, 231.Lewis, D. D. and Ringuette, M. 1994. A comparison of two learning algorithms for textcategorization. In Proceedings of SDAIR94, 3rd Annual Symposium on Document Analysisand Information Retrieval Las Vegas, US, 1994, pp. 8193.Lewis, D. D., Schapire, R. E., Callan, J. P., and Papka, R. 1996. Training algorithmsfor linear text classifiers. In Proceedings of SIGIR96, 19th ACM International Conferenceon Research and Development in Information Retrieval Zurich, CH, 1996, pp. 298306.Machine Learning in Automated Text Categorization  53Li, H. and Yamanishi, K. 1999. Text classification using ESCbased stochastic decisionlists. In Proceedings of CIKM99, 8th ACM International Conference on Information andKnowledge Management Kansas City, US, 1999, pp. 122130.Li, Y. H. and Jain, A. K. 1998. Classification of text documents. The Computer Journal 41, 8, 537546.Liddy, E. D., Paik, W., and Yu, E. S. 1994. Text categorization for multiple users based onsemantic features from a machinereadable dictionary. ACM Transactions on InformationSystems 12, 3, 278295.Liere, R. and Tadepalli, P. 1997. Active learning with committees for text categorization.In Proceedings of AAAI97, 14th Conference of the American Association for ArtificialIntelligence Providence, US, 1997, pp. 591596.Lim, J. H. 1999. Learnable visual keywords for image classification. In Proceedings of DL99, 4th ACM Conference on Digital Libraries Berkeley, US, 1999, pp. 139145.Manning, C. and Schutze, H. 1999. Foundations of Statistical Natural Language Processing. The MIT Press, Cambridge, US.Maron, M. 1961. Automatic indexing an experimental inquiry. Journal of the Associationfor Computing Machinery 8, 3, 404417.Masand, B. 1994. Optimising confidence of text classification by evolution of symbolicexpressions. In K. E. Kinnear Ed., Advances in genetic programming , Chapter 21, pp.459476. Cambridge, US The MIT Press.Masand, B., Linoff, G., and Waltz, D. 1992. Classifying news stories using memorybased reasoning. In Proceedings of SIGIR92, 15th ACM International Conference on Research and Development in Information Retrieval Kobenhavn, DK, 1992, pp. 5965.McCallum, A. K. and Nigam, K. 1998. Employing EM in poolbased active learning fortext classification. In Proceedings of ICML98, 15th International Conference on MachineLearning Madison, US, 1998, pp. 350358.McCallum, A. K., Rosenfeld, R., Mitchell, T. M., and Ng, A. Y. 1998. Improvingtext classification by shrinkage in a hierarchy of classes. In Proceedings of ICML98, 15thInternational Conference on Machine Learning Madison, US, 1998, pp. 359367.Merkl, D. 1998. Text classification with selforganizing maps Some lessons learned. Neurocomputing 21, 13, 6177.Mitchell, T. M. 1996. Machine learning. McGraw Hill, New York, US.Mladenic, D. 1998. Feature subset selection in text learning. In Proceedings of ECML98,10th European Conference on Machine Learning Chemnitz, DE, 1998, pp. 95100.Mladenic, D. and Grobelnik, M. 1998. Word sequences as features in textlearning. InProceedings of ERK98, the Seventh Electrotechnical and Computer Science ConferenceLjubljana, SL, 1998, pp. 145148.Moulinier, I. and Ganascia, J.G. 1996. Applying an existing machine learning algorithmto text categorization. In S. Wermter, E. Riloff, and G. Scheler Eds., Connectionist,statistical, and symbolic approaches to learning for natural language processing Heidelberg,DE, 1996, pp. 343354. Springer Verlag.Moulinier, I., Raskinis, G., and Ganascia, J.G. 1996. Text categorization a symbolicapproach. In Proceedings of SDAIR96, 5th Annual Symposium on Document Analysis andInformation Retrieval Las Vegas, US, 1996, pp. 8799.Myers, K., Kearns, M., Singh, S., and Walker, M. A. 2000. A boosting approach totopic spotting on subdialogues. In Proceedings of ICML00, 17th International Conferenceon Machine Learning Stanford, US, 2000.Ng, H. T., Goh, W. B., and Low, K. L. 1997. Feature selection, perceptron learning, anda usability case study for text categorization. In Proceedings of SIGIR97, 20th ACM International Conference on Research and Development in Information Retrieval Philadelphia,US, 1997, pp. 6773.Nigam, K., McCallum, A. K., Thrun, S., and Mitchell, T. M. 2000. Text classificationfrom labeled and unlabeled documents using EM. Machine Learning 39, 23, 103134.54  F. SebastianiOh, H.J., Myaeng, S. H., and Lee, M.H. 2000. A practical hypertext categorizationmethod using links and incrementally available class information. In Proceedings of SIGIR00, 23rd ACM International Conference on Research and Development in InformationRetrieval Athens, GR, 2000, pp. 264271.Pazienza, M. T. Ed. 1997. Information extraction. Number 1299 in Lecture Notes in Computer Science. Springer, Heidelberg, DE.Riloff, E. 1995. Little words can make a big difference for text classification. In Proceedings of SIGIR95, 18th ACM International Conference on Research and Development inInformation Retrieval Seattle, US, 1995, pp. 130136.Riloff, E. and Lehnert, W. 1994. Information extraction as a basis for highprecisiontext classification. ACM Transactions on Information Systems 12, 3, 296333.Robertson, S. E. and Harding, P. 1984. Probabilistic automatic indexing by learningfrom human indexers. Journal of Documentation 40, 4, 264270.Robertson, S. E. and Sparck Jones, K. 1976. Relevance weighting of search terms. Journal of the American Society for Information Science 27, 3, 129146. Also reprinted inWillett 1988, pp. 143160.Roth, D. 1998. Learning to resolve natural language ambiguities a unified approach. InProceedings of AAAI98, 15th Conference of the American Association for Artificial Intelligence Madison, US, 1998, pp. 806813.Ruiz, M. E. and Srinivasan, P. 1999. Hierarchical neural networks for text categorization. In Proceedings of SIGIR99, 22nd ACM International Conference on Research andDevelopment in Information Retrieval Berkeley, US, 1999, pp. 281282.Sable, C. L. and Hatzivassiloglou, V. 2000. Textbased approaches for nontopical imagecategorization. International Journal of Digital Libraries 3, 3, 261275.Salton, G. and Buckley, C. 1988. Termweighting approaches in automatic text retrieval.Information Processing and Management 24, 5, 513523. Also reprinted in Sparck Jonesand Willett 1997, pp. 323328.Salton, G., Wong, A., and Yang, C. 1975. A vector space model for automatic indexing.Communications of the ACM 18, 11, 613620. Also reprinted in Sparck Jones and Willett1997, pp. 273280.Saracevic, T. 1975. Relevance a review of and a framework for the thinking on the notionin information science. Journal of the American Society for Information Science 26, 6, 321343. Also reprinted in Sparck Jones and Willett 1997, pp. 143165.Schapire, R. E. and Singer, Y. 2000. BoosTexter a boostingbased system for textcategorization. Machine Learning 39, 23, 135168.Schapire, R. E., Singer, Y., and Singhal, A. 1998. Boosting and Rocchio applied to textfiltering. In Proceedings of SIGIR98, 21st ACM International Conference on Research andDevelopment in Information Retrieval Melbourne, AU, 1998, pp. 215223.Schutze, H. 1998. Automatic word sense discrimination. Computational Linguistics 24, 1,97124.Schutze, H., Hull, D. A., and Pedersen, J. O. 1995. A comparison of classifiers anddocument representations for the routing problem. In Proceedings of SIGIR95, 18th ACMInternational Conference on Research and Development in Information Retrieval Seattle,US, 1995, pp. 229237.Scott, S. and Matwin, S. 1999. Feature engineering for text classification. In Proceedingsof ICML99, 16th International Conference on Machine Learning Bled, SL, 1999, pp.379388.Sebastiani, F., Sperduti, A., and Valdambrini, N. 2000. An improved boosting algorithm and its application to automated text categorization. In Proceedings of CIKM00,9th ACM International Conference on Information and Knowledge Management McLean,US, 2000, pp. 7885.Singhal, A., Mitra, M., and Buckley, C. 1997. Learning routing queries in a queryzone. In Proceedings of SIGIR97, 20th ACM International Conference on Research andDevelopment in Information Retrieval Philadelphia, US, 1997, pp. 2532.Machine Learning in Automated Text Categorization  55Singhal, A., Salton, G., Mitra, M., and Buckley, C. 1996. Document length normalization. Information Processing and Management 32, 5, 619633.Slonim, N. and Tishby, N. 2001. The power of word clusters for text classification. InProceedings of ECIR01, 23rd European Colloquium on Information Retrieval ResearchDarmstadt, DE, 2001.Sparck Jones, K. and Willett, P. Eds. 1997. Readings in information retrieval. MorganKaufmann, San Mateo, US.Taira, H. and Haruno, M. 1999. Feature selection in SVM text categorization. In Proceedings of AAAI99, 16th Conference of the American Association for Artificial IntelligenceOrlando, US, 1999, pp. 480486.Tauritz, D. R., Kok, J. N., and SprinkhuizenKuyper, I. G. 2000. Adaptive informationfiltering using evolutionary computation. Information Sciences 122, 24, 121140.Tumer, K. and Ghosh, J. 1996. Error correlation and error reduction in ensemble classifiers. Connection Science 8, 34, 385403.Tzeras, K. and Hartmann, S. 1993. Automatic indexing based on Bayesian inferencenetworks. In Proceedings of SIGIR93, 16th ACM International Conference on Researchand Development in Information Retrieval Pittsburgh, US, 1993, pp. 2234.van Rijsbergen, C. J. 1977. A theoretical basis for the use of cooccurrence data in information retrieval. Journal of Documentation 33, 2, 106119.van Rijsbergen, C. J. 1979. Information Retrieval Second ed.. Butterworths, London,UK. Available at httpwww.dcs.gla.ac.ukKeith.Weigend, A. S., Wiener, E. D., and Pedersen, J. O. 1999. Exploiting hierarchy in textcategorization. Information Retrieval 1, 3, 193216.Weiss, S. M., Apte, C., Damerau, F. J., Johnson, D. E., Oles, F. J., Goetz, T., andHampp, T. 1999. Maximizing textmining performance. IEEE Intelligent Systems 14, 4,6369.Wiener, E. D., Pedersen, J. O., and Weigend, A. S. 1995. A neural network approach totopic spotting. In Proceedings of SDAIR95, 4th Annual Symposium on Document Analysisand Information Retrieval Las Vegas, US, 1995, pp. 317332.Willett, P. Ed. 1988. Document retrieval systems. Taylor Graham, London, UK.Wong, J. W., Kan, W.K., and Young, G. H. 1996. Action automatic classification forfulltext documents. SIGIR Forum 30, 1, 2641.Yang, Y. 1994. Expert network effective and efficient learning from human decisions intext categorisation and retrieval. In Proceedings of SIGIR94, 17th ACM InternationalConference on Research and Development in Information Retrieval Dublin, IE, 1994,pp. 1322.Yang, Y. 1995. Noise reduction in a statistical approach to text categorization. In Proceedings of SIGIR95, 18th ACM International Conference on Research and Development inInformation Retrieval Seattle, US, 1995, pp. 256263.Yang, Y. 1999. An evaluation of statistical approaches to text categorization. InformationRetrieval 1, 12, 6990.Yang, Y. and Chute, C. G. 1994. An examplebased mapping method for text categorization and retrieval. ACM Transactions on Information Systems 12, 3, 252277.Yang, Y. and Liu, X. 1999. A reexamination of text categorization methods. In Proceedings of SIGIR99, 22nd ACM International Conference on Research and Development inInformation Retrieval Berkeley, US, 1999, pp. 4249.Yang, Y. and Pedersen, J. O. 1997. A comparative study on feature selection in textcategorization. In Proceedings of ICML97, 14th International Conference on MachineLearning Nashville, US, 1997, pp. 412420.Yang, Y., Slattery, S., and Ghani, R. 2001. A study of approaches to hypertext categorization. Journal of Intelligent Information Systems. Forthcoming.Yu, K. L. and Lam, W. 1998. A new online learning algorithm for adaptive text filtering. In Proceedings of CIKM98, 7th ACM International Conference on Information andKnowledge Management Bethesda, US, 1998, pp. 156160.
