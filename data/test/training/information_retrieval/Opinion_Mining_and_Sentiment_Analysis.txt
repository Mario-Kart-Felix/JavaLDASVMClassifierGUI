Foundations and TrendsR inInformation RetrievalVol. 2, Nos. 12 2008 1135c 2008 B. Pang and L. LeeDOI 10.15611500000001Opinion Mining and Sentiment AnalysisBo Pang1 and Lillian Lee21 Yahoo Research, 701 First Avenue, Sunnyvale, CA 94089, USA,bopangyahooinc.com2 Computer Science Department, Cornell University, Ithaca, NY 14853,USA, lleecs.cornell.eduAbstractAn important part of our informationgathering behavior has alwaysbeen to find out what other people think. With the growing availabilityand popularity of opinionrich resources such as online review sitesand personal blogs, new opportunities and challenges arise as peoplenow can, and do, actively use information technologies to seek out andunderstand the opinions of others. The sudden eruption of activity inthe area of opinion mining and sentiment analysis, which deals withthe computational treatment of opinion, sentiment, and subjectivityin text, has thus occurred at least in part as a direct response to thesurge of interest in new systems that deal directly with opinions as afirstclass object.This survey covers techniques and approaches that promise todirectly enable opinionoriented informationseeking systems. Ourfocus is on methods that seek to address the new challenges raised bysentimentaware applications, as compared to those that are alreadypresent in more traditional factbased analysis. We include materialon summarization of evaluative text and on broader issues regardingprivacy, manipulation, and economic impact that the development ofopinionoriented informationaccess services gives rise to. To facilitatefuture work, a discussion of available resources, benchmark datasets,and evaluation campaigns is also provided.1IntroductionRomance should never begin with sentiment. It shouldbegin with science and end with a settlement. Oscar Wilde, An Ideal Husband1.1 The Demand for Information on Opinionsand SentimentWhat other people think has always been an important piece of information for most of us during the decisionmaking process. Long beforeawareness of the World Wide Web became widespread, many of usasked our friends to recommend an auto mechanic or to explain whothey were planning to vote for in local elections, requested referenceletters regarding job applicants from colleagues, or consulted ConsumerReports to decide what dishwasher to buy. But the Internet and theWebhave now among other things made it possible to find out about theopinions and experiences of those in the vast pool of people that are neither our personal acquaintances nor wellknown professional critics that is, people we have never heard of. And conversely, more and morepeople are making their opinions available to strangers via the Internet.12 IntroductionIndeed, according to two surveys of more than 2000 American adultseach 63, 127, 81 of Internet users or 60 of Americans have done onlineresearch on a product at least once 20 15 of all Americans do so on a typical day among readers of online reviews of restaurants, hotels, andvarious services e.g., travel agencies or doctors, between73 and 87 report that reviews had a significant influenceon their purchase1 consumers report being willing to pay from 20 to 99 morefor a 5starrated item than a 4starrated item the variancestems from what type of item or service is considered 32 have provided a rating on a product, service, or person via an online ratings system, and 30 including 18of online senior citizens have posted an online comment orreview regarding a product or service.2We hasten to point out that consumption of goods and servicesis not the only motivation behind peoples seeking out or expressingopinions online. A need for political information is another importantfactor. For example, in a survey of over 2500 American adults, Rainieand Horrigan 248 studied the 31 of Americans  over 60 millionpeople  that were 2006 campaign internet users, defined as those whogathered information about the 2006 elections online and exchangedviews via email. Of these, 28 said that a major reason for these online activities wasto get perspectives from within their community, and 34said that a major reason was to get perspectives from outsidetheir community 27 had looked online for the endorsements or ratings ofexternal organizations1Section 6.1 discusses quantitative analyses of actual economic impact, as opposed to consumer perception.2 Interestingly, Hitlin and Rainie 123 report that Individuals who have rated somethingonline are also more skeptical of the information that is available on the Web.1.1 The Demand for Information on Opinions and Sentiment 3 28 said that most of the sites they use share their pointof view, but 29 said that most of the sites they use challenge their point of view, indicating that many people are notsimply looking for validations of their preexisting opinionsand 8 posted their own political commentary online.The user hunger for and reliance upon online advice and recommendations that the data above reveals is merely one reason behindthe surge of interest in new systems that deal directly with opinions asa firstclass object. But, Horrigan 127 reports that while a majority ofAmerican internet users report positive experiences during online product research, at the same time, 58 also report that online informationwas missing, impossible to find, confusing, andor overwhelming. Thus,there is a clear need to aid consumers of products and of informationby building better informationaccess systems than are currently inexistence.The interest that individual users show in online opinions aboutproducts and services, and the potential influence such opinions wield,is something that vendors of these items are paying more and moreattention to 124. The following excerpt from a whitepaper is illustrative of the envisioned possibilities, or at the least the rhetoric surrounding the possibilitiesWith the explosion of Web 2.0 platforms such as blogs,discussion forums, peertopeer networks, and variousother types of social media . . . consumers have at theirdisposal a soapbox of unprecedented reach and powerby which to share their brand experiences and opinions,positive or negative, regarding any product or service.As major companies are increasingly coming to realize,these consumer voices can wield enormous influence inshaping the opinions of other consumers  and, ultimately, their brand loyalties, their purchase decisions,and their own brand advocacy. . . . Companies canrespond to the consumer insights they generate throughsocial media monitoring and analysis by modifying their4 Introductionmarketing messages, brand positioning, product development, and other activities accordingly. Zabin and Jefferies 327But industry analysts note that the leveraging of new media for thepurpose of tracking product image requires new technologies here is arepresentative snippet describing their concernsMarketers have always needed to monitor media forinformation related to their brands  whether itsfor public relations activities, fraud violations,3 orcompetitive intelligence. But fragmenting media andchanging consumer behavior have crippled traditionalmonitoring methods. Technorati estimates that 75,000new blogs are created daily, along with 1.2 million newposts each day, many discussing consumer opinionson products and services. Tactics of the traditionalsort such as clipping services, field agents, and ad hocresearch simply cant keep pace. Kim 154Thus, aside from individuals, an additional audience for systems capable of automatically analyzing consumer sentiment, as expressed in nosmall part in online venues, are companies anxious to understand howtheir products and services are perceived.1.2 What Might be Involved An ExampleExamination of the Construction ofan OpinionReview Search EngineCreating systems that can process subjective information effectivelyrequires overcoming a number of novel challenges. To illustrate someof these challenges, let us consider the concrete example of what building an opinion or reviewsearch application could involve. As we havediscussed, such an application would fill an important and prevalent3Presumably, the author means the detection or prevention of fraud violations, asopposed to the commission thereof.1.2 What Might be Involved 5information need, whether one restricts attention to blog search 213or considers the more general types of search that have been describedabove.The development of a complete review or opinionsearch application might involve attacking each of the following problems.1 If the application is integrated into a generalpurpose searchengine, then one would need to determine whether the useris in fact looking for subjective material. This may or maynot be a difficult problem in and of itself perhaps queries ofthis type will tend to contain indicator terms like review,reviews, or opinions, or perhaps the application wouldprovide a checkbox to the user so that he or she could indicate directly that reviews are what is desired but in general,query classification is a difficult problem  indeed, it wasthe subject of the 2005 KDD Cup challenge 185.2 Besides the stillopen problem of determining which documents are topically relevant to an opinionoriented query,an additional challenge we face in our new setting issimultaneously or subsequently determining which documents or portions of documents contain reviewlike or opinionated material. Sometimes this is relatively easy, as intexts fetched from reviewaggregation sites in which revieworiented information is presented in relatively stereotypedformat examples include Epinions.com and Amazon.com.However, blogs also notoriously contain quite a bit of subjective content and thus are another obvious place to look andare more relevant than shopping sites for queries that concern politics, people, or other nonproducts, but the desiredmaterial within blogs can vary quite widely in content, style,presentation, and even level of grammaticality.3 Once one has target documents in hand, one is still faced withthe problem of identifying the overall sentiment expressedby these documents andor the specific opinions regarding particular features or aspects of the items or topics inquestion, as necessary. Again, while some sites make this6 Introductionkind of extraction easier  for instance, user reviews postedto Yahoo Movies must specify grades for predefined sets ofcharacteristics of films  more freeform text can be muchharder for computers to analyze, and indeed can pose additional challenges for example, if quotations are included in anewspaper article, care must be taken to attribute the viewsexpressed in each quotation to the correct entity.4 Finally, the system needs to present the sentiment information it has garnered in some reasonable summary fashion.This can involve some or all of the following actionsa Aggregation of votes that may be registeredon different scales e.g., one reviewer uses a starsystem, but another uses letter grades.b Selective highlighting of some opinions.c Representation of points of disagreement andpoints of consensus.d Identification of communities of opinion holders.e Accounting for different levels of authorityamong opinion holders.Note that it might be more appropriate to produce a visualization of sentiment data rather than a textual summary ofit, whereas textual summaries are what is usually created instandard topicbased multidocument summarization.1.3 Our Charge and ApproachChallenges 2, 3, and 4 in the above list are very active areas ofresearch, and the bulk of this survey is devoted to reviewing work inthese three subfields. However, due to space limitations and the focusof the journal series in which this survey appears, we do not and cannotaim to be completely comprehensive.In particular, when we began to write this survey, we were directlycharged to focus on informationaccess applications, as opposed to workof more purely linguistic interest. We stress that the importance of workin the latter vein is absolutely not in question.1.4 Early History 7Given our mandate, the reader will not be surprised that we describethe applications that sentimentanalysis systems can facilitate andreview many kinds of approaches to a variety of opinionoriented classification problems. We have also chosen to attempt to draw attentionto single and multidocument summarization of evaluative text, especially since interesting considerations regarding graphical visualizationarise. Finally, we move beyond just the technical issues, devoting significant attention to the broader implications that the development ofopinionoriented informationaccess services have we look at questionsof privacy, manipulation, and whether or not reviews can have measurable economic impact.1.4 Early HistoryAlthough the area of sentiment analysis and opinion mining hasrecently enjoyed a huge burst of research activity, there has been asteady undercurrent of interest for quite a while. One could countearly projects on beliefs as forerunners of the area 48, 317. Later workfocused mostly on interpretation of metaphor, narrative, point of view,affect, evidentiality in text, and related areas 121, 133, 149, 262, 306,310, 311, 312, 313.The year 2001 or so seems to mark the beginning of widespreadawareness of the research problems and opportunities that sentimentanalysis and opinion mining raise 51, 66, 69, 79, 192, 215, 221, 235,291, 296, 298, 305, 326, and subsequently there have been literallyhundreds of papers published on the subject.Factors behind this land rush include the rise of machine learning methods in natural languageprocessing and information retrieval the availability of datasets for machine learning algorithmsto be trained on, due to the blossoming of the World WideWeb and, specifically, the development of reviewaggregationwebsites and, of course realization of the fascinating intellectual challenges and commercial and intelligence applications that the area offers.8 Introduction1.5 A Note on Terminology Opinion Mining, SentimentAnalysis, Subjectivity, and All thatThe beginning of wisdom is the definition of terms,wrote Socrates. The aphorism is highly applicable whenit comes to the world of social media monitoring andanalysis, where any semblance of universal agreementon terminology is altogether lacking.Today, vendors, practitioners, and the media alike callthis stillnascent arena everything from brand monitoring, buzz monitoring and online anthropology, tomarket influence analytics, conversation mining andonline consumer intelligence. . . . In the end, the termsocial media monitoring and analysis is itself a verbalcrutch. It is placeholder sic, to be used until somethingbetter and shorter takes hold in the English languageto describe the topic of this report. Zabin and Jefferies 327The above quotation highlights the problems that have arisen intrying to name a new area. The quotation is particularly apt in thecontext of this survey because the field of social media monitoringand analysis or however one chooses to refer to it is precisely onethat the body of work we review is very relevant to. And indeed, therehas been to date no uniform terminology established for the relativelyyoung field we discuss in this survey. In this section, we simply mentionsome of the terms that are currently in vogue, and attempt to indicatewhat these terms tend to mean in research papers that the interestedreader may encounter.The body of work we review is that which deals with the computational treatment of in alphabetical order opinion, sentiment, and subjectivity in text. Such work has come to be known as opinion mining,sentiment analysis, andor subjectivity analysis. The phrases reviewmining and appraisal extraction have been used, too, and there are someconnections to affective computing, where the goals include enablingcomputers to recognize and express emotions 239. This proliferationof terms reflects differences in the connotations that these terms carry,1.5 A Note on Terminology 9both in their original generaldiscourse usages4 and in the usages thathave evolved in the technical literature of several communities.In 1994, Wiebe 311, influenced by the writings of the literarytheorist Banfield 26, centered the idea of subjectivity around that ofprivate states, defined by Quirk et al. 245 as states that are not open toobjective observation or verification. Opinions, evaluations, emotions,and speculations all fall into this category but a canonical exampleof research typically described as a type of subjectivity analysis is therecognition of opinionoriented language in order to distinguish it fromobjective language. While there has been some research selfidentifiedas subjectivity analysis on the particular application area of determining the value judgments e.g., four stars or C expressed in theevaluative opinions that are found, this application has not tended tobe a major focus of such work.The term opinion mining appears in a paper by Dave et al. 69that was published in the proceedings of the 2003 WWW conferencethe publication venue may explain the popularity of the term withincommunities strongly associated with Web search or informationretrieval. According to Dave et al. 69, the ideal opinionmining toolwould process a set of search results for a given item, generating a listof product attributes quality, features, etc. and aggregating opinions4To see that the distinctions in common usage can be subtle, consider how interrelated thefollowing set of definitions given in MerriamWebsters Online Dictionary areSynonyms opinion, view, belief, conviction, persuasion, sentiment meana judgment one holds as true. Opinion implies a conclusion thought out yet open to disputeeach expert seemed to have a different opinion. View suggests a subjective opinion very assertive in statinghis views. Belief implies often deliberate acceptance and intellectualassent a firm belief in her partys platform. Conviction applies to a firmly and seriously held belief theconviction that animal life is as sacred as human. Persuasion suggests a belief grounded on assurance as byevidence of its truth was of the persuasion that everythingchanges. Sentiment suggests a settled opinion reflective of ones feelingsher feminist sentiments are wellknown.10 Introductionabout each of them poor, mixed, good. Much of the subsequentresearch selfidentified as opinion mining fits this description in itsemphasis on extracting and analyzing judgments on various aspectsof given items. However, the term has recently also been interpretedmore broadly to include many different types of analysis of evaluativetext 190.The history of the phrase sentiment analysis parallels that of opinion mining in certain respects. The term sentiment used in referenceto the automatic analysis of evaluative text and tracking of the predictive judgments therein appears in 2001 papers by Das and Chen 66and Tong 296, due to these authors interest in analyzing market sentiment. It subsequently occurred within 2002 papers by Turney 298 andPang et al. 235, which were published in the proceedings of the annualmeeting of the Association for Computational Linguistics ACL andthe annual conference on Empirical Methods in Natural Language Processing EMNLP. Moreover, Nasukawa and Yi 221 entitled their 2003paper, Sentiment analysis Capturing favorability using natural language processing, and a paper in the same year by Yi et al. 323 wasnamed Sentiment Analyzer Extracting sentiments about a given topicusing natural language processing techniques. These events togethermay explain the popularity of sentiment analysis among communities selfidentified as focused on NLP. A sizeable number of papersmentioning sentiment analysis focus on the specific application ofclassifying reviews as to their polarity either positive or negative, afact that appears to have caused some authors to suggest that thephrase refers specifically to this narrowly defined task. However, nowadays many construe the term more broadly to mean the computationaltreatment of opinion, sentiment, and subjectivity in text.Thus, when broad interpretations are applied, sentiment analysisand opinion mining denote the same field of study which itself canbe considered a subarea of subjectivity analysis. We have attemptedto use these terms more or less interchangeably in this survey. This is inno small part because we view the field as representing a unified bodyof work, and would thus like to encourage researchers in the area toshare terminology regardless of the publication venues at which theirpapers might appear.2ApplicationsSentiment without action is the ruin of the soul. Edward AbbeyWe used one application of opinion mining and sentiment analysis as amotivating example in the Introduction, namely, web search targetedtoward reviews. But other applications abound. In this section, we seekto enumerate some of the possibilities.It is important to mention that because of all the possible applications, there are a good number of companies, large and small, that haveopinion mining and sentiment analysis as part of their mission. However, we have elected not to mention these companies individually dueto the fact that the industrial landscape tends to change quite rapidly,so that lists of companies risk falling out of date rather quickly.2.1 Applications to ReviewRelated WebsitesClearly, the same capabilities that a revieworiented search enginewould have could also serve very well as the basis for the creation andautomated upkeep of review and opinionaggregation websites. That is,as an alternative to sites like Epinions that solicit feedback and reviews,1112 Applicationsone could imagine sites that proactively gather such information. Topicsneed not be restricted to product reviews, but could include opinionsabout candidates running for office, political issues, and so forth.There are also applications of the technologies we discuss to moretraditional reviewsolicitation sites, as well. Summarizing user reviewsis an important problem. One could also imagine that errors in userratings could be fixed there are cases where users have clearly accidentally selected a low rating when their review indicates a positiveevaluation 47. Moreover, as discussed later in this survey see Section 5.2.4, for example, there is some evidence that user ratings canbe biased or otherwise in need of correction, and automated classifierscould provide such updates.2.2 Applications as a SubComponent TechnologySentimentanalysis and opinionmining systems also have an importantpotential role as enabling technologies for other systems.One possibility is as an augmentation to recommendation systems292, 293, since it might behoove such a system not to recommenditems that receive a lot of negative feedback.Detection of flames overly heated or antagonistic language inemail or other types of communication 276 is another possible use ofsubjectivity detection and classification.In online systems that display ads as sidebars, it is helpful to detectwebpages that contain sensitive content inappropriate for ads placement 137 for more sophisticated systems, it could be useful to bringup product ads when relevant positive sentiments are detected, and perhaps more importantly, nix the ads when relevant negative statementsare discovered.It has also been argued that information extraction can be improvedby discarding information found in subjective sentences 256.Question answering is another area where sentiment analysis canprove useful 274, 284, 189. For example, opinionoriented questionsmay require different treatment. Alternatively, Lita et al. 189 suggestthat for definitional questions, providing an answer that includes moreinformation about how an entity is viewed may better inform the user.2.3 Applications in Business and Government Intelligence 13Summarization may also benefit from accounting for multiple viewpoints 265.Additionally, there are potentially relations to citation analysis,where, for example, one might wish to determine whether an authoris citing a piece of work as supporting evidence or as research thathe or she dismisses 238. Similarly, one effort seeks to use semanticorientation to track literary reputation 287.In general, the computational treatment of affect has been motivated in part by the desire to improve humancomputer interaction188, 192, 295.2.3 Applications in Business and Government IntelligenceThe field of opinion mining and sentiment analysis is wellsuited tovarious types of intelligence applications. Indeed, business intelligenceseems to be one of the main factors behind corporate interest in thefield.Consider, for instance, the following scenario the text of which alsoappears in Lee 181. A major computer manufacturer, disappointedwith unexpectedly low sales, finds itself confronted with the questionWhy arent consumers buying our laptop While concrete data suchas the laptops weight or the price of a competitors model are obviouslyrelevant, answering this question requires focusing more on peoplespersonal views of such objective characteristics. Moreover, subjectivejudgments regarding intangible qualities  e.g., the design is tackyor customer service was condescending  or even misperceptions e.g., updated device drivers are not available when such device driversdo in fact exist  must be taken into account as well.Sentimentanalysis technologies for extracting opinions fromunstructured humanauthored documents would be excellent toolsfor handling many businessintelligence tasks related to the one justdescribed. Continuing with our example scenario it would be difficultto try to directly survey laptop purchasers who have not bought thecompanys product. Rather, we could employ a system that a findsreviews or other expressions of opinion on the Web  newsgroups,individual blogs, and aggregation sites such as Epinions are likely to14 Applicationsbe productive sources  and then b creates condensed versions ofindividual reviews or a digest of overall consensus points. This wouldsave an analyst from having to read potentially dozens or even hundreds of versions of the same complaints. Note that Internet sources canvary wildly in form, tenor, and even grammaticality this fact underscores the need for robust techniques even when only one languagee.g., English is considered.Besides reputation management and public relations, one might perhaps hope that by tracking public viewpoints, one could perform trendprediction in sales or other relevant data 214. See our discussion ofBroader Implications Section 6 for more discussion of potential economic impact.Government intelligence is another application that has been considered. For example, it has been suggested that one could monitorsources for increases in hostile or negative communications 1.2.4 Applications Across Different DomainsOne exciting turn of events has been the confluence of interest in opinions and sentiment within computer science with interest in opinionsand sentiment in other fields.As is well known, opinions matter a great deal in politics. Somework has focused on understanding what voters are thinking 83, 110,126, 178, 219, whereas other projects have as a long term goal the clarification of politicians positions, such as what public figures support oroppose, to enhance the quality of information that voters have accessto 27, 111, 294.Sentiment analysis has specifically been proposed as a key enablingtechnology in eRulemaking, allowing the automatic analysis of the opinions that people submit about pending policy or governmentregulationproposals 50, 175, 271.On a related note, there has been investigation into opinion miningin weblogs devoted to legal matters, sometimes known as blawgs 64.Interactions with sociology promise to be extremely fruitful. Forinstance, the issue of how ideas and innovations diffuse 258 involvesthe question of who is positively or negatively disposed toward whom,2.4 Applications Across Different Domains 15and hence who would be more or less receptive to new informationtransmission from a given source. To take just one other examplestructural balance theory is centrally concerned with the polarityof ties between people 54 and how this relates to group cohesion. These ideas have begun to be applied to online media analysis58, 144.3General Challenges3.1 Contrasts with Standard FactBased Textual AnalysisThe increasing interest in opinion mining and sentiment analysis ispartly due to its potential applications, which we have just discussed.Equally important are the new intellectual challenges that the fieldpresents to the research community. So what makes the treatmentof evaluative text different from classic text mining and factbasedanalysisTake text categorization, for example. Traditionally, text categorization seeks to classify documents by topic. There can be many possiblecategories, the definitions of which might be user and applicationdependent and for a given task, we might be dealing with as few astwo classes binary classification or as many as thousands of classese.g., classifying documents with respect to a complex taxonomy. Incontrast, with sentiment classification see Section 4.1 for more detailson precise definitions, we often have relatively few classes e.g., positive or 3 stars that generalize across many domains and users.In addition, while the different classes in topicbased categorizationcan be completely unrelated, the sentiment labels that are widely163.2 Factors that Make Opinion Mining Difficult 17considered in previous work typically represent opposing if the task isbinary classification or ordinalnumerical categories if classification isaccording to a multipoint scale. In fact, the regressionlike nature ofstrength of feeling, degree of positivity, and so on seems rather uniqueto sentiment categorization although one could argue that the samephenomenon exists with respect to topicbased relevance.There are also many characteristics of answers to opinionorientedquestions that differ from those for factbased questions 284. As aresult, opinionoriented information extraction, as a way to approachopinionoriented question answering, naturally differs from traditionalinformation extraction IE 49. Interestingly, in a manner that is similar to the situation for the classes in sentimentbased classification, thetemplates for opinionoriented IE also often generalize well across different domains, since we are interested in roughly the same set of fields foreach opinion expression e.g., holder, type, strength regardless of thetopic. In contrast, traditional IE templates can differ greatly from onedomain to another  the typical template for recording informationrelevant to a natural disaster is very different from a typical templatefor storing bibliographic information.These distinctions might make our problems appear deceptivelysimpler than their counterparts in factbased analysis, but this is farfrom the truth. In the next section, we sample a few examples to showwhat makes these problems difficult compared to traditional factbasedtext analysis.3.2 Factors that Make Opinion Mining DifficultLet us begin with a sentiment polarity textclassification example. Suppose we wish to classify an opinionated text as either positive ornegative, according to the overall sentiment expressed by the authorwithin it. Is this a difficult taskTo answer this question, first consider the following example,consisting of only one sentence by Mark Twain Jane Austens booksmadden me so that I cant conceal my frenzy from the reader. Justas the topic of this text segment can be identified by the phrase JaneAusten, the presence of words like madden and frenzy suggests18 General Challengesnegative sentiment. So one might think this is an easy task, andhypothesize that the polarity of opinions can generally be identifiedby a set of keywords.But, the results of an early study by Pang et al. 235 on moviereviews suggest that coming up with the right set of keywords might beless trivial than one might initially think. The purpose of Pang et al.spilot study was to better understand the difficulty of the documentlevel sentimentpolarity classification problem. Two human subjectswere asked to pick keywords that they would consider to be good indicators of positive and negative sentiment. As shown in Figure 3.1, theuse of the subjects lists of keywords achieves about 60 accuracy whenemployed within a straightforward classification policy. In contrast,word lists of the same size but chosen based on examination of thecorpus statistics achieves almost 70 accuracy  even though someof the terms, such as still, might not look that intuitive at first.However, the fact that it may be nontrivial for humans to comeup with the best set of keywords does not in itself imply that theproblem is harder than topicbased categorization. While the featurestill might not be likely for any human to propose from introspection,given training data, its correlation with the positive class can bediscovered via a datadriven approach, and its utility at least inProposed word lists Accuracy Ties Human 1 positive dazzling, brilliant, phenomenal, excellent,fantastic58 75negative suck, terrible, awful, unwatchable,hideousHuman 2 positive gripping, mesmerizing, riveting,spectacular, cool, awesome, thrilling, badass,excellent, moving, exciting64 39negative bad, cliched, sucks, boring, stupid, slowStatisticsbased positive love, wonderful, best, great, superb, still,beautiful69 16negative bad, worst, stupid, waste, boring, , Fig. 3.1 Sentiment classification using keyword lists created by human subjects Human1 and Human 2, with corresponding results using keywords selected via examinationof simple statistics of the test data Statisticsbased. Adapted from Figures 1 and 2 inPang et al. 235.3.2 Factors that Make Opinion Mining Difficult 19the movie review domain does make sense in retrospect. Indeed,applying machine learning techniques based on unigram models canachieve over 80 in accuracy 235, which is much better than the performance based on handpicked keywords reported above. However, thislevel of accuracy is not quite on par with the performance one wouldexpect in typical topicbased binary classification.Why does this problem appear harder than the traditional taskwhen the two classes we are considering here are so different from eachother Our discussion of algorithms for classification and extractionSection 4 will provide a more indepth answer to this question, butthe following are a few examples from among the many we knowshowing that the upper bound on problem difficulty, from the viewpointof machines, is very high. Note that not all of the issues these examplesraise have been fully addressed in the existing body of work in thisarea.Compared to topic, sentiment can often be expressed in a moresubtle manner, making it difficult to be identified by any of a sentence ordocuments terms when considered in isolation. Consider the followingexamples If you are reading this because it is your darling fragrance,please wear it at home exclusively, and tape the windowsshut. review by Luca Turin and Tania Sanchez of theGivenchy perfume Amarige, in Perfumes The Guide, Viking2008. No ostensibly negative words occur. She runs the gamut of emotions from A to B. DorothyParker, speaking about Katharine Hepburn. No ostensiblynegative words occur.In fact, the example that opens this section, which was taken fromthe following quote from Mark Twain, is also followed by a sentencewith no ostensibly negative wordsJane Austens books madden me so that I cant concealmy frenzy from the reader. Everytime I read Pride andPrejudice I want to dig her up and beat her over theskull with her own shinbone.20 General ChallengesA related observation is that although the second sentence indicatesan extremely strong opinion, it is difficult to associate the presence ofthis strong opinion with specific keywords or phrases in this sentence.Indeed, subjectivity detection can be a difficult task in itself. Considerthe following quote from Charlotte Bronte, in a letter to George LewesYou say I must familiarise my mind with the fact thatMiss Austen is not a poetess, has no sentiment you scornfully enclose the word in inverted commas,has no eloquence, none of the ravishing enthusiasm ofpoetry and then you add, I must learn to acknowledge her as one of the greatest artists, of the greatestpainters of human character, and one of the writers withthe nicest sense of means to an end that ever lived.Note the fine line between facts and opinions while Miss Austenis not a poetess can be considered to be a fact, none of the ravishingenthusiasm of poetry should probably be considered as an opinion,even though the two phrases s arguably convey similar information.1Thus, not only can we not easily identify simple keywords for subjectivity, but we also find that like the fact that do not necessarilyguarantee the objective truth of what follows them  and bigrams likeno sentiment apparently do not guarantee the absence of opinions,either. We can also get a glimpse of how opinionoriented information1One can challenge our analysis of the poetess clause, as an anonymous reviewer indeeddid  which disagreement perhaps supports our greater point about the difficulties thatcan sometimes present themselves.Different researchers express different opinions about whether distinguishing betweensubjective and objective language is difficult for humans in the general case. For example,Kim and Hovy 159 note that in a pilot study sponsored by NIST, human annotatorsoften disagreed on whether a belief statement was or was not an opinion. However, otherresearchers have found interannotator agreement rates in various types of subjectivityclassification tasks to be satisfactory 45, 273, 274, 309 a summary provided by one ofthe anonymous referees is that although there is variation from study to study, onaverage, about 85 of annotations are not marked as uncertain by either annotator, andfor these cases, intercoder agreement is very high kappa values over 80. As in othersettings, more careful definitions of the distinctions to be made tend to lead to betteragreement rates.In any event, the points we are exploring in the Bronte quote may be made more clearby replacing Jane Austen is not a poetess with something like Jane Austen does notwrite poetry for a living, but is also no poet in the broader sense.3.2 Factors that Make Opinion Mining Difficult 21extraction can be difficult. For instance, it is nontrivial to recognizeopinion holders. In the example quoted above, the opinion is not thatof the author, but the opinion of You, which refers to George Lewesin this particular letter. Also, observe that given the context youscornfully enclose the word in inverted commas, together with thereported endorsement of Austen as a great artist, it is clear that hasno sentiment is not meant to be a showstopping criticism of Austenfrom Lewes, and Brontes disagreement with him on this subject is alsosubtly revealed.In general, sentiment and subjectivity are quite contextsensitive,and, at a coarser granularity, quite domain dependent in spite of thefact that the general notion of positive and negative opinions is fairlyconsistent across different domains. Note that although domain dependency is in part a consequence of changes in vocabulary, even the exactsame expression can indicate different sentiment in different domains.For example, go read the book most likely indicates positive sentiment for book reviews, but negative sentiment for movie reviews.This example was furnished to us by Bob Bland. We will discusstopicsentiment interaction in more detail in Section 4.4.It does not take a seasoned writer or a professional journalist toproduce texts that are difficult for machines to analyze. The writingsof Web users can be just as challenging, if not as subtle, in their ownway  see Figure 3.2 for an example. In the case of Figure 3.2, itshould be pointed out that might be more useful to learn to recognizethe quality of a review see Section 5.2 for more detailed discussionson that subject. Still, it is interesting to observe the importance ofmodeling discourse structure. While the overall topic of a documentFig. 3.2 Example of movie reviews produced by web users a slightly reformatted screenshot of user reviews for The Nightmare Before Christmas.22 General Challengesshould be what the majority of the content is focusing on regardlessof the order in which potentially different subjects are presented, foropinions, the order in which different opinions are presented can resultin a completely opposite overall sentiment polarity.In fact, somewhat in contrast with topicbased text categorization,order effects can completely overwhelm frequency effects. Consider thefollowing excerpt, again from a movie reviewThis film should be brilliant. It sounds like a great plot,the actors are first grade, and the supporting cast isgood as well, and Stallone is attempting to deliver agood performance. However, it cant hold up.As indicated by the inserted emphasis, words that are positive inorientation dominate this excerpt,2 and yet the overall sentiment isnegative because of the crucial last sentence whereas in traditionaltext classification, if a document mentions cars relatively frequently,then the document is most likely at least somewhat related to cars.Order dependence also manifests itself at more finegrained levels ofanalysis A is better than B conveys the exact opposite opinion fromB is better than A.3 In general, modeling sequential information anddiscourse structure seems more crucial in sentiment analysis furtherdiscussion appears in Section 4.7.As noted earlier, not all of the issues we have just discussed havebeen fully addressed in the literature. This is perhaps part of the charmof this emerging area. In the following sections, we aim to give anoverview of a selection of past heroic efforts to address some of theseissues, and march through the positives and the negatives, charged withunbiased feeling, armed with hard facts.Fasten your seat belts. Its going to be a bumpy night Bette Davis, All About Eve,screenplay by Joseph Mankiewicz2One could argue about whether in the context of movie reviews the word Stallone hasa semantic orientation.3Note that this is not unique to opinion expressions A killed B and B killed A alsoconvey different factual information.4Classification and ExtractionThe Bucket List, which was written by Justin Zackham and directed by Rob Reiner, seems to have beencreated by applying algorithms to sentiment. David Denby movie review,The New Yorker, January 7, 2007A fundamental technology in many current opinionmining andsentimentanalysis applications is classification  note that in this survey, we generally construe the term classification broadly, so that itencompasses regression and ranking. The reason that classification is soimportant is that many problems of interest can be formulated as applying classificationregressionranking to given textual units examplesinclude making a decision for a particular phrase or document howpositive is it, ordering a set of texts rank these reviews by how positive they are, giving a single label to an entire document collectionwhere on the scale between liberal and conservative do the writings ofthis author lie, and categorizing the relationship between two entities based on textual evidence does A approve of Bs actions. Thissection is centered on approaches to these kinds of problems.2324 Classification and ExtractionPart One p. 24ff. covers fundamental background. Specifically,Section 4.1 provides a discussion of key concepts involved in commonformulations of classification problems in sentiment analysis and opinion mining. Features that have been explored for sentiment analysistasks are discussed in Section 4.2.Part Two p. 37ff. is devoted to an indepth discussion of differenttypes of approaches to classification, regression, and ranking problems.The beginning of Part Two should be consulted for a detailed outline,but it is appropriate here to indicate how we cover extraction, since itplays a key role in many sentimentoriented applications and so somereaders may be particularly interested in it.First, extraction problems e.g., retrieving opinions on various features of a laptop are often solved by casting many subproblems asclassification problems e.g., given a text span, determine whetherit expresses any opinion at all. Therefore, rather than have a separate section devoted completely to the entirety of the extraction task,we have integrated discussion of extractionoriented classification subproblems into the appropriate places in our discussion of different typesof approaches to classification in general Sections 4.34.8. Section 4.9covers those remaining aspects of extraction that can be thought of asdistinct from classification.Second, extraction is often a means to the further goal of providing effective summaries of the extracted information to users. Detailson how to combine information mined from multiple subjective textsegments into a suitable summary can be found in Section 5.Part One Fundamentals4.1 Problem Formulations and Key ConceptsMotivated by different realworld applications, researchers have considered a wide range of problems over a variety of different types ofcorpora. We now examine the key concepts involved in these problems.This discussion also serves as a loose grouping of the major problems,where each group consists of problems that are suitable for similartreatment as learning tasks.4.1 Problem Formulations and Key Concepts 254.1.1 Sentiment Polarity and Degrees of PositivityOne set of problems share the following general character given anopinionated piece of text, wherein it is assumed that the overall opinion in it is about one single issue or item, classify the opinion as fallingunder one of two opposing sentiment polarities, or locate its positionon the continuum between these two polarities. A large portion of workin sentimentrelated classificationregressionranking falls within thiscategory. Eguchi and Lavrenko 84 point out that the polarity or positivity labels so assigned may be used simply for summarizing the content of opinionated text units on a topic, whether they be positive ornegative, or for only retrieving items of a given sentiment orientationsay, positive.The binary classification task of labeling an opinionated documentas expressing either an overall positive or an overall negative opinion is called sentiment polarity classification or polarity classification.Although this binary decision task has also been termed sentiment classification in the literature, as mentioned above, in this survey we willuse sentiment classification to refer broadly to binary categorization,multiclass categorization, regression, andor ranking.Much work on sentiment polarity classification has been conductedin the context of reviews e.g., thumbs up or thumbs down formovie reviews. While in this context positive and negative opinions are often evaluative e.g., like vs. dislike, there are otherproblems where the interpretation of positive and negative is subtly different. One example is determining whether a political speech isin support of or opposition to the issue under debate 27, 294 a relatedtask is classifying predictive opinions in election forums into likely towin and unlikely to win 160. Since these problems are all concerned with two opposing subjective classes, as machine learning tasksthey are often amenable to similar techniques. Note that a number ofother aspects of politically oriented text, such as whether liberal orconservative views are expressed, have been explored since the labelsused in those problems can usually be considered properties of a set ofdocuments representing authors attitudes over multiple issues ratherthan positive or negative sentiment with respect to a single issue, we26 Classification and Extractiondiscuss them under a different heading further below viewpoints andperspectives, Section 4.1.4.The input to a sentiment classifier is not necessarily always strictlyopinionated. Classifying a news article into good or bad news has beenconsidered a sentiment classification task in the literature 168. Buta piece of news can be good or bad news without being subjectivei.e., without being expressive of the private states of the author forinstance, the stock price rose is objective information that is generallyconsidered to be good news in appropriate contexts. It is not our mainintent to provide a cleancut definition for what should be consideredsentiment polarity classification problems,1 but it is perhaps useful topoint out that a in determining the sentiment polarity of opinionatedtexts where the authors do explicitly express their sentiment throughstatements like this laptop is great, arguably objective informationsuch as long battery life2 is often used to help determine the overallsentiment b the task of determining whether a piece of objectiveinformation is good or bad is still not quite the same as classifying itinto one of several topicbased classes, and hence inherits the challengesinvolved in sentiment analysis and c as we will discuss in more detaillater, the distinction between subjective and objective information canbe subtle. Is long battery life objective Also consider the differencebetween the battery lasts 2 hours vs. the battery only lasts 2 hours.Related categories. An alternative way of summarizing reviews is toextract information on why the reviewers liked or disliked the product.Kim and Hovy 158 note that such pro and con expressions can differfrom positive and negative opinion expressions, although the two concepts  opinion I think this laptop is terrific and reason for opinionThis laptop only costs 399  are for the purposes of analyzingevaluative text strongly related. In addition to potentially formingthe basis for the production of more informative sentimentorientedsummaries, identifying pro and con reasons can potentially be used to1While it is of utter importance that the problem itself should be welldefined, it is ofless, if any, importance to decide which tasks should be labeled as polarity classificationproblems.2Whether this should be considered as an objective statement may be up for debate onecan imagine another reviewer retorting, you call that long battery life4.1 Problem Formulations and Key Concepts 27help decide the helpfulness of individual reviews evaluative judgmentsthat are supported by reasons are likely to be more trustworthy.Another type of categorization related to degrees of positivity isconsidered by Niu et al. 226, who seek to determine the polarity ofoutcomes improvement vs. death, say described in medical texts.Additional problems related to the determination of degree of positivity surround the analysis of comparative sentences 139. The mainidea is that sentences such as The new model is more expensive thanthe old one or I prefer the new model to the old model are importantsources of information regarding the authors evaluations.Rating inference ordinal regression. The more general problem ofrating inference, where one must determine the authors evaluation withrespect to a multipoint scale e.g., one to five stars for a review canbe viewed simply as a multiclass text categorization problem. Predicting degree of positivity provides more finegrained rating informationat the same time, it is an interesting learning problem in itself.But in contrast to many topicbased multiclass classificationproblems, sentimentrelated multiclass classification can also be naturally formulated as a regression problem because ratings are ordinal.It can be argued to constitute a special type of ordinal regressionproblem because the semantics of each class may not simply directlycorrespond to a point on a scale. More specifically, each class mayhave its own distinct vocabulary. For instance, if we are classifyingan authors evaluation into one of the positive, neutral, and negativeclasses, an overall neutral opinion could be a mixture of positive andnegative language, or it could be identified with signature words such asmediocre. This presents us with interesting opportunities to explorethe relationships between classes.Note the difference between rating inference and predicting strengthof opinion discussed in Section 4.1.2 for instance, it is possible to feelquite strongly high on the strength scale that something is mediocremiddling on the evaluation scale.Also, note that the label neutral is sometimes used as a label forthe objective class lack of opinion in the literature. In this survey,we use neutral only in the aforementioned sense of a sentiment that liesbetween positive and negative.28 Classification and ExtractionInterestingly, Cabral and Hortacsu 47 observe that neutral comments in feedback systems are not necessarily perceived by users aslying at the exact midpoint between positive and negative commentsrather, the information contained in a neutral rating is perceived byusers to be much closer to negative feedback than positive. On theother hand, they also note that in their data, sellers were less likelyto retaliate against neutral comments, as opposed to negatives . . . abuyer leaving a negative comment has a 40 chance of being hit back,while a buyer leaving a neutral comment only has a 10 chance ofbeing retaliated upon by the seller.Agreement. The opposing nature of polarity classes also gives rise toexploration of agreement detection, e.g., given a pair of texts, decidingwhether they should receive the same or differing sentimentrelatedlabels based on the relationship between the elements of the pair. Thisis often not defined as a standalone problem but considered as a subtask whose result is used to improve the labeling of the opinions held bythe entities involved 272, 294. A different type of agreement task hasalso been considered in the context of perspectives, where, for example,a label of conservative tends to indicate agreement with particularpositions on a wide variety of issues.4.1.2 Subjectivity Detection and Opinion IdentificationWork in polarity classification often assumes the incoming documentsto be opinionated. For many applications, though, we may need todecide whether a given document contains subjective information ornot, or identify which portions of the document are subjective. Indeed,this problem was the focus of the 2006 Blog track at TREC 227.At least one opiniontracking system rates subjectivity and sentimentseparately 108. Mihalcea et al. 209 summarize the evidence of several projects on subsentential analysis 12, 90, 289, 319 as followsthe problem of distinguishing subjective versus objective instances hasoften proved to be more difficult than subsequent polarity classification,so improvements in subjectivity classification promise to positivelyimpact sentiment classification.4.1 Problem Formulations and Key Concepts 29Early work by Hatzivassiloglou and Wiebe 120 examined theeffects of adjective orientation and gradability on sentence subjectivity. The goal was to tell whether a given sentence is subjective or notjudging from the adjectives appearing in that sentence. A number ofprojects address sentencelevel or subsentencelevel subjectivity detection in different domains 33, 156, 232, 255, 308, 315, 319, 326.Wiebeet al. 316 present a comprehensive survey of subjectivity recognitionusing different clues and features.Wilson et al. 320 address the problem of determining clauselevelopinion strength e.g., how mad are you. Note that the problem ofdetermining opinion strength is different from rating inference. Classifying a piece of text as expressing a neutral opinion giving it a midpoint score for rating inference does not equal classifying that piece oftext as objective lack of opinion one can have a strong opinion thatsomething is mediocre or soso.Recent work also considers relations between word sense disambiguation and subjectivity 307.Subjectivity detection or ranking at the document level can bethought of as having its roots in studies in genre classification seeSection 4.1.5 for more detail. For instance, Yu and Hatzivassiloglou326 achieve high accuracy 97 with a Naive Bayes classifier on aparticular corpus consisting of Wall Street Journal articles, where thetask is to distinguish articles under News and Business facts fromarticles under Editorial and Letter to the Editor opinions. This taskwas suggested earlier by Wiebe et al. 315, and a similar corpus wasexplored in previous work 308, 316. Work in this direction is not limited to the binary distinction between subjective and objective labels.Recent work includes the research by participants in the 2006 TRECBlog track 227 and others 69, 97, 222, 223, 234, 279, 316, 326.4.1.3 Joint TopicSentiment AnalysisOne simplifying assumption sometimes made by work on documentlevel sentiment classification is that each document under considerationis focused on the subject matter we are interested in. This is in partbecause one can often assume that the document set was created30 Classification and Extractionby first collecting only ontopic documents e.g., by first running atopicbased query through a standard search engine. However, it ispossible that there are interactions between topic and opinion thatmake it desirable to consider the two simultaneously for example,Rilof et al. 256 find that topicbased text filtering and subjectivity filtering are complementary in the context of experiments in informationextraction.Also, even a relevant opinionbearing document may contain offtopic passages that the user may not be interested in, and so one maywish to discard such passages.Another interesting case is when a document contains material onmultiple subjects that may be of interest to the user. In such a setting, it is useful to identify the topics and separate the opinions associated with each of them. Two examples of the types of documents forwhich this kind of analysis is appropriate are 1 comparative studiesof related products, and 2 texts that discuss various features, aspects,or attributes.34.1.4 Viewpoints and PerspectivesMuch work on analyzing sentiment and opinions in politically oriented text focuses on general attitudes expressed through texts thatare not necessarily targeted at a particular issue or narrow subject. Forinstance, Grefenstette et al. 112 experimented with determining thepolitical orientation of websites essentially by classifying the concatenation of all the documents found on that site. We group this type ofwork under the heading of viewpoints and perspectives, and includeunder this rubric work on classifying texts as liberal, conservative, libertarian, etc. 219, placing texts along an ideological scale 178, 202,or representing Israeli versus Palestinian viewpoints 186, 187.Although binary or nary classification may be used, here, theclasses typically correspond not to opinions on a single, narrowlydefined topic, but to a collection of bundled attitudes and beliefs.This could potentially enable different approaches from polarity3When the context is clear, we often use the term feature to refer to feature, aspect, orattribute in this survey.4.1 Problem Formulations and Key Concepts 31classification. On the other hand, if we treat the set of documents asa metadocument, and the different issues being discussed as metafeatures, then this problem still shares some common ground withpolarity classification or its multiclass, regression, and ranking variants. Indeed, some of the approaches explored in the literature for thesetwo problems individually could very well be adapted to work for eitherone of them.The other point of departure from the polarity classification problemis that the labels being considered are more about attitudes that donot naturally correspond with degree of positivity. While assigningsimple labels remains a classification problem, if we move farther awayand aim at serving more expressive and openended opinions to theuser, we need to solve extraction problems. For instance, one may beinterested in obtaining descriptions of opinions of a greater complexitythan simple labels drawn from a very small set, i.e., one might beseeking something more like achieving world peace is difficult thanlike mildly positive. In fact, much of the prior work on perspectivesand viewpoints seeks to extract more perspectiverelated informatione.g., opinion holders. The motivation was to enable multiperspectivequestion answering, where the user could ask questions such as what isMiss Americas perspective on world peace rather than a factbasedquestion e.g., who is the new Miss America. Naturally, such workis often framed in the context of extraction problems, the particularcharacteristics of which are covered in Section 4.9.4.1.5 Other NonFactual Information in TextResearchers have considered various affect types, such as the sixuniversal emotions 86 anger, disgust, fear, happiness, sadness, andsurprise 192, 9, 285. An interesting application is in humancomputerinteraction if a system determines that a user is upset or annoyed, forinstance, it could switch to a different mode of interaction 188.Other related areas of research include computational approachesfor humor recognition and generation 210. Many interesting affectualaspects of text like happiness or mood are also being explored inthe context of informal text resources such as weblogs 224. Potential32 Classification and Extractionapplications include monitoring levels of hateful or violent rhetoric,perhaps in multilingual settings 1.In addition to classification based on affect and emotion, anotherrelated area of research that addresses nontopicbased categorizationis that of determining the genre of texts 97, 98, 150, 153, 182, 277.Since subjective genres, such as editorial, are often one of the possiblecategories, such work can be viewed as closely related to subjectivitydetection. Indeed, this relation has been observed in work focused onlearning subjective language 316.There has also been research that concentrates on classifying documents according to their source or source style, with statisticallydetected stylistic variation 38 serving as an important cue. Authorship identification is perhaps the most salient example  Mosteller andWallaces 216 classic Bayesian study of the authorship of the Federalist Papers is one wellknown instance. ArgamonEngelson et al. 18consider the related problem of identifying not the particular authorof a text, but its publisher e.g., the New York Times vs. The DailyNews the work of Kessler et al. 153 on determining a documentsbrow e.g., highbrow vs. popular, or lowbrow has similar goals.Several recent workshops have been dedicated to style analysis in text15, 16, 17. Determining stylistic characteristics can be useful in multifaceted search 10.Another problem that has been considered in intelligence and security settings is the detection of deceptive language 46, 117, 329.4.2 FeaturesConverting a piece of text into a feature vector or other representation that makes its most salient and important features available is animportant part of datadriven approaches to text processing. There isan extensive body of work that addresses feature selection for machinelearning approaches in general, as well as for learning approaches tailored to the specific problems of classic text categorization and information extraction 101, 263. A comprehensive discussion of such workis beyond the scope of this survey. In this section, we focus on findingsin feature engineering that are specific to sentiment analysis.4.2 Features 334.2.1 Term Presence vs. FrequencyIt is traditional in information retrieval to represent a piece of text asa feature vector wherein the entries correspond to individual terms.One influential finding in the sentimentanalysis area is as follows.Term frequencies have traditionally been important in standard IR,as the popularity of tfidf weighting shows but in contrast, Pang et al.235 obtained better performance using presence rather than frequency.That is, binaryvalued feature vectors in which the entries merely indicate whether a term occurs value 1 or not value 0 formed a moreeffective basis for review polarity classification than did realvaluedfeature vectors in which entry values increase with the occurrence frequency of the corresponding term. This finding may be indicative of aninteresting difference between typical topicbased text categorizationand polarity classification While a topic is more likely to be emphasized by frequent occurrences of certain keywords, overall sentimentmay not usually be highlighted through repeated use of the same terms.We discussed this point previously in Section 3.2 on factors that makeopinion mining difficult.On a related note, hapax legomena, or words that appear a singletime in a given corpus, have been found to be highprecision indicatorsof subjectivity 316. Yang et al. 322 look at rare terms that are notlisted in a preexisting dictionary, on the premise that novel versions ofwords, such as bugfested, might correlate with emphasis and hencesubjectivity in blogs.4.2.2 Termbased Features Beyond Term UnigramsPosition information finds its way into features from time to time. Theposition of a token within a textual unit e.g., in the middle vs. nearthe end of a document can potentially have important effects on howmuch that token affects the overall sentiment or subjectivity statusof the enclosing textual unit. Thus, position information is sometimesencoded into the feature vectors that are employed 158, 235.Whether higherorder ngrams are useful features appears to be amatter of some debate. For example, Pang et al. 235 report that unigrams outperform bigrams when classifying movie reviews by sentiment34 Classification and Extractionpolarity, but Dave et al. 69 find that in some settings, bigrams andtrigrams yield better productreview polarity classification.Riloff et al. 254 explore the use of a subsumption hierarchy toformally define different types of lexical features and the relationshipsbetween them in order to identify useful complex features for opinionanalysis. Airoldi et al. 5 apply a Markov Blanket Classifier to thisproblem together with a metaheuristic search strategy called Tabusearch to arrive at a dependency structure encoding a parsimoniousvocabulary for the positive and negative polarity classes.The contrastive distance between terms  an example of a highcontrast pair of words in terms of the implicit evaluation polarity theyexpress is delicious and dirty  was used as an automaticallycomputed feature by Snyder and Barzilay 272 as part of a ratinginference system.4.2.3 Parts of SpeechPartofspeech POS information is commonly exploited in sentimentanalysis and opinion mining. One simple reason holds for general textual analysis, not just opinion mining partofspeech tagging can beconsidered to be a crude form of word sense disambiguation 318.Adjectives have been employed as features by a number ofresearchers 217, 303. One of the earliest proposals for the datadrivenprediction of the semantic orientation of words was developed foradjectives 119. Subsequent work on subjectivity detection revealeda high correlation between the presence of adjectives and sentencesubjectivity 120. This finding has often been taken as evidence thatcertain adjectives are good indicators of sentiment, and sometimeshas been used to guide feature selection for sentiment classification,in that a number of approaches focus on the presence or polarity ofadjectives when trying to decide the subjectivity or polarity status oftextual units, especially in the unsupervised setting. Rather than focusing on isolated adjectives, Turney 298 proposed to detect documentsentiment based on selected phrases, where the phrases are chosen viaa number of prespecified partofspeech patterns, most including anadjective or an adverb.4.2 Features 35The fact that adjectives are good predictors of a sentence beingsubjective does not, however, imply that other parts of speech do notcontribute to expressions of opinion or sentiment. In fact, in a studyby Pang et al. 235 on moviereview polarity classification, using onlyadjectives as features was found to perform much worse than using thesame number of most frequent unigrams. The researchers point outthat nouns e.g., gem and verbs e.g., love can be strong indicators for sentiment. Riloff et al. 257 specifically studied the extractionof subjective nouns e.g., concern, hope via bootstrapping. Therehave been several targeted comparisons of the effectiveness of adjectives, verbs, and adverbs, where further subcategorization often playsa role 34, 221, 316.4.2.4 SyntaxThere have also been attempts at incorporating syntactic relationswithin feature sets. Such deeper linguistic analysis seems particularlyrelevant with short pieces of text. For instance, Kudo and Matsumoto173 report that for two sentencelevel classification tasks, sentimentpolarity classification and modality identification opinion, assertion, or description, a subtreebased boosting algorithm usingdependencytreebased features outperformed the bagofwords baseline although there were no significant differences with respect to usingngrambased features. Nonetheless, the use of higherorder ngramsand dependency or constituentbased features has also been considered for documentlevel classification Dave et al. 69 on the one handand Gamon 103, Matsumoto et al. 204, and Ng et al. 222 on theother hand come to opposite conclusions regarding the effectiveness ofdependency information. Parsing the text can also serve as a basis formodeling valence shifters such as negation, intensifiers, and diminishers152. Collocations and more complex syntactic patterns have also beenfound to be useful for subjectivity detection 255, 316.4.2.5 NegationHandling negation can be an important concern in opinion andsentimentrelated analysis. While the bagofwords representations36 Classification and Extractionof I like this book and I dont like this book are consideredto be very similar by most commonlyused similarity measures, theonly differing token, the negation term, forces the two sentences intoopposite classes. There does not really exist a parallel situation inclassic IR where a single negation term can play such an instrumentalrole in classification except in cases like this document is about carsvs. this document is not about cars.It is possible to deal with negations indirectly as a secondorderfeature of a text segment, that is, where an initial representation, suchas a feature vector, essentially ignores negation, but that representationis then converted into a different representation that is negationaware.Alternatively, as was done in previous work, negation can be encodeddirectly into the definitions of the initial features. For example, Dasand Chen 66 propose attaching NOT to words occurring close tonegation terms such as no or dont, so that in the sentence Idont like deadlines, the token like is converted into the new tokenlikeNOT.However, not all appearances of explicit negation terms reverse thepolarity of the enclosing sentence. For instance, it is incorrect to attachNOT to best in No wonder this is considered one of the best.Na et al. 220 attempt to model negation more accurately. They lookfor specific partofspeech tag patterns where these patterns differ fordifferent negation words, and tag the complete phrase as a negationphrase. For their dataset of electronics reviews, they observe about 3improvement in accuracy resulting from their modeling of negations.Further improvement probably needs deeper syntactic analysis of thesentence 152.Another difficulty with modeling negation is that negation canoften be expressed in rather subtle ways. Sarcasm and irony can bequite difficult to detect, but even in the absence of such sophisticatedrhetorical devices, we still see examples such as it avoids all clichesand predictability found in Hollywood movies internet review byMargie24  the word avoid here is an arguably unexpectedpolarity reverser. Wilson et al. 319 discuss other complex negationeffects.4.2 Features 374.2.6 TopicOriented FeaturesInteractions between topic and sentiment play an important role inopinion mining. For example, in a hypothetical article on Walmart, thesentences Walmart reports that profits rose and Target reports thatprofits rose could indicate completely different types of news good vs.bad regarding the subject of the document, Walmart 116. To someextent, topic information can be incorporated into features.Mullen and Collier 217 examine the effectiveness of various featuresbased on topic e.g., they take into account whether a phrase follows areference to the topic under discussion under the experimental condition that topic references are manually tagged. Thus, for example, ina review of a particular work of art or music, references to the itemreceive a THIS WORK tag.For the analysis of predictive opinions e.g., whether a message Mwith respect to party P predicts P to win, Kim and Hovy 160 proposeto employ feature generalization. Specifically, for each sentence in M ,each party name and candidate name is replaced by PARTY i.e., P or OTHER not P . Patterns such as PARTY will win, go PARTYagain, and OTHER will win are then extracted as ngram features.This scheme outperforms using simple ngram features by about 10 inaccuracy when classifying which party a given message predicts to win.Topicsentiment interaction has also been modeled through parsetree features, especially in opinion extraction tasks. Relationshipsbetween candidate opinion phrases and the given subject in a dependency tree can be useful in such settings 244.Part Two ApproachesThe approaches we will now discuss all share the common theme ofmapping a given piece of text, such as a document, paragraph, orsentence, to a label drawn from a prespecified finite set or to a realnumber.4 As discussed in Section 4.1, opinionoriented classification canrange from sentimentpolarity categorization in reviews to determining4However, unlike classification and regression, ranking does not require such a mapping foreach individual document.38 Classification and Extractionthe strength of opinions in news articles to identifying perspectivesin political debates to analyzing mood in blogs. Part of what is particularly interesting about these problems is the new challenges andopportunities that they present to us. In the remainder of this section,we examine different solutions proposed in the literature to these problems, loosely organized around different aspects of machine learningapproaches. Although these aspects may seem to be general themesunderlying most machine learning problems, we attempt to highlightwhat is unique for sentiment analysis and opinion mining tasks. Forinstance, some unsupervised learning approaches follow a sentimentspecific paradigm for how labels for words and phrases are obtained.Also, supervised and semisupervised learning approaches for opinionmining and sentiment analysis differ from standard approaches to classification tasks in part due to the different features involved but wealso see a great variety of attempts at modeling various kinds of relationships between items, classes, or subdocument units. Some of theserelationships are unique to our tasks some become more imperative tomodel due to the subtleties of the problems we address.The rest of this section is organized as follows. Section 4.3 covers theimpact that the increased availability of labeled data has had, includingthe rise of supervised learning. Section 4.4 considers issues surrounding topic and domain dependencies. Section 4.5 describes unsupervisedapproaches. We next consider incorporating relationships between various types of entities Section 4.6. This is followed by a section onincorporating discourse structure 4.7. Section 4.8 is concerned withthe use of language models. Finally, Section 4.9 investigates certainissues in extraction that are somewhat particular to it, and thus arenot otherwise discussed in the sections that precede it. One such issueis the identification of features and expressions of opinions in reviews.Another set of issues arise when opinionholder identification needs tobe applied.4.3 The Impact of Labeled DataWork up to the early 1990s on sentimentrelated tasks, such as determination of point of view and other types of complex recognition4.3 The Impact of Labeled Data 39problems, generally assumed the existence of subsystems for sometimes rather sophisticated NLP tasks, ranging from parsing to the resolution of pragmatic ambiguities 121, 262, 310, 311, 313. Given thestate of the art of NLP at the time and, just as importantly, the lack ofsufficient amounts of appropriate labeled data, the research describedin these early papers necessarily considered only proposals for systemsor prototype systems without largescale empirical evaluation typically, no learning component was involved an interesting exception isWiebe and Bruce 306, who proposed but did not evaluate the useof decomposable graphical models. Operational systems were focusedon simpler classification tasks, relatively speaking e.g., categorizationaccording to affect, and relied instead on relatively shallow analysisbased on manually constructed discriminantword lexicons 133, 296,since with such a lexicon in hand, one can classify a text unit by considering which indicator terms or phrases from the lexicon appear inthe given text.The rise of the widespread availablity to researchers of organizedcollections of opinionated documents two examples financialnewsdiscussion boards and review aggregation sites such as Epinions andof other corpora of more general texts e.g., newswire and of otherresources e.g., WordNet was a major contributor to a large shift indirection toward datadriven approaches. To begin with, the availabilityof the raw texts themselves made it possible to learn opinionrelevantlexicons in an unsupervised fashion, as is discussed in more detail inSection 4.5.1, rather than create them manually. But the increase in theamount of labeled sentimentrelevant data, in particular  where thelabels are derived either through explicit researcherinitiated manualannotation efforts or by other means see Section 7.1.1  was a majorcontributing factor to activity in both supervised and unsupervisedlearning. In the unsupervised case, described in Section 4.5, it facilitated research by making it possible to evaluate proposed algorithmsin a largescale fashion. Unsupervised and supervised learning alsobenefitted from the improvements to subcomponent systems fortagging, parsing, and so on that occurred due to the application ofdatadriven techniques in those areas. And, of course, the importanceto supervised learning of having access to labeled data is paramount.40 Classification and ExtractionOne very active line of work can be roughly glossed as the application of standard textcategorization algorithms, surveyed by Sebastiani 263, to opinionoriented classification problems. For example,Pang et al. 235 compare Naive Bayes, Support Vector Machines,and maximumentropybased classification on the sentimentpolarityclassification problem for movie reviews. More extensive comparisonsof the performance of standard machine learning techniques with othertypes of features or feature selection schemes have been engaged inlater work 5, 69, 103, 204, 217 see Section 4.2 for more detail.We note that there has been some research that explicitly considersregression or ordinalregression formulations of opinionmining problems 109, 201, 233, 320 example questions include, how positive isthis text and how strongly held is this opinionAnother role that labeled data can play is in lexicon induction,although, as detailed in Section 4.5.1, the use of the unsupervisedparadigm is more common. Morinaga et al. 215 and Bethard et al.37 create an opinionindicator lexicon by looking for terms that tendto be associated more highly with subjectivegenre newswire, such aseditorials, than with objectivegenre newswire. Das and Chen 66, 67start with a manually created lexicon specific to the finance domainexample terms bull, bear, but then assign discrimination weightsto the items in the lexicon based on their cooccurrence with positivelylabeled vs. negatively labeled documents.Other topics related to supervised learning are discussed in some ofthe more specific sections that follow.4.4 Domain Adaptation and TopicSentiment Interaction4.4.1 Domain ConsiderationsThe accuracy of sentiment classification can be influenced by thedomain of the items to which it is applied 21, 40, 88, 249, 298.One reason is that the same phrase can indicate different sentimentin different domains recall the Bob Bland example mentioned earlier, where go read the book most likely indicates positive sentiment for book reviews, but negative sentiment for movie reviews orconsider Turneys 298 observation that unpredictable is a positive4.4 Domain Adaptation and TopicSentiment Interaction 41description for a movie plot but a negative description for a cars steering abilities. Difference in vocabularies across different domains alsoadds to the difficulty when applying classifiers trained on labeled datain one domain to test data in another.Several studies show concrete performance differences from domainto domain. In an experiment auxiliary to their main work, Dave et al.69 apply a classifier trained on a preassembled dataset of reviews ofa certain type to product reviews of a different type. But they do notinvestigate the effect of trainingtest mismatch in detail. Engstrom 88studies how the accuracy of sentiment classification can be influencedby topic. Read 249 finds standard machine learning techniques foropinion analysis to be both domaindependent with domains rangingfrom movie reviews to newswire articles and temporally dependentbased on datasets spanning different ranges of time periods but writtenat least one year apart. Owsley et al. 229 also show the importanceof building a domainspecific classifier.Aue and Gamon 21 explore different approaches to customizing asentiment classification system to a new target domain in the absence oflarge amounts of labeled data. The different types of data they considerrange from lengthy movie reviews to short, phraselevel user feedbackfrom web surveys. Due to significant differences in these domains alongseveral dimensions, simply applying the classifier learned on data fromone domain barely outperforms the baseline for another domain. In fact,with 100 or 200 labeled items in the target domain, an EM algorithmthat utilizes indomain unlabeled data and ignores outofdomain dataaltogether outperforms the method based exclusively on both in andoutofdomain labeled data.Yang et al. 321 take the following simple approach to domaintransfer they find features that are good subjectivity indicators in bothof two different domains in their case, movie reviews versus productreviews, and consider these features to be good domainindependentfeatures.Blitzer et al. 40 explicitly address the domain transfer problem for sentiment polarity classification by extending the structuralcorrespondence learning algorithm SCL 11, achieving an average of46 improvement over a supervised baseline for sentiment polarity42 Classification and Extractionclassification of 5 different types of product reviews mined from Amazon.com. The success of SCL depends on the choice of pivot featuresin both domains, based on which the algorithm learns a projectionmatrix that maps features in the target domain into the feature spaceof the source domain. Unlike previous work that applied SCL for tagging, where frequent words in both domains happened to be goodpredictors for the target labels partofspeech tags, and were therefore good candidates for pivots, here the pivots are chosen from thosewith highest mutual information with the source label. The projection is able to capture correspondences in terms of expressed sentiment polarity between predictable for book reviews and poorlydesigned for kitchen appliance reviews. Furthermore, they also showthat a measure of domain similarity can correlate well with the easeof adaptation from one domain to another, thereby enabling betterscheduling of annotation efforts.Crosslingual adaptation. Much of the literature on sentiment analysis has focused on text written in English. As a result, most of theresources developed, such as lexica with sentiment labels, are in English.Adapting such resources to other languages is related to domain adaptation the former aims at adapting from the source language to thetarget language in order to utilize existing resources in the source language whereas the latter seeks to adapt from one domain to anotherin order to utilize the labeled data available in the source domain.Not surprisingly, we observe parallel techniques instead of projectingunseen tokens from the new domain into the old one via cooccurrenceinformation in the corpus 40, expressions in the new language canbe aligned with expressions in the language with existing resources.For instance, one can determine crosslingual projections through bilingual dictionaries 209, or parallel corpora 159, 209. Alternatively,one can simply apply machine translation as a sentimentanalysis preprocessing step 32.4.4.2 Topic and subtopic or feature ConsiderationsEven when one is handling documents in the same domain, there isstill an important and related source of variation document topic. It is4.4 Domain Adaptation and TopicSentiment Interaction 43true that sometimes the topic is predetermined, such as in the case offreeform responses to survey questions. However, in many sentimentanalysis applications, topic is another important consideration forinstance, one may be searching the blogosphere just for opinionatedcomments about Cornell University.One approach to integrating sentiment and topic when one islooking for opinionated documents on a particular userspecified topicis to simply first perform one analysis pass, say for topic, and then analyze the results with respect to sentiment 134. See Sebastiani 263for a survey of machine learning approaches to topicbased text categorization. Such a twopass approach was taken by a number of systemsat the TREC Blog track in 2006, according to Ounis et al. 227, andothers 234. Alternatively, one may jointly model topic and sentimentsimultaneously 84, 206, or treat one as a prior for the other 85.But even in the case where one is working with documents knownto be ontopic, not all the sentences within these documents need to beontopic. Hurst and Nigam 134, 225 propose a twopass process similarto that mentioned above, where each sentence in the document is firstlabeled as ontopic or offtopic, and sentiment analysis is conductedonly for those that are found to be ontopic. Their work relies on acollocation assumption that if a sentence is found to be topical and toexhibit a sentiment polarity, then the polarity is expressed with respectto the topic in question. This assumption is also used by Nasukawa andYi 221 and Gamon 103.A related issue is that it is also possible for a document to containmultiple topics. For instance, a review can be a comparison of two products. Or, even when a single item is discussed in a document, one canconsider features or aspects of the product to represent multiple subtopics. If all but the main topic can be disregarded, then one possibility is as follows simply consider the overall sentiment detected withinthe document  regardless of the fact that it may be formed froma mixture of opinions on different topics  to be associated with theprimary topic, leaving the sentiment toward other topics undeterminedindeed, these other topics may never be identified. But it is morecommon to try to identify the topics and then determine the opinionsregarding each of these topics separately. In some work, the important44 Classification and Extractiontopics are predefined, making this task easier 323. In other workin extraction, this is not the case the problem of the identification ofproduct features is addressed in Section 4.9, and Section 4.6.3 discussestechniques that incorporate relationships between different features.4.5 Unsupervised Approaches4.5.1 Unsupervised Lexicon InductionQuite a number of unsupervised learning approaches take the tack offirst creating a sentiment lexicon in an unsupervised manner, and thendetermining the degree of positivity or subjectivity of a text unit viasome function based on the positive and negative or simply subjectiveindicators, as determined by the lexicon, within it. Early examples ofsuch an approach include Hatzivassiloglou and Wiebe 120, Turney298, and Yu and Hatzivassiloglou 326. Some interesting variants ofthis general technique are to use the polarity of the previous sentence asa tiebreaker when the scoring function does not indicate a definitiveclassification of a given sentence 130, or to incorporate informationdrawn from some labeled data as well 33.A crucial component to applying this type of technique is, of course,the creation of the lexicon via the unsupervised labeling of words orphrases with their sentiment polarity also referred to as semantic orientation in the literature or subjectivity status 12, 45, 89, 90, 91, 92,119, 130, 143, 146, 257, 286, 288, 289, 290, 299, 303, 304.In early work, Hatzivassiloglou and McKeown 119 present anapproach based on linguistic heuristics.5 Their technique is built onthe fact that in the case of polarity classification, the two classes ofinterest represent opposites, and we can utilize opposition constraintsto help make labeling decisions. Specifically, constraints between pairsof adjectives are induced from a large corpus by looking at whetherthe two words are linked by conjunctions such as but evidence foropposing orientations elegant but overpriced or and evidencefor the same orientation clever and informative. The task is thencast as a clustering or binarypartitioning problem where the inferredconstraints are to be obeyed.5For the purposes of the current discussion, we ignore the supervised aspects of their work.4.5 Unsupervised Approaches 45Once the clustering has been completed, the labels of positiveorientation and negative orientation need to be assigned ratherthan use external information to make this decision, Hatzivassiloglouand McKeown 119 simply give the positive orientation label to theclass whose members have the highest average frequency. But in otherwork, seed words for which the polarity is already known are assumed tobe supplied, in which case labels can be determined by propagating thelabels of the seed words to terms that cooccur with them in general textor in dictionary glosses, or to synonyms, words that cooccur with themin other WordNetdefined relations, or other related words and, alongthe same lines, opposite labels can be given based on similar information 12, 20, 89, 90, 130, 146, 148, 155, 288, 298, 299. The joint use ofmutual information and cooccurrence in a general corpus with a smallset of seed words, a technique employed by a number of researchers, wassuggested by Turney 298 his idea was to essentially compare whethera phrase has a greater tendency to cooccur within certain context windows with the word poor or with the word excellent, taking care toaccount for the frequencies with which poor and excellent occur,where the data on which such computations are to be made come fromthe results of particular types of Web searchengine queries.Much of the work cited above focuses on identifying the prior polarity of terms or phrases, to use the terminology of Wilson et al. 319, orwhat we might by extension call terms and phrases prior subjectivitystatus, meaning the semantic orientation that these items might be saidto generally bear when taken out of context. Such prior information ismeant, of course, to serve toward further identifying contextual polarityor subjectivity 242, 319.Lexicons for generation. It is worth noting that Higashinaka et al.122 focus on a lexiconinduction task that facilitates natural languagegeneration. They consider the problem of learning a dictionary thatmaps semantic representations to verbalizations, where the data comesfrom reviews. Although reviews are not explicitly marked up withrespect to their semantics, they do contain explicit rating and aspectindicators. For example, from such data, they learn that one way toexpress the concept atmosphere rating5 is nice and comfortable.46 Classification and Extraction4.5.2 Other Unsupervised ApproachesBootstrapping is another approach. The idea is to use the outputof an available initial classifier to create labeled data, to which asupervised learning algorithm may be applied. Riloff and Wiebe 255use this method in conjunction with an initial highprecision classifierto learn extraction patterns for subjective expressions. An interesting,if simple, pattern discovered the noun fact, as in The fact is . . . ,exhibits high correlation with subjectivity. Kaji and Kitsuregawa 142use a similar method to automatically construct a corpus of HTMLdocuments with polarity labels. Similar work involving selftraining isdescribed in Wiebe and Riloff 314 and Riloff et al. 257.Pang and Lee 234 experiment with a different type of unsupervised approach. The problem they consider is to rank search resultsfor reviewseeking queries so that documents that contain evaluativetext are placed ahead of those that do not. They propose a simpleblank slate method based on the rarity of words within the searchresults that are retrieved as opposed to within a training corpus. Theintuition is that words that appear frequently within the set of documents returned for a narrow topic the search set are more likely todescribe objective information, since objective information should tendto be repeated within the search set in contrast, it would seem thatpeoples opinions and how they express them may differ. Counterintuitively, though, Pang and Lee find that when the vocabulary to beconsidered is restricted to the most frequent words in the search set asa noisereduction measure, the subjective documents tend to be thosethat contain a higher percentage of words that are less rare, perhapsdue to the fact that most reviews cover the main features or aspects ofthe object being reviewed. This echoes our previous observation thatunderstanding the objective information in a document can be critical for understanding the opinions and sentiment it expresses. Theperformance of this simple method is on par with that of a methodbased on a stateoftheart subjectivity detection system, OpinionFinder 255, 314.A comparison of supervised and unsupervised methods can be foundin Chaovalit and Zhou 55.4.6 Classification Based on Relationship Information 474.6 Classification Based on Relationship Information4.6.1 Relationships Between Sentences and BetweenDocumentsOne interesting characteristic of documentlevel sentiment analysis isthe fact that a document can consist of subdocument units paragraphs or sentences with different, sometimes opposing labels, wherethe overall sentiment label for the document is a function of the setor sequence of labels at the subdocument level. As an alternative totreating a document as a bag of features, then, there have been various attempts to model the structure of a document via analysis ofsubdocument units, and to explicitly utilize the relationships betweenthese units, in order to achieve a more accurate global labeling. Modeling the relationships between these subdocument units may lead tobetter subdocument labeling as well.An opinionated piece of text can often consist of evaluative portionsthose that contribute to the overall sentiment of the document, e.g.,this is a great movie and nonevaluative portions e.g., the Powerpuff girls learned that with great power comes great responsibility.The overlap between the vocabulary used for evaluative portions andnonevaluative portions makes it particularly important to model thecontext in which these text segments occur. Pang and Lee 232 propose a twostep procedure for polarity classification for movie reviews,wherein they first detect the objective portions of a document e.g.,plot descriptions and then apply polarity classification to the remainder of the document after the removal of these presumably uninformative portions. Importantly, instead of making the subjectiveobjectivedecision for each sentence individually, they postulate that there mightbe a certain degree of continuity in subjectivity labels an author usually does not switch too frequently between being subjective and beingobjective, and incorporate this intuition by assigning preferences forpairs of nearby sentences to receive similar labels. All the sentences inthe document are then labeled as being either subjective or objectivethrough a collective classification process, where this process employsa reformulation of the task as one of finding a minimum st cut in theappropriate graph 165. Two key properties of this approach are 1 it48 Classification and Extractionaffords the finding of an exact solution to the underlying optimizationproblem via an algorithm that is efficient both in theory and in practice, and 2 it makes it easy to integrate a wide variety of knowledgesources both about individual preferences that items may have for oneor the other class and about the pairwise preferences that items mayhave for being placed in the same class regardless of which particularclass that is. Followup work has used alternate techniques to determineedge weights within a minimumcut framework for various types ofsentimentrelated binary classification problems at the document level3, 27, 111, 294. The more general ratinginference problem can also, inspecial cases, be solved using a minimumcut formulation 233. Othershave considered more sophisticated graphbased techniques 109.4.6.2 Relationships Between Discourse ParticipantsAn interesting setting for opinion mining is when the texts to be analyzed form part of a running discussion, such as in the case of individualturns in political debates, posts to online discussion boards, and comments on blog posts. One fascinating aspect of this kind of setting is therich information source that references between such texts represent,since such information can be exploited for better collective labeling ofthe set of documents. Utilizing such relationships can be particularlyhelpful because many documents in the settings we have describedcan be quite terse or complicated, and hence difficult to classify ontheir own, but we can easily categorize a difficult document if we findwithin it indications of agreement with a clearly, say, positive text.Based on manual examination of 100 responses in newsgroupsdevoted to three distinct controversial topics abortion, gun control andimmigration, Agrawal et al. 4 observe that the relationship betweentwo individuals in the respondedto network is more likely to beantagonistic  overall, 74 of the responses examined were found tobe antagonistic, whereas only 7 were found to be reinforcing. By thenassuming that respondto links imply disagreement, they effectivelyclassify users into opposite camps via graph partitioning, outperforming methods that depend solely on the textual information within aparticular document.4.6 Classification Based on Relationship Information 49Similarly, Mullen and Malouf 218 examine quoting behavioramong users of the politics.com discussion site  a user can refer toanother post by quoting part of it or by addressing the other user byname or user ID  who have been classified as either liberal or conservative. The researchers find that a significant fraction of the postsof interest to them contain quoted material, and that, in contrast tointerblog linking patterns discussed in Adamic and Glance 2, whereliberal and conservative blog sites were found to tend to link to sites ofsimilar political orientations, and in accordance with the Agrawal et al.4 findings cited above, politics.com posters tend to quote users at theopposite end of the political spectrum. To perform the final politicalorientation classification, users are clustered so that those who tendto quote the same entities are placed in the same cluster. Efron 83similarly uses cocitation analysis for the same problem.Rather than assume that quoting always indicates agreement ordisagreement regardless of the context, Thomas et al. 294 build anagreement detector for the task of analyzing transcripts of congressionalfloordebates, where the classifier categorizes certain explicit referencesto other speakers as representing agreement e.g., I heartily supportMr Smiths views or disagreement. They then encode evidence ofa high likelihood of agreement between two speakers as a relationshipconstraint between the utterances made by the speakers, and collectively classify the individual speeches as to whether they support oroppose the legislation under discussion, using a minimumcut formulation of the classification problem, as described above. Followup workattempts to make more refined use of disagreement information 27.4.6.3 Relationships Between Product FeaturesPopescu and Etzioni 244 treat the labeling of opinion words regarding product features as a collective labeling process. They proposean iterative algorithm wherein the polarity assignments for individualwords are collectively adjusted through a relaxationlabeling process.Starting from global word labels computed over a large text collection that reflect the sentiment orientation for each particular word ingeneral settings, Popescu and Etzioni gradually redefine the label from50 Classification and Extractionone that is generic to one that is specific to a review corpus to one thatis specific to a given product feature to, finally, one that is specific to theparticular context in which the word occurs. They make sure to respectsentencelevel local constraints that opinions connected by connectivessuch as but or and should receive opposite or the same polarities.The idea of utilizing discourse information to help with theinference of relationships between product attributes can also befound in the work of Snyder and Barzilay 272, who utilize agreementinformation in a task where one must predict ratings for multipleaspects of the same item e.g., food and ambiance for a restaurant.Their approach is to construct a linear classifier to predict whetherall aspects of a product are given the same rating, and combine thisprediction with that of individualaspect classifiers so as to minimizea certain loss function which they term the grief. Interestingly,Snyder and Barzilay 272 give an example where a collection of independent aspectrating predictors cannot assign a correct set of aspectratings, but augmentation with their agreement classification allowsperfect rating assignment in their specific example, the agreementclassifier is able to use the presence of the phrase but not to predicta contrasting rating between two aspects. An important observationthat Snyder and Barzilay 272 make about their formulation is thathaving the piece of information that all aspect ratings agree cuts downthe space of possible rating tuples to a far greater degree than havingthe information that not all the aspect ratings are the same.Note that the considerations discussed here relate to the topicspecific nature of opinions that we discussed in the context of domainadaptation in Section 4.4.4.6.4 Relationships Between ClassesRegression formulations where we include ordinal regression under thisumbrella term are quite wellsuited to the rating reference problemof predicting the degree of positivity in opinionated documents suchas product reviews, and to similar problems such as determining thestrength with which an opinion is held. In a sense, regression implicitly models similarity relationships between classes that correspond to4.6 Classification Based on Relationship Information 51points on a scale, such as the number of stars given by a reviewer. Incontrast, standard multiclass categorization focuses on capturing thedistinct features present in each class, and ignores the fact that 5 starsis much more like 4 stars than 2 stars. On a movie review dataset,Pang and Lee 233 observe that a onevsall multiclass categorization scheme can outperform regression for a threeclass classificationproblem positive, neutral, and negative, perhaps due to each classexhibiting a sufficiently distinct vocabulary, but for more finegrainedclassification, regression emerges as the better of the two.Furthermore, while regressionbased models implicitly encode theintuition that similar items should receive similar labels, Pang and Lee233 formulate rating inference as a metric labeling problem164, sothat a natural notion of distance between classes 2 stars and 3stars are more similar to each other than 1 star and 4 stars areis captured explicitly. More specifically, an optimal labeling is computedthat balances the output of a classifier that considers items in isolationwith the importance of assigning similar labels to similar items.Koppel and Schler 167 consider a similar version of this problem,but where one of the classes, corresponding to objective, does not lieon the positivetonegative continuum. Goldberg and Zhu 109 presenta graphbased algorithm that addresses the rating inference problemin the semisupervised learning setting, where a closedform solutionto the underlying optimization problem is found through computationon a matrix induced by a graph representing interdocument similarityrelationships, and the loss function encodes the desire for similar itemsto receive similar labels. Mao and Lebanon 201 Mao and Lebanon200 is a shorter version propose to use isotonic conditional randomfields to capture the ordinal labels of local sentencelevel sentiments.Given words that are strongly associated with positive and negativesentiment, they formulate constraints on the parameters to reflect theintuition that adding a positive negative word should affect the localsentiment label positively negatively.Wilson et al. 320 treat intensity classification e.g., classifying anopinion according to its strength as an ordinal regression task.McDonald et al. 205 leverage relationships between labels assignedat different classification stages, such as the word level or sentence level,52 Classification and Extractionfinding that a finetocoarse categorization procedure is an effectivestrategy.4.7 Incorporating Discourse StructureCompared to the case for traditional topicbased information accesstasks, discourse structure e.g., twists and turns in documents tendsto have more effect on overall sentiment labels. For instance, Pang et al.235 observe that some form of discourse structure modeling can helpto extract the correct label in the following exampleI hate the Spice Girls. . . . 3 things the author hatesabout them. . .Why I saw this movie is a really, really,really long story, but I did, and one would think Iddespise every minute of it. But. . . Okay, Im reallyashamed of it, but I enjoyed it. I mean, I admit itsa really awful movie, . . . they act wacky as hell . . . theninth floor of hell . . . a cheap beep movie . . . The plotis such a mess that its terrible. But I loved it.In spite of the predominant number of negative sentences, the overallsentiment toward the movie under discussion is positive, largely due tothe order in which these sentences are presented. Needless to say, suchinformation is lost in a bagofwords representation.Early work attempts to partially address this problem via incorporating location information in the feature set 235. Specifically, theposition at which a token appears can be appended to the token itself toform positiontagged features, so that the same unigram appearing in,say, the first quarter and the last quarter of the document are treatedas two different features but the performance of this simple schemedoes not differ greatly from that which results from using unigramsalone.On a related note, it has been observed that position matters inthe context of summarizing sentiment in a document. In particular, incontrast to topicbased text summarization, where the beginnings ofarticles usually serve as strong baselines in terms of summarizing theobjective information in them, the last n sentences of a review have4.8 Language Models 53been shown to serve as a much better summary of the overall sentimentof the document than the first n sentences, and to be almost as goodas using the n most automaticallycomputed subjective sentences, interms of how accurately they represent the overall sentiment of thedocument 232.Theories of lexical cohesion motivate the representation used byDevitt and Ahmad 73 for sentiment polarity classification of financial news.Another way of capturing discourse structure information in documents is to model the global sentiment of a document as a trajectory of local sentiments. For example, Mao and Lebanon 200 proposeusing sentiment flow as a sequential model to represent an opinionateddocument. More specifically, each sentence in the document receives alocal sentiment score from an isotonicconditionalrandomfieldbasedsentence level predictor. The sentiment flow is defined as a functionh  0,1  O the ordinal set, where the interval t  1n,tn ismapped to the label of the tth sentence in a document with n sentences. The flow is then smoothed out through convolution with asmoothing kernel. Finally, the distances between two flows e.g., Lp distance between the two smoothed, continuous functions should reflect,to some degree, the distances between global sentiments. On a smalldataset, Mao and Lebanon observe that the sentiment flow representation especially when objective sentences are excluded outperforms aplain bagofwords representation in predicting global sentiment witha nearest neighbor classifier.4.8 Language ModelsThe rise of the use of language models in information retrieval hasbeen an interesting recent development 65, 177, 179, 243. They havebeen applied to various opinionmining and sentimentanalysis tasks,and in fact the subjectivityextraction work of Pang and Lee 232 is ademo application for the heavily languagemodelingoriented LingPipesystem.66httpaliasi.comlingpipedemostutorialsentimentreadme.html.54 Classification and ExtractionOne characteristic of language modeling approaches that differentiates them somewhat from other classificationoriented datadriventechniques we have discussed so far is that language models are oftenconstructed using labeled data, but, given that they are mechanismsfor assigning probabilities to text rather than labels drawn from a finiteset, they cannot, strictly speaking, be defined as either supervised orunsupervised classifiers. On the other hand, there are various ways toconvert their output to labels when necessary.An example of work in the languagemodeling vein is that of Eguchiand Lavrenko 84, who rank sentences by both sentiment relevancy andtopic relevancy, based on previous work on relevance language models179. They propose a generative model that jointly models sentimentwords, topic words, and sentiment polarity in a sentence as a triple.Lin and Hauptmann 186 consider the problem of examining whethertwo collections of texts represent different perspectives. In their study,employing Reuters data, two examples of different perspectives are thePalestinian viewpoint vs. the Israeli viewpoint in written text and Bushvs. Kerry in presidential debates. They base their notion of difference inperspective upon the KullbackLeibler KL divergence between posterior distributions induced from document collection pairs, and discoverthat the KL divergence between different aspects is an order of magnitude smaller than that between different topics. This perhaps providesyet another reason that opinionoriented classification has been foundto be more difficult than topicbased classification.Research employing probabilistic latent semantic analysis PLSA125 or latent Dirichlet allocation LDA 39 can also be cast aslanguagemodeling work 41, 195, 206. The basic idea is to infer language models that correspond to unobserved factors in the data, withthe hope that the factors that are learned represent topics or sentimentcategories.4.9 Special Considerations for ExtractionOpinionoriented extraction. Many applications, such as summarization or question answering, require working with pieces of informationthat need to be pulled from one or more textual units. For example,4.9 Special Considerations for Extraction 55a multiperspective questionanswering MPQA system might need torespond to opinionoriented questions such as Was the most recentpresidential election in Zimbabwe regarded as a fair election 51 theanswer may be encoded in a particular sentence of a particular document, or may need to be stitched together from pieces of evidencefound in multiple documents. Information extraction IE  is preciselythe field of natural language processing devoted to this type of task49. Hence, it is not surprising that the application of informationextraction techniques to opinion mining and sentiment analysis hasbeen proposed 51, 79. In this survey, we use the term opinionorientedinformation extraction opinionoriented IE  to refer to informationextraction problems particular to sentiment analysis and opinion mining. We sometimes shorten the phrase to opinion extraction, whichshould not be construed narrowly as focusing on the extraction of opinion expressions for instance, determining product features is includedunder the umbrella of this term.Past research in this area has been dominated by work on two typesof texts Opinionoriented information extraction from reviews has,as noted above, attracted a great deal of interest in recentyears. In fact, the term opinion mining, when construedin its narrow sense, has often been used to describe workin this context. Reviews, while typically but not alwaysdevoted to a single item, such as a product, service, or event,generally comment on multiple aspects, facets, or featuresof that item, and all such commentary may be important.Extracting and analyzing opinions associated with each individual aspect can help provide more informative summarizations or enable more finegrained opinionoriented retrieval. Other work has focused on newswire. Unlike reviews, a newsarticle is relatively likely to contain descriptions of opinionsthat do not belong to the articles author an example is aquotation from a political figure. This property of journalistictext makes the identification of opinion holders also knownas opinion sources and the correct association of opinion56 Classification and Extractionholders with opinions important tasks, whereas for reviews,all expressed opinions are typically those of the author,so opinionholder identification is a less salient problem.Thus, when newswire articles are the focus, the emphasishas tended to be on identifying expressions of opinions, theagent expressing each opinion, andor the type and strengthof each opinion. Early work in this direction first carefullydeveloped and evaluated a lowlevel opinion annotationscheme 45, 283, 309, 312, which facilitated the study ofsubtasks such as identifying opinion holders and analyzingopinions at the phrase level 37, 42, 43, 51, 60, 61, 157, 320.It is important to understand the similarities and differencesbetween opinionoriented IE and standard factoriented IE. They sharesome subtasks in common, such as entity recognition for example, asmentioned above, determination of opinion holders is an active lineof research 37, 42, 61, 158. What truly sets the problem apart fromstandard or classic IE is the specific types of entities and relationsthat are considered important. For instance, although identification ofproduct features is in some sense a standard entity recognition problem, an opinion extraction system would be mostly interested in features for which associated opinions exist similarly, an opinion holderis not just any named entity in a news article, but one that expressesopinions. Examples of the types of relations particularly pertinent toopinion mining are those centered around comparisons  consider,for example, the relations encoded by such sentences as The newmodel is more expensive than the old one or I prefer product A overproduct B 139, 191, longer version of the latter available as Jindaland Liu 138  or between agents and reported beliefs, as describedin Section 4.9.2. Note that the relations of interest can form a complexhierarchical structure, as in the case where an opinion is attributed toone party by another, so that it is unclear whether the first party trulyholds the opinion in question 42.It is also important to understand which aspects of opinionorientedextraction are mentioned in this section as opposed to the previous sections. As discussed earlier, many subproblems of opinion extraction are4.9 Special Considerations for Extraction 57in fact classification problems for relatively small textual units. Examples include both determining whether or not a text span is subjectiveand classifying a given text span already determined to be subjectiveby the strength of the opinion expressed. Thus, many key techniquesinvolved in building an opinion extraction system are already discussedin the previous sections. In this section, we instead focus on the missing pieces, describing approaches to problems that are somewhat special to extraction tasks in sentiment analysis. While these subtaskscan be and often are cast as classification problems, they do not havenatural counterparts outside of the extraction context. Specifically, Section 4.9.1 is devoted to the identification of features and expressions ofopinions in reviews. Section 4.9.2 considers techniques that have beenemployed when opinionholder identification is an issue.Finally, we make the following organizational note. One may oftenwant to present the output of opinion extraction in summarized formconversely, some forms of sentiment summarization rely on the outputof opinion extraction. Opinionoriented summarization is discussed inSection 5.4.9.1 Identifying Product Features and Opinionsin ReviewsIn the context of review mining 130, 166, 215, 244, 323, 324, twoimportant extractionrelated subtasks are1 The identification of product features, and2 the extraction of opinions associated with these features.While the key features or aspects are known in some cases, manysystems start from problem 1.As noted above, identification of product features is in some sense astandard information extraction task with little to distinguish it fromother nonsentimentrelated problems. After all, the notion of the features that a given product has seems fairly objective. However, Hu andLiu 130 show that one can benefit from light sentiment analysis evenfor this subtask, as described shortly.58 Classification and ExtractionExisting work on identifying product features discussed in reviewstask 1 often relies on the simple linguistic heuristic that explicitfeatures are usually expressed as nouns or noun phrases. This narrowsdown the candidate words or phrases to be considered, but obviouslynot all nouns or noun phrases are product features. Yi et al. 323 consider three increasingly strict heuristics to select from noun phrasesbased on partofspeechtag patterns. Hu and Liu 130 follow the intuition that frequent nouns or noun phrases are likely to be features. Theyidentify frequent features through association mining, and then applyheuristicguided pruning aimed at removing a multiword candidatesin which the words do not appear together in a certain order, and bsingleword candidates for which subsuming superstrings have beencollected the idea is to concentrate on more specific concepts, so that,for example, life is discarded in favor of battery life. These techniques by themselves outperform a generalpurpose termextractionand indexing system known as FASTR 135. Furthermore  and hereis the observation that is relevant to sentiment  the Fmeasure can befurther improved although precision drops slightly via the followingexpansion procedure adjectives appearing in the same sentence as frequent features are assumed to be opinion words, and nouns and nounphrases cooccurring with these opinion words in other sentences aretaken to be infrequent features.In contrast, Popescu and Etzioni 244 consider product features tobe concepts forming certain relationships with the product for example, for a scanner, its size is one of its properties, whereas its cover isone of its parts and seek to identify the features connected with theproduct name through corresponding meronymy discriminators. Notethat this approach, which does not involve sentiment analysis per sebut simply focuses more on the task of identifying different types offeatures, achieved better performance than that yielded by the techniques of Hu and Liu 130.There has also been work that focuses on extracting attributevalue pairs from textual product descriptions, but not necessarily inthe context of opinion mining. Of work in this vein, Ghani et al.105 directly compare against the method proposed by Hu andLiu 130.4.9 Special Considerations for Extraction 59To identify expressions of opinions associated with features task2, a simple heuristic is to simply extract adjectives that appear inthe same sentence as the features 130. Deeper analyses can make use ofparse information and manually or semiautomatically developed rulesor sentimentrelevant lexicons 215, 244.4.9.2 Problems Involving Opinion HoldersIn the context of analysis of newswire and related genres, we needto identify text spans corresponding both to opinion holders and toexpressions of the opinions held by them.As is true with other segmentation tasks, identifying opinion holderscan be viewed as a sequence labeling problem. Choi et al. 61 experiment with an approach that combines Conditional Random FieldsCRFs 176 and extraction patterns. A CRF model is trained on acertain collection of lexical, syntactic, and semantic features. In particular, extraction patterns are learned to provide semantic tagging aspart of the semantic features. CRFs have also been used to detectopinion expressions 43.Alternatively, given that the status of an opinion holder dependsby definition on the expression of an opinion, the identification ofopinion holders can benefit from, or perhaps even require, accounting for opinion expressions either simultaneously or as a preprocessingstep.One example of simultaneous processing is the work of Bethardet al. 37, who specifically address the task of identifying both opinions and opinion sources. Their approach is based on semantic parsingwhere semantic constituents of sentences e.g., agent or proposition are marked. By utilizing opinion words automatically learnedby a bootstrapping approach, they further refine the semantic roles toidentify propositional opinions, i.e., opinions that generally function asthe sentential complement of a predicate. This enables them to concentrate on verbs and extract verbspecific information from semanticframes such as are defined in FrameNet 25 and PropBank 230.As another example of the simultaneous approach, Choi et al. 60employ an integer linear programming approach to handle the joint60 Classification and Extractionextraction of entities and relations, drawing on the work of Roth andYih 260 on using global inference based on constraints.As an alternative to the simultaneous approach, a system can startby identifying opinion expressions, and then proceed to the analysisof the opinions, including the identification of opinion holders. Indeed,Kim and Hovy 159 define the problem of opinion holder identificationas identifying opinion sources given an opinion expression in a sentence.In particular, structural features from a syntactic parse tree are selectedto model the longdistance, structural relation between a holder andan expression. Kim and Hovy show that incorporating the patterns ofpaths between holder and expression outperforms a simple combinationof local features e.g., the type of the holder node and other nonstructural features e.g., the distance between the candidate holdernode and the expression node.One final remark is that the task of determining which mentionsof opinion holders are coreferent source coreference resolution differsin practice in interesting ways from typical noun phrase coreferenceresolution, due in part to the way in which opinionoriented datasetsmay be annotated 282.5SummarizationSo far, we have talked about analyzing and extracting opinion information from individual documents. The focus of this section is onaggregating and representing sentiment information drawn from anindividual document or from a collection of documents. For example, auser might desire an ataglance presentation of the main points madein a single review creating such singledocument sentiment summariesis described in Section 5.1. Another application considered within thisparadigm is the automatic determination of market sentiment, or themajority leaning of an entire body of investors, from the individualremarks of those investors 66, 67 this is a type of multidocumentopinionoriented summarization, described in Section 5.2.5.1 SingleDocument OpinionOriented SummarizationThere is clearly a tight connection between extraction of topicbasedinformation from a single document 49 and topicbased summarization of that document, since the information that is pulled out can serveas a summary see Radev et al. 247, Section 2.1 for a brief review.6162 SummarizationObviously, this connection between extraction and summarizationholds in the case of sentimentbased summarization, as well.One way in which this connection is made manifest in singledocument opinionoriented summarization is as follows there areapproaches that create textual sentiment summaries based on extraction of sentences or similar text units. For example, Beineke et al.33 attempt to select a single passage1 that reflects the opinion ofthe documents authors, mirroring the practice of film advertisements that present snippets from reviews of the movie. Training and test data is acquired from the website Rotten Tomatoeshttpwww.rottentomatoes.com, which provides a roughly sentencelength snippet for each review. However, Beineke et al. 33 notethat low accuracy can result even for highquality extraction methods because the Rotten Tomatoes data includes only a single snippetper review, whereas several sentences might be perfectly viable alternatives. In terms of creating longer summaries, Mao and Lebanon 200suggest that by tracking the sentiment flow within a document  i.e.,how sentiment orientation changes from one sentence to the next, asdiscussed in Section 4.7  one can create sentiment summaries bychoosing the sentences at local extrema of the flow plus the first andlast sentence. An interesting feature of this approach is that by incorporating a documents flow, the technique takes into account the entiredocument in a holistic way. Both approaches just mentioned seek toselect the absolutely most important sentences to present. Alternatively, one could simply extract all subjective sentences, as was doneby Pang and Lee 232 to create subjectivity extracts. They suggested that these extracts could be used as summaries, although, asmentioned above, they focused on the use of these extracts as an aidto downstream polarity classification, rather than as summaries perse. Finally, we note that sentences are also used in multidocumentsentiment summarization as well, as described in Section 5.2.Other sentiment summarization methods can work directly offthe output of opinionoriented informationextraction systems. Indeed,1Beineke et al. 33 use the term sentiment summary to refer to a single passage, but weprefer to not restrict that terms definition so tightly.5.1 SingleDocument OpinionOriented Summarization 63Cardie et al. 51, speaking about the more restricted type of extractionreferred to by the technical term information extraction, propose toview . . . summary representations as information extraction IE scenario templates . . . thus we postulate that methods from informationextraction . . . will be adequate for the automatic creation of opinionbased summary representations. A similar observation was made byDini and Mazzini 79. Note that these IE templates do not form coherent text on their own. However, they can be incorporated as is intovisual summaries.Indeed, one interesting aspect of the problem of extracting sentiment information from a single document or from multiple documents,as discussed in Section 5.2 is that sometimes graphbased output seemsmuch more appropriate or useful than textbased output. For example,graphbased summaries are very suitable when the information that ismost important is the set of entities described and the opinions thatsome of these entities hold about each other 305. Figure 5.1 showsan example of a humangenerated summary in the form of a graphdepicting various negative opinions expressed during the aftermath ofHurricane Katrina. Note the inclusion of text snippets on the arrowsto support the inference of a negative opinion2 in general, providingsome sense of the evidence from which opinions are inferred is likely tobe helpful to the user.While summarization technologies may not be able to achievethe level of sophistication of information presentation exhibited byFigure 5.1, current research is making progress toward that goal. InFigure 5.2, we see a proposed summary where opinion holders and theobjects of their opinions are connected by edges, and various annotations derived from IE output are included, such as the strength ofvarious attitudes.Of course, graphical elements can also be used to represent a single bit, number or grade as a very succinct summary of a documents2The exceptions are the edges from news mediaand the edges from people who didntevacuate. It is perhaps intentionally ambiguous whether the lack of supporting quotesis due merely to the lack of sufficiently juicy ones or is meant to indicate that it isutterly obvious that these entities blame many others. We also note that the hurricaneitself is not represented.64 SummarizationFig. 5.1 Graphic by Bill Marsh for The New York Times, October 1, 2005, depicting negative opinions of various entities toward each other in the aftermath of Hurricane Katrina.Relation to opinion summarization pointed out by Eric Breck Claire Cardie, personalcommunication.sentiment. Variations of stars, letter grades, and thumbs upthumbsdown icons are common. More complex visualization schemes appliedon a sentencebysentence basis have also been proposed 7.5.2 MultiDocument OpinionOriented SummarizationLanguage is itself the collective art of expression,a summary of thousands upon thousands of individualintuitions. The individual gets lost in the collective creation, but his personal expression has left some tracein a certain give and flexibility that are inherent in all5.2 MultiDocument OpinionOriented Summarization 65Fig. 5.2 Figure 2 labeled 3 of Cardie et al. 51 proposal for a summary representationderived from the output of an informationextraction system.collective works of the human spirit.  Edward Sapir,Language and Literature, 1921. Connection to sentimentanalysis pointed out by Das and Chen 67.5.2.1 Some Problem ConsiderationsThere never was in the world two opinions alike, nomore than two hairs, or two grains the most universalquality is diversity. Michel de Montaigne, EssaysWhere an opinion is general, it is usually correct. Jane Austen, Mansfield ParkWe briefly discuss here some points to keep in mind in regardsto multidocument sentiment summarization, although to a certaindegree, work in sentiment summarization has not yet reached a levelwhere these problems have come to the fore.Determining which documents or portions of documents express thesame opinion is not always an easy task but, clearly it is one that needsto be addressed in the summarization setting, since readers of sentimentsummaries surely are interested in the overall sentiment in the corpus which means the system must determine shared sentiments within thedocument collection at hand.66 SummarizationThis issue can still arise even when labels have been predetermined, if the items that have been prelabeled come from differentsubcollections. For instance, some documents may have polarity labels,whereas others may contain ratings on a 1to5 scale. And even whenthe ratings are drawn from the same set, calibration issues mayarise. Consider the following from Rotten Tomatoes frequentlyaskedquestions page httpwww.rottentomatoes.compagesfaqjudgeOn the Blade 2 reviews page, you have a negative reviewfrom James Berardinelli 2.54 stars, and a positivereview from Eric Lurio 2.55. Why is Berardinellisreview labeled Rotten and Lurios review labeled FreshYoure seeing this discrepancy because star systemsare not consistent between critics. For critics like RogerEbert and James Berardinelli, 2.5 stars or lower out of 4stars is always negative. For other critics, 2.5 stars caneither be positive or negative. Even though Eric Luriouses a 5 star system, his grading is very relaxed. So, 2stars can be positive. Also, theres always the possibilityof the webmaster or critic putting the wrong rating ona review.As another example, in reconciling reviews of conference submissions,programcommittee members must often take into account the fact thatcertain reviewers always tend to assign low scores to papers, while others have the opposite tendency. Indeed, we believe this calibration issuemay be the reason why reviews of cars on Epinions come not only witha number of stars annotation, but also a thumbs upthumbs downindicator, in order to clarify whether, regardless of the rating assigned,the review author actually intends to make a positive recommendationor not.An additional observation to take note of is the fact that whentwo reviewers agree on a rating, they may have different reasons fordoing so, and it may be important to indicate these reasons in thesummary. A related point is that when a reviewer assigns a middlingrating, it may be because he or she thinks that most aspects of the itemunder discussion are soso, but it may also be because he or she sees5.2 MultiDocument OpinionOriented Summarization 67both strong positives and strong negatives. Or, reviewers may havethe same opinions about individual item features, but weight theseindividual factors differently, leading to a different overall sentiment.Indeed, Rotten Tomatoes summarizes a set of reviews both with theTomatometer  percentage of reviews judged to be positive  andan average rating on a 1to10 scale. The idea, again according to theFAQ httpwww.rottentomatoes.compagesfaqavgvstmeter, is asfollowsThe Average Rating measures the overall quality of aproduct based on an average of individual critic scores.The Tomatometer simply measures the percentage ofcritics who recommend a certain product.For example, while Men in Black scored 90 on theTomatometer, the average rating is only 7.510. Thatmeans that while youre likely to enjoy MIB, it probablywasnt a contender for Best Picture at the Oscars.In contrast, Toy Story 2 received a perfect 100 onthe Tomatometer with an average rating of 9.610. Thatmeans, not only are you certain to enjoy it, youll alsobe impressed with the direction, story, cinematography,and all the other things that make truly great filmsgreat.The problem of deciding whether two sentences or text passages have the same semantic content is one that is faced not justby opinionoriented multidocument summarizers, but by topicbasedmultidocument summarizers as well 247 this has been one of themotivations behind work on paraphrase recognition 29, 30, 231 andtextual entailment 28. But, as pointed out in Ku et al. 170, whilein traditional summarization redundant information is often discarded,in opinion summarization one wants to track and report the degreeof redundancy, since in the opinionoriented setting the user is typically interested in the relative number of times a given sentiment isexpressed in the corpus.Carenini et al. 52 note that a challenge in sentiment summarization is that the pieces of information to be summarized  peoples68 Summarizationopinions  are often conflicting, which is a bit different from the usualsituation in topicbased summarization, where typically one does notassume that there are conflicting sets of facts in the document setalthough there are exceptions 301, 302.5.2.2 Textual SummariesIn standard topicbased multidocument summarization, creating textual summaries has been a main focus of effort. Hence, despite the differences in topic and opinionbased summarization mentioned above,several researchers have developed systems that create textual summaries of opinionoriented information.5.2.2.1 Leveraging Existing TopicBased TechnologiesOne line of attack is to adapt existing topicbased multidocument summarization algorithms to the sentiment setting.Sometimes the adaptation consists simply of modifying the inputto these preexisting algorithms. For instance, Seki et al. 264propose that one apply standard multidocument summarization toa subcollection of documents that are on the same topic and thatare determined to belong to some relevant genre of text, such asargumentative.In other cases, preexisting topicbased summarization techniquesare modified. For example, Carenini et al. 52 generate naturallanguage summaries in the form of an evaluative argument usingthe classic naturallanguage generation pipeline of content selection,lexical selection and sentence planning, and sentence realization 251,assuming the existence of a predefined productfeature hierarchy. Thesystem explicitly produces textual descriptions of aggregate information. The system is capable of relaying data about the average sentiment and signaling, if appropriate, that the distribution of responsesis bimodal this allows one to report split votes. They comparethis system against a modification of an existing sentenceextractionsystem, MEAD 246. The former approach seems more wellsuited forgeneral overviews, whereas the latter seems better at providing morevariety in expression and more detail see Figure 5.3. Related to the5.2 MultiDocument OpinionOriented Summarization 69Summary created via a true naturallanguagegeneration approachAlmost all users loved the Canon G3 possibly because some usersthought the physical appearance was very good. Furthermore, several users found the manual features and the special features to bevery good. Also, some users liked the convenience because someusers thought the battery was excellent. Finally, some users foundthe editingviewing interface to be good despite the fact that several customers really disliked the viewfinder. However, there weresome negative evaluations. Some customers thought the lens waspoor even though some customers found the optical zoom capabilityto be excellent. Most customers thought the quality of the imageswas very good.Summary created by a modified sentenceextraction systemBottom line, well made camera, easy to use, very flexible and powerful features to include the ability to use external flash and lensefilterschoices. It has a beautiful design, lots of features, very easy touse, very configurable and customizable, and the battery durationis amazing Great colors, pictures, and white balance. The camera isa dream to operate in automode, but also gives tremendous flexibilityin aperture priority, shutter priority, and manual modes. Id highlyrecommend this camera for anyone who is looking for excellent quality pictures and a combination of ease of use and the flexibility toget advanced with many options to adjust if you like.Fig. 5.3 Sample automatically generated summaries. Adapted from Figure 2 of Careniniet al. 52.latter approach, sentence extraction methods have also been used tocreate summaries for opinionoriented queries or topics 265, 266.While we are not aware of the following technique being used instandard topicbased summarization, we see no reason why it is notapplicable to that setting, at least in principle. Ku et al. 170 shortversion available as Ku et al. 169 propose the following simple schemeto create a textual summary of a set of documents known in advanceto be on the same topic. Sentences considered to be representativeof the topic are collected, and the polarity of each such sentence iscomputed based on what sentimentbearing words it contains, withnegation taken into account. Then, to create a summary of the positivedocuments, the system simply returns the headline of the documentwith the most positive ontopic sentences, and similarly for the negative70 Summarizationdocuments. The authors show the following examples for the positiveand the negative summary, respectively Positive Chinese Scientists Suggest Proper Legislation forClone Technology. Negative UK Government Stops Funding for Sheep CloningTeam.The cleverness of this method is that headlines are, by construction,good summaries at least of the article they are drawn from, so thatfluency and informativeness, although perhaps not appropriateness, areguaranteed.Another perhaps unconventional type of multidocument summary is the selection of a few documents of interest from the corpusfor presentation to the user. In this vein, Kawai et al. 151 have developed a news portal site called Fair News Reader that attempts todetermine the affect characteristics of articles the user has been reading so far e.g., happiness or fear and then recommends articlesthat are on the same topic but have opposite affect characteristics. Onecould imagine extending this concept to a news portal that presented tothe user opinions opposing his or her preconceived ones Phoebe Sengers, personal communication. On a related note, Liu 190 mentionsthat one might desire a summarization system to present a representative sample of opinions, so that both positive and negative points ofview are covered, rather than just the dominant sentiment. As of thetime of this writing, Amazon presents the most helpful favorable reviewsidebyside with the most helpful critical review if one clicks on thex customer reviews link next to the stars indicator. Additionally,one could interpret the opinionleader identification work of Song et al.275 as suggesting that blog posts written by opinion leaders couldserve as an alternative type of representative sample.Summarizing online discussions and blogs is an area of related work131, 300, 330. The focus of such work is not on summarizing theopinions per se, although Zhou and Hovy 330 note that one maywant to vary the emphasis on the opinions expressed versus the factsexpressed.5.2 MultiDocument OpinionOriented Summarization 715.2.2.2 Textual Summarization Without TopicbasedSummarization TechniquesOther work in the area of textual multidocument sentiment summarization departs from topicbased work. The main reason seems to bethat redundancy elimination is much less of a concern users may wishto look at many individual opinions regardless of whether these individual opinions express the same overall sentiment, and these users maynot particularly care whether the textual overview they peruse is coherent. Thus, in several cases, textual summaries are generated simplyby listing some or all opinionated sentences. These are often grouped byfeature subtopic andor polarity, perhaps with some ranking heuristic such as feature importance applied 129, 170, 324, 332.5.2.3 Nontextual SummariesIn the previous section, we have discussed the creation of textual summaries of the opinion information expressed within a corpus. But insettings where the polarity or orientation of each individual document within a collection is summed up in a single bit e.g., thumbsupthumbs down, number e.g., 3.5 stars, or grade e.g., B, analternative way to obtain a succinct summary of the overall sentimentis to report summary statistics, such as the number of reviews that arethumbs up or the average number of stars or average grade. Manysystems take this approach to summarization.Summary statistics are often quite suited to graphical representations we describe some noteworthy visual aspects of these summarieshere evaluation of the userinterface aspects has not been a focus ofattention in the community to date.5.2.3.1 Bounded Summary Statistics Averagesand Relative FrequenciesWe use the term bounded to refer to summary statistics that lie within apredetermined range. Examples are the average number of stars range0 to 5 stars, say or the percentage of positive opinions range 0to 100.72 SummarizationThermometertype images are one means for displaying suchstatistics. One example is the Tomatometer on the Rotten Tomatoeswebsite, which is simply a bar broken into two differently coloredportions the portion of the bar that is colored red indicates thefraction of positive reviews of a given movie. This representationextends straightforwardly to nary categorization schemes, such as positivemiddlingnegative, via the use of n colors. The thermometergraphic concept also generalizes in other ways for instance, thedepiction of a number of stars can be considered to be a variant ofthis idea.Instead of using size or extent to depict bounded summary statistics,as is done with thermometer representations, one can use color shading. This choice seems particularly appropriate in settings where theamount of display realestate that can be devoted to any particular item under evaluation is highly limited or where size or location is reserved to represent some other information. For instance,Gamon et al. 104 use color to represent the general assessmentof automatically determined product features. In Figure 5.4, wesee that each of many features or topics, such as handling orvw, service, is represented by a shaded box. The colors for anygiven box range from red to white to green, indicating gradationsof the average sentiment toward that topic, moving from negativeto neutral or objective to positive, respectively. Note that onecan quickly glean from such a display what was liked and whatwas disliked about the product under discussion, despite the largenumber of topics under evaluation  people like driving this carbut dislike the service. As shown in Figure 5.5, a similar interface together with a usability study is presented in Carenini et al.53. Some differences are that naturallanguage summarization isalso employed, so that the summary is both verbal and visualthe features are grouped into a hierarchy, thus leveraging the ability of Treemaps 270 to display hierarchical data via nesting and theinterface also includes a way not depicted in the figure to see anataglance summary of the polarities of the individual sentencescommenting on a particular feature. A demo is available online athttpwww.cs.ubc.ca careninistorageSEAdemo.html.5.2 MultiDocument OpinionOriented Summarization 73Fig. 5.4 Figure 2 of Gamon et al. 104, depicting automatically determined topics discussed in reviews of the Volkswagen Golf. The size of each topic box indicates the numberof mentions of that topic. The shading of each topic box, ranging from red to white togreen, indicates the average sentiment, ranging from negative to neutralnone to negative,respectively. At the bottom, the sentences most indicative of negative sentiment for thetopic vw, service are displayed.5.2.3.2 Unbounded Summary StatisticsAs just described, thermometer graphics and color shading can beused to represent bounded statistics such as the mean or, in thecase of ncolor thermometers, relative distributions of ratings acrossdifferent classes. But bounded statistics by themselves do not provideother important pieces of information, such as the actual numberof opinions within each class. We consider raw frequencies to beconceptually unbounded, although there are practical limits to howmany opinions can be accounted for. Intuitively, the observation that50 of the reviews of a particular product are negative3 is more of a3We admit to being glasshalfempty people.74 SummarizationFig. 5.5 Figure 4 of Carenini et al. 53, showing a summary of reviews of a particularproduct. An automatically generated text summary is on the left a visual summary ison the right. The size of each each box in the visual summary indicates the number ofmentions of the corresponding topic, which occupy a predefined hierarchy. The shading ofeach topic box, ranging from red to black to green, indicates the average sentiment, rangingfrom negative to neutralnone to negative, respectively. At the bottom is shown the sourcefor the portion of the generated naturallanguage summary that has been labeled with thefootnote 4.big deal if that statistic is based on 10,000 reviews than if it based ononly two.Another problem specific to the mean as a summary statistic isthat reviewaggregation sites seem to often exhibit highly skewed rating distributions, with a particular bias toward highly positive reviews74, 59, 128, 253, 132, 240.4 Since there can often be a second mode, orbump, at the extreme low end of the rating scale, indicating polarization  for example, Hu et al. 132 remark that 54 of the items in asample of Amazon book, DVD, and video products with more than 20reviews fail both statistical normality and unimodality tests  reporting only the mean rating score may not provide enough information.To put it another way, divulging the average does not give the user4On a related note, William Safires New York Times May 1, 2005 article Blurbospherequotes Charles McGrath, former editor of the New York Times Book Review, as asking,has there ever been a book that wasnt acclaimed5.2 MultiDocument OpinionOriented Summarization 75enough information to distinguish between a set of middling reviewsand a set of polarized reviews.On the other hand, it is worth pointing out that just giving the number of positive and negative reviews, respectively, on the assumptionthat the user can always derive the percentages from these counts, maynot suffice. Cabral and Hortacsu 47 observe that once eBay switchedto displaying the percentage of pieces of feedback on sellers that werenegative, as opposed to simply the raw numbers, then negative reviewsbegan to have a measurable economic impact see Section 6.Hence, not surprisingly, sentiment summaries tend to include dataon the average rating, the distribution of ratings, andor the numberof ratings.Visualization of unbounded summary statistics. Of the two systemsdescribed above that represent the average polarity of opinions viacolor, both represent the quantity of the opinions on a given topicvia size. This means that the count data for positive and for negativeopinions are not explicitly presented separately. In other systems, thisis not the case rather, frequencies for different classes are broken outand displayed.For instance, as of the time of this writing, Amazon displays anaverage rating as a number of stars with the number of reviews nextto it mousing over the stars brings up a histogram of reviewer ratingsannotated with counts for the 5star reviews, 4star reviews, etc. Further mousing over the bars of the histogram brings up the percentageof reviews that each of those counts represent.As another example, a sample output of the Opinion Observer system 191 is depicted in Figure 5.6, where the portion of a bar projecting above the centered horizon line represents the number of positiveopinions about a certain product feature, and the portion of the barbelow the line represents the number of negative opinions. The sameidea can be used to represent percentages too, of course. A nice featureof this visualization is that because of the use of a horizon line, twoseparate frequency datapoints  the positive and negative counts can be represented by what is visually one object, namely, a solid bar,and one can easily simultaneously compare negatives against negatives76 SummarizationFig. 5.6 Figure 2 of Liu et al. 191. Three cellphones are represented, each by a differentcolor. For each feature General, LCD, etc., three bars are shown, one for each ofthree cellphones. For a given feature and phone, the portions of the bar above and belowthe horizontal line represent the number of reviews that express positive or negative viewsabout that cameras feature, respectively. The system can also plot the percentages ofpositive or negative opinions, rather than the raw numbers. The pane on the upperrightdisplays the positive sentences regarding one of the products.and positives against positives. This simultaneous comparison is mademuch more difficult if the bars all have one end planted at the samelocation, as is the case for standard histograms such as the one depictedin Figure 5.7.While the data for the features are presented sequentially inFigure 5.6 first General, then LCD, and so forth, an alternativevisualization technique called a rose plot is exemplified in Figure 5.8,which depicts a sample output of the system developed by Gregoryet al. 113. The median and quartiles across a document subcollectionof the percentage of positive and negative words per document, togetherwith similar data for other possible affectclassification dimensions, arerepresented via a variant of box plots. Adaptation to raw counts rather5.2 MultiDocument OpinionOriented Summarization 77Fig. 5.7 A portion of Figure 4 of Yi and Niblack 324, rotated to save space and facilitatecomparison with Figure 5.6. Notice that simultaneous comparison of the negative countsand the positive counts for two different products is not as easy as it is in Figure 5.6.than percentages is straightforward. Mapping this idea to productcomparisons in the style of Opinion Observer, one could associate different features with different compass directions, e.g., the featurebattery life with southwest, as long as the number of features beingreported on is not too large. The reason that this representation mightprove advantageous in some settings is that in some situations, a circular arrangement may be more compact than a sequential one, and itmay be easier for a user to remember a feature as being southwestthan as being the fifth of eight. An additional functionality of thesystem that is not shown in the figure is the ability to depict how muchan individual documents positivenegative percentage differs from theaverage for a given document group to which the document belongs.A similar circular layout is proposed in Subasic and Huettner 285 forvisualizing various dimensions of affect within a single document.Morinaga et al. 215 opt to represent degrees of association betweenproducts and opinionindicative terms of a prespecified polarity. First,78 SummarizationFig. 5.8 Figure 7 of Gregory et al. 113. On the right are two rose plots, one for each oftwo products on the left is the plots legend. In each rose plot, the upper two petalsrepresent positivity and negativity, respectively of the other six petals, the bottom two arevice and virtue, etc.. Similarly to box plots, the median value is indicated by a dark arc,and the quartiles by colored bands around the median arc. Darker shading for one of thetwo petals in a pair e.g., positive and negative are meant to indicate the negative endof the spectrum for the affect dimension represented by the given petal pair. The histogrambelow each rose relates to the number of documents represented.opinions are gathered using the authors preexisting system 291.Codinglength and probabilistic criteria are used to determine whichterms to focus on, and principal component analysis is then appliedto produce a twodimensional visualization, such that nearness corresponds to strength of association, as in the authors previous work 184.Thus, in Figure 5.9, we see that cellphone A is associated with whatwe recognize as positive terms, whereas cellphone C is associated withnegative terms.5.2 MultiDocument OpinionOriented Summarization 79Fig. 5.9 Figure 5 of Morinaga et al. 215 principalcomponentsanalysis visualization ofassociations between products squares and automatically selected opinionoriented termscircles.5.2.3.3 Temporal Variation and Sentiment TimelinesSo far, the summaries we have considered do not explicitly incorporate any temporal dimension. However, time is often an importantconsideration.First, users may wish to view individual reviews in reverse chronological order, i.e., newest first. Indeed, at the time of this writing, thisis one of the two sorting options that Amazon presents.80 SummarizationSecond, in many applications, analysts and other users are interested in tracking changes in sentiment about a product, political candidate, company, or issue over time. Clearly, one can create a sentimenttimeline simply by plotting the value of a chosen summary statistic atdifferent times the chosen statistic can reflect the prevailing polarity170, 296 or simply the number of mentions, in which case what isbeing measured is perhaps not so much public opinion, but rather public awareness 102, 197, 211, 212. Such work is strongly related at aconceptual level to topic detection and tracking 8, a review of whichis beyond the scope of this survey.Mishne and de Rijke 212 also depict the derivative of the summarystatistic considered as a function of time.5.2.4 Reviewer QualityHow do we identify what is good And how do wecensure what is bad We will argue that developinga humane reputation system ecology can provide better answers to these two general questions  restraining the baser side of human nature, while liberatingthe human spirit to reach for ever higher goals. Manifesto for the reputation society. Masum andZhang 203When creating summaries of reviews or opinionated text, an important type of information that deserves careful consideration is whetheror not individual reviews are helpful or useful. For example, a systemmight want to downweight or even discard unhelpful reviews before creating summaries or computing aggregate statistics, as in Liu et al. 193.Alternatively, the system could use all reviews, but provide helpfulnessindicators for individual reviews as a summary of their expected utility. Indeed, nonsummarization systems could use such information,too for instance, a revieworiented search engine could rank its searchresults by helpfulness.Some websites already gather helpfulness information from humanreaders. For example, Amazon.com annotates reviews with commentslike 120 of 140 people found the following review helpful, meaning5.2 MultiDocument OpinionOriented Summarization 81that of the 140 people who pressed one of the yes or no buttons in response to the question Was this review helpful to you we deem these 140 people utility evaluators  120 chose yes. Similarly, the Internet Movie Database IMDb, httpwww.imdb.com alsoannotates user comments with x out of y people found the followingcomment useful. This similarity is perhaps not surprising due to thefact that Amazon owns IMDb, although from a research point of view,note that the two populations of users are probably at least somewhatdisjoint, meaning that there might be interesting differences betweenthe sources of data. Other sites soliciting utility evaluations includeYahoo Movies and Yahoo TV, which allow the user to sort reviews byhelpfulness CitySearch, which solicits utility evaluations from generalusers and gives more helpful reviews greater prominence and Epinions, which only allows registered members to rate reviews and doesnot appear to have helpfulness as a sort criterion, at least for nonregistered visitors.5 We learned about the solicitation of utility evaluations by IMDb from Zhuang et al. 332 and by Citysearch fromDellarocas 71.Despite the fact that many reviewaggregation sites already providehelpfulness information gathered from human users, there are still atleast two reasons why automatic helpfulness classification is a usefulline of work to pursue.Items that lack utility evaluations. Many reviews receive very fewutility evaluations. For example, 38 of a sample of roughly 20,000Amazon MP3player reviews, and 31 of those aged at least threemonths, received three or fewer utility evaluations 161. Similarly, Liuet al. 193 confirm ones prior intuitions that Amazon reviews that areyoungest and reviews that are most lowly ranked i.e., determined tobe least helpful by the site receive the fewest utility evaluations.5We note that we were unable to find Amazons definition of helpful, and concludethat they do not supply one. In contrast, Yahoo specifies the following Was a reviewinformative, well written or amusing  above all was it was helpful to you in learning about the film or show If so, then you should rate that review as helpful.It might be interesting to investigate whether these differing policies have implications. There have in fact been some comments that Amazon should clarify its question httpwww.amazon.comWasthisreviewhelpfulyouforumFx1JS1YLZ490S1OTx3QHE2JPEXQ1V71 encodingUTF8asin B000FL7CAU.82 SummarizationPerhaps some reviews receive no utility evaluations simply becausethey are so obviously bad that nobody bothers to rate them. But thisdoes not imply that reviews without utility evaluations must necessarilybe unhelpful certainly we can not assume this of reviews too recentlywritten to have been read by many people. One important role thatautomated helpfulness classifiers can play, then, is to provide utilityratings in the many cases when human evaluations are lacking.Skew in utility evaluations. Another interesting potential applicationof automated helpfulness classification is to correct for biases in humangenerated utility evaluations.We first consider indirect evidence that such biases exist. It turnsout that just as the distribution of ratings can often be heavily skewedtoward the positive end, as discussed in Section 5.2.3.2, the distributionof utility evaluations can also be heavily skewed toward the helpful end,probably due at least in part to similar reasons as in the productratingscase. In a crawl of approximately 4 million unique Amazon reviews forabout 670,000 books excluding alternate editions, the average percentage of yes responses among the utility evaluations is between74 and 70, depending on whether reviews with fewer than 10 utility evaluations are excluded Gueorgi Kossinets and Cristian DanescuNiculescuMizil, personal communication. Similarly, half of a sampleof about 23,000 Amazon digitalcamera reviews had helpfulunhelpfulvote ratios of over 9 to 1 193. As in the ratings distribution case, onesintuition is that the percentage of reviews that are truly helpful is notas high as these statistics would seem to indicate. Another type of indirect evidence of bias is that the number of utility evaluations receivedby a review appears to decrease exponentially in helpfulness rank ascomputed by Amazon 193. Certainly there has to be some sort ofdecrease, since Amazons helpfulness ranking is based in part on thenumber of utility evaluations a review receives. Liu et al. 193 conjecture that reviews that have many utility evaluations will have a disproportionate influence on readers and utility evaluators because they areviewed as more authoritative, but reviews could get many utility evaluations only because they are more prominently displayed, not becausereaders actually compared them against other reviews. Liu et al. 1935.2 MultiDocument OpinionOriented Summarization 83call this tendency for oftenevaluated reviews to quickly accumulateeven more utility evaluations as winner circle bias in other literature on powerlaw effects, related phenomena are also referred to asrichgetricher.As for more direct evidence Liu et al. 193 conduct an reannotation study in which the Amazon reviewers utility evaluationsoften did not match those of the human relabelers. However, this latterevidence should be taken with a grain of salt. First, in some of theexperiments in the study, ground truth helpfulness was measured by,among other things, the number of aspects of a product that are discussed by a review. Second, in all experiments, the test items appear tohave consisted of only the text of a single review considered in isolation.It is not clear that the first point corresponds to the standard that allAmazon reviewers used, or should be required to use, and clearly, thesecond point describes an isolatedtext setting that is not the one thatreal Amazon reviewers work in. To exemplify both these objections avery short review written by a reputable critic e.g., a top reviewerthat points out something that other reviews missed can, indeed, bequite helpful, but would score poorly according to the specification ofLiu et al. 193. Indeed, the sample provided of a review that should belabeled bad starts,I want to point out that you should never buy a genericbattery, like the person from San Diego who reviewedthe S410 on May 15, 2004, was recommending. Yesyoud save money, but there have been many reports ofgeneric batteries exploding when charged for too long.We would view this comment, if true, to be quite helpful, despite thefact that it fails the specification. Another technical issue is that therelabelers used a fourclass categorization scheme, whereas essentiallyevery possible percentage of positive utility evaluations could form adistinct class for the Amazon labels it might have been better to treatreviews with helpfulness percentages of 60 and 61 as equivalent,rather than saying that Amazon reviewers rated the latter as betterthan the former.84 SummarizationNonetheless, given the large predominance of helpful among utility evaluations despite the fact that anecdotal evidence we have gathered indicates that not all reviews deserve to be called helpful,and given the suggestive results of the reannotation experiment justdescribed, it is likely that some of the human utility evaluations arenot strongly related to the quality of the review at hand. Thus, webelieve that correction of these utility evaluations by automatic meansis a valid potential application.A note regarding the effect of utility evaluations. It is important tomention one caveat before proceeding to describe research in this area.Park et al. 236 attempted to determine what the effect of review quality actually is on purchasing intention, running a study in which subjects engaged in hypothetical buying behavior. They found nonuniformeffects lowinvolvement i.e., motivated consumers are affected by thequantity rather than the quality of reviews ... highinvolvement consumers are affected by review quantity mainly when the review qualityis high...The effect of review quality on highinvolvement consumers ismore pronounced with a sizable number of reviews, whereas the effectof review quantity is significant even when the review quality is low.More on the economic impacts of sentiment analysis is described inSection 6.5.2.4.1 Methods for Automatically Determining ReviewQualityIn a way, one could consider the reviewquality determination problemas a type of readability assessment and apply essayscoring techniques19, 99. However, while some of the systems described below do try totake into account some readabilityrelated features, they are tailoredspecifically to product reviews.Kim et al. 161, Zhang and Varadarajan 328, and Ghose andIpeirotis 106 attempt to automatically rank certain sets of reviewson the Amazon.com website according to their helpfulness or utility,using a regression formulation of the problem. The domains considered are a bit different MP3 players and digital cameras in the firstcase Canon electronics, engineering books, and PG13 movies in the5.2 MultiDocument OpinionOriented Summarization 85second case and AV players plus digital cameras in the third case.Liu et al. 193 convert the problem into one of lowquality reviewdetection i.e., binary classification, experimenting mostly with manually reannotated reviews of digital cameras, although CNet editorialratings were also considered on the assumption that these can be considered trustworthy. Rubin and Liddy 261 also sketch a proposal toconsider whether reviews can be considered credible.Kim et al. 161 study which of a multitude of lengthbased, lexical,POScount, productaspectmention count, and metadata features aremost effective when utilizing SVM regression. The best feature combination turned out to be review length plus tfidf scores for lemmatizedunigrams in the review plus the number of stars the reviewer assignedto the product. Somewhat disappointingly, the best pair of featuresamong these was the length of the review and the number of stars.Using number of stars as the only feature yielded similar results tousing just the deviation of the number of stars given by the particularreviewer from the average number of stars granted by all reviewers forthe item. The effectiveness of using all unigrams appears to subsumethat of using a select subset, such as sentimentbearing words from theGeneral Inquirer lexicon 281.Zhang and Varadarajan 328 use a different feature set. Theyemploy a finer classification of lexical types, and more sources for subjective terms, but do not include any metadata information. Interestingly, they also consider the similarity between the review in questionand the product specification, on the premise that a good review shoulddiscuss many aspects of the product and they include the reviewssimilarity to editorial reviews, on the premise that editorial reviewsrepresent highquality examples of opinionoriented text. David andPinch 70 observe, however, that editorial reviews for books are paidfor and are meant to induce sales of the book. However, these latter twooriginal features do not appear to enhance performance. The featuresthat appear to contribute the most are the class of shallow syntactic features, which, the authors speculate, seem to characterize styleexamples include counts of words, sentences, whwords, comparativesand superlatives, proper nouns, etc. Review length seems to be veryweakly correlated with utility score.86 SummarizationWe thus see that Kim et al. 161 find that metadata and verysimple term statistics suffice, whereas Zhang and Varadarajan 328observe that more sophisticated cues that appear correlated with linguistic aspects appear to be most important. Possibly, the difference isa result of the difference in domain choice we speculate that book andmovie reviews can involve more sophisticated language use than whatis exhibited in reviews of electronics.Declaring themselves influenced by prior work on creating subjectivity extracts 232, Ghose and Ipeirotis 106 take a different approach.They focus on the relationship between the subjectivity of a reviewand its helpfulness. The basis for measuring review subjectivity isas follows using a classifier that outputs the probability of a sentence being subjective, one can compute for a given review the average subjectivenessprobability over all its sentences, or the standarddeviation of the subjectivity scores of the sentences within the review.They found that both the standard deviation of the sentence subjectivity scores and a readability score review length in characters dividedby number of sentences have a strongly statistically significant effecton utility evaluations, and that this is sometimes true of the averagesubjectivenessprobability as well. They then suggest on the basis ofthis and other evidence that it is extreme reviews that are consideredto be most helpful, and develop a helpfulness predictor based on theiranalysis.Liu et al. 193 considered features related to review and sentencelength brand, product and productaspect mentions, with special consideration for appearances in review titles sentence subjectivity andpolarity and paragraph structure. This latter refers to paragraphsas delimited by automatically determined keywords. Interestingly, thetechnique of taking the 30 most frequent pairs of nouns or nounphrases that appear at the beginning of a paragraph as keywordsyields separator pairs such as proscons, strengthweakness,and the upsidesdownsides. Note that this differs from identifying pro or con reasons themselves 157, or identifying the polarityof sentences. Note also that other authors have claimed that different techniques are needed for situations in which procon delimitersare mandated by the format imposed by a review aggregation site5.2 MultiDocument OpinionOriented Summarization 87but a separate detailed textual description must also be included, asin Epinions, as opposed to settings where such delimiters need notbe present or where all text is placed in the context of such delimiters 191. Somewhat unconventionally with respect to other textcategorization work, the baseline was taken as SVMlight run with threesentencelevel statistics as features that is, the performance of a classifier trained using bagofword features is not reported. Given thisunconventional starting point, the addition of the features that do notreflect subjectivity or sentiment help. Including subjectivity and polarity on top of what has already been mentioned does not yield furtherimprovement, and use of titleappearance for mentions did not seemto help.Review or opinionspam detection  the identification of deliberately misleading reviews  is a line of work by Jindal and Liu 141,short version available as Jindal and Liu 140 in the same vein. Onechallenge these researchers faced was the difficulty in obtaining groundtruth. Therefore, for experimental purposes they first reframed theproblem as one of trying to recognize duplicate reviews, since a prioriit is hard to see why posting repeats of reviews is justified. However,one potential problem with the assumption that repeated reviews constitute some sort of manipulation attempt, at least for the Amazondata that was considered, is that Amazon itself crossposts reviewsacross different products  where different includes different instantiations e.g., ebook vs. hardcover or subsequent editions of the sameitem Gueorgi Kossinets and Cristian Danescu NiculescuMizil, personal communication. Specifically, in a sample of over 1 million Amazon book reviews, about onethird were duplicates, but these were alldue to Amazons crossposting. Human error e.g., accidentally hittingthe submit button twice causes other cases of nonmalicious duplicates. A second round of experiments attempted to identify reviewson brands only, ads, and other irrelevant reviews containing no opinions e.g., questions, answers, and random texts. Some of the featuresused were similar to those employed in the studies described aboveothers included features on the review author and the utility evaluations themselves. The overall message was that this kind of spam isrelatively easy to detect.88 Summarization5.2.4.2 ReviewerIdentity ConsiderationsIn the above, we have discussed determining the quality of individualreviews. An alternate approach is to look at the quality of the reviewers doing so can be thought of as a way of classifying all the reviewsauthored by the same person at once.Interestingly, one study has found that there is a real economiceffect to be observed when factoring in reviewer credibility Gu et al.114 note that a weighted average of messageboard postings in whichposter credibility is factored in has prediction power over future abnormal returns of the stock, but if postings are weighted uniformly, thepredictive power disappears.There has been work in a number of areas in the humanlanguagetechnologies community that incorporates the authority,trustworthiness, influentialness, or credibility of authors 94, 96, 141,275. PageRank 44, 241 and hubs and authorities also known asHITS 163 are very influential examples of work in link analysis onidentifying items of great importance. Trust metrics also appear inother work, such as research into peertopeer and reputation networksand information credibility 71, 115, 147, 174, 252.6Broader ImplicationsSentiment is the mightiest force in civilization . . .J. Ellen Foster, What America Owes to Women, 1893As we have seen, sentimentanalysis technologies have many potential applications. In this section, we briefly discuss some of the largerimplications that the existence of opinionoriented informationaccessservices has.Privacy. One point that should be mentioned is that applications thatgather data about peoples preferences can trigger concerns about privacy violations. We suspect that in many peoples minds, having onespublic blog scanned by a coffee company for positive mentions of itsproduct is one thing having ones cellphone conversations monitoredby the ruling party of ones own country for negative mentions of government officials is quite another. It is not our intent to comment further here on privacy issues, these not being issues on which we arequalified to speak rather, we simply want to be thorough by reminding the reader that these issues do exist and are important, and thatthese concerns apply to all datamining technologies in general.8990 Broader ImplicationsManipulation. But even if we restrict attention to the apparentlyfairly harmless domain of business intelligence, certain questionsregarding the potential for manipulation do arise. Companies alreadyparticipate in managing online perceptions as part of the normal courseof publicrelations efforts. . . companies cant control consumergenerated content. They can, however, pay close attention to it. Inmany cases, often to a large degree, they can even influence it. In fact, in a survey conducted by Aberdeen ofmore than 250 enterprises using social media monitoring and analysis solutions in a diverse set of enterprises, more than twice as many companies with socialmedia monitoring capabilities actively contribute toconsumer conversations than remain passive observers67 versus 33. Over a third of all companies 39contribute to online conversations on a frequent basis,interacting with consumers in an effort to sway opinion,correct misinformation, solicit feedback, reward loyalty,test new ideas, or for any number of other reasons. Zabin and Jefferies 327And it is also the case that some arguably mild forms of manipulation have been suggested. For instance, one set of authors, in studyingthe strategic implications for a company of offering online consumerreviews, notes that if it is possible for the seller to decide the timing to offer consumer reviews at the individual product level, it maynot always be optimal to offer consumer reviews at a very early stageof new product introduction, even if such reviews are available 57,quotation from the July 2004 workingpaper version, and others haveworked on a manufactureroriented system that ranks reviews according to their expected effect on sales, noting that these might not bethe ones that are considered to be most helpful to users 106.But still, there are concerns that corporations might try to furthergame the system by taking advantage of knowledge of how rankingsystems work in order to suppress negative publicity 124 or engagein other socalled blackhat search engine optimization and related91activities. Indeed, there has already been a term  sock puppet coined to refer to ostensibly distinct online identities created to givethe false impression of external support for a position or opinion Stoneand Richtel 280 list several rather attentiongrabbing examples of wellknown writers and CEOs engaging in sockpuppetry. On a related note,Das and Chen 67 recommend Leinweber and Madhavan 183 as aninteresting review of the history of market manipulation through disinformation.One reason these potentials for abuse are relevant to this surveyis that, as pointed out earlier in the Introduction, sentimentanalysistechnologies allow users to consult many people who are unknown tothem but this means precisely that it is harder for users to evaluate thetrustworthiness of those people or people they are consulting. Thus,opinionmining systems might potentially make it easier for users to bemisled by malicious entities, a problem that designers of such systemsmight wish to prevent. On the flip side, an informationaccess systemthat is perhaps unfairly perceived to be vulnerable to manipulationis one that is unlikely to be widely used thus, again, builders of suchsystems might wish to take measures to make it difficult to game thesystem.In the remainder of this section, then, we discuss several aspectsof the problem of possible manipulation of reputation. In particular,we look at evidence as to whether reviews have a demonstrable economic impact if reviews do significantly affect customer purchases,then there is arguably an economic incentive for companies to engagein untoward measures to manipulate public perception if reviews donot significantly affect customer purchases, then there is little reason, from an economic point of view, for entities to try to artificiallychange the output of sentimentanalysis systems  or, as Dewally 74asserts, the stock market does not appear to react to these recommendations. . . . The fears raised by the media about the destabilizingpower of such traders who participate in these discussions are thusgroundless. If such claims are true, then it would seem that tryingto manipulate perceptions conveyed by online reviewaccess systemswould offer little advantages to companies, and so they would notengage in it.92 Broader Implications6.1 Economic Impact of ReviewsAs mentioned earlier in the Introduction to this survey, many readers ofonline reviews say that these reviews significantly influence their purchasing decisions 63. However, while these readers may have believedthat they were significantly influenced, perception and reality candiffer. A key reason to understand the real economic impact of reviewsis that the results of such an analysis have important implications forhow much effort companies might or should want to expend on onlinereputation monitoring and management.Given the rise of online commerce, it is not surprising that a bodyof work centered within the economics and marketing literature studies the question of whether the polarity often referred to as valenceandor volume of reviews available online have a measurable, significant influence on actual consumer purchasing. Ever since the classic market for lemons paper 6 demonstrating some problems formakers of highquality goods, economists have looked at the value ofmaintaining a good reputation as a means to overcome these problems 77, 162, 268, 269, among other strategies. See the introductionto Dewally and Ederington 75, from which the above references havebeen taken, for a brief review. One way to acquire a good reputation is,of course, by receiving many positive reviews of oneself as a merchantanother is for the products one offers to receive many positive reviews.For the purposes of our discussion, we regard experiments wherein thebuying is hypothetical as being out of scope instead, we focus on economic analyses of the behavior of people engaged in real shopping andspending real money.11Note that researchers in the economics community have a tradition of circulating andrevising working papers, sometimes for years, before producing an archival version. In thereferences that follow, we have cited the archival version when journalversion publicationdata has been available to us, in order to enable the interested reader to access the final,peerreviewed version of the work. But because of this policy, the reader who wishes todelve into this literature further should keep in mind the following two points. First, manycitations within the literature are to preliminary working papers. This means that ourcitations may not precisely match those given in the papers themselves e.g., there may betitle mismatches. Second, work that was done earlier may be cited with a later publicationdate therefore, the dates given in our citations should not be taken to indicate researchprecedence.6.1 Economic Impact of Reviews 93The general form that most studies take is to use some form of hedonic regression 259 to analyze the value and the significance of different item features to some function, such as a measure of utility to thecustomer, using previously recorded data. Exceptions include Resnicket al. 253, who ran an empirical experiment creating new sellers oneBay, and Jin and Kato 136, who made actual purchases to validateseller claims. Specific economic functions that have been examinedinclude revenue boxoffice take, sales rank on Amazon, etc., revenuegrowth, stock trading volume, and measures that auctionsites like eBaymake available, such as bid price or probability of a bid or sale beingmade. The type of product considered varies although, understandably, those offered by eBay and Amazon have received more attentionexamples include books, collectible coins, movies, craft beer, stocks, andused cars. It is important to note that some conclusions drawn from onedomain often do not carry over to another for instance, reviews seemto be influential for bigticket items but less so for cheaper items. Butthere are also conflicting findings within the same domain. Moreover,different subsegments of the consumer population may react differentlyfor example, people who are more highly motivated to purchase maytake ratings more seriously. Additionally, in some studies, positive ratings have an effect but negative ones do not, and in other studies theopposite effect is seen the timing of such feedback and various characteristics of the merchant or of the feedback itself e.g., volume mayalso be a factor.Nonetheless, to gloss over many details for the sake of brevityif one allows any effect  including correlation even if said correlation is shown to be not predictive  that passes a statistical significance test at the 0.05 level to be classed as significant, thenmany studies find that review polarity has a significant economic effect13, 14, 23, 31, 35, 47, 59, 62, 68, 72, 75, 76, 81, 82, 128, 136, 145, 180,195, 196, 198, 207, 208, 214, 237, 250, 253, 278, 297, 331. But there area few studies that conclude emphatically that review positivity or negativity has no significant economic effect 56, 74, 80, 87, 100, 194, 325.Duan et al. 80 explicitly relate their findings to the issue of corporate manipulation From the managerial perspective, we show thatconsumers are rational in inferring movie quality from online user94 Broader Implicationsreviews without being unduly influenced by the rating, thus presentinga challenge to businesses that try to influence sales through plantingonline wordofmouth.With respect to effects that have been found, the literature surveycontained in Resnick et al. 253 states thatAt the larger end of effect sizes for positive evaluations,the model in Livingston 196 finds that sellers withmore than 675 positive comments earned a premiumof 45.76, more than 10 of the mean selling price, ascompared to new sellers with no feedback. . . . At thelarger end of effect sizes for negatives, LuckingReileyet al. 198, looking at collectible coins, finds that amove from 2 to 3 negatives cuts the price by 11, about19 from a mean price of 173.But in general, the claims of statistically significant effects that havebeen made tend to be a qualified by a number of important caveats,and b quite small in absolute terms per item, although on the otherhand again, small effects per item can add up when many items areinvolved. With regard to this discussion, the following excerpt fromHouser and Wooders 128 is perhaps illuminating. . . on average, 3.46 percent of sales is attributable tothe sellers positive reputation stock. Similarly, our estimates imply that the average cost to sellers stemmingfrom neutral or negative reputation scores is 2.28, or0.93 percent of the final sales price. If these percentages are applied to all of eBays auctions 1.6 billionin the fourth quarter of 2000, this would imply thatsellers positive reputations added more than 55 million to the value of sales, while nonpositives reducedsales by about 15 million.Ignoring for the moment the fact that, as mentioned above, otherpapers report differing or even opposite findings, we simply note thatthe choice of whether to focus on 0.93, 2.28, or 55 million6.1 Economic Impact of Reviews 95and whether to view the latter amount as seeming particularly largeor not is one we prefer to leave to the reader.Let us now mention some particular papers and findings of particular interest.6.1.1 Surveys Summarizing Relevant Economic LiteratureResnick et al. 253 and Bajari and Hortacsu 24 are good entry pointsinto this body of literature. They provide very thorough overviewsand discussion of the methodological issues underlying the studiesmentioned above. Hankin 118 supplies several visual summaries thatare modeled after the literaturecomparison tables in Dellarocas 71,Resnick et al. 253, and Bajari and Hortacsu 24. A list of a numberof papers on the general concept of sentiment in behavioral finance canbe found at httpsentiment.behaviouralfinance.net.6.1.2 EconomicImpact Studies Employing AutomatedText AnalysisIn most of the studies cited above, the orientation of a review wasderived from an explicit rating indication such as number of stars, buta few studies applied manual or automatic sentiment classification toreview text 13, 14, 35, 47, 67, 68, 214, 237.At least one related set of studies claims that the text of the reviewscontains information that influences the behavior of the consumers, andthat the numeric ratings alone cannot capture the information in thetext 106  see also Ghose et al. 107, who additionally attemptto assign a dollar value to various adjectivenoun pairs, adverbverbpairs, or similar lexical configurations. In a related vein, Pavlou andDimoka 237 suggest that the apparent success of feedback mechanisms to facilitate transactions among strangers does not mainly comefrom their crude numerical ratings, but rather from their rich feedback text comments. Also, Chevalier and Mayzlin 59 interpret theirfindings on the effect of review length as providing some evidence thatpeople do read the reviews rather than simply relying on numericalratings.96 Broader ImplicationsOn the other hand, Cabral and Hortacsu 47, in an interestingexperiment, look at 41 odd cases of feedback on sellers posted on eBaywhat was unusual was that the feedback text was clearly positive, butthe numerical rating was negative presumably due to user error. Analysis reveals that these reviews have a strongly significant both economically and statistically detrimental effect on sales growth rate indicating that customers seemed to ignore the text in favor of theincorrect summary information.In some of these textbased studies, what was analyzed was notsentiment per se but the degree of polarization disagreement amonga set of opinionated documents 13, 68 or, inspired in part by Pang andLee 233, the average probability of a sentence being subjective withina given review 106. Ghose and Ipeirotis 106 also take into accountthe standard deviation for sentence subjectivity within a review, inorder to examine whether reviews containing a mix of subjective andobjective sentences seem to have different effects from reviews that aremostly purely subjective or purely objective.Some initially unexpected text effects are occasionally reported. Forexample, Archak et al. 14 found that amazing camera, excellentcamera, and related phrases have a negative effect on demand. Theyhypothesize that consumers consider such phrases, especially if fewdetails are subsequently furnished in the review, to indicate hyperboleand hence view the review itself as untrustworthy. Similarly, Archaket al. 14 and Ghose et al. 107 discover that apparently positive comments like decent quality or good packaging also had a negativeeffect, and hypothesize that the very fact that many reviews containhyperbolic language mean that words like decent are interpreted aslukewarm.These findings might seem pertinent to the distinction between theprior polarity and the contextual polarity of terms and phrases, borrowing the terminology of Wilson et al. 319. Prior polarity refers tothe sentiment a term evokes in isolation, as opposed to the sentimentthe term evokes within a particular surrounding context Polanyi andZaenen 242 point out that identifying prior polarity alone may not suffice. With respect to this distinction, the status of the observations ofArchak et al. 14 just mentioned is not entirely clear. The superlatives6.2 Implications for Manipulation 97amazing are clearly intended to convey positive sentiment regardless of whether the review authors actually managed to convince readers that is, context is only needed to explain the economic effect oflowered sales, not the interpretation of the review itself. In the caseof words like decent, one could potentially make the case that theprior orientation of such words is in fact neutral rather than positivebut alternatively, one could argue instead that in a setting where manyreviews are highly enthusiastic, the contextual orientation of decentis indeed different from its prior orientation.6.1.3 Interactions with Word of Mouth WOMOne factor that some studies point out is that the number of reviews,positive or negative, may simply reflect word of mouth, so that insome cases, what is really the underlying correlative if any of economicimpact is not the amount of positive feedback per se but merely theamount of feedback in total. This explains why in some settings but notall, negative feedback is seen to increase sales the increased buzzbrings more attention to the product or perhaps simply indicates moreattention is being paid to the product, in which case it would not bepredictive per se.6.2 Implications for ManipulationRegarding the incentives for manipulation, it is difficult to draw a conclusion one way or the other from the studies we have just examined.One cautious way to read the results summarized in the previoussection is as follows. While there may be some economic benefit insome settings for a corporation to plant positive reviews or otherwiseattempt to use untoward means to manufacture an artificially inflatedreputation or suppress negative information, it seems that in general,a great deal of effort and resources would be required to do so forperhaps fairly marginal returns. More work is clearly required, thoughas Bajari and Hortacsu 24 conclude, There is still plenty of work tobe done to understand how market participants utilize the informationcontained in the feedback forum system. Surveying the state of theart in this subject is beyond the scope of this survey a fairly concise98 Broader Implicationsreview of issues regarding online reputation systems may be found inDellarocas 71.We would like to conclude, though, by pointing out a result thatindicates that even if illegitimate reviews do get through, opinionmining systems can still be valuable to consumers. Awerbuch andKleinberg 22 study the competitive collaborative learning settingin which some of the n users are assumed to be Byzantine malicious, dishonest, coordinated, and able to eavesdrop on communications, and product or resource quality varies over time. The authorsformulate the product selection problem as a type of multiarmed bandit problem. They show the striking result that even if only a constantfraction of users are honest and unbeknownst to them grouped intok market segments such that all members of a block share the sameproduct preferences  with the implication that the recommendationsof an honest user may be useless to honest users in different marketsegments  then there is still an algorithm by which, in time polynomial in k logn, the average regret per honest user is arbitrarily smallassuming that the number of products or resources on offer is On.Roughly speaking, the algorithm causes users to tend to raise the probability of getting recommendations from valuable sources. Thus, evenin the face of rather stiff odds and formidable adversaries, honest userscan  at least in theory  still get good advice from sentimentanalysissystems.7Publicly Available Resources7.1 Datasets7.1.1 Acquiring Labels for DataOne source of opinion, sentiment, and subjectivity labels is, of course,manual annotation 172, 309.However, researchers in the field have also managed to find ways toavoid manual annotation by leveraging preexisting resources. A common technique is to use labels that have been manually assigned, butnot by the experimenters themselves this explains why researchers inopinion mining and sentiment analysis have taken advantage of RottenTomatoes, Epinions, Amazon, and other sites where users furnish ratings along with their reviews. Some other noteworthy techniques are asfollows Sentiment summaries can be gathered by treating the reviewsnippets that Rotten Tomatoes furnishes as onesentencesummaries 33. Subjective vs. nonsubjective texts on the same topic can begathered by selecting editorials versus noneditorial newswire99100 Publicly Available Resources308, 326 or by selecting movie reviews versus plot summaries 222, 232. If sentimentoriented search engines already exist one example used to be Opinmind, then one can issue topicalqueries to such search engines and harvest the results to getsentimentbearing sentences more or less guaranteed to beontopic 206. On the other hand, there is something circular about this approach, since it bootstraps off of someoneelses solution to the opinionmining problem. One might be able to derive affect labels from emoticons249. Text polarity may be inferred from correlations with stockmarket behavior or other economic indicators 168, 107. Viewpoint labels can be derived from images of party logosthat users display 160. Negative opinions can be gathered by assuming that whenone newsgroup post cites another, it is typically done to indicate negative sentiment toward the cited post 4. A morerefined approach takes into account indications of shouting, such as text rendered all in capital letters 110.One point to mention with regards to sites where users rate thecontributions of other users  such as the examples of Amazon andEpinions mentioned above  is a potential bias toward positive scores59, 74, 128, 132, 240, 253, as we have mentioned above. In somecases, this comes about because of sociological effects. For example,Pinch and Athanasiades 240, in a study of a musicoriented site calledACIDplanet, found that various forces tend to cause users to give highratings to each others music. The users themselves refer to this phenomenon as RR review me and I will review you, among other,less polite, names, and the ACIDplanet administrators introduced aform of anonymous reviewing to avoid this issue in certain scenarios.Thus, there is the question of whether one can trust the automatically determined labels that one is training ones classifiers upon. Afterall, you often get what you pay for, as they say. Indeed, Liu et al. 193essentially relabeled their reviewquality Amazon data due to concerns7.1 Datasets 101about bias, as discussed in Section 5.2.4. On the other hand, whilethis phenomenon implies that reviewers may not always be sincere, wehypothesize that this phenomenon does not greatly affect the qualityof the authors metadata labels at reflecting the intended sentiment ofthe review itself. That is, we hypothesize that in many cases one canstill trust the reviews label, even if one does not trust the review.7.1.2 An Annotated List of DatasetsThe following list is in alphabetical order.Blog06registration and fee requiredThe University of Glasgow distributes this 25GB TREC test collection, consisting of blog posts over a range of topics. Accessinformation is available at httpir.dcs.gla.ac.uktest collectionsaccess to data.html. Included in the data set are top blogs that wereprovided by Nielsen BuzzMetrics and supplemented by the Universityof Amsterdam 227, and some spam blogs, also known as splogs,that were planted in the corpus in order to simulate a more realistic setting. Assessments include relevance judgments and labels as to whetherposts contain relevant opinions and what the polarity of the opinionswas positive, negative, or a mixture of both. Macdonald and Ounis199 give more details on the creation of the corpus and the collectionsfeatures, and include some comparison with another collection of blogpostings, the BlogPulse dataset contact information can be found onthe following agreement form httpwww.blogpulse.comwww2006workshopdatashareagreement.pdf, but it may be out of date.Congressional floordebate transcriptsURL httpwww.cs.cornell.eduhomelleedataconvote.htmlThis dataset, first introduced in Thomas et al. 294, includes speechesas individual documents together with Automatically derived labels for whether the speaker supported or opposed the legislation discussed in the debate thespeech appears in, allowing for experiments with this kind ofsentiment analysis.102 Publicly Available Resources Indications of which debate each speech comes from, allowing for consideration of conversational structure. Indications of byname references between speakers, allowing for experiments on agreement classification if one assignsgoldstandard agreement labels from the supportopposelabels assigned to the pair of speakers in question. The edge weights and other information derived to create thegraphs used in Thomas et al. 294, facilitating implementation of alternative graphbased methods upon the graphsconstructed in that earlier work.Cornell moviereview datasetsURL httpwww.cs.cornell.edupeoplepabomoviereviewdataThese corpora, first introduced in Pang and Lee 232, 233, consist ofthe following datasets, which include automatically derived labels. Sentiment polarity datasets documentlevel polarity dataset v2.0 1000 positiveand 1000 negative processed reviews. An earlier version of this dataset v1.0 was first introduced in Panget al. 235. sentencelevel sentence polarity dataset v1.0 5331positive and 5331 negative processed sentencessnippets. Sentimentscale datasets scale dataset v1.0 a collection ofdocuments whose labels come from a rating scale. Subjectivity dataset v1.0 5000 subjective and 5000 objectiveprocessed sentences.We should point out that the existence of the polaritybaseddatasets does not indicate that the curators i.e., us believe thatreviews with middling ratings are not important to consider in practiceindeed, the sentimentscale corpora contain such documents. Rather,the rationale in creating the polarity dataset was as follows. At the7.1 Datasets 103time the corpus creation was begun, the application of machine learning techniques to sentiment classification was very new, and, as discussed in Section 3, it was natural to assume that the problem couldbe very challenging to such techniques. Therefore, the polarity corpus was constructed to be as easy for textcategorization techniquesas possible the documents fell into one of two wellseparated andsizebalanced categories. The point was, then, to use this corpus asa lens to study the relative difficulty of sentiment polarity classification as compared to standard topicbased classification, where twobalancedclass problems with wellseparated categories pose very littlechallenge.A list of papers that use or report performance on the Cornellmoviereview datasets can be found at httpwww.cs.cornell.edupeoplepabomoviereviewdataotherexperiments.html.Customer review datasetsURL httpwww.cs.uic.eduliubFBSCustomerReviewData.zipThis dataset, introduced in Hu and Liu 129, consists of reviewsof five electronics products downloaded from Amazon and Cnet.The sentences have been manually labeled as to whether an opinion is expressed, and if so, what feature from a predefined list isbeing evaluated. An addendum with nine products is also availablehttpwww.cs.uic.eduliubFBSReviews9products.rar and hasbeen utilized in recent work 78. The curator, Bing Liu, also distributesa comparativesentence dataset that is available by request.EconominingURL httpeconomining.stern.nyu.edudatasets.htmlThis site, hosted by the Stern School at New York University, consistsof three sets of data Transactions and price premiums. Feedback postings for merchants at Amazon.com. Automatically derived sentiment scores for frequent evaluation phrases at Amazon.com.104 Publicly Available ResourcesThese formed the basis for the work reported in Ghose et al. 107, whichfocuses on interactions between sentiment, subjectivity, and economicindicators.French sentencesURL httpwww.psor.ucl.ac.bepersonalybResource.htmlThis dataset, introduced in Bestgen et al. 36, consists of 702 sentencesfrom a BelgianFrench newspaper, with labels assigned by ten judgesas to unpleasant, neutral or pleasant content, using a sevenpoint scale.MPQA CorpusURL httpwww.cs.pitt.edumpqadatabasereleaseThe MPQA Opinion Corpus contains 535 news articles from a widevariety of news sources, manually annotated at the sentential and subsentential level for opinions and other private states i.e., beliefs, emotions, sentiments, speculations, and so on. Wiebe et al. 309 describesthe overall annotation scheme Wilson et al. 319 describes the contextual polarity annotations and an agreement study.Multipleaspect restaurant reviewsURL httppeople.csail.mit.edubsnydernaacl07The corpus, introduced in Snyder and Barzilay 272, consists of 4,488reviews, both in rawtext and in featurevector form. Each review givesan explicit 1to5 rating for five different aspects  food, ambiance, service, value, and overall experience  along with the text of the reviewitself, all provided by the review author. A rating of five was the mostcommon over all aspects, and Snyder and Barzilay 272 report that30.5 of the 3,488 reviews in their randomly selected training set hada rating of five for all five aspects, although no other tuple of ratings wasrepresented by more than 5 of the training set. The code used in Snyder and Barzilay 272 is also distributed at the aforementioned URL.The original source for the reviews was httpwww.we8there.comdata from the same website was also used by Higashinaka et al. 122.MultiDomain Sentiment DatasetURL httpwww.cis.upenn.edumdredzedatasetssentimentThis dataset, introduced in Blitzer et al. 40, consists of product7.2 Evaluation Campaigns 105reviews from several different product types taken from Amazon.com,some with 1to5 star labels, some unlabeled.NTCIR multilingual corpusregistration requiredThe corpus for the NTCIR 6 pilot task consists of news articles inJapanese, Chinese, and English and formed the basis of the OpinionAnalysis Task at NTCIR6 267. The training data contains annotationsregarding opinion holders, the opinions held by opinion holder, andsentiment polarity, as well as relevance information for a set of predetermined topics.The corpus of the NTCIR Multilingual OpinionAnalysis TaskMOAT is drawn from Japanese, Chinese, and English blogs.Reviewsearch results setsURL httpwww.cs.cornell.eduhomelleedatasearchsubj.htmlThis corpus, used by Pang and Lee 234, consists of the top 20 resultsreturned by the Yahoo search engine in response to each of a set of69 queries containing the word review. The queries were drawn fromthe publicly available list of real MSN users queries released for the2005 KDD Cup competition 185 the KDD data itself is availableat httpwww.acm.orgsigssigkddkdd2005Labeled800Queries.zip.The searchengine results in the corpus are annotated as to whetherthey are subjective or not. Note that sales pitches were marked objective on the premise that they represent biased reviews that users mightwish to avoid seeing.7.2 Evaluation Campaigns7.2.1 TREC OpinionRelated CompetitionsThe TRECBLOG wiki, httpir.dcs.gla.ac.ukwikiTRECBLOG, is a useful source of information on the competitions sketchedbelow.TREC 2006 Blog Track. TREC 2006 involved a Blog track, with anopinion retrieval task designed precisely to focus on the opinionatedcharacter that many blogs have participating systems had to retrieve106 Publicly Available Resourcesblog posts expressing an opinion about a specified topic. Fourteengroups participated Ounis et al. 227 give an overview of the results.Some findings are as follows. With respect to performance on opiniondetection, the participating systems seemed to fall into two groups.Opiniondetection ability and relevancedetermination ability seemedto be strongly correlated. While the best systems were about equallygood at detecting negative sentiment as positive sentiment, systemsperforming at the median seemed to be a bit more effective at locating documents with negative sentiment. Most participants followed apipelined approach, where first topic relevance was tackled, and thenopinion detection was applied upon the results. Perhaps the most surprising observation was that the organizers discovered that it was possible to achieve very good relative performance by omitting the secondphase of the pipeline but we take heart in the fact that the field is stillrelatively young and has room to grow and mature.TREC 2007 Blog Track. The TREC 2007 Blog track retained theopinion retrieval task and instituted determining the sentiment statuspositive, negative, or mixed of the retrieved opinions as a subtask.The 2007 and 2006 Blog Track results are analyzed in Ounis et al.228. They found that lexiconbased approaches  either where thediscriminativeness of terms was determined on labeled training dataor where the terms were manually compiled  constituted the maineffective approaches.TREC 2008 Blog Track. In the TREC 2008 Blog track, the polarityidentification problem was reposed as one of ranking of positivepolarity retrieved documents by degree of positivity, and, similarly,ranking of negativepolarity retrieved documents by degree of negativity. Mixed opinionated documents were not to be included in theserankings.7.2.2 NTCIR OpinionRelated CompetitionsThe National Institute of Informatics NII runs annual meetings codenamed NTCIR NII Test Collection for Information Retrieval Systems.Opinion analysis was featured at an NTCIR5 workshop, and served asa pilot task at NTCIR6 and a fullblown task at NTCIR7.7.2 Evaluation Campaigns 107NTCIR6 opinion analysis pilot task. The dataset consists ofnewswire documents in Chinese, Japanese, and English the organizers describe this as what we believe to be the first multilingual opinion analysis data set over comparable data 93. The four constituenttasks, intentionally designed to be fairly simple so as to encourage participation from many groups, were as follows Detection of opinionated sentences. Detection of opinion holders. optional Polarity labeling of opinionated sentences as positive, negative, or neutral. optional Detection of sentences relevant to a given topic.Due to variation in annotator labelings, two evaluation standards weredefined. In the strict evaluation, an answer is considered correct if allthree annotators agreed on it. In the lenient evaluation, only a majorityi.e., two of the annotators were required to agree with an answer forit to be considered correct.Seki et al. 267 give an overview and the results of this evaluationexercise, noting that differences between languages make direct comparison difficult, especially since precision and recall were defined slightlydifferently across languages. A shortened version of this overview alsoexists 93.NTCIR7 Multilingual opinion analysis task MOAT, 2008. Subsequent to the NTCIR6 pilot task, a new dataset was selected, drawnfrom blogs in Japanese, traditional and simplified Chinese, and Englishaccording to the organizers, We plan to select and balance useful topics for opinion mining researchers, such as topics concerning productreviews, movie reviews, and so on. This exercise involves six subtasks Detection of opinionated sentences and opinion fragmentswithin opinionated sentences. Polarity labeling of opinion fragments as positive, negativeor neutral. optional Strength labeling of opinion fragments as veryweak, average, or very strong.108 Publicly Available Resources optional Detection of opinion holders. optional Detection of opinion targets. optional Detection of sentences that are relevant to a giventopic.As in the previous competition, both strict and lenient evaluation standards are to be applied.OpQA Corpusavailable by requestStoyanov et al. 283 describes the construction of this corpus, which is acollection of opinion questions and answers together with 98 documentsselected from the MPQA dataset.7.3 Lexical ResourcesThe following list is in alphabetical order.General InquirerURL httpwww.wjh.harvard.eduinquirerThis site provides entrypoints to various resources associated with theGeneral Inquirer 281. Included are manuallyclassified terms labeledwith various types of positive or negative semantic orientation, andwords having to do with agreement or disagreement.NTU Sentiment Dictionaryregistration requiredThis sentiment dictionary listing the polarities of many Chinesewords was developed by a combination of automated and manualmeans 171. A registration form for acquiring it is available athttpnlg18.csie.ntu.edu.tw8080opinionuserform.jsp.OpinionFinders Subjectivity LexiconURL httpwww.cs.pitt.edumpqaThe list of subjectivity clues that is part of OpinionFinder is availablefor download. These clues were compiled from several sources, representing several years of effort, and were used in Wilson et al. 319.7.4 Tutorials, Bibliographies, and Other References 109SentiWordnetURL httpsentiwordnet.isti.cnr.itSentiWordnet 91 is a lexical resource for opinion mining. Each synsetof WordNet 95, a publicly available thesauruslike resource, is assignedone of three sentiment scores  positive, negative, or objective  wherethese scores were automatically generated using a semisupervisedmethod described in Esuli and Sebastiani 90.Taboada and Grieves Turney adjective listavailable through the Yahoo sentimentAI groupReported are the semanticorientation values according to the methodproposed by Turney 298 for 1700 adjectives.7.4 Tutorials, Bibliographies, and Other ReferencesBing Liu has a chapter on opinion mining in his book onWeb data mining 190. Slides for that chapter are available at httpwww.cs.uic.eduliubteachcs583spring07opinionmining.pdf.Slides for Janyce Wiebes tutorial, Semantics, opinion, and sentiment in text, at the EUROLAN 2007 Summmer School are available at httpwww.cs.pitt.eduwiebepubspapersEUROLAN07eurolan07wiebe.ppt.The following are online bibliographies that contain information inBibTeX format http   www.cs.cornell.edu  home  llee  opinionminingsentimentanalysissurvey.html, the main website for thissurvey, http   liinwww.ira.uka.debibliographyMiscSentiment.html, maintained by Andrea Esuli, http   research.microsoft.com   jtsun  OpinionMiningPaperList. html, maintained by JianTao Sun, http   www.cs.pitt.edu   wiebe  pubs  papers  EUROLAN07eurolan07bib.html with actual .bib file at http  110 Publicly Available Resourceswww.cs.pitt.edu   wiebe  pubs  papers  EUROLAN07eurolan07.bib, maintained by Janyce Wiebe.Esuli and Wiebes sites have additional search capabilities.Members of the Yahoo group sentimentAI httptech.groups.yahoo.comgroupSentimentAI have access to the resources thathave been contributed there such as some links to corpora andpapers and are subscribed to the associated mailing list. Joiningis free.8Concluding RemarksWhen asked how he knew a piece was finished, heresponded, When the dinner bell rings. apocryphal anecdote about Alexander CalderOur goal in this survey has been to cover techniques and approachesthat promise to directly enable opinionoriented informationseekingsystems, and to convey to the reader a sense of our excitement about theintellectual richness and breadth of the area. We very much encouragethe reader to take up the many open challenges that remain, and hopewe have provided some resources that will prove helpful in this regard.On the topic of resources we have already indicated above thatthe bibliographic database used in this survey is publicly available. Infact, the URL mentioned above, httpwww.cs.cornell.eduhomelleeopinionminingsentimentanalysissurvey.html, is our personally maintained homepage for this survey. Any subsequent editions or versions ofthis survey that may be produced, or related news, will be announcedthere.11 Indeed, we have vague aspirations to producing a directors cut one day. We certainlyhave accumulated some number of outtakes we did not manage to find a way to work111112 Concluding RemarksSpeaking of resources, we have drawn considerably on those of manyothers during the course of this work. We thus have a number of sincereacknowledgments to make.This survey is based upon work supported in part by the NationalScience Foundation under grant no. IIS0329064, a Cornell University Provosts Award for Distinguished Scholarship, a Yahoo ResearchAlliance gift, and an Alfred P. Sloan Research Fellowship. Any opinions, findings, and conclusions or recommendations expressed are thoseof the authors and do not necessarily reflect the views or official policies, either expressed or implied, of any sponsoring institutions, the USgovernment, or any other entity.We would like to wholeheartedly thank the anonymous referees, whoprovided outstanding feedback astonishingly quickly. Their insightscontributed immensely to the final form of this survey on many levels. It is hard to describe our level of gratitude to them for their timeand their wisdom, except to say this we have, in various capacities, seenmany examples of reviewing in the community, but this is the best wehave ever encountered. We also thank Eric Breck for his careful readingof and commentary on portions of this survey. All remaining errors andfaults are, of course, our own.We are also very thankful to Fabrizio Sebastiani, for all of his editorial guidance and care. We owe him a great debt. We also greatlyappreciate the help we received from Jamie Callan, who, along withFabrizio, serves as Editor in Chief of the Foundations and Trends inInformation Retrieval series, and James Finlay, of Now Publishers, thepublisher of this series.Finally, a number of unexpected health problems arose in our families during the writing of this survey. Despite this, it was our familieswho sustained us with their cheerful and unlimited support on manylevels, not the other way around. Thus  to end on a sentimentalnote  this work is dedicated to them.some variant of Once more, with feeling into the title, or to find a place for the headingSentiment of a woman, or to formally prove a potential undecidability result for subjectivity detection Jon Kleinberg, personal communication based on reviews of Brotherhoodof the Wolf its the best darned French werewolf kungfu movie Ive ever seen.References1 A. Abbasi, Affect intensity analysis of dark web forums, in Proceedings ofIntelligence and Security Informatics ISI, pp. 282288, 2007.2 L. A. Adamic and N. Glance, The political blogosphere and the 2004 U.S.election Divided they blog, in Proceedings of LinkKDD, 2005.3 A. Agarwal and P. Bhattacharyya, Sentiment analysis A new approach foreffective use of linguistic knowledge and exploiting similarities in a set ofdocuments to be classified, in Proceedings of the International Conference onNatural Language Processing ICON, 2005.4 R. Agrawal, S. Rajagopalan, R. Srikant, and Y. Xu, Mining newsgroups usingnetworks arising from social behavior, in Proceedings of WWW, pp. 529535,2003.5 E. M. Airoldi, X. Bai, and R. Padman, Markov blankets and metaheuristicsearch Sentiment extraction from unstructured text, Lecture Notes in Computer Science, vol. 3932 Advances in Web Mining and Web Usage Analysis,pp. 167187, 2006.6 G. A. Akerlof, The market for Lemons Quality uncertainty and the marketmechanism, The Quarterly Journal of Economics, vol. 84, pp. 488500, 1970.7 S. M. Al Masum, H. Prendinger, and M. Ishizuka, SenseNet A linguistic toolto visualize numericalvalence based sentiment of textual data, in Proceedings of the International Conference on Natural Language Processing ICON,pp. 147152, 2007. Poster paper.8 J. Allan, Introduction to topic detection and tracking, in Topic Detectionand Tracking Eventbased Information Organization, J. Allan, ed., pp. 116,Norwell, MA, USA Kluwer Academic Publishers, ISBN 0792376641, 2002.113114 References9 C. O. Alm, D. Roth, and R. Sproat, Emotions from text Machine learningfor textbased emotion prediction, in Proceedings of the Human LanguageTechnology Conference and the Conference on Empirical Methods in NaturalLanguage Processing HLTEMNLP, 2005.10 A. Anagnostopoulos, A. Z. Broder, and D. Carmel, Sampling searchengineresults, World Wide Web, vol. 9, pp. 397429, 2006.11 R. K. Ando and T. Zhang, A framework for learning predictive structures from multiple tasks and unlabeled data, Journal of Machine LearningResearch, vol. 6, pp. 18171853, 2005.12 A. Andreevskaia and S. Bergler, Mining WordNet for a fuzzy sentiment Sentiment tag extraction from WordNet glosses, in Proceedings of the EuropeanChapter of the Association for Computational Linguistics EACL, 2006.13 W. Antweiler and M. Z. Frank, Is all that talk just noise The information content of internet stock message boards, Journal of Finance, vol. 59,pp. 12591294, 2004.14 N. Archak, A. Ghose, and P. Ipeirotis, Show me the money Deriving thepricing power of product features by mining consumer reviews, in Proceedingsof the ACM SIGKDD Conference on Knowledge Discovery and Data MiningKDD, 2007.15 S. Argamon, ed., Proceedings of the IJCAI Workshop on DOING IT WITHSTYLE Computational Approaches to Style Analysis and Synthesis. 2003.16 S. Argamon, J. Karlgren, and J. G. Shanahan, eds., Proceedings of the SIGIRWorkshop on Stylistic Analysis of Text For Information Access. ACM, 2005.17 S. Argamon, J. Karlgren, and O. Uzuner, eds., Proceedings of the SIGIR Workshop on Stylistics for Text Retrieval in Practice. ACM, 2006.18 S. ArgamonEngelson, M. Koppel, and G. Avneri, Stylebased text categorization What newspaper am I reading in Proceedings of the AAAI Workshop on Text Categorization, pp. 14, 1998.19 Y. Attali and J. Burstein, Automated essay scoring with erater v.2, Journalof Technology, Learning, and Assessment, vol. 26, February 2006.20 A. Aue and M. Gamon, Automatic identification of sentiment vocabularyExploiting low association with known sentiment terms, in Proceedings ofthe ACL Workshop on Feature Engineering for Machine Learning in NaturalLanguage Processing, 2005.21 A. Aue and M. Gamon, Customizing sentiment classifiers to new domainsA case study, in Proceedings of Recent Advances in Natural Language Processing RANLP, 2005.22 B. Awerbuch and R. Kleinberg, Competitive collaborative learning, in Proceedings of the Conference on Learning Theory COLT, pp. 233248, 2005.Journal version to appear in Journal of Computer and System Sciences, special issue on computational learning theory.23 P. Bajari and A. Hortacsu, The winners curse, reserve prices, and endogenousentry Empirical insights from eBay auctions, RAND Journal of Economics,vol. 34, pp. 329355, 2003.24 P. Bajari and A. Hortacsu, Economic insights from internet auctions, Journal of Economic Literature, vol. 42, pp. 457486, 2004.References 11525 C. F. Baker, C. J. Fillmore, and J. B. Lowe, The Berkeley Framenet Project,in Proceedings of COLINGACL, 1998.26 A. Banfield, Unspeakable Sentences Narration and Representation in the Language of Fiction. Routledge and Kegan Paul, 1982.27 M. Bansal, C. Cardie, and L. Lee, The power of negative thinking Exploitinglabel disagreement in the mincut classification framework, in Proceedings ofthe International Conference on Computational Linguistics COLING, 2008.Poster paper.28 R. BarHaim, I. Dagan, B. Dolan, L. Ferro, D. Giampiccolo, B. Magnini, andI. Szpektor, The second PASCAL recognising textual entailment challenge,in Proceedings of the Second PASCAL Challenges Workshop on RecognisingTextual Entailment, 2006.29 R. Barzilay and L. Lee, Learning to paraphrase An unsupervised approachusing multiplesequence alignment, in Proceedings of the Joint Human Language TechnologyNorth American Chapter of the ACL Conference HLTNAACL, pp. 1623, 2003.30 R. Barzilay and K. McKeown, Extracting paraphrases from a parallelcorpus, in Proceedings of the Association for Computational LinguisticsACL, pp. 5057, 2001.31 S. Basuroy, S. Chatterjee, and S. A. Ravid, How critical are critical reviewsThe box office effects of film critics, star power and budgets, Journal ofMarketing, vol. 67, pp. 103117, 2003.32 M. Bautin, L. Vijayarenu, and S. Skiena, International sentiment analysis fornews and blogs, in Proceedings of the International Conference on Weblogsand Social Media ICWSM, 2008.33 P. Beineke, T. Hastie, C. Manning, and S. Vaithyanathan, Exploring sentiment summarization, in Proceedings of the AAAI Spring Symposium onExploring Attitude and Affect in Text, AAAI technical report SS0407, 2004.34 F. Benamara, C. Cesarano, A. Picariello, D. Reforgiato, and V. S. Subrahmanian, Sentiment analysis Adjectives and adverbs are better than adjectivesalone, in Proceedings of the International Conference on Weblogs and SocialMedia ICWSM, 2007. Short paper.35 J. Berger, A. T. Sorensen, and S. J. Rasmussen, Negative publicityWhen is negative a positive, Manuscript. PDF files last modificationdate October 16, 2007, URL httpwww.stanford.eduasorensepapersNegative Publicity.pdf, 2007.36 Y. Bestgen, C. Fairon, and L. Kerves, Un barometre affectif effectif Corpusde reference et methode pour determiner la valence affective de phrases, inJournees internationales danalyse statistique des donnes textuelles JADT,pp. 182191, 2004.37 S. Bethard, H. Yu, A. Thornton, V. Hatzivassiloglou, and D. Jurafsky, Automatic extraction of opinion propositions and their holders, in Proceedingsof the AAAI Spring Symposium on Exploring Attitude and Affect in Text,2004.38 D. Biber, Variation Across Speech and Writing. Cambridge University Press,1988.116 References39 D. M. Blei, A. Y. Ng, and M. I. Jordan, Latent Dirichlet allocation, Journalof Machine Learning Research, vol. 3, pp. 9931022, 2003.40 J. Blitzer, M. Dredze, and F. Pereira, Biographies, Bollywood, boomboxesand blenders Domain adaptation for sentiment classification, in Proceedingsof the Association for Computational Linguistics ACL, 2007.41 S. R. K. Branavan, H. Chen, J. Eisenstein, and R. Barzilay, Learningdocumentlevel semantic properties from freetext annotations, in Proceedings of the Association for Computational Linguistics ACL, 2008.42 E. Breck and C. Cardie, Playing the telephone game Determining the hierarchical structure of perspective and speech expressions, in Proceedings ofthe International Conference on Computational Linguistics COLING, 2004.43 E. Breck, Y. Choi, and C. Cardie, Identifying expressions of opinion in context, in Proceedings of the International Joint Conference on Artificial Intelligence IJCAI, Hyderabad, India, 2007.44 S. Brin and L. Page, The anatomy of a largescale hypertextual web searchengine, in Proceedings of the 7th International World Wide Web Conference,pp. 107117, 1998.45 R. F. Bruce and J. M. Wiebe, Recognizing subjectivity A case study inmanual tagging, Natural Language Engineering, vol. 5, 1999.46 J. K. Burgoon, J. P. Blair, T. Qin, and J. F. Nunamaker, Jr., Detecting deception through linguistic analysis, in Proceedings of Intelligence and SecurityInformatics ISI, number 2665 in Lecture Notes in Computer Science, p. 958,2008.47 L. Cabral and A. Hortacsu, The dynamics of seller reputation Theory andevidence from eBay, Working Paper, downloaded version revised in March,2006, URL httppages.stern.nyu.edulcabralworkingpapersCabralHortacsu Mar06.pdf, 2006.48 J. Carbonell, Subjective Understanding Computer Models of Belief Systems.PhD thesis, Yale, 1979.49 C. Cardie, Empirical methods in information extraction, AI Magazine,vol. 18, pp. 6579, 1997.50 C. Cardie, C. Farina, T. Bruce, and E. Wagner, Using natural languageprocessing to improve eRulemaking, in Proceedings of Digital GovernmentResearch dg.o, 2006.51 C. Cardie, J. Wiebe, T. Wilson, and D. Litman, Combining lowlevel andsummary representations of opinions for multiperspective question answering, in Proceedings of the AAAI Spring Symposium on New Directions inQuestion Answering, pp. 2027, 2003.52 G. Carenini, R. Ng, and A. Pauls, Multidocument summarization of evaluative text, in Proceedings of the European Chapter of the Association forComputational Linguistics EACL, pp. 305312, 2006.53 G. Carenini, R. T. Ng, and A. Pauls, Interactive multimedia summaries ofevaluative text, in Proceedings of Intelligent User Interfaces IUI, pp. 124131, ACM Press, 2006.54 D. Cartwright and F. Harary, Structural balance A generalization of Heiderstheory, Psychological Review, vol. 63, pp. 277293, 1956.References 11755 P. Chaovalit and L. Zhou, Movie review mining A comparison betweensupervised and unsupervised classification approaches, in Proceedings of theHawaii International Conference on System Sciences HICSS, 2005.56 P.Y. S. Chen, S.Y. Wu, and J. Yoon, The impact of online recommendationsand consumer feedback on sales, in International Conference on InformationSystems ICIS, pp. 711724, 2004.57 Y. Chen and J. Xie, Online consumer review Wordofmouth as a newelement of marketing communication mix, Management Science, vol. 54,pp. 477491, 2008.58 P. Chesley, B. Vincent, L. Xu, and R. Srihari, Using verbs and adjectives toautomatically classify blog sentiment, in AAAI Symposium on ComputationalApproaches to Analysing Weblogs AAAICAAW, pp. 2729, 2006.59 J. A. Chevalier and D. Mayzlin, The effect of word of mouth on sales Onlinebook reviews, Journal of Marketing Research, vol. 43, pp. 345354, August2006.60 Y. Choi, E. Breck, and C. Cardie, Joint extraction of entities and relations foropinion recognition, in Proceedings of the Conference on Empirical Methodsin Natural Language Processing EMNLP, 2006.61 Y. Choi, C. Cardie, E. Riloff, and S. Patwardhan, Identifying sources of opinions with conditional random fields and extraction patterns, in Proceedings ofthe Human Language Technology Conference and the Conference on EmpiricalMethods in Natural Language Processing HLTEMNLP, 2005.62 E. K. Clemons, G. Gao, and L. M. Hitt, When online reviews meet hyperdifferentiation A study of the craft beer industry, Journal of ManagementInformation Systems, vol. 23, pp. 149171, 2006.63 comScorethe Kelsey group, Online consumergenerated reviews have significant impact on offline purchase behavior, Press Release, httpwww.comscore.compressrelease.asppress1928, November 2007.64 J. G. Conrad and F. Schilder, Opinion mining in legal blogs, in Proceedingsof the International Conference on Artificial Intelligence and Law ICAIL,pp. 231236, New York, NY, USA ACM, 2007.65 W. B. Croft and J. Lafferty, eds., Language modeling for information retrieval.Number 13 in the Information Retrieval Series. KluwerSpringer, 2003.66 S. Das and M. Chen, Yahoo for Amazon Extracting market sentiment fromstock message boards, in Proceedings of the Asia Pacific Finance AssociationAnnual Conference APFA, 2001.67 S. R. Das and M. Y. Chen, Yahoo for Amazon Sentiment extraction fromsmall talk on the Web, Management Science, vol. 53, pp. 13751388, 2007.68 S. R. Das, P. Tufano, and F. de Asis MartinezJerez, eInformation A clinicalstudy of investor discussion and sentiment, Financial Management, vol. 34,pp. 103137, 2005.69 K. Dave, S. Lawrence, and D. M. Pennock, Mining the peanut gallery Opinion extraction and semantic classification of product reviews, in Proceedingsof WWW, pp. 519528, 2003.70 S. David and T. J. Pinch, Six degrees of reputation The use and abuseof online review and recommendation systems, First Monday, July 2006.Special Issue on Commercial Applications of the Internet.118 References71 C. Dellarocas, The digitization of wordofmouth Promise and challengesof online reputation systems, Management Science, vol. 49, pp. 14071424,2003. Special issue on ebusiness and management science.72 C. Dellarocas, X. Zhang, and N. F. Awad, Exploring the value of onlineproduct ratings in revenue forecasting The case of motion pictures, Journalof Interactive Marketing, vol. 21, pp. 2345, 2007.73 A. Devitt and K. Ahmad, Sentiment analysis in financial news A cohesionbased approach, in Proceedings of the Association for Computational Linguistics ACL, pp. 984991, 2007.74 M. Dewally, Internet investment advice Investing with a rock of salt, Financial Analysts Journal, vol. 59, pp. 6577, JulyAugust 2003.75 M. Dewally and L. Ederington, Reputation, certification, warranties, andinformation as remedies for sellerbuyer information asymmetries Lessonsfrom the online comic book market, Journal of Business, vol. 79, pp. 693730,March 2006.76 S. Dewan and V. Hsu, Adverse selection in electronic markets Evidence fromonline stamp auctions, Journal of Industrial Economics, vol. 52, pp. 497516,December 2004.77 D. W. Diamond, Reputation acquisition in debt markets, Journal of Political Economy, vol. 97, pp. 828862, 1989.78 X. Ding, B. Liu, and P. S. Yu, A holistic lexiconbased approach to opinion mining, in Proceedings of the Conference on Web Search and Web DataMining WSDM, 2008.79 L. Dini and G. Mazzini, Opinion classification through information extraction, in Proceedings of the Conference on Data Mining Methods andDatabases for Engineering, Finance and Other Fields Data Mining,pp. 299310, 2002.80 W. Duan, B. Gu, and A. B. Whinston, Do online reviews matter An empirical investigation of panel data, Social Science Research NetworkSSRN Working Paper Series, httpssrn.compaper616262, version as ofJanuary, 2005.81 D. H. Eaton, Valuing information Evidence from guitar auctions on eBay,Journal of Applied Economics and Policy, vol. 24, pp. 119, 2005.82 D. H. Eaton, The impact of reputation timing and source on auction outcomes, The B. E. Journal of Economic Analysis and Policy, vol. 7, 2007.83 M. Efron, Cultural orientation Classifying subjective documents by cociation sic analysis, in Proceedings of the AAAI Fall Symposium on Style andMeaning in Language, Art, Music, and Design, pp. 4148, 2004.84 K. Eguchi and V. Lavrenko, Sentiment retrieval using generative models,in Proceedings of the Conference on Empirical Methods in Natural LanguageProcessing EMNLP, pp. 345354, 2006.85 K. Eguchi and C. Shah, Opinion retrieval experiments using generative models Experiments for the TREC 2006 blog track, in Proceedings of TREC,2006.86 P. Ekman, Emotion in the Human Face. Cambridge University Press, Seconded., 1982.References 11987 J. Eliashberg and S. M. Shugan, Film critics Influencers or predictors,Journal of Marketing, vol. 61, pp. 6878, April 1997.88 C. Engstrom, Topic Dependence in Sentiment Classification. Masters thesis,University of Cambridge, 2004.89 A. Esuli and F. Sebastiani, Determining the semantic orientation of termsthrough gloss analysis, in Proceedings of the ACM SIGIR Conference onInformation and Knowledge Management CIKM, 2005.90 A. Esuli and F. Sebastiani, Determining term subjectivity and term orientation for opinion mining, in Proceedings of the European Chapter of theAssociation for Computational Linguistics EACL, 2006.91 A. Esuli and F. Sebastiani, SentiWordNet A publicly available lexicalresource for opinion mining, in Proceedings of Language Resources and Evaluation LREC, 2006.92 A. Esuli and F. Sebastiani, PageRanking WordNet synsets An applicationto opinion mining, in Proceedings of the Association for Computational Linguistics ACL, 2007.93 D. K. Evans, L.W. Ku, Y. Seki, H.H. Chen, and N. Kando, Opinion analysisacross languages An overview of and observations from the NTCIR6 opinionanalysis pilot task, in Proceedings of the Workshop on CrossLanguage Information Processing, vol. 4578 Applications of Fuzzy Sets Theory of LectureNotes in Computer Science, pp. 456463, 2007.94 A. Fader, D. R. Radev, M. H. Crespin, B. L. Monroe, K. M. Quinn, andM. Colaresi, MavenRank Identifying influential members of the US senateusing lexical centrality, in Proceedings of the Conference on Empirical Methods in Natural Language Processing EMNLP, 2007.95 C. Fellbaum, ed., Wordnet An Electronic Lexical Database. MIT Press, 1998.96 D. Feng, E. Shaw, J. Kim, and E. Hovy, Learning to detect conversationfocus of threaded discussions, in Proceedings of the Joint Human LanguageTechnologyNorth American Chapter of the ACL Conference HLTNAACL,pp. 208215, 2006.97 A. Finn and N. Kushmerick, Learning to classify documents according togenre, Journal of the American Society for Information Science and Technology JASIST, vol. 7, 2006. Special issue on computational analysis ofstyle.98 A. Finn, N. Kushmerick, and B. Smyth, Genre classification and domaintransfer for information filtering, in Proceedings of the 24th BCSIRSG European Colloquium on IR Research Advances in Information Retrieval, number2291 in Lecture Notes in Computer Science, pp. 353362, Glasgow, 2002.99 P. W. Foltz, D. Laham, and T. K. Landauer, Automated essay scoring Applications to education technology, in Proceedings of EDMEDIA, pp. 939944,1999.100 C. Forman, A. Ghose, and B. Wiesenfeld, Examining the relationshipbetween reviews and sales The role of reviewer identity disclosure in electronic markets, Information Systems Research, vol. 19, 2008. Special issueon the interplay between digital and social networks.120 References101 G. Forman, An extensive empirical study of feature selection metrics for textclassification, Journal of Machine Learning Research, vol. 3, pp. 12891305,2003.102 T. Fukuhara, H. Nakagawa, and T. Nishida, Understanding sentiment ofpeople from news articles Temporal sentiment analysis of social events, inProceedings of the International Conference on Weblogs and Social MediaICWSM, 2007.103 M. Gamon, Sentiment classification on customer feedback data Noisy data,large feature vectors, and the role of linguistic analysis, in Proceedings of theInternational Conference on Computational Linguistics COLING, 2004.104 M. Gamon, A. Aue, S. CorstonOliver, and E. Ringger, Pulse Mining customer opinions from free text, in Proceedings of the International Symposiumon Intelligent Data Analysis IDA, number 3646 in Lecture Notes in Computer Science, pp. 121132, 2005.105 R. Ghani, K. Probst, Y. Liu, M. Krema, and A. Fano, Text mining for productattribute extraction, SIGKDD Explorations Newsletter, vol. 8, pp. 4148,2006.106 A. Ghose and P. G. Ipeirotis, Designing novel review ranking systems Predicting usefulness and impact of reviews, in Proceedings of the InternationalConference on Electronic Commerce ICEC, 2007. Invited paper.107 A. Ghose, P. G. Ipeirotis, and A. Sundararajan, Opinion mining using econometrics A case study on reputation systems, in Proceedings of the Associationfor Computational Linguistics ACL, 2007.108 N. Godbole, M. Srinivasaiah, and S. Skiena, Largescale sentiment analysisfor news and blogs, in Proceedings of the International Conference on Weblogsand Social Media ICWSM, 2007.109 A. B. Goldberg and X. Zhu, Seeing stars when there arent manystars Graphbased semisupervised learning for sentiment categorization, inTextGraphs HLTNAACL Workshop on Graphbased Algorithms for NaturalLanguage Processing, 2006.110 A. B. Goldberg, X. Zhu, and S. Wright, Dissimilarity in graphbased semisupervised classification, in Artificial Intelligence and Statistics AISTATS,2007.111 S. Greene, Spin Lexical Semantics, Transitivity, and the Identification ofImplicit Sentiment. PhD thesis, University of Maryland, 2007.112 G. Grefenstette, Y. Qu, J. G. Shanahan, and D. A. Evans, Couplingniche browsers and affect analysis for an opinion mining application, inProceedings of Recherche dInformation Assistee par Ordinateur RIAO,2004.113 M. L. Gregory, N. Chinchor, P. Whitney, R. Carter, E. Hetzler, and A. Turner,Userdirected sentiment analysis Visualizing the affective content of documents, in Proceedings of the Workshop on Sentiment and Subjectivity in Text,pp. 2330, Sydney, Australia, July 2006.114 B. Gu, P. Konana, A. Liu, B. Rajagopalan, and J. Ghosh, Predictive value ofstock message board sentiments, McCombs Research Paper No. IROM1106,version dated November, 2006.References 121115 R. V. Guha, R. Kumar, P. Raghavan, and A. Tomkins, Propagation of trustand distrust, in Proceedings of WWW, pp. 403412, 2004.116 B. A. Hagedorn, M. Ciaramita, and J. Atserias, World knowledge in broadcoverage information filtering, in Proceedings of the ACM Special InterestGroup on Information Retrieval SIGIR, 2007. Poster paper.117 J. T. Hancock, L. Curry, S. Goorha, and M. Woodworth, Automated linguistic analysis of deceptive and truthful synchronous computermediated communication, in Proceedings of the Hawaii International Conference on SystemSciences HICSS, p. 22c, 2005.118 L. Hankin, The effects of user reviews on online purchasing behavioracross multiple product categories, Masters final project report,UC Berkeley School of Information, httpwww.ischool.berkeley.edufileslhankin report.pdf, May 2007.119 V. Hatzivassiloglou and K. McKeown, Predicting the semantic orientation ofadjectives, in Proceedings of the Joint ACLEACL Conference, pp. 174181,1997.120 V. Hatzivassiloglou and J. Wiebe, Effects of adjective orientation and gradability on sentence subjectivity, in Proceedings of the International Conference on Computational Linguistics COLING, 2000.121 M. Hearst, Directionbased text interpretation as an information accessrefinement, in TextBased Intelligent Systems, P. Jacobs, ed., pp. 257274,Lawrence Erlbaum Associates, 1992.122 R. Higashinaka, M. Walker, and R. Prasad, Learning to generate naturalisticutterances using reviews in spoken dialogue systems, ACM Transactions onSpeech and Language Processing TSLP, 2007.123 P. Hitlin and L. Rainie, The use of online reputation and rating systems,Pew Internet  American Life Project Memo, October 2004.124 T. Hoffman, Online reputation management is hot  but is it ethicalComputerworld, February 2008.125 T. Hofmann, Probabilistic latent semantic indexing, in Proceedings ofSIGIR, pp. 5057, 1999.126 D. Hopkins and G. King, Extracting systematic social science meaning fromtext,. Manuscript available at httpgking.harvard.edufileswords.pdf,2007 version was the one most recently consulted, 2007.127 J. A. Horrigan, Online shopping, Pew Internet  American Life ProjectReport, 2008.128 D. Houser and J. Wooders, Reputation in auctions Theory, and evidence from eBay, Journal of Economics and Management Strategy, vol. 15,pp. 252369, 2006.129 M. Hu and B. Liu, Mining and summarizing customer reviews, in Proceedings of the ACM SIGKDD Conference on Knowledge Discovery and DataMining KDD, pp. 168177, 2004.130 M. Hu and B. Liu, Mining opinion features in customer reviews, in Proceedings of AAAI, pp. 755760, 2004.131 M. Hu, A. Sun, and E.P. Lim, Commentsoriented blog summarizationby sentence extraction, in Proceedings of the ACM SIGIR Conference on122 ReferencesInformation and Knowledge Management CIKM, pp. 901904, 2007. Posterpaper.132 N. Hu, P. A. Pavlou, and J. Zhang, Can online reviews reveal a products truequality Empirical findings and analytical modeling of online wordofmouthcommunication, in Proceedings of Electronic Commerce EC, pp. 324330,USA, New York, NY ACM, 2006.133 A. Huettner and P. Subasic, Fuzzy typing for document management, inACL 2000 Companion Volume Tutorial Abstracts and Demonstration Notes,pp. 2627, 2000.134 M. Hurst and K. Nigam, Retrieving topical sentiments from online documentcollections, in Document Recognition and Retrieval XI, pp. 2734, 2004.135 C. Jacquemin, Spotting and Discovering Terms through Natural Language Processing. MIT Press, 2001.136 G. Jin and A. Kato, Price, quality and reputation Evidence from an onlinefield experiment, The RAND Journal of Economics, vol. 37, 2006.137 X. Jin, Y. Li, T. Mah, and J. Tong, Sensitive webpage classification forcontent advertising, in Proceedings of the International Workshop on DataMining and Audience Intelligence for Advertising, 2007.138 N. Jindal and B. Liu, Identifying comparative sentences in text documents,in Proceedings of the ACM Special Interest Group on Information RetrievalSIGIR, 2006.139 N. Jindal and B. Liu, Mining comparative sentences and relations, in Proceedings of AAAI, 2006.140 N. Jindal and B. Liu, Review spam detection, in Proceedings of WWW,2007. Poster paper.141 N. Jindal and B. Liu, Opinion spam and analysis, in Proceedings of theConference on Web Search and Web Data Mining WSDM, pp. 219230,2008.142 N. Kaji and M. Kitsuregawa, Automatic construction of polaritytagged corpus from HTML documents, in Proceedings of the COLINGACL Main Conference Poster Sessions, 2006.143 N. Kaji and M. Kitsuregawa, Building lexicon for sentiment analysis frommassive collection of HTML documents, in Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning EMNLPCoNLL, pp. 10751083,2007.144 A. Kale, A. Karandikar, P. Kolari, A. Java, T. Finin, and A. Joshi, Modelingtrust and influence in the blogosphere using link polarity, in Proceedings ofthe International Conference on Weblogs and Social Media ICWSM, 2007.Short paper.145 K. Kalyanam and S. H. McIntyre, The role of reputation in online auctionmarkets, Santa Clara University Working Paper 020310WP, 2001, datedJune 26.146 J. Kamps, M. Marx, R. J. Mokken, and M. de Rijke, Using WordNet tomeasure semantic orientation of adjectives, in Proceedings of LREC, 2004.References 123147 S. D. Kamvar, M. T. Schlosser, and H. GarciaMolina, The Eigentrust algorithm for reputation management in P2P networks, in Proceedings of WWW,pp. 640651, New York, NY, USA ACM, ISBN 1581136803, 2003.148 H. Kanayama and T. Nasukawa, Fully automatic lexicon expansion fordomainoriented sentiment analysis, in Proceedings of the Conference onEmpirical Methods in Natural Language Processing EMNLP, Sydney,Australia, pp. 355363, July 2006.149 M. Kantrowitz, Method and apparatus for analyzing affect and emotion intext, U.S. Patent 6622140, Patent filed in November 2000, 2003.150 J. Karlgren and D. Cutting, Recognizing text genres with simple metricsusing discriminant analysis, in Proceedings of COLING, pp. 10711075, 1994.151 Y. Kawai, T. Kumamoto, and K. Tanaka, Fair news reader Recommending news articles with different sentiments based on user preference, in Proceedings of KnowledgeBased Intelligent Information and Engineering SystemsKES, number 4692 in Lecture Notes in Computer Science, pp. 612622,2007.152 A. Kennedy and D. Inkpen, Sentiment classification of movie reviews usingcontextual valence shifters, Computational Intelligence, vol. 22, pp. 110125,2006.153 B. Kessler, G. Nunberg, and H. Schutze, Automatic detection of text genre,in Proceedings of the ThirtyFifth Annual Meeting of the Association for Computational Linguistics and Eighth Conference of the European Chapter of theAssociation for Computational Linguistics, pp. 3238, 1997.154 P. Kim, The forrester wave Brand monitoring, Q3 2006, Forrester Wavewhite paper, 2006.155 S.M. Kim and E. Hovy, Determining the sentiment of opinions, in Proceedings of the International Conference on Computational Linguistics COLING, 2004.156 S.M. Kim and E. Hovy, Automatic detection of opinion bearing words andsentences, in Companion Volume to the Proceedings of the International JointConference on Natural Language Processing IJCNLP, 2005.157 S.M. Kim and E. Hovy, Identifying opinion holders for question answering inopinion texts, in Proceedings of the AAAI Workshop on Question Answeringin Restricted Domains, 2005.158 S.M. Kim and E. Hovy, Automatic identification of pro and con reasons inonline reviews, in Proceedings of the COLINGACL Main Conference PosterSessions, pp. 483490, 2006.159 S.M. Kim and E. Hovy, Identifying and analyzing judgment opinions, inProceedings of the Joint Human Language TechnologyNorth American Chapter of the ACL Conference HLTNAACL, 2006.160 S.M. Kim and E. Hovy, Crystal Analyzing predictive opinions on the web,in Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning EMNLPCoNLL, 2007.161 S.M. Kim, P. Pantel, T. Chklovski, and M. Pennacchiotti, Automaticallyassessing review helpfulness, in Proceedings of the Conference on Empirical124 ReferencesMethods in Natural Language Processing EMNLP, pp. 423430, Sydney,Australia, July 2006.162 B. Klein and K. Leffler, The role of market forces in assuring contractualperformance, Journal of Political Economy, vol. 89, pp. 615641, 1981.163 J. Kleinberg, Authoritative sources in a hyperlinked environment, in Proceedings of the 9th ACMSIAM Symposium on Discrete Algorithms SODA,pp. 668677, 1998. Extended version in Journal of the ACM, 46604632,1999.164 J. Kleinberg and E. Tardos, Approximation algorithms for classification problems with pairwise relationships Metric labeling and Markov random fields,Journal of the ACM, vol. 49, pp. 616639, ISSN 00045411, 2002.165 J. Kleinberg and E. Tardos, Algorithm Design. Addison Wesley, 2006.166 N. Kobayashi, K. Inui, Y. Matsumoto, K. Tateishi, and T. Fukushima, Collecting evaluative expressions for opinion extraction, in Proceedings of theInternational Joint Conference on Natural Language Processing IJCNLP,2004.167 M. Koppel and J. Schler, The importance of neutral examples for learningsentiment, in Workshop on the Analysis of Informal and Formal InformationExchange During Negotiations FINEXIN, 2005.168 M. Koppel and I. Shtrimberg, Good news or bad news Let the marketdecide, in Proceedings of the AAAI Spring Symposium on Exploring Attitudeand Affect in Text Theories and Applications, pp. 8688, 2004.169 L.W. Ku, L.Y. Li, T.H. Wu, and H.H. Chen, Major topic detection andits application to opinion summarization, in Proceedings of the ACM SpecialInterest Group on Information Retrieval SIGIR, pp. 627628, 2005. Posterpaper.170 L.W. Ku, Y.T. Liang, and H.H. Chen, Opinion extraction, summarizationand tracking in news and blog corpora, in AAAI Symposium on Computational Approaches to Analysing Weblogs AAAICAAW, pp. 100107, 2006.171 L.W. Ku, Y.T. Liang, and H.H. Chen, Tagging heterogeneous evaluationcorpora for opinionated tasks, in Conference on Language Resources andEvaluation LREC, 2006.172 L.W. Ku, Y.S. Lo, and H.H. Chen, Test collection selection and gold standard generation for a multiplyannotated opinion corpus, in Proceedings ofthe ACL Demo and Poster Sessions, pp. 8992, 2007.173 T. Kudo and Y. Matsumoto, A boosting algorithm for classification of semistructured text, in Proceedings of the Conference on Empirical Methods inNatural Language Processing EMNLP, 2004.174 S. Kurohashi, K. Inui, and Y. Kato, eds., Workshop on Information Credibilityon the Web, 2007.175 N. Kwon, S. Shulman, and E. Hovy, Multidimensional text analysis for eRulemaking, in Proceedings of Digital Government Research dg.o, 2006.176 J. Lafferty, A. McCallum, and F. Pereira, Conditional random fields Probabilistic models for segmenting and labeling sequence data, in Proceedings ofICML, pp. 282289, 2001.References 125177 J. D. Lafferty and C. Zhai, Document language models, query models,and risk minimization for information retrieval, in Proceedings of SIGIR,pp. 111119, 2001.178 M. Laver, K. Benoit, and J. Garry, Extracting policy positions from political texts using words as data, American Political Science Review, vol. 97,pp. 311331, 2003.179 V. Lavrenko and W. Bruce Croft, Relevancebased language models, inProceedings of SIGIR, pp. 120127, 2001.180 C. G. Lawson and V. C. Slawson, Reputation in an internet auction market,Economic Inquiry, vol. 40, pp. 533650, 2002.181 L. Lee, Im sorry Dave, Im afraid I cant do that Linguistics, statistics,and natural language processing circa 2001, in Computer Science Reflectionson the Field, Reflections from the Field, Committee on the Fundamentalsof Computer Science Challenges and Opportunities, Computer Science andTelecommunications Board, National Research Council, ed., pp. 111118, TheNational Academies Press, 2004.182 Y.B. Lee and S. H. Myaeng, Text genre classification with genrerevealingand subjectrevealing features, in Proceedings of the ACM Special InterestGroup on Information Retrieval SIGIR, 2002.183 D. Leinweber and A. Madhavan, Three hundred years of stock market manipulation, Journal of Investing, vol. 10, pp. 716, Summer 2001.184 H. Li and K. Yamanishi, Mining from open answers in questionnaire data,in Proceedings of the ACM SIGKDD Conference on Knowledge Discovery andData Mining KDD, pp. 443449, 2001. Journal version in IEEE IntelligentSystems vol. 17, no. 5, pp. 5863, 2002.185 Y. Li, Z. Zheng, and H. Dai, KDD CUP2005 report Facing a great challenge, SIGKDD Explorations, vol. 7, pp. 9199, 2005.186 W.H. Lin and A. Hauptmann, Are these documents written from differentperspectives A test of different perspectives based on statistical distributiondivergence, in Proceedings of the International Conference on ComputationalLinguistics COLINGProceedings of the Association for Computational Linguistics ACL, pp. 10571064, Sydney, Australia Association for Computational Linguistics, July 2006.187 W.H. Lin, T. Wilson, J. Wiebe, and A. Hauptmann, Which side are you onIdentifying perspectives at the document and sentence levels, in Proceedingsof the Conference on Natural Language Learning CoNLL, 2006.188 J. Liscombe, G. Riccardi, and D. HakkaniTur, Using context to improveemotion detection in spoken dialog systems, in Interspeech, pp. 18451848,2005.189 L. V. Lita, A. H. Schlaikjer, W. Hong, and E. Nyberg, Qualitative dimensionsin question answering Extending the definitional QA task, in Proceedings ofAAAI, pp. 16161617, 2005. Student abstract.190 B. Liu, Web data mining Exploring hyperlinks, contents, and usage data,Opinion Mining. Springer, 2006.191 B. Liu, M. Hu, and J. Cheng, Opinion observer Analyzing and comparingopinions on the web, in Proceedings of WWW, 2005.126 References192 H. Liu, H. Lieberman, and T. Selker, A model of textual affect sensing usingrealworld knowledge, in Proceedings of Intelligent User Interfaces IUI,pp. 125132, 2003.193 J. Liu, Y. Cao, C.Y. Lin, Y. Huang, and M. Zhou, Lowquality product review detection in opinion summarization, in Proceedings of theJoint Conference on Empirical Methods in Natural Language Processing andComputational Natural Language Learning EMNLPCoNLL, pp. 334342,2007. Poster paper.194 Y. Liu, Wordofmouth for movies Its dynamics and impact on box officerevenue, Journal of Marketing, vol. 70, pp. 7489, 2006.195 Y. Liu, J. Huang, A. An, and X. Yu, ARSA A sentimentaware model forpredicting sales performance using blogs, in Proceedings of the ACM SpecialInterest Group on Information Retrieval SIGIR, 2007.196 J. A. Livingston, How valuable is a good reputation A sample selectionmodel of internet auctions, The Review of Economics and Statistics, vol. 87,pp. 453465, August 2005.197 L. Lloyd, D. Kechagias, and S. Skiena, Lydia A system for largescalenews analysis, in Proceedings of String Processing and Information RetrievalSPIRE, number 3772 in Lecture Notes in Computer Science, pp. 161166,2005.198 D. LuckingReiley, D. Bryan, N. Prasad, and D. Reeves, Pennies from eBayThe determinants of price in online auctions, Journal of Industrial Economics, vol. 55, pp. 223233, 2007.199 C. Macdonald and I. Ounis, The TREC Blogs06 collection Creating andanalysing a blog test collection, Technical Report TR2006224, Departmentof Computer Science, University of Glasgow, 2006.200 Y. Mao and G. Lebanon, Sequential models for sentiment prediction, inICML Workshop on Learning in Structured Output Spaces, 2006.201 Y. Mao and G. Lebanon, Isotonic conditional random fields and local sentiment flow, in Advances in Neural Information Processing Systems, 2007.202 L. W. Martin and G. Vanberg, A robust transformation procedure for interpreting political text, Political Analysis, vol. 16, pp. 93100, 2008.203 H. Masum and Y.C. Zhang, Manifesto for the reputation society, FirstMonday, vol. 9, 2004.204 S. Matsumoto, H. Takamura, and M. Okumura, Sentiment classification usingword subsequences and dependency subtrees, in Proceedings of PAKDD05,the 9th PacificAsia Conference on Advances in Knowledge Discovery andData Mining, 2005.205 R. McDonald, K. Hannan, T. Neylon, M. Wells, and J. Reynar, Structuredmodels for finetocoarse sentiment analysis, in Proceedings of the Associationfor Computational Linguistics ACL, pp. 432439, Prague, Czech RepublicAssociation for Computational Linguistics, June 2007.206 Q. Mei, X. Ling, M. Wondra, H. Su, and C. X. Zhai, Topic sentiment mixtureModeling facets and opinions in weblogs, in Proceedings of WWW, pp. 171180, New York, NY, USA ACM Press, 2007. ISBN 9781595936547.References 127207 M. I. Melnik and J. Alm, Does a sellers eCommerce reputation matter Evidence from eBay auctions, Journal of Industrial Economics, vol. 50, pp. 337349, 2002.208 M. I. Melnik and J. Alm, Seller reputation, information signals, and prices forheterogeneous coins on eBay, Southern Economic Journal, vol. 72, pp. 305328, 2005.209 R. Mihalcea, C. Banea, and J. Wiebe, Learning multilingual subjective language via crosslingual projections, in Proceedings of the Association forComputational Linguistics ACL, pp. 976983, Prague, Czech Republic, June2007.210 R. Mihalcea and C. Strapparava, Learning to laugh automatically Computational models for humor recognition, Journal of Computational Intelligence, 2006.211 G. Mishne and M. de Rijke, Capturing global mood levels using blog posts,in AAAI Symposium on Computational Approaches to Analysing WeblogsAAAICAAW, pp. 145152, 2006.212 G. Mishne and M. de Rijke, Moodviews Tools for blog mood analysis,in AAAI Symposium on Computational Approaches to Analysing WeblogsAAAICAAW, pp. 153154, 2006.213 G. Mishne and M. de Rijke, A study of blog search, in Proceedings of theEuropean Conference on Information Retrieval Research ECIR, 2006.214 G. Mishne and N. Glance, Predicting movie sales from blogger sentiment,in AAAI Symposium on Computational Approaches to Analysing WeblogsAAAICAAW, pp. 155158, 2006.215 S. Morinaga, K. Yamanishi, K. Tateishi, and T. Fukushima, Mining productreputations on the Web, in Proceedings of the ACM SIGKDD Conference onKnowledge Discovery and Data Mining KDD, pp. 341349, 2002. Industrytrack.216 F. Mosteller and D. L. Wallace, Applied Bayesian and Classical Inference TheCase of the Federalist Papers. SpringerVerlag, 1984.217 T. Mullen and N. Collier, Sentiment analysis using support vector machineswith diverse information sources, in Proceedings of the Conference on Empirical Methods in Natural Language Processing EMNLP, pp. 412418, July2004. Poster paper.218 T. Mullen and R. Malouf, Taking sides User classification for informal onlinepolitical discourse, Internet Research, vol. 18, pp. 177190, 2008.219 T. Mullen and R. Malouf, A preliminary investigation into sentimentanalysis of informal political discourse, in AAAI Symposium on Computational Approaches to Analysing Weblogs AAAICAAW, pp. 159162,2006.220 J.C. Na, H. Sui, C. Khoo, S. Chan, and Y. Zhou, Effectiveness of simple linguistic processing in automatic sentiment classification of product reviews, inConference of the International Society for Knowledge Organization ISKO,pp. 4954, 2004.221 T. Nasukawa and J. Yi, Sentiment analysis Capturing favorability usingnatural language processing, in Proceedings of the Conference on KnowledgeCapture KCAP, 2003.128 References222 V. Ng, S. Dasgupta, and S. M. N. Arifin, Examining the role of linguistic knowledge sources in the automatic identification and classification ofreviews, in Proceedings of the COLINGACL Main Conference Poster Sessions, pp. 611618, Sydney, Australia Association for Computational Linguistics, July 2006.223 X. Ni, G.R. Xue, X. Ling, Y. Yu, and Q. Yang, Exploring in the weblog spaceby detecting informative and affective articles, in Proceedings of WWW, 2007.Industrial practice and experience track.224 N. Nicolov, F. Salvetti, M. Liberman, and J. H. Martin, eds., AAAI Symposium on Computational Approaches to Analysing Weblogs AAAICAAW.AAAI Press, 2006.225 K. Nigam and M. Hurst, Towards a robust metric of polarity, in ComputingAttitude and Affect in Text Theories and Applications, number 20 in TheInformation Retrieval Series, J. G. Shanahan, Y. Qu, and J. Wiebe, eds.,2006.226 Y. Niu, X. Zhu, J. Li, and G. Hirst, Analysis of polarity information inmedical text, in Proceedings of the American Medical Informatics Association2005 Annual Symposium, 2005.227 I. Ounis, M. de Rijke, C. Macdonald, G. Mishne, and I. Soboroff, Overviewof the TREC2006 blog track, in Proceedings of the 15th Text Retrieval Conference TREC, 2006.228 I. Ounis, C. Macdonald, and I. Soboroff, On the TREC blog track, inProceedings of the International Conference on Weblogs and Social MediaICWSM, 2008.229 S. Owsley, S. Sood, and K. J. Hammond, Domain specific affective classification of documents, in AAAI Symposium on Computational Approaches toAnalysing Weblogs AAAICAAW, pp. 181183, 2006.230 M. Palmer, D. Gildea, and P. Kingsbury, The proposition bank A corpusannotated with semantic roles, Computational Linguistics, vol. 31, March2005.231 B. Pang, K. Knight, and D. Marcu, Syntaxbased alignment of multiple translations Extracting paraphrases and generating new sentences, in Proceedingsof HLTNAACL, 2003.232 B. Pang and L. Lee, A sentimental education Sentiment analysis using subjectivity summarization based on minimum cuts, in Proceedings of the Association for Computational Linguistics ACL, pp. 271278, 2004.233 B. Pang and L. Lee, Seeing stars Exploiting class relationships for sentimentcategorization with respect to rating scales, in Proceedings of the Associationfor Computational Linguistics ACL, pp. 115124, 2005.234 B. Pang and L. Lee, Using very simple statistics for review search An exploration, in Proceedings of the International Conference on Computational Linguistics COLING, 2008. Poster paper.235 B. Pang, L. Lee, and S. Vaithyanathan, Thumbs up Sentiment classification using machine learning techniques, in Proceedings of the Conferenceon Empirical Methods in Natural Language Processing EMNLP, pp. 7986,2002.References 129236 D.H. Park, J. Lee, and I. Han, The effect of online consumer reviewson consumer purchasing intention The moderating role of involvement,International Journal of Electronic Commerce, vol. 11, pp. 125148, ISSN10864415, 2007.237 P. A. Pavlou and A. Dimoka, The nature and role of feedback text commentsin online marketplaces Implications for trust building, price premiums, andseller differentiation, Information Systems Research, vol. 17, pp. 392414,2006.238 S. Piao, S. Ananiadou, Y. Tsuruoka, Y. Sasaki, and J. McNaught, Miningopinion polarity relations of citations, in International Workshop on Computational Semantics IWCS, pp. 366371, 2007. Short paper.239 R. Picard, Affective Computing. MIT Press, 1997.240 T. Pinch and K. Athanasiades, ACIDplanet A study of users of an onlinemusic community, 2005. httpsts.nthu.edu.twsts campfilesACIDplanet20by20Trevor20Pinch.ppt, Presented at the 50th Society for Ethnomusicology SEM conference.241 G. Pinski and F. Narin, Citation influence for journal aggregates of scientificpublications Theory, with application to the literature of physics, Information Processing and Management, vol. 12, pp. 297312, 1976.242 L. Polanyi and A. Zaenen, Contextual lexical valence shifters, in Proceedingsof the AAAI Spring Symposium on Exploring Attitude and Affect in Text,AAAI technical report SS0407, 2004.243 J. M. Ponte and W. Bruce Croft, A language modeling approach to information retrieval, in Proceedings of SIGIR, pp. 275281, 1998.244 A.M. Popescu and O. Etzioni, Extracting product features and opinionsfrom reviews, in Proceedings of the Human Language Technology Conferenceand the Conference on Empirical Methods in Natural Language ProcessingHLTEMNLP, 2005.245 R. Quirk, S. Greenbaum, G. Leech, and J. Svartvik, A comprehensive grammarof the English language. Longman, 1985.246 D. Radev, T. Allison, S. BlairGoldensohn, J. Blitzer, A. Celebi, S. Dimitrov,E. Drabek, A. Hakim, W. Lam, D. Liu, J. Otterbacher, H. Qi, H. Saggion,S. Teufel, M. Topper, A. Winkel, and Z. Zhang, MEAD  A platform formultidocument multilingual text summarization, in Conference on LanguageResources and Evaluation LREC, Lisbon, Portugal, May 2004.247 D. R. Radev, E. Hovy, and K. McKeown, Introduction to the special issueon summarization, Computational Linguistics, vol. 28, pp. 399408, ISSN08912017, 2002.248 L. Rainie and J. Horrigan, Election 2006 online, Pew Internet  AmericanLife Project Report, January 2007.249 J. Read, Using emoticons to reduce dependency in machine learning techniques for sentiment classification, in Proceedings of the ACL StudentResearch Workshop, 2005.250 D. A. Reinstein and C. M. Snyder, The influence of expert reviews on consumer demand for experience goods A case study of movie critics, Journalof Industrial Economics, vol. 53, pp. 2751, 2005.130 References251 E. Reiter and R. Dale, Building Natural Language Generation Systems. Cambridge, 2000.252 P. Resnick, K. Kuwabara, R. Zeckhauser, and E. Friedman, Reputationsystems, Communications of the Association for Computing MachineryCACM, vol. 43, pp. 4548, ISSN 00010782, 2000.253 P. Resnick, R. Zeckhauser, J. Swanson, and K. Lockwood, The value of reputation on eBay A controlled experiment, Experimental Economics, vol. 9,pp. 79101, 2006.254 E. Riloff, S. Patwardhan, and J. Wiebe, Feature subsumption for opinionanalysis, in Proceedings of the Conference on Empirical Methods in NaturalLanguage Processing EMNLP, 2006.255 E. Riloff and J. Wiebe, Learning extraction patterns for subjective expressions, in Proceedings of the Conference on Empirical Methods in NaturalLanguage Processing EMNLP, 2003.256 E. Riloff, J. Wiebe, and W. Phillips, Exploiting subjectivity classificationto improve information extraction, in Proceedings of AAAI, pp. 11061111,2005.257 E. Riloff, J. Wiebe, and T. Wilson, Learning subjective nouns using extraction pattern bootstrapping, in Proceedings of the Conference on Natural Language Learning CoNLL, pp. 2532, 2003.258 E. Rogers, Diffusion of Innovations. Free Press, New York, 1962. ISBN0743222091. Fifth edition dated 2003.259 S. Rosen, Hedonic prices and implicit markets Product differentiation in purecompetition, The Journal of Political Economy, vol. 82, pp. 3455, JanFeb1974.260 D. Roth and W. Yih, Probabilistic reasoning for entity and relation recognition, in Proceedings of the International Conference on Computational Linguistics COLING, 2004.261 V. L. Rubin and E. D. Liddy, Assessing credibility of weblogs, in AAAI Symposium on Computational Approaches to Analysing Weblogs AAAICAAW,pp. 187190, 2006.262 W. Sack, On the computation of point of view, in Proceedings of AAAI,p. 1488, 1994. Student abstract.263 F. Sebastiani, Machine learning in automated text categorization, ACMComputing Surveys, vol. 34, pp. 147, 2002.264 Y. Seki, K. Eguchi, and N. Kando, Analysis of multidocument viewpointsummarization using multidimensional genres, in Proceedings of the AAAISpring Symposium on Exploring Attitude and Affect in Text Theories andApplications, pp. 142145, 2004.265 Y. Seki, K. Eguchi, N. Kando, and M. Aono, Multidocument summarizationwith subjectivity analysis at DUC 2005, in Proceedings of the DocumentUnderstanding Conference DUC, 2005.266 Y. Seki, K. Eguchi, N. Kando, and M. Aono, Opinionfocused summarizationand its analysis at DUC 2006, in Proceedings of the Document UnderstandingConference DUC, pp. 122130, 2006.267 Y. Seki, D. Kirk Evans, L.W. Ku, H.H. Chen, N. Kando, and C.Y. Lin,Overview of opinion analysis pilot task at NTCIR6, in Proceedings of theReferences 131Workshop Meeting of the National Institute of Informatics NII Test Collection for Information Retrieval Systems NTCIR, pp. 265278, 2007.268 C. Shapiro, Consumer information, product quality, and seller reputation,Bell Journal of Economics, vol. 13, pp. 2035, 1982.269 C. Shapiro, Premiums for high quality products as returns to reputations,Quarterly Journal of Economics, vol. 98, pp. 659680, 1983.270 B. Shneiderman, Tree visualization with treemaps 2d spacefillingapproach, ACM Transactions on Graphics, vol. 11, pp. 9299, 1992.271 S. Shulman, J. Callan, E. Hovy, and S. Zavestoski, Language processing technologies for electronic rulemaking A project highlight, in Proceedings of Digital Government Research dg.o, pp. 8788, 2005.272 B. Snyder and R. Barzilay, Multiple aspect ranking using the Good Griefalgorithm, in Proceedings of the Joint Human Language TechnologyNorthAmerican Chapter of the ACL Conference HLTNAACL, pp. 300307, 2007.273 S. Somasundaran, J. Ruppenhofer, and J. Wiebe, Detecting arguing andsentiment in meetings, in Proceedings of the SIGdial Workshop on Discourseand Dialogue, 2007.274 S. Somasundaran, T. Wilson, J. Wiebe, and V. Stoyanov, QA with attitudeExploiting opinion type analysis for improving question answering in onlinediscussions and the news, in Proceedings of the International Conference onWeblogs and Social Media ICWSM, 2007.275 X. Song, Y. Chi, K. Hino, and B. Tseng, Identifying opinion leaders in theblogosphere, in Proceedings of the ACM SIGIR Conference on Informationand Knowledge Management CIKM, pp. 971974, 2007.276 E. Spertus, Smokey Automatic recognition of hostile messages, in Proceedings of Innovative Applications of Artificial Intelligence IAAI, pp. 10581065, 1997.277 E. Stamatatos, N. Fakotakis, and G. Kokkinakis, Text genre detection usingcommon word frequencies, in Proceedings of the International Conference onComputational Linguistics COLING, 2000.278 S. S. Standifird, Reputation and ecommerce eBay auctions and the asymmetrical impact of positive and negative ratings, Journal of Management,vol. 27, pp. 279295, 2001.279 A. Stepinski and V. Mittal, A factopinion classifier for news articles,in Proceedings of the ACM Special Interest Group on Information RetrievalSIGIR, pp. 807808, New York, NY, USA ACM Press, 2007. ISBN 9781595935977.280 B. Stone and M. Richtel, The hand that controls the sock puppet could getslapped, The New York Times, July 16 2007.281 P. J. Stone, The General Inquirer A Computer Approach to Content Analysis.The MIT Press, 1966.282 V. Stoyanov and C. Cardie, Partially supervised coreference resolution foropinion summarization through structured rule learning, in Proceedings of theConference on Empirical Methods in Natural Language Processing EMNLP,pp. 336344, Sydney, Australia Association for Computational Linguistics,July 2006.132 References283 V. Stoyanov, C. Cardie, D. Litman, and J. Wiebe, Evaluating an opinion annotation scheme using a new multiperspective question and answercorpus, in Proceedings of the AAAI Spring Symposium on Exploring Attitudeand Affect in Text, AAAI Technical Report SS0407.284 V. Stoyanov, C. Cardie, and J. Wiebe, Multiperspective question answeringusing the OpQA corpus, in Proceedings of the Human Language Technology Conference and the Conference on Empirical Methods in Natural Language Processing HLTEMNLP, pp. 923930, Vancouver, British Columbia,Canada Association for Computational Linguistics, October 2005.285 P. Subasic and A. Huettner, Affect analysis of text using fuzzy semantictyping, IEEE Transactions on Fuzzy Systems, vol. 9, pp. 483496, 2001.286 M. Taboada, C. Anthony, and K. Voll, Methods for creating semantic orientation dictionaries, in Conference on Language Resources and EvaluationLREC, pp. 427432, 2006.287 M. Taboada, M. A. Gillies, and P. McFetridge, Sentiment classification techniques for tracking literary reputation, in LREC Workshop Towards Computational Models of Literary Analysis, pp. 3643, 2006.288 H. Takamura, T. Inui, and M. Okumura, Extracting semantic orientation ofwords using spin model, in Proceedings of the Association for ComputationalLinguistics ACL, pp. 133140, 2005.289 H. Takamura, T. Inui, and M. Okumura, Latent variable models for semantic orientations of phrases, in Proceedings of the European Chapter of theAssociation for Computational Linguistics EACL, 2006.290 H. Takamura, T. Inui, and M. Okumura, Extracting semantic orientationsof phrases from dictionary, in Proceedings of the Joint Human LanguageTechnologyNorth American Chapter of the ACL Conference HLTNAACL,2007.291 K. Tateishi, Y. Ishiguro, and T. Fukushima, Opinion information retrievalfrom the internet, Information Processing Society of Japan IPSJ SIGNotes, 2001, vol. 69, no. 7, pp. 7582, 2001. Also cited as A reputationsearch engine that gathers peoples opinions from the Internet, IPSJ Technical Report NL14411. In Japanese.292 J. Tatemura, Virtual reviewers for collaborative exploration of moviereviews, in Proceedings of Intelligent User Interfaces IUI, pp. 272275,2000.293 L. Terveen, W. Hill, B. Amento, D. McDonald, and J. Creter, PHOAKSA system for sharing recommendations, Communications of the Associationfor Computing Machinery CACM, vol. 40, pp. 5962, 1997.294 M. Thomas, B. Pang, and L. Lee, Get out the vote Determining support oropposition from congressional floordebate transcripts, in Proceedings of theConference on Empirical Methods in Natural Language Processing EMNLP,pp. 327335, 2006.295 R. Tokuhisa and R. Terashima, Relationship between utterances and enthusiasm in nontaskoriented conversational dialogue, in Proceedings of theSIGdial Workshop on Discourse and Dialogue, pp. 161167, Sydney, AustraliaAssociation for Computational Linguistics, July 2006.References 133296 R. M. Tong, An operational system for detecting and tracking opinions inonline discussion, in Proceedings of the Workshop on Operational Text Classification OTC, 2001.297 R. Tumarkin and R. F. Whitelaw, News or noise Internet postings andstock prices, Financial Analysts Journal, vol. 57, pp. 4151, MayJune2001.298 P. Turney, Thumbs up or thumbs down Semantic orientation applied tounsupervised classification of reviews, in Proceedings of the Association forComputational Linguistics ACL, pp. 417424, 2002.299 P. D. Turney and M. L. Littman, Measuring praise and criticism Inferenceof semantic orientation from association, ACM Transactions on InformationSystems TOIS, vol. 21, pp. 315346, 2003.300 S. Wan and K. McKeown, Generating overview summaries of ongoing emailthread discussions, in Proceedings of the International Conference on Computational Linguistics COLING, pp. 549555, Geneva, Switzerland, 2004.301 M. White, C. Cardie, and V. Ng, Detecting discrepancies in numericestimates using multidocument hypertext summaries, in Proceedings of theConference on Human Language Technology, pp. 336341, 2002.302 M. White, C. Cardie, V. Ng, K. Wagstaff, and D. McCullough, Detecting discrepancies and improving intelligibility Two preliminary evaluations of RIPTIDES, in Proceedings of the Document Understanding Conference DUC,2001.303 C. Whitelaw, N. Garg, and S. Argamon, Using appraisal groups for sentimentanalysis, in Proceedings of the ACM SIGIR Conference on Information andKnowledge Management CIKM, pp. 625631, ACM, 2005.304 J. Wiebe, Learning subjective adjectives from corpora, in Proceedings ofAAAI, 2000.305 J. Wiebe, E. Breck, C. Buckley, C. Cardie, P. Davis, B. Fraser, D. Litman,D. Pierce, E. Riloff, T. Wilson, D. Day, and M. Maybury, Recognizing andorganizing opinions expressed in the world press, in Proceedings of the AAAISpring Symposium on New Directions in Question Answering, 2003.306 J. Wiebe and R. Bruce, Probabilistic classifiers for tracking point of view,in Proceedings of the AAAI Spring Symposium on Empirical Methods in Discourse Interpretation and Generation, pp. 181187, 1995.307 J. Wiebe and R. Mihalcea, Word sense and subjectivity, in Proceedings ofthe Conference on Computational Linguistics  Association for ComputationalLinguistics COLINGACL, 2006.308 J. Wiebe and T. Wilson, Learning to disambiguate potentially subjectiveexpressions, in Proceedings of the Conference on Natural Language LearningCoNLL, pp. 112118, 2002.309 J. Wiebe, T. Wilson, and C. Cardie, Annotating expressions of opinions andemotions in language, Language Resources and Evaluation formerly Computers and the Humanities, vol. 39, pp. 164210, 2005.310 J. M. Wiebe, Identifying subjective characters in narrative, in Proceedingsof the International Conference on Computational Linguistics COLING,pp. 401408, 1990.134 References311 J. M. Wiebe, Tracking point of view in narrative, Computational Linguistics,vol. 20, pp. 233287, 1994.312 J. M. Wiebe, R. F. Bruce, and T. P. OHara, Development and use of agold standard data set for subjectivity classifications, in Proceedings of theAssociation for Computational Linguistics ACL, pp. 246253, 1999.313 J. M. Wiebe and W. J. Rapaport, A computational theory of perspective andreference in narrative, in Proceedings of the Association for ComputationalLinguistics ACL, pp. 131138, 1988.314 J. M. Wiebe and E. Riloff, Creating subjective and objective sentenceclassifiers from unannotated texts, in Proceedings of the Conference onComputational Linguistics and Intelligent Text Processing CICLing, number3406 in Lecture Notes in Computer Science, pp. 486497, 2005.315 J. M. Wiebe, T. Wilson, and M. Bell, Identifying collocations for recognizing opinions, in Proceedings of the ACLEACL Workshop on CollocationComputational Extraction, Analysis, and Exploitation, 2001.316 J. M. Wiebe, T. Wilson, R. Bruce, M. Bell, and M. Martin, Learning subjective language, Computational Linguistics, vol. 30, pp. 277308, September2004.317 Y. Wilks and J. Bien, Beliefs, points of view and multiple environments,in Proceedings of the international NATO symposium on artificial and humanintelligence, pp. 147171, USA, New York, NY Elsevier NorthHolland, Inc.,1984.318 Y. Wilks and M. Stevenson, The grammar of sense Using partofspeechtags as a first step in semantic disambiguation, Journal of Natural LanguageEngineering, vol. 4, pp. 135144, 1998.319 T. Wilson, J. Wiebe, and P. Hoffmann, Recognizing contextual polarity inphraselevel sentiment analysis, in Proceedings of the Human Language Technology Conference and the Conference on Empirical Methods in Natural Language Processing HLTEMNLP, pp. 347354, 2005.320 T. Wilson, J. Wiebe, and R. Hwa, Just how mad are you Finding strong andweak opinion clauses, in Proceedings of AAAI, pp. 761769, 2004. Extendedversion in Computational Intelligence, vol. 22, no. 2, pp. 7399, 2006.321 H. Yang, L. Si, and J. Callan, Knowledge transfer and opinion detection inthe TREC2006 blog track, in Proceedings of TREC, 2006.322 K. Yang, N. Yu, A. Valerio, and H. Zhang, WIDIT in TREC2006 blog track,in Proceedings of TREC, 2006.323 J. Yi, T. Nasukawa, R. Bunescu, and W. Niblack, Sentiment analyzerExtracting sentiments about a given topic using natural language processingtechniques, in Proceedings of the IEEE International Conference on DataMining ICDM, 2003.324 J. Yi and W. Niblack, Sentiment mining in WebFountain, in Proceedings ofthe International Conference on Data Engineering ICDE, 2005.325 P.L. Yin, Information dispersion and auction prices, Social ScienceResearch Network SSRN Working Paper Series, Version dated March 2005.326 H. Yu and V. Hatzivassiloglou, Towards answering opinion questions Separating facts from opinions and identifying the polarity of opinion sentences,References 135in Proceedings of the Conference on Empirical Methods in Natural LanguageProcessing EMNLP, 2003.327 J. Zabin and A. Jefferies, Social media monitoring and analysis Generating consumer insights from online conversation, Aberdeen Group BenchmarkReport, January 2008.328 Z. Zhang and B. Varadarajan, Utility scoring of product reviews, in Proceedings of the ACM SIGIR Conference on Information and Knowledge Management CIKM, pp. 5157, 2006.329 L. Zhou, J. K. Burgeon, and D. P. Twitchell, A longitudinal analysis oflanguage behavior of deception in email, in Proceedings of Intelligence andSecurity Informatics ISI, number 2665 in Lecture Notes in Computer Science, p. 959, 2008.330 L. Zhou and E. Hovy, On the summarization of dynamically introduced information Online discussions and blogs, in AAAI Symposium on ComputationalApproaches to Analysing Weblogs AAAICAAW, pp. 237242, 2006.331 F. Zhu and X. Zhang, The influence of online consumer reviews on thedemand for experience goods The case of video games, in International Conference on Information Systems ICIS, 2006.332 L. Zhuang, F. Jing, X.Y. Zhu, and L. Zhang, Movie review mining andsummarization, in Proceedings of the ACM SIGIR Conference on Informationand Knowledge Management CIKM, 2006.
