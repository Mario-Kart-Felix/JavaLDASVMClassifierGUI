Inverted Files for Text Search EnginesJUSTIN ZOBELRMIT University, AustraliaANDALISTAIR MOFFATThe University of Melbourne, AustraliaThe technology underlying text search engines has advanced dramatically in the past decade. The development of a family of new index representations has led to a wide range of innovations in index storage, indexconstruction, and query evaluation. While some of these developments have been consolidated in textbooks,many specific techniques are not widely known or the textbook descriptions are out of date. In this tutorial,we introduce the key techniques in the area, describing both a core implementation and how the core canbe enhanced through a range of extensions. We conclude with a comprehensive bibliography of text indexingliterature.Categories and Subject Descriptors H.3.1 Information Storage and Retrieval Content Analysis andIndexingIndexing methods H.3.2 Information Storage and Retrieval Information StorageFileorzanization H.3.3 Information Storage and Retrieval Information Search and RetrievalQuery formulation, retrieval models, search process D.4.3 Operating Systems File Systems ManagementAccessmethods, file organization E.4 Coding and Information TheoryData compaction and compressionE.5 FilesOrganizationstructure F.2.2 Analysis of Algorithms and Problem Complexity Nonnumerical Algorithms and ProblemsSorting and searching I.7.3 index GenerationGeneral Terms Algorithms, Experimentation, PerformanceAdditional Key Words and Phrases Inverted file indexing, document database, text retrieval, informationretrieval, Web search engine1. INTRODUCTIONText search is a key technology. Search engines that index the Web provide a breadthand ease of access to information that was inconceivable only a decade ago. Text searchhas also grown in importance at the other end of the size spectrum. For example, thehelp services built into operating systems rely on efficient text search, and desktopsearch systems help users locate files on their personal computers.The Australian Research Council and the Victorian Partnership for Advanced Computing have providedvaluable financial support as have our two home institutions.Authors addresses J. Zobel, School of Computer Science  Information Technology, RMIT University, Australia email jzcs.rmit.edu.au A. Moffat, Department of Computer Science  Software Engineering, TheUniversity of Melbourne, Australia.Permission to make digital or hard copies of part or all of this work for personal or classroom use is grantedwithout fee provided that copies are not made or distributed for profit or direct commercial advantage andthat copies show this notice on the first page or initial screen of a display along with the full citation. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit ispermitted. To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any componentof this work in other works requires prior specific permission andor a fee. Permissions may be requestedfrom Publications Dept., ACM, Inc., 1515 Broadway, New York, NY 10036 USA, fax 1 212 8690481, orpermissionsacm.org.c2006 ACM 03600300200607ART6 5.00 DOI httpdoi.acm.org10.1145ACM Computing Surveys, Vol. 38, No. 2, Article 6, Publication date July 2006.10.1145113295611329591132956.11329592 J. Zobel and A. MoffatSearch engines are structurally similar to database systems. Documents are storedin a repository, and an index is maintained. Queries are evaluated by processing theindex to identify matches which are then returned to the user. However, there are alsomany differences. Database systems must contend with arbitrarily complex queries,whereas the vast majority of queries to search engines are lists of terms and phrases.In a database system, a match is a record that meets a specified logical condition in asearch engine, a match is a document that is appropriate to the query according to statistical heuristics and may not even contain all of the query terms. Database systemsreturn all matching records search engines return a fixed number of matches, which areranked by their statistical similarity. Database systems assign a unique access key toeach record and allow searching on that key for querying on a Web collection, there maybe many millions of documents with nonzero similarity to a query. Thus, while searchengines do not have the costs associated with operations such as relational join, thereare significant obstacles to fast response, that is, a query term may occur in a large number of the documents, and each document typically contains a large number of terms.The challenges presented by text search have led to the development of a wide rangeof algorithms and data structures. These include representations for text indexes, indexconstruction techniques, and algorithms for evaluation of text queries. Indexes basedon these techniques are crucial to the rapid response provided by the major Web searchengines. Through the use of compression and careful organization, the space neededfor indexes and the time and disk traffic required during query evaluation are reducedto a small fraction of previous requirements.In this tutorial, we explain how to implement highperformance text indexing. Ratherthan explore all the alternative approaches that have been described some speculative, some ineffective, and some proven in practice, we describe a simple, effectivesolution that has been shown to work well in a range of contexts. This indexing coreincludes algorithms for construction of a documentlevel index and for basic rankedquery evaluation.Given the indexing core, we then sketch some of the principal refinements that havebeen devised. These include index reorganization, phrase querying, distribution, indexmaintenance, and, in particular, index compression. The most compelling applicationof the indexing techniques described in this tutorial is their use in Web search enginesand so we briefly review search in the context of the Web. In the context of the differentapproaches, we identify significant papers and indicate their contribution.Other research topics in text search include innovations such as metasearch, compression techniques for stored documents, and improvements to fundamental technologies such as sorting, storing, and searching of sets of strings. Domains of applicabilitybeyond text include genomic string searching, pattern matching, and proprietary document management systems. These developments are of wide practical importance butare beyond the scope of this tutorial.This article has a slightly unusual structure. The citations are collected into Section 13 which is a critical survey of work in the area. The earlier sections are free ofcitations, and reflect our desire to present the material as a tutorial that might be useful for people new to the area. To allow the corresponding citations to be accessed, thestructure within Section 13 of the previous sections. That is, Section 13 can be used asa further reading overview for each topic.2. TEXT SEARCH AND INFORMATION RETRIEVALSearch engines are tools for finding the documents in a collection that are good matchesto user queries. Typical kinds of document collection include Web pages, newspaper articles, academic publications, company reports, research grant applications, manualACM Computing Surveys, Vol. 38, No. 2, Article 6, Publication date July 2006.Inverted Files for Text Search Engines 3pages, encyclopedias, parliamentary proceedings, bibliographies, historical records,electronic mail, and court transcripts.These collections range dramatically in size. The plain text of a complete set of paperswritten by a researcher over ten years might occupy 10 megabytes, and the same researchers plain text, nonspam 10year email archive might occupy 100 megabytes. Athousand times bigger, the text of all the books held in a small university library mightoccupy around 100 gigabytes. In 2005, the complete text of the Web was probably someseveral tens of terabytes.Collections also vary in the way they change over time. A newswire archive or digitallibrary might grow only slowly, perhaps by a few thousand documents a day deletionsare rare. Web collections, in contrast, can be highly dynamic. Fortunately, many of thesame search and storage techniques are useful for these collections.Text is not the only kind of content that is stored in document collections. Researchpapers and newspaper articles include images, email includes attachments, and Webcollections include audio and video formats. The sizes discussed previously are for textonly the indexing of media other than text is beyond the scope of this tutorial.Query Modes. In traditional databases, the primary method of searching is by key orrecord identifier. Such searching is rare in text databases. Text in some kinds of collections does have structured attributes such as author tags and metadata such asthe subject labels used for categorizing books in libraries, but these are only occasionally useful for contentbased search and are not as useful as are keys in a relationaldatabase.The dominant mode of text search is by its content in order to satisfy an informationneed. People search in a wide variety of ways. Perhaps the commonest mode of searchingis to issue an initial query, scan a list of suggested answers, and follow pointers tospecific documents. If this approach does not lead to discovery of useful documents, theuser refines or modifies the query and may use advanced querying features such asrestricting the search domain or forcing inclusion or omission of specific query terms.In this model of searching, an information need is represented by a query, and the usermay issue several queries in pursuit of one information need. Users expect to be ableto match documents according to any of the terms they contain.Both casual users and professionals make extensive use of search engines but theirtypical strategies differ. Casual users generally examine only the first page or so returned by their favorite search engine, while professionals may use a range of searchstrategies and tools and are often prepared to scrutinize hundreds of potential answers.However, the same kinds of retrieval technique works for both types of searcher.Another contrast with traditional databases is the notion of matching. A recordmatches an SQL query if the record satisfies a logical condition. A document matchesan information need if the user perceives it to be relevant. But relevance is inexact, anda document may be relevant to an information need even though it contains none ofthe query terms or irrelevant even though it contains them all. Users are aware thatonly some of the matches returned by the system will be relevant and that differentsystems can return different matches for the same query. This inexactitude introducesthe notion of effectiveness informally, a system is effective if a good proportion of thefirst r matches returned are relevant. Also, different search mechanisms have differentcomputational requirements and so measurement of system performance must thusconsider both effectiveness and efficiency.There are many ways in which effectiveness can be quantified. Two commonly usedmeasures are precision and recall, respectively the fraction of the retrieved documentsthat are relevant and the fraction of relevant documents that are retrieved. There aremany other metrics in use with differing virtues and differing areas of application. InACM Computing Surveys, Vol. 38, No. 2, Article 6, Publication date July 2006.4 J. Zobel and A. MoffatFig. 1. The Keeperdatabase. It consists of six oneline documents.this tutorial, our focus is on describing indexing techniques that are efficient, and wedo not review the topic of effectiveness. Worth noting, however, is that the researchthat led to these efficient indexing techniques included demonstrations that they donot compromise effectiveness.With typical search engines, the great majority of information needs are presentedas bagofword queries. Many bagofword queries are in fact phrases such as propernames. Some queries have phrases marked up explicitly, in quotes. Another commonapproach is to use Boolean operators such as AND, perhaps to restrict answers to aspecific language, or to require that all query terms must be present in an answer.Example Collections. A sample collection, used as an example through this tutorial, isshown in Figure 1. In this Keeper database, only document 2 is about a big old house.But with a simple matching algorithm, the bagofwords query big old house matchesdocuments 2 and 3, and perhaps also documents 1 and 4 which contain old, but not theother terms. The phrase query big old house would match only document 2.Another issue is the parsing method used to extract terms from text. For example,when HTML documents are being indexed, should the markup tags be indexed orterms within tags And should hyphenated terms be considered as one word or twoAn even more elementary issue is whether to stem and casefold, that is, remove variantendings from words, and convert to lowercase. Choice of parsing technique has littleimpact, however, on indexing. On the other hand, the issue of stopping does affectthe cost of indexing and removal of common words or function words such as the andfurthermore can have a significant effect. Confusingly, in some information retrievalliterature, the task of parsing is known as index term extraction or simply indexing. Inthis article, indexing is the task of constructing an index.Without stemming, but with casefolding, the vocabulary of Keeper isand big dark did gown had house in keep keeper keeps lightnever night old sleep sleeps the town whereStemming might reduce the vocabulary toand big dark did gown had house in keep light never night oldsleep the town wherewith the exact result dependent on the stemming method used. Stopping might thenreduce the Keeper vocabulary tobig dark gown house keep light night old sleep townIn addition to the example Keeper collection, two hypothetical collections are used toillustrate efficiency issues. The characteristics of these collections are shown in Table I,and, while they are not actual data sets, they are based on experience with real text.Specifically, they are similar to two types of collections provided by the TREC projectrun by the United States National Institute for Standards and Technology NIST seetrec.nist.gov. TREC has been a catalyst for research in information retrieval since1992, and without it, robust measurement of the techniques described in this tutorialACM Computing Surveys, Vol. 38, No. 2, Article 6, Publication date July 2006.Inverted Files for Text Search Engines 5Table I. Characteristics of Two Hypothetical Text Databases, Used asExamples in This TutorialNewsWire WebSize gigabytes 1 100Documents 400,000 12,000,000Word occurrences without markup 180,000,000 11,000,000,000Distinct words after stemming . . . , 400,000 16,000,000per document, totaled 70,000,000 3,500,000,000would have been difficult or impossible. The last line in the table is the number ofdistinct worddocument pairs, that is, the number of word occurrences when duplicateswithin a document are not counted.Most of the costs required for additional structures scale more or less linearly for collections larger than a gigabyte. For example, in Web data, new distinct words continueto occur at a typical rates of about one per 5001000 word occurrences. Typical queryterms occur in 0.11 of the indexed documents.Taking all of these factors into account, implementors of search engines must designtheir systems to balance a range of technical requirementseffective resolution of queriesuse of features of conventional text, such as query term proximity, that improveeffectivenessuse of features of hyperlinked text, such as anchor strings and URL terms, thatimprove effectivenessfast resolution of queriesminimal use of other resources disk, memory, bandwidthscaling to large volumes of datachange in the set of documents andprovision of advanced features such as Boolean restriction and phrase querying.This tutorial describes techniques for supporting all of these requirements.Similarity Measures. All current search engines use ranking to identify potential answers. In a ranked query, a statistical similarity heuristic or similarity measure is usedto assess the closeness of each document to the textual query. The underlying principleis that the higher the similarity score awarded to a document, the greater the estimatedlikelihood that a human would judge it to be relevant. The r most similar according tothe heuristic documents are returned to the user as suggested answers. To describe theimplementation of text retrieval, we first consider evaluation of bagofwords queriesin which similarity is determined by simple statistics. In Section 4, we extend thesetechniques to phrase queries.Most similarity measures use some composition of a small number of fundamentalstatistical values fd,t , the frequency of term t in document d  fq,t , the frequency of term t in the query ft, the number of documents containing one or more occurrences of term tFt, the number of occurrences of term t in the collectionN, the number of documents in the collection andn, the number of indexed terms in the collection.These basic values are combined in a way that results in three monotonicity observations being enforced.ACM Computing Surveys, Vol. 38, No. 2, Article 6, Publication date July 2006.6 J. Zobel and A. Moffat1 Less weight is given to terms that appear in many documents2 More weight is given to terms that appear many times in a document and3 Less weight is given to documents that contain many terms.The intention is to bias the score towards relevant documents by favoring terms that appear to be discriminatory and reducing the impact of terms that appear to be randomlydistributed.A typical older formulation that is effective in practice calculates the cosine of theangle in ndimensional space between a query vector wq,t and a document vectorwd,t. There are many variations of the cosine formulation. An example iswq,t  ln1  Nftwd,t  1  ln fd,tWd tw2d,t Wq t w2q,tSq,d t wd,t  wq,tWd  Wq .1The term Wq can be neglected as it is a constant for a given query and does not affectthe ordering of documents. Variations on this theme are to avoid use of logarithms, orreplace N by maxt ft in the expression for wq,t, or multiply the queryterm weightswq,t by 1  ln fq,t when queries are long, or use a different way of combining wq,t andwd,t, or take Wd to be the length of the document in words or in bytes, and so on.What all of these variants share is that the quantity wq,t typically captures the property often described as the inverse document frequency of theterm, or IDF, while wd,t captures the term frequency, or TF, hence the commondescription of similarity measures as TFIDF formulations. Observing that the negative log of a probability is the information content, the score assigned to a documentcan very loosely be interpreted from an entropybased perspective as being a sum ofinformation conveyed by the query terms in that document, which maps to the sameordering as the product of their respective probabilities.Similarity formulations that are directly grounded in statistical principles have alsoproven successful in TREC. The best known of these is the Okapi computation,wq,t  lnN  ft  0.5ft  0.5 k3  1  fq,tk3  fq,twd,t  k1  1 fd,tKd  fd,tKd  k11  b  bWdWA2Sq,d tqwq,t  wd,t,in which the values k1 and b are parameters, set to 1.2 and 0.75 respectively k3 is aparameter that is set to , so that the expression k3 1  fq,tk3  fq,tis assumed tobe equivalent to fq,t and Wd and WA are the document length and average documentlength, in any suitable units.ACM Computing Surveys, Vol. 38, No. 2, Article 6, Publication date July 2006.Inverted Files for Text Search Engines 7Fig. 2. Exhaustive computation of cosine similarity between a query q andevery document in a text collection. This approach is suitable only when thecollection is small or is highly dynamic relative to the query rate.More recent probabilistic approaches are based on language models. There are manyvariants a straightforward languagemodel formulation iswd,t  log d d    fd,td  d    FtC3Sq,d tqfq,t  wd,t,where d  respectively, C is the number of term occurrences in document d respectively, collection C and  is a parameter, typically set to 2500. The lefthand side ofthe sum is the observed likelihood of the term in the document, while the righthandside modifies this using Dirichlet smoothing by combining it with the observed likelihood of the term in the collection. Taking the smoothed value as an estimate of theprobability of the term in the document, this formulation is rank equivalent to orderingdocuments by the extent to which the querys entropy in the documents model differsfrom the querys entropy in the collection as a whole.In this languagemodel formulation, the value wd,t is nonzero even if t is not in d ,presenting difficulties for the termordered query evaluation strategies we explain later.However, this problem can be addressed by transforming it into a rankequivalentmeasureSq,drank q  logd   tqdfq,t  logfd,t CFt 1, 4where the termoriented component log1  fd,t  CFt is zero when t is not in d .In all of these formulations, documents can score highly even if some of the queryterms are missing. This is a common attribute of similarity heuristics. We do not exploresimilarity formulations in detail and take as our brief the need to implement a systemin which any such computation can be efficiently computed.Given a formulation, ranking a query against a collection of documents is in principlestraightforward each document is fetched in turn, and the similarity between it andthe query calculated. The documents with the highest similarities can then be returnedto the user. An algorithm for exhaustive ranking using the cosine measure is shown inFigure 2.The drawback of this approach is that every document is explicitly considered, butfor typical situations in which r  N , only a tiny fraction of documents are returned as answers. For most documents, the vast majority of similarity values areACM Computing Surveys, Vol. 38, No. 2, Article 6, Publication date July 2006.8 J. Zobel and A. Moffatinsignificant. The exhaustive approach does, however, illustrate the main features ofcomputing a ranking, and, with a simple reorganization, a more efficient computationcan be achieved based on the key observation that, to have a nonzero score, a documentmust contain at least one query term.3. INDEXING AND QUERY EVALUATIONFast query evaluation makes use of an index a data structure that maps terms to thedocuments that contain them. For example, the index of a book maps a set of selectedterms to page numbers. With an index, query processing can be restricted to documentsthat contain at least one of the query terms.Many different types of index have been described. The most efficient index structurefor text query evaluation is the inverted file a collection of lists, one per term, recordingthe identifiers of the documents containing that term. Other structures are brieflyconsidered in Section 11, but they are not useful for generalpurpose querying.Baseline Inverted File. An inverted file index consists of two major components. Thesearch structure or vocabulary stores for each distinct word t,a count ft of the documents containing t, anda pointer to the start of the corresponding inverted list.Studies of retrieval effectiveness show that all terms should be indexed, even numbers.In particular, experience with Web collections shows that any visible component ofa page might reasonably be used as a query term, including elements such as thetokens in the URL. Even stopwordswhich are of questionable value for bagofwordsquerieshave an important role in phrase queries.The second component of the index is a set of inverted lists where each list stores forthe corresponding word t,the identifiers d of documents containing t, represented as ordinal document numbers andthe associated set of frequencies fd ,t of terms t in document d .The lists are represented as sequences of d, fd,t pairs. As described, this is a documentlevel index in that word positions within documents are not recorded. Together with anarray of Wd values stored separately, these components provide all the informationrequired for both Boolean and ranked query evaluation. A complete inverted file for theKeeper database is shown in Figure 3.In a complete text database system, there are several other structures, includingthe documents themselves and a table that maps ordinal document numbers to disklocations or other forms of document locator such as a filename or other key. We donot explore these structures in this tutorial.In a simple representation, for the NewsWire data, the total index size would beapproximately 435MB megabytes or around 40 of the size of the original data. Thisis comprised of 12MB for 400,000 words, pointers, and counts 1.6MB for 400,000 Wdvalues 280MB for 70,000,000 document identifiers four bytes each and 140MB for70,000,000 document frequencies two bytes each. For the Web data, the total size isabout 21GB gigabytes or just over 20 of the original text. The difference betweenthese two collections is a consequence of Web pages tending to contain large volumesof unindexed markup.An assumption made in these calculations is that each inverted list is stored contiguously. The alternative is that lists are composed of a sequence of blocks that are linkedor indexed in some way. The assumption of contiguity has a range of implications. First,ACM Computing Surveys, Vol. 38, No. 2, Article 6, Publication date July 2006.Inverted Files for Text Search Engines 9Fig. 3. Complete documentlevel inverted file forthe Keeper database. The entry for each term t iscomposed of the frequency ft and a list of pairs, eachconsisting of a document identifier d and a documentfrequency fd,t. Also shown are the Wd values as computed for the cosine measure shown in Equation 1.it means that a list can be read or written in a single operation. Accessing a sequenceof blocks scattered across a disk would impose significant costs on query evaluationas the list for a typical query term on the Web data would occupy 100kB kilobytes to1MB, and the inverted list for a common term could be many times this size. Addingto the difficulties for the great majority of terms, the inverted list is much less thana kilobyte, placing a severe constraint on feasible size for a fixedsize block. Second,no additional space is required for nextblock pointers. Third, index update proceduresmust manage variablelength fragments that vary enormously in size, from tiny to vastour experience, however, is that the benefits of contiguity greatly outweigh these costs.An issue that is considered in detail in Section 8 is how to represent each stored valuesuch as document numbers and indocument frequencies. The choice of any fixed number of bits or bytes to represent a value is clearly arbitrary and has potential implications for scaling fixedlength values can overflow and efficiency inflation in the volumeof data to be managed. Using the methods described later in this article, large gains inperformance are available through the use of compressed representations of indexes.To facilitate compression, dgaps are stored rather than straight document identifiers. For example, the sorted sequence of document numbers7, 18, 19, 22, 23, 25, 63, . . .can be represented by gaps7, 11, 1, 3, 1, 2, 38, . . . .ACM Computing Surveys, Vol. 38, No. 2, Article 6, Publication date July 2006.10 J. Zobel and A. MoffatFig. 4. Indexed computation of cosine similarity between a query q and a textcollection.Fig. 5. Using an inverted file and a set of accumulators to calculate document similarity scores.While this transformation does not reduce the maximum magnitude of the stored numbers, it does reduce the average, providing leverage for the compression techniquesdiscussed later. Section 8 gives details of mechanisms that can exploit the advantagethat is created by gaps.Baseline Query Evaluation. Ranking using an inverted file is described in Figure 4and illustrated in Figure 5. In this algorithm, the query terms are processed one at atime. Initially each document has a similarity of zero to the query this is representedby creating an array A of N partial similarity scores referred to as accumulators, onefor each document d . Then, for each term t, the accumulator Ad for each document dmentioned in t s inverted list is increased by the contribution of t to the similarity of dACM Computing Surveys, Vol. 38, No. 2, Article 6, Publication date July 2006.Inverted Files for Text Search Engines 11to the query. Once all query terms have been processed, similarity scores Sd are calculated by dividing each accumulator value by the corresponding value of Wd . Finally, ther largest similarities are identified, and the corresponding documents returned to theuser.The cost of ranking via an index is far less than with the exhaustive algorithmoutlined in Figure 2. Given a query of three terms, processing a query against the Webdata involves finding the three terms in the vocabulary fetching and then processingthree inverted lists of perhaps 100kB to 1MB each and making two linear passes overan array of 12,000,000 accumulators. The complete sequence requires well under asecond on current desktop machines.Nonetheless, the costs are still significant. Disk space is required for the index at2060 of the size of the data for an index of the type shown in Figure 3 memory isrequired for an accumulator for each document and for some or all of the vocabularyCPU time is required for processing inverted lists and accumulators and disk trafficis used to fetch inverted lists. Fortunately, compared to the implementation shown inFigure 4, all of these costs can be dramatically reduced.Indexing Word Positions. We have described inverted lists as sequences of index entries,each a d, fd,t pair. An index of this form is documentlevel since it indicates whether aterm occurs in a document but does not contain information about precisely where theterm appears. Given that the frequency fd,t represents the number of occurrences of tin d , it is straightforward to modify each entry to include the fd,t ordinal word positionsp at which t occurs in d and create a wordlevel inverted list containing pointers of theform d , fd ,t , p1, . . . , pfd ,t . Note that in this representation positions are word counts,not byte counts, so that they can be used to determine adjacency.Word positions can be used in a variety of ways during query evaluation. Section 4discusses one of these, phrase queries in which the user can request documents witha sequence rather than bagofwords. Word positions can also be used in bagofwordqueries, for example, to prefer documents where the terms are close together or areclose to the beginning of the document. Similarity measures that make use of suchproximity mechanisms have not been particularly successful in experimental settingsbut, for simple queries, adjacency and proximity do appear to be of value in Webretrieval.If the source document has a hierarchical structure, that structure can be reflectedby a similar hierarchy in the inverted index. For example, a document with a structureof chapters, sections, and paragraphs might have word locations stored as c, s, p, wtuples coded as a sequence of nested runs of cgaps, sgaps, pgaps, and wgaps. Suchan index allows withinsameparagraph queries as well as phrase queries, for example,and with an appropriate representation, is only slightly more expensive to store thana nonhierarchical index.Core Ideas. To end this section, we state several key implementation decisions.Documents have ordinal identifiers, numbered from one.Inverted lists are stored contiguously.The vocabulary consists of every term occurring in the documents and is stored in asimple extensible structure such as a Btree.An inverted list consists of a sequence of pairs of document numbers and indocumentfrequencies, potentially augmented by word positions.The vocabulary may be preprocessed, by stemming and stopping.Ranking involves a set of accumulators and termbyterm processing of inverted lists.ACM Computing Surveys, Vol. 38, No. 2, Article 6, Publication date July 2006.12 J. Zobel and A. MoffatThis set of choices constitutes a core implementation in that it provides an approach thatis simple to implement and has been used in several publicdomain search systems and,we believe, many proprietary systems. In the following sections, we explore extensionsto the core implementation.4. PHRASE QUERYINGA small but significant fraction of the queries presented to Web search engines includean explicit phrase, such as philip glass opera or the great flydini. Users alsooften enter phrases without explicit quotes, issuing queries such as Albert Einstein orSan Francisco hotel. Intuitively it is appealing to give high scores to pages in whichterms appear in the same order and pattern as they appear in the query, and low scoresto pages in which the terms are separated.When phrases are used in Boolean queries, it is clear what is intendedthe phraseitself must exist in matching documents. For example, the Boolean query old nightkeeper would be evaluated as if it contains two query terms, one of which is a phrase,and both terms would be required for a document to match.In a ranked query, a phrase can be treated as an ordinary term, that is, a lexical entitythat occurs in given documents with given frequencies, and contributes to the similarityscore for that document when it does appear. Similarity can therefore be computed inthe usual way, but it is first necessary to use the inverted lists for the terms in thephrase to construct an inverted list for the phrase itself, using a Boolean intersectionalgorithm. A question for information retrieval research and outside the scope of thistutorial is whether this is the best way to use phrases in similarity estimation. A goodquestion for this tutorial is how to findin a strictly Boolean sensethe documents inwhich a given sequence of words occur together as a phrase since, regardless of howthey are eventually incorporated into a matching or ranking scheme, identification ofphrases is the first step.An obvious possibility is to use a parser at index construction time that recognizesphrases that might be queried and to index them as if they were ordinary documentterms. The set of identified phrases would be added to the vocabulary and have theirown inverted lists users would then be able to query them without any alteration toquery evaluation procedures. However, such indexing is potentially expensive. Thereis no obvious mechanism for accurately identifying which phrases might be used inqueries, and the number of candidate phrases is enormous since even the number ofdistinct twoword phrases grows far more rapidly than the number of distinct terms.The hypothetical Web collection shown in Table I could easily contain a billion distincttwoword pairs.Three main strategies for Boolean phrase query evaluation have been developed.Process phrase queries as Boolean bagsofwords so that the terms can occur anywhere in matching document, then postprocess the retrieved documents to eliminatefalse matches.Add word positions to some or all of the index entries so that the locations of termsin documents can be checked during query evaluation.Use some form of partial phrase index or wordpair index so that phrase appearancescan be directly identified.These three strategies can complement each other. However, a pure bagofwords approach is unlikely to be satisfactory since the cost of fetching just a few nonmatchingdocuments could exceed all other costs combined, and, for many phrases, only a tinyfraction of the documents that contain the query words also contain them as a phrase.ACM Computing Surveys, Vol. 38, No. 2, Article 6, Publication date July 2006.Inverted Files for Text Search Engines 13Phrase Query Evaluation. When only documentlevel querying is required, inclusion ofpositional information in the index not only takes space, but also slows query processingbecause of the need to skip over the positional information in each pointer. And, asdiscussed in more detail in the following, if bagofwords ranked queries are to besupported efficiently, then other index organizations, such as frequency and impactsorted arrangements, need to be considered.Taken together, these considerations suggest that maintenance of two separate indexes may be attractive, a wordlevel index for Boolean searching and phrase identification, and a documentlevel impact or frequencysorted index for processing rankedqueries. Given that documentlevel indexes are small, the space overhead of havingmultiple indexes is low. Separating the indexes also brings flexibility and allows consideration of index structures designed explicitly for phrases.Many phrases include common words. The cost of phrase query processing using awordlevel inverted index is then dominated by the cost of fetching and decoding listsfor those words which typically occur at the start of or in the middle of a phraseconsider the house in the town, for example. One way of avoiding this problemwould be to neglect certain stop words and index them at the documentlevel only. Forexample, to evaluate the query the house in the town processing could proceed byintersecting the lists for house, and town, looking for positions p of house such that townis at p  3. False matches could be eliminated by postprocessing, that is, by fetchingcandidate documents and examining them directly. The possibility of false matchescould also simply be ignored. However, in some phrase queries, the common words playan important semantic role and must be included.Phrase Indexes. It is also possible to build a complete index of twoword phrases usinga hierarchical storage structure to avoid an overly large vocabulary. Experiments showthat such an index occupies around 50 of the size of the source data, that is, perhaps50GB for the Web collection. The inverted lists for phrases are on average much shorterthan those of the individual words, but there are many more of them, and the vocabularyis also much bigger.Using a twoword phrase index, evaluation of the phrase query the house in thetown which matches line three of the Keeper collection involves processing the inverted lists for, say, the phrases the house, house in, and the town. The pairin the is also a phrase but is covered by the others andmaking an arbitrary choicebetween this phrase and house inits inverted list does not need to be processed.For phrases composed of rare words, having a phrase index yields little advantage,as processing savings are offset by the need to access a much larger vocabulary. Asuccessful strategy is to have an index for word pairs that begin with a common wordand combine it with a wordlevel inverted index. For example, the preceding query couldbe evaluated by intersecting the inverted lists for the house, in, and the town. Anindex of all word pairs beginning with any one of the three commonest words is about 1of the size of the Web data but allows phrase querying time to be approximately halved.5. INDEX CONSTRUCTIONThe single key problem that makes index construction challenging is that the volumeof data involved cannot be held in main memory in a dynamic data structure of thekind typically used for crossreference generation. The underlying task that needs to beperformed is essentially that of matrix transposition. But the documentsterms matrixis very sparse, and is far too large to be manipulated directly as an array. Instead, indexconstruction techniques make use of index compression methods and either distributiveor comparisonbased sorting techniques.ACM Computing Surveys, Vol. 38, No. 2, Article 6, Publication date July 2006.14 J. Zobel and A. MoffatFig. 6. Inmemory inversion.InMemory Inversion. A simple inmemory inversion algorithm is shown in Figure 6.The key idea is that a first pass through the documents collects term frequency information, sufficient for the inverted index to be laid out in memory in template form. Asecond pass then places pointers into their correct positions in the template, makinguse of the randomaccess capabilities of main memory. The advantage of this approachis that almost no memory is wasted compared to the final inverted file size since thereis negligible fragmentation. In addition, if compression is used, the index can be represented compactly throughout the process. This technique is viable whenever the mainmemory available is about 1020 greater than the combined size of the index andvocabulary that are to be produced. It is straightforward to extend the inmemory algorithm to include word positions, but the correspondingly larger final index will morequickly challenge the memory capacity of whatever hardware is being used since individual list entries may become many kilobytes long.It is also possible to extend the inmemory technique to data collections where indexsize exceeds memory size by laying out the index skeleton on disk, creating a sequenceof partial indexes in memory, and then transferring each in a skipsequential mannerto a template that has been laid out as a disk file. With this extended method, andmaking use of compression, indexes can be built for multigigabyte collections usingaround 1020MB of memory beyond the space required for a dynamic vocabulary.SortBased Inversion. A shortcoming of twopass techniques of the kind sketched inFigure 6 is that document parsing and fetching is a significant component of index construction costs, perhaps half to twothirds of the total time for Web data. The documentscould be stored parsed during index construction, but doing so implies substantial diskoverheads, and the need to write the parsed text may outweigh the cost of the secondparsing.Other index construction methods are based on explicit sorting. In a simple form ofthis approach, an array or file of t, d , fd ,t triples is created in document number order,sorted into term order, and then used to generate the final inverted file.With careful sequencing and use of a multiway merge, the sort can be carried out inplace on disk using compressed blocks. The disk space overhead is again about 10 ofthe final compressed index size, and memory requirements and speed are also similarto partitioned inversion. As for partitioned inversion, the complete vocabulary must bekept in memory, limiting the volume of data that can be indexed on a single machine.MergeBased Inversion. As the volumes of disk and data grow, the cost of keeping thecomplete vocabulary in memory is increasingly significant. Eventually, the index mustbe created as an amalgam of smaller parts, each of which is constructed using one of theACM Computing Surveys, Vol. 38, No. 2, Article 6, Publication date July 2006.Inverted Files for Text Search Engines 15Fig. 7. Mergebased inversion.Fig. 8. Mergebuild process.previous techniques or using purely inmemory structures. Figures 7 and 8 illustratethis process.In mergebased inversion, documents are read and indexed in memory until a fixedcapacity is reached. Each inverted list needs to be represented in a structure that cangrow as further information about the term is encountered, and dynamically resizablearrays are the best choice. When memory is full, the index including its vocabulary isflushed to disk as a single run with the inverted lists in the run stored in lexicographicorder to facilitate subsequent merging. As runs are never queried, the vocabulary of arun does not need to be stored as an explicit structure each term can, for example, bewritten at the head of its inverted list. Once the run is written, it is entirely deletedfrom memory so that construction of the next run begins with an initially emptyvocabulary.When all documents have been processed, the runs are merged to give the final index.The merging process builds the final vocabulary on the fly and, if a large read bufferis allocated to each run, is highly efficient in terms of disk accesses. If disk space isscarce, the final index can be written back into the space occupied by the runs as theyare processed as the final index is typically a little smaller than the runsvocabularyACM Computing Surveys, Vol. 38, No. 2, Article 6, Publication date July 2006.16 J. Zobel and A. Moffatinformation is not duplicated, and the final inverted lists can be represented moreefficiently.Mergebased index construction is practical for collections of all sizes. In particular,it scales well and operates effectively in as little as 100MB of memory. In addition, diskspace overheads can be restricted to a small fraction of the final index only one parsingpass is required over the data and the method extends naturally to phrase indexing.Finally, the compression techniques described in Section 8 can further reduce the costof index construction by reducing the number of runs required.6. INDEX MAINTENANCEInserting one document into a text databases involves, in principle, adding a few bytesto the end of every inverted list corresponding to a term in the document. For a documentof reasonable size, such an insertion involves fetching and slightly extending severalhundred inverted lists and is likely to require 1020 seconds on current hardware.In contrast, with mergebased inversion, the same hardware can index around 1,000documents per second. That is, there is a 10,000fold disparity in cost between thesetwo approaches.For fast insertion, it is necessary to avoid accessing the diskresident inverted lists ofeach term. The only practical solution is to amortize the update costs over a sequence ofinsertions. The properties of text databases, fortunately, allow several strategies for costamortization. In particular, for ranking, it is not always necessary for new documents tobe immediately available for searches. If they are to be searchable, new documents canbe made available through a temporary inmemory indexin effect, the last subindexin the merging strategy.Three broad categories of update strategies are available rebuild from scratch, mergean existing index with an index of new documents, and incremental update.Rebuild. In some applications, the index may not to be updated online at all. Instead,it can be periodically rebuilt from scratch. Consider, for example, a university Website. New documents are only discovered through crawling and immediate update isnot essential. For a gigabyte of data, rebuilding takes just a few minutes, a small costcompared to that of fetching the documents to index.Intermittent Merge. The inverted lists for even large numbers of documents can easilybe maintained in the memory of a standard desktop computer. If the lists are in memory,it is cheap to insert new documents as they arrive indeed, there is no difference betweenmaintaining such lists and the algorithm described in Figure 7.If existing documents are indexed in a standard inverted file and new documents areindexed in memory, the two indexes can share a common vocabulary, and all documentscan be made available to queries via a straightforward adaptation of the methods described earlier. Then when memory is full, or some other criterion is met, the inmemoryindex is merged with the ondisk index. The old index can be used until the merge iscomplete, at the cost of maintaining two complete copies of the inverted lists. During themerge either a new inmemory index must be created or insertions must temporarily beblocked, and thus, during the merge, new documents are only available via exhaustivesearch.It was argued earlier that inverted lists should be stored contiguously as accessing alarge number of blocks would be a dominant cost of query evaluation. However, if thenumber of blocks is constrained for instance, an average of three per term, the timeto evaluate queries can similarly be controlled. In addition, if the index is arranged asa sequence of subindexes, each one no greater than a given fraction of the size of thenext that is, the sizes form a geometric sequence, then only a small part of the indexis involved in most merge operations. This combination of techniques allow an index toACM Computing Surveys, Vol. 38, No. 2, Article 6, Publication date July 2006.Inverted Files for Text Search Engines 17be built incrementally, and be simultaneously available for querying, in just twice thetime required by an offline mergebased build.Incremental Update. A final alternative is to update the main index term by term, asand when opportunity arises, with some terms inmemory lists covering more documents than others. This process is similar to the mechanisms used for maintainingvariablelength records in a conventional database system. In such an asynchronousmerge, a list is fetched from disk, the new information is integrated into the list, andthe list is then written back. Using standard freespace management, the list can eitherbe written back in place or, if there is insufficient space, written to a new location.The perlist updates should be deferred for as long as possible to minimize the numberof times each list is accessed. The simplest approach is to process as for the mergingstrategy and, when a memory limit is reached, then proceed through the whole index,amending each list in turn. Other possibilities are to update a list only when it is fetchedin response to a query or to employ a background process that slowly cycles through theinmemory index, continuously updating entries. In practice, these methods are not asefficient as intermittent merge, which processes data on disk sequentially.Choosing an Update Strategy. For reasonable collection sizes, merging is the most efficient strategy for update but has the drawback of requiring significant disk overheads.It allows relatively simple recovery as reconstruction requires only a copy of the indexand the new documents. In contrast, incremental update proceeds in place with somespace lost due to fragmentation. But recovery in an incremental index may be complexdue to the need to track which inverted lists have been modified.For smaller, relatively static collections, the cost of rebuild may exceed that of othermethods, but still be of little consequence compared to the other costs of maintainingthe database. And if the collection is highly dynamic, such as a Web site in whichdocuments are edited as well as inserted, then inserting or deleting a single word in adocument may affect all the word positions, for example, and rebuild may be the onlyplausible option.7. DISTRIBUTED INFORMATION RETRIEVALWhen large volumes of data are involved or when high query volumes must be supported, one machine may be inadequate to support the load even when the variousenhancements surveyed earlier are incorporated. For example, in mid2004, the Googlesearch engine processed more than 200 million queries a day against more than 20TBof crawled data, using more than 20,000 computers.To handle the load, a combination of distribution and replication is required. Distribution refers to the fact that the document collection and its index are split acrossmultiple machines and that answers to the query as a whole must be synthesized fromthe various collection components. Replication or mirroring then involves makingenough identical copies of the system so that the required query load can be handled.DocumentDistributed Architectures. The simplest distribution regime is to partition thecollection and allocate one subcollection to each of the processors. A local index isbuilt for each subcollection when queries arrive, they are passed to every subcollection and evaluated against every local index. The sets of subcollection answers arethen combined in some way to provide an overall set of answers. The advantages ofsuch a document partitioned system are several collection growth is accommodatedby designating one of the hosts as the dynamic collection so that only it needs torebuild its index and the computationally expensive parts of the process are distributed equally across all of the hosts in the computer cluster. The dashed regionACM Computing Surveys, Vol. 38, No. 2, Article 6, Publication date July 2006.18 J. Zobel and A. MoffatFig. 9. Two ways in which the index of Figure 3 might be partitioned and distributed across a cluster ofmachines.in Figure 9 shows one component of a documentdistributed retrieval system withthis one processor indexing all terms that appear in the first two documents of thecollection.TermDistributed Architectures. An alternative strategy is term partitioning. In a termpartitioned index, the index is split into components by partitioning the vocabulary,with one possible partition shown by the dotted horizontal split in Figure 9. Eachprocessor has full information about a subset of the terms, meaning that to handle aquery, only the relevant subset of the processors need to respond. Term partitioninghas the advantage of requiring fewer disk seek and transfer operations during queryevaluation than documentpartitioning because each terms inverted list is still storedcontiguously on a single machine rather than in fragments across multiple machines.On the other hand, those disk transfer operations each involve more data. More significantly, in a termpartitioned arrangement, the majority of the processing load fallsto the coordinating machine, and experiments have shown that it can easily become abottleneck and starve the other processors of work.Choosing a Distribution Strategy. Document distribution typically results in a betterbalance of workload than does term partitioning and achieves superior query throughput. It also allows more naturally for index construction and for document insertion.On the other hand, index construction in a termpartitioned index involves first of alldistributing the documents and building a documentpartitioned index, and then exchanging index fragments between all pairs of processors after the vocabulary splithas been negotiated. Document distribution also has the pragmatic advantage of stillallowing a search service to be provided even when one of the hosts is offline for somereason since any answers not resident on that machine remain available to the system.ACM Computing Surveys, Vol. 38, No. 2, Article 6, Publication date July 2006.Inverted Files for Text Search Engines 19On the other hand, in a termdistributed system, having one machine offline is likelyto be immediately noticeable.The Google implementation uses a documentpartitioned index with massive replication and redundancy at all levels the machine, the processor cluster, and the site.It is also worth noting that document partitioning remains effective even if the collaborating systems are independent and unable to exchange their index data. A distributedsystem in which a final result answer list is synthesized from the possiblyoverlappinganswer sets provided by a range of different services is called a metasearcher.8. EFFICIENT INDEX REPRESENTATIONSA central idea underpinning several aspects of efficient indexing and query processingis the use of compression. With appropriate compression techniques, it is possible tosimultaneously reduce both space consumption and disk traffic. Compression also reduces overall query evaluation time. This section introduces a range of integer codingtechniques, and describes their use in inverted file compression.Compact Storage of Integers. In a simple implementation of an inverted file index, 32bitand 16bit integers might be used respectively for document identifiers and term frequencies since these sizes are universally supported by compilers and are large enoughto hold the likely values. However, fixedwidth integers are not particularly spaceefficient, and large savings can be obtained using quite simple compression techniques.An efficient representation for an inverted list requires a method for coding integersin a variable number of bits. Using a fixed number of bits for each number, whether 32or 20, is inconvenient it limits the maximum value that can be stored, and is wasteful ifmost numbers are small. Variablelength codes can be infinite, avoiding the problem ofhaving a fixed upper bound and can be constructed to favor small values at the expenseof large.Parameterless Codes. The simplest variablebit infinite code is unary which representsthe value x  0 as x  1 1 bits followed by a terminating 0 bit.Unary is an example of a fixed code corresponding to a fixed distribution and isunparameterized. Other unparameterized codes for integers are Elias gamma anddelta codes. In the gamma code, integer x  0 is factored into 2e d where e  log2 x,and 0  d  2e. The codeword is formed as the concatenation of e  1 represented inunary and d represented in binary in e bits. In the delta code, the value of e  1 isrepresented using the gamma code and is followed by d in binary, as for gamma.In both codes, each codeword has two parts, a prefix and a suffix. The prefix indicatesthe binary magnitude of the value and tells the decoder how many bits there are in thesuffix part. The suffix indicates the value of the number within the corresponding binaryrange. Table II gives some example codewords for each of these three unparameterizedcodes. The colons used in the codewords to separate the prefix and suffix parts arepurely illustrative.Which code is preferable depends on the probability distribution Prx governing thevalues x that are being coded. Using Shannons relationship between the probability ofa symbol and its ideal codeword length, lenx   log2 Prx, it can be seen that unarycorresponds to the probability distribution Prx  2x . That is, when half of all valuesare the number 1, a quarter are 2, an eighth are 3, and so on, then unary is the mostefficient coding mechanism. When approximately half of all values are the number 1, aquarter are equally one of 2 or 3, an eighth are equally one of 4, 5, 6, or 7, and so on,and in general Prx  12x2, then gamma is the most efficient coding mechanism.However, inspecting a sequence of bits one by one is relatively costly on machines forwhich the basic unit of access is multiples of eight bits. If each code occupies a sequenceACM Computing Surveys, Vol. 38, No. 2, Article 6, Publication date July 2006.20 J. Zobel and A. MoffatTable II. Example Codewords Using the Unary, Gamma, and Delta Codesvalue unary gamma delta1 0 0 02 10 100 10003 110 101 10014 1110 11000 1010010 1111111110 1110010 11000010100 1111110100100 110111001001,000 1111111110111101000 1110010111101000Colons are used as a guide to show the prefix and suffix components ineach codeword. All three codes can represent arbitrarily large numbersthe unary codewords for 100 and 1,000 are omitted only because of theirlength.Fig. 10. Encoding and decoding variablelengthbytealigned codes. Input values to the encoder mustsatisfy x  1.of whole bytes, the bit operations can be eliminated. With this in mind, another simpleunparameterized code is to use a sequence of bytes to code a value x  1, shown inFigure 10. The idea is very simple if x  128, then a single byte is used to representx  1 in binary, with a leading 0 bit otherwise, the loworder seven bits of x  1 arepacked into a byte with a leading 1 bit, and the quantity x div 128 is recursivelycoded the same way.Thus the byte 2, with the bitpattern 0000 0010, represents the integer 3 the byte9, with the bitpattern 0000 1001, represents the decimal integer 10 and the doublebyte 1477, with the bitpattern 1110 0111  0000 0111, represents 1044 since 1044 128  7  1  147  127. This approach is a form of gamma code in which the suffixlength is a multiple of seven instead of a multiple of one. Several studies have exploredbytewise coding and found it to be more efficient than bitwise alternatives. An enhancedversion in which the split between continuing and terminating bytes is signaled by adifferent value than 128 has also been described and offers the possibility of improvedcompression effectiveness.As well as offering economical decoding, the bytewise coding mechanism facilitatesfast stepping through a compressed stream looking for the kth subsequent code. Sinceeach codeword ends with a byte in which the top bit is a 0, it is easy to step through acompressed sequence and skip over exactly k coded integers without having to decodeeach of them in full.ACM Computing Surveys, Vol. 38, No. 2, Article 6, Publication date July 2006.Inverted Files for Text Search Engines 21Fig. 11. Encoding using a Golomb code with parameter b. Input values must satisfyx  1.Table III. Example Codewords Using Three Different Golomb Codesvalue b  3 b  5 b  161 00 000 000002 001 001 000013 011 010 000104 100 0110 0001010 11100 10110 01001Colons are used as a guide to show the prefix and suffix components ineach codeword. All three codes can represent arbitrarily large numbers.Golomb and Rice Codes. The Elias codes just described have the property that theinteger 1 is always encoded in one bit. However, dgaps of 1 are not particularly commonin inverted lists. More pertinently, within the context of a single inverted list withknown length, the likely size of the numbers can be accurately estimated. It is thereforedesirable to consider schemes that can be parameterized in terms of the average sizeof the values to be coded.Golomb codes, described in Figure 11, have this property. Integer x  1 is codedin two partsa unary bucket selector, and a binary offset within that bucket. Thedifference between this and the Elias codes is that, in Golomb codes, all buckets arethe same size. The use of a variablelength binary code for the remainder r means thatno bit combinations are wasted even when the parameter b is not an exact power oftwo. Table III gives some examples of the codewords generated for different values ofb. When b is a power of two, b  2k , the suffix part always contains exactly k bits.These are known as Rice codes and allow simpler encoding and decoding procedures.An example Rice code appears in the last column of Table III.Matching Code to Distribution. The implied probability distribution associated withthe Elias gamma code was described earlier Golomb codes are similarly minimumredundancy whenPrx  1  px1 p ,provided thatb log2  p log1  p 0.69  1p,where p is the parameter of the geometric distribution the probability of success ina sequence of independent trials. As we now explain, an inverted list can be represented as a sequence of integers that are, under a reasonable assumption, a sequenceof independent or Bernoulli trials.Binary Codes. Other coding mechanisms can also perform well and even binary codescan realize compact representations if the data is locally homogeneous. At the sametime, they can provide decoding speeds as good as bytewise codes. In the simplestACM Computing Surveys, Vol. 38, No. 2, Article 6, Publication date July 2006.22 J. Zobel and A. Moffatexample of this approach, a sequence of values is represented by fitting as many binarycodes as possible into the next 32bit output word. For example, if all of the next sevensequence values are smaller than 17, then a set of 4bit codewords can be packed intoa single output word. Similarly, if all of the next nine values are smaller than 9, anoutput word containing 3bit codewords can be constructed. To allow decoding, eachword of packed codes is prefixed by a short binary selector that indicates how that wordshould be interpreted.As with the bytewise coding method, this approach supports fast identification of thekth subsequent codeword since all that is required is that the selector of each word beexamined to know how many codewords it contains.Compressing Inverted Lists. The representation of inverted lists introduced earlier described each inverted list as a sequence of d, fd,t values with the restriction that eachdocument identifier d be an ordinal number. From a database perspective, imposingan ordinal identifier appears to be a poor design decision because it has implicationsfor maintenance. However, text databases are not manipulated and updated in theways that relational databases are, and, in practice, the use of a simple mapping tableobviates the difficulties.The frequencies fd,t are usually small with a typical median of 1 or 2 and can beefficiently represented using unary or gamma. But representing raw document identifiers using these codes gives no saving since the median value in each inverted listis N2.On the other hand, if document identifiers are sorted and firstorder differences d gaps are stored, significant savings are possible. If a term appears in a random subsetof ft of the N documents in the collection, the dgaps conform to a geometric distributionwith probability parameter p  ftN and can thus be coded effectively using a Golombcode or one of the other codes. For example, a Golombgammacoded index of all wordsand numbers occurring in the documents for the NewsWire collection would occupy about7 of the text size. In this approach, the representation for each inverted list consists ofalternating dgap values and fd,t values, each occupying a variable number of bits. Thedgap values are represented as Golomb codes, using a parameter b determined fromthe number of d, fd,t pairs in that inverted list, and the fd,t values are representedusing the gamma code.Use of Golomb codes does, however, present problems for the update strategy of intermittent merge. If Golomb codes are used for representing document identifiers, themerge process involves decoding the existing list and recoding with new parameters,but processing the existing list will then be the dominant cost of update, and shouldbe avoided. One solution is that the old Golomb parameter could continue to be usedto compress the added material at some small risk of gradual compression degradation. Alternatively, if static codes are used and lists are documentordered, new d, fd,tvalues can be directly appended to the end of the inverted lists. For other list orderings, such as those described in Section 9, there may be no alternative to completereconstruction.Compression Effectiveness. The effectiveness of compression regimes is particularly evident for inverted lists representing common words. To take an extreme case, the wordthe is likely to occur in almost every document in an Englishlanguage collection, andthe vast majority of dgaps in its inverted list will be 1 and represented in just a singlebit. Allowing for the corresponding fd,t value to be stored in perhaps at most 1011bits, around 12 bits is required per d, fd,t pointer for common words, or onequarterof the 48 bits that would be required if the pointer was stored uncompressed. The additional space savings that can be achieved with stopping are, for this reason, small.ACM Computing Surveys, Vol. 38, No. 2, Article 6, Publication date July 2006.Inverted Files for Text Search Engines 23However, stopping does yield significant advantages during index maintenance becausestopping means that updates to the longest lists those of common words are avoided.Less frequent words require longer codes for their dgaps but, in general, shortercodes for their fd,t values. As a general ruleofthumb, each d, fd,t pointer in a complete documentlevel inverted index requires about 8 bits when compressed using acombination of Golomb and Elias codes. Use of the lessprecise bytewise code for oneof the two components adds around 4 bits per pointer and, for both components, addsaround 8 bits per pointer. The wordaligned code has similar performance to the bytewise code for large dgaps but obtains better compression when the dgaps are small.For example, a bytewisegammacoded index for the NewsWire data with the dgapsrepresented in a bytewise code and the fd,t values coded using gamma would occupyabout 70106 12 bits or approximately 10 of the source collection. In this index, thedocument identifiers are byte aligned, but the fd,t values are not, so the list is organizedas a vector of document numbers and a separate vector of frequencies with the two listsdecoded in parallel when both components are required. While the compression savingsare not as great as those attained by Golomb codes, the bytewise and wordaligned codesbring other benefits.Compression of Word Positions. Uncompressed, at, for instance, two bytes each so nodocument can exceed 65,536 words, word positions immediately account for the bulkof index size 360MB for NewsWire, and 22GB for Web. However, these costs can be reduced by taking differences, just as for document identifiers, and Golomb codes canbe used to represent the differences in either a localized withinthisdocument sense,or, more usefully, in an averageacrossalldocuments manner. In the latter case, thevocabulary stores two b values, one used to code the dgaps in the way that was described earlier, and a second to code the wgaps counting the word intervals betweenappearances of that term. Static codesgamma, delta, bytewise, or wordalignedalso give acceptable compression efficiency and may be attractive because of theirsimplicity.The choice of coding scheme also affects total fetchanddecode times with the bytewise and wordaligned codes enjoying a clear advantage in this regard. Bitwise codescannot be easily stepped through, and queries that require only bagofwords processingcan be considerably slower with an interleaved wordlevel index than with a documentlevel index.An aspect of inverted indexes that is dramatically altered by the introduction of wordpositions is the cost of processing common words. In a documentlevel index, commonwords such as the are relatively cheap to store. In a wordlevel index, the average perdocument requirement for common words is much larger because of the comparativelylarge number of fd,t wordgap codes that must be stored. In the case of the Web data, justa handful of inverted lists account for more than 10 of the total index size. Processingor even storage of such lists should be avoided whenever possible.Nonrandom Term Appearances. If documents are chronological, terms tend to cluster.For example, in the 242,918 documents of the Associated Press component of TREC,there are two definite clusters for hurricane, one due to hurricane Gilbert in September1988, and a second resulting from hurricane Hugo in September 1989. Similar clustering arises with web crawls. Some sites are topicspecific so that words that are rareoverall may be common within a group of pages. A particular cause of this effect isindexing of documents that are written in a variety of languages.The nonuniform distribution can be exploited by an interpolative code which transmits the midpoint of the list of document numbers in binary, then recursively handlesthe two sublists in the resulting narrowed ranges. For typical collections, this codeACM Computing Surveys, Vol. 38, No. 2, Article 6, Publication date July 2006.24 J. Zobel and A. Moffatresults in an average of 0.5 to 1.5 bits saved per d, fd,t pointer compared to a Golombcode. The drawback of the interpolative code is that it is more complex to implementand slower in decoding.The wordaligned binary coding method is also sensitive to localized clustering, a runof consistently small dgaps is packed more tightly into output words than is a sequencecontaining occasional larger ones.Pros and Cons of Compression. Compression has immediately obvious benefits. It reduces the disk space needed to store the index and during query evaluation, it reducestransfer costs the lists are shorter and seek times the index is smaller. Compressionalso reduces the costs of index construction and maintenance.A less obvious benefit of index compression is that it improves caching. If a typicalinverted list is compressed by a factor of six, then the number of inverted lists that canbe retained in memory is increased by that same factor. In a retrieval system, queriesarrive with a skew distribution with some queries and query terms much more commonthan others. Queries for Chicago weather forecast far exceed those for Kalgoolieweather forecast, for example. There can also be marked temporal effects. Peopleuse Web search engines to check questions on broadcast quiz shows, for example, sothe same unusual query might arrive thousands of times in a short space of time.Increasing the effectiveness of caching can dramatically cut the cost of resolving astream of queries.The principal disadvantage of compression is that inverted lists must be decodedbefore they are used. A related problem is that they may need to be recoded when theyare updated, for example, if parameters change. For some of the codes considered, theaddition of new information can require complex recoding.On current desktop machines, the decoding cost is more than offset by the reductionin disk seek costs. More importantly, as the ratio of the speed between processors anddisk continues to diverge, the performance benefit available through compression isincreasing. For byte and wordaligned codes, which allow each document identifier tobe decoded in just a few instruction cycles, the processing cost is more than offset bythe reduction in memorytocache transfer costs.Thus, if the index is larger than the available main memory or cannot be bufferedfor some other reason, there is no disadvantage to compression. And even if the indexis in memory, processing can be faster than for uncompressed data. Use of appropriateindex compression techniques is an important facet of the design of an efficient retrievalsystem.9. LIMITING MEMORY REQUIREMENTSIf the standard query evaluation algorithm is used for queries that involve commonwords, most accumulators are nonzero, and an array of N entries Ad is the most spaceand timeefficient structure. But the majority of those accumulator values are triviallysmall, as the only matching terms are one or more common words. Analysis of searchengine logs has demonstrated that common terms are not the norm in queries, andanalysis of relevance has demonstrated that common terms are of low importance. Itthus makes sense to ask if there are better structures for the accumulators, requiringfewer than N elements.Accumulator Limiting. If only documents with low ft i.e., rare query terms are allowed to have an accumulator, the number of accumulators can be greatly reduced.This strategy is most readily implemented by imposing a limit L on the number ofaccumulators as shown in Figure 12.ACM Computing Surveys, Vol. 38, No. 2, Article 6, Publication date July 2006.Inverted Files for Text Search Engines 25Fig. 12. The limiting method for restricting the number of accumulators duringranked query evaluation. The accumulator limit L must be set in advance. Thethresholding method involves a similar computation, but with a different test atstep 2bi.In ranking, using the models described earlier, a document can have a high similarityscore even if several query terms are missing. In the limiting approach, as each d, fd,tvalue is processed, a check is made to determine whether there is space in the setof accumulators to consider additional documents. Thus the algorithm enforces thepresence of the query terms that are weighted highly because of their frequency ofoccurrence in the query or because of their rareness in the collection. While the behaviorof the ranking is altered, it is arguably for the better since a document that is missingthe most selective of the query terms is less likely to be relevant. Some of the Web searchengines only return matches in which all of the query terms are present, a strategy thatappears to be effective in practice for short queries but, in experimental settings, hasbeen found to be less compelling.Accumulator Thresholding. Another approach of a similar type is to use partialsimilaritiesthe contribution made by a term to a documents similarity, or wq,t wd ,tto determine whether an accumulator should be created. In this approach, accumulatorsare created if it seems likely that the document will ultimately have a sufficiently highsimilarity, with the test A  L in Figure 12 replaced by a test wq,t wd ,t  S, for somevalue S. The threshold S is initially small but is increased during query processingso that it becomes harder for documents to create an accumulator as the computationproceeds.For a set of accumulators that is limited to approximately 15 of the numberof documents, in the context of TRECstyle long queries and evaluation, there is nonegative impact on retrieval effectiveness.However, with Webstyle queries, both the limiting and the thresholding methodscan be poor. In the limiting method, the limit tends to be reached part way through theprocessing of a query term, and thus the method favors documents with low ordinalidentifiers but if the limit is only tested between lists, then the memory requirementsare highly unpredictable. Similar problems affect thresholding. A solution is to modifythe thresholding approach so that, not only are accumulators created when the newcontribution is significantly large, but existing accumulators are discarded when theyare too small. With this approach, the number of accumulators can be fixed at less than1 of the number of documents for large Weblike collections.Data Structures. Comparing the two approaches, the limit method gives precise control over memory usage, whereas the threshold method is less rigid and allows evencommon terms to create accumulators for particular documents if they are sufficientlyACM Computing Surveys, Vol. 38, No. 2, Article 6, Publication date July 2006.26 J. Zobel and A. Moffatfrequent in those documents. Both methods reduce the number of nonzero accumulators, saving memory space without affecting retrieval effectiveness. And, as examplesof more general query pruning methods, they can also reduce disk traffic and CPU time,using methods that are discussed shortly.To maintain the set of accumulators in either of these two approaches, a data structure is required. An obvious solution is to store the accumulators as a list ordered bydocument number and successively merge it with each terms inverted list.Representing Document Lengths. Storage of the set of document lengths Wd is anotherdemand on memory during ranking. Figures 2, 4, and 12 suggest that the Wd valuesbe stored in a file on disk, but execution is faster if they can be retained in memory.There are two aspects of their use that make memory residence possible. First, theyare an attribute of the document collection rather than of any particular query. Thismeans that the array of W values can be initialized at system startup time rather thanfor every query and that, in a multithreaded evaluation system, all active queries canaccess the same shared array.The second aspect that allows memoryresidence is that, like many other numericquantities associated with ranked query evaluation, they are imprecise numbers andresult from a heuristic rather than an exact process. Representing them to 64 or 32 bitsof precision is, therefore, unnecessary. Experiments have shown that use of 8 or 16bitapproximate weights has negligible effect on retrieval effectiveness.Storage for the Vocabulary. Except for the document mapping table and assorted buffers,the only remaining demand on main memory is the term lookup dictionary or vocabulary. For collections such as NewsWire and Web, retaining the vocabulary in main memoryis expensive since each entry includes a word, ancillary fields such as the weight of thatterm, and the address of the corresponding inverted list.Fortunately, access to the vocabulary is only a small component of query processingif a Btreelike structure is used with the leaf nodes on disk and internal nodes inmemory, then a terms information can be accessed using just a single disk access,and only a relatively small amount of main memory is permanently consumed. Forexample, consider the Web collection described in Table I. The 16 million distinct termsin that collection correspond to a vocabulary file of more than 320MB but if each leafnode contains say 8kB of data, around 400 vocabulary entries, then the inmemorypart of the structure contains only 40,000 words, occupying less than 1MB. As for thedocument weights, this memory is shared between active queries and is initialized atstartup rather than once per query.10. REDUCING RETRIEVAL COSTSThe core implementation, together with simple strategies for reducing memory consumption, provides a functional query evaluation system. For example, in the contextof a desktop machine and a gigabytescale text collection, typical queries of a few termscan be resolved using these techniques in well under a second. In addition, with accumulator limiting, compact document weights, and a Btree for the vocabulary, thememory footprint can be limited to a dozen or so megabytes. However, substantial further savings can be made, and, with careful attention to detail, a single standard PC caneasily provide text search for a large organization such as a university or corporation.In the core implementation Figure 4, the principal cost is retrieval and decodingof inverted lists. That is, every d, fd,t pair contributes to a valid accumulator. Withlimitedaccumulator query evaluation Figure 12, however, most d, fd,t pairs do notcorrespond to a valid accumulator, and processing time spent decoding these pairsis wasted. We also argued earlier that to facilitate compression and the gains thatACM Computing Surveys, Vol. 38, No. 2, Article 6, Publication date July 2006.Inverted Files for Text Search Engines 27compression brings, the document numbers in each list should be sorted. However, useof compression means that each number is stored in a variable number of bits or bytesso random access e.g., to support binary search into inverted lists is not possible.Skipping. A proposal from the mid1990s is that decoding costs be reduced by theinsertion of additional structure into inverted lists. In this skipping approach, descriptors are periodically embedded in each compressed inverted list, dividing the list intochunks. The descriptor can be used to determine whether any d, fd,t value in thechunk corresponds to a valid accumulator if not, that chunk does not need to be decoded, and processing can immediately move to the next chunk. However, all chunksstill need to be fetched. Note also that, if the chunks are as large as disk blocks, thegreat majority will need to be decoded, and, if they are smaller, more chunks can beskipped but most disk blocks will still contain a valid chunk. In experiments at the time,we found that skipping yielded worthwhile performance gains. Since then, processorshave become much faster, while disk access times have not greatly improved, and thebenefits provided by skipping have eroded.To reduce disk transfer costs, it is necessary to avoid fetching the inverted lists intheir entirety. One obvious method is to only fetch inverted lists for rare terms, butexperiments consistently show that all terms should be processed if effectiveness is tobe maintained. The more attractive option is to rearrange inverted lists so that, in atypical query, only part of each relevant list need be fetched. The schemes describedearlier for accumulator limitation provide criteria for deciding whether a d, fd,t valuewill be used we now explain how these criteria can be used to restructure inverted listswithout seriously degrading compression effectiveness.FrequencyOrdered Inverted Lists. The principle of accumulator limitation is that largevalues of wq,t wd ,t should be processed first. The default heuristic for identifying thesevalues is to process the low ft terms first as their lists are likely to contain more largewq,t  wd ,t values than would the lists of high ft terms. However, in a typical invertedlist, most fd,t values are small, while only a few are large. If only the large fd,t valuesare interesting, that is, can contribute to a useful wq,t wd ,t value, then they should bestored at the beginning of their inverted list rather than somewhere in the middle ofthe documentbased ordering. For a given threshold S and term t, all wq,t  wd ,t  Svalues see Equation 1 will then be stored before any smaller ones.To make use of this idea, the index can be reorganized. A standard inverted list issorted by document number, for example,12, 2 17, 2 29, 1 32, 1 40, 6 78, 1 101, 3 106, 1.When the list is reordered by fd,t, the example list is transformed into40, 6 101, 3 12, 2 17, 2 29, 1 32, 1 78, 1 106, 1.The repeated frequency information can then be factored out into a prefix componentwith a counter inserted to indicate how many documents there are with this fd,t value6  1  40 3  1  101 2  2  12, 17 1  4  29, 32, 78, 106.Finally, if differences are taken in order to allow compression, we get6  1  40 3  1  101 2  2  12, 5 1  4  29, 3, 46, 28.Repeated frequencies fd,t are not stored, giving a considerable saving. But within eachequalfrequency segment of the list, the dgaps are now on average larger when thedocument identifiers are sorted, and so the document number part of each pointerACM Computing Surveys, Vol. 38, No. 2, Article 6, Publication date July 2006.28 J. Zobel and A. MoffatFig. 13. Interleaved processing of inverted lists using a frequency or impactsorted index.Blocks from the front of each list are applied to the set of accumulators in decreasing scoreorder until some stopping condition has been met. At the instant shown, 11 blocks have beenprocessed, and, if processing continues, the next block to be processed would be for the termold.increases in cost. In combination, these two effects typically result in frequencysortedindexes that are slightly smaller than documentsorted indexes.Using a frequencysorted index, a simple query evaluation algorithm is to fetch eachlist in turn, processing d, fd,t values only while wq,t  wd ,t  S, where S is thethreshold. If disk reads are performed one disk block at a time rather than on a wholeofinvertedlist basis, this strategy significantly reduces the volume of index data to befetched without degrading effectiveness.A practical alternative is for the first disk block of each list to be used to hold thed, fd,t values with high fd,t. These important blocks could then all be processed beforethe remainder of any lists, ensuring that all terms are given the opportunity to createaccumulators. The remainder of the list is then stored in document order.A more principled method of achieving the same aim is to interleave the processing ofthe inverted lists. In particular, once the first block of each list has been fetched and isavailable in memory, the list with the highest value of wq,t wd ,t can be selected, and itsfirst run of pointers processed. Attention then switches to the list with the nexthighestwq,t  wd ,t run which might be in a different list or might be in the same list.During query processing, each list could be visited zero, one, or multiple times, depending only on the perceived contribution of that term to the query. The interleavedprocessing strategy also raises the possibility that query evaluation can be terminatedby a timebound rather than a threshold since the most significant index information isprocessed first. Figure 13 shows the interleaved evaluation strategy with list sectionspresumed to be applied to the set of accumulators in decreasing order of numeric score.The shaded sections of each list are never applied.ImpactOrdered Inverted Lists. An issue that remains even in the frequencysorted approach is that wq,t  wd ,tWd is the true contribution of term t to the similarity scoreSq,d , rather than wq,t  wd ,t . If it makes sense to frequencysort the inverted lists intodecreasing wd,t order, then it makes even better sense to order them in decreasing impact order using wd ,tWd as a sort key. Then all that remains is to multiply each storedvalue by wq,t and add it to the corresponding accumulator. That is, an impact valueincorporates the division by Wd into the index.ACM Computing Surveys, Vol. 38, No. 2, Article 6, Publication date July 2006.Inverted Files for Text Search Engines 29Fig. 14. Impactordered query evaluation.If the inverted lists are then sorted by decreasing impact, the same interleaved processing strategy can be employed. Now the blocks in the inverted list must be created artificially rather than occurring naturally, since wd ,tWd is not integervalued,and exact sorting would destroy the dgap compression. Hence, to retain compression, the impact values are quantized so that each stored value is one of a small number of distinct values, and integer surrogates are stored in the inverted lists in placeof exact wd ,tWd values. Experiments with this technique have shown that approximately 1030 distinct values suffice to give unaltered retrieval effectiveness compared to full evaluation. And, unlike frequencybased methods, impactsorted listsare just as efficient for evaluation of the Okapi similarity heuristic described inEquation 2.With this change, each list is a documentsorted sequence of blocks of approximatelyequalimpact pointers, and no fd,t values are stored. Compared to document andfrequencysorted indexes, the compressed size grows slightly because the average dgaps are bigger. Nevertheless, the index is still a small fraction of the size of the textbeing indexed.Query evaluation with an impactordered index becomes a matter of processing asmany blocks as can be handled in the time that is available. To process a block, aninteger contribution score that incorporates the impact score associated with that blockand the query term weight wq,t which might be unity is added to the accumulator ofevery document recorded in the block. All querytime operations are on integers, alsomaking the final extraction phase significantly easier to manage. Impactordered queryevaluation is shown in Figure 14.With an impactsorted inverted file and integer surrogate weights, query processing isextremely fast. Just as important is that time consumed by each query can be bounded,independent of the number of terms in the query. When query load is light or when apremium service has been purchased, a greater fraction of the blocks in each query canbe used to influence the result ranking.Impacts provide another illustration of the flexible constraints on design of algorithms for information retrieval. Information can be selectively thrown away or approximated without detriment to the output of the algorithm, although there may bea beneficial change to efficiency. While the set of documents returned by impactbasedprocessing will not be the same as the set returned by the underlying computation inEquation 1, the quality of the set proportion of documents that are relevant may beunchanged. In practice, the effect is likely to be that one method gives better resultsACM Computing Surveys, Vol. 38, No. 2, Article 6, Publication date July 2006.30 J. Zobel and A. Moffatthan the other for some queries and worse results than the other for other queries. Suchchangebutnodifference effects are commonly observed in development of algorithmsfor text query evaluation.Other Considerations. When the inverted lists are impact or frequencysorted, theyare read in blocks rather than in their entirety, and contiguous storage is no longera necessity. Blocks with highimpact information could be clustered on disk, furtheraccelerating query processing. The lists for common termsthe ones with many document pointerswill never be fully read, and a great deal of disk traffic is avoided.Experiments with these techniques, using collections such as NewsWire, show that disktraffic and processing time are reduced by a factor of up to three compared to previous techniques involving limited accumulators. The likely gain is even greater for thelarger Web collection. As a final advantage, the impactsorted index dispenses with theneed for an array or file of Wd values.The disadvantage of both frequency and impactsorted indexes is that Booleanqueries and index updates are more complex. To handle Boolean queries efficiently,other index structures have been proposed. One approach is to use a blocked index.Rather than use dgaps that are relative to the previous document number, it is feasible to code dgaps relative to the start of a block. This allows use of a form of binarysearch at some loss of compression efficiency. Conjunctive Boolean queries can then beprocessed extremely quickly.However, updates remain complex. With documentordered list organizations, newd, fd,t values can be appended to the list. In the enhanced organizations, the wholeof each list must be fetched and decoded. Whether this is a significant disadvantagedepends on the application.Summary. We have described compressionbased indexing and query evaluation techniques for bagofword queries on a text database. Compared to the uncompressed,documentsorted implementation described in Figure 4,disk space for a documentlevel index requires about 79 of the size of the dataand has been reduced by a factor of around fivememory space for accumulators requires a dozen bytes for 5 of the documents in asmall collection or less than 1 of the documents in a large collection and has beenreduced by a factor of around twentyCPU time for processing inverted lists and updating accumulators is reduced by afactor of three or moredisk traffic to fetch inverted lists is reduced by a factor of five in volume and of twoor more in time The relative savings increase with collection sizethroughput of queries is reduced by making better use of memory, allowing moreeffective caching of vocabulary entries, lists, and answersconstruction time for indexes is cut by a factor of more than two, through reductionin the disktraffic bottleneck and reduction in the number of runs required to buildan index.11. OTHER APPROACHES TO INDEXINGQuerying can also be carried out without an index. For typical data and queries, buildingan index requires about 1050 times the cost of a string search. If queries are rare, orthe data is extremely volatile, it is reasonable to use a documentoriented queryingstrategy of the type shown in Figure 2. An example is searching of a personal emaildirectory or search within a small collection of files.ACM Computing Surveys, Vol. 38, No. 2, Article 6, Publication date July 2006.Inverted Files for Text Search Engines 31Fig. 15. Example of a suffix array.For querying of larger collections, inverted files are not the only technology that hasbeen proposed. The two principal alternatives are suffix arrays and signature files. Inthis section, we briefly outline these approaches and explain why they are not competitive with inverted files for the applications considered in this article.Suffix Arrays. A suffix array stores pointers into a source string, ordered according tothe strings they point at. The index can be of every character position or, for typicalretrieval applications, every word position. The structure provides access via a binarysearchlike process which is fast if it can be assumed that both the array and theunderlying source string are available in memory.Consider the first line of the Keeper collection annotated with byte offsets in thestring, commencing at zero, as shown in part A of Figure 15. In a wordlevel suffixarray, a list is made of the byte addresses at which words start, shown in the first rowof part B of Figure 15. The second row of part B shows the initial two characters of thesemiinfinite string that is the target of each pointer. To form the suffix array, the bytepointers are reordered so that the underlying strings are sorted, using as many characters of the semiinfinite strings as are necessary to break ties, as shown in part C. Notethat the second of these two lines is provided only to allow the destinations of the pointers to be visualized by the reader. All that is actually stored is the original string, and anarray A containing one pointer for each word in the string, with A036, A131, andso on.One way of interpreting the resultant array of byte pointers is that they representa combined vocabulary and inverted index of every suffix string in the original textwith access provided directly to the bytes rather than via ordinal document identifiers.To use the suffix array, the sorted byte pointers are binary searched, again using theunderlying strings to drive the comparisons. The output of each search is a pair ofindices into the suffix array, describing the first and last locations whose strings areconsistent with the supplied pattern. For example, to locate occurrences of keep in theexample array, the binary search would return 1 and 3 since all of A1, A2, and A3contain byte addresses 31, 14, and 21, respectively pointing at semiinfinite stringsin the original text that commence with the prefix keep. If exactly the word keep wasrequired rather than as a prefix, a trailing delimiter could be added to the search key,and then only A131 would be returned.It is straightforward to extend suffix arrays to phrase searching, and a range of simple techniques, such as conflating all nonalphabetic characters to a single separatorcharacter for the purposes of string comparison, allow the same binarysearching process to be applied regardless of the length or frequency of the phrase or words in it.ACM Computing Surveys, Vol. 38, No. 2, Article 6, Publication date July 2006.32 J. Zobel and A. MoffatCasefolding can also be achieved by making the string comparison function caseinsensitive as has been presumed in the example.For largescale applications, suffix arrays have significant drawbacks. The pointerarray is accessed via binary search so compression is not an option. For a wordalignedsuffix array, a 4byte pointer is needed for each 6 bytes or so of text, and the underlyingtext must also be retained. In total, the indexing system requires around 170 of thespace required by the input, all of it memoryresident. A suffix arraybased retrievalsystem for 1GB of text demands a computer with 2GB or more of memory, a requirement that scales linearly as the source message grows. Suffix array indexes are largecompared to inverted files because the repeated information cannot be usefully factoredout and because byte addresses are used rather than wordoffset addresses. If the basetext is large, the suffix array itself can be indexed and partial strings stored in index toavoid needing to access the base text. For very large texts, the pointers can be managedhierarchically. However, these techniques add considerable complexity to the buildingand searching processes.Another drawback is that there is no equivalent of ranked querying. All but simplestemming regimes are also problematic. On the other hand, suffix arrays offer increasedstring searching functionality compared to inverted filescomplex patterns, such aswildcharacters, can be handled. The principal strength of suffix arrays is that theygreatly accelerate grepstyle pattern matching, swapping decreased time for increasedspace.Worth noting is that much of the functionality offered by a suffix array can be achievedin the context of an inverted file by indexing the vocabulary either with a suffix arrayspanning the vocabulary strings themselves or by a secondary inverted index makinguse of the character bigrams or trigrams that comprise the vocabulary terms.Signature Files. Signature files are a probabilistic indexing method. For each term,s bits are set in a signature of w bits. The term descriptors for the terms that appear ineach document are superimposed i.e., ORed together to obtain a document descriptor,and the document descriptors are then stored in a signature file of size wN bits forN documents.To query on a term, the query descriptor for that term is formed all of the documentswhose document descriptors are a superset of the query descriptor are fetched and thefalse matches are separated from the true matches with only the latter returned to theuser as answers.Assuming that all records are the same length, that is, then contain the same numberof distinct terms all terms occurring in over 5 of records are stopped and there is onaverage just one false match per singleterm query, then the index size is about 20 ofthe text. With an unstopped index, signatures must be much wider, giving a 40oftextindex.Variability in document length significantly complicates these calculations. With afixedlength signature, more bits are set for long documents than short. These documents are then more likely to be selected as false matches, and because they are long,the cost of falsematch checking is further increased. To get a falsematch volume of oneaverage documentlength, a signature file needs to be much larger than the indicativesizes given here.The simplest organization for a signature file is a sequence of signatures, which implies that the whole index is processed in response to each query. A range of alternativeorganizations of two kinds have been proposed. One is bitslicing in which the signaturefile, viewed as a matrix, is transposed in which case it is only necessary to fetch a smallnumber of slicesperhaps 10 to 15in response to a query. Each slice has one bit perstored record, and so, for the Web collection, would contain 1.5MB of data.ACM Computing Surveys, Vol. 38, No. 2, Article 6, Publication date July 2006.Inverted Files for Text Search Engines 33Another alternative organization is the use of descriptors in which signatures arepartitioned according to a small number of selected bits and only matching partitionsare fetched for each query. We are unaware of any practical demonstration of the merits of such descriptor schemes. Bitslicing, however, has been explored experimentally,including a range of variations designed to reduce costs. For example, slice length canbe dramatically reduced with a small increase in the number of slices fetched, althoughcosts remain linear in the number of stored documents.Even for the relatively unsophisticated task of Boolean retrieval, signature files haveseveral crucial defects. One is the need to eliminate false matches. For a given signaturewidth, the number of false matches is linear in the collection size and hence, as thenumber of indexed documents grows, the number of documents unnecessarily fetchedincreases. For any reasonable parameter settings, this cost must dominate. And thelonger a document, the higher the likelihood that it will be retrieved as a false match.Second, signature file indexes are large compared to compressed inverted files and areno easier to construct or maintain. Third, they require more disk accesses for shortqueriesexactly the kind that is commonplace in Web searching. And fourth, they areeven more inefficient if Boolean OR and NOT operators are permitted in addition to AND.More critically, there is no sensible way of using signature files for handling rankedqueries or for identifying phrases. That is, there are no situations in which signaturefiles are the method of choice for text indexing even for exactmatch Boolean stylesearching.12. BEYOND AN ELEMENTARY SEARCH ENGINEWe have outlined indexing and query evaluation algorithms that can be used in apractical search engine. These algorithms can be used to index large volumes of dataand provide rapid response to user queries. However, in specific searching applications,further improvements in performance are available. In this section, we briefly reviewsome of these options.Crawling. Data acquisition is a key activity in dynamic systems. In unusual cases,the revised content will be delivered as it is generated, and we need do no more thanindex it. For example, a commercial arrangement might result in the daily delivery ofupdated catalog entries for an online store.More usually, however, the owners of data change it without regard for whether or notit is indexed. Crawling is the process of seeking out changed data and feeding it backto the retrieval system for incorporation into updated indexes. In principle, crawling iseasy, a set of seed URLs is used as the basis for the search, and those pages are fetchedone by one. Each hyperlink within those pages is extracted and, if it has not beenexplored yet, appended to a queue of pending pages. Eventually, all pages reachablefrom the seed set will have been accessed, and the process can be repeated.However, there are many subtle issues that require attention if a crawler is to beefficient. First, not all pages are of the same importance, and, while crawling the CNNWeb site at hourly intervals might be appropriate, crawling a set of University pagesprobably is not. Indeed, the volume of data available might mean that the crawl simplydoes not finish before it is necessary to start it again. In this case, a strategy for prioritizing page visits and aborting the crawl when only lowpriority pages remain may berequired.Second, not all pages are edited at the same rate, and editing may follow a regularpattern. An adaptive crawler might be able to significantly reduce crawl volume byidentifying the update strategy that applies to pages and learning which pages arealmost never modified.ACM Computing Surveys, Vol. 38, No. 2, Article 6, Publication date July 2006.34 J. Zobel and A. MoffatThird, there are crawling traps that need to be anticipated and avoidedunintentional traps such as recursive scriptbased Web pages that implement calendarswith a next month link are just one example. Fourth, the crawler needs to be sensitiveto the requirements of the site it is visiting and must stagger its requests so as to avoidflooding it over a short period of time. Fifth, the crawler must be alert to the problem of duplicate pages and mirrored sites. And, finally, crawling is expensive becauseit involves either payment per gigabyte of data fetched for some connection modeswhether it is useful or not, or paying for a highcapacity connection to the Internet sothat the necessary bandwidth can be achieved.Caching. In many contexts, the same queries and query terms recur. For example,many queries to the Web search engines are topical, and queries to a sitespecific searchengine may also be clustered. Many of the queries on the Web site of one of our universities contain the query term results. Over half of these queries are posed on thetwo single days after first and second semester on which student grades are released.A search engine can take advantage of this behavior by caching.There are several kinds of information that can be cached. The inverted lists thatare fetched in response to a query can be explicitly kept in memory. Alternatively, ifthese lists are maintained in disk blocks, the operating system is likely to cache themautomatically on the basis of mostrecentlyused. As was noted earlier, compressionhelps by increasing the impact of this caching.It is tempting to store the vocabulary in memory because doing so means that a diskaccess is avoided for every query term. However, if the vocabulary is large, keeping itin memory reduces the space available for caching of other information and may notbe beneficial overall. Relying on the operating system swapping pages of vocabularyinformation is almost certainly a poor decision, as the distribution of hot query termsamong pages is unlikely to be clustered in a useful way. An alternative is to managethe vocabulary in a Btree as discussed earlier and to explicitly cache recentlyaccessedor frequentlyaccessed query terms in a separate small table.Perhaps the most effective caching is of answer lists. If queries recur, it makes senseto store their answer lists rather than recompute them. Even keeping them on disk iseffective as one short answer set can be fetched much more rapidly than can multipleinverted lists. For a typical search engine, most users will only view the first r answers,but it may be effective to keep the next r answers handy for the small percentage ofcases in which they are requested.Another form of caching is phrase indexing. Indexing all phrases is impractical, butusing termbased inverted lists to identify which documents contain a phrase is expensive. The cost of phrase query processing could be significantly reduced if invertedlists are explicitly maintained for common term combinations. That is, the users themselves, via their querying behavior, can guide the evaluation as to which phrases mightbe worth explicitly storing when the index is next rebuilt.One of the major bottlenecks of query evaluation is the need to fetch the documentsthat are presented in the answer lists most search engines return not only a documentidentifier but a corresponding document summary or snippet. Typically these summaries are based on the query and thus cannot be computed ahead of time. Saving complete answer pages, including all of the required snippets, can thus be a dramatic saving.PreOrdering. In the context of Web retrieval, techniques such as Googles PageRankcan be used to determine a static score for each page to complement the dynamic scorecalculated by whatever similarity function is being used. The final rank ordering is thena blend of the static score which can be regarded as an a priori probability orderingthat a page is relevant to queries in general, and a dynamic score which reflects theprobability that a page is relevant to this particular query.ACM Computing Surveys, Vol. 38, No. 2, Article 6, Publication date July 2006.Inverted Files for Text Search Engines 35Static rankings are only useful in contexts where additional nontextual informationcan be used such as in the case of PageRank the link structure. The PageRank of aWeb page is based on the link structure of the Web. Each page is assigned a score thatsimulates the actions of a random Web surfer, someone who either with probability pis equally likely to follow any of the links out of the page they are currently on, or withprobability 1  p jumps to a new page chosen at random. An iterative computation canbe used to compute the longterm probability that such a user is visiting any particularpage and that probability is then used to set the PageRank. The effect is that pages withmany inlinks tend to be assigned high PageRank values, especially if the pages thathost those links themselves have a high PageRank pages with low inlink counts, or inlinks from only relatively improbable pages, are themselves deemed to be improbableas answers.HITS is a similar technique which scores pages as both authorities and hubs, againusing a matrix computation based on the connectivity graph established by the Webshyperlinks. Loosely, a good hub is one that contains links to many good authorities,and a good authority is one that is linked to from many good hubs. Hubs are usefulinformation aggregations and often provide some broad categorization of a whole topic,whereas authorities tend to provide detailed information about a narrower facet of atopic. Both may be useful to queriers.Pages might also be assigned static scores contributions based on how similar theyare to an amalgam of previous queries, or how often users of the search service haveclicked through them to read their underlying content, or how deep they are in thedirectory structure of that particular site.Where static weights are available from some source such as PageRank or HITS, theycan potentially be used to eliminate some of the cost of ranking. For example, supposethat inverted lists are maintained in decreasing PageRank order. Then a topr rankingcould be rapidly computed by taking the Boolean intersection of the inverted listscorresponding to the query terms, terminating as soon as r documents containing all ofthe query terms have been found. These would be presented in decreasing PageRankorder. If the user then requests more documents, presumably because the top r areunsatisfactory, then the system simply computes the top 2r, and discards the first r ofthem.User Feedback. Relevance feedback has been widely explored in the context of information retrieval research. In explicit feedback systems, users identify the answers thatare of value and perhaps others that are not, and this information is incorporated intoa revised query, to improve the overall quality of the ranking. Much of this researchassumes that queries are independent, and, in practical systems, it has proven difficultto gather relevance assessments from users.However, in a system processing large numbers of queries, it is straightforward toidentify which answers users choose to view. The action of a user clicking on a link can beinterpreted as a vote for a document. Such voting can be used to alter the static weightsof documents and thus their ordering in the ranking generated for subsequent querieseven if the subsequent queries differ from the one that triggered the click though.Index Terms. In structured documents, such as those stored as HTML, different emphasis can be placed on terms that appear in different parts of the document. Forexample, greater weight might be placed on terms that appear in a documents titletags or as part of an h1 heading.Another important difference between Web searching and more general documentretrieval is that the anchor text associated with a link in one page is in many casesan accurate summary of what the target page is about. Links are usually manuallyACM Computing Surveys, Vol. 38, No. 2, Article 6, Publication date July 2006.36 J. Zobel and A. Moffatinserted, and unless they say click here,1 represent a succinct assessment to a humanauthor as to the content of the target page. Indexing the anchor text as if it were partof the target pageperhaps even as if it were a title, or headingcan significantlyimprove retrieval effectiveness in Web search applications.Another useful technique in Web searching is indexing the URL string itself. Forexample, the page at www.qantas.com shows the word Qantas as an image rather than atextual title and unless the URL were parsed into terms and indexed with the document,would risk not being searchable. Page importance can also be biased according to thelength of the URL that accesses it on the assumption that URLs with fewer slasheslead to more important pages.Finally, it is worth noting that the alt text associated with images is a usefulindication of what the image content is, and indexing of alt text is part of the Googleimage search mechanism.Manual Intervention. If some queries are particularly common, then a reasonable strategy for improving perceived performance is to manually alter the behavior of thesequeries. For example, it might make sense for the term Quantas to be manually associated with the Qantas page, given the high incidence of confusion over the exact name. Bythe same token, if the query CNN does not lead directly to www.cnn.com as a result of theranking heuristic, then direct manipulation to make it so could well be appropriate. Oneof the most common queries in the search engine of one of our institutions is the singleword library sadly, the home page of the library is only the fourteenth highest rankedanswer.13. BIBLIOGRAPHYThe material presented in the previous sections is based on an extensive literaturespanning more than 40 years. This section provides a highlevel critical overview ofthat literature and can be regarded as further reading that augments the tutorial.Our examination of past work is through the prism of current interests rather thana complete historical study. For example, in the first decades of work on informationretrieval, both Boolean and ranked queries were widely investigated but, in the lasttwenty years, most work on search has focused on ranking the research on Booleanquerying is increasingly of historical interest only and is largely outside our scope. Note,too, that terminology has changed since the 1980s. In many older papers, retrieval wastaken to mean Boolean matching. Scoring and ranking of matches was often describedas nearest neighbor searching. Some retrieval papers focused on cluster retrieval, anactivity that now receives relatively little attention.Within the narrow area of inverted file indexing for text, this bibliography is reasonably complete. However, some minor papers have been omitted on grounds such as lackof availability in particular, technical reports and theses and, where a preliminarypaper has later been expanded and republished, the preliminary paper is not cited. Inthe noncore areas, such as signature files, the bibliography is far from exhaustive. Inthese cases, we have focused on papers with innovations in index structures or queryevaluation algorithms.Some related topics are not considered. We do not explore vocabulary representationssuch as hash tables and Btrees or inmemory text search structures such as suffixarrays and we do not discuss use of text indexes in other applications or specialpurpose1In late 2005, for the query click hereGoogle returns the Adobe Acrobat Reader home page, the MacromediaFlash player download page, and the Apple QuickTime download page as the three topranked answersbecause of the hundreds of thousands of sites that point at the these pages using the anchor text to obtaina copy, click here.ACM Computing Surveys, Vol. 38, No. 2, Article 6, Publication date July 2006.Inverted Files for Text Search Engines 37text indexes such as those used for plagiarism detection. Nor do we discuss structureddocument retrieval, or use of inverted files for relations or other data. The purpose ofthis bibliography is to review and categorize the literature on freetext indexing andranked query evaluation.Books and Overviews. In contrast to areas such as database systems, there have beenrelatively few textbooks on information retrieval or text search. Only three recent bookshave substantial coverage of algorithms and data structures for information retrieval,those by Frakes and BaezaYates 1992, Grossman and Frieder 2004, and, in particular, Witten et al. 1999. The implementation described in the tutorial is similar tothe methods presented by Witten et al. 1999 and, more briefly, in a chapter of Bertinoet al. 1997. However, several of the extensions and methods summarized here weredeveloped since those texts were written.BaezaYates et al. 2002 include an overview of index representation and queryevaluation as well as topics such as string searching, suffix arrays, and issues for distributed processing. Frakes and BaezaYates 1992 and Salton 1968 provide usefulbackground, but they do not consider current approaches. Faloutsos and Oard 1995give an introduction to the field of text searching in a technical report. Arusu et al.2001 review text retrieval in the wider context of Web search, including issues suchas distribution and crawling.There are several older textbooks on information retrieval and retrieval systemsbut with only limited material on indexes and indexing including Salton 1971, 1981Lancaster and Fayen 1973 van Rijsbergen 1979 and Salton and McGill 1983. Foran overview of information retrieval research through approximately 1995, see SparckJones and Willett 1997, an extensive, annotated compilation of many classic information retrieval papers. A text from this same era that does consider indexing andcompressionbut not rankingis Heaps 1978 which discusses topics such as implementation issues for search of bibliographic records in this context, Heaps 1978reports that index size is expected to be about 70 of data size. For a general introduction to information retrieval including an overview of indexing, see BaezaYatesand RibeiroNeto 1999. Kobayashi and Takeda 2000 review information retrievaland search on the Web. Voorhees and Harman 2005 is an overview and history of theTREC project. Zobel et al. 1996 discuss criteria against which an indexing methodmight be assessed.Books that consider relevant compression techniques include Witten et al. 1999,Moffat and Turpin 2002, Salomon 2000, and Sayood 2000. Only the first of thesediscusses the application of compression to indexing.Text Search and Information Retrieval. Segesta and ReidGreen 2002 briefly reviewa 19538 implementation of associative text indexing using punch cards and magnetic tape. The indexes were, in modern terms, inverted lists represented as bitmapsmanual searching was reportedly easier than search via the computer. Ivie 1966 describes a range of ways similarity computations might be performed, and Matthew andThomson 1967 describe a successful implementation of a tapebased inverted index inwhich answers are ranked according to usersupplied term importance. Salton 1968describes a rich system that makes use of syntactic analysis, thesauri, dictionaries,and the vector space model to search tapes for matches. Salton 1972 gives reasonswhywith the machines, algorithms, and collections of the timeinverted files are notalways practical. Haskin 1980 reports that space estimates for storing an invertedindex range from 50 to 300 of the space needed to store the text itself, a quotationthat continues to be used to justify research into alternative indexing methods despitethe fact that it refers to a primitive form of inverted file with storage of redundantACM Computing Surveys, Vol. 38, No. 2, Article 6, Publication date July 2006.38 J. Zobel and A. Moffatinformation and no compression. The 50300 figure is originally due to Bird et al.1978 who report that the inverted file sizes of a range of then current systems rangein size from .5 to 3 times the size of the indexed data. The exact structure of the inverted indexes in these systems is not given, but each posting appears to contain asearch key. Thus this widelycited figure is based on structures quite different frominverted files as understood today.Bird et al. 1978 also report the cost of a search on a fulltext database typically20 to 30 per query, and sometimes as high as 100. Assuming 2005 commodity hardware pricing a cost multiple of ten to allow for data gathering, software purchases, andmaintenance and very conservatively one second per search then a typical querytoday might cost 0.03 cents, around 100,000 times less. That is, on average the cost hashalved every eighteen months, not even taking into account that the volume of databeing searched has probably grown by a factor of 1,000 or more during the same period, while the cost of accessing disk, then as now a key bottleneck in query processing,has fallen by a factor of no more than 100. Bird et al. 1977 state that it has becomeobvious to many researchers in the text retrieval disciplines that new hardware devices are needed continued reliance on software ingenuity will probably not producea spectacular breakthrough. The gains of the last three decades have not confirmedtheir pessimistic assessment.Harper 1982 tested four commercial systems ASSASSIN, BASIS, STAIRS, andSTATUS and notes that all use a form of inverted file for the 62MB collectionused in the experiments, the blockoriented index of STATUS was estimated to require around 45MB. A categorization and partial bibliography of pre1983 paperson indexing and search is given by Eastman 1983 this paper demonstrates thata wide range of indexing methods were under consideration at the time. A corresponding survey by Faloutsos 1985a also considers a range of techniques. Faloutsos1985a reports that inversion is adopted in most of the commercial systems, buthas significant disadvantages, and that, for the office environment, the signaturemethod is the most promising. These conclusions have been invalidated by subsequentwork.Cluster retrieval was another alternative that is now little investigated. In some oldertextbooks, cluster searching is regarded as the most promising technique for search.However, Voorhees 1986 compared cluster and inverted file search and found thatinverted file search is more efficient. Salton 1989 wrote that invertedfile search issubstantially faster . . . it is likely that inverted file searches will always remain fasterthan cluster searches, no matter how refined a cluster organization is available. Can1994 proposed a combination of cluster search and inverted file search, arguing that itis advantageous to organize inverted list entries by cluster, but interest in clusterbasedsearching has greatly declined since the 1980s there is little evidence that clusteringprovides a viable alternative to inverted indexes.Early work on inverted files for text used methods developed for databases such asthat of Bayer and McCreight 1972, Cardenas 1975, and McDonell 1977. In theseapproaches, the cost of update was a key concern, leading to representations of invertedlists in sequences of blocks. The focus on reducing update costs thus led to increasedspace consumption and slower query evaluation. Stellhorn 1977 considered invertedfiles designed for text, arguing for the use of specialpurpose hardware to allow rapidmerging the arguments do not apply to current systems.Much of the work on inverted indexes assumes that lists are directly managed viaa file system, an approach that allows control over fragmentation and use of specialpurpose freespace management algorithms. Zobel et al. 1993b consider layout of listson disk and management of short lists with blocks, showing that their hybrid approachcan lead to low fragmentation.ACM Computing Surveys, Vol. 38, No. 2, Article 6, Publication date July 2006.Inverted Files for Text Search Engines 39Alternatively, relational database systems can be used as file systems, for example,an inverted list can be stored as a contiguous blob in a record. Such an approach isexplored by, among others, Brown et al. 1994, Melnik et al. 2001, and Vasanthakumaret al. 1996, who show that it simplifies implementation. There are costs, however inaddition to the extra layer of management that is introduced by use of a database systemfor file management, this organization removes flexibility such as the ability to retrievelists in stages and to ensure contiguous storage. Grabs et al. 2001 consider whether adistributed relational database provides advantages for information retrieval comparedto a monolithic relational database, but the generality of the results is unclear due touse of a collection that fits in memory and lack of explanation of query evaluationtechniques. The potential advantages during update are considered by Brown et al.1994, but this method assumes segmentation of lists into 8kilobyte blocks and doesnot allow efficient query processing.Standard relational indexing techniques are an alternative to indexes designed fortext. Numerous approaches of this kind have been described in which an inverted listfor a term is represented typically by a series of tuples, one per document containing theterm. Such implementations are orders of magnitude slower than textspecific designs,and we do not explore this literature of unsuccessful methods. Also, there have beenmany descriptions of specialpurpose implementations most of these are of at bestlimited interest.Similarity Measures. Improvement in similarity measurement is a key aim of information retrieval research and, over the years, has been the subject of hundreds of papersin the annual SIGIR Information Retrieval Conference. A survey of even the principalcontributions is beyond the scope of this article, and we simply note a small number ofkey papers.The most effective similarity formulations have been based on models of documents,queries, and the retrieval process. One such approach is the vector space model, formallyexplained by Salton et al. 1975 but in use much earlier. The first reference to the cosinemeasure that we know of is by Salton 1962 who considers the vectors of documentproperties as vectors in mspace, and takes as a distance function the cosine of theangle between each vector pair.2 Other early work is by Ivie 1966 who also consideredsimilarity functions.The cosine measurewhich has consistently been the most successful similarity measure described under the vector space modelhas undergone continuing refinement,in particular with regard to term weighting. Arguably, the most recent significant innovation was the introduction of documentlength pivoting Singhal et al. 1996 whichaddresses the issue that document length and likelihood of relevance are correlated. Arange of other extensions of this type, for instance, introduction of a tunable constantthrough the use of training data, have followed.For much of the history of information retrieval, the principal alternative to thevector space model has been probabilistic models. Statistical approaches to informationretrieval were articulated during the early days of document computing for example,Luhn 1957 wrote thata standard program could serve to direct the machine to compare the question with the documentsof the collection. Since an identical match is highly improbable, this process would be carried out on astatistical basis by asking for a given degree of similarity.2We thank Michael Lesk for locating and forwarding to us this key early information. Also worth notingis that the Salton Award, presented every three years for research in the area of information retrieval, isnamed after Gerard Salton 19271995, the pioneering researcher who penned this statement.ACM Computing Surveys, Vol. 38, No. 2, Article 6, Publication date July 2006.40 J. Zobel and A. MoffatThe probabilistic models basis is the probability ranking principle, explored by Maronand Kuhns 1960 and later formalized by Robertson 1977. Similarly, Edmundson andWyllys 1961 wroteIn preparation for the widespread use of automatic scanners which will read documents and transmittheir contents to other machines for analysis, this report presents a new concept in automatic analysisthe relativefrequency approach to measuring the significance of words, word groups, and sentences.Many of the underpinnings of probabilistic information retrieval are summarized bySparck Jones et al. 2000 who give a detailed derivation of the probabilistic similaritymeasure often referred to as the Okapi measure or BM25 Robertson et al. 1994.A recent innovation is the use of language models, introduced by Ponte and Croft1998, where similarity is based on the kinds of word probability estimates used instatistical modeling Manning and Schutze 1999 and text compression Bell et al.1990. Many of the key issues in language models are examined in the papers collectedby Croft and Lafferty 2003. Language modeling is an active area of research.We are aware of no recent surveys or overviews of similarity measurement. Harman1992 lists a range of similarity measures. Salton and Buckley 1988b surveyed different retrieval mechanisms and introduced notation to describe them Zobel and Moffat1998 combined a range of measures into an orthogonal framework and compared themon several different test collections.Phrase Querying. An aspect of query evaluation that has attracted only a little published research is the algorithmic problem of ranking with phrases. Lee et al. 1996compare inverted files and signature files for keywordbased retrieval of structureddocuments. Williams et al. 1999 describe an index organization for word pairs, allowing phrase queries of arbitrary length. Williams et al. 2004 show that a small, partialphrase index can be used in conjunction with a conventional index and a phrase cacheto achieve most of the speed gains attained by explicit wordpair indexes at much lowerspace overhead.Index Construction. Although the problem of index construction for text search hassimilarities to both standard index construction and external sorting, there are particular features of text indexes that mean that specialpurpose algorithms can be moreefficient than a generalpurpose algorithm.There are several alternative approaches Witten et al. 1999, Chapter 5. Fox and Lee1991 proposes a twopass method in which the index is structured during one passthrough the data and constructed during multiple second passes. This method requireslittle overhead space but substantial overhead time for the repeated passes over thedata.Many authors, well into the 1990s, appear to have assumed that indexes would beconstructed with the methods used for relational databases. Others assumed that anindex should be built by generating tuples representing list entries in occurrence order and sorting them into term order. Harman and Candela 1990 and Rogers et al.1995 explored a method based on writing temporary list entries to disk which, whilefaster than the simplistic sorting methods, is still impractical. Moffat 1992 describes ascheme that assumes that the entire compressed index can fit into main memory. Moffatand Bell 1995 describe a process that makes use of all of compression, inplace ondisksorting, and onepass processing to build a compressed index within only a moderateamount of memory in excess of that required by the vocabulary. Heinz and Zobel 2003experimentally examine several techniques and demonstrate that a straightforward approach based on building and merging runs in limited memory is, in practice, the mostefficient.ACM Computing Surveys, Vol. 38, No. 2, Article 6, Publication date July 2006.Inverted Files for Text Search Engines 41Index Maintenance. Cutting and Pedersens 1990 work was one of the first examinations of the particular problems posed by update of text indexes, including sequential management of list entries, and makes practical recommendations that are stillrelevant such as the need to make updates in batches rather than termatatime ordocumentatatime. Brown et al. 1994, as noted earlier, use a relational system tomanage list entries as they are incrementally modified. Motzkin 1994 describes amethod based on storing list entries in a Btree, giving analytical results, but thesmall blocks of list entries that result are likely to lead to inefficient query evaluation.Burkowski 1990 considers the problem of freespace management in the context of anindex organized for fast updates while the issues raised are interesting, the solutionis inapplicable to current implementations of indexing.Lester et al. 2006 experimentally compare different approaches to update, findingthat, for large numbers of new documents, inplace update is costly compared to mergingold and new indexes. Tomasic et al. 1994 and Shoens et al. 1994 evaluate incrementalupdate strategies on synthetic data, considering a range of approaches to managementof space allocation and free lists. Clarke et al. 1994 and Clarke and Cormack 1995consider the costs of batch update strategies in the context of a method for updating instages. Lester et al. 2005 and, in a case of independent discovery, Buttcher and Clarke2005 describe the intermittent merging approach in which the index is partitioned intoa sequence of fragments of increasing size. This strategy reduces both the underlyingcomplexity and the inpractice costs.Barbara et al. 1996 consider caching strategies for reducing the costs of updates andqueries, arguing afresh that buffering of updates is likely to significantly reduce costs.Much of the paper considers locking strategies, an operation that is neglected or viewedas unnecessary in many text retrieval applications. Shieh and Chung 2005 proposean overallocation strategy designed to balance space wastage against the cost of toofrequent list relocation. The results in this paper and others indicate that some formof overallocation is essential. Lim et al. 2003 reconsider an assumption widely madein older work, that document update or deletion is rare, noting that this assumption isfalse for Web data. Using a simple index representation, they show that update costscan be reduced through use of structured rather than ordinal word positions.Distributed Information Retrieval. There is a wide literature on distributed informationretrieval. Many of the papers in this area concern topics such as database selection andresult fusion rather than index structures or algorithms for query evaluation. However,there are numerous exceptions. Most of the algorithmoriented papers on distributedinformation retrieval concern proposals for specific architectures and measurement orestimation of efficiency under the architecture.Stanfill et al. 1989, 1999 describe how an index might be distributed among alarge number of tightly coupled lowpower parallel processors. The underlying indexdesign is relatively simplistic, and it is not clear that the predicted speedup, based onextrapolating to collections thousands of times larger than those tested, could havebeen achieved in practice. Cringean et al. 1990 explore similar issues with similar limitations in a transputer network. A related design was explored by Reddaway1991. Salton and Buckley 1988a critically evaluate these approaches. Couvreur et al.1994 compare the suitability of different index types inverted files, signature files,and specialpurpose hardware for parallel text search for both Boolean and wildcardqueries.Harman et al. 1991 describe an implementation based on document distributionacross servers with separate collections, used as a library information system. Thissystem was successfully used in practice. Macleod et al. 1987 use simulation to compare approaches to distribution, including having documents on one server and theACM Computing Surveys, Vol. 38, No. 2, Article 6, Publication date July 2006.42 J. Zobel and A. Moffatindex on another, as well as the more usual approaches to division of workload viapartitioning and replication. They found distribution to be advantageous but under assumptions that do not correspond to current systems such as low workload and Booleanquerying. Martin et al. 1990 found via simulation that caching of material at clientor server can improve performance. Again, many of the assumptions are not applicable to current systems, but one key findingexplicit caching is superior to relying onthe operating systems cachingremains valid. Martin and Russell 1991, however,question whether clientside caching is of value.Cahoon and McKinley 1996 and Cahoon et al. 2000 describe a distributed systembuilt on the Inquery server, showing primarily by simulation that distributing documents among servers can improve response until the client becomes choked. Largegains in performance were obtained by reducing the numbers of answers required. Luand McKinley 2003 report substantial further gains with carefully targeted replication, an alternative or enhancement to caching in distributed systems.RibertoNeto et al. 1999 explore alternative ways of building a termdistributedstatic index in a distributed environment, showing that it is best to distribute invertedlists during construction of runs. This means that the vocabulary distribution must beknown in advance.Barroso et al. 2003 provide an overview of the Google architecture. In this system,the index is distributed documentwise among several servers, giving a server cluster. Documents are stored on separate machines. An alternative documentdistributedarchitecture is explored by Hawking 1997 with experiments showing that it scaleswell. Another documentdistributed architecture is explored by Melnik et al. 2001.de Kretser et al. 1998 outline how queries on a documentdistributed system mightbe evaluated, considering communication costs and correspondence to a monolithic system, arguing that a key optimization is to minimize the number of handshakes betweenclients and servers.Xi et al. 2002a, 2002b describe an approach in which each node contains a fixed fraction of each inverted list. Whether this approach is in practice significantly different todocument distribution is unclear their experiments indicated small improvements overa short sequence of queries and used the assumption that lists have to be completelyprocessed during query evaluation.Jeong and Omiecinski 1995 compare document distribution to term distribution,finding experimentally but on artificial data that document distribution is likely to besuperior in practice. MacFarlane et al. 2000, using a large data set, a small numberof real queries, and a single serialized client, fount that document partitioning is superior. Tomasic and GarcaMolina 1993 compare distributed index organizations, usingsimulation, and found that document distribution is superior to term distribution forBoolean queries. Tomasic and GarcaMolina 1996 undertook a similar investigation,using bibliographic records instead of fulltext documents, and found that distributionwithout replication is unhelpful. RibeiroNeto and Barbosa 1998 compare term anddocument distribution with simulations and found that term distribution is likely tooutperform document distribution. Badue et al. 2001 compare the same approaches,finding experimentally with artificial queries that term distribution is clearly superior.Cacheda et al. 2004 compare distribution with replication, finding the latter to bemore efficient, using simulation, artificial queries, and extrapolation due to simplifying assumptions such as that the servers are memoryless, the results are unrealistic.However, this is an active area of research at the moment there is no clear best design, and choices between approaches depend on a range of machine and applicationspecifics.More recently, Moffa et al. 2005 describe a termoriented system in which partialanswers are transferred between servers rather than inverted lists. Their results showACM Computing Surveys, Vol. 38, No. 2, Article 6, Publication date July 2006.Inverted Files for Text Search Engines 43that the revised method is competitive with document distribution in terms of querythroughput but has problems with load balancing and is thus not as scalable.The contrasts between these many papers highlight the difficulties in experimentalor simulated work of this kind high throughput can be achieved at the expense ofpoor response time to individual queries caching of inverted lists and answer setsaffects different architectures in different ways if indexes are smaller than availablememory, timings may be highly unrealistic the relative importance of the componentcosts changes dramatically with the volume of data to be indexed and artificial queriesare likely to be unrealistic. These kinds of problems lead to question marks over muchof this work. In some papers, for example, it is assumed that the distribution of queryterms is the same as that of document terms, leading to the conclusion that long listsmust be processed in full for typical queries. Failure to model caching effects leads todrastic inaccuracies experiments with small numbers of queries, even if realistic, saylittle about behavior with large numbers of queries where repetition is observed.Comparisons are meaningful only if both implementations are of high quality or if,improbably, in our view, simulations are fully realistic without simplifying assumptions.Moffat and Zobel 2004 comment on some of these issues.Efficient Index Representations. The majority of the representations used in index compression are based on integer coding methods, in particular those of Golomb 1966,Gallager and Van Voorhis 1975, Elias 1975, and Rice 1979.Early approaches to index compression were based on interpreting inverted lists asbitmaps. Most of these schemes did not consider indocument frequencies but, in hindsight, incorporation of these frequencies would be straightforward. Jakobsson 1978gives a compression method based on interpreting bitvectors as sequences of kbitsymbols each of which can be allocated a Huffman code. Fraenkel and Klein 1985explore variants of this scheme. Bookstein and Klein 1992 and Choueka et al. 1986describe a refinement in which an initial index is used to eliminate blocks in whichall bits are zero. All of these schemes, at best, simply approximate the Golomb codeand its excellent behavior when the appearances of each term are random in the setof documents. Other refinements are described by Bookstein and Klein 1991a, 1991b,1991c.Schuegraf 1976 described another variant based on bitmap compression, using analysis, to demonstrate that, in principle, large savings were available. However, it is notclear that this method is significantly different from the other bitmapbased approachesdiscussed previously.Choueka et al. 1987 extended the bitmap approach to allow determination ofwhether words occur in proximity to each other. Compression of a wordlevel indexis explored by Choueka et al. 1988, using a range of purposedesigned representations for tuples representing word positions. Some of the later methods can be viewedas generalizations, with greater compression effectiveness, of this approach. Klein et al.1989 briefly outline an organization for an inverted index based on these tuples inwhich word positions are described as coordinates of document, paragraph, and sentence number in a static database, allowing space savings compared to an uncompressedrepresentation.Prior to much of this work, Teuhola 1978 proposed use of Golomblike codes forcompressing bit vectors, a principled approach that achieves better compression thanmany of the alternatives described since.The compressed index representation outlined in Section 8 was developed in a sequence of papers in the early 1990s Witten et al. 1991, Moffat and Zobel 1992a, 1992bZobal et al. 1992 Bell et al. 1993. Refinements cover including additional structureto accelerate processing Moffat and Zobel 1996 Anh and Moffat 1998 exploiting anyACM Computing Surveys, Vol. 38, No. 2, Article 6, Publication date July 2006.44 J. Zobel and A. Moffatclustering that may be available Moffat and Stuiver 2000 Blandford and Blelloch 2002Silvestri et al. 2004 exploration of bytewise codes Williams and Zobel 1999 Scholeret al. 2002 Brisaboa et al. 2003 Culpepper and Moffat 2005 exploration of fixed binarycodes Anh and Moffat 2005 suppression of unimportant index information Carmelet al. 2001 and experimental comparison of alternative representations Scholer et al.2002 Trotman 2003. Much of the current research in the area of efficient indexing andquery evaluation is based on these papers.Linoff and Stanfill 1993 describe an approach based on bitwise codes that, in the bestcase, are equivalent to an Elias gamma code. The compression achieved is poor. Shiehet al. 2003 propose reorganization of lists into trees to allow fast Boolean and rankedquerying. It seems unlikely that this approach is practical as the reorganization appearsto introduce large numbers of pointers. Use of Bayesian models is explored by Booksteinet al. 2000 the results, on common terms in small collections, are not comparable tothose in other work. Shieh et al. 2003 describe a similaritybased reordering of thestored documents to reduce median gap values and thus improve compression withElias codes. Such techniques may be of value for static collections.Faloutsos and Jagadish 1992 analyze search and construction times for an uncompressed index that is a mix of inverted lists and bitmaps without word frequencies, andconsider practical issues such as bitmap update. Such methods are superseded by theimplementation described in the tutorial, as a posting list with Elias or Golomb codingis almost always shorter than the corresponding bitmap regardless of term frequency.Limiting Memory Requirements. Early implementations of ranked query evaluationassumed an array of accumulators, one for each document in the collection, as explained, for example, by Buckley and Lewit 1985. Perry and Willett 1983 surveyranking methods based on inverted files. They report that, prior to about 1980, whilesome papers described systems that directly ranked documents, the great majorityof . . . interactive document retrieval services are based on partial match, Boolean searchprocedures Perry and Willett 1983. Noreault et al. 1977 showed that inverted filescan be used to support ranking with only slightly increased costs compared to Booleanquerying.For a query of many terms, a key optimization to standard ranking methods is toavoid processing of inverted lists corresponding to words of low weight. Smeaton andvan Rijsbergen 1981 and Buckley and Lewit 1985 describe early schemes of this kind,using term statistics to determine when the set of top documents cannot be altered byfurther index processing though their order might change. Lucarella 1988 and Wongand Lee 1993 describe further variants to these schemes. For short queries, however,these approaches cannot be effective, and the overhead of dynamically maintainingthe necessary statistics during query evaluation is high. Perhaps most seriously, ascollection size grows, the difference in score between highlyranked documents becomessmall, and the termination condition is unlikely to be met until it is too late to achievesignificant savings.An alternative stopping condition was described by Moffat and Zobel 1996. In thisapproach, the total number of accumulators was fixed. Only the document identifierscorresponding to the first inverted lists processed could be included. Once the limit inthe number of accumulators is reached, query processing can stopgiving the final setof documents, but possibly in a poor orderor can continue, updating existing accumulators but not adding new ones. Persin et al. 1996 described a thresholding methodfor throttling the number of accumulators created, without specifying a precise limit.The impactsorted indexes of Anh et al. 2001 and Anh and Moffat 2002 also support early query termination, trading effectiveness for efficiency. Lester et al. 2005described a thresholding method designed for Webstyle queries where accumulatorsACM Computing Surveys, Vol. 38, No. 2, Article 6, Publication date July 2006.Inverted Files for Text Search Engines 45are both created and discarded as each list is processed. With the growth in the sizeof typical collections, all practical implementations now use some method for limitingnumbers of accumulators.In uncompressed inverted lists, a common assumption in work on query evaluationprior to 1993 or so, the document weights could be incorporated into the inverted lists.With the compression methods described in the early 1990s, it was much cheaper tostore frequencies which are small integers, whereas normalized frequencies are floating point numbers drawn from a large domain. Moffat et al. 1994 showed that document weights could be stored in a few bits each the accuracy of, for instance, 32 bitsis unnecessary given the existing approximations inherent in ranking. As the array ofdocument weights needs to be randomly accessed, this can greatly reduce the memoryrequired for a search engine.Turtle and Flood 1995 argue that documentordered query processing in whichlists are merged rather than processed in turn in termordered query processing, maybe more efficient in practical multiuser environments. Kaszkiel et al. 1999 comparetermordered and documentordered processing for whole documents and for passagesand find that documentordered processing can be effective if some query terms arerare. However, the distinction between the methods is unclear in the context of limitednumbers of accumulators. Strohman et al. 2005 carry out further experiments with adocumentordered processing regime.A spacesaving approach is to trade off indexprocessing cost against documentretrieval cost. Signature files exemplify this technique. As the amount of informationheld about each document is reduced, the likelihood of fetching a document by mistakegrows. In such an approach, it is necessary to postprocess documents to ensure thatthey do in fact match the querya cost that in a large collection must easily dominateall other components of query processing. Other approaches of this kind were proposedby Manber and Wu 1994, BaezaYates and Navarro 2000, and Navarro et al. 2000in which words are indexed by approximate location, such as a group of documents,which, after index processing, could then be exhaustively processed.A radically different approach to query evaluation is described by de Kretser andMoffat 1999, Clarke and Cormack 2000, and Clarke et al. 2000. In this approach,word positions are stored in inverted lists and documents are ranked according tothe proximity of the query terms a document that has a region that is dense withquery terms is highly ranked. This approach can be very effective, particularly on longor poorly defined documents. Passage retrieval, in which the document collection isbroken into fixedsize passages for the purpose of similarity checking, has also hadsome success Kaszkiel et al. 1999 Zobel et al. 1995.Reducing Retrieval Costs. Perhaps the single most effective heuristic for reducing queryevaluation costs is to reorder inverted lists according to frequency or contribution.Until the mid1990s, most of the query evaluation schemes were based either on theassumption that inverted lists were sorted by document identifier, or made no use of anordering assumption at all. Wong and Lee 1993 describe a scheme in which lists aresorted by decreasing frequency. This initial description of the approach was marred bythe assumption that inverted lists would consist of short blocks of document identifiersleading to high retrieval costs but allowed interleaved processing of inverted lists. Itis the first description of this sorting principle which gives substantial savings.Persin et al. 1996 proposed that inverted lists be sorted by indocument frequency.As in the method described by Wong and Lee 1993, query evaluation then processesthe front part of each inverted list how much is processed is determined by parametersreflecting system load and memory availability. A practical refinement to this approachwas proposed by Brown 1995 who suggested that, for each list of sufficient length,ACM Computing Surveys, Vol. 38, No. 2, Article 6, Publication date July 2006.46 J. Zobel and A. Moffatthe list begin with a fixedlength block of document identifiers in which the term isfrequent. An alternative is to store normalized frequencies in the inverted lists asdescribed by Hawking 1998, Anh et al. 2001, and Anh and Moffat 2002. To avoidloss of compression efficiency in this impactordering approach, the resulting absoluteterm weights are quantized. Large efficiency gains can be obtained through this methodwhich is also suitable for evaluating queries within a fixed time bound.Carmel et al. 2001 propose the static pruning of inverted lists, discarding at indexconstruction time entries that are heuristically judged to be unlikely to affect ranking. While successful at reducing index size, the method is less effective and less efficient than the dynamic pruning methods. Garcia et al. 2004 describe another ordering variant where, within each inverted list, documents are ordered according tothe number of times the document is highly ranked by a training query. The savingsyielded are not as high as for impactordering and large numbers of training queries arerequired.Caching reduces the cost of accessing disk. The impact of answer and list caches isexplored by Saraiva et al. 2001 who use a large collection and a query log to showthat both kinds of cache contribute to dramatically improved throughput. Lempel andMoran 2005 give theoretical bounds for the benefits of caching. Technical aspects ofcache implementation are considered by Shieh et al. 2003 who show that, unsurprisingly, the costs of running a retrieval system can be significantly reduced by the useof caches experiments with simulated queries are used to compare caching schemes,but the results which depend on query distribution, a small collection, and specifichardware seem unlikely to generalize. Jonsson et al. 1998 show that cache replacement policy has a significant effect on performance, while Lempel and Moran 2004demonstrate efficiency gains in the context of a distributed system by prefetching andanticipating events such as users proceeding to the next page of answers. In this context, as illustrated by the work of Spink et al. 2001 and Spink and Xu 2000 on querylogs, even though query evaluation can in principle focus on the rarest of the queryterms provided, terms in typical queries occur in up to 1 of the indexed documents.Thus the inverted lists of query terms can be expected to be at least a megabyte ina large text collection and caching is likely to lead to significant savings. Lempel andMoran 2003 consider answer caching strategies in practice, using a trace of sevenmillion reallife queries from a search engine log.Web Searching. Brin and Page 1998 describe the PageRank mechanism, in effect aqueryindependent similarity measure, that led them to develop the first version ofthe Google search engine. Other aspects of the Google search engine are presented byBarroso et al. 2003. The HITS algorithm is due to Kleinberg 1999.Other simpler metrics for assigning static weights to pages include counting the indegree and the outdegree of pages and various techniques based on statistics of thepage, for example, the ratio of normal text to other content. Many of the weighting factors used by commercial search engines are proprietary and the subject of considerablespeculation on the part of both consumers of the services they provide and also thespammers who seek to manipulate those services.Other Approaches to Indexing. There are many papers on exhaustive string search butonly de Kretser and Moffat 2004 consider how to rank exhaustively. Suffix arrays canbe used for a wide range of searching tasks, including the Boolean retrieval more usuallyundertaken using inverted files. BaezaYates and RibeiroNeto 1999 and BaezaYateset al. 2002 provide an introduction to the literature in the area. Note that suffix arraysprovide exactmatch searching there is no concept of ranking or of similarity scoring.On the other hand, some of the flexibility of string searching can be achieved if theACM Computing Surveys, Vol. 38, No. 2, Article 6, Publication date July 2006.Inverted Files for Text Search Engines 47vocabulary, rather than the underlying set of documents, is searched for possible queryterms Zobel et al. 1993aThere is a wide literature on signature files, dating back to the 1970s. Many of thepapers in the area are principally concerned with the organization of the files anddo not consider at least, not in significant detail how they would be applied to theproblem of Boolean or ranked text search. Only a few papers concern the problem ofranking using signature file indexes, in particular Croft and Savino 1988 and Leeet al. 1995. The schemes described in these papers are not competitive with invertedfile implementations.Zobel et al. 1998 give a detailed comparison of inverted files and signature files.Using analysis and experiments with data sets of under a gigabyte, it was concludedthat signature files were highly inefficient as a Boolean retrieval mechanism for text.Since that time, the size of typical collections has greatly increased extrapolation of thecost components of those experiments suggests that, for todays collections, signaturefiles are relatively even less efficient. When inverted files can find matches in 100GB inless than a tenth of a second, there is little reason to use a signature file that requiresseconds to search a gigabyte. Prior to the investigation of Zobel et al. 1998, Couvreuret al. 1994 compared inverted files to signature files in the context of a parallel systemand found signature files to be lacking.A focus of signature file research has been methods for rearranging signatures toreduce costs. Most of these methods propose, in effect, division of the file to reduce thelength of bit vectors by a fixed ratio. Under these schemes, typically, the cost of queryresolution remains linear to collection size, and there is no amelioration of the problemof false matches. Examples include work by Bookstein and Klein 1990, Ciaccia et al.1996, Kocberbera and Can 1997, Kent et al. 1990, and Zezula et al. 1991. It is notclear that any of these schemes are of value in practice. For example, Kocberbera andCan 1997 note a problem with scaling and removed the longest documents in order toreduce false matches to predicted levels.A problem in much of the signature file work is the assumption of the independentprobability of a bit being set. See, for example, Faloutsos 1985a, 1985b who analyzesfalse drop probabilities on this assumption for different signature approaches see alsoCiaccia and Zezula 1993 and Lee and Leng 1989. SacksDavis et al. 1987, in contrast, explicitly relate the probability of a bit being set to word probabilities, whilecontinuing to assume that documents contain a fixed number of words.Nothing Is Truly New. We close our article with a salutary quote, drawn from an articlealso published in ACM Computing Surveys Severance and Carlis 1977When compared to a sequentially organized database, all of the preceding variant index organizationshave several disadvantages the software required for retrieval is much more complex file retrievalgenerally requires at least one secondary memory access for each record retrieved the space occupied bypointers constitutes a storage overhead typically, 10 to 70 of total space requirements and finally,the management and maintenance of data for a large set of lists represent a secondorder databasedesign problem whose solution will critically affect the total systems performance. Nevertheless, whenrapid retrieval of small subsets of records from a large database is an essential system requirement, oneof these file structures must be employed. The inverted list structure is generally preferred.Current research directions in inverted files often relate to evaluation methods forthe innovative similarity metrics that continue to appear. However, the core principleshave become well established. Efficient ranking requires compact inverted lists, limited accumulators, effective pruning strategies, and distribution. In combination, thesetechniques allow ranking to be applied to collections of any likely size or characteristics.ACM Computing Surveys, Vol. 38, No. 2, Article 6, Publication date July 2006.48 J. Zobel and A. MoffatACKNOWLEDGMENTSWe thank the many collaborators, assistants, and students who have contributed to our work on indexing.Jamie Callan, Bruce Croft, Donna Harman, Mike Lesk, and Ellen Voorhees helped us identify some of theearly work in the area.REFERENCESANH, V. N., DE KRETSER, O., AND MOFFAT, A. 2001. Vectorspace ranking with effective early termination. InProceedings of the 24th Annual International ACM SIGIR Conference on Research and Development inInformation Retrieval. New Orleans, LA. 3542.ANH, V. N. AND MOFFAT, A. 1998. Compressed inverted files with reduced decoding overheads. In Proceedingsof the 21st Annual International ACM SIGIR Conference on Research and Development in InformationRetrieval. Melbourne, Australia. 291298.ANH, V. N. AND MOFFAT, A. 2002. Impact transformation Effective and efficient web retrieval. In Proceedingsof the 25th Annual International ACM SIGIR Conference on Research and Development in InformationRetrieval. Tampere, Finland. 310.ANH, V. N. AND MOFFAT, A. 2005. Inverted index compression using wordaligned binary codes. KluwerInternational Journal of Information Retrieval 8, 1, 151166.ARUSU, A., CHO, J., GARCIAMOLINA, H., PAEPCKE, A., AND RAGHAVAN, S. 2001. Searching the Web. ACM Trans.Internet Technol. 1, 1, 243.BADUE, C., BAEZAYATES, R., RIBEIRONETO, B., AND ZIVIANI, N. 2001. Distributed query processing usingpartitioned inverted files. In Proceedings of String Processing and Information Retrieval Symposium,Laguna de San Rafael, Chile. G. Navarro, Ed. IEEE Computer Society, 1020.BAEZAYATES, R., MOFFAT, A., AND NAVARRO, G. 2002. Searching large text collections. In Handbook of MassiveData Sets, J. Abello, P. Pardalos, and M. Resende, Eds. Kluwer Academic Publishers, Boston, MA. 195244.BAEZAYATES, R. AND RIBEIRONETO, B. 1999. Modern Information Retrieval. ACM Press, New York, NY.BAEZAYATES, R. A. AND NAVARRO, G. 2000. Block addressing indices for approximate text retrieval. J. Amer.Soc. Inform. Science 51, 1, 6982.BARBARA, D., MEHROTRA, S., AND VALLABHANENI, P. 1996. The Gold text indexing engine. In Proceedings ofIEEE International Conference on Data Engineering, New Orleans, LA. S. Y. W. Su, Ed. IEEE ComputerSociety Press, Los Alamitos, CA. 172179.BARROSO, L. A., DEAN, J., AND HOLZLE, U. 2003. Web search for a planet The Google cluster architecture.IEEE Micro 23, 2 April, 2228.BAYER, R. AND MCCREIGHT, R. 1972. Organization and maintenance of large ordered indexes. Acta Informatica 1, 173189.BEAULIEU, M., BAEZAYATES, R., MYAENG, S. H., AND JARVELIN, K., EDS. 2002. Proceedings of the 25th Annual International ACM SIGIR Conf. on Research and Development in Information Retrieval. Tampere,Finland, ACM Press.BELL, T. C., CLEARY, J. G., AND WITTEN, I. H. 1990. Text Compression. PrenticeHall, Englewood Cliffs, NJ.BELL, T. C., MOFFAT, A., NEVILLMANNING, C. G., WITTEN, I. H., AND ZOBEL, J. 1993. Data compression infulltext retrieval systems. J. Amer. Soc. Inform. Science 44, 9 Oct., 508531.BERTINO, E., OOI, B. C., SACKSDAVIS, R., TAN, K.L., ZOBEL, J., SHIDLOVSKY, B., AND CATANIA, B. 1997. IndexingTechniques for Advanced Database Systems. Kluwer Academic Publishers, Boston, MA.BIRD, R. M., NEWSBAUM, J. B., AND TREFFTZS, J. L. 1978. Text file inversion An evaluation. In Proceedingsof the 4th Workshop on Computer Architecture for NonNumeric Processing. Blue Mountain Lake, NY,ACM Press, 4250.BIRD, R. M., TU, J. C., AND WORTHY, R. M. 1977. Associativeparallel processors for searching very largetextual data bases. In Proceedings of the 3rd NonNumeric Workshop. Syracuse, NY, ACM Press, 816.BLANDFORD, D. AND BLELLOCH, G. 2002. Index compression through document reordering. In Proceedings ofthe IEEE Data Compression Conference, Snowbird, UT, J. A. Storer and M. Cohn, Eds. IEEE ComputerSociety Press, Los Alamitos, CA. 342351.BOOKSTEIN, A. AND KLEIN, S. T. 1990. Using bitmaps for medium sized information retrieval systems. Inform.Proces. Manag. 26, 525533.BOOKSTEIN, A. AND KLEIN, S. T. 1991a. Compression of correlated bitvectors. Inform. Syst. 16, 4, 387400.ACM Computing Surveys, Vol. 38, No. 2, Article 6, Publication date July 2006.Inverted Files for Text Search Engines 49BOOKSTEIN, A. AND KLEIN, S. T. 1991b. Flexible compression for bitmap sets. In Proceedings of the IEEE DataCompression Conference, Snowbird, UT, J. Storer and M. Cohn, Eds. IEEE Computer Society Press, LosAlamitos, CA. 402410.BOOKSTEIN, A. AND KLEIN, S. T. 1991c. Generative models for bitmap sets with compression applications. InProceedings of the 14th Annual International ACM SIGIR Conference on Research and Development inInformation Retrieval, Chicago, IL. A. Bookstein, Y. Chiaramella, G. Salton, and V. V. Raghavan, Eds.ACM Press, 6371.BOOKSTEIN, A. AND KLEIN, S. T. 1992. Models of bitmap generation A systematic approach to bitmap compression. Inform. Proc. Manag. 28, 6, 735748.BOOKSTEIN, A., KLEIN, S. T., AND RAITA, T. 2000. Simple Bayesian model for bitmap compression. Kluwer Int.J. Inform. Retriev. 1, 4, 315328.BRIN, S. AND PAGE, L. 1998. The anatomy of a largescale hypertextual Web search engine. Comput. Netw.ISDN Syst. 30, 17, 107117.BRISABOA, N. R., FARINA, A., NAVARRO, G., AND ESTELLER, M. F. 2003. S, Cdense coding An optimized compression code for natural language text databases. In Proceedings of String Processing and InformationRetrieval Symposium, Manaus, Brazil, M. A. Nascimento, Ed. Springer, 122136.BROWN, E. W. 1995. Fast evaluation of structured queries for information retrieval. In Proceedings of the18th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, Seattle, WA, E. A. Fox, P. Ingwersen, and R. Fidel, Eds. ACM Press, 3038.BROWN, E. W., CALLAN, J. P., AND CROFT, W. B. 1994. Fast incremental indexing for fulltext informationretrieval. In Proceedings of the International Conference on Very Large Databases, Santiago, Chile, J. B.Bocca, M. Jarke, and C. Zaniolo, Eds. Morgan Kaufmann, 192202.BROWN, E. W., CALLAN, J. P., CROFT, W. B., ELIOT, J., AND MOSS, B. 1994. Supporting fulltext informationretrieval with a persistent object store. In Proceedings of the International Conference on Advances inDatabase Technology EDBT, Cambridge, UK, M. Jarke, J. A. B. Jr., and K. G. Jeffery, Eds. Springer,365378.BUCKLEY, C. AND LEWIT, A. F. 1985. Optimisation of inverted vector searches. In Proceedings of the 8thAnnual International ACM SIGIR Conference on Research and Development in Information Retrieval.Montreal, Canada, 97110.BURKOWSKI, F. J. 1990. Surrogate subsets a free space management strategy for the index of a text retrieval system. In Proceedings of the 13th Annual International ACM SIGIR Conference on Research andDevelopment in Information Retrieval. Brussels, Belgium, 211226.BUTTCHER, S. AND CLARKE, C. L. A. 2005. Indexing time vs. query time tradeoffs in dynamic informationretrieval systems. In Proceedings of the International Conference on Information and Knowledge Management, Bremen, Germany, A. Chowdhury, N. Fuhr, M. Ronthaler, H.J. Schek, and W. Teiken, Eds.ACM Press, 317318.CACHEDA, F., PLACHOURAS, V., AND OUNIS, I. 2004. Performance analysis of distributed architectures to index one terabyte of text. In Proceedings of the European Conference on IR Research, Sunderland, UK,S. McDonald and J. Tait, Eds. 395408. Lecture Notes in Computer Science, Springer, vol. 2997.CAHOON, B. AND MCKINLEY, K. S. 1996. Performance evaluation of a distributed architecture for informationretrieval. In Proceedings of the 19th Annual International ACM SIGIR Conference on Research andDevelopment in Information Retrieval. Zurich, Switzerland, 110118.CAHOON, B., MCKINLEY, K. S., AND LU, Z. 2000. Evaluating the performance of distributed architectures forinformation retrieval using a variety of workloads. ACM Trans. Inform. Syst. 18, 1 Jan. 143.CAN, F. 1994. On the efficiency of bestmatch cluster searches. Inform. Proc. Manag. 30, 3, 343361.CARDENAS, A. 1975. Analysis and performance of inverted data base structures. Comm. ACM 18, 5, 253263.CARMEL, D., COHEN, D., FAGIN, R., FARCHI, E., HERSCOVICI, M., MAAREK, Y. S., AND SOFFER, A. 2001. Static indexpruning for information retrieval systems. In Proceedings of the 24th Annual International Conferenceon Research and Development in Information Retrieval. New Orleans, LA. 4350.CHOUEKA, Y., FRAENKEL, A., KLEIN, S., AND SEGAL, E. 1987. Improved techniques for processing queries infulltext systems. In Proceedings of the 10th Annual International ACM SIGIR Conference on Researchand Development in Information Retrieval, New Orleans, LA, C. T. Yu and C. J. V. Rijsbergen, Eds. ACMPress, 306315.CHOUEKA, Y., FRAENKEL, A. S., AND KLEIN, S. T. 1988. Compression of concordances in fulltext retrievalsystems. In Proceedings of the 11th Annual International ACM SIGIR Conference on Research andDevelopment in Information Retrieval, Grenoble, France, Y. Chiaramella, Ed. ACM Press, 597612.CHOUEKA, Y., FRAENKEL, A. S., KLEIN, S. T., AND SEGAL, E. 1986. Improved hierarchical bitvectorcompression in document retrieval systems. In Proceedings of the 9th Annual InternationalACM Computing Surveys, Vol. 38, No. 2, Article 6, Publication date July 2006.50 J. Zobel and A. MoffatACM SIGIR Conference on Research and Development in Information Retrieval. Pisa, Italy, 8897.CIACCIA, P., TIBERIO, P., AND ZEZULA, P. 1996. Declustering of keybased partitioned signature files. ACMTrans. Datab. Syst. 21, 3 Sept., 295338.CIACCIA, P. AND ZEZULA, P. 1993. Estimating accesses in partitioned signature file organizations. ACM Trans.Inform. Syst. 11, 2, 133142.CLARKE, C. L. A. AND CORMACK, G. V. 1995. Dynamic inverted indexes for a distributed fulltext retrievalsystem. Tech. rep. MT9501, Department of Computer Science, University of Waterloo, Waterloo,Canada.CLARKE, C. L. A. AND CORMACK, G. V. 2000. Shortestsubstring retrieval and ranking. ACM Trans. Inform.Syst. 18, 1, 4478.CLARKE, C. L. A., CORMACK, G. V., AND BURKOWSKI, F. J. 1994. Fast inverted indexes with online update.Tech. rep. CS9440, Department of Computer Science, University of Waterloo, Waterloo, Canada.CLARKE, C. L. A., CORMACK, G. V., AND TUDHOPE, E. A. 2000. Relevance ranking for one to three term queries.Inform. Proc. Manage. 36, 2, 291311.COUVREUR, T. R., BENZEL, R. N., MILLER, S. F., ZEITLER, D. N., LEE, D. L., SINGHAL, M., SHIVARATRI, N., AND WONG,W. Y. P. 1994. An analysis of performance and cost factors in searching large text databases usingparallel search systems. J. Amer. Soc. Inform. Science 45, 7, 443464.CRINGEAN, J. K., ENGLAND, R., MANSON, G. A., AND WILLETT, P. 1990. Parallel text searching in serial filesusing a processor farm. In Proceedings of the 13th Annual International ACM SIGIR Conference onResearch and Development in Information Retrieval, Brussels, Belgium, 429453.CROFT, W. B., HARPER, D. J., KRAFT, D. H., AND ZOBEL, J., EDS. 2001. Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, New Orleans,LA. ACM Press.CROFT, W. B. AND LAFFERTY, J. 2003. Language Modeling for Information Retrieval. Kluwer Academic Publishers, Dordrecht, The Netherlands.CROFT, W. B., MOFFAT, A., VAN RIJSBERGEN, C. J., WILKINSON, R., AND ZOBEL, J., Eds. 1998. Proceedings ofthe 21th Annual International ACM SIGIR Conference on Research and Development in InformationRetrieval, Melbourne, Australia. ACM Press.CROFT, W. B. AND SAVINO, P. 1988. Implementing ranking strategies using text signatures. ACM Trans.Office Inform. Syst. 6, 1, 4262.CULPEPPER, J. S. AND MOFFAT, A. 2005. Enhanced byte codes with restricted prefix properties. In Proceedingsof the 12th International Symposium on String Processing and Information Retrieval. Buenos Aires,Argentina, M. P. Consens and G. Navarro, Eds. Lecture Notes in Computer Science, vol. 3772, Springer,112.CUTTING, D. AND PEDERSEN, J. 1990. Optimisations for dynamic inverted index maintenance. In Proceedingsof the ACMSIGIR International Conference on Research and Development in Information Retrieval.Brussels, Belgium, J.L. Vidick, Ed. ACM Press, 405411.DE KRETSER, O. AND MOFFAT, A. 1999. Effective document presentation with a localitybased similarityheuristic. In Proceedings of the 22nd Annual International ACM SIGIR Conference on Research andDevelopment in Information Retrieval. San Francisco, CA. 113120.DE KRETSER, O. AND MOFFAT, A. 2004. Seft A search engine for text. Softw.Prac. Exper. 34, 10 Aug.,10111023.DE KRETSER, O., MOFFAT, A., SHIMMIN, T., AND ZOBEL, J. 1998. Methodologies for distributed informationretrieval. In Proceedings of the IEEE International Conference on Distributed Computing Systems. Amsterdam, The Netherlands. M. P. Papazoglou, M. Takizawa, B. Kramer, and S. Chanson, Eds. IEEEComputer Society Press, Los Alamitos, CA. 6673.EASTMAN, C. 1983. Current practice in the evaluation of multikey search algorithms. In Proceedings ofthe 6th International ACM SIGIR Conference on Research and Development in Information Retrieval.Washington DC. J. J. Kuehn, Ed. ACM Press. 197204.EDMUNDSON, H. P. AND WYLLYS, R. E. 1961. Automatic abstracting and indexingsurvey and recommendations. Comm. ACM 4, 5 May, 226234.ELIAS, P. 1975. Universal codeword sets and representations of the integers. IEEE Trans. Inform. Theory IT21, 2 March, 194203.FALOUTSOS, C. 1985a. Access methods for text. Comput. Surv. 17, 1, 4974.FALOUTSOS, C. 1985b. Signature files Design and performance comparison of some signature extractionmethods. In Proceedings of the 8th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, Montreal, Canada, 6382.ACM Computing Surveys, Vol. 38, No. 2, Article 6, Publication date July 2006.Inverted Files for Text Search Engines 51FALOUTSOS, C. AND JAGADISH, H. V. 1992. Hybrid index organizations for text databases. In Proceedings ofthe International Conference on Extending Database Technology, Vienna, Austria, A. Pirotte, C. Delobel,and G. Gottlob, Eds. Lecture Notes in Computer Science, vol. 580, Springer, 310327.FALOUTSOS, C. AND OARD, D. W. 1995. A survey of information retrieval and filtering methods. Tech. rep.,University of Maryland Institute for Advanced Computer Studies Report, University of Maryland atCollege Park, MD.FOX, E. A. AND LEE, W. C. 1991. FASTINV A fast algorithm for building large inverted files. Tech. rep. TR9110, Virginia Polytechnic Institute and State University, Blacksburg, VA.FRAENKEL, A. S. AND KLEIN, S. T. 1985. Novel compression of sparse bitstringsPreliminary report. InCombinatorial Algorithms on Words, Volume 12, A. Apostolico and Z. Galil, Eds. NATO ASI Series F.Springer, Berlin, Germany, 169183.FRAKES, W. B. AND BAEZAYATES, R., EDS. 1992. Information Retrieval Data Structures and Algorithms.PrenticeHall, Englewood Cliffs, NJ.FREI, H.P., HARMAN, D., SCHAUBLE, P., AND WILKINSON, R., EDS. 1996. Proceedings of the 19th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, Zurich,Switzerland. ACM Press.GALLAGER, R. G. AND VAN VOORHIS, D. C. 1975. Optimal source codes for geometrically distributed integeralphabets. IEEE Trans. Inform. Theory IT21, 2 March, 228230.GARCIA, S., WILLIAMS, H. E., AND CANNANE, A. 2004. Accessordered indexes. In Proceedings of the Australasian Computer Science Conference, Dunedin, New Zealand. V. EstivillCastro, Ed. Australian Computer Society, 714.GOLOMB, S. W. 1966. Runlength encodings. IEEE Trans. Inform. Theory IT12, 3 July, 399401.GRABS, T., BOHM, K., AND SCHEK, H.J. 2001. PowerDBIR Information retrieval on top of a database cluster. In Proceedings of the CIKM International Conference on Information and Knowledge Management,Atlanta, GA. H. Paques, L. Liu, and D. Grossman, Eds. ACM Press, 411418.GROSSMAN, D. A. AND FRIEDER, O. 2004. Information Retrieval Algorithms and Heuristics, 2nd Ed. KluwerAcademic Publishers, Dordrecht, The Netherlands.HARMAN, D., MCCOY, W., TOENSE, R., AND CANDELA, G. 1991. Prototyping a distributed information retrievalsystem using statistical ranking. Inform. Proc. Manag. 27, 5, 449460.HARMAN, D. K. 1992. Ranking algorithms. In Information Retrieval Data Structures and Algorithms. W.B. Frakes and R. BaezaYates, Eds. Prentice Hall, Englewood Cliffs, NJ. 362392.HARMAN, D. K. AND CANDELA, G. 1990. Retrieving records from a gigabyte of text on a minicomputer usingstatistical ranking. J. Amer. Soc. Inform. Science 41, 8 Aug., 581589.HARPER, D. J. 1982. An evaluation of four information storage and retrieval packages. Tech. rep. 7, CSIRODivision of Computing Research, Canberra, Australia.HASKIN, R. L. 1980. Hardware for searching very large text databases. In Proceedings of the Workshop onComputer Architecture for NonNumeric Processing, Pacific Grove, CA, ACM Press, 4956.HAWKING, D. 1997. Scalable text retrieval for large digital libraries. In Proceedings of the European Conference on Research and Advanced Technology for Digital Libraries, Pisa, Italy. C. Thanos, Ed. LectureNotes in Computer Science, vol. 1324. Springer, 127145.HAWKING, D. 1998. Efficiencyeffectiveness tradeoffs in query processing. ACM SIGIR Forum 32, 2, 1622.HEAPS, H. S. 1978. Information Retrieval, Computational and Theoretical Aspects. Academic Press, NewYork, NY.HEARST, M., GEY, F., AND TONG, R., EDS. 1999. Proceedings of the 22nd Annual International ACM SIGIRConference on Research and Development in Information Retrieval, San Francisco. CA, ACM Press.HEINZ, S. AND ZOBEL, J. 2003. Efficient singlepass index construction for text databases. J. Amer. Soc.Inform. Science Techn. 54, 8, 713729.IVIE, E. L. 1966. Search procedure based on measures of relatedness between documents. Ph.D. thesis.MIT, Cambridge, MA.JAKOBSSON, M. 1978. Huffman coding in bitvector compression. Inform. Pro. Let. 7, 6 Oct. 304307.JEONG, B. S. AND OMIECINSKI, E. 1995. Inverted file partitioning schemes in multiple disk systems. IEEETran. Parall. Distrib. Syst. 6, 2, 142153.JONSSON, B. T., FRANKLIN, M. J., AND SRIVASTAVA, D. 1998. Interaction of query evaluation and buffer management for information retrieval. In Proceedings of the ACMSIGMOD International Conference on theManagement of Data, Seattle, WA. ACM Press, 118129.KASZKIEL, M., ZOBEL, J., AND SACKSDAVIS, R. 1999. Efficient passage ranking for document databases. ACMTrans. Inform. Syst. 17, 4 Oct., 406439.ACM Computing Surveys, Vol. 38, No. 2, Article 6, Publication date July 2006.52 J. Zobel and A. MoffatKENT, A. J., SACKSDAVIS, R., AND RAMAMOHANARAO, K. 1990. A signature file scheme based on multiple organisations for indexing very large text databases. J. Amer. Soc. Inform. Science 41, 7, 508534.KLEIN, S. T., BOOKSTEIN, A., AND DEERWESTER, S. 1989. Storing text retrieval systems on CDROM Compression and encryption considerations. ACM Trans. Office Inform. Syst. 7, 3, 230245.KLEINBERG, J. M. 1999. Authoritative sources in a hyperlinked environment. J. ACM 46, 5, 604632.KOBAYASHI, M. AND TAKEDA, K. 2000. Information retrieval on the web. Comput. Surv. 32, 2, 144173.KOCBERBERA, S. AND CAN, F. 1997. Vertical framing of superimposed signature files using partial evaluationof queries. Inform. Proc. Manag. 33, 3, 353376.LANCASTER, F. W. AND FAYEN, E. G. 1973. Information Retrieval OnLine. Melville, Los Angeles, CA.LEE, D. L., KIM, Y. M., AND PATEL, G. 1995. Efficient signature file methods for text retrieval. IEEE Tran.Knowl. Data Eng. 7, 3 June 423435.LEE, D. L. AND LENG, C.W. 1989. Partitioned signature files Design issues and performance evaluation.ACM Trans. Inform. Syst. 7, 2, 158180.LEE, Y. K., YOO, S. J., YOON, K., AND BERRA, P. B. 1996. Index structures for structured documents. InProceedings of the ACM Digital Libraries, Bethesda, MD, E. A. Fox and G. Marchionini, Eds. ACM Press,9199.LEMPEL, R. AND MORAN, S. 2003. Predictive caching and prefetching of query results in search engines. InProceedings of the World Wide Web Conference. Budapest, Hungary. ACM Press, 1928.LEMPEL, R. AND MORAN, S. 2004. Optimal result prefetching in web search engines with segmented indices.ACM Trans. Internet Techn. 4, 1, 3159.LEMPEL, R. AND MORAN, S. 2005. Competitive caching of query results in search engines. Theoret. Comput.Science 324, 253271.LESTER, N., MOFFAT, A., WEBBER, W., AND ZOBEL, J. 2005. Spacelimited ranked query evaluation using adaptive pruning. In Proceedings of the 6th International Conference on Web Informations Systems. LectureNotes in Computer Science, vol. 3806, Springer, 470477.LESTER, N., MOFFAT, A., AND ZOBEL, J. 2005. Fast online index construction by geometric partitioning. InProceedings of the CIKM International Conference on Information and Knowledge Management, Bremen,Germany, A. Chowdhury, N. Fuhr, M. Ronthaler, H.J. Schek, and W. Teiken, Eds. ACM Press, 776783.LESTER, N., ZOBEL, J., AND WILLIAMS, H. E. 2006. Efficient online index maintenance for text retrieval systems. Inform. Proce. Manag. To appear.LIM, L., WANG, M., PADMANABHAN, S., VITTER, J. S., AND AGARWAL, R. 2003. Dynamic maintenance of webindexes using landmarks. In Proceedings of the WorldWide Web Conference, Budapest, Hungary. Y.F. R.Chen, L. Kovacs, and S. Lawrence, Eds. ACM Press, 102111.LINOFF, G. AND STANFILL, C. 1993. Compression of indexes with full positional information in very large textdatabases. In Proceedings of the 16th Annual International ACM SIGIR Conference on Research andDevelopment in Information Retrieval, Pittsburg, PA. R. Korfhage, E. Rasmussen, and P. Willett, Eds.ACM Press, 8897.LU, Z. AND MCKINLEY, K. S. 2003. Partial collection replication for information retrieval. Kluwer Int. J.Inform. Retrie. 6, 2, 159198.LUCARELLA, D. 1988. A document retrieval system based upon nearest neighbour searching. J. Inform.Science 14, 2533.LUHN, H. P. 1957. A statistical approach to mechanised encoding and searching of library information.IBM J. Resear. Develop. 1, 309317.MACFARLANE, A., MCCANN, J. A., AND ROBERTSON, S. E. 2000. Parallel search using partitioned inverted files. In Proceedings of the String Processing and Information Retrieval Symposium. ACoruna, Spain. P. de la Fuente, Ed. IEEE Computer Society Press, Los Alamitos, CA. 209220.MACLEOD, I. A., MARTIN, T. P., NORDIN, B., AND PHILLIPS, J. R. 1987. Strategies for building distributed information retrieval systems. Inform. Proc. Manag. 23, 6, 511528.MANBER, U. AND WU, S. 1994. GLIMPSE a tool to search through entire file systems. In USENIX WinterTechnical Conference. San Francisco CA. USENIX Association, Berkeley, CA. 2332.MANNING, C. D. AND SCHUTZE, H. 1999. Foundations of Statistical Natural Language Processing. MIT PressCambridge, MA.MARON, M. E. AND KUHNS, J. L. 1960. On relevance, probabilistic indexing and information retrieval. J.ACM 7, 3, 216244.MARTIN, T. P., MACLEOD, I. A., RUSSELL, J. I., LEESE, K., AND FOSTER, B. 1990. A case study of caching strategiesfor a distributed full text retrieval system. Inform. Proc. Manag. 26, 2, 227247.ACM Computing Surveys, Vol. 38, No. 2, Article 6, Publication date July 2006.Inverted Files for Text Search Engines 53MARTIN, T. P. AND RUSSELL, J. I. 1991. Data caching strategies for distributed full text retrieval systems.Inform. Syst. 16, 1, 111.MATTHEW, F. W. AND THOMSON, L. 1967. Weighted term search a computer program for an inverted coordinate search on magnetic tape. J. Chem. Document. 7, 1, 4956.MCDONELL, K. J. 1977. An inverted index implementation. Comput. J. 20, 1, 116123.MELNIK, S., RAGHAVAN, S., YANG, B., AND GARCIAMOLINA, H. 2001. Building a distributed fulltext index forthe web. ACM Trans. Inform. Syst. 19, 3, 217241.MOFFAT, A. 1992. Economical inversion of large text files. Comput. Syst. 5, 2, 125139.MOFFAT, A. AND BELL, T. A. H. 1995. Insitu generation of compressed inverted files. J. Ame. Soci. Inform.Science 46, 7 Aug. 537550.MOFFAT, A. AND STUIVER, L. 2000. Binary interpolative coding for effective index compression. Kluwer Int.J. Inform. Retriev. 3, 1 July 2547.MOFFAT, A. AND TURPIN, A. 2002. Compression and Coding Algorithms. Kluwer Academic Publishers,Boston, MA.MOFFAT, A., WEBBER, W., ZOBEL, J., AND BAEZAYATES, R. 2005. A pipelined architecture for distributed textquery evaluation. Submitted for publication.MOFFAT, A. AND ZOBEL, J. 1992a. Coding for compression in fulltext retrieval systems. In Proceedings ofthe IEEE Data Compression Conference, Snowbird, UT, J. A. Storer and M. Cohn, Eds. IEEE ComputerSociety Press, Los Alamitos, CA, 7281.MOFFAT, A. AND ZOBEL, J. 1992b. Parameterised compression for sparse bitmaps. In Proceedings of the 5thAnnual International ACM SIGIR Conference on Research and Development in Information Retrieval,Copenhagen, Denmaaank, N. J. Belkin, P. Ingwersen, and A. M. Pejtersen, Eds. ACM Press, 274285.MOFFAT, A. AND ZOBEL, J. 1996. Selfindexing inverted files for fast text retrieval. ACM Trans. Informa.Syst. 14, 4 Oct. 349379.MOFFAT, A. AND ZOBEL, J. 2004. What does it mean to measure performance In Proceedings of the 5thInternational Conference on Web Informations Systems, Brisbane, Australia. X. Zhou, S. Su, M. P. Papazoglou, M. E. Owlowska, and K. Jeffrey, Eds. Lecture Notes in Computer Science, vol. 3306. Springer,112.MOFFAT, A., ZOBEL, J., AND SACKSDAVIS, R. 1994. Memory efficient ranking. Inform. Proc. Manag. 30, 6 Nov.733744.MOTZKIN, D. 1994. On high performance of updates within an efficient document retrieval system. Inform.Proc. Manag. 30, 1, 93118.NAVARRO, G., DE MOURA, E., NEUBERT, M., ZIVIANI, N., AND BAEZAYATES, R. 2000. Adding compression to blockaddressing inverted indexes. Kluwer Int. J. Inform. Retriev. 3, 1, 4977.NOREAULT, T., KOLL, M., AND MCGILL, M. J. 1977. Automatic ranked output from Boolean searches in SIRE.J. Amer, Soc. Inform. Science 28, 333339.PERRY, S. A. AND WILLETT, P. 1983. A review of the use of inverted files for best match searching in information retrieval systems. J. Inform. Science 6, 5966.PERSIN, M., ZOBEL, J., AND SACKSDAVIS, R. 1996. Filtered document retrieval with frequencysorted indexes.J. Amer. Soc. Inform. Science 47, 10, 749764.PONTE, J. M. AND CROFT, W. B. 1998. A language modelling approach to information retrieval. In Proceedingsof the 21st Annual International ACM SIGIR Conference on Research and Development in InformationRetrieval. Melbourne, Australia, 275281.RABITTI, F., Ed. 1986. In Proceedings of the 9th Annual International ACM SIGIR Conference on Researchand Development in Information Retrieval. Pisa, Italy, ACM Press.REDDAWAY, S. F. 1991. High speed text retrieval from large databases on a massively parallel processor.Inform. Proc. Manag. 27, 4, 311316.RIBEIRONETO, B. AND BARBOSA, R. 1998. Query performance for tightly coupled distributed digital libraries.In Proceedings of the ACM Digital Libraries, Pittsburgh, PA, I. Witten, R. Akscyn, and F. M. S. III, Eds.ACM Press, 182190.RIBERTONETO, B., DE MOURA, E. S., NEUBERT, M. S., AND ZIVIANI, N. 1999. Efficient distributed algorithms tobuild inverted files. In Proceedings of the 22nd Annual International ACM SIGIR Conference on Researchand Development in Information Retrieval. San Francisco, CA, 105112.RICE, R. F. 1979. Some practical universal noiseless coding techniques. Tech. Rep. 7922, Jet PropulsionLaboratory, Pasadena, CA.ROBERTSON, S. E. 1977. The probability ranking principle in IR. J. Document. 33, 4 Dec. 294304.ROBERTSON, S. E., WALKER, S., JONES, S., HANCOCKBEAULIEU, M. M., AND GATFORD, M. 1994. Okapi at TREC3.ACM Computing Surveys, Vol. 38, No. 2, Article 6, Publication date July 2006.54 J. Zobel and A. MoffatIn Overview of the 3rd TREC Text REtrieval Conference, Gaithersburg, MD, D. Harman, Ed. NIST, NISTSpecial Publication 500226.ROGERS, W., CANDELA, G., AND HARMAN, D. 1995. Space and time improvements for indexing in informationretrieval. In Proceedings of the Annual Symposium on Document Analysis and Information Retrieval,Las Vegas, NV, L. Spitz and D. D. Lewis, Eds.SACKSDAVIS, R., KENT, A. J., AND RAMAMOHANARAO, K. 1987. Multikey access methods based on superimposed coding techniques. ACM Trans. Datab. Syst. 12, 4, 655696.SALOMON, D. 2000. Data Compression The Complete Reference, 2nd Ed. Springer, Berlin, Germany.SALTON, G. 1962. The use of citations as an aid to automatic content analysis. Tech. Rep. ISR2, SectionIII, Harvard Computation Laboratory, Cambridge, MA.SALTON, G. 1968. Automatic Index Organization and Retrieval. McGrawHill, New York, NY.SALTON, G., Ed. 1971. The SMART Retrieval System Experiments in Automatic Document Processing.PrenticeHall, Englewood Cliffs, NJ.SALTON, G. 1972. Dynamic document processing. Comm. ACM 15, 7, 658668.SALTON, G. 1989. Automatic Text Processing The Transformation, Analysis, and Retrieval of Informationby Computer. AddisonWesley, Reading, MA.SALTON, G. AND BUCKLEY, C. 1988a. Parallel text search methods. Comm. ACM 31, 2 Feb. 202215.SALTON, G. AND BUCKLEY, C. 1988b. Termweighting approaches in automatic text retrieval. Inform. Proc.Manag. 24, 5, 513523.SALTON, G. AND MCGILL, M. J. 1983. Introduction to Modern Information Retrieval. McGrawHill, New York,NY.SALTON, G., WONG, A., AND WANG, C. S. 1975. A vector space model for automatic indexing. Comm. ACM 18, 11Nov. 613620.SARAIVA, P. C., DE MOURA, E. S., ZIVIANI, N., FONSECA, R., MEIRA, W., MURTA, C., AND RIBEIRONETO, B. 2001.Rankpreserving twolevel caching for scalable search engines. In Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. New Orleans,LA. 5158.SAYOOD, K. 2000. Introduction to Data Compression 2nd Ed. Morgan Kaufmann, San Francisco, CA.SCHOLER, F., WILLIAMS, H. E., YIANNIS, J., AND ZOBEL, J. 2002. Compression of inverted indexes for fast queryevaluation. In Proceedings of the 25th Annual International ACM SIGIR Conference on Research andDevelopment in Information Retrieval. Tampere, Finland. 222229.SCHUEGRAF, E. J. 1976. Compression of large inverted files with hyperbolic term distribution. Inform. Proc.Manag. 12, 377384.SEGESTA, J. AND REIDGREEN, K. 2002. Harley Tillitt and computerized library searching. IEEE Ann. HistoryComput. 24, 3 Sept. 2334.SEVERANCE, D. G. AND CARLIS, J. V. 1977. A practical approach to selecting record access paths. Comput.Surv. 9, 4, 259272.SHIEH, W.Y., CHEN, T.F., AND CHUNG, C.P. 2003. A treebased inverted file for fast rankeddocument retrieval. In Proceedings of the International Conference on Information and Knowledge Engineering. LasVegas, NV. H. R. Arabnia, Ed. CSREA Press, 6469.SHIEH, W.Y., CHEN, T.F., SHANN, J. J.J., AND CHUNG, C.P. 2003. Inverted file compression through documentidentifier reassignment. Inform. Proc. Manag. 39, 1, 117131.SHIEH, W.Y. AND CHUNG, C.P. 2005. A statisticsbased approach to incrementally update inverted files.Inform. Proc. Manag. 41, 2, 275288.SHIEH, W.Y., SHANN, J. J.J., AND CHUNG, C.P. 2003. An inverted file cache for fast information retrieval. J.Inform. Science Eng. 19, 4, 681695.SHOENS, K., TOMASIC, A., AND GARCAMOLINA, H. 1994. Synthetic workload performance analysis of incremental updates. In Proceedings of the 17th Annual International ACM SIGIR Conference on Researchand Development in Information Retrieval. Dublin, Ireland, W. B. Croft and C. J. van Rijsbergen, Eds.ACM Press, 329338.SILVESTRI, F., ORLANDO, S., AND PEREGO, R. 2004. Assigning identifiers to documents to enhance the clusteringproperty of fulltext indexes. In Proceedings of the 27th Annual International ACM SIGIR Conference onResearch and Development in Information Retrieval. Sheffield, England, M. Sanderson, K. Jarvelin,J. Allan, and P. Bruza, Eds. ACM Press, 305312.SINGHAL, A., BUCKLEY, C., AND MITRA, M. 1996. Pivoted document length normalization. In Proceedings ofthe 19th Annual International ACM SIGIR Conference on Research and Development in InformationRetrieval. Zurich, Switzerland, 2129.ACM Computing Surveys, Vol. 38, No. 2, Article 6, Publication date July 2006.Inverted Files for Text Search Engines 55SMEATON, A. AND VAN RIJSBERGEN, C. J. 1981. The nearest neighbour problem in information retrieval. InProceedings of the 4th Annual International ACM SIGIR Conference on Research and Development inInformation Retrieval. Oakland, CA, C. J. Crouch, Ed. ACM Press, 8387.SPARCK JONES, K., WALKER, S., AND ROBERTSON, S. E. 2000. A probabilistic model of information retrievaldevelopment and comparative experiments. parts 12. Inform. Proc. Manag. 36, 6, 779840.SPARCK JONES, K. AND WILLETT, P., Eds. 1997. Readings in Information Retrieval. Academic PressMorganKaufmann, San Francisco, CA.SPINK, A., WOLFRAM, D., JANSEN, B. J., AND SARACEVIC, T. 2001. Searching the Web The public and theirqueries. J. Amer. Soci. Inform. Science 52, 3, 226234.SPINK, A. AND XU, J. L. 2000. Selected results from a large study of web searching The Excite study. Inform.Resear.Int. Electron. J. 6, 1.STANFILL, C. 1990. Partitioned posting files a parallel inverted file structure for information retrieval. InProceedings of the 13th Annual International ACM SIGIR Conference on Research and Development inInformation Retrieval. Brussels, Belgium. 413428.STANFILL, C., THAU, R., AND WALTZ, D. 1989. A parallel indexed algorithm for information retrieval. In Proceedings of the 12th Annual International ACM SIGIR Conference on Research and Development inInformation Retrieval. Cambridge, MA, , N. J. Belkin and C. J. van Rijsbergen, Eds. ACM Press, 8897.STELLHORN, W. H. 1977. An inverted file processor for information retrieval. IEEE Trans. Comput. 26, 12,12581267.STROHMAN, T., TURTLE, H., AND CROFT, W. B. 2005. Optimization strategies for complex queries. In Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. Salvador, Brazil, G. Marchionini, A. Moffat, J. Tate, R. BaezaYates, and N. Ziviani,Eds. ACM Press, 219225.TAGUE, J., Ed. 1985. Proceedings of the 8th Annual International ACM SIGIR Conference on Research andDevelopment in Information Retrieval. Montreal, Canada, ACM Press.TEUHOLA, J. 1978. A compression method for clustered bitvectors. Inform. Proc. Lett. 7, 6 Oct. 308311.TOMASIC, A. AND GARCAMOLINA, H. 1993. Performance of inverted indices in sharednothing distributedtext document information retrieval systems. In Proceedings of the International Conference on Paralleland Distributed Information Systems. San Diego, CA, M. J. Carey and P. Valduriez, Eds. IEEE ComputerSociety Press, 817.TOMASIC, A. AND GARCAMOLINA, H. 1996. Performance issues in distributed sharednothing informationretrieval systems. Inform. Proc. Manag. 32, 6, 647665.TOMASIC, A., GARCAMOLINA, H., AND SHOENS, K. 1994. Incremental updates of inverted lists for text document retrieval. In Proceedings of the ACMSIGMOD International Conference on the Management ofData. Minneapolis, MA, R. T. Snodgrass and M. Winslett, Eds. ACM Press, 289300.TROTMAN, A. 2003. Compressing inverted files. Kluwer Int. J. Inform. Retriev. 6, 519.TURTLE, H. AND FLOOD, J. 1995. Query evaluation strategies and optimizations. Inform. Proc. Manag. 31, 1Nov., 831850.VAN RIJSBERGEN, C. J. 1979. Informat. Retriev., 2nd Ed. Butterworths, London, UK.VASANTHAKUMAR, S. R., CALLAN, J. P., AND CROFT, W. B. 1996. Integrating INQUERY with an RDBMS tosupport text retrieval. Bull. Techn. Comm. Data Eng. 19, 1, 2433.VIDICK, J. L., Ed. 1990. Proceedings of the 13th Annual International ACM SIGIR Conference on Researchand Development in Information Retrieval. Brussels, Belgium. ACM Press,VOORHEES, E. M. 1986. The efficiency of inverted index and cluster searches. In Proceedings of the 9thAnnual ACM SIGIR Conference on Research and Development in Information Retrieval. Pisa, Italy. 164174.VOORHEES, E. M. AND HARMAN, D. K. 2005. TREC Experiment and evaluation in information retrieval. MITPress, Cambridge, MA.WILLIAMS, H. E. AND ZOBEL, J. 1999. Compressing integers for fast file access. Comput. J. 42, 3, 193201.WILLIAMS, H. E., ZOBEL, J., AND ANDERSON, P. 1999. Whats next Index structures for efficient phrase querying. In Proceedings of the Australasian Database Conference. Auckland, New Zealand. M. Orlowska, Ed.Australian Computer Society, 141152.WILLIAMS, H. E., ZOBEL, J., AND BAHLE, D. 2004. Fast phrase querying with combined indexes. ACM Trans.Inform. Syst. 22, 4, 573594.WITTEN, I. H., BELL, T. C., AND NEVILL, C. G. 1991. Models for compression in fulltext retrieval systems. InProceedings of the IEEE Data Compression Conference. Snowbird, UT, J. A. Storer and J. H. Reif, Eds.IEEE Computer Society Press, Los Alamitos, CA. 2332.ACM Computing Surveys, Vol. 38, No. 2, Article 6, Publication date July 2006.56 J. Zobel and A. MoffatWITTEN, I. H., MOFFAT, A., AND BELL, T. C. 1999. Managing Gigabytes Compressing and Indexing Documentsand Images, 2nd Ed. Morgan Kaufmann, San Francisco, CA.WONG, W. Y. P. AND LEE, D. K. 1993. Implementations of partial document ranking using inverted files. Inform.Proc. Manag. 29, 5 Sept., 647669.XI, W., SORNIL, O., AND FOX, E. A. 2002a. Hybrid partition inverted files for largescale digital libraries. InProceedings of the Digital Library IT Opportunities and Challenges in the New Millennium. BeijingLibrary Press, Beijing, China, 404418.XI, W., SORNIL, O., LUO, M., AND FOX, E. A. 2002b. Hybrid partition inverted files Experimental validation.In Proceedings of the European Conference on Research and Advanced Technology for Digital Libraries.Rome, Italy, M. Agosti and C. Thanos, Eds. Lecture Notes in Computer Science, vol. 2458, Springer,422413.ZEZULA, P., RABITTI, F., AND TIBERIO, P. 1991. Dynamic partitioning of signature files. ACM Tran. Inform.Syst. 9, 4 Oct., 336369.ZOBEL, J. AND MOFFAT, A. 1998. Exploring the similarity space. SIGIR Forum 32, 1, 1834.ZOBEL, J., MOFFAT, A., AND RAMAMOHANARAO, K. 1996. Guidelines for presentation and comparison of indexingtechniques. SIGMOD Record 25, 3 Oct., 1015.ZOBEL, J., MOFFAT, A., AND RAMAMOHANARAO, K. 1998. Inverted files versus signature files for text indexing.ACM Trans. Datab. Syst. 23, 4 Dec., 453490.ZOBEL, J., MOFFAT, A., AND SACKSDAVIS, R. 1992. An efficient indexing technique for fulltext database systems. In Proc. VLDB Int. Conf. on Very Large Databases, L.Y. Yuan, Ed. Morgan Kaufmann, Vancouver,352362.ZOBEL, J., MOFFAT, A., AND SACKSDAVIS, R. 1993a. Searching large lexicons for partially specified termsusing compressed inverted files. In Proceedings of the International Conference on Very Large Databases.Dublin, Ireland, R. Agrawal, S. Baker, and D. Bell, Eds. Morgan Kaufmann, 290301.ZOBEL, J., MOFFAT, A., AND SACKSDAVIS, R. 1993b. Storage management for files of dynamic records. InProceedings of the Australasian Database Conference. Brisbane, Australia, 2638.ZOBEL, J., MOFFAT, A., WILKINSON, R., AND SACKSDAVIS, R. 1995. Efficient retrieval of partial documents.Inform. Proc. Manag. 31, 3, 361377.Received September 2004 revised November 2005 accepted March 2006ACM Computing Surveys, Vol. 38, No. 2, Article 6, Publication date July 2006.
