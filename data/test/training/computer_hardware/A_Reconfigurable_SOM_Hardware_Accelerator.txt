A Reconfigurable SOM Hardware AcceleratorM. Porrmann, M. Franzmeier, H. Kalte, U. Witkowski, U. RckertHeinz Nixdorf Institute, System and Circuit Technology, University of Paderborn, Germanyemail porrmannhni.upb.de, WWW httpwwwhni.upb.desctAbstract. A dynamically reconfigurable hardware accelerator for selforganizing feature maps is presented. The system is based on the universal rapid prototyping system RAPTOR2000 that has been developed by the authors. Themodular prototyping system is based on XILINX FPGAs and is capable of emulating hardware implementations with a complexity of more than 24 million system gates. RAPTOR2000 is linked to its host  a standard personal computer orworkstation  via the PCI bus. For the simulation of selforganizing maps a module has been designed for the RAPTOR2000 system, that embodies an FPGA ofthe Xilinx Virtex series and optionally up to 128 MBytes of SDRAM. A speedup of about 50 is achieved with five FPGA modules on the RAPTOR2000 system compared to a software implementation on a state of the art personal computer for typical applications of selforganizing maps.1. IntroductionSelforganizing maps SOMs 1 are successfully used for a wide range of technicalapplications, in particular, dealing with noisy or incomplete data. Examples of use arecontrolling tasks, data analysis and pattern matching. In cooperation with an industrialpartner we are using SOMs for the analysis of IC Integrated Circuits fabrication processes. The large amount of data, that is captured during operation, has to be analyzedin order to optimize the process and to avoid a decrease of yield 2, 3. Software simulations of medium sized maps on state of the art workstations require calculation timesfrom hours up to several days for these large datasets. The simulation of large mapsi.e. more than one million neurons with vector dimensions in the order of thousandsseems promissing but is not feasible with state of the art PCs or workstations. From thevarious possibilities to speed up neural computations we have chosen the design of ahardware accelerator for neural netwoks. Our goal is to integrate the system into stateof the art workstations if very high performance is required and to enable access to theaccelerator via the internet if the accelerator is only sporadically used.In recent years various hardware implementations for different neural network architectures have been presented 4. But many of the proposed architectures are dedicatedto single neural network algorithms or groups of similar algorithms. The aim of ourproject is to deliver a system that is capable of accelerating a wide range of differentneural algorithms. In most applications different methods of information processingare combined. For example, artificial neural networks are combined with fuzzy logicor with techniques for knowledge based information processing. In contrast to implementing different components for data pre and postprocessing and for neural networks we use a dynamically i.e. during runtime configurable hardware accelerator toimplement all of the algorithms that are required for a special application. The systemESANN2002 proceedings  European Symposium on Artificial Neural NetworksBruges Belgium, 2426 April 2002, dside publi., ISBN 2930307021, pp. 337342can be reconfigured for the different tasks in one application e.g. different configurations for pre and postprocessing may be selected. Because of the reconfigurabilitythe hardware can be mapped optimally to the requirements of the application, thusallowing an easy expansion by new algorithms to improve flexibility and performance. 2. Implementing Selforganizing Maps in HardwareSelforganizing maps as proposed by Kohonen 1 use an unsupervised learning algorithm to form a nonlinear mapping from a given high dimensional input space to alower dimensional in most cases twodimensional map of neurons. Our goal is tofind an efficient implementation on state of the art FPGAs that, on the one hand delivers the required high performance and, on the other hand, fits into the limited ressources of current FPGAs. Because of their internal structure FPGAs seem to be very wellsuited for the implementation of neural networks. Previous work of the authors concerning highly optimized ASIC implementations of selforganizing maps has emphasized, that avoiding memory bottlenecks by using onchip memory is a must in orderto achieve optimal performance 5. We use XILINX Virtex FPGAs for our implementations, because these devices come with large internal SRAM blocks that can be usedfor internal weight storage. In oder to limit the hardware requirements for the implementation, the original SOMalgorithm has been simplified. In particular the Manhattan distance is used for calculating the distance between the input vector and the model vectors to avoid multiplications as required for the euclidean distance which is typically used in SOM implementations. The internal precision is set to 16 bit and the accuracy of the input vectorcomponents and of the model vectors is set to eight bit. Restricting the values of theneighborhood function to negative powers of two gives us the opportunity to replacethe multiplications that are required for adaptation by shift operations. Of cause thesesimplifications do not come for free e.g. the convergence time may increase in somecases, but it has been shown that the simplified algorithm is well suited for a lot ofapplications 6. Furthermore the actual generation of Xilinx FPGAs Virtes II comeswith integrated multipliers and our implementations on these chips will thus be able touse euclidean distance instead of manhattan distance with no loss in performance. Data pre and postprocessing is a crucial and often ignored aspect in neurocomputerdesign. The use of reconfigurable hardware enables us to implement optimally fittinghardware implementations  not only for neural networks but also for pre and postprocessing. As an example, we have integrated the main visualization techniques for selforganizing maps, that had to be performed in software so far, into hardware. The visualization of component maps and pattern position maps is supported as well as all kindof distance matrices like the UMatrix. Implementing these algorithms in hardwaredramatically reduces communication and thus enables a more efficient utilization ofthe hardware accelerator 7.Our SOMimplementation consists of processing elements that are working in SIMDmanner and that are controlled by an external controller. Nearly all calculations areperformed in parallel on all processing elements. A bidirectional bus is used for datatransfers to dedicated elements and for broadcasting data to groups of processor elements or to all processor elements. Single elements and groups of elements areaddressed by row and column lines that are connected to the twodimensional matrix.The externally generated instructions are transferred to the processor elements via anadditional control bus. Two more signals are used for status messages from and to theESANN2002 proceedings  European Symposium on Artificial Neural NetworksBruges Belgium, 2426 April 2002, dside publi., ISBN 2930307021, pp. 337342controller. The architecture is able to simulate virtual maps, i.e. it is possible to simulate maps that are larger than the array of processor elements that is implemented.Apart from the typical two dimensional grid of neurons, any other kind of networktopology can be implemented e.g. one dimensional or toroidal maps. Because thevalues for the adaptation factors are provided by an external controller, any adaptationfunction and neighborhood function may be realized with the proposed hardwarewithout any changes in the FPGA configuration.3. Architecture of the Hardware AcceleratorThe hardware accelerator that is presented in this paper is based on the modular rapidprototyping system RAPTOR2000. The system consists of a motherboard and up to sixapplication specific modules ASMs. Basically, the motherboard provides the communication infrastructure between the ASMs and links the RAPTOR2000 system viaPCI bus to a host computer. Additionally, management functions like bus arbitration,memory management and error detection services are integrated in two Complex Programmable Logic Devices CPLD. The various communication schemes that can beused between different ASMs and between the ASMs and the host computer aredepicted in the block diagram in figure 1. Every ASM slot is connected to the LocalBus for internal communication with other devices or ASMs and for external communication with the host processor or with other PCI bus devices. An additional Broadcast Bus can be used for simultaneous communication between the ASMs. Additionally, a dual port SRAM can be accessed by all ASMs via the Broadcast Bus e.g. utilized as a buffer for fast direct memory accesses to the main memory of the host system. Direct communication between adjacent ASMs is realized by 128 signals thatcan be variably used, depending on the actual implementation.A crucial aspect concerning FPGA designs is the configuration of the devices. EachASM that carries an FPGA has to be configured by an application specific data streamthat determines the function of the device. In order to utilize dynamic reconfigurationi.e. during runtime it is necessary to minimize this reconfiguration time, therefor theconfiguration algorithms have been implemented in hardware. Reconfiguration of anASM can be started by the host computer, another PCI bus device or by another ASM.Module 4CTRL CPLDArbiter, MMUDiagnose, CLKConfig. CPLDPCI, JTAGPCI Bus PCI BusBridgeMaster, Slave,DMALocal BusFlash ROMDualPortSRAM85CTRL 85CTRL85CTRL128Module 6Configuration,JTAGConfiguration,JTAGConfiguration,JTAGModule 1128Module 2128Module 312875 75 75Broadcast BusFigure 1. Architecture of the Raptor2000 rapid prototyping systemESANN2002 proceedings  European Symposium on Artificial Neural NetworksBruges Belgium, 2426 April 2002, dside publi., ISBN 2930307021, pp. 337342Thus, it is possible that an FPGA autonomously reconfigures itself by configurationdata that is located anywhere in the system. Due to the hardware implementation of thereconfiguration algorithm, a Xilinx Virtex 1000 FPGA can be completely reconfiguredwithin less than 20 ms. The configuration algorithm implemented into the hardwarealso supports the partial reconfiguration of the system 8. For the simulation of selforganizing feature maps the module DBVS has beendesigned for the RAPTOR2000 system. This ASM embodies an FPGA of the XilinxVirtex E series and optionally up to 128 MBytes of SDRAM. The ASM can beequipped with various chips, that emulate circuits with a complexity of 400,000 to 4million system gates. The SDRAM controller is integrated into the FPGA logic. Aphoto of the RAPTOR2000 system with two DBVS modules is shown in figure 2. Inthe context of this paper we focus on the implementation of selforganizing featuremaps on RAPTOR2000. Because of the flexibility of the system many other neural andconventional algorithms may be mapped to the system. As another example for neuralnetworks we have analyzed the implementation of neural associative memories on theRAPTOR2000 system 9. Another case study focuses on the implementation of octreebased 3D graphics 10.4. SOMImplementation on RAPTOR2000In order to implement the SOMalgorithm on the RAPTOR2000 rapid prototyping system, five DBVS modules are applied. Four ASMs are used to implement a matrix ofprocessing elements while the fifth is used to implement the matrix controller, an IOcontroller for the connection to the local bus of the RAPTOR2000 system and a dualport SRAM that is used to buffer input and outputdata. An integrated SDRAM interface controls the external 128 MBytes of SDRAM. The dual port SRAM is used tostore single input vectors, commands from the host computer and the results e.g. bestmatch positions or postprocessed visualizations of the map. The large SDRAM isused to store one or more input data sets. During learning of a selforganizing map, theFigure 2. Photo of the RAPTOR2000 System with the FPGA module DBVSESANN2002 proceedings  European Symposium on Artificial Neural NetworksBruges Belgium, 2426 April 2002, dside publi., ISBN 2930307021, pp. 337342whole input data set has to be presented to the map for several times. Thus it is recommendable to transfer the whole data set in one fast block transfer via the PCI bus to thehardware accelerator in order to minimize the load of the PCI bus. The design has been described in VHDL and synthesized using the Synopsys FPGAcompiler and the Xilinx Alliance tools for place and route. Using Virtex XCV812E6devices, NPE81 processing elements can be implemented, each equipped with 2kBytes of internal SRAM. The utilization of the FPGAs is about 68. The numbers ofclock cycles that are required for recall crecall and for learning cadapt of an inputvector with a dimension of l are1The variable nv is the number of neurons, that is emulated by one processing element.Using Virtex XCV812E6 devices, a clock frequency of 65 MHz has been achieved forthe FPGAs. The maximum performance is achieved, if every processing element represents one neuron nv1. In this case about 17,500 MCPS Million Connections perSecond can be achieved during recall and 2000 MCUPS during learning. Simulatinglarger maps, the performance decreases. Using a benchmark data set with a vectordimension of l9, maps with up to 250250 Neurons can be simulated with a performance of 4,400 MCPS and 665 MCUPS, respectively. With a software implementationon a state of the art personal computer AMD Athlon, 1 GHz only a performance of85 MCPS and 22 MCUPS can be achieved for this problem. Apart from the describedXilinx XCV812E devices, other FPGAs with BG560 package may be used on DBVS.Xilinx Virtex devices may be used as well as Virtex E devices. The design has beensynthesized to different Xilinx devices leading to a maximum performance of morethan 50 GCPS and 5,7 GCUPS on Xilinx XCV3200E6 devices. The dynamicallyreconfiguration that is provided by the RAPTOR2000 system can be used to adapt thesize of the map that is simulated in order to achieve optimal performance. Additionally, dynamic reconfiguration is used to adapt the precision of the processing elements.Starting with a low precision e.g. 8 bit, a rough ordering of the map can be achieved.For fine tuning of the map the precision of the processing elements is increased e.g. to16 bit, by loading a new configuration file.ControlFPGADPRAMMatrixControllerIOControllerSDRAMController128 MByteSDRAM PEMatrix 4  Xilinx VirtexRAPTOR2000  LocalBusFigure 3. Architecture of the hardware acceleratorcrecall nv l 2 ld l 255  4  cadapt nv 2l 2 ld l 255  12  ESANN2002 proceedings  European Symposium on Artificial Neural NetworksBruges Belgium, 2426 April 2002, dside publi., ISBN 2930307021, pp. 3373425. ConclusionA dynamically reconfigurable hardware accelerator for the simulation of selforganizing feature maps has been presented. Equipped with five FPGA modules, the systemachieves a maximum performance of more than 50 GCPS Giga Connections per Second during recall and more than 5 GCUPS Giga Connection Updates per Secondduring learning. Even higher performance numbers can be achieved by using newFPGA architectures like the Xilinx VirtexE series. Apart from the high performancethe system is capable of doing pre and postprocessing tasks  either by use of theimplemented visualization features or by dynamically reconfiguring the devices duringruntime. The latter is supported by the RAPTOR2000 rapid prototyping system bymeans of the ability to reconfigure the FPGAs very fast via the PCI bus.AcknowledgementThis work has been partly supported by the Deutsche Forschungsgemeinschaft German Research Council DFG Graduate Center Parallele Rechnernetzwerke in derProduktionstechnik and by the Robert Bosch GmbH, Automotive Equipment Division 8, Reutlingen, Germany.References1 Kohonen, T. SelfOrganizing Maps, SpringerVerlag, Berlin, 1995.2 Goser, K., Marks, K.M., Rckert, U., Selbstorganisierende Parameterkarten zurProzeberwachung und voraussage, InformatikFachberichte Nr. 227, 1989, pp. 225237, Springer, Mnchen.3 Rping, S., Mller, J. Analysis of IC Fabrication Processing using SelfOrganizingMaps, Proc. of ICANN99, Edinburgh, 7.10. Sept. 1999, S. 631636.4 Rckert, U. ULSI Implementations for Artificial Neural Networks, 9th Euromicro Workshop on Parallel and Distr. Processing 2001, Feb. 79, 2001, Mantova, Italien, S.436442.5 Porrmann, M., Rping, S., Rckert, U. The Impact of Communication on HardwareAccelerators for Neural Networks, Proc. of SCI 2001 Orlando, Floriada USA, 22.25. Juli2001, pp. 248253.6 Rping, S., Porrmann, M., Rckert, U., SOM Accelerator System, Neurocomputing 21,pp. 3150, 1998 .7 Porrmann, M., Rping, S., Rckert, U., SOM Hardware with Acceleration Module forGraphical Representation of the Learning Process, Proc. of the 7th Int. Conference onMicroelectronics for Neural, Fuzzy and BioInspired Systems, pp. 380386, Granada, 1999.8 Porrmann, M., Kalte, H., Witkowski, U., Niemann, J.C., Rckert, U. A DynamicallyReconfigurable Hardware Accelerator for SelfOrganizing Feature Maps, Proc. of SCI2001 Orlando, Florida USA, 22.25. Juli, 2001, pp. 242247.9 Porrmann, M., Witkowski, U., Kalte, H., Rckert, U. Implementation of Artificial NeuralNetworks on a Reconfigurable Hardware Accelerator, 10th Euromicro Workshop on Parallel, Distributed and Networkbased Processing PDP 2002, 9.11. Januar 2002, GranCanaria Island, Spain, to be published.10 Kalte, H., Porrmann, M., Rckert, U., Using a Dynamically Reconfigurable System toAccelerate Octree Based 3D Graphics, PDPTA2000, June 2629, 2000 Monte CarloResort, Las Vegas, Nevada, USA, pp. 28192824ESANN2002 proceedings  European Symposium on Artificial Neural NetworksBruges Belgium, 2426 April 2002, dside publi., ISBN 2930307021, pp. 337342
