In Proceedings of the8th International Workshop on Field Programmable Logic and Applications FPL98,pages 6978, Tallinn, Estonia, SpringerVerlag, 1998.Acceleration of Satisfiability Algorithms byReconfigurable HardwareMarco Platzner and Giovanni De MicheliComputer Systems Laboratory, Stanford UniversityStanford, CA 94305, U.S.A.marco.platznercomputer.orgAbstract. We present different architectures to solve Boolean satisfiability problems in instancespecific hardware. A simulation of these architectures shows that for examples from the DIMACS benchmark suite,high raw speedups over software can be achieved. We present a designtool flow and prototype implementation of an instancespecific satisfiability solver and discuss experimental results. We measure the overallspeedup of the instancespecific architecture that takes the hardwarecompilation time into account. The results prove that many of the DIMACS examples can be accelerated with current FPGA technology.1 IntroductionThe Boolean satisfiability problem SAT is a fundamental problem in mathematical logic and computing theory with many practical applications in areas suchas computeraided design of digital systems, automated reasoning, and machinevision. In computeraided design, tools for synthesis, optimization, verification,timing analysis, and test pattern generation use variants of SAT solvers as corealgorithms. The SAT problem is commonly defined as follows 1 Given a set of n Boolean variables x1, x2, . . . , xn, a set of literals, where a literal is a variable xi or the complement of a variablexi, and a set of m distinctive clauses C1, C2, . . . , Cm, where each clause consists ofliterals combined by the logical or connective ,determine, whether there exists an assignment of truth values to the variablesthat makes the Conjunctive Normal Form CNFC1  C2  . . .  Cm 1true, where  denotes the logical and connective.Since the general SAT problem is NPcomplete, exact methods to solve SATproblems show an exponential worstcase runtime complexity. This limits theapplicability of exact SAT solvers in many areas. Heuristics can be used to findsolutions faster, but they may fail to prove satisfiability.The SAT problem is a discrete, constrained decision problem 1. A straightforward but inefficient procedure to solve it exactly is to enumerate all possibletruth value assignments and check if one satisfies the CNF. Many of the improved techniques that have been proposed to solve SAT problems eliminate onevariable from the CNF at a time. There are two basic methods splitting andresolution. Resolution was implemented in the original DavisPutnam DP algorithm 2. Splitting was used first in Lovelands modification to DP, the DPLalgorithm 3. In splitting, a variable is selected from the CNF and two subCNFsare generated by setting the variable to 0 and 1, respectively. The iterative application of splitting generates a search tree a leaf of the tree denotes a fullassignment of values to variables. Most practical SAT solvers use the splittingtechnique and combine it with backtracking. Backtracking searches the searchtree in a depthfirst order and thus avoids excessive memory requirements.A general template for backtracking SAT solvers is described in 4 and includes 3 steps decision, deduction, and diagnosis. In the decision step, a variableis selected for the next assignment. In the deduction step, information is inferredfrom the current partial assignment. This information is then used to guide thesearch process, e.g., to prune the search tree. If the current partial assignmentleads to a contradiction, a diagnosis step can be used to analyze this situationand to avoid running into the same contradiction in future.Existing software SAT solvers use a wide variety of backtracking methodsand strategies for decision, deduction, and diagnosis. GRASP 4 is a sophisticated SAT solver that implements all steps of the described template. We useGRASP as software reference system in our work. The powerful strategies thatare implemented by sophisticated SAT solvers reduce the number of variableassignments required to find a solution or to prove that there is no solution.However, these strategies can be computationally very expensive.The goal of our work is to speed up exact SAT solvers by exploiting the finegrain parallelism in the SAT problem instances. For each new problem instanceCNF, a new hardware is generated that reflects the particular structure of theCNF. This class of hardware architectures is called instancespecific and relys onfinegrained reconfigurable computing structures, e.g., FPGAs. InstancespecificSAT solvers use less powerful strategies than software solvers for decision anddeduction diagnosis methods in hardware have not been reported at all. Theadvantage of SAT in hardware is that the deduction step can be implementedvery fast. This is because many deduction strategies operate on values in 2, 3,or 4valued logic and show large amounts of finegrained parallelism. This makesfinegrained parallel computing structures, such as FPGAs, an optimal target.2 Related WorkIn this section, we mention related projects that apply reconfigurable hardwareto solve the SAT problem. Zhong et al. 5 6 described an instancespecificarchitecture to solve SAT problems that uses Boolean constraint propagation asdeduction strategy and models the variables in 4valued logic. They simulatedtheir architecture and reported speedups in the order of several magnitudes forthe DIMACS benchmarks 7. Their prototype translates a SAT problem into alogic description in VHDL. This description is partitioned and mapped onto anarray of Xilinx XC4K FPGAs by an IKOS logic emulation system. Suyama et al.8 proposed an architecture for SAT that combines a forward checking techniquewith nonchronological backtracking. They model the variables in 2valued logic.Their tool flow also targets the Xilinx XC4K line.In 9, an instancespecific architecture for SAT problems in arbitrary Booleanexpressions was presented. This architecture uses a strategy similar to the PODEM algorithm for automatic test pattern generation. The same architectureis used in 10, with an emphasis on a fast hardware compilation. To addressthis issue, the use of Xilinx XC62xx FPGAs is proposed which allows to developSATspecific tools for synthesis, partition, placement, and routing.3 Hardware ArchitecturesThe basic architecture for backtracking search is shown in Figure 1 and consistsof three blocks i an array of finite state machines FSMs, ii a datapath,and iii a global controller. Each variable of the CNF corresponds to one FSM.The FSMs are connected in a onedimensional array each FSM can activateits two neighboring FSMs at the top and at the bottom. The datapath is acombinational circuit that takes the variables as input and computes outputsthat are fed back to the FSMs. The global controller starts the computationand handles IO communication. All the architectures presented in this sectionconsist of these three blocks. However, they differ in the modeling of the variablesand the used deduction strategy, which is reflected in the actual implementationof the datapath and the FSM.3.1 Architecture CECE CNF evaluation models the variables in 3valued logic. A variable can takeon the values 0, 1, X, where X denotes an unassigned variable. The datapathcomputes the 3valued result of the CNF expression. Initially, all variables areunassigned which also leads to CNF value X , and the global controller activatesthe topmost FSM. The state diagram for an FSM is shown in Figure 2. Anactivated FSM assigns 0 to its variable and checks the resulting CNF value.If the CNF value is 1, the partial assignment already satisfied the CNF andthe computation stops. If the CNF value is 0, the partial assignment made theCNF unsatisfiable. In this case, the FSM assigns the complementary value to itsvariable. If the CNF value is X , the partial assignment did neither satisfy theCNF nor did it make the CNF unsatisfiable. In this case, the FSM activates thenext FSM at the bottom. If both value assignments have been tried, the FSMrelaxes its variable by assigning X to it, and activates the previous FSM at thetop. When the first FSM relaxes its variable and activates the global controller,the SAT problem is proven to be unsatisfiable. By this procedure, the array ofinterconnected FSMs implements chronological backtracking.x2x3xnx1222221n32host interfaceCNFglobal controllerdatapath Fig. 1. Block diagram for the basic architecture CE, consisting of an array of FSMs1 . . . n, a datapath, and a global controller. The variables xi and the CNF aremodeled in 3valued logic.true0, 0, 0initFB FBCNFL CNFLFTCNFX1, 0, 0 X, 1, 0X, 0, 01, 0, 10, 0, 1CNFXFig. 2. State diagram for an FSM of the architecture CE. The inputs are FT fromtop and FB from bottom that activate the FSM, and the 3valued CNF. The outputsignals displayed inside the states are the variable value, and the signals TT to topand TB to bottom that activate the previous and next FSM.3.2 Architecture CEDCCEDC CE  dont cares extends CE by introducing dont care variables.Dont care variables are unassigned variables that appear only in clauses thatare already satisfied. For example, the assignment x1  1 for the CNFx1  x2  x1  x3  x4  x2  x4 2makes x3 a dont care variable. Dont care variables cannot change the CNFvalue and should not be selected for assignment. This strategy is similar toclauseorder backtracking in software.The dont care condition for a variable is a Boolean function of x1, . . . , xnand can be easily derived from the CNF. In the above example, the dont carecondition for variable x3 is x1  x4. In the architecture CEDC, the datapathcomputes the CNF value and the dont care conditions for all variables in parallel.The FSM accepts an additional input, the dont care condition for its variable.If the FSM is activated while this condition is set, it passes control directly tothe next or previous FSM.3.3 Architecture IMIM propagation of implications exploits logical implications that are causedby value assignments. For example, the assignment x1  1 for the CNF inFormula 2 implies the variable x2, i.e., x2 must be assigned 1 to satisfy the firstclause. An implied variable can in turn imply other variables. An implicationcondition can be derived from the CNF for every literal. In the above example,the implication condition for x2 is x1  x4. If both literals of a variable areimplied, a contradiction has occurred. To model the variables, IM uses 4valuedlogic with the values 0, 1, X, C, where C denotes the contradiction.An FSM in this architecture sets its variable according to value assignmentsor value implications. If a contradiction occurs, the FSM sets a local contradiction flag. The datapath takes as input the variables as well as the local contradiction flags and generates as output the implications for all literals of the CNFand a global contradiction flag in parallel.Resolving logical implications in CNFs is known as the unitclause rule andis the basic mechanism in the DP algorithm. The iterative application of theunitclause rule is called Boolean constraint propagation. Using this method ininstancespecific hardware was first proposed by 5 a detailed description of thedatapath and the FSM can be found in 6.3.4 IMCE, IMDCIMCE and IMDC are combinations of the previous architectures. IMCE combines propagation of implications with the evaluation of the CNF expression.This can be helpful in cases, where a partial assignment already satisfies theCNF, but the IM strategy continues to assign values to unassigned variables.IMDC combines propagation of implications with dont cares, and has potentially the most deductive power.4 SimulationIn order to compare the different architectures we have implemented a programin C, that solves SAT problems by simulating the different hardware architectures. The simulator estimates performance and hardware cost. The performanceis measured in number of visited levels in the search tree, number of value assignments, and number of clock cycles. The hardware cost is estimated in numberof gates NOT and 2input ANDOR and flipflops FFs.In this paper, we report on simulation results for three benchmark classesfrom the DIMACS satisfiability benchmarks suite 7 class par instances fromlearning the parity function, class jnh randomly generated instances, and classhole instantiations of the pigeon hole problem. These classes are wellsuited forevaluation, as they include examples with long software runtimes.Table 4 presents the simulation results for the architecture IM. The speedupSraw is defined as tswthw, the ratio of software and hardware execution timesand does not include the hardware compilation time. The speedups in Table4 are remarkably high and motivate solving SAT in instancespecific hardware.Similar speedup numbers have also been reported in 5. Excellent candidatesfor instancespecific hardware are SAT problems, where high raw speedups arecombined with long software runtimes. The estimation further shows that forthe targeted FPGA line Xilinx XC4K, the combinational logic dominates thehardware cost. Although the estimation of hardware cost is not very accurate,most of the examples in Table 4 should fit into one FPGA.The comparison of the different hardware architectures relative to each otherrevealed the following facts The architectures can be divided into two groups, CE, CEDC and IM,IMCE, IMDC. Inside each group, the performance differences are below1. For classes par and jnh, IM performs at least 100 x better than CE for classhole, IM performs about 10 x better than CE. The estimated hardware cost for IM is at most twice the cost for CE.Hardware is used more efficiently by IM, as this architecture achieves withat most twice the hardware cost a performance at least 10 times better thanCE. However, CE may be an option when the hardware resources are limited.Further, the performance measure counts clock cycles. Architectures that havemore complex hardware designs will also have lower clock frequencies. CE is lesscomplex than IM and will very likely lead to faster FPGA designs.The results presented in this section depend strongly on the benchmark class.The exact tradeoffs between the architectures in terms of performance andhardware cost must be evaluated for each new benchmark class.5 Prototype ImplementationThe design tool flow of our prototype implementation is shown in Figure 3 andconsists of three parts the frontend, the generator, and the backend. The frontbenchmark variables clauses tsw simulated number Sraw hardware costs of cycles at 10 MHz Kgatespar161c 317 1264 203.03 63171 32140 30 Kpar161 1015 3310 321.25 158934 20212 87 Kpar162c 349 1392 3111.20 225408 138025 32 Kpar162 1015 3334 1009.00 422295 23893 88 Kjnh16 100 850 2.11 20052 1052 26 Kjnh19 100 850 0.11 6432 171 26 Khole7 56 204 4.56 351042 129 4.7 Khole8 72 297 54.98 4342574 126 6.1 Khole9 90 415 627.52 60162652 104 7.3 Khole10 110 562 7616.40 922461250 83 9.7 KTable 1. Simulation results for examples from the DIMACS benchmark suite. Thetable shows the problem size in number of variables and clauses, the runtime of thesoftware SAT solver GRASP, the simulated number of cycles for the IM architecture, the raw speedup at an assumed clock frequency of 10 MHz, and the estimatedhardware cost. GRASP was executed with parameters bD dDLIS on a PentiumII300MHz128MB RAM PC platform running Linux.end reads a SAT problem and checks for special cases, such as clauses that arealways satisfied, reorders the variables and computes the assignment order. Thegenerator compiles this modified SAT problem into a configuration bitstreamfor a Xilinx XC4K device. The backend loads the bitstream onto the FPGAand waits for the end of the computation. If there is a solution, the backendreads the FPGA register configuration, and extracts the variable values. Thegenerator consists again of two blocks. The first block is the generation of theFPGA netlist, the second block invokes the Xilinx M1 design implementationtools for mapping, placement, and routing.The architectures presented in Section 3 consist of the three blocks array ofFSMs, datapath, and global controller. The global controller and the single FSMdepend only on the chosen architecture, they do not change with the probleminstance. Therefore, these components are predesigned, i.e., they are specifiedin Verilog HDL, synthesized and optimized by Synopsys FPGA Express II, andstored in a library as FPGA netlists. At hardware generation time, the requirednumber of FSMs is instantiated and placed. Placement is done for two reasonsFirst, placing the FSMs allows the backend to extract the result of the computation by the readback facility of the FPGAs. Second, placement of the FSMsresults in faster designs. The datapath depends totally on the probleminstanceand is generated directly as FPGA netlist.Our prototype is implemented on a PC platform running Windows NT4.0. Asreconfigurable resource we use a Digital PCI Pamette board, which is equippedwith 4 FPGAs of the type Xilinx XC4020. In our current experiments, we useonly one of these FPGAs for implementing the SAT architecture.The overall runtime for computing a SAT problem in hardware consists of thehardware compilation time, tcomp, the time for configuring the FPGA, tconfig,frontendgeneratorbackendgeneratenetlistplace,map,routeresultSAT problemFig. 3. Prototype design tool flow.the actual hardware execution time, thw, and the time for reading back andextracting the result, tread.toverall  tcomp  tconfig  thw  tread 3The overall speedup Soverall is then given by tswtoverall. With our designtool flow, the times for FPGA configuration and readback can be neglectedcompared to the hardware compilation time, which itself is strongly dominatedby the Xilinx design implementation tools. At the time of this writing, we havesuccessfully compiled and run CE architectures for the hole benchmark class.The examples hole7 to hole9 can be mapped onto one Xilinx XC4020, for hole10an FPGA of type XC4025XC4028 is required. All the FPGA designs run at 20MHz. Table 5 and Figure 4 present the experimental resultsbenchmark tsw s tcomp s thw s Sraw Soverallhole7 4.56 134 0.181 25.14 0.03hole8 54.98 249 2.398 22.93 0.22hole9 627.52 439 35.229 17.81 1.32hole10 7616.40 597 567.255 13.43 6.54Table 2. Experimental results for the CE architecture and the hole benchmark class.The table shows the GRASP runtime, the hardware compilation time, the hardwareexecution time, and the resulting speedups.The results show that we could generate faster designs as assumed in Section4. The execution times of hardware and software SAT solvers increase with the100001000100101runtimesbenchmarkhole7hole8hole9hole10softwarehardware13.4322.9325.1417.81Fig. 4. Software runtime tsw, hardware runtime thw, and the resulting raw speedupSraw for the CE architecture and the hole benchmark class.problem size more rapidly than the hardware compilation time. This leads to acrossover point in the overall speedup around hole9, i.e., here the SAT solver inreconfigurable hardware is for the first time faster than the software SAT solver.For hole10 we achieve a speedup of 6.54, which reduces the runtime from morethan 2 hours in software to about 20 minutes in hardware.For the hole benchmarks, the architecture IM requires about 10 times lessclock cycles than CE. This would lead to an overall speedup of 8.77 for hole10,assuming the same hardware compilation time than for CE. However, IM forhole10 will not fit onto one FPGA XC4025.6 Conclusion, Further WorkWe have presented different architectures for solving SAT problems in instancespecific hardware. These architectures offer tradeoffs between performance andhardware cost, depending on the benchmark class. Simulations revealed thatfor larger problems from the DIMACS benchmark suite, instancespecific SATsolvers can achieve significant raw speedups over software SAT solvers. We haveimplemented a prototypical design tool flow and discussed first experimentalresults. The results show that architectures with less deductive power can becompetitive when the hardware compilation time is not neglectable comparedto the hardware execution time. This is the case for all currently implementedbenchmarks.As the density of FPGAs increases, many interesting SAT problems canbe accelerated by instancespecific hardware. Although FPGAbased computingmachines still require relatively long compilation times, instancespecific architectures are promising for hard SAT problems, where software algorithms showa long runtime.Further work includes Implementation of instancespecific architectures for minimumcost problems. A SAT problem with unit cost or integer cost values assigned to thevariables forms a minimization problem. Finding minimumcost solutions toSAT problems is a frequent task in CAD algorithms. Application of the instancespecific SAT solver to CAD tools. CAD applications have to be found, that generate SAT problems that are hard to solvein software, i.e., problems that have a relatively small number of variablesbut show long software runtimes.AcknowledgmentThis work was partially supported by the Austrian National Science FoundationFWF under grant number J01412MAT. We would also like to thank AlessandroBogliolo and Luca Benini for their contributions and discussions in the earlyphases of this work.References1. Jun Gu, Paul W. Purdom, John Franco, and Benjamin W. Wah. Algorithms for theSatisfiability SAT Problem A Survey. DIMACS Series in Discrete Mathematicsand Theoretical Computer Science, 3519151, 1997.2. M. Davis and H. Putnam. A computing procedure for quantification theory. Journal of the ACM, 7201215, 1960.3. M. Davis, G. Logemann, and D. Loveland. A machine program for theorem proving.Communications of the ACM, 5394397, 1962.4. J. Silva and K. Sakallah. GRASP  A New Search Algorithm for Satisfiability.In IEEE ACM International Conference on CAD 96, pages 220227, November1996.5. Peixin Zhong, Margaret Martonosi, Sharad Malik, and Pranav Ashar. Implementing Boolean Satisfiability in Configurable Hardware. In Logic Synthesis Workshop,May 1997.6. Peixin Zhong, Margaret Martonosi, Pranav Ashar, and Sharad Malik. AcceleratingBoolean Satisfiability with Configurable Hardware. In IEEE Symposium on FPGAsfor Custom Computing Machines, April 1998.7. DIMACS satsifiability benchmark suite,available at ftpdimacs.rutgers.edupubchallengesatbenchmarkscnf.8. Takayuki Suyama, Makoto Yokoo, and Hiroshi Sawada. Solving Satisfiability Problems on FPGAs. In International Workshop on FieldProgrammable Logic andApplications FPL, pages 136145, 1996.9. Miron Abramovici and Daniel Saab. Satisfiablity on Reconfigurable Hardware.In International Workshop on FieldProgrammable Logic and Applications FPL,pages 448456, 1997.10. Azra Rashid, Jason Leonard, and William H. MangioneSmith. Dynamic CircuitGeneration for Solving Specific Problem Instances of Boolean Satisfiablity. InIEEE Symposium on FPGAs for Custom Computing Machines, April 1998.
