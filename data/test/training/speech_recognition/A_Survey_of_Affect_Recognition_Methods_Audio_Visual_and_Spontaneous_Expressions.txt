A Survey of Affect Recognition Methods Audio,Visual, and Spontaneous ExpressionsZhihong Zeng, Member, IEEE Computer Society, Maja Pantic, Senior Member, IEEE,Glenn I. Roisman, and Thomas S. Huang, Fellow, IEEEAbstractAutomated analysis of human affective behavior has attracted increasing attention from researchers in psychology,computer science, linguistics, neuroscience, and related disciplines. However, the existing methods typically handle only deliberatelydisplayed and exaggerated expressions of prototypical emotions, despite the fact that deliberate behavior differs in visual appearance,audio profile, and timing from spontaneously occurring behavior. To address this problem, efforts to develop algorithms that canprocess naturally occurring human affective behavior have recently emerged. Moreover, an increasing number of efforts are reportedtoward multimodal fusion for human affect analysis, including audiovisual fusion, linguistic and paralinguistic fusion, and multicuevisual fusion based on facial expressions, head movements, and body gestures. This paper introduces and surveys these recentadvances. We first discuss human emotion perception from a psychological perspective. Next, we examine available approaches forsolving the problem of machine understanding of human affective behavior and discuss important issues like the collection andavailability of training and test data. We finally outline some of the scientific and engineering challenges to advancing human affectsensing technology.Index TermsEvaluationmethodology, humancentered computing, affective computing, introductory, survey.1 INTRODUCTIONA widely accepted prediction is that computing willmove to the background, weaving itself into the fabricof our everyday living spaces and projecting the humanuser into the foreground. Consequently, the futureubiquitous computing environments will need to havehumancentered designs instead of computercentereddesigns 26, 31, 100, 107, 109. Current humancomputer interaction HCI designs, however, usuallyinvolve traditional interface devices such as the keyboardand mouse and are constructed to emphasize the transmission of explicit messages while ignoring implicit information about the user, such as changes in the affective state.Yet, a change in the users affective state is a fundamentalcomponent of humanhuman communication. Some affective states motivate human actions, and others enrich themeaning of human communication. Consequently, thetraditional HCI, which ignores the users affective states,filters out a large portion of the information available inthe interaction process. As a result, such interactions arefrequently perceived as cold, incompetent, and sociallyinept. The human computing paradigm suggests that userinterfaces of the future need to be anticipatory and humancentered, built for humans, and based on naturallyoccurring multimodal human communication 100, 109.Specifically, humancentered interfaces must have theability to detect subtleties of and changes in the usersbehavior, especially hisher affective behavior, and toinitiate interactions based on this information rather thansimply responding to the users commands.Examples of affectsensitive multimodal HCI systemsinclude the following1. the system of Lisetti and Nasoz 85, which combinesfacial expression and physiological signals to recognize the users emotions, like fear and anger, andthen to adapt an animated interface agent to mirrorthe users emotion,2. the multimodal system of Duric et al. 39, whichapplies a model of embodied cognition that canbe seen as a detailed mapping between theusers affective states and the types of interfaceadaptations,3. the proactive HCI tool of Maat and Pantic 89,which is capable of learning and analyzing theusers contextdependent behavioral patterns frommultisensory data and of adapting the interactionaccordingly,4. the automated Learning Companion of Kapoor et al.72, which combines information from cameras, asensing chair, and mouse, wireless skin sensor, andtask state to detect frustration in order to predictwhen the user needs help, andIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 31, NO. 1, JANUARY 2009 39. Z. Zeng and T.S. Huang are with the Beckman Institute, University ofIllinois at UrbanaChampaign, 405 N. Mathews Ave., Urbana, 61801.Email zhzeng, huangifp.uiuc.edu.. M. Pantic is with the Department of Computing, Imperial College London,180 Queens Gate, London SW7 2AZ, United Kingdom, and with theFaculty of Electrical Engineering, Mathematics, and Computer Science,University of Twente, The Netherlands.Email m.panticimperial.ac.uk.. G.I. Roisman is with the Psychology Department, University of Illinois atUrbanaChampaign, 603 East Daniel St., Champaign, IL 61820.Email roismanuiuc.edu.Manuscript received 12 Aug. 2007 revised 30 Dec. 2007 accepted 25 Jan.2008 published online 27 Feb. 2008.Recommended for acceptance by T. Darrell.For information on obtaining reprints of this article, please send email totpamicomputer.org, and reference IEEECS Log NumberTPAMI2007080496.Digital Object Identifier no. 10.1109TPAMI.2008.52.016288280925.00  2009 IEEE Published by the IEEE Computer SocietyAuthorized licensed use limited to IEEE Transactions on SMC Associate Editors. Downloaded on November 30, 2008 at 1010 from IEEE Xplore.  Restrictions apply.5. the multimodal computeraided learning system1 inthe Beckman Institute, University of Illinois, UrbanaChampaign UIUC, where the computer avataroffers an appropriate tutoring strategy based onthe information of the users facial expression,keywords, eye movement, and task state.These systems represent initial efforts toward the futurehumancentered multimodal HCI.Except in standard HCI scenarios, potential commercialapplications of automatic human affect recognition includeaffectsensitive systems for customer services, call centers,intelligent automobile systems, and game and entertainment industries. These systems will change the ways inwhich we interact with computer systems. For example, anautomatic service call center with an affect detector wouldbe able to make an appropriate response or pass controlover to human operators 83, and an intelligent automobilesystem with a fatigue detector could monitor the vigilanceof the driver and apply an appropriate action to avoidaccidents 69.Another important application of automated systems forhuman affect recognition is in affectrelated research e.g.,in psychology, psychiatry, behavioral science, and neuroscience, where such systems can improve the quality ofthe research by improving the reliability of measurementsand speeding up the currently tedious manual task ofprocessing data on human affective behavior 47. Theresearch areas that would reap substantial benefits fromsuch automatic tools include social and emotional development research 111, motherinfant interaction 29,tutoring 54, psychiatric disorders 45, and studies onaffective expressions e.g., deception 65, 47. Automateddetectors of affective states and moods, including fatigue,depression, and anxiety, could also form an important steptoward personal wellness and assistive technologies 100.Because of this practical importance and the theoreticalinterest of cognitive scientists, automatic human affectanalysis has attracted the interest of many researchers inthe last three decades. Suwa et al. 127 presented an earlyattempt in 1978 to automatically analyze facial expressions.The vocal emotion analysis has an even longer history,starting with the study of Williams and Stevens in 1972145. Since the late 1990s, an increasing number of effortstoward automatic affect recognition were reported in theliterature. Early efforts toward machine affect recognitionfrom face images include those of Mase 90, and Kobayashiand Hara 76 in 1991. Early efforts toward the machineanalysis of basic emotions from vocal cues include studieslike that of Dellaert et al. in 1996 33. The study of Chenet al. in 1998 22 represents an early attempt towardaudiovisual affect recognition. For exhaustive surveys of thepast work in the machine analysis of affective expressions,readers are referred to 115, 31, 102, 49, 96, 105,130, 121, and 98, which were published in 1992 to 2007,respectively.Overall, most of the existing approaches to automatichuman affect analysis are the following. approaches that are trained and tested on adeliberately displayed series of exaggerated affectiveexpressions,. approaches that are aimed at recognition of a smallnumber of prototypical basic expressions of emotion i.e., happiness, sadness, anger, fear, surprise,and disgust, and. singlemodal approaches, where information processed by the computer system is limited to eitherface images or the speech signals.Accordingly, reviewing the efforts toward the singlemodal analysis of artificial affective expressions have beenthe focus in the previously published survey papers, amongwhich the papers of Cowie et al. in 2001 31 and of Panticand Rothkrantz in 2003 102 have been the mostcomprehensive and widely cited in this field to date. Atthe time when these surveys were written, most of theavailable data sets of affective displays were small andcontained only deliberate affective displays mainly of thesix prototypical emotions recorded under highly constrained conditions. Multimedia data were rare, and therewas no 3D data on facial affective behavior, there was nodata of combined face and body displays of affectivebehavior, and it was rare to find data that includedspontaneous displays of affective behavior.Hence, while automatic detection of the six basicemotions in posed controlled audio or visual displays canbe done with reasonably high accuracy, detecting theseexpressions or any expression of human affective behaviorin less constrained settings is still a very challengingproblem due to the fact that deliberate behavior differs invisual appearance, audio profile, and timing from spontaneously occurring behavior. Due to this criticism receivedfrom both cognitive and computer scientists, the focus ofthe research in the field started to shift to the automaticanalysis of spontaneously displayed affective behavior.Several studies have recently emerged on the machineanalysis of spontaneous facial expressions e.g., 10, 28,135, and 4 and vocal expressions e.g., 12 and 83.Also, it has been shown by several experimental studiesthat integrating the information from audio and videoleads to an improved performance of affective behaviorrecognition. The improved reliability of audiovisual approaches in comparison to singlemodal approaches can beexplained as follows Current techniques for the detectionand tracking of facial expressions are sensitive to headpose, clutter, and variations in lighting conditions, whilecurrent techniques for speech processing are sensitive toauditory noise. Audiovisual fusion can make use of thecomplementary information from these two channels. Inaddition, many psychological studies have theoreticallyand empirically demonstrated the importance of theintegration of information from multiple modalities vocaland visual expression in this paper to yield a coherentrepresentation and inference of emotions 1, 113, 117.As a result, an increased number of studies on audiovisualhuman affect recognition have emerged in recent yearse.g., 17, 53, and 151.This paper introduces and surveys these recent advancesin the research on human affect recognition. In contrast to40 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 31, NO. 1, JANUARY 20091. httpitr.beckman.uiuc.edu.Authorized licensed use limited to IEEE Transactions on SMC Associate Editors. Downloaded on November 30, 2008 at 1010 from IEEE Xplore.  Restrictions apply.previously published survey papers in the field, it focuseson the approaches that can handle audio andor visualrecordings of spontaneous as opposed to posed displays ofaffective states. It also examines the stateoftheart methodsthat have not been reviewed in previous survey papers butare important, specifically for advancing human affectsensing technology. Finally, we discuss the collection andavailability of training and test data in detail. This paper isorganized as follows Section 2 describes the humanperception of affect from a psychological perspective.Section 3 provides a detailed review of the related studies,including multimedia emotion databases and existinghuman affect recognition methods. Section 4 discussessome of the challenges that researchers face in this field. Asummary and closing remarks conclude this paper.2 HUMAN AFFECT EMOTION PERCEPTIONAutomatic affect recognition is inherently a multidisciplinary enterprise involving different research fields, includingpsychology, linguistics, computer vision, speech analysis,and machine learning. There is no doubt that the progressin automatic affect recognition is contingent on the progressof the research in each of those fields 44.2.1 The Description of AffectWe begin by briefly introducing three primary ways thataffect has been conceptualized in psychological research.Research on the basic structure and description of affect isimportant in that these conceptualizations provide information about the affective displays that automatic emotionrecognition systems are designed to detect.Perhaps the most longstanding way that affect has beendescribed by psychologists is in terms of discrete categories,an approach that is rooted in the language of daily life 40,41, 46, 131. The most popular example of thisdescription is the prototypical basic emotion categories,which include happiness, sadness, fear, anger, disgust, andsurprise. This description of basic emotions was speciallysupported by the crosscultural studies conducted byEkman 40, 42, indicating that humans perceive certainbasic emotions with respect to facial expressions in the sameway, regardless of culture. This influence of a basic emotiontheory has resulted in the fact that most of the existingstudies of automatic affect recognition focus on recognizingthese basic emotions. The main advantage of a categoryrepresentation is that people use this categorical scheme todescribe observed emotional displays in daily life. Thelabeling scheme based on category is very intuitive and thusmatches peoples experience. However, discrete lists ofemotions fail to describe the range of emotions that occur innatural communication settings. For example, althoughprototypical emotions are key points of emotion reference,they cover a rather small part of our daily emotionaldisplays. Selection of affect categories that can describe thewide variety of affective displays that people show in dailyinterpersonal interactions needs to be done in a pragmaticand contextdependent manner 102, 105.An alternative to the categorical description of humanaffect is the dimensional description 58, 114, 140, wherean affective state is characterized in terms of a smallnumber of latent dimensions rather than in terms of a smallnumber of discrete emotion categories. These dimensionsinclude evaluation, activation, control, power, etc. Inparticular, the evaluation and activation dimensions areexpected to reflect the main aspects of emotion. Theevaluation dimension measures how a human feels, frompositive to negative. The activation dimension measureswhether humans are more or less likely to take an actionunder the emotional state, from active to passive. In contrastto categorical representation, dimensional representationenables raters to label a range of emotions. However, theprojection of the highdimensional emotional states onto arudimentary 2D space results, to some degree, in the loss ofinformation. Some emotions become indistinguishable e.g.,fear and anger, and some emotions lie outside the spacee.g., surprise. This representation is not intuitive, andraters need special training to use the dimensional labelingsystem e.g., the Feeltrace system 30. In automaticemotion recognition systems that are based on the 2Ddimensional emotion representation e.g., 17 and 53, theproblem is often further simplified to twoclass positiveversus negative and active versus passive or fourclassquadrants of 2D space classification.One of the most influential emotion theories in modernpsychology is the appraisalbased approach 117, whichcan be regarded as the extension of the dimensionalapproach described above. In this representation, anemotion is described through a set of stimulus evaluationchecks, including the novelty, intrinsic pleasantness, goalbased significance, coping potential, and compatibility withstandards. However, translating this scheme into oneengineering framework for purposes of automatic emotionrecognition remains challenging 116.2.2 Association between Affect, Audio, and VisualSignalsAffective arousal modulates all human communicativesignals. Psychologists and linguists have various opinionsabout the importance of different cues audio and visualcues in this paper in human affect judgment. Ekman 41found that the relative contributions of facial expression,speech, and body gestures to affect judgment depend bothon the affective state and the environment where theaffective behavior occurs. On the other hand, some studiese.g., 1 and 92 indicated that a facial expression in thevisual channel is the most important affective cue andcorrelates well with the body and voice. Many studies havetheoretically and empirically demonstrated the advantageof the integration of multiple modalities vocal and visualexpression in human affect perception over single modalities 1, 113, 117.Different from the traditional message judgment, inwhich the aim is to infer what underlies a displayedbehavior such as affect or personality, another majorapproach to human behavior measurement is the signjudgment 26. The aim of sign judgment is to describe theappearance, rather than the meaning, of the shown behaviorsuch as facial signal, body gesture, or speech rate. Whilemessage judgment is focused on interpretation, signjudgment attempts to be an objective description, leavingthe inference about the conveyed message to highlevelZENG ET AL. A SURVEY OF AFFECT RECOGNITION METHODS AUDIO, VISUAL, AND SPONTANEOUS EXPRESSIONS 41Authorized licensed use limited to IEEE Transactions on SMC Associate Editors. Downloaded on November 30, 2008 at 1010 from IEEE Xplore.  Restrictions apply.decision making. As indicated by Cohn 26, the mostcommonly used sign judgment method for the manuallabeling of facial behavior is the Facial Action CodingSystem FACS proposed by Ekman et al. 43. FACS is acomprehensive and anatomically based system that is usedto measure all visually discernible facial movements interms of atomic facial actions called Action Units AUs. AsAUs are independent of interpretation, they can be used forany highlevel decisionmaking process, including therecognition of basic emotions according to Emotional FACSEMFACS rules2, the recognition of various affective statesaccording to the FACS Affect Interpretation DatabaseFACSAID2 introduced by Ekman et al. 43, and therecognition of other complex psychological states such asdepression 47 or pain 144. AUs of the FACS are verysuitable to use in studies on human naturalistic facialbehavior, as the thousands of anatomically possible facialexpressions independent of their highlevel interpretationcan be described as combinations of 27 basic AUs and anumber of AU descriptors. It is not surprising, therefore,that an increasing number of studies on human spontaneous facial behavior are based on automatic AU recognition e.g., 10, 27, 135, 87, and 134.Speech is another important communicative modality inhumanhuman interaction. Speech conveys affective information through explicit linguistic and implicit paralinguistic messages that reflect the way that the words arespoken. As the linguistic content is concerned, someinformation about the speakers affective state can beinferred directly from the surface features of words, whichwere summarized in some affective word dictionaries andlexical affinity 110, 142, and the rest of affectiveinformation lies below the text surface and can only bedetected when the semantic context e.g., discourse information is taken into account. However, findings in basicresearch 1, 55 indicate that linguistic messages are ratherunreliable means of analyzing human affective behavior,and it is very difficult to anticipate a persons word choiceand the associated intent in affective expressions. Inaddition, the association between linguistic content andemotion is language dependent, and generalizing from onelanguage to another is very difficult to achieve.When it comes to implicit paralinguistic messages thatconvey affective information, basic researchers have notidentified an optimal set of voice cues that reliablydiscriminate among emotions. Nonetheless, listeners seemto be accurate in decoding some basic emotions fromprosody 70 and some nonbasic affective states such asdistress, anxiety, boredom, and sexual interest fromnonlinguistic vocalizations like laughs, cries, sighs, andyawns 113. Cowie et al. 31 provided a comprehensivesummary of qualitative acoustic correlations for prototypical emotions.In summary, a large number of studies in psychologyand linguistics confirm the correlation between someaffective displays especially prototypical emotions andspecific audio and visual signals e.g., 1, 47, and 113.The human judgment agreement is typically higher forfacial expression modality than for vocal expressionmodality. However, the amount of the agreement dropsconsiderably when the stimuli are spontaneously displayedexpressions of affective behavior rather than posed exaggerated displays. In addition, facial expression and thevocal expression of emotion are often studied separately.This precludes finding evidence of the temporal correlationbetween them. On the other hand, a growing body ofresearch in cognitive sciences argues that the dynamics ofhuman behavior are crucial for its interpretation e.g., 47,113, 116, and 117. For example, it has been shown thattemporal dynamics of facial behavior represent a criticalfactor for distinction between spontaneous and posed facialbehavior e.g., 28, 47, 135, and 134 and for categorization of complex behaviors like pain, shame, andamusement e.g., 47, 144, 4, and 87. Based on thesefindings, we may expect that the temporal dynamics of eachmodality facial and vocal and the temporal correlationsbetween the two modalities play an important role in theinterpretation of human naturalistic audiovisual affectivebehavior. However, these are virtually unexplored areas ofresearch.Another largely unexplored area of research is that ofcontext dependency. The interpretation of human behavioral signals is context dependent. For example, a smile can bea display of politeness, irony, joy, or greeting. To interpret abehavioral signal, it is important to know the context inwhich this signal has been displayed, i.e., where theexpresser is e.g., inside, on the street, or in the car, whatthe expressers current task is, who the receiver is, and whothe expresser is 113.3 THE STATE OF THE ARTRather than providing exhaustive coverage of all pastefforts in the field of automatic recognition of human affect,we focus here on the efforts recently proposed in theliterature that have not been reviewed elsewhere, thatrepresent multimodal approaches to the problem of humanaffect recognition, that address the problem of the automatic analysis of spontaneous affective behavior, or thatrepresent exemplary approaches to treating a specificproblem relevant for achieving a better human affectsensing technology. Due to limitations on space and ourknowledge, we sincerely apologize to those authors whosework is not included in this paper. For exhaustive surveysof the past efforts in the field, readers are referred to thefollowing articles. Overviews of early work on facial expressionanalysis 115, 101, and 49.. Surveys of techniques for automatic facial muscleaction recognition and facial expression analysis130 and 98.. Overviews of multimodal affect recognition methods 31, 102, 105, 121, 68, and 152 this is ashort preliminary version of the survey presented inthis current paper.In this section, we first offer an overview of the existingdatabases of audio andor visual recordings of humanaffective displays, which provide the basis of automatic42 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 31, NO. 1, JANUARY 20092. httpfaceandemotion.comdatafacegeneralhomepage.jsp.Authorized licensed use limited to IEEE Transactions on SMC Associate Editors. Downloaded on November 30, 2008 at 1010 from IEEE Xplore.  Restrictions apply.affect analysis. Next, we examine available computingmethods for automatic human affect recognition.3.1 DatabasesHaving enough labeled data of human affective expressionsis a prerequisite in designing automatic affect recognizer.Authentic affective expressions are difficult to collectbecause they are relatively rare, short lived, and filled withsubtle contextbased changes that make it difficult to elicitaffective displays without influencing the results. Inaddition, manual labeling of spontaneous emotional expressions for the ground truth is very time consuming, errorprone, and expensive. This state of affairs makes theautomatic analysis of spontaneous emotional expression avery difficult task. Due to these difficulties, most of theexisting studies on the automatic analysis of humanaffective displays have been based on the artificialmaterial of deliberately expressed emotions, elicited byasking the subjects to perform a series of emotionalexpressions in front of a camera andor the microphone.However, increasing evidence suggests that deliberatebehavior differs in visual appearance, audio profile, andtiming from spontaneously occurring behavior. For example, Whissell shows that the posed nature of emotions inspoken language may differ in the choice of words andtiming from corresponding performances in natural settings142. When it comes to facial behavior, there is a large bodyof research in psychology and neuroscience demonstratingthat spontaneous deliberately displayed facial behavior hasdifferences both in utilized facial muscles and theirdynamics e.g., 47. For instance, many types of spontaneous smiles e.g., polite are smaller in amplitude, longerin total duration, and slower in onset and offset times thanposed smiles e.g., 28, 47, and 134. Similarly, it hasbeen shown that spontaneous brow actions AU1, AU2, andAU4 in the FACS system have different morphological andtemporal characteristics intensity, duration, and occurrenceorder than posed brow actions 135. It is not surprising,therefore, that methods of automated human affect analysisthat have been trained on deliberate and often exaggeratedbehaviors usually fail to generalize to the subtlety andcomplexity of spontaneous affective behavior.In addition, most of the current human affect recognizersare evaluated using clear constrained input e.g., highquality visual and audio recording, nonoccluded, and frontor profileview face, which is different from the inputcoming from a natural setting. In addition, most of theemotion expressions that occur in a realistic interpersonal orHCI are nonbasic emotions 32. Yet, the majority of theexisting systems for human affect recognition aim atclassifying the input expression as the basic emotioncategory e.g., 31, 102, and 105.These findings and the general lack of a comprehensivereference set of audio andor visual recordings of humanaffective displays motivated several efforts aimed at thedevelopment of data sets that could be used for training andtest of automatic systems for human affect analysis. Table 1lists some noteworthy audio, visual, and audiovisual dataresources that were reported in the literature. For eachdatabase, we provide the following information1. affect elicitation method i.e., whether the elicitedaffective displays are posed or spontaneous,2. size the number of subjects and available datasamples,3. modality audio andor visual,4. affect description category or dimension,5. labeling scheme, and6. public accessibility.For other surveys of existing databases of human affectivebehavior, the readers are referred to 32, 59, and 106.As far as the databases of deliberate affective behaviorare concerned, the following databases need to bementioned. The CohnKanade facial expression database71 is the most widely used database for facial expressionrecognition. The BU3DFE database of Yin and colleagues148 contains 3D range data of six prototypical facialexpressions displayed at four different levels of intensity.The FABO database of Gunes and Piccardi 63 containsvideos of facial expressions and body gestures portrayingposed displays of basic and nonbasic affective states sixprototypical emotions, uncertainty, anxiety, boredom, andneutral. The MMI facial expression database 106, 98 is,to our knowledge, the most comprehensive data set offacial behavior recordings to date. It contains both posedexpressions and spontaneous expressions of facial behavior. The available recordings of deliberate facial behaviorare both static images and videos, where a large part ofvideo recordings were recorded in both the frontal and theprofile views of the face. The database represents a facialbehavior data repository that is available, searchable, anddownloadable via the Internet.3 Although there are manydatabases of acted emotional speech,4 a large majority ofthese data sets contain unlabeled data, which makes themunsuitable for research on automatic vocal affect recognition. The BanseScherer vocal affect database 8 and theDanish Emotional Speech database5 are the two mostwidely used databases in the research on vocal affectrecognition from acted emotional speech. Finally, theChenHuang audiovisual database 21 is, to our knowledge, the largest multimedia database containing facialand vocal deliberate displays of basic emotions and fourcognitive states1. interest,2. puzzlement,3. frustration, and4. boredom.The existing data sets of spontaneous affective behaviorwere collected in one of the following scenarios humanhuman conversation, HCI, and use of a video kiosk.Humanhuman conversation scenarios include facetofaceinterviews e.g., 10, 38, 111, and 65, phone conversations e.g., 34, and meetings e.g., 15 and AMI6. HCIscenarios include Wizard of Oz scenarios e.g., 13 andSAL7 and computerbased dialogue systems e.g., 83 andZENG ET AL. A SURVEY OF AFFECT RECOGNITION METHODS AUDIO, VISUAL, AND SPONTANEOUS EXPRESSIONS 433. httpwww.mmifacedb.com.4. httpemotionresearch.netwikiDatabases.5. httpcpk.auc.dktbspeechEmotions.6. httpcorpus.amiproject.org.7. httpemotionresearch.nettoolboxtoolboxdatabase.20060926.5667892524.Authorized licensed use limited to IEEE Transactions on SMC Associate Editors. Downloaded on November 30, 2008 at 1010 from IEEE Xplore.  Restrictions apply.86. In the video kiosk settings e.g., 95, 98, and 123,the subjects affective reactions are recorded while thesubjects are watching emotioninducing videos.In most of the existing databases, discrete emotioncategories are used as the emotion descriptors. The labelsof prototypical emotions are often used, especially in thedatabases of deliberate affective behavior. In databases ofspontaneous affective behavior, coarse affective states likepositive versus negative e.g., 15 and 83, dimensionaldescriptions in the evaluationactivation space e.g., SAL,and some applicationdependent affective states are usuallyused as the data labels. Some typical examples of the usedapplicationdependent affectinterpretative labels e.g., 95,63, 13, and 111 are the following1. interest,2. boredom,3. confusion,4. frustration,5. fatigue,44 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 31, NO. 1, JANUARY 2009TABLE 1Audio andor Visual Databases of Human Affective BehaviorA audio, V video, AV audiovisual, NA not available, Y yes, and N not yet.Authorized licensed use limited to IEEE Transactions on SMC Associate Editors. Downloaded on November 30, 2008 at 1010 from IEEE Xplore.  Restrictions apply.6. empathy,7. stress,8. irony,9. annoyance,10. amusement,11. helplessness,12. panic,13. shame,14. reprehension, and15. rebelliousness.As explained above, AUs are very suitable to describethe richness of spontaneous facial behavior, as the thousands of anatomically possible facial expressions can berepresented as combination of a few dozens of AUs. Hence,the labeling schemes used to code data include FACS AUse.g., 10, 71, 106, 98, and 111, the Feeltrace systemfor evaluationactivation dimensional description e.g., 38and SAL, self report e.g., 123 and 65, and humanobserver judgment e.g., 13, 15, 83, 95, and 98.The current situation of emotion database research isconsiderably different from what was described in thecomprehensive surveys written by Pantic and Rothkrantz102 and Cowie et al. 31. The current state of the art isadvanced and can be summarized as follows. A database of 3D recordings of acted facial affect148 and a database of faceandbody recordingsof acted affective displays 63 have been madeavailable.. A collection of acted facial affect displays made fromthe profile view is shared on the Internet 106.. Several large audio, visual, and audiovisual sets ofhuman spontaneous affective behavior have beencollected, some of which are released for public use.The existence of these data sets of spontaneous affectivebehavior is very promising, and we expect that this willproduce a major shift in the course of the research in thefield from the analysis of exaggerated expressions of basicemotions to the analysis of naturalistic affective behavior.We also expect subsequent shifts in research in variousrelated fields such as ambient intelligence, transportation,and personal wellness technologies.3.2 VisionBased Affect RecognitionBecause of the importance of face in emotion expressionand perception, most of the visionbased affect recognitionstudies focus on facial expression analysis. We candistinguish two main streams in the current research onthe machine analysis of facial expressions 26, 98 therecognition of affect and the recognition of facial muscleaction facial AUs. As explained above, facial AUs are arelatively objective description of facial signals and can bemapped to the emotion categories based on a highlevelmapping such as EMFACS and FACSAID or to any otherset of highorder interpretation categories, including complex affective states like depression 47 or pain 144.As far as automatic facial affect recognition is concerned,most of the existing efforts studied the expressions of the sixbasic emotions due to their universal properties, theirmarked reference representation in our affective lives, andthe availability of the relevant training and test materiale.g., 71. There are a few tentative efforts to detectnonbasic affective states from deliberately displayed facialexpressions, including fatigue 60, 69, and mental stateslike agreeing, concentrated, interested, thinking, confused,and frustrated e.g., 48, 72, 73, 129, and 147.Most of the existing works on the automatic facialexpression recognition are based on deliberate and oftenexaggerated facial displays e.g., 130. However, severalefforts have been recently reported on the automaticanalysis of spontaneous facial expression data e.g., 9,10, 11, 27, 28, 67, 88, 123, 135, 149, 87, 4, and134. Some of them study the automatic recognition ofAUs, rather than emotions, from spontaneous facial displays e.g., 9, 10, 11, 27, 28, 135, and 134. Studiesreported in 28, 135, 134, and 87 investigated explicitlythe difference between spontaneous and deliberate facialbehavior. In particular, the studies of Valstar et al. 135,134 and the study of Littlewort et al. 87 are the firstreported efforts to date to automatically discern posed fromspontaneous facial behavior. It is interesting to note that,confirming with research findings in psychology e.g., 47,the systems proposed by Valstar et al. were built tocharacterize temporal dynamics of facial actions andemploy parameters like speed, intensity, duration, and thecooccurrence of facial muscles activations to classify facialbehavior present in a video as either deliberate orspontaneous.Some of the studies on the machine analysis ofspontaneous facial behavior were conducted using the datasets listed in Table 1 e.g., 10, 149, and 134. For otherstudies, new data sets were collected. Overall, the utilizeddata were collected in the following dataelicitation scenarios humanhuman conversation e.g., 10, 11, 28, 135,149, and 4, Wizard of Oz scenarios e.g., 67, or TVbroadcast e.g., 147. The studies reported in 123 and147 explored the automatic recognition of a subset of basicemotional expressions. The study of Zeng et al. 149investigated separating emotional state from nonemotionalstates during the Adult Attachment Interview. Studies onseparating posed from genuine smiles were reported in 28and 134, and studies on the recognition of pain from facialbehavior were reported in 4 and 87.Most of the existing facial expression recognizers employvarious pattern recognition approaches and are based on2D spatiotemporal facial features. The usually extractedfacial features are either geometric features such as theshapes of the facial components eyes, mouth, etc. and thelocation of facial salient points corners of the eyes, mouth,etc. or appearance features representing the facial texture,including wrinkles, bulges, and furrows. Typical examplesof geometricfeaturebased methods are those of Changet al. 19, who used a shape model defined by 58 faciallandmarks, of Pantic et al. 98, 99, 103, 135, 134, whoused a set of facial characteristic points around the mouth,eyes, eyebrows, nose, and chin, and of Kotsia and Pitas 77,who used the Candide grid. Typical examples of appearancefeaturebased methods are listed as follows1. Bartlett et al. 9, 10, 11, 87 and Guo and Dyer64, who used Gabor wavelets,2. Whitehill and Omlin 143, who used Haar features,ZENG ET AL. A SURVEY OF AFFECT RECOGNITION METHODS AUDIO, VISUAL, AND SPONTANEOUS EXPRESSIONS 45Authorized licensed use limited to IEEE Transactions on SMC Associate Editors. Downloaded on November 30, 2008 at 1010 from IEEE Xplore.  Restrictions apply.3. Anderson and McOwen 2, who used a holisticspatial ratio face template,4. Valstar et al. 136, who used temporal templates,and5. Chang et al. 18, who built a probabilistic recognition algorithm based on the manifold subspace ofaligned face appearances.As suggested in several studies e.g., 99, using bothgeometric and appearance features might be the best choicefor designing automatic facial expression recognizers.Typical examples of hybrid geometric and appearancefeaturebased methods are those proposed by Tian et al.130, who used facial component shapes and the transientfeatures like crowfeet wrinkles and nasallabial furrows,and that of Zhang and Ji 158, who used 26 facial pointsaround the eyes, eyebrows, and mouth, and the transientfeatures proposed by Tian et al. Another example of such amethod is that proposed by Lucey et al. 88, who used theActive Appearance Model AAM to capture the characteristics of the facial appearance and the shape of facialexpressions.Most of the existing 2Dfeaturebased methods aresuitable for the analysis of facial expressions under a smallrange of head motions. Thus, most of these methods focuson the recognition of facial expressions in nearfrontalviewrecordings. An exemplar exception is the study of Panticand Patras 99, who explored automatic analysis of facialexpressions from the profileview face.A few approaches to automatic facial expression analysisare based on 3D face models. Huang and colleagues i.e.,25, 123, 141, and 149 used features extracted by a 3Dface tracker called the Piecewise Bezier Volume Deformation Tracker 128. Cohn et al. 27 focused on the analysis ofbrow AUs and head movement based on a cylindrical headmodel 146. Chang et al. 20 and Yin et al. 139, 148 used3D expression data for facial expression recognition. Theprogress of the methodology based on 3D face models mayyield viewindependent facial expression recognition,which is important for spontaneous facial expressionrecognition, because the subject can be recorded in lesscontrolled realworld settings.Some efforts are reported to decompose multiple factorse.g., the facial expression, face style, or pose from faceimages. Typical examples are those of Wang and Ahuja137, who used a multilinear subspace method, and of Leeand Elgammal 81, who proposed decomposable nonlinearmanifold to estimate facial expression and face stylesimultaneously. The study of Zhu and Ji 160 used anormalized SVD decomposition to recover facial expressionand pose.Relatively few studies investigated the fusion of theinformation from facial expressions and head movemente.g., 27, 69, 158, 160, and 134, the fusion of facialexpression and body gesture e.g., 7, 61, 62, and 134,and the fusion of facial expressions and postures from asensor chair e.g., 72 and 73, with the aim of improvingaffect recognition performance.Finally, virtually all present approaches to automaticfacial expression analysis are context insensitive. Exceptionsto this overall state of the art in the field include just a fewstudies. For example, Pantic and Rothkrantz 104 and Faselet al. 50 investigated the interpretation of facial expressions in terms of userdefined interpretation labels. Ji et al.69 investigated the influence of context work condition,sleeping quality, circadian rhythm, environment, andphysical condition on fatigue detection, and Kapoor andPicard 73 investigated the influence of the task statesdifficulty level and game state on interest detection.Table 2 provides an overview of the currently existingexemplar systems for visionbased affect recognition withrespect to the utilized facial features, classifier, andperformance. While summarizing the performance of thesurveyed systems, we also mention a number of relevantaspects, including the following1. type of the utilized data spontaneous or posed, thenumber of different subjects, and sample size,2. whether the system is person dependent or independent,3. whether it performs in a realtime condition,4. what the number of target classification categories is,5. whether and which other cues, aside from the face,have been used in the classification head, body,eye, posture, task state, and other contexts,6. whether the system processes still images or videos,and7. how accurately it performs the target classification.A missing entry means that the matter at issue was notreported or it remained unclear from the availableliterature. For instance, some studies did not explicitlyindicate whether the recordings of the same subjects wereused as both the testing data and the training data. Hence, itremains unclear whether these systems perform in asubjectindependent manner. It is important to stress thatwe cannot rank the performances of the surveyed systemsbecause each of the relevant studies has been conductedunder different experimental conditions using differentdata, different testing methods such as person dependentindependent, and different performance measurementsaccuracy, equal error rate, etc..The research on the machine analysis of facial affect hasseen a lot of progress when compared to that described inthe survey paper of Pantic and Rothkrantz 102. Thecurrent state of the art in the field is listed as follows. Methods have been proposed to detect attitudinaland nonbasic affective states such as confusion,boredom, agreement, fatigue, frustration, and painfrom facial expressions e.g., 69, 72, 129, 147,and 87.. Initial efforts were conducted to analyze andautomatically discern posed deliberate facial displays from genuine spontaneous displays e.g.,135 and 134.. First attempts are reported toward the visionbasedanalysis of spontaneous human behavior based on3D face models e.g., 123 and 149, based onfusing the information from facial expressions andhead gestures e.g., 27 and 134, and based on46 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 31, NO. 1, JANUARY 2009Authorized licensed use limited to IEEE Transactions on SMC Associate Editors. Downloaded on November 30, 2008 at 1010 from IEEE Xplore.  Restrictions apply.fusing the information from facial expressions andbody gestures e.g., 61.. Few attempts have also been made toward thecontextdependent interpretation of the observedfacial behavior e.g., 50, 69, 72, and 104.. Advanced techniques in feature extraction andclassification have been applied and extended inthis field. A few realtime robust systems have beenbuilt e.g., 11 thanks to the advance of relevanttechniques such as realtime face detection andobject tracking.3.3 AudioBased Affect RecognitionResearch on vocal affect recognition is also largelyinfluenced by a basic emotion theory. In turn, most of theexisting efforts in this direction aim at the recognition of asubset of basic emotions from speech signals. However, afew tentative studies were published recently on theinterpretation of speech signals in terms of certain applicationdependent affective states. These studies are those ofthe followingZENG ET AL. A SURVEY OF AFFECT RECOGNITION METHODS AUDIO, VISUAL, AND SPONTANEOUS EXPRESSIONS 47TABLE 2VisionBased Affect Recognitionexp SpontaneousPosed expression, per person DependentIndependent, ImVi ImageVideo based, cues other cues aside from the face HeadBodyEyeSkinPostureTask statepressure MouseUserdefined classesotherContext, rea real time Y yes, and N no, class number of classes,sub number of subjects, samp sample size, acc Accuracy, AUs AUs corresponding to AU detection, min minutes, EER equal error rate, FARfalse acceptance rate, and GP Gaussian process.AAI, BU, CH, CK, FABO, MMI, RU, and SD are the database names listed in Table 1.EH the EkmanHager database, OD Other database, and  missing entry.Authorized licensed use limited to IEEE Transactions on SMC Associate Editors. Downloaded on November 30, 2008 at 1010 from IEEE Xplore.  Restrictions apply.1. Hirschberg et al. 65 and Graciarena et al. 57, whoattempted deception detection,2. Liscombe et al. 84, who focused on detectingcertainty,3. Kwon et al. 79, who reported on stress detection,4. Zhang et al. 157, who investigated speechbasedanalysis of confidence, confusion, and frustration,5. Batliner et al. 12, who aimed at detecting trouble,6. Ang et al. 3, who explored speechbased recognition of annoyance and frustration, and7. Steidl et al. 125, who conducted studies on thedetection of empathy.In addition, few efforts toward the automatic recognition ofnonlinguistic vocalizations like laughters 133, coughs 91and cries 97 have also been reported recently. This is ofparticular importance for the research on the machineanalysis of human affects since recent studies in cognitivesciences showed that listeners seem to be rather accurate indecoding some nonbasic affective states such as distress,anxiety, boredom, and sexual interest from nonlinguisticvocalizations like laughs, cries, sighs, and yawns 113.Most of the existing systems for automatic vocal affectrecognition were trained and tested on speech data that wascollected by asking actors to speak prescribed utteranceswith certain emotions e.g., 6 and 79. As the utterancesare isolated from the interaction context, this experimentalstrategy precludes finding and using correlations betweenthe paralinguistic displays and the linguistic content, whichseem to play an important role for affect recognition in dailyinterpersonal interactions.Based on the above consideration, researchers started tofocus on affect recognition in naturalistic audio recordingscollected in call centers e.g., 35, 82, 83, and 94,meetings e.g., 94, Wizard of Oz scenarios e.g., 12,interviews e.g., 65, and other dialogue systems e.g., 14and 86. In these natural interaction data, affect displaysare often subtle, and basic emotion expressions seldomoccur. It is therefore not surprising that recent studies in thefield, which are based on such data, attempt to detect eithercoarse affective states, i.e., positive, negative, and neutralstates e.g., 82, 83, 86, and 94 or applicationdependent states mentioned above rather than basicemotions.Most of the existing approaches to vocal affect recognition used acoustic features as classification input based onthe acoustic correlation for emotion expressions that weresummarized in 31. The popular features are prosodicfeatures e.g., pitchrelated feature, energyrelated features,and speech rate and spectral features e.g., MFCC andcepstral features. Many studies show that pitch and energyamong these features contribute the most to affect recognition e.g., 79. An exemplar effort is that of Vasilescu andDevillers 36, who show the relevance of speech disfluencies e.g., filler and silence pauses to affect recognition.With the research shift toward the analysis of spontaneous human behavior, the analysis of acoustic informationwill not only suffice for identifying subtle changes in vocalaffect expression. As indicated by Batliner et al. 12, Thecloser we get to a realistic scenario, the less reliable isprosody as an indicator of the speakers emotional state. Inthe preliminary experiments of Devillers and Vidrascu 35,using lexical cues resulted in a better performance thanusing paralinguistic cues to detect relief, anger, fear, andsadness in humanhuman medical call conversations. Inturn, several studies investigated the combination ofacoustic features and linguistic features language anddiscourse to improve vocal affect recognition performance.Typical examples of linguisticparalinguistic fusion methods are those of the following1. Litman and ForbesRiley 86 and Schuller et al.120, who used spoken words and acoustic features,2. Lee and Narayanan 83, who used prosodic features,spoken words and information of repetition,3. Graciarena et al. 57, who combined prosodic,lexical and cepstral features, and4. Bartliner et al. 12, who used prosodic features, partof speech POS, dialogue act DA, repetitions,corrections, and syntacticprosodic boundary to inferthe emotion.Litman et al. 86 and ForbesRiley and Litman 52 alsoinvestigated the role of the context information e.g.,subject, gender, and turnlevel features representing localand global aspects of the dialogue on audio affectiverecognition.Although the above studies indicated recognition improvement by using information on language, discourse,and context, the automatic extraction of these relatedfeatures is a difficult problem. First, existing automaticspeech recognition ASR systems cannot reliably recognizethe verbal content of emotional speech e.g., 5. Second,extracting semantic discourse information is even morechallenging. Most of these features are typically extractedmanually or directly from transcripts.Table 3 provides an overview of the currently existingexemplar systems for audiobased affect recognition withrespect to the utilized auditory features, classifier, andperformance. As in Table 2, we specify relevant aspects inTable 3 to summarize the reported performance of surveyedsystems.The current state of the art in the research field ofautomatic audiobased affect recognition can be summarized as follows. Methods have been proposed to detect nonbasicaffective states, including coarse affective states suchas negative and nonnegative states e.g., 83,applicationdependent affective states e.g., 3,12, 65, 79, 157, and 125, and nonlinguisticvocalizations like laughter and cry e.g., 133, 91,and 97.. A few efforts have been made to integrate paralinguistic features and linguistic features such aslexical, dialogic, and discourse features e.g., 12,35, 57, 83, 86, and 120.. Few investigations have been conducted to make useof contextual information to improve the affectrecognition performance e.g., 52 and 86.. Few reported studies have analyzed the affectivestates across languages e.g., 94 and 133.48 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 31, NO. 1, JANUARY 2009Authorized licensed use limited to IEEE Transactions on SMC Associate Editors. Downloaded on November 30, 2008 at 1010 from IEEE Xplore.  Restrictions apply.. Some studies have investigated the influence ofambiguity of human labeling on recognition performance e.g., 3 and 86 and proposed measures ofcomparing human labelers and machine classifierse.g., 125.. Advanced techniques in feature extraction, classification, and natural language processing have beenapplied and extended in this field. Some studieshave been tested on commercial call data e.g., 83and 35.3.4 Audiovisual Affect RecognitionIn the survey of Pantic and Rothkrantz 102, only fourstudies were found to focus on audiovisual affect recognition. Since then, an increasing number of efforts arereported in this direction. Similar to the state of the art insinglemodal affect recognition, most of the existingaudiovisual affect recognition studies investigated therecognition of the basic emotions from deliberate displays.Relatively few efforts have been reported toward thedetection of nonbasic affective states from deliberatedisplays. Those include the work of Zeng et al. 150,153, 154, 155 and that of Sebe et al. 122, who addedfour cognitive states, considering the importance of thesecognitive states in HCI1. interest,2. puzzlement,3. frustration, and4. boredom.Related studies conducted on naturalistic data includethat of Pal et al. 97, who designed a system for detectinghunger and pain, as well as sadness, anger, and fear, frominfant facial expressions and cries, and that of Petridisand Pantic 108, who investigated separating speechZENG ET AL. A SURVEY OF AFFECT RECOGNITION METHODS AUDIO, VISUAL, AND SPONTANEOUS EXPRESSIONS 49TABLE 3AudioBased Affect Recognitionexp SpontaneousPosed expression, per person DependentIndependent, cont contextual information SubjectGenderTaskSpeakerRoleSpeakerDependentFeature, class the number of classes, sub the number of subjects. samp sample size the number of utterances, accaccuracy,  missing entry, BL Baseline, EER equal error rate, NPN negativeneutralpositive, NnN negativenonnegative, EnE emotionalnonemotional, M male, F female, A actor data, R reading data, W data of Wizard of Oz, and OD other database.ACC, AIBO, CSC, and ISL are the database names listed in Table 1.Authorized licensed use limited to IEEE Transactions on SMC Associate Editors. Downloaded on November 30, 2008 at 1010 from IEEE Xplore.  Restrictions apply.from laughter episodes based on both facial and vocalexpression.Most of the existing methods for audiovisual affectanalysis are based on deliberately posed affect displayse.g., 16, 56, 66, 122, 124, 138, 150, 153, 154, and155. Recently, a few exceptional studies have beenreported toward audiovisual affect analysis in spontaneousaffect displays e.g., 17, 53, 74, 97, 151, and 108.Zeng et al. 151 used the data collected in psychologicalresearch interview Adult Attachment Interview, Pal et al.97 used recordings of infants 97, and Petridis and Pantic108 used the recordings of people engaged in meetingsAMI corpus. On the other hand, Fragopanagos and Taylor53, Caridakis et al. 17, and Karpouzis et al. 74, used thedata collected in Wizard of Oz scenarios. Since the availabledata were usually insufficient to build a robust machinelearning system for the recognition of finegrained affectivestates e.g., basic emotions, the recognition of coarseaffective states was attempted in most of the aforementioned studies. The studies of Zeng et al. focus onaudiovisual recognition of positive and negative affect151, while other studies report on the classification ofaudiovisual input data into the quadrants in the evaluationactivation space 17, 53, 74. The studies reported in 17,53, and 74 applied the Feeltrace system that enablesraters to continuously label changes in affective expressions. However, note that the study discussed in 53reported on a considerable labeling variation among fourhuman raters due to the subjectivity of audiovisual affectjudgment. More specifically, one of the raters mainly reliedon audio information when making judgments, whileanother rater mainly relied on visual information. Thisexperiment actually also reflects the asynchronization ofaudio and visual expression. In order to reduce thisvariation of human labels, the studies of Zeng et al. 151made the assumption that facial expression and vocalexpression have the same coarse emotional states positiveand negative and then directly used FACSbased labels offacial expressions as audiovisual expression labels.The data fusion strategies utilized in the current studieson audiovisual affect recognition are featurelevel, decisionlevel, or modellevel fusion. Typical examples of featurelevel fusion are those reported in 16, 118, and 156,which concatenated the prosodic features and facialfeatures to construct joint feature vectors, which are thenused to build an affect recognizer. However, the differenttime scales and metric levels of features coming fromdifferent modalities, as well as increasing featurevectordimensions influence the performance of an affect recognizer based on a featurelevel fusion. The vast majority ofstudies on bimodal affect recognition reported on decisionlevel data fusion e.g., 16, 56, 66, 97, 151, 153, 155,138, and 108. In the decisionlevel data fusion, the inputcoming from each modality is modeled independently, andthese singlemodal recognition results are combined in theend. Since humans display audio and visual expressions ina complementary redundant manner, the assumption ofconditional independence between audio and visual datastreams in decisionlevel fusion is incorrect and results inthe loss of information of mutual correlation between thetwo modalities. To address this problem, a number ofmodellevel fusion methods have been proposed which aimat making use of the correlation between audio and visualdata streams and relaxing the requirement of synchronization of these streams e.g., 17, 53, 122, 124, 150, and154. Zeng et al. 154 presented a Multistream FusedHMM to build an optimal connection among multiplestreams from audio and visual channels according to themaximum entropy and the maximum mutual informationcriterion. Zeng et al. 150 extended this fusion frameworkby introducing a middlelevel training strategy, underwhich a variety of learning schemes can be used to combinemultiple component HMMs. Song et al. 124 presented atripled HMM to model the correlation properties of threecomponent HMMs that are based individually on upperface, lower face, and prosodic dynamic behaviors. Fragopanagos and Taylor 53 proposed an artificial neuralnetwork NN with a feedback loop called ANNA tointegrate the information from face, prosody, and lexicalcontent. Caridakis et al. 17, Karpouzis et al. 74, andPetridis and Pantic 108 investigated combining the visualand audio data streams by using NNs. Sebe et al. 122 useda Bayesian network BN to fuse the facial expression andprosody expression.Table 4 provides an overview of the currently existingexemplar systems for audiovisual affect recognition withrespect to the utilized auditory and visual features,classifier, and performance. As in Tables 2 and 3, we alsospecify a number of relevant issues in Table 4 to summarizethe reported performance of surveyed systems.In summary, research on audiovisual affect recognitionhas witnessed significant progress in the last few years asfollows. Efforts have been reported to detect and interpretnonbasic genuine spontaneous affective displays interms of coarse affective states such as positive andnegative affective states e.g., 151, quadrants in theevaluationactivation space e.g., 17, 53, and 74,and applicationdependent states e.g., 122, 154,97, and 108.. Few studies have been reported on efforts tointegrate other affective cues aside from the faceand the prosody such as body and lexical featurese.g., 53 and 74.. Few attempts have been made to recognize affectivedisplays in specific naturalistic settings e.g., in a car66 and in multiple languages e.g., 138.. Various multimodal data fusion methods have beeninvestigated. In particular, some advanced datafusion methods have been proposed, such asHMMbased fusion e.g., 124, 154, and 150,NNbased fusion e.g., 53 and 74, and BNbasedfusion e.g., 122.4 CHALLENGESThe studies reviewed in Section 3 indicate two new trendsin the research on automatic human affect recognition the50 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 31, NO. 1, JANUARY 2009Authorized licensed use limited to IEEE Transactions on SMC Associate Editors. Downloaded on November 30, 2008 at 1010 from IEEE Xplore.  Restrictions apply.analysis of spontaneous affective behavior and the multimodal analysis of human affective behavior, includingaudiovisual analysis, combined linguistic and nonlinguisticanalysis, and multicue visual analysis based on facialexpressions, head movements, andor body gestures.Several previously recognized problems have been addressed, including the development of more comprehensivedata sets of training and testing materials. At the same time,several new challenging issues have been recognized,including the necessity of studying the temporal correlations between the different modalities audio and visualand between various behavioral cues e.g., facial, head, andbody gestures. This section discusses these issues in detail.4.1 DatabasesAcquiring valuable spontaneous affective behavior dataand the related ground truth is far from being solved. Whileit is relatively easy to elicit joyful laughter by showing thesubjects clips from comedies, the majority of affective statesare much more difficult if possible at all to elicit e.g., fear,stress, sadness, or anger, which is particularly difficult toelicit in any laboratory setting, including facetofaceconversation 23. Social psychology has provided a hostof creative strategies for inducing emotion, which seem tobe useful for collecting affective expressions that aredifficult to elicit in the laboratory and affective expressionsthat are contextually complex such as embarrassment orfor research programs that emphasize the mundanerealism of experimentally elicited emotions 23. However,engineers, who are usually the designers of the databases ofhuman behavior data, are often not even aware of thesestrategies, let alone putting them into practice. Thissituation needs to be changed if the challenging and crucialissue of collecting valuable data on human spontaneousaffective behavior is to be addressed.Although many efforts have been done toward thecollection of databases of spontaneous human affectivebehavior, most of the data contained in the availabledatabases currently lack labels. In other words, no metadatais available which could identify the affective state displayed in a video sample and the context in which thisaffective state was displayed. There are some related issues.First, it is not clear which kind of metadata needs to beprovided. While data labeling is easy to accomplish in thecase of prototypical expressions of emotions, it becomes areal challenge once we move beyond the six basic emotions.To reduce the subjectivity of data labeling, it is generallyaccepted that human facial expression data need to beFACS coded. The main reason is that FACS AUs areobjective descriptors and independent of interpretation andcan be used for any highlevel decisionmaking process,including the recognition of affective states. However, whilethis solves the problem of attaining objective facial behaviorcoding, how one can objectively code vocal behaviorZENG ET AL. A SURVEY OF AFFECT RECOGNITION METHODS AUDIO, VISUAL, AND SPONTANEOUS EXPRESSIONS 51TABLE 4Audiovisual Affect RecognitionFusion FeatureDecisionModellevel, exp SpontaneousPosed expression, per personDependentIndependent, class the number of classes,sub the number of subjects, samp sample size the number of utterances, cue other cues LexicalBody, acc accuracy, RR mean with weightedrecall values, FAP facial animation parameter, and  missing entry.AAI, CH, SAL, and SD are the database names listed in Table 1.Authorized licensed use limited to IEEE Transactions on SMC Associate Editors. Downloaded on November 30, 2008 at 1010 from IEEE Xplore.  Restrictions apply.remains an open issue. Nonlinguistic vocalizations likelaughter, coughs, cries, etc., can be labeled as such, but thereis no set of interpretationindependent codes for labelingemotional speech. Another related issue is that of cultureand context dependency. The metadata about the context inwhich the recordings were made, such as the utilizedstimuli, the environment, and the presence of other people,is needed, since these contextual variables may influencemasking of the emotional reactions.Second, even if labeled data are available, engineersresponsible for designing an automated human affectanalyzer usually assume that the data are accuratelylabeled. This assumption may or may not be accurate 26,125. The reliability of the coding can be ensured by askingseveral independent human observers to conduct thecoding. If the interobserver reliability is high, the reliabilityof the coding is assured. Interobserver reliability can beimproved by providing thorough training to observers onthe utilized coding schemes such as FACS. When it comesto data coding in terms of affect labels, a possible method isto use a multilabel multitimescale system in order toreduce the subjectivity of human judgment and to representcomprehensive properties of affect displays 37, 80.Third, human labeling of affective behavior is very timeconsuming and expensive. In the case of facial expressiondata, it takes more than 1 hour to manually score 100 stillimages or 1 minute of video sequence in terms of AUs 43.A remedy could be the semisupervised active learningmethod 159, which combines semisupervised learning 24and active learning 51. The semisupervised learningmechanism aims at making use of the unlabeled data, andthe active learning mechanism aims at enlarging the usefulinformation conveyed by human feedback annotation inthis application and provides the annotators with the mostambiguous samples according to the current emotionclassifier. More specifically, several promising prototypesystems were reported in the last few years which canrecognize deliberately produced AUs in either nearfrontal view face images e.g., 98 and 130 or profileview face images e.g., 99. Although these systems willnot be always able to be generalized to the subtlety andcomplexity of human affective behavior occurring in realworld settings, they can be used to attain an initial datalabeling that can be subsequently controlled and correctedby human observers. However, as this has not beenattempted in practice, there is no guarantee that such anapproach will actually reduce the time needed for obtainingthe ground truth. Future research is needed to determinewhether this attempt is feasible.Although much effort has been done toward thecollection of databases of spontaneous human affectivebehavior, many of these data sets are not publicly availablesee Table 1. Some are still under construction, some are inthe process of data publication, and some seem to have dimprospects of being published due to the lack of appropriateagreement of subjects. More specifically, spontaneous displays of emotions, especially in audiovisual format, revealpersonal and intimate experience privacy issues jeopardizethe public accessibility of many databases.Aside from these problems concerned with acquiringvaluable data, the related ground truth, and the agreementof subjects to make the data publicly available, anotherimportant issue is how one can construct and administersuch a large affective expression benchmark database. Anoteworthy example is the MMI facial expression database98, 106, which was built as a Webbased directmanipulation application, allowing easy access and easysearch of the available images. In general, in the case ofpublicly available databases, once the permission for usageis issued, large unstructured files of material are sent. Suchunstructured data is difficult to explore and manage. Panticet al. 102, 106 and Cowie et al. 32 emphasized a numberof specific research and development efforts needed tobuild a comprehensive readily accessible reference set ofaffective displays that could provide a basis for benchmarksfor all different efforts in the research on the machineanalysis of human affective behavior. Nonetheless, note thattheir list of suggestions and recommendations is notexhaustive of worthwhile contributions.4.2 VisionBased Affect RecognitionAlthough several of the efforts discussed in Section 3.2 wererecently reported on the machine analysis of spontaneousfacial expressions, the problem of the automatic analysis offacial behavior in unconstrained environments is still farfrom being solved.Existing methods for the machine analysis of facial affecttypically assume that the input data are near frontal orprofileview face image sequences, showing nonoccludedfacial displays captured under constant lighting conditionagainst a static background. In real interaction environments, such an assumption is often invalid. The development of robust face detectors, head, and facial featuretrackers, which will be robust to arbitrary head movement,occlusions, and scene complexity like the presence of otherpeople and dynamic background, forms the first step in therealization of facial affect analyzers capable of handlingunconstrained environments. Viewindependent facial expression recognition based on 3D face models e.g., 20 and148 or multiview face models e.g., 160 may be a partof the solution.As mentioned already in Section 2, a growing body ofresearch in cognitive sciences argues that the dynamics ofhuman behavior are crucial for its interpretation e.g., 47and 113. For instance, it has been shown that spontaneoussmiles are longer in the total duration, can have multipleapexes multiple rises of the mouth corners, appear beforeor simultaneously with other facial actions such as the riseof the cheeks, and are slower in the onset and offset timesthan the posed smiles e.g., a polite smile 28. In spite ofthese findings, the vast majority of the past work in the fielddoes not take the dynamics of facial expressions intoaccount when analyzing shown facial behavior. Some of thepast work in the field has used aspects of temporal structureof facial expression such as the speed of a facial pointdisplacement or the persistence of facial parameters overtime e.g., 87, 132, and 158. However, just a few recentstudies analyze explicitly the temporal structure of facialexpressions e.g., 98, 99, 135, 132, and 134. Inaddition, it remains unresolved how the grammar of facialbehavior can be learned and how this information can beproperly represented and used to handle ambiguities in theinput data 100, 102.Except for a few studies e.g., 27 and 61, existingefforts toward the machine analysis of human facial52 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 31, NO. 1, JANUARY 2009Authorized licensed use limited to IEEE Transactions on SMC Associate Editors. Downloaded on November 30, 2008 at 1010 from IEEE Xplore.  Restrictions apply.behavior focus only on the analysis of facial gestureswithout taking into consideration other visual cues likehead movements, gaze direction, and body gestures.However, research on cognitive science reports that humanjudgments of behavioral cues are the most accurate whenboth the face and the body are taken into account e.g., 1and 117. This seems to be of particular importance whenjudging certain complex mental states such as embarrassment 75. However, integration, temporal structures, andtemporal correlations between different visual cues arevirtually unexplored areas of research. One noteworthystudy that investigated fully the automatic coding of humanbehavior dynamics with respect to both the temporalsegments onset, apex, offset, and neutral of various visualcues and the temporal correlation between different visualcues facial, head, and shoulder movements is that ofValstar et al. 134, who investigated separating posed fromgenuine smiles in video sequences.4.3 AudioBased Affect RecognitionOne challenge in audio expression analysis is how we canidentify affectrelated features in speech signals. When ouraim is to detect spontaneous emotion expressions, we haveto take into account both linguistic and paralinguistic cuesthat mingle together in audio channel. Although a numberof linguistic and paralinguistic features e.g., prosodic,dysfluency, lexicon, and discourse features were proposedin the body of literature on vocal affect recognition, theoptimal feature set has not yet been established.Another challenge is how we can reliably extract theselinguistic and paralinguistic features from the audio signalsin an automatic way. When prosody is analyzed in anaturalistic conversation, we have to consider the multiplefunctions of prosody that include information about theexpressed affect and a variety of linguistic functions 93. Aprosodic event model that could reflect both linguistic andparalinguistic affective functions simultaneously wouldbe an ideal solution. Automatic extraction of spoken wordsfrom spontaneous emotional speech is still a difficultproblem the recognition rate of the exiting ASR systemsdrops significantly as soon as emotional speech needs to beprocessed. Some tentative studies on adapting an ASRsystem to emotional speech were reported in 5 and 119.We hope that, in the future, more such studies will beconducted. In addition, automatic extraction of highlevelsemantic linguistic information e.g., DA, repetitions,corrections, and syntactic information is an even morechallenging problem in the research field of naturallanguage processing.It is interesting to note that some mental states such asfrustration and boredom seem to be identifiable fromnonlinguistic vocalizations like sighs and yawns 113.Few efforts toward the automatic recognition of nonlinguistic vocalizations like laughers 133, 108, cries 97,and coughs 91 have also been recently reported. However,no effort toward human affect analysis based on vocaloutbursts has been reported so far.4.4 Audiovisual Affect RecognitionThe research on audiovisual affect analysis in naturalisticdata is still in its pioneering phase. While all agree thatmultisensory fusion, including audiovisual data fusion,linguistic and paralinguistic data fusion, and multivisualcue data fusion, would be highly beneficial for the machineanalysis of human affect, it remains unclear how this shouldbe accomplished. Studies in neurology on the fusion ofsensory neurons 126 are supportive of early data fusioni.e., featurelevel data fusion rather than of late data fusioni.e., decisionlevel fusion. However, it is an open issuehow one can construct suitable joint feature vectorscomposed of features from different modalities withdifferent time scales, different metric levels, and differenttemporal structures. Simply concatenating audio and videofeatures into a single feature vector, as done in the currenthuman affect analyzers that use feature level data fusion, isobviously not the solution to the problem.Due to these difficulties, most researchers choosedecisionlevel fusion, in which the input coming from eachmodality is modeled independently, and these singlemodalrecognition results are combined in the end. Decisionlevelfusion, also called classifier fusion, is now an active area inthe machine learning and pattern recognition fields. Manystudies have demonstrated the advantage of classifierfusion over the individual classifiers due to the uncorrelated errors from different classifiers e.g., 78 and 112.Various classifier fusion methods fixed rules and trainedcombiners have been proposed in the literature, butoptimal design methods for classifier fusion are still notavailable. In addition, since humans simultaneously employthe tightly coupled audio and visual modalities, themultimodal signals cannot be considered mutually independent and should not be combined only in the end, as inthe case of decisionlevel fusion.Modellevel fusion or hybrid fusion that aims atcombining the benefits of both featurelevel and decisionlevel fusion methods may be a good choice for this fusionproblem. However, based on existing knowledge andmethods, how one can model multimodal fusion based onmultilabel multitimescale labeling scheme mentionedabove is largely unexplored. A number of issues relevantto fusion require further investigation, such as the optimallevel of integrating these different streams, the optimalfunction for the integration, and the inclusion of suitableestimations of the reliability of each stream. In addition,how one can build contextdependent multimodal fusion isan open and highly relevant issue.Here, we want to stress that temporal structures of themodalities facial and vocal and their temporal correlationsplay an extremely important role in the interpretation ofhuman naturalistic audiovisual affective behavior seeSection 2 for a discussion. Yet, these are virtuallyunexplored areas of research due to the fact that the facialexpression and vocal expression of emotion are usuallystudied separately.4.5 A Few Additional Related IssuesContext. An important related issue that should beaddressed in all visual, vocal, and audiovisual affectrecognition is how one can make use of information aboutthe context environment, observed subject, or the currenttask, in which the observed affective behavior wasdisplayed. Affects are intimately related to a situation beingexperienced or imagined by humans. Without context,humans may misunderstand the observed persons emotionexpressions. Yet, with the exception of a few studies thatZENG ET AL. A SURVEY OF AFFECT RECOGNITION METHODS AUDIO, VISUAL, AND SPONTANEOUS EXPRESSIONS 53Authorized licensed use limited to IEEE Transactions on SMC Associate Editors. Downloaded on November 30, 2008 at 1010 from IEEE Xplore.  Restrictions apply.investigated the influence of context on affect recognitione.g., 50, 52, 69, 72, 86, and 104, virtually allexisting approaches to the machine analysis of human affectare context insensitive. Building a context model thatincludes person ID, gender, age, conversation topic, andworkload need help from other research fields like facerecognition, gender recognition, age recognition, topicdetection, and task tracking. Since the problem of contextsensing is very difficult to solve, pragmatic approachese.g., activity andor subjectprofiled approaches should betaken.Segmentation. Almost all of the existing methods aretested just on presegmented emotion sequences or images,except for a few studies e.g., 11 and 25 that useheuristic methods for segmenting the emotions from videos.Automatic continuous emotion recognition is a dynamicsearching process that continuously makes emotion inference in the presence of signal ambiguity and context. This israther complicated, since the search algorithm has toconsider the possibility of each emotion starting at anyarbitrary time frame. Furthermore, the number of emotionschanging in a video is not known, and the boundariesbetween different emotional expressions are full of ambiguity. It becomes more challenging in multimodal affectrecognition because different modalities e.g., face, body,and vocal expressions have different temporal structuresand often do not synchronize. If we aim at developing apractical affect recognizer, the emotion segmentation is oneof the most important issues but has not been largelyunexplored so far.Evaluation. The existing methods for the machineanalysis of human affect surveyed and discussed throughout this paper are difficult to compare because they arerarely if ever tested on a common data set. United effortsof the relevant research communities are needed to specifyevaluation procedures that could be used for establishingreliable measures of systems performance based on acomprehensive readily accessible benchmark database.5 CONCLUSIONResearch on the machine analysis of human affect haswitnessed a good deal of progress when compared to thatdescribed in the survey papers of Pantic and Rothkrantz in2003 102 and Cowie et al. in 2001 31. At those times, afew smallsized data sets of affective displays existed, andalmost all methods for the machine analysis of human affectwere unimodal based on deliberate displays of either facialexpressions or vocal expressions of six prototypical emotions. Available data was not shared among researchers,multimedia data and multimodal human affect analyzerswere rare, and the machine analysis of spontaneousdisplays of affective behavior seemed to be in a distantfuture. Today, several large collections of acted affectivedisplays are shared by the researchers in the field, and somedata sets of spontaneously displayed expressions have beenrecently made available. A number of promising methodsfor visionbased, audiobased, and audiovisual analysis ofhuman spontaneous behavior have been proposed. Thispaper focused on surveying and discussing these novelapproaches to the machine analysis of human affect and onsummarizing the issues that have not received sufficientattention but are crucial for advancing the machineinterpretation of human behavior in naturalistic contexts.The most important of these issues yet to be addressed inthe field include the following. building a comprehensive readily accessible reference set of affective displays, which could provide abasis for benchmarks for all different efforts in theresearch on the machine analysis of human affectivebehavior, and defining the appropriate evaluationprocedures,. developing methods for spontaneous affective behavior analysis, which are robust to observed personsarbitrary movement, occlusion, and complex andnoisy background,. devising models and methods for human affectanalysis, which take into consideration the temporalstructures of the modalities and the temporalcorrelations between the modalities andor multiple cues and context subject, the task, andenvironment, and. developing better methods for multimodal fusion.Since the complexity of these issues concerned with theinterpretation of human behavior at a deeper level istremendous and spans several different disciplines incomputer and social sciences, we believe that a largeinterdisciplinary international program directed towardcomputer understanding of human behavioral patternsshould be established if we are to experience true breakthroughs in this and the related research fields. Theprogress in research on the machine analysis of humanaffect can aid in the creation of a new paradigm for HCIaffectsensitive interfaces and socially intelligent environments and advance the research in several related fields,including psychology, psychiatry, and education.ACKNOWLEDGMENTSThe authors would like to thank Qiang Ji and theanonymous reviewers for encouragement and valuablecomments. This paper is a collaborative work. ThomasHuang is the leader of this team work but prefers to be thelast in the author list. Zhihong Zeng wrote the first draft,Maja Pantic significantly improved it by rewriting it andoffering important advice, and Glenn Roisman providedimportant comments and polished the whole paper.Zhihong Zeng and Thomas S. Huangs work in this paperwas supported in part by a Beckman Postdoctoral Fellowship, US National Science Foundation Grant CCF 0426627and the US Government VACE Program. Maja Panticsresearch that lead to these results was funded in part by theEC FP7 Programme FP720072013 under grant agreement no. 211486 SEMAINE and the European ResearchCouncil under the ERC Starting Grant agreement No. ERC2007StG203143 MAHNOB.REFERENCES1 N. Ambady and R. Rosenthal, Thin Slices of Expressive Behavioras Predictors of Interpersonal Consequences, A MetaAnalysisPsychological Bull., vol. 111, no. 2, pp. 256274, 1992.54 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 31, NO. 1, JANUARY 2009Authorized licensed use limited to IEEE Transactions on SMC Associate Editors. Downloaded on November 30, 2008 at 1010 from IEEE Xplore.  Restrictions apply.2 K. Anderson and P.W. McOwan, A RealTime AutomatedSystem for Recognition of Human Facial Expressions, IEEETrans. Systems, Man, and Cybernetics Part B, vol. 36, no. 1, pp. 96105, 2006.3 J. Ang et al., ProsodyBased Automatic Detection of Annoyanceand Frustration in HumanComputer Dialog, Proc. Eighth IntlConf. Spoken Language Processing ICSLP, 2002.4 A.B. Ashraf, S. Lucey, J.F. Cohn, T. Chen, Z. Ambadar, K.Prkachin, P. Solomon, and B.J. Theobald, The Painful Face PainExpression Recognition Using Active Appearance Models, Proc.Ninth ACM Intl Conf. Multimodal Interfaces ICMI 07, pp. 914,2007.5 T. Athanaselis, S. Bakamidis, I. Dologlou, R. Cowie, E. DouglasCowie, and C. Cox, ASR for Emotional Speech Clarifying theIssues and Enhancing Performance, Neural Networks, vol. 18,pp. 437444, 2005.6 A. Austermann, N. Esau, L. Kleinjohann, and B. Kleinjohann,ProsodyBased Emotion Recognition for MEXI, Proc. IEEERSJIntl Conf. Intelligent Robots and Systems IROS 05, pp. 11381144,2005.7 T. Balomenos, A. Raouzaiou, S. Ioannou, A. Drosopoulos, K.Karpouzis, and S. Kollias, Emotion Analysis in ManMachineInteraction Systems, LNCS 3361, pp. 318328, 2005.8 R. Banse and K.R. Scherer, Acoustic Profiles in Vocal EmotionExpression, J. Personality Social Psychology, vol. 70, no. 3, pp. 614636, 1996.9 M.S. Bartlett, G. Littlewort, P. Braathen, T.J. Sejnowski, and J.R.Movellan, A Prototype for Automatic Recognition of Spontaneous Facial Actions, Advances in Neural Information ProcessingSystems, vol. 15, pp. 12711278, 2003.10 M.S. Bartlett, G. Littlewort, M. Frank, C. Lainscsek, I. Fasel, and J.Movellan, Recognizing Facial Expression Machine Learning andApplication to Spontaneous Behavior, Proc. IEEE Intl Conf.Computer Vision and Pattern Recognition CVPR 05, pp. 568573,2005.11 M.S. Bartlett, G. Littlewort, M.G. Frank, C. Lainscsek, I. Fasel, andJ. Movellan, Fully Automatic Facial Action Recognition inSpontaneous Behavior, Proc. IEEE Intl Conf. Automatic Face andGesture Recognition AFGR 06, pp. 223230, 2006.12 A. Batliner, K. Fischer, R. Hubera, J. Spilkera, and E. Noth, Howto Find Trouble in Communication, Speech Comm., vol. 40,pp. 117143, 2003.13 A. Batliner et al., You Stupid Tin BoxChildren Interactingwith the AIBO Robot A CrossLinguistic Emotional Speech,Proc. Fourth Intl Conf. Language Resources and Evaluation, 2004.14 C. Blouin and V. Maffiolo, A Study on the Automatic Detectionand Characterization of Emotion in a Voice Service Context, Proc.Ninth European Conf. Speech Comm. and Technology INTERSPEECH05, pp. 469472, 2005.15 S. Burger, V. MacLaren, and H. Yu, The ISL Meeting Corpus TheImpact of Meeting Type on Speech Style, Proc. Eighth Intl Conf.Spoken Language Processing ICSLP, 2002.16 C. Busso et al., Analysis of Emotion Recognition Using FacialExpressions, Speech and Multimodal Information, Proc. SixthACM Intl Conf. Multimodal Interfaces ICMI 04, pp. 205211, 2004.17 G. Caridakis, L. Malatesta, L. Kessous, N. Amir, A. Paouzaiou,and K. Karpouzis, Modeling Naturalistic Affective States viaFacial and Vocal Expression Recognition, Proc. Eighth ACM IntlConf. Multimodal Interfaces ICMI 06, pp. 146154, 2006.18 Y. Chang, C. Hu, and M. Turk, Probabilistic Expression Analysison Manifolds, Proc. IEEE Intl Conf. Computer Vision and PatternRecognition CVPR 04, vol. 2, pp. 520527, 2004.19 Y. Chang, C. Hu, R. Feris, and M. Turk, Manifold Based Analysisof Facial Expression, J. Image and Vision Computing, vol. 24, no. 6,pp. 605614, 2006.20 Y. Chang, M. Vieira, M. Turk, and L. Velho, Automatic 3D FacialExpression Analysis in Videos, Proc. IEEE Intl Workshop Analysisand Modeling of Faces and Gestures AMFG 05, vol. 3723, pp. 293307, 2005.21 L.S. Chen, Joint Processing of AudioVisual Information for theRecognition of Emotional Expressions in HumanComputerInteraction, PhD dissertation, Univ. of Illinois, UrbanaChampaign, 2000.22 L. Chen, T.S. Huang, T. Miyasato, and R. Nakatsu, MultimodalHuman EmotionExpression Recognition, Proc. IEEE Intl Conf.Automatic Face and Gesture Recognition AFGR 98, pp. 396401,1998.23 J.A. Coan and J.J.B. Allen, Handbook of Emotion Elicitation andAssessment. Oxford Univ. Press, 2007.24 I. Cohen, F. Cozman, N. Sebe, M. Cirelo, and T.S. Huang, SemiSupervised Learning of Classifiers Theory, Algorithms, and TheirApplications to HumanComputer Interaction, IEEE Trans.Pattern Analysis and Machine Intelligence, vol. 26, no. 12, pp. 15531567, Dec. 2004.25 L. Cohen, N. Sebe, A. Garg, L. Chen, and T. Huang, FacialExpression Recognition from Video Sequences Temporal andStatic Modeling, Computer Vision and Image Understanding, vol. 91,nos. 12, pp. 160187, 2003.26 J.F. Cohn, Foundations of Human Computing Facial Expressionand Emotion, Proc. Eighth ACM Intl Conf. Multimodal InterfacesICMI 06, pp. 233238, 2006.27 J.F. Cohn, L.I. Reed, Z. Ambadar, J. Xiao, and T. Moriyama,Automatic Analysis and Recognition of Brow Actions and HeadMotion in Spontaneous Facial Behavior, Proc. IEEE Intl Conf.Systems, Man, and Cybernetics SMC 04, vol. 1, pp. 610616, 2004.28 J.F. Cohn and K.L. Schmidt, The Timing of Facial Motion inPosed and Spontaneous Smiles, Intl J. Wavelets, Multiresolutionand Information Processing, vol. 2, pp. 112, 2004.29 J.F. Cohn and E.Z. Tronick, MotherInfant Interaction TheSequence of Dyadic States at Three, Six, and Nine Months,Development Psychology, vol. 23, pp. 6877, 1988.30 R. Cowie, E. DouglasCowie, S. Savvidou, E. McMahon, M.Sawey, and M. Schroder, Feeltrace An Instrument for Recording Perceived Emotion in Real Time, Proc. ISCA Workshop Speechand Emotion, pp. 1924, 2000.31 R. Cowie, E. DouglasCowie, N. Tsapatsoulis, G. Votsis, S. Kollias,W. Fellenz, and J.G. Taylor, Emotion Recognition in HumanComputer Interaction, IEEE Signal Processing Magazine, pp. 3280,Jan. 2001.32 R. Cowie, E. DouglasCowie, and C. Cox, Beyond EmotionArchetypes Databases for Emotion Modeling Using NeuralNetworks, Neural Networks, vol. 18, pp. 371388, 2005.33 F. Dellaert, T. Polzin, and A. Waibel, Recognizing Emotion inSpeech, Proc. Fourth Intl Conf. Spoken Language Processing ICSLP96, pp. 19701973, 1996.34 L. Devillers and I. Vasilescu, Reliability of Lexical and ProsodicCues in Two RealLife Spoken Dialog Corpora, Proc. Fourth IntlConf. Language Resources and Evaluation LREC, 2004.35 L. Devillers and I. Vasilescu, RealLife Emotions Detection withLexical and Paralinguistic Cues on HumanHuman Call CenterDialogs, Proc. Ninth Intl Conf. Spoken Language Processing ICSLP,2006.36 I. Vasilescu and L. Devillers, Detection of RealLife Emotions inCall Centers, Proc. Ninth European Conf. Speech Comm. andTechnology INTERSPEECH, 2005.37 L. Devillers, L. Vidrascu, and L. Lamel, Challenges in RealLifeEmotion Annotation and Machine Learning Based Detection,Neural Networks, vol. 18, pp. 407422, 2005.38 E. DouglasCowie, N. Campbell, R. Cowie, and P. Roach,Emotional Speech Towards a New Generation of Database,Speech Comm., vol. 40, nos. 12, pp. 3360, 2003.39 Z. Duric, W.D. Gray, R. Heishman, F. Li, A. Rosenfeld, M.J.Schoelles, C. Schunn, and H. Wechsler, Integrating Perceptualand Cognitive Modeling for Adaptive and Intelligent HumanComputer Interaction, Proc. IEEE, vol. 90, no. 7, pp. 12721289,2002.40 P. Ekman, Universals and Cultural Differences in Facial Expressions of Emotion, Proc. Nebraska Symp. Motivation, pp. 207283,1971.41 Emotion in the Human Face, P. Ekman, ed., second ed. CambridgeUniv. Press, 1982.42 P. Ekman, Strong Evidence for Universals in Facial ExpressionsA Reply to Russells Mistaken Critique, Psychological Bull.,vol. 115, no. 2, pp. 268287, 1994.43 P. Ekman, W.V. Friesen, and J.C. Hager, Facial Action CodingSystem, A Human Face, 2002.44 NSF Understanding the Face A Human Face eStore, P. Ekman,T.S. Huang, T.J. Sejnowski, and J.C. Hager, eds., 1993.45 P. Ekman, D. Matsumoto, and W.V. Friesen, Facial Expressionin Affective Disorders, What the Face Reveals, P. Ekman andE.L. Rosenberg, eds., pp. 429439, 2005.46 P. Ekman and H. Oster, Facial Expressions of Emotion, Ann.Rev. Psychology, vol. 30, pp. 527554, 1979.ZENG ET AL. A SURVEY OF AFFECT RECOGNITION METHODS AUDIO, VISUAL, AND SPONTANEOUS EXPRESSIONS 55Authorized licensed use limited to IEEE Transactions on SMC Associate Editors. Downloaded on November 30, 2008 at 1010 from IEEE Xplore.  Restrictions apply.47 P. Ekman and E.L. Rosenberg, What the Face Reveals Basic andApplied Studies of Spontaneous Expression Using the Facial ActionCoding System, second ed. Oxford Univ. Press, 2005.48 R. El Kaliouby and P. Robinson, RealTime Inference of ComplexMental States from Facial Expression and Head Gestures, Proc.IEEE Intl Conf. Computer Vision and Pattern Recognition CVPR 04,vol. 3, p. 154, 2004.49 B. Fasel and J. Luttin, Automatic Facial Expression AnalysisSurvey, Pattern Recognition, vol. 36, no. 1, pp. 259275, 2003.50 B. Fasel, F. Monay, and D. GaticaPerez, Latent SemanticAnalysis of Facial Action Codes for Automatic Facial ExpressionRecognition, Proc. Sixth ACM Intl Workshop Multimedia Information Retrieval MIR 04, pp. 181188, 2004.51 C.N. Fiechter, Efficient Reinforcement Learning, Proc. Seventh Ann. ACM Conf. Computational Learning Theory, pp. 8897,1994.52 K. ForbesRiley and D. Litman, Predicting Emotion in SpokenDialogue from Multiple Knowledge Sources, Proc. HumanLanguage Technology Conf. North Am. Chapter of the Assoc.Computational Linguistics HLTNAACL, 2004.53 F. Fragopanagos and J.G. Taylor, Emotion Recognition inHumanComputer Interaction, Neural Networks, vol. 18, pp. 389405, 2005.54 E. Fried, The Impact of Nonverbal Communication of FacialAffect on Childrens Learning, PhD dissertation, Rutgers Univ.,1976.55 G. Furnas, T. Landauer, L. Gomes, and S. Dumais, TheVocabulary Problem in HumanSystem Communication, Comm.ACM, vol. 30, no. 11, pp. 964972, 1987.56 H.J. Go, K.C. Kwak, D.J. Lee, and M.G. Chun, EmotionRecognition from Facial Image and Speech Signal, Proc. IntlConf. Soc. of Instrument and Control Engineers, pp. 28902895,2003.57 M. Graciarena, E. Shriberg, A. Stolcke, F. Enos, J. Hirschberg,and S. Kajarekar, Combining Prosodic, Lexical and CepstralSystems for Deceptive Speech Detection, Proc. Intl Conf.Acoustics, Speech and Signal Processing ICASSP 06, vol. I,pp. 10331036, 2006.58 M. Greenwald, E. Cook, and P. Lang, Affective Judgment andPsychophysiological Response Dimensional Covariation in theEvaluation of Pictorial Stimuli, J. Psychophysiology, vol. 3, pp. 5164, 1989.59 R. Gross, Face Databases, Handbook of Face Recognition, S.Z. Liand A.K. Jain, eds., pp. 301328, Springer, 2005.60 H. Gu and Q. Ji, An Automated Face Reader for FatigueDetection, Proc. IEEE Intl Conf. Automatic Face and GestureRecognition AFGR 04, pp. 111116, 2004.61 H. Gunes and M. Piccardi, Affect Recognition from Face andBody Early Fusion versus Late Fusion, Proc. IEEE Intl Conf.Systems, Man, and Cybernetics SMC 05, pp. 34373443, 2005.62 H. Gunes and M. Piccardi, Fusing Face and Body Display for BiModal Emotion Recognition Single Frame Analysis and MultiFrame PostIntegration, Proc. First Intl Conf. Affective Computingand Intelligent Interaction ACII 05, pp. 102111, 2005.63 H. Gunes and M. Piccardi, A Bimodal Face and Body GestureDatabase for Automatic Analysis of Human Nonverbal AffectiveBehavior, Proc. 18th Intl Conf. Pattern Recognition ICPR 06,vol. 1, pp. 11481153, 2006.64 G. Guo and C.R. Dyer, Learning from Examples in the SmallSample Case Face Expression Recognition, IEEE Trans. Systems,Man, and Cybernetics Part B, vol. 35, no. 3, pp. 477488, 2005.65 J. Hirschberg, S. Benus, J.M. Brenier, F. Enos, and S. Friedman,Distinguishing Deceptive from NonDeceptive Speech, Proc.Ninth European Conf. Speech Comm. and Technology INTERSPEECH05, pp. 18331836, 2005.66 S. Hoch, F. Althoff, G. McGlaun, and G. Rigoll, Bimodal Fusionof Emotional Data in an Automotive Environment, Proc. 30th IntlConf. Acoustics, Speech, and Signal Processing ICASSP 05, vol. II,pp. 10851088, 2005.67 S. Ioannou, A. Raouzaiou, V. Tzouvaras, T. Mailis, K. Karpouzis,and S. Kollias, Emotion Recognition through Facial ExpressionAnalysis Based on a Neurofuzzy Method, Neural Networks,vol. 18, pp. 423435, 2005.68 A. Jaimes and N. Sebe, Multimodal Human Computer Interaction A Survey, Proc. 11th IEEE Intl Workshop Human ComputerInteraction HCI, 2005.69 Q. Ji, P. Lan, and C. Looney, A Probabilistic Framework forModeling and RealTime Monitoring Human Fatigue, IEEESystems, Man, and Cybernetics Part A, vol. 36, no. 5, pp. 862875,2006.70 P.N. Juslin and K.R. Scherer, Vocal Expression of Affect, TheNew Handbook of Methods in Nonverbal Behavior Research,J. Harrigan, R. Rosenthal, and K. Scherer, eds., Oxford Univ.Press, 2005.71 T. Kanade, J. Cohn, and Y. Tian, Comprehensive Database forFacial Expression Analysis, Proc. IEEE Intl Conf. Face and GestureRecognition AFGR 00, pp. 4653, 2000.72 A. Kapoor, W. Burleson, and R.W. Picard, Automatic Predictionof Frustration, Intl J. HumanComputer Studies, vol. 65, no. 8,pp. 724736, 2007.73 A. Kapoor and R.W. Picard, Multimodal Affect Recognition inLearning Environment, Proc. 13th ACM Intl Conf. MultimediaMultimedia 05, pp. 677682, 2005.74 K. Karpouzis, G. Caridakis, L. Kessous, N. Amir, A. Raouzaiou, L.Malatesta, and S. Kollias, Modeling Naturalistic Affective Statesvia Facial, Vocal, and Bodily Expression Recognition, LNAI 4451,pp. 91112, 2007.75 D. Keltner, Signs of Appeasement Evidence for the DistinctDisplays of Embarrassment, Amusement and Shame, J. Personality and Social Psychology, vol. 68, no. 3, pp. 441454, 1995.76 H. Kobayashi and F. Hara, The Recognition of Basic FacialExpressions by Neural Network, Proc. IEEE Intl Joint Conf.Neural Networks IJCNN 91, pp. 460466, 1991.77 I. Kotsia and I. Pitas, Facial Expression Recognition in ImageSequences Using Geometric Deformation Features and SupportVector Machines, IEEE Trans. Image Processing, vol. 16, no. 1,pp. 172187, 2007.78 L.I. Kuncheva, Combining Pattern Classifier Methods and Algorithms.John Wiley  Sons, 2004.79 O.W. Kwon, K. Chan, J. Hao, and T.W. Lee, Emotion Recognitionby Speech Signals, Proc. Eighth European Conf. Speech Comm. andTechnology EUROSPEECH, 2003.80 K. Laskowski and S. Burger, Annotation and Analysis ofEmotionally Relevant Behavior in the ISL Meeting Corpus, Proc.Fifth Intl Conf. Language Resources and Evaluation LREC, 2006.81 C. Lee and A. Elgammal, Facial Expression Analysis UsingNonlinear Decomposable Generative Models, Proc. Second IEEEIntl Workshop Analysis and Modeling of Faces and Gestures AMFG,2005.82 C. Lee and S. Narayanan, Emotion Recognition Using a DataDriven Fuzzy Inference System, Proc. Eighth European Conf.Speech Comm. and Technology EUROSPEECH 03, pp. 157160,2003.83 C.M. Lee and S.S. Narayanan, Toward Detecting Emotions inSpoken Dialogs, IEEE Trans. Speech and Audio Processing, vol. 13,no. 2, pp. 293303, 2005.84 J. Liscombe, J. Hirschberg, and J.J. Venditti, Detecting Certainness in Spoken Tutorial Dialogues, Proc. Ninth European Conf.Speech Comm. and Technology INTERSPEECH, 2005.85 C.L. Lisetti and F. Nasoz, MAUI A Multimodal Affective UserInterface, Proc. 10th ACM Intl Conf. Multimedia Multimedia 02,pp. 161170, 2002.86 D.J. Litman and K. ForbesRiley, Predicting Student Emotions inComputerHuman Tutoring Dialogues, Proc. 42nd Ann. Meetingof the Assoc. Computational Linguistics ACL 04, July 2004.87 G.C. Littlewort, M.S. Bartlett, and K. Lee, Faces of PainAutomated Measurement of Spontaneous Facial Expressions ofGenuine and Posed Pain, Proc. Ninth ACM Intl Conf. MultimodalInterfaces ICMI 07, pp. 1521, 2007.88 S. Lucey, A.B. Ashraf, and J.F. Cohn, Investigating SpontaneousFacial Action Recognition through AAM Representations of theFace, Face Recognition, K. Delac, and M. Grgic, eds., pp. 275286,ITech Education and Publishing, 2007.89 L. Maat and M. Pantic, GazeX Adaptive Affective MultimodalInterface for SingleUser Office Scenarios, Proc. Eighth ACM IntlConf. Multimodal Interfaces ICMI 06, pp. 171178, 2006.90 K. Mase, Recognition of Facial Expression from Optical Flow,IEICE Trans., vol. 74, no. 10, pp. 34743483, 1991.91 S. Matos, S.S. Birring, I.D. Pavord, and D.H. Evans, Detection ofCough Signals in Continuous Audio Recordings Using HMM,IEEE Trans. Biomedical Eng., vol. 53, no. 6, pp. 10781083, 2006.92 A. Mehrabian, Communication with Words, Psychology Today,vol. 2, no. 4, pp. 5356, 1968.56 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 31, NO. 1, JANUARY 2009Authorized licensed use limited to IEEE Transactions on SMC Associate Editors. Downloaded on November 30, 2008 at 1010 from IEEE Xplore.  Restrictions apply.93 S. Mozziconacci, Prosody and Emotions, Proc. First Intl Conf.Speech Prosody Speech Prosody 02, pp. 19, 2002.94 D. Neiberg, K. Elenius, and K. Laskowski, Emotion Recognitionin Spontaneous Speech Using GMM, Proc. Intl Conf. SpokenLanguage Processing ICSLP 06, pp. 809812, 2006.95 A.J. OToole et al., A Video Database of Moving Faces andPeople, IEEE Trans. Pattern Analysis and Machine Intelligence,vol. 27, no. 5, pp. 812816, May 2005.96 P.Y. Oudeyer, The Production and Recognition of Emotions inSpeech Features and Algorithms, Intl J. HumanComputerStudies, vol. 59, pp. 157183, 2003.97 P. Pal, A.N. Iyer, and R.E. Yantorno, Emotion Detection fromInfant Facial Expressions and Cries, Proc. IEEE Intl Conf.Acoustics, Speech and Signal Processing ICASSP 06, vol. 2,pp. 721724, 2006.98 M. Pantic and M.S. Bartlett, Machine Analysis of FacialExpressions, Face Recognition, K. Delac and M. Grgic, eds.,pp. 377416, ITech Education and Publishing, 2007.99 M. Pantic and I. Patras, Dynamics of Facial ExpressionRecognition of Facial Actions and Their Temporal Segments FormFace Profile Image Sequences, IEEE Trans. Systems, Man, andCybernetics Part B, vol. 36, no. 2, pp. 433449, 2006.100 M. Pantic, A. Pentland, A. Nijholt, and T.S. Huang, HumanComputing and Machine Understanding of Human Behavior ASurvey, Proc. Eighth ACM Intl Conf. Multimodal Interfaces ICMI06, pp. 239248, 2006.101 M. Pantic and L.J.M. Rothkrantz, Automatic Analysis of FacialExpressions The State of the Art, IEEE Trans. Pattern Analysis andMachine Intelligence, vol. 22, no. 12, pp. 14241445, Dec. 2000.102 M. Pantic and L.J.M. Rothkrantz, Toward an AffectSensitiveMultimodal HumanComputer Interaction, Proc. IEEE, vol. 91,no. 9, pp. 13701390, Sept. 2003.103 M. Pantic and L.J.M. Rothkrantz, Facial Action Recognition forFacial Expression Analysis from Static Face Images, IEEE Trans.Systems, Man, and Cybernetics Part B, vol. 34, no. 3, pp. 14491461,2004.104 M. Pantic and L.J.M. Rothkrantz, CaseBased Reasoning forUserProfiled Recognition of Emotions from Face Images, Proc.12th ACM Intl Conf. Multimedia Multimedia 04, pp. 391394,2004.105 M. Pantic, N. Sebe, J.F. Cohn, and T. Huang, Affective Multimodal HumanComputer Interaction, Proc. 13th ACM Intl Conf.Multimedia Multimedia 05, pp. 669676, 2005.106 M. Pantic, M.F. Valstar, R. Rademaker, and L. Maat, WebBasedDatabase for Facial Expression Analysis, Proc. 13th ACM IntlConf. Multimedia Multimedia 05, pp. 317321, 2005.107 A. Pentland, Socially Aware, Computation and Communication, Computer, vol. 38, pp. 3340, 2005.108 S. Petridis and M. Pantic, Audiovisual Discrimination betweenLaughter and Speech, IEEE Intl Conf. Acoustics, Speech, and SignalProcessing ICASSP, pp. 51175120, 2008.109 R.W. Picard, Affective Computing. MIT Press, 1997.110 R. Plutchik, Emotion A Psychoevolutionary Synthesis. Harper andRow, 1980.111 G.I. Roisman, J.L. Tsai, and K.S. Chiang, The EmotionalIntegration of Childhood Experience Physiological, Facial Expressive, and SelfReported Emotional Response during the AdultAttachment Interview, Developmental Psychology, vol. 40, no. 5,pp. 776789, 2004.112 Proc. Intl Workshop Multiple Classifier Systems MCS, F. Roli et al.,eds., 20012005.113 J.A. Russell, J. Bachorowski, and J. FernandezDols, Facial andVocal Expressions of Emotion, Ann. Rev. Psychology, vol. 54,pp. 329349, 2003.114 J. Russell and A. Mehrabian, Evidence for a ThreeFactorTheory of Emotions, J. Research in Personality, vol. 11, pp. 273294, 1977.115 A. Samal and P.A. Iyengar, Automatic Recognition and Analysisof Human Faces and Facial Expressions A Survey, PatternRecognition, vol. 25, no. 1, pp. 6577, 1992.116 D. Sander, D. Grandjean, and K.R. Scherer, A System Approachto Appraisal Mechanisms in Emotion, Neural Networks, vol. 18,pp. 317352, 2005.117 K.R. Scherer, Appraisal Theory, Handbook of Cognition andEmotion, T. Dalgleish and M.J. Power, eds., pp. 637663, Wiley,1999.118 B. Schuller, R. Muller, B. Hornler, A. Hothker, H. Konosu, and G.Rigoll, Audiovisual Recognition of Spontaneous Interest withinConversations, Proc. Ninth ACM Intl Conf. Multimodal InterfacesICMI 07, pp. 3037, 2007.119 B. Schuller, J. Stadermann, and G. Rigoll, AffectRobust SpeechRecognition by Dynamic Emotional Adaptation, Proc. SpeechProsody, Special Session on Prosody in Automatic SpeechRecognition, 2006.120 B. Schuller, R.J. Villar, G. Rigoll, and M. Lang, MetaClassifiers inAcoustic and Linguistic Feature FusionBased Affect Recognition, Proc. IEEE Intl Conf. Acoustics, Speech, and Signal ProcessingICASSP 05, pp. 325328, 2005.121 N. Sebe, I. Cohen, and T.S. Huang, Multimodal EmotionRecognition, Handbook of Pattern Recognition and Computer Vision,World Scientific, 2005.122 N. Sebe, I. Cohen, T. Gevers, and T.S. Huang, EmotionRecognition Based on Joint Visual and Audio Cues, Proc. 18thIntl Conf. Pattern Recognition ICPR 06, pp. 11361139, 2006.123 N. Sebe, M.S. Lew, I. Cohen, Y. Sun, T. Gevers, and T.S. Huang,Authentic Facial Expression Analysis, Proc. IEEE Intl Conf.Automatic Face and Gesture Recognition AFGR, 2004.124 M. Song, J. Bu, C. Chen, and N. Li, AudioVisualBased EmotionRecognition A New Approach, Proc. Intl Conf. Computer Visionand Pattern Recognition CVPR 04, pp. 10201025, 2004.125 S. Steidl, M. Levit, A. Batliner, E. Noth, and H. Niemann, Off AllThings the Measure Is Man Automatic Classification of Emotionsand InterLabeler Consistency, Proc. Intl Conf. Acoustics, Speech,and Signal Processing ICASSP 05, vol. 1, pp. 317320, 2005.126 B. Stein and M.A. Meredith, The Merging of Senses. MIT Press, 1993.127 M. Suwa, N. Sugie, and K. Fujimora, A Preliminary Note onPattern Recognition of Human Emotional Expression, Proc. IntlJoint Conf. Pattern Recognition, pp. 408410, 1978.128 H. Tao and T.S. Huang, ExplanationBased Facial MotionTracking Using a Piecewise Bezier Volume Deformation Mode,Proc. IEEE Intl Conf. Computer Vision and Pattern RecognitionCVPR 99, vol. 1, pp. 611617, 1999.129 A. Teeters, R.E. Kaliouby, and R.W. Picard, SelfCam Feedbackfrom What Would Be Your Social Partner, Proc. ACM SIGGRAPH06, p. 138, 2006.130 Y.L. Tian, T. Kanade, and J.F. Cohn, Facial Expression Analysis,Handbook of Face Recognition, S.Z. Li and A.K. Jain, eds., pp. 247276, Springer, 2005.131 S.S. Tomkins, Affect, Imagery, Consciousness, vol. 1. Springer, 1962.132 Y. Tong, W. Liao, and Q. Ji, Facial Action Unit Recognition byExploiting Their Dynamics and Semantic Relationships, IEEETrans. Pattern Analysis and Machine Intelligence, vol. 29, no. 10,pp. 16831699, 2007.133 K.P. Truong and D.A. van Leeuwen, Automatic Discriminationbetween Laughter and Speech, Speech Comm., vol. 49, pp. 144158,2007.134 M.F. Valstar, H. Gunes, and M. Pantic, How to DistinguishPosed from Spontaneous Smiles Using Geometric Features,Proc. Ninth ACM Intl Conf. Multimodal Interfaces ICMI 07,pp. 3845, 2007.135 M. Valstar, M. Pantic, Z. Ambadar, and J.F. Cohn, Spontaneousversus Posed Facial Behavior Automatic Analysis of BrowActions, Proc. Eight Intl Conf. Multimodal Interfaces ICMI 06,pp. 162170, 2006.136 M. Valstar, M. Pantic, and I. Patras, Motion History for FacialAction Detection from Face Video, Proc. IEEE Intl Conf. Systems,Man, and Cybernetics SMC 04, vol. 1, pp. 635640, 2004.137 H. Wang and N. Ahuja, Facial Expression Decomposition, Proc.Ninth IEEE Intl Conf. Computer Vision ICCV 03, p. 958, 2003.138 Y. Wang and L. Guan, Recognizing Human Emotion fromAudiovisual Information, Proc. Intl Conf. Acoustics, Speech, andSignal Processing ICASSP 05, pp. 11251128, 2005.139 J. Wang, L. Yin, X. Wei, and Y. Sun, 3D Facial ExpressionRecognition Based on Primitive Surface Feature Distribution,Proc. IEEE Intl Conf. Computer Vision and Pattern RecognitionCVPR 06, vol. 2, pp. 13991406, 2006.140 D. Watson, L.A. Clark, and A. Tellegen, Development andValidation of Brief Measures of Positive and Negative Affect ThePANAS Scales, J. Personality and Social Psychology, vol. 54,pp. 10631070, 1988.141 Z. Wen and T.S. Huang, Capturing Subtle Facial Motions in 3DFace Tracking, Proc. Ninth IEEE Intl Conf. Computer Vision ICCV03, pp. 13431350, 2003.ZENG ET AL. A SURVEY OF AFFECT RECOGNITION METHODS AUDIO, VISUAL, AND SPONTANEOUS EXPRESSIONS 57Authorized licensed use limited to IEEE Transactions on SMC Associate Editors. Downloaded on November 30, 2008 at 1010 from IEEE Xplore.  Restrictions apply.142 C.M. Whissell, The Dictionary of Affect in Language, EmotionTheory, Research and Experience. The Measurement of Emotions,R. Plutchik and H. Kellerman, eds., vol. 4, pp. 113131, AcademicPress, 1989.143 J. Whitehill and C.W. Omlin, Haar Features for FACS AURecognition, Proc. IEEE Intl Conf. Automatic Face and GestureRecognition AFGR 06, pp. 217222, 2006.144 A.C. de C. Williams, Facial Expression of Pain An EvolutionaryAccount, Behavioral and Brain Sciences, vol. 25, no. 4, pp. 439488,2002.145 C. Williams and K. Stevens, Emotions and Speech SomeAcoustic Correlates, J. Acoustic Soc. of Am., vol. 52, no. 4,pp. 12381250, 1972.146 J. Xiao, T. Moriyama, T. Kanade, and J.F. Cohn, Robust FullMotion Recovery of Head by Dynamic Templates and ReRegistration Techniques, Intl J. Imaging Systems and Technology,vol. 13, no. 1, pp. 8594, 2003.147 M. Yeasin, B. Bullot, and R. Sharma, Recognition of FacialExpressions and Measurement of Levels of Interest from Video,IEEE Trans. Multimedia, vol. 8, no. 3, pp. 500507, June 2006.148 L. Yin, X. Wei, Y. Sun, J. Wang, and M.J. Rosato, A 3D FacialExpression Database for Facial Behavior Research, Proc. IEEEIntl Conf. Automatic Face and Gesture Recognition AFGR 06,pp. 211216, 2006.149 Z. Zeng, Y. Fu, G.I. Roisman, Z. Wen, Y. Hu, and T.S. Huang,Spontaneous Emotional Facial Expression Detection, J. Multimedia, vol. 1, no. 5, pp. 18, 2006.150 Z. Zeng, Y. Hu, M. Liu, Y. Fu, and T.S. Huang, TrainingCombination Strategy of MultiStream Fused Hidden MarkovModel for AudioVisual Affect Recognition, Proc. 14th ACM IntlConf. Multimedia Multimedia 06, pp. 6568, 2006.151 Z. Zeng, Y. Hu, G.I. Roisman, Z. Wen, Y. Fu, and T.S. Huang,AudioVisual Spontaneous Emotion Recognition, ArtificialIntelligence for Human Computing, T.S. Huang, A. Nijholt,M. Pantic, and A. Pentland, eds., pp. 7290, Springer, 2007.152 Z. Zeng, M. Pantic, G.I. Roisman, and T.S. Huang, A Survey ofAffect Recognition Methods Audio, Visual, and SpontaneousExpressions, Proc. Ninth ACM Intl Conf. Multimodal InterfacesICMI 07, pp. 126133, 2007.153 Z. Zeng, J. Tu, M. Liu, T. Zhang, N. Rizzolo, Z. Zhang, T.S. Huang,D. Roth, and S. Levinson, Bimodal HCIRelated EmotionRecognition, Proc. Sixth ACM Intl Conf. Multimodal InterfacesICMI 04, pp. 137143, 2004.154 Z. Zeng, J. Tu, P. Pianfetti, M. Liu, T. Zhang, Z. Zhang, T.S.Huang, and S. Levinson, AudioVisual Affect Recognitionthrough MultiStream Fused HMM for HCI, Proc. IEEE IntlConf. Computer Vision and Pattern Recognition CVPR 05, pp. 967972, 2005.155 Z. Zeng, J. Tu, M. Liu, T.S. Huang, B. Pianfetti, D. Roth, and S.Levinson, AudioVisual Affect Recognition, IEEE Trans. Multimedia, vol. 9, no. 2, pp. 424428, Feb. 2007.156 Z. Zeng, Z. Zhang, B. Pianfetti, J. Tu, and T.S. Huang, AudioVisual Affect Recognition in ActivationEvaluation Space, Proc.13th ACM Intl Conf. Multimedia Multimedia 05, pp. 828831, 2005.157 T. Zhang, M. HasegawaJohnson, and S.E. Levinson, ChildrensEmotion Recognition in an Intelligent Tutoring Scenario, Proc.Eighth European Conf. Speech Comm. and Technology INTERSPEECH, 2004.158 Y. Zhang and Q. Ji, Active and Dynamic Information Fusion forFacial Expression Understanding from Image Sequences, IEEETrans. Pattern Analysis and Machine Intelligence, vol. 27, no. 5,pp. 699714, 2005.159 Z.H. Zhou, K.J. Chen, and H.B. Dai, Enhancing RelevanceFeedback in Image Retrieval Using Unlabeled Data, ACM Trans.Information Systems, vol. 24, no. 2, pp. 219244, 2006.160 Z. Zhu and Q. Ji, Robust RealTime Face Pose and FacialExpression Recovery, Proc. IEEE Intl Conf. Computer Vision andPattern Recognition CVPR 06, vol. 1, pp. 681688, 2006.Zhihong Zeng received the PhD degree fromthe Chinese Academy of Sciences in 2002. He iscurrently a Beckman postdoctoral fellow at theBeckman Institute, University of Illinois, UrbanaChampaign UIUC. His research interests include multimodal affective computing, multimodal humancomputer interaction, and computervision. He is a member of the IEEE ComputerSociety.Maja Pantic received the MSc and PhDdegrees in computer science from the DelftUniversity of Technology, The Netherlands, in1997 and 2001, respectively. She is currently areader in Multimodal HCI in the Department ofComputing, Imperial College London, and aprofessor of affective and behavioral computingin the Department of Computer Science, University of Twente. She is an associate editor forthe IEEE Transactions on Systems, Man, andCybernetics Part B and the Image and Vision Computing Journal. Sheis a guest editor, organizer, and committee member of more than10 major journals and conferences. Her research interests includecomputer vision and machine learning applied to face and body gesturerecognition, multimodal humancomputer interaction HCI, contextsensitive HCI, affective computing, and elearning tools. She is a seniormember of the IEEE.Glenn I. Roisman received the PhD degreefrom the University of Minnesota in 2002. He iscurrently an assistant professor in the Department of Psychology, University of Illinois, UrbanaChampaign UIUC. His research interestsinclude social and emotional developmentacross the lifespan. He has published more than25 scholarly journal articles and book chapters.He received the Award for Early ResearchContributions from the Society for Research inChild Development in 2007.Thomas S. Huang received the DSc degree inelectrical engineering from the MassachusettsInstitute of Technology MIT. He has been withthe faculty of MIT and Purdue University. In1980, he joined the University of Illinois, UrbanaChampaign, where he is currently the William L.Everitt Distinguished Professor of Electrical andComputer Engineering, a research professor inthe Coordinated Science Laboratory, and acochair of the Human Computer IntelligentInteractive Major Research Team, Beckman Institute. He has published21 books and more than 600 papers in network theory, tpdedigitalholograph, image and video compression, multimodal human computerinterfaces, and multimedia databases. He is a fellow of the IEEE and amember of the National Academy of Engineering. He has received manyhonors and awards, including the IEEE Jack S. Kilby Signal ProcessingMedal and the KingSun Fu Prize from the International Association ofPattern Recognition.. For more information on this or any other computing topic,please visit our Digital Library at www.computer.orgpublicationsdlib.58 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 31, NO. 1, JANUARY 2009Authorized licensed use limited to IEEE Transactions on SMC Associate Editors. Downloaded on November 30, 2008 at 1010 from IEEE Xplore.  Restrictions apply.
