Indexing Confusion Networks for MorphbasedSpoken Document RetrievalVille T. Turunen and Mikko KurimoAdaptive Informatics Research CentreHelsinki University of TechnologyPO Box 5400, FI02015, TKK, Finlandville.t.turunen,mikko.kurimotkk.fiABSTRACTIn this paper, we investigate methods for improving theperformance of morphbased spoken document retrieval inFinnish by extracting relevant index terms from confusionnetworks. Our approach uses morphemelike subword unitsmorphs for recognition and indexing. This alleviatesthe problem of outofvocabulary words, especially with inflectional languages like Finnish. Confusion networks offera convenient representation of alternative recognition candidates by aligning mutually exclusive terms and by giving the posterior probability of each term. The rank of thecompeting terms and their posterior probability is used toestimate term frequency for indexing. Comparing against 1best recognizer transcripts, we show that retrieval effectiveness is significantly improved. Finally, the effect of pruningin recognition is analyzed, showing that when recognitionspeed is increased, the reduction in retrieval performancedue to the increase in the 1best error rate can be compensated by using confusion networks.Categories and Subject DescriptorsH.3.3 Information Storage and Retrieval Information Search and Retrieval I.2.7 Artificial IntelligenceNatural Language ProcessingSpeech recognition and synthesisGeneral TermsAlgorithms, Languages, PerformanceKeywordsSpoken document retrieval, Subword indexing, Morphemes,Confusion networks, Lattices1. INTRODUCTIONAs more and more spoken information is produced andarchived, there is an increasing need for indexing and retrieving audio material based on the speech content. InPermission to make digital or hard copies of all or part of this work forpersonal or classroom use is granted without fee provided that copies arenot made or distributed for profit or commercial advantage and that copiesbear this notice and the full citation on the first page. To copy otherwise, torepublish, to post on servers or to redistribute to lists, requires prior specificpermission andor a fee.SIGIR07, July 2327, 2007, Amsterdam, The Netherlands.Copyright 2007 ACM 9781595935977070007 ...5.00.addition to TV and radio broadcasts, increasing amount ofaudio and video material is distributed on the Internet, e.g.in the form of podcasts and video sharing web sites such asYouTube. Currently, these archives can only be retrievedbased on humaninputted metadata rather than their actual content. As the available material becomes more diverse, the requirements for the retrieval systems increase.Furthermore, different languages pose different challengesfor retrieval. Most research is done for English, but theseresults can not always successfully be applied to other languages with different morphology and other properties.Content based retrieval of speech data utilizes an automatic speech recognition ASR system to produce a transcript of the speech for indexing. Two main approaches havecommonly been used for spoken document retrieval SDR.In phonebased retrieval, the speech is transcribed into astring of phonemes. Query words are also transformed tophoneme strings and then matched to the recognizer outputs. The second, wordbased, approach uses a large vocabulary continuous speech recognition LVCSR system totranscribe the speech into words and then applies standardtext retrieval methods to the transcripts. This has been themost successful approach in the TREC SDR tracks 3.However, wordbased methods suffer from the limited vocabulary of the speech recognizer. Any word in speech thatis not in the vocabulary outofvocabulary, OOV will always be misrecognized and is replaced by an alternativethat is deemed probable by the acoustic and language models. Phoneme recognizers are not limited to any vocabulary,but their performance is hurt by higher error rates. Typically, the vocabulary consists of the most frequent words inthe language model training corpus. For retrieval this is especially problematic, since the less frequent words, such asproper names, are usually the most interesting from retrievalpoint of view.The problem of limited vocabulary can be alleviated bybackingoff to the phoneme transcription at locations whereno word of the vocabulary fits. This is the basic principleof OOVdetection proposed by Hazen and Bazzi 4. Othermethods include query and document expansion where relevant terms are extracted from a parallel text corpus. Theadded semantically related terms help retrieve documentswith missing OOV terms 20.Our approach is based on morphemelike subword unitslearned in an unsupervised manner. We call these unitsstatistical morphs for short. The recognizer transcribes thespeech as a string of morphs, leaving almost no words out ofvocabulary. This approach is especially useful for languageswith rich morphology e.g. Finnish, Turkish, that havea high number of different inflected word forms and thuscannot use traditional language modeling based on wholewords. The morph language model assigns word break positions and thus both word level and morph level informationcan be used for indexing. The data driven algorithm is easilyapplicable for other languages with similar properties.Retrieval using the recognizer transcripts as they were error free has proved successful for low error rates 3. Stateoftheart systems can achieve low enough error rates betterthan 90 accuracy for broadcast news material in English.But as the databases grow larger, the amount of CPU powerthat can be used for every fixed time of speech decreases.Also, it is more demanding to index speech recordings thathave less optimal properties than noiseless nonspontaneousspeech, e.g. recordings of meetings, telephone conversations,etc. Thus, it is not always possible to obtain recognizer transcripts that are accurate enough for successful retrieval.Retrieval performance can be increased by extracting alternative hypotheses from the recognizer in addition to themost probable 1best candidate. A lattice is a graph containing a number of most probable hypotheses consideredby the recognizer and can be used as a source for extractingadditional terms. A more compact representation for thehypotheses is the word confusion network WCN, whichoffers a convenient representation of competing terms alongwith the posterior probability for each term. Mamou etal. 11 have shown improvements of SDR performance inlow accuracy conditions by indexing and weighting termsin confusion networks based on their probability and rankamong competitors.In this paper, we investigate methods for improving theperformance of morphbased spoken document retrieval inFinnish by extracting relevant index terms from confusionnetworks. Comparing against 1best transcripts and errorfree human transcripts, we show that retrieval effectivenessis significantly improved. As far as we know, this is the firsttime such methods have been applied to retrieval of speechin a highly inflectional language like Finnish.This paper is organized as follows. In section 2, we presentthe methods used in this work. Especially, the morphbasedretrieval scheme is described in more detail as well as thegeneration and use of the confusion networks. Section 3presents the experiments and the obtained results. Overviewof related work is presented in Section 4. Finally, our conclusions are given in Section 5.2. METHODS2.1 Morphbased retrievalMost research on speech retrieval is focused on Englishdata, but different languages have different properties thatmake methods developed for one language less usable forothers. One such property is the level of agglutination.Finnish is a highly agglutinative language, which means thatwords are formed by joining together morphemes and thusthere are a high number of distinct inflected word forms.This affects both the recognition and retrieval phase of theSDR process.2.1.1 Recognition phaseStatistical language models for speech recognition are typically built by observing cooccurrence statistics such as ngrams in a large text corpus. This works for English as areasonably sized lexicon can cover the language well. Fora highly inflective language with a huge number of distinctword forms, constructing a fixed lexicon of words becomesinfeasible. Also, training an effective language model usinginflected words as units becomes very hard as the amountof training data needed to cover enough instances of all thedifferent forms grows too large. One solution is to use subword language model units instead of whole words. If theunits are chosen well, the size of the lexicon and the amountof training data that are needed to cover the language aregreatly reduced.An unsupervised algorithm for finding suitable subwordunits has been developed by Creutz 1. Based on the Minimum Description Length MDL principle, the algorithmtakes in an unsegmented training corpus and finds a set ofunits that is compact but models the training set effectively.An ngram model can then be built over a corpus that is segmented using these units. The units produced by the algorithm are referred to as statistical morphs as the algorithmchooses the units based on statistical criteria and as theboundaries between the units in segmented word forms oftencoincide with grammatical morpheme boundaries. Speechrecognition accuracy in Finnish has been greatly improvedby utilizing statistical morphs in the language model 5. Asthe algorithm is completely data driven, it can be easily applied to other languages. An example transcript producedby the morphbased recognizer is shown in Figure 1.2.1.2 Retrieval phaseA speech recognizer with morph language modeling transcribes the speech into a string of morphs with markers atword break positions. Thus, both morphlevel and wordlevel information can be used for indexing. Typically, inretrieval of an inflectional language, a morphological analyzer is used to lemmatize each inflected word form beforeindexing. However, not all languages have a morphologicalanalyzer available as building one requires special linguistic knowledge. Furthermore, in the case of speech retrieval,errors in the transcript can cause the morphological analysis to fail and produce spurious results. This happens ifa morph in a word is misrecognized and the resulting wordis grammatically incorrect thus confusing the morphological analyzer. Also, the word break positions are sometimeswrongly assigned, again leading to confusion. The languagemodel should prevent most of these situations, but not allof these errors can be avoided.Because the statistical morphs resemble grammatical morphemes, they are also an appealing candidate to be used asindex terms as such. For retrieval, query terms are alsosegmented to morphs using the same segmentation algorithm. Thus, the need for the morphological analyzer canbe avoided. This resembles stemming as it separates theaffix morphs from the stems. Spoken document retrieval inFinnish using morphs as index terms produces results thatare about equal compared to the lemmatized transcripts 8.However, best results have been achieved by combining bothmethods.The morphbased approach provides also alleviation forthe OOV query term problem as it is now possible to recognize almost any word in speech by recognizing its componentmorphs. In practice, this means that the rare words, such assome proper names, get transcribed into many small morphsw valtio ta w yhteis ymmarrys w saa ttaaw purka utua w jo w anta isi w jaady ttaaw itsenaisyys julistuksen sa w sadaksi wpaiva ksi w ei w merkitse w si ta w ettaw riippuma ttomaksi w tasa valla ksi wjulista utuneen w liettua w tinki si witsenaistymis tavoitte istaan w han wkoro sti w liettua n w paaministeri wkat si mie ra w p ru n ski ene w joka wsaapu i w lauantai na w kotka n w uusi whansa w seminaari n wBaltiassa yhteisymmarrys, saartopurkautuu. Liettuan paatos jaadyttaaitsenaisyysjulistuksensa sadaksipaivaksi ei merkitse sita, ettariippumattomaksi tasavallaksijulistautunut Liettua tinkisiitsenaistymistavoitteestaan. Tatakorosti Liettuan paaministeriKatsimiera Prunskiene, jokasaapui lauantaina Kotkaan UusiHansa seminaariin.Figure 1 Transcripts of a part of a Finnish news story about the independence struggles of Lithuania. Left,the recognized 1best transcript of morphs with word break position marked with w. Right, the manualtranscript, aligned by line. The morph boundaries are marked with . Notice the recognition of the nameKazimiera Prunskiene. The letter z is transformed to the closest Finnish pronunciation ts.while the more common words are formed of bigger piecesor just one morph. This behavior is caused by the statisticalnature of the segmentation algorithm. However, errors arestill often made, especially with foreign names that containforeign sounds.A problem similar to understemming and overstemmingarises from nonideal segmentation of inflected word forms.Sometimes different inflected forms of the same base formproduce different stems as the boundary is placed at a wrongplace. In some cases, it is not even possible to find positionsfor the boundaries in all the different inflected forms to produce a stem that is not confused with the stems of otherwords. This problem can be alleviated by the use of queryexpansion 9 or latent semantic indexing 19.2.2 Lattices and Confusion NetworksThe speech recognizer takes as input the speech signaland generates morph lattices that represent a large number of alternative hypotheses in the form of directed acyclicgraphs. Each node of the graph has a timestamp. Each edgeis labeled with a morph hypothesis and its acoustic and language model likelihoods. The edge corresponds to the signaldelimited by the timestamps of the start and end nodes.A more compact representation of a lattice called wordconfusion network WCN or sausage has been proposed byMangu et. al 12. A confusion network contains a numberof alignment positions and, in each position, a set of mutually exclusive word hypotheses called the confusion set.Each word in a confusion set is associated with its posteriorprobability i.e. the probability of the word given the signalat that time interval. Sentence hypotheses can be generatedby freely combining hypotheses at each alignment position.The 1best path in a confusion network is simply obtainedby picking the term with the highest posterior probability ateach alignment position. The error rates are in general lowerfor the 1best paths obtained from the confusion networksthan for the 1best paths of the lattice 12.For indexing, confusion networks offer a convenient sourcefor expanding the transcript with alternative recognitioncandidates. Confusion networks are more compact than lattices and they also provide alignment for all the terms in thelattice. With confusion networks, it is easy to rank locallycompeting terms by their posterior probability and use theinformation for indexing.At general level, the algorithm for transforming a latticeto a confusion network consists of the following steps1. Compute the posterior probability for all edges in thelattice2. Pruning remove all edges with posterior probabilitybelow some threshold3. Intraword clustering merge together edges corresponding to the same word instance and sum their posteriorprobabilities4. Interword clustering group different words which compete around the same time interval and have similarphonetic properties to form confusion setsFor a detailed description of the algorithm, see 12.Pruning is needed to achieve better alignment of competing terms as it removes constraining low probability paths.This results in more accurate 1best paths as explained in12. Removing very low probability terms can also increaseretrieval performance as these terms were not likely spokenin reality. However, if the pruning threshold is too high,there is a risk of removing correct terms and thus reducingrecall.With our morphbased recognizer, the confusion networksconsist of morphs instead of words. A special marker indicates word break positions. An example morph confusionnetwork is presented in Figure 2. The network correspondsto the beginning of the transcript of Figure 1.2.3 Indexing and ranking confusion networksRetrieval performance is decreased if a relevant term thatis spoken is misrecognized and is thus missing from the transcript. However, it is possible that the correct term was considered by the recognizer but was not the top choice. In thatcase, the term will appear in the confusion network. Addingthese alternative hypothised terms to the index is expectedto increase recall. However, as most of the candidates in theconfusion network were in fact not spoken, we need to becareful so that the spurious terms do not hurt precision toomuch.Following the notation in 11, let D be a document modeled by a confusion network. We use two pieces of information in the confusion network for each occurrence of a termt at position o its posterior probability Prto, D and rankamong competitors rankto, D. Posterior probability tellsFigure 2 Beginning of the confusion network for the story of Figure 1. w marks word break positions andDEL deletions empty hypotheses. The 1best path is w valtio ta w of the state. The correct resultw baltia ssa w in the Baltic is also present in the network.how confident the recognizer is that the term occurs in thesignal at that position. Rank of the term reflects the importance of the term relative to the other alternatives. Inretrieval, document with a higher probability andor rankof a term should be preferred to one with lower values.The classical vector space model with tfidf weights andcosine distance relevance measure is used for ranking thesearch results 15. Normally, term frequency tf is the number of times a term occurs in a document. In our case, weneed to estimate a value for term frequency based on theposterior probabilities and ranks of each occurrence of theterm in the confusion network of a document. We comparetwo methods for the estimate.In the first method, term frequency is evaluated by summing the posterior probabilities of all of its occurrences inthe confusion network. This means, that if the recognizer isconfident that the term at a location is correctly recognizedposterior probability close to one, term frequency is addedby close to one as in the case of indexing error free textdocuments. Less weight is given to terms with less confidence. Thus, the term frequency of a term t in a documentD, tft, D is defined confidence level or CLmethodtft, D occt,DXi1Prtoi, D 1This is the same as used by Mamou et al. 11, except thatwe omit the boosting vector, which would assign a boostingfactor to each rank of the different hypotheses. In our case,experimenting with different values of boosting did not improve the results.Instead, we use the ranks in a different way in our secondmethod for estimating the term frequency. Siegler 17 notedthat, in the case of lattices, the probability values do notnecessarily give good estimates for term frequencies. Better results were achieved by using only the ranks of locallycompeting terms. Motivated by this, our second method forestimating term frequency is defined by rankmethodtft,D occt,DXi11ranktoi, D 2This means that the highest ranked terms of each alignmentof the lattice, which correspond to the 1best result, getweights of one. The 1best result is then expanded by competing terms, which are given less and less weight as theirrank increases.The inverse document frequency idf indicates the relativeimportance of a term in the corpus. Traditionally, idf is afunction of the number of documents in the collection theterm occurs in. In our case, we simply counted the number ofconfusion networks that the term occurs in at any position.In other words, term occurrence ot, D was estimated byot, D 1, if tft,D  00, otherwise3Now, the inverse document frequency for a term t isidft  logNXDot, D, 4where N is the number of documents in the collection.In the equation for ot, D, the value of tft, D couldalso be thresholded by using a value greater than zero toeliminate the effect of terms with low estimated frequency.That resembles the method used by Siegler 17 for estimating term presence by thresholding estimated probability ofoccurrence. In our case, however, thresholding did not improve the results. This may be due to pruning at the confusion network calculation where the terms with very lowprobability are already removed.3. EXPERIMENTS3.1 Experimental setupThe corpus consisted of 288 spoken news stories in Finnishread by single female speaker 2. Each story was about oneminute long. The manual reference transcripts of the documents were also available. Each story belonged to exactlyone of 17 different topics, assigned by multiple independentjudges. The topic descriptions were used as queries.An unlimited vocabulary continuous speech recognizer5 was used to recognize the speech into morph lattices.Lattices were transformed to confusion networks with theSRI Language Modeling Toolkit 18. The decoder pruningparameters were varied to analyze the effect of the recognition running time and to obtain confusion networks withdifferent error rates and sizes.We used speaker independent acoustic models, trained onseparate speech data consisting of 26 hours of speech from207 speakers as in 14.The language model was trained on a corpus consisting of30 million words from electronic books, newspaper text andshort news stories. Most of the text was similar to the styleof the spoken news stories but from a different time period.Before training, the corpus was segmented to morphemelike units using the unsupervised segmentation algorithm asexplained in Section 2.1. The size of the lexicon was about26000 morphs.The confusion networks were indexed using the estimatesfor term frequency and inverse document frequency fromSection 2.3. The 1best path was also extracted from theconfusion networks and indexed as any text using traditionalvalues for tf and idf based on term counts. Similarly, theretrieval experiments were also performed on the error freereference text to analyze the decline in performance due torecognition errors. Before indexing, the reference text wassegmented to morphs using the same lexicon as with thelanguage model training corpus.As the correct relevance information was known, we coulduse standard IR measures provided by the trec eval program 3 mean average precision MAP, precision at 15documents P15, precision at 5 documents P5 andprecision at R PR, where R is the number of relevantdocuments. We also plotted average interpolated recallprecision curves.The amount of errors in the 1best transcripts is measuredby word error rate WER and term error rate TER. WERis the total number of errors substitutions, deletions andinsertions divided by the number of words in the referencetext. For an agglutinative language like Finnish where thewords are formed by joining together morphemes, the WERtends to be higher than e.g. English. This is because anexpression that takes many words in English may be expressed in just one long word in Finnish. An error in oneof the morphs in the word results in the whole word to becounted as wrong. In English, on the other hand, if one ofthe words in the expression is misrecognized, several correctwords remain.For retrieval, a more relevant measure of error is obtainedby comparing how much the index terms morphs in ourcase differ. Term error rate is the difference of term frequency histograms between the indexes 7TER Pttfref t tfrectPttfref t, 5where tfref t is the term frequency in the reference text andtfrect is the term frequency in the recognized transcript.It is also possible to compare the confusion network to thereference transcript and count the oracle error rate i.e. theminimum number of error counts of any path through thenetwork. The oracle error rate indicates the upper limit forthe improvements that can be obtained from the confusionnetwork.3.2 ResultsThe object of the experiments was to examine if morphbased spoken document retrieval could be improved by extracting terms from confusion networks. Retrieval effectiveness between the following indexes were compared 1 reference text, 2 1best recognition result, 3 confusion network with term frequency estimated by CLmethod of Equation 1, 4 confusion network with term frequency estimatedby rankmethod of Equation 2.It was also examined how the performance changes withconfusion networks produced by different decoder parameters to see how much the speed of recognition, the size ofthe lattices and the resulting 1best error rates affect theretrieval effectiveness.Statistical analysis of the results was performed along theTable 1 Recognition statistics. The lattice, confusion network CN and index sizes are given relativeto the size of the respective 1best transcription orindex.measure  setup 1 2 3 41best WER  47.76 40.89 38.13 37.341best TER  58.00 47.14 41.89 40.29oracle TER  43.12 26.72 16.87 12.50RTfactor 0.95 2.10 4.86 7.56lattice size ratio 24.95 60.71 204.41 526.29CN size ratio 3.62 6.22 12.51 20.63index size ratio 2.31 3.45 6.14 9.02lines of 6, using the MATLAB Statistics Toolbox. Performance measures were first transformed with arcsinxfunction to make them closer to normal distribution. TheLilliefors test and the JarqueBera test were used to test thenormality assumption, which always held. Twoway Analysis of Variance ANOVA was performed to examine differences between the methods. 5 significance level was usedin all cases.Table 1 shows WER, TER, oracle TER and realtimeRT factors of the different recognizer runs. RT factorindicates the ratio between the time required for recognition and the length of the audio. Also presented are theresulting total sizes of uncompressed morphlattices, confusion networks and the size of the uncompressed index usingthe rankmethod. The CLmethod produced indexes aroundthe same size. The sizes are given relative to the respective1best transcription or index sizes. The size of the 1bestindex was about 1100 kB for all setups.As the level of pruning is decreased, the search space expands and the time of recognition increases as indicated bythe increase in the RT factor. The resulting 1best errorrates decrease for the first three setups but stays around thesame for the third and fourth. The increase in search spacecan also be seen in the size of the resulting lattice. The sizesof the confusion network and the index also increase but bysmaller factors. The bigger lattices and confusion networksoffer more potential for expanding the index with competingterms, which can be seen by the decrease in the oracle TER.On the other hand, they also require more computationalpower and the risk of inserting spurious terms increases.Retrieval performance statistics for the four recognizer setups are shown in Table 2. The performance of the error freereference index is also presented. It can be seen that bothexpansion methods improve the performance over the 1bestindex in all cases and by all measures. Also, the rank methodoutperforms the CLmethod in all cases and by all measuresexcept two P15 for setup 3 and PR for setups 3 and 4,where the performance is in practice equal. As with theerror rates, the performance of setups 3 and 4 are almostequal, with and without the expansion methods. This indicates that we have reached the level, where the pruning nolonger limits the performance.Statistical testing revealed that with all recognition setups the improvements in MAP are significant for the rankmethod over the 1best method. With the CLmethod, significant improvements were achieved only with the two firstrecognizer setups with the highest error rates. Similar results hold for P15, with the exception of setup 4, whereimprovements over the 1best case were not significant forTable 2 Retrieval performance statistics. Statistically significant improvements over the 1best baseline are highlighted.setup measure 1best CL rankMAP 0.716 0.758 0.7741 PR 0.657 0.701 0.713P5 0.847 0.871 0.871P15 0.620 0.651 0.659MAP 0.739 0.801 0.8332 PR 0.692 0.756 0.777P5 0.871 0.918 0.929P15 0.627 0.698 0.714MAP 0.765 0.817 0.8503 PR 0.723 0.781 0.787P5 0.871 0.894 0.941P15 0.643 0.706 0.702MAP 0.768 0.823 0.8524 PR 0.724 0.797 0.786P5 0.882 0.894 0.929P15 0.651 0.706 0.710MAP 0.864ref. PR 0.817 NA NAtext P5 0.929P15 0.749either method. For P5, significant improvements wereachieved only in setup 2 for rankmethod over the 1best.This is not surprising, because the expansion is expected toincrease recall rather than precision. Thus, no improvementswere expected at lower cutoff levels where the precision isalready high.Similar behavior can be seen in the interpolated averagedrecallprecision curves for the different setups in Figures 36.For all the setups and at almost all levels of recall, the methods are again ordered from lower precision to higher 1best,CLmethod, rankmethod. The reference index performanceis marked with the dashed line. As the pruning levels aredecreased, both expansion methods move the performancecloser to the reference. For the third and fourth setup, theperformance is again almost equal.At recall levels of 20 and lower, all indexes performaround the same level and their exact ordering is dominated by chance. For higher levels of recall, the increasein performance is bigger as expected. This indicates thatthe expansion helps retrieve relevant documents that werepreviously ranked lower and that have query terms in theconfusion sets.4. RELATED WORKVarious subword based methods have been previously usedfor retrieval. They usually consist of extracting sequencesof phonemes from the phoneme recognizer transcripts asin 13. Our morphbased method is different, however, asit utilizes the variable sized subword units in the languagemodel enabling more accurate recognition of inflectional languages. These morphemelike units also work well as indexterms. More similar to our work, Logan et al. 10 utilizesyllablelike units called particles and report improvementsin retrieval of English broadcast news with high OOV ratiowhen combined with a wordbased system. Like us, they usea data driven algorithm to find the subword units.0 10 20 30 40 50 60 70 80 90 1000102030405060708090100RecallPrecision  reference text1best transcriptionCLmethodrankmethodFigure 3 Recallprecision, setup 1, 1bestWER47.760 10 20 30 40 50 60 70 80 90 1000102030405060708090100RecallPrecision  reference text1best transcriptionCLmethodrankmethodFigure 4 Recallprecision, setup 2, 1bestWER40.890 10 20 30 40 50 60 70 80 90 1000102030405060708090100RecallPrecision  reference text1best transcriptionCLmethodrankmethodFigure 5 Recallprecision, setup 3, 1bestWER38.130 10 20 30 40 50 60 70 80 90 1000102030405060708090100RecallPrecision  reference text1best transcriptionCLmethodrankmethodFigure 6 Recallprecision, setup 4, 1bestWER37.34In our previous work 9, we presented a method for extracting alternative recognition candidates for Finnish SDR.The method is based on examining the hypothesis stack ofthe decoder during recognition and picking the most likelyterms before they are pruned. The terms are then added tothe index, unweighted. In comparison, confusion networksoffer a much more flexible framework for term extractionand make possible to estimate proper values for term frequencies.Lattices have been used for improving performance ofwordbased retrieval. Siegler 17 investigates methods forextracting relevant information from word lattices and nbest lists. Similarly to the confusion network method, mutually competing terms are located from the lattice and theirprobabilities and ranks are used for indexing, showing improvements in retrieval performance. Saraclar and Sproat16 use phoneme and word lattices to improve word spotting accuracy in English speech. Their method uses latticearc probabilities to derive a confidence measure for the termsin the lattice. However, as the terms in the lattice are notaligned, measures based on ranking of competing terms cannot be used.The most similar to this work is the approach by Mamouet al. 11. They use information provided by word confusionnetworks to improve performance of SDR from callcenterconversations in English. Compared to our work, the biggestdifference is that instead of words, we use morphs for indexing, which makes our approach better suited to retrieval ofinflectional languages like Finnish. Also, we had availablethe human relevance judgments for the speech documentsin the corpus where they compared the results against thesearch results from the reference manual transcripts, whichmight introduce bias. We also changed the method for estimating the idf as the estimation used in 11 did not workwell for our database. Their work also provides a good analysis on the effect of WER on retrieval, showing that confusion networks can improve the performance especially athigh error levels. We also produced recognizer transcriptswith different error levels. Our analysis is different in nature however, as the recognizer pruning parameters have adirect effect on the size of the lattice and thus limits thebest possible improvement that the expansion can offer.5. CONCLUSIONSIn this work, we have successfully used confusion networksof morphemelike units to improve performance of Finnishspoken document retrieval. Confusion networks offer a convenient representation of alternative recognition candidates.Both posterior probability and rank of the locally competing terms can be used to weigh the index terms. In ourexperiments, discarding the probability and using only therank to estimate the term frequency offered the best results.However, further research is needed to find the optimal wayto use the information provided by the confusion networks.Significant improvements were obtained in mean averageprecision and precision at 15 documents. Precision at 5 documents was not improved but was not decreased either. Thisshows that the estimation scheme used helps to retrieve morerelevant documents but also the possibly erroneous termsthat are added to the index are downweighted enough sothat they do not hurt the results.Experiments were also carried out with different recognition pruning parameters. They showed among other thingsthat the increase in 1best WER, which happens when pruning is increased, can be compensated by using the confusionnetworks at the indexing phase. This helps indexing of largedatabases where fast recognition speed is essential.Future work in using confusion networks for morphbasedretrieval is still needed. That includes verifying the resultswith bigger databases and using different languages. Previously, it has been noted that morphbased retrieval worksbest when combining both morphs and lemmatized words8. Thus, extracting word level information from confusionnetworks for morphological analysis could possibly be usedto improve the results. Also, other methods for estimatingtfidf values from posterior probabilities and ranks, as wellas using retrieval models other than the vector space shouldbe researched. Further improvements could be obtained bycombining the confusion network approach with other methods such as query expansion and latent semantic indexing.6. ACKNOWLEDGMENTSWe thank Inger Ekman and the Department of Information Studies at the University of Tampere for the SDR evaluation data. We are also grateful to the rest of the speechrecognition team for developing the speech recognizer andthe morpheme discovery. We also thank ComMIT graduateschool in Computational Methods of Information Technology for funding. The work was supported by the Academyof Finland in the project New adaptive and learning methods in speech recognition. This publication reflects only theauthors views.7. REFERENCES1 M. Creutz. Induction of the Morphology of NaturalLanguage Unsupervised Morpheme Segmentation withApplication to Automatic Speech Recognition. Doctoralthesis, Helsinki University of Technology, 2006.2 I. Ekman. Suomenkielinen puhehaku Finnish spokendocument retrieval. Masters thesis, University ofTampere, Finland, 2003. in Finnish.3 J. S. Garofolo, C. G. P. Auzanne, and E. M. Voorhees.The TREC spoken document retrieval track Asuccess story. In Proceedings of the Ninth TextRetrieval Conference TREC9. National Institute ofStandards and Technology NIST, 2000.4 T. J. Hazen and I. Bazzi. A comparison andcombination of methods for OOV word detection andword confidence scoring. In Proceedings of theInternational Conference on Acoustics, Speech andSignal Processing, Salt Lake City, Utah, USA, 2001.5 T. Hirsimaki, M. Creutz, V. Siivola, M. Kurimo,S. Virpioja, and J. Pylkkonen. Unlimited vocabularyspeech recognition with morph language modelsapplied to Finnish. Computer Speech and Language,2006.6 D. A. Hull. Using statistical testing in the evaluationof retrieval experiments. In SIGIR 93 Proceedings ofthe 16th annual international ACM SIGIR conferenceon Research and development in information retrieval,pages 329338, New York, NY, USA, 1993. ACMPress.7 S. Johnson, P. Jourlin, G. Moore, K. Sparck Jones,and P. C. Woodland. The cambridge university spokendocument retrieval system. In Proceedings of IEEEInternational Conference on Acoustics, Speech andSignal Processing 99, volume 1, pages 4952,Phoenix, AZ, 1999.8 M. Kurimo and V. Turunen. An evaluation of aspoken document retrieval baseline system in Finnish.In Proceedings of the International Conference onSpoken Language Processing ICSLP 2004, Jeju Island,Korea, October 2004.9 M. Kurimo and V. Turunen. To recover from speechrecognition errors in spoken document retrieval. InProceedings of the 9th European Conference on SpeechCommunication and Technology Interspeech2005Eurospeech, pages 605608, Lisbon, Portugal,September 2005.10 B. Logan, P. Moreno, and O. Deshmukh. Word andsubword indexing approaches for reducing the effectsof OOV queries on spoken audio. In Proceedings ofHLT2002 Human Language Technology Conference,2002.11 J. Mamou, D. Carmel, and R. Hoory. Spokendocument retrieval from callcenter conversations. InSIGIR 06 Proceedings of the 29th annualinternational ACM SIGIR conference on Research anddevelopment in information retrieval, pages 5158,New York, NY, USA, 2006. ACM Press.12 L. Mangu, E. Brill, and A. Stolcke. Finding consensusin speech recognition word error minimization andother applications of confusion networks. ComputerSpeech And Language, 14373400, 2000.13 K. Ng. Subwordbased Approaches for SpokenDocument Retrieval. PhD thesis, MassachusettsInstitute of Technology, 2000.14 J. Pylkkonen. New pruning criteria for efficientdecoding. In Proceedings of the 9th EuropeanConference on Speech Communication and Technology,pages 581584, Lisbon, Portugal, September 2005.15 G. Salton, A. Wong, and C. S. Yang. A vector spacemodel for automatic indexing. Communications of theACM, 1811613620, 1975.16 M. Saraclar and R. Sproat. Latticebased search forspoken utterance retrieval. In HTLNAACL MainProceedings, pages 129136, Boston, Massachusetts,USA, 2004.17 M. A. Siegler. Integration of Continuous SpeechRecognition and Information Retrieval for MutuallyOptimal Performance. PhD thesis, Carnegie MellonUniversity, 1999.18 A. Stolcke. SRILM  an extensible language modelingtoolkit. In Proceedings of International Conference onSpoken Language Processing, pages 901904, 2002.19 V. T. Turunen and M. Kurimo. Using latent semanticindexing for morphbased spoken document retrieval.In Proceedings of the 9th International Conference onSpoken Language Processing Interspeech2006ICSLP, pages 341344, Pittsburgh PA, USA,September 2006.20 P. C. Woodland, S. E. Johnson, P. Jourlin, andK. Sparck Jones. Effects of out of vocabulary words inspoken document retrieval. In SIGIR 00 Proceedingsof the 18th Annual International ACMSIGIRConference on Research and Development inInformation Retrieval, pages 372374, 2000.
