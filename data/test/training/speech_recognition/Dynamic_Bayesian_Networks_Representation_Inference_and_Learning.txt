Dynamic Bayesian NetworksRepresentation, Inference and LearningbyKevin Patrick MurphyB.A. Hon. Cambridge University 1992M.S. University of Pennsylvania 1994A dissertation submitted in partial satisfactionof the requirements for the degree ofDoctor of PhilosophyinComputer Sciencein theGRADUATE DIVISIONof theUNIVERSITY OF CALIFORNIA, BERKELEYCommittee in chargeProfessor Stuart Russell, ChairProfessor Michael JordanProfessor Peter BickelProfessor Jeffrey BilmesFall 2002The dissertation of Kevin Patrick Murphy is approvedChairDateDateDateDateUniversity of California, BerkeleyFall 2002Dynamic Bayesian NetworksRepresentation, Inference and LearningCopyright 2002byKevin Patrick MurphyABSTRACTDynamic Bayesian NetworksRepresentation, Inference and LearningbyKevin Patrick MurphyDoctor of Philosophy in Computer ScienceUniversity of California, BerkeleyProfessor Stuart Russell, ChairModelling sequential data is important in many areas of science and engineering. Hidden Markov modelsHMMs and Kalman filter models KFMs are popular for this because they are simple and flexible. Forexample, HMMs have been used for speech recognition and biosequence analysis, and KFMs have beenused for problems ranging from tracking planes and missiles to predicting the economy. However, HMMsand KFMs are limited in their expressive power. Dynamic Bayesian Networks DBNs generalize HMMsby allowing the state space to be represented in factored form, instead of as a single discrete random variable.DBNs generalize KFMs by allowing arbitrary probability distributions, not just unimodal linearGaussian.In this thesis, I will discuss how to represent many different kinds of models as DBNs, how to perform exactand approximate inference in DBNs, and how to learn DBN models from sequential data.In particular, the main novel technical contributions of this thesis are as follows a way of representingHierarchical HMMs as DBNs, which enables inference to be done in OT  time instead of OT 3, whereT is the length of the sequence an exact smoothing algorithm that takes Olog T  space instead of OT a simple way of using the junction tree algorithm for online inference in DBNs new complexity boundson exact online inference in DBNs a new deterministic approximate inference algorithm called factoredfrontier an analysis of the relationship between the BK algorithm and loopy belief propagation a way ofapplying RaoBlackwellised particle filtering to DBNs in general, and the SLAM simultaneous localizationand mapping problem in particular a way of extending the structural EM algorithm to DBNs and a varietyof different applications of DBNs. However, perhaps the main value of the thesis is its catholic presentationof the field of sequential data modelling.1DEDICATIONTo my parentsfor letting me pursue my dreamfor so longso far away from homeTo my wifefor giving menew dreams to pursueiACKNOWLEDGMENTSI would like to thank my advisor, Stuart Russell, for supporting me over the years, and for giving me somuch freedom to explore and discover new areas of probabilistic AI. My other committee members have alsobeen very supportive. Michael Jordan has long been an inspiration to me. His classes and weekly meetingshave proved to be one of my best learning experiences at Berkeley. Jeff Bilmes proved to be a most thoroughreviewer, as I expected, and has kept me honest about all the details. Peter Bickel brought a useful outsidersperspective to the thesis, and encouraged me to make it more accessible to non computer scientists althoughany failings in this regard are of course my fault.I would like to thank my many friends and colleagues at Berkeley with whom I have had the pleasure ofworking over the years. These include Eyal Amir, David Andre, Serge Belongie, Jeff Bilmes, Nancy Chang,Nando de Freitas, Nir Friedman, Paul Horton, Srini Narayanan, Andrew Ng, Mark Paskin, Sekhar Tatikonda,Yair Weiss, Erix Xing, Geoff Zweig, and all the members of the RUGS and IR groups.I would like to thank Jim Rehg for hiring me as an intern at DECCompaqHP Cambridge Research Labin 1997, where my Bayes Net Toolbox BNT was born. I would like to thank Gary Bradski for hiring meas an intern at Intel in 2000 to work on BNT, and for providing me with the opportunity to work with peoplespanning three countries formerly known as superpowers  USA, China and Russia. In particular, I wouldlike to thank Wei Hu and Yimin Zhang, of ICRC, for their help with BNT. I would also like to thank the manypeople on the web who have contributed bug fixes to BNT. By chance, I was able to work with SebastianThrun during part of my time with Intel, for which I am very grateful.I would like to thank my friends in Jennie Nation and beyond for providing a welcome distraction fromschool. Finally, I would like to thank my wife Margaret for putting up with my weekends in the office, forlistening to my sagas from Soda land, and for giving me the motivation to finish this thesis.iiContents1 Introduction 11.1 Statespace models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11.1.1 Representation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31.1.2 Inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41.1.3 Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71.2 Hidden Markov Models HMMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91.2.1 Representation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91.2.2 Inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101.2.3 Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101.2.4 The problem with HMMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111.3 Kalman Filter Models KFMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121.3.1 Representation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121.3.2 Inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131.3.3 Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131.3.4 The problem with KFMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141.4 Overview of the rest of the thesis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141.5 A note on software . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161.6 Declaration of previous work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 162 DBNs Representation 182.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182.2 DBNs defined . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182.3 Representing HMMs and their variants as DBNs . . . . . . . . . . . . . . . . . . . . . . . . 202.3.1 HMMs with mixtureofGaussians output . . . . . . . . . . . . . . . . . . . . . . . 212.3.2 HMMs with semitied mixtures . . . . . . . . . . . . . . . . . . . . . . . . . . . . 222.3.3 Autoregressive HMMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 232.3.4 Buried Markov Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24iii2.3.5 Mixedmemory Markov models . . . . . . . . . . . . . . . . . . . . . . . . . . . . 242.3.6 Inputoutput HMMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 252.3.7 Factorial HMMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 262.3.8 Coupled HMMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 272.3.9 Hierarchical HMMs HHMMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . 282.3.10 HHMMs for Automatic speech recognition ASR . . . . . . . . . . . . . . . . . . 352.3.11 Asynchronous IOHMMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 412.3.12 Variableduration semiMarkov HMMs . . . . . . . . . . . . . . . . . . . . . . . 412.3.13 Mixtures of HMMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 432.3.14 Segment models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 442.3.15 Abstract HMMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 462.4 Continuousstate DBNs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 492.4.1 Representing KFMs as DBNs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 492.4.2 Vector autoregressive VAR processes . . . . . . . . . . . . . . . . . . . . . . . . 492.4.3 Switching KFMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 512.4.4 Fault diagnosis in hybrid systems . . . . . . . . . . . . . . . . . . . . . . . . . . . 522.4.5 Combining switching KFMs with segment models . . . . . . . . . . . . . . . . . . 532.4.6 Data association . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 552.4.7 Tracking a variable, unknown number of objects . . . . . . . . . . . . . . . . . . . 562.5 First order DBNs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 573 Exact inference in DBNs 583.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 583.2 The forwardsbackwards algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 583.2.1 The forwards pass . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 593.2.2 The backwards pass . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 603.2.3 An alternative backwards pass . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 603.2.4 Twoslice distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 613.2.5 A twofilter approach to smoothing . . . . . . . . . . . . . . . . . . . . . . . . . . 613.2.6 Time and space complexity of forwardsbackwards . . . . . . . . . . . . . . . . . . 623.2.7 Abstract forwards and backwards operators . . . . . . . . . . . . . . . . . . . . . . 633.3 The frontier algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 633.3.1 Forwards pass . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 643.3.2 Backwards pass . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64iv3.3.3 Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 653.3.4 Complexity of the frontier algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . 663.4 The interface algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 673.4.1 Constructing the junction tree . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 703.4.2 Forwards pass . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 713.4.3 Backwards pass . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 723.4.4 Complexity of the interface algorithm . . . . . . . . . . . . . . . . . . . . . . . . . 723.5 Computational complexity of exact inference in DBNs . . . . . . . . . . . . . . . . . . . . 733.5.1 Offline inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 733.5.2 Constrained elimination orderings . . . . . . . . . . . . . . . . . . . . . . . . . . . 733.5.3 Consequences of using constrained elimination orderings . . . . . . . . . . . . . . . 743.5.4 Online inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 763.5.5 Conditionally tractable substructure . . . . . . . . . . . . . . . . . . . . . . . . . . 783.6 Continuous state spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 803.6.1 Inference in KFMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 803.6.2 Inference in general linearGaussian DBNs . . . . . . . . . . . . . . . . . . . . . . 813.6.3 Switching KFMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 823.6.4 Nonlinear nonGaussian models . . . . . . . . . . . . . . . . . . . . . . . . . . . 823.7 Online and offline inference using forwardsbackwards operators . . . . . . . . . . . . . . . 823.7.1 Spaceefficient offline smoothing the Island algorithm . . . . . . . . . . . . . . . 833.7.2 Fixedlag online smoothing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 873.7.3 Online filtering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 894 Approximate inference in DBNs deterministic algorithms 904.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 904.2 Discretestate DBNs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 914.2.1 The BoyenKoller BK algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . 914.2.2 The factored frontier FF algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . 954.2.3 Loopy belief propagation LBP . . . . . . . . . . . . . . . . . . . . . . . . . . . . 954.2.4 Experimental comparison of FF, BK and LBP . . . . . . . . . . . . . . . . . . . . . 974.3 Switching KFMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 984.3.1 GPB moment matching algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . 984.3.2 Viterbi approximation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1024.3.3 Expectation propagation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102v4.3.4 Variational methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1034.4 Nonlinear nonGaussian models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1044.4.1 Filtering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1044.4.2 Sequential parameter estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1044.4.3 Smoothing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1045 Approximate inference in DBNs stochastic algorithms 1055.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1055.2 Particle filtering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1065.2.1 Particle filtering for DBNs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1075.3 RaoBlackwellised Particle Filtering RBPF . . . . . . . . . . . . . . . . . . . . . . . . . 1115.3.1 RBPF for switching KFMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1125.3.2 RBPF for simultaneous localisation and mapping SLAM . . . . . . . . . . . . . . 1145.3.3 RBPF for general DBNs towards a turnkey algorithm . . . . . . . . . . . . . . . . 1225.4 Smoothing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1235.4.1 RaoBlackwellised Gibbs sampling for switching KFMs . . . . . . . . . . . . . . . 1236 DBNs learning 1266.1 Differences between learning static and dynamic networks . . . . . . . . . . . . . . . . . . 1266.1.1 Parameter learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1266.1.2 Structure learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1276.2 Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1286.2.1 Learning genetic network topology using structural EM . . . . . . . . . . . . . . . 1286.2.2 Inferring motifs using HHMMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1316.2.3 Inferring peoples goals using abstract HMMs . . . . . . . . . . . . . . . . . . . . . 1336.2.4 Modelling freeway traffic using coupled HMMs . . . . . . . . . . . . . . . . . . . . 1376.2.5 Online parameter estimation and model selection for regression . . . . . . . . . . . 144A Graphical models representation 148A.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148A.2 Undirected graphical models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148A.2.1 Representing potential functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152A.2.2 Maximum entropy models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152A.3 Directed graphical models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152A.3.1 Bayes ball . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154viA.3.2 Parsimonious representations of CPDs . . . . . . . . . . . . . . . . . . . . . . . . . 155A.4 Factor graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159A.5 Firstorder probabilistic models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160A.5.1 Knowledgebased model construction KBMC . . . . . . . . . . . . . . . . . . . . 161A.5.2 Objectoriented Bayes nets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 162A.5.3 Probabilistic relational models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163B Graphical models inference 164B.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164B.2 Variable elimination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164B.3 From graph to junction tree . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 166B.3.1 Elimination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 166B.3.2 Triangulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 170B.3.3 Elimination trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 170B.3.4 Junction trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171B.3.5 Finding a good elimination ordering . . . . . . . . . . . . . . . . . . . . . . . . . . 174B.3.6 Strong junction trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 174B.4 Message passing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 175B.4.1 Initialization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 175B.4.2 Parallel protocol . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 175B.4.3 Serial protocol . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 176B.4.4 Absorption via separators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 178B.4.5 Hugin vs ShaferShenoy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 178B.4.6 Message passing on a directed polytree . . . . . . . . . . . . . . . . . . . . . . . . 179B.4.7 Correctness of message passing . . . . . . . . . . . . . . . . . . . . . . . . . . . . 180B.4.8 Handling evidence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181B.5 Message passing with continuous random variables . . . . . . . . . . . . . . . . . . . . . . 182B.5.1 Pure Gaussian case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182B.5.2 Conditional Gaussian case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 185B.5.3 Arbitrary CPDs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 187B.6 Speeding up exact discrete inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 188B.6.1 Exploiting causal independence . . . . . . . . . . . . . . . . . . . . . . . . . . . . 188B.6.2 Exploiting context specific independence CSI . . . . . . . . . . . . . . . . . . . . 189B.6.3 Exploiting deterministic CPDs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 189viiB.6.4 Exploiting the evidence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 190B.6.5 Being lazy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 190B.7 Approximate inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191B.7.1 Loopy belief propagation LBP . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191B.7.2 Expectation propagation EP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 195B.7.3 Variational methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 198B.7.4 Sampling methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 198B.7.5 Other approaches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 199C Graphical models learning 200C.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 200C.2 Known structure, full observability, frequentist . . . . . . . . . . . . . . . . . . . . . . . . 202C.2.1 Multinomial distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203C.2.2 Conditional linear Gaussian distributions . . . . . . . . . . . . . . . . . . . . . . . 203C.2.3 Other CPDs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205C.3 Known structure, full observability, Bayesian . . . . . . . . . . . . . . . . . . . . . . . . . 207C.3.1 Multinomial distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 207C.3.2 Gaussian distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 210C.3.3 Conditional linear Gaussian distributions . . . . . . . . . . . . . . . . . . . . . . . 210C.3.4 Other CPDs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 210C.4 Known structure, partial observability, frequentist . . . . . . . . . . . . . . . . . . . . . . . 210C.4.1 Gradient ascent . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 211C.4.2 EM algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 212C.4.3 EM vs gradient methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 213C.4.4 Local minima . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 218C.4.5 Online parameter learning algorithms . . . . . . . . . . . . . . . . . . . . . . . . . 218C.5 Known structure, partial observability, Bayesian . . . . . . . . . . . . . . . . . . . . . . . . 220C.6 Unknown structure, full observability, frequentist . . . . . . . . . . . . . . . . . . . . . . . 220C.6.1 Search space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 221C.6.2 Search algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 222C.6.3 Scoring function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 224C.7 Unknown structure, full observability, Bayesian . . . . . . . . . . . . . . . . . . . . . . . . 226C.7.1 The proposal distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 227C.8 Unknown structure, partial observability, frequentist . . . . . . . . . . . . . . . . . . . . . . 228viiiC.8.1 Approximating the marginal likeihood . . . . . . . . . . . . . . . . . . . . . . . . . 228C.8.2 Structural EM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 229C.9 Unknown structure, partial observability, Bayesian . . . . . . . . . . . . . . . . . . . . . . 230C.10 Inventing new hidden nodes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 232C.11 Derivation of the CLG parameter estimation formulas . . . . . . . . . . . . . . . . . . . . . 232C.11.1 Estimating the regression matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . 232C.11.2 Estimating a full covariance matrix . . . . . . . . . . . . . . . . . . . . . . . . . . 233C.11.3 Estimating a spherical covariance matrix . . . . . . . . . . . . . . . . . . . . . . . 233D Notation and abbreviations 235ixChapter 1Introduction1.1 Statespace modelsSequential data arises in many areas of science and engineering. The data may either be a time series,generated by a dynamical system, or a sequence generated by a 1dimensional spatial process, e.g., biosequences. One may be interested either in online analysis, where the data arrives in realtime, or in offlineanalysis, where all the data has already been collected.In online analysis, one common task is to predict future observations, given all the observations up tothe present time, which we will denote by y1t  y1, . . . , yt. In this thesis, we only consider discretetimesystems, hence t is always an integer. Since we will generally be unsure about the future, we would like tocompute a best guess. In addition, we might want to know how confident we are of this guess, so we canhedge our bets appropriately. Hence we will try to compute a probability distribution over the possible futureobservations we denote this by P ythy1t, where h  0 is the horizon, i.e., how far into the future wewant to predict.Sometimes we have some control over the system we are monitoring. In this case, we would like topredict future outcomes as a function of our inputs. Let u1t denote our past inputs, and ut1th denote ournext h inputs. Now the task is to compute P ythu1th, y1t.Classical approaches to timeseries prediction use linear models, such as ARIMA, ARMAX, etc. seee.g., Ham94, or nonlinear models, such as neural networks either feedforward or recurrent or decisiontrees MCH02. For discrete data, it is common to use ngram models see e.g., Jel97 or variablelengthMarkov models RST96, McC95.There are several problems with the classical approach. First, we must base our prediction of the futureon only a finite window into the past, say ytt, where   0 is the lag, if we are to do constant workper time step. If we know that the system we are modelling is Markov with an order  , we will sufferno loss of performance, but in general the order may be large and unknown. Recurrent neural nets try toovercome this problem by using internal state, but they are still not able to model longdistance dependencies1BF95. Second, it is difficult to incorporate prior knowledge into the classical approach much of ourknowledge cannot be expressed in terms of directly observable quantities, and blackbox models, such asneural networks, are notoriously hard to interpret. Third, the classical approach has difficulties when we havemultidimensional multivariate inputs andor outputs. For instance, consider the problem of predictingand hence compressing the next frame in a video stream using a neural network. Actual video compressionschemes such as MPEG try to infer the underlying cause behind what they see, and use that to predict thenext frame. This is the basic idea behind statespace models, which we discuss next.In a statespace model, we assume that there is some underlying hidden state of the world that generatesthe observations, and that this hidden state evolves in time, possibly as a function of our inputs.1 In an onlinesetting, the goal is to infer the hidden state given the observations up to the current time. If we letXt representthe hidden state at time t, then we can define our goal more precisely as computing P Xty1t, u1t this iscalled the belief state.Astrom Ast65 proved that the belief state is a sufficient statistic for predictioncontrol purposes, i.e.,we do not need to keep around any of the past observations.2 We can update the belief state recursively usingBayes rule, as we explain below. As in the case of prediction, we maintain a probability distribution overXt,instead of just a best guess, in order to properly reflect our uncertainty about the true state of the world.This can be useful for information gathering for instance, if we know we are lost, we may choose to ask fordirections.Statespace models are better than classical timeseries modelling approaches in many respects Aok87,Har89, WH97, DK00, DK01. In particular, they overcome all of the problems mentioned above they donot suffer from finitewindow effects, they can easily handle discrete and multivariate inputs and outputs,and they can easily incorporate prior knowledge. For instance, often we know that there are variables thatwe cannot measure, but whose state we would like to estimate such variables are called hidden or latent.Including these variables allows us to create models which may be much closer to the true causal structureof the domain we are modelling Pea00.Even if we are only interested in observable variables, introducing fictitious hidden variables oftenresults in a much simpler model. For example, the apparent complexity of an observed signal may be moresimply explained by imagining it is a result of two simple processes, the true underlying state, whichmay evolve deterministically, and our measurement of the state, which is often noisy. We can then explain1The term statespace model is often used to imply that the hidden state is a vector in IRK , for some K I use the term moregenerally to mean a dynamical model which uses any kind of hidden state, whether it is continuous, discrete or both. e.g., I considerHMMs an example of a statespace model. In contrast to most work on time series analysis, this thesis focuses on models with discreteand mixed discretecontinuous states. One reason for this is that DBNs have their biggest payoff in the discrete setting combiningmultiple continuous variables together results in a polynomial increase in complexity see Section 2.4.2, but combining multiple discretevariables results in an exponential increase in complexity, since the new mega statespace is the cross product of the individualvariables statespaces DBNs help ameliorate this combinatorial explosion, as we shall see in Chapter 2.2This assumes that the hidden state space is sufficiently rich. We discuss some ways to learn the hidden state space in Chapter 6.However, learning hidden state representations is difficult, which has motivated alternative forms of sufficient statistics LSS01.2away unexpected outliers in the observations in terms of a faulty sensor, as opposed to strange fluctuationsin reality. The underlying state may be of much lower dimensionality than the observed signal, as in thevideo compression example mentioned above.In the following subsections, we discuss, in general terms, how to represent statespace models, how touse them to update the belief state and perform other related inference problems, and how to learn such models from data. We then discuss the two most common kinds of statespace models, namely Hidden MarkovModels HMMs and Kalman Filter Models KFMs. In subsequent chapters of this thesis, we will discussrepresentation, inference and learning of more general statespace models, called Dynamic Bayesian Networks DBNs. A summary of the notation and commonly used abbreviations can be found in Appendix D.1.1.1 RepresentationAny statespace model must define a prior, P X1, a statetransition function, P XtXt1, and an observation function, P YtXt. In the controlled case, these become P XtXt1, Ut and P YtXt, Ut we allowthe observation to depend on the control so that we can model active perception. For most of this thesis, wewill omit Ut from consideration, for notational simplicity.We assume that the model is firstorder Markov, i.e., P XtX1t1  P XtXt1 if not, we canalways make it so by augmenting the statespace. For example, if the system is secondorder Markov, we justdefine a new statespace, Xt  Xt, Xt1, and setP Xt  xt, xt1Xt1  xt1, xt2  xt1, xt1P xtxt1, xt2.Similarly, we can assume that the observations are conditionally firstorder Markov P YtY1t1, Xt P YtXt, Yt1. This is usually further simplified by assuming P YtYt1, Xt  P YtXt. These conditional independence relationships will be explained more clearly in Chapter 2.We assume that the transition and observation functions are the same for all time the model is said tobe timeinvariant or homogeneous. Without this assumption, we could not model infinitely long sequences.If the parameters do change over time, we can just add them to the state space, and treat them as additionalrandom variables, as we will see in Section 6.1.1.There are many ways of representing statespace models, the most common being Hidden MarkovModels HMMs and Kalman Filter Models KFMs. HMMs assume Xt is a discrete random variable3,Xt  1, . . . ,K, but otherwise make essentially no restrictions on the transition or observation functionwe will explain HMMs in more detail in Section 1.2. Kalman Filter Models KFMs assume Xt is a vectorof continuous random variables, Xt  RK , and that X1T and Y1T are jointly Gaussian. We will explain3In this thesis, all discrete random variables will be considered unordered cardinal, as opposed to ordered ordinal, unless otherwisestated. For example, Xt  male, female is cardinal, but Xt  low, medium, high is ordinal. Ordinal values are sometimes usefulfor qualitative probabilistic networks Wel90.3tttTtfilteringpredictionfixedlagsmoothingofflinesmoothingfixed intervalhlFigure 1.1 The main kinds of inference for statespace models. The shaded region is the interval for whichwe have data. The arrow represents the time step at which we want to perform inference. t is the currenttime, and T is the sequence length. See text for details.KFMs in more detail in Section 1.3. Dynamic Bayesian Networks DBNs DK89, DW91 provide a muchmore expressive language for representing statespace models we will explain DBNs in Chapter 2.A statespace model is a model of how Xt generates or causes Yt and Xt1. The goal of inference isto invert this mapping, i.e., to infer X1t given Y1t. We discuss how to do this below.1.1.2 InferenceWe now discuss the main kinds of inference that we might want to perform using statespace models seeFigure 1.1 for a summary. The details of how to perform these computations depend on which model andwhich algorithm we use, and will be discussed later.FilteringThe most common inference problem in online analysis is to recursively estimate the belief state using BayesruleP Xty1t  P ytXt, y1t1P Xty1t1 P ytXtxt1P Xtxt1P xt1y1t1where the constant of proportionality is 1ct  1P yty1t1. We are licensed to replaceP ytXt, y1t1 by P ytXt because of the Markov assumption on Yt. Similarly, the onestepahead prediction, P Xty1t1, can be computed from the prior belief state, P Xt1y1t1, because of the Markov4assumption on Xt.We see that recursive estimation consists of two main steps predict and update predict means computingP Xty1t1, sometimes written as Xtt1, and update means computing P Xty1t, sometimes written asXtt. Once we have computed the prediction, we can throw away the old belief state this operation issometimes called rollup. Hence the overall procedure takes constant space and time i.e., independent of tper time step.This task is traditionally called filtering, because we are filtering out the noise from the observationssee Section 1.3.1 for an example. However, in some circumstances the term monitoring might be moreappropriate. For example, Xt might represent the state of a factory e.g., which pipes are malfunctioning,and we wish to monitor the factory state over time.SmoothingSometimes we want to estimate the state of the past, given all the evidence up to the current time, i.e.,compute P Xty1t, where   0 is the lag, e.g., we might want to figure out whether a pipe broke Lminutes ago given the current sensor readings. This is traditionally called fixedlag smoothing, althoughthe term hindsight might be more appropriate. In the offline case, this is called fixedinterval smoothingthis corresponds to computing P Xty1T  for all 1  t  T .Smoothing is important for learning, as we discuss in Section 1.1.3.PredictionIn addition to estimating the current or past state, we might want to predict the future, i.e., computeP Xthy1t,where h  0 is how far we want to lookahead. Once we have predicted the future hidden state, we can easilyconvert this into a prediction about the future observations by marginalizing out XthP Yth  yy1t xP Yth  yXth  xP Xth  xy1tIf the model contains input variablesUt, we must specify ut1th in order to predict the effects of our actionsh steps into the future, since this is a conditional likelihood model.ControlIn control theory, the goal is to learn a mapping from observations or belief states to actions a policy so as tomaximize expected utility or minimize expected cost. In the special case where our utility function rewardsus for achieving a certain value for the output of the system reaching a certain observed state, it is sometimespossible to pose the control problem as an inference problem Zha98a, DDN01, BMI99. Specifically, weset Yth to the desired output value and leave Yt1th1 hidden, and then infer the values if any forUt1, . . . , Uth which will achieve this, where h is our guess about how long it will take to achieve the goal.5We can use dynamic programming to efficiently search over h. The cost function gets converted into a prioron the controlinput variable. For example, if the prior overUt is Gaussian,N Ut 0,, Ut  N 0,, thenthe mode of the posterior P Ut1thy1t, yth, u1t will correspond to a sequence of minimal controlsminimal in the sense of having the smallest possible length, as measured by a Mahalanobis distance using which achieves the desired output sequence.If Ut is discrete, inference amounts to enumerating all possible assignments to Ut1th, as in a decisiontree this is called receeding horizon control. We can approximately solve the infinite horizon control usingsimilar methods so long as we discount future rewards at a suitably high rate KMN99.The general solution to control problems requires the use of influence diagrams see e.g., CDLS99, ch8,LN01. We will not discuss this topic further in this thesis.Viterbi decodingIn Viterbi decoding also called abduction or computing the most probable explanation, the goal is tocompute the most likely sequence of hidden states given the datax1t  argmaxx1tP x1ty1tIn the following subsection, we assume that the state space is discrete.By Bellmans principle of optimality, the most likely to path to reach state xt consists of the most likelypath to some state at time t 1, followed by a transition to xt. Hence we can compute the overall most likelypath as follows. In the forwards pass, we computetj  P ytXt  j maxiP Xt  jXt1  it1iwheretjdef maxx1t1P X1t  x1t1, Xt  jy1t.This is the same as the forwards pass of filtering, except we replace sum with max see Section B.2. Inaddition, we keep track of the identity of the most likely predecessor to each statetj  arg maxiP Xt  jXt1  it1iIn the backwards pass, we can compute the identity of the most likely path recursively as followsxt  t1xt1Note that this is different than finding the most likely marginal state at time t.One application of Viterbi is in speech recognition. Here, Xt typically represents a phoneme or syllable,and Yt typically represents a feature vector derived from the acoustic signal Jel97. x1t is the most likely6hypothesis about what was just said. We can compute the N best hypotheses in a similar manner Nil98,NG01.Another application of Viterbi is in biosequence analysis, where we are interested in offline analysis ofa fixedlength sequence, y1T . Yt usually represents the DNA basepair or amino acid at location t in thestring. Xt often represents whether Yt was generated by substitution, insertion or deletion compared to someputative canonical family sequence. As in speech recognition, we might be interested in finding the mostlikely parse or interpretation of the data, so that we can align the observed sequence to the family model.ClassificationThe likelihood of a model,M , is P y1tM, and can be computed by multiplying together all the normalizingconstants that arose in filteringP y1t  P y1P y2y1P y3y12 . . . P yT y1T1 Tt1ct 1.1which follows from the chain rule of probability. This can be used to classify a sequence as followsCy1T   argmaxCP y1T CP Cwhere P y1T C is the likelihood according to the model for class C, and P C is the prior for class C. Thismethod has the advantage of being able to handle sequences of variablelength. By contrast, most classifierswork with fixedsized feature vectors.4SummaryWe summarize the various inference problems in Figure 1.1. In Section 3.7, we will show how all of theabove algorithms can be formulated in terms of a set of abstract operators which we will call forwards andbackwards operators. There are many possible implementations of these operators which make differenttradeoffs between accuracy, speed, generality, etc. In Sections 1.2 and 1.3, we will see how to implementthese operators for HMMs and KFMs. In later chapters, we will see different implementations of theseoperators for general DBNs.1.1.3 LearningA statespace model usually has some free parameters  which are used to define the transition model,P XtXt1, and the observation model, P YtXt. Learning means estimating these parameters from datathis is often called system identification.4One could pretend that successive observations in the sequence are iid, and then apply a naive Bayes classifier P y1T C Tt1 P ytC. However, often it matters in what order the observations arrive, e.g., in classifying a string of letters as a word. Therehas been some work on applying support vector machines to variable length sequences JH99, but this uses an HMM as a subroutine.7The usual criterion is maximumlikelihood ML, which is suitable if we are doing offline learning withlarge data sets. Suppose, as is typical in speech recognition and biosequence analysis, that we have Ntrainiid sequences, Y  y11T , . . . yNtrain1T , where we have assumed each sequence has the same length T fornotational simplicity. Then the goal of learning is to computeML  arg maxP Y   arg maxlogP Y where the loglikelihood of the training set islogP Y   logNtrainm1P ym1T  Ntrainm1logP ym1T A minor variation is to include a prior on the parameters and compute the MAP maximum a posteriorisolutionMAP  arg maxlogP Y   logP This can be useful when the number of free parameters is much larger than the size of the dataset the prioracting like a regularizer to prevent overfitting, and for online learning where at timestep t the dataset onlyhas size t.What makes learning statespace models difficult is that some of the variables are hidden. This meansthat the likelihood surface is multimodal, making it difficult to find the globally optimal parameter value.5Hence most learning methods just attempt to find a locally optimal solution.The two standard techniques for MLMAP parameter learning are gradient ascent6, and EM expectationmaximization, both of which are explained in Appendix C. Note that both methods use inference as asubroutine, and hence efficient inference is a prerequisite for efficient learning. In particular, for offlinelearning, we need need to perform fixedinterval smoothing i.e., computing P Xty1T ,  for all t learningwith filtering may fail to converge correctly. To see why, consider learning to solve murders hindsight isalways required to infer what happened at the murder scene.7For online learning, we can use fixedlag smoothing combined with online gradient ascent or online EM.Alternatively, we can adopt a Bayesian approach and treat the parameters as random variables, and just addthem to the statespace. Then learning just amounts to filtering i.e., sequential Bayesian updating in theaugmented model, P Xt, ty1t. Unfortunately, inference in such models is often very difficult, as we willsee.A much more ambitious task than parameter learning is to learn the structure parametric form of themodel. We will discuss this in Chapter 6.5One trivial source of multimodality has to do with symmetries in the hidden statespace. Often we can permute the labels of thehidden states without affecting the likelihood.6Ascent, rather than descent, since we are trying to maximize likelihood.7This example is from RN02.81.2 Hidden Markov Models HMMsWe now give a brief introduction to HMMs.8 The main purpose of this section is to introduce notation andconcepts in a simple and hopefully familiar context these will be generalized to the DBN case later.1.2.1 RepresentationAn HMM is a stochastic finite automaton, where each state generates emits an observation. We will useXt to denote the hidden state and Yt to denote the observation. If there are K possible states, then Xt 1, . . . ,K. Yt might be a discrete symbol, Yt  1, . . . , L, or a featurevector, Yt  IRL.The parameters of the model are the initial state distribution, i  P X1  i, the transition model,Ai, j  P Xt  jXt1  i, and the observation model P YtXt. represents a multinomial distribution. The transition model is usually characterized by a conditionalmulitnomial distribution Ai, j  P Xt  jXt1  i, where A is a stochastic matrix each row sumsto one. The transition matrix A if often sparse the structure of the matrix is often depicted graphically,as in Figure 1.2 which depicts a lefttoright transition matrix. This means low numbered states can onlymake transitions to higher numbered states or to themselves. Such graphs should not be confused with thegraphical models we will introduce in Chapter 2.1 2 3 4Figure 1.2 A lefttoright state transition diagram for a 4state HMM. Nodes represent states, and arrowsrepresent allowable transitions, i.e., transitions with nonzero probability. The selfloop on state 2 meansP Xt  2Xt1  2  A2, 2  0.If the observations are discrete symbols, we can represent the observation model as a matrix Bi, k P Yt  kXt  i. If the observations are vectors in IRL, it is common to represent P YtXt as a GaussianP Yt  yXt  i  N yi,iwhereN y, is the Gaussian density with mean  and covariance  evaluated at yN y,  12L212exp 12 y  1y  A more flexible representation is a mixture of M GaussiansP Yt  yXt  i Mm1P Mt  mXt  iN ym,i,m,iwhere Mt is a hidden variable that specifies which mixture component to use, and P Mt  mXt  i Ci,m is the conditional prior weight of each mixture component. For example, one mixture component8See Rab89 for an excellent tutorial, and Ben99 for a review of more recent developments MZ97 provides a thorough mathematical treatise on HMMs. The book DEKM98 provides an excellent introduction to the application of HMMs to biosequence analysis,and the book Jel97 describes how HMMs are used in speech recognition.9might be a Gaussian centered at the expected output for state i and with a narrow variance, and the secondcomponent might be a Gaussian with zero mean and a very broad variance the latter approximates a uniformdistribution, and can account for outliers, making the model more robust.In speech recognition, it is usual to assume that the parameters are stationary or timeinvariant, i.e.,that the transition and observation models are shared tied across time slices. This allows the model to beapplied to sequences of arbitrary length. In biosequence analysis, it is common to use positiondependentobservation models, Bti, k  P Yt  kXt  i, since certain positions have special meanings. Thesemodels can only handle fixedlength sequences. However, by adding a background state with a positioninvariant distribution, the models can be extended to handle varying length sequences. In this thesis, we willusually assume timepositioninvariant parameters, but this is mostly for notational simplicity.1.2.2 InferenceOffline smoothing can be performed in an HMM using the wellknown forwardsbackwards algorithm, whichwill be explained in Section 3.2. In the forwards pass, we recursively compute the filtered estimate ti P Xt  iy1t, and in the backwards pass, we recursively compute the smoothed estimate ti  P Xt iy1T  and the smoothed twoslice estimate t1,tT i, j  P Xt1  i,Xt  jy1T  which is needed forlearning.9If X can be in K possible states, filtering takes OK2 operations per time step, since we must do amatrixvector multiply at every step. Smoothing therefore takes OK2T  time in the general case. If A issparse, and each state has at most Fin predecessors, then the complexity is KFinT .1.2.3 LearningIn this section we give an informal explanation of how to do offline maximum likelihood ML parameterestimation for HMMs using the EM BaumWelch algorithm. This will form the basis of the generalizationsin Chapter 6.If we could observeX1T , learning would be easy. For instance, the ML estimate of the transition matrixcould be computed by normalizing the matrix of cooccurrences countsAMLi, j Ni, jk Ni, kwhereNi, j Tt2IXt1  i,Xt  j9It is more common to define  as an unconditional joint probability, ti  P Xt  i, y1t. Also, it is more common to definethe backwards pass as computing ti  P yt1T Xt  i ti and t1,tT can then be derived from ti and ti. Thesedetails will be explained in Section 3.2. The chosen notation is designed to bring out the similarity with inference in KFMs and generalDBNs.10and IE is a binary indicator that is 1 if event E occurs and is 0 otherwise. Hence Ni, j is the number ofi  j transitions in a given sequence. We have assumed there is a single training sequence for notationalsimplicity. If we have more than one sequence, we simply sum the counts across sequences. We can estimateP YtXt and P X1 similarly.The problem, however, is that X1T is hidden. The basic idea of the EM algorithm, roughly speaking, isto estimate P X1T y1T  using inference, and to use the expected pairwise counts instead of the real countsto estimate the parameters . Since the expectation depends on the value of , and the value of  depends onthe expectation, we need to iterate this scheme.We start with an initial guess of , and then perform the E expectation step. Specifically, at iteration kwe computeENi, jk  ETt2IXt1  i,Xt  jy1T  Tt2P Xt1  i,Xt  jy1T  Tt2t1,tT i, j can be computed using the forwardsbackwards algorithm, as discussed in Section 3.2. ENi, jk iscalled the expected sufficient statistic ESS for A, the transition matrix. We compute similar ESSs for theother parameters.We then perform an M maximization step. This tries to maximize the value of the expected completedata loglikelihoodk1  argmaxQkwhere Q is the auxiliary functionQk  EX1TP y1T , X1T kFor the case of multinomials, it is easy to show that this amounts to normalizing the expected countsAk1ML i, j  ENi, jkBPSW70, DLR77 proved that the EM algorithm is guaranteed to increase the likelihood at each stepuntil a critical point usually a local maximum is reached. In practice, we declare convergence when therelative change in the loglikelihood is less than some threshold.10 See Section C.4.2 for more details of theEM algorithm.1.2.4 The problem with HMMsSuppose we want to track the state e.g., the position of N objects in an image sequence. Let each object bein one of k possible states. ThenXt  X1t , . . . , XNt  can haveK  kN possible values, since we must form10The fact that the likelihood stops changing does not mean that the parameters stop changing it is possible for EM to cause theestimate to oscillate in parameter space, without changing the likelihood.11the Cartesian product of the statespaces of each individual object. This means that we require an exponentialnumber of parameters exponential in the number of objects to specify the transition and observation models,which means we will need a lot of data to learn the model high sample complexity. In addition, inferencetakes exponential time, e.g., forwardsbackwards takes OTk2Nhigh computational complexity. DBNswill help ameliorate both of these problems.1.3 Kalman Filter Models KFMsWe now give a brief introduction to KFMs, also known as linear dynamical systems LDSs, statespace models, etc.11 The main purpose of this section is to introduce notation and concepts in a simple and hopefullyfamiliar context these will form the basis of future generalizations.1.3.1 RepresentationA KFM assumes Xt  IRNx , Yt  IRNy , Ut  IRNu , and that the transition and observation functions arelinearGaussian, i.e.,P Xt  xtXt1  xt1, Ut  u  N xtAxt1 Bu X , QandP Yt  yXt  x, Ut  u  N yCx Du Y , RIn other words, Xt  AXt1  BUt  Vt, where Vt  N X , Q is a Gaussian noise term. Similarly,Yt  CXt  DUt  Wt, where Wt  N Y , R is another Gaussian noise term assumed independentof Vt. The noise terms are assumed to be temporally white, which means Vt  Vt i.e., Vt is marginallyindependent of Vt  for all t 6 t, and similarly for Wt,A is a Nx Nx matrix, B is a Nx Nu matrix, C is a Ny Nx matrix, D is a Ny Nu matrix, Q is aNx Nx positive semidefinite psd matrix called the process noise, and R is a Ny Ny psd matrix calledthe observation noise. As for HMMs, we assume the parameters are timeinvariant.Without loss of generality, we can assume X and Y are 0, since we can always augment Xt with theconstant 1, and add X muY  to the first column of A C respectively. Similarly, we can assume Q or R isdiagonal see RG99 for details.ExampleSuppose we are tracking an object as it moves through IR2. Let Xt  xt, yt, xt, yt represent the positionand velocity of the object. Consider a constantvelocity model however, we assume the object will get11See RG99, Min99 for good tutorials on KFMs from the DBN perspective. There are many textbooks that give a more classicaltreatment of KFMs, see e.g., AM79, BSF88.12buffetted around by some unknown source e.g., the wind, which we will model as Gaussian noise. Hencextytxtyt 1 0  00 1 0 0 0 1 00 0 0 1xt1yt1xt1yt1 Vtwhere  is the sampling period, Vt  N 0, Q is the noise, and Q is the following covariance matrixQ Qx Qx,y 0 0Qx,y Qy 0 00 0 0 00 0 0 0Qx is the variance of the noise in the x direction,Qy is the variance of the noise in the y direction, andQx,y isthe crosscovariance. If the bottom right matrix were nonzero, this would be called a random accelerationmodel, since it would add noise to the velocities.Assume also that we only observe the position of the object, but not its velocity. Hencexotyot1 0 0 00 1 0 0xtytxtytWtwhere Wt  N 0, R.Intuitively we will be able to infer the velocity by taking successive differences between the observedpositions however, we need to filter out the noise first. This is exactly what the Kalman filter will do for us,as we will see below.1.3.2 InferenceThe equations for Kalman filteringsmoothing can be derived in an analogous manner to the equations forHMMs see Section 3.6.1.ExampleWe illustrate the Kalman filter and smoother by applying them to the tracking problem in Section 1.3.1.Suppose we start out at position 10, 10 moving to the right with velocity 1, 0. We sampled a randomtrajectory of length 15, and show the filtered and smoothed trajectories in Figure 1.3.The mean squared error of the filtered estimate is 4.9 for the smoothed estimate it is 3.2. Not only is thesmoothed estimate better, but we know that it is better, as illustrated by the smaller uncertainty ellipses thiscan help in e.g., data association problems see Section 2.4.6.1.3.3 LearningIt is possible to compute ML or MAP estimates of the parameters of a KFM using gradient methods Lju87or EM GH96b, Mur98. We do not give the equations here because they are quite hairy, and in any case are135 10 15 20 25 30202468101214xytrueobservedfiltered5 10 15 20 25 30202468101214xytrueobservedsmootheda bFigure 1.3 Results of inference for the the tracking problem in Section 1.3.1. a Filtering. b Smoothing.Boxes represent the true position, stars represent the observed position, crosses represented the estimatedmean position, ellipses represent the uncertainty covariance in the position estimate. Notice that thesmoothed covariances are smaller than the filtered covariances, except at t  T , as expected.just a special case of the equations we present in Section C.2.2. Suffice it to say that, conceptually, themethods are identical to the learning methods for HMMs.1.3.4 The problem with KFMsKFMs assume the system is jointly Gaussian. This means the belief state must be unimodal, which is inappropriate for many problems, especially those involving qualitative discrete variables. For example, somesystems have multiple modes or regimes of behavior an example is given in Figure 1.4 either the bird movesto the left, to the right, or it moves straight which can be modelled as a equal mixture of the left and rightmodels, i.e., the dynamics is piecewise linear. This model is called a switching KFM we will discuss thisand related models in Section 2.4.3. Unfortunately, the belief state at time t may have OK t modes indeed,in general inference in this model is NPhard, as we will see in Section 3.6.3. We will consider a variety ofapproximate inference schemes for this model.Some systems have unimodal posteriors, but nonlinear dynamics. It is common to use the extended seee.g., BSF88 or unscented Kalman filter see e.g., WdM01 as an approximation in such cases.1.4 Overview of the rest of the thesisThe rest of this thesis is concerned with representation, inference and learning in a class of models calleddynamic Bayesian networks DBNs, of which HMMs and KFMs are just special cases. By using DBNs, we14Figure 1.4 If a birdplane is heading towards an obstacle, it is more likely to swerve to one side or another,hence the prediction should be multimodal, which a KFM cannot do. This figure is from RN02.are able to represent, and hence learn, much more complex models of sequential data, which hopefully arecloser to reality. The price to be paid is increased algorithmic and computational complexity.In Chapter 2, we define what DBNs are, and give a series of examples to illustrate their modellingpower. This should provide sufficient motivation to read the rest of the thesis. The main novel contribution ofthis chapter is a way to model hierarchical HMMs FST98 as DBNs MP01. This change of representationmeans we can use the algorithms in Chapter 3, which takeOT  time, whereas the original algorithm FST98takes OT 3 time. The reduction in complexity from cubic to linear allows HHMMs to be applied to longsequences of data e.g., biosequences. We then discuss, at some length, the relationship between HHMMsand other models, including abstract HMMs, semiMarkov models and models used for speech recognition.In Chapter 3, we discuss how to do exact inference in DBNs. The novel contributions are a new wayof applying the junction tree algorithm to DBNs, and a way of trading time for space when doing offlinesmoothing BMR97a. In particular, we show how to reduce the space requirements fromOT  to Olog T ,where T is the length of the sequence, if we increase the running time by a logT factor. This algorithmenables us to learn models from very long sequences of data e.g., biosequences.In Chapter 4, we discuss how to speed up inference using a variety of deterministic approximationalgorithms. The novel contributions are a new algorithm, called the factored frontier FF MW01, and ananalysis of the relationship between FF, loopy belief propagation see Section B.7.1, and the BoyenKollerBK98b algorithm. We also compare these algorithms empirically on the problem of modeling freewaytraffic using coupled HMMs KM00. We then survey algorithms for approximate inference in switchingKFMs.In Chapter 5, we discuss how to use Sequential Monte Carlo sampling methods for approximate filtering. The novel contributions are an explanation of how to apply RaoBlackwellised particle filtering RBPF15to general DBNs DdFMR00, MR01, and the application of RBPF to a problem in mobile robotics calledSLAM Simultaneous Localization and Mapping Mur00. This enables one to learn maps with orders ofmagnitude more landmarks than is possible using conventional extended Kalman filter based techniques.For completeness, we also discuss how to apply RBPF and RaoBlackwellised Gibbs sampling to switchingKFMs.In Chapter 6, we explain how to do learning, i.e., estimate the parameters and structure of a DBN fromdata. The main novel contributions are an extension of the structural EM algorithm Fri97, Fri98 to theDBN case FMR98, plus various applications of DBNs, including discovering motifs from synthetic DNAsequences, predicting peoples movements based on tracking data, and modelling freeway traffic data.The appendices contain background material on graphical models that is not specific to DBNs. Appendix A defines various kinds of probabilistic graphical models, and introduces some conditional probabilitydistributions that will be used throughout the thesis. Appendix B contains some novel material on ways ofhandling evidence in the junction tree algorithm Section B.4.8, and a variational approximation for inference in BNs that have discrete nodes with continuous parents Section B.5.3. Appendix C contains somenovel material on computing ML estimates for tied conditional linear Gaussian distributions Section C.2.2,and an experimental comparison of the speed of EM vs gradient methods Section C.4.3. Although the remaining material is not novel, we do not know of any books or articles that provide such a broad treatment ofthe field as such, we believe the appendices have merit in their own right. Appendix D defines some of themore frequently used notation and abbreviations.1.5 A note on softwareMany of the algorithms and examples in this thesis have been implemented using my Bayes Net Toolbox forMatlab BNT Mur01b. This is opensource and is freely available fromwww.cs.berkeley.edumurphykBayesbnt.html.1.6 Declaration of previous workThis thesis is based on the following previously published material Spaceefficient inference in dynamic probabilistic networks, J. Binder, K. Murphy and S. Russell.IJCAI 1997. Section 3.7.1. Learning the structure of dynamic probabilistic networks, N. Friedman, K. Murphy and S. Russell.UAI 1998. Section 6.2.1. A Variational Approximation for Bayesian Networks with Discrete and Continuous Latent Variables,K. Murphy. UAI 1999. Section B.5.3.16 Loopy Belief Propagation for Approximate Inference an Empirical Study, K. Murphy, Y. Weiss andM. Jordan. UAI 1999. Section B.7.1. A Dynamic Bayesian Network Approach to Figure Tracking Using Learned Dynamic Models, V.Pavlovic, J. Rehg, TJ. Cham, and K. Murphy. ICCV 1999. Section 4.3. Bayesian Map Learning in Dynamic Environments, K. Murphy. NIPS 2000. Section 5.3.2. RaoBlackwellised Particle Filtering for Dynamic Bayesian Networks, A. Doucet, N. de Freitas, K.Murphy and S. Russell. UAI 2000. Section 5.3. RaoBlackwellised Particle Filtering for Dynamic Bayesian Networks, K. Murphy and S. Russell. InSequential Monte Carlo Methods in Practice, Doucet et al eds, 2001. Section 5.3. The Factored Frontier Algorithm for Approximate Inference in DBNs, K. Murphy and Y. Weiss.UAI 2001. Section 4.2. Linear time inference in hierarchical HMMs, K. Murphy and M. Paskin. NIPS 2001. Section 2.3.9. The Bayes Net Toolbox for Matlab, K. Murphy. Computing Science and Statistics Proceedings ofthe Interface, 2001. Appendices. A Coupled HMM for AudioVisual Speech Recognition, A. Nefian, L. Liang, X. Pi, L. Xiaoxiang,C. Mao and K. Murphy. ICASSP 2002. Section 2.3.8.17Chapter 2DBNs Representation2.1 IntroductionIn this chapter, I define what DBNs are, and then give a laundry list of examples of increasing complexity.This demonstrates the versatility expressive power of DBNs as a modelling language, and shows that DBNsare useful for a wide range of problems. This should serve as motivation to read the rest of the thesis.The unifying perspective of DBNs brings out connections between models that had previously beenconsidered quite different. This, plus the existence of general purpose DBN software, such as BNT Mur01band GMTK BZ02, will hopefully discourage people from writing new code and new papers everytimethey make what often amounts to just a small tweak to some existing model.The novel contribution of this chapter is a way to represent hierarchical HMMs HHMMs FST98 asDBNs MP01 this is discussed in Section 2.3.9. Once an HHMM is represented as a DBN, any of theinference and learning techniques discussed in this thesis can be applied. In particular, exact inference usingthe junction tree algorithm see Chapter 3 enables smoothing to be performed in OT  time, whereas theoriginal algorithm required OT 3 time. This is just one example of the benefits of thinking in terms ofDBNs.Since DBNs are an extension of Bayes nets BNs, the reader is assumed to already be familiar withBNs read Appendix A for a refresher if necessary.2.2 DBNs definedA dynamic Bayesian network DBN DK89 is a way to extend Bayes nets to model probability distributionsover semiinfinite collections of random variables, Z1, Z2, . . .. Typically we will partition the variables intoZt  Ut, Xt, Yt to represent the input, hidden and output variables of a statespace model. We only considerdiscretetime stochastic processes, so we increase the index t by one every time a new observation arrives.The observation could represent that something has changed as in e.g., NB94, making this a model of adiscreteevent system. Note that the term dynamic means we are modelling a dynamic system, not that18the network changes over time. See Section 2.5 for a discussion of DBNs which change their structure overtime.A DBN is defined to be a pair, B1, B, where B1 is a BN which defines the prior P Z1, and Bis a twoslice temporal Bayes net 2TBN which defines P ZtZt1 by means of a DAG directed acyclicgraph as followsP ZtZt1 Ni1P Zit PaZitwhere Zit is the ith node at time t, which could be a component of Xt, Yt or Ut, and PaZit are the parentsof Zit in the graph. The nodes in the first slice of a 2TBN do not have any parameters associated with them,but each node in the second slice of the 2TBN has an associated conditional probability distribution CPD,which defines P Zit PaZit for all t  1. The form of these CPDs is arbitrary see Tables A.1 and A.2 forsome examples.The parents of a node, PaZ it, can either be in the same time slice or in the previous time slice, i.e.,we assume the model is firstorder Markov. However, this is mostly for notational simplicity there is nofundamental reason why we cannot allow arcs to skip across slices. The arcs between slices are from left toright, reflecting the causal flow of time. If there is an arc from Z it1 to Zit , this node is called persistent. Thearcs within a slice are arbitrary, so long as the overall DBN is a DAG.1 Intuitively, directed arcs within a slicerepresent instantaneous causation. It is also useful to allow undirected arcs within a slice, which modelcorrelation or constraints rather than causation the resulting model is then called a dynamic chain graphDah00. However, we will not consider such models in this thesis.We assume the parameters of the CPDs are timeinvariant, i.e., the model is timehomogeneous. Ifparameters can change, we can add them to the statespace and treat them as random variables. Alternatively,if there is only a finite set of possible parameter values e.g., corresponding to different regimes, we can adda hidden variable which selects which set of parameters to use.The semantics of a DBN can be defined by unrolling the 2TBN until we have T timeslices. Theresulting joint distribution is then given byP Z1T  Tt1Ni1P Zit PaZitThe difference between a DBN and an HMM is that a DBN represents the hidden state in terms of aset of random variables, X1t , . . . , XNht , i.e., it uses a distributed representation of state. By contrast, in anHMM, the state space consists of a single random variableXt. The difference between a DBN and a KFM isthat a KFM requires all the CPDs to be linearGaussian, whereas a DBN allows arbitrary CPDs. In addition,1The intraslice topology of the first slice may be different from the other slices, since the first slice may either represent the stationarydistribution of the chain if we assume the process started at t  , or the initial conditions of the chain if we assume the processstarted at t  1.19HMMs and KFMs have a restricted topology, whereas a DBN allows much more general graph structures.The examples below will make this clearer.Before diving into a series of DBN examples, we remark that some other ways of representing time inthe context of BNs have been proposed in the UAI community, e.g., AC95, DG95, SY99 and the referencestherein. However, very few of these formalisms have genuinely more expressive power as opposed to justhaving nicer syntactic sugar than DBNs, and those that do are generally intractable, from the point ofview of inference, learning or both. In the engineering community, DBNs have become the representationof choice because they embody a good tradeoff between expressiveness and tractability, and include the vastmajority of models that have proved succesful in practice, as we will see below.2.3 Representing HMMs and their variants as DBNsX1 X2 X3Y1 Y2 Y3Figure 2.1 An HMM represented as an instance of a DBN, unrolled for 3 slices. Since the structure repeats,the model can be defined by showing just the first two slices. AX1 X2 X3Y1 Y2 Y3BFigure 2.2 An HMM in which we explicitly represent the parameters nodes with outgoing dotted arcs andthe fact that they are tied across timeslices. The parameters are P X1  i  i, P Xt  jXt1  i Ai, j, and P Yt  jXt  i  Bi, j. If the CPD for Y is a Gaussian, we would replace the B nodewith the mean and covariance parameters.We can represent an HMM as a DBN as shown in Figure 2.1. We follow standard convention and useshading to mean a node is observed clear nodes are hidden. This graph represents the following conditionalindependence assumptions Xt1  Xt1Xt the Markov property and Yt  Yt Xt, for t 6 t. The latterassumption can be relaxed, as we discuss in Section 2.3.3.As is usual for Bayesian networks, we must define the conditional probability distribution CPD of eachnode given its parents. For the HMM in Figure 2.1, this means defining P X1, P XtXt1 and P YtXt.20The CPD for P X1 is usually represented as a vector, which represents a multinomial distribution, i.e.,P X1  i  i, where 0  i  1 andi i  1. The CPD for P XtXt1 is usually representedas a stochastic matrix, i.e., P Xt  jXt1  i  Ai, j where each row which represents a conditionalmultinomial sums to 1. The CPD for P YtXt can take a variety of forms. If Yt is discrete, we coulduse a conditional multinomial, represented as a stochastic matrix P Yt  jXt  i  Bi, j. If Yt iscontinuous, we could use a conditional Gaussian or a conditional mixture of Gaussians see Section 2.3.1.We discuss more exotic types of CPDs, which use fewer parameters, in Section A.3.2.If we assume that the parameters are timeinvariant, we only need to specify P X1, P X2X1 andP Y1X1 the CPDs for future slices are assumed to be the same as in the first two slices. This can massivelyreduce the amount of data needed to learn them. We can model this timeinvariance assumption explicitly byviewing the parameters as random variables see Figure 2.2.One advantage of representing HMMs as DBNs is that it becomes easy to create variations on the basictheme. We discuss some of these models below.2.3.1 HMMs with mixtureofGaussians outputX1 X2 X3M1 M2 M3Y1 Y2 Y3Figure 2.3 An HMM with mixture of Gaussians output.In speech recognition, it is common to represent P YtXt  i using a mixture of Gaussians MoGfor each state i. We can either treat MoG as a primitive CPD type, or we can explicitly model the mixturevariable as shown in Figure 2.3. The CPDs for the Y and M nodes are as followsP YtXt  i,Mt  m  N yti,m,i,mP Mt  mXt  i  Ci,mSuppose we modify the observation CPD so it first performs a linear projection of the data onto a subspace, i.e.,P YtXt  i,Mt  m  N Ayti,m,i,mMaximum likelihood in this model for a single t is equivalent to heteroscedastic mixture discriminant analysis KA96 heteroscedastic since the covariance is not shared across classes values of Xt, and mixture21because of theMt term. Unfortunately, there is no closed form solution for the M step in this case if we hadfully tied , we could at least use standard and fast eigenvector methods see KA96 for details.2.3.2 HMMs with semitied mixturesX1 X2 X3M1 M2 M31 1 A1 2 2 A2 3 3 A3Y1 Y2 Y3  AFigure 2.4 An HMM with semitied covariance matrices. The set of global parameters are the nodes atthe bottom with outgoing dotted arcs. The local parameters for a slice, t, t, and At, are determinsiticallychosen from the global parameters using Mt and optionallyXt.In speech recognition, it is common that Yt  IR39, so estimating a full covariance matrix for each stateofXt andMt requires a lot of data. An alternative is to assume there is a single global pool of Gaussians, andeach state corresponds to a different mixture over this pool. This is called a semicontinuous or tiedmixtureHMM Gal99. In this case, there is no arc from Xt to Yt, so the CPD for Y becomesP YtMt  m  N ytm,mThe effective observation model becomesP YtXt  i mP Mt  mXt  iN ytm,mHence all information about Yt gets to Xt via the bottleneckMt. The advantages of doing this are not onlyreducing the number of parameters, but also reducing the number of Gaussian density calculations, whichspeeds up inference dramatically. For instance, the system can calculate N ytm,m for each m, andthen reuse these in a variety of different models by simple reweighting typically P Mt  mXt  i isnonzero for only a small number of ms Jel97, p161. The Mt  Yt arc is like vector quantization, and theXt Mt arc is like a dynamic reweighting of the codebook.More sophisticated parameter tying schemes are also possible. Figure 2.4 represents an HMM with22semitied covariance matrices Gal99. The CPDs for this model are as followsP Yt  yt,t, At  N ytt, AttAtP t,Mt  m,Xt  i  t, i,mP At A,Xt  i  At, AiP t,Mt  m,Xt  i  t,m,iTypically each m,i is assumed to be diagonal, but Ai converts this into a full matrix. In practice, wecan compile out the deterministic CPDs, so that the overall graph looks like Figure 2.3, but where theobservation CPD is defined as followsP Yt  yMt  m,Xt  i  N yti,m, Aii,mAiTo compute ML parameter estimates in this model, we must use a nonlinear optimization method in the Mstep of EM see Section C.4.2, since the covariance is bilinear in Ai and i,m. We can further reduce thenumber of parameters by associating an A matrix with a set of states of Xt, instead of having one matrix perstate.2.3.3 Autoregressive HMMsX1 X2 X3Y1 Y2 Y3Figure 2.5 An autoregressive HMM.The standard HMM assumption that Yt  Yt Xt is quite strong, and can be relaxed at little extra cost asshown in Figure 2.5. We will call this model an autoregressive HMM ARHMM.2 The ARHMM modelreduces the effect of the Xt bottleneck, by allowing Yt1 to help predict Yt as well this often results inmodels with higher likelihood.If Y is discrete e.g., for language modelling see Section 2.3.5, the CPD for Y can be represented as atable. If Y is continuous, one possibility for the CPD for Y isP Yt  ytXt  i, Yt1  yt1  N ytWiyt1  i,iwhereWi is the regression matrix given thatXt is in state i. This model is also known as a correlation HMM,conditionally Gaussian HMM switching regression model, switching Markov model Ham90, or switchingregression model.2Confusingly, the term autoregressive HMM refers to two different models, the one being discussed here and the one in Rab89,which is also called a linear predictive or hidden filter HMM, which looks like a regular HMM when represented as a DBN, but whichuses a statedependent autocorrelation function for the observation distribution.232.3.4 Buried Markov ModelsXt1  xt1 Xt  xt Xt1 xt1Y1Xt1  xt1 Xt  xt Xt1 xt1Y2Y1Y2Figure 2.6 A buried Markov model. Depending on the value of the hidden variables, Xt, the effective graphstructure between the components of the observed variables, Yt, can change. Two different instantiations areshown. Thanks to Jeff Bilmes for this figure.Buried Markov models Bil98 generalize autoregressive HMMs by allowing nonlinear dependenciesbetween the observable nodes. Furthermore, the nature of the dependencies can change depending on thevalue of Xt see Figure 2.6 for an example. Such a model is called a Bayesian multi net. Bil00introduces the EAR metric, which is a way to learn such structures in a discriminative way.32.3.5 Mixedmemory Markov modelsX1 X2 X3 X4Figure 2.7 A trigram secondorder Markov model, which defines P XtXt1, Xt2.St3 St2 St1 StXt3 Xt2 Xt1 XtSt  2St  1Figure 2.8 A mixedmemory Markov model. The dashed arc from St to Xt means St is a switching parentit is used to decide which of the other parents to use, either Xt1 or Xt2 or both. The conditions underwhich each arc is active are shown for the Xt node only, to reduce clutter. The dotted arcs from Xt1 andXt2 to St are optional. See text for details. Based on Figure 15 of Bil01.As we mentioned in the introduction, one of the simplest approaches to modelling sequential data is toconstruct a Markov model of order n 1. These are often called ngram models, e.g., when n  2, we get a3The basic idea is to avoid modelling features of the data which are common to all classes, but instead to focus on aspects of thestructure that affect the decision boundaries.24bigram model, and when n  3, we get a trigram model, as shown in Figure 2.7.When Xt is a discrete random variable with many possible values e.g., if Xt represents words, thenthere might not be enough data to reliably estimate P Xt  kXt1  j,Xt2  i. A common approachis to create a mixture of lowerorder Markov modelsP XtXt1, Xt2 3Xt1, Xt2fXtXt1, Xt2  2Xt1, Xt2fXtXt1  1Xt1, Xt2fXtwhere the  coefficients may optionally depend on the history, and f is an arbitrary conditional probability distribution. This is the basis of the method called deleted interpolation Jel97, CG96.As with mixtures of Gaussians, we can either represent mixtures of multinomials as a primitive CPD,or we can explicitely model the latent mixture variable as in Figure 2.8. Here St  1 means use a unigram,St  2 means use bi and unigrams, and St  3 means use tri, bi and unigrams. St is called a switchingparent, since it determines which of the other parents links are active, i.e., the effective topology of the graphchanges depending on the value of the switch the result is called a dynamic Bayesian multinet Bil00.Since St is hidden, the net effect is to use a mixture of all of these. The arcs from Xt2 and Xt1 to Stmodel the fact that the coefficients can depend on the identity of the words in the window.A very similar approach, called mixedmemory Markov models SJ99, is to always use mixtures ofbigrams if St  1, we use P XtXt1, and if St  2, we use P XtXt2. This can be achieved bysimply changing the guard condition on the Xt1 to Xt link to be St  1 instead of St  1 similarly, theguard on theXt2 toXt link becomes St  2 instead of St  2. Hence St acts like the input to a multiplexer,choosing one of the parents to feed into Xt Using the terminology of BZ02, St is the switching parent, andthe other parents are the conditional parents. The overall effect is to define the CPD as followsP XtXt1, . . . , Xtn ni1AiXti, XtiAn alternative is to cluster the words and then try to predict the next word based on the current clusterthis is called a classbased language model. The cluster identity is a hidden random variable, as in a regularHMM the words are the observations.Of course, the mixedmemory and classbased techniques can be combined by using an autoregressiveHMM, to get the best of both worlds.2.3.6 Inputoutput HMMsAn inputoutput HMM BF96 is a probabilistic mapping from inputs, U1T , to outputs, Y1T . We relax therequirement that the input and output be the same length in Section 2.3.11. This can be modelled as a DBNas shown in Figure 2.9. If the inputs are discrete, the CPD for X can be represented as a 3dimensional25U1 U2 U3X1 X2 X3Y1 Y2 Y3Figure 2.9 An inputoutput HMM. The dotted arcs are optional.array, P Xt  jXt1  i, Ut  k  Ai, j, k. If the input is continuousvalued, the CPD for X can berepresented as a conditional softmax function see Section A.3.2, or as a neural network.2.3.7 Factorial HMMsX11 X12 X13X21 X22 X23X31 X32 X33Y1 Y2 Y3Figure 2.10 A factorial HMM with 3 chains.Factorial HMMs GJ97 use a single output variable but have a distributed representation for the hiddenstate, as shown in Figure 2.10. Note that, although all the chains are a priori independent, once we conditionon the evidence, they all become coupled this is due to the explaining away phenomenon Pea88. Thismakes inference intractable if there are too many chains, as we discuss in Chapter 3.The CPDs for the hidden nodes, P X dt Xdt1, can be represented by Nx  Nx matrices assuming each chain can be in one of Nx states. A naive representation for the CPD for the observed nodes,P YtX1t , . . . , XDt , which we shall abbreviate to P YtX1Dt , would require ONxD parameters,for each possible combination of the parents. We discuss some more parsimonious representations in Section A.3.2.A factorial HMM, like any DBN, can be converted to a regular HMM by creating a single mega variable, Xt, whose state space is the Cartesian product of the component state spaces. However, this is a badidea the resulting flat representation is hard to interpret, inference in the flat model will be exponentiallyslower see Chapter 3, and learning will be harder because there may be exponentially many more parameters. In particular, although the entries of the transition matrix of the flat HMM would have constraints26between them, it is hard to exploit these constraints either in inference or learning. For example, if D  2,we haveP X1t  j1, X2t  j2X1t1  i1, X2t1  i2  P X1t  j1X1t1  i1 P X2t  j2X2t1  i2.That is, the entries of the transition matrix for the flat HMM are computed by multiplying together the entriesof the transition matrices for each chain in the factorial HMM.If all CPDs are linearGaussian, graphical structure corresponds to sparse matrices in the traditional senseof having many zeros see Section 2.4.2 in such a case, the compound DBN can be converted to a flat modelwithout loss of efficiency or information.2.3.8 Coupled HMMsX11 X12 X13Y 11 Y12 Y13X21 X22 X23Y 21 Y22 Y23X31 X32 X33Y 31 Y32 Y33Figure 2.11 A coupled HMM with 3 chains.In a coupled HMM SJ95, Bra96, the hidden variables are assumed to interact locally with their neighbors. Also, each hidden node has its own private observation. This is shown in Figure 2.11.I have used CHMMs for two different applications audiovisual speech recognition AVSR and formodelling freeway traffic. In the AVSR application, there are two chains, one represents the audio stream andone representing the video stream please see NLP02 for details. NY00 also used a twochain CHMM forautomatic speech recognitions ASR. To reduce the number of parameters, they defined P X 1t X1t1, X0t1as a mixture of two bigram models c.f., Section 2.3.5P X1t X1t1, X0t1  A11X1t X1t1P St  1 A10X1t X0t1P St  0In the traffic application, Xdt represents the hidden traffic status freeflowing or congested at locationd on the freeway at time t. Y dt represents the reading returned by a magnetic loop detector, which roughly27speaking is an indicator of traffic speed. The underlying traffic state at location d and time t is assumed todepend on its previous state and the previous states of its immediate spatial neighbors. Since the detectorsare placed in a line down the middle of the highway, each location only has 2 neighbors, upstream anddownstream. Please see KM00 for details.2.3.9 Hierarchical HMMs HHMMsThe Hierarchical HMM HHMM FST98 is an extension of the HMM that is designed to model domainswith hierarchical structure andor dependencies at multiple lengthtime scales. In an HHMM, the states ofthe stochastic automaton can emit single observations or strings of observations. Those that emit singleobservations are called production states, and those that emit strings are termed abstract states. Thestrings emitted by abstract states are themselves governed by subHHMMs, which can be called recursively.When the subHHMM is finished, control is returned to wherever it was called from the calling context ismemorized using a depthlimited stack.Example of an HHMM1 23 4 56 7 8 9 10 11 12 13a b c d14 15 16x yFigure 2.12 State transition diagram for a fourlevel HHMM representing the regular expressionaaxybcxyd. Solid arcs represent horizontal transitions between states dotted arcs represent vertical transitions, i.e., calling a subHMM. Doubleringed states are accepting end states we assume thereis at least one per subHMM when we enter such a state, control is returned to the parent calling state.Dahsed arcs are emissions from production concrete states In this example, each production state emits asingle symbol, but in general, it can have a distribution over output symbols.We illustrate the generative process with Figure 2.12, which shows the state transition diagram of anexample HHMM which models the regular expression aaxybcxyd.4 We start in the root state.Since this is an abstract state, it calls its subHHMM, entering into state 3 or 4 this is called a vertical4This means the model must produce one or more as, or one or more as followed by one or xs and ys followed by a single b,or a c followed by one or more xs and ys followed a d.28transition. Suppose it enters state 3. Since 3 is abstract, it enters its child HMM via its unique entry point,state 6. Since 6 is a production state, it emits the symbol a. It may then loop around and exit, or make ahorizontal transition to 7. State 7 then enters its subHMM state 14, which emits x. State 14 makes ahorizontal transition to state 15, and then emits y. Suppose at this point we make a horizontal transitionto state 16, which is the end state for this subHMM. This returns control to wherever we were called from in this case, state 7. State 7 then makes a horizontal transition to state 8, which emits b. State 8 is thenforced to make a horizontal transition to the end state, which returns control to state 3. State 3 then enters itsend state 5, and returns control to the root 1. The root can then either reenter its subHMM, or enter itsend state, which terminates the process.An HHMM cannot make a horizontal transition before it makes a vertical one hence it cannot producethe empty string. For example, in Figure 2.12, it is not possible to transition directly from state 1 to 2.Related modelsWhile HHMMs are less powerful than stochastic contextfree grammars SCFGs and recursive transitionnetworks RTNs JM00, both of which can handle recursion to an unbounded depth, unlike HHMMs, theyare sufficiently expressive for many practical problems, which often only involve tailrecursion i.e., selftransitions to an abstract state. Furthermore, HHMMs can be made much more computationally efficientthan SCFGs. In particular, while the original inference algorithm for HHMMs was OT 3, as for SCFGs,once we have converted the HHMM to a DBN see Section 2.3.9, we can use any of the techniques inChapter 3 to do inference in OT  time MP01.5HHMMs are also closely related to cascades of finite state automata Moh96, PR97b, but are fullyprobabilistic. A somewhat similar model, called a cascaded Markov model, was proposed in Bra99b in thismodel, each HMM state represents a nonterminal in an SCFG, which can be used to generate a substring.Other HMMSCFG hybrids have been proposed for example, IB00 suggest creating one HMM to recognizeeach terminal symbol, and then combing them using an SCFG. Unfortunately, inference and learning in suchmodels is much more complicated than with an HHMM, not only because inference takes OT 3 time, butalso because the output of the bank of HMMs is a probability distribution over symbols at each position, asopposed to a single symbol as the standard SCFG inference routines expect.PW96 discuss a way of combining static Bayes nets with SCFGs, but their goal is to represent aprobability distribution over the parse trees obtained from an SCFG. ND01 show how Bayes nets can beused as a model of how humans combine cues when parsing ambiguous sentences online.29a b c dx y x yFigure 2.13 State transition diagram of the HHMM in Figure 2.12 flattened to a regular HMM. Each state islabelled by the symbol it emits  represents the empty string. The initial and final states are omitted.Converting an HHMM to an HMMAny HHMM can be converted to a regular HMM by creating an HMM state for every leaf in the HHMM statetransition diagram i.e., every legal HHMM stack configuration. The resulting statespace may be smaller,because it does not contain abstract states, but may also be larger, because shared substructure such as thesubexpression xy must be duplicated. See Figure 2.13 for an example.Computing the parameters of the resulting flat HMM is not always trivial. The probability of an i to jtransition in the flat model is the sum over all paths from i to j in the hierarchical model which only passthrough abstract nonemitting states. For example, in Figure 2.13, the probability of a selftransition fromstate a to a is given byPflata a  Ph6 6  Ph6 9 3 6  Ph66  Ph96Ph63where Ph represents probabilities in the HHMM.The HMMs used in speech recognition are essentially hierarchical see Section 2.3.10, but are alwaysflattened into a single level state space for speed and ease of processing. Simiarly, combinations of weightedtransducers Moh96, PR97b are always flattened into a single transducer before use. Although it is alwayspossible to convert an HHMM to an HMM, just as with any DBN, there are several disadvantages to doingso Flattening loses modularity, since the parameters of the subHMMs get combined in a complex way,as we have just seen. A flat HMM cannot easily provide a multiscale interpretation of the data.6 Training HMMs separately and combining them into a hierarchical model requires segmented data.This is the approach adopted in HIM00 for example.5The inference algorithm for SCFGs, which is called the insideoutside algorithm see e.g., JLM92, JM00, takes OT 3 no matterwhat the grammar is.6It is also possible to interpret a single integer, representing the flat state, as a vector of integers, representing the HHMM state.However, sometimes it is useful to explicitly represent the different variables, instead of doing this postprocessing interpretation seeSection 2.3.10 for example.30 The parameters in a flat HMM are constrained, but it is hard to exploit these constraints during inferenceand learning, because the constrained parameters are products of the free parameters, as we saw in theexample above, and in the case of flattening factorial HMMs in Section 2.3.7. By contrast, it is easy todo parameter estimation in an HHMM. Furthermore, the fact that submodels are reused in differentcontexts can be represented, and hence learned, using an HHMM, but not using an HMM.Representing the HHMM as a DBNF 11 F12 F13Q11 Q12 Q13F 21 F22 F23Q21 Q22 Q23F 31 F32 F33Q31 Q32 Q33Y1 Y2 Y3Figure 2.14 A 3level HHMM represented as a DBN. Qdt is the state at time t, level d Fdt  1 if the HMMat level d has finished entered its exit state, otherwise F dt  0. Shaded nodes are observed the remainingnodes are hidden. In some applications, the dotted arcs fromQ1 and the dashed arcs fromQ2 may be omitted.We can represent the HHMM as a DBN as shown in Figure 2.14. We assume for simplicity that allproduction states are at the bottom of the hierarchy this restriction is lifted in Section 2.3.9. The state of theHMM at level d and time t is represented by Qdt . The state of the whole HHMM is encoded by the vectorQt  Q1t , . . . , QDt  intuitively, this encodes the contents of the stack, that specifies the complete path totake from the root to the leaf state in the state transition diagram.F dt is a binary indicator variable that is on has value 1 if the HMM at level d and time t has justfinished i.e., is about to enter an end state, otherwise it is off has value 0. Note that if F dt  1, thenF dt  1 for all d  d hence the number of F nodes that are off represents the effective height of thecontext stack, i.e., which level of the hierarchy we are currently on.31The downward arcs between theQ variables represent the fact that a state calls a substate. The upwardarcs between the F variables enforce the fact that a higherlevel HMM can only change state when the lowerlevel one is finished. This ensures proper nesting of the parse trees, and is the key difference between anHHMM and a hidden Markov decision tree JGS96. An HMDT looks similar to Figure 2.14, except it lacksthe F nodes and the upward going arcs.We now define the conditional probability distributions CPDs of each of the node types below, whichwill complete the definition of the model. We consider the bottom, middle and top layers of the hierarchyseparately since they have different local topology, as well as the first, middle and last time slices.Bottom level d  D, t  2  T  1 QD follows a Markov chain with parameters determined by whichsubHMM it is in, which is encoded by the vector of higherup state variables Q1D1t , which we willrepresent by the integer k for brevity.7 Instead of QD entering its end state, it turns on FD instead,to mean it is finished see Section 2.3.9 for more details This will be a signal that higherlevel HMMscan now change state. In addition, it will be a signal that the next value of QD should be drawn fromits prior distribution representing a vertical transition, instead of its transition matrix representing ahorizontal transition. Formally, we can write this as followsP QDt  jQDt1  i, FDt1  f,Q1D1t  k ADk i, j if f  0Dk j if f  1where we have assumed i, j 6 end, where end represents the endstate for this HMM. ADk is thetransition matrix for level D given that the parent variables are in state k, and ADk is just a rescaledversion of this see Section 2.3.9 for details on the rescaling. Similarly, Dk is the initial distributionfor level D given that the parent variables are in state k. The equation for FD is simplyP FDt  1Q1D1t  k,QDt  i  ADk i, end.Intermediate levels d  2  D  1, t  2  T  1 As before,Qd follows a Markov chain with parametersdetermined by Q1d1, and F d specifies whether we should use the transition matrix or the prior. Thedifference is that we now also get a signal from below, F d1, specifying whether the submodel hasfinished or not if it has, we are free to change state, otherwise we must remain in the same state.Formally, we can write this as followsP Qdt  jQdt1  i, F d1t1  b, F dt1  f,Q1d1t  k i, j if b  0Adki, j if b  1 and f  0dkj if b  1 and f  12.1F d should turn on only if Qd is allowed to enter a final state, the probability of which depends on7If the state transition diagram is sparse, many of these joint configurations will be impossible k will only index the valid configurations. Such sparse CPDs can be conveniently represented using decision trees see Section A.3.2.32the current context Q1d1. Formally, we can write this as followsP F dt  1Qdt  i, Q1d1t  k, F d1t  b 0 if b  0Adki, end if b  12.2Top level d  1, t  2  T  1 The top level differs from the intermediate levels in that theQ node has noQ parent to specify which distribution to use. The equations are the same as above, except we eliminatethe conditioning on Q1d1t  k. Equivalently, we can imagine a dummy top layer HMM, which isalways in state 1 Q0t  1. This is often how HHMMs are represented, so that this toplevel state is theroot of the overall parse tree, as in Figure 2.12.Initial slice t  1, d  1  D The CPDs for the nodes in the first slice are as follows P Q11  j  1jfor the top level and P Qd1  jQ1d11  k  dkj, for d  2, . . . , D.Final slice t  T , d  1  D To ensure that all subHMMs have reached their end states by the time wereach the end of sequence, we can clamp F dT  1 for all d. In fact, it is sufficient to clamp F1T  1,since this implies F dT  1 for all d. A similar trick was used in Zwe98, to force all segmentations tobe consistent with the known length of the sequence.Observations If the observations are discrete symbols, we may represent P Ot Qt as a table. If the observations are realvalued vectors, we can use a Gaussian for each value of Qt. We discuss moreparsimonious representations in Section A.3.2. Alternatively, we can condition Ot only on some ofits parents. This is often done in speech recognition the acoustic observations depend only on thebottom two levels of the hierarchy the subphones and phones, and not on the top level the wordssee Section 2.3.10 for details.Handling end statesUnlike the automaton representation, the DBN never actually enters an end state i.e., Qdt can never taken onthe value end if it did, what should the corresponding observation be Unlike an HMM, a DBN mustgenerate an observation at every time step. Instead, Qdt causes Fdt to turn on, and then enters a new nonterminal state at time t 1, chosen from its initial state distribution. This means that the DBN and HHMMtransition matrices are not identical. However, they satisfy the following equationAdki, j1Adki, end Adki, jwhere A represents the HHMM transition matrix, A represents the DBN transition matrix, and Adki, end isthe probability of terminating from state i. The reason is that the probability of each horizontal transition inthe DBN gets multiplied by the probability that F dt  0, which is 1Adki, end this product should matchthe original probability. It is easy to see that the new matrix is stochastic i.e.,Kj1 Adki, j  1, since theoriginal matrix satisfiesKj1 Adki, j Adki, end  1, where K is the number of states.33Allowing production states at different levels1 23 4 56 7 8 9 10 11 12 1314 15 16 17 18 19 20a b x y c dFigure 2.15 State transition diagram of the HHMM in Figure 2.12, except all production states are at theleaves.In FST98, the authors remark that putting all the production concrete states at the bottom level didnot result in learning interesting hierarchical structure i.e., the results were essentially the same as usinga flat HMM. They therefore constructed the automaton topology by hand, and allowed production states atmultiple levels of the hierarchy, at randomly chosen locations. We discuss HHMM learning in Chapter 6 fornow we assume that the parameters  and hence the topology of the statetransition diagram  are known.So far, the DBN representation has assumed that all the production states are at the bottom level. Thiscan always be made the case, as illustrated in Figure 2.15. However, we now develop a way to directly encodein the DBN the fact that concrete states can occur at different levels in the hierarchy. We simply ensure thatwhen Qdt enters a concrete state, it causes all the Q nodes below it to enter a special uninformative ornull state. In addition, all the F nodes below level d will be forced on, so that level d is free to changestate without waiting for its children to finish. The observation distribution will ignore null states, so thedistribution simplifies from P OtQ1Dt  to P OtQ1dt  if Qdt is a concrete state. It is straightforward tomodify the CPD definitions to cause this behavior.The above solution is somewhat inefficient since it introduces an extra dummy value, thus increasing thesize of the state space of each level by one. A more elegant solution is to allow some arcs to disappearconditionally on the values of the Q nodes, i.e., let the Q nodes act both as regular nodes and as switchingparents c.f., Section 2.3.5. Specifically, when Qdt enters a concrete state, all the outgoing links from nodesQdt and Fdt , for d  d, would be rendered effectively i.e., parametrically redundant. It is straightforwardto modify the CPD definitions to cause this behavior.34Advantages of representing the HHMM as a DBNThere are several advantages of representing the HHMM as a DBN rather than using the original formulationFST98 in terms of nested statetransition diagrams. First, we can use generic DBN inference and learningprocedures, instead of having to derive fairly complicated formulas by hand. Second, exact DBN inferencetakes OQDT  time, whereas the algorithm proposed in FST98 takes OQDT 3 time. Third, it is easier tochange the model once it is represented as a DBN, e.g., to allow factored hierarchical state spaces.2.3.10 HHMMs for Automatic speech recognition ASRa n a7 8 91 2 3b a n 4 5 6ah n erFigure 2.16 HMM state transition diagram for a model encoding two possible pronunciations of banana,the British version bottom fork middle a is long, and final a has r appended, and the Americanversion top fork all as are equal. Dotted lines from numbers to letters represent deterministic emissions.The states may have selfloops not shown to model duration.Qh1 Qh2 Qh3Q1 Q2 Q3F s1 Fs2 Fs3S1 S2 S3Y1 Y2 Y3Figure 2.17 A DBN for modelling the pronunciation of a single word. Qh is the state position in the wordHMM Q is the phone S is the state position within the phone HMM, i.e., the subphone Y is the acousticvector. F s is a binary indicator variable that turns on when the phone HMM has finished.Consider modelling the pronunciation of a single word. In the simplest case, the can be described as asequence of phones, e.g., dog is d  o  g. This can be represented as a lefttoright finite state automaton.35Qh1 Qh2 Qh3F s1 Fs2 Fs3Q1 Q2 Q3Y1 Y2 Y3Figure 2.18 A DBN for modelling the pronunciation of a single word, where each phone is modelled by asingle state. This is the simplification of Figure 2.17 if S has only one possible value. This corresponds tothe model used in Zwe98.But consider a word like yamaha the first a is followed by m, but the second a is followed by h.Hence some automaton states need to share the same phone labels. Put another way, given a phone label, wecannot always uniquely identify our position within the automatonword. This suggests that we use an HMMto model word pronunciation. Such a model can also cope with multiple pronunciations, e.g., the British orAmerican pronunciation of banana see Figure 2.16.8Let Qht be the hidden automaton state at time t, and Qt be the observed phone. For the yamaha model,we have Qht  1, . . . , 9 and Qt  y, aa, m, a, h. Note that P Qt  kQht  q  Bq, k is adelta function e.g., state 2 only emits aa. Also, the transition matrix, P Qht  qQht1  q  Aq, q,can always be made upper diagonal by renumbering states, and will typically be very sparse.Now suppose that we do not observe the phones directly, but that each phone can generate a sequence ofacoustic vectors we will model the acoustic appearance and duration of each phone with a phone HMM.The standard approach is to embed the appropriate phone HMMs into each state of the word HMM. However,we then need to tie the transition and observation parameters between different states.An alternative is to make the parameter tying graphically explicit by using HHMMs this allows the tyingpattern to be learned. Specifically, let each state of the word model,Qht , emitQt which then calls the phoneHMM. The state of the phone HMM at time t is St this is called the subphone. Since we dont know howlong each phone lasts, we introduce F S which only turns on, allowing Qht to make a transition, when thephone HMM has finished. This can be accomplished as shown in Figure 2.17. F S is conditioned on Qt, notQht , which represents the fact that the duration depends on the phone, not the automaton state i.e., durationinformation is tied. Similarly, Yt is conditioned on Qt but not Qht .Normally the phone HMMs are 3state lefttoright HMMs. However, if we use a single state HMM,we can simplify the model. In particular, if St can have only one possible value, it can be removed from8The yamaha example is from Jeff Bilmes, the banana example is from Mark Paskin.36the graph the duration of the phone will be determined by the selfloop probability on the correspondinghidden state in the word HMM. The resulting DBN is shown in Figure 2.18, and corresponds to the modelused in Zwe98. If the word HMM has a strict lefttoright topology,Qht deterministically increases by 1 iffFSt  1 hence P FSt  1Qt  k  1  Ai, i, where i is any state that deterministically emits thephone k. If the word HMM is not lefttoright, then F S can specify which outarc to take from state Qht1,so P Qht Qht1, F st1 becomes deterministic ZR97.The above was a model for a single word. If we have a sequence of words, we add another layer to thehierarchy, and condition the variables inside the word HMM i.e., Qh and Q on the hidden identity of theword see Figure 2.19. The fact that the duration and appearance are not conditioned on the word representsthe fact that the phones are shared across words see Figure 2.20. Sharing the phone models dramaticallyreduces the size of the state space. Nevertheless, with, say, 6000 words, 60 phones, and 3 subphones, thereare still about 1 million unique states, so a lot of training data is required to learn such models. In reality,things are even worse because of the need to use triphone contexts, although these are clustered, to avoidhaving 603 combinations.Note that the number of legal values for Qht can vary depending on the value of Wt, since each word hasa differentsized HMM pronunciation model. Also, note that silence a gap between words is different fromsilent nonemitting states silence has a detectable acoustic signature, and can be treated as just anotherword whose duration is of course unknown, whereas silent states do not generate any observations.We now explicitly define all the CPDs for t  1 in the model shown in Figure 2.19.P Wt  wWt1  w,FWt1  f w,w if f  0Aw,w if f  1P FWt  f Qht  q,Wt  w,FSt  b f, 0 if b  01Awq, end if b  1 and f  0Awq, end if b  1 and f  1P Qht  qQht1  q,Wt  w,FWt1  f, FSt1  b q, q if b  0Awq, q if b  1 and f  0wq if b  1 and f  1P Qt  kQht  q,Wt  w  Bwq, kP F st  1St  j,Qt  k  Akj, endP St  jSt1  i, Qt  k, Ft1  f Aki, j if f  0kj if f  1whereAw,w is the word bigram probabilityAwq, q is the transition probability between states in wordmodel w, wq is the initial state distribution, and Bwq is the phone deterministically emitted by state qin the HMM for word w, and similarly, Aki, j kj and Bkj, yt are the parameters for the kth phoneHMM. We could model P YtSt  j,Qt  k using a mixture of Gaussians, for example.37W1 W2 W3Fw1 Fw2 Fw3Qh1 Qh2 Qh3Q1 Q2 Q3F s1 Fs2 Fs3S1 S2 S3Y1 Y2 Y3Figure 2.19 A DBN for continuous speech recognition. Note that the observations, Yt, the transition probabilities, St, and the termination probabilities, F St , are conditioned on the phone Qt, not the position withinthe phone HMM Qht , and not on the word, Wt.needon thewordsphonessubphonesaa n end n iy d dhnaxiyendendendendFigure 2.20 An example HHMM for an ASR system which can recognize 3 words adapted from JM00.The phone models bottom level are shared tied amongst different words only some of them are shown.38wW h1 Wh2 Wh3W1 W2 W3Fw1 Fw2 Fw3Qh1 Qh2 Qh3Q1 Q2 Q3F s1 Fs2 Fs3S1 S2 S3Y1 Y2 Y3Figure 2.21 A DBN for training a continuous speech recognition system from a known word sequence w.In practice, the w node can be absorbed into the CPDs for W . Also, if the mapping from words to phones isfixed, we can also compile out W h, W , and FW , resulting in the much simpler model shown in Figure 2.17or Figure 2.18 if we dont use subphones.39Training HHMMs for ASRWhen training an HMM from a known word sequence, we can use the model in Figure 2.21. We specify thesequence of words, w, but not when they occur. If W ht  k, it means we should use the kth word at timet hence P Wt  wW ht  k, w  w,wk is a delta function, c.f., the relationship between Qh and Q.Also, W h increases by 1 iff FW  1, to indicate the next word in the sequence.In practice, we can compile out the w node, by changing the CPDs for the W nodes for each trainingsequence. If the mapping from words to phones is fixed, we can also compile outW h, W , and FW , resultingin the model shown in Figure 2.17 or Figure 2.18 if we dont use subphones. Now Qh acts as an indexinto the known subphone sequence corresponding to w, so its CPD becomes deterministic increase Qhby 1 iff F St1  1, which occurs iff the phone HMM has finished, and the CPD for the phone becomesP Qt  kQht  i  Bwq, k, if i corresponds to state q in word HMM w. This is equivalent to combiningall the word HMMs together, and giving all their states unique numbers, which is the standard way of trainingHMMs from a known phonetic sequence.Note that the number of possible values for W h in Figure 2.21 is now Tin, the number of words in theinput training sequence. One concern is that this might make inference intractable. The simplest approachto inference is to combine all the interface variables see Section 3.4 into one mega variable, and then usea modified forwardsbackwards procedure, which has complexity OToutN2, where N is the number ofstates of the mega variable, and Tout is the length number of frames of the output observed sequence. Theinterface variables the ones with outgoing temporal arcs in Figure 2.21 are as follows W ht  1, . . . , Tin,Qht  1, . . . , P, St  1, . . . ,K, FWt  0, 1, F St  0, 1, where P is the max number of states inany word model, and K is the maximum number of states in each phone HMM typically 3. Hence it wouldappear that exact smoothing requiresOToutT 2inP2K2 time. However, because of the lefttoright nature ofthe transition matrices for W h, Qh and S, this can be reduced to OToutTinPK time. This is the same asfor a standard HMM up to constant factors, since there are N  TinPK states created by concatenatingthe word HMMs for the whole sequence.When training, we know the length of the sequence, so we only want to consider segmentations thatuse up all the phones. We can do this by introducing an extra end of sequence eos variable, thatis clamped to some observed value, say 1, and whose parents are QhT and FsT  we define its CPD to beP eos  1QhT  i, F sT  f  1 iff i is the last phone in the training sequence, and f  1. The reason werequire f  1 is that this represents the event that we are just about to transition to a silent accepting state.This is the analog of only examining paths that reach the top right part of the HMM trellis, where the toprow represents the final accepting state, and the right column represents the final time slice. This trick wasfirst suggested in Zwe98. A simpler alternative is simply to set QhT  i and FsT  1 as evidence this hasthe advantage of not needing any extra variables, although it is only possible if there is only one state namely40i immediately preceeding the final state which will be true for a counting automaton of the kind consideredhere.General DBNs for ASRSo far, we have focussed on HHMMs for ASR. However, it is easy to create more flexible models. Forexample, the observation nodes can be represented in factored form, instead of as a homogeneous vectorvalued node. Note that this only affects the computation of the conditional likelihood terms, Bti, i P ytQt  i, so the standard forwardsbackwards algorithm can be used for inference. More generalhidden nodes can also be used, representing the state of the articulators, e.g., position of the tongue, size ofmouth opening, etc Zwe98. If the training set just consists of standard speech corpora, there is nothing toforce the hidden variables to have this meaningful intrepretation. Row99, RBD00 trained on data wherethey were able to observe the articulators directly the resulting models are much easier to intrepret. SeeBil01 for a more general review of how graphical models can be used for ASR.2.3.11 Asynchronous IOHMMsWhen training an HHMM for ASR, we are essentially learning a probabilistic mapping from w  W1Tinto Y1Tout , where typically Tin  Tout. This is like an asychronous inputoutput HMM BB96, which canhandle input and output sequences of different lengths. Note that the complexity of the inference algorithmfor asychronous IOHMMs in BB96 is OTinTout, just like the DBN method. By contrast, inference is asynchronous IOHMM, where Tin  Tout  T , only takes time OT .Bengio Ben99 suggested using IOHMMs for ASR, treating the acoustic sequence as input and thewords as output, the opposite of what we discussed above. There are two problems with this. First, we maybe forced to assign high probability to unlikely observation sequences this is because we are conditioningon the acoustics instead of generating them if there is only one outgoing arc from a state, it must haveprobability one no matter what the input is, because we normalize transition probabilities on a perstate level.This has been called the label bias problem MFP00. A second and more important problem is that themeaning of the states in an IOHMM is different than in an HMM. In particular, the states summarize pastacoustics, rather than past words, so the kind of sharing of phone models we discussed above, that is vital tosuccessful performance, cannot be used.92.3.12 Variableduration semiMarkov HMMsThe selfarc on a state in an HMM defines a geometric distribution over waiting times. Specifically, theprobability we remain in state i for exactly d steps is pd  1 ppd1, where p  Ai, i is the selfloop9Y. Bengio, personal communication, 31802.41Q1 Q2 Q3F1 F2 F3QD1 QD2 QD3Y1 Y2 Y3Figure 2.22 A variableduration HMM modelled as a 2level HHMM. Qt represents the state, and QDtrepresents how long we have been in that state duration.probability. To allow for more general durations, one can use a semiMarkov model. It is called semiMarkov because to predict the next state, it is not sufficient to condition on the past state we also need toknow how long weve been in that state. We can model this as a special kind of 2level HHMM, as shown inFigure 2.22. The reason there is no Q to F arc is that the termination process is deterministic. In particular,the bottom level counts how long we have been in a certain state when the counter expires reaches 0, theF node turns on, the parent node Q can change state, and the duration counter, QD, resets. Hence the CPDfor the bottom level is as followsP QDt  dQDt1  d,Qt  k, Ft1  1  pkdP QDt  dQDt1  d,Qt  k, Ft1  0 1 if d  1 and d  d 10 otherwiseNote that pkd could be represented as a table a nonparametric approach or as some kind of parametricfunction. If pkd is a geometric distribution, this emulates a standard HMM.The naive approach to inference in this HHMM takes OTD2K2 time, where D is the maximumnumber of steps we can spend in any state, and K is the number of HMM states this is the same complexityas the original algorithm Rab89. However, we can exploit the fact that the CPD for QD is deterministic toreduce this to OTDK2. This is faster than the original algorithm because we do dynamic programmingon the joint statespace of Q and QD, instead of just Q. Also, we can apply standard inference and learningprocedures to this model, without having to derive the somewhat hairy formulas by hand.A more efficient, but less flexible, way to model nongeometric waiting times is to replace each state withn new states, each with the same emission probabilities as the original state DEKM98, p69. For example,consider this model.1 2 3 41 p 1 p 1 pp p p p42Obviously the smallest sequence this can generate is of length n  4. Any path of length l through themodel has probability pln1 pn the number of possible paths isl1n1, so the total probability of a pathof length l ispl l  1n 1pln1 pnThis is the negative binomial distribution. By adjusting n and the selfloop probabilities of each state, we canmodel a wide range of waiting times.2.3.13 Mixtures of HMMsX1 X2X11 X12 X13 X21 X22 X23Y11 Y12 Y13 Y21 Y22 Y23Figure 2.23 A dynamical mixture of HMMs, aka an embedded HMM. Here we have 2 sequences, both oflength 3. Obviously we could have more sequences of different lengths, and multiple levels of the hierarchy.A static mixture of HMMs would combineX2 and X1.X1,1 X1,2 X 1,3X1X2X2,1 X2,2 X2,3Y1,1 Y1,2 Y1,3Y2,1 Y2,2 Y2,3foreheadeyesnosemouthchinendendendendenda bFigure 2.24 An embedded HMM for face recognition represented a as a DBN and b as the state transitiondiagram of an HHMM dotted arcs represent calling a subHMM. We assume there are 2 rows and 3 columnsin the image, which is clear from the DBN representation, but not from the HHMM representation. Thenumber of states in each submodel can be different, which is clear from the HHMM but not the DBN. TheDBN in a is just a rotated version of the one in Figure 2.23.Smy96 defines a mixture of HMMs, which can be used for clustering presegmented sequences. Thebasic idea is to add a hidden cluster variable as a parent to all the nodes in the HMM hence all the distributions43are conditional on the hidden class variable. A generalization of this, which I call a dynamical mixture ofHMMs, adds Markovian dynamics to the mixtureclass variable, as in Figure 2.23. This model has severalnames in the literature hierarchical mixture of Markov chains Hoe01, embedded HMM NI00, andpseudo2D HMM KA94. The reason for the term pseudo2D is because this model provides a way ofextending HMMs to 2D data such as images. The basic idea is that each row of an image is modelled bya row HMM, all of whose states are conditioned on the state of the column HMM. The overall effect is toallow dynamic warping in both the horizontal and vertical directions although the warping in each row isindependent, as noted by Mark Paskin.Nefian NI00 used embedded 2D HMMs for face recognition. In this model, the state of each rowcould be forehead, eyes, nose, mouth or chin each pixel within each row could be in one of 3 states if therow was in the forehead or chin state or in one of 6 states otherwise. The meaning of the horizontal stateswas not defined they just acted as positions within a lefttoright HMM, to allow horizontal warping of eachfeature. See Figure 2.24.The key difference between an embedded HMM and a hierarchical HMM is that, in an embedded HMM,the ending time of each subHMM is known in advance, since each horizontal HMM models exactly Cobservations, where C is the number of columns in the image. Hence the problem of deciding when to returncontrol to the unique parent state is avoided. This makes inference in embedded HMMs very easy.Embedded HMMs should not be confused with switching HMMs, shown in Figure 2.25. This is differentfrom an embedded HMM because each substate emits a single observation, not a fixedlength vector also, itis different from a 2level HHMM, because there is nothing forcing the top level to change more slowly thanthe bottom level.Q11 Q12 Q13Q21 Q22 Q23Y1 Y2 Y3Figure 2.25 A switching HMM, aka a 2level hidden Markov decision tree. This is different from a 2levelHHMM, because there is nothing forcing the top level to change more slowly than the bottom level.2.3.14 Segment modelsThe basic idea of the segment model ODK96 is that each HMM state can generate a sequence of observations instead of just a single observation. The difference from an HHMM is that the length of the sequencegenerated from state qi is explicitly determined by a random variable li, as opposed to being implicitly de44Q1 Q2 Ql2 ll1 ......... ... ......X1 X2 Xl1 Xl11 Xl1l2 Xli XTFigure 2.26 A schematic depiction of a segment model, from Bil01. The Xt nodes are observed, the restare hidden.S1 S2 S3Q1 Q2 Q3F1 F2 F3L1 L2 L3R1 R2 R3Y1 Y2 Y3Figure 2.27 A stochastic segment model.termined by when its subHHMM enters its end state. Furthermore, the total number of segments,  , is arandom variable. Hence the likelihood is given byP y1T  q1l1lii1P P qiqi1, P liqiP yt0it1iqi, liwhere and t0i i1j1 lj  1 and t1i  t0i  li  1 are the start and ending times of segment i.A first attempt to represent this as a graphical model is shown in Figure 2.26. This is not a fully specifiedgraphical model, since the Lis are random variables, and hence the topology is variable. Also, there aresome dependencies that are not shown e.g., the fact thati1 li  T , and some dependencies mightnot exist e.g., the Yis in a segment may not be fully interconnected. We will give a more accurate DBN45representation of the model below.Let us consider a particular segment and renumber so that t0  1 and t1  l. In a variablelength HMM,P y1lq, l lk1P ykqIf plq is a geometric distribution, this becomes a regular HMM. A simple generalization is to conditionon some function of the position within the segmentP y1lq, l lk1P ykq, rkwhere rk is the region corresponding to position k within the segment. This can be modelled as shownin Figure 2.27. We have added a deterministic counter, St, which counts the number of segments. Weconstrain ST   , rather like the endofsequence trick for HHMMs see Section 2.3.10. The Lt nodesis a deterministic downcounter, and turns on F when it reaches zero, as in a semiMarkov model seeSection 2.3.12.Alternatively, we can define a conditional Gaussian segment model DRO93P y1lq, l lk1P ykyk1, q, rkWe just modify Figure 2.27 by adding an arc from Yt1 to Yt however, we want this arc to disappear whenFt1  1, since that indicates the end of a segment. Hence we need to add an arc from Ft1 to Yt as well.We then defineP YtYt1, Rt  r,Qt  q, Ft1  f N ytAr,qyt1,r,q if f  0N ytr,q,r,q if f  1In general, P y1lq, l can be an arbitrary joint distribution, e.g., consider an undirected graphical modelconnecting yk to yk1 and yk1. However, representing this graphically can get messy.2.3.15 Abstract HMMsBVW00a describes the Abstract HMM AHMM, which is very closely related to HHMMs. They have3 slightly different models in BVW00a, BVW00b, BVW01 we consider the version in BVW00b. Theseauthors are interested in inferring what policy an agent is following by observing its effects in the world. Wedo not observe the world state, but we assume that the agent does.A stochastic policy is a probabilistic mapping from fully observed states to actions s, a P do action ain state s. An abstract stochastic policy is a probabilistic mapping from states to lowerlevel abstract policies, or macro actions. An abstract policy can call a subpolicy, which runs until termination, returning control to the parent policy the subpolicy can in turn call lowerlevel abstract policies, untilwe reach the bottom level of the hierarchy, where a policy can only produce concrete actions. BVW00a46F 11 F12 F13abstract  Q11 Q12 Q13F 21 F22 F23concrete  Q21 Q22 Q23action A1 A2 A3state S1 S2 S3obs Y1 Y2 Y3Figure 2.28 The DBN for a 3level abstract HMM, from BVW00b. Note that we call the top layer level 1,they call it level D. Note also that we draw F dt above Qdt , they draw it below, but the topology is the same.The lines from the global world state St are shown dotted merely to reduce clutter. The top level encodesthe probability of choosing an abstract policy P Q1t  St1  s  s, . The second level encodesthe probability of choosing a concrete policy P Q2t  St1  s,Q1t    s, . The third levelencodes the probability of choosing a concrete action P At  aSt1  s,Q2t    s, a. The lastlevel encodes the world model P St  sSt1  s, At  a.consider abstract policies of the options kind SPS99, which is equivalent to assuming that there are nohorizontal transitions within a level. HAMs PR97a generalize this by allowing horizontal transitions i.e.,internal state within a controller. This simplifies the CPDs, as we see below.This can be modelled by the DBN shown in Figure 2.28. It is obviously very similar to an HHMM, butis different in several respects, which we now discuss. There is a global world state variable St, that is a parent of everything. The bottom level represents a concrete action, which always finishes in one step hence there is noFAt  F3t node, and no At1 to At arc. There is no arc from F 2t to Q1t1, since levels cannot make horizontal transitions so F 1t turns on assoon as F 2t does. There is no arc fromQ1t to At or Yt. In general, they assume Qdt only depends on its immediate parent,Qd1t , not its whole context,Q1d1t . This substantially reduces the number of parameters and allowsfor more efficient approximate inference10. However, it may also lose some information. For example,10We can apply RaoBlackwellised particle filtering to sample the F and S nodes if each Q node has a single parent, the Q nodesform a chain, and can be integrated out exactly.47if the bottom level controller determines how to leave a room via a door, we may be able to predict thefuture location of the agent e.g., will it leave via the left or right door more accurately if we take intoaccount the complete calling context e.g., the toplevel goals. There is no arc from Q1t to F 2t . They assume the termination probability depends on the current policyonly, not on the calling context.Given these assumptions, the CPDs simplify as follows.P F dt  1Qdt  , St  s, F d1t  b 0 if b  0s if b  1where s is the probability of terminating in state s given that the current policy is .P Qdt  jQdt1  i, Qd1t  , St1  s, F dt1  f i, j if f  0s, j if f  1where s, j is the probability that abstract policy  chooses to call subpolicy j in state s.1level AHMMsG1 G2 G3FG1 FG2 FG3S1 S2 S3Y1 Y2 Y3Figure 2.29 A 1level AHMM. FGt turns on if state St statisfies goalGt this causes a new goal to be chosen.However, the new state may depend on the old state hence there is no arc from FGt1 to St to reset thesubmodel.In BVW01, they consider the specialcase of a 1level AHMM, which is shown in Figure 2.29. This hasa particularly simple interpretation Q1t  Gt represents the goal that the agent currently has, and Q2t  Stis the world state. It is similar to a variable duration HMM see Section 2.3.12, except the duration dependson how long it takes to satisfy the goal.The transition matrix for G is the same as for Q1 in an HMMM, except we never reset.P Gt  jGt1  i, FGt1  f i, j if f  0AGi, j if f  1BVW01 consider the special case where AGi, j is a deterministic successorstate function for goals.48The transition matrix for FG is the same as for FD in an HMMMP FGt  1Gt  g, St  i  ASg i, endThe transition matrix for S is the same as for QD in an HHMM, except we never reset, since FG isnot a parent since the next state always depends on the previous state, even if the goal changesP St  jGt1  g, St1  i  ASg i, j2.4 Continuousstate DBNsSo far, all the DBNs we have looked at have only had discretevalued hidden nodes. We now considerDBNs with continuousvalued hidden nodes, or mixed discretecontinuous systems sometimes called hybridsystems. In this section, we adopt the nonstandard convention of representing discrete variables as squaresand continuous variables as circles.112.4.1 Representing KFMs as DBNsThe graph structure for a KFM looks identical to that for an HMM or an IOHMM see Figures 2.1 andFigure 2.9, since it makes the same conditional independence assumptions. However, all the nodes arecontinuous, and the CPDs are all linearGaussian, i.e.,P Xt  xtXt1  xt1, Ut  u  N xtAxt1 Bu X , QP Yt  yXt  x, Ut  u  N yCxDu Y , RIn the linearGaussian case, but not the HMM case, there is a onetoone correspondence between zeros inthe parameter matrices and absent arcs in the graphical model, as we discuss in Section 2.4.2.2.4.2 Vector autoregressive VAR processesA vector autoregressive VAR processes of order L has the formXt  A1Xt1    ALXtL  twhere t  N 0,. This is equivalent to a DBN of order L where all CPDs are linearGaussian. There is aonetoone correspondence between zeros in the regression matrices and absent interslice arcs of the DBN.Specifically, we can show that DE00Xatu  Xbt iff Aub, a 6 011In an influence diagram Sha88, square nodes represent decision or action variables however, in this thesis, we will not considerinfluence diagrams. When we do have control variables, they will denoted as shaded round root nodes  shaded because we assumethey are always known, round because we are agnostic about whether they are discrete or continuous, and roots because we assume theyare exogeneous we do not model what causes them.49Figure 2.30 A VAR2 process represented as a graphical model. From DE00. The link from X 3t3 X4t1 exists because A24, 3 15  0.for 1  u  L. If the intraslice connections are modelled as undirected arcs so the overall model becomesa time series chain graph, we can also show a onetoone correspondence between zeros in the precisionconcentration matrix K  1 and absent intraslice arcs Lau96Xat Xbt iff Ka, b 6 0For example, consider the following VAR2 process with parametersA1 35 015 0 00 35 0  15 0251335 0 00 0 0  12 150 0 15 025, A2 0 0  15 0 00 0 0 0 00 0 0 0 00 0 15 0130 0 0 0  15and 1 1213 0 012 1  13 0 013  13 1 0 00 0 0 1 00 0 0 0 1, 1 2.13 1.47 1.2 0 01.47 2.13 1.2 0 01.2 1.2 1.8 0 00 0 0 1 00 0 0 0 1This is illustrated in Figure 2.30.Since sparse graphical structure is isomorphic to sparse matrices in a linearGaussian system, traditionaltechniques for representation using block matrices, inference12 and learning suffice graphical models dontprovide much added value. Hence in the rest of this section we focus on nonlinear nonGaussian and mixeddiscretecontinuous systems.A model called a causality graph DE00, Dah00, Eic01 can be derived from the timeseries chain graphby aggregatingX it for all t into a single nodeXi. The result is a graph in which nodes represent components12In a VAR process, all the variables are assumed to be observed this is the standard assumption in timeseries analysis. Hencethere is no need to do state estimation. Prediction is straightforward, since there are no future observations to condition on. The onlychallenging problem is structure learning model selection, which we discuss in Chapter 6.50of whole time series, and absence of edges correspond to what are called Granger noncausality relationships see DE00, Dah00, Eic01 for details. The causality graph can be further simplified to an undirectedcorrelation graph by a procedure akin to moralization. In the resulting graph, the edge X i  Xj is missingiff timeseriesX i andXj are uncorrelated conditioned on all the remaining timeseries. If we have nonlinearnonGaussian relationships, uncorrelated must be replaced by the stronger notion of independent which, ofcourse, is hard to measure in the nondiscrete case see BJ01 for a promising kernelbased approach in thecontext of ICA.2.4.3 Switching KFMsSX1 SX2 SX3X1 X2 X3Y1 Y2 Y3SY1 SY2 SY3Figure 2.31 A switching Kalman filter model. The dotted arcs are optional. Square nodes are discrete, roundnodes are continuous.In Figure 2.31 we show a switching KFM, where we have omitted the input U for simplicity. Thismodel has various other names, including switching linear dynamical system LDS, switching statespacemodel SSM, jumpMarkov model, jumplinear system, conditional dynamic linear model DLM, etc. Theterm jump or switch implies St is discrete, whereas conditional leaves open the possibility that St iscontinuous. Although it is traditional to have only one discrete switch variable, I have shown two, one forthe dynamics and one for the observations, since their semantics is quite different see below. The CPDs forthis model are as followsP Xt  xtXt1  xt1, SXt  i  N xtAixt1  Xi , QiP Yt  yXt  x, SYt  j  N yCjx Yj , RjP SXt  jSXt1  i  AXi, jP SYt  jSYt1  i  AY i, jSwitching dynamics is useful for modelling piecewise linear behavior one way of approximating nonlinear models, or multiple types or modes of behavior, e.g., I have succesfully used switching KFMs forvisual tracking of people as they walk, run, skip, etc. Mur98 see also the manoeuvering plane example inFigure 1.4.51Switching observations can be used to approximate nonGaussian noise models by a mixture, to modeloutliers see Section 1.2.1, or to model data association ambiguity see Section 2.4.6. Modelling outliersusing mixtures of Gaussians was discussed in Section 1.2.1. Note that if the outlier is caused by the sensorbeing broken, we might expect such outliers to persist this is modelled by the SYt1  SYt arc see Section 2.4.4. If we are just trying to approximate nonGaussian noise, it would make more sense to remove theSYt1  SYt arc.Of course, we may have many discrete variables. For example, if Xt is vector valued, it is sometimesuseful to associate one discrete variable for each component ofXt, so each can have its own piecewise lineardynamics. Similarly, if Yt is a vector, SY may be factored this is useful for fault diagnosis applications seeSection 2.4.4. For another example, consider the problem of online blind deconvolution LC95. In thisproblem, we seeyt qi0isti  twhere the mixing coefficients  are unknown, St is a hidden discrete random variable, and t is a Gaussiannoise term. For example, when q  2,stst1st2 0 0 01 0 00 1 0st1st2st3100stWe can represent this in matrix form as followst  t1xt  Hxt1 Wstyt  txt  twhere xt  sttq, t  0qt ,W  1, 0, . . . , 0, andH is the matrix shown above for the case q  2We can represent this as a DBN as shown in Figure 2.32. This is clearly a conditionally linear dynamicalsystem.Unfortunately, exact inference in switching KFMs is intractable, since the belief state at time t hasOK tmodes, where K is the number of discrete values, for reasons we explain in Section 3.6.3. We thereforeneed to use approximate inference. We discuss deterministic approximations in Section 4.3, and stochasticapproximations in Section 5.3.1.2.4.4 Fault diagnosis in hybrid systemsOne of the most important applications of hybrid systems is fault diagnosis. Consider the model in Figure 2.33, which is a benchmark problem in the fault diagnosis community MB99 typically one considersn tanks, for n  2. This is a nonlinear system, since flow  pressure  resistance or flow  pressure 52St1 StSt2 St1St3 St2y1 y1t1 tFigure 2.32 A DBN for blind deconvolution.Figure 2.33 The twotank system. The goal is to infer when pipes are blocked or have burst, or sensors havebroken, from noisy observations of the flow out of tank 1, F1o, out of tank 2, F2o, or between tanks 1 and2, F12. R1o is a hidden variable representing the resistance of the pipe out of tank 1, P1 is a hidden variablerepresenting the pressure in tank 1, etc. From Figure 11 of KL01.conductance. More problematically, the values of the resistances can slowly drift, or change discontinuouslydue to burst pipes. Also, the sensors can fail intermittently and give erroneous results. We can model all ofthis using a DBN as shown in Figure 2.34. As in any switching KFM, inference in this model is intractable,but particle filtering KL01 and other approximate inference algorithms LP01 have been used successfullyon this model.2.4.5 Combining switching KFMs with segment modelsAlthough inference in general switching KFMs is intractable, there are tractable special cases. A trivialone is where the switch nodes dont actually switch. More interesting is the model in Figure 2.35 basedon DRO93, where the switch nodes are piecewise constant they remain fixed during a segment, which ismodelled using a single KFM, but are allowed to switch at segment boundaries whose locations are of courseunknown. In addition, the model is assumed to reset once it crosses a segment boundary, i.e., the state inthe first slice of a new segment is independent of the state in the last slice of the previous segment. This53RF 11 RF12R11 R12MF 11 MF12F 11 M11 F12 M12P 11 P12MF 121 MF122F 121 M121 F122 M122R121 R122RF 121 RF122P 21 P22F 21 M21 F22 M22MF 21 MF22R21 R22RF 21 RF22Figure 2.34 The two tanks system of Figure 2.33 modelled as a DBN. Discrete nodes are squares, continuousnodes are circles. Abbreviations R  resistance, P  pressure, F  flow, M  measurement, RF  resistancefailure, MF  measurement failure. Adapted from Figure 12 of KL01.is not evident from the graph structure, but is implicit in the CPDs. Hence we can reason about segmentindependently conditioned on knowing the boundaries, by running K Kalman filters per segment we usethe forwardsbackwards algorithm to figure out boundary locations. For details, see DRO93.54Q1 Q2 Q3F1 F2 F3QD1 QD2 QD3X1 X2 X3Y1 Y2 Y3Figure 2.35 A switching KFM segment model hybrid. Square nodes are discrete, round nodes are continuous. We have omitted the segment counting control apparatus shown in Figure 2.27.X11 X12 X13X21 X22 X23S1 S2 S3Y1 Y2 Y3Figure 2.36 A DBN for the data association problem. The observation Yt is from one of two independentKFMs, as determined by the multiplexer node St. Square nodes are discrete, round nodes are continuous.2.4.6 Data associationWe now discuss a special kind of switching KFM which we will use later in the thesis. Consider a factorialHMM where all of the chains have linearGaussian CPDs except one, call it St, which is discrete and is usedto select which of the hidden chains to pass through to the output, i.e., the CPD for P YtX1Dt , St is aGaussian multiplexerP Yt  ySt  i,X1t , . . . , XDt   N yWiXit  i,iThis is shown in Figure 2.36. This is what GH98 call a switching statespace model, although only theobservations switch, not the dynamics.One important application for this model is for representing the data association correspondence problem BSF88, BS90, BSL93. In this case, one of the objects X1t , . . . , XDt , is assumed to have generatedthe observation Yt, but we just dont know which. St is a latent variable which specifies the identity of thesource of the observation Yt.The data association problem is extremely common. For example, consider tracking a single target in55clutter e.g., one missile surrounded by decoys. If St  1, we know the measurement was generated bythe missile, so we update its state estimate using Yt if St  0, we know the measurement was generatedby the background, so we ignore it. Of course, we do not know the value of St, so in principle we shouldconsider both values this is called multiple hypothesis tracking MHT BSF88, CH96. Unfortunately, thehypothesis tree at time t has 2t leaves, corresponding to all possible assigments to S1t, so some pruning isnecessary. Alternatively, we can use Monte Carlo techniques PROR99.13A simple alternative to MHT in the single target case is to see if an observation falls within the 95confidence ellipse validation gate given by CovYtXt1, which is computed using an extended Kalmanfilter if it does we assume the observation was generated by the target, otherwise we assume it was generatedby the background. If there is more than one observation inside the ellipse, we pick the nearest most likely.Of course, since we using a 95 threshold, we expect this to fail once every 20 time steps however, we cansometimes recover from incorrect associations using future observations.In the multitarget case, an observation might fall inside the validation gate confidence ellipse of severalobjects. In this case, we can either assign the observation to the nearest target using Mahalanobis distance,or compute the likelihood of all possible joint assignments of observations to targets, and pick the most likelyone CH96. Note that the nearest neighbor rule might assign the same measurement to multiple objects,which leads to inaccuracies.2.4.7 Tracking a variable, unknown number of objectsWhen we are tracking multiple targets, the measurement could have been caused by any of them, the background, or perhaps a new target that has entered the scene. This problem also arises in mobile robotics,in particular in the SLAM simultaneous localization and mapping problem, which we will discuss in Section 5.3.2. A standard way to detect the presence of new objects is if an observation arises that does notfall inside the validation gate confidence ellipse of any existing object in this case, it could either be dueto a new object, or due to background clutter, so we consider both hypotheses, and add the new object to aprovisional list. Once the object on the provisional list receives a minimum number of measurements insideits validation gate, it is added to the state space. It is also possible for an object to be removed from the statespace if it has not been updated recently e.g., it left the area of interest. Hence in general we must allow thestate space to grow and shrink dynamically. We discuss this further in Section 2.5.13Note that, if all hidden variables are discrete, including the X is, exact inference using constant time per update is always possibleunfortunately, each step takes OKD time, where D is the number of objects and K is the number of states each object can be in seeSection 3.5 Data association and tracking using discretestate DBNs is discussed in MF92, NB94.562.5 First order DBNsIn Section 2.4.7, we discussed tracking an unknown, variable number of objects. In AI, such models are calledfirst order, as opposed to propositional models, which deal with a fixed set of attributes. See Section A.5for a brief discussion of firstorder probabilistic models, and BRP01 for a discussion of firstorder MDPs.In a first order DBN, we must decide when to create a new object, and when to delete an old onenot necessarily because it ceased to exist e.g., was blown up by a missile, but maybe just because we nolonger want to reason about it, c.f., the frame problem in AI. In the tracking case, we used an heuristic theprovisional list to decide when to add an object, triggered by surprising observations. In online modelselection see Section 6.1.1, we propose new basis functions at random, and check to see if they increasedthe model fit. Obviously we could use smarter proposal distributions.In addition to creating a new object, we must decide how the new object relates to the existing ones.In the previous examples, we implicitly assumed the new object was unrelated to existing ones, i.e., hadzero crosscovariance. However, in certain applications, the relationships between objects might be moreimportant andor easier to estimate than the object properties themselves Gui02. In general, we need toestimate both object properties and their relations, e.g., for image understanding Gre93, BGP97, naturallanguage understanding GC92, etc.57Chapter 3Exact inference in DBNs3.1 IntroductionThe goal of inference in a DBN is to compute the marginals P X it y1  if   t this is filtering, if   t,this is smoothing, and if   t this is prediction. For learning, we must also be able to compute the familymarginals P PaX it, Xit y1 .In this chapter, we show to perform these inferences using a pair of abstract forwards backwards operators c.f., LBN96 and RN02, ch.17. We will then discuss increasingly efficient ways to implement theseoperators for general DBNs, culminating in a discussion of the lower bounds on complexity of exact inference in DBNs. For most of this chapter, we assume all hidden variables are discrete. We address inference inmodels with continuous or mixed discretecontinuous statespaces in Section 3.6.The main novel contributions are the island algorithm  a way of doing offline smoothing in Olog T space instead of the usual OT  space  and the interface algorithm  a simple way to implement theforwards backwards operators using the junction tree algorithm.Since inference in DBNs uses inference in BNs as a subroutine, you are strongly recommended to readAppendix B before reading this chapter.3.2 The forwardsbackwards algorithmWe will now review how to do fixedinterval smoothing in an HMM using the wellknown forwardbackwards FB algorithm. This algorithm can be applied to any discretestate DBN, by converting theDBN to an HMM. We also present a number of variations on the standard algorithm, which will prove to beuseful later in the thesis.The basic idea of the FB algorithm is to recursively compute tidefP Xt  iy1t in the forwardspass1, to recursively compute tidefP yt1T Xt  i in the backwards pass, and then to combine them to1It is more common see e.g., Rab89 to define ti  P Xt  i, y1t the difference is discussed in Section 3.2.1.58produce the final answer tidefP Xt  iy1T P Xt  iy1T  1P y1T P yt1T Xt  i, y1tP Xt  i, y1t1P y1T P yt1T Xt  iP Xt  iy1tort  t.  twhere . denotes elementwise product, i.e., ti  titi.3.2.1 The forwards passWe compute  recursively as follows.tjP Xt  jy1t 1ctP Xt  j, yty1t1whereP Xt  j, yty1t1 iP Xt  jXt1  iP Xt1  iy1t1P ytXt  j 3.1andct  P yty1t1 jP Xt  j, yty1t1 3.2In vectormatrix notation, this becomest  OtAt1 3.3where A denotes the transpose of A and Oti, idefP ytXt  i is a diagonal matrix containing the conditional likelihood of the evidence at time t. This is the only way in which the evidence affects the algorithm.Hence we can use any model for P YtXt  i.The base case is1j  P X1  jy1 1c1P X1  jP Y1X1  jor1  O1If we did not normalize at each step, then we would be computing ti  P Xt  i, y1t. However,this joint probability will become very small for large t, and hence this quantity will rapidly underflow. Onesolution is to work in the logdomain. An alternative is to normalize, i.e., to computeti  P Xt  iy1t,which always sums to one. Not only does this prevent underflow, but it is also a much more meaningfulquantity a filtered state estimate. We keep track of the normalizing constants so that we can compute thelikelihood of the sequenceP y1T   P y1P y2y1P y3y12 . . . P yT y1T1 Tt1ct593.2.2 The backwards passWe compute the s recursively as follows. The base case isT i  1since PryT1T XT  i  PrXT  i  1. The recursive step isP yt1T Xt  i jP yt2T , Xt1  j, yt1Xt  ijP yt2T Xt1  j, yt1, Xt  iP Xt1  j, yt1Xt  ijP yt2T Xt1  jP yt1Xt1  jP Xt1  jXt  iort  AOt1t1Note that the backwards algorithm requires the evidence stream in the form of Ot, but can be run independently of the forwards algorithm. This is sometimes called the two filter approach to smoothing. However,t  P yt1T Xt is a conditional likelihood, not a filtered estimate see Section 3.2.5 for a propertwofilter smoothing algorithm.Since t  P yt1T Xt is a conditional likelihood, it need not sum to one. To prevent underflow, wenormalize at every step. The normalization constant is arbitrary, since it will cancel when we compute tP Xty1T   P Xt, y1tP yt1T Xt  Cttt  Cttdttwhere Ct ti1 ct  P y1t and dt is any constant. Rabiner Rab89 chooses dt  ct, but in BNT2, I usedt i ti, which is computable without reference to .3.2.3 An alternative backwards passThe fact that t is not a true probability distribution, besides being unintuitive, presents problems when weconsider Kalman filter models.3 Hence we now derive a way of computing t recursively without first havingto first compute t. I call this the  algorithm instead of the  algorithm. This derivation turns out to beequivalent to the junction tree algorithm see Section B.4 applied to this model SHJ97.The  algorithm exploits the fact that Xt1  yt1T Xt as follows.t1i jt1,tT i, j2Specifically, in the function BNTHMMforwardsbackwards.m.3In KFMs, t can be represented in moment form i.e., in terms of EXty1t and CovXty1t, or in canonical informationform see Section B.5.1. However, t must be represented in canonical form, since it is not necessarily normalizable. Unfortunately,even in this form, the required inverses may not exist. Hence in the backwards smoothing pass, it is better to compute t directly,which can always be represented in moment form, since it is a probability distribution. This is also essential for switching KFMs,since the standard momentmatching weak marginalisation approximation is only applicable if the message is in moment form seeSection B.5.2.60wheret1,tT i, jdef P Xt1  i,Xt  jy1T  P Xt1  iXt  j, y1tP Xt  jy1T  P Xt1  i,Xt  jy1tP Xt  jy1T P Xt  jy1t t1,tti, jtjtjandt1,tti, jdef P Xt1  i,Xt  jy1t P ytXt  jP Xt  jXt1  iP Xt1  iy1t1 Otj, jAi, jt1iWe can think of the ratio P Xtjy1T P Xtjy1t as an update factor, or equivalently, as the backwards message,sinceP Xt  jy1T P Xt  jy1tP Xt  j, y1T P Xt  jy1tP y1T P Xt  jy1tP yt1T Xt  jP Xt  jy1tP y1T  tj.This update factor is absorbed into the twoslice distribution P Xt1, Xty1t, which is then marginalizeddown to yield P Xt1y1T . This is the standard way of performing inference in the junction tree algorithmsee Section B.4.4.3.2.4 Twoslice distributionsWhen we do parameter estimation see Section 1.2.3, we will also need to compute the twoslice distributiont1,tT i, jdefP Xt1  i,Xt  jy1T . If we use the  junction tree algorithm, the twoslicedistribution has already been computed. If we use the  algorithm, this can be computed usingP Xt1  i,Xt  jy1T   P Xt  jXt1  iP Xt1  iy1t1P ytXt  jP yt1T Xt  jort1,tT  A.  t1Ottwhere the normalizing constant ensuresi,j t1,tT i, j  1.3.2.5 A twofilter approach to smoothingWe have already remarked that t  P yt1T Xt is a conditional likelihood it would seem more natural touse the backwards filtered estimate tdefP Xtyt1T  we start at t  1 so we dont doublecount yt when61we combine with t. We now derive a novel recursive backwards update for t. An analogous result forKFMs can be found in FP69.First we define the reverse transition matrix as followsArj, idefP Xt  iXt1  j P Xt1  jXt  iP Xt  iP Xt1  jLet ti, idefP Xt  i be a diagonal matrix. Then we can writeArj, i Ai, jti, itj, jThis is only defined if tj, j  0 for all j and t. This is not true for a lefttoright transition matrix, forexample.Assuming Ar exists, we can derive the backwards filter as follows.P Xt  iyt1T   P Xt  i, yt1yt2T jP Xt  iyt1, Xt1  j, yt2T P yt1, Xt1  jyt2T jP Xt  iXt1  jP yt1Xt1  jP Xt1  jyt2T or, more concisely,t  ArOt1t1where the constant of proportionality is 1P yt1yt2T . This has an appealing symmetry with the forwardsalgorithm, but is only applicable if Ar exists.Finally, can combine the two filters as follows.P Xt  iy1T   P Xt  iy1tP yt1T Xt  i P Xt  iy1tP yt1T , Xt  iP Xt  i P Xt  iy1tP Xt  iyt1T P Xt  iorti tititi, i3.2.6 Time and space complexity of forwardsbackwardsIf X can be in K possible states, filtering takes OK2 operations per time step, since we must do a matrixvector multiply at every step. Smoothing therefore takes OK2T  time. If A is sparse, and each state has atmost Fin predecessors, then the complexity is KFinT , e.g., for a lefttoright HMM, Fin  2 predecessorstate and selfarc, so the complexity is OKT . Smoothing also needs OKT  space, since we must store62t for t  1, . . . , T until we do the backwards pass. For complex models, in which K can be very large,and long sequences, in which T is large, we often find that we run out of space. Section 3.7.1 discusses away to reduce the space complexity to OK logT , while still performing exact inference at the expense ofa slowdown by a Olog T  factor.3.2.7 Abstract forwards and backwards operatorsWe have now seen several different ways of performing filtering and smoothing in HMMs, depending onwhether we use , , etc. To hide these details from higherlevel inference algorithms, we define abstractforwards and backwards operators.The forward operator is denoted byftt, Lt  Fwdft1t1, ytwhere ftt is the forwards message4, and Lt  logP yty1t1 is the conditional loglikelihood of theobservation this is a measure of surprise the innovation probability. The backward operator is denotedbybtT  Backbt1T , fttwhere btT is the backwards message. The base cases will be denoted by f11, L1  Fwd1y1 andbT T  backTfT T .The meaning of the forwards and backwards messages depends on which algorithm we use. In generalall we can say is that btT is some object in the programming language sense from which we can computeP Zit y1T  and P Zit , PaZity1T  for any node Zit and its parents.3.3 The frontier algorithmThe forwardsbackwards algorithm for HMMs works by exploiting the fact that Xt dseparates the past fromthe future. For a DBN, the set of all hidden nodes, X 1Dt , dseparates the past from the future. The frontieralgorithm Zwe96 is a way of updating the joint distribution on this set of nodes without needing to create, yetalone multiply by, an OKD KD transition matrix. The basic idea is to sweep a Markov blanket acrossthe DBN, first forwards and then backwards. We shall call the nodes in the Markov blanket the frontier set,and denote it by F  the nodes to the left and right of the frontier will be denoted by L and R. At every stepof the algorithm, we ensure F dseparates L and R. The forwards pass corresponds to the computation of ,and the backwards pass to the computation of . We give the details below.54This notation is based on the convention from Kalman filtering, where EXty1  is denoted by x .5A special case of the frontier algorithm, applied to factorial HMMs, was published in Appendix B of GJ97. Since there are nocross links between the hidden nodes in an FHMM, there are no constraints on the order in which nodes are added to or removed fromthe frontier, so the resulting algorithm is much simpler than the general case presented here. The frontier algorithm is itself a specialcase of the junction tree algorithm. We describe it here because it is simple to understand, and because it is the inference algorithm usedin GMTK BZ02.633.3.1 Forwards passWe use the original notation from Zwe96, so hF refers to the hidden nodes in F , eF refers to the evidence in F , eL refers to the evidence in L, and eR refers to the evidence in R. In the forwards pass,P F defP hF , eF , eL in practice, we normalize to get P hF eF , eL. We can compute this recursively asfollows. We can add a node N to the frontier move it from R to F  when all its parents are already in thefrontierP eL, eF , hF , N  P eL, eF , hF P N eF , hF since NeLeF , hF . P eL, eF , hF  is available by inductive assumption. In other words, adding a nodeconsists of multiplying its CPD onto the frontier.We can remove a node N move it from F to L when all its children are in the frontier. This can bedone as follows. Let LN representLN, and F N represent F  N. Consider first the case that Nis hidden, so eLN  eL and eFN  eF .P eLN , eFN , hFN   P eL, eF , hFN NP eL, eF , N, hFNNP eL, eF , hF where the last line follows since hFNN  hF . In other words, removing a node simply meansmarginalizing it out. If we replace sum with max, we get what Zwe98 calls the chain decoding algorithm. The case where N is observed is similarP eLN , eFN , hFN  P eLN , eFN , hF  P eL, eN , eFN , hF  P eL, eF , hF We can skip the marginalization since N is observed hence this is essentially a noop. Note that this procedure is equivalent to the variable elimination algorithm see Section B.2 with a specific ordering.3.3.2 Backwards passIn the backwards pass, P F defP eRhF , eF . We can advance the frontier from slice t  1 to slice t byadding and removing nodes in the opposite order that we used in the forwards pass, e.g., suppose in theforward pass we add X1t , remove X1t1, add X2t , remove X2t1 then in the backwards pass we would addX2t1, remove X2t , add X1t1, remove X1t . Adding a node means moving it from L to F  removing a nodemeans moving it from F to R hence R is the set of nodes that have been processed, the opposite of theforwards pass.64When we add node N to F , we want to compute P eReF , hF , N. Because N was removed at thisstep in the forwards pass, we know that all N s children are in F , which shield N from eR henceP eReF , hF , N  P eReF , hF . So adding N simply means expanding the domain of the frontier tocontain it, by duplicating all the existing entries, once for each possible value of N .When we remove node N from F and add it to R, we want to compute P eRN eFN , hFN fromP eReF , hF . Consider first the case that N is a hidden node, so eRN  eR, and eFN  eF . Then wehaveP eRN eFN , hFN  P eReF , hFN NP N, eReF , hFN NP N eF , hFNP eRN, eF , hFN NP N eF , hFNP eReF , hF since hFNN  hF . In other words, to remove nodeN , we multiply in N s CPD this is possible sinceall of N s parents will be in F , since N was added at this step in the forwards pass and then marginalize outN .If N is observed, the procedure is basically the same, except we dont need to marginalize out N , sinceit only has one possible value.P eRN eFN , hFN   P eRN eFN , hF  P eN , eReFN , hF  P eN eFN , hF P eReN , eFN , hF 3.3.3 ExampleWe now give an example of the forwards pass applied to a coupled HMM refer to Figure 3.1. The frontierinitially contains all the nodes in slice t  1 Ft,0deft1  P X1Dt1 y1t1. We then advance the frontierby movingX1t from R to F . To do this, we multiply in its CPD P X1t X1t1, X2t1Ft,1  P X1t , X1Dt1 y1t1  P X1t X1t1, X2t1 Ft,0Next we add in X2t Ft,2  P X12t , X1Dt1 y1t1 P X2t X1t1, X2t1, X3t1 Ft,165X11X22X12. . . .add X1t add X2t remove X1t1 remove X2t1add X3tX32X21Figure 3.1 The frontier algorithm applied to a coupled HMM with 5 chains see Figure 2.11 observedleaves i.e., y1T  are omitted for clarity. Nodes inside the box are in the frontier. The node being operated onis shown shaded only connections with its parents and children are shown other arcs are omitted for clarity.See text for details.Now all of the nodes that depend on X1t1 are in the frontier, so we can marginalize X1t1 out move it fromF to LFt,3  P X12t , X2Dt1 y1t1 X1t1Ft,2The process continues in this way until we computeFt,D  P X1Dt y1t1Finally, we weight this factor by the likelihood to getFt1,0  t  P X1Dt y1t  P ytX1Dt  Ft,D Di1P yitX it Ft,D3.3.4 Complexity of the frontier algorithmIt is clear that in the above example, exact smoothing takes OTDKD2 time and space, since the frontiernever contains more thanD2 nodes, and it takesOD steps to sweep the frontier from t1 to t. In general,the running time of the frontier algorithm is exponential in the size of the largest frontier. We would thereforelike to keep the frontiers as small as possible. Unfortunately, computing an order in which to add and removenodes so as to minimize the sum of the frontier sizes is equivalent to finding an optimal elimination ordering,which is known to be NPhard Arn85. Nevertheless, heuristics methods, such as greedy search Kja90,often perform as well as exhaustive search using branch and bound Zwe96. See Section B.3.5 for more onelimination orderings.In the best case, which occurs when all the chains are disconnected, as in a factorial HMM, the frontieralgorithm does OD multiplications by OK K matrices to advance the frontier from slice t 1 to slicet and similarly in the backwards direction. Each of theseKK matrices must be multiplied onto the joint,66Figure 3.2 A worst case DBN from a complexity point of view, since we must create a clique which containsall nodes in both slices.which has size OKD. Hence the total amount of work, in the best case, is ODKD1 per time step for aregular HMM D  1, this becomes OK2, as expected. In the worst case, which occurs for a DBN likethe one in Figure 3.2, the total amount of work is OK2D, as in the FB algorithm. To see this, note thatwe must add all the nodes in slice t, while keeping all the nodes in slice t  1 in the frontier, before we canremove any nodes this is because the bottom node depends on all nodes in slice t and all nodes in slice t 1,i.e., the corresponding moral graph is a clique containing both slices. Adding the nodes in slice t one by oneto the frontier takesDi1KDi  KD KD11K1  OK2D time, since the frontier grows from size KDto K2D marginalizing them out also takes OK2D time. In general, the algorithm will always take at leastOTKDFin1, where Fin is the maximal number of parents of any node within the same slice.3.4 The interface algorithmThe frontier algorithm uses all the hidden nodes in a slice to dseparate the past from the future. This set islarger than it needs to be, and hence the algorithm is suboptimal see Table 3.1. I claim that the set of nodeswith outgoing arcs to the next timeslice is sufficient to dseparate the past from the future following Dar01,I will call this set the forward interface. For example, the forward interface for Figure 3.4 is X 1t , X4t . Inow state and prove this result formally.Definition. Let the set of temporal arcs between slices t 1 and t be denoted by E tmpt  u, v Eu  Vt1, v  Vt, where Vt are the nodes in slice t. The forward interface is defined as Itdef u Vtu, v  Etmpt  1, v  Vt1, i.e., the nodes which have children in the next slice. The set ofnoninterface nodes is Nt  Vt  It.Lemma. V1t1, Nt  Vt1T It, i.e., the forward interface dseparates the past from the future, wherethe past is Nt and earlier nodes, and the future is Vt1 and later nodes.Proof. Let I be a node in the interface, connected to a node P in the past and a node F in the future which67FungiMildewLAIPrecipMicroTempPhotoSolarDryFungiMildewLAIPrecipMicroTempPhotoSolarDry123456789Figure 3.3 The Mildew DBN, designed for foreacasting the gross yield of wheat based on climatic data,observations of leaf area index LAI and extension of mildew, and knowledge of amount of fungicides usedand time of usage Kja95. Nodes are numbered in topological order, as required by BNT.DBN Figure Slicesize Frontier Back FwdBAT 4.2 28 18 9 10Water 4.1 12 8 8 8Mildew 3.3 9 9 4 5Cascade 3.5 4 4 4 1Uffe 3.4 4 4 3 2Table 3.1 Sizes of various separating sets for different DBNs. The slicesize is the total number of nodes perslice. The frontier is the total number of hidden nodes per slice. Back is the size of the backwards interface,i.e., the number of nodes with incoming temporal arcs, plus parents of such nodes in the same slice see textfor precise definition. Fwd is the size of the forward interface, i.e., the number of nodes with children in thenext slice.must be a child of I , by definition. If P is a parent, the graph looks like this P  I  F . If P is a child,the graph looks like this P  I  F . Either way, we have P  F I , since I is never at the bottom ofa vstructure. Since all paths between any node in the past and any node in the future are blocked by somenode in the interface, the result follows. Kjaerulff Kja95 defines a related quantity, which he called the interface, but which I will call thebackward interface, to avoid confusion with the forward interface. He defines the backward interface tobe all nodes v s.t. v, or one of its children, has a parent in slice t  1, i.e., intt  v  Vtu, v Etmpt or w  chv  u,w  Etmpt, u  Vt1. The reason for this definition is the followingwhen we eliminate a node from slice t 1, we will create a term which involves all nodes which depend onit, including those in slice t some of the slice t terms will involve parents in slice t as well. For example,68X11 X21 X12 X22X31 X32X41 X42Figure 3.4 The Uffe DBN, from Figure 3 of Kja95. Nodes are numbered topologically, as required byBNT. The dotted undirected arcs are moralization arcs. The backward interface is shaded.Figure 3.5 The cascade DBN, from Figure 2 of Dar01. This graph has a treewidth of 2.consider eliminating the nodes from slice 1 of Figure 3.4. We haveX31P X41 X31 X21X11P X11 P X21 X11 P X31 X11 , X21 P X12 X11   X31 ,X12   X12 ,X41 andX41P X42 X32 , X41 X12 , X41   X12 ,X42 ,X32 Clearly we have coupled all the nodes in the backward interface, since the backward interface for Figure 3.4is X1t , X3t , X4t .The forward interface can sometimes be dramatically smaller than the backward interface see Table 3.1.For an extreme example, consider Figure 3.5 The forward interface is 1 but the backward interface is allthe nodes in the slice. It is easy to see that the size of the forward interface is never larger than the size of thebackward interface if all temporal arcs are persistence arcs, i.e., edges of the form X it1 to Xit .The other problem with the backward interface is that using it does not lead to a simple online inference69C2D2 D3 C3I1I2222 33J J J1 32C1 D11 1I I I I IN NN1Figure 3.6 A schematic illustration of how to join the junction trees for each 1 12 slice DBN. It are theinterface nodes for slice t, Nt are the noninterface nodes. Dt is the clique in Jt containing It1. Ct is theclique in Jt containing It. The square box represents a separator, whose domain is It.X1 X1 X2 X2 X3S3Y3S2Y1S1X1 X2Y2Figure 3.7 The DAGs for an HMM with mixture of Gaussians output, glued together by their interface nodes,Xt. The noninterface is Nt  St, Yt.algorithm the one in Kja95 involves a complicated procedure for dynamically modifying jtrees. Below Ipresent a much simpler algorithm, which always uses the same jtree structure, constructed from a modifiedtwoslice temporal Bayes net 2TBN using an unconstrained elimination ordering, but with the restrictionthat the nodes in the forward interface must belong to one clique.3.4.1 Constructing the junction treeLet Gt be the DAG created from slices t  1 and t from the unrolled DBN. A 1 12 slice DBN Ht H forhalf is the DAG created by eliminating all noninterface nodes and their arcs from the first slice of Gt, i.e.,Ht  It1Vt. For t  1, H1  V1.We now construct a jtree, Jt, for each Ht. See Section B.3 for details on how to construct a jtree. Wemust enforce the constraint that It1 and It each form a clique, since we need P It1 and P It. This canbe ensured by simply adding edges to the moral graph between all nodes in It1, and similarly for It, beforeconstrucing Jt.We can now glue all the junction trees together via their interfaces, as shown in Figures 3.6 and 3.7.We can perform inference in each tree separately, and then pass messages between them via the interfacenodes, first forwards and then backwards. We give the details below.703.4.2 Forwards passIn the forwards pass, we are given a prior belief state P It1y1t1, which is passed from Ct1 to Dt,where Ct1 is the clique in Jt1 containing It1 and Dt is the clique in Jt containing It1. We then callcollectevidence on Jt with Ct as the root, which has the effect of doing one step of Bayesian updating.Finally we marginalize down the distribution over Ct onto It to compute P Ity1t, and pass this to the nextslice. In more detail, the steps are as follows.1. Construct Jt, where all clique and separator potentials are initialized to the identity element 1s in thecase of discrete potentials.2. From Jt1, extract the potential on Ct1, and marginalize it down to It1 this represents the prior,P It1y1t1. Multiply the prior onto the potential for Dt.3. Multiply the CPDs for each node in slice t onto the appropriate potential in Jt, using yt where necessary.4. Collect evidence to the root Ct.5. Return all clique and separator potentials in Jt.We will denote this operator abstractly asftt, Lt  Fwdft1t1, ytwhere ftt contains the clique and separator potentials in Jt. As with HMMs, to prevent underflow, we mustnormalize all the messages or else work in the log domain. The normalization constant at the root, Ct, willbe ct  P yty1t, which can be used to compute the loglikelihood,Lt.For the first slice, we skip the step involving the prior step 2. This will be denoted byf11, L1  Fwd1y1After collecting to Ct, not all nodes cliques in Jt will have seen all the evidence y1t. For example,in Figure 3.7, if we collect to X2, then the distribution on S2 will be P S2y2 rather than the full filtereddistribution, P S2y12, since S2 will only have received a message from Y2 below, and not from X2 above.Hence we must additionally perform a distributeevidence operation fromCt to compute the distribution overall nodes in Jt this will be performed in the backwards pass. Hence even when filtering, we must perform abackwards pass, but only within a single slice.713.4.3 Backwards passIn the backwards pass, we distribute evidence fromCt, and then pass a message fromDt to Ct1. The detailsare as follows.1. The input is ftt, which contains the clique and separator potentials over all nodes in Jt, and bt1T ,which contains the clique and separator potentials over all nodes in Jt1.2. From bt1T , extract the potential on Dt1, and marginalize it down to It this represents P Ity1T .3. Update the potential on Ct in Jt by absorbing from the potential on Dt1 in Jt1 as followsC  C DC DCD Cwhere C  Ct and D  Dt1.64. Distribute evidence from the root Ct.5. Return all clique and optionally separator potentials.We will denote this operator bybtT  Backbt1T , fttTo start the process at the final slice, we just distribute evidence from CT no need to absorb from JT1,which does not exist.bT T  BackTfT T 3.4.4 Complexity of the interface algorithmThe complexity of the interface algorithm can be characterized as follows.Theorem. The complexity of the interface algorithm is between KI1 and OKID, where I isthe size of the forwards interface,D is the number of hidden nodes per slice, and K is the maximum numberof values each discrete hidden node can take.Proof.7 When we create the 1 12 slice jtree, the nodes are It1Vt. Let w be the size of the maximum cliquecreated by eliminating this set according to some ordering . Clearly I  1  w, all the nodes in It1 havea target in Vt, and w  I D, since each node in It1 can be connected to at most D nodes in Vt. We nowshow these bounds are tight. The lower bound can be achieved by a Markov chain, where I  1, so w  2the cliques correspond to Xt1, Xt. The upper bound can be achieved by the DBN shown in Figure 3.2.6In the forwards pass, we implicitely computed D  D CD CDC Dwhere C  Ct1 and D  Dt. However, since Dt isinitially all 1s, we simplified this to D CD C  P It1y1t1. In otherwords, we simply set the potential on Dt to beP It1y1t1, with a suitably extended domain.7This proof is based on similar ones in Dar01.72X11 X12 X13 X14X21 X22 X23 X24X31 X32 X33 X34X41 X42 X43 X44Figure 3.8 A DBN in which all the nodes in the last slice become connected when we eliminate the first 3 ormore slices, even though the max clique size is 2.3.5 Computational complexity of exact inference in DBNsWe proved above that the interface algorithm always takes at least KI1 time, where I is the size of theforwards interface. But this is a lower bound on the algorithm, not on the problem itself. To get a lowerbound on the problem, we need to distinguish offline and online inference.3.5.1 Offline inferenceThe simplest way to do exact inference in a DBN is to unroll the DBN for T slices and then apply any inference algorithm to the resulting static Bayes net. As discussed in Appendix B, the cost of this is determinedby the tree widthTheorem. Consider a 2TBN G with D nodes per slice, each of which has a maximum of K possible discrete values. Let GT  UT G be this DBN unrolled for T slices. Then the complexity ofany offline inference algorithm is at least Kw, where w is the treewidth, i.e., the max clique size oftriangulatemoralizeGT  using an optimal elimination ordering.In general, w  I , the size of the forwards interface, but this is not always the case. For example, inFigure 3.8, I  4 but w  2, since the graph is a tree.3.5.2 Constrained elimination orderingsWhen doing inference with sequences that can have variable lengths, it becomes too expensive to repeatedlyunroll the DBN and convert it to a jtree. The approach taken in Zwe98 is to unroll the DBN once, to somemaximum length Tmax, construct a corresponding jtree, and then to splice out redundant cliques from thejtree when doing inference on a shorter sequence.To ensure there is some repetitive structure to the junction tree which can be spliced out, we must use a73constrained elimination ordering, in which we eliminate all nodes from slice t before any from slice t  1.The temporal constraint ensures that we create vertical cliques, which only contain nodes from neighboringtimeslices, instead of horizontal cliques, which can span many timeslices. The resulting jtree is said to beconstrainedly triangulated.For example, consider the DBN in Figure 3.3. Using the constrained minfill heuristic, as implementedin BNT, resulted in the following elimination ordering 7, 1, 2, 4, 5, 6, 7, 3, 8. The corresponding jtree, for4 slices, is shown in Figure 3.9. The cliques themselves are shown in Figure 3.11. Notice how the jtreeessentially has a head, a repeating body, and then a tail the head and tail are due to the boundaries on the leftand right. This is shown schematically in Figure 3.10. Zweig Zwe98 shows how to identify the repeatingstructure by looking for isomorphic cliques this can then be spliced out, to create a jtree suitable foroffline inference on shorter sequences. By contrast, Figure 3.12 shows the cliques which result from anunconstrained elimination ordering, for which there is no repeating pattern.Figure 3.9 The jtree for 4 slices of the Mildew DBN Figure 3.3 created using a constrained eliminationordering. Notice how the backward interface cliques 9, 17 and 25 are separators. Node 28 in slice 4 is notconnected to the rest of the DBN. The corresponding clique 26 is arbitrarily connected to clique 1 nodes3,5,7,8 in slice 1 to make the jtree a tree instead of a forest.Figure 3.10 A schematic illustration of a generic jtree for a DBN. The diamonds represent the head and tailcliques, the boxes represent the body cliques, that get repeated. The dotted arc illustrates how some slicescan be spliced out to create a shorter jtree. Based on Figure 3.10 of Zwe98.3.5.3 Consequences of using constrained elimination orderingsUsing a constrained elimination ordering for offline inference is suboptimal. For example, Figure 3.8 showsa DBN where the treewidth is 2 since the graph is essentially a tree, but a constrained elimination ordering741 2 3 4 5 6 7 8 9 10111213141516171819202122232425262728293031323334353612345678910111213141516171819202122232425262728Figure 3.11 A representation of the cliques in the constrained Mildew jtree Figure 3.9. Each row representsa clique, each column represents a DBN variable. The repetitive structure in the middle of the jtree is evident.For example, rows 17 and 25 are isomorphic, because their bit pattern is the same 101100011, and isexactly 9 columns apart. The bit pattern refers to the 0s and 1s, which is a way of representing which DBNnodes belong to each clique set.1 2 3 4 5 6 7 8 9 1011121314151617181920212223242526272829303132333435361234567891011121314151617181920212223242526272829Figure 3.12 This is like Figure 3.11, but using an unconstrained elimination ordering. Notice how somecliques span more than two consecutive time slices, e.g., cliques 22 is 18, 26, 27, 30.creates cliques of size D  4 for slices after t  4.8 This is a consequence of the following theorem.Theorem Constrained elimination ordering RTL76. Let A1, . . . , An be an elimination sequence triangulating the moral graph G, and let Ai, Aj be two nonneighbors in G, i  j. Then the eliminationsequence introduces the fillinAiAj iff there is a pathAiX1 . . .Aj such that all intermediate nodesXk are eliminated before Ai.For example, in Figure 3.8, for any t  4, every node in slice t is connected to every other node in slice t8You can verify this and other facts experimentally using the following BNT script BNTexamplesdynamicjtreeclqtest.m.75Figure 3.13 A coupled HMM with 4 chains. Even though chain 1 is not directly connected to chain 4, theybecome correlated once we unroll the DBN, as indicated by the dotted line.via some undirected path through the past hence all the nodes in slice t become connected in one big cliquein the triangulated graph.Another example is the coupled HMM in Figure 3.13. Even though chain 1 is not directly connectedto chain 4, they become correlated once we unroll the DBN. Indeed, the unrolled DBN looks rather like agridstructured Markov Random Field, for which exact inference is known to be intractable. More precisely,the tree width of an N  n n 2D grid with nearestneighbor 4 or 8 connectivity is On RS91.Although a constrained elimination order cannot yield smaller cliques than an unconstrained one, inmy experience the constraint nearly always helps the minfill heuristic find much better elimination orderssimilar findings are reported in Dar01. Recall that finding the optimal elimination order is NPhard.However, Bilmes reports personal communication that by using the unconstrained minfill heuristic withmultiple restarts, the best such ordering tends to be one that eliminates nodes which are not temporally farapart, i.e., it is similar to imposing a constraint that we eliminate all nodes within a small temporal window,but without having to specify the width of the window ahead of time since the optimal window might spanseveral slices. The cost of finding this optimal elimination ordering can be amortized over all inference runs.3.5.4 Online inferenceFor an online algorithm to use constant space and time per iteration, there must be some finite time t at whichit eliminates all earlier nodes. Hence online inference must used constrained elimination orderings.The above suggests that perhaps online inference always takes at least K I1 time. However, this isfalse Figure 3.14 provides a counterexample. In this case, the largest clique in both the constrained andunconstrained jtree is the triple X3t1, X4t1, X4t , because there is no path through the past Xs whichconnects all the nodes in the last slice. So in this case inference is cheaper than OK I .76X11 X12 X13 X14X21 X22 X23 X24X31 X32 X33 X34X41 X42 X43 X44Figure 3.14 A DBN in which the nodes in the last slice do not become connected when we eliminate the first3 or more slices. Dotted lines represent moralization arcs.1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16123456789101112131415161 2 3 4 5 6 7 8 9 10 11 12 13 14 15 1612345678910111213141516a bFigure 3.15 The adjacency matrices for the closure of the moralized unrolled graphs for two DBNs. a TheDBN in Figure 3.8 the bottom right 4  4 block is all 1s, indicating that the interface is all the nodes. bThe DBN in Figure 3.14 the bottom right 4 4 block is sparse, indicating that the interface is 1, 2, and3, 4.The interface in Figure 3.14 has size 4, even though it could be represented in factored form without lossof accuracy. We can identify this sort of situation as follows unroll the DAG for D slices, where D is thenumber of nodes per slice moralize the unrolled graph find the transitive closure of the moral graph extractthe subgraph corresponding to the last slice finally, find the strongly connected components of the subgraph,and call them C1, . . . , CC  these are the factors in the interface. See Figure 3.15 for some examples.We will denote this sequence of operations asC1, . . . , CC  cc clmUDGVDwhere ccG means returns the connected components of G, GVD means the subgraph of G containingnodes in slice D, clG means the transitive closure of G, mG means the moral graph of G, and UDGmeans unroll the 2TBN G for D slices. We summarize our results as follows.77Conjecture. Consider a 2TBN G with D nodes per slice, each of which has a maximum of K possiblediscrete values. Let C1, . . . , CC  cc clmUDGVD be the connected components, as describedabove. Let m  maxCi1 Ci be the size of the largest interface factor, and let Fin be the maximal number ofparents of any node within the same slice. Then the complexity of any online inference algorithm is at leastKmFin1.We can achieve this lower bound using what I call the factored interface algorithm this is a simplemodification to the interface algorithm, which maintains the distributions over the Cis in factored form.3.5.5 Conditionally tractable substructureThe pessimistic result in the previous section is purely graphtheoretic, and hence is a worstcase distributionfree result. It sometimes happens that the CPDs encode conditional independencies that are not evident inthe graph structure. This can lead to significant speedups. For example, consider Figure 3.16. This is aschematic for two processes here represented by single nodes,B and C, both of which only interact witheach other via an interface layer, R.The forward interface is R,B,C since B and C represent whole subprocesses, this might be quitelarge. Now suppose we remove the dotted arcs, so R becomes a root, which influences B and C. Again,the forward interface is R,B,C. Finally, suppose that R is in fact a static node, i.e., P RtRt1 Rt, Rt1. For example, R might be a fixed parameter. In this case, the model can be simplified asshown in Figure 3.17. This model enjoys the property that, conditioned onR, the forward interface factorizesTDW02P R,Bt, Cty1t  P BtR, y1tP CtR, y1tP Ry1t  P BtR, yB1tP CtR, yC1tP Ry1twhere yt  yBt , yCt . This follows since BtyC1tR and CtyB1tR, i.e., evidence which is local to aprocess does not influence other processes the R node acts like a barrier. Hence we can recursively updateeach subprocess separatelyP BtR, yB1t  P BtR, yB1t1, yBt  P yBt BtbP BtBt1  b, RP Bt1  bR, yB1t1The distribution over the root variable can be computed at any time usingP Ry1t bP RP Bt  bR, y1tThe reason we cannot apply the same factoring trick to the model in Figure 3.16 is that P BtRt, y1t 6P BtRt, yB1t, since there is a path e.g., via Rt1 connecting Y Ct to Bt. If we could condition on the78Y C1 YC2 YC3C1 C2 C3R1 R2 R3B1 B2 B3Y B1 YB2 YB3Figure 3.16 Two processes, here represented by single nodes, B and C, only interact with each other viaan interface layer, R.Y C1 YC2 YC3C1 C2 C3RB1 B2 B3Y B1 YB2 YB3Figure 3.17 Conditioned on the static root, the interface is fully factored.whole chain R1t instead of just on Rt, we could factor the problem. Of course, we cannot condition on allpossible values of R1t, since there are Kt of them however, we can sample representative instantiations.Given these samples, we can update the processes exactly. See Section 5.3.3 for an application of this idea tothe SLAM problem.We discuss how to exploit other kinds of parametric nongraphical conditional independence forexact inference in Section B.6.2. In general, the nodes in the interface will not be conditionally independent,even if we exploit parametric properties in the CPDs. However, they may be only weakly correlated. This isthe basis of the approximation algorithms we discuss in the next two chapters.793.6 Continuous state spaces3.6.1 Inference in KFMsThe equations for Kalman filtering smoothing can be derived in an analogous manner to the equations forHMMs, except the algebra is somewhat heavier. Please see e.g., BSL93, Min99, Jor02 for derivations. Herewe just state the final results without proof.Forwards passLet us denote the mean and covariance of the belief state P Xty1t by xtt, Vtt. The forward operator,xtt, Vtt, Lt  Fwdxt1t1, Vt1t1, ytAt, Ct, Qt, Rtis defined as follows.9 First, we compute the predicted mean and variance.xtt1  Axt1t1Vtt1  AVt1t1A QThen we compute the error in our prediction the innovation, the variance of the error, the Kalman gainmatrix, and the conditional loglikelihood of this observation.et  yt  Cxtt1St  CVtt1C RKt  Vtt1CS1tLt  logN et 0, StFinally, we update our estimates of the mean and variance.xtt  xtt1 KtetVtt  I KtCVtt1  Vtt1 KtStK tThese equations are more intuitive than they may seem. For example, our expected belief about xt is equalto our prediction, xtt1, plus a weighted error term, Ktet, where the weight Kt  Vtt1C S1t , dependson the ratio of our prior uncertainty, Vtt1, to the uncertainty in our error measurement, St.Backwards passThe backwards operatorxtT , VtT , Vt1,tT   Backxt1T , Vt1T , xtt, VttAt1, Qt19Normally random variables are upper case letters, and their values are lower case however, in this and other sections, we follow theother convention that scalars and vectors are lower case, and matrices are upper case.80is defined as follows this is the analog of the  recursion in Section 3.2.3. First we compute the followingpredicted quantities or we could pass them in from the filtering stagext1t  At1xttVt1t  At1VttAt1 Qt1Then we compute the smoother gain matrix.Jt  VttAt1V1t1tFinally, we compute our estimates of the mean, variance, and cross varianceVt,t1T  CovXt1, Xty1T .10xtT  xtt  Jtxt1T  xt1tVtT  Vtt  JtVt1T  Vt1tJ tVt1,tT  Jt1VtTThese equations are known as the RauchTungStriebel RTS equations. It is also possible to derive a twofilter approach to smoothing, as for HMMs see Jor02, ch13 and FP69. In this case, the analog of t ist, which can be computed from the following Lyapunov equationt  EXtXt  EAXt1 WtAXt1 Wt  AEXt1Xt1A EWtWt   At1A QThe limit of this, limt t, is known as the Riccati matrix.Time and space complexity of Kalman filteringsmoothingIf Xt  IRNx , and Yt  IRNy , then Kt is a Nx Ny matrix, St is a Ny Ny matrix, and At is a Nx Nxmatrix. The cost per update is OminN 2x , N3y  the N2x term arises from the Axt1t1 computation theN3y term arises from inverting St. For sparse matrices, it is possible to reduce the computational complexityconsiderably, especially if we use the information filter. If we want to track N objects, each of size Nx,inference takes ONxN2 hence KFMs do not suffer from the exponential growth in complexity thatHMMs do. However, they are rather restrictive in their assumptions see Section 1.3.4.3.6.2 Inference in general linearGaussian DBNsAny DBN in which all CPDs are linearGaussian can be converted to a KFM, just as any DBN in whichall hidden variables are discrete can be converted to an HMM. If we have D hidden continuous variables,each a vector of size K, the compound statespace will have size KD, and inference using the Kalmanfiltersmoother will take OKD2 time per step. Hence the complexity only grows polynomially in D,10The cross variance term is usually computed recursively, but this is unnecessary Min99, c.f., in an HMM, we can compute t1,tdirectly rather than recursively, as shown in Section 3.2.4.81unlike the discrete case, in which the complexity grows exponentially in D. Furthermore, the matrices ofthe compound KFM will be sparse, often block diagonal, reflecting the graph structure. Hence a standardKalman filtersmoother will usually be as efficient as using the junction tree algorithm.3.6.3 Switching KFMsConsider the switching KFM in Figure 2.31. This is a conditionally Gaussian CG model see Section B.5if we knew S1T , the distribution overX1T , Y1T would be jointly Gaussian. LSK01 prove that inference inCG networks is NPhard, even if the structure is a polytree, such as a switching KFM where the horizontalarcs between the switch nodes are absent. The proof is by reduction from subset sum, but the intuition is asfollows the prior P X1 is a mixture of K Gaussians, depending on S1 when we marginalize out S1, weare still left with a mixture of K Gaussians each one of these gets passed through K different matrices,depending on the value of S2, resulting in a mixture of K2 Gaussians, etc in general, the belief state attime t is a mixture of Kt Gaussians. We discuss some approximate inference algorithms for this model inSections 4.3 and 5.3.1.3.6.4 Nonlinear nonGaussian modelsKFMs support exact inference because of two facts a Gaussian pushed through a linear transformation,and subjected to additive Gaussian noise, still results in a Gaussian and updating a Gaussian prior with aGaussian likelihood, using Bayes rule, results in a Gaussian posterior. In other words, Gaussians are closedunder Bayesian updating in KFMs. A few other distributions enjoy this property see WH97, but in general,exact inference in DBNs with hidden continuous nodes which have CPDs other than linearGaussian is notpossible.11 In particular, exact inference in systems which are nonlinear andor nonGaussian is usually notpossible. We must therefore resort to approximations. There are essentially two classes of approximations,deterministic and stochastic see Chapters 4 and 5 respectively.3.7 Online and offline inference using forwardsbackwards operatorsWe have now seen several different ways of performing filtering and smoothing in DBNs. To hide thesedetails from higherlevel inference algorithms, we defined the abstract forwards and backwards operators. Inthis section, we use these operators to derive generic efficient filtering and smoothing algorithms.11Exact inference in DBNs in which all hidden nodes are discrete is always possible, although it may not be efficient. Continuousnodes that are observed do not cause a problem, no matter what their distribution, since they only affect the inference by means of theconditional likelihood, c.f., the Oti, i  P ytXt  i term for HMMs.82function smoothery1T f11  Fwd1y1for t  2  Tftt  Fwdft1t1, ytbT T  BackTfT T for t  T  1  1btT  Backbt1T , fttFigure 3.18 Pseudocode for offline smoothing.Sensorvalid.t0Leftclr.t0Leftclrsens.t0Rightclr.t0Rightclrsens.t0Lataction.t0Engstatus.t0Inlane.t0Xdot.t0Xdotsens.t0Ydot.t0Ydotsens.t0Stoppedn.t0Fclr.t0Fclrsens.t0FYdotdiff.t0FYdotdiffsens.t0Fcloseslow.t0Bclr.t0Bclrsens.t0BYdotdiff.t0BYdotdiffsens.t0BXdot.t0BXdotsens.t0 Bclosefast.t0Turnsignal.t0Sensorvalid.t1Leftclr.t1Leftclrsens.t1Rightclr.t1Rightclrsens.t1Lataction.t1Engstatus.t1Inlane.t1Xdot.t1Xdotsens.t1Ydot.t1Ydotsens.t1Stoppedn.t1Fclr.t1Fclrsens.t1FYdotdiff.t1FYdotdiffsens.t1Fcloseslow.t1Bclr.t1Bclrsens.t1BYdotdiff.t1BYdotdiffsens.t1BXdot.t1BXdotsens.t1 Bclosefast.t1Turnsignal.t1Sensorvalid.t2Leftclr.t2Leftclrsens.t2Rightclr.t2Rightclrsens.t2Lataction.t2Engstatus.t2Inlane.t2Xdot.t2Xdotsens.t2Ydot.t2Ydotsens.t2Stoppedn.t2Fclr.t2Fclrsens.t2FYdotdiff.t2FYdotdiffsens.t2Fcloseslow.t2Bclr.t2Bclrsens.t2BYdotdiff.t2BYdotdiffsens.t2BXdot.t2BXdotsens.t2 Bclosefast.t2Turnsignal.t2Sensorvalid.t3Leftclr.t3Leftclrsens.t3Rightclr.t3Rightclrsens.t3Lataction.t3Engstatus.t3Inlane.t3Xdot.t3Xdotsens.t3Ydot.t3Ydotsens.t3Stoppedn.t3Fclr.t3Fclrsens.t3FYdotdiff.t3FYdotdiffsens.t3Fcloseslow.t3Bclr.t3Bclrsens.t3BYdotdiff.t3BYdotdiffsens.t3BXdot.t3BXdotsens.t3 Bclosefast.t3Turnsignal.t3Sensorvalid.t4Leftclr.t4Leftclrsens.t4Rightclr.t4Rightclrsens.t4Lataction.t4Engstatus.t4Inlane.t4Xdot.t4Xdotsens.t4Ydot.t4Ydotsens.t4Stoppedn.t4Fclr.t4Fclrsens.t4FYdotdiff.t4FYdotdiffsens.t4Fcloseslow.t4Bclr.t4Bclrsens.t4BYdotdiff.t4BYdotdiffsens.t4BXdot.t4BXdotsens.t4 Bclosefast.t4Turnsignal.t4Figure 3.19 The BATnetwork see Figure 4.2 unrolled for 5 slices.3.7.1 Spaceefficient offline smoothing the Island algorithmIn Figure 3.18 we give pseudocode for computing b1T , . . . , bT T  using a generalized forwardsbackwardsalgorithm. Rather than returning b1T , . . . , bT T , we assume the output is emitted to some consumerprocess which e.g., computes the sufficient statistics needed for learning,t btT , which can be representedin OS space, where S is the size of a forwards or backwards message for a single timeslice.12Although the expected sufficient statistics take constant space, the above algorithm uses OST  temporary storage, since it needs to store ftt for t  1, . . . , T . To see that this might be a problem, considerthe BATnet shown in Figure 3.19. Here, S  103. To learn the parameters of this model, we need trainingsequences of length at least T  105. Hence the storage requirements are  108 bytes. Longer sequences ormore complex models e.g., for speech recognition or biosequence analysis will clearly require much morespace.We now present a way to reduce the space requirements from OST  to OSC logC T , where C is atunable parameter of the algorithm to be explained below. The catch is that the running time increases fromOT  toOT logC T . This algorithm was first presented in BMR97a an equivalent algorithm is presentedin ZP00.831218 242411 6 1212Figure 3.20 The basic idea behind the Island algorithm, with T  24, C  1. At the top level, we callIslandf0, b25, y124, which computes b1, b12 and b24. f0 and b25 are dummy boundary conditions used atthe top level. Then we call Islandf1, b12, y211, followed by Islandf12, b24, y1323.Basic ideaThe key insight is that, given a boundary condition on the left, we can do the forwards pass in constantspace by recursively computing ftt  Fwdft1t1, yt and throwing away the result at each step andsimilarly for the backwards pass, given a boundary condition on the right. In the island algorithm, we runthe forwardsbackwards algorithm, but instead of throwing away the results, we store the messages at Cislands or checkpoints plus at the two boundaries. This divides the problem into C  1 subproblems,on which we can call the algorithm recursively. See Figure 3.20.13Time and space complexity of the island algorithmThe total running time of the algorithm satisfies the following recurrenceRT   CTC CRTCsince we are dividing a sequence of length T into C sequences of length TC, each of which takes OTCtime to compute then we must call the function C times recursively. Solving this recurrences gives RT  OT logC T .The total amount of space is bounded by the depth of the tree, D  logC T , times the amount ofspace needed at each recursive invocation, which is OC  2S. This is because we must store checkpoints at C positions plus the two boundaries at every level on the call stack. So the total space required isOSC logC T .12In the Hugin architecture, we store clique and separator potentials for the 1 12slice DBN for the ShaferShenoy architecture, wejust store the clique potentials see Section B.4.5. When applied to an HMM with K states, Hugin uses S  K2 space, whereasShaferShenoy uses S  K space.13A related approach to spaceefficient inference in arbitrary Bayes nets appears in Dar00. Standard exact inference takes Onewtime and space, where n is the number of nodes and w is the treewidth. Darwiches algorithm reduces the space requirement to Onbut increases the time requirement to Onew log n. In the current context, n  OT , so this is not a big win.840 1 2 3 4 5 6 7 8 9 10x 104101100101102103104105IslandD Space complexity on mbat.netNumber of SlicesMega bytesstatically unrolled networkisland D1island D2island D3island D4Figure 3.21 Results of the Island algorithm on the BATnet. D represents the maximum depth of recursionallowed before switching over to the linearspace algorithm.We can tradeoff the space vs time requirements by varying the number of checkpoints, C. For example,for the BATnet, forC  2, we reduce the space requirements from 1011 to 107, but the running time increasesby a factor of 16. If we use C T  316 checkpoints, the space decreases from 1011 to 108, but therunning time only doubles since logT T  2. We can also vary the depth at which we bottom out, i.e.,switch over to the linear space algorithm. Some results on the BATnet are shown in Figure 3.21PseudocodeIn Figure 3.22, we give pseudocode for computing b1T , . . . , bT T in a nonsequential order using space forC  2 messages. When the length of the sequence is less than Tmin, we switch over to the usual OT spacesmoothing routine. To simplify notation, we assume that Fwdf, yt calls Fwd1yt when f is the initialempty message  similarly, Backb, f calls BackTf when b is the initial empty message. This saves usfrom having to clutter the code with ifthen statements that check for boundary conditions. Also, we use theMatlab notation a  s  b to denote the list a, a  s, . . . , a  cs, where c  bb  asc. a  1  b will beshortened to a  1, and a  1  b counts down. We start by calling Island, , y1T , C, Tmin.ExampleSuppose C  1, T  24 and Tmin  6, as in Figure 3.20. We compute f 1  f11, f 2  f1212,f 3  f2424, b1  b124, b2  b1224, and b3  b2424 we also store the positions of the checkpointst1  1, t2  12, t3  24. After emitting b1, b2 and b3, we call Islandf 1, b2, y211 and thenIslandf 2, b3, y1323.85function Islandf, b, y,C, TminT  lengthyif T  Tminbasecasef, b, y returnf 1  f  t1  1 k  2stepsize  bTC  1ccheckpoints  1  stepsize  T  Tfor t  1  Tf  Fwdf, ytif t  checkpointsf k  f  tk  t k  k  1k  C  2 bk  bfor t  T  1b  Backb, fif t  checkpointsbk  b emit b k  k  1for k  2  C  2Islandf k  1, bk, ytk  1  1  tk  1function basecasef, b, yT  lengthyf 0  ffor t  1  Tf t  Fwdf t 1, ytfor t  T downto 1b  Backb, f temit bFigure 3.22 Pseudocode for the Island algorithm spaceefficient smoothing.86Constantspace smoothingHere is a trivial algorithm to do smoothing in O1 space i.e., independent of T  unfortunately it takesOT 2 time, rendering it useless in practice. The algorithm is as follows Dar01 for each 1  k  T , dothe following first run a forwards filter from t  1 to t  k, to compute k in constant space only keepingt1 and t in memory at each step t, then run a backwards filter from t  T downto t  k, to compute kin constant space, and then combine the results.Here is a hypothetical algorithm for smoothing in O1 space and OT  time. First run the forwardsalgorithm, ftt  Fwdft1t1, yt, forgetting all the intermediate results i.e., only keeping fT T , and thenrun the backwards algorithm, computing ftt  Fwd1ft1t1, yt1 and btT  Backbt1T , ftt inparallel. We call this the country dance algorithm, because you can think of the forward pass as running tothe end to pick up its partner, bT T , and the two of them moving together back to the start.Unfortunately, inverting the forward operator is impossible in general to compute P Xty1t fromP Xt1y1t1 requires figuring out how to undo the effect of the transition and the observation yt1.This is most easily seen from the forwards equation for HMMst1  Ot1AtInverting this yieldst  A1O1t1t1But any system in which there is more than one way of reaching a state cannot be inverted. Also, any systemwith invertible observations is essentially fully observed, so no inference is necessary. The same argumentholds for inverting the backwards operator. Although this does not constitute a formal proof, I conjecturethat smoothing in OS space and OST  time is impossible see also Section 3.7.2.3.7.2 Fixedlag online smoothingFixedlag smoothing estimates P XtLy1t, where L  0 is the lag. If the delay can be tolerated, then thisis clearly a more accurate estimate than the filtered quantity P XtLy1tL, which does not take futureevidence into account. The benefit of using a smoother depends on the signaltonoise ratio.One way to implement this is to augment the statespace of the DBN with lagged copies of Xt as inthe blind deconvolution model in Section 2.4.3. In the case of KFMs, the the Kalman filter equations canbe modified in a straightforward way Moo73 to implement filtering in this model, which is equivalent tofixedlag smoothing in the original model.For the KFM case, the statespace grows from OS to OLS, and the computation grows from OS2toOLS2 per time step, where S  X . However, discrete statespaces grow fromOS toOSL, and the87f11  Fwd1y1for t  2  Lftt  Fwdft1t1, ytfor t  L 1  ftt  Fwdft1t1, ytbtt  BackTfttfor   t 1 downto t Lb t  Backb1t, f  emit btLtFigure 3.23 Pseudocode for fixedlag smoothing, first attempt.f 1  Fwd1y1for t  2  Lf t  Fwdf t 1, ytk  L 1for t  L 1  btLt, f 1  L, k  FLSyt, f 1  L, kfunction b, f 1  L, k  FLSyt, f 1  L, kL  lengthfk  k  1f k  Fwdf k, ytb  BackTf kfor   1  Lb  Backb, f kk  k  1k  k  1Figure 3.24 Pseudocode for fixedlag smoothing, final version.computation also grows exponentially. Hence we now present an algorithm that uses OSL time and spaceto do fixedlag smoothing in arbitrary DBNs.In Figure 3.23, we show a first attempt at implementing a fixedlag smoother. Suppose L  2 aftert  2, this will emit b13, b24, b35, etc. to some consumer process. Since this is an online algorithm, wecannot store the messages for all t. Hence we must use a wraparound buffer of lengthL1, as in Figure 3.24.f i is the ith entry in this buffer k is a pointer to the position in the buffer that contains the most recentforward message. We assume onebased indexing, and use the notation t 1 and t 1 to represent additionsubtraction modulo L.Time and space complexity of fixedlag smoothingIt is clear that the FLS algorithm takes OLS time and space. We could implement this in OS time andspace if we could compute btLt from btL1t1 see e.g., RN02, ch.17. However, this requires that themodel be invertible, as in Section 3.7.1. I conjecture that constant time fixedlag smoothing i.e., an algorithm88f11  Fwd1y1b11  BackTf11for t  2  ftt  Fwdft1t1, ytbtt  BackTfttFigure 3.25 Pseudocode for online filtering.which does an amount of work which is independent of the lag is impossible in general.Constanttime FLS would entail changing our beliefs about an event long in the past without having toconsider any intermediate events this amounts to action at a distance, which seems impossible. However, ifwe had a hierarchical model, we might be able to approximate this behavior by reasoning message passingat a longer lengthtime scale see Section 2.3.9 for a discussion of such a hierarchical model.3.7.3 Online filteringFiltering is just fixedlag smoothing where the lag L  0. In this case, the above code simplifies to the codeshown in Figure 3.25. Note that, in general, we have to call the backwards operator even though we are doingfiltering however, we only do the backwards pass within one slice. This will be explained in Section 3.4.2.Since this is an online algorithm, we cannot store the messages for all t. However, we can update fttand btt in place. Hence filtering takes OS space and time per step where the constant depends on the sizeof the model, but not on t.1414This of course assumes the basic operators can be implemented in closed form. This is not true e.g., for a switching KFM. SeeSection 3.6.389Chapter 4Approximate inference in DBNsdeterministic algorithms4.1 IntroductionIn the case of discrete statespaces, exact inference is always possible, but may be computationally prohibitive, since it takes KmaxiCiFin1 operations per time step, where Ci is the size of the ith cliquein the forward interface, and Fin is the maximal numer of parents within the same slice of any node seeSection 3.5.4. Typically maxCi  OD, the number of hidden persistent nodes per slice, making exactinference intractable for large discrete DBNs.A standard approximation in the discrete case, known as the BoyenKoller BK algorithm BK98b, isto approximate the joint distribution over the interface as a product of marginals. Unfortunately, sometimeseven this approximation can be intractable, since BK does exact inference in a 2slice DBN. This motivatesthe factored frontier FF algorithm MW01, which can be seen as a more aggressive form of approximationthan BK. I show that BK and FF are both special cases of loopy belief propagation LBP. In Section 4.2.4,I experimentally compare exact, FF, BK, and LBP on a number of DBNs. The FF algorithm, the connectionbetween FF, BK and LBP, and the experimental evaluation are the main novel contributions of this chapter.In the case of continuous statespaces, if everything is linearGaussian, we can apply the Kalman filtersmoother. But if we have nonlinear andor nonGaussian distributions, exact inference is usually notpossible. I briefly review some deterministic approximations for this case.Finally, I consider the problem of mixed discretecontinuous statespaces. As we saw in Section 3.6.3,exact inference in a switching KFM takes OK t operations at the tth time step. In Section 4.3, I review theclassica moment matching approximate filtering algorithm BSL93, and its extension to the smoothing caseMur98. I also describe how to apply expectation propagation EP Min01 to a switching KFM HZ02. Itturns out that in this case EP is just an iterated version of moment matching, just as LBP is just an iteratedversion of FF.90123456781112910131415161718192021222324252627282930313233343635Figure 4.1 The water DBN, designed to monitor a waste water treatement plant. This model is originally fromJKOP89, and was modified by BK98b to include discrete evidence nodes. The dotted arcs group togethernodes that will be used in the BK approximation see Section 4.2.1. Nodes are numbered in topological order,as required by BNT.Since inference in DBNs uses inference in BNs as a subroutine, you are strongly recommended to readAppendix B before reading this chapter.Very little is known about the accuracy of the approximation algorithms discussed in this chapter incontrast to the Monte Carlo approximations in the next chapter. In Section 4.2.1, we state the main resultfor the BK algorithm which shows the approximation error remains bounded however, it is unclear howtight the bound is. The theoretical results on loopy belief propagation LBP discussed in Section B.7.1 alsoapply to the DBN case again, however, it is often intractable to compute the expression which determines theaccuracy. At the time of writing, the author is unaware of any theoretical results on expectation propagationand hence of moment matching, which is just a special case of EP.4.2 Discretestate DBNs4.2.1 The BoyenKoller BK algorithmIf the interface cliques are too large, one approach is to approximate the joint on the interface as the productof marginals of smaller terms this is the basic idea behind the BK algorithm BK98b, BK98a, BK99.More precisely, BK constructs the junction tree for the 1 12 slice DBN Ht, but does not require all theinterface nodes to be in the same clique i.e., P Ity1t is no longer represented as a single clique potential.Instead, we approximate the belief state by a product of marginals, P Ity1t Cc1 P Ict y1t, whereP Ict y1t is the distribution on nodes in cluster c. The set of clusters partitions the nodes in the interface.9114719171620232125276121310841328262224181511952SensorValid1FYdotDiff1FcloseSlow1Xdot0 Xdot1InLane0 InLane1LeftClr0 LeftClr1RightClr0 RightClr1LatAction0 LatAction1FwdAction0 FwdAction1Ydot0 Ydot1Stopped0 Stopped1 BXdot1EngStatus0 EngStatus1 BcloseFast1FrontBackStatus0 FrontBackStatus1 BYdotDiff1Fclr1Bclr1XdotSens1YdotSens1LeftClrSens1RightClrSens1TurnSignal1FYdotDiffSens1FclrSens1BXdotSens1BclrSens1BYdotDiffSens1slice t slice t1 evidenceFigure 4.2 The BATnet, designed to monitor the state of an autonomous car the Bayesian Automated Taxi,or BATmobile FHKR95. The transient nodes are only shown for the second slice, to minimize clutter. Thedotted arcs group together nodes that will be used in the BK approximation see Section 4.2.1. Thanks toDaphne Koller for providing this figure. Nodes are numbered in topological order, as required by BNT.1 2 3 4 5 6 7 8 9 10 110246810121416clique sizenumber1 2 3 4 5 6 7 80246810121416clique sizenumber1 2 3 4 5 6024681012141618clique sizenumbera b cFigure 4.3 Clique size distribution for different levels of BK approximation applied to the BATnetFigure 4.2. The cliques of size 2 are due to the observed leaves. a Exact the interface is7, 14, 16, 17, 19, 20, 21, 23, 25, 27. b Partial approximation the interface sets are 7, 14, 16, 17, 19 and20, 21, 23, 25, 27, as in Figure 4.2. c Fully factorized approximation the interface sets are the singletons.Hence rather than connecting together all the nodes in the interface, we just connect together the nodes ineach cluster. The assumption is that this will result in smaller cliques in the 2TBN. Figures 4.3 and 4.4 showthat the max clique size is indeed reduced in some cases. The basic reason is that we only had to triangulatethe twoslice network, not the unrolled network.The accuracy of the BK algorithm depends on the clusters that we use to approximate the belief state.Exact inference corresponds to using a single cluster, containing all the interface nodes. The most aggressiveapproximation corresponds to using D clusters, one per variable we call this the fully factorized approximation.We can implement BK by slightly modifying the interface algorithm see Section 3.4, as we explainbelow.921 2 3 4 5 6 7 8 9 10 11 120123456789clique sizenumber1 2 3 4 5 6 70123456789clique sizenumber1 2 3 4 5 601234567891011clique sizenumbera b cFigure 4.4 Clique size distribution for different levels of BK approximation applied to the water DBNFigure 4.1. The cliques of size 2 are due to the observed leaves. a Exact the interface is 1, . . . , 8.b Partial approximation the interface sets are 1, 2, 3, 4, 5, 6 and 7, 8, as in Figure 4.1. c Fullyfactorized approximation the interface sets are the singletons.Forwards pass1. Initialize all clique and separator potentials in Jt to 1s, as before.2. Include the CPDs and evidence for slice t as before.3. To incorporate the prior, we proceed as follows. For each cluster c, we find the smallest cliques Ct1and Dt in Jt1 and Jt whose domains include c we then marginalize Ct1 onto Ict1 and multiplythis onto Dt .4. We collect and distribute evidence tofrom any clique.The reason we must collect and distribute evidence is because the clusters which constitute the interface maybe in several cliques, and hence all of them must be updated before being used as the prior for the next step.If there is only one cluster, it suffices to collect to the clique that contains this cluster.Backwards passA backwards smoothing pass for BK was described in BK98a. KLD01 describes a similar algorithmfor general BNs called up and down mini buckets. Again, it can be implemented by a slight modificationto the interface algorithm1. Construct Jt from ftt as before.2. To incorporate the backwards message, we proceed as follows. For each cluster c, we find the smallestcliques Dt1 in Jt1 and Ct in Jt whose domains include c we then marginalize Dt1 onto Ict andabsorb it into Ct .3. We collect and distribute evidence tofrom any clique.93t t1 coupledt1 t t1 factoredU P U PFigure 4.5 The BK algorithm as a series of update U and projection P steps.Analysis of error in BKThe forwards pass may be viewed more abstractly as shown in Figure 4.5. We start out with a factoredprior, t1, do one step of exact Bayesian updating using the junction tree algorithm which may couplesome of the clusters together, and then project the coupled posterior t down to a factored posterior, t,by computing marginals. This projection step computes t  arg minqS Dtq, where S is the setof factored distributions, and Dpqdef x px log pxqx is the KullbackLiebler divergence CT91.This is an example of assumed density filtering ADF Min01.As an algorithm, BK is quite straightforward. The main contribution of BK98b was the proof that theerror remains bounded over time, in the following senseE KLtt twhere the expectation is take over the possible observation sequences,  is the mixing rate of the DBN, andt is the additional error incurred by projection, over and above any error inherited from the factored priort  KLttKLtt.The intuition is that, even though projection introduces an error at every time step, the stochastic nature ofthe transitions, and the informative nature of the observations, reduces the error sufficiently to stop it buildingup.The best case for BK is when the observations are perfect noiseless, or when the transition matrix ismaximally stochastic i.e., P XtXt1 is independent of Xt1, since in either case, errors in the prior areirrelevant. Conversely, the worst case for BK is when the observations are absent or uninformative, and thetransition matrix is deterministic, since in this case, the error grows over time. This is consistent with thetheorem, since deterministic processes can have infinite mixing time. In Section 5.2, we will see that thebest case for BK is the worst case for particle filtering when we propose from the prior, and vice versa.944.2.2 The factored frontier FF algorithmJust as BK represents the interface distribution in factored form, so FF represents the frontier distribution infactored form. Of course, we dont need the whole frontier, only the interface, so the real difference is inthe way these approximate distributions are updated. BK takes in a factored prior, does one step of exactupdating using the jtree algorithm, and then projects the results back down to factored form. Unfortunately,for some models, even one step of exact updating is intractable. For example, consider a model of a videosequence, in which each time slice is an N  n n 2D lattice, and in which Xti, j has a connection fromXt1i, j. in this case, the largest clique in the 2TBN has size at least n N , since this is the size of themax clique in a 2D grid without any additional incoming temporal arcs. Hence the running time of BK is atleast TNKN , even in the case of fully factorized BK.Instead of doing exact updating and then projecting, FF computes the marginals directly. That is, whenwe add a node to the frontier, we do not multiply its CPD onto the joint frontier, but just compute a localjoint over the parents, multiply the CPD onto this, and then marginalize outP X it E  P X it PaX it uPaX it P uEwhere the marginal distributions over the parents, P uE, are available from the frontier since we never adda node until all its parents are in the frontier, and E is the evidence in, and to the left of, the frontier. Addinga node to the frontier entails adding its marginal, computed as above, to the list of marginals. Removinga node from the frontier entails removing the corresponding marginal from the list, which of course takesconstant time. The backwards pass is analogous. This algorithm clearly takes OTDKFin1 time, whereD is the number of nodes per slice, no matter what the DBN topology.4.2.3 Loopy belief propagation LBPLoopy belief propagation for general graphical models is explained in Section B.7.1.FF is a special case of LBPThe key assumption in LBP is that the messages coming into a node are independent which is true in atree. But this is exactly the same assumption that we make in the FF algorithm In fact, it is easy to seethat FF is just LBP with a specific forwardsbackwards FB message passing schedule, instead of the morecommon parallel protocol see Section B.4.2. The fixed points of LBP are the same, no matter what protocolis used. However, if there is not a unique fixed point, the algorithms may end up at different answers. Theycan also have different behavior in the short term. In particular, if the DBN is in fact a tree, then a singleFB iteration 2TD message computations will result in the exact posteriors, whereas it requires T iterationsof the decentralized protocol each iteration computing 2TD messages in parallel to reach the same result95abFigure 4.6 Illustration of the clustering process. a This is a modified version of a DBN with 4 persistentvariables. The big mega nodes contain the joint distribution on the whole slice. We have omitted the nonpersisitent variables e.g., observations for clarity. LBP applied to this graph is equivalent to BK. b This islike a, except we have created overlapping clusters of size 2, for additional accuracy.hence the centralized algorithm is more efficient PS91. For loopy graphs, it is not clear which protocol isbetter it depends on whether local or global information is more important for computing the posteriors. Onecan imagine performing iterations within a slice and also between slices in an interleaved manner.BK is a special case of LBPIt turns out that BK is also equivalent to a single forwardsbackwards pass of LBP, although on a slightlymodified graph see Figure 4.6. The key insight is that BK also uses a factored prior, but does an exact update step. To simulate the exact update, we can create two mega nodes that contain all the hidden forwardinterface nodes in that slice. The messages coming into the first mega node are assumed independent theyare then multiplied together to form the approximate prior a single message is then sent to the second meganode, corresponding to the exact update step finally, the individual marginals are computed by marginalization, and the process is repeated. Of course, BK does not actually construct the mega nodes, and does theexact update using junction tree, but the two algorithms are functionally equivalent.To simulate BK when the clusters contain more than one node, and possible overlap, we simply createmeganodes, one per cluster, before applying the above transformation see Figure 4.6 for an example.Iteration helpsSince FF and BK are equivalent to one iteration of LBP, on the regular and clustered graphs respectively,we can improve on both of these algorithms by iterating more than once. This gives the algorithm theopportunity to recover from its incorrect independence assumptions. This is also the intuition behindexpectation propagation Min01. We will see in the Section 4.2.4 that even a small number of iterations canhelp dramatically.960 200 400 6000246x 104L1 error0 200 400 6000246x 104L1 error1 iter 2 iter0 200 400 6000246x 104L1 error0 200 400 6000246x 104L1 error3 iter 4 iterFigure 4.7 L1 error on marginal posteriors vs. timeslice after iterations 14 of undamped LBP applied to thetraffic CHMM. The L1 error oscillates with a period of 2 as seen by the similarity between the graphs foriterations 13 and 24 this implies that the underlying marginals are oscillating with the same period.4.2.4 Experimental comparison of FF, BK and LBPIn this section, we compare the BK algorithm with k iterations of LBP on the original graph, using the FBprotocol k  1 iteration corresponds to FF. We used a coupled HMM model with N  10 chains trainedon some real freeway traffic data using exact EM KM00. We define the total L1 error in the marginals ast Ni1Ks1 P X it  sy1T   P X it  sy1T , where P  is the exact posterior and P  is theapproximate posterior. We could have used KL divergence instead. In Figure 4.7, we plot this against t for14 iterations of LBP. Clearly, the posteriors are oscillating, and this happens on many sequences with thismodel. We therefore used the damping trick described in Section B.7.1 let m be the damping factor. It isclear from Figure 4.8 that damping helps considerably. The results are summarised in Figure 4.9, where wesee that after a small number of iterations, LBP with m  0.1 is doing better than BK. Other sequences givesimilar behavior.To check that these results are not specific to this model data set, we also compared the algorithms on thewater DBN shown in Figure 4.1. We generated observation sequences of length 100 from this model usingrandom parameters and binary nodes, and then compared the marginal posteriors as a function of number ofiterations and damping factor. The results for a typical sequence are shown in Figure 4.10. This time we seethat there is no oscillation this is typical for data sampled from a model with random parameters, and thatas few as two iterations of LBP can outperform BK.970 200 400 6000246x 104L1 error0 200 400 6000246x 104L1 error1 iter 2 iter0 200 400 6000246x 104L1 error0 200 400 6000246x 104L1 error18 iter BKFigure 4.8 Iterations 1, 2, and 18 of LBP with damping factorm  0.1, and after using 1 iteration of BK, onthe traffic CHMM.In addition to accuracy, it is also interesting to see how the algorithms compare in terms of speed. Wetherefore generated random data from CHMMs with N  1, 3, . . . , 11 chains, and computed the posteriorsusing the different methods. The running times are shown in Figure 4.11. It is clear that both BK and FFLBPhave a running time which is linear in N for this CHMM model, but the constant factors of BK are muchhigher, due to the complexity of the algorithm, and in particular, the need to perform repeated marginalisations. This is also why BK is slower than exact inference for N  11, even though it is asymptotically moreefficient.4.3 Switching KFMsAs we saw in Section 3.6.3, the belief state in a switching KFM at time t has size OK t. We now discusssome approximations.4.3.1 GPB moment matching algorithmThe exponential belief state in a switching KFM arises because CG potentials are not closed under marginalization see Section B.5.2. A common approximation is to always represent the belief state using Kn1Gaussians this is sometimes called the GPBn Generalized Pseudo Bayesian approximation SM80,TSM85, BSL93, Kim94, WH97, and is an instance of assumed density filtering ADF. For example, inthe GBP2 algorithm, the prior has K modes, each gets passed through the K different filters, and the re980 5 10 15 2000.010.020.030.040.050.060.070.08iterationsL1 error0  0.10.20.40.60.8Figure 4.9 Results of applying LBP to the traffic CHMM with 10 chains. The lower solid horizontal line isthe error incurred by BK. The oscillating line is the error incurred by LBP using damping factor m  0 thelowest curve corresponds to m  0.1 and the highest to m  0.8. The upper horizontal line corresponds tonot updating the messages at all m  1, and gives an indication of performance based on local evidencealone.1 1.5 2 2.5 3 3.5 4 4.5 5456789101112x 103iterationsL1 error0  0.10.2Figure 4.10 Same as Figure 4.9, but for the water network. The dotted lines, from bottom to top, representm  0, m  0.1 and m  0.2. The solid line represents BK.991 3 5 7 9 1100.050.10.150.20.250.30.350.40.450.5elapsed time per slice secondsnum. chainsjtree  bk     loopy 1loopy 2loopy 3loopy 4Figure 4.11 Running time on CHMMs as a function of the number of chains. The vertical axis is the totalrunning time divided by the length of the sequence. The horizontal axis is the number of chains. The dottedcurve is junction tree, the steep straight line is BK, and the shallow straight lines are LBP loopy k meansk iterations of LBP.St1 St St1Xt1 Xt Xt1Yt1 Yt Yt1Figure 4.12 A switching Kalman filter model. Square nodes are discrete, round nodes are continuous.100Zt1, Zt Zt Zt, Zt1 Zt1 Zt1, Zt2Figure 4.13 A jtree for 4 slices of the switching KFM in Figure 4.12. We use the shorthand Zt  St, Xt,and have omitted the observation nodes Yt for simplicity. It is possible to construct a jtree with smallercliques, namely St1, Xt1, St Xt1, St Xt1, Xt, St Xt, St, etc.b1t1b2t1RRFilter 1Filter 2Filter 1Filter 2b1,1tb1,2tb2,1tb2,2tRBBBBBBBNMergeMergeb1tb2tFigure 4.14 The GPB2 algorithm for the case of a binary switch. bit1  P Xt1, St1  iy1t1 is theprior, bi,jt  P Xt, St1  i, St  yy1t, and bjt  P XtSt  yy1t is the approximate posterior. Thefilter box implements the Kalman filter equations see Section 3.6.1. The merge box implements the weakmarginalization moment matching equations see Section B.5.2.sulting mixture of K2 Gaussians is collapsed to the best mixture of K Gaussians, one Gaussian per valueof St. This is shown schematically in Figure 4.14. A fast approximation to the GPB2 algorithm, whichrequires K Kalman filter updates per step rather than K2, known as the interacting multiple models IMMapproximation, is shown schematically in Figure 4.15 BSL93.The best Gaussian approximation to a mixture of K Gaussians in the sense of minimizing KL distanceis obtained by moment matching, also known as weak marginalization see Section B.5.2. In fact, the GPB2procedure is identical to the forwards pass of the standard jtree algorithm applied to the jtree shown inFigure 4.13. Note that this is a weak jtree see Section B.3.6.One advantage of the jtree formulation is that it is simple to do backwards smoothing we just applythe standard message passing scheme to the jtree in the backwards direction. In particular, we do not needto make the additional approximation used in Kim94, Mur98. Since the collapsing approximation is onlyapplicable to messages represented in moment form see Section B.5.1, we use the  formulation insteadof the  formulation see Section 3.2, since  is a conditional likelihood that cannot be represented inmoment form.101b1t1b2t1Mixb1t1b2t1Filter 1Filter 2b1tb2tFigure 4.15 The IMM interacting multiple models algorithm for the case of a binary switch.4.3.2 Viterbi approximationAlthough we cannot computeSt1Xt1St1, Xt1, St, Xt exactly, and have to use moment matchingsee Section B.5.2, we can compute maxSt1Xt1St1, Xt1, St, Xt for each St, we simply pick themost likely St1, and integrate out the correspondingXt1. In general, one can maintainH hypotheses in theprior, update each of them to giveKH posterior modes, and then select theH most likely. This classical ideacan be extended to the smoothing context as follows PRCM99 compute P Xt1, Xt, St1, Sty1t, s1t2by doing a forwards Viterbi pass in the jtree, where s1t  arg maxs1t P s1ty1t, and then computeP Xt1, Xts1T , y1T  using the RTS smoother or the jtree algorithm with parameters specified by s1T .Note we keep H  K hypotheses in the belief state at every step.Note that this is not the same as computing P Xt1, St1, Xt, Sty1T  which is what we would reallylike in order to get the exact expected sufficient statistics needed for EM. For instance, the Viterbi approximation says P Xt, St  iy1T   0 for all i 6 st . However, if the regimes are easy to segment, theresponsibilities P Sty1T  are approximately deltafunctions, so the Viterbi approximation is a good one.This is the basis of Viterbi training, which is widely used in speech recognition. If the Viterbi approximation does not hold, one can obviously sample S1T see Section 5.4.1.4.3.3 Expectation propagationConsider the jtree in Figure 4.13. If we could marginalize CG potentials in closed form, a single forwardsbackwards pass would suffice to do exact smoothing furthermore, since this tree is a chain, the  and messages would not interact see Section B.4.6. However, since the messages only contain the first twomoments, rather than the full belief state, multiple iterations may help improve accuracy this is the basicidea behind expectation propagation EP Min01. The general algorithm is explained in Section B.7.2 aspecial case of it, applied to the switching KFM, is shown in Figure 4.16 based on HZ02. This could be102for t  1  Ttzt  1tzt  1if t  11z1  P y1z1P z1elsetzt1, zt  P ytztP ztzt1while not convergedfor t  1  Tif t  1pz1  1z11z1elsepzt1, zt  t1zt1tzt1, zttztqzt  Collapsest1xt1pzt1, zttzt qzttztfor t  T  1pzt1, zt  t1zt1tzt1, zttztqzt1  Collapsestxtpzt1, ztt1zt1 qzt1t1zt1Figure 4.16 Pseudocode for EP applied to a switching KFM, based on HZ02. Zt  Xt, St. Collapserefers to weak marginalization.converted to an online algorithm if we used a fixed lag window.Note that the BK algorithm is also an instance of EP, where the messages are factored. In this case, Ztrefers to all the nodes in slice t of the DBN, and the computation of t1zt1tzt1, zttzt is doneusing jtree. The collapse procedure simply means computing the desired product of marginals on the nodesin the forward interface.4.3.4 Variational methodsA structured variational approximation for a special case of a switching KFM, in which the discrete variableselects which hidden variable gets observed see Section 2.4.6, was proposed in GH98. This decoupled theproblem into parallel chains, very much like the variational approximation for factorial HMMs GJ97. Thevariational parameter for the discrete chain plays the role of the conditional likelihood term P YtSt  i ina regular HMM the variational parameters for the continuous chains represent the responsibility of that chainfor the evidence this weights the observations by changing the observation covariance matrix.A structured variational approximation for the arguably more useful switching KFM in which the discrete variable changes the dynamics see Section 2.4.3 was sketched in PRM00. Again, the proposedapproximating structure consists of two chains, one discrete and one continuous. The discrete variationalparameters are the conditional likelihoods, as before, but for the continuous chain, it is the system covarianceand transition matrices which represent the variational parameters since they summarize influence from the103discrete nodes above.4.4 Nonlinear nonGaussian models4.4.1 FilteringThe standard way to do filtering in nonlinear models is to do a local linearization around xtt1, and thenapply the Kalman filter to the resulting model this is called the iterated Extended Kalman filter EKF. Analternative to the EKF is the unscented Kalman filter see WdM01 for a review, which avoids the need tocompute the Jacobian, and generally approximates the covariance more accurately.For nonGaussian noise models, other techniques can be used. For example, suppose P YtXt is ageneralized linear model MN83 e.g., Poisson or lognormal. The overall model is called a dynamic generalized linear model. One can then use moment matching and approximate the posterior P Xty1t asGaussian WH97. It is straightforward to generalize this approximation to the DBN case SSG99.In all of the above algorithms, the posterior is being approximated by a Gaussian. If the true posterior ishighly multimodal, it is probably better to use particle filtering, which we discuss in Chapter 5.4.4.2 Sequential parameter estimationOnline parameter estimation is equivalent to sequential Bayesian updating see Section 6.1.1. If the parameters have conjugate priors and the model is fully observed see Appendix C, this can be done in closed formsee e.g., CDLS99, sec 9.4 for the case of multinomial CPDs with Dirichlet priors, or WH97 for variouscontinuous densities.Difficulties arise if the priors are not conjugate andor there is partial observability. Various classicaltechniques for collapsing the mixture of parameters that result from partial observability are discussed inCDLS99, sec. 9.7. Alternatively, we can use sequential variational Bayes HT01. If the goal is to dosimultaneous state and parameter estimation, we can run two EKFs in parallel WN01. Stochastic solutions,based on particle filtering, are discussed in Section 5.2. Methods based on online EM are discussed inSection C.4.5.4.4.3 SmoothingIn an offline setting, one can afford more expensive techniques. For state estimation one can use the extendedKalman smoother GR98, and for parameter estimation, one can use EM see Section C.4.2. Stochasticsolutions, mostly based on Gibbs sampling, are discussed in Section 5.4. See DK00 for a recent review ofnonlinear nonGaussian timeseries analysis techniques.104Chapter 5Approximate inference in DBNsstochastic algorithms5.1 IntroductionStochastic sampling algorithms come in two flavors offline and online. Offline methods are usually basedon importance sampling likelihood weighting or Monte Carlo Markov Chain MCMC GRS96, whichincludes Gibbs sampling and simulated annealing as special cases. Online methods usually use particlefiltering PF, also known as sequential importance sampling SIS, sequential Monte Carlo, the bootstrapfilter Gor93, the condensation algorithm IB96, survival of the fittest KKR95, etc. see AMGC02, LC98,Dou98 for good tutorials, and DdFG01 for a collection of recent papers. DBNs specify P XtXt1 in acompact form, and hence can be plugged in to any PF algorithm. I discuss this in Section 5.2.Sampling algorithms have several advantages over deterministic approximation algorithms they areeasy to implement, they work on almost any kind of model all kinds of CPDs can be mixed together, thestatespace can have variable size, the model structure can change, they can convert heuristics into provablycorrect algorithms by using them as proposal distributions, and, in the limit of an infinite number of samplesin asymptotia, they are guaranteed to give the exact answer.The main disadvantage of sampling algorithms is speed they are often significantly slower than deterministic methods, often making them unsuitable for large models andor large data sets. In this chapter, Idiscuss how to get the best of both worlds by combining exact and stochastic inference. The basic idea isto integrate out some of the variables using exact inference, and apply sampling to the remaining onesthis is called RaoBlackwellisation CR96. When combined with particle filtering, the resulting algorithm iscalled RaoBlackwellised particle filtering RBPF. In Section 5.3.1, I explain how RBPF can be applied to aswitching KFM AK77, CL00, DGK99. The first novel contribution appears in Section 5.3.2, where I applyRBPF to the SLAM simultaneous localization and mapping problem. In Section 5.3.3, I discuss how to apply RBPF to arbitrary DBNs. Finally, in Section 5.4.1, I briefly discuss how to combine RaoBlackwellisation105and MCMC.5.2 Particle filteringThe basic idea behind particle filtering PF is to approximate the belief state by a set of weighted particlesor samplesP Xty1t Nsi1witXt, XitIn this chapter, X it means the ith sample of Xt, and Xti means the ith component of Xt. Given a priorof this form, we can compute the posterior using importance sampling. In importance sampling, we assumethe target distribution, x, is hard to sample from instead, we sample from a proposal or importancedistribution qx, and weight the sample according to wi  xqx. After we have finished sampling,we can normalize all the weights soi wi  1. We can use this to sample paths with weightswit P xi1ty1tqxi1ty1tP x1ty1t can be computed recursively using Bayes rule. Typically we will want the proposal distributionto be recursive also, i.e., qx1ty1t  qxtx1t1, y1tqx1t1y1t1. In this case we havewit P ytxitP xitxit1P xi1t1y1t1qxitxi1t1, y1tqxi1t1y1t1P ytxitP xitxit1qxitxi1t1, y1twit1def wit  wit1where we have defined wit to be the incremental weight.For filtering, we only care about P Xty1t, as opposed to the whole trajectory, so we use the followingproposal, qxtxi1t1, y1t  qxtxit1, yt, so we only need to store xit1 instead of the whole trajectory.In this case the weights simplify towit P ytxitP xitxit1qxitxit1, yt5.1The most common proposal is to sample from the prior qxtxit1, yt  P xtxit1. In this case, theweights simplify to wit  P ytxit. For predicting the future, sampling from the prior is adequate, since thereis no evidence. This technique can be used e.g., to stochastically evaluate policies for POMDPs KMN99.But for monitoring filtering, it is not very efficient, since it amounts to guess until you hit. For example,if the transitions are highly stochastic, sampling from the prior will result in particles being proposed all overthe space if the observations are highly informative, most of the particles will get killed off i.e., assignedlow weight. In such a case, it makes more sense to first look at the evidence, yt, and then proposeqxtxit1, yt  P xtxit1, yt  P ytxtP xtxit1106In fact, one can prove this is the optimal proposal distribution, in the sense of minimizing the variance ofthe weights a balanced distribution being more economical, since particles with low weight are wasted.Unfortunately, it is often hard to sample from this distribution, and to compute the weights, which are givenby the normalizing constant of the optimal proposalwit  P ytxit1 xtP ytxtP xtxit1In Section 5.2.1, we will discuss when it is tractable to use the optimal proposal distribution.Applying importance sampling in this way is known as sequential importance sampling SIS. A wellknown problem with SIS is that the number of particles with nonzero weight rapidly goes to zero, even if weuse the optimal proposal distribution this is called particle impoverishment. An estimate of the effectivenumber of samples is given byNeff 1Nsi1wit25.2If this drops below some threshold, we can sample with replacement from the current belief state. Essentiallythis throws out particles with low weight and replicates those with high weight hence the term survivalof the fittest. This is called resampling, and can be done in ONs time. After resampling, the weightsare reset to the uniform distribution the past weights are reflected in the frequency with which particles aresampled, and do not need to be kept. Particle filtering is just sequential importance sampling with resamplingSISR. The resampling step was the key innovation in the 90s SIS itself has been around since at least the50s. The overall algorithm is sketched in Figure 5.1.Although resampling kills off unlikely particles, it also reduces the diversity of the population whichis why we dont do it at every time step if we did, then wit  wit. This a particular severe problem is thesystem is highly deterministic e.g., if the state space contains static parameters. A simple solution is toapply a kernel around each particle and then resample from the kernel. An alternative is to use an MCMCsmoothing step a particularly succesful version of this is the resamplemove algorithm BG01.5.2.1 Particle filtering for DBNsTo apply PF to a DBN, we use the likelihood weighting LW routine FC89, SP89 in Figure 5.2 to samplexit and compute wit. The proposal distribution that LW corresponds to depends on which nodes of the DBNare observed. In the simplest case of an HMM, where the observation is at a leaf node, LW samples from theprior, P X it xit1, and the computes the weight as w  P ytxit. We discuss how to improve this below.See also CD00.In general, some of the evidence might occur at arbitrary locations within the DBN slice. In this case,the proposal is qxt, yt j P xtjPaXtj, and the weight is wxt, yt j P ytjPaYtj,where xtj is the value of the jth hidden node at time t, and ytj is the value of the jth observed107function xit, witNsi1  PFxit1, wit1Nsi1, ytfor i  1  NsSample xit  qxit1, ytCompute wit from Equation 5.1wit  wit  wit1Compute wt Nsi1 witNormalize wit  witwtCompute Neff from Equation 5.2if Neff  threshold  resamplewitNsi1xt  xtwit  1NsFigure 5.1 Pseudocode for a generic particle filter. The resample step samples indices with replacementaccording to their weight the resulting set of sampled indices is called . The line xt  xt simply duplicatesor removes particles according to the chosen indices.function xit, wit  LWxit1, ytwit  1xit  empty vector of length Nfor each node i in topological orderLet u be the value of PaXit  in xit1, xitIf Xit not in ytSample xit  P Xit PaXit   uelsexit  the value of Xit in ytwit  wit  P Xit  xitPaXit   uFigure 5.2 Pseudocode for likelihood weighting.108node at time t, and the parents of both Xtj and Ytj may contain evidence. This is consistent, since asobserved in RN02P xt, yt jP xtjPaXtj jP ytjPaYtj  qxt, ytwxt, ytOptimal proposal distributionSince the evidence usually occurs at the leaves, likelihood weighting effectively samples from the prior,without looking at the evidence. A general way to take the evidence into account while sampling, suggestedin FC89, KKR95, is called evidence reversal. This means applying the rules of arc reversal Sha86until all the evidence nodes become parents instead of leaves. To reverse an arc from X  Y , we must addY s unique parents, Yp, to X , and add Xs unique parents, Xp, to Y both nodes may also share commonparents, C see Figure 5.3. The CPDs in the new network are given byP Y Xp, C, Yp xP Y C, YpP xXp, CP X Xp, C, Yp P Y X,C, YpP X Xp, CP Y Xp, C, YpNote that Xp, Yp and C could represent sets of variables. Hence the new CPDs could be much larger thanbefore the arc reversal. One way to ameliorate this affect, for treestructured CPDs, is discussed in CB97.Of course, if there are multiple evidence nodes, not all of the arcs have to be reversed. In the case of DBNs,the operation is shown in Figure 5.4. The new CPDs areP YtXt1 xtP YtxtP xtXt1P XtXt1, Yt P YtXtP XtXt1P YtXt1Arc reversal was proposed by Sha86 as a general means of inference in Bayesian networks. Since then,the junction tree jtree algorithm has come to dominate, since it is more efficient. It is possible to efficientlysample from P X E by first building a jtree, collecting evidence to the root, and then, in the distributephase, drawing a random sample from P XCiSi xSi , E for each cliqueCi, where Si is the separator nearerto the root Daw92. This is the optimal method.Unfortunately, for continuous valued variables when PF is most useful, it is not always possible tocompute the optimal proposal distribution, because the new CPDs required by arc reversal, or the potentialsrequired by jtree, cannot be computed. An important exception is when the observation model, P YtXt, isconditionally linearGaussian, and the process noise is Gaussian although the dynamics can be nonlinear,i.e.,P Xtxit1  N Xt ftxit1, QtP YtXt  N ytHtXt, Rt109X YXp C YpX YXp C Ypa bFigure 5.3 Arc reversal. a The original network. b The network after reversing the X  Y arc. Themodified network encodes the same probability distribution as the original network.Xt1 XtYtXt1 XtYta bFigure 5.4 A DBN a before and b after evidence reversal.In this case, one can use the standard Kalman filter rules to show that AMGC02P Xtxit1, yt  N Xtmt,twit  P ytxit1  N ytHtftxt1, Qt HtRtH twhere1t  Q1t HtR1t Htmt  tQ1t ftxit1 HtR1t ytIf the model does not satisfy these requirements, one can still use a Gaussian approximation of the formqXtxit1, yt constructed e.g., using the unscented transform vdMDdFW00. This is the sense in whichany heuristic can be converted into an optimal algorithm. If the process noise is nonGaussian, but theobservation model is conditional linearGaussian, once can propose from the likelihood and weight by thetransition prior. This observation was first made by Nando de Freitas, and has been exploited in FTBD01.Smoothing discrete statespacesWhen applying PF to low dimensional continuous statespaces, it is easy to place a Gaussian kernelaround each particle before resampling, to prevent particle collapse. However, most DBNs that have beenstudied have discrete statespaces. In this case, we can use the smoothing technique proposed in KL01. LetWt Nsi1 wit. We add a certain fraction, , of Wt to all entries in the statespace that are consistent withthe evidence i.e., which give it nonzero likelihood, and then renormalize. This is like using a uniformDirichlet prior. That is, the smoothed approximate belief state isP xy1t ixitxwitZ110if x is consistent with yt, and P xy1t  0 otherwise. The sum is over all particles that are equal to x ifthere are no such particles, the numerator has value . The normalizing constant is Z  Wt M , whereMis the total number of states consistent with yt. We discuss how to computeM below. We can sample fromthis smoothed belief state as follows. With probability WtZ, we select a particle as usual with probabilitywit, otherwise we select a state which is consistent with the evidence uniformly at random.Computing M is equivalent to counting the number of satisfying assignments, which in general is Phard worse than NPhard. If all CPDs are stochastic, we can computeM using techniques similar to Bayesnet inference. Unfortunately, the cost of exactly computing M may be as high as doing exact inference. Aquick and dirty solution is to add probability mass of  to all states, whether or not they are consistent withthe evidence.Combining PF with BKNPP02 suggested combining particle filtering with the BoyenKoller algorithm see Section 4.2.1, i.e.,approximating the belief state byP Xty1t Cc11NcNci1Xt,c, xit,cwhere C is the number of clusters, and Nc is the number of particles in each cluster assumed uniformlyweighted. By applying PF to a smaller statespace each cluster, they reduce the variance, at a cost ofincreasing the bias by using a factored representation. They show that this method outperforms standardPF sampling from the prior on some small, discrete DBNs. However, they do not compare with BK.Furthermore, it might be difficult to extend the method to work with continuous statespaces when PF ismost useful, since the two proposed methods of propagating the factored particles  using jtree and usingan equijoin operator  only work well with discrete variables.5.3 RaoBlackwellised Particle Filtering RBPFThe RaoBlackwell theorem shows how to improve upon any given estimator under every convex loss function. At its core is the following wellknown identityVarX,R  VarEX,RR EVarX,RRwhere X,R is some estimator of X andR. Hence  X,R  EX,RR is a lower variance estimator. So if we can sample R and compute this conditional expectation analytically, we will need less samplesfor a given accuracy. Of course, less samples does not necessarily mean less time it depends on whetherwe can compute the conditional expectation efficiently or not. In the following, we will always assume thisis the case.111function sit, it,it, wit  RBPFSKFMpriorsit1, it1,it1, wit1, ytfor i  1  NsSample sit  P Stsit1it,it, wit  KFit1,it1, yt, sit wit  wit  wit1Compute wt Nsi1 witNormalize wit  witwtCompute Neff from Equation 5.2if Neff  threshold  resamplewitNsi1st  st , t  t , t  twit  1NsFigure 5.5 Pseudocode for RBPF applied to a switching KFM, where we sample from the prior.5.3.1 RBPF for switching KFMsConsider the switching KFM in Figure 4.12. If we knew S1t, we could compute P Xty1t, s1t exactlyusing a Kalman filter. Hence we can apply particle filtering to St, which lives in a small, discrete space,and integrate out Xt, which lives in a potentially large, continuous space. This idea was first proposed inAK77 without the resampling step and independently in CL00, DGK99 using the modern PF method.Algorithmically, what this means is that each particle i contains a sampled value for St, which implicitelyrepresents a whole trajectory, si1t, plus the sufficient statistics for the Kalman filter conditioned on thistrajectory, it  EXty1t, si1t and it  CovXty1t, si1t. If we propose from the prior, qStsit1 P Stsit1, we can compute the weight using the onestepahead prediction likelihoodwit P ytsitP sitsit1P sitsit1xtP ytxt, sitP xtsi1t, y1t1 P yty1t1, si1t N ytCsAsit1, CsAsit1As QsC s Rswhere s  sit. This term is just a byproduct of applying the Kalman filtering equations to the sufficient statistics it1,it1 with the parameter set s  As, Cs, Qs, Rs. The overall algorithm is shown in Figure 5.5.Note that we can use this algorithm even if St is not discrete.If the number of values of St is sufficiently small, and the ratio of transition noise for St to observationnoise sufficiently high, it might be beneficial to use the optimal proposal distribution, which is given byP St  ssi1t1, y1t  P ytSt  s, si1t1, y1t1P St  ssit1112function sit, it,it, wit  RBPFSKFMoptsit1, it1,it1, wit1, ytfor i  1  Nsfor each ss,s, Ls  KFit1,it1, yt, sqs  Ls  P St  ssit1ws s qsNormalize qs  qswsSample s  qsit  s, it  s, it  s, wit  wswit  wit  wit1. . .Figure 5.6 Pseudocode for RBPF applied to a switching KFM, where we sample from the optimal proposal.The third return argument from the KF routine is Ls  P ytSt  s, si1t1, y1t1. . . . means the codecontinues as in Figure 5.5.In this case, the incremental weight is just the normalizing constant for the optimal proposalwit sP ytSt  s, si1t1, y1t1P St  ssit1The modified algorithm is shown in Figure 5.6. This is more expensive than sampling from the prior, sincefor each particle i, we must loop over all states s. However, this may require fewer particles, making it fasteroverall.Fault diagnosisIn principle, it is straightforward to apply RBPF to problems in fault diagnosis, such as the DBN in Figure 2.33. Unfortunately, in that model, there are a large number of discrete variables, so it is too expensive touse the optimal proposal. Obviously one could propose from the prior, but there is an alternative, deterministic heuristic that works well in this case LP01, namely enumerate the discretes in order according to theirprior probability. This exploits the fact that it is more likely that there is a single fault than that there are twofaults, etc. Since this is a nonlinear model, even conditioned on the discretes, one must apply the EKF orUPF, instead of the KF, to each particle.Enumerating in a priori order is not applicable if the discrete variables represented cardinal as opposedto ordinal quantities, as occurs e.g., in data association problems. Another problem with this scheme isthat although faults may have low prior probability, they may have a high cost. One should therefore takethe loss function into account when proposing samples TLV01. An alternative technique in CT02 triesto identify the most likely assignment to the discretes by allocating a different number of samples to eachdiscrete hypothesis in an adaptive way this is similar to Hoeffding races MM93.113Stochastic vs deterministic approximationsRBPF for switching KFMs is very similar to classical multiple hypothesis tracking MHT. It is not clearwhich is better. The argument between stochastic vs deterministic approximations often boils down to mattersof taste.5.3.2 RBPF for simultaneous localisation and mapping SLAMWhen a mobile robot moves through the world, it is important that it knows where it is this is called thelocalization problem. It is not always possible to use GPS to solve this problem e.g., in indoor environments.However, by comparing what it sees with a map of the world, the robot can infer its location not alwaysuniquely. There are two popular representations for maps metric gridbased and topological. We restrictour attention to the first.A metric map is usually represented as an occupancy grid ME85, which is a binary matrix where 1srepresent cells which contain obstacles and 0s represent free space. Since we have discretized space, wecan think of the map assumed to be static as defining the observation and transition matrix of an HMM.Hence the map can be learned using EM TBF98. EM provides a solution to the following chickenandeggproblem to localize, the robot must know the map, but the learn the map, the robot must know its location.This learning is usually done offline, after a human has joysticked the robot through the environment.Localization simply means inferring P Lty1t, u1t, where Lt is the location of the robot, y1t arethe sensor measurements typically from a sonar or laser range finder, plus odometry information from thewheels and u1t are the control signals sent to the motors. Henceforth we shall use z1t  y1t, u1tfor brevity. In principle, this inference problem can be solved exactly by using the forwards algorithm forHMMs. In practice, this is too slow, since the statespace, though discrete, is very large millions of cells.However, one can apply particle filters in a straightforward way FTBD01.Offline map learning with EM is unsatisfactory for many reasons. An online solution, where we regardthe map as a random variable and perform joint Bayesian inference to compute P Lt,Mtz1t, is much moreuseful, since it can handle maps that change, and it gives a confidence estimate in the map which can be usedto guide exploration. This joint estimation problem is usually called SLAM simultaneous localization andmapping.The standard approach to SLAM see e.g., DNCDW01 for a recent account is to represent the mapas a list of locations of landmarks. One then just applies the extended Kalman filter. Unfortunately, ifthere are NL landmarks, this has complexity ONL2, since the covariance matrix has size NL  NL. InSection 5.3.2, I show how RBPF can reduce this to ONs  NL, where Ns is the number of particles. Byusing an appropriate control policy,Ns can be kept constant. Consequently, we can solve the SLAM problemin much larger environments than traditional techniques. By using a treebased datastructure, MTKW02114reduced the complexity even further, to ONs  logNL, and demonstrated that the algorithm works on areal robot in an environment with 50,000 landmarks.A disadvantage of representing maps as a list of landmark locations is that this requires identifyinglandmarks, which requires extracting features such as corners from the sensors and then solving the dataassociation problem see Section 2.4.6. This is often solved by placing beacons with unique identifiersinto the environment e.g., DurrantWhyte submerges beacons below the sea so he can map the seabedusing autonomous submarines. The advantage of gridbased maps is that they merely require evaluatingP ytLt,Mt, where yt could be a complex signal e.g., a dense set of laser range finder distance readingsfrom an unmodifed environment. P ytLt,Mt is often represented using a neural network Thr98. Ofcourse, the disadvantage of occupancy grids is that they provide a very lowlevel hardtointrepret representation. Below I show how RBPF can also be applied to the SLAM problem using an occupancy gridrepresentation.The basic ideaConsider a robot that can move on a discrete, twodimensional grid, and which can observe the color ofthe cells in a 3  3 neighborhood centered on its current location, LXt , LYt . The color of a cell couldrepresent whether it is occupied or not. However, for simplicity of exposition, we shall assume that the robotcan move anywhere  think of the robot as gliding over a multicolored checkerboard. This is a standardassumption. Hence the map which stores the color of each cell affects the observation, Yt, but not theposition see Figure 5.7.If we could do inference in this DBN, we would have solved the SLAM problem. Unfortunately, exactinference is intractable. If there are K colors and the grid has size NL, the belief state, P Mtz1t, hassize OKNL, because all the grid cells become correlated due to sharing a common observed child c.f., theexplaining away phenomenon in factorial HMMs Section 2.3.7.However, note the following crucial insight Mur00 if we knew L1t, the grid cells would not becomecorrelated, because we would always know exactly which cells to update for each measurement, c.f., the dataassociation problem Section 2.4.6. Hence if we sample paths L1t, we can represent the belief state asP Mt, Ltz1t Nsi1jP Mtjz1t, Li1tP Litz1twhere Mtj is the jth grid cell at time t. I have assumed a factored prior, P M1 j P M1j. Thiscan be implemented similarly to RBPF for switching KFMs each particle contains a sample of Lit and aproduct of factors, P Mtjz1t, Li1t, only one of which will get updated at every step namely the entryj  Lit.11Pfe01 shows that maintaining the marginals of a belief state is sufficient for exact inference of future marginals iff the CPDs havethe form of a multiplexer. However, he does not consider the case in which the evidence node also has this form, as we assume here.115Mt1 MtYtYt1t1ttUt1RXt1YXYRRRFigure 5.7 The DBN used in the maplearning problem. M represents the map, R the robots location, Ythe observation, and U the input control. There is one arc from each i, j cell of Mt to Yt. Similarly, eachi, j cell of Mt connects to the corresponding i, j cell in Mt1. P YtMt, Lt is a multiplexer CPD, i.e.,P YtMt, Lt  j  fYt,Mtj.116Results for a 1D gridTo evaluate the effectiveness of this idea, we constructed a simple 1D grid world which was small enough toallow for an exact solution. Specifically, we used the grid in Figure 5.8, with NL  8 and K  2, so exactinference takes about 50,000 operations per time step .We assume that the robot sees a noisy version of the color of the cell at its current locationP ytm1, . . . ,mNL , Lt  l1 po if yt  mlpo if yt 6 mlwhere po is the probability that a color gets misread observation error. We also assume that the robot canonly move left or right, depending on the control input, Ut. The robot moves in the desired direction withprobability 1pa, and otherwise stays put. In addition, it cannot move off the edges of the grid. Algebraically,this becomesP Lt  lLt1  l, Ut  1 pa if l  l  1 and 1  l  NLpa if l  l and 1  l  NL1 if l  l  NL0 otherwiseThe equation for P Lt  lLt1  l, Ut  is analogous. In Section 5.3.2, we will consider the case of arobot moving in two dimensions. In that case, the motion model will be slightly more complex.Finally, we need to specify the prior. The robot is assumed to know its initial location, so the priorP L1 is a delta function P L1  1  1. If the robot did not know its initial location, there would be nowelldefined origin to the map in other words, the map is relative to the initial location. Also, the robot hasa uniform prior over the colors of the cells. However, it knows the size of the environment, so the state spacehas fixed size.For simplicity, we fixed the control policy as follows the robot starts at the left, moves to the end,and then returns home. Suppose there are no sensor errors, but there is a single slippage error at t  4.What this means is that the robot issued the go right command, but the robot actually stayed in the sameplace. It has no way of knowing this, except by comparing what it sees with what it already knows about theenvironment, which is very little by the time the slippage occurs, all the robot knows is the color of the cellsit has already seen cells to the right could have any color, hence if it sees two black cells in a row, it cannottell if this is how the world really is, or whether it has got stuck, and hence is seeing the same cell twice.Clearly this is an extreme case, but it serves to illustrate the point.We summarize the evidence sequence below, where Ut represents the input control action at time t.t 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16Lt 1 2 3 4 4 5 6 7 8 7 6 5 4 3 2 1Yt 0 1 0 1 1 0 1 0 1 0 1 0 1 0 1 0Ut                117Figure 5.8 A onedimensional grid world.time tgrid cell iMarginal map, i.e., PMit  y1t2 4 6 8 10 12 14 1612345678 0.10.20.30.40.50.60.70.80.9time tgrid cell iProb. location, i.e., PLti  y1t2 4 6 8 10 12 14 161234567800.10.20.30.40.50.60.70.80.91a bFigure 5.9 Results of exact inference on the 1D grid world. a A plot of P Mti  1y1t, where i is thevertical axis and t is the horizontal axis lighter cells are more likely to be color 1 white. The pattern inthe right hand column consists of alternating white and black stripes, which means it has learned the correctmap. Notice how the ambiguity about the colors of cells 57 at time 9 gets resolved by time 11. b A plot ofP Lt  iy1t, i.e., the estimated location of the robot at each time step.time tgrid cell iMarginal map, i.e., PMit  y1t, 50 particles, seed 12 4 6 8 10 12 14 16123456780.10.20.30.40.50.60.70.80.9time tgrid cell iProb. location, i.e., PLti  y1t, 50 particles, seed 12 4 6 8 10 12 14 161234567800.10.20.30.40.50.60.70.80.91a bFigure 5.10 Results of the RBPF algorithm on the 1D grid world using 50 particles.118time tgrid cell iBK Marginal map, i.e., PMti  y1t2 4 6 8 10 12 14 16123456780.10.20.30.40.50.60.70.80.9time tgrid cell iBK Prob. location, i.e., PLti  y1t2 4 6 8 10 12 14 161234567800.10.20.30.40.50.60.70.80.91a bFigure 5.11 Results of the BK algorithm on the 1D grid world.2 4 6 8 1012345678910Figure 5.12 A simple 2D grid world. Grey cells denote doors which are either open or closed, black denoteswalls, and white denotes free space.The exact marginal belief states, P Ltz1t and P Mtz1t, are shown in Figure 5.9. At each time step,the robot thinks it is moving one step to the right, but the uncertainty gradually increases. However, as therobot returns to familiar territory, it is able to better localize itself, and hence the map becomes sharper,even in cells far from its current location. Note that this effect only occurs because we are modelling thecorrelation between all the cells. The need for this was first pointed out in SSC88.In Figure 5.10, we show the results obtained using RBPF with 50 particles proposed from the prior.The results shown are for a particular random number seed other seeds produce qualitatively very similarresults, indicating that 50 particles are in fact sufficient in this case. Obviously, as we increase the number ofparticles, the error and variance decrease, but the running time increases linearly. We discuss the question ofhow many particles we need in Section 5.3.2.For comparison purposes, we also tried the fully factorized version of the BK algorithm see Section 4.2.1. The results of using BK are shown in Figure 5.11. As we can see, it performs very poorly inthis case, because it ignores correlation between the cells. Of course, it is possible to use larger clusters, butunless we model the correlation between all of the cells, we will not get good results.119free2 4 6 8 10246810open2 4 6 8 10246810wall2 4 6 8 10246810posterior prob. of each cell type at t50closed2 4 6 8 10246810Figure 5.13 Results of RBPF on the 2D grid world using 200 particles. Cell i, j in the top left figurerepresents P Mti, j  freez1t using a gray scale, and similarly for the other three figures. Hence theopen doors correspond to white blobs on the top right figure, etc.Results for a 2D gridNow we now consider the 10 10 grid world in Figure 5.12. We use four colors, which represent closeddoors, open doors, walls, and free space. Doors can toggle between open and closed independently with probability pc  0.1, but the other colors remain static hence the cell transition matrix, P MtjMt1j,is 1 pc pc 0 0pc 1 pc 0 00 0 1 00 0 0 1The robot observes a 33 neighborhood centered on its current location. The probability that each pixel getsmisclassified is po  0.1. The robot can move north, east, south or west there is a pa  0.1 chance it willaccidently move in a direction perpendicular to the one specified by Ut.We use a control policy that alternates between exploring new territory when the robot is confident of itslocation, and returning to familiar territory to relocalize itself when the entropy of P Lty1t becomes toohigh, as in FBT98.The results of applying the RBPF algorithm to this problem, using 200 particles, are shown in Figure 5.13. We see that by time 50, it has learnt an almost perfect map, even though the state space has size4100.1200 50 100 150 200 25000.511.522.5Accuracy of FastSLAM as Number of Particles IncreasesNumber of ParticlesPosition Error standard deviationFigure 5.14 Accuracy vs num. particles for an environment with 50,000 landmarks. Solid line robotlocation, dotted line landmark location.Results for IR2Recently the above idea has been extended so it works with maps represented in terms of landmark x, ylocations instead of occupancy grids MTKW02. In this case, we sample the sequence of poses position plus orientation of the robot, Lt  IR3 conditioned on a trajectory, the map factorizes as before,P MtLi1t, z1t j P MtjLi1t, z1t, only now Mtj  IR2, and P MtjLi1t, z1t is representedas a Gaussian distribution using a mean vector and a 2  2 covariance matrix. This technique, which takesONLNs operations per time step, scales much better than the standard EKF technique, which has complexity ONL2, assuming the number of particles is reasonable see Section 5.3.2. Some results are shownin Figure 5.14. In fact, by using a treebased data structure, we can share submatrices between particles,even if they have different histories this reduces the update cost of each particle from ONL to OlogNLMTKW02. Unfortunately, this scheme is rather complex and has not yet been implemented Thrun, personal communication.How many particles do we needThe number of particles required depends both on the noise parameters and the structure of the environment.If every cell has a unique color, localization, and hence map learning, is easy. Since we are samplingtrajectories, the number of hypotheses grows exponentially with time. In the worst case, the number ofparticles needed may depend on the length of the longest cycle in the environment, since this determines how121long it might take for the robot to return to familiar territory and kill off some of the hypotheses sincea uniform prior on the map cannot be used to determine Lt when the robot is visiting places for the firsttime. In the 1D example, the robot was able to localize itself quite accurately when it reached the end of thecorridor, since it knew that this corresponded to cell 8. In the 2D example, to keep the number of particlesconstant, I found it necessary to use a control policy that backtracked whenever the robot got lost. Thrun etal. found that a similar policy worked well in the continuous case.5.3.3 RBPF for general DBNs towards a turnkey algorithmWe have now seen two different examples of RBPF, applied to a switching KFM and to the SLAM DBN. Inboth cases, a human decided which nodes to sample and which to integrate out. However, if we are to buildan autonomous lifelong learning agent, it must decide by itself how to do inference especially if it is doingonline structure learning. Given an arbitrary DBN, how should it decide which nodes to sampleThe key requirement is that, conditioned on the sampled nodes, call them Rt, we should be able tocompute P Xtri1t, y1t more efficiently than by sampling everything. Determining whether this is the caseor not depends not only on the graph structure, but also on the parameterization. For example, if the systemis linearGaussian, conditional on Rt, we know we can perform the computation in closed form using theKalman filter. To automate this kind of knowledge would require a theorem prover symbolic algebra packagec.f., the AutoBayes project FS02. If the system is not linearGaussian, it might be nearly so, conditioned onsome other variables, and hence we could apply an EKF or UKF. It will be very hard to automatically detectthis kind of situation.Sometimes the graph structure will be revealing, even without knowing the details of the CPDs. Forexample, consider sampling Rt in Figure 5.15. Even to sample from the prior, we must computeP Rtri1t1, y1t1 xt1P Rtri1t1, y1t1, xt1P xt1ri1t1, y1t1xt1P Rtrit1, xt1P xt1ri1t1, y1t1Since this requires summing or integrating over Xt1, this will in general be infeasible. Hence we typicallyassume that the nodes that are being sampled have no incoming arcs from nonsampled nodes in this case,we require there to be no arc fromXt1 to Rt. Hence the sampled nodes will generally be roots of the graphR for root, or children of sampled roots. Note that parameters are usually root nodes once the parametershave been sampled, the remaining variables can often be handled exactly.In the best case, the problem decomposes into two or more completely separate subproblems, conditionedon the roots. An example was shown in Figure 3.16, where it is crucial that the dotted arcs coming into theroot are absent. If the dotted arcs were present, Rt would act like a common observed child, correlating thesubprocesses instead of separating them.122R1 R2X1 X2Y1 Y2Figure 5.15 Example of an DBN for which RBPF is unsuitable.5.4 SmoothingSo far, we have only considered filtering problems. Often we want to compute P Xty1T , or the fixed lagversion, P XtLy1t. This is especially useful for learning see Chapter 6. This can be done by samplingtrajectories using particle filtering, and then recalculating the particles weights via a backwards recursionGDW00. However, we would really like future evidence to affect the positions of the particles as well astheir weights. It is possible to run a PF in both the forwards and backwards directions IB98, but this hascomplexityONs2, so is not popular.The most widely used technique is based on MCMC, in particular, Gibbs sampling see e.g., DK02for a very recent approach. Unfortunately, Gibbs sampling mixes very slowly for time series, because thedata are correlated. It is therefore crucial to use RaoBlackwellisation. The most important case where this ispossible is in conditionally linearGaussian models, which we discuss below.5.4.1 RaoBlackwellised Gibbs sampling for switching KFMsCarter and Kohn CK96 proposed the following RaoBlackwellised MCMC algorithm for inference inswitching KFMs. KN98 is a whole book devoted to MCMC for switching KFMs.1. Randomly initialise s01T .2. For i  1, 2, . . . until convergencea For t  1, . . . , T , sample sit  P Sty1T , sitb Compute xi0T  Ex0T yi1t, si1Twhere sitdefsi1, . . . , sit1, si1t1, . . . , si1Tcontains new values of St to the left of t, and old values to theright. The final estimate is then obtained by averaging the samples si1T and xi1T , possibly discarding aninitial segment corresponding to the burnin period of the Markov chain.The main issue is how to efficiently compute the sampling distribution P Sty1T , sit. CK96 proposean OT  algorithm which involves first running the backwards Kalman filter2, and then working forwards,2CK96 assume that the transition matrices are all invertible, so they can compute the backwards filter in moment form c.f.,Section 3.2.5. DA99 use the information form of the backwards filter, which does not require invertible dynamics.123sampling St and doing Bayesian updating conditional on the sampled value.The paper is hard to read because of the heavy notation involved in explicitely manipulating Gaussians.Here, we rederive the algorithm for the simpler case in which X is a discrete random variable. Y can havean arbitrary distribution, since it is observed. The original algorithm can then be recovered by replacing thediscrete multiplication and addition operators on the Xt nodes with their Gaussian equivalents, as discussedin Section B.5. We then show how to apply the same idea to arbitrary DBNs using the jtree algorithm.Computing the sampling distributionThe sampling distribution isP sty1T , st  P y1T s1T P ststThe first term is given byP y1T s1T  iP yt1T y1t, s1T , Xt  iP y1t, Xt  is1T iP yt1T st1T , Xt  iP y1t, Xt  is1t P y1ts1tiP yt1T st1T , Xt  iP Xt  iy1t, s1t P y1t1s1t1P yty1t1, s1tiP yt1T st1T , Xt  iP Xt  iy1t, s1tHence, dropping terms that are independent of st,P sty1T , st  P stst1P st1stP yty1t1, s1tiP yt1T st1T , Xt  iP Xt  iy1t, s1tThe P Xt  iy1t, s1t and P yty1t1, s1t terms can be computed by running the forwards algorithm onthe XY HMM, sampling St as we go. The P yt1T st1T , Xt  i term can be computed by using thebackwards algorithm on the XY HMM. The key observation is that this term is independent of sk1t , andhence can be computed ahead of time, before sampling s1t on the forwards pass.Using the jtree algorithmWe now show how to implement the above scheme using the jtree algorithm. The jtree is shown in Figure 4.13. Let us make the first clique on the left the root. Initially the clique potentials will contain theCPDs Xt1, St1, Xt, St  P ytXt, StP StSt1P XtXt1, St. After instantiating S1T , anddoing a backwards pass collect to root, the separator potentials contain terms of the form Xt, St P yt1T st1T , Xt, st, c.f., t in an HMM. Now, in the forwards pass distribute from root, we sample allthe S variables in a clique using Dawids algorithm Daw92 see Section 5.2.1. Hence the updated separatorpotentials have the form Xt1, St1  P Xt1y1T , s1t1, stT , c.f. t in an HMM. The message124entering a clique is nowXt1, St1Xt1, St1 P Xt1, y1t1s1t1since t  tt. Combining, we havetXt1, St1, Xt, Stt  P Xt1, St1, Xt, Sty1T , s1t1, s1tHence we can sample a new St and continue. This is equivalent to the algorithm above.Stochastic vs deterministic approximationsAn obvious question is how does the above RaoBlackwellised Gibbs sampler compare to offline deterministic approximations such as variational methods and expectation propagation, in terms of accuracy vscomputation time. As far as I know, this have never been studied.125Chapter 6DBNs learningThe techniques for learning DBNs are mostly straightforward extensions of the techniques for learning BNssee Appendix C we discuss the differences in Section 6.1. The novel contributions of this chapter arethe application of these algorithms. In Section 6.2.1, we discuss how to use the structural EM algorithm tolearn genetic network topologies from synthetic data FMR98. In Section 6.2.2, we discuss how to usehierarchical HMMs to discover motifs buried in synthetic data. In Section 6.2.3, we discuss how to useabstract HMMs to predict a persons goal as they move through a building, based on real tracking data. InSection 6.2.4, we discuss how to model some real traffic flow data using coupled HMMs KM00. Note thatmost of these applications are more proof of concept rather than serious attempts to solve the respectiveproblems. However, we believe they illustrate the applicability of DBNs.6.1 Differences between learning static and dynamic networks6.1.1 Parameter learningOffline learningTo do offline parameter estimation, we can use the techniques discussed in Appendix C, such as EM. Herewe just comment on some points that are applicable specifically to dynamical systems. Parameters must be tied across timeslices, so we can model sequences of unbounded length. This isstraightforward to handle we simply pool the expected sufficient statistics for all nodes that share thesame parameters. The parameters, , for P X1 are usually taken to represent the initial state of the dynamical systemin this case,  can be estimated independently of the transition matrix. However, if  represents thestationary distribution of the system, the parameters of  and the transition model become coupled. Away to compute maximum likelihood estimates MLEs in this case is discussed in Nik98. LinearGaussian DBNs i.e., KFMs have been studied very extensively. In some cases, closedform126solutions to the MLEs can be found. In particular, for a KFM with no output noise, one can usesubspace methods OM96.1 This is analogous to the fact that PCA can be used instead of EM fornoisefree factor analysis RG99.Online learningTo do online parameter estimation, we simple add the parameters to the state space and then do online inference i.e., filtering. We give some examples in Section 6.2.5. For nonBayesian techniques for recursiveonline parameter estimation in partially observed linear dynamical systems, see LS83, Lju87.6.1.2 Structure learningWhen learning the structure of a DBN, we can learn the intraslice connectivity, which must be a DAG, andthe interslice connectivity, which is equivalent to the variable selection problem, since for each node in slicet, we must choose its parents from slice t 1. If we assume the intraslice connections are fixed, this meansstructure learning for DBNs reduces to feature selection.If the system is fully observed, we can apply standard feature selection algorithms, such as forward orbackwards stepwise selection, or the leaps and bounds algorithm HTF01, p55. Hierarchical priors, whichencode the fact that we only expect a subset of the inputs to be relevant to each output, are discussed inNJ01. These can be used inside a greedy search or an MCMC algorithm.When the system is partially observed, structure learning becomes computationally intensive. One practical approach is the structural EM SEM algorithm see Section C.8.2. It is straightforward to extend this toDBNs FMR98. We give an example in Section 6.2.1. An extension to the structural EM algorithm for DBNsis presented in BFK99. These authors add extra hidden variables when violations of the firstorder Markovcondition are detected. A very similar idea was proposed in McC95 for learning POMDP statespaces.See also ELFK00 for methods to discover hidden variables in static Bayes nets.Learning uses inference as a subroutine, so learning can be slow. We can obviously use standard approximate DBN inference algorithms to speed things up, but there is an additional, natural approximation whicharises in the context of the SEM algorithm Since SEM needs to compute the joint distribution over sets ofnodes which may not all belong to the same clique, one may approximate the joint as a product of marginalsBFK99.1These are implemented in the n4sys function in the Matlab system identification toolbox. For an impressive application of thistechnique to synthesizing dynamic visual textures, see SDW01.1276.2 Applications6.2.1 Learning genetic network topology using structural EMHere we describe some initial experiments using DBNs to learn small artificial examples typical of the causalprocesses involved in genetic regulation. We generate data from models of known structure, learn DBNmodels from the data in a variety of settings, and compare these with the original models. The main purposeof these experiments is to understand how well DBNs can represent such processes, how many observationsare required, and what sorts of observations are most useful. We refrain from describing any particularbiological process, since we do not yet have sufficient real data on the processes we are studying to learn ascientifically useful model.Simple genetic systems are commonly described by a pathway modela graph in which vertices represent genes or larger chromosomal regions and arcs represent causal pathways Figure 6.1a. A vertexcan either be offnormal state 0 or onabnormal state 1. The system starts in a state which is all 0s,and vertices can spontaneously turn on due to unmodelled external causes with some probability per unittime. Once a vertex is turned on, it stays on, but may trigger other neighboring vertices to turn on as wellagain, with a certain probability per unit time. The arcs on the graph are usually annotated with the halflifeparameter of the triggering process. Note that pathway models, unlike BNs, can contain directed cycles. Formany important biological processes, the structure and parameters of this graph are completely unknowntheir discovery would constitute a major advance in scientific understanding.Pathway models have a very natural representation as DBNs each vertex becomes a state variable, andthe triggering arcs are represented as links in the transition network of the DBN. The tendency of a vertexto stay on once triggered is represented by persistence links in the DBN. Figure 6.1b shows a DBNrepresentation of the fivevertex pathway model in Figure 6.1a. The nature of the problem suggests thatnoisyORs or noisyANDs should provide a parsimonious representation of the CPD at each node seeSection A.3.2. To specify a noisyOR for a node with k parents, we use parameters q1, . . . , qk, where qi isthe probability the child node will be in state 0 if the ith parent is in state 1. In the fivevertex DBN modelthat we used in the experiments reported below, all the q parameters except for the persistence arcs havevalue 0.2. For a strict persistence model vertices stay on once triggered, q parameters for persistence arcsare fixed at 0. To learn such noisyOR distributions, we used the EM techniques discussed in Section C.2.3.We also tried using gradient descent, following BKRK97, but encountered difficulties with convergencein cases where the optimal parameter values were close to the boundaries 0 or 1. To prevent structuraloverfitting, we used a BIC penalty, where the number of parameters per node was set equal to the number ofparents.In all our experiments, we enforced the presence of the persistence arcs in the network structure. We128ABCDEABCDEAB CDE0.9 0.80.7 0.60.5baFigure 6.1 a A simple pathway model with five vertices. Each vertex represents a site in the genome, andeach arc is a possible triggering pathway. b A DBN model that is equivalent to the pathway model shown ina. c The DBN model extended by adding a switch node S and an observation node O, whose value eitherindicates that this slice is hidden, or it encodes the state of all of the nodes in the slice.0 10 20 30 40 50 60 70 80 90 10010123456Number of training sequencesHamming distance0 2040600 10 20 30 40 50 60 70 80 90 10010123456Number of training sequencesHamming distance0 204060Figure 6.2 Results of structural EM for the pathway model in Figure 6.1. We plot the number of incorrectedges against number of training slices, for different levels of partial observability. 20 means that 20 ofthe slices, chosen at random, were fully hidden. Top tabular CPDs bottom noisyOR CPDs.1290 10 20 30 40 50 60 70 80 90 10000.511.52Number of training sequencesRelative logloss0 2040600 10 20 30 40 50 60 70 80 90 10000.511.52Number of training sequencesRelative logloss0 204060Figure 6.3 As in Figure 6.2, except we plot relative logloss compared with the generating model on anindependent sample of 100 sequences. Top tabular CPDs bottom noisyOR CPDs.used two alternative initial topologies one that has only persistence arcs so the system must learn to addarcs and one that is fully interconnected so the system must learn to delete arcs. Performance in the twocases was very similar. We assumed there were no arcs within a slice.We experimented with three observation regimes that correspond to realistic settings The complete state of the system is observed at every time step. Entire time slices are hidden uniformly at random with probability h, corresponding to intermittentobservation. Only two observations are made, one before the process begins and another at some unknown time tobsafter the process is initiated by some external or spontaneous event. This might be the case with somedisease processes, where the DNA of a diseased cell can be observed but the elapsed time since thedisease process began is not known. The initial observation is of the DNA of some other, healthycell from the same individual.This last case, which obtains in many realistic situations, raises a new challenge for machine learning. Weresolve it as follows we supply the network with the diseased observation at time slice T , where T iswith high probability larger than the actual elapsed time tobs since the process began.2 We also augment the2With the q parameters set to 0.2 in the true network, the actual system state is all1s with high probability after about T  20, sothis is the length used in training and testing.130DBN model with a hidden switch variable S that is initially off, but can come on spontaneously. Whenthe switch is off, the system evolves according to its normal transition model P XtXt1, S  0, which isto be determined from the data. Once the switch turns on, however, the state of the system is frozenthatis, the conditional probability distribution P XtXt1, S is fixed so that Xt  Xt1 with probability 1.The persistence parameter for S determines a probability distribution over tobs by fixing this parameter suchthat a priori tobs  T with high probability, we effectively fix a scale for time, which would otherwise bearbitrary. The learned network will, nonetheless, imply a more constrained distribution for tobs given a pairof observations.We consider two measures of performance. One is the number of different edges in the learned networkcompared to the generating network, i.e., the Hamming distance between their adjacency matrices. Thesecond is the difference in the logloss of the learned model compared to the generating model, measuredon an independent test set. Our results for the fivevertex model of Figure 6.1a are shown in Figures 6.2and 6.3. We see that noisyORs perform much better than tabular CPTs when the amount of missing data ishigh. Even with 40 missing slices, the exact structure is learned from only 30 examples by the noisyORnetwork.3 However, when allbuttwo slices are hidden, the system needs 10 times as much data to learn. Thecase in which we do not even know the time at which the second observation is made which we modeledwith the switching variable is even harder to learn results not shown.Of course, it will not be possible to learn real genetic networks using techniques as simple as this. Forone thing, the data sets e.g., microarrays are noisy, sparse and sampled at irregular intervals.4 Second,the space of possible models is so huge that it will be necessary to use strong prior domain knowledge tomake the task tractable. Succesful techniques will probably be more similar to computer assisted pathwayrefinement ITR01 than to de novo learning.6.2.2 Inferring motifs using HHMMsMotifs are short patterns which occur in DNA and have certain biological significance see e.g., XJKR02and references therein. In the simplest case, a motif can be represented as a string over the alphabet  A,C,G, T. More realistically, a motif can be represented as a probability distribution over strings of afixed length if each position column is assumed to be independent, this is often called a profile.Given a set of sequences, the problem is discover all the motifs in the set. It is common to wish awaythe model selection problem by assuming the number of motif types, and their lengths, are known already.It is also common to assume motifs do not overlap, and to model the background by a single multinomial,which contains the overall probabilities of each letter in the alphabet. With these assumptions, we can model3What is not clear from the graphs, however, is that noisyORs require many more EM iterations to converge, and each iteration ismuch slower this is because of the extra hidden nodes we introduce to learn the noisyOR CPDs.4See BJGGJ02 for a splinebased interpolation approach which could be useful as a preprocessing step for learning model structure.131the problem by using a 2level HHMM, as shown in Figure 6.4.M1 B M2M11 M12 end B1 end M21 M22 M23 endFigure 6.4 State transition diagram of an HHMM for modelling two types of motifs of lengths 2 and 3, anda singlestate background model.The transition probabilities for the top level of this model encode the probability of transition from thebackground to a motif of type 1 or 2. We assume we cannot go directly from one motif to another. Theselfloop probability for state B1 encodes the expected length of the intermotif background segments byusing a geometric distribution. The output distribution of state M ji represents the profile column for positioni in motif type j.I conducted the following very simple experiment, to see how feasible it is to learn motifs in an unsupervised fashion. I created a set of Ntrain  100 strings, each of length T  20, with each position drawnuniformly at random from . I then embedded a single occurrence of a known motif, say accca, at arandom position away from the boundaries in each string. Here is a sample of the training data.gcggccgtccacccacagttcagcagtgttaacccaggttcgtatacccagctagctcgaI created an HHHM like the one in Figure 6.4, but with a single motif type of length 4. I initializedparameters randomly, but subject to the known state transition topology, and ran EM until convergence. Ithen computed the MAP motif by looking at the most likely output symbol for eachM ji . Even using multiplerestarts, I found that it was almost impossible to recover the true motif.To make the problem easier, I added the prior knowledge that I expected the output distributions of statesin the motif model but not the background model to be nearly deterministic, i.e., each column in the motifshould only emit a single symbol, although I did not specify which one. This can be modelled by using aminimum entropy prior Bra99aP   eHwhere  are the multinomial parameters for a given column, and H  i i log i is the entropy.5With the minent prior, EM reliably recovered the true motif.6 For example, here is the learned profile for5An alternative prior would be to use a mixture of Dirichlets BHK93, with one mixture component for each base, plus a fifthcomponent representing a uniform Dirichlet prior. A more biologically sophisticated prior, which models characteristic shapes in themotif such as the U shape which represented conserved low entropy ends with variable high entropy middles, is discussed inXJKR02.6This example is included in the BNT distribution in BNTexamplesdynamicHHMMMotiflearnmotifhhmm.m.132each state, where the rows represent A,C,G, T in that order, and columns represent the 5 states of the motifsubHMM.0.95977 0.0182 0.0024127 3.9148e05 0.961260.0097752 0.96917 0.97285 0.9954 0.021660.00060423 0.012363 0.023105 0.0003953 0.00536520.029847 0.00026544 0.0016343 0.0041678 0.011715Taking the most probable entry per column indicates that the system has learned the accca pattern.In the future I plan to work with Eric Xing on extending this model so that it can be applied to real motifdata.6.2.3 Inferring peoples goals using abstract HMMsFigure 6.5 Laser range finding data from the ground floor of a house Dark regions correspond to obstacleslike walls and furniture, white regions correspond to free space. Once a map has been learned, any newobstacles which are not in the map are assumed to be people. In this way, the trajectory of a moving personshown as a wiggly blue line can be inferred. Various places where the person likes to spend a lot of timeare labelled, and are considered landmarks or goals. This data is from BBT02.We now show how a DBN can be used to infer peoples intentional states by observing their behavior133K K DRDR515.7528.2514.2526.1550.2500.1560.7498.9DR938.3482.6Figure 6.6 Semisupervised training of a 2level HHMM. The persons goal and x, y positions are observed, but the state, St, and the finished status, Ft, are hidden. K  kitchen, DR  dining room, etc.c.f., Section 2.3.15. The raw data consists of the estimated x, y position of a person as they move throughthe ground floor of a house this is inferred from a series of stationary laser range finders, as shown inFigure 6.5. The aim is to predict which landmark they will go to next. The landmarks are manually chosenlocations within the building, and correspond to top level goal states. In addition to the persons goal, Gt,the actual position of the person, St, is represented using another discrete state variable this is essentially avector quantization of their actual, observed but noisy x, y position, Yt.7 The overall DBN is shown inFigure 2.29.We trained this model using EM from a single training sequence of length T  2000. We automaticallysegmented the sequence into pieces by detecting periods of no motion which simulated the person spendingtime at the goal landmark, and then manually labelled each segment with the its goal endpoint see Figure 6.6. After training, the observed nodes essentially tiled the space where the person walked, as shownin Figure 6.7. The transition matrix for the state nodes, P StSt1, Gt, is like a conditional plan, in thatit specifies what state the person goes to next, given the state they were in and conditioned on their goal.However, it is unlike a plan in that it does not explicitly optimize any cost function. An example is shown inFigure 6.8. The transition matrix for the goal nodes is shown in Figure 6.9. This can be estimated by simplecounting, since we assume the goals are always observed in the training data.To demonstrate that the abstract HMM was better than the HMM at predicting, we compute P Gty1tusing two models, one with the learned P GtGt1 and one with a uniform P GtGt1. We compared thiswith the ground truth, which is available from the handlabelled data. The results are shown in Figure 6.10.The bimodality of the posterior during time 5785 using the uniform bottom model indicates an ambiguitybetween the two goals states 5 TV and 7 study. By using longer range information about what order theperson moves from goal to goal, we can eliminate the ambiguity middle graph in figure, and correctly inferthat the persons goal is state 3 armchair.7By modelling P YtSt as a mixture of Gaussians, we can get some robustness to outliers.1341234567891011 12 13141516171819Figure 6.7 Learned observation distributions P YtSt. Each ellipse represents the coavriance matrix of theGaussian for a given state St.Figure 6.8 Learned state transition matrix P St1St, Gt1  1.1358 kitchen7 TV1 kitchen 2 dining rm6 dining rm5 study3 armchair4 entranceFigure 6.9 Learned goal transition matrix P GtGt1. Only the nonzero arcs are shown.0  0.51  1 29 57 85 113 141 169 197 225 25324680  0.51  1 29 57 85 113 141 169 197 225 25324680  0.51  1 29 57 85 113 141 169 197 225 2532468Figure 6.10 Inferring goals using three different methods. Rows represent states goal locations, columnsrepresent time. Shading represents P Gty1t. First line ground truth. Second line using the learned goaltransition matrix. Third line using a uniform goal transition matrix.1366.2.4 Modelling freeway traffic using coupled HMMsMany urban freeways are equipped with induction loop detectors. These detectors can report three kinds ofinformation in realtime Hal96 the number of vehicles passing the location during a given time intervalflow rate, the fraction of time that vehicles are over the detector occupancy, and the vehicle velocitiesaveraged over the time interval. Here, we will restrict ourselves to aggregated velocity data. See Figures 6.11and 6.12 for a sample of the data we are using, which was collected from I880 in Oakland, California. Fordetails about this dataset, see KCB00.In Figure 6.11, we can clearly see onset, propagation and dissipation of traffic congestion this appearsas the wellknown inverted triangular shape May90. Essentially, traffic gets slower downstream, this effectpropagates upstream, only to eventually disappear. There are various theories which try to explain this kind ofbehavior, based on models of fluid flow, cellular automata, and microscopic computer simulations. But all ofthese approaches have limitations, particularly the need to tune many parameters specific to each individualfreeway.Figure 6.11 The full data set contains the velocity field for 20 weekdays, 27pm, between February 22 andMarch 19, 1993. The measurements come from 10 loop detectors 0.6 miles apart in the middle lane ofI880, with 1 measurement per 30 seconds. Here we plot the first four days, temporally subsampled by 2.The xaxis corresponds to time, and the yaxis to space. Vehicles travel upward in this diagram. The darkestgrayscale corresponds to the average velocity of 20 miles per hour mph and the lightest to 70 mph. Theisolated dark blob on the right edge of the fourth day is probably due to a sensor failure.010203040506070location 2010203040506070location 4010203040506070location 6010203040506070location 8Figure 6.12 The velocity measurements for day 4 at locations 2, 4, 6 and 8. The xaxis is minutes 0 to 300,the yaxis is mph. The sudden drop from 64 to 10 mph at location 6 which corresponds to the dark blob inFigure 6.11 is probably due to a sensor failure.137In this section, we try to learn a model of traffic velocities from data. We assume that there is a hiddenvariable at each of the L detector locations, which takes on K possible values, typically two free flow andcongestion, and that the observed velocity is a noisy representation of the current state. We use a discretehidden variable, since the time series of the average velocity vector is highly nonlinear.The most naive model is to assume each hidden state variable evolves independently, resulting in Lindependent HMMs. However, such a model cannot capture spatial correlation, and hence is incapable ofcapturing the global dynamics, such as the inverted triangle shape. The other extreme is to assume eachvariable at time t depends on all the other hidden variables at time t 1 this results in a single HMM witha state space of size KL, which requires KLKL  1 parameters just to specify its transition matrix. Wetherefore adopt a middle ground, and assume each variable only depends on its local neighbors in space andtime such a model has been called a coupled HMM see Section 2.3.8. We describe the model in more detailbelow.We will estimate the parameters using EM. The M step is straightforward, but the E step is in generalcomputationally intractable. We therefore need to use approximate inference. Below we compare particlefiltering Section 5.2 and the BoyenKoller BK algorithm Section 4.2.1. We then give the results oflearning using exact and approximate inference in the E step. Finally, we discuss the adequacy of the modelin light of our results.The modelAssume there are L loop detector stations indexed by l  1, ..., L, from upstream to downstream. Theobserved aggregated velocity yl,t mph at location l and time t has a distribution that depends only on theunderlying state variable xl,t  S  s1,    , sK. The simplest model is a binary chain with K  2,where the two states sC and sF correspond to congestion and free flow. We initialise the mean for thecongested state to be less than the mean for the freeflow state, although we do not enforce this constraintduring learning.We assume that the hidden process of xt  x1,t,    , xL,t  SL is Markovian and its transitionprobability can be decomposed as P xt1xt Ll1 P xl,t1xt Ll1 P xl,t1xl1,t, xl,t, xl1,t,i.e., the traffic state at a location is affected only by the previous state of the neighboring locations. The initialdistribution on the state is assumed to decompose as P X1 Ll1 P Xl,1. Finally, the observed velocityis a Gaussian whose mean and variance depends on the underlying state at the location P yl,txl,t  sk Nl,k, 2l,k. We need LK3K  1  2K2K  1 parameters to specify the transition model, 2LK tospecify the observation model. and LK to specify the initial distributions. If L  10 and K  2, this is atotal of 88  40  20  148 parameters.Because of the nonstationary nature of the data, we split the 5 hour period of each day into five 1hour138blocks 60 time slices. Then we fit a binary state K  2 CHMM for each block separately, considering 20days as independent replicates of the process. We use days 1,3,...,19 as the training set and 2,4,...,20 as thetest set. Hence we have 10 60 observations to fit 148 parameters per model.InferenceExact inference in this model takes OT L  1KL2 time and space. For L  10, K  2 and T  60,this is about 2 million operations. Although this is tractable, in the future we hope to scale up to modelingcomplex freeway networks with L  100 detectors and K  5 states, so we wanted to examine the performance of approximate inference. We tried both particle particle filtering Section 5.2 and the BK algorithmSection 4.2.1.For particle filtering, we used the optimal proposal distribution, and between Ns  100 and Ns  1000particles. Note that our samples are complete paths from t  1, . . . , T , so we can easily estimate the smoothedjoint posterior P X,t1, Xl,ty1T , where   l 1, l, l 1 are the parents of l we need this quantity tocompute the expected sufficient statistics for EM. For BK, we used the fully factorized approximation, withone variable per cluster.To compare the accuracy of PF and BK relative to exact inference, we computed the smoothed posteriormarginal estimates P Xl,ty1T  using each of the methods on each of the test sequences, and using theestimated parameters. The results for test sequence 2 using the 45pm model are shown in Figure 6.13,using parameters estimated using EMPF. BK yields posterior estimates that are indistinguishable from exactinference to at least three decimal places. PF yields a noisier estimate, but it is still very accurate definel,t to be the L1 difference of the estimates computed using exact and PF then the empirical mean of thisquantity is 0.01390.0093 for 100 particles, and 0.01350.0028 for 1000 particles. We see that using moreparticles slightly increases the accuracy and reduces the variance, but it seems that 100 particles is sufficient.The reason for this is the neardeterministic nature of the transitions see below and the informativeness ofthe observations.Since the inference algorithms perform similarly, we expect the estimated parameters to be similar, too.This is indeed the case for the  and  parameters of the observation model, where the differences are notstatistically significant even using only 100 particles. PF does a poorer job at estimating the transitionparameters, however, due to the fact that it only counts 100 sample paths per sequence. The total normalizedL1 error is 4.9 for BK and 8.0 for PF. Using more particles would obviously help.In addition to accuracy, speed is a major concern. A single E step takes about 1 secondslice using exactinference, about 1.3 sslice using BK, and about 0.1 sslice using PF with 100 particles.8 The reason thatBK is slower than exact in this case is because of the high constant factors, due to the complexity of the8Jaimyoung Kwon implemented PF in SPlus on a 400 MHz PC. Kevin Murphy implemented exact inference and BK in Matlab ona Sun Ultra. The latter code is part of BNT. These times were measured using sequences of length T  60.1390 20 40 6000.510 20 40 6000.510 20 40 6000.510 20 40 6000.510 20 40 6000.510 20 40 6000.510 20 40 6000.510 20 40 6000.510 20 40 6000.510 20 40 6000.51Figure 6.13 Posterior estimates of the probability of being in the freeflow state for the 45pm model ontest sequence 2. Dotted lines represent exactBK which are indistinguishable at this resolution, dotdash isPF with 100 particles, and dashdash is PF with 1000 particles. Plots denotes locations 1 to 10 in leftright,topbottom order. Notice how the change from freeflow to congested takes place earlier in the downstreamlocations.140algorithm, and especially the need to perform the projection marginalisation step. Of course, the asymptoticcomplexity of BK is linear in L, while exact inference is exponential in L, so it is clear that for larger models,BK will rapidly become faster than exact.The learned modelTo aid interpretability of the parameters, we initialised the means for state 0 congestion to be 40 mph, andfor state 1 free flow to be 60 mph. All other parameters were initialised randomly. We ran EM until thechange in loglikelihood was less than 104, which usually took 1020 iterations. Some of the learned  and values using exact inference are shown in Figure 6.14. The parameters for the models for 34pm, 45pmand 56pm are all very similar we call this the rushhour model. The parameters for the models for 23pmand 67pm are also very similar we call this the offpeak model.It is clear that when the traffic is slow, the variance is high, but when the traffic is fast, the variance islow. It is also clear that congestion gets worse as one approaches location 10, which corresponds to the partof I880 that is near the San Mateo Bridge, a notorious bottleneck. Thus the learned  and  values seemsensible. Unfortunately, we were not able to intrepret the transition parameters in any meaningful way. Theestimated parameter values are fairly insensitive to the initialisation and the inference algorithm used in theE step.One of the advantages of a generative model is that we can simulate future traffic patterns. A typicalsample drawn from this model is shown in Figure 6.15. We see that this resembles the training data inparticular, the model is capable of generating the triangular shape.Using the model for predictionOnce trained, we can use the model to do online prediction. For exact inference, we first compute tt P Xty1t  Mtt, where M is the transition matrix of the equivalent HMM we then computeP Yty1t, which is a mixture of Gaussians with tt as the mixing weights. See Figure 6.16 for anexample. We compared these predictions to the naive approach of predicting that the future is the same asthe present, yt  yt, for leads up to 20 minutes ahead. For the sequence in Figure 6.16, the rms error is10.83 for the naive method and 9.76 for the modelbased method. We are ignoring the predicted , i.e., theconfidence in the prediction, which is only available for the modelbased approach. Other sequences givesimilar results. It is clear from these numbers, and from the figure, that our predictions are not very accurate.We discuss ways to improve the model in the next section.DiscussionPerhaps the most serious problem with our approach is that we have learned a separate model for each 1hour period between 27pm, making it tricky to predict across boundaries. One approach would be to use1410 5 10010203040506070block 1, mean0 5 10051015202530block 1, std0 5 10010203040506070block 3, mean0 5 10051015202530block 3, stdFigure 6.14 Maximum likelihood estimates of the mean and standard deviations of the models for block 123 pm and block 3 45 pm. Crosses refer to the congested state, circles to freeflow.100 200 300 400 500 60012345678910Figure 6.15 Data sampled from the learned model for 45pm.the posterior from the previous model as the prior for the next. Alternatively, we could fit a single mixturemodel, by adding an extra hidden variable to each time slice to represent the current regime all the other1420 501520253035404550556065site 70 501520253035404550556065site 80 501520253035404550556065site 90 501520253035404550556065site 10Figure 6.16 20 minuteahead predictions. Solid is the truth, dotted is the naive prediction, dashed in themiddle of the error bars uses the model.variables could then be conditioned on this. In this case, the fully factorized version of BK takes OTLK 5,since the maximum clique size is 5. Such a switching model could capture periodic nonstationarities. Thenumber of regimes could be chosen using cross validation, although our results suggest that two might besufficient, corresponding to rushhour and offpeak.We could also choose the number of hidden states for each location based on crossvalidation. However,it is clear that K  2 is inadequate, since it is incapable of distinguishing whether congestion is increasing ordecreasing. It would be straightforward to use K  2, or to make the model secondorder Markov by addinglonger distance dependencies, or to add extra variables to represent the sign of the derivative of the speed.A third weakness is our assumption of Gaussian noise on the observations. Sensor failures, such as thoseshown in Figure 6.11, clearly invalidate this assumption. We can use a mixture of Gaussians as the noisemodel to handle this.In the future, we plan to try using K  2 with the switching model. We will also include a deterministicnode that encodes the current time thus predictions will be based both on historical patterns using the timenode and the current state of the system using the other hidden nodes. Ultimately, our goal is to predicttravel time, rather than just velocity. We plan to build a model that can take as input the belief state aboutthe current conditions, and combine it with historical supervised data, to provide a realtime travel forecastengine.1430 1 2y1 y2x1 x2Figure 6.17 A DBN for the recursive least squares problem.6.2.5 Online parameter estimation and model selection for regressionHere we discuss how to do online learning in the context of linear and nonlinear regression. This section isbased on Jor02 and AdFD00.Linear regressionSuppose we want to recursively i.e., sequentially, or online estimate the coefficients, , in a linear regressionmodel, where we assume the noise level, R, is known. This can be modelled as shown in Figure 6.17. TheCPDs are as followsP 0  N 0 0,IP ytxt, t  N ytxtt, RP tt1  N tt1, 0We do not need to specify the CPD for xt, since in linear regression, we assume the input is always known,so it suffices to use a conditional likelihood model. The infinite variance for 0 is an uninformative prior.The zero variance for t reflects the fact that the parameter is constant.We can perform exact inference in this model using the Kalman filter, where t is the hidden state, andwe use a timevarying output matrix Ct  xt we set the transition matrix to A  I , the transition noise toK  0, and the observation noise to R. Now recall the Kalman filter equations from Section 3.6.1xtt  Axtt1 Ktyt  CAxt1t1Kt  VttCR1This equation for Kt can be derived using the matrix inversion lemma. Applied to this particular problem,we havett  t1t1  VttxtR1yt  xtt1t1  t1t1  VttR1yt  xtt1t1xtThis equation, plus the update for Vtt, is known as the recursive least squares RLS algorithm if we approximate VttR1 by a constant, this reduces to the least mean squares LMS algorithm Jor02.1440 1 20 1 20 1 2y1 y2x1 x2Figure 6.18 A DBN for sequential Bayesian nonlinear regression.Nonlinear regressionNow consider a nonlinear regression model AdFD00yt kj1aj,txt  j,t  bt  txt  ntwhere nt  N 0, t is a noise term and xt  j,t is e.g., a radial basis function RBF if k  0, thisreduces to a linear regression model. We can write this in vectormatrix form as followsyt  Dt, xtt  ntwhere t  at, bt, t are all the weights. The resulting DBN is shown in Figure 6.18. If we allow theparameters to change over time modelled by a random walk, the CPDs areP tt1  N tt1, IP tt1  N tt1, IP log t logt1  N log t logt1, IP ytxt, t, t  N ytDt, xtt, tI have omitted the priors at time 1 for simplicity.We can do inference in this model using particle filtering.9 In fact, since this model is linear in t, weonly need to sample t and t, and can integrate out t using a Kalman filter see Section 5.3. Alternative,fully deterministic approximations are discussed in Section 4.4.2.Nonlinear regression with online model selectionNow we will let the number of basis functions and hence the size of the state space vary over time AdFD00.Let Kt be the number of bases at time t, and let Mt be a random variable with five possible values Bbirth,9If we do not add noise to the parameters i.e., if   0, the particle filter will not work see LW01 for a possible fix.145K0 K1 K2M1 M20 1 20 1 20 1 2y1 y2x1 x2Figure 6.19 A DBN for sequential Bayesian model selection.Ddeath, Ssplit, Mmerge and Nnochange. These specify the ways in which Kt can increasedecreaseby 1 at each step. Birth means we create a new basis function its position can depend on the previous basisfunction centers, t1, as well as the current data point, xt death means we remove a basis function atrandom, split means we split one center into two, and merge means we combine two centers into one. Weenforce that 0  Kt  Kmax at all times. The DBN is shown in Figure 6.19, and the CPDs are as follows.P Mt  B,D, S,M,NKt1  k  12 , 0, 0, 0,12  if k  0 14 ,14 ,14 , 0,14  if k  10, 13 , 0,13 ,13  if k  Kmax 15 ,15 ,15 ,15 ,15  otherwiseP Kt  kKt1  k,Mt  m k, k  1 if m  B or m  Sk, k  1 if m  D or m  Mk, k if m  NP tt1,Mt  m N tt1, I if m  NN t birtht1,  if m  BN t deatht1,  if m  DN t splitt1,  if m  SN t merget1,  if m  MP tt1,Mt  m N tt1, I if m  NN t growt1,  if m  B or m  SN t shrinkt1,  if m  D or m  MP logt logt1  N log t logt1, IP ytxt, t, t  N ytDt, xtt, tThe function birtht1 takes in the vector of RBF centers and adds a new one to the end, according to someheuristic. In principle, this heuristic could depend on xt as well not shown. Call the new center birth.The confidence associated with birth is yet another free parameter suppose it is Qbirth then the full CPD146would beP tt1,Mt  B  Ntt1 birth,0 00 QbirthThe 0 terms in the covariance matrix do not mean we are certain about the locations of the other centers, onlythat we have not introduced any extra noise, i.e.,Covty1t,Mt  B Covt1y1t1 00 QbirthOf course, we could relax this assumption. If we were fullblooded Bayesians, we would now add priors toall of our parameters such as Qbirth, , etc. these priors would in turn have their own hyperparameterswe would continue in this way until the resulting model is insenstive to the parameters we choose. This iscalled hierarchical Bayesian modelling.Despite the apparent complexity, it is actually quite easy to apply particle filtering to this model. AdFD00suggest a more complex scheme, which combines particle filtering with reversible jump MCMC Gre98.Regular MCMC can only be applied if the statespace has a fixed size. We can use the simpler method ofparticle filtering because we included Mt in the statespace.147Appendix AGraphical models representationA.1 IntroductionProbabilistic graphical models are graphs in which nodes represent random variables, and the lack of arcsrepresent conditional independence assumptions. Hence they provide a compact representation of joint probability distributions. For example, if we have N binary random variables, an atomic representation of thejoint, P X1, . . . , XN , needs O2N  parameters, whereas a graphical model may need exponentially fewer,depending on which conditional assumptions we make. This can help both inference and learning, as weexplain below.There are two main kinds of graphical models undirected and directed. Undirected graphical models,also known as Markov networks or Markov random fields MRFs, are more popular with the physics andvision communities. Loglinear models are a special case of undirected graphical models, and are popular instatistics. Directed graphical models, also known as Bayesian networks BNs1, belief networks, generativemodels, causal models, etc. are more popular with the AI and machine learning communities. It is alsopossible to have a model with both directed and undirected arcs, which is called a chain graph.Figures A.1 and A.2 give a summary of the relationship between various popular graphical models.Please see CDLS99, Jor99, Jen01, Jor02 for more information,A.2 Undirected graphical modelsThe conditional independence statements encoded by an MRF are easy to state XAXB XC iff all pathsbetween all nodes in A and all nodes in B are blocked by some node in C, i.e., there is some interveningc  C on every path between every a  A to every b  B. This is called the global Markov property. Thisimplies that a single nodeXi is independent of all the other nodes in the graph given its neighbors which arecalled Xis Markov blanket this is known as the local Markov property.1Note that, despite the name, Bayesian networks do not necessarily imply a commitment to Bayesian methods rather, they are socalled because they use Bayes rule for inference see Appendix B.148directedHMM KFM other mixturemodelsdimensionalityreductionPCA ICAregressionDBNs  BNGMchain graph dependencyBoltzmannMRFundirectedBNsnetothermaxentmodelsmachineFigure A.1 A partial hierarchy relating different kinds of graphical models. Abbreviations used GM graphical model, BN  Bayes net, MRF  Markov Random field, DBN  dynamic Bayes net, HMM  hiddenMarkov model, KFM  Kalman filter model, PCA  principal components analysis, ICA  independentcomponents analysis, maxent  maximum entropy see Section A.2.2. Dependency nets are defined inHCM00.The joint distribution of an MRF is defined byP x 1ZCCCxCwhere C is the set of maximal cliques2, CxC is a potential function a positive, but otherwise arbitrary,realvalued function on the clique xC , and Z is the normalization factorZ xCCCxC.For example, the model in Figure A.3 representsP X15 1ZX1, X2, X3X3, X4X4, X5The potential functions may be defined in terms of subsets of their arguments, e.g.,X1, X2, X3  X1, X2X2, X3X1, X3If all potential functions are defined on pairs of nodes i.e., edges, the model is called a pairwise MRF, orMRF2. Another example is shown in Figure A.4. In this case, the joint distribution isP x, y  x1, x2x1, x3x2, x4x3, x44i1xi, yi2A clique in a graph is a set of nodes all of which are interconnected. A maximal clique is one which is not a proper subset of anyother clique in the graph. Sometimes the term cliqueis used to mean maximal clique , but we will find it useful to distinguish maximalfrom nonmaximal.149GaussianFactor AnalysisPCAMixture of Factor AnalyzersMixture of GaussiansVQCooperativeVectorQuantizationSBN,BoltzmannMachinesFactorial HMMHMMMixture ofHMMsSwitchingStatespaceModelsICALinearDynamicalSystems SSMsMixture ofLDSsNonlinearDynamicalSystemsNonlinearGaussianBelief Netsmixmixmixswitchreddimreddimdyndyndyndyndynmixdistribhiernonlinhiernonlindistribmix  mixturereddim  reduced             dimensiondyn  dynamicsdistrib  distributed      representationhier  hierarchicalnonlin  nonlinearswitch  switchingFigure A.2 A generative model for generative models. Thanks to Sam Roweis and Zoubin Ghahramani forproviding this figure.150X1 X2X3X4 X5Figure A.3 A simple Markov random field.Y3 Y4Y1 Y2X3 X4X1 X2Figure A.4 A pairwise MRF defined on a lattice. Each hidden variable Xi has its own private observedchild Yi. Technically, this is a chain graph since the arcs from Xi to Yi are directed.In lowlevel vision problems e.g., GG84, FPC00, the Xis are usually hidden, and each Xi node hasits own private observation node Yi, as in Figure A.4. The potential xi, yi  P yixi encodes thelocal likelihood this is often a conditional Gaussian, where Yi is the image intensity of pixel i, and Xi is theunderlying discrete scene label.Lattice MRFs are identical to what physicists call Potts models. They define the probability distributionover nodes using the Boltzman distributionP x1N  1ZeEx1N Twhere T is the temperature and the energy is defined byEx1N   ijJijxi, xjihixiThe notationij means summing over all pairs i, j s.t., i 6 j. If we set Jijxi, xj  ln i,jxi, xj andhixi  ln xi, yi, this becomes identical to a pairwise MRF. The Ising model is a special case of thePotts model with binary random variables. The Boltzmann machine is also a binary pairwise MRF, althoughthe structure need not be a 2D lattice.151A.2.1 Representing potential functionsFor discrete random variables, we can represent the potential functions as tables. To keep the number ofparameters tractable, it is common to make factorization assumptions, e.g.,X1, X2, X3  X1, X2X2, X3X1, X3.For continuous random variables, we can model the potential function as a Gaussian or mixture of Gaussians.For mixed discretecontinuous, we can again use a mixture of Gaussians.A.2.2 Maximum entropy modelsWhen the discrete random variables can have a large number of values e.g., if they represent words, it canbe useful to represent joint probability distributions in terms of features, fiP X1, . . . , XN 1ZexpFi1ifiX1N where Z x1NexpFi1 ifix1N . This is called a Gibbs distribution, or a loglinear model. Wecan represent this as an MRF where we add connections between nodes which cooccur in the domain of thesame feature. However, the graph will not reflect the fact that the potentials may have a restricted form. Forexample, consider the following loglinear modellogP X1, X2, X3  1f1X1, X2  2f2X1, X3  3f1X1, X3 logZThe corresponding graph is a triangle clique on X1, X2, X3, which does not reflect the fact the potentialdoes not have any threeway interaction terms. The factor graph representation discussed in Section A.4makes such restrictions graphically explicit.It turns out that maximum likelihood estimation of the above density is equivalent to maximum entropyestimation of an arbitrary distribution p subject to the expectation constraintsx pxfix  i, wherethe i are known constants e.g., empirical counts. See Ber and Jor02, ch.19 for details. Such modelsare popular in the text processing community see e.g., PPL97, LMP01. See also Goo01 for a way to fitmaxent models by using EM training of HMMs.3A.3 Directed graphical modelsIn a directed graphical model i.e., a Bayesian network, an arc from A to B can be informally interpreted asindicating that A causes B. Hence directed cycles are disallowed, i.e., the graph is a directed acyclic graphDAG. See Figure A.5 for an example.3Unfortunately, these HMMs contain silent loops, making it hard to represent them as DBNs.152CS RWFigure A.5 A simple Bayes net. Ccloudy, SSprinkler, Rrain, Wwet grass. The wet grass can either becaused by rain or by a water sprinkler. Clouds make it less likely the sprinkler will turn on, but more likely itwill rain.a bFigure A.6 Local Markov properties for Bayes nets. a A node X is conditionally independent of its nondescendants e.g., Z1j, . . . , Znj given its parents U1, . . . , Um. b A node X is conditionally independentof all other nodes in the network given its Markov blanket shaded. From RN02.We now define the semantics more formally. As in an MRF, a node in a BN is independent of all theother nodes in the graph given its Markov blanket. However, in the case of a BN, the Markov blanket of anode is the nodes parents, children and childrens parents see Figure A.6.The reason why we include the childrens parents can be explained by considering the example in Figure A.5 but without the C node, so what is left is just the vstructure S  W  R. Let us suppose allnodes are binary, i.e., have values in 0, 1. Consider the Rain node, and suppose it is true R  1 if weobserve that the grass is wet W  1, then it is now less likely that the sprinkler is on than if we did not knowit was raining i.e., P S  1W  1, R  1  P S  1W  1, since the rain has explained awaythe fact that the grass is wet Pea88. Hence R 6 SW , i.e., R is correlated with its childrens parents givenits children. In an MRF, we would have RSW , since W separates S and R however, in a BN, we musttake into account the directionality of the arcs this gives rise to the notion of dseparation d for directedsee Section A.3.1. Hence when converting a BN to an MRF, we must connect together unmarried parentssuch as S and R who share a common child this is called moralization.An alternative definition of independence for BNs, known as the directed local Markov property, is thefollowing a node is conditionally independent of its nondescendants given its parents. If we topologically153Dpa Cpa Name CPD  Multinomial P Y  j  ji  Cond. multinomial P Y  jX  i  Ai, j u Softmax P Y  jU  u  u,W, ji u Cond. softmax P Y  jU  u,X  i  u,Wi, jTable A.1 Some common CPDs for when we have 0 or 1 discrete parents Dpa, 0 or 1 continuous parents Cpa, and the child is discrete. X represents the discrete parent, U represents the continuous parent,and Y represents the discrete child. cond stands for conditional. u,W, j is is the softmax functionsee Section A.3.2. If we have more than one discrete parent, they can all be combined to make one discrete mega parent with exponentially many values more parsimonious representations will be discussed inSection A.3.2.Dpa Cpa Name CPD  Gaussian P Y  y  N y,i  Cond. Gaussian P Y  yX  i  N yi,i u Linear Gaussian P Y  yU  u  N yWu ,i u Cond. linear Gaussian P Y  yU  u,X  i  N yWiu i,iTable A.2 Some common CPDs for when we have 0 or 1 discrete parents Dpa, 0 or 1 continuous parentsCpa, and the child is continuous. X represents the discrete parent, U represents the continuous parent,and Y represents the continuous child. cond stands for conditional. If we have more than one continuousparent, they can all be combined into one large vectorvalued parent.order the nodes parents before children as 1, . . . , N , this means we can write the joint distribution as followsP X1, . . . , XN   P X1P X2X1P X3X1, X2   P XN X1, . . . , XN1Ni1P XiX1i1Ni1P XiPaXiwhere X1i1  X1, . . . , Xi1 and PaXi are the parents of node Xi. This is known as the directedfactorization property. The first line follows from the chain rule of probability, the second line is the sameas the first, and the third line follows because node Xi is independent of all its ancestors, X1i1, given itsparents. For example, the joint implied by Figure A.5 isP C, S,R,W   P CP SCP RCP W S,RThe function P XiPaXi is called node is conditional probability distribution CPD. This can be anarbitrary distribution see Tables A.1 and A.2 for some possibilities. In Section A.3.2, we discuss representations of CPDs which require fewer parameters we will use these extensively in Chapter 2.A.3.1 Bayes ballWe now discuss how to infer global indepenence relationships from a BN using the Bayes ball algorithmSha98, which is equivalent to dseparation Pea88. XAXB XC iff an imaginary ball, starting at any node154Figure A.7 The rules of Bayes ball. Shaded nodes are conditioned on. See text for details.in A, can not reach any node in B, by following the rules in Figure A.7, where shaded nodes correspond tonodes in C. Curved dotted arrows mean that the ball is turned back blocked by a node straight arrowsmeans the ball can pass through. For example, in Figure A.5, we have SRC, since the path via C isblocked since C is shaded and the path via W is blocked since W is not shaded. However, S 6 RC,W ,and S 6 RW , since W will now be shaded this is the explaining away phenomenon.A.3.2 Parsimonious representations of CPDsIf a node Y has many discrete parents, X1, . . . , XN , the number of parameters needed to specify the CPDis OKN , where we assume each Xi can have K values. In this section we consider more parsimoniousrepresentations, i.e., ones which require fewer parameters. Such representations are always easier to learn4,are often easier to intepret, and sometimes lead to faster inference see Section B.6.Mixtures of small multinomialsIf Y is discrete, we can represent P Y X1N  as a mixture of smaller multinomials, as in a mixedmemoryMarkov model Section 2.3.5. In particular, we can define a discrete multiplexer CPD as follows, where Sis a switching parentP Y  yX1N  x1N , S  idefP Y  yXi  xidefAiy, xiSince S is hidden, we can marginalize it out to get the final CPDP yx1N  iP S  iAiy, xiIf we assume Xi and Y can have K values, this CPD requires NK2 parameters to represent, whereas a fullmultinomial would require KN1.4Parsimonious CPDs have lower sample complexity require less data, but often have higher computational complexity requiremore time. Specifically, for many parsimonious CPDs, the M step of EM cannot be solved in closed form hence it is necessary to useiterative methods. For example, softmax CPDs can use IRLS iteratively reweighted least squares, a kind of Newton method, and MLPmultilayer perceptron CPDs can use backpropagation gradient ascent. The resulting algorithm is called generalized EM, and is stillguaranteed to converge to a local maximum, but is usually much slower than regular EM. See Appendix C for details.155X1 X2 X3U1 U2 U3YFigure A.8 A noisyOR CPD, P Y X1, X2, X3, represented in terms of noisy hidden variables Ui and adeterministic OR gate.Contextspecific independenceIf Y is discrete, we can define P Y X1N  using a classification tree, by storing a histogram over Y s valuesin each leaf. If Y is continuous, we can define P Y X1N using a regression tree, by storing the conditionalmean and variance in each leaf. In both cases, if the tree is not full, we will need less thanOKN  parameters.Any tree can be flattened into a table. Parameter learning in this flattened table is equivalent to parameter learning in the tree, if we exploit the fact that some of the entries should be tied. If the tree structureis unknown, it can be learned using standard decision tree methods. Treestructured CPDs can also helpstructure learning FG96.In a nonfull tree, not all parents are always relevant in determining the output child distribution thisproperty is called contextspecific independence CSI BFGK96. This just means a conditional independence relationship which only holds under certain instantiations of some of the conditioning variables. Wediscuss how to exploit this for inference in Section B.6.2.Causal independenceCausal independence arises when a CPD has the special property that the contribution of each parent onthe child can be computed in terms of an associative and commutative operator such as or, sum ormax. A standard example is the noisyOR distribution Pea88. The noisyOR is applicable if the childY and all its parents are binary values in 0, 1. A noisyOR function is like a regular OR function,except some connections between the inputs parents and the output child might fail. Failures areassumed to be independent, and to map parents which have value 1 to value 0 to suppress parents whichare on qi is the probability that the ith input fails in this way. This is illustrated in Figure A.8. We defineP Ui  0Xi  1  qi, P Ui  0Xi  0  1 and P yu1, . . . , uN   y, u1    uN . The overallCPD has the following distributionP Y  0x1N  ixi1qi Ni1qxii A.1It is common to add a leak term, which is a dummy parent which is always on this represents all othercauses. Let q0 to be the probability that the leak is suppressed. Since P Y  1X1N  1  P Y 1560X1N, we may writeP yx1N  1 q0Ni1qxiiyq0Ni1qxii1yIt is easy to generalize noisyOR to noisyMAX, etc. Die93, Hec93, HB94, Sri93, MH97, RD98. Suchmodels have a number of parameters that is linear in the number of parents.Logconcave CPDsWe define a logconcave CPD to be one which has the formP Y  yx1N   Gyjjxjwhere logGy is a concave function. Such CPDs can be exploited by approximate variational inferenceJJ96. This class includes sigmoid CPDs Nea92 and noisyOR CPDs Pea88, both defined for binarynodes. To see this, note that a sigmoid CPD is defined asP Y  1x1N   jjxjwhere x  11ex is the sigmoid logistic function Jor95. In other words,P Y  1x1N  11  euy 11  eu1ywhere u  x, since 1 u  11eu  u. To express a noisyOR CPD in this form, note thatjqxjj  exp logjqxjj  expjxk log qj  ejjxjwhere we have defined j   log qj . Hence the noisyOR CPD can be written asP Y  1x1N   1 euyeu1ySoftmaxThe softmax function MN83 is a generalization of the sigmoid function to the case where the output hasK possible values, instead of just two. In statistics, it is called a multilogistic regression. We defineP Y  jU  u  u,W, jwhereu,W, j expuWjk expuWkand Wk is the kth column of W . When the output is binary, the softmax function reduces to the logisticfunctionu,W, 1 expuW1expuW1  expuW011  expuW0 W1 uw157where w  W1 W0.The softmax function takes a continuous vector u as input. To use it in the case of discrete parents, wemust represent the values of the discrete parents as a vector. We therefore define Xi to be a bit vector oflength K, with value 1 in the position corresponding to the value of Xi, and value 0 in all other positions aoneofK distributed representation X is the concatentation of all the Xi vectors. Given this representation,we can represent P Y X1N  as a softmax even if some of the parents Xi are discrete Pfl98.Alternatively, we might want to use one or more of the discrete parents to specify what parameters touse a conditional softmax function, e.g., P Y  jX1  i, X2  x  x,Wi, j. In BNT, the optionalparameter dps as cts to the softmax CPD specifies which discrete parents should be treated as continuousconverted to distributed form, and which should be used as indices.Conditional linear GaussianWe have already defined the conditional linear Gaussian CLG CPD to beP Y  yU  u,X  i  N yWiu i,iIf we have many discrete parents, we can convert them to continuous vectors P Y  yX1N  N yWX,.For example, suppose N  3, X1 and X3 are binary, X2 is ternary, and Y  IR2. We can think of W asthe concatenation of N smaller weight matrices. If X1  1, X2  3, X3  2, then X1 1 0,X2 0 0 1, and X3 0 1, so the mean is given byWX W 111 W112 W211 W212 W213 W311 W312W 121 W122 W221 W222 W223 W321 W3221000101W 111W 121W 213W 223W 312W 322Another way of writing this isWX iW iXi iW i, Xiwhere W i, Xi is the Xith column of W i. This representation was suggested for factorial HMMs inGJ97, and is commin in the neural network field.Other representationsIt is straightforward to use any function approximator to define a CPD, e.g., multilayer perceptrons feedforward neural networks Bis95, Gaussian processes FN00, etc. However, it is not always easy to usethem for efficient inference or learning.158A.4 Factor graphsFactor graphs fgraphs KFL01 provide a convenient representation that unifies directed and undirectedgraphical models. In a factor graph, there are two kinds of nodes, representing factors local terms andvariables. Variable node Xi is connected to all factor nodes Fi which contain Xi in their domain. Hencefactor graphs are bipartite. Variable nodes are represented by circles, and factor nodes by squares. SeeFigure A.9 for an example.X1 X2 X3 X4 X51,2 2,3 1,3 3,4 4,5Figure A.9 Converting the MRF in Figure A.3 to a factor graph, where we have assumed X1, X2, X3 X1, X2X2, X3X1, X3.A factor graph specifies how a function of many variables can be decomposed into a set of local functions.For example, Figure A.9 specifiesP X15  1,2X1, X22,3X2, X31,3X1, X33,4X3, X43,5X3, X5We can convert an fgraph back to an MRF by adding a link between all nodes that share the same factor. SeeFigure A.11 for an example. Applying this procedure to the fgraph in Figure A.9 recovers Figure A.3. Notethat the MRF representation cannot graphically represent the factorized form that we have assumed for thepotential on clique X1, X2, X3X1, X2, X3  X1, X2X2, X3X1, X3.We can convert a Bayes net to an fgraph as shown in Figure A.10. This encodes the following factorizationP C, S,R,W   P CP SCP RCP W S,RThis fgraph encodes the conditional independence statements corresponding to the moral graph of the Bayesnet, i.e., it does not graphically represent the fact that SRC.Figure A.11 shows how to convert an fgraph back to a BN this uses the following standard trick torepresent soft or hard constraints or relations on a set of nodes, X1, . . . , Xk, we simply make all ofthem parents of a new dummy binary node Y whose CPD is P Y  1X1k  X1k by clampingY  1 i.e., making it observed, we enforce the constraint. Note that convert a BN to an fgraph and backagain does not recover the original BN, because the fgraph only encodes conditional independencies that arepresent in the moral undirected graph.55These transformations are discussed in YFW01.159CS RWFCCFS FRS RFWWa bFigure A.10 Converting a Bayes net a to a factor graph b. The factors are FW  P W S,R, etc.F1 F2X1 F3 X2X3X1 X2X3F1 F2X1 F3 X2X3a b cFigure A.11 Converting a factor graph a to an MRF b or a Bayes net c.A.5 Firstorder probabilistic modelsGraphical models assumed a fixedsize statespace, i.e., a fixed number of random variables. Such modelsare called propositional, since they just assign probabilities to propositions of the form X1N  x1N . Therehas been a lot of work in the AI community on what is called firstorder probabilistic logic FOPL. Below, Iwill briefly review this work. See Pfe00, ch.9 for a more thorough review and list of references.The three main components of a firstorder language are objects, relations and quantifiers. Objectsare basically groups of attributes which belong together, c.f. a structure in a programming language,or an AI frame. To specify the behavior of an object, it is convenient to use universal quantifiers, e.g.,x.humanx  mortalx. This is usually modelled using classes all objects that are instances of thehuman class have the property that they are mortal. Hence classes are just a way of specifying parameter tying,c.f., the way DBNs allow us to define probability distributions over semiinfinite sequences by parameter tyingquantifying over time t.P Xt  iXt1  j  Ai, j. Existential quantifiers e.g., x.enemyx nearbyx in FOPL are handled by checking whether the predicate is true for any object in the current worldstate.Finally, an nary relation can be thought of as a binary random variable R which has n objects as160parents, X1n R is true iff X1n satisfies the relation, e.g., siblingofX1, X2 is true iff X1 is a siblingof X2. Often it is very useful to find all X  s.t. RX,X  is true this is a multivalued function, e.g.,siblingsofX1  X2  siblingofX1, X2  true. If this is guaranteed to be a singlevalued function,we can write e.g.,X1.mother to mean motherofX1, etc. This is sometimes called a reference slot, since itsvalue is a pointer to another object. Structural relational uncertainty means that we are uncertain of the valueof a reference slot. A simple example is data association ambiguity we do not know which object caused theobservation. So we can write P Y Y.cause  Xi  fXi to denote that what we expect to see depends onthe properties of the object we are actually looking at whose identity will generally be uncertain.A.5.1 Knowledgebased model construction KBMCKBMC uses a knowledge base to specify set of rules which can be used to create BN structure on a casebycase basis see WBG92 for an early review. Probabilistic logic programming PLP NH97 is an exampleof KBMC.Situation calculus see e.g., RBKK95 extends first order logic to temporal reasoning by adding a timeargument to all predicates. A similar approach was used in GK95 to extend PLP to the temporal case. Forexample, consider the ruleX,T.aidsX,T  aidsX,T  1  Y.aidsY, T  1  contactX,Y, T  1In Prolog, this is usually written asaidsX,T  aidsX,T1aidsX,T  existsY, aidsY,T1, contactX,Y,T1Given a ground query i.e., one which does not contain any free variables, such as aidsPat,2, the system performs backwards chaining, matching rules whose heads left hand side match the query. This generates two new queries or proof obligations, aidsPat,1 and aidsY,1  contactX,Y,1the latter is then instantianted for each possible value of Y , and the corresponding nodes are added tothe network. If the input facts are aidsAl,11, contactAl,Pat,11, aidsJan,10,aidsPat,10, contactJan,Pat,10, then we create the Bayes net shown below.cAl,Pat,1 cPat,Jan,1aAl,1 aPat,1 aJan,1aPat,2The rules which have the same head are combined in using a mechanism like noisyOR, which canhandle a variable number of parents. We can then apply any standard inference algorithm to the resulting BN.161cAl,Pat,1 cPat,Jan,1 aAl,Jan,1aAl,1 aPat,1 aJan,1aAl,2 aPat,2 aJan,2cAl,Pat,2 cPat,Jan,2aPat,3Figure A.12 A Bayes net created from a temporal knowledge base. Some lines are shown dotted merely toreduce clutter. aX,T is true if objectX has aids at time T  cX,Y,T is true if X and Y had sexual contact attime T . Based on Figure 4 of GK95.If the query becomes aidsPat,3, we create the more complicated network shown in Figure A.12.It is clear that the models created in this way have an irregular structure, which is hard to exploit forefficient inference, especially online inference. Also, KBMC does not allow one to express structural orrelational uncertainty, since the structure of the graph is automatically generated given the rules and thebackground facts. We will address both of these problems in the following sections.A.5.2 Objectoriented Bayes netsObject oriented Bayes nets OOBNs were introduced in KP97. Similar ideas have been proposed inLM97, BW00. The basic idea is that each object has a set of input, output and internal value attributesgiven the inputs and outputs the objects interface, the internal attributes are dseparated from the rest ofthe graph. This, plus the hierarchical structure of the model, allows for more efficient inference than ispossible with KBMCgenerated flat models c.f., XPB93. Furthermore, all instances of a class share thesame parameters, making learning easier LB01, BLN01.FKP98 introduce the concept of a dynamic object oriented Bayes net DOOBN. Each object can eitherbe transient or persistent if it is persistent, it is allowed to refer to the old values of its internal attributes, i.e.,the ones in the previous time slice. The objects can evolve at different time scales, simply by copying slowlyevolving ones less often. Objects can also interact intermittently this can be modelled by adding switchingguard condition nodes, which effectively disable links until certain conditions are met c.f., MP95.The DOOBN can be flattened to a regular DBN in a straightforward way for inference purposes. Unfortunately, unlike the static case, the object oriented structure does not help speedup exact inference, since,as we shall see in Section 3.5, essentially all objects become correlated. However, the structure may be exploitable by certain approximation algorithms such as BK see Section 4.2.1. Note that, if objects evolve atdifferent time scales, the resulting structure will be irregular, as with KBMCs. For online inference, it willoften be necessary to copy all objects at every step.162A.5.3 Probabilistic relational modelsProbabilistic relational models FGKP99, Pfe00 extend object oriented Bayes nets by allowing general relations between objects, not just part of relations. In an OOBN, the inputs to an object are part of theenclosing object the model must be strictly hierarchical. In a PRM, each object can have attributes andreference slots, which are pointers to other objects. A reference slot is just like a foreign key in a relational database. An example might be course.instructor, where course is an instance of the Course class, andinstructor is a pointer to an instance of the Instructor class.Reference uncertainty means the value of the pointer reference slot is unknown. This can easily bemodelled by add all possible objects as parents, and using a multiplexer, as in Section 2.4.6. Alternatively,we can reify a relation into an object itself, e.g., a Registration object might have two pointers, one to aCourse and one to a Student. We can then define a joint probability distribution over the values of the Courseand Student fields, which might depend on attributes of the Course and Student e.g., undergrads might beless likely to take grad classes.PRMs have not yet been applied to temporal reasoning, but it should be straightforward to do so, at leastif there is no structural uncertainty. Unfortunately, the flattened graphs may be less structured than in thecase of DOOBNs, potentially making inference harder. An MCMCbased approach to inference in PRMs isdescribed in PR01.163Appendix BGraphical models inferenceB.1 IntroductionIn this appendix, I give a tutorial on how to do exact and approximate inference in Bayesian networks. Nearlyall of the techniques are also applicable to undirected graphical models MRFs as well. This tutorial bringstogether a lot of material which cannot be found in any one place. I start with the variable eliminationalgorithm, and then show how this implicitly creates a junction tree jtree. The jtree can then be used asthe basis of a message passing procedure that computes the marginals on all the nodes in a single forwardsbackwards pass. I go on to discuss how to handle continuous variables and how to do approximate inferenceusing algorithms based on message passing, including belief propagation and expectation propagation.B.2 Variable eliminationProbabilistic inference means computing P XQXE  xE, where XQ is a set of query variables, and XEis a set of evidence variables. For instance, in medical diagnosis, XE might be the variables representingthe observed symptoms and XQ might be the variables representing causes of these symptoms. This can becomputed from the joint distribution P X1, . . . , XN  using Bayes ruleP XQXE P XQ, XEP XEh6QE P XH  h,XQ, XEh6E P XH  h,XEHence inference boils down to marginalizing joint distributions.If all variables are binary, then computingh P X1, . . . , XN takes O2N  time. We would like to dothis more efficiently. If the joint is represented by a Bayes net, the joint can be written in factored formP X1, . . . , XN  Ni1P XiPaXiMarginalization can then be done efficiently by pushing sums inside of products, as we show in the examplebelow.164AB CD GFFigure B.1 A Bayesian network from KDLC01. The node E is omitted to avoid confusion with theevidence.Consider the Bayes net in Figure B.1. The joint probability distribution can be written asP A,B,C,D, F,G  P AP BAP CAP DB,AP F B,CP GF Suppose we want to marginalize out all the variables from this distribution. Obviously this will give theanswer 1.0 however, the same idea holds for cases where we only marginalize out a subset of the variables.We can write this as followsA,B,C,D,F,GP A,B,C,D, F,G AP ABP BACP CADP DB,AFP F B,CGP GF Working from right to left, we getAP ABP BACP CADP DB,AFP F B,CGF F where GF F  G P GF . Obviously in this case GF F   1 for all F , but this may not be truein general. At the next step we getAP ABP BACP CAFCB,CDP DB,Awhere FCB,C F P F C,BGF F .1 Notice how we put this term next to the summation overC, skipping the summation over D.In general, we try to move terms as far left as possible, so as to minimize the computational workthe restriction is that all of the variables appearing in a term must be in the scope of the appropriate sumoperator. The notation FCB,C means this is a term arising from the summation over F and going1For discrete variables, multiplication should be interpreted elementwise, with the domains of the functions being replicated wherenecessary, e.g., g1B, C,F   g2F  means compute g1b, c, f  g2f for each joint assignment to b, c, f . See HD96 for someof the implementation details on how to perform this efficiently. Multiplying and marginalizing multidimensional arrays is the maincomputational bottleneck in exact inference with discrete variables. For continuous variables, see Section B.5.1.165to the summation over C it goes to C rather than B because C is higher in the elimination summationordering. It should be clear that the order in which we perform the summations determines the size of theintermediate  terms, which can affect the computational complexity substantially we discuss this issue inSection B.3.5. For future reference, we will call this elimination ordering .We can continue in this way until we have computed the desired marginal. The idea of distributingsums over products has been independently invented several times, and has various names peeling CTS78,symbolic probabilistic inference SPI LD94, variable elimination ZP96, bucket elimination Dec98, etc.The idea behind variable elimination can be generalized greatly to apply to any commutative semiringAM00. For example, we can replace sumproduct with maxproduct to get Viterbis algorithm for findingthe most probable assignment to the variables xQ  argmaxxQ P xQxE. Or we can replace sumproductwith minsum, and replace the local termsFi  P XiPaXi with realvalued cost functions fXi, PaXito get a nonserial dynamic programming solution to combinatorial optimization BB72.We can also cast all of the following algorithms as instances of variable elimination, by using the appropriate semiring the Hadamard and fast Fourier transforms KFL01, adaptive consistency for constraintsatisfaction problems CSPs BMR97b, directional resolution DavisPutnam for propositional satisfiability Dec98, various grammar parsing algorithms Goo99, etc.B.3 From graph to junction treeSuppose we want to compute P XiXE for all i 6 E.2 We could call variable eliminationON times, oncefor each i, but that would be unnecessarily inefficient, since we would repeat many of the same computations.In particular, since variable elimination takes ON time, N calls to it would take ON 2 time. We nowdiscuss a way to compute all N marginals in ON time.The basic idea is to store the intermediate  terms that we created while working righttoleft through thesummations, and then to reuse them as we work lefttoright. The  terms will be stored in a tree structuredgraph called a junction tree. In Section B.4, we discuss how to use this tree to compute all the marginals inON time by message passing. But first we must discuss how to create the junction tree. The presentationin this section is based in part on KDLC01, Coz00.B.3.1 EliminationThe computations performed by variable elimination are depicted graphically in Figure B.2 and Figure B.3.Let us call each intermediate  term we create a message, and say that all the terms in the scope of the2This is useful for speech recognition, image reconstruction, decoding messages sent over noisy channels, etc., where we care aboutall the hidden variables. For parameter learning, we need to compute the family marginals P Xi, PaXiE, which is straightforwardgiven P XiE. For structure learning, and a few other tasks, we need to compute P XS E for a potentially arbitrary subset of nodesS. The optimal way to do this, using junction trees, is described in EH01.166G P GF F P F B,C GF F D P DA,BC P CA FCB,CB P BA DBA,B CBA,BA P A BAAFigure B.2 Variable elimination applied to Figure B.1 using the ordering A,B,C,D, F,G. The arrow fromF to C means we compute the message FCB,C F P F C,BGF F  and send it to Csbucket, etc.P GF P F B,C P DA,BP CAP BAP AGF F FGF FCB,CCF B,CCBA,BBCA,BDBA,BBDA,BBAAABAFigure B.3 The elimination tree derived from Figure B.2. Ovals represent buckets the CPDs inside themare the terms they contain after initialisation. Note that the domain of the C bucket is A,B,C, eventhough it is initialized with P CA. The empty circles represent separators.  messages flow from theleaves to the root the A node then  messages flow from the root to the leaves.167AB CD GFFigure B.4 The moral graph of Figure B.1. Dotted edges are moralization arcs. Start with all vertices unnumbered, set counter i  N . While there are still some unnumbered vertices Let vi  i. Form the set Ci consisting of vi and its unnumbered uneliminated neighbors. Fill in edges between all pairs of vertices in Ci. Eliminate vi and decrement i by 1.Figure B.5 Pseudocode for elimination c.f., algorithm 4.13 of CDLS99, p58.  is the elimination ordering,which we use in reverse order.summation operatorXibelong to bucket Bi. Note that the contents of the buckets change over time.However, when we eliminate bucket i i.e., performXi, Bi will be full let us call all the variables in Biat this time Bis domain. For example the domain of bucket C is A,B,C, since it contains P CA andFCB,C when we sum out C.In general, we can compute the domain of each bucket as follows.3 First we moralize the DAG byconnecting together unmarried parents nodes which share a child, and then dropping the directionality onall arcs. The result is an undirected graph with an edge between i and j if there is some term e.g., CPDwhich involves both Xi and Xj  see Figure B.4 for an example. We will call this graph G. Moralizationis unnecessary if we start with an undirected graphical model. Then we apply the algorithm in Figure B.5,which takes as input G and an elimination ordering , and returns a set of elimination sets, C1, . . . , CN . Thedomain of the bucket for Xi is C1i, where 1i is the position of Xi in the ordering.In Figure B.6, we show an example of eliminating with the ordering   A,B,C,D, F,G. In thiscase, we do not need to add any fillin arcs. However, in Figure B.7, we show an example of eliminatingwith the ordering   A,F,D,C,B,G when we eliminate B, we connect all its uneliminated lower3Precomputing the domains is useful for two reasons. Firstly, the computational complexity depends exponentially on the size ofthe largest domain, so we will want to search over summation orderings to try to minimize this see Section B.3.5. Secondly, but moreprosaically, it is very useful to preallocate space to the buckets.168AB CD GFAB CDFAB CDeliminate G eliminate F12 3456Figure B.6 Eliminating the nodes in Figure B.4, with   A,B,C,D, F,G.eliminate GACDFAB CDFeliminate BAB CD GF124563Figure B.7 Eliminating the nodes in Figure B.4, with   A,F,D,C,B,G. Dotted edges are fillin arcs.When we eliminate B, all the remaining nodes are lower in the ordering, and hence all neighbors of B getconnected.169numbered neighbors, which results in the elimination set for B being A,C,D, F this is easily seen fromthe following decompositionA,F,D,C,B,GP A,B,C,D, F,G AP AFDCP CABP DB,AP F B,CP BAGP GF The size of the largest elimination set is called the induced width of the graph. In this case, the width inducedby   A,B,C,D, F,G is 3, which is better than   A,F,D,C,B,G, which induces a width of 4.In Section B.3.5, we discuss ways to search for a good elimination ordering in the sense of minimizing theinduced width.B.3.2 TriangulationThe extra edges we introduce in the elimination process if any are called fillin edges. With the ordering  A,B,C,D, F,G, there are no fillin edges, but with the ordering   A,F,D,C,B,G, we addedfillin edges A  F , D  F and C D when we eliminated B. If we add these fillin edges to the originalgraph G, the result is a triangulated or chordal graph, GT . A triangulated graph is one which does notpossess any cycles of length 4 without a chord which breaks the cycle.A graph is triangulated iff it possess a perfect elimination ordering. This is an ordering such that if weeliminate in that ordering, we will not introduce any fillin edges. An algorithm called maximum cardinalitysearch MCS TY84 can be used to find a perfect elimination ordering given a triangulated graph. AlthoughPea88, p112 suggests using MCS to triangulate a graph, CDLS99, p58 claim that this often results in morefillin edges than necessary. We discuss better ways to find elimination orderings for nonchordal graphs inSection B.3.5. Hence MCS is more suited to testing chordality than to ensuring it.In the current example,   A,B,C,D, F,G is a perfect elimination ordering, which tells us that thegraph in Figure B.4 is already triangulated. If a graph is not triangulated, it can be made so by using theprocedure in Figure B.5.B.3.3 Elimination treesThe elimination sets created during triangulation can be arranged into a tree, called a bucket tree Dec98or an elimination tree etree CDLS99, p.59 see Figure B.8 for an example. We simply connect Ci toCj , where j is the largest index of a vertex in Ci  Vi. Let us call this procedure etreefromesetsalgorithm 4.14 of CDLS99, p59.The nodes in the elimination tree are sets of nodes in the original graphG. In fact, they are cliques of thetriangulated graph, but not necessarily maximal cliques. In Figure B.8, the elimination sets are C1  A,170GFFBCDABCABBAAFigure B.8 An elimination tree derived from Figure B.6. The notation C  A,B means, when C waseliminated, A,B were the uneliminated neighbors.1 253 4Figure B.9 The numbering of the vertices is perfect, but the cliques, numbered as 1, 2, 3, 4, 2, 3, 5,do not satisfy RIP hence this numbering could not have been generated by max cardinality search. FromFigure 4.9 of CDLS99.C2  A,B, C3  A,B,C, C4  A,B,D, C5  B,C, F and C6  F,G, which are clearlynonmaximal cliques of the triangulated graph in Figure B.4.B.3.4 Junction treesThe elimination sets satisfy the running intersection property RIP, which is defined as follows. An ordered sequence of sets C1, . . . , Ck is said to satisfy RIP if, for all 1  j  k, there is an i  j s.t.CjC1    Cj1  Ci, e.g., C4C1, C2, C3  A,B,DA,B,C  C2. Figure B.9 shows anexample of a sequence of sets that does not satisfy RIP.Any sequence of sets C1, . . . , Ck that satisfies RIP can be converted into a junction or join tree jtree,whose nodes are the Cis. A tree is said to be a junction tree, or to have the junction tree property, if C1C2is contained in every Ci i 6 1, 2 on the unique path between C1 and C2, for every pair C1, C2. In otherwords, all the sets containing some Xj form a connected tree along any path, Xj cannot disappear and thenreappear later.We can construct a jtree from a sequence of sets which satisfy RIP by connecting Cj to any Ci, fori  1, . . . , j  1, s.t. CjC1    Cj1  Ci. Let us call this procedure jtreefromRIPclqsalgorithm 4.8 in CDLS99, p55 see also Pea88, p113. Note that this way of constructing a jtree is notvalid unless the order of the sets satisfies RIP.The elimination tree is a jtree of sets, but is not a jtree of maximal cliques. It is conventional to removethe redundant sets.4 We can do this by a simple pruning procedure when we create Cj , we check if there4It is not always a good idea to only use maximal cliques. One advantage of an etree is that Cj  Cij is always a singleton, where1711 21 32 43523Figure B.10 Elimination tree obtained from the chordal graph of Figure B.9 using the numbering 1, . . . , 5,i.e., we first eliminate 5, and ensure lowernumbered neighbors 2, 3 are connected, etc. From Figure 4.10of CDLS99.moralizeprunetriangulateclqsizesetreefromesetsetreeDAGGgreedyjtreeRIPprunejgraphfrommaxclqsjgraphmaxspanningtreemaxcardinalitysearchsim. annealingelimorderGTperfectelimorderelimsetsjtreefromRIPclqsRIPmaxclqsfromMCSmaxclqsRIPmaxclqsnodesizesFigure B.11 Summary of the steps involved in converting a DAG to a junction tree. The cost of a nodeis qi, the number of discrete values it can take on. GT is the triangulated version of G. We now specifythe correspondence between some of the square boxes and algorithms in CDLS99. Greedy is Algorithm4.13. Simulated annealing is another way of trying to compute an optimal elimination ordering see Section B.3.5. maxcardinalitysearch is Algorithm 4.9. RIPmaxclqsfromMCS is Algorithm4.11. RIPprune is Lemma 4.16. jtreefromRIPclqs is Algorithm 4.8. etreefromesetsis Algorithm 4.14. See text for details.172BCFABCABDFGFigure B.12 A jtree for Figure B.1 constructed using the elimination ordering A,B,C,D, F,G. Comparewith the etree in Figure B.8.is any higher numbered already created clique, Cj  Ci for i  j, and if so, we simply omit Cj from theoutput sequence. Unfortunately, the resulting sequence may no longer satisfy RIP. For example, consider theelimination tree in Figure B.10, with sequence of elimination sets C1  1, C2  1, 2, C3  2, 3,C4  3, 4 and C5  2, 3, 5. If we delete C3, we violate RIP.There is a way to remove redundant sets and keep RIP which we shall call RIPprune see Lemma 4.16of CDLS99, p60 for details. Alternatively, we can use max cardinality search to find a perfect eliminationordering, and then use Algorithm 4.11 of CDLS99, p56 which we call RIPmaxclqsfromMCSto find the maximal cliques of the chordal graph, in an order that satisfies RIP. Either way, once we havea sequence of maximal cliques that satisfies RIP, we can use jtreefromRIPclqs to create a jtreeof maximal cliques. For example, see the jtree Figure B.12 compare this with the etree in Figure B.8. Ingeneral, a jtree may be much smaller than an etree.Fortunately, there is a much simpler way to construct a jtree from a set of maximal cliques, even if theirordering does not satisfy RIP. We first create a junction graph, in which we add an edge between i and j ifCiCj 6 . We set the weight of this edge to be CiCj , i.e., the number of variables they have in common.Any maximal weight spanning tree MST of the jgraph is a junction tree see JJ94a or AM00 for a proof.We can construct an MST using Kruskals algorithm in OE log E time, or using Prims algorithm inON2 time CLR90. BNT uses Prims algorithm, because Kruskals requires a priority queue, which isinefficient in Matlab. The best MST is the one which minimizes the sum of the costs on each edge, wherethe cost of an edge e  v, w is defined as e  qv qqqvw. qv is the cost of node v in the jgraph, andis defined as qv iv qi, where qi is the number of discrete values Xi can take on. If we use e to breakties when constructing the MST, the result will be an optimal junction tree for a fixed elimination orderingJJ94a.The overall procedure for converting a DAG to a jtree is summarized in Figure B.11.Cij is the intersection between two neighboring cliques. This means that we only have to marginalize one variable at a time duringmessage passing see Section B.4. When variables can be of mixed types, e.g., random variables and decision variables in an influencediagram, this can result in simpler algorithms see e.g., CDLS99, ch.8. GLS99 also gives cases where it is better not to require thatthe cliques be maximal, in the context of constraint satisfaction problems.173B.3.5 Finding a good elimination orderingWe have mentioned several times that the cost of inference is exponential in the size of the largest clique term bucket  elimination set. We would like to minimize the max clique size. However, this problem isNPcomplete Yan81, ACP87. A standard greedy approximation is the following eliminate any node whichwould not result in any fillins i.e., all of whose uneliminated neighbors already form a clique if there isno such node, eliminate the node which would result in the minimum number of fillin edges. This is calledthe minfill heuristic. An alternative that works better in practice is the minweight heuristic, where wetry to minimize the weight of the created cliques, where the weight of a clique C isiC qi, where qi is thecardinality of Xi. If all nodes have the same weight, this is the same as minfill.Of course, many other methods are possible. Kja90, Kja92 compared simulated annealing with theabove greedy method, and found that it sometimes works better although it is much slower. MJ97 approximate the discrete optimization problem by a continuous optimization problem. BG96 present a randomizedapproximation algorithm. See Ami01 for various recent constantfactor appoximation algorithms.B.3.6 Strong junction treesWhen we consider graphical models with more than one type of node we sometimes have to use a constrainedelimination ordering. For example, in an influence diagram, we must max out a decision node before wesum out the random information variables that it depends on JJD94. Also, in a conditionally GaussianCG Bayes net see Section B.5, if we want to get exact answers we must integrate out a continuous nodebefore marginalizing out the discrete parents it depends on, i.e., we writeix fx, i,i, rather thanxi fx, i,i. In general, if we have a partial ordering, where i  j means we must eliminate ibefore j, we try to find the best elimination ordering consistent with the partial ordering. A valid eliminationordering  will such that the reverse of  is a total ordering of . The reason why we consider the reverseof  is that we work backwards recall from Section B.2 that if   A,B,C,D, F,G, we first summedover G. In the case of CG networks, we ensure that all the continuous nodes are eliminated before any ofthe discrete nodes. This results in a strongly triangulated graph, which can be converted to a junction tree ofnot necessarily maximal cliques in the usual way e.g., using max spanning tree.In addition to using a restricted elimination ordering, we must choose the root R to be a clique which isstrong. A root R is strong if, for any pair C1, C2 of adjacent cliques with C1 closer to R than C2, thereexists an ordering ofC2 that respects, and withC1C2  C2C1 JJD94. In the case of CG networks, wehave   , where  is the set of Gaussian variables and  is the set of discrete variables, so this definitioncan be restated as follows Lau92C2  C1   or C1C2  174In other words, if the residual C2  C1 novel variables further from the root contains a discrete variable,then the separator is purely discrete. Roughly speaking, the tree becomes more and more discrete as we movetowards the root.It can be shown that any strongly triangulated graph has a jtree with a strong root. Furthermore, thelast clique C1 created by the standard onestep lookahead strong triangulation routine see Figure B.5 is astrong root, and this will obviously not get pruned out.B.4 Message passingWe now explain how to compute all N marginals in ON time by using the junction tree. We will presentseveral different algorithms from the literature, all of which are essentially the same.B.4.1 InitializationLet clique i have domain Si, and let Sij  SiSj be the separator between cliques i and j. Let us associatea potential with each clique. A potential is a nonnegative function of its arguments if the arguments arediscrete, we can represent potentials as multidimensional arrays if the arguments are continuous, we maybe able to use a parametric representation such as a Gaussian or other member of the exponentialy family ifsome arguments are discrete and some are continuous, we can use mixtures of Gaussians see Section B.5.2.We initialize potentials to be the identity element for the semiring we are considering, which is 1 in the caseof sumproduct with discrete potentials.Each term Fi in the original problem the CPD for Xi in the case of Bayes nets is associated with aclique Sj that contains all of Fis domain. In an etree, we have one clique per term see Figure B.3, but ina jtree, we may have to associate multiple terms with the same clique, e.g., in Figure B.12, we can associateP A and P BA with cliques ABC or ABD. Hence the clique ABD may contain terms P A, P BAand P DA,B. All the terms associated with a clique are multiplied together elementwise to get a singleinitial clique potential, j .B.4.2 Parallel protocolPerhaps the simplest formulation of message passing, which is often called belief propagation, is as followsAM00. At each step, every node in the jtree in parallel collects all the messages from its neighbors andcomputesj inbrjijjwhere ijSij is the message sent from node i to node j, and jSj is the term assigned to node j theinitial clique potential. Once a node has received messages from all its neighbors, it can then send a message175function ,   collectF for each j in postorderj iassj Fi ipredj ijk  parentjjk  j  Sjkfunction ,   distribute, for each j in preorderk  parentjj  normalizej  kjfor each child iji jSijijFigure B.13 Pseudocode for the JLOGDL algorithm using a serial updating protocol. JLO  Jensen,Lauritzen and Olesen JLO90 GDL  generalized distributive law AM00. Calibrating the jtree meanscalling collect followed by distribute. We assume a product over an empty set returns 1. assj are theterms CPDs assigned to clique j. Postorder means children before parents, preorder means parents before children predj means predecessors of j in the postorder. j has domain Sj , and jk has domainSjk  SjSk. This code is designed to be as similar as possible to belief propagation on a factor graphsee Figure B.20. See Figure B.14 for a slightly different implementation. Normalization is one way toprevent numerical underflow. It also ensures that, upon termination, the clique potentials can be interpretedas conditional probabilities j  P Sj e, where e is all the evidence.to each of them, as followsjk inbrj,i6kijj  Sjk jkj Sjk j  Sjkkjwhere the notation S  T meansST S, i.e., marginalization projection onto set domain T . Fordiscrete variables, division is elementwise, but we define 00  0 for division of continuous variables, seeSection B.5.1. If we initialise all messages to the identity element 1s, each node can apply the above rulesin parallel. After D steps, where D  N is the diameter of the graph, the final j values will be proportionalto P Sj e, where e is the evidence. For a proof of this, see Section B.4.7. Since the final j values will benormalized, we are free to scale the messages arbitrarily, since the scaling factors will cancel at the end thisis very useful to prevent underflow in large networks and iterative algorithms.B.4.3 Serial protocolIn the above parallel protocol, all nodes are performing a computation at every step, so on a serial machine,it would take ON2 time to get the exact answers for a tree. Other message passing schedules are possiblesee AM00 for conditions. We now present a particularly simple serial protocol which can compute allmarginals in two passes over the tree, i.e., in ON time see Figure B.13. The algorithm has two phasescollect to root, and distribute from root. The root node can be chosen arbitrarily. Performing collect andthen distribute is sometimes called calibrating the jtree. In the collect phase, a node collects messages from176 For each j in postorder j iassj Fi Let k be the parent of j in the jtree. jk  j  Sjk k  k  jk For each j in preorder For each child i of j i iij ij  i  Sij i  i  ij Normalize each j .Figure B.14 An alternative implementation of the JLO algorithm. This is how it is implemented in BNT.The disadvantage of this method is that node j does not update its local potential j , but instead updates thepotentials of its parent k or children i. Also, the similarity to BP on a factor graph see Figure B.20 isobscured in this version.all its children, and then passes a message to its unique parent, i.e., nodes are processed in postorder. In thedistribute phase, a node collects a message from its unique parent, and passes it to all its children, i.e., nodesare processed in preorder.Consider a generic node j with children i and i and unique parent k. In the collect phase, kj  1,since all messages are initialized to 1s. Hence we can skip the division by kj , so the update rules simplifiesas followsj  ichjijjjk  j  SjkThis corresponds exactly to the variable elimination algorithm computing j is what happens when wecombine all the terms in bucket j, and computing jk corresponds to marginalizing out Sj  Sjk , which,for an etree, is always a single variable, namely Xj .In the distribute phase, node j computes its new potentialj  j  kjwhere k is the parent of j. Then it sends the following message to its child iji ichj,i 6iijkjj  Sij jij Sij j SijijThe overall algorithm is summarized in Figure B.13.177B.4.4 Absorption via separatorsThe most common presentation of message passing in junction trees see Figure B.14 looks different fromwhat we have just seen, but we now show that it is equivalent. The basic operation is absorption if nodeclique i is connected to j via a separator s, then we say j absorbs from i ifj  jsswhere s  i  s. The asterisk denotes updated potential.The potential on a separator is equivalent to what we have been calling a message. The potential on Sijstores ij if it was just updated by i, or ji if it was just updated by j.Suppose we want to update j to reflect the new message arriving from i, ij . The new potential canbe computed asj  inbrj,i 6iijijj jijij  jijijThis is what is usually meant by the junction tree algorithm, also known as clustering, the Hugin algorithm the name of a famous commercial program that implements it, the JLO algorithm JLO90, cliquetree propagation, etc. However, it is useful to distinguish the junction tree as a data structure from the kindof message passing that is performed on it. Also, it is useful to distinguish between a jtree of maximal cliquesas used by Hugin, a jtree of elimination sets an etree, binary jtrees She97, jtrees of maximal primesubgraphs OM99, etc.B.4.5 Hugin vs ShaferShenoyThe version of message passing discussed in Section B.4.4 is often known as the Hugin architecture. Itstores the product of messages at each clique and uses division to implement the product of allbutone of themessages. An alternative is known as the Shafer Shenoy architecture SS SS88, SS90a, SS90b, She92,which does not does not store the product of messages at each clique and does not involve a division operator.It might seem that division for Gaussian distributions is not welldefined, but in fact it is see Section B.5.1.However, mixtures of Gaussians cause problems see Section B.5.2.Hugin uses more space because it must store clique and separator potentials, but less time because itdoes not need to repeatedly remultiply messages. The need to repeatedly multiply messages in SS can beameliorated using binary join trees She97, SS98. This supposedly makes SS about as fast as Hugin. SeeLS98 for a detailed comparison of the SS and Hugin architectures.Lazy junction trees MJ99 only multiply together those terms which are assigned to a clique on ademandbasis, i.e., if the terms contain a variable which is about to be marginalized out. In other words, lazyHugin maintains clique potentials as a product of factors. Division is implemented by multiplying allbutoneof the relevant factors, as in SS.178X1 X1 X1, X2 X2 X2, X3 X3 X3, X4Figure B.15 A junction tree for a Markov chain X1  X2  X3  X4 square nodes represent separators,ovals represent cliques. The X1 clique and separator is not strictly necesssary, but is included to ensure a11 correspondence between separators and nodes in the original graph. The clique potentials are P X1,P X2X1, P X3X2, P X4X3, etc.B.4.6 Message passing on a directed polytreeBelief propagation was first presented in Pea88 for directed polytrees. A polytree is a directed tree withmore than one root hence a node can have several parents as well as several children, but the overall graphhas no cycles, directed or undirected. We now show that this is equivalent to the message passing scheme wehave just presented, in the case that the original graph is a directed chain the general polytree case followsanalogously.If the DAG is a chain, the cliques of the corresponding jtree will be families nodes plus parents, and theseparators will be nodes. See Figure B.15 for an example. In the JLOGDL algorithm, cliques sent messagesto cliques via separators. Now we will let separators send messages to separators via cliques. Messages fromchildren to parents are called  messages, and the messages from parents to children are called  messagessee Figure B.3.Consider a node j with unique child i and unique parent k. Since j  P jk and Sjk  k, the message isjkk kijjP jkand the  message isjij kkjkP jkNote that this is equivalent to the forwardsbackwards algorithm for HMMs, where    and   SHJ97. Also, note that the  and  messages can be computed independently of each other. However,this is only true for a chain for a tree, the s depend on the s computed in a bottom up pass, and for apolytree, the s and s both depend on each other in the following wayjkk iijjk 6kkjkP jk, k  Sjkj,kjjP jk, kk 6kkjkwhere jj ichj ijj, andjij i 6iijjkkj k P jk, k  Si,j jji 6iijj179Ejji i rFigure B.16 Conditioning onXj divides the tree into the two sets, one which contains evidence in the subtreerooted at Xj , which we call Ej , and the other which contains all the other evidence, which we call Ej .where jj k,k P jk, kk kj k.The final beliefs are given by P Xj e  jj . See Pea88 for a proof.Cutset conditioningOne of the earliest approaches to exact inference in DAGs with undirected cycles was cutset conditioningPea88. This works by finding a set of nodes, called a cutset, which, when removed, turns the remaininggraph into one or more polytrees. Exact inference can be performed by instantiating the cutset to each of itspossible values, and running message passing in each polytree, and combining the results PS91. Althoughintuitively appealing this algorithm is similar to reasoning by cases, it takes time exponential in the size ofthe cutset, which is always at least as large as the tree width max clique size. Hence cutset conditioning isnever any faster than message passing in a junction tree SAS94, and is not widely used.Applying Pearls algorithm to jtreesTo apply Pearls algorithm to a jtree, we root the tree, and treat the cliques as meganodes. To do this,we must compute P jk for neighboring cliques j and k. For discrete nodes, this can be done by ensuringP jk  0 if j and k are inconsistent on shared values, and otherwise multiplying together the CPDsassigned to clique j see Zwe98 for details. Note that this transformation may destroy any structure apartfrom 0s in the local CPDs. Also, it is not clear how to do this for linearGaussians CPDs.B.4.7 Correctness of message passingWe have already remarked that the collectevidence procedure is identical to variable elimination, which isobviously correct. The question remains of why message passing in general is correct.If the original Bayes net is a polytree i.e., has no undirected cycles, Pearl Pea88 proved that message passing is correct by using dseparation properties of the DAG. The basic idea is that by condition180ing on a node Xj , we partition the tree into two sets, one of which contains the evidence in the subtreerooted at Xj , call it Ej , and the other which contains the remaining evidence, Ej  see Figure B.16. NowP Xj Ej , Ej   P Xj Ej P Ej Xi, so we can compute j  P Xj Ej  and j  P Ej Xjseparately and then combine them.For junction trees, things are trickier, because the nodes represent sets of random variables. However, thefact that we only assign each term Fi to a unique clique means that we do not overcount information. Also,the junction tree property ensures that when two or more subtrees send a message to a node, their informationcan be fused consistently. See AM00, app.A for a simple inductive proof of correctness. Historically, thefirst proof of correctness for general semirings was SS88 for the SS architecure and LJ97 for the Huginarchitecture.B.4.8 Handling evidenceThe standard implementation of the junction tree algorithm see e.g., CDLS99 is to compute the initialclique potentials j by multiplying all the CPDs assigned to clique j, and then to calibrate the tree. Theclique potentials now represent unconditional joint distributions j  P Sj marginals over the variablesin the clique.Now suppose hard evidence arrives on node Xi, i.e., we observe Xi  xi. We find any clique thatcontains Xi, and set the elements of the clique potential that are inconsistent with the evidence to 0, e.g., ifwe have a clique containing two binary nodes,X1 andX2, and we observeX2  1, then we set x1, x2  0if x2 6 1. Note that many of the potentials may end up being sparse. If we have soft virtual evidence, wesimply reweight the terms. We then perform another calibration to propagate the evidence.When dealing with Gaussian distributions see Section B.5.1, adding evidence to a node reduces thesize of the Gaussian potential, so we must add the evidence to every potential that contains an observednode since we cant have 0s in the covariance matrix. This can be done in the discrete case too, but isunnecessary, since 0s will be propagated.The above scheme will fail if we have a model with conditionalonly CPDs, e.g., a regression model withtwo nodes, X and Y , where we specify P Y X but not the prior P X, since we cannot evaluate P Y Xuntil we knowX . However, there is a simple solution first proposed in Mur99 delay converting the CPDsto potential form until after the evidence arrives. Hence we eliminate the initial calibration phase completely.This has the additional advantage that we know that discrete observed variables effectively only have onepossible value, and continuous observed values are zerodimensional points i.e., just scale factors thismeans we can allocate potentials of just the right size no need for zero compression or shrinking of Gaussianpotentials.181B.5 Message passing with continuous random variablesSo far, we have assumed that potentials are represented as tables multidimensional arrays multiplicationand division are defined pointwise, and marginalization is just summation over the appropriate dimensions.We now discuss what to do if some of the variables are continuous. We start by assuming all CPDs are linearGaussian, in which case the joint distribution is just a large, sparse multivariate Gaussian. In this case, exactinference always takes at most ON 3 time, since we can just create the corresponding Gaussian. However,N might be very large, rendering this approach impractical.Then we consider the case where some CPDs are conditional linear Gaussian CLG, i.e., of the formP Y  yU  u,X  i  N yWiui,i. We also allow discrete CPDs of any kind, but only if theyhave no continuous parents. The result is just a a mixture of sparse Gaussians, where the mixture distributionis factorized as in a discrete Bayes net, and the sparsity pattern can vary between mixture components. Thisis called a conditional Gaussian CG distribution LW89, Lau92, Ole93, CDLS99.Finally we consider arbitrary combinations of CPDs, where exact inference is usually not possible.B.5.1 Pure Gaussian caseMoment and canonical characteristicsWe can represent a Gaussian distribution in moment form or in canonical information form. In momentform we havex p, ,  p exp 12 x  1x where p  2n212 is the normalizing constant that ensuresx x p, ,  1. n is the dimensionality of X . Expanding out the quadratic form and collecting terms we get the canonical formx g, h,K  expg  xh 12xKxwhereK  1h  1g  log p 12KK is often called the precision matrix. We can write the above in scalar formx g, h,K  expg ihixi  12ikKijxixjNow XY Z iff the density factors as P X,Y, Z  gX,ZhY, Z for some functions g and h. Hencewe see that XiXj rest iff Kij  0.182Note that we can only convert from canonical to moment form if K is positive semi definite psd, whichit need not be in general see Section B.5.1. Similarly, we can only convert from moment to canonical formif  is psd but this will always be the case, unless we have deterministic linear CPDs in which case   0.Note that potentials need not be probability distributions, and need not be normalizable integrate to 1. Wekeep track of the constant terms p or g so we can compute the likelihood of the evidence.Outline of algorithmFor each nodeXi, we convertXis CPD to a potential, Fi if there is evidence on any ofXs family, we enterit into Fi we then multiply Fi onto a clique potential containing Xs family. Then we calibrate the jtree asbefore, but using the new definitions of multiplication, division and marginalization.We must represent the initial potentials in canonical form, because they may represent conditional likelihoods, not probability distributions c.f., the distinction between  and  in an HMM. For example, considerFigure B.15. The initial clique potential for 1, 2 will be P X1P X2X1  P X1, X2, which is a joint pdf,but the initial potential for 2, 3 will be P X3X2, which is a conditional likelihood. Conditional likelihoodscan be represented in canonical form, but not in moment form, since the unconditional mean does not exist.We can only compute EX3 from P X3X2 once P X2 becomes available. This will be explained inmore detail below.Since the potentials are initially created in canonical form, we perform all the other operations multiplication, division and marginalization in this form, too. However, at the end, we usually convert to momentform, since that is easier for the user to understand, and is also a more convenient form for learning wesimply sum the means and covariances. Note that when this algorithm is applied to a Kalman Filter Model,the result is identical to the information form of the Kalman smoother.Converting a linearGaussian CPD to a canonical potentialFor a vector node, the conditional distribution has the formfxz  c exp 12x BT zT 1x BT z exp 12x z 1 1BTB1T B1BTxzx z 1B1 12T 1 log cwhere c  2n212 . Hence we set the canonical characteristics tog   12T 1n2log2 12 log h 1B1K 1 1BTBT B1BT. IB1I B183This generalizes the result in Lau92 to the vector case. In the scalar case, 1  1 so  represents thevariance, not the standard deviation,B  b and n  1, so the above becomesg 22 12 log2h 1bK 11 bTb bbT.Once we have the canonical characteristics, we can compute the initial potentials for each clique by multiplying together the potentials associated with each variable which is assigned to this clique. Unfortunately,we cannot convert these canonical characteristics to moment characteristics because K is not of full rank,and hence is not invertible.Entering evidence into a canonical potentialIf we observe that a continuous variable y takes on a specific value y, we must modify the potentials of all thecliquesseparators that contain y, since their dimensionality will be reduced. Consider a clique with domainx, y. The new potential isx  expg xT yThXhY 12xT yTKXX KXYKY X KY Yxy expg  hTY y  12yTKY Y y xT hX KXY y 12xTKXXxThis generalizes the corresponding equation in Lau92 to the vector case.Multiplication and division of canonical potentialsIn the discrete case, we use multiplication and division to update potentials when new evidence arrivesPrW  PrWPrSPrS, where S is a separator and W is a clique. Notice that PrWPrS  PrW S, so we are reallycomputing a conditional distribution on the fly and multiplying in new information.We can define multiplication and division in the Gaussian case in terms of canonical characteristics, asfollows. To multiply 1x1, . . . , xk  g1, h1,K1 by 2xk1, . . . , xn g2, h2,K2, we extend them both tothe same domain x1, . . . , xn by adding zeros to the appropriate dimensions, and computeg1, h1,K1  g2, h2,K2  g1  g2, h1  h2,K1 K2Division is defined as followsg1, h1,K1g2, h2,K2  g1  g2, h1  h2,K1 K2184Marginalization of a canonical potentialLet W be a potential over a set W of variables. We can compute the potential over a subset V  W ofvariables by marginalizing, denoted V WV W . Lety y1y2, h h1h2, K K11 K12K21 K22,with y1 having dimension p and y2 having dimension q. It can be shown thaty1y1, y2 g, h,K  y2 g, h, Kwhereg  g  12p log2 log K11 hT1K111 h1h  h2 K21K111 h1K  K22 K21K111 K12B.5.2 Conditional Gaussian caseIn the CG case, potential functions can be represented as a list of canonical characteristics, one for eachpossible combination of discrete values. In addition to g, h and K, we store i, which is a multiplicativeconstant outside of the exp operator if i  0, it means the ith element has probability 0 otherwise weset i  1. All the operations go through as before, except for marginalization, which we discuss below.Strong marginalizationIf we marginalize out over some continuous nodes, we can proceed as in Section B.5.1 above, once for eachvalue of the discrete nodes, i.e.,y1i, y1, y2 gi, hi,Ki  i, y2 gi, hi, KiIf we marginalize out over some discrete variables, say j, we distinguish two cases. First, if h and K donot depend on j, i.e., hi, j  hi and Ki, j  Ki, then we defineji, j, y ji, j expgi, j  hiy  12yKiy exphiy  12yKiyji, j exp gi, jHencegi  logji,j1exp gi, j, hi  hi, Ki  Ki.If h or K depends on j, we need to perform weak marginalization, which we discuss next.185Weak marginalizationCG potentials are not closed under addition. For example, suppose i and j have K possible values each, soy, i, j is a mixture of K2 Gaussians. When we marginalize out j, we are still left with a mixture of K2Gaussiansjx, i, ji,j ,i,j jx, ii,j ,i,jThis cannot be simplified any further, and must be kept as a list of terms. If this is then multiplied byanother mixture of K Gaussians, we end up with K3 terms. One way to stop this exponential explosion isto collapse the mixture of K2 Gaussians to a mixture of K Gaussians. We would like to do this in anoptimal way, i.e., minimize the KL distance between the true distribution,j x, j, i, and the approximatedistribution, x, i, on a case by case basis i.e., for each i. We can do this by moment matching for eachi. This requires that we convert  to moment form, and hence requires that Ki, j be psd. We then proceedas follows.pi jpijpij  pijpii jij piji jij pij jij  i ij  iT pijIf we use a strong junction tree see Section B.3.6, then all marginalizations on the collect phase arestrong, hence we do not inherit any errors. Hence the above operation will give the correct mean andvariance. However, even though the first two moments are correct and hence this is the optimal single Gaussian approximation, they may not be meaningful if the posterior is multimodal. An expensive alternativeis to use computer algebra to compute the exact posterior symbolically CGH97, Cur97. The paper CF95,whose title is Symbolic Probabilistic Inference with both Discrete and Continuous Variables, sounds likeits doing this, but in fact SPI is just another name for the variable elimination algorithm, which is numerical.Note also that a special case of these rules were derived for Pearls polytree algorithm in DM95.Computational complexity of exact CG inferenceLSK01 prove that inference in CG networks is NPhard, even if the structure is a polytree, such as the onein Figure B.17. The proof is by reduction from subset sum, but the intuition is as follows the prior P X1 isa mixture of K Gaussians, depending on S1 when we marginalize out S1, we are still left with a mixture ofK Gaussians each one of these gets passed through K different matrices, depending on the value of S2,resulting in a mixture ofK2 Gaussians, etc in general, the belief state at time t is a mixture ofK t Gaussians.186S1 S2 S3X1 X2 X3Figure B.17 A switching Markov chain. Square nodes are discrete, round nodes are continuous.The problem above arose because we used a strong jtree. This requires us to eliminate all the discretenodes before any of the continuous nodes hence we are forced to form one clique which contains all thediscrete nodes.5 Unfortunately, in the case of hybrid DBNs, the need to eliminate all the continuous nodesbefore their discrete ancestors clashes with our desire to eliminate all the nodes in slice t before we eliminateany in slice t1. Hence it is common to remove the strong triangulation requirement, resulting in the jtree inFigure 4.13. In this case, both the collect forwards and the distribute backwards passes will involve weakmarginalization. This is equivalent to the standard momentmatching approximation for filtering in switchingKalman filter models see e.g., TSM85, BSL93, Kim94, WH97. We discuss how to improve on this usingexpectation propagation in Section B.7.2.Numerically stable CG inferenceThe scheme outlined above, whether exact using a strong jtree or approximate using a regular jtree or evenan untriangulated graph, is subject to numerical errors because of the need to convert between canonical andmoment form. In addition, the fact that we represent initial clique potentials in canonical form means wecannot use deterministic linear CPDs with   0, which can be useful for some models. A solution to bothof these problems is proposed in LJ01. Unfortunately, the incorporation of evidence is quite different fromthe above framework, and requires fairly significant changes to the architecture.B.5.3 Arbitrary CPDsExact inference algorithms have so far only been derived for networks which satisfy the following restrictions all hidden nodes must be discrete with discrete parents, or have conditionally linear Gaussian CLGCPDs which of course includes as special cases linear Gaussian and unconditional Gaussian distributions.Observed nodes may have any distribution if all their parents are discrete or observed, but must have haveCLG CPDs if they have any hidden continuous parents.These restrictions rule out many combinations, but a particularly useful one would be discrete nodeswith hidden continuous parents e.g., using a softmax CPD this can be used to model switchingthresholdbehavior as well as for dimensionality reduction of discrete data Tip98. I proposed a variational approxima5For example, consider Figure B.17. With the elimination ordering X1, . . . ,XT , S1, . . . , ST , the cliques areS1, S2,X1,X2, . . . , S1t,Xt1,Xt, . . . , S1T ,XT1, XT .187X1 X2 X3 X4Y1 Y2 Y3 YX1 X2 X3 X4Y1 Y2Ya bFigure B.18 A noisyOR CPD for P Y X14 represented in a more computationally revealing form. aThe temporal transformation. b Parent divorcing.tion for this case in Mur99, based on JJ00 Wie00 extended this to a mixture variational approximation.LSK01 proposed a hybrid jtreenumerical integration algorithm for this problem.In general, the current state of the art for general continuous CPDs is to use sampling methods see Section B.7.4. Of course, it is always possible to discretize all the continuous variables, or to apply numericalintegration, but such approaches usually suffer from the curse of dimensionality.B.6 Speeding up exact discrete inferenceThere are at least three kinds of indepencies we can exploit to speed up exact inference. The first is conditionalindependence, which is reflected in the graph structure we have already discussed how to exploit this. Thesecond is causal independence e.g., noisyOR, and the third is context specific independence CSI. Wediscuss these, and other tricks, below.B.6.1 Exploiting causal independenceCPDs with causal independence were introduced in Section A.3.2. The canonical example is noisyOR.Pearl Pea88 showed how to exploit the structure of the noisyOR CPD to compute the  and  messagesin time linear in the number of parents ZP96, RD98 showed how to exploit it in variable elimination, andHec89 showed how to exploit in BN20 binary node 2level noisyOR networks such as QMRDT usingthe Quickscore algorithm.To exploit causal independence in the jtree algorithm, we have to make graphically explicit the localconditional independencies which are hidden inside the CPD. The two standard techniques for this are thetemporal transformation Hec93, HB94 and parent divorcing OKJ89 see Figure B.18. We introduceextra hidden nodes Yi which accumulate a partialOR of previous parents. In general, to benefit from such atransformation, we must eliminate parents before children, otherwise we end up creating a clique out of theoriginal family. However, sometimes this is not the best ordering. See RD98 for a discussion of this pointsee also ZY97, MD99.Interestingly, it is not always possible to exploit causal independence if we are doing maxproduct188X1 X2 X3 X4Y1 X5 Y2YFigure B.19 A treestructured CPD for P Y X15 represented in a more computationally useful form. Letall nodes be binary for simplicity. We assume Y is conditionally independent of X34 given that X5  1,i.e., P Y X14, X5  1  P Y1X1, X2  fY,X1, X2. Similarly, assume P Y X14, X5  2 P Y2X3, X4  gY,X3, X4. Hence X5 is a switching parent P Y Y1, Y2, X5  i  Y, Yi. Thistree requires 23 parameters, whereas a table would require 25 furthermore, the clique size is reduced from 6to 4. Obviously the benefits are potentially greater for nodes with more parents and for nodes with biggerdomains.Viterbi, as opposed to sumproduct, because we must always sum out the dummy hidden nodes however, the max and sum operators do not commute, so this imposes a restriction on the possible orderings c.f.,Section B.3.6 which can eliminate any potential gains.B.6.2 Exploiting context specific independence CSICPDs with CSI were introduced in Section A.3.2. The canonical example is a treeCPD. Such CPDs canbe exploited for inference by the jtree algorithm using a network transformation BFGK96, as illustratedin Figure B.19. This is analogous to the transformations introduced to exploit causal independence seeSection B.6.1. More aggressive optimizations are possible if we use the variable elimination algorithmZha98b, ZP99. A way of exploiting CSI in the jtree algorithm is discussed in Pfe01.TreeCPDs have been widely used for many years and are especially popular in the decision makingcommunity, e.g., see BDG01 for a review of the work on factored Markov decision processes MDPs.Kim01, ch5 discusses how to use trees to do structured linear algebra. Recently the MDP communityhas started to investigate algebraic decision diagrams ADDs BFG93, which are very computationallyefficient ways of representing and manipulating realvalued functions of binary variables. These are moreefficient than trees because they permit sharing of substructure just as important, there is a fast, freelyavailable program for manipulating them called CUDD.6 ADDs have also been used for speeding up exactinference in discrete Bayes nets NWJK00.B.6.3 Exploiting deterministic CPDsDeterministic discrete CPDs are quite common, as we saw in the HHMM models in Chapter 2. Multiplexernodes are also deterministic. This means the resulting CPD has lots of zeros. The technique of zerocompression removes 0s from the potentials so that multiplication and marginalization can be performed6httpvlsi.colorado.edu fabioCUDDcuddIntro.html189more efficiently, without any loss in accuracy JA90, HD96.Zwe98 develops a technique for exploiting deterministic CPDs in Pearls directed message passingalgorithm on a jtree. Specifically, he requires the jtree satisfy what he calls IRP the immediate resolutionproperty, which says that, in a preorder traversal of the tree root to leaves, the first time a deterministicnode appears in a clique, its parents must also be present. This can be ensured by eliminating parents beforechildren, i.e., in some total ordering of the DAG see Section B.3.6. The advantage of this is that it is possibleto detect which elements of P CcCp will be zero, where Cc is the child clique and Cp is its parent oneor the other of these can be separators. It is not clear if this technique is relevant to the undirected form ofmessage passing, which does not require computation of terms like P CcCp. In particular, it seems thatusing sparse potentials should achieve the same effect.B.6.4 Exploiting the evidenceSometimes, with deterministic CPDs, evidence on the child or one or more parents can render the remainingfamily members effectively observed, e.g., in an ORgate, if the child is off, all the parents must be off, orif any of the parents is on, the child must be on. Such constraints can be exploited by running arc consistencybefore probabilistic message passing, to reduce the effective domain size of each node. This technique iswidely used in genetic linkage analysis FGL00.B.6.5 Being lazyIn general there is a tradeoff between being lazy, i.e., waiting until the query, evidence, structure and parameters have all been specified, and being eager, i.e., precomputing stuff as soon as possible, so the cost can beamortized across many future operations e.g., we would not want to create a jtree from scratch every timethe evidence or query changed. Usually the jtree is constructed based only on the graph structure. However,by constructing the jtree later in the pipeline, we can avail of the following kinds of information If we know how many values each node can take on, we know how heavy the cliques will be thiswill affect the search for the elimination ordering. If we know what kinds of CPD each node has, we can perform network transformations if necessary.Also, we know if we need to construct a strong jtree or not. If we know which nodes will be observed, it will affect how many values each node can have, the kindsof CPDs it can support, whether we need to build a strong jtree, etc. If we know the parameter values, we might be able to perform optimizations such as zero compression. If we know which nodes will be queried and which nodes will be observed, we can figure which nodesare relevant for answering the query.190 If we know the values of the evidence nodes, we might be able to do some preprocessing e.g., constraint satisfaction before running probabilistic inference.Most inference routines choose an elimination ordering based on the first two or three pieces of information. Variable elimination can easily exploit the remaining information, as can lazy Hugin MJ99.QueryDAGs DP97 are precompiled structures for answering specific queries, and can be highly optimized.B.7 Approximate inferenceExact inference in discrete networks is exponential in the size of the largest clique. The techniques in Section B.6 can help for certain networks, but are generally no use for large structured networks such as grids.The max clique for an N  nn grid has size ON. In the worsr case, exact inference is exponentiallyhard in fact exact inference is known to be Phard, i.e., strictly harder than NPhard DL93. We thereforeneed to resort to approximations.Even if the graph is tree structured, exact inference in networks with hidden continuous variables isonly possible in very restricted circumstances, as mentioned in Section B.5.3. We therefore need to resort toapproximations in this case, too.Approximate inference is a huge subject. In this section, I concentrate on methods that are very similaralgorithmically speaking to the exact message passing schemes we have just discussed this makes themefficient and easy to implement. For completeness, I briefly mention some other approaches at the end.B.7.1 Loopy belief propagation LBPBelief propagation is a way of computing exact marginal posterior probabilities in graphs with no undirectedcycles loops essentially it generalises the forwardsbackwards algorithm to trees see Section B.4. To besure of getting exact results, we must convert a graph with loops into a junction tree jtree see Section B.3,and run belief propagation on the resulting jtree. Unfortunately, the nodes in the jtree which are cliques inthe original graph may be exponentially large, making this procedure prohibitively slow.One natural approximation, first suggested in Pea88, is to apply belief propagation to the originalgraph, even if it has loops this is often called loopy belief propagation LBP. In principle, this runs therisk of double counting information, and of either not converging or of converging to the wrong answer.However, in practice the method often works surprisingly well. The most important success story has beendecoding messages sent over noisy channels at rates near to the Shannon limit see MMC98 and referencestherein, but the technique has also been succesfully applied to general Bayes nets MWJ99, 2D latticeMRFs FPC00, Wei01, etc. Various improvements to the algorithm have been made, including versions thatalways converge e.g., Yui00, WT01 and versions that yield higher accuracy by considering embeddedtrees WJW01 or higherorder interactions beyond pairwise YFW01 the Kikuchi method.191A number of theoretical results have now been proved about LBP, explaining why it gives exact answersfor networks in which all nodes are Gaussian WF99, for networks in which there is only a single loopWei00, and for general networks but using the maxproduct Viterbi version instead of the sumproductversion of the algorithm FW00. Connections to variational methods have also been made WT02, TJ02. Ingeneral, however, all we can say is that, if the algorithm converges, it will reach a stationary point in practice,usually a local minimum of the Bethe free energy YFW00. This ensures things are locally, but perhapsnot globally, consistent.The LBP algorithmTo apply LBP to a Bayes net, we can use Pearls formulation in terms of  and  messages. But how dowe apply LBP to an MRF After all, the nodes do not have local terms associated with them. For pairwiseMRFs, it is simple to formulate the message passing rules in terms of matrixvector multiplication Wei00the pairwise Gaussian case is also easy WF99. Any MRF with potentials defined on larger sets of nodes canbe converted into a pairwise MRF by creating meganodes WF99, but this is not always desirable, since thestatespace of the meganodes will be much larger, and hence computing the messages may be exponentiallyslower. It is possible to derive the rules for arbitrary MRFs, but they are a bit messy. Besides, we would liketo use the same code for Bayes nets and MRFs. We therefore convert both BNs and MRFs to factor graphssee Section A.4. We now derive the LBP algorithm for factor graphs fgraphs.The basic rules for message passing in an fgraph are as follows. Each variable node x sends a messageto each neighboring factor node f of the formxf x g 6fgxxand each factor node f sends a message to each neighboring variable node x of the formfxx uxfuy 6xyf ywhere we have assumed u is the domain of f . Obviously we could update all nodes in parallel, but inFigure B.20, I give a more efficient serial updating scheme. If the fgraph is tree structured, one forwardsand one backwards sweep suffices to compute the exact marginals at every node. The serial protocol usesan arbitrary, but fixed, order. For chains, the natural ordering is lefttoright, for trees it is leavestorootpreorder, and for DAGs, it is a topological ordering. Undirected graphs dont have a natural ordering,so we can pick any one. Each factor node divides its neighboring variables into its predecessors lower inthe ordering, and its successors higher in the ordering. On the forwards sweep, a node combines all theincoming messages from its predecessors, and puts a message on its out arcs to each of its successors. On thebackwards sweep, a node combines all its incoming messages from its successors, and puts a message on itsout arcs to each of its predecessors.192function ,   BPfgraphF, order,maxiter, toliter  1converged  falseinitialize j iassj Fi, x  1, fx  1, xx  1while not converged and iter  maxiter,   sweep, , order,   sweep, , reverseorderconverged  testconvergence, , toliter  iter  1function ,   sweepold, old, orderfor each factor f in orderf  oldffor each variable x in predf , orderxf  oldx oldfxf  f  xf oldxf for each variable x in succf ,orderfx  f  xoldxf x  oldx  fxoldfxFigure B.20 Pseudocode for belief propagation on a factor graph using a serial updating protocol.DampingLBP can result in posterior marginals that oscillate. Previous experience MWJ99 has shown that oneeffective solution to this problem is to only update the messages partially towards their new value at eachiteration. That is, we use a convex combination1mt mt1of the new message and its previous value, where 0  m  1 is a momentum term and  represents amessage. m  0 corresponds to the usual LBP algorithm m  1 corresponds to no updating at all. It iseasy to see that fixed points of this modified algorithm are also fixed points of the original system since ift  t1  , then 1mt mt1  .If damped LBP does not converge, we can use one of several alternative algorithms that directly minimizes the Bethe free energy and which always converge WT01. However, empirically it has been foundthat in such cases, the accuracy of the answers is poor, i.e., oscillation seems to be a sign that the BetheLBPapproximation is a bad one WT02.Efficient computation of messagesComputing a message from a factor to a variable takes OKFin time, where K is the number of discretevalues a node can take on, and Fin is the fanin of the factor one plus the number of parents of a node in thecase of a BN, or the number of terms in the clique potential in the case of an MRF. It is sometimes claimed193e.g., Bar01 that undirected message passing is therefore more efficient, but in fact it has exactly the sametime complexity when we convert the Bayes net to an MRF, we need to moralize the parents, creating aclique of size KFin1. In general, message passing with tabular representations of CPDspotentials alwaystakes time exponential in the clique size.Pea88 shows how to exploit the structure of the noisyOR CPD to compute the  and  messages intime linear in the number of parents, instead of exponential. This method can be generalized to any CPDthat enjoys the property of causal independence RD98, including multiplexer nodes. In the fgraph context, this means that certain kinds of factor nodes will have specialized methods for computing messagesfurthermore, there will be an asymmetry between messages sent to children and message sent to parents.Bar01 shows how to compute the messages for any CPD which is a linear function of its parents using aFourier transform, which can sometimes be approximated using saddle point techniques.If the factor is a mixture of Gaussians, we may have to use weak marginalization see Section B.5.2when computing the message.If the fgraph was derived from a jtree, the cost of computing a message is OKw in the worst case,wherew is the induced width of the graph i.e., maximal clique size. Furthermore, since potentials factorsmay be products of many different types of CPDs, it is much harder to exploit local structure, unless it isexposed graphically see Section B.6.2.LBP on a jtreeIf the fgraph is a tree, LBP will give the exact answers. Any graph can be converted to a junction tree, aswe discussed in Section B.3. If we could convert this jtree to an fgraph and run LBP, we would get exactanswers. This would allow us to use a single inference algorithm shown in Figure B.20 for both exact andapproximate inference. We can convert a jtree to an fgraph by making the following correspondence thefactors correspond to cliques and the variables correspond to separators.7In the case of a jtree, a separator variable is always connected to exactly two cliques factors. Thismeans we can simplify the algorithm. For example, suppose j is a variable node separator and i and kare neighboring factors cliques, The variable node separator computes its product of incoming messages,j  ij  kj , and then sends out a new message, jk  jkj  ij . Factor node cliquek then absorbs this message. This is clearly equivalent to factor node clique i sending ij to factor nodeclique k directly. Hence it is sufficient for cliques to send messages to each other directly separators do notneed to send messages instead, they simply store the potentials messages. This simplification results in theJLO algorithm in Figure B.13.7As far as I know, this is a novel observation.194B.7.2 Expectation propagation EPExpectation propagation EP Min01 is like belief propagation except it requires that the posteriors beliefson each variable have a restricted form. Specifically, the posterior must be in the exponential family, i.e., ofthe form q  exp f, where  is a variable. This ensures that beliefs can be represented using a fixednumber of sufficient statistics. We choose the parameters of the beliefs s.t.  arg minDpqwherep qprior tZis the exact posterior, Z qprior  t is the exact normalizing constant, t is the likelihood termmessage coming in from a factor and q is the approximate posterior. This can be solved by momentmatching. When we combine sequential Bayesian updating with this approximation projection after everyupdate step, the result is called Assumed Density Filtering ADF, or the probabilistic editor SM80. It isclear that the BK algorithm see Section 4.2.1 is an example of ADF, as is the standard GBP algorithm forswitching Kalman filters see Section 4.3.One drawback with ADF is its dependence on the order in which the updates are performed. EP, whichis a batch algorithm, reduces the sensitivity to ordering by iterating. Intuitively, this allows EP to go back andreoptimize each belief in the revised context of all the other updated beliefs. This requires that we store themessages so their effect can be later undone by division. In EP, rather than approximating the messagesdirectly which is hard, since they represent conditional likelihoods, we approximate the posterior usingmoment matching, and then infer the corresponding equivalent message. We explain this below.To explain the difference between EP and BP procedurally, consider the simple factor graph shown inFigure B.22. We send a message from f to x, and then update xs belief, as followspriorx  xoldfx  oldgxpriorf  foldxf  fx, yoldyf yfx  priorf  x yfx, yoldyf yx  priorx  fx  oldgx  fxThe terms after the second equality on each line are for this particular example.In EP, we compute the approximate posterior x first, and then derive the message fx, which, had it195been combined with the prior priorx , would result in the same approximate posteriorpriorx  oldx oldfxpriorf  foldxfx, Z  ADF priorx  priorf  xfx  Zxpriorx where q, Z  ADF p produces the best approximation q to p within a specified family of distributions.The code for EP is shown in Figure B.21.For example, consider the switching Markov chain in Figure B.17. The factor graph for the first fewslices is shown in Figure B.22, where the variables are the separators, y  S1, X1, x  S2, X2, etc.,and the factors are the clique potentials, hy  P S1P X1S1, fx, y  P S2S1P X2X1, S1, etc.When f sends a message to x, it needs to computexX2, S2  ADFX1S1priorx X2, S2 priorf X2, S2, X1, S1which can be done by weak marginalization moment matching for a Gaussian mixture.In the special case in which we only have a single variable node as Minka typically assumes, thingssimplify, since the factor nodes do not contain any other variables, e.g., priorf  fx, as opposed topriorf  fx, y  yf y, and hence the projection onto x is unnecessary. In this case, the ADF stepsimplifies tox, Z  ADF priorx  fxTo map this to Minkas notation, use x  , x  q, and fx  ti.We now discuss how to implement q, Z  ADF qpriort for different kinds of distributionq and likelihood terms t.EP with a Gaussian posteriorWe can compute the parameters of an approximate Gaussian posterior q, as followsmpost  Ep  m VmVpost  Ep EpEp  V  V mm  2V Vwhere m  mprior, V  Vprior , m  m logZm,V , V  V logZm,V  and Zm,V   tqpriorm,V  Min01, p15,p45. These equations are properties of the Gaussian distribution andhold for any ti. Minka works the details out for the example of a likelihood term that is a mixture ofGaussians modelling a point buried in clutterti  1 wN xi  , I  wN xi 0, 10I196function ,   sweepold, old, orderfor each factor f in orderf  oldffor each variable x in predf , orderxf  oldx oldfxf  f  xf oldxf for each variable x in succf ,orderpriorx  oldx oldfxpriorf  foldxfx, Z  ADF priorx  priorf  xfx  Zxpriorx Figure B.21 Pseudocode for expectation propagation on a factor graph using a serial updating protocol.This sweep function is called iteratively, as in Figure B.20.y xhy fx, y gxFigure B.22 A simple factor graph. Square nodes are factors, round nodes are variables.for a fixed, known w.Once we have approximated the beliefs, we can compute the new message by division. In the case ofGaussians, the easiest way to do this is to convert the beliefs to canonical form, and then divide. Note that thisdivision might result in a negative or infinite variance term since division in canonical form is implementedby subtracting precision matrices. A negative variance represents a function that curves upwards like aU, rather than the usual downward bellshaped curve of a Gaussian an infinite variance corresponds toa flat uniform distribution zero variance corresponds to a constant. It is okay if messages have negativevariance, but a belief with a negative variance is a problem. In this case, one crude approximation is to toreplace a negative variance with an infinite variance, which means the absorbing variable will ignore thismessage since it is completely uncertain. This will always result in convergence, but Min01, p22 says thatwhen EP does converge with negative vis, the result is always better than having forced vi  0 where thevariance is viI in the case of a spherical Gaussian.EP with a Dirichlet posteriorIt is possible to use EP to compute a Dirichlet approximation to P y even when the likelihood functionshave the form of a mixture, P yi z P zP yiz. This has been applied to a model in which the ysrepresent words, the zs represent latent topics, and  is a distribution over topics for a particular documentML02.197EP with mixed types of posteriorIt is not clear how to use EP for arbitrary factor graphs, where the belief on each factor and hence thecorresponding outgoing messages might be a different member of the exponential family. This is a subjectfor future research.EP with a fully factorized posterior is LBPMinka points out that LBP is a special case of EP when we make the approximation that the posterior is fullyfactorized qx Ni1 qixi. To see this, consider combining this factored prior with a term tix P XiPaXi to yield to posterior px. We now seek the distribution q s.t., Dpxqx is minimized,subject to the constraint that q be fully factorized. This meansqixi  pxi xxipxi.e., the marginals must match. These are expectation constraintsEq xi  v  Epxi, vfor all values v and all nodes i. This corresponds to running LBP on a factor graph with no need for the ADFstep to approximate messages.B.7.3 Variational methodsThe simplest example of a variational method is the meanfield approximation, which, roughly speaking,exploits the law of large numbers to approximate large sums of random variables by their means. In particular,we essentially decouple all the nodes, and introduce a new parameter, called a variational parameter, for eachnode, and iteratively update these parameters so as to minimize the crossentropy KL distance between theapproximate and true probability distributions, i.e., we seek to minimizeDqp, where q is the approximatedistribution. EP, by contrast, locally minimizes Dpq. Updating the variational parameters becomes aproxy for inference. The meanfield approximation produces a lower bound on the likelihood.It is possible to combine variational and exact inference this is called a structured variational approximation. See JGJS98, Jaa01 for good general tutorials.B.7.4 Sampling methodsStochastic sampling algorithms for static Bayesian networks are usually based on importance samplinglikelihood weighting or Monte Carlo Markov Chain MCMC see e.g., Nea93, GRS96, Mac98 for goodintroductions, which includes Gibbs sampling and simulated annealing as special cases.Sampling algorithms have several advantages over deterministic approximation algorithms they areeasy to implement, they work on almost any kind of model all kinds of CPDs can be mixed together, the198statespace can have variable size, the model structure can change, they can convert heuristics into provablycorrect algorithms by using them as proposal distributions, and, in the limit of an infinite number of samples,they are guaranteed to give the exact answer.The main disadvantage of sampling algorithms is speed they are often significantly slower than deterministic methods, often making them unsuitable for large models andor large data sets. However, it ispossible to combine exact and stochastic inference. The basic idea is to integrate out some of the variablesusing exact inference, and apply sampling to the remaining ones this is called RaoBlackwellisation CR96.B.7.5 Other approachesThere are a variety of other approaches we have not mentioned, most of which are designed for discretestatespaces.8 Truncating small numbers simply force small numbers in the clique potentials to zero, and then usezerocompression. If done carefully, the overall error introduced by this procedure can be boundedand computed JA90. Structural simplifications make smaller cliques by removing some of the edges from the triangulatedgraph Kja94 again the error can be bounded. A very similar technique is called mini bucketsDec98, but has no formal guarantees. Bounded cutset conditioning. Instead of instantiating exponentially many values of the cutset, simple instantiate a few of them HSC88, Dar95. The error introduced by this method can sometimesbe bounded. Alternatively, we can sample the cutsets jointly, a technique known as blocking Gibbssampling JKK95.8The following web page contains a list of approximate inference methods c. 1996.camis.stanford.edupeoplepradhanapprox.html199Appendix CGraphical models learningC.1 IntroductionThere are many different kinds of learning. We can distinguish between the following axes Parameter learning or structure learning. For linearGaussian models, these are more or less the samething see Section 2.4.2, since 0 weights in a regression matrix correspond to absent directed edges,and 0 weights in a precision matrix correspond to absent undirected edges. For HMMs, structurelearning usually refers to learning the structure of the transition matrix, i.e., identifying the 0s inthe CPD for P XtXt1. We consider this parameter learning with a sparseness prior, c.f., entropiclearning Bra99a. In general, structure learning refers to learning the graph topology no matter whatparameterization is used. Structure learning is often called model selection. Fully observed or partially observed. Partially observed refers to the case where the values of some ofthe nodes in some of the cases are unknown. This may be because some data is missing, or becausesome nodes are latent hidden. Learning in the partially observed case is much harder the likelihoodsurface is multimodal, so one usually has to settle for a locally optimal solution, obtained using EM orgradient methods. Frequentist or Bayesian. A frequentist tries to learn a single best parameter model. In the case ofparameters, this can either be the maximum likelihood ML or the maximum a posteriori MAPestimate. In the case of structure, it must be a MAP estimate, since the ML estimate would be thefully connected graph. By contrast, a Bayesian tries to learn a distribution over parameters models.This gives one some idea of confidence in ones estimate, and allows for predictive techniques such asBayesian model averaging. Although more elegant, Bayesian solutions are usually more expensive toobtain. Directed or undirected model. It is easy to do parameter learning in the fully observed case for directedmodels BNs, because the problem decomposes into a set of local problems, one per CPD in particu200lar, inference is not required. However, parameter learning for undirected models MRFs, even in thefully observed case, is hard, because the normalizing term Z couples all the parameters together inparticular, inference is required. Of course, parameter learning in the partially observed case is hard inboth models. Conversely, structure learning in the directed case is harder than in the undirected case,because one needs to worry about avoiding directed cycles, and the fact that many directed graphs maybe Markov equivalent, i.e., encode the same conditional independencies. Static or dynamic model. Most techniques designed for learning static graphical models also applyto learning dynamic graphical models DBNs and dynamic chain graphs, but not vice versa. In thischapter, we only talk about general techniques we reserve discussion of DBNspecific techniques toChapter 6. Offline or online. Offline learning refers to estimating the parameters structure given a fixed batch ofdata. Online learning refers to sequentially updating an estimate of the parameters structure as eachdata point arrives. Bayesian methods are naturally suited to online learning. Note that one can learna static model online and a dynamic model offline these are orthogonal issues. If the training set ishuge, online learning might be more efficient than offline learning. Often one uses a compromise, andprocesses mini batches, i.e., sets of training cases at a time. Discriminative or not. Discriminative training is very useful when the model is going to be used forclassification purposes NJ02. In this case, it is not so important that each model be able to explaingenerate all of the data it only matters that the true model gets higher likelihood than the rival models.Hence it is more important to focus on the differences in the data from each class than to focus on allof the characteristics of the data. Typically discriminative training requries that the models for eachclass all be trained simultaneously because of the sumtoone constraint, which is often intractable.Various approximate techniques have been developed. Note that discriminative training can be appliedto parameter andor structure learning. Active or passive. In supervised learning, active learning means choosing which inputs you would liketo see output labels for, either by selecting from a pool of examples, or by asking arbitrary questionsfrom an oracle teacher. In unsupervised learning, active learning means choosing where in thesample space the training data is drawn from usually the learner has some control over where it is instatespace, e.g., in reinforcement learning. The control case is made harder because there is usuallysome cost involved in moving to unexplored parts of the state space this gives rise to the explorationexploitation tradeoff. The optimal Bayesian solution to this problem, for the case of discrete MDPs, isdiscussed in Duf02. In the context of causal models, active learning means choosing which perfectinterventions Pea00, SGS00 to perform. A perfect intervention corresponds to setting a node to a201specific value, and then cutting all incoming links to that node, to stop information flowing upwards.1A realworld example would be knocking out a gene.In this chapter, we focus on the following subset of the above topics passive, nondiscriminative, offline,static, and directed. That leaves three variables parameters or structure, full or partial observability, andfrequentist or Bayesian. For the cases that we will not focus on here, here are some pointers to relevantpapers or sections of this thesis. Discriminative parameter learning Jeb01 discuss maximum entropy discrimination for the exponential family, and reverse JensenEM to handle latent variables EL01 discuss the TM algorithm formaximizing a conditional likelihood function from fully observed data RR01 discuss deterministicannealing applied to discriminative training of HMMs. Discriminative structure learning Bil98, Bil00 learns the interconnectivity between observed nodesin a DBN for isolated word speech recognition. Online parameter learning see Sections 4.4.2 and C.4.5. Online structure learning FG97 discuss keeping a pool of candidate BN models, and updating itsequentially. Dynamic models see Chapter 6. Undirected parameter learning using IPF, IIS and GIS, etc. see e.g., JP95, Ber, Jor02. Undirected structure learning see e.g., Edw00, DGJ01. Active learning of BN parameters TK00. Active learning of BN structure TK01, Mur01a, SJ02.Note that the case most relevant to an autonomous lifelong learning agent is also the hardest online,active, discriminative, Bayesian structure learning of a dynamic chaingraph model in a partially observedenvironment. We leave this case to future work.C.2 Known structure, full observability, frequentistWe assume that the goal of learning in this case is to find the maximum likelihood estimates MLEs of theparameters of each CPD, i.e., the parameter values which maximize the likelihood of the training data, which1For example, consider the 2 node BN where smoking yellowfingers if we observe yellow fingers, we may assume it is due tonicotine, and infer that the person is a smoker but if we paint someones fingers yellow, we are not licensed to make that inference, andhence must sever the incoming links to the yellow node, to reflect the fact that we forced yellow to true, rather than observed that it wastrue.202contains M cases assumed to be independent. The loglikelihood of the training set D  D1, . . . , DMis a sum of terms, one for each nodeL  logMm1PrDmG ni1Mm1logP XiPaXi, Dmwhere PaXi are the parents of Xi. We see that the loglikelihood scoring function decomposes into a seriesof terms, one per node. It is simple to modify the above to handle tied shared parameters we simply poolthe data from all the nodes whose parameters are tied.All that remains is to specify how to estimate the parameters of each type of CPD given its local dataDmXi, PaXi. If the CPD is in the exponential family, this set can be summarized in terms of finitei.e., independent of M  sufficient statisticsmDmXi, PaXi. A huge arsenal of techniques fromsupervised learning can be applied at this point, since we are just estimating a function from inputs parentvalues to outputs child probability distribution.When we consider the case of parameter learning with partial observability, we will replace the sufficientstatistics with their expectation. Hence we phrase the results below in this more general way, in preparationfor EM.C.2.1 Multinomial distributionsIn the case of tabular CPDs, where we define ijkdefP Xi  kPaXi  j, the loglikelihood becomesL imlogj,kIijkmijkimj,kIijkm log ijkijkNijk log ijk C.1where Iijkmdef IXi  k, PaXi  jDm is 1 if the event Xi  k, PaXi  j occurs in case Dm, andhence Nijkdefm IXi  k, PaXi  jDm is the number of times the event Xi  k, PaXi  j wasseen in the training set. For a Markov chain, this corresponds to counting transitions. It is easy to showtaking derivatives and using a Lagrange multiplier to ensurek ijk  1 for all i, j that the MLE isijk Nijkk NijkC.2C.2.2 Conditional linear Gaussian distributionsNow consider a conditional linear Gaussian CPD for node Y with continuous parent X and discrete parentQ, i.e.,pyx,Q  i  ci12 exp 12 y Bix1i y Bix203where c  2d2 is a constant and y  d. The jth row of Bi is the regression vector for the jthcomponent of y given that Q  i. To allow for an offset, we shall assume the last component of x is fixed to1, and the last column of Bi is i.Special cases of this CPD occur in the following common models Mixture of Gaussians. X does not exist, Q is hidden, and Y is observed. The temporal version of thisis an HMM with Gaussian outputs Q  Qt is the discrete hidden state, and Y  Yt is the Gaussianobservation. Factor analysis. Q does not exist,  is assumed diagonal, X is hidden and Y is observed. Thetemporal version of this is the Kalman filter model for the observation CPD, X  Xt and Y  Yt forthe transition CPD, X  Xt1 and Y  Xt, so X and Y will be hidden. Mixture of factor analyzers GH96a. i is diagonal, Q and X are hidden, Y is observed. Thetemporal version of this is a switching Kalman filter for the observation CPD, X  Xt, Q  Qt andY  Yt for the transition CPD,X  Xt1,Q  Qt and Y  Xt, so the whole family will be hidden.Since we are interested in estimating timeinvariant dynamic models such as Kalman filters modelswhose parameters are tied across all time slices, we express all the estimates in terms of expected sufficientstatistics, whose size is independent of the number of samples time steps. This is different from the usualpresentation, which states the formulas in terms of the raw data matrix.The loglikelihood islogMm1Ki1Prymxm, Qm  i,Dmqimwhere the indicator variable qim  1 if Q has value i in the mth data case, and 0 otherwise. Since Q, X andY may all be unobserved, we compute the expected completedata log likelihood as followsL   12mEiqim log i qimym Bixm1i ym Bixm  DmWe note thatEqimxmxmDm  EqimDmExmxmQm  i,Dmdefwimxmxmiwhere the posterior probabilities wim  PrQ  iDm are weights, and xmxmi is a conditional secondmoment.Since a full covariance matrix has dd12 parameters, we are often interested in placing restrictions onthe form of this matrix. We consider the following kinds of restrictions. i is tied across all states, i.e., i   for all i. i is diagonal.204 i is spherical isotropic, i.e., i  2i I .It turns out that the ML estimate of a diagonal covariance matrix is the same as the ML estimate of thefull matrix, with the offdiagonal entries set to 0 hence we will not consider this case further. This leavesus with four combinations full or spherical, tied or not. In addition, it is useful to consider the specialcase where there is no parent except the constant X  1, so Bi  i this gives 8 formulas. These aresummarized in Table C.1. These formulae are derived in Section C.11. Although they look hairy, we wouldlike to emphasize the generality of these results it subsumes the derivation of the M step for a large class ofpopular models, including HMMs with mixtures of Gaussian outputs, mixtures of factor analysis models,switching Kalman filter models, etc.We can see from the table that we need the following expected sufficient statistics m wim,m wimymymi If X exists m wimxmymi,m wimxmxmi. If X does not exist m wimymi. If X exists and i  2i I m wimBiBixmxmi,m wimBiixmymi.Note that, if, as is commonly the case, Y is observed, we can make the following simplifications xmymi xmiym, ymymi  ymym, and ymymi  ymym.C.2.3 Other CPDsNoisyOR One way to estimate these is to represent the CPDs as mini Bayes nets, as in Figure A.8. Thedeterministic link from Ui to Y is of course fixed the link from Xi to Ui is just a binomial binarymultinomial distribution, representing the reliability of the link its parameters can be estimatedusing EM MH97 we need EM since the Ui are hidden.Trees MLEs of trees can be computed using the standard greedy treegrowing methods such as CART andC4.5.Softmax Parameter estimating for softmax CPDs can be done using the iteratively reweighted least squaresIRLS algorithm see e.g., JJ94b, App.A, or gradient descent.Feedforward neural networks Multilayer perceptrons can be fit using gradient descent often called backpropagation in this context see e.g., Bis95.205Eqn Cov. B Tied Formula1 Full B U i mwimymymimwim Bimwimxmymimwim2 Full B T   1Mim wimymymi 1MiBimwimxmymi3 Full  U i mwimymymimwim ii4 Full  T   1Mim wimymymi 1Mimwimii5 Sph. B U 2i 1dmwimTrm wimymymi m wimBiBixmxmi2m wimBixmymi6 Sph. B T 2  1MdTrim wimymymi im wimBiBixmxmi2im wimBixmymi7 Sph.  U 2i 1dmwimymymimwim ii8 Sph.  T 2  1Mdm wimymymi  m wimii9  B  Bi mwimymxmi m wimxmxmi110    i mwimymimwimTable C.1 Parameter estimation for a CLG CPD P Y X,Q  i  N Y BiX,i. The covariance matrixcan be full or spherical sph., tied T or untied U, and the regression matrix can be a full matrix B, orjust a mean column vector  if X  1. M is the number of training cases, d is the dimensionality of Y .206C.3 Known structure, full observability, BayesianIf there are a small number of training cases compared to the number of parameters, we should use a prior toregularize the problem. In this case, we call the estimates maximum a posterori MAP estimates, as opposedto maximum likelihood estimates. Since the model is fully observable, the posterior over parameters will beunimodal. If we use a conjugate prior, we can compute this posterior in closed form.C.3.1 Multinomial distributionsFor discrete nodes, it is very common to assume the local CPDs are multinomial, i.e., represented as a tableof the form PrXi  ki  j  ijk , for k  1, . . . , ri and j  1, . . . , qi, where ri is the number ofvalues node i can take on, and qi li rl is the number of values node is parents, i, can take on. Theseparameters satisfy the constraints 0  ijk  1 andk ijk  1.Following common practice, we will make two assumptions. First, global parameter independenceP  ni1 P i, where i  ijk , j  1, . . . , qi, k  1, . . . , ri are the parameters for node i. Second,local parameter independence P i qij1 ij , where ij  ijk, k  1, . . . , ri are the parametersfor the jth row of Xis table i.e., parameters for the jth instantiation of Xis parents. Given a factoredprior and complete data, the posterior over parameters will also be factored SL90. We will give the form ofthis posterior below. Note that, if we have missing data, the parameter posterior will no longer be factored.Hence assuming parameter independence is equivalent to assuming that ones prior knowledge was derivedfrom a fully observed virtual database.GH97, RG01 prove that the assumptions of global and local parameter independence, plus an additionalassumption called likelihood equivalence2, imply that the prior must be Dirichlet. Fortunately, the Dirichletprior is the conjugate prior for the multinomial Ber85, which makes analysis easier, as we will see below.For this reason, the Dirichlet is often used even if the assumption of likelihood equivalence is violated.Note that, in the case of binary nodes, the multinomial becomes the Bernoulli distribution, and the Dirichletbecomes the Beta.Given global and local independence, each CPD P XiUi  j  ij is a multinomial random variablewith ri possible values. The Dirichlet prior, ij  Dij1, . . . , i,j,ri, is defined asP ij ij rik1ijk1ijk 1Bij1, . . . , i,j,riThe normalizing constant is the ridimensional Beta functionB1, . . . , r k krk1 k2Two graph structures are likelihood equivalent if they assign the same marginal likelihood to data, i.e., P DG1  P DG2.This is weaker than the assumption of Markov hypothesis equivalence, which says two graphs are equivalent if they encode the sameset of conditional independence assumptions. Hypothesis equivalence is clearly violated if we adopt a causal interpretation of BNs. Inaddition, likelihood equivalence is violated if we have interventional data. See Hec95 for a discussion.207where  is the gamma function for positive integers, n  n 1.The hyperparameters, ijk  0, have a simple interpretation as pseudo counts. The quantity ijk  1represents the number of imaginary cases in which event Xi  k,i  j has already occured in somevirtual prior database. Upon seeing a database D in which the event Xi  k,i  j occurs Nijk times,the parameter posterior becomesij D  Dij1 Nij1, . . . , i,j,ri Ni,j,riThe posterior mean isEijk D ijk Nijkril1 ijl NijlC.3and the posterior mode MAP estimate isarg maxP ijk D ijk Nijk  1ril1 ijl Nijl  riC.4and the predictive distribution isP x ni1qij1 rik11ijkxijk P ijkdijkni1qij1rik1Eijk1ijkx C.5where 1ijkx is an indicator function that is 1 if the event Xi  k,i  j occurs in case x, and is 0otherwise.To compute the marginal likelihood for a database ofM cases,D  x1, . . . , xM , we can use sequentialBayesian updating SL90P DG  P x10P x20, x1 . . . P xM 0, x1M1 P x10P x21 . . . P xM M1where 0   is our prior, and t is the result of updating t1 with xt. HGC95 call P DGP G theBayesian Dirichlet BD score for a model.Assessing Dirichlet priorsIt is clearly impossible for the user to specify parametric priors for O2n2 graph structures. HGC95 showthat, under some assumptions3, it is possible to derive the Dirichlet parameters for an arbitrary graph from asingle prior network, plus a confidence factor, . Specifically, ijk  P Xi  k,i  jGc, where Gc isthe complete graph. When priors are derived in this manner, the BD score is called BDe BD with likelihood3The assumptions are global and local parameter independence, likelihood equivalence, parameter modularity and structural possibility. Parameter modularity says that P ij is the same for any two graph structures in which Xi has the same set of parents. Structuralpossibility says that all complete fully connected graph structures are possible a priori.208equivalence. Unfortunately, it might be difficult to parameterize the prior network especially because of thecounterintuitive conditioning on Gc see HGC95 for a discussion. In addition, computing the parameterpriors for an arbitrary graph structure from such a prior network requires running an inference algorithm,which can be slow. SDLC93 suggest a similar way of computing Dirichlet priors from a prior network.A much simpler alternative is to use a noninformative prior. A natural choice is ijk  0, whichcorresponds to maximum likelihood. In the binary case, this is called Haldanes prior. However, this is animproper prior. More importantly, this will cause the loglikelihood to explode if a future case contains anevent that was not seen in the training data.If we set 0  ijk  1, we encourage the parameter values ijk to be near 0 or 1, thus encoding neardeterministic distributions. This might be desirable in some domains. Bra99a explicitely encodes this biasusing an entropic prior of the formP ij  eHij kijkijk .Unfortunately, the entropic prior is not a conjugate prior, which means we must use iterative techniques tofind the MAP estimate.CH92 suggest the uniform prior, ijk  1. This is a noninformative prior since it does not affect theposterior although it does affect the marginal likelihood. Unfortunately, it is not entirely uninformative,because it is not transformation invariant. The fully noninformative prior is called a Jeffreys prior. For thespecial case of a binary root node i.e., a node with no parents and a beta distribution, the Jeffreys prior isjust ijk  12 . Computing a Jeffreys prior for an arbitrary BN is hard see KMS98.Bun91 suggests the prior ijk  riqi, where  is an equivalent sample size. This induces thefollowing distributionP Xi  ki  j  Eijk  riqiqi1riThis is a special case of the BDe metric where the prior network assigns a uniform distribution to the jointdistribution hence HGC95 call this the BDeu metric. Not surprisingly, experiments reported in HGC95suggest that, for small databases, it is better to use the uninformative prior BDeu metric that an informative,but incorrect, prior BDe metric.A more sophisticated approach is to use a hierarchical prior, where we place a prior on the hyperparameters themselves. For example, let ijk  ij0ri for each i, j, where ij0 is the prior precision for ij .ij0 is itself an r.v., and can be given e.g., a gamma distribution. Unfortunately, we can no longer computethe marginal likelihood in closed form using such hierarchical priors, and must resort to sampling, as inGGT00, DF99. A more efficient method, known as empirical Bayes or maximum likelihood type II, is toestimate the hyperparameters from data see Min00b for details.209C.3.2 Gaussian distributionsIf P Y  is a Gaussian, then the ML estimates of  and  are the sample mean and covariance of the observationsML  EY Y  1MmymymA suitable prior is the NormalWishart DeG70, Min00c. In the special case of a zeromean, diagonalWishart, this amounts to adding i to the diagonal elements of the empirical covariance matrixMAP 1MM ML  where   diag1, . . . , d.C.3.3 Conditional linear Gaussian distributionsSee Min00a for an uptodate treatment of multivariate Bayesian linear regression, which uses the matrixnormal distribution.C.3.4 Other CPDsNoisyOR For noisyOR, we can simply put an independent Dirichlet prior on the suppression probabilitiesof each parent.Trees Bayesian approaches to decision trees are discussed in Bun89.Softmax There is no conjugate prior for the softmax function, so one must resort to approximations, e.g.,JJ00.Feedforward neural networks Bayesian estimation of the parameters of MLPs is discussed in e.g., Bis95.C.4 Known structure, partial observability, frequentistIn the partially observable case, the loglikelihood isL mlogP DmmloghP H  h, V  Dm C.6where the innermost sum is over all the assignments to the hidden nodes H , and V  Dm means thevisible nodes take on the values specificied by case Dm. Unlike the fully observed case, this does notdecompose into a sum of local terms, one per node although it does decompose across training cases. Wewill see two different solutions to this below, based on gradient ascent and EM, which we will then compareexperimentally in terms of speed.210C.4.1 Gradient ascentThe obvious way to maximize likelihood is to do gradient ascent. Following BKRK97, we show below thatthe gradient is a sum of family marginals, which can be computed using an inference engine since a nodeand its parents will always be in the same clique due to moralization. This turns out to be very similar to EMsee Section C.4.2. This similarity was also noted in BC94 for the case of gradient ascent HMM training.Consider first the case of tabular CPDs. Let wijk  P Xi  jPaXi  k soj wijk  1.LwijkMm1 logPwDmwijkMm1PwDmwijkPwDmwhich at least decomposes into a sum of terms, one per data case because we assumed they were independent. The key trick is that we can simplify the numerator of the term inside the sum by conditioning on theparentsPwDmwijkwijkj ,kPwDmXi  j, PaXi  kPwXi  jPaXi  kPwPaXi  k PwDmXi  j, PaXi  kPwPaXi  ksince the derivative plucks out the term involving j  j  and k  k, and since PwXi  jPaXi  k wijk . HencePwDmwijkPwDmPwDmXi  j, PaXi  kPwPaXi  kPwDmPwXi  j, PaXi  kDmPwDmPwPaXi  kPwXi  j, PaXi  kPwDmPwXi  j, PaXi  kDmwijkwhere the second line follow from Bayes rule. The numerator, PwXi  j, PaXi  kDm, can becomputed using inference with the evidence set to Dm.The extension of the above to nontabular CPDs follows by using the chain rule logPwDpi,j,k logPwDwijk wijkpwhere p are the parameters of the CPD.Handling constraints on parametersGenerally we have constraints on the parameters. For example, for an HMM transition matrix, we require0  Ai, j  1 for all i, j and j Ai, j  1 for all i. Hence we must work inside the unit cube to ensure0  Ai, j  1 and project onto the constraint surface to ensure j Ai, j. It is possible to reparameterize211this problem using a softmax transform, Ai, j  expAi,jexpkAi,k , to eliminate both of these requirements.However, reparameterization is generally not recommended for constrained optimization problems, becauseit changes the nature of the surface GMW81.Sometimes the constraints are more complicated. For example, when estimating Gaussian distributions,we must ensure the covariance matrix is positive semidefinite. In this case, we can reparameterize and dounconstrained optimization of 12 .C.4.2 EM algorithmThe basic idea behind EM is to apply Jensens inequality CT91 to Equation C.6 to get a lower bound onthe loglikelihood, and then to iteratively maximize this lower bound. Jensens inequality says that, for anyconcave function f ,fjjyj jjfyjwherej j  1. Informally, this says that f of the average is bigger than the average of the f s, which iseasily seen by drawing f . Since the log function is concave, we can apply Jensens to Equation C.6 to getL mloghPH  h, VmmloghqhVmPH  h, VmqhVmmhqhVm logPH  h, VmqhVmmhqhVm logPH  h, VmmhqhVm log qhVmwhere q is a function s.t.h qhVm  1 and 0  qhVm  1, but is otherwise arbitrary.4 Maximizingthe lower bound with respect to q givesqhVm  PhVmThis is called the E expectation step, and makes the bound tight. Approximate inference increases thisbound, but may not maximize it NH98.Maximizing the lower bound with respect to the free parameters  is equivalent to maximizing theexpected completedata loglikelihoodlcq mhqhVm logPH  h, Vm4In physics, Lq,  is called the variational free energy, and consists of the expected energy,  log PH  h, Vmq , andthe entropy, Hq. Loopy belief propagation see Section B.7.1 approximates Hq by considering only single and pairwise terms,resulting in the Bethe approximation to the free energy.212This is called the M maximization step. This is efficient if the corresponding completedata problem istractable, and q has a tractable form.If we use qhVm  PhVm, as in exact EM, then the above is often written asQ mhP hVm,  logP h, VmDempster et al. DLR77 proved that choosing  s.t. Q  Q is guaranteed to ensure P D P D, i.e., increasing the expected completedata loglikelihood will increase the actual partial data loglikeliood. This is because using qhVm  PhVm in the E step makes the lower bound touch the actualloglikelihood curve, so raising the lower bound at this point will also raise the actual loglikelihood curve.If we do an approximate E step, we do not have such a guarantee.In the case of multinomial CPDs, the expected completedata loglikelihood becomes based on Equation C.1Q ijkENijk  log ijkwhere ENijk m P Xi  k, PaXi  jDm, , so the Mstep,   arg max Q, becomesbased on Equation C.2ijk ENijkk ENijkThis is a generalization of the EM algorithm for HMMs often called BaumWelch, which was alreadydescribed in Section 1.2.3. This idea can be applied to any Bayes net Lau95 compute the expected sufficientstatistics ESS,m P Xi, PaXiDm, old, using an inference algorithm, and use these in the M stepSection C.2 as if they were actually sufficient statistics computed from the data, and then repeat.Much has been written about EM. Please see MvD97, MK97, UN98, NH98, Mar01 for details. Forarticles specifically about EM for Bayes nets, see Lau95, BKS97, Zha96.C.4.3 EM vs gradient methodsThe title of this section is a little misleading, since EM is implicitly a gradient method, as we show below.However, it is still an interesting question whether one should explicitely try to minimize the gradient, or justuse EM.5We shall restrict our attention to deterministic algorithms with the following form of additive updaterule6t1  t  tdt, C.7where dt is the direction in which to move at iteration t, and t is the step size. Even within the confines ofthis form, many variations are possible. For example, how do we choose the direction How do we choose the5This subsection is based on my class project for Stats 200B in 1997.6Multiplicative update rules exponentiated gradient methods are also possible, but BKS97 has shown them to be inefficient in thiscontext.213step size How do we maintain the parameter constraints Are t and dt just functions of the tth trainingcase i.e., an online algorithm, or can they depend on all the training data i.e., a batch algorithm Wediscuss these and other issues below.The performance of the algorithms can be measured in two ways the quality of the local optimum whichis reached, and the speed with which it is reached. Clearly, both answers will depend on the topology of thespace, the starting point, and the direction of the walk. The topology of the space may depend on the networkstructure, the amount of missing data, and the number of training cases. In our experimental setup, we varyall three of these factors to see how robust our conclusions are. For any fixed space, we start all algorithmsoff at the same point, and use the same stopping criterion. An algorithm which converges faster is alwaysbetter, since, in any fixed amount of time, we can afford to try restarting from many different points the finalanswer can then be the best point visited, or some combination of all of them. We vary the starting point totest the robustness of our conclusions.The directionThe most obvious choice for the direction vector is the gradient of the loglikelihood functiongt  l1,    , lnt .As we saw in Section C.4.1, for tabular CPDs with parameters ijkdefP Xi  kPaXi  j  wikj , thisis given by log PrV ijkMm1PrXi  k,i  jVmijkC.8Another choice is the generalized gradient gt  tgt, where  is some negative definite projectionmatrix. It can be shown JJ93 that this is essentially what EM is doing, where t  Q, 1, andQ, i,j 2Q, ijis the Hessian of Q evaluated at , some interior point of the space, and Q is the expected completedataloglikelihoodQ hPrhV, log PrV, h Eh log PrV,H V,with H being the hidden variables and V the visibles. Thus EM is a quasiNewton variable metric optimization method.In XJ96 and JX95 they give exact formulas for  for a mixture of Gaussians model and a mixture ofexperts model, but in general  will be unknown. However, we can still compute the generalized gradient as214followsgt  Utt C.9where the EM update operator ist1  Ut  argmaxQt.Conjugate directionsIt is well known that making the search directions conjugate can dramatically speed up gradient descentalgorithms. The FletcherReevesPolakRibiere formula PVTF88 isd0  g0dt1  gt1  tdtt gTt1gt1gTt gtIt is also possible to compute conjugate generalized gradients. Jamshidian and Jennrich JJ93 propose thefollowing update ruled0  g0dt1  gt1  tdtt gTt1gt  gt1dTt gt1  gtThey show that this method is faster than EM for a variety of nonbelief net problems. Thiesson Thi95 hasapplied this method to belief nets, but does not report any experimental results.The step sizeBy substituting equation C.9 into equation C.7, we can rewrite the EM update rule ast1  t  tUt tt  1 tt  tUt. C.10We shall call this the EM rule. In BKS97, they derive this rule from an online learning perspective.The line minimization method suggests choosing a step size of t  arg max lt  dt. Sincethis can be quite slow, a simplification is to use a constant step size t  . For   1, this correspondsto the standard EM rule, and is guaranteed to converge to a local maximum, and to satisfy the positivityand summation constraints. Bauer et al. BKS97 show that sometimes the optimal learning rate is givenby   1 however, if   1, the algorithm is only guaranteed to converge to a local maxmimum if itstarts close enough to that maximum and if   2, there are no convergence guarantees at all. In the215OtherCarCostSocioEconAgeGoodStudentExtraCarMileageVehicleYearRiskAversionSeniorTrainDrivingSkill MakeModelDrivingHistDrivQualityAntilockAirbag CarValue HomeBase AntiTheftTheftOwnDamageOwnCarCostPropertyCostLiabilityCostMedicalCostCushioningRuggednessAccidentFigure C.1 A simple model of the car insurance domain, from RN95. Shaded nodes are hidden.experiments of JX95 on mixtures of experts, they found that using   1 often led to divergence, whereasin the experiments of BKS97, on relatively large belief nets the Alarm network and the Insurance network,they found that using   1.8 always led convergence, presumably because of the greater number of localmaxima. This rule is also studied in RW84 and elsewhere.Experimental comparisonWe generated 25 training cases from a small Bayes net Figure C.1 with random tabular CPDs we thenrandomly hid 20 and 50 of the nodes, and timed how long it took each algorithm to find a MLE.7 Someresults are shown in Tables C.2 and C.2. It can be seen that all methods reach similar quality local minima, butthat conjugate generalized gradient without linesearch is the fastest, and non conjugate generalized gradientwithout linesearch which is equivalent to EM is the second fastest. Conjugate methods are always faster,but, perhaps surprisingly, linesearching never seems to pay off, at least in this experiment.Note that a direct implementation of EM i.e., one which does not incur the overhead of explicitly computing the generalized gradient is about 1.6 faster than the results shown in the tables, which eliminates7The gradient descent code is closely based on the implementation given in PVTF88. Constraints were ensured by reparameterizingas follows ijk ijk2k ijk gradients were computed wrt ijk using the chain rule. The stopping criterion was when the percentagechange in the negative log likelihood dropped below 0.1. The belief net inference package was written by Geoff Zweig in C. Allexperiments were performed on a Sun Ultra Sparc, where 1 CPU second corresponds to roughly 1 real wall clock second this was1997.216Linemin. Gen. grad. Conj. final NLL iter CPUs fn.   300.0 105 74.5 210   300.4 115 88.2 230   294.8 70 29.6 70   297.9 33 14.0 66   297.1 62 229.6 677   296.3 26 50.0 301   294.0 60 83.6 547  296.4 28 41.3 129Table C.2 Performance of the various algorithms for a single trial with 20 hidden data and 25 trainingcases. Remember, the goal is to minimize the negative loglikelihood NLL. iter counts the number ofiterations of the outer loop. fn counts the number of times the likelihood function was called. The stepsize is fixed at 1.0. EM corresponds to the third line.Linemin. Gen. grad. Conj. final NLL iter CPUs fn.   210.5 129 121.8 258   210.4 122 112.6 244   202.5 80 44.3 160   215.3 23 12.6 46   215.4 53 215.3 580   210.3 46 100.3 524   201.4 79 143.0 737  204.1 55 102.4 528Table C.3 Performance of the various algorithms with 50 hidden data. The starting point is the same as theprevious experiment. Surprisingly, the best NLL is lower in this case. EM corresponds to the third line.much of the performance advantage of the conjugate method. JJ93, Thi95 suggest starting with EM, andthen only switching over to a conjugate gradient method when near a local optimum.On balance, it seems that EM has several advantages over gradient ascent. There is no stepsize parameter. It automatically takes care of parameter constraints. It can handle conjugate priors easily. It can handle deterministic constraints e.g., wijk  0. It is simple to implement.On the other hand, if the CPD does not have a closed form MLE, it is necessary to use gradient methodsin the M step this is called generalized EM. In this case, the argument in favor of EM is somewhat lesscompelling. Unfortunately, we did not compare the algorithms in this case.217C.4.4 Local minimaThe posterior over parameters in the partially observable case is heavily multimodal. This means that localsearch algorithms, like gradient ascent and EM, are prone to get stuck in local optima. A simple solution ismultiple restarts. Another popular solution is simulated annealing KJV94 unfortunately, this can be slow,because of the wasteful propose, evaluate, reject cycle.Deterministic annealing DA Ros98 is a faster alternative to simulated annealing, and works by enforcing a certain level of entropy noise in the system, which is gradually reduced. Recall from Section C.4.2that the variational free energy is the expected energy,  logPH  h, Vmq , plus the entropy,Hq. Theidea behind DA is to multiply the entropy by a temperature term T  initially the temperature is high, whichsmooths out the energy surface, so it is easy to find the maximum then the temperature is gradually reduced to T  1, which corresponds to the original problem. If the temperature is reduced slowly enough, itis possible to track the global optimum this is called a continuation method.In an EM context UN98, premultiplyingHq by T changes the E step high temperatures essentiallymake assignments to discrete variables softer. The M step is unchanged. For clustering problems, thereis a single discrete hidden variable, so we can change the E step in the following straightforward way theposterior P H v  P H, vh P h, v becomesfH v  P H, vh P h, vwhere   1T is an inverse temperature. In the case of more complex graphical models, the posterior is overjoint assignments controlling the entropy of this distribution requires modifying the inference algorithm. Forexample, RR01 explains how to apply DA to discriminatively train a bank of HMMs in addition to the and , two new quantities need to be computed in the forwards and backwards passes.A very simple and recent approach is presented in ENFS02. This exploits the fact that in many machinelearning problems, the score of the hypothesis decomposes into a sum of terms, one per training case. Byrandomly reweighting the training cases, the search algorithm can be encouraged to explore new parts ofspace. This is different from boosting, which generates an ensemble of models which are then averagedtogether. The reweighting method is simple to implement and demonstrated big performance increases onBN structure and parameter learning problems.C.4.5 Online parameter learning algorithmsThe formula for computing the gradient of the loglikelihood and the formula for computing the ESS neededfor the EM update both involve summing over all training cases. It is a simple modification to consider asingle training example at a time, and thus to derive an online algorithm see Figure C.2. This makes sensesince initially the parameters will be unreliable, so the ESS will be inaccurate hence it is best to update the218while not convergedfor each case mfor each node ioldESSi,m  ESSi,mESSi,m  computeESSDmXi, PaXi, iESSi  ESSi,m  oldESSi,mi  maximizeParamsESSi, iFigure C.2 Pseudocode for online EM for a Bayes net where all CPDs are in the exponential family, andhence have finite expected sufficient statistics ESS, based on NH98.parameters as soon as possible. This is sometimes called online or incremental EM NH98.The algorithm in Figure C.2 requires storing ESS for each CPD which might be less than the numberof nodes, if there is parameter tying and for each datacase. An approximation is to use an exponentiallydecaying average of recently visited data pointsESSi  ESSi  computeESSDmXi, PaXi, i,where 0    1. This will not converge to the right answer, at least not if  is held fixed, but using   1often gives a good approximation, and uses constant space essential if the data is streaming in.Instead of updating the parameters after every case, which can be slow, we can update after every Bcases, where B is the batch size. B  10 if often much faster than B  1 and B  M which is batch EMSto94, p40. A way to compute the optimal batch size is given in TMH01. For large data sets, it is oftenunnecessary to use all the data MTH01, HD02, which can result in huge time savings.An alternative online learning algorithm for Bayes nets is presented in BKS97 see also SW96 forthe HMM case, based on the online learning framework of KW97. In this framework the new set ofparameters t1 is chosen so that it maximises the normalized loglikelihood, but is not too different fromthe old parameters t. More precisely, we choose t1 to maximiseF   l d,t.To make the problem more tractable, l is replaced by a linear approximation. They show that by choosingthe distance function d to be the L2 norm, they get the projected gradient ascent rule of BKRK97 bychoosing d to be 2, they get the EM rule and by choosing d to be KLdistance, they get a generalizedexponentiated gradient rule EG. Their experimental results show that EM is much better than EG,and that EM for   1 converges much faster than for standard EM, which corresponds to   1, asdiscussed. They also suggest using small batches to compute the expected sufficient statistics in the EMupdate step, but do not test the performance of this experimentally. That is, the rule becomest1ijk  Et Xi  k, PaXi  jDEt PaXi  jD 1 tijk219whereEt xki , ji D 1BBm1Pt Xi  k, PaXi  jDmis a samplebased average. This has the advantage over the Neal and Hinton method of not needing to storemany tables ESSm, since we are adding the normalized ESS for the current case to the old parametervalues.It is folk wisdom in the neural network community that online i.e., stochastic gradient descent8 is notonly faster than batch gradient descent, but also works better, perhaps because by following an approximationto the total gradient, the algorithm is less likely to get stuck in local minima. Hence even in an offline setting,it can pay to do online updates.For a Bayesian approach to online parameter learning, see Section 4.4.2.C.5 Known structure, partial observability, BayesianEM and gradient ascent return a point estimate of the parameters. For many purposes, it is useful to return a distribution over the parameters. The most common method for achieving this is to use Gibbs sampling GG84, GRS96, which can be thought of as a stochastic version of EM. One can also use variational Bayes JJ00, Att00, GB00, which is based on assuming a factorized variational posterior of the formP ,H V   qV qH V . An alternative is expectation propagation Min01, which is an iterativeversion of the classical moment matching approach which is common in online Bayesian inference see Section B.7.2. See also RS98, which uses a moment matching technique, where the weights correspond topossible completions of the dataset hence this technique is only applicable to discrete data. The deterministic approximations usually assume unimodal posteriors hence they are essentially MAP estimates with errorbars.C.6 Unknown structure, full observability, frequentistWhen learning structure, we must consider the following issues What is the hypothesis space We can imagine searching the space of DAGs, equivalence classes ofDAGs PDAGs, undirected graphs, trees, node orderings, etc. What is the evaluation scoring function i.e., how do we decide which model we prefer What is the search algorithm We can imagine local search e.g., greedy hill climbing, possibly withmultiple restarts or global search e.g., simulated annealing or genetic algorithms.We discuss these issues in more detail below.8The term stochastic EM usually refers to using a sampling algorithm in the E step Mar01.220C.6.1 Search spaceTreesIf we are willing to restrict our hypothesis space to trees, we can find the optimal ML tree in OMN 2 timewhere N is the number of nodes and M is the number of data cases using the famous ChowLiu algorithmCL68, Pea88. We can use EM to extend this to mixtures of trees MJ00. Interestingly, learning the optimalML path a spanning tree in which no vertex has degree higher than two is NPhard Mee01.DAGsThe most common approach is to search through the space of DAGs. Unfortunately, the number of DAGs onN variables is 2ON2 log N Rob73, FK00. A more obvious upper bound is O2n2, which is the number ofN N adjacency matrices. For example, there are 543 DAGs on 4 nodes, and O1018 DAGs on 10 nodes.This means that attempts to find the true DAG, or even just to explore the posterior modes, are doomedto failure, although one might be able to find a good local maximum, which should be sufficient for densityestimation purposes.PDAGsA PDAG partially directed acyclic graph, also called a pattern or essential graph, represents a whole classof Markov equivalent DAGs. Two DAGs are Markov equivalent if they imply the same set of conditionalindependencies. For example, X  Y  Z, X  Y  Z and X  Y  Z are Markov equivalent, sincethey all represent X  ZY . In general, two graphs are Markov equivalent iff they have the same structureignoring arc directions, and the same vstructures VP90. A vstructure consists of converging directededges into the same node, such as X  Y  Z. Since we cannot distinguish members of the same Markovequivalence class if we only have observational data CY99, Pea00, it makes sense to search in the space ofPDAGs, which is smaller than the space of all DAGs about 3.714 times smaller GP01, Ste00. We discussmethods to do this in Section C.6.2. Of course, using PDAGs is not appropriate if we have experimentalinterventional as well as observational data. An intervention means setting forcing a node to a specificvalue, as opposed to observing that it has some value Pea00.Variable orderingsGiven a total ordering, the likelihood decomposes into a product of terms, one per family, since the parentsfor each node can be chosen independently there is no global acyclicity constraint. The following equation221was first noted in Bun91P D  GGni1scoreXi, PaGXiDiUU,iscoreXi, U D C.11G is the set of graphs consistent with the ordering, and U,i is the set of legal parents for node i consistentwith . If we bound the fanin number of parents by k, each summation in Equation C.11 takesnk nktime to compute, so the whole equation takes Onk1 time.Given an ordering, we can the find the best DAG consistent with that ordering using greedy selection, asin the K2 algorithm CH92, or more sophisticated variable selection methods see Section 6.1.2.If the ordering is unknown, we can search for it, e.g, using MCMC FK00 this is an example of RaoBlackwellisation. Not surprisingly, they claim this mixes much faster than MCMC over DAGs see Section C.7. However, the space of orderings has size N , which is still huge.Interestingly, interventions give us some hints about the ordering. For example, in a biological setting,if we knockout gene X1, and notice that genes X2 and X3 change from their wildtype state, but genesX4 and X5 do not, it suggests that X1 is the ancestor of X2 and X3. This heuristic, together with a setcovering algorithm, was used to learn acyclic boolean networks i.e., binary, deterministic Bayes nets frominterventional data ITK00.Undirected graphsWhen searching for undirected graphs, it is common to restrict attention to decomposable undirected graphs,so that the parameters of the resulting model can be estimated efficiently. See DGJ01 for a stepwise selectionapproach, and GGT00 for an MCMC approach.C.6.2 Search algorithmFor local search whether deterministic or stochastic, the operators that move through space are usuallyadding, deleting or reversing a single arc this defines the neighborhood of a graph, nbdG. KC01 considera richer set of local DAG transformations that gives better results. In addition, we must specify a startingpoint initial graph this could be chosen using the PC algorithm see below. As an example of a localsearch algorithm, the code for hill climbing is shown in Figure C.3.When we make a local change to a model, we would like the change in its score to be local see Section C.6.3 that way, evaluating the cost of many neighbors is fast. Similarly, if the current graph has acertain required property e.g., acyclicity, we would like the cost of checking if each of the neighbors hasthis property to be constant time i.e., independent of the model size. GC01 present a method for checking222Choose G somehowWhile not convergedFor each G in nbdGCompute scoreGG  arg maxG scoreGIf scoreG  scoreGthen G  Gelse converged  trueFigure C.3 Pseudocode for hillclimbing. nbdG is the neighborhood of G, i.e., the models that can bereached by applying a single local change operator.acyclicity in constant time by using an auxiliary data structure called the ancestor matrix, and GGT00 giveefficient ways to check for decomposability of undirected graphs.Global search comes in two flavors stochastic local search, where we allow downhill moves e.g.,MCMC, which includes simulated annealing as a special case, and search algorithms that make nonlocalchanges such as genetic algorithms.The PC algorithmWe can find the globally optimal PDAG inON k1Ntrain time, where there areN nodes,Ntrain data cases,and each node has at most k neighbors, using the PC algorithm SGS00, p84. This is an extension of theIC algorithm PV91, Pea00, which takes ONNNtrain time. This algorithm, an instance of the constraintbased approach, works as follows start with a fully connected undirected graph, and remove an arc betweenX and Y if there is some set of nodes S s.t., XY S we search for such separating subsets in increasingorder of size at the end, we can orient some of the undirected edges, so that we recover all the vstructuresin the PDAG.The PC algorithm will provably recover the generating PDAG if the conditional independency CI testsare all correct. For continuous data, we can implement the CI test using Fishers z test for discrete data, wecan use a 2 test SGS00, p95. Testing if XY S for discrete random variables requires creating a tablewith OK S2 entries, which requires a lot of time and samples. This is one of the main drawback of thePC algorithm. CGK02 contains a more efficient algorithm. The other difficulty with the PC algorithm ishow to implement CI tests on nonGaussian data e.g., mixed discretecontinuous. The analogous problemfor the the search  score methods is how to define the scoring function for complex CPDs. One approachis discussed in MT01. A way of converting a Bayesian scoring metric into a CI test is given in the appendixof Coo97.A more sophisticated version of the PC algorithm, called FCI fast causal inference, can handle the casewhere there is confounding due to latent common causes. However, FCI cannot handle models such as the223one in Figure C.7, where there is a nonroot latent variable. The ability to handle arbitrary latentvariablemodels is one of the main strengths of the search and score techniques.C.6.3 Scoring functionIf the search space is restricted e.g., to trees, maximum likelihood is an adequate criterion. However, if thesearch space is any possible graph, then maximum likelihood would choose the fully connected completegraph, since this has the greatest number of parameters, and hence can achieve the highest likelihood. In sucha case we will need to use other scoring metrics, which we discuss below.A wellprincipled way to avoid this kind of overfitting is to put a prior on models. By Bayes rule, theMAP model is the one that maximizesPrGD  PrDG PrGPrDwhere P D is a constant independent of the model. If the prior probability is higher for simpler modelse.g., ones which are sparser in some sense, the P G term has the effect of penalizing complex models.Interestingly, it is not necessary to explicitly penalize complex structures through the structural prior.The marginal likelihood sometimes called the evidence,P DG P DG, P Gautomatically penalizes more complex structures, because they have more parameters, and hence cannot giveas much probability mass to the region of space where the data actually lies, because of the sumtooneconstraint. In other words, a complex model is more likely to be right by chance, and is therefore lessbelievable. This phenomenon is called Ockhams razor see e.g., Mac95. Of course, we can combine themarginal likelihood with a structural prior to getscoreGdefP DGP GIf we assume all the parameters are independent, the marginal likelihood decomposes into a product oflocal terms, one per nodeP DG ni1iP XiPaXi, iP idefni1scorePaXi, XiUnder certain assumptions global and local parameter independence see Section C.3.1 plus conjugate priors, each of these integrals can be performed in closed form, so the marginal likelihood can be computedvery efficiently. For example, in Section C.6.3, we discuss the case of multinomial CPDs with Dirichlet priors. See GH94 for the case of linearGaussian CPDs with NormalWishart priors, and BS94, Bun94 for adiscussion of the general case.224If the priors are not conjugate, one can try to approximate the marginal likelihood. For example, Hec98shows that a Laplace approximation to the parameter posterior has the formlog PrDG  log PrDG, Gd2logMwhere M is the number of samples, G is the ML estimate of the parameters and d is the dimension numberof free parameters of the model. This is called the Bayesian Information Criterion BIC, and is equivalentto the Minimum Description Length MDL approach. The first term is just the likelihood and the secondterm is a penalty for model complexity. Note that the BIC score is independent of the parameter prior. TheBIC score also decomposes into a product of local terms, one per node. For example, for multinomials, wehave following Equation C.1BICscoreG imlogP XiPaXi, i, Dmdi2logMijkNijk log ijk di2logM C.12where di  qiri  1 is the number of parameters in Xis CPT.A major advantage of the fact that the score decomposes is that graphs that only differ by a singlelink have marginal likelihoods that differ by at most two terms, since all the others cancel. For example,let G1 be the chain X1  X2  X3  X4, and G2 be the same but with the middle arc reversedX1  X2  X3  X4. ThenP DG2P DG1scoreX1 scoreX1, X2, X3 scoreX3 scoreX3, X4scoreX1 scoreX1, X2 scoreX2, X3 scoreX3, X4scoreX1, X2, X3 scoreX3scoreX1, X2 scoreX2, X3In general, if an arc to Xi is added or deleted, only scoreXii needs to be evaluated if an arc betweenXi and Xj is reversed, only scoreXii and scoreXj j need to be evaluated. This is important since,in greedy search, we need to know the score of On2 neighbors at each step, but only On of these scoreschange if the steps change one edge at a time.The traditional limitation to local search in PDAG space has been the fact that the scoring function doesnot decompose into a set of local terms. This problem has recently been solved Chi02.Computing the marginal likelihood for the multinomialDirichletFor the case of multinomial CPDs with Dirichlet priors, we discussed how to compute the marginal likelihoodsequentially in Section C.3.1. To compute this in batch form, we simply compute the posterior means of theparameters Equation C.3, and plug these expected values into the sample likelihood equationP DG  P D,G ni1qij1rik1Nijkijk C.13225Alternatively, this can be written as follows CH92P DG ni1qij1Bij1 Nij1, . . . , i,j,ri Ni,j,riBij1, . . . , i,j,rini1qij1ijij Nijrik1ijk NijkijkC.14HGC95 call this the Bayesian Dirichlet BD score.For interventional data, Equation C.14 is modified by defining Nijk to be the number of times Xi  kis passively observed in the context i  j, as shown by CY99. The intuition is that setting Xi  k doesnot tell us anything about how likely this event is to occur by chance, and hence should not be counted.Hence, in addition to D, we need to keep a record of which variables were clamped if any in each data case.C.7 Unknown structure, full observability, BayesianSince we cannot hope to return the posterior over all models there are way too many of them, we try toevaluate the probability that certain features existP f D GGP GDfGfor some indicator function f of interest e.g., fG  1 if graph G contains a certain edge. This is calledBayesian model averaging HMRV99. For example, we can summarize the posterior by creating a graphwhere the weight of an edge is proportional to the marginal probability that it exists.Rather than trying to sum over all legal graphs, we can restrict our attention to those graphs that arerelatively likely relative to the most probable model found so far this is called Occams window MR94.In both cases, we need to computeP GD  P DGP GG P DGP GThe normalizing constant P D G P DGP G is intractable to compute, because there a superexponential number of graphs. To avoid this intractability, we can use the MetropolisHastings MH algorithm, which only requires that we be able to compute the posterior odds between the current candidatemodel, G1, and the proposed new model, G2P G2DP G1DP G2P G1 P DG2P DG1C.15The ratio of the evidences, P DG2P DG1 , is called the Bayes factor, and is the Bayesian equivalent of the likelihoodratio test.The idea of applying the MH algorithm to graphical models was first proposed in MY95, who calledthe technique MC3, for MCMC Model Composition. The basic idea is to construct a Markov Chain whose226Choose G somehowWhile not convergedPick a G u.a.r. from nbdGCompute R  P GDqGGP GDqGGSample u  Unif0, 1If u  min1, Rthen G  GFigure C.4 Pseudocode for the MC3 algorithm. u.a.r. means uniformly at random.state space is the set of all DAGs and whose stationary distribution is P GD. We achieve this as follows.Define a transition matrix or kernel, qGG. The only constraints on q are that the resulting chain shouldbe irreducible and aperiodic. We sample a new state G from this proposal distribution, G  qG, andaccept this new state with probability min1, R, where R is the acceptance rateR P GDP GD qGGqGGIf the kernel is symmetric, so qGG  qGG, the last term cancels, and the algorithm is called theMetropolis algorithm. The idea is to sample from this chain for long enough to ensure it has converged toits stationary distribution this is called the burnin time and throw these samples away any further samplesare then nonindependent samples from the true posterior, P GD, and can be used to estimate manyquantities of interest, such as P f D. This algorithm is given in pseudocode in Figure C.4.The issue of diagnosing convergence of MCMC algorithms is discussed in GRS96. The number ofsamples needed after reaching convergence depends on how rapidly the chain mixes i.e., moves aroundthe posterior distribution. To get a ballbark figure, GC01 useMC3 to find a distribution over the 3,781,503DAGs with 6 binary nodes of course, many of these are Markov equivalent, using a fully observed datasetwith 1,841 cases. They used T  100, 000 iterations with no burnin, but it seemed to converge after only10,000 iterations. To scale up to larger problems, it will obviously be necessary to use RaoBlackwellisation,such as searching over orderings instead of graphs see Section C.6.1.C.7.1 The proposal distributionMY95 suggested the following kernel. Define the neighborhood of the current state, nbdG, to be theset of DAGs which differ by 1 edge from G, i.e., we can generate nbdG by considering all single edgeadditions, deletions and reversals, subject to the acyclicity constraint. Then let qGG  1nbdG, forG  nbdG, and qGG  0 for G 6 nbdG, soR nbdGP GP DGnbdGP GP DG227The main advantage of this proposal distribution is that it is efficient to compute R when G and G onlydiffer by a single edge assuming complete data see Section C.6.3.The singleedgechange proposal can lead to high acceptance rates, but slow mixing, because it takessuch small steps through the space. An alternative would be to propose large changes, such as swappingwhole substructures, as in genetic programming. If one is not sure which proposal to use, one can alwayscreate a mixture distribution. The weights of this mixture are parameters that have to be tuned by hand.GRG96 suggest that the kernel should be designed so that the average acceptance rate is 0.25.A natural way to speed up mixing is to reduce the size of the search space. Suppose that, for each node,we restrict the maximum number of parents to be k, instead of n. This is reasonable, since we expect thefanin to be small. This reduces the number of parent sets we need to evaluate from O2n tonk nk.Some heuristics for choosing the set of k potential parents are given in FNP99. As long as we give nonzeroprobability to all possible edge changes in our proposal, we are guaranteed to get the correct answer sincewe can get from any graph to any other by single edge changes, and hence the chain is irreducible heuristicsmerely help us reach the right answer faster. The heuristics in FNP99 change with time, since they look forobserved dependencies that cannot be explained by the current model. Such adaptive proposal distributionscause no theoretical problems for convergence.C.8 Unknown structure, partial observability, frequentistIn the partially observable case, computing the marginal likelihood is intractable, requiring that we sum outall the latent variables Z as well as integrate out all the parameters P X G ZP X,ZG, P GThis score is hard to compute, and does not decompose into a product of local terms.There are two approaches to this problem try to approximate the marginal likelihood, and embed theapproximation into any of the search spacesalgorithms discussed above, or use a different scoring functionthe expected completedata marginal likelihood which does decompose. We will discuss each in turn.C.8.1 Approximating the marginal likeihoodOne accurate way of approximating the marginal likelihood, known as the Candidates method Chi95,Raf96, is as follows. We pick an arbitrary value G e.g., the MAP value, and computeP DG  P DG, GP GGP GD,GComputing P GG is trivial, and computing P DG, G can be done using any BN inference algorithm.The denominator can be approximated using Gibbs sampling see CH97 for details. See also SNR00 fora related approach, based on the harmonic mean estimator.228Various large sample approximations to the marginal likelihood, which are computationally cheaper, arecompared in CH97. The conclusion is that the CheesemanStutz CS approximation, which is a variation ofBIC see Section C.6.3, is the most accurate, at least in the case of naiveBayes mixture models. However,the accuracy of these techniques for small samples is suspect. Even for large samples, these approximationsare inaccurate at estimating P DG1P DG2, where G1 is the most probable model, and G2 is the second most probable. That is, these approximations can be useful for model selection, but less so for modelaveraging.Although the BIC and CS scores decompose, local search algorithms are still expensive, because weneed to run EM at each step to compute . Instead of doing EM inside of search, we can do search inside ofEM this turns out to be much faster see Section C.8.2.C.8.2 Structural EMThe key insight behind structural EM Fri97 is that the expected completedata loglikelihood, and hence theBIC score, does decompose. The algorithm works as follows use the current model and parameters, G, ,to compute the expected sufficient statistics ESS for all models in the neighborhood of G use ESSG toevaluate the expected BIC score of each neighbor G pick the best neighbor iterate until convergence. SeeFigure C.5. The algorithm can also be extended to use an approximate BDe score instead of BIC Fri98.The analysis of the algorithm relies on the auxiliary Q function, just as in parametric EM see Section C.4.2. In particular, defineQ, G,G hP hV, ,G logP V, h, G  penalty, GThe penalty depends on the dimensionality ofG. The dimensionality of a model with latent variables is lessthan the number of its free parameters GHM96. Fri97 proves the following theorem, by analogy withparametric EMTheorem Friedman. If QG, G,   QG, G, , then SG,   SG, , where SG,  logP DG,   penaltyG, .In other words, increasing the expected BIC score is guaranteed to increase the actual BIC score. In thecase of multinomials, we define the expected BIC score by analogy with Equation C.12 to beEBICscoreG ijkNijk log ijk di2logM C.16where di  qiri  1 is the number of parameters in Xis CPT in G,  are the MLE parameters for Gderived from Nijk, andNijk mP Xi  k, PaGXi  jDm, , G229Choose G somehowChoose  somehowWhile not convergedImprove  using parametric EMFor each G in nbdGCompute ESSG using G,  E stepCompute G using ESSGCompute EscoreG using ESSG, GG  arg maxG EscoreGIf EscoreG  EscoreGthen G  G structural M step  G parametric M stepelse converged  trueFigure C.5 Pseudocode for Structural EM. Compare with Figure C.3.This expected BIC score decomposes just like the regular BIC score.Since we may add extra edges, G might have families that are not in G. This means we may have tocompute joint probability distributions on sets of nodes that are not in any clique EH01 describes efficientways to do this using the jtree algorithm.It is difficult to choose the initial graph structure. We cannot use the PC algorithm because we have nonroot latent variables. On the other hand, we cannot choose, say, the empty graph, because our initial estimateof  will be poor. We therefore assume we have some good initial guess based on domain knowledge.C.9 Unknown structure, partial observability, BayesianSince we are already using MCMC to do Bayesian model averaging, it is natural to extend the statespace ofthe Markov chain so that it not only searches over models, but also over the values of the unobserved nodesYMHL95. The idea is to use Gibbs sampling, where we alternate between sampling a new model given thecurrent completed data set, and sampling a completed data set given the current model. This is basically anextension of the IP algorithm for data augmentation TW87, and is similar to structural EM.At a high level, the algorithm cycles through the following steps where V is the observed data and H isthe hidden data1. Sample Gt1  P GD,Ht  P GP D,HtG2. Compute P t1D,Ht, Gt13. Sample Zt1  P H D,Gt1, t1To avoid computing the normalizing constant implicit in the first step, we use the MH algorithm, andapproximate the Bayes factor by using the current completed dataset. This is an approximation to the expected230Choose G1 somehowCompute  using EMSample H1  P H G1, for t  1, 2, . . .Pick a G u.a.r. from nbdGtCompute B  P D,HtGP D,HtGtCompute R  nbdGtP GnbdGP GtBSample u  Unif0, 1If u  min1, Rthen Gt1  GCompute P t1D,Ht, Gt1 using Bayesian updatingCompute t1 from P t1D,Ht, Gt1Sample Ht1  P H D,Gt1, t1 using Gibbs samplingFigure C.6 Pseudocode for the MC3 algorithm modified to handle missing data. Choosing the initial graphstructure is hard, as discussed in Section C.8.2.a bFigure C.7 a A BN with a hidden variable H. b The simplest network that can capture the same distributionwithout using a hidden variable created using arc reversal and node elimination. If H is binary and the othernodes are trinary, and we assume full CPTs, the first network has 45 independent parameters, and the secondhas 708.completedata Bayes factor Bun94, Raf96P DGP DGt EP D,H GP D,H Gt D,Gt P D,HtGP D,HtGtC.17The second step can be performed in closed form for conjugate distributions see Section C.3.1 for themultinomialDirichlet case. Finally, we sample from the predictive distribution, P H D,Gt1, by usingthe mean parameter values, t1 see Section C.3.1, with Gibbs sampling. The overall algorithm is sketchedin Figure C.6. As far as I know, this has never been tried, probably because this algorithm is not only tryingto search the space of all DAGs, but also the space of all assignments to the hidden variables.231C.10 Inventing new hidden nodesSo far, structure learning has meant finding the right connectivity between preexisting nodes. A more challenging problem is inventing hidden nodes on demand. Hidden nodes can make a model much more compact,as we see in Figure C.7.The standard approach is to keep adding hidden nodes one at a time, performing structure learning ateach step, until the score drops. There has been some recent work on more intelligent heuristics. For example,dense cliquelike graphs such as that in Figure C.7b suggest that a hidden node should be added ELFK00.When learning DBNs, one can use violations of the Markov condition to suggest that a hidden node shouldbe added BFK99.C.11 Derivation of the CLG parameter estimation formulasIn this section, we derive the equations in Table C.1, since I have not seen a published derivation at this levelof generality before although some of these details appear in Bil99. Recall that the expected completedataloglikelihood is as followsL   12mEiqim log i qimym Bixm1i ym Bixm  DmC.11.1 Estimating the regression matrixTo find the ML estimates, we take derivatives of L w.r.t. Bi and i, set to 0, and solve. Using the identity Xa bCXa bX C  C Xa bawe haveBiL  12lwimE21i yl Bixlxllwim1i ymxmi lwim1i Bixmxmi  0HenceBi lwimymxmilwimxmxmi1In the special case that X is absent, so Bi  i and xl  1, this formula becomesi l wimymil wim232C.11.2 Estimating a full covariance matrixUsing the identitiesaXbX ab, ln X X X 1 and ln X    ln X1we have1iL   12lwimyl Bixlyl Bixli  12lwimi  12lwim ymymi Bixmymi  ymxmiBi BixmxmiBi  12lwimi  0Using the new value of Bi and the fact that i is symmetric, we findBilwimxmxmiBi lwimymxmilwimxmxmi1lwimxmymi BilwimxmymilwimymxmiBiHencei l wimymymil wimBil wimxmymil wimC.18If the covariance matrix is tied across states, we get1L   12liwimymymi Bixmymi  12liwim  0Sinceli wim  N we have il wimymymiNil wimBixmymiNC.19If X is absent, so Ai  i and xl  1, Equation C.18 becomes using the new estimate for ii l wimymymil wim ii C.20In the tied case, we get il wimymymiNil wimiymiNil wimymymiNil wimiiNC.21C.11.3 Estimating a spherical covariance matrixIf we have the constraint that i  2i I is isotropic, the conditional density of Y becomespyx,Q  i  cdi exp 122i y Bix2233HenceL liwimd logi  122i y Bix2isoiL  dlwim1i  3i wimyl Bixl2i  0and2i 1dl wimlwimyl Bixl2iNowyl Bixl2  yl Bixlyl Bixl  ylyl  xlBiBixl  2ylBixlTo compute the expected value of this distance, we use the fact that xAy  TrxAy  TrAyx, soExAy  TrAEyx. HencelwimylBixlylBixli  TrlwimymymiTrlwimBiBixmxmi2TrlwimBixmymiNotice that, if Y is observed, this becomeslwimyl Bixlyl Bixli lwimylyl  TrlwimBiBixmxmi 2lylwimBixmiWe will see later that, if Ai  i and X  1, we can eliminate all the trace operators.Now TrA  TrB  TrAB, so2i 1dl wimTrlwimymymi lwimBiBixmxmi  2lwimBixmymiC.22If 2i is tied, we get2 1dNTrilwimymymi iiwimBiBixmxmi  2ilwimBixmymiC.23In the special case that X is absent, so Bi  i and xl  1, this formula becomes much simpler, sincelwimyl Bixlyl Bixli lwim ymymi  ii  2ymiiso, using the new estimate for i,2i 1dl wimymymil wim iiC.24For the tied case, we get2 1Ndilwimymymi ilwimiiC.25234Appendix DNotation and abbreviationsA summary of the most frequently used notation and abbreviations appears in the tables below. We also adoptthe standard convention that random variables are denoted as capital letters, and instantiations of randomvariables values are denoted as lowercase letters. We do not explicitely distinguish between scalars andvectors. When working with linear algebra, we denote vectors which may be random variables as lowercase, to distinguish them from matrices, which are always upper case.Symbol MeaningUt Control intput at time tXt Hidden state variable at time tYt Observation output at time tJ Size of input state spaceK Size of hidden state spaceL Size of output state spaceT Length of sequencel Lagh Prediction horizonNtrain Num. training sequencesym1T Training sequence mTable D.1 Notation for general statespace models.235Symbol MeaningZit ith variable in timeslice tNh Num. hidden variables per timesliceN Num. variables per timesliceS Num. hidden states, KNhTable D.2 Notation for DBNs.Symbol Meaningj Initial state probability P X1  jAi, j Transition probability P Xt  jXt1  iBi, j Discrete observation probability P Yt  jXt  iOti, i Observation likelihood at time t P ytXt  iti P Xt  iy1tti P yt1T Xt  iti P Xt  iy1T t1,ti, j P Xt1  i,Xt  jy1T Table D.3 Notation for HMMs.Symbol MeaningX it ith sample of hidden variable XtX itj ith sample of jth component of hidden variable XtNs Num. samplesNL Num. landmarksTable D.4 Notation for particle filtering.P Xt  xtXt1  xt1, Ut  u  N xtAxt1 Bu X , QP Yt  yXt  x, Ut  u  N yCx Du Y , RN y,  12L212exp 12 y  1y  Table D.5 Notation for a Kalman filter model.236Abbreviation MeaningBK BoyenKollerCLG Conditional linear GaussianCPD Conditional probability distributionDBN Dynamic Bayesian networkEM Expectation maximizationESS Expected sufficient statisticsFF Factored frontierFgraph factor graphHMM Hidden Markov modelHHMM Hierarchical HMMiid independent, identically distributedJtree Junction treeKFM Kalman filter modelLBP Loopy belief propagationMAP Maximum a posterioriMLE Maximum likelihood estimatePF Particle filteringRBPF RaoBlackwellised particle filteringSLAM Simultaneous localization and mappingTable D.6 List of abbrevations.237BibliographyAC95 C. F. Aliferis and G. F. Cooper. A Structurally and Temporally Extended Bayesian BeliefNetwok Model Definitions, Properties and Modeling Techniques. In UAI, 1995.ACP87 S. Arnborg, D. G. Corneil, and A. Proskurowski. Complexity of finding embeddings in aktree. SIAM J. on Algebraic and Discrete Methods, 8277284, 1987.AdFD00 C. Andrieu, N. de Freitas, and A. Doucet. Sequential Bayesian estimation and model selection for dynamic kernel machines. Technical report, Cambridge Univ., 2000.AK77 H. Akashi and H. Kumamoto. Random sampling approach to state estimation in switchingenvironments. Automatica, 13429434, 1977.AM79 B. Anderson and J. Moore. Optimal Filtering. PrenticeHall, 1979.AM00 S. M. Aji and R. J. McEliece. The generalized distributive law. IEEE Trans. Info. Theory,462325343, March 2000.AMGC02 M. Arulampalam, S. Maskell, N. Gordon, and T. Clapp. A Tutorial on Particle Filters forOnline NonlinearNonGaussian Bayesian Tracking. IEEE Trans. on Signal Processing,502174189, February 2002.Ami01 E. Amir. Efficient approximation for triangulation of minimum treewidth. In UAI, 2001.Aok87 M. Aoki. State space modeling of time series. Springer, 1987.Arn85 S. A. Arnborg. Efficient algorithms for combinatorial problems on graphs with boundeddecomposability  a survey. Bit, 25223, 1985.Ast65 K. Astrom. Optimal control of Markov decision processes with incomplete state estimation.J. Math. Anal. Applic., 10174205, 1965.Att00 H. Attias. A variational Bayesian framework for graphical models. In NIPS12, 2000.238Bar01 D. Barber. Tractable approximate belief propagation. In M. Opper and D. Saad, editors,Advanced mean field methods. MIT Press, 2001.BB72 U. Bertele and F. Brioschi. Nonserial Dynamic Programming. Academic Press, 1972.BB96 S. Bengio and Y. Bengio. An EM algorithm for asynchronous inputoutput HMMs. In Proc.Intl. Conf. on Neural Info. Processing ICONIP, 1996.BBT02 M. Bennewitz, W. Burgard, and S. Thrun. Learning motion patterns of persons for mobileservice robots. In Proc. Intl. Conf. on Robotics and Automation ICRA, 2002.BC94 P. Baldi and Y. Chauvin. A smooth learning algorithm for hidden Markov models. NeuralComputation, 6305316, 1994.BDG01 C. Boutilier, R. Dearden, and M. Goldszmidt. Stochastic dynamic programming with factored representations. Artificial Intelligence, 2001.Ben99 Y. Bengio. Markovian models for sequential data. Neural Computing Surveys, 2129162,1999.Ber A. Berger. Maxent and exponential models. httpwww2.cs.cmu.edu abergermaxent.html.Ber85 J. Berger. Statistical Decision Theory and Bayesian Analysis. SpringerVerlag, 1985.BF95 Y. Bengio and P. Frasconi. Diffusion of context and credit information in markovian models.J. of AI Research, 3249270, 1995.BF96 Y. Bengio and P. Frasconi. Inputoutput HMMs for sequence processing. IEEE Trans. onNeural Networks, 7512311249, 1996.BFG93 R. I. Bahar, E. A. Frohm, C. M. Gaona, G. D. Hachtel, E. Macii, A. Pardo, and F. Somenzi.Algebraic decision diagrams and their applications. In Intl. Conf. on ComputedAided Design, pages 188191, 1993.BFGK96 C. Boutilier, N. Friedman, M. Goldszmidt, and D. Koller. ContextSpecific Independence inBayesian Networks. In UAI, 1996.BFK99 X. Boyen, N. Friedman, and D. Koller. Discovering the hidden structure of complex dynamicsystems. In UAI, 1999.BG96 A. Becker and D. Geiger. A sufficiently fast algorithm for finding close to optimal junctiontrees. In UAI, 1996.239BG01 C. Berzuini and W. Gilks. RESAMPLEMOVE Filtering with CrossModel Jumps. InA. Doucet, N. de Freitas, and N. Gordon, editors, Sequential Monte Carlo Methods in Practice. Springer, 2001.BGP97 E. Bienenstock, S. Geman, and D. Potter. Compositionality, MDL priors, and object recognition. In NIPS, 1997.BHK93 M. P. Brown, R. Hughey, A. Krogh, I. S. Mian, K. Sjolander, and D. Haussler. Usingdirichlet mixtures priors to derive hidden Markov models for protein families. In Intl. Conf.on Intelligent Systems for Molecular Biology, pages 4755, 1993.Bil98 J. Bilmes. Datadriven extensions to HMM statistical dependencies. In Intl. Conf. SpeechLang. Proc., 1998.Bil99 J. Bilmes. Natural Statistical Models for Automatic Speech Recognition. PhD thesis, CSDivision, U.C. Berkeley, 1999.Bil00 J. Bilmes. Dynamic Bayesian multinets. In UAI, 2000.Bil01 J. A. Bilmes. Graphical models and automatic speech recognition. Technical ReportUWEETR20010005, Univ. Washington, Dept. of Elec. Eng., 2001.Bis95 C. M. Bishop. Neural Networks for Pattern Recognition. Clarendon Press, 1995.BJ01 F. Bach and M. I. Jordan. Kernel independent component analysis. Technical Report CSD011166, Comp. Sci. Div., UC Berkeley, 2001.BJGGJ02 Z. BarJoseph, G. Gerber, D. Gifford, and T. Jaakkola. A new approach to analyzing geneexpression time series data. In 6th Annual Intl. Conf. on Research in Computational Molecular Biology, 2002.BK98a X. Boyen and D. Koller. Approximate learning of dynamic models. In NIPS11, 1998.BK98b X. Boyen and D. Koller. Tractable inference for complex stochastic processes. In UAI, 1998.BK99 X. Boyen and D. Koller. Exploiting the architecture of dynamic systems. In AAAI, 1999.BKRK97 J. Binder, D. Koller, S. J. Russell, and K. Kanazawa. Adaptive probabilistic networks withhidden variables. Machine Learning, 29213244, 1997.BKS97 E. Bauer, D. Koller, and Y. Singer. Batch and online parameter estimation in Bayesiannetworks. In UAI, 1997.240BLN01 O. Bangso, H. Langseth, and T. Nielsen. Structural learning in object oriented domains. InProc. 14th Intl Florida Articial Intelligence Research Society Conference FLAIRS2001,2001.BMI99 C. Bielza, P. Mueller, and D. Rios Insua. Decision analysis by augmented probability simulation. Management Science, 4579951007, 1999.BMR97a J. Binder, K. Murphy, and S. Russell. Spaceefficient inference in dynamic probabilisticnetworks. In IJCAI, 1997.BMR97b S. Bistarelli, U. Montanari, and F. Rossi. Semiringbased constraint satisfaction and optimization. J. of the ACM, 442201236, 1997.BPSW70 L. E. Baum, T. Petrie, G. Soules, and N. Weiss. A maximization technique occuring in thestatistical analysis of probabalistic functions in markov chains. The Annals of MathematicalStatistics, 41164171, 1970.Bra96 M. Brand. Coupled hidden Markov models for modeling interacting processes. TechnicalReport 405, MIT Lab for Perceptual Computing, 1996.Bra99a M. Brand. Structure learning in conditional probability models via an entropic prior andparameter extinction. Neural Computation, 1111551182, 1999.Bra99b T. Brants. Cascaded markov models. In Proc. 9th Conf. of European Chapter of ACL, 1999.BRP01 C. Boutilier, R. Reiter, and B. Price. Symbolic Dynamic Programming for FirstOrderMDPs. In IJCAI, 2001.BS90 Y. BarShalom, editor. Multitargetmultisensor tracking  advanced applications. ArtechHouse, 1990.BS94 J. Bernardo and A. Smith. Bayesian Theory. John Wiley, 1994.BSF88 Y. BarShalom and T. Fortmann. Tracking and data association. Academic Press, 1988.BSL93 Y. BarShalom and X. Li. Estimation and Tracking Principles, Techniques and Software.Artech House, 1993.Bun89 W. Buntine. Decision tree induction systems a Bayesian analysis. Uncertainty in AI, 3109127, 1989.Bun91 W. Buntine. Theory refinemement on Bayesian networks. In UAI, 1991.241Bun94 W. L. Buntine. Operations for learning with graphical models. J. of AI Research, pages159225, 1994.BVW00a H. Bui, S. Venkatesh, and G. West. On the recognition of abstract Markov policies. In AAAI,2000.BVW00b H. Bui, S. Venkatesh, and G. West. Policy Recognition in the Abstract Hidden MarkovModel. Technical Report 42000, School of Computing Science, Curtin Univ. of Technology,Perth, 2000.BVW01 H. Bui, S. Venkatesh, and G. West. Tracking and surveillance in widearea spatial environments using the Abstract Hidden Markov Model. Intl. J. of Pattern Rec. and AI, 2001.BW00 O. Bangso and P. Wuillemin. Topdown construction and repetitive structures representationin Bayesian networks. In FLAIRS, 2000.BZ02 J. Bilmes and G. Zweig. The graphical models toolkit An open source software systemfor speech and timeseries processing. In Intl. Conf. on Acoustics, Speech and Signal Proc.,2002.CB97 A. Y. W. Cheuk and C. Boutilier. Structured arc reversal and simulation of dynamic probabilistic networks. In UAI, 1997.CD00 J. Cheng and M. Druzdzel. AISBN An adaptive importance sampling algorithm for evidential reasoning in large Bayesian networks. J. of AI Research, 13155188, 2000.CDLS99 R. G. Cowell, A. P. Dawid, S. L. Lauritzen, and D. J. Spiegelhalter. Probabilistic Networksand Expert Systems. Springer, 1999.CF95 K. C. Chang and R. M. Fung. Symbolic probabilistic inference with both discrete andcontinuous variables. IEEE Trans. on Systems, Man, and Cybernetics, 256910917, 1995.CG96 S. Chen and J. Goodman. An empirical study of smoothing techniques for language modeling. In Proc. 34th ACL, pages 310318, 1996.CGH97 E. Castillo, J. M. Gutierrez, and A. S. Hadi. Expert systems and probabilistic networkmodels. Springer, 1997.CGK02 J. Cheng, R. Greiner, J. Kelly, D. Bell, and W. Liu. Learning bayesian networks from dataAn informationtheory based approach. Artificial Intelligence Journal, 2002. To appear.242CH92 G. Cooper and E. Herskovits. A Bayesian method for the induction of probabilistic networksfrom data. Machine Learning, 9309347, 1992.CH96 I.J. Cox and S.L. Hingorani. An Efficient Implementation of Reids Multiple HypothesisTracking Algorithm and its Evaluation for the Purpose of Visual Tracking. IEEE Trans. onPattern Analysis and Machine Intelligence, 182138150, 1996.CH97 D. Chickering and D. Heckerman. Efficient approximations for the marginal likelihood ofincomplete data given a Bayesian network. Machine Learning, 29181212, 1997.Chi95 S. Chib. Marginal likelihood from the Gibbs output. JASA, 9013131321, 1995.Chi02 David Maxwell Chickering. Learning equivalence classes of Bayesiannetwork structures.Journal of Machine Learning Research, 2445498, February 2002.CK96 C. Carter and R. Kohn. Markov chain Monte Carlo in conditionally Gaussian state spacemodels. Biometrika, 83589601, 1996.CL68 C. K. Chow and C. N. Liu. Approximating discrete probability distributions with dependence trees. IEEE Trans. on Info. Theory, 1446267, 1968.CL00 R. Chen and S. Liu. Mixture Kalman filters. J. Royal Stat. Soc. B, 2000.CLR90 T. H. Cormen, C. E. Leiserson, and R. L. Rivest. An Introduction to Algorithms. MIT Press,1990.Coo97 G. F. Cooper. A simple constraintbased algorithm for efficiently mining observationaldatabases for causal relationships. Data Mining and Knowledge Discovery, 1203224,1997.Coz00 F. Cozman. Generalizing variableelimination in Bayesian Networks. In Workshop on Prob.Reasoning in Bayesian Networks at SBIAIberamia, pages 2126, 2000.CR96 G Casella and C P Robert. RaoBlackwellisation of sampling schemes. Biometrika,8318194, 1996.CT91 T. M. Cover and J. A. Thomas. Elements of Information Theory. John Wiley, 1991.CT02 K. C. Chang and Z. Tian. Efficient Inference for Mixed Bayesian Networks. In Proc. 5thISIFIEEE Intl. Conf. on Info. Fusion Fusion 02, 2002.CTS78 C. Cannings, E. A. Thompson, and M. H. Skolnick. Probability functions in complex pedigrees. Advances in Applied Probability, 102661, 1978.243Cur97 R. M. Curds. Propagation Techniques in Probabilistic Expert Systems. PhD thesis, Dept.Statistical Science, Univ. College London, 1997.CY99 G. Cooper and C. Yoo. Causal discovery from a mixture of experimental and observationaldata. In UAI, 1999.DA99 A. Doucet and C. Andrieu. Iterative Algorithms for State Estimation of Jump Markov LinearSystems. Technical report, Cambridge Univ. Engineering Dept., 1999.Dah00 R. Dahlhaus. Graphical interaction models for multivariate time series. Metrika, 51157172, 2000.Dar95 A. Darwiche. Conditioning algorithms for exact and approximate inference in causal networks. In UAI, 1995.Dar00 A. Darwiche. Recursive conditioning Any space conditioning algorithm with treewidthbounded complexity. Artificial Intelligence Journal, 2000.Dar01 A. Darwiche. Constant Space Reasoning in Dynamic Bayesian Networks. Intl. J. of Approximate Reasoning, 26161178, 2001.Daw92 A. P. Dawid. Applications of a general propagation algorithm for probabilistic expert systems. Statistics and Computing, 22536, 1992.DdFG01 A. Doucet, N. de Freitas, and N. J. Gordon. Sequential Monte Carlo Methods in Practice.Springer Verlag, 2001.DdFMR00 A. Doucet, N. de Freitas, K. Murphy, and S. Russell. Raoblackwellised particle filtering fordynamic Bayesian networks. In UAI, 2000.DDN01 R. Deventer, J. Denzler, and H. Niemann. Bayesian Control of Dynamic Systems. Technicalreport, Lehrstuhl fur Mustererkennung, Institut fur Informatik, Uni. Nurnberg, 2001.DE00 R. Dahlhaus and M. Eichler. Causality and graphical models for time series. In P. Green,N. Hjort, and S. Richardson, editors, Highly structured stochastic systems. Oxford University Press, 2000.Dec98 R. Dechter. Bucket elimination a unifying framework for probabilistic inference. In M. Jordan, editor, Learning in Graphical Models. MIT Press, 1998.DeG70 M. DeGroot. Optimal Statistical Decisions. McGrawHill, 1970.244DEKM98 R. Durbin, S. Eddy, A. Krogh, and G. Mitchison. Biological Sequence Analysis Probabilistic Models of Proteins and Nucleic Acids. Cambridge University Press, Cambridge,1998.DF99 P. Dellaportas and J. Forster. Markov chain Monte Carlo model determination for hierarchical and graphical loglinear models. Biometrika, 1999. To appear.DG95 P. Dagum and A. Galper. Timeseries prediction using belief network models. Intl. J. ofHumanComputer Studies, 42617632, 1995.DGJ01 A. Deshpande, M. N. Garofalakis, and M. I. Jordan. Efficient stepwise selection in decomposable models. In UAI, 2001.DGK99 A. Doucet, N. Gordon, and V. Krishnamurthy. Particle Filters for State Estimation of JumpMarkov Linear Systems. Technical report, Cambridge Univ. Engineering Dept., 1999.Die93 F. J. Diez. Parameter adjustment in Bayes networks. The generalized noisyor gate. In UAI,1993.DK89 T. Dean and K. Kanazawa. A model for reasoning about persistence and causation. ArtificialIntelligence, 9312127, 1989.DK00 J. Durbin and S. J. Koopman. Time series analysis of nonGaussian observations based onstate space models from both classical and Bayesian perspectives with discussion. J. RoyalStat. Soc. B, 2000. To appear.DK01 J. Durbin and S. J. Koopman. Time Series Analysis by State Space Methods. Oxford University Press, 2001.DK02 J. Durbin and S.J. Koopman. A simple and efficient smoother for state space time seriesanalysis. Biometrika, 2002. To appear.DL93 P. Dagum and M. Luby. Approximating probabilistic inference in Bayesian belief networksis NPhard. Artificial Intelligence, 60141153, 1993.DLR77 A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete datavia the EM algorithm. J. of the Royal Statistical Society, Series B, 34138, 1977.DM95 E. Driver and D. Morrel. Implementation of continuous Bayesian networks usings sums ofweighted Gaussians. In UAI, pages 134140, 1995.245DNCDW01 M. G. Dissanayake, P. Newman, S. Clark, and H. F. DurrantWhyte. A Solution to theSimultaneous Localization and Map Building SLAM Problem. IEEE Trans. Robotics andAutomation, 173229241, 2001.Dou98 A Doucet. On sequential simulationbased methods for Bayesian filtering. Technical reportCUEDFINFENGTR 310, Department of Engineering, Cambridge University, 1998.DP97 A. Darwiche and G. Provan. Query DAGs A practical paradigm for implementing beliefnetwork inference. J. of AI Research, 6147176, 1997.DRO93 V. Digalakis, J. R. Rohlicek, and M. Ostendorf. ML estimation of a stochastic linear systemswith the EM algorithm and its application to speech recognition. IEEE Trans. on Speech andAudio Proc., 14421442, 1993.Duf02 M. Duff. Optimal Learning Computational procedures for Bayesadaptive Markov decisionprocesses. PhD thesis, U. Mass. Dept. Comp. Sci., 2002.DW91 T. Dean and M. Wellman. Planning and Control. Morgan Kaufmann, 1991.Edw00 D. Edwards. Introduction to graphical modelling. Springer, 2000. 2nd edition.EH01 T. ElHay. Efficient methods for exact and approximate inference in graphical models. Masters thesis, Hebrew Univ., Dept. Comp. Sci., 2001.Eic01 M. Eichler. Markov properties for graphical time series models. Technical report, Dept.Statistics, Univ. Heidelberg, 2001.EL01 D. Edwards and S. L. Lauritzen. The TM algorithm for Maximizing a Conditional Likelihood Function. Biometrika, 88961972, 2001.ELFK00 G. Elidan, N. Lotner, N. Friedman, and D. Koller. Discovering hidden variables A structurebased approach. In NIPS, 2000.ENFS02 G. Elidan, M. Ninion, N. Friedman, and D. Schuurmans. Data perturbation for escapinglocal maxima in learning. In AAAI, 2002.FBT98 D. Fox, W. Burgard, and S. Thrun. Active Markov localization for mobile robots. Roboticsand Autonomous Systems, 1998.FC89 R. Fung and K. Chang. Weighting and integrating evidence for stochastic simulation inBayesian networks. In UAI, 1989.246FG96 N. Friedman and M. Goldszmidt. Learning Bayesian networks with local structure. In UAI,1996.FG97 N. Friedman and M. Goldszmidt. Sequential update of Bayesian network structure. In UAI,1997.FGKP99 N. Friedman, L. Getoor, D. Koller, and A. Pfeffer. Learning probabilistic relational models.In IJCAI, 1999.FGL00 N. Friedman, D. Geiger, and N. Lotner. Likelihood computations using value abstraction.In UAI, 2000.FHKR95 J. Forbes, T. Huang, K. Kanazawa, and S. Russell. The BATmobile Towards a Bayesianautomated taxi. In IJCAI, 1995.FK00 N. Friedman and D. Koller. Being Bayesian about network structure. In UAI, 2000.FKP98 N. Friedman, D. Koller, and A. Pfeffer. Structured representation of complex stochasticsystems. In AAAI, 1998.FMR98 N. Friedman, K. Murphy, and S. Russell. Learning the structure of dynamic probabilisticnetworks. In UAI, 1998.FN00 N. Friedman and I. Nachman. Gaussian Process Networks. In UAI, 2000.FNP99 N. Friedman, I. Nachman, and D. Peer. Learning Bayesian network structure from massivedatasets The sparse candidate algorithm. In UAI, 1999.FP69 D. C. Fraser and J. E. Potter. The optimum linear smoother as a combination of two optimumlinear filters. IEEE Trans. on Automatical Control, pages 387390, August 1969.FPC00 W. T. Freeman, E. C. Pasztor, and O. T. Carmichael. Learning lowlevel vision. Intl. J.Computer Vision, 2000.Fri97 N. Friedman. Learning Bayesian networks in the presence of missing values and hiddenvariables. In UAI, 1997.Fri98 N. Friedman. The Bayesian structural EM algorithm. In UAI, 1998.FS02 B. Fischer and J. Schumann. Generating data analysis programs from statistical models. J.Functional Programming, 2002.247FST98 S. Fine, Y. Singer, and N. Tishby. The hierarchical Hidden Markov Model Analysis andapplications. Machine Learning, 3241, 1998.FTBD01 D. Fox, S. Thrun, W. Burgard, and F. Dellaert. Particle filters for mobile robot localization.In A. Doucet, N. de Freitas, and N. Gordon, editors, Sequential Monte Carlo Methods inPractice. Springer, 2001.FW00 W. Freeman and Y. Weiss. On the fixed points of the maxproduct algorithm. IEEE Trans.on Info. Theory, 2000. To appear.Gal99 M. J. F. Gales. Semitied covariance matrices for hidden Markov models. IEEE Trans. onSpeech and Audio Processing, 73272281, 1999.GB00 Z. Ghahramani and M. Beal. Propagation algorithms for variational Bayesian learning. InNIPS13, 2000.GC92 R. P. Goldman and E. Charniak. Probabilistic text understanding. Statistics and Computing,22105114, 1992.GC01 P. Giudici and R. Castelo. Improving Markov chain Monte Carlo model search for datamining. Machine Learning, 2001. To appear.GDW00 S. Godsill, A. Doucet, and M. West. Methodology for Monte Carlo Smoothing with Application to TimeVarying Autoregressions. In Proc. Intl. Symp. on Frontiers of Time SeriesModelling, 2000.GG84 S. Geman and D. Geman. Stochastic relaxation, Gibbs distributions, and the Bayesianrestoration of images. IEEE Trans. on Pattern Analysis and Machine Intelligence, 66,1984.GGT00 P. Giudici, P. Green, and C. Tarantola. Efficient model determination for discrete graphicalmodels. Biometrika, 2000. To appear.GH94 D. Geiger and D. Heckerman. Learning Gaussian networks. In UAI, volume 10, pages235243, 1994.GH96a Z. Ghahramani and G. Hinton. The EM algorithm for mixtures of factor analyzers. Technicalreport, Dept. of Comp. Sci., Uni. Toronto, 1996.GH96b Z. Ghahramani and G. Hinton. Parameter estimation for linear dynamical systems. TechnicalReport CRGTR962, Dept. Comp. Sci., Univ. Toronto, 1996.248GH97 D. Geiger and D. Heckerman. A characterization of Dirchlet distributions through local andglobal independence. Annals of Statistics, 2513441368, 1997.GH98 Z. Ghahramani and G. Hinton. Variational learning for switching statespace models. NeuralComputation, 124963996, 1998.GHM96 D. Geiger, D. Heckerman, and C. Meek. Asymptotic model selection for directed networkswith hidden variables. In UAI, 1996.GJ97 Z. Ghahramani and M. Jordan. Factorial hidden Markov models. Machine Learning,29245273, 1997.GK95 S. Glesner and D. Koller. Constructing flexible dynamic belief networks from firstorderprobalistic knowledge bases. In Symbolic and Quantitative Approaches to Reasoning andUncertainty, pages 217226, 1995.GLS99 G. Gottlob, N. Leone, and F. Scarello. A comparison of structural CSP decompositionmethods. In IJCAI, 1999.GMW81 P. E. Gill, W. Murray, and M. H. Wright. Practical Optimization. Academic Press, 1981.Goo99 J. Goodman. Semiring parsing. Computational Linguistics, 254573605, December 1999.Goo01 J. Goodman. Reduction of maximum entropy models to hidden markov models. Technicalreport, Microsoft Research, 2001.Gor93 N. Gordon. Novel approach to nonlinearnonGaussian Bayesian state estimation. IEEProceedings F, 1402107113, 1993.GP01 S. Gillispie and M. Perlman. Enumerating Markov Equivalence Classes of Acyclic DigraphModels. In UAI, 2001.GR98 Z. Ghahramani and S. Roweis. Learning Nonlinear Stochastic Dynamics using the Generalized EM Algorithm. In NIPS, 1998.Gre93 U. Grenander. General Pattern Theory. Oxford University Press, 1993.Gre98 P. Green. Reversible jump Markov chain Monte Carlo computation and Bayesian modeldetermination. Biometrika, 82711732, 1998.GRG96 A. Gelman, G. Roberts, and W. Gilks. Efficient Metropolis jumping rules. In J. Bernardo,J. Berger, A. Dawid, and A. Smith, editors, Bayesian Statistics 5. Oxford, 1996.249GRS96 W. Gilks, S. Richardson, and D. Spiegelhalter. Markov Chain Monte Carlo in Practice.Chapman and Hall, 1996.Gui02 L. Guibas. Sensing, tracking, and reasoning with relations. IEEE Signal Processing Magazine, March 2002.Hal96 F. L. Hall. Traffic stream characteristics. In N.H. Gartner, C.J. Messer, and A.K. Rathi,editors, Traffic Flow Theory. US Federal Highway Administration, 1996.Ham90 J. Hamilton. Analysis of time series subject to changes in regime. J. Econometrics, 453970, 1990.Ham94 J. Hamilton. Time Series Analysis. Wiley, 1994.Har89 A. C. Harvey. Forecasting, Structural Time Series Models, and the Kalman Filter. Cambridge Univerity Press, 1989.HB94 D. Heckerman and J.S. Breese. Causal Independence for Probability Assessment and Inference Using Bayesian Networks. IEEE Trans. on Systems, Man and Cybernetics, 266826831, 1994.HCM00 D. Heckerman, D. Chickering, C. Meek, R. Rounthwaite, and C. Kadie. Dependency networks for density estimation, collaborative filtering, and data visualization. Technical ReportMSRTR0016, Microsoft Research, 2000.HD96 C. Huang and A. Darwiche. Inference in belief networks A procedural guide. Intl. J.Approx. Reasoning, 153225263, 1996.HD02 G. Hulten and P. Domingos. Learning from infinite data in finite time. In NIPS14, 2002.Hec89 D. Heckerman. A tractable inference algorithm for diagnosing multiple diseases. In UAI,1989.Hec93 D. Heckerman. Causal independence for knowledge acquistion and inference. In UAI, 1993.Hec95 D. Heckerman. A Bayesian approach to learning causal networks. In UAI, 1995.Hec98 D. Heckerman. A tutorial on learning with Bayesian networks. In M. Jordan, editor, Learning in Graphical Models. MIT Press, 1998.HGC95 D. Heckerman, D. Geiger, and M. Chickering. Learning Bayesian networks the combination of knowledge and statistical data. Machine Learning, 1995.250HIM00 M. Hu, C. Ingram, M.Sirski, C. Pal, S. Swamy, and C. Patten. A Hierarchical HMM Implementation for Vertebrate Gene Splice Site Prediction. Technical report, Dept. ComputerScience, Univ. Waterloo, 2000.HMRV99 J. Hoeting, D. Madigan, A. Raftery, and C. Volinsky. Bayesian model averaging A tutorial.Statistical Science, 44, 1999.Hoe01 J. Hoey. Hierarchical unsupervised learning of facial expression categories. In ICCV Workshop on Detection and Recognition of Events in Video, 2001.HSC88 E. Horvitz, H. Suermondt, and G. Cooper. Bounded cutset conditioning An incrementalrefinement approach to belief under uncertain resources. Technical Report KSL8836, Stanford Univ., 1988.HT01 K. Humphreys and M. Titterington. Some examples of recursive variational approximationsfor Bayesian inference. In M. Opper and D. Saad, editors, Advanced mean field methods.MIT Press, 2001.HTF01 T. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistical Learning. Springer,2001.HZ02 T. Heskes and O. Zoeter. Expectation propagation for approximate inference in dynamicBayesian networks. In UAI, 2002.IB96 M. Isard and A. Blake. Contour tracking by stochastic propagation of conditional density.In Proc. European Conf. on Computer Vision, volume 1, pages 343356, 1996.IB98 M. Isard and A. Blake. A smoothing filter for condensation. In Proc. European Conf. onComputer Vision, volume 1, pages 767781, 1998.IB00 Y. Ivanov and A. Bobick. Recognition of visual activities and interactions by stochasticparsing. IEEE Trans. on Pattern Analysis and Machine Intelligence, 228852872, 2000.ITK00 T. Ideker, V. Thorsson, and R. Karp. Discovery of regulatory interactions through perturbation inference and experimental design. In Proc. of the Pacific Symp. on Biocomputing,2000.ITR01 T. Ideker, V. Thorsson, J. Ranish, R. Christmas, J. Buhler, R. Bumgarner, R. Aebersold, andL. Hood. Integrated genomic and proteomic analysis of a systematically perturned metabolicnetwork. Science, 2001. Submitted.251JA90 F. Jensen and S. K. Andersen. Approximations in Bayesian belief universes for knowledgebased systems. In UAI, 1990.Jaa01 T. Jaakkola. Tutorial on variational approximation methods. In M. Opper and D. Saad,editors, Advanced mean field methods. MIT Press, 2001.Jeb01 T. Jebara. Discriminative, Generative and Imitative Learning. PhD thesis, Media Lab, MIT,2001.Jel97 F. Jelinek. Statistical methods for speech recognition. MIT Press, 1997.Jen01 F. V. Jensen. Bayesian Networks and Decision Graphs. SpringerVerlag, 2001.JGJS98 M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K. Saul. An introduction to variationalmethods for graphical models. In M. Jordan, editor, Learning in Graphical Models. MITPress, 1998.JGS96 M. I. Jordan, Z. Ghahramani, and L. K. Saul. Hidden Markov decision trees. In NIPS, 1996.JH99 T. S. Jaakkola and D. Haussler. Exploiting generative models in discriminative classifiers.In NIPS10, 1999.JJ93 M. Jamshidian and R. I. Jennrich. Conjugate gradient acceleration of the EM algorithm.JASA, 88421221228, 1993.JJ94a F. V. Jensen and F. Jensen. Optimal junction trees. In UAI, 1994.JJ94b M. I. Jordan and R. A. Jacobs. Hierarchical mixtures of experts and the EM algorithm.Neural Computation, 6181214, 1994.JJ96 T. Jaakkola and M. Jordan. Computing upper and lower bounds on likelihoods in intractablenetworks. In UAI, 1996.JJ00 T. S. Jaakkola and M. I. Jordan. Bayesian parameter estimation via variational methods.Statistics and Computing, 102537, 2000.JJD94 F. V. Jensen, F. Jensen, and S. L. Dittmer. From influence diagrams to junction trees. InUAI, 1994.JKK95 C. S. Jensen, A. Kong, and U. Kjaerulff. Blockinggibbs sampling in very large probabilisticexpert systems. Intl. J. HumanComputer Studies, pages 647666, 1995.252JKOP89 F. V. Jensen, U. Kjaerulff, K. G. Olesen, and J. Pedersen. An expert system for control ofwaste water treatment  a pilot project. Technical report, Univ. Aalborg, Judex Datasystemer, 1989. In Danish.JLM92 F. Jelinek, J. D. Lafferty, and R. L. Mercer. Basic methods of probabilistic contextfreegrammars. Computational Linguistics, To appear, 1992.JLO90 F. Jensen, S. L. Lauritzen, and K. G. Olesen. Bayesian updating in causal probabilisticnetworks by local computations. Computational Statistics Quarterly, 4269282, 1990.JM00 D. Jurafsky and J. H. Martin. Speech and language processing An Introduction to NaturalLanguage Processing, Computational Linguistics, and Speech Recognition. PrenticeHall,2000.Jor95 M. I. Jordan. Why the logistic function A tutorial discussion on probabilities and neuralnetworks. Technical Report 9503, MIT Computational Cognitive Science Report, August1995.Jor99 M. I. Jordan, editor. Learning in Graphical Models. MIT Press, 1999.Jor02 M. I. Jordan. An introduction to probabilistic graphical models, 2002. In preparation.JP95 R. Jirousek and S. Preucil. On the effective implementation of the iterative proportionalfitting procedure. Computational Statistics  Data Analysis, 19177189, 1995.JX95 M. I. Jordan and L. Xu. Convergence results for the EM approach to mixtures of expertsarchitectures. Neural Networks, 814091431, 1995.KA94 S. Kuo and O. Agazzi. Keyword spotting in poorly printed documents using pseudo 2D Hidden Markov Models. IEEE Trans. on Pattern Analysis and Machine Intelligence,16842848, 1994.KA96 N. Kumar and A. G. Andreou. A generalization of linear discriminant analysis in maximum likelihood framework. In Proc. of the Joint Statistical Meeting, Statistical Computingsection, 1996.KC01 T. Kocak and R. Castelo. Improved Learning of Bayesian Networks. In UAI, 2001.KCB00 J. Kwon, B. Coifman, and P. Bickel. Daytoday travel time trends and travel time predictionfrom loop detector data. Transportation Research Record, 1554, 2000.253KDLC01 K. Kask, R. Dechter, J. Larrosa, and F. Cozman. Bucketelimination for automated reasoning. Technical Report R92, UC Irvine ICS, 2001.KFL01 F. Kschischang, B. Frey, and HA. Loeliger. Factor graphs and the sumproduct algorithm.IEEE Trans Info. Theory, February 2001.Kim94 CJ. Kim. Dynamic linear models with Markovswitching. J. of Econometrics, 60122,1994.Kim01 KE. Kim. Representations and Algorithms for Large Stochastic Planning Problems. PhDthesis, Brown U., Dept. Comp. Sci., 2001.Kja90 U. Kjaerulff. Triangulation of graphs  algorithms giving small total state space. TechnicalReport R9009, Dept. of Math. and Comp. Sci., Aalborg Univ., Denmark, 1990.Kja92 U. Kjaerulff. Optimal decomposition of probabilistic networks by simulated annealing. InStatistics and Computing, volume 2, pages 717, 1992.Kja94 U. Kjaerulff. Reduction of computational complexity in bayesian networks through removalof weak dependencies. In UAI, 1994.Kja95 U. Kjaerulff. dHugin A computational system for dynamic timesliced Bayesian networks.Intl. J. of Forecasting, 1189111, 1995.KJV94 S. Kirkpatrick, C. Gelatt Jtr., and M. Vecchi. Optimization by simulated annealing. Science,2207686, 1994.KKR95 K. Kanazawa, D. Koller, and S. Russell. Stochastic simulation algorithms for dynamicprobabilistic networks. In UAI, 1995.KL01 D. Koller and U. Lerner. Sampling in Factored Dynamic Systems. In A. Doucet, N. de Freitas, and N. Gordon, editors, Sequential Monte Carlo Methods in Practice. Springer, 2001.KLD01 K. Kask, J. Larrosa, and R. Dechter. Up and down minibuckets A scheme for approximating combinatorial optimization tasks. Technical Report R91, UC Irvine ICS, 2001.KM00 J. Kwon and K. Murphy. Modeling freeway traffic with coupled HMMs. Technical report,Univ. California, Berkeley, 2000.KMN99 M. Kearns, Y. Mansour, and A. Ng. Approximate planning in large POMDPs via reusabletrajectories. In NIPS12, 1999.254KMS98 P. Kontkanen, P. Mullymaki, T. Silander, H. Tirri, and P. Grunwald. A comparison of noninformative priors for Bayesian networks. In The Yearbook of the Finnish Statistical Society1997, pages 5362. , 1998.KN98 CJ. Kim and C. Nelson. StateSpace Models with RegimeSwitching Classical and GibbsSampling Approaches with Applications. MIT Press, 1998.KP97 D. Koller and A. Pfeffer. ObjectOriented Bayesian Networks. In UAI, 1997.KW97 J. Kivinen and M. Warmuth. Additive versus exponentiated gradient updates for linear prediction. Info. and Computation, 1321164, 1997.Lau92 S. L. Lauritzen. Propagation of probabilities, means and variances in mixed graphical association models. JASA, 8742010981108, December 1992.Lau95 S. L. Lauritzen. The EM algorithm for graphical association models with missing data.Computational Statistics and Data Analysis, 19191201, 1995.Lau96 S. Lauritzen. Graphical Models. OUP, 1996.LB01 H. Langseth and O. Bangs. Parameter learning in object oriented Bayesian networks. Annals of Mathematics and Artificial Intelligence, 2001. Forthcomming.LBN96 B. Levy, A. Benveniste, and R. Nikoukhah. Highlevel primitives for recursive maximumlikelihood estimation. IEEE Trans. Automatic Control, 41811251145, 1996.LC95 J. S. Liu and R. Chen. Blind deconvolution via sequential imputations. JASA, 90567576,1995.LC98 J. Liu and R. Chen. Sequential Monte Carlo methods for dynamic systems. JASA, 9310321044, 1998.LD94 Z. Li and B. DAmbrosio. Efficient inference in Bayes networks as a combinatorial optimization problem. Intl. J. Approximate Reasoning, 1115581, 1994.LJ97 S. L. Lauritzen and F. V. Jensen. Local computation with valuations from a commutativesemigroup. Annals of Mathematics and Artificial Intelligence, 2115169, 1997.LJ01 S. Lauritzen and F. Jensen. Stable local computation with conditional Gaussian distributions.Statistics and Computing, 11191203, 2001.Lju87 L. Ljung. System Identificiation Theory for the User. Prentice Hall, 1987.255LM97 K. Laskey and S. M. Mahoney. Network fragments Representing knowledge for constructing probabilistic models. In UAI, 1997.LMP01 J. Lafferty, A. McCallum, and F. Pereira. Conditional random fields Probabilistic modelsfor segmenting and labeling sequence data. In Intl. Conf. on Machine Learning, 2001.LN01 S. Lauritzen and D. Nilsson. Representing and solving decision problems with limited information. Management Science, 4712381251, 2001.LP01 U. Lerner and R. Parr. Inference in hybrid networks Theoretical limits and practical algorithms. In UAI, 2001.LS83 L. Ljung and T. Soderstrom. Theory and Practice of Recursive Identification. MIT Press,1983.LS98 V. Lepar and P. P. Shenoy. A Comparison of LauritzenSpiegelhalter, Hugin and ShenoyShafer Architectures for Computing Marginals of Probability Distributions. In G. Cooperand S. Moral, editors, UAI, pages 328337. Morgan Kaufmann, 1998.LSK01 U. Lerner, E. Segal, and D. Koller. Exact inference in networks with discrete children ofcontinuous parents. In UAI, 2001.LSS01 M. Littman, R. Sutton, and S. Singh. Predictive representations of state. In NIPS, 2001.LW89 S. L. Lauritzen and N. Wermuth. Graphical models for associations between variables, someof which are qualitative and some quantitative. Annals of Statistics, 173157, 1989.LW01 J. Liu and M. West. Combined Parameter and State Estimation in SimulationBased Filtering. In A. Doucet, N. de Freitas, and N. Gordon, editors, Sequential Monte Carlo Methodsin Practice. Springer, 2001.Mac95 D. MacKay. Probable networks and plausible predictions  a review of practical Bayesianmethods for supervised neural networks. Network, 1995.Mac98 D. MacKay. Introduction to Monte Carlo methods. In M. Jordan, editor, Learning in graphical models. MIT Press, 1998.Mar01 I. C. Marschner. On stochastic version of the EM algorithm. Biometrika, 88281286, 2001.May90 A. D. May. Traffic Flow Fundamentals. Prentice Hall, 1990.256MB99 P. J. Mosterman and G. Biswas. Diagnosis of continuous valued systems in transient operating regions. IEEE Trans. on Systems, Man, and Cybernetics, Part A, 296554565,1999.McC95 A. McCallum. Reinforcement Learning with Selective Perception and Hidden State. PhDthesis, Univ. Rochester, 1995.MCH02 Christopher Meek, David Maxwell Chickering, and David Heckerman. Autoregressive treemodels for timeseries analysis. In Proceedings of the Second International SIAM Conference on Data Mining, pages 229244, Arlington, VA, April 2002. SIAM.MD99 A. Madsen and B. DAmbrosio. A factorized representation of independence of causalinfluence and lazy propagation. In AAAI, 1999.ME85 H. Moravec and A. Elfes. High resolution maps from wide angle sonar. In ICRA, 1985.Mee01 Christopher Meek. Finding a path is harder than finding a tree. Journal of Artificial Intelligence Research, 15383389, 2001. httpwww.jair.orgabstractsmeek01a.html.MF92 F. Martinirie and P. Forster. Data Association and Tracking Using Hidden Markov Modelsand Dynamic Programming. In Intl. Conf. on Acoustics, Speech and Signal Proc., 1992.MFP00 A. McCallum, D. Freitag, and F. Pereira. Maximum Entropy Markov Models for Information Extraction and Segmentation. In Intl. Conf. on Machine Learning, 2000.MH97 C. Meek and D. Heckerman. Structure and parameter learning for causal independence andcausal interaction models. In UAI, pages 366375, 1997.Min99 T. Minka. From Hidden Markov Models to Linear Dynamical Systems. Technical report,MIT, 1999.Min00a T. Minka. Bayesian linear regression. Technical report, MIT, 2000.Min00b T. Minka. Estimating a Dirichlet distribution. Technical report, MIT, 2000.Min00c T. Minka. Inferring a Gaussian distribution. Technical report, MIT, 2000.Min01 T. Minka. A family of algorithms for approximate Bayesian inference. PhD thesis, MIT,2001.MJ97 M. Meila and M. Jordan. Triangulation by continuous embedding. Technical Report 1605,MIT AI Lab, 1997.257MJ99 A. Madsen and F. Jensen. Lazy propagation a junction tree inference algorithm based onlazy evaluation. Artificial Intelligence, 113203245, 1999.MJ00 M. Meila and M. I. Jordan. Learning with mixtures of trees. J. of Machine Learning Research, 1148, 2000.MK97 G. J. McLachlan and T. Krishnan. The EM Algorithm and Extensions. Wiley, 1997.ML02 T. Minka and J. Lafferty. Expectation propagation for the generative aspect model. In UAI,2002.MM93 O. Maron and A. W. Moore. Hoeffding races Accelerating model selection search forclassification and function approximation. In NIPS6, 1993.MMC98 R. J. McEliece, D. J. C. MacKay, and J. F. Cheng. Turbo decoding as an instance of Pearlsbelief propagation algorithm. IEEE J. on Selectred Areas in Comm., 162140152, 1998.MN83 McCullagh and Nelder. Generalized Linear Models. Chapman and Hall, 1983.Moh96 M. Mohri. Finitestate transducers in language and speech processing. Computational Linguistics, 201133, 1996.Moo73 J. Moore. Discretetime fixedlag smoothing algorithms. Automatica, 9163173, 1973.MP95 A. Manna and A. Pnueli. Temporal verification of Reactive Systems. Springer, 1995.MP01 K. Murphy and M. Paskin. Linear time inference in hierarchical HMMs. In NIPS, 2001.MR94 D. Madigan and A. Raftery. Model selection and accounting for model uncertainty in graphical models using Occams window. JASA, 8915351546, 1994.MR01 K. Murphy and S. Russell. RaoBlackwellised Particle Filtering for Dynamic BayesianNetworks. In A. Doucet, N. de Freitas, and N. Gordon, editors, Sequential Monte CarloMethods in Practice. Springer, 2001.MT01 D. Margaritis and S. Thrun. A Bayesian Multiresolution Independence Test for ContinuousVariables. In UAI, 2001.MTH01 C. Meek, B. Thiesson, and D. Heckerman. A learningcurve approach to clustering. Technical Report MSRTR0134, Microsoft Research, 2001.MTKW02 M. Montemerlo, S. Thrun, D. Koller, and B. Wegbreit. FastSLAM A Factored Solution tothe Simultaneous Localization and Mapping Problem. In AAAI, 2002.258Mur98 K. P. Murphy. Switching Kalman filters. Technical report, DECCompaq Cambridge Research Labs, 1998.Mur99 K. P. Murphy. A variational approximation for Bayesian networks with discrete and continuous latent variables. In UAI, 1999.Mur00 K. P. Murphy. Bayesian map learning in dynamic environments. In NIPS12, pages 10151021, 2000.Mur01a K. Murphy. Active learning of causal Bayes net structure. Technical report, Comp. Sci. Div.,UC Berkeley, 2001.Mur01b K. Murphy. The Bayes Net Toolbox for Matlab. In Computing Science and StatisticsProceedings of the Interface, volume 33, 2001.MvD97 X. L. Meng and D. van Dyk. The EM algorithm  an old folk song sung to a fast new tunewith Discussion. J. Royal Stat. Soc. B, 59511567, 1997.MW01 K. Murphy and Y. Weiss. The Factored Frontier Algorithm for Approximate Inference inDBNs. In UAI, 2001.MWJ99 K. Murphy, Y. Weiss, and M. Jordan. Loopy belief propagation for approximate inferencean empirical study. In UAI, 1999.MY95 D. Madigan and J. York. Bayesian graphical models for discrete data. Intl. Statistical Review,63215232, 1995.MZ97 I. MacDonald and W. Zucchini. Hiden Markov and Other Models for Discrete Valued TimeSeries. Chapman Hall, 1997.NB94 A. E. Nicholson and J. M. Brady. Dynamic belief networks for discrete monitoring. IEEESystems, Man and Cybernetics, 241115931610, 1994.ND01 S. Narayanan and D.Jurafsky. A Bayesian Model Predicts Parse Preferences and ReadingTimes in Sentence Comprehension. In NIPS, 2001.Nea92 R. Neal. Connectionist learning of belief networks. Artificial Intelligence, 5671113, 1992.Nea93 R. Neal. Probabilistic Inference Using Markov Chain Monte Carlo Methods. Technicalreport, Univ. Toronto, 1993.NG01 D. Nilsson and J. Goldberger. Sequentially finding the NBest List in Hidden Markov Models. In IJCAI, pages 12801285, 2001.259NH97 Liem Ngo and Peter Haddawy. Answering queries from contextsensitive probabilisticknowledge bases. Theoretical Computer Science, 17112147177, 1997.NH98 R. M. Neal and G. E. Hinton. A new view of the EM algorithm that justifies incrementaland other variants. In M. Jordan, editor, Learning in Graphical Models. MIT Press, 1998.NI00 A. Nefian and M. Hayes III. Maximum likelihood training of the embedded HMM for facedetection and recognition. In IEEE Intl. Conf. on Image Processing, 2000.Nik98 D. Nikovski. Learning stationary temporal probabilistic networks. In Conf. on AutomatedLearning and Discovery, 1998.Nil98 D. Nilsson. An efficient algorithm for finding the M most probable configurations in aprobabilistic expert system. Statistics and Computing, 8159173, 1998.NJ01 A. Y. Ng and M. Jordan. Convergence rates of the Voting Gibbs classifier, with applicationto Bayesian feature selection. In Intl. Conf. on Machine Learning, 2001.NJ02 A. Y. Ng and M. I. Jordan. On discriminative vs. generative classifiers A comparison oflogistic regression and naive bayes. In NIPS14, 2002.NLP02 A. Nefian, L. Liang, X. Pi, L. Xiaoxiang, C. Mao, and K. Murphy. A Coupled HMM forAudioVisual Speech Recognition. In Intl. Conf. on Acoustics, Speech and Signal Proc.,2002.NPP02 B. Ng, L. Peshkin, and A. Pfeffer. Factored particles for scalable monitoring. In UAI, 2002.NWJK00 T. Nielsen, P. Wuillemin, F. Jensen, and U. Kjaerulff. Using ROBDDs for inference inBayesian networks with troubleshooting as an example. In UAI, 2000.NY00 H. J. Nock and S. J. Young. Loosely coupled HMMs for ASR. In Intl. Conf. on Acoustics,Speech and Signal Proc., 2000.ODK96 M. Ostendorf, V. Digalakis, and O. Kimball. From HMMs to segment models a uniedview of stochastic modeling for speech recognition. IEEE Trans. on Speech and AudioProcessing, 45360378, 1996.OKJ89 K. G. Olesen, U. Kjaerulff, F. Jensen, B. Falck, S. Andreassen, and S. K. Andersen. A muninnetwork for the median nerve  a case study on loops. Applied AI, 3384403, 1989.Ole93 K. G. Olesen. Causal probabilistic networks with both discrete and continuous variables.IEEE Trans. on Pattern Analysis and Machine Intelligence, 315, 1993.260OM96 P. Van Overschee and B. De Moor. Subspace Identification for Linear Systems Theory,Implementation, Applications. Kluwer Academic Publishers, 1996.OM99 K. Olesen and A. Madsen. Maximal prime subgraph decomposition of bayesian networks.Technical Report R995006, Dept. Comp. Sci., Aalborg Univ., 1999.Pea88 J. Pearl. Probabilistic Reasoning in Intelligent Systems Networks of Plausible Inference.Morgan Kaufmann, 1988.Pea00 J. Pearl. Causality Models, Reasoning and Inference. Cambridge Univ. Press, 2000.Pfe00 A. Pfeffer. Probabilistic Reasoning for Complex Systems. PhD thesis, Dept. Comp. Sci.,Stanford Univ., 2000.Pfe01 A. Pfeffer. Sufficiency, separability and temporal probabilistic models. In UAI, 2001.Pfl98 K. Pfleger. Categorical Boltzmann Machines. Technical Report KSL9805, Knowledgesystems lab, Computer Science Department, Stanford University, 1998.PPL97 S. Della Pietra, V. Della Pietra, and J. Lafferty. Inducing features of random fields. IEEETrans. on Pattern Analysis and Machine Intelligence, 194, 1997.PR97a R. Parr and S. Russell. Reinforcement learning with hierarchies of machines. In NIPS, 1997.PR97b F. Pereira and M. Riley. Speech recognition by composition of weighted finite automata. InFiniteState Language Processing, pages 431453. MIT Press, 1997.PR01 H. Pasula and S. Russell. Approximate inference for firstorder probabilistic languages. InIJCAI, 2001.PRCM99 V. Pavlovic, J. Rehg, TJ. Cham, and K. Murphy. A Dynamic Bayesian Network Approachto Figure Tracking Using Learned Dynamic Models. In IEEE Conf. on Computer Vision andPattern Recognition, 1999.PRM00 V. Pavlovic, J. M. Rehg, and J. MacCormick. Learning switching linear models of humanmotion. In NIPS13, 2000.PROR99 H. Pasula, S. Russell, M. Ostland, and Y. Ritov. Tracking many objects with many sensors.In IJCAI, 1999.PS91 M. Peot and R. Shachter. Fusion and propogation with multiple observations in belief networks. Artificial Intelligence, 48299318, 1991.261PV91 J. Pearl and T. Verma. A theory of inferred causation. In Knowledge Representation, pages441452, 1991.PVTF88 W. Press, W. Vetterling, S. Teukolosky, and B. Flannery. Numerical Recipes in C The Artof Scientific Computing. Cambridge University Press, second edition, 1988.PW96 D. Pynadath and M. Wellman. Generalized Queries on Probabilistic ContextFree Grammars. In UAI, 1996.Rab89 L. R. Rabiner. A tutorial on Hidden Markov Models and selected applications in speechrecognition. Proc. of the IEEE, 772257286, 1989.Raf96 A. Raftery. Hypothesis testing and model selection via posterior simulation. In MarkovChain Monte Carlo in Practice. Chapman and Hall, 1996.RBD00 M. Richardson, J. Bilmes, and C. Diorio. Hiddenarticulatory Markov models for speechrecognition. In Intl. Conf. on Acoustics, Speech and Signal Proc., 2000.RBKK95 S. Russell, J. Binder, D. Koller, and K. Kanazawa. Local learning in probabilistic networkswith hidden variables. In IJCAI, 1995.RD98 I. Rish and R. Dechter. On the impact of causal independence. Technical report, Dept.Information and Computer Science, UCI, 1998.RG99 S. Roweis and Z. Ghahramani. A Unifying Review of Linear Gaussian Models. NeuralComputation, 112, 1999.RG01 D. Rusakov and D. Geiger. On the parameter priors for discrete DAG models. In AIStats,2001.RN95 S. Russell and P. Norvig. Instructors Solution Manual to Accompany Artificial IntelligenceA Modern Approach. Prentice Hall, Englewood Cliffs, NJ, 1995.RN02 S. Russell and P. Norvig. Artificial Intelligence A Modern Approach. Prentice Hall, 2002.2nd edition, in preparation.Rob73 R. W. Robinson. Counting labeled acyclic digraphs. In F. Harary, editor, New Directions inthe Theory of Graphs, pages 239273. Academic Press, 1973.Ros98 K. Rose. Deterministic annealing for clustering, compression, classification, regression, andrelated optimization problems. Proc. IEEE, 8022102239, November 1998.Row99 S. Roweis. Constrained hidden Markov models. In NIPS12, 1999.262RR01 A. Rao and K. Rose. Deterministically Annealed Design of Hidden Markov Model SpeechRecognizers. IEEE Trans. on Speech and Audio Proc., 92111126, February 2001.RS91 Neil Robertson and Paul D. Seymour. Graph minors  X Obstructions to treedecompositions. J. Comb. Theory Series B, 52153190, 1991.RS98 M. Ramoni and P. Sebastiani. Parameter Estimation in Bayesina networks from incompletedatabases. Intelligent Data Analysis Journal, 21, 1998.RST96 Dana Ron, Yoram Singer, and Naftali Tishby. The power of amnesia Learning probabilisticautomata with variable memory length. Machine Learning, 25, 1996.RTL76 D. Rose, R. Tarjan, and G. Lueker. Algorithmic aspects of vertex elimination on graphs.SIAM J. on Computing, 5266283, 1976.RW84 R. A. Redner and H. F. Walker. Mixture densities, maximum likelihood, and the EM algorithm. SIAM Review, 26195239, 1984.SAS94 R. Shachter, S. Andersen, and P. Szolovits. Global conditioning for probabilistic inferencein belief networks. In UAI, 1994.SDLC93 David J. A. Spiegelhalter, Philip Dawid, Steffen L. Lauritzen, and Robert G. Cowell.Bayesian analysis in expert systems. Statistical Science, 83219283, 1993.SDW01 S. Soatto, G. Doretto, and Y.N. Wu. Dynamic textures. In IEEE Conf. on Computer Visionand Pattern Recognition, volume 2, pages 439446, 2001.SGS00 P. Spirtes, C. Glymour, and R. Scheines. Causation, Prediction, and Search. MIT Press,2000. 2nd edition.Sha86 R. Shachter. Evaluating influence diagrams. Operations Research, 336871882, 1986.Sha88 R. Shachter. Probabilistic inference and influence diagrams. Oper. Res., 364589604,1988.Sha98 R. Shachter. Bayesball The rational pastime for determining irrelevance and requisiteinformation in belief networks and influence diagrams. In UAI, 1998.She92 P. P. Shenoy. Valuationbased systems for Bayesian Decision Analysis. Operations Research, 40463484, 1992.She97 P. Shenoy. Binary join trees for computing marginals in the shenoyshafer architecture. Intl.J. of Approximate Reasoning, 1723239263, 1997.263SHJ97 P. Smyth, D. Heckerman, and M. I. Jordan. Probabilistic independence networks for hiddenMarkov probability models. Neural Computation, 92227269, 1997.SJ95 L. Saul and M. Jordan. Boltzmann chains and hidden Markov models. In NIPS7, 1995.SJ99 L. Saul and M. Jordan. Mixed memory markov models Decomposing complex stochasticprocesses as mixture of simpler ones. Machine Learning, 3717587, 1999.SJ02 H. Steck and T. Jaakkola. Unsupervised active learning in large domains. In UAI, 2002.SL90 D. J. Spiegelhalter and S. L. Lauritzen. Sequential updating of conditional probabilities ondirected graphical structures. Networks, 20, 1990.SM80 A. Smith and U. Makov. Bayesian detection and estimation of jumps in linear systems. InO. Jacobs, M. Davis, M. Dempster, C. Harris, and P. Parks, editors, Analysis and optimization of stochastic systems. 1980.Smy96 P. Smyth. Clustering sequences with hidden Markov models. In NIPS, 1996.SNR00 J. Satagopan, M. Newton, and A. Raftery. Easy Estimation of Normalizing Constants andBayes Factors from Posterior Simulation Stabilizing the Harmonic Mean Estimator. Technical report, U. Washington, 2000.SP89 R. D. Shachter and M. A. Peot. Simulation approaches to general probabilistic inference onbelief networks. In UAI, volume 5, 1989.SPS99 R.S. Sutton, D. Precup, and S. Singh. Between MDPs and semiMDPs A framework fortemporal abstraction in reinforcement learning. Artificial Intelligence, 112181211, 1999.Sri93 S. Srinivas. A generalization of the noisyor model. In UAI, 1993.SS88 G. R. Shafer and P. P. Shenoy. Local computation in hypertrees. Technical Report 201,School of Business, Uni. Kansas, 1988.SS90a G. R. Shafer and P. P. Shenoy. Probability propagation. Annals of Mathematics and AI,2327352, 1990.SS90b P. P. Shenoy and G. R. Shafer. Axioms for probability and belieffunction propagation. InUAI, 1990.SS98 T. Schmidt and P. P. Shenoy. Some Improvements to the ShenoyShafer and Hugin Architectures for Computing Marginals. Artificial Intelligence, 1022323333, 1998.264SSC88 R. Smith, M. Self, and P. Cheeseman. Estimating uncertain spatial relationships in robotics.In Lemmer and Kanal, editors, Uncertainty in Artificial Intelligence, volume 2, pages 435461. Elsevier, 1988.SSG99 R. Settimi, J. Smith, and A. Gargoum. Approximate learning in complex dynamuc Bayesiannetworks. In UAI, 1999.Ste00 B. Steinsky. Enumeration of labelled chain graphs and labelled essential directed acyclicgraphs. Technical report, Univ. Salzburg, Austria, 2000.Sto94 A. Stolcke. Bayesian Learning of Probabilistic Language Models. PhD thesis, ICSI, UCBerkeley, 1994.SW96 Y. Singer and M. Warmuth. Training Algorithms for Hidden Markov Models Using EntropyBased Distance Function. In NIPS9, 1996.SY99 Eugene Santos, Jr. and Joel D. Young. Probabilistic temporal networks A unified frameworkfor reasoning with time and uncertainty. Intl. J. of Approximate Reasoning, 203191216,1999.TBF98 S. Thrun, W. Burgard, and D. Fox. A probabilistic approach to concurrent mapping andlocalization for mobile robots. Machine Learning, 312953, 1998.TDW02 M. Takikawa, B. DAmbrosio, and E. Wright. Realtime infernece with largescale temporalbayes nets. In UAI, 2002. Submitted.Thi95 B. Thiesson. Accelerated quantification of bayesian networks with incomplete data. In Proc.of the Intl Conf. on Knowledge Discovery and Data Mining, 1995.Thr98 S. Thrun. Bayesian landmark learning for mobile robot localization. Machine Learning,331, 1998.Tip98 M. Tipping. Probabilistic visualization of highdimensional binary data. In NIPS, 1998.TJ02 S. C. Tatikonda and M. I. Jordan. Loopy Belief Propagation and Gibbs Measures. In UAI,2002.TK00 Simon Tong and Daphne Koller. Active learning for parameter estimation in bayesian networks. In NIPS, pages 647653, 2000.TK01 S. Tong and D. Koller. Active learning for structure in Bayesian networks. In IJCAI, 2001.TLV01 S. Thrun, J. Langford, and V. Verma. Risk sensitive particle filters. In NIPS, 2001.265TMH01 B. Thiesson, C. Meek, and D. Heckerman. Accelerating EM for Large Databases. TechnicalReport MSRTR9931, Microsoft Research, 2001.TSM85 D. M. Titterington, A. F. M. Smith, and U. E. Makov. Statistical analysis of finite mixturedistributions. Wiley, 1985.TW87 M. Tanner and W. Wong. The calculation of posterior distributions by data augmentation.JASA, 82398528540, 1987.TY84 R. E. Tarjan and M. Yannakakis. Simple lineartime algorithms to test chordality of graphs,test acyclicity of hypergraphs, and selectively reduce acyclic hypergraphs. SIAM J. on Computing, 13566579, 1984.UN98 N. Ueda and R. Nakano. Deterministic annealing EM algorithm. Neural Networks, 11271282, 1998.vdMDdFW00 R. van der Merwe, A. Doucet, N. de Freitas, and E. Wan. The unscented particle filter. InNIPS13, 2000.VP90 T. Verma and J. Pearl. Equivalence and synthesis of causal models. In UAI, 1990.WBG92 M. P. Wellman, J. S. Breese, and R. P. Goldman. From knowledge bases to decision models.The Knowledge Engineering Review, 713553, 1992.WdM01 E. A. Wan and R. Van der Merwe. The Unscented Kalman Filter. In S. Haykin, editor,Kalman Filtering and Neural Networks. Wiley, 2001.Wei00 Y. Weiss. Correctness of local probability propagation in graphical models with loops. Neural Computation, 12141, 2000.Wei01 Y. Weiss. Comparing the mean field method and belief propagation for approximate inference in MRFs. In Saad and Opper, editors, Advanced Mean Field Methods. MIT Press,2001.Wel90 M. P. Wellman. Fundamental concepts of qualitative probabilistic networks. Artificial Intelligence, 443257303, 1990.WF99 Y. Weiss and W. T. Freeman. Correctness of belief propagation in Gaussian graphical modelsof arbitrary topology. In NIPS12, 1999.WH97 Mike West and Jeff Harrison. Bayesian forecasting and dynamic models. Springer, 1997.266Wie00 W. Wiegerinck. Variational approximations between mean field theory and the junction treealgorithm. In UAI, pages 626633, 2000.WJW01 M. Wainwright, T. Jaakkola, and A. Willsky. Treebased reparameterization for approximateestimation on loopy graphs. In NIPS14, 2001.WN01 E. A. Wan and A. T. Nelson. Dual EKF Methods. In S. Haykin, editor, Kalman Filteringand Neural Networks. Wiley, 2001.WT01 M. Welling and YW. Teh. Belief optimization for binary networks a stable alternative toloopy belief propagation. In UAI, 2001.WT02 M. Welling and Y. W. Teh. Inference in Boltzmann Machines, Mean Field, TAP and BetheApproximations. Technical report, U. Toronto, Dept. Comp. Sci., 2002.XJ96 L. Xu and M. I. Jordan. On convergence properties of the EM algorithm for Gaussianmixtures. Neural Computation, 8129151, 1996.XJKR02 E. P. Xing, M. I. Jordan, R. M. Karp, and S. J. Russell. A Hierarchical Bayesian MarkovianModel for Motifs in Biopolymer Sequences. In NIPS, 2002. Submitted.XPB93 Y. Xiang, D. Poole, and M. P. Beddoes. Multiply sectioned Bayesian networks and junction forests for large knowledgebased systems. Computational Intelligence, 92171220,1993.Yan81 M. Yannakakis. Computing the minimum fillin is NPcomplete. SIAM J. Alg. DiscreteMethods, 27779, 1981.YFW00 J. Yedidia, W. T. Freeman, and Y. Weiss. Generalized belief propagation. In NIPS13, 2000.YFW01 J. Yedidia, W. T. Freeman, and Y. Weiss. Understanding belief propagation and its generalizations. In IJCAI, 2001.YMHL95 J. York, D. Madigan, I. Heuch, and R. Lie. Birth defects registered by double samplinga Bayesian approach incorporating covariates and model uncertainty. Applied Statistics,442227242, 1995.Yui00 A. Yuille. A double loop algorithm to minimize the Bethe and Kikuchi free energies. InNIPS, 2000.Zha96 N. L. Zhang. Irrelevance and parameter learning in Bayesian networks. Artificial Intelligence Journal, 88359373, 1996.267Zha98a N. Zhang. Probabilistic Inference in Influence Diagrams. Computational Intelligence,144475497, 1998.Zha98b Nevin Zhang. Inference in Bayesian Networks The Role of ContextSpecific Independence.Technical Report HKUSTCS9809, Dept. Comp. Sci., Hong Kong Univ., 1998.ZP96 N. Zhang and D. Poole. Exploiting causal independence in Bayesian network inference. J.of AI Research, pages 301328, 1996.ZP99 N. L. Zhang and D. Poole. On the role of contextspecific independence in probabilisticreasoning. In IJCAI, pages 12881293, 1999.ZP00 G. Zweig and M. Padmanabhan. Exact alphabeta computation in logarithmic space withapplication to map word graph construction. In Proc. Intl. Conf. Spoken Lang. Proc., 2000.ZR97 G. Zweig and S. Russell. Compositional modelling with DPNs. UC Berkeley CS Dept.,1997.Zwe96 G. Zweig. A forwardbackward algorithm for inference in Bayesian networks and an empirical comparison with HMMs. Masters thesis, Dept. Comp. Sci., U.C. Berkeley, 1996.Zwe98 G. Zweig. Speech Recognition with Dynamic Bayesian Networks. PhD thesis, U.C. Berkeley, Dept. Comp. Sci., 1998.ZY97 N. L. Zhang and Li Yan. Independence of causal influence and clique tree propagation. Intl.J. of Approximate Reasoning, 19335349, 1997.268
