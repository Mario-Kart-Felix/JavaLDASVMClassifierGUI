Automatic Analysis of Facial ExpressionsThe State of the ArtMaja Pantic, Student Member, IEEE, and Leon J.M. RothkrantzAbstractHumans detect and interpret faces and facial expressions in a scene with little or no effort. Still, development of anautomated system that accomplishes this task is rather difficult. There are several related problems detection of an image segment asa face, extraction of the facial expression information, and classification of the expression e.g., in emotion categories. A system thatperforms these operations accurately and in real time would form a big step in achieving a humanlike interaction between man andmachine. This paper surveys the past work in solving these problems. The capability of the human visual system with respect to theseproblems is discussed, too. It is meant to serve as an ultimate goal and a guide for determining recommendations for development ofan automatic facial expression analyzer.Index TermsFace detection, facial expression information extraction, facial action encoding, facial expression emotionalclassification.1 INTRODUCTIONAS pointed out by Bruce 6, Takeuchi and Nagao 84,and Hara and Kobayashi 28, human facetofacecommunication is an ideal model for designing a multimodalmedia humancomputer interface HCI. The maincharacteristics of human communication are multiplicityand multimodality of communication channels. A channelis a communication medium while a modality is a senseused to perceive signals from the outside world. Examplesof human communication channels are auditory channelthat carries speech, auditory channel that carries vocalintonation, visual channel that carries facial expressions,and visual channel that carries body movements. Thesenses of sight, hearing, and touch are examples ofmodalities. In usual facetoface communication, manychannels are used and different modalities are activated.As a result, communication becomes highly flexible androbust. Failure of one channel is recovered by anotherchannel and a message in one channel can be explained byanother channel. This is how a multimediamodal HCIshould be developed for facilitating robust, natural,efficient, and effective manmachine interaction.Relatively few existing works combine different modalities into a single system for human communicativereaction analysis. Examples are the works of Chen et al.9 and De Silva et al. 15 who studied the effects of acombined detection of facial and vocal expressions ofemotions. So far, the majority of studies treat varioushuman communication channels separately, as indicated byNakatsu 58. Examples for the presented systems areemotional interpretation of human voices 35, 66, 68,90, emotion recognition by physiological signals patternrecognition 67, detection and interpretation of handgestures 64, recognition of body movements 29, 97,46, and facial expression analysis this survey.The terms facetoface and interface indicate that theface plays an essential role in interpersonal communication.The face is the mean to identify other members of thespecies, to interpret what has been said by the means oflipreading, and to understand someones emotional stateand intentions on the basis of the shown facial expression.Personality, attractiveness, age, and gender can also be seenfrom someones face. Considerable research in socialpsychology has also shown that facial expressions helpcoordinate conversation 4, 82, and have considerablymore effect on whether a listener feels liked or disliked thanthe speakers spoken words 55. Mehrabian indicated thatthe verbal part i.e., spoken words of a message contributesonly for 7 percent to the effect of the message as a whole, thevocal part e.g., voice intonation contributes for 38 percent,while facial expression of the speaker contributes for55 percent to the effect of the spoken message 55. Thisimplies that the facial expressions form the major modalityin human communication.Recent advances in image analysis and pattern recognition open up the possibility of automatic detection andclassification of emotional and conversational facial signals.Automating facial expression analysis could bring facialexpressions into manmachine interaction as a new modality and make the interaction tighter and more efficient.Such a system could also make classification of facialexpressions widely accessible as a tool for research inbehavioral science and medicine. The goal of this paper is tosurvey the work done in automating facial expressionanalysis in facial images and image sequences. Section 2identifies three basic problems related to facial expressionanalysis. These problems are face detection in a facialimage or image sequence, facial expression data extraction,and facial expression classification. The capability of the1424 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 22, NO. 12, DECEMBER 2000. The authors are with the Department of Media, Engineering andMathematics, Delft University of Technology, PO Box 356, 2600 AJDelft, The Netherlands.Email M.Pantic, L.J.M.Rothkrantzcs.tudelft.nl.Manuscript received 29 June 1999 revised 16 March 2000 accepted 12 Sept.2000.Recommended for acceptance by K. Bowyer.For information on obtaining reprints of this article, please send email totpamicomputer.org, and reference IEEECS Log Number 110156.016288280010.00  2000 IEEEhuman visual system with respect to these problems isdescribed. It defines, in some way, the expectations for anautomated system. The characteristics of an ideal automated system for facial expression analysis are given inSection 3. Section 4 surveys the techniques presented in theliterature in the past decade for facial expression analysis bya computer. Their characteristics are summarized in respectto the requirements posed on the design of an ideal facialexpression analyzer. We do not attempt to provide anexhaustive review of the past work in each of the problemsrelated to automatic facial expression analysis. Here, weselectively discuss systems which deal with each of theseproblems. Possible directions for future research arediscussed in Section 5. Section 6 concludes the paper.2 FACIAL EXPRESSION ANALYSISOur aim is to explore the issues in design and implementation of a system that could perform automated facialexpression analysis. In general, three main steps can bedistinguished in tackling the problem. First, before a facialexpression can be analyzed, the face must be detected in ascene. Next is to devise mechanisms for extracting the facialexpression information from the observed facial image orimage sequence. In the case of static images, the process ofextracting the facial expression information is referred to aslocalizing the face and its features in the scene. In the case offacial image sequences, this process is referred to as trackingthe face and its features in the scene. At this point, a cleardistinction should be made between two terms, namely,facial features and face model features. The facial features arethe prominent features of the faceeyebrows, eyes, nose,mouth, and chin. The face model features are the featuresused to represent model the face. The face can berepresented in various ways, e.g., as a whole unit holisticrepresentation, as a set of features analytic representationor as a combination of these hybrid approach. The appliedface representation and the kind of input images determinethe choice of mechanisms for automatic extraction of facialexpression information. The final step is to define some setof categories, which we want to use for facial expressionclassification andor facial expression interpretation, and todevise the mechanism of categorization.Before an automated facial expression analyzer is built, oneshould decide on the systems functionality. A good referencepoint is the functionality of the human visual system. After all,it is the best known facial expression analyzer. This sectiondiscusses the three basic problems related to the process offacial expression analysis as well as the capability of thehuman visual system with respect to these.2.1 Face DetectionFor most works in automatic facial expression analysis, theconditions under which a facial image or image sequence isobtained are controlled. Usually, the image has the face infrontal view. Hence, the presence of a face in the scene isensured and some global location of the face in the scene isknown a priori. However, determining the exact location ofthe face in a digitized facial image is a more complexproblem. First, the scale and the orientation of the face canvary from image to image. If the mugshots are taken with afixed camera, faces can occur in images at various sizes andangles due to the movements of the observed person. Thus,it is difficult to search for a fixed pattern template in theimage. The presence of noise and occlusion makes theproblem even more difficult.Humans detect a facial pattern by casual inspection ofthe scene. We detect faces effortlessly in a wide range ofconditions, under bad lightning conditions or from a greatdistance. It is generally believed that twograylevelsimages of 100 to 200 pixels form a lower limit for detectionof a face by a human observer 75, 8. Another characteristic of the human visual system is that a face is perceived asa whole, not as a collection of the facial features. Thepresence of the features and their geometrical relationshipwith each other appears to be more important than thedetails of the features 5. When a face is partially occludede.g., by a hand, we perceive a whole face, as if ourperceptual system fills in the missing parts. This is verydifficult if possible at all to achieve by a computer.2.2 Facial Expression Data ExtractionAfter the presence of a face has been detected in theobserved scene, the next step is to extract the informationabout the encountered facial expression in an automaticway. If the extraction cannot be performed automatically, afully automatic facial expression analyzer cannot bedeveloped. Both, the applied face representation and thekind of input images affect the choice of the approach tofacial expression information extraction.One of the fundamental issues about the facial expression analysis is the representation of the visual informationthat an examined face might reveal 102. The results ofJohanssons pointlight display experiments 1, 5, gave aclue to this problem. The experiments suggest that thevisual properties of the face, regarding the informationabout the shown facial expression, could be made clear bydescribing the movements of points belonging to the facialfeatures eyebrows, eyes, and mouth and then by analyzingthe relationships between those movements. This triggeredthe researchers of visionbased facial gesture analysis tomake different attempts to define pointbased visualproperties of facial expressions. Various analytic facerepresentations yielded, in which the face is modeled as aset of facial points e.g., Fig. 6, 42, Fig. 7, 61 or as a set oftemplates fitted to the facial features such as the eyes andthe mouth. In another approach to face representationholistic approach, the face is represented as a whole unit.A 3D wireframe with a mapped texture e.g., 86 and aspatiotemporal model of facial image motion e.g., Fig. 8,2 are typical examples of the holistic approaches to facerepresentation. The face can be also modeled using a socalled hybrid approach, which typifies a combination ofanalytic and holistic approaches to face representation. Inthis approach, a set of facial points is usually used todetermine an initial position of a template that models theface e.g., Fig 10, 40.Irrespectively of the kind of the face model applied,attempts must be made to model and then extract theinformation about the displayed facial expression withoutlosing any or much of that information. Several factorsmake this task complex. The first is the presence of facialPANTIC AND ROTHKRANTZ AUTOMATIC ANALYSIS OF FACIAL EXPRESSIONS THE STATE OF THE ART 1425hair, glasses, etc., which obscure the facial features. Anotherproblem is the variation in size and orientation of the face ininput images. This disables a search for fixed patterns in theimages. Finally, noise and occlusion are always present tosome extent.As indicated by Ellis 23, human encoding of the visualstimulus face and its expression may be in the form of aprimal sketch and may be hardwired. However, not muchelse is known in terms of the nature of internal representation of a face in the human brain.2.3 Facial Expression ClassificationAfter the face and its appearance have been perceived, thenext step of an automated expression analyzer is toidentify the facial expression conveyed by the face. Afundamental issue about the facial expression classificationis to define a set of categories we want to deal with. Arelated issue is to devise mechanisms of categorization.Facial expressions can be classified in various waysinterms of facial actions that cause an expression, in terms ofsome nonprototypic expressions such as raised brows orin terms of some prototypic expressions such as emotionalexpressions.The Facial Action Coding System FACS 21 is probablythe most known study on facial activity. It is a system thathas been developed to facilitate objective measurement offacial activity for behavioral science investigations of theface. FACS is designed for human observers to detectindependent subtle changes in facial appearance caused bycontractions of the facial muscles. In a form of rules, FACSprovides a linguistic description of all possible, visuallydetectable, facial changes in terms of 44 socalled ActionUnits AUs. Using these rules, a trained human FACScoder decomposes a shown expression into the specific AUsthat describe the expression. Automating FACS wouldmake it widely accessible as a research tool in thebehavioral science, which is furthermore the theoreticalbasis of multimodalmedia user interfaces. This triggeredresearchers of computer vision field to take differentapproaches in tackling the problem. Still, explicit attemptsto automate the facial action coding as applied to automatedFACS encoding are few see 16 or 17 for a review of theexisting methods as well as Table 7 of this survey.Most of the studies on automated expression analysisperform an emotional classification. As indicated byFridlund et al. 25, the most known and the mostcommonly used study on emotional classification of facialexpressions is the crosscultural study on existence ofuniversal categories of emotional expressions. Ekmandefined six such categories, referred to as the basic emotionshappiness, sadness, surprise, fear, anger, and disgust 19.He described each basic emotion in terms of a facialexpression that uniquely characterizes that emotion. In thepast years, many questions arose around this study. Are thebasic emotional expressions indeed universal 33, 22, orare they merely a stressing of the verbal communicationand have no relation with an actual emotional state 76,77, 26 Also, it is not at all certain that each facialexpression able to be displayed on the face can be classifiedunder the six basic emotion categories. Nevertheless, mostof the studies on visionbased facial expression analysis relyon Ekmans emotional categorization of facial expressions.The problem of automating facial expression emotionalclassification is difficult to handle for a number of reasons.First, Ekmans description of the six prototypic facialexpressions of emotion is linguistic and, thus, ambiguous.There is no uniquely defined description either in terms offacial actions or in terms of some other universally definedfacial codes. Hence, the validation and the verification ofthe classification scheme to be used are difficult and crucialtasks. Second, classification of facial expressions in tomultiple emotion categories should be feasible e.g., raisedeyebrows and smiling mouth is a blend of surprise andhappiness, Fig. 1. Still, there is no psychological scrutiny onthis topic.Three more issues are related to facial expressionclassification in general. First, the system should be capableof analyzing any subject, male or female of any age andethnicity. In other words, the classification mechanism maynot depend on physiognomic variability of the observedperson. On the other hand, each person has hisher ownmaximal intensity of displaying a particular facial expression. Therefore, if the obtained classification is to bequantified e.g., to achieve a quantified encoding of facialactions or a quantified emotional labeling of blendedexpressions, systems which can start with a genericexpression classification and then adapt to a particularindividual have an advantage. Second, it is important torealize that the interpretation of the body language issituationdependent 75. Nevertheless, the informationabout the context in which a facial expression appears isvery difficult to obtain in an automatic way. This issue hasnot been handled by the currently existing systems. Finally,there is now a growing psychological research that arguesthat timing of facial expressions is a critical factor in theinterpretation of expressions 1, 5, 34. For the researchers of automated visionbased expression analysis, thissuggests moving towards a realtime wholeface analysis offacial expression dynamics.While the human mechanisms for face detection are veryrobust, the same is not the case for interpretation of facialexpressions. It is often very difficult to determine the exactnature of the expression on a persons face. According toBassili 1, a trained observer can correctly classify faces1426 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 22, NO. 12, DECEMBER 2000Fig. 1. Expressions of blended emotions surprisehappiness.showing six basic emotions with an average of 87 percent.This ratio varies depending on several factors the familiarity with the face, the familiarity with the personality ofthe observed person, the general experience with differenttypes of expressions, the attention given to the face and thenonvisual cues e.g., the context in which an expressionappears. It is interesting to note that the appearance of theupper face features plays a more important role in faceinterpretation as opposed to lower face features 22.3 AN IDEAL SYSTEM FOR FACIAL EXPRESSIONANALYSISBefore developing an automated system for facial expression analysis, one should decide on its functionality. A goodreference point is the best known facial expressionanalyzerthe human visual system. It may not be possibleto incorporate all features of the human visual system intoan automated system, and some features may even beundesirable, but it can certainly serve as a reference point.A first requirement that should be posed on developingan ideal automated facial expression analyzer is that all ofthe stages of the facial expression analysis are to beperformed automatically, namely, face detection, facialexpression information extraction, and facial expressionclassification. Yet, actual implementation and integration ofthese stages into a system are constrained by the systemsapplication domain. For instance, if the system is to be usedas a tool for research in behavioral science, realtimeperformance is not an essential property of the system.On the other hand, this is crucial if the system would form apart of an advanced userinterface. Long delays make theinteraction desynchronized and less efficient. Also, havingan explanation facility that would elucidate facial actionencoding performed by the system might be useful if thesystem is employed to train human experts in using FACS.However, such facility is superfluous if the system is to beemployed in videoconferencing or as a stressmonitoringtool. In this paper, we are mainly concerned with two majorapplication domains of an automated facial expressionanalyzer, namely, behavioral science research and multimodalmedia HCI. In this section, we propose an idealautomated facial expression analyzer Table 1 which couldbe employed in those application domains and has theproperties of the human visual system.Considering the potential applications of an automatedfacial expression analyzer, which involve continuous observation of a subject in a time interval, facial imageacquisition should proceed in an automatic way. In order tobe universal, the system should be capable of analyzingsubjects of both sexes, of any age and any ethnicity. Also, noconstraints should be set on the outlook of the observedsubjects. The system should perform robustly despitechanges in lightning conditions and distractions likeglasses, changes in hair style, and facial hair like moustache,beard and growntogether eyebrows. Similarly to thehuman visual system, an ideal system would fill inmissing parts of the observed face and perceive a wholeface even when a part of it is occluded e.g., by hand. Inmost reallife situations, complete immovability of theobserved subject cannot be assumed. Hence, the systemshould be able to deal with rigid head motions. Ideally, thesystem would perform robust facial expression analysisdespite large changes in viewing conditions it would becapable of dealing with a whole range of head movements,from frontal view to profile view acquired by a fixedcamera. This could be achieved by employing several fixedcameras for acquiring different facial views of the examinedface such as frontal view, and right and left profile viewsand then approximating the actual view by interpolationamong the acquired views. Having no constraints set on therigid head motions of the subject can also be achieved byhaving a camera mounted on the subjects head and placedin front of hisher face.An ideal system should perform robust automatic facedetection and facial expression information extraction in theacquired images or image sequences. Considering the stateoftheart in image processing techniques, inaccurate, noisy,and missing data could be expected. An ideal systemshould be capable of dealing with these inaccuracies. Inaddition, certainty of the extracted facial expressioninformation should be taken into account.An ideal system should be able to perform analysis of allvisually distinguishable facial expressions. Welldefinedface representation is a prerequisite for achieving this. Theface representation should be such that a particularalteration of the face model uniquely reveals a particularfacial expression. In general, an ideal system should be ableto distinguish1. all possible facial expressions a reference point isa total of 44 facial actions defined in FACS 21PANTIC AND ROTHKRANTZ AUTOMATIC ANALYSIS OF FACIAL EXPRESSIONS THE STATE OF THE ART 1427TABLE 1Properties of an Ideal Analyzerwhose combinations form the complete set of facialexpressions,2. any bilateral or unilateral facial change 21, and3. facial expressions with a similar facial appearancee.g., upward pull of the upper lip and nosewrinkling which also causes the upward pull ofthe upper lip 21.In practice, it may not be possible to define a face modelthat can satisfy both, to reflect each and every change infacial appearance and whose features are detectable in afacial image or image sequence. Still, the set of distinct facialexpressions that the system can distinguish should be ascopious as possible.If the system is to be used for behavioral science researchpurposes it should perform facial expression recognition asapplied to automated FACS encoding. As explained byBartlett et al. 16, 17, this means that it should accomplishmultiple quantified expression classification in terms of44 AUs defined in FACS. If the system is to be used as a partof an advanced multimodalmedia HCI, the system shouldbe able to interpret the shown facial expressions e.g., interms of emotions. Since psychological researchers disagree on existence of universal categories of emotionalfacial displays, an ideal system should be able to adapt theclassification mechanism according to the users subjectiveinterpretation of expressions, e.g., as suggested in 40. Also,it is definitely not the case that each and every facialexpression able to be displayed on the face can be classifiedinto one and only one emotion class. Think about blendedemotional displays such as raised eyebrow and smilingmouth Fig. 1. This expression might be classified in twoemotion categories defined by Ekman and Friesen20surprise and happiness. Yet, according to the descriptions of these prototypic expressions given by Ekman andFriesen 20, the left hand side facial expression shown inFig. 1 belongs more to the surprisethan to thehappiness class. For instance, in the left hand side imagethe percentage of shown surprise is higher than thepercentage of shown happiness while those percentagesare approximately the same in the case of the right handside image. In order to obtain an accurate categorization, anideal analyzer should perform quantified classification offacial expression into multiple emotion categories.4 AUTOMATIC FACIAL EXPRESSION ANALYSISFor its utility in application domains of human behaviorinterpretation and multimodalmedia HCI, automatic facialexpression analysis has attracted the interest of manycomputer vision researchers. Since the mid 1970s, differentapproaches are proposed for facial expression analysis fromeither static facial images or image sequences. In 1992,Samal and Iyengar 79 gave an overview of the earlyworks. This paper explores and compares approaches toautomatic facial expression analysis that have been developed recently, i.e., in the late 1990s. Before surveying theseworks in detail, we are giving a short overview of thesystems for facial expression analysis proposed in theperiod of 1991 to 1995.Table 2 summarizes the features of these systems inrespect to the requirements posed on design of an idealfacial expression analyzer. None of these systems performsa quantified expression classification in terms of facialactions. Also, except the system proposed by Moses et al.57, no system listed in Table 2 performs in realtime.Therefore, these properties i.e., columns 15 and 20 havebeen excluded from Table 2 . stands for yes, X stands forno, and  represents a missing entry. A missing entryeither means that it is not reported on the issue or that theissue is not applicable to the system in question. Aninapplicable issue, for instance, is the issue of dealing withrigid head motions and inaccurate facial data in the caseswhere the input data were hand measured e.g., 40. Someof the methods listed in Table 2 do not perform automaticfacial data extraction see column 8 the others achieve thisby using facial motion analysis 52, 100, 101, 73, 57.Except the method proposed by Kearney and McKenzie40, which performs facial expression classification in1428 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 22, NO. 12, DECEMBER 2000TABLE 2Early Methods for Automatic Facial Expression Analysisterms of facial actions as well as in terms of interpretationcategories defined by users, the systems listed in Table 2perform expression classification into a number of basicemotion categories. The utilized techniques include holisticspatial analysis 13, 70, 54, spatiotemporal analysis 52,100, 73, 57, and analytic spatial analysis 93, 43, 44,92, 40. The correct recognition rates reported range from7092 percent when recognizing 38 emotion categories.The survey of the works presented in the literaturebetween 1996 and 2000 is divided into three parts, based onthe problems discussed in Section 2face detection, facialexpression information extraction, and facial expressionclassification. We do not attempt to provide an exhaustivereview of the past work in each of the problems related toautomatic facial expression analysis. Here, we selectivelydiscuss recently developed systems, which deal with both,facial expression detection and classification. Table 3summarizes the properties of the surveyed facial expressionanalyzers in respect to the requirements posed on design ofan ideal facial expression analyzer Table 1. None of thesesystems are able to perform facial expression informationextraction from images of occluded faces or a quantifiedexpression classification in terms of facial actions. Therefore, these properties i.e., columns 4 and 15 have beenexcluded from Table 3. In the case of systems where facialexpression information is manually extracted, we aredeclaring that those can deal with the subjects of anyethnicity. In the case of automatic facial expression dataextraction, the value of the column 2 in Table 3 representsthe range in ethnicity of testing subjects. The number oftesting images, the number of subjects used to make thetesting images, and the overall performance of the surveyedsystems is summarized in Table 8.The approaches that have been explored lately alsoinclude systems for automatic analysis and synthesis offacial expressions 86, 56, 39, 47, 89, 88, 53, 14,19 for a broader list of references see 87. Although theimage analysis techniques in these systems are relevant tothe present goals, the systems themselves are of limited usefor behavioral science investigations of the face and formultimodalmedia HCI. These systems primarily concernfacial expression animation and do not attempt to classifythe observed facial expression either in terms of facialactions or in terms of emotion categories. For this reason,these and similar methods are out of the scope of this paper,which goal is to explore and compare imagebasedapproaches to facial expression detection and classification.4.1 Face DetectionFor most of the work in automatic facial expressionanalysis, the conditions under which an image is obtainedare controlled. The camera is either mounted on a helmetlike device worn by the subject e.g., 62, 59 or placed insuch a way that the image has the face in frontal view.Hence, the presence of the face in the scene is ensured andsome global location of the face in the scene is knowna priori. Yet, in most of the reallife situations where anautomated facial expression analyzer is to be employede.g., in a multimodalmedia HCI, the location of a face inthe image is not known a priori. Recently, the problem ofautomatic face detection in an arbitrary scene has drawngreat attention e.g., see 74, 83, 85.Independently of the kind of input imagesfacialimages or arbitrary imagesdetection of the exact faceposition in an observed image or image sequence has beenapproached in two ways. In the holistic approach, the face isdetermined as a whole unit. In the second, analyticPANTIC AND ROTHKRANTZ AUTOMATIC ANALYSIS OF FACIAL EXPRESSIONS THE STATE OF THE ART 1429TABLE 3Recent Approaches to Automatic Facial Expression Analysisapproach, the face is detected by detecting some importantfacial features first e.g., the irises and the nostrils. Thelocation of the features in correspondence with each otherdetermines then the overall location of the face. Table 4provides a classification of facial expression analyzersaccording to the kind of input images and the appliedmethod.4.1.1 Face Detection in Facial ImagesTo represent the face, Huang and Huang 32 apply a pointdistribution model PDM. In order to achieve a correctplacement of an initial PDM in an input image, Huang andHuang utilize a Canny edge detector to obtain a roughestimate of the face location in the image. The valley in pixelintensity that lies between the lips and the two symmetricalvertical edges representing the outer vertical boundaries ofthe face generate a rough estimate of the face location. Theface should be without facial hair and glasses, no rigid headmotion may be encountered and illumination variationsmust be linear for the system to work correctly.Pantic and Rothkrantz 62 detect the face as a wholeunit, too. As input to their system, they use dualview facialimages. To determine the vertical and horizontal outerboundaries of the head, they analyze the vertical andhorizontal histogram of the frontalview image. To localizethe contour of the face, they use an algorithm based on theHSV color model, which is similar to the algorithm basedon the relative RGB model 103. For the profile view imagethey apply a profiledetection algorithm, which represents aspatial approach to sampling the profile contour from athresholded image. For thresholding of the input profileimage, the Value of the HSV model is exploited. No facialhair or glasses are allowed.Kobayashi and Hara 42 apply an analytic approach toface detection. They are using a CCD camera in monochrome mode to obtain brightness distribution data of thehuman face. First, base brightness distribution wascalculated as an average of brightness distribution dataobtained from 10 subjects. Then, the system extracts theposition of the irises by utilizing a crosscorrelationtechnique on the base data and the currently examineddata. Once the irises are identified, the overall location ofthe face is determined by using relative locations of thefacial features in the face. The observed subject should facethe camera while siting at approximately 1m distance infront of it.Yoneyama et al. 104 use an analytic approach to facedetection too. The outer corners of the eyes, the height of theeyes, and the height of the mouth are extracted in anautomatic way. Once these features are identified, the sizeof the examined facial area is normalized and an 8 10rectangular grid is placed over the image. It is not statedwhich method has been applied and no limitation of theused method has been reported by Yoneyama et al.Kimura and Yachida 41 utilize a Potential Net for facerepresentation. An input image is normalized first by usingthe centers of the eyes and the center of the mouth tracked bythe method proposed by Wu et al. 99. This algorithmapplies an integral projection method, which synthesizes thecolor and the edge information. Then, the Potential Net isfitted to the normalized image to model the face and itsmovement. The face should be without facial hair and glassesand in a direct facetoface position with the camera 99.1430 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 22, NO. 12, DECEMBER 2000TABLE 4Summary of the Methods for Automatic Face Detection4.1.2 Face Detection in Arbitrary ImagesTwo of the works surveyed in this paper perform automaticface detection in an arbitrary scene. Hong et al. 30 utilizethe PersonSpotter system 81 in order to perform a realtime tracking of the head. The box bounding the head isused then as the image to which an initial labeled graph isfitted. The headtracking module of the PersonSpottersystem applies first a spatiotemporal filtering of the inputimage sequence. Then, a stereo algorithm determines thestereo disparities of the pixels that have been changed dueto the movement. By inspecting the local maximums of thedisparity histogram, image regions confined to a certaindisparity interval are selected. The skin color detector andthe convex region detector are applied to those regions. Thebounding boxes confining the clusters of the outputs of bothdetectors are likely to correspond to heads and hands. Forthe case that the person is not moving any longer, the headtracking module memorizes the last position of the person.The estimation of the current position and velocity of thehead is achieved by using a linear predictive filter. Steffenset al. 81 reported that their system performs well in thepresence of background motion, but fails in the case ofcovered or too much rotated faces.Essa and Pentland 24 use the eigenspace method ofPentland et al. 65 to locate faces in an arbitrary scene. Themethod employs eigenfaces approximated using PrincipalComponent Analysis PCA on a sample of 128 facialimages. The eigenfaces define the subspace of sampleimages, i.e., socalled face space 91. To detect thepresence of a face in a single image, the distance of theobserved image from the face space is calculated using theprojection coefficients and the signal energy. To detect thepresence of faces in an image sequence, a spatiotemporalfiltering is performed, the filtered image is thresholded inorder to analyze motion blobs, and each motion blob thatcan represent a human head is then evaluated as a singleimage. The method of Pentland et al. 65 is realtime andhas been successfully tested on a database of 7,562 imagesof some 3,000 people of both sexes, ranged in age andethnicity, having varying head positions, headwear, andfacial hair.4.2 Facial Expression Data ExtractionAfter the presence of a face is detected in the observedscene, the next step is to extract the information about theshown facial expression. Both the applied face representation and the kind of input images affect the choice of theapproach to facial expression data extraction.In general, three types of face representation are mainlyused in facial expression analysis holistic e.g., isodensitymaps 38, analytic e.g., deformable templates 105, andhybrid e.g., analytictoholistic approach 45. The facerepresentations used by the surveyed systems are listed inTable 5. Depending on the face model a templatebased or afeaturebased method is applied for facial expression dataextraction. Templatebased methods fit a holistic face modelto the input image or track it in the input image sequence.Featurebased methods localize the features of an analyticface model in the input image or track them in the inputsequence. The methods utilized by the surveyed systemsare listed in Table 6.4.2.1 Facial Data Extraction from Static ImagesTemplateBased MethodsAs shown in Table 3, several surveyed systems can beclassified as methods for facial expression analysis fromstatic images. A first category of these utilizes a holistic or ahybrid approach to face representation Table 5 andapplies a templatebased method for facial expressioninformation extraction from an input image.Edwards et al. 18 utilize a holistic face representation,which they refer to as the Active Appearance ModelAAM. To build their model they used facial images thatwere manually labeled with 122 points localized around thefacial features. To generate a statistical model of shapevariation, Edwards et al. aligned all training images into acommon coordinate frame and applied PCA to get a meanshape. To build a statistical model of graylevel appearance,they warped each training image, by using a triangulationalgorithm, so that its control points match the mean shape.By applying PCA to the graylevel information extractedfrom the warped images, they obtained a mean normalizedgraylevel vector. By applying PCA once more, an 80Dvector of appearance parameters controlling both, the shapeand the graylevels of the model, has been obtained. To fitthe AAM to an input image, Edwards et al. apply anAAM search algorithm which implies two stages 12. In thetraining stage, for each of 88 training images labeled with122 landmark points, known model displacements areapplied and the corresponding difference vector is recorded. After the training data are generated, a multivariatemultiple regression analysis is applied to model therelationship between the model displacement and thePANTIC AND ROTHKRANTZ AUTOMATIC ANALYSIS OF FACIAL EXPRESSIONS THE STATE OF THE ART 1431TABLE 5The Utilized Face Models1432 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 22, NO. 12, DECEMBER 2000TABLE 6The Methods for Automatic Facial Expression Data Extractionimage difference. In the recognition stage, the learnedregression model is used to determine the movement of theface model. The AAM search algorithm has been tested on100 hand labeled face images. Of these, 19.2 percent failedto converge to a satisfactory result 12. The method workswith images of faces without facial hair and glasses, whichare handlabeled with the landmark points beforehandapproximated with the proposed AAM.Hong et al. 30 utilize a labeled graph to represent theface. Each node of the graph consists of an array, which iscalled jet. Each component of a jet is the filter response of acertain Gabor wavelet 50 extracted at a point of the inputimage. Hong et al. use wavelets of five different frequencies and eight different orientations. They defined twodifferent labeled graphs, called General Face KnowledgeGFK. A big GFK is a labeled graph with 50 nodesFig. 2, where to each node a 40component jet of thecorresponding landmark extracted from 25 individual faceshas been assigned. A small GFK is a labeled graph with16 nodes Fig. 2. Each node contains a 12component jetfour wave field orientations and three frequencies thathas been extracted from a set of eight faces. The small GFKis used to find the exact face location in an input facialimage and the big GFK is used to localize the facialfeatures. Hong et al. utilize the PersonSpotter system 81and the method of elastic graph matching proposed byWiskott 98 to fit the modelgraph to a surface image. First,the small GFK is moved and scaled over the input imageuntil a place of the best match is found. After the matchingis performed, the exact face position is derived from thecanonical graph size value the mean Euclidean distance ofall nodes from the center of gravity. Then the big GFK isfitted to the cropped face region and a nodeweightingmethod is applied. A low weight is assigned to the nodeson the face and hair boundary and a high weight isassigned to the nodes on the facial features. The big GFKwith weighted nodes is used further to emotionally classifythe shown facial expression. Although Hong et al. utilizethe PersonSpotter system 81, which deals with realtimeprocessing of video sequences, they perform facial expression analysis from static images. The dense modelgraphseems very suitable for facial action coding based on theextracted deformations of the graph. However, this issuehas not been discussed by Hong et al. 30.To represent the face, Huang and Huang 32 utilize apoint distribution model PDM, 11. The used PDM hasbeen generated from 90 facial feature points that have beenmanually localized in 90 images of 15 Chinese subjectsshowing six basic emotions Figs. 3 and 4. The mouth isincluded in the model by approximating the contour of themouth with three parabolic curves. Since the proposedmodel is a combination of the PDM and a mouth template,it is arguably as close to a featurebased model as to atemplatebased model. We classified it as a holistic facemodel since the PDM models the face as a whole andinteracts with the estimated face region of an input image asentire. After an initial placement of the PDM in the inputimage Section 4.1., the method of Huang and Huangmoves and deforms the entire PDM simultaneously. Here, agradientbased shape parameters estimation, which minimizes the overall graylevel model fitness measure, isapplied. The search for the mouth starts by defining anappropriate search region on basis of the fitted PDM. Thenthe darkest point in each vertical strip of the search regionis found. A graylevel thresholding is applied to eliminatethe misleading points and a parabolic curve is used toapproximate the mouththrough line. The edges with thestrongest gradient, located above this line, are approximated with another parabolic curve to represent the upperlip. The same method is applied to find the lower lip. Themethod will not work out if the teeth are visible, i.e., if thereis no dark region between the lips. Also, successfulness ofthe method is strongly constrained Section 4.1.1.PANTIC AND ROTHKRANTZ AUTOMATIC ANALYSIS OF FACIAL EXPRESSIONS THE STATE OF THE ART 1433Fig. 2. A small modelgraph small GFK and a dense modelgraph bigGFK 81.Fig. 3. Aligned training set for generation of PDM model reprinted from32 with permission from Academic Press  1997 Academic Press.Fig. 4. Fitted PDM model reprinted from 32 with permission fromAcademic Press  1997 Academic Press.Padgett and Cottrell 61 also use a holistic facerepresentation, but they do not deal with facial expressioninformation extraction in an automatic way. They made useof the facial emotion database assembled by Ekman andFriesen 20, 21, digitized 97 images of six basic emotionalfacial expressions, and scaled them so that the prominentfacial features were located in the same image region. Then,in each image, the area around each eye was divided intotwo vertically overlapping 32 32 pixel blocks and the areaaround the mouth was divided into three horizontallyoverlapping 32 32 pixel blocks. PCA of 32 32 pixelblocks randomly taken over the entire image was applied inorder to generate the eigenvectors. The input to a NN usedfor emotional classification of an expression was thenormalized projection of the seven extracted blocks on thetop 15 principal components.Yoneyama et al. 104 use a hybrid approach to facerepresentation. They fit an 8 10 quadratic grid to a normalized facial image see Section 4.1.1. Then, an averaged opticalflow is calculated in each of the 8 10 regions. To calculatethe optical flow between a neutral and an examined facialexpression image, they use the optical flow algorithmproposed by Horn and Schunck 31. The magnitude andthe direction of the calculated optical flows are simplified to aternary value magnitude in only the vertical direction. Theinformation about a horizontal movement is excluded.Hence, the method will fail to recognize any facial appearancechange that involves a horizontal movement of the facialfeatures. The face should be without facial hair and glassesand no rigid head motion may be encountered for the methodto work correctly.Zhang et al. 106 use a hybrid approach to facerepresentation, but do not deal with facial expressioninformation extraction in an automatic way. They use34 facial points Fig. 5 for which a set of Gabor waveletcoefficients is extracted. Wavelets of three spatial frequencies and six orientations have been utilized. Zhang et al. dealonly with 256 256 pixels frontal view images of ninefemale Japanese subjects, manually normalized so that thedistance between the eyes is 60 pixels. A similar facerepresentation was recently used by Lyons et al. 51 forexpression classification into the six basic plus neutralemotion categories. They used a fiducial grid of manuallypositioned 34 nodes on 256 256 pixels images used in106, but apply wavelets of five spatial frequencies and sixangular orientations.4.2.2 Facial Data Extraction from Static ImagesFeatureBased MethodsThe second category of the surveyed methods for automaticfacial expression analysis from static images uses ananalytic approach to face representation Table 3, Table 5and applies a featurebased method for expression information extraction from an input image.In their earlier work 43, 44, Kobayashi and Haraproposed a geometric face model of 30 FCPs Fig. 6. In theirlater work 42, they utilize a CCD camera in monochromemode to obtain a set of brightness distributions of 13 verticallines crossing the FCPs. First, they normalize an inputimage by using an affine transformation so that the distancebetween irises becomes 20 pixels. From the distancebetween the irises, the length of the vertical lines isempirically determined. The range of the acquired brightness distributions is normalized to 0,1 and these data aregiven further to a trained NN for expression emotionalclassification. A shortcoming of the proposed face representation is that the facial appearance changes encounteredin a horizontal direction cannot be modeled. The realtimesystem developed by Kobayashi and Hara works withonline taken images of subjects with no facial hair or glassesfacing the camera while siting at approximately 1m distancefrom it.Pantic and Rothkrantz 62 are utilizing a pointbasedmodel composed of two 2D facial views, the frontal and theside view. The frontalview face model is composed of30 features. From these, 25 features are defined incorrespondence with a set of 19 facial points Fig. 7 andthe rest are some specific shapes of the mouth and chin. Theutilized sideview face model consists of 10 profile points,which correspond with the peaks and valleys of thecurvature of the profile contour function Fig. 7. To localizethe contours of the prominent facial features and thenextract the model features in an input dualview, Pantic andRothkrantz apply multiple feature detectors for each1434 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 22, NO. 12, DECEMBER 2000Fig. 5. Fiducial grid of facial points 51.Fig. 6. Facial Characteristic Points 42.prominent facial feature eyebrows, eyes, nose, mouth, andprofile. For example, to localize the eyes they use methodsproposed in 94, and 37 with 96. Then, the best of theacquired redundant results is chosen. This is done basedon both, the knowledge about the facial anatomy used tocheck the correctness of the result of a certain detector andthe confidence in the performance of a specific detectorassigned to it based on its testing results. The performanceof the detection scheme was tested on 496 dual views.Human observers in 89 percent approved when visuallyinspected the achieved localization of the facial features.The system cannot deal with minor inaccuracies of theextracted facial data and it deals merely with images offaces without facial hair or glasses.Zhao et al. 107 also utilize a pointbased frontalview facemodel but do not deal with automatic facial expression dataextraction. They utilize 10 facial distances, to manuallymeasure 94 images selected from the facial emotion databaseassembled by Ekman and Friesen 20, 21. These data areused further for expression emotional classification.4.2.3 Facial Data Extraction from Image SequencesTemplateBased MethodsA first category of the surveyed approaches to automaticfacial expression analysis from image sequences uses aholistic or a hybrid approach to face representation Table 3,Table 5 and applies a templatebased method for facialexpression information extraction from an input imagesequence.Black and Yacoob 2 are using local parameterizedmodels of image motion for facial expression analysis. Theyutilize an affine, a planar, and an affinepluscurvatureflow model. The planar model is used to represent rigidfacial motions. The motion of the plane is used to stabilizetwo frames of the examined image sequence and themotions of the facial features are then estimated relativelyto the stabilized face. Nonrigid motions of facial featureswithin the local facial areas of the eyebrows, eyes, andmouth Fig. 8 are represented by affinepluscurvaturemodel. To recover the parameters of the flow models, arobust regression scheme based on the brightness constancyassumption is employed. To cope with large motions, acoarsetofine gradientdescent strategy is used. In theapproach proposed by Black and Yacoob, the initial regionsfor the head and the facial features were selected by handand thereafter automatically tracked.By applying an adapted gradientbased optical flowalgorithm 3, Otsuka and Ohya 59 are estimating themotion in the local facial areas of the right eye and themouth Fig. 9. The input facial images are taken by acamera mounted on a helmet worn by the subject andsubsampled by eight in both directions 59. After theopticalflow algorithm is applied, a 2D Fourier transform isutilized to the horizontal and the vertical velocity field andthe lowerfrequency coefficients are extracted as a 15Dfeature vector, which is used further for facial expressionemotional classification. Otsuka and Ohya are takingadvantage of the face symmetry when estimating themotion just in the local facial areas of the right eye andthe mouth. As a consequence, their method is not sensitiveto unilateral appearance changes of the left eye.Essa and Pentland 24 are utilizing a hybrid approach toface representation. First, they applied the eigenspacemethod 65 to automatically track the face in the sceneSection 4.1.2 and extract the positions of the eyes, nose,and mouth. The method for extracting the prominent facialfeatures employs eigenfeatures approximated using PCAon a sample of 128 images. The eigenfeatures define a socalled feature space. To detect the location of theprominent facial features in a given image, the distance ofeach featureimage from the relevant feature space iscomputed using a FFT and a local energy computation.The extracted position of the prominent facial features isfurther used to normalize the input image. A 2D spatiotemporal motion energy representation of facial motionestimated from two consecutive normalized frames is usedas a dynamic face model. Essa and Pentland use the opticalflow computation method proposed by Simoncelli 80. Thisapproach uses a multiscale coarsetofine Kalman filter toobtain motion estimates and errorcovariance information.The method computes first a mean velocity vector, whichrepresents the estimated flow from consecutive normalizedfacial images of a video sequence. The flow covariancesbetween different frames are stored and used together withPANTIC AND ROTHKRANTZ AUTOMATIC ANALYSIS OF FACIAL EXPRESSIONS THE STATE OF THE ART 1435Fig. 7. Facial points of the frontalview face model and the sideviewface model 63.Fig. 8. Planar model for representing rigid face motions and affinepluscurvature model for representing nonrigid facial motions 3.the recursive continuous time Kalman filter to calculate theerror predictions, based on previous data, and to obtain acorrected, noisefree 2D motion field. The method hasbeen applied to frontalview facial image sequences.Another system, where a hybrid approach to facerepresentation is utilized, is proposed by Kimura andYachida 41. They are utilizing a Potential Net. To fit thePotential Net to a normalized facial image see Section 4.1.1,they compute first the edge image by applying a differentialfilter. Then, in order to extract the external force, which is asmooth gradient of the edge image, they are applying aGaussian filter. The filtered image is referred to as apotential field to which the elastic net model Fig. 10 isplaced. The net deforms further governed by the elasticforce of the potential field. The method seems suitable forfacial action encoding based on the extracted deformationsof the net. However, Kimura and Yachida have notdiscussed this issue.Wang et al. 95 use a hybrid approach to facerepresentation too. They utilize 19 facial feature pointsFFPsseven FFPs to preserve the local topology and12 FEFPs depicted as . in Fig. 11 for facial expressionrecognition. The FFPs are treated as nodes of a labeledgraph that are interconnected with links representing theEuclidean distance between the nodes. The links areweighted with empirically set parameters denoting someproperties of the facial features to which the FFPs belong.For example, the links between the mouth nodes areweighted with a smaller weight since the mouth candeform violently. The initial location of the FFPs in thefirst frame of an input image sequence is assumed to beknown. To track the FFPs in the rest of the frames, Wang etal. use a system that consists of two layers, a memory layerand an input layer. The correspondence between the FFPstracked in two consecutive frames is treated as a labeledgraph matching problem as proposed by Buhmann et al. 7,where the antecedent frame is treated as the memory layerand the current frame as the input layer. The graphmatching is realized as a dynamic process of node diffusion,which minimizes a cost function based on a simulatedannealing procedure. The observed faces should be withoutfacial hair or glasses, no rigid head motion may beencountered, the first frame of the examined imagesequence should represent an expressionless face and theFFPs should be marked in the first frame for the method towork correctly. The face model used by Wang et al.represents a way of improving the labeledgraphbasedmodels e.g., 30 to include intensity measurement of theencountered facial expressions based on the informationstored in the links between the nodes.4.2.4 Facial Data Extraction from Image SequencesFeatureBased MethodsOnly one of the surveyed methods for automatic facialexpression analysis from image sequences utilizes ananalytic face representation Table 3, Table 5 and appliesa featurebased method for facial expression informationextraction. Cohn et al. 10 use a model of facial landmarkpoints localized around the facial features, handmarkedwith a mouse device in the first frame of an examinedimage sequence. In the rest of the frames, a hierarchicaloptical flow method 49 is used to track the optical flows of13 13 windows surrounding the landmark points. Thedisplacement of each landmark point is calculated bysubtracting its normalized position in the first frame fromits current normalized position all frames of an input1436 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 22, NO. 12, DECEMBER 2000Fig. 10. Potential Field and corresponding Potential Net 41.Fig. 9. Motion vector field represented in the deformation of twogrids 60.Fig. 11. The FFPs 95.sequence are manually normalized. The displacementvectors, calculated between the initial and the peak frame,represent the facial information used for recognition of thedisplayed facial actions. The face should be without facialhair glasses, no rigid head motion may be encountered,the first frame should be an expressionless face, and thefacial landmark points should be marked in the first framefor the method to work correctly.4.3 Facial Expression ClassificationThe last step of facial expression analysis is to classifyidentify, interpret the facial display conveyed by the face.The surveyed facial expression analyzers classify theencountered expression i.e., the extracted facial expressioninformation either as a particular facial action or aparticular basic emotion. Some of the systems performboth. Independent of the used classification categories, themechanism of classification applied by a particular surveyed expression analyzer is either a templatebased or aneuralnetworkbased or a rulebased classificationmethod. The applied methods for expression classificationin terms of facial actions are summarized in Table 7. Table 8summarizes the utilized methods for facial expressionemotional classification.If a templatebased classification method is applied, theencountered facial expression is compared to the templatesdefined for each expression category. The best matchdecides the category of the shown expression. In general,it is difficult to achieve a templatebased quantifiedrecognition of a nonprototypic facial expression. There areinfinitely a lot of combinations of different facial actions andtheir intensities that should be modeled with a finite set oftemplates. The problem becomes even more difficult due tothe fact that everybody has hisher own maximal intensityof displaying a certain facial action.Although the neural networks represent a  black boxapproach and arguably could be, classified as templatebased methods, we are classifying the neuralnetworkbased methods separately. We are doing so because atypical neural network can perform a quantified facialexpression categorization into multiple classes while, ingeneral, the templatebased methods cannot achieve such aperformance. In a neuralnetworkbased classification approach, a facial expression is classified according to thecategorization process that the network learned during atraining phase. Most of the neuralnetworkbased classification methods utilized by the surveyed systems performfacial expression classification into a single category.Recognition of nonprototypic facial expressions is feasible,however, if each neural network output unit is associatedwith a weight from the interval 0,1, as proposed in 106,44, 71, 56, instead of being associated with either 0 or 1e.g., 42, 107. As it can be seen from Table 8, weclassified some of the expression classifiers as templatebased methods although they utilize a neural network i.e.,Yonoyama et al. 104. We are doing so because the overallcharacteristics of these methods fit better the overallproperties of the templatebased expression classificationapproaches.The rulebased classification methods, utilized by thesurveyed systems, classify the examined facial expressioninto the basic emotion categories based on the previouslyencoded facial actions Table 7, Table 8. The prototypicexpressions, which characterize the emotion categories, arefirst described in terms of facial actions. Then, the shownexpression, described in terms of facial actions, is comparedto the prototypic expressions defined for each of theemotion categories and classified in the optimal fittingcategory.4.3.1 Expression Classification from Static ImagesTemplateBased MethodsA first category of the surveyed methods for automaticexpression analysis from static images applies atemplatebased method for expression classification. Themethods in this category perform expression classificationinto a single basic emotion category.Given a new example of a face and the extractedparameters of AAM Section 4.2.1, the main aim ofEdwards et al. 18 is to identify the observed individualin a way which is invariant to confounding factors such aspose and facial expression. To achieve this goal, theyPANTIC AND ROTHKRANTZ AUTOMATIC ANALYSIS OF FACIAL EXPRESSIONS THE STATE OF THE ART 1437TABLE 7Facial Expression Classification in Terms of Facial Actionsutilized the Mahalonobis distance measure 27 on arepresentative set of training facial images. This classifierassumes that the intraclass variation pose and expressionis very similar for each individual. Edwards et al. usedLinear Discriminant Analysis LDA to separate linearly theinterclass variability identity from the intraclass variability. They showed that a subspace could be constructedthat is orthogonal to matrix D a matrix of orthogonalvectors describing the principal types of interclass variation, which models only the intraclass variation due tochange in pose, expression, and lightning. The expressionrecognition performance of the AAM has been trained andtested on 2 200 images of six basic emotional expressionsshown by 25 subjects. The images were chosen for limitedpose and lightning variation. The achieved recognition ratefor the six basic and neutral emotion categories was74 percent. Edwards et al. explain the low recognition rateby the limitations and unsuitability of the utilized linearclassifier 18. It is not known how the method will behavein the case of an unknown subject.To achieve expression classification into one of the sixbasic plus neutral emotion categories, Hong et al. 30made the assumption that two persons who look alike havea similar way for showing the same expression. First, theyfit a labeled graph Fig. 2 to an input facial imageSection 4.2.1. Then, the best matching person, whosepersonalized gallery is available, is found by applying themethod of elastic graph matching proposed by Wiskot 98.The personalized galleries of nine people have beenutilized, where each gallery contained 28 images fourimages per expression. The personalized gallery of the bestmatching person is used to make the judgement on thecategory of the observed expression. The method has beentested on images of 25 subjects. The achieved recognitionrate was 89 percent in the case of the familiar subjects and73 percent in the case of unknown persons. As indicated byHong et al., the availability of the personalized galleries ofmore individuals would probably increase the systemsperformance. The time necessary for performing a fullanalysis of an incoming facial image is about eight seconds.1438 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 22, NO. 12, DECEMBER 2000TABLE 8The Methods for Facial Expression Emotional ClassificationIn order to perform emotional classification of theobserved facial expression, Huang and Huang 32 performan intermediate step by calculating 10 Action ParametersAPs, Fig. 12. The difference between the model featureparameters Figs. 3 and 4 found in an expressionless face andthose found in the examined facial expression of the sameperson generates the APs. Experimentally, they found outthat the first two terms of eigenvalues could represent morethan 90 percent of APs variations. They used a minimumdistance classifier to cluster the two principal action parameters of 90 training image samples into six clustersrepresenting six basic emotional expressions. Since theprincipal component distribution of each expression isoverlapped with the distribution of at least two otherexpressions, three best matches are selected. The highestscore of the three correlation determines the final classification of the examined expression. The proposed method hasbeen tested on another 90 images shown by the same subjects.The achieved correct recognition ratio was 84.5 percent. It isnot known how the method will behave in the case ofunknown subjects. Also, the descriptions of the emotionalexpressions, given in terms of facial actions, are incomplete.For example, an expression with lowered mouth corners andraised eyebrows will be classified as sadness.On a fiducial grid of manually positioned 34 nodesFig. 5, Lyons et al. 51 sample the amplitude of thecomplex valued Gabor transform coefficients and combinethese data into a single vector which they call labeledgraphvector LG vector. The ensemble of LG vectors from atraining set of images is further subjected to PCA. Theensemble of LGPCA vectors from the training set is thenanalyzed using LDA in order to separate vectors intoclusters having different facial attributes. Lyons et al.experimented with the binary classifiers for the presenceor absence of a particular facial expression. They built sixbinary classifiers, one for each basic emotion category, andcombined them into a single facial expression classifier. Aninput LG vector is classified by being projected along thediscriminant vectors calculated for each independentlytrained binary classifier. For an input image that ispositively classified for two or more emotion categories,the normalized distances to the cluster centers are used as adeciding factor. The input sample is classified as a memberof the nearest cluster. An input image that is not positivelyclassified for any category is classified as neutral. To testtheir method, Lyons et al. used a set of 193 images ofdifferent facial expressions displayed by nine Japanesefemales, which has been used by Zhang et al. 106 seeSection 4.3.2. The entire set of images was divided into10 segments the discriminant vectors were calculated usingnine of these segments and the generalization performancewas tested on the remaining segment the results wereaveraged over all 10 distinct partitions. The generalizationrate was 92 percent. The method was also tested on thisimage set only partitioned into nine segments, eachcorresponding to one expresser. The generalization ratewas 75 percent for recognition of expression of a novelsubject.Yoneyama et al. 104 extract 80 facial movementparameters, which describe the change between an expressionless face and the currently examined facial expression of the same subject Section 4.2.1. To recognize fourtypes of expressions sadness, surprise, anger, and happiness, they use 2 bits to represent the values of 80 parameters1 1 for up, 1 1 for down and 1 1 for no movementand two identical discrete Hopfield networks. Each networkconsists of 14 14 neurons, where 36 neurons are added toform a neurons square and have 1 as an initial value. Thefirst net NN1 is trained on 40 data representing fourexpressions shown by 10 subjects. NN2 is trained just onfour data representing the most clearly shown fourexpressions. The NNs were trained using the Personnazlearning rule 36. For each examined image, the output ofthe NN1 is matched with all of the examples used fortraining of the NN1 and the Euclidean distances arecalculated. The distances are averaged per expression. Ifthe difference between the minimal average and the secondminimal average is greater than 1, the category of theexamined expression is decided. Otherwise, the output ofthe NN2 is matched to the examples used for training of theNN2 in order to decide a final category of the shownexpression. The average recognition rate was 92 percent.The images used for training of the networks were also usedfor their testing.4.3.2 Expression Classification from Static ImagesNeuralNetworkBased MethodsA second category of the surveyed methods for automaticfacial expression analysis from static images applies aneural network for facial expression classification. Exceptthe method proposed by Zhang et al. 106, the methodsbelonging to this category perform facial expressionclassification into a single basic emotion category.For classification of expression into one of six basicemotion categories, Hara and Kobayashi 42 apply a234 50 6 backpropagation neural network. The unitsof the input layer correspond to the number of thebrightness distribution data extracted from an input facialimage Section 4.2.2 while each unit of the output layerPANTIC AND ROTHKRANTZ AUTOMATIC ANALYSIS OF FACIAL EXPRESSIONS THE STATE OF THE ART 1439Fig. 12. APs reprinted from 32 with permission from Academic Press 1997 Academic Press.corresponds to one emotion category. The neural networkhas been trained on 90 images of six basic facialexpressions shown by 15 subjects and it has been testedon a set of 90 facial expressions images shown by another15 subjects. The average recognition rate was 85 percent.The process takes 66.7ms.For emotional classification of an input facial image intoone of 6 basic plus neutral emotion categories, Padgettand Cottrell 61 utilize a backpropagation neural network.The input to the network consists of the normalizedprojection of seven 32 32 pixel blocks on the first15 principal components of previously generated randomblockseigenspace Section 4.2.1. The hidden layer of theNN contains 10 nodes and employs a nonlinear Sigmoidactivation function. The output layer of the NN containsseven units, each of which corresponds to one emotioncategory. Padgett and Cottrell used the images of six basicplus neutral expressions shown by 12 subjects. Theytrained the network on the images of 11 subjects and testedit on the images of the 12th subject. By changing the trainingand the testing set, they trained 12 networks. The averagecorrect recognition rate achieved was 86 percent.Zhang et al. 106 employ a 680 7 7 neural networkfor facial expression classification into six basic plusneutral emotion categories. The input to the networkconsists of the geometric position of the 34 facial pointsFig. 11 and 18 Gabor wavelet coefficients sampled at eachpoint. The neural network performs a nonlinear reductionof the input dimensionality and makes a statistical decisionabout the category of the observed expression. Each outputunit gives an estimation of the probability of the examinedexpression belonging to the associated category. The network has been trained using a resilient propagation 72. Aset of 213 images of different expressions displayed by nineJapanese females has been used to train and test the usednetwork. The database has been partitioned into tensegments. Nine segments have been used to train thenetwork while the remaining segment has been used to testits recognition performance. This process has been repeatedfor each of the 10 segments and the results of all 10 trainednetworks have been averaged. The achieved recognitionrate was 90.1 percent. The performance of the network isnot tested for recognition of expression of a novel subject.Zhao and Kearney 107 utilize a 10 10 3 backpropagation neural network for facial expression classification into one of six basic emotion categories. They used94 images of six basic facial expressions selected from theemotion database assembled by Ekman and Friesen 20,21. On each image, 10 distances were manually measured.The difference between a distance measured in an examined image and the same distance measured in anexpressionless face of the same person was normalized.Then, each such measure was mapped into one of the eightsignaled intervals of the appropriate standard deviationfrom the corresponding average. These intervals formed theinput to the NN. The output of the NN represents theassociated emotion e.g., the string 001 is used torepresent happiness. The NN was trained and tested onthe whole set of data 94 images with 100 percentrecognition rate. It is not known how the method willbehave in the case of an unknown subject.4.3.3 Expression Classification from Static ImagesRuleBased MethodsJust one of the surveyed methods for automatic facialexpression analysis from static images applies a rulebasedapproach to expression classification. The method proposedby Pantic and Rothkrantz 62 achieves automatic facialaction coding from an input facial dualview in few steps.First, a multidetector processing of the system performsautomatic detection of the facial features in the examinedfacial image Section 4.2.2. From the localized contours ofthe facial features, the model features Fig. 7 are extracted.Then, the difference is calculated between the currentlydetected model features and the same features detected inan expressionless face of the same person. Based on theknowledge acquired from FACS 21, the production rulesclassify the calculated model deformation into the appropriate AUsclasses total number of classes is 31. Theperformance of the system in automatic facial action codingfrom dualview images has been tested on a set of 496 dualviews 31 expressions of separate facial actions shown twiceby eight human experts. The average recognition rate was92 percent for the upper face AUs and 86 percent for thelower face AUs.Classification of an input facial dualview into multipleemotion categories is performed by comparing theAUcoded description of the shown facial expression toAUcoded descriptions of six basic emotional expressions,which have been acquired from the linguistic descriptionsgiven by Ekman 22. The classification into and, then,quantification of the resulting emotion labels is based on theassumption that each subexpression of a basic emotionalexpression has the same influence on scoring that emotioncategory. The overall performance of the system has beentested on a set of 265 dual facial views representing six basicand various blended emotional expressions shown by eightsubjects. A correct recognition ratio of 91 percent has beenreported. The dualviews used for testing of the systemhave been recorded under constant illumination and noneof the subjects had a moustache, a beard, or wear glasses.4.3.4 Expression Classification from Image SequencesTemplateBased MethodsThefirstcategoryof thesurveyedmethodsforautomaticfacialexpression analysis from facial image sequences applies atemplatebased method for expression classification.The facial action recognition method proposed byCohn et al. 10 applies separate discriminant functionanalyzes within facial regions of the eyebrows, eyes, andmouth. Predictors were facial points displacementsSection 4.2.4 between the initial and peak frames in aninput image sequence. Separate group variancecovariancematrices were used for classification. Image sequences504 containing 872 facial actions displayed by 100 subjects have been used. The images have been recordedunder constant illumination, using fixed light sources andnone of the subjects wear glasses 48. Data wererandomly divided into training and test sets of imagesequences. They used two discriminant functions forthree facial actions of the eyebrow region, two discriminant functions for three facial actions of the eye region,1440 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 22, NO. 12, DECEMBER 2000and five discriminant functions for nine facial actions ofthe nose and mouth region. The accuracy of theclassification was 92 percent for the eyebrow region,88 percent for the eye region and 83 percent for the nosemouth region. The method proposed by Cohn et al. dealsneither with image sequences containing several facialactions in a row, nor with inaccurate facial data, nor withfacial action intensity yet the concept of the methodmakes it possible.Essa and Pentland 24 use a controltheoretic method toextract the spatiotemporal motionenergy representation offacial motion for an observed expression Section 4.2.3. Bylearning ideal 2D motion views for each expressioncategory, they generated the spatiotemporal templatesFig. 13 for six different expressionstwo facial actionssmile and raised eyebrows and four emotional expressionssurprise, sadness, anger, and disgust. Each template hasbeen delimited by averaging the patterns of motiongenerated by two subjects showing a certain expression.The Euclidean norm of the difference between the motionenergy template and the observed image motion energy isused as a metric for measuring similaritydissimilarity.When tested on 52 frontalview image sequences of eightpeople showing six distinct expressions, a correct recognition rate of 98 percent has been achieved.Kimura and Yachida 41 fit a Potential Net to each frameof the examined facial image sequence Section 4.2.3,Fig. 10. The pattern of the deformed net is compared tothe pattern extracted from an expressionless face the firstframe of a sequence and the variation in the position of thenet nodes is used for further processing. Kimura andYachida built an emotion space by applying PCA on siximage sequences of three expressionsanger, happiness,and surpriseshown by a single person gradually, fromexpressionless to a maximum. The eigenspace spanned bythe first three principal components has been used as theemotion space, onto which an input image is projected for aquantified emotional classification. The proposed methodhas been unsuccessfully tested for image sequences ofunknown subjects. A very small number of trainingexamples six sequences and an insufficient diversity ofthe subjects one person have probably caused this.Otsuka and Ohya 60 match the temporal sequence ofthe 15D feature vector Section 4.2.3 to the models of the sixbasic facial expressions by using a lefttoright HiddenMarkov Model. The used HMM consists of five states,namely, relaxed S1, S5, contracted S2, apex S3, andrelaxing S4. To facilitate recognition of a single imagesequence, the transition from the final state to the initialstate is added. To make recognition of multiple sequencesof expression images feasible, the transition from a finalstate to the initial states of other categories is added. Thetransition probability and the output probability of eachstate are obtained from sample data by using the BaumWelch algorithm. The initial probability is estimated byapplying a kmeans clustering algorithm on the sample datain which a squared sum of vector components is added asan extra component. The HMM was trained on 120 imagesequences, shown by two male subjects. The method wastested on image sequences shown by the same subjects.Therefore, it is not known how the method will behave inthe case of an unknown expresser. Although Otsuka andOhya claim that the recognition performance was good,they do not define the extent of good.Wang et al. 95 utilize a 19points labeled graph withweighted links to represent the face Section 4.2.3, Fig. 11.Twelve of these points FEFPs are used for expressionrecognition. For each of three emotion categoriesanger,happiness, and surprisethey use 12 average FEFPBspline curves, one for each FEFP, to construct theexpression model. Each curve describes the relationshipbetween the expression change and the displacement of thecorresponding FEFP. Each expression model has beendefined from 10 image sequences displayed by five subjects.The category of an expression is decided by determining theminimal distance between the actual trajectory of FEFPsand the trajectories defined by the models. The distancefunctions are minimized using the method proposed byBrent 69. The degree of expression change is determinedbased on the displacement of the FEFPs in the consecutiveframes. The method has been tested on 29 image sequencesof three emotional expressions shown by eight subjectsyoung and of Asian ethnicity. The images were acquiredunder constant illumination and none of the subjects had amoustache, a beard or wear glasses. The average recognition rate was 95 percent. The average process time was2.5sframe.4.3.5 Expression Classification from Image SequencesRuleBased MethodsJust one of the surveyed methods for automatic facialexpression analysis from image sequences applies a rulebased approach to expression classification. Black andYacoob 2, 3, utilize local parameterized models of imagemotion to represent rigid head motions and nonrigid facialmotions within the local facial areas Fig. 8. The motionparameters e.g., translation and divergence are used toderive the midlevel predicates that describe the motion ofthe facial features. Each midlevel predicate is represented ina form of a rule, where the left part of the rule is acomparison of a motion parameter to a certain thresholdand the right part of the rule is the derived predicate. Thethresholds are dependent on the face size in the image andwere set empirically from a few sequences. Black andYacoob did not give a full list of the midlevel predicates andthe number of different facial actions that the method canrecognize is not known. In their method, the facialexpression emotional classification considers the temporalconsistency of the midlevel representation predicates. ForPANTIC AND ROTHKRANTZ AUTOMATIC ANALYSIS OF FACIAL EXPRESSIONS THE STATE OF THE ART 1441Fig. 13. The spatiotemporal template for surprise 24.each of six basic emotional expressions, they developed amodel represented by a set of rules for detecting thebeginning and ending of the expression. The rules areapplied to the predicates of the midlevel representation.The method has been tested on 70 image sequencescontaining 145 expressions shown by 40 subjects rangedin ethnicity and age. The expressions were displayed one atthe time. The achieved recognition rate was 88 percent. Lipbiting is sometimes mistakenly identified as a smile 2.Also, the method does not deal with blends of emotionalexpressions. For example, a blend of the angry and scaredexpressions is recognized as disgust. The reason lies in therules used for classification.5 DISCUSSIONWe have explored and compared a number of differentrecently presented approaches to facial expression detectionand classification in static images and image sequences. Theinvestigation compared automatic expression informationextraction using facial motion analysis 2, 10, 48, 24,60, holistic spatial pattern analysis 18, 30, 32, 104,41, 95, and analysis of facial features and their spatialarrangement 42, 62, 10. This investigation also compared facial expression classification using holistic spatialanalysis 18, 30, 32, 51, 104, 61, holistic spatiotemporal analysis 2, 24, 41, 60, 95, graylevel patternanalysis using local spatial filters 51, 106, and analyticfeaturebased spatial analysis 32, 42, 107, 62. Thenumber of the surveyed systems is rather large and thereader might be interested in the results of the performedcomparison in terms of the best performances. Yet, wedeliberately didnt make an attempt to label some of thesurveyed systems as being better than some other systemspresented in the literature. We believe that a welldefinedand commonly used single database of testing imagesimage sequences is the necessary prerequisite for ranking the performances of the proposed systems in anobjective manner. Since such a single testing data set has notbeen established yet, we left the reader to decide theranking of the surveyed systems according to hisher ownpriorities and based on the overall properties of thesurveyed systems Tables 3, 4, 5, 6, 7, and 8.5.1 Detection of the Face and Its FeaturesMost of the currently existing systems for facial expressionanalysis assume that the presence of a face in the scene isensured. However, in many instances, the systems do notutilize a camera setting that will ascertain the correctness ofthat assumption. Only two of the surveyed systems processimages acquired by a mounted camera 59, 62 and onlytwo systems deal with the automatic face detection in anarbitrary scene 30, 24. In addition, in many instancesstrong assumptions are made to make the problem of facialexpression analysis more tractable Table 6. Some commonassumptions are the images contain frontal facial view, theillumination is constant, the light source is fixed, the facehas no facial hair or glasses, the subjects are young i.e.,without permanent wrinkles, and of the same ethnicity.Also, in most of the reallife situations, it cannot be assumedthat the observed subject will remain immovable, asassumed by some methods. Thus, if a fixed camera acquiresthe images, the system should be capable of dealing withrigid head motions. Only three of the surveyed systems dealto some extent with rigid head motions 2, 18, 30. Forthe sake of universality, the system should be able toanalyze facial expressions of any person independently ofage, ethnicity, and outlook. Yet, only the method proposedby Essa and Pentland 24 deals with the facial images offaces with facial hair andor eyeglasses.For the researchers of automated visionbased facialexpression analysis, this suggest investigating towards arobust detection of the face and its features despite thechanges in viewing and lightning conditions and thedistractions like glasses, facial hair or changes in hair style.Another interesting but yet not investigated ability of thehuman visual system is filling in missing parts of theobserved face and perceiving a whole face even when apart of it is occluded e.g., by hand.5.2 Facial Expression ClassificationIn general, the existing expression analyzers perform asingular classification of the examined expression into oneof the basic emotion categories proposed by Ekman andFriesen 20. This approach to expression classification hastwo main limitations. First, pure emotional expressionsare seldom elicited. Most of the time, people show blends ofemotional expressions. Therefore, classification of anexpression into a single emotion category isnt realistic.An automated facial expression analyzer should realizequantified classification into multiple emotion categories.Only two of the surveyed systems 62, 106 performquantified facial expression classification into multiple basicemotion categories. Second, it is not at all certain that allfacial expressions displayed on the face can be classifiedunder the six basic emotion categories. So even if anexpression analyzer performs a quantified expressionclassification into multiple basic emotion categories, itwould probably not be capable of interpreting each andevery encountered expression. A psychological discussionon the topic can be found in 33, 76, 77, and 26. Someexperimental proofs can be found in the studies of Asianresearchers e.g., 32, 106, which reported that theirAsian subjects have difficulties to express some of the basicexpressions e.g., disgust and fear. Defining interpretationcategories into which any facial expression can be classifiedis one of the key challenges in the design of a realistic facialexpression analyzer. The lack of psychological scrutiny onthe topic makes the problem even harder. A way of dealingwith this problem is to build a system that learns its ownexpertise and allows the user to define hisher owninterpretation categories e.g., see 40.If the system is to be used for behavioral scienceinvestigations of the face, it should perform expressionrecognition as applied to automated FACS encoding. Inother words, it should accomplish facial action coding frominput images and quantification of those codes 16, 17.Four of the surveyed systems perform facial action codingfrom an input image or an image sequence Table 7. Yet,none of these systems performs quantification of the facialaction codes. This task is particularly difficult to accomplishfor a number of reasons. First, there are merely fivedifferent AUs for which FACS provides an option to scoreintensity on a 3level intensity scale low, medium, and1442 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 22, NO. 12, DECEMBER 2000high. Second, some facial actions such as blink, wink, andlips sucked into the mouth, are either encountered or not. Itis not reasonable to talk about a blink having a higherintensity than another blink. In addition, each person hashisher own maximal intensity of displaying a particularfacial action. Therefore, it should be aimed on design of asystem that can start with a generic facial action classification and then adapt to a particular individual to perform aquantification of the accomplished coding for the facialactions for which measuring the activation intensity isreasonable. Also, none of the surveyed systems candistinguish all 44 AUs defined in FACS. This remains a keychallenge for the researchers of automated FACS encoding.Another appealing but still not investigated property of thehuman visual system is assigning a higher priority to theupper face features than to the lower face features sincethey play a more important role in facial expressioninterpretation 22.6 CONCLUSIONAnalysis of facial expressions is an intriguing problemwhich humans solve with quite an apparent ease. We haveidentified three different but related aspects of the problemface detection, facial expression information extraction, andfacial expression classification. Capability of the humanvisual system in solving these problems has been discussed.It should serve as a reference point for any automaticvisionbased system attempting to achieve the samefunctionality. Among the problems, facial expressionclassification has been studied most, due to its utility inapplication domains of human behavior interpretation andHCI. Most of the surveyed systems, however, are based onfrontal view images of faces without facial hair and glasseswhat is unrealistic to expect in these application domains.Also, all of the proposed approaches to automatic expression analysis perform only facial expression classificationinto the basic emotion categories defined by Ekman andFriesen 20. Nevertheless, this is unrealistic since it is not atall certain that all facial expressions able to be displayed onthe face can be classified under the six basic emotioncategories. Furthermore, some of the surveyed methodshave been tested only on the set of images used for training.We hesitate in belief that those systems are personindependent what, in turn, should be a basic property ofa behavioral science research tool or of an advanced HCI.All the discussed problems are intriguing and none hasbeen solved, in the general case. We expect that they wouldremain interesting to the researchers of automated visionbased facial expression analysis for some time.ACKNOWLEDGMENTSThe authors would like to thank J. Steffens, M.J. Lyons, andT. Otsuka for providing us their high quality images used inthe paper, and Dr. Kevin Bowyer as well as the anonymousreviewers for helping to improve the paper. Also, we wouldlike to thank the IEEE Press and the Academic Press forgranting us permission to reprint the figures appearing inthe paper.REFERENCES1 J.N. Bassili, Facial Motion in the Perception of Faces and ofEmotional Expression, J. Experimental Psychology 4, pp. 373379,1978.2 M.J. Black and Y. Yacoob, Recognizing Facial Expressions inImage Sequences Using Local Parameterized Models of ImageMotion, lntl J. Computer Vision, vol. 25, no. 1, pp. 2348, 1997.3 M.J. Black and Y. Yacoob, Tracking and Recognizing Rigid andNonRigid Facial Motions Using Local Parametric Models ofImage Motions, Proc. Intl Conf. Computer Vision, pp. 374381,1995.4 E. Boyle, A.H. Anderson, and A. Newlands, The Effects ofVisibility on Dialogue in a Cooperative Problem Solving Task,Language and Speech, vol. 37, no. 1, pp. 120, 1994.5 V. Bruce, Recognizing Faces. Hove, East Sussex Lawrence ErlbaumAssoc., 1986.6 V. Bruce, What the Human Face Tells the Human Mind SomeChallenges for the RobotHuman Interface, Proc. Intl WorkshopRobot and Human Comm., pp. 4451, 1992.7 J. Buhmann, J. Lange, and C. von der Malsburg, DistortionInvariant Object RecognitionMatching Hierarchically LabelledGraphs, Proc. Intl Joint Conf. Neural Networks, pp. 155159, 1989.8 F.W. Campbell, How Much of the Information Falling on theRetina Reaches the Visual Cortex and How Much is Stored in theMemory Seminar at the Pontificae Academiae Scientiarium ScriptaVaria, 1983.9 L.S. Chen, T.S. Huang, T. Miyasato, and R. Nakatsu, MultimodalHuman EmotionExpression Recognition, Proc. Intl Conf. Automatic Face and Gesture Recognition, pp. 366371, 1998.10 J.F. Cohn, A.J. Zlochower, J.J. Lien, and T. Kanade, FeaturePointTracking by Optical Flow Discriminates Subtle Differences inFacial Expression, Proc. Intl Conf. Automatic Face and GestureRecognition, pp. 396401, 1998.11 T.F. Cootes, C.J. Taylor, D.H. Cooper, and J. Graham, ActiveShape ModelsTraining and Application, Computer Vision ImageUnderstanding, vol. 61, no. 1, pp. 3859, 1995.12 T.F. Cootes, G.J. Edwards, and C.J. Taylor, Active AppearanceModels, Proc. European Conf. Computer Vision, vol. 2, pp. 484498,1998.13 G.W. Cottrell and J. Metcalfe, EMPATH Fface, Emotion, GenderRecognition Using Holons, Advances in Neural InformationProcessing Systems 3, R.P. Lippman, ed., pp. 564571, 1991.14 D. DeCarlo, D. Metaxas, and M. Stone, An Anthropometric FaceModel Using Variational Techniques, Proc. SIGGRAPH, pp. 6774, 1998.15 L.C. De Silva, T. Miyasato, and R. Nakatsu, Facial EmotionRecognition Using Multimodal Information, Proc. Information,Comm., and Signal Processing Conf., pp. 397401, 1997.16 G. Donato, M.S. Bartlett, J.C. Hager, P. Ekman, and T.J. Sejnowski,Classifying Facial Actions, IEEE Trans. Pattern Analysis andMachine Intelligence, vol. 21, no. 10, pp. 974989, Oct. 1999.17 M.S. Bartlett, J.C. Hager, P. Ekman, and T.J. Sejnowski, Measuring Facial Expressions by Computer Image Analysis, Psychophysiology, vol. 36, pp. 253263, 1999.18 G.J. Edwards, T.F. Cootes, and C.J. Taylor, Face RecognitionUsing Active Appearance Models, Proc. European Conf. ComputerVision, vol. 2, pp. 581695, 1998.19 P. Eisert and B. Girod, Analysing Facial Expressions for VirtualConferencing, IEEE Trans Computer Graphics and Applications,vol. 18, no. 5, pp. 7078, 1998.20 P. Ekman and W.V. Friesen, Unmasking the Face. New JerseyPrentice Hall, 1975.21 P. Ekman and W.V. Friesen, Facial Action Coding System FACSManual. Palo Alto Consulting Psychologists Press, 1978.22 P. Ekman, Emotion in the Human Face. Cambridge Univ. Press,1982.23 H.D. Ellis, Process Underlying Face Recognition, The Neuropsychology of Face Perception and Facial Expression, R. Bruyer, ed. pp. 127, New Jersey Lawrence Erlbaum Assoc., 1986.24 I. Essa and A. Pentland, Coding, Analysis Interpretation,Recognition of Facial Expressions, IEEE Trans. Pattern Analysisand Machine Intelligence, vol. 19, no. 7, pp. 757763, July 1997.25 A.J. Fridlund, P. Ekman, and H. Oster, Facial Expressions ofEmotion Review Literature 19701983, Nonverbal Behavior andCommunication, A.W. Siegman and S. Feldstein, eds., pp. 143224.Hillsdale NJ Lawrence Erlbaum Assoc., 1987.PANTIC AND ROTHKRANTZ AUTOMATIC ANALYSIS OF FACIAL EXPRESSIONS THE STATE OF THE ART 144326 A.J. Fridlund, Evolution and Facial Action in Reflex, SocialMotive, and Paralanguage, Biological Psychology, vol. 32, pp. 3100, 1991.27 D.J. Hand, Discrimination and Classification. John Wiley and Sons,1981.28 F. Hara and H. Kobayashi, State of the Art in ComponentDevelopment for Interactive Communication with Humans,Advanced Robotics, vol. 11, no. 6, pp. 585604, 1997.29 R.J. Holt, T.S. Huang, A.N. Netravali, and R.J. Qian, DeterminingArticulated Motion from Perspective Views, Pattern Recognition,vol. 30, no. 9, pp. 14351,449, 1997.30 H. Hong, H. Neven, and C. von der Malsburg, Online FacialExpression Recognition Based on Personalized Galleries, Proc.Intl Conf. Automatic Face and Gesture Recognition, pp. 354359, 1998.31 B. Horn and B. Schunck, Determining Optical Flow, ArtificialIntelligence, vol. 17, pp. 185203, 1981.32 C.L. Huang and Y.M. Huang, Facial Expression RecognitionUsing ModelBased Feature Extraction and Action ParametersClassification, J. Visual Comm. and Image Representation, vol. 8,no. 3, pp. 278290, 1997.33 C.E. Izard, The Face of Emotion. New York AppletonCenturyCrofts, 1971.34 C.E. Izard, Facial Expressions and the Regulation of Emotions,J. Personality and Social Psychology, vol. 58, no. 3, pp. 487498, 1990.35 T. Johanstone, R. Banse, and K.S. Scherer, Acoustic Profiles inPrototypical Vocal Expressions of Emotions, Proc. Intl Conf.Phonetic Science, vol. 4, pp. 25, 1995.36 I. Kanter and H. Sompolinsky, Associative Recall of Memorywithout Errors Physical Review, vol. 35, no. 1, pp. 380392, 1987.37 M. Kass, A. Witkin, and D. Terzopoulos, Snake Active ContourModel, Proc. Intl Conf. Computer Vision, pp. 259269, 1987.38 M. Kato, I. So, Y. Hishinuma, O. Nakamura, and T. Minami,Description and Synthesis of Facial Expressions Based onIsodensity Maps, Visual Computing, T. Kunii, ed., pp. 3956.Tokyo SpringerVerlag, 1991.39 F. Kawakami, M. Okura, H. Yamada, H. Harashima, and S.Morishima, 3D Emotion Space for Interactive Communication,Proc. Computer Science Conf., pp. 471478, 1995.40 G.D. Kearney and S. McKenzie, Machine Interpretation ofEmotion Design of MemoryBased Expert System for InterpretingFacial Expressions in Terms of Signaled Emotions JANUS,Cognitive Science, vol. 17, no. 4, pp. 589622, 1993.41 S. Kimura and M. Yachida, Facial Expression Recognition and ItsDegree Estimation, Proc. Computer Vision and Pattern Recognition,pp. 295300, 1997.42 H. Kobayashi and F. Hara, Facial Interaction between Animated3D Face Robot and Human Beings, Proc. Intl Conf. Systems, Man,Cybernetics,, pp. 3,7323,737, 1997.43 H. Kobayashi and F. Hara, Recognition of Six Basic FacialExpressions and Their Strength by Neural Network, Proc. IntlWorkshop Robot and Human Comm., pp. 381386, 1992.44 H. Kobayashi and F. Hara, Recognition of Mixed FacialExpressions by Neural Network, Proc. Intl Workshop Robot andHuman Comm., pp. 387391, 1992.45 K.M. Lam and H. Yan, An AnalytictoHolistic Approach forFace Recognition Based on a Single Frontal View, IEEE Trans.Pattern Analysis and Machine Intelligence, vol. 20, no. 7, pp. 673686,July 1998.46 H.K. Lee and J.H. Kim, An HMMBased Threshold ModelApproach for Gesture Recognition, IEEE Trans. Pattern Analysisand Machine Intelligence, vol. 21, no. 10, pp. 961973, Oct. 1999.47 H. Li and P. Roivainen, 3D Motion Estimation in ModelBasedFacial Image Coding, IEEE Trans. Pattern Analysis and MachineIntelligence, vol. 15, no. 6, pp. 545555, 1993.48 J.J. Lien, T. Kanade, J.F. Cohn, and C.C. Li, Automated FacialExpression Recognition Based on FACS Action Units, Proc. IntlConf. Automatic Face and Gesture Recognition, pp. 390395, 1998.49 B. Lucas and T. Kanade, An Iterative Image RegistrationTechnique with an Application to Stereo Vision, Proc. Joint Conf.Artificial Intelligence, pp. 674680, 1981.50 M.J. Lyons, S. Akamatsu, M. Kamachi, and J. Gyoba, CodingFacial Expressions with Gabor Wavelets, Proc. Intl Conf.Automatic Face and Gesture Recognition, pp. 200205, 1998.51 M.J. Lyons, J. Budynek, and S. Akamatsu, Automatic Classification of Single Facial Images, IEEE Trans. Pattern Analysis andMachine Intelligence, vol. 21, no. 12, pp. 1,3571,362, 1999.52 K. Mase, Recognition of Facial Expression from Optical Flow,IEICE Trans., vol. E74, no. 10, pp. 3,4743,483, 1991.53 K. Matsumura, Y. Nakamura, and K. Matsui, MathematicalRepresentation and Image Generation of Human Faces byMetamorphosis, Electronics and Comm. in Japan3, vol. 80, no. 1,pp. 3646, 1997.54 K. Matsuno, C.W. Lee, and S. Tsuji, Recognition of FacialExpression with Potential Net, Proc. Asian Conf. Computer Vision,pp. 504507, 1993.55 A. Mehrabian, Communication without Words, PsychologyToday, vol. 2, no. 4, pp. 5356, 1968.56 S. Morishima, F. Kawakami, H. Yamada, and H. Harashima, AModelling of Facial Expression and Emotion for Recognition andSynthesis, Symbiosis of Human and Artifact, Y. Anzai, K. Ogawaand H. Mori, eds., pp. 251256, Amsterdam Elsevier Science BV,1995.57 Y. Moses, D. Reynard, and A. Blake, Determining FacialExpressions in Real Time, Proc. Intl Conf. Automatic Face andGestureRecognition, pp. 332337, 1995.58 R. Nakatsu, Toward the Creation of a New Medium for theMultimedia Era, Proc. IEEE, vol. 86, no. 5, pp. 825836, 1998.59 T. Otsuka and J. Ohya, Recognition of Facial Expressions UsingHMM with Continuous Output Probabilities, Proc. Intl WorkshopRobot and Human Comm., pp. 323328, 1996.60 T. Otsuka and J. Ohya, Spotting Segments Displaying FacialExpression from Image Sequences Using HMM, Proc. Intl Conf.Automatic Face and Gesture Recognition, pp. 442447, 1998.61 C. Padgett and G.W. Cottrell, Representing Face Images forEmotion Classification, Proc. Conf. Advances in Neural InformationProcessing Systems, pp. 894900, 1996.62 M. Pantic and L.J.M. Rothkrantz, Expert System for AutomaticAnalysis of Facial Expression, Image and Vision Computing J.,vol. 18, no. 11, pp. 881905, 2000.63 M. Pantic and L.J.M. Rothkrantz, An Expert System for MultipleEmotional Classification of Facial Expressions Proc. Intl Conf.Tools with Artificial Intelligence, pp. 113120, 1999.64 V.I. Pavlovic, R. Sharma, and T.S. Huang, Visual Interpretation ofHand Gestures for HumanComputer Interaction Review, IEEETrans. Pattern Analysis and Machine Intelligence, vol. 19, no. 7,pp. 677695, 1997.65 A. Pentland, B. Moghaddam, and T. Starner, ViewBased andModular Eigenspaces for Face Recognition, Proc. Computer Visionand Pattern Recognition, pp. 8491, 1994.66 V.A. Petrushin, Emotion in Speech Recognition and Applicationto Call Centers, Proc. Conf. Artificial Neural Networks in Eng., 1999.67 R.W. Picard and E. Vyzas, Offline and Online Recognition ofEmotion Expression from Physiological Data, EmotionBasedAgent Architectures Workshop Notes, Intl Conf. Autonomous Agents,pp. 135142, 1999.68 T.S. Polzin and A.H. Waibel, Detecting Emotions in Speech,Proc. Conf. Cooperative Multimedia Comm., 1998.69 W.H. Press, S.A. Teukolsky, W.T. Vetterling, and B.P. Flannery,Numerical Recipes in C, Cambridge Univ. Press, 1992.70 A. Rahardja, A. Sowmya, and W.H. Wilson, A Neural NetworkApproach to Component versus Holistic Recognition of FacialExpressions in Images, SPIE, Intelligent Robots and ComputerVision X Algorithms and Techniques, vol. 1,607, pp. 6270, 1991.71 A. Ralescu and R. Hartani, Some Issues in Fuzzy and LinguisticModeling Proc. Conf. Fuzzy Systems, pp. 1,9031,910, 1995.72 M. Riedmiller and H. Braun, A Direct Adaptive Method forFaster Backpropagation Learning The RPROP Algorithm, Proc.Intl Conf. Neural Networks, pp. 586591, 1993.73 M. Rosenblum, Y. Yacoob, and L. Davis, Human EmotionRecognition from Motion Using a Radial Basis Function NetworkArchitecture, Proc. IEEE Workshop on Motion of NonRigid andArticulated Objects, pp. 4349, 1994.74 H.A. Rowley, S. Baluja, and T. Kanade, Neural NetworkBasedFace Detection, IEEE Trans. Pattern Analysis and MachineIntelligence, vol. 20, no. 1, pp. 2338, Jan. 1998.75 The Psychology of Facial Expression, J.A. Russell and J.M. FernandezDols, eds. Cambridge Cambridge Univ. Press, 1997.76 J.A. Russell, Is There Universal Recognition of Emotion fromFacial Expression Psychological Bulletin, vol. 115, no. 1, pp. 102141, 1994.77 P. Ekman, Strong Evidence for Universals in Facial ExpressionsA Reply to Russells Mistaken Critique, Psychological Bulletin,vol. 115, no. 2, pp. 268287, 1994.1444 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 22, NO. 12, DECEMBER 200078 A. Samal, Minimum Resolution for Human Face Detection andIdentification, SPIE Human Vision, Visual Processing, and DigitalDisplay II, vol. 1,453, pp. 8189, 1991.79 A. Samal and P.A. Iyengar, Automatic Recognition and Analysisof Human Faces and Facial Expressions A Survey, PatternRecognition, vol. 25, no. 1, pp. 6577, 1992.80 E. Simoncelli, Distributed Representation and Analysis of VisualMotion, PhD thesis, Massachusetts Inst. of Technology, 1993.81 J. Steffens, E. Elagin, and H. Neven, PersonSpotterFast andRobust System for Human Detection, Tracking, and Recognition,Proc. Intl Conf. Automatic Face and Gesture Recognition, pp. 516521,1998.82 G.M. Stephenson, K. Ayling, D.R. Rutter, The Role of VisualCommunication in Social Exchange, Britain J. Social ClinicalPsychology, vol. 15, pp. 113120, 1976.83 K.K. Sung and T. Poggio, ExampleBased Learning for ViewBased Human Face Detection IEEE Trans. Pattern Analysis andMachine Intelligence, vol. 20, no. 1, pp. 3951, Jan. 1998.84 A. Takeuchi and K. Nagao, Communicative Facial Displays as aNew Conversational Modality, Proc. ACM INTERCHI, pp. 187193, 1993.85 J.C. Terrillon, M. David, and S. Akamatsu, Automatic Detection ofHuman Faces in Natural Scene Images by Use of a Skin ColorModel of Invariant Moments Proc. Intl Conf. Automatic Face andGesture Recognition, pp. 112117, 1998.86 D. Terzopoulos and K. Waters, Analysis and Synthesis of FacialImage Sequences Using Physical and Anatomical Models, IEEETrans. Pattern Analysis and Machine Intelligence, vol. 15, no. 6,pp. 569579, June 1993.87 N.M. Thalmann and D. Thalmann, 600 Indexed References onComputer Animation, J. Visualisation and Computer Animation,vol. 3, pp. 147174, 1992.88 N.M. Thalmann, P. Kalra, and M. Escher, Face to Virtual Face,Proc. IEEE, vol. 86, no. 5, pp. 870883, 1998.89 N.M. Thalmann, P. Kalra, and I.S. Pandzic, Direct FacetoFaceCommunication between Real and Virtual Humans Intl J.Information Technology, vol. 1, no. 2, pp. 145157, 1995.90 N. Tosa and R. Nakatsu, LifeLike Communication AgentEmotion Sensing Character MIC and Feeling Session CharacterMUSE, Proc. Conf. Multimedia Computing and Systems, pp. 1219,1996.91 M. Turk and A. Pentland, Eigenfaces for Recognition, J. CognitiveNeuroscience, vol. 3, no. 1, pp. 7186, 1991.92 H. Ushida, T. Takagi, and T. Yamaguchi, Recognition of FacialExpressions Using Conceptual Fuzzy Sets Proc. Conf. FuzzySystems, vol. 1, pp. 594599, 1993.93 P. Vanger, R. Honlinger, and H. Haken, Applications ofSynergetics in Decoding Facial Expression of Eemotion, Proc.Intl Conf. Automatic Face and Gesture Recognition, pp. 2429, 1995.94 J.M. Vincent, D.J. Myers, and R.A. Hutchinson, Image FeatureLocation in MultiResolution Images Using a Hierarchy of MultiLayer Preceptors, Neural Networks for Speech, Vision, and NaturalLanguage, pp. 1329, Chapman  Hall, 1992.95 M. Wang, Y. Iwai, and M. Yachida, Expression Recognition fromTimeSequential Facial Images by Use of Expression ChangeModel, Proc. Intl Conf. Automatic Face and Gesture Recognition,pp. 324329, 1998.96 D.J. Williams and M. Shah, A Fast Algorithm for Active Contoursand Curvature Estimation, Computer Vision and Image ProcessingImage Understanding, vol. 55, no. 1, pp. 1426, 1992.97 A.D. Wilson and A.F. Bobick, Parametric Hidden MarkovModels for Gesture Recognition IEEE Trans. Pattern Analysisand Machine Intelligence, vol. 21, no. 9, pp. 884900, Sept. 1999.98 L. Wiskott, Labelled Graphs and Dynamic Link Matching forFace Recognition and Scene Analysis, Reihe Physik, vol. 53,Frankfurt a.m. Main Verlag Harri Deutsch, 1995.99 H. Wu, T. Yokoyama, D. Pramadihanto, and M. Yachida, Faceand Facial Feature Extraction from Color Image, Proc. Intl Conf.Automatic Face and Gesture Recognition, pp. 345350, 1996.100 Y. Yacoob and L. Davis, Recognizing Facial Expressions bySpatioTemporal Analysis, Proc. Intl Conf. Pattern Recognition,vol. 1, pp. 747749, 1994.101 Y. Yacoob and L. Davis, Computing SpatioTemporal Representations of Human Faces, Proc. Computer Vision and PatternRecognition, pp. 7075, 1994.102 H. Yamada, Visual Information for Categorizing Facial Expressions of Emotions, Applied Cognitive Psychology, vol. 7, pp. 257270, 1993.103 J. Yang and A. Waibel, A RealTime Face Tracker, WorkshopApplications of Computer Vision, pp. pp. 142147, 1996.104 M. Yoneyama, Y. Iwano, A. Ohtake, and K. Shirai, FacialExpressions Recognition Using Discrete Hopfield Neural Networks, Proc. Intl Conf. Information Processing, vol. 3, pp. 117120,1997.105 A.L. Yuille, D.S. Cohen, and P.W. Hallinan, Feature Extractionfrom Faces Using Deformable Templates, Proc. Computer Visionand Pattern Recognition, pp. 104109, 1989.106 Z. Zhang, M. Lyons, M. Schuster, and S. Akamatsu, Comparisonbetween GeometryBased and Gabor WaveletsBased FacialExpression Recognition Using MultiLayer Perceptron, Proc. IntlConf. Automatic Face and Gesture Recognition, pp. 454459, 1998.107 J. Zhao and G. Kearney, Classifying Facial Emotions byBackpropagation Neural Networks with Fuzzy Inputs, Proc.Conf. Neural Information Processing, vol. 1, pp. 454457, 1996.Maja Pantic received the MS degree in computer science cum laude from the Department ofComputer Science, Faculty of Technical Mathematics and Computer Science at the DelftUniversity of Technology, the Netherlands, in1997. She is a PhD candidate in computerscience at the Department of Computer Science,Faculty of Information Technology and Systemsat the Delft University of Technology, the Netherlands. Her research interests are in the areas ofmultimedia multimodal manmachine interfacesand artificial intelligence including knowledgebased systems, distributive AI, machine learning, and applications of artificial intelligence inintelligent multimodal user interfaces. She is a student member of theIEEE..Leon Rothkrantz received the MSc degree inmathematics from the University of Utrecht, theNetherlands, in 1971, the PhD degree inmathematics from the University of Amsterdam,the Netherlands, in 1980, and the MSc degree inpsychology from the University of Leiden, theNetherlands, in 1990. As an associate professor,he joined the Knowledge Based Systems groupof the Faculty of Technical Mathematics andComputer Science at the Delft University ofTechnology, the Netherlands, in 1992. The longrange goal of Dr. Rothkrantzs research is design and development ofhumanpsychologybased anthropomorphic multimodal multimediafourth generation manmachine interface. His interests include applyingcomputational technology to analysis of all aspects of human behavior.PANTIC AND ROTHKRANTZ AUTOMATIC ANALYSIS OF FACIAL EXPRESSIONS THE STATE OF THE ART 1445
