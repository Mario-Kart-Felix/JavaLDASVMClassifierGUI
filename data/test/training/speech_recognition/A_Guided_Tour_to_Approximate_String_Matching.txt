A Guided Tour to Approximate String MatchingGONZALO NAVARROUniversity of ChileWe survey the current techniques to cope with the problem of string matching thatallows errors. This is becoming a more and more relevant issue for many fast growingareas such as information retrieval and computational biology. We focus on onlinesearching and mostly on edit distance, explaining the problem and its relevance, itsstatistical behavior, its history and current developments, and the central ideas of thealgorithms and their complexities. We present a number of experiments to compare theperformance of the different algorithms and show which are the best choices. Weconclude with some directions for future work and open problems.Categories and Subject Descriptors F.2.2 Analysis of algorithms and problemcomplexity Nonnumerical algorithms and problemsPattern matching,Computations on discrete structures H.3.3 Information storage and retrievalInformation search and retrievalSearch processGeneral Terms AlgorithmsAdditional Key Words and Phrases Edit distance, Levenshtein distance, online stringmatching, text searching allowing errors1. INTRODUCTIONThis work focuses on the problem of stringmatching that allows errors, also calledapproximate string matching. The generalgoal is to perform string matching of a pattern in a text where one or both of themhave suffered some kind of undesirablecorruption. Some examples are recoveringthe original signals after their transmission over noisy channels, finding DNA subsequences after possible mutations, andtext searching where there are typing orspelling errors.Partially supported by Fondecyt grant 1990627.Authors address Department of Computer Science, University of Chile, Blanco Erncalada 2120, Santiago,Chile, email gnavarrodec.uchile.cl.Permission to make digital or hard copies of part or all of this work for personal or classroom use is grantedwithout fee provided that copies are not made or distributed for profit or direct commercial advantage andthat copies show this notice on the first page or initial screen of a display along with the full citation.Copyrights for components of this work owned by others than ACM must be honored. Abstracting withcredit is permitted. To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use anycomponent of this work in other works, requires prior specific permission andor a fee. Permissions maybe requested from Publications Dept, ACM Inc., 1515 Broadway, New York, NY 10036 USA, fax 1 2128690481, or permissionsacm.org.c2001 ACM 036003000103000031 5.00The problem, in its most general form,is to find a text where a text given pattern occurs, allowing a limited number oferrors in the matches. Each applicationuses a different error model, which defineshow different two strings are. The idea forthis distance between strings is to makeit small when one of the strings is likely tobe an erroneous variant of the other underthe error model in use.The goal of this survey is to presentan overview of the state of the art in approximate string matching. We focus ononline searching that is, when the textACM Computing Surveys, Vol. 33, No. 1, March 2001, pp. 3188.For Evaluation Only.Copyright c by Foxit Software Company, 2004Edited by Foxit PDF Editor32 G. Navarrocannot be preprocessed to build an index on it, explaining the problem and itsrelevance, its statistical behavior, its history and current developments, and thecentral ideas of the algorithms and theircomplexities. We also consider some variants of the problem. We present a number of experiments to compare the performance of the different algorithms andshow the best choices. We conclude withsome directions for future work and openproblems.Unfortunately, the algorithmic nature ofthe problem strongly depends on the typeof errors considered, and the solutionsrange from linear time to NPcomplete.The scope of our subject is so broad that weare forced to narrow our focus on a subsetof the possible error models. We consideronly those defined in terms of replacingsome substrings by others at varying costs.In this light, the problem becomes minimizing the total cost to transform the pattern and its occurrence in text to makethem equal, and reporting the text positions where this cost is low enough.One of the best studied cases of this error model is the socalled edit distance,which allows us to delete, insert and substitute simple characters with a differentone in both strings. If the different operations have different costs or the costs depend on the characters involved, we speakof general edit distance. Otherwise, if allthe operations cost 1, we speak of simpleedit distance or just edit distance ed . Inthis last case we simply seek for the minimum number of insertions, deletions andsubstitutions to make both strings equal.For instance ed survey,surgery 2. The edit distance has received a lotof attention because its generalized version is powerful enough for a wide rangeof applications. Despite the fact thatmost existing algorithms concentrate onthe simple edit distance, many of themcan be easily adapted to the generalizededit distance, and we pay attention tothis issue throughout this work. Moreover, the few algorithms that exist forthe general error model that we consider are generalizations of edit distancealgorithms.On the other hand, most of the algorithms designed for the edit distance areeasily specialized to other cases of interest. For instance, by allowing only insertions and deletions at cost 1, we cancompute the longest common subsequenceLCS between two strings. Another simplification that has received a lot of attention is the variant that allows only substitutions Hamming distance.An extension of the edit distance enriches it with transpositions i.e. a substitution of the form ab  ba at cost 1.Transpositions are very important in textsearching applications because they aretypical typing errors, but few algorithmsexist to handle them. However, many algorithms for edit distance can be easily extended to include transpositions, and wekeep track of this fact in this work.Since the edit distance is by far thebest studied case, this survey focuses basically on the simple edit distance. However, we also pay attention to extensionssuch as generalized edit distance, transpositions and general substring substitution, as well as to simplifications such asLCS and Hamming distance. In addition,we also pay attention to some extensionsof the type of pattern to search when thealgorithms allow it, we mention the possibility of searching some extended patternsand regular expressions allowing errors.We now point out what we are not covering in this work.First, we do not cover other distancefunctions that do not fit the model ofsubstring substitution. This is becausethey are too different from our focus andthe paper would lose cohesion. Someof these are Hamming distance shortsurvey in Navarro 1998, reversalsKececioglu and Sankoff 1995 whichallows reversing substrings, block distance Tichy 1984 Ehrenfeucht andHaussler 1988 Ukkonen 1992 Loprestiand Tomkins 1997 which allows rearranging and permuting the substrings,qgram distance Ukkonen 1992 basedon finding common substrings of fixedlength q, allowing swaps Amir et al.1997b Lee et al. 1997, etc. HammingACM Computing Surveys, Vol. 33, No. 1, March 2001.For Evaluation Only.Copyright c by Foxit Software Company, 2004Edited by Foxit PDF EditorA Guided Tour to Approximate String Matching 33distance, despite being a simplificationof the edit distance, is not covered because specialized algorithms for it existthat go beyond the simplification of anexisting algorithm for edit distance.Second, we consider pattern matchingover sequences of symbols, and at mostgeneralize the pattern to a regular expression. Extensions such as approximate searching in multidimensionaltexts short survey in Navarro andBaezaYates 1999a, in graphs Amiret al. 1997a Navarro 2000a or multipattern approximate searching Muthand Manber 1996 BaezaYates andNavarro 1997 Navarro 1997a BaezaYates and Navarro 1998 are not considered. None of these areas is very developed and the algorithms should beeasy to grasp once approximate patternmatching under the simple model is wellunderstood. Many existing algorithmsfor these problems borrow from those wepresent here.Third, we leave aside nonstandard algorithms, such as approximate,1 probabilistic or parallel algorithms Tarhioand Ukkonen 1988 Karloff 1993Atallah et al. 1993 Altschul et al. 1990Lipton and Lopresti 1985 Landau andVishkin 1989.Finally, an important area that we leaveaside in this survey is indexed searching, i.e. the process of building a persistent data structure an index on thetext to speed up the search later. Typicalreasons that prevent keeping indices onthe text are extra space requirementsas the indices for approximate searching tend to take many times the textsize, volatility of the text as buildingthe indices is quite costly and needs to1 Please do not confuse an approximate algorithmwhich delivers a suboptimal solution with some suboptimality guarantees with an algorithm for approximate string matching. Indeed approximate stringmatching algorithms can be regarded as approximation algorithms for exact string matching wherethe maximum distance gives the guarantee of optimality, but in this case it is harder to find the approximate matches, and of course the motivation isdifferent.be amortized over many searches andsimply inadequacy as the field of indexed approximate string matching isquite immature and the speedup thatthe indices provide is not always satisfactory. Indexed approximate searching is a difficult problem, and the areais quite new and active Jokinen andUkkonen 1991 Gonnet 1992 Ukkonen1993 Myers 1994a Holsti and Sutinen1994 Manber and Wu 1994 Cobbs1995 Sutinen and Tarhio 1996 Araujoet al. 1997 Navarro and BaezaYates1999b BaezaYates and Navarro 2000Navarro et al. 2000. The problem isvery important because the texts insome applications are so large thatno online algorithm can provide adequate performance. However, virtuallyall the indexed algorithms are stronglybased on online algorithms, and therefore understanding and improving thecurrent online solutions is of interestfor indexed approximate searching aswell.These issues have been put aside to keepa reasonable scope in the present work.They certainly deserve separate surveys.Our goal in this survey is to explain thebasic tools of approximate string matching, as many of the extensions we areleaving aside are built on the basic algorithms designed for online approximatestring matching.This work is organized as follows. InSection 2 we present in detail some of themost important application areas for approximate string matching. In Section 3we formally introduce the problem and thebasic concepts necessary to follow the restof the paper. In Section 4 we show someanalytical and empirical results about thestatistical behavior of the problem.Sections 58 cover all the work of interest we could trace on approximate stringmatching under the edit distance. Wedivided it in four sections that correspondto different approaches to the problemdynamic programming, automata, bitparallelism, and filtering algorithms.Each section is presented as a historicaltour, so that we do not only explain theACM Computing Surveys, Vol. 33, No. 1, March 2001.34 G. Navarrowork done but also show how it wasdeveloped.Section 9 presents experimental resultscomparing the most efficient algorithms.Finally, we give our conclusions and discuss open questions and future work inSection 10.There exist other surveys on approximate string matching, which are howevertoo old for this fast moving area Halland Dowling 1980 Sankoff and Kruskal1983 Apostolico and Galil 1985 Galil andGiancarlo 1988 Jokinen et al. 1996 thelast one was in its definitive form in 1991.So all previous surveys lack coverage ofthe latest developments. Our aim is toprovide a long awaited update. This workis partially based in Navarro 1998, butthe coverage of previous work is muchmore detailed here. The subject is alsocovered, albeit with less depth, in sometextbooks on algorithms Crochemore andRytter 1994 BaezaYates and RibeiroNeto 1999.2. MAIN APPLICATION AREASThe first references to this problem wecould trace are from the sixties and seventies, where the problem appeared in anumber of different fields. In those times,the main motivation for this kind of searchcame from computational biology, signalprocessing, and text retrieval. These arestill the largest application areas, and wecover each one here. See also Sankoff andKruskal 1983, which has a lot of information on the birth of this subject.2.1 Computational BiologyDNA and protein sequences can be seenas long texts over specific alphabets e.g.A,C,G,T in DNA. Those sequences represent the genetic code of living beings.Searching specific sequences over thosetexts appeared as a fundamental operation for problems such as assembling theDNA chain from the pieces obtained by theexperiments, looking for given features inDNA chains, or determining how differenttwo genetic sequences are. This was modeled as searching for given patterns ina text. However, exact searching was oflittle use for this application, since the patterns rarely matched the text exactly theexperimental measures have errors of different kinds and even the correct chainsmay have small differences, some of themsignificant due to mutations and evolutionary alterations and others unimportant. Finding DNA chains very similar tothose sought represent significant resultsas well. Moreover, establishing how different two sequences are is important to reconstruct the tree of the evolution phylogenetic trees. All these problems requireda concept of similarity, as well as an algorithm to compute it.This gave a motivation to search allowing errors. The errors were those operations that biologists knew were commonin genetic sequences. The distance between two sequences was defined as theminimum i.e. more likely sequence of operations to transform one into the other.With regard to likelihood, the operationswere assigned a cost, such that the morelikely operations were cheaper. The goalwas then to minimize the total cost.Computational biology has since thenevolved and developed a lot, with a specialpush in recent years due to the genomeprojects that aim at the complete decodingof the DNA and its potential applications.There are other, more exotic, problemssuch as structure matching or searchingfor unknown patterns. Even the simpleproblem where the pattern is known isvery difficult under some distance functions e.g. reversals.Some good references for the applications of approximate pattern matching tocomputational biology are Sellers 1974,Needleman and Wunsch 1970, Sankoffand Kruskal 1983, Altschul et al. 1990,Myers 1991, 1994b, Waterman 1995,Yap et al. 1996, and Gusfield 1997.2.2 Signal ProcessingAnother early motivation came from signal processing. One of the largest areasdeals with speech recognition, where thegeneral problem is to determine, givenan audio signal, a textual message whichis being transmitted. Even simplifiedACM Computing Surveys, Vol. 33, No. 1, March 2001.For Evaluation Only.Copyright c by Foxit Software Company, 2004Edited by Foxit PDF EditorA Guided Tour to Approximate String Matching 35problems such as discerning a word from asmall set of alternatives is complex, sinceparts of the the signal may be compressedin time, parts of the speech may not be pronounced, etc. A perfect match is practicallyimpossible.Another problem is error correction. Thephysical transmission of signals is errorprone. To ensure correct transmission overa physical channel, it is necessary to beable to recover the correct message aftera possible modification error introducedduring the transmission. The probabilityof such errors is obtained from the signal processing theory and used to assigna cost to them. In this case we may noteven know what we are searching for, wejust want a text which is correct according to the error correcting code used andclosest to the received message. Althoughthis area has not developed much withrespect to approximate searching, it hasgenerated the most important measureof similarity, known as the Levenshteindistance Levenshtein 1965 1966 alsocalled edit distance.Signal processing is a very active areatoday. The rapidly evolving field of multimedia databases demands the ability tosearch by content in image, audio andvideo data, which are potential applications for approximate string matching. Weexpect in the next years a lot of pressure on nonwritten humanmachine communication, which involves speech recognition. Strong error correcting codes arealso sought, given the current interest inwireless networks, as the air is a low quality transmission medium.Good references for the relations ofapproximate pattern matching with signal processing are Levenshtein 1965,Vintsyuk 1968, and Dixon and Martin1979.2.3 Text RetrievalThe problem of correcting misspelledwords in written text is rather old, perhaps the oldest potential application forapproximate string matching. We couldfind references from the twenties Masters1927, and perhaps there are older ones.Since the sixties, approximate stringmatching is one of the most popular toolsto deal with this problem. For instance,80 of these errors are corrected allowingjust one insertion, deletion, substitution,or transposition Damerau 1964.There are many areas where this problem appears, and Information RetrievalIR is one of the most demanding. IR isabout finding the relevant information ina large text collection, and string matching is one of its basic tools.However, classical string matching isnormally not enough, because the text collections are becoming larger e.g. the Webtext has surpassed 6 terabytes Lawrenceand Giles 1999, more heterogeneous different languages, for instance, and moreerror prone. Many are so large and growso fast that it is impossible to control theirquality e.g. in the Web. A word which isentered incorrectly in the database cannot be retrieved anymore. Moreover, thepattern itself may have errors, for instance in crosslingual scenarios where aforeign name is incorrectly spelled, or inold texts that use outdated versions of thelanguage.For instance, text collections digitalizedvia optical character recognition OCRcontain a nonnegligible percentage of errors 716. The same happens withtyping 13.2 and spelling 1.52.5errors. Experiments for typing Dutch surnames by the Dutch reached 38 ofspelling errors. All these percentages wereobtained from Kukich 1992. Our own experiments with the name Levenshtein inAltavista gave more than 30 of errors allowing just one deletion or transposition.Nowadays, there is virtually no text retrieval product that does not allow someextended search facility to recover from errors in the text or pattern. Other text processing applications are spelling checkers,natural language interfaces, commandlanguage interfaces, computer aided tutoring and language learning, to name a few.A very recent extension which becamepossible thanks to wordoriented text compression methods is the possibility to perform approximate string matching at theword level Navarro et al. 2000. ThatACM Computing Surveys, Vol. 33, No. 1, March 2001.36 G. Navarrois, the user supplies a phrase to searchand the system searches the text positionswhere the phrase appears with a limitednumber of word insertions, deletions andsubstitutions. It is also possible to disregard the order of the words in the phrases.This allows the query to survive from different wordings of the same idea, whichextends the applications of approximatepattern matching well beyond the recovery of syntactic mistakes.Good references about the relation ofapproximate string matching and information retrieval are Wagner and Fisher1974, Lowrance and Wagner 1975,Nesbit 1986, Owolabi and McGregor1988, Kukich 1992, Zobel and Dart1996, French et al. 1997, and BaezaYates and RibeiroNeto 1999.2.4 Other AreasThe number of applications for approximate string matching grows every day.We have found solutions to the mostdiverse problems based on approximatestring matching, for instance handwritingrecognition Lopresti and Tomkins 1994,virus and intrusion detection Kumarand Spaffors 1994, image compressionLuczak and Szpankowski 1997, datamining Das et al. 1997, pattern recognition Gonzalez and Thomason 1978, optical character recognition Elliman andLancaster 1990, file comparison Heckel1978, and screen updating Gosling1991, to name a few. Many more applications are mentioned in Sankoff andKruskal 1983 and Kukich 1992.3. BASIC CONCEPTSWe present in this section the importantconcepts needed to understand all the development that follows. Basic knowledgeof the design and analysis of algorithmsand data structures, basic text algorithms,and formal languages is assumed. If thisis not the case we refer the reader to goodbooks on these subjects, such as Aho et al.1974, Cormen et al. 1990, Knuth 1973for algorithms, Gonnet and BaezaYates1991, Crochemore and Rytter 1994,Apostolico and Galil 1997 for text algorithms, and Hopcroft and Ullman 1979for formal languages.We start with some formal definitionsrelated to the problem. Then we coversome data structures not widely knownwhich are relevant for this survey theyare also explained in Gonnet and BaezaYates 1991 and Crochemore and Rytter1994. Finally, we make some commentsabout the tour itself.3.1 Approximate String MatchingIn the discussion that follows, we use s,x, y , z, v, w to represent arbitrary strings,and a, b, c, . . . to represent letters. Writinga sequence of strings andor letters represents their concatenation. We assume thatconcepts such as prefix, suffix and substring are known. For any string s  6we denote its length as s. We also denote si the ith character of s, for an integer i  1..s. We denote si.. j  sisi1    sjwhich is the empty string if i j . Theempty string is denoted as .In the Introduction we have defined theproblem of approximate string matchingas that of finding the text positions thatmatch a pattern with up to k errors. Wenow give a more formal definition.Let 6 be a finite2 alphabet of size6   .Let T  6 be a text of length n  T .Let P  6 be a pattern of lengthm  P .Let k  R be the maximum error allowed.Let d  6  6  R be a distancefunction.The problem is given T , P , k and d ,return the set of all the text positionsj such that there exists i such thatd P, Ti.. j   k.2 However, many algorithms can be adapted to infinite alphabets with an extra Olog m factor in theircost. This is because the pattern can have at mostm different letters and all the rest can be considered equal for our purposes. A table of size  couldbe replaced by a search structure over at most m 1different letters.ACM Computing Surveys, Vol. 33, No. 1, March 2001.Find substring Ti,j with  k errors vs pattern PFor Evaluation Only.Copyright c by Foxit Software Company, 2004Edited by Foxit PDF EditorA Guided Tour to Approximate String Matching 37Note that endpoints of occurrences arereported to ensure that the output is of linear size. By reversing all strings we canobtain start points.In this work we restrict our attention toa subset of the possible distance functions.We consider only those defined in the following formThe distance d x, y between two stringsx and y is the minimal cost of a sequenceof operations that transform x into y and if no such sequence exists. The cost ofa sequence of operations is the sum of thecosts of the individual operations. The operations are a finite set of rules of the formz, w  t, where z and w are differentstrings and t is a nonnegative real number. Once the operation has converted asubstring z into w, no further operationscan be done on w.Note especially the restriction that forbids acting many times over the samestring. Freeing the definition from thiscondition would allow any rewriting system to be represented, and thereforedetermining the distance between twostrings would not be computable ingeneral.If for each operation of the formz, w there exists the respective operation w, z at the same cost, then the distance is symmetric i.e. d x, y  d  y , x.Note also that d x, y  0 for all strings xand y , that d x, x  0, and that it alwaysholds d x, z  d x, yd  y , z. Hence,if the distance is symmetric, the space ofstrings forms a metric space.General substring substitution has beenused to correct phonetic errors Zobel andDart 1996. In most applications, however,the set of possible operations is restrictedtoInsertion , a, i.e. inserting the lettera.Deletion a, , i.e. deleting the lettera.Substitution or Replacement a, b fora 6 b, i.e. substituting a by b.Transposition ab, ba for a 6 b, i.e.swap the adjacent letters a and b.We are now in position to define themost commonly used distance functionsalthough there are many others.Levenshtein or edit distanceLevenshtein 1965 allows insertions, deletions and substitutions. Inthe simplified definition, all the operations cost 1. This can be rephrasedas the minimal number of insertions,deletions and substitutions to maketwo strings equal. In the literature thesearch problem in many cases is calledstring matching with k differences.The distance is symmetric, and it holds0  d x, y  maxx,  y .Hamming distance Sankoff andKruskal 1983 allows only substitutions, which cost 1 in the simplifieddefinition. In the literature the searchproblem in many cases is called stringmatching with k mismatches. Thedistance is symmetric, and it is finitewhenever x   y . In this case it holds0  d x, y  x.Episode distance Das et al. 1997allows only insertions, which cost 1.In the literature the search problem inmany cases is called episode matching, since it models the case wherea sequence of events is sought, whereall of them must occur within a shortperiod. This distance is not symmetric,and it may not be possible to convertx into y in this case. Hence, d x, y iseither  y   x or.Longest common subsequence distanceNeedleman and Wunsch 1970 Apostolico and Guerra 1987 allows onlyinsertions and deletions, all costing 1.The name of this distance refers to thefact that it measures the length of thelongest pairing of characters that canbe made between both strings, sothat the pairings respect the orderof the letters. The distance is thenumber of unpaired characters. Thedistance is symmetric, and it holds0  d x, y x   y .In all cases, except the episode distance,one can think that the changes can bemade over x or y . Insertions on x are theACM Computing Surveys, Vol. 33, No. 1, March 2001.t  costcost to transform string z to w  tFor Evaluation Only.Copyright c by Foxit Software Company, 2004Edited by Foxit PDF Editor38 G. Navarrosame as deletions in y and vice versa, andsubstitutions can be made in any of thetwo strings to match the other.This paper is most concerned with thesimple edit distance, which we denoteed . Although transpositions are of interest especially in case of typing errors,there are few algorithms to deal withthem. However, we will consider them atsome point in this work note that a transposition can be simulated with an insertion plus a deletion, but the cost is different. We also point out when the algorithms can be extended to have differentcosts of the operations which is of special interest in computational biology, including the extreme case of not allowingsome operations. This includes the otherdistances mentioned.Note that if the Hamming or edit distance are used, then the problem makessense for 0  k  m, since if we can perform m operations we can make the pattern match at any text position by meansof m substitutions. The case k  0 corresponds to exact string matching and istherefore excluded from this work. Under these distances, we call   km theerror level, which given the above conditions, satisfies 0    1. This value givesan idea of the error ratio allowed in thematch i.e. the fraction of the pattern thatcan be wrong.We finish this section with some notesabout the algorithms we are going to consider. Like string matching, this area issuitable for very theoretical and for verypractical contributions. There exist a number of algorithms with important improvements in their theoretical complexity, butthey are very slow in practice. Of course,for carefully built scenarios say, m 100,000 and k  2 these algorithms couldbe a practical alternative, but these casesdo not appear in applications. Therefore,we now point out the parameters of theproblem that we consider practical, i.e.likely to be of use in some applications, andwhen we later say in practice we meanunder the following assumptions.The pattern length can be as short as 5letters e.g. text retrieval and as longas a few hundred letters e.g. computational biology.The number of errors allowed k satisfiesthat km is a moderately low value. Reasonable values range from 1m to 12.The text length can be as short as a fewthousand letters e.g. computational biology and as long as megabytes or gigabytes e.g. text retrieval.The alphabet size  can be as low as fourletters e.g. DNA and a high as 256 letters e.g. compression applications. It isalso reasonable to think in even largeralphabets e.g. oriental languages orword oriented text compression. The alphabet may or may not be random.3.2 Suffix Trees and Suffix AutomataSuffix trees Weiner 1973 Knuth 1973Apostolico and Galil 1985 are widely useddata structures for text processing Apostolico 1985. Any position i in a string Sautomatically defines a suffix of S, namelySi.. S . In essence, a suffix tree is a triedata structure built over all the suffixesof S. At the leaf nodes the pointers to thesuffixes are stored. Each leaf represents asuffix and each internal node represents aunique substring of S. Every substring ofS can be found by traversing a path fromthe root. Each node representing the substring ax has a suffix link that leads to thenode representing the substring x.To improve space utilization, this trie iscompacted into a Patricia tree Morrison1968. This involves compressing unarypaths. At the nodes that root a compressedpath, an indication of which character toinspect is stored. Once unary paths arenot present the tree has OS nodes instead of the worstcase OS2 of the triesee Figure 1. The structure can be builtin time OS McCreight 1976 Ukkonen1995.A DAWG Deterministic Acyclic WordGraph Crochemore 1986 Blumer et al.1985 built on a string S is a deterministic automaton able to recognize all thesubstrings of S. As each node in the suffix tree corresponds to a substring, theDAWG is no more than the suffix treeACM Computing Surveys, Vol. 33, No. 1, March 2001.A Guided Tour to Approximate String Matching 39Fig. 1. The suffix trie and suffix tree for a sample string. The  is a special marker to denote the end ofthe text. Two suffix links are exemplified in the trie from abra to bra and then to ra. The internalnodes of the suffix tree show the character position to inspect in the string.augmented with failure links for the letters not present in the tree. Since finalnodes are not distinguished, the DAWGis smaller. DAWGs have similar applications to those of suffix trees, and alsoneed O S  space and construction time.Figure 2 illustrates.A suffix automaton on S is an automatonthat recognizes all the suffixes of S. Thenondeterministic version of this automaton has a very regular structure and isshown in Figure 3 the deterministic version can be seen in Figure 2.3.3 The TourSections 58 present a historical touracross the four main approaches to online approximate string matching see Figure 4. In those historical discussions, keepin mind that there may be a long gap between the time when a result is discoveredand when it finally gets published in itsdefinitive form. Some apparent inconsistencies can be explained in this way e.g.algorithms which are finally analyzedbefore they appear. We did our best in thebibliography to trace the earliest versionof the works, although the full referencecorresponds generally to the final version.At the beginning of each of these sections we give a taxonomy to help guide thetour. The taxonomy is an acyclic graphwhere the nodes are the algorithms andthe edges mean that the work lower downcan be seen as an evolution of the work inthe upper position although sometimesthe developments are in fact independent.Finally, we specify some notation regarding time and space complexity. Whenwe say that an algorithm is Ox time werefer to its worst case although sometimeswe say that explicitly. If the cost is average, we say so explicitly. We also sometimes say that the algorithm is Ox cost,meaning time. When we refer to spacecomplexity we say so explicitly. The average case analysis normally assumes arandom text, where each character is selected uniformly and independently fromthe alphabet. The pattern is not normallyassumed to be random.4. THE STATISTICS OF THE PROBLEMA natural question about approximatesearching is what is the probabilityof a match This question is not onlyACM Computing Surveys, Vol. 33, No. 1, March 2001.For Evaluation Only.Copyright c by Foxit Software Company, 2004Edited by Foxit PDF Editor40 G. NavarroFig. 2. The DAWG or the suffix automaton for the sample string. If all the states are final, it is a DAWG.If only the 2nd, 5th and rightmost states are final then it is a suffix automaton.interesting in itself, but also essential forthe average case analysis of many searchalgorithms, as will be seen later. We nowpresent the existing results and an empirical validation. In this section we considerthe edit distance only. Some variants canbe adapted to these results.The effort in analyzing the probabilisticbehavior of the edit distance has not givengood results in general Kurtz and Myers1997. An exact analysis of the probabilityof the occurrence of a fixed pattern allowing k substitution errors i.e. Hammingdistance can be found in Regnier andSzpankowski 1997, although the resultis not easy to average over all the possible patterns. The results we present hereapply to the edit distance model and, although not exact, are easier to use ingeneral.The result of Regnier and Szpankowski1997 holds under the assumption thatthe characters of the text are independently generated with fixed probabilities,i.e. a Bernoulli model. In the rest of thispaper we consider a simpler model, theuniform Bernoulli model, where all thecharacters occur with the same probability 1 . Although this is a gross simplification of the real processes that generatethe texts in most applications, the resultsobtained are quite reliable in practice. Inparticular, all the analyses apply quitewell to biased texts if we replace  by 1p,where p is the probability that two random text characters are equal.Although the problem of the averageedit distance between two strings is closelyrelated to the better studied LCS, thewell known results of Chvatal and Sankoff1975 and Deken 1979 can hardly be applied to this case. It can be shown that theaverage edit distance between two randomstrings of length m tends to a constantfraction of m as m grows, but the fraction is not known. It holds that for anytwo strings of length m, m  lcs  ed 2m lcs, where ed is their edit distanceand lcs is the length of their longest common subsequence. As proved in Chvataland Sankoff 1975, the average LCS is between m and me for large  , andtherefore the average edit distance is between m 1e  and 2m 11 . Forlarge  it is conjectured that the true valueis m 1  1  Sankoff and Mainville1983.For our purposes, bounding the probability of a match allowing errors is moreimportant than the average edit distance.Let f m, k be the probability of a randompattern of length m matching a given textposition with k errors or less under the editdistance i.e. the text position is reportedas the end of a match. In BaezaYatesand Navarro 1999, Navarro 1998, andNavarro and BaezaYates 1999b upperand lower bounds on the maximum errorlevel  for which f m, k is exponentiallydecreasing on m are found. This is important because many algorithms search forpotential matches that have to be verifiedlater, and the cost of such verifications ispolynomial in m, typically Om2. Therefore, if that event occurs with probabilityOm for some   1 then the total costof verifications is Om2m  o1, whichmakes the verification cost negligible.We first show the analytical bounds forf m, k, then give a new result on averageACM Computing Surveys, Vol. 33, No. 1, March 2001.A Guided Tour to Approximate String Matching 41Fig. 3. A nondeterministic suffix automaton to recognize any suffix of abracadabra. Dashed lines represent transitions i.e. they occur without consuming any input.edit distance, and finally present an experimental verification.4.1 An Upper BoundThe upper bound for  comes from theproof that the matching probability isf m, k  Om for 121 1 21e2 1 211where we note that  is 1 for   0 andgrows to 1 as  grows. This matching probability is exponentially decreasing on m aslong as   1, which is equivalent to  1 e O1   1 e2Therefore,  1 e is a conservative condition on the error level which ensures few matches. Therefore, the maximum level  satisfies  1 e .The proof is obtained using a combinatorial model. Based on the observationthat m  k common characters must appear in the same order in two strings thatmatch with k errors, all the possible alternatives to select the matching charactersfrom both strings are enumerated. Thismodel, however, does not take full advantage of the properties of the edit distanceeven if m  k characters match, the distance can be larger than k. For example,in ed abc, bcd   2, i.e. although two characters match, the distance is not 1.4.2 A Lower BoundOn the other hand, the only optimisticbound we know of is based on the consideration that only substitutions are allowedi.e. Hamming distance. This distance issimpler to analyze but its matching probability is much lower. Using a combinatorialmodel again it is shown that the matchingprobability is f m, k  m m12, where 11 1Therefore an upper bound for the maximum  value is   11 , since otherwise it can be proved that f m, k is notexponentially decreasing on m i.e. it ism12.4.3 A New Result on Average Edit DistanceWe can now prove that the average editdistance is larger than m 1  e  forany  recall that the result of Chvatal andSankoff 1975 holds for large  . We definepm, k as the probability that the edit distance between two strings of length m isat most k. Note that pm, k  f m, k because in the latter case we can match withany text suffix of length from mk to mk.Then the average edit distance ismk 0kPred  k mk 0Pred  kmk 01 pm, k  mmk 0pm, kwhich, since pm, k increases with k, islarger thanmK pm, K mK   K 1pm, K for any K of our choice. In particular, for K m 1  e we have thatpm, K   f m, K   Om for  1.Therefore choosing K m 1 e  1yields that the edit distance is at leastACM Computing Surveys, Vol. 33, No. 1, March 2001.42 G. NavarroFig. 4. Taxonomy of the types of solutions for online searching.m 1  e O1, for any  . As wesee later, this proof converts a conjectureabout the average running time of an algorithm Chang and Lampe 1992 into a fact.4.4 Empirical VerificationWe verify the analysis experimentally inthis section this is also taken from BaezaYates and Navarro 1999 and Navarro1998. The experiment consists of generating a large random text n  10 MB andrunning the search of a random pattern onthat text, allowing k  m errors. At eachtext character, we record the minimum allowed error k for which that text positionmatches the pattern. We repeat the experiment with 1,000 random patterns.Finally, we build the cumulative histogram, finding how many text positionshave matched with up to k errors, for eachk value. We consider that k is low enoughup to where the histogram values becomesignificant, that is, as long as few text positions have matched. The threshold is set tonm2, since m2 is the normal cost of verifying a match. However, the selection of thisthreshold is not very important, since thehistogram is extremely concentrated. Forexample, for m in the hundreds, it movesfrom almost zero to almost n in just five orsix increments of k.Figure 5 shows the results for   32. Onthe left we show the histogram we havebuilt, where the matching probabilityundergoes a sharp increase at . On theright we show the  value as m grows. It isclear that  is essentially independent ofm, although it is a bit lower for short patterns. The increase in the left plot at  isso sharp that the right plot would be thesame if we plotted the value of the averageedit distance divided by m.Figure 6 uses a stable m  300 toshow the  value as a function of  . Thecurve  1 1 is included to show itscloseness to the experimental data. Leastsquares give the approximation  1 1.09 , with a relative error smallerthan 1. This shows that the upperbound analysis Eq. 2 matches realitybetter, provided we replace e by 1.09 inthe formulas.Therefore, we have shown that thematching probability has a sharp behavior for low  it is very low, not as low as1m like exact string matching, but stillexponentially decreasing in m, with anexponent base larger than 1 . At some value that we call  it sharply increasesand quickly becomes almost 1. This pointis close to   1 1 in practice.This is why the problem is of interest only up to a given error level, sincefor higher errors almost all text positionsmatch. This is also the reason that somealgorithms have good average behavioronly for low enough error levels. The point  1 1 matches the conjecture ofSankoff and Mainville 1983.ACM Computing Surveys, Vol. 33, No. 1, March 2001.4 approaches to approx. string matchingFor Evaluation Only.Copyright c by Foxit Software Company, 2004Edited by Foxit PDF EditorDPDyn. Progr.A Guided Tour to Approximate String Matching 43Fig. 5. On the left, probability of an approximate match as a function of the error level m 300. On the right, the observed  error level as a function of the pattern length. Both casescorrespond to random text with   32.5. DYNAMIC PROGRAMMING ALGORITHMSWe start our tour with the oldest amongthe four areas, which directly inheritsfrom the earliest work. Most of the theoretical breakthroughs in the worstcase algorithms belong to this category, althoughonly a few of them are really competitive inpractice. The latest practical work in thisarea dates back to 1992, although thereare recent theoretical improvements. Themajor achievements are Okn worstcasealgorithms and Okn averagecase algorithms, as well as other recent theoretical improvements on the worstcase.We start by presenting the first algorithm that solved the problem and thengive a historical tour on the improvementsover the initial solution. Figure 7 helpsguide the tour.5.1 The First AlgorithmWe now present the first algorithm to solvethe problem. It has been rediscoveredmany times in the past, in different areas, e.g. Vintsyuk 1968, Needleman andWunsch 1970, Sankoff 1972, Sellers1974, Wagner and Fisher 1974, andLowrance and Wagner 1975 there aremore in Ullman 1977, Sankoff andKruskal 1983, and Kukich 1992. However, this algorithm computed the edit distance, and it was converted into a searchalgorithm only in 1980 by Sellers Sellers1980. Although the algorithm is not veryefficient, it is among the most flexible onesin adapting to different distance functions.We first show how to compute the editdistance between two strings x and y .Later, we extend that algorithm to searcha pattern in a text allowing errors. Finally,we show how to handle more general distance functions.5.1.1 Computing Edit Distance. The algorithm is based on dynamic programming.Imagine that we need to compute ed x, y.A matrix C0..x,0.. y  is filled, where Ci, j represents the minimum number of operations needed to match x1..i to y1.. j . This iscomputed as followsCi,0  iC0, j  jCi, j  if xi  y j  then Ci1, j1else 1 minCi1, j , Ci, j1, Ci1, j1where at the end Cx, y   ed x, y. The rationale for the above formula is as follows.First, Ci,0 and C0, j represent the edit distance between a string of length i or j andthe empty string. Clearly i respectivelyj  deletions are needed on the nonemptystring. For two nonempty strings of lengthi and j , we assume inductively that allthe edit distances between shorter stringshave already been computed, and try toconvert x1..i into y1.. j .Consider the last characters xi and y j .If they are equal, then we do not need toACM Computing Surveys, Vol. 33, No. 1, March 2001.Text searching pp 46delete i characters to get 0 length stringinsert j charactersFor Evaluation Only.Copyright c by Foxit Software Company, 2004Edited by Foxit PDF Editor44 G. NavarroFig. 6. Theoretical and practical values for , for m  300 and different  values.consider them and we proceed in the bestpossible way to convert x1..i1 into y1.. j1.On the other hand, if they are not equal,we must deal with them in some way. Following the three allowed operations, wecan delete xi and convert in the best wayx1..i1 into y1.. j , insert y j at the end ofx1..i and convert in the best way x1..i intoy1.. j1, or substitute xi by y j and convertin the best way x1..i1 into y1.. j1. In allcases, the cost is 1 plus the cost for the restof the process already computed. Noticethat the insertions in one string are equivalent to deletions in the other.An equivalent formula which is alsowidely used isCi, j  minCi1, j1  xi, y j ,Ci1, j  1, Ci, j1  1where a, b  0 if a  b and 1 otherwise.It is easy to see that both formulas areequivalent because neighboring cells differ in at most one just recall the meaningof Ci, j , and therefore when xi, y j   0we have that Ci1, j1 cannot be largerthan Ci1, j  1 or Ci, j1  1.The dynamic programming algorithmmust fill the matrix in such a way thatthe upper, left, and upperleft neighborsof a cell are computed prior to computingthat cell. This is easily achieved by eithera rowwise lefttoright traversal or acolumnwise toptobottom traversal, butwe will see later that, using a differencerecurrence, the matrix can also be filledby upperleft to lowerright diagonals orsecondary upperright to lowerleft diagonals. Figure 8 illustrates the algorithmto compute ed survey, surgery.Therefore, the algorithm is Ox y time in the worst and average case.However, the space required is onlyOminx,  y . This is because, in thecase of a columnwise processing, only theprevious column must be stored in orderto compute the new one, and thereforewe just keep one column and update it.We can process the matrix rowwise orcolumnwise so that the space requirement is minimized.On the other hand, the sequences of operations performed to transform x into ycan be easily recovered from the matrix,simply by proceeding from the cell Cx, y to the cell C0,0 following the path i.e. sequence of operations that matches the update formula multiple paths may exist.In this case, however, we need to store thecomplete matrix or at least an area aroundthe main diagonal.This matrix has some properties thatcan be easily proved by induction see,e.g. Ukkonen 1985a and which make itACM Computing Surveys, Vol. 33, No. 1, March 2001.For Evaluation Only.Copyright c by Foxit Software Company, 2004Edited by Foxit PDF EditorA Guided Tour to Approximate String Matching 45Fig. 7. Taxonomy of algorithms based on the dynamic programming matrix. References areshortened to first letters single authors or initials multiple authors, and to the last two digitsof years.Key Vin68  Vintsyuk 1968, NW70  Needleman and Wunsch 1970, San72  Sankoff 1972,Sel74 Sellers 1974, WF74 Wagner and Fisher 1974, LW75 Lowrance and Wagner 1975,Sel80  Sellers 1980, MP80  Masek and Paterson 1980, Ukk85a  Ukk85b  Ukkonen1985a 1985b, Mye86a  Mye86b  Myers 1986a 1986b, LV88  LV89  Landau and Vishkin1988 1989, GP90  Galil and Park 1990, UW93  Ukkonen and Wood 1993, GG88  Galiland Giancarlo 1988, CL92 Chang and Lampe 1992, CL94 Chang and Lawler 1994, SV97Sahinalp and Vishkin 1997, CH98  Cole and Hariharan 1998, and BYN99  BaezaYatesand Navarro 1999.ACM Computing Surveys, Vol. 33, No. 1, March 2001.46 G. NavarroFig. 8. The dynamic programming algorithm tocompute the edit distance between survey andsurgery. The bold entries show the path to thefinal result.possible to design better algorithms. Someof the most useful are that the values ofneighboring cells differ in at most one,and that upperleft to lowerright diagonals are nondecreasing.5.1.2 Text Searching. We now show how toadapt this algorithm to search a short pattern P in a long text T . The algorithmis basically the same, with x P andy T proceeding columnwise so thatOm space is required. The only difference is that we must allow that any textposition is the potential start of a match.This is achieved by setting C0, j  0 forall j  0..n. That is, the empty patternmatches with zero errors at any text position because it matches with a text substring of length zero.The algorithm then initializes its column C0..m with the values Ci  i, and processes the text character by character. Ateach new text character Tj , its column vector is updated to C0..m. The update formula isCi  if Pi  Tj  then Ci1else 1minCi1, Ci, Ci1and the text positions are where Cm  k isreported.The search time of this algorithm isOmn and its space requirement is Om.This is a sort of worst case in the analysis of all the algorithms that we considerlater. Figure 9 exemplifies this algorithmapplied to search the pattern survey inthe text surgery a very short text inFig. 9. The dynamic programming algorithm tosearch survey in the text surgery with two errors. Each column of this matrix is a value of theC vector. Bold entries indicate matching text positions.deed with at most k  2 errors. In thiscase there are 3 occurrences.5.1.3 Other Distance Functions. It is easyto adapt this algorithm for the otherdistance functions mentioned. If the operations have different costs, we addthe cost instead of adding 1 when computing Ci, j , i.e.C0,0  0Ci, j  minCi1, j1  xi , y j ,Ci1, j  xi, , Ci, j1  , y j where we assume a, a  0 for any a  6and that C1, j  Ci,1   for all i, j .For distances that do not allow some operations, we just take them out of the minimization formula, or, which is the same,we assign to their  cost. For transpositions, we allow a fourth rule that says thatCi, j can be Ci2, j2  1 if xi1xi  y j y j1Lowrance and Wagner 1975.The most complex case is to allow general substring substitutions, in the form ofa finite set R of rules. The formula is givenin Ukkonen 1985a.C0,0  0Ci, j  minCi1, j1 if xi  y j ,Cis1, js2  s1, s2for each s1, s2  R, x1..i  x s1,y1.. j  y s2An interesting problem is how to compute this recurrence efficiently. A naiveapproach takes ORmn, where R is thesum of all the lengths of the strings inACM Computing Surveys, Vol. 33, No. 1, March 2001.22 means same characterdelete char0  1 replace characterFor Evaluation Only.Copyright c by Foxit Software Company, 2004Edited by Foxit PDF EditorI downloaded Sellers paperI think its hard to apply Sellers alg. on genes...n is too big too much storage space neededA Guided Tour to Approximate String Matching 47Fig. 10. The Masek and Paterson algorithm partitions the dynamic programming matrix in cellsr  2 in this example. On the right, we shaded the entries of adjacent cells that influence thecurrent one.R. A better solution is to build two AhoCorasick automata Aho and Corasick1975 with the left and right hand sidesof the rules, respectively. The automataare run as we advance in both strings lefthand sides in x and right hand sides iny. For each pair of states i1, i2 of the automata we precompute the set of substitutions that can be tried i.e. those s whoseleft and right hand sides match the suffixes of x and y , respectively, representedby the automata states. Hence, we knowin constant time per cell the set of possible substitutions. The complexity is nowmuch lower, in the worst case it is Ocmnwhere c is the maximum number of rulesapplicable to a single text position.As mentioned, the dynamic programming approach is unbeaten in flexibility,but its time requirements are indeed high.A number of improved solutions have beenproposed over the years. Some of themwork only for the edit distance, while others can still be adapted to other distancefunctions. Before considering the improvements, we mention that there exists a wayto see the problem as a shortest path problem on a graph built on the pattern andthe text Ukkonen 1985a. This reformulation has been conceptually useful for morecomplex variants of the problem.5.2 Improving the Worst Case5.2.1 Masek and Paterson 1980. It is interesting that one important worstcasetheoretical result in this area is as oldas the Sellers algorithm Sellers 1980 itself. In 1980, Masek and Paterson 1980found an algorithm whose worst case costis Omn log2 n and requires On extraspace. This is an improvement over theOmn classical complexity.The algorithm is based on the FourRussians technique Arlazarov et al.1975. Basically, it replaces the alphabet6 by rtuples i.e. 6r  for a small r. Considered algorithmically, it first builds a table of solutions of all the possible problemsi.e. portions of the matrix of size r  r,and then uses the table to solve the original problem in blocks of size r. Figure 10illustrates.The values inside the r  r size cells depend on the corresponding letters in thepattern and the text, which gives  2r possibilities. They also depend on the valuesin the last column and row of the upperand left cells, as well as the bottomrightstate of the upper left cell see Figure 10.Since neighboring cells differ in at mostone, there are only three choices for adjacent cells once the current cell is known.Therefore, this adds only m 32r  possibilities. In total, there are m 3 2r differentcells to precompute. Using On memorywe have enough space for r  log3 n, andsince we finally compute mnr2 cells, thefinal complexity follows.The algorithm is only of theoretical interest, since as the same authors estimate,it will not beat the classical algorithm forACM Computing Surveys, Vol. 33, No. 1, March 2001.For Evaluation Only.Copyright c by Foxit Software Company, 2004Edited by Foxit PDF Editor48 G. NavarroFig. 11. On the left, the Ok2 algorithm to compute the edit distance. Onthe right, the way to compute the strokes in diagonal transition algorithms.The solid bold line is guaranteed to be part of the new stroke of e errors,while the dashed part continues as long as both strings match.texts below 40 GB size and it would needthat extra space. Adapting it to otherdistance functions does not seem difficult,but the dependencies among differentcells may become more complex.5.2.2 Ukkonen 1983. In 1983, Ukkonen1985a presented an algorithm able tocompute the edit distance between twostrings x and y in Oed x, y2 time, orto check in time Ok2 whether that distance was k or not. This is the firstmember of what has been called diagonal transition algorithms, since it is basedon the fact that the diagonals of the dynamic programming matrix running fromthe upperleft to the lowerright cells aremonotonically increasing more than that,Ci1, j1  Ci, j , Ci, j 1. The algorithm isbased on computing in constant time thepositions where the values along the diagonals are incremented. Only Ok2 suchpositions are computed to reach the lowerright decisive cell.Figure 11 illustrates the idea. Each diagonal stroke represents a number of errors, and is a sequence where both stringsmatch. When a stroke of e errors starts, itcontinues until the adjacent strokes of e1errors continue or until it keeps matchingthe text. To compute each stroke in constant time we need to know at what pointit matches the text. The way to do this inconstant time is explained shortly.5.2.3 Landau and Vishkin 1985. In 1985and 1986, Landau and Vishkin found thefirst worstcase time improvements for thesearch problem. All of them and the threadthat followed were diagonal transition algorithms. In 1985, Landau and Vishkin1988 showed an algorithm which wasOk2n time and Om space, and in 1986they obtained Okn time and On spaceLandau and Vishkin 1989.The main idea in Landau and Vishkinwas to adapt the Ukkonens diagonaltransition algorithm for edit distanceUkkonen 1985a to text searching. Basically, the dynamic programming matrixwas computed diagonalwise i.e. strokeby stroke instead of columnwise. Theywanted to compute the length of eachstroke in constant time i.e. the pointwhere the values along a diagonal wereto be incremented. Since a text positionwas to be reported when matrix row m wasreached before incrementing more than ktimes the values along the diagonal, thisimmediately gave the Okn algorithm.Another way to see it is that each diagonal is abandoned as soon as the kth strokeends, there are n diagonals and hence nkstrokes, each of them computed in constant time recall Figure 11.A recurrence on diagonals d  and number of errors e, instead of rows i andcolumns  j , is set up in the followingwayACM Computing Surveys, Vol. 33, No. 1, March 2001.For Evaluation Only.Copyright c by Foxit Software Company, 2004Edited by Foxit PDF EditorA Guided Tour to Approximate String Matching 49Fig. 12. The diagonal transition matrix to searchsurvey in the text surgery with two errors. Boldentries indicate matching diagonals. The rows are evalues and the columns are the d values.Ld ,1  Ln1,e  1, for all e, dLd ,d 2  d   2, for k  1  d  1Ld ,d 1  d   1, for k  1  d  1Ld ,e  i maxPi1..iTdi1..diwhere i  maxLd ,e1  1,Ld1,e1, Ld1,e1  1where the external loop updates e from0 to k and the internal one updates dfrom e to n. Negative numbered diagonals are those virtually starting beforethe first text position. Figure 12 shows oursearch example using this recurrence.Note that the L matrix has to be filledby diagonals, e.g. L0,3, L1,2, L2,1, L0,4, L1,3,L2,2, L0,5, . . . . The difficult part is how tocompute the strokes in constant time i.e.the max. The problem is equivalentto knowing which is the longest prefix ofPi..m that matches Tj ..n. This data is calledmatching statistics. The algorithms ofthis section differ basically in how theymanage to quickly compute the matchingstatistics.We defer the explanation of Landau andVishkin 1988 for later together withGalil and Park 1990. In Landau andVishkin 1989, the longest match is obtained by building the suffix tree seeSection 3.2 of T  P text concatenatedwith pattern, where the huge On extra space comes from. The longest prefixcommon to both suffixes Pi..m and Tj ..n canbe visualized in the suffix tree as followsimagine the root to leaf paths that end ineach of the two suffixes. Both parts sharethe beginning of the path at least theyshare the root. The last suffix tree nodecommon to both paths represents a substring which is precisely the longest common prefix. In the literature, this last common node is called lowest common ancestorLCA of two nodes.Despite being conceptually clear, it isnot easy to find this node in constant time.In 1986, the only existing LCA algorithmwas that of Harel and Tarjan 1984, whichhad constant amortized time, i.e. it answered nn LCA queries in On time.In our case we have kn queries, so eachone finally cost O1. The resulting algorithm, however, is quite slow in practice.5.2.4 Myers 1986. In 1986, Myers alsofound an algorithm with Okn worstcasebehavior Myers 1986a, 1986b. It neededOn extra space, and shared the idea ofcomputing the k new strokes using theprevious ones, as well as the use of asuffix tree on the text for the LCA algorithm. Unlike other algorithms, this oneis able to report the Okn matching substrings of the text not only the endpointsin Okn time. This makes the algorithmsuitable for more complex applications,for instance in computational biology. Theoriginal reference is a technical report andnever went to press, but it has recentlybeen included in a larger work Landauet al. 1998.5.2.5 Galil and Giancarlo 1988. In 1988,Galil and Giancarlo 1988 obtained thesame time complexity as Landau andVishkin using Om space. Basically, thesuffix tree of the text is built by overlapping pieces of size Om. The algorithm scans the text four times, being evenslower than Landau and Vishkin 1989.Therefore, the result was of theoretical interest.5.2.6 Galil and Park 1989. One year later,in 1989, Galil and Park 1990 obtainedOkn worstcase time and Om2 space,worse in theory than Galil and Giancarlo1988 but much better in practice. Theiridea is rooted in the work of Landauand Vishkin 1988 which had obtainedOk2n time. In both cases, the idea is tobuild the matching statistics of the pattern against itself longest match betweenPi..m and Pj ..m, resembling in some sensethe basic ideas of Knuth et al. 1977. Butthis algorithm is still slow in practice.ACM Computing Surveys, Vol. 33, No. 1, March 2001.For Evaluation Only.Copyright c by Foxit Software Company, 2004Edited by Foxit PDF Editor50 G. NavarroFig. 13. On the left, the progress of the strokewise algorithm. The relevantstrokes are enclosed in a dotted triangle and the last k strokes computed are inbold. On the right, the selection of the k relevant strokes to cover the last textarea. We put in bold the parts of the strokes that are used.Consider again Figure 11, and in particular the new stroke with e errors at theright. The beginning of the stroke is dictated by the three neighboring strokes ofe 1 errors, but after the longest of thethree ceases to affect the new stroke, howlong it continues dashed line dependsonly on the similarity between pattern andtext. More specifically, if the dotted linesuffix of a stroke at diagonal d spansrows i1 to i1  , the longest match between Tdi1.. and Pi1.. has length . Therefore, the strokes computed by the algorithm give some information about longestmatches between text and pattern. Thedifficult part is how to use that information.Figure 13 illustrates the algorithm. Asexplained, the algorithm progresses bystrokes, filling the matrix of Figure 12 diagonally, so that when a stroke is computed, its three neighbors are alreadycomputed. We have enclosed in a dotted triangle the strokes that may containthe information on longest matches relevant to the new strokes that are beingcomputed. The algorithm of Landau andVishkin 1988 basically searches the relevant information in this triangle andhence it is in Ok2n time.This is improved in Galil and Park1990 to Okn by considering carefullythe relevant strokes. Let us call estrokea stroke with e errors. First consider a0stroke. This full stroke not only a suffix represents a longest match betweenpattern and text. So, from the k previous0strokes we can keep the one that lastslonger in the text, and up to that text position we have all the information we needabout longest matches. We consider nowall the 1strokes. Although only a suffix ofthose strokes really represents a longestmatch between pattern and text, we knowthat this is definitely true after the lasttext position is reached by a 0stroke sinceby then no 0stroke can help a 1stroketo last longer. Therefore, we can keep the1stroke that lasts longer in the text anduse it to define longest matches betweenpattern and text when there are no moreactive 0strokes. This argument continuesfor all the k errors, showing that in factthe complete text area that is relevant canbe covered with just k strokes. Figure 13right illustrates this idea.The algorithm of Galil and Park 1990basically keeps this list of k relevantstrokes3 up to date all the time. Each timea new estroke is produced, it is comparedagainst the current relevant estroke, andif the new one lasts longer in the text thanthe old one, it replaces the old stroke. Sincethe algorithm progresses in the text, oldstrokes are naturally eliminated with thisprocedure.A final problem is how to use the indirect information given by the relevantstrokes to compute the longest matches3 Called reference triples there.ACM Computing Surveys, Vol. 33, No. 1, March 2001.A Guided Tour to Approximate String Matching 51between pattern and text. What we haveis a set of longest matches covering thetext area of interest, plus the precomputedlongest matches of the pattern against itself starting at any position. We nowknow where the dashed line of Figure 11starts say it is Pi1 and Tdi1  and wantto compute its length. To know wherethe longest match between pattern andtext ends, we find the relevant strokewhere the beginning of the dashed linefalls. That stroke represents a maximalmatch between Tdi1.. and some Pj1... Aswe know by preprocessing the longestmatch between Pi1.. and Pj1.., we can derive the longest match between Pi1.. andTdi1... There are some extra complications to take care of when both longestmatches end at the same position or onehas length zero, but all them can be sortedout in Ok time per diagonal of theL matrix.Finally, Galil and Park show that theOm2 extra space needed to store the matrix of longest matches can be reduced toOm by using a suffix tree of the patternnot the text as in previous work and LCAalgorithms, so we add different entries inFigure 7 note that Landau and Vishkin1988 already had Om space. Galil andPark also show how to add transpositionsto the edit operations at the same complexity. This technique can be extended toall these diagonal transition algorithms.We believe that allowing different integralcosts for the operations or forbidding someof them can be achieved with simple modifications of the algorithms.5.2.7 Ukkonen and Wood 1990. An ideasimilar to that of using the suffix tree ofthe pattern and similarly slow in practicewas independently discovered by Ukkonen and Wood in 1990 Ukkonen and Wood1993. They use a suffix automaton described in Section 3.2 on the pattern tofind the matching statistics, instead of thetable. As the algorithm progresses overthe text, the suffix automaton keeps countof the pattern substrings that match thetext at any moment. Although they reportOm2 space for the suffix automaton, itcan take Om space.5.2.8 Chang and Lawler 1994. In 1990,Chang and Lawler 1994 repeated theidea that was briefly mentioned in Galiland Park 1990 that matching statisticscan be computed using the suffix tree ofthe pattern and LCA algorithms. However,they used a newer and faster LCA algorithm Schieber and Vishkin 1988, trulyO1, and reported the best time amongalgorithms with guaranteed Okn performance. However, the algorithm is still notcompetitive in practice.5.2.9 Cole and Hariharan 1998. In 1998,Cole and Hariharan 1998 presented analgorithm with worst case On1kcm,where c  3 if the pattern is mostly aperiodic and c  4 otherwise.4 The idea isthat, unless a pattern has a lot of selfrepetition, only a few diagonals of a diagonal transition algorithm need to be computed.This algorithm can be thought of as a filter see the following sections with worstcase guarantees useful for very small k. Itresembles some ideas about filters developed in Chang and Lawler 1994. Probably other filters can be proved to have goodworst cases under some periodicity assumptions on the pattern, but this threadhas not been explored up to now. This algorithm is an improvement over a previousone Sahinalp and Vishkin 1997, which ismore complex and has a worse complexity,namely Onk8 log n1 log 3. In any case,the interest of this work is theoretical too.5.3 Improving the Average Case5.3.1 Ukkonen 1985. The first improvement to the average case is due to Ukkonen in 1985. The algorithm, a shortnote at the end of Ukkonen 1985b, improved the dynamic programming algorithm to Okn average time and Omspace. This algorithm was later called thecutoff heuristic. The main idea is that,since a pattern does not normally matchin the text, the values at each column4 The definition of mostly aperiodic is rather technical and related to the number of autorepetitionsthat occur in the pattern. Most patterns are mostlyaperiodic.ACM Computing Surveys, Vol. 33, No. 1, March 2001.52 G. Navarrofrom top to bottom quickly reach k  1i.e. mismatch, and that if a cell has avalue larger than k  1, the result ofthe search does not depend on its exactvalue. A cell is called active if its value isat most k. The algorithm simply keepscount of the last active cell and avoidsworking on the rest of the cells.To keep the last active cell, we mustbe able to recompute it for each new column. At each new column, the last active cell can be incremented in at mostone, so we check if we have activated thenext cell at O1 cost. However, it is alsopossible that the last active cell now becomes inactive. In this case we have tosearch upwards for the new last active cell.Although we can work Om in a givencolumn, we cannot work more than Onoverall, because there are at most n increments of this value in the whole process,and hence there are no more than n decrements. Hence, the last active cell is maintained at O1 amortized cost per column.Ukkonen conjectured that this algorithm was Okn on average, but this wasproven only in 1992 by Chang and Lampe1992. The proof was refined in 1996 byBaezaYates and Navarro 1999. The result can probably be extended to morecomplex distance functions, although withsubstrings the last active cell must exceedk by enough to ensure that it can never return to a value smaller than k. In particular, it must have the value k 2 if transpositions are allowed.5.3.2 Myers 1986. An algorithm inMyers 1986a is based on diagonal transitions like those in the previous sections,but the strokes are simply computed bybrute force. Myers showed that the resulting algorithm was Okn on average. Thisis clear because the length of the strokesis   1O1 on average. The samealgorithm was proposed again in 1989 byGalil and Park 1990. Since only the kstrokes need to be stored, the space isOk.5.3.3 Chang and Lampe 1992. In 1992,Chang and Lampe 1992 produced a newalgorithm called column partitioning,based on exploiting a different propertyof the dynamic programming matrix.They again consider the fact that, alongeach column, the numbers are normallyincreasing. They work on runs of consecutive increasing cells a run ends whenCi1 6 Ci  1. They manage to workO1 per run in the column actualizationprocess.To update each run in constant time,they precompute loc j , x  min j  j Pj  x for all pattern positions j and all characters x hence it needs Om  space. Ateach column of the matrix, they considerthe current text character x and the current row j , and know in constant timewhere the run is going to end i.e. nextcharacter match. The run can end beforethis, namely where the parallel run of theprevious column ends.Based on empirical observations, theyconjecture that the average length of theruns is O . Notice that this matchesour result that the average edit distanceis m 1e , since this is the number ofincrements along columns, and thereforethere are Om  nonincrements i.e.runs. From there it is clear that eachrun has average length O . Therefore,we have just proved Chang and Lampesconjecture.Since the paper uses the cutoff heuristic of Ukkonen, their average search timeis Okn . This is, in practice, thefastest algorithm of this class.Unlike the other algorithms in thissection, it seems difficult to adapt Changand Lampe 1992 to other distance functions, since their idea relies strongly onthe unitary costs. It is mentioned thatthe algorithm could run in average timeOkn log logm  but it would not bepractical.6. ALGORITHMS BASED ON AUTOMATAThis area is also rather old. It is interesting because it gives the best worstcasetime algorithm On, which matches thelower bound of the problem. However,there is a time and space exponentialdependence on m and k that limits itspracticality.ACM Computing Surveys, Vol. 33, No. 1, March 2001.A Guided Tour to Approximate String Matching 53Fig. 14. Taxonomy of algorithms based on deterministic automata. References are shortenedto first letters single authors or initials multiple authors, and to the last two digits of years.Key Ukk85b  Ukkonen 1985b, MP80  Masek and Paterson 1980, Mel96  Melichar 1996,Kur96  Kurtz 1996, Nav97b  Navarro 1997b, and WMM96  Wu et al. 1996.We first present the basic solution andthen discuss the improvements. Figure 14shows the historical map of this area.6.1 An Automaton for Approximate SearchAn alternative and very useful way to consider the problem is to model the searchwith a nondeterministic automatonNFA. This automaton in its deterministic form was first proposed in Ukkonen1985b, and first used in nondeterministic form although implicitly in Wu andManber 1992b. It is shown explicitly inBaezaYates 1991, BaezaYates 1996,and BaezaYates and Navarro 1999.Consider the NFA for k  2 errorsunder the edit distance shown in Figure 15. Every row denotes the number oferrors seen the first row zero, the secondrow one, etc.. Every column representsmatching a pattern prefix. Horizontalarrows represent matching a characteri.e. if the pattern and text charactersmatch, we advance in the pattern and inthe text. All the others increment thenumber of errors move to the next rowvertical arrows insert a character in thepattern we advance in the text but notin the pattern, solid diagonal arrowssubstitute a character we advance in thetext and pattern, and dashed diagonalarrows delete a character of the patternthey are transitions, since we advancein the pattern without advancing inthe text. The initial selfloop allows amatch to start anywhere in the text. Theautomaton signals the end of a matchwhenever a rightmost state is active. Ifwe do not care about the number of errorsin the occurrences, we can consider finalstates those of the last full diagonal.It is not hard to see that once a statein the automaton is active, all the statesof the same column and higher numbered rows are active too. Moreover, ata given text character, if we collect thesmallest active rows at each column,we obtain the vertical vector of thedynamic programming algorithm inthis case 0, 1, 2, 3, 3, 3, 2 compare toFigure 9.Other types of distances Hamming,LCS, and Episode are obtained bydeleting some arrows of the automaton.Different integer costs for the operationscan also be modeled by changing thearrows. For instance, if insertions cost2 instead of 1, we make the verticalarrows move from rows i to rows i 2.Transpositions are modeled by adding anextra state Si, j between each pair of statesACM Computing Surveys, Vol. 33, No. 1, March 2001.54 G. NavarroFig. 15. An NFA for approximate string matching of the pattern survey with two errors. Theshaded states are those active after reading the text surgery.at position i, j  and i 1, j  2, andarrows labeled Pi 2 from state i, j  to Si, jand Pi 1 between Si, j and i 1, j  2Melichar 1996. Adapting to general substring substitution needs more complexsetups but it is always possible.This automaton can simply be madedeterministic to obtain On worstcasesearch time. However, as we see next, themain problem becomes the constructionof the DFA deterministic finite automaton. An alternative solution is based onsimulating the NFA instead of making itdeterministic.6.2 Implementing the Automaton6.2.1 Ukkonen 1985. In 1985, Ukkonenproposed the idea of a deterministicautomaton for this problem Ukkonen1985b. However, an automaton likethat of Figure 15 was not explicitlyconsidered. Rather, each possible set ofvalues for the columns of the dynamicprogramming matrix is a state of theautomaton. Once the set of all possiblecolumns and the transitions among themwere built, the text was scanned with theresulting automaton, performing exactlyone transition per character read.The big problem with this scheme wasthat the automaton had a potentiallyhuge number of states, which had to bebuilt and stored. To improve space usage,Ukkonen proved that all the elementsin the columns that were larger thank 1 could be replaced by k 1 withoutaffecting the output of the search thelemma was used in the same paper todesign the cutoff heuristic described inSection 5.3. This reduced the potentialnumber of different columns. He alsoshowed that adjacent cells in a columndiffered in at most one. Hence, the columnstates could be defined as a vector of mincremental values in the set 1, 0, 1.All this made it possible in Ukkonen1985b to obtain a nontrivial bound onthe number of states of the automaton,namely Omin3m, m2m k. This size,although much better than the obviousOk 1m, is still very large except forshort patterns or very low error levels.The resulting space complexity of thealgorithm is m times the above value.This exponential space complexity hasto be added to the On time complexity,as the preprocessing time to build theautomaton.As a final comment, Ukkonen suggestedthat the columns could be computed onlypartially up to, say, 3k2 entries. Sincehe conjectured and later was provedcorrect in Chang and Lampe 1992 thatthe columns of interest were Ok onaverage, this would normally not affectthe algorithm, though it will reduce thenumber of possible states. If at somepoint the states not computed were reallyACM Computing Surveys, Vol. 33, No. 1, March 2001.A Guided Tour to Approximate String Matching 55Fig. 16. On the left, the automaton of Ukkonen 1985b where each column isa state. On the right, the automaton of Wu et al. 1996 where each region is astate. Both compute the columns of the dynamic programming matrix.needed, the algorithm would computethem by dynamic programming.Notice that to incorporate transpositions and substring substitutions intothis conception we need to consider thateach state is the set of the j last columnsof the dynamic programming matrix,where j is the longest lefthand side ofa rule. In this case it is better to buildthe automaton of Figure 15 explicitly andmake it deterministic.6.2.2 Wu, Manber and Myers 1992. It wasnot until 1992 that Wu et al. looked intothis problem again Wu et al. 1996. Theidea was to trade time for space usinga Four Russians technique Arlazarovet al. 1975. Since the cells could be expressed using only values in 1, 0, 1, thecolumns were partitioned into blocks of rcells called regions which took 2r bitseach. Instead of precomputing the transitions from a whole column to the next,the transitions from a region to the nextregion in the column were precomputed,although the current region could nowdepend on three previous regions seeFigure 16. Since the regions were smallerthan the columns, much less space wasnecessary. The total amount of workwas Omr per column in the worstcase, and Okr on average. The spacerequirement was exponential in r. Byusing On extra space, the algorithm wasOkn log n on average and Omn log nin the worst case. Notice that this sharesthe Four Russians approach with Masekand Paterson 1980, but there is an important difference the states in this casedo not depend on the letters of the patternand text. The states of the automaton ofMasek and Paterson 1980, on the otherhand, depend on the text and pattern.This Four Russians approach is so flexible that this work was extended to handleregular expressions allowing errors Wuet al. 1995. The technique for exact regular expression searching is to pack portions of the deterministic automaton inbits and compute transition tables foreach portion. The few transitions amongportions are left nondeterministic andsimulated one by one. To allow errors,each state is no longer active or inactive,but they keep count of the minimumnumber of errors that makes it active, inOlog k bits.6.2.3 Melichar 1995. In 1995, Melichar1996 again studied the size of the deterministic automaton. By consideringthe properties of the NFA of Figure 15,he refined the bound of Ukkonen 1985bto Omin3m, m2mtk , k 2mkk 1,where t minm 1,  . The space complexity and preprocessing time of theautomaton is t times the number ofstates. Melichar also conjectured that thisautomaton is bigger when there are periodicities in the pattern, which matchesthe results of Cole and Hariharan 1998Section 5.2, in the sense that periodicACM Computing Surveys, Vol. 33, No. 1, March 2001.56 G. Navarropatterns are more problematic. This isin fact a property shared with otherproblems in string matching.6.2.4 Kurtz 1996. In 1996, Kurtz 1996proposed another way to reduce the spacerequirements to at most Omn. It is anadaptation of BaezaYates and Gonnet1994, who first proposed it for the Hamming distance. The idea was to build theautomaton in lazy form, i.e. build only thestates and transitions actually reached inthe processing of the text. The automatonstarts as just one initial state and thestates and transitions are built as needed.By doing this, all those transitions thatUkkonen 1985b considered and thatwere not necessary were not built in fact,without the need to guess. The price wasthe extra overhead of a lazy constructionversus a direct construction, but the ideapays off. Kurtz also proposed buildingonly the initial part of the automatonwhich should be the most commonlytraversed states to save space.Navarro 1997b 1998 studied thegrowth of the complete and lazy automataas a function of m, k and n this lastvalue for the lazy automaton only. Theempirical results show that the lazyautomaton grows with the text at a rateof On, for 0    1, depending on , m, and k. Some replacement policiesdesigned to work with bounded memoryare proposed in Navarro 1998.7. BITPARALLELISMThese algorithms are based on exploitingthe parallelism of the computer when itworks on bits. This is also a new after1990 and very active area. The basic ideais to parallelize another algorithm using bits. The results are interesting fromthe practical point of view, and are especially significant when short patterns areinvolved typical in text retrieval. Theymay work effectively for any error level.In this section we find elements whichcould strictly belong to other sections,since we parallelize other algorithms.There are two main trends parallelize thework of the nondeterministic automatonthat solves the problem Figure 15,or parallelize the work of the dynamicprogramming matrix.We first explain the technique and thenthe results achieved by using it. Figure 17shows the historical development of thisarea.7.1 The Technique of BitParallelismThis technique, in common use in stringmatching BaezaYates 1991 1992, wasintroduced in the Ph.D. thesis of BaezaYates 1989. It consists in takingadvantage of the intrinsic parallelism ofthe bit operations inside a computer word.By using this fact cleverly, the numberof operations that an algorithm performscan be cut down by a factor of at mostw, where w is the number of bits in acomputer word. Since in current architectures w is 32 or 64, the speedup is verysignificant in practice and improves withtechnological progress. In order to relatethe behavior of bitparallel algorithmsto other work, it is normally assumedthat w2log n, as dictated by the RAMmodel of computation. We, however, preferto keep w as an independent value. Wenow introduce some notation we use forbitparallel algorithms.The length of a computer word in bitsis w.We denote as b..b1 the bits of a maskof length . This mask is stored somewhere inside the computer word. Sincethe length w of the computer word isfixed, we are hiding the details on wherewe store the  bits inside it.We use exponentiation to denote bit repetition e.g. 031  0001.We use Clike syntax for operationson the bits of computer words  isthe bitwiseor,  is the bitwiseand,   is the bitwisexor, and  complements all the bits. The shiftleftoperation, , moves the bits to theleft and enters zeros from the right,i.e. bmbm1..b2b1  r  bmr ...b2b10r .The shiftright  moves the bitsin the other direction. Finally, weACM Computing Surveys, Vol. 33, No. 1, March 2001.A Guided Tour to Approximate String Matching 57Fig. 17. Taxonomy of bitparallel algorithms. References are shortened to first letterssingle authors or initials multiple authors, and to the last two digits of years.Key BY89 BaezaYates 1989, WM92b Wu and Manber 1992b, Wri94 Wright1994, BYN99  BaezaYates and Navarro 1999, and Mye99  Myers 1999.can perform arithmetic operationson the bits, such as addition andsubtraction, which operate the bits asif they formed a number. For instance,b..bx10000 1  b..bx01111.We now explain the first bitparallelalgorithm, ShiftOr BaezaYates andGonnet 1992, since it is the basis ofmuch of what follows. The algorithmsearches a pattern in a text withouterrors by parallelizing the operation ofa nondeterministic finite automaton thatlooks for the pattern. Figure 18 illustratesthis automaton.This automaton has m 1 states, andcan be simulated in its nondeterministicform in Omn time. The ShiftOr algorithm achieves Omnw worstcase timei.e. optimal speedup. Notice that if weconvert the nondeterministic automatonto a deterministic one with On searchtime, we get an improved version of theKMP algorithm Knuth et al. 1977. However KMP is twice as slow for m  w.The algorithm first builds a table Bwhich for each character c stores a bitmask Bc  bm..b1. The mask in Bc hasthe bit bi set if and only if Pi  c. Thestate of the search is kept in a machineword D  dm..d1, where di is 1 wheneverP1..i matches the end of the text readup to now i.e. the state numbered i inFigure 18 is active. Therefore, a match isreported whenever dm  1.D is set to 1m originally, and for eachnew text character Tj , D is updated usingthe formula5D  D  1  0m11  BTj 5 The real algorithm uses the bits with the inversemeaning and therefore the operation  0m11 is notnecessary. We preferred to explain this more didacticversion.ACM Computing Surveys, Vol. 33, No. 1, March 2001.58 G. NavarroFig. 18. Nondeterministic automaton that searches survey exactly.The formula is correct because the ithbit is set if and only if the i 1th bit wasset for the previous text character and thenew text character matches the pattern atposition i. In other words, Tji 1.. j  P1..iif and only if Tji 1.. j1  P1..i1 and Tj Pi. It is possible to relate this formula tothe movement that occurs in the nondeterministic automaton for each new textcharacter each state gets the value of theprevious state, but this happens only if thetext character matches the correspondingarrow.For patterns longer than the computerword i.e. mw, the algorithm usesdmwe computer words for the simulationnot all them are active all the time. Thealgorithm is On on average.It is easy to extend ShiftOr to handleclasses of characters. In this extension,each position in the pattern matches aset of characters rather than a singlecharacter. The classical string matchingalgorithms are not extended so easily.In ShiftOr, it is enough to set the ithbit of Bc for every c  Pi Pi is now aset. For instance, to search for surveyin caseinsensitive form, we just set to1 the first bit of Bs and BS, andthe same with the rest. ShiftOr can alsosearch for multiple patterns where thecomplexity is Omnw if we consider thatm is the total length of all the patternsit was later enhanced Wu and Manber1992b to support a larger set of extendedpatterns and even regular expressions.Many online text algorithms can beseen as implementations of an automatonclassically, in its deterministic form.Bitparallelism has since its inventionbecome a general way to simulate simplenondeterministic automata instead ofconverting them to deterministic form.It has the advantage of being muchsimpler, in many cases faster since itmakes better usage of the registers ofthe computer word, and easier to extendin handling complex patterns than itsclassical counterparts. Its main disadvantage is the limitation it imposes on thesize of the computer word. In many casesits adaptations in coping with longerpatterns are not very efficient.7.2 Parallelizing Nondeterministic Automata7.2.1 Wu and Manber 1992. In 1992, Wuand Manber 1992b published a numberof ideas that had a great impact on the future of practical text searching. They firstextended the ShiftOr algorithm to handlewild cards i.e. allow an arbitrary number of characters between two given positions in the pattern, and regular expressions the most flexible pattern that can besearched efficiently. Of more interest tous is that they presented a simple schemeto combine any of the preceding extensionswith approximate string matching.The idea is to simulate, using bitparallelism, the NFA of Figure 15, so thateach row i of the automaton fits in a computer word Ri each state is representedby a bit. For each new text character,all the transitions of the automaton aresimulated using bit operations amongthe k 1 computer words. Notice that allthe k 1 computer words have the samestructure i.e. the same bit is aligned onthe same text position. The update formula to obtain the new R i values at textposition j from the current Ri values isR 0  R0  1  0m11  BTj R i 1  Ri 1  1  BTj   Ri Ri  1  R i  1and we start the search with Ri  0m i1i.As expected, R0 undergoes a simpleACM Computing Surveys, Vol. 33, No. 1, March 2001.A Guided Tour to Approximate String Matching 59ShiftOr process, while the other rowsreceive ones i.e. active states from previous rows as well. In the formula for R i 1,expressed in that order, are horizontal,vertical, diagonal and dashed diagonalarrows.The cost of this simulation is Okdmwen in the worst and average case, whichis Okn for patterns typical in text searching i.e. m  w. This is a perfect speedupover the serial simulation of the automaton, which would cost Omkn time. Noticethat for short patterns, this is competitiveto the best worstcase algorithms.Thanks to the simplicity of the construction, the rows of the pattern canbe changed by a different automaton. Aslong as one is able to solve a problemfor exact string matching, make k 1copies of the resulting computer word,and perform the same operations in thek 1 words plus the arrows that connectthe words, one has an algorithm to findthe same pattern allowing errors. Hence,with this algorithm one is able to performapproximate string matching with setsof characters, wild cards, and regularexpressions. The algorithm also allowssome extensions unique in approximatesearching a part of the pattern can besearched with errors that another maybe forced to match exactly, and differentinteger costs of the edit operations canbe accommodated including not allowingsome of them. Finally, one is able tosearch a set of patterns at the same time,but this capability is very limited since allthe patterns must fit in a computer word.The great flexibility obtained encouraged the authors to build a softwarecalled Agrep Wu and Manber 1992a,6where all these capabilities are implemented although some particular casesare solved in a different manner. Thissoftware has been taken as a reference inall the subsequent research.7.2.2 BaezaYates and Navarro 1996. In1996, BaezaYates and Navarro presenteda new bitparallel algorithm able toparallelize the computation of the au6 Available at ftp.cs.arizona.edu.tomaton even more BaezaYates andNavarro 1999. The classical dynamicprogramming algorithm can be thoughtof as a columnwise parallelization ofthe automaton BaezaYates 1996 Wuand Manber 1992b proposed a rowwiseparallelization. Neither algorithm wasable to increase the parallelism evenif all the NFA states fit in a computerword because of the transitions ofthe automaton, which caused what wecall zerotime dependencies. That is, thecurrent values of two rows or two columnsdepend on each other, and hence cannotbe computed in parallel.In BaezaYates and Navarro 1999the bitparallel formula for a diagonalparallelization was found. They packedthe states of the automaton along diagonals instead of rows or columns, whichrun in the same direction of the diagonalarrows notice that this is totally different from the diagonals of the dynamicprogramming matrix. This idea had beenmentioned much earlier by BaezaYates1991 but no bitparallel formula wasfound. There are m k 1 complete diagonals the others are not really necessarywhich are numbered from 0 to m k. Thenumber Di is the row of the first activestate in diagonal i all the subsequentstates in the diagonal are active because ofthe transitions. The new Di values afterreading text position j are computed asDi  minDi  1, Di 1 1, g Di1, Tj where the first term represents the substitutions, the second term the insertions,and the last term the matches deletionsare implicit since we represent only thelowestrow active state of each diagonal.The main problem is how to compute thefunction g , defined asg Di, Tj   mink 1 rr  Di  Pi r  Tj Notice that an active state that crossesa horizontal edge has to propagate all theway down by the diagonal. This was finallysolved in 1996 BaezaYates and Navarro1999 Navarro 1998 by representingACM Computing Surveys, Vol. 33, No. 1, March 2001.60 G. Navarrothe Di values in unary form and usingarithmetic operations on the bits whichhave the desired propagation effects. Theformula can be understood either numerically operating the Di s or logicallysimulating the arrows of the automaton.The resulting algorithm is On worstcase time and very fast in practice ifall the bits of the automaton fit in thecomputer word while Wu and Manber1992b keeps Okn. In general, it isOdkm  kwen worst case time, andOdk2wen on average since the Ukkonencutoff heuristic is used see Section 5.3.The scheme can handle classes of characters, wild cards and different integralcosts in the edit operations.7.3 Parallelizing the DynamicProgramming Matrix7.3.1 Wright 1994. In 1994, Wright1994 presented the first work usingbitparallelism on the dynamic programming matrix. The idea was to considersecondary diagonals i.e. those that runfrom the upperright to the bottomleftof the matrix. The main observation isthat the elements of the matrix follow therecurrence7Ci, j  Ci1, j1 if Pi Tjor Ci1, j Ci1, j1  1or Ci, j1 Ci1, j1  1Ci1, j1  1 otherwisewhich shows that the new secondarydiagonal can be computed using the twoprevious ones. The algorithm stores thedifferences between Ci, j and Ci1, j1 andrepresents the recurrence using modulo4 arithmetic. The algorithm packs manypattern and text characters in a computerword and performs in parallel a numberof pattern versus text comparisons, thenusing the vector of the results of thecomparisons to update many cells of thediagonal in parallel. Since it has to storecharacters of the alphabet in the bits,7 The original one in Wright 1994 has errors.the algorithm is Onm log w in theworst and average case. This was competitive at that time for very small alphabetse.g. DNA. As the author recognizes,it seems quite difficult to adapt thisalgorithm for other distance functions.7.3.2 Myers 1998. In 1998, Myers 1999found a better way to parallelize the computation of the dynamic programmingmatrix. He represented the differencesalong columns instead of the columnsthemselves, so that two bits per cell wereenough in fact this algorithm can be seenas the bitparallel implementation of theautomaton which is made deterministicin Wu et al. 1996, see Section 6.2. Anew recurrence is found where the cellsof the dynamic programming matrix areexpressed using horizontal and verticaldifferences, i.e. 1vi, j  Ci, j  Ci1, j and1hi, j  Ci, j  Ci, j11vi, j  minEqi, j ,1vi, j1,1hi1, j  11hi1, j 1hi, j  minEqi, j ,1vi, j1,1hi1, j  11vi, j1where Eqi, j is 1 if Pi  Tj and zerootherwise. The idea is to keep packedbinary vectors representing the currenti.e. j th values of the differences, andfinding the way to update the vectorsin a single operation. Each cell Ci, j isseen as a small processor that receivesinputs 1vi, j1, 1hi1, j , and Eqi, j andproduces outputs 1vi, j and 1hi, j . Thereare 3  3  2  18 possible inputs, and asimple formula is found to express the celllogic unlike Wright 1994, the approachis logical rather than arithmetical. Thehard part is to parallelize the work alongthe column because of the zerotimedependency problem. The author findsa solution which, despite the fact that avery different model is used, resemblesthat of BaezaYates and Navarro 1999.The result is an algorithm that usesthe bits of the computer word better, witha worst case of Odmwen and an average case of Odkwen since it uses theUkkonen cutoff Section 5.3. The updateformula is a little more complex than thatACM Computing Surveys, Vol. 33, No. 1, March 2001.A Guided Tour to Approximate String Matching 61of BaezaYates and Navarro 1999 andhence the algorithm is a bit slower, but itadapts better to longer patterns becausefewer computer words are needed.As it is difficult to surpass Oknalgorithms, this algorithm may be thelast word with respect to asymptoticefficiency of parallelization, except for thepossibility to parallelize an Okn worstcase algorithm. As it is now common toexpect of bitparallel algorithms, thisscheme is able to search some extendedpatterns as well, but it seems difficult toadapt it to other distance functions.8. FILTERING ALGORITHMSOur last category is quite new, startingin 1990 and still very active. It is formedby algorithms that filter the text, quicklydiscarding text areas that do not match.Filtering algorithms address only theaverage case, and their major interest isthe potential for algorithms that do notinspect all text characters. The major theoretical achievement is an algorithm withaverage cost Onk log mm, whichwas proven optimal. In practice, filteringalgorithms are the fastest too. All of them,however, are limited in their applicabilityby the error level . Moreover, they need anonfilter algorithm to check the potentialmatches.We first explain the general concept andthen consider the developments that haveoccurred in this area. See Figure 19.8.1 The Concept of FilteringFiltering is based on the fact that it maybe much easier to tell that a text positiondoes not match than to tell that it matches.For instance, if neither surnor vey appear in a text area, then survey cannotbe found there with one error under theedit distance. This is because a single editoperation cannot alter both halves of thepattern.Most filtering algorithms take advantage of this fact by searching pieces of thepattern without errors. Since the exactsearching algorithms can be much fasterthan approximate searching ones, filtering algorithms can be very competitivein fact, they dominate in a large range ofparameters.It is important to notice that a filteringalgorithm is normally unable to discoverthe matching text positions by itself.Rather, it is used to discard hopefullylarge areas of the text that cannot containa match. For instance, in our example, itis necessary that either sur or veyappear in an approximate occurrence, butit is not sufficient. Any filtering algorithmmust be coupled with a process thatverifies all those text positions that couldnot be discarded by the filter.Virtually any nonfiltering algorithmcan be used for this verification, and inmany cases the developers of a filteringalgorithm do not care to look for the bestverification algorithm, but just use thedynamic programming algorithm. Theselection is normally independent, butthe verification algorithm must behavewell on short texts because it can bestarted at many different text positionsto work on small text areas. By carefulprogramming it is almost always possibleto keep the worstcase behavior of theverifying algorithm i.e. avoid verifyingoverlapping areas.Finally, the performance of filteringalgorithms is very sensitive to the errorlevel . Most filters work very well on lowerror levels and very badly otherwise. Thisis related to the amount of text that thefilter is able to discard. When evaluatingfiltering algorithms, it is important notonly to consider their time efficiency butalso their tolerance for errors. One possible measure for this filtration efficiency isthe total number of matches found dividedby the total number of potential matchespointed out by the filtration algorithmSutinen 1998.A term normally used when referringto filters is sublinearity. It is said thata filter is sublinear when it does notinspect all the characters in the text likethe BoyerMoore algorithms Boyer andMoore 1977 for exact searching, whichcan at best be Onm. However, noonline algorithm can be truly sublinear,i.e. on, if m is independent of n. This isonly achievable with indexing algorithms.ACM Computing Surveys, Vol. 33, No. 1, March 2001.62 G. NavarroFig. 19. Taxonomy of filtering algorithms. Complexities are all on average. References are shortenedto first letters single authors or initials multiple authors, and to the last two digits of years.Key TU93  Tarhio and Ukkonen 1993, JTU96  Jokinen et al. 1996, Nav97a  Navarro 1997a,CL94 Chang and Lawler 1994, Ukk92 Ukkonen 1992, BYN99 BaezaYates and Navarro 1999,WM92b  Wu and Manber 1992b, BYP96  BaezaYates and Perleberg 1996, Shi96  Shi 1996,NBY99c Navarro and BaezaYates 1999c, Tak94 Takaoka 1994, CM94 Chang and Marr 1994,NBY98a  Navarro and BaezaYates 1998a, NR00  Navarro and Raffinot 2000, ST95  Sutinenand Tarhio 1995, and GKHO97  Giegerich et al. 1997.We divide this area in two parts moderate and very long patterns. The algorithmsfor the two areas are normally different,since more complex filters are only worthwhile for longer patterns.8.2 Moderate Patterns8.2.1 Tarhio and Ukkonen 1990. Tarhioand Ukkonen 19938 launched this areain 1990, publishing an algorithm that8 See also Jokinen et al. 1996, which has a correction to the algorithm.used BoyerMooreHorspool techniquesBoyer and Moore 1977 Horspool 1980to filter the text. The idea is to align thepattern with a text window and scanthe text backwards. The scanning endswhere more than k bad text charactersare found. A bad character is one that notonly does not match the pattern positionit is aligned with, but also does not matchany pattern character at a distance of kcharacters or less. More formally, assumethat the window starts at text positionj  1, and therefore Tj  i is aligned withACM Computing Surveys, Vol. 33, No. 1, March 2001.A Guided Tour to Approximate String Matching 63Pi. Then Tj  i is bad when Bad i, Tj  i,where Bad i, c has been precomputed asc 6 Pik , Pik 1, . . . , Pi, . . . , Pi k.The idea of the bad characters is that weknow for sure that we have to pay an errorto match them, i.e. they will not match asa byproduct of inserting or deleting othercharacters. When more than k charactersthat are errors for sure are found, the current text window can be abandoned andshifted forward. If, on the other hand, thebeginning of the window is reached, thearea Tj  1k.. jm must be checked with aclassical algorithm.To know how much we can shift thewindow, the authors show that there isno point in shifting P to a new positionj  where none of the k 1 text characters that are at the end of the currentwindow Tj mk, .., Tj m match thecorresponding character of P , i.e. whereTj mr 6 Pmr j  j . If those differencesare fixed with substitutions, we makek 1 errors, and if they can be fixedwith less than k 1 operations, then it isbecause we aligned some of the involvedpattern and text characters using insertions and deletions. In this case, we wouldhave obtained the same effect by aligningthe matching characters from the start.So for each pattern position i m  k..m and each text character athat could be aligned to position i i.e.for all a  6, the shift to align a in thepattern is precomputed, i.e. Shifti, a mins0Pisa or m if no such s exists.Later, the shift for the window is computed as minimk..m Shifti, Tj  i. Thislast minimum is computed together withthe backward window traversal.The analysis in Tarhio and Ukkonen1993 shows that the search time isOknk  1m  k, without considering verification. In Appendix A.1 weshow that the amount of verification isnegligible for   e2k 1 . The analysisis valid for m  k, so we can simplifythe search time to Ok2n . The algorithm is competitive in practice for lowerror levels. Interestingly, the versionk  0 corresponds exactly to the Horspoolalgorithm Horspool 1980. Like Horspool,it does not take proper advantage of verylong patterns. The algorithm can probably be adapted to other simple distancefunctions if we define k as the minimumnumber of errors needed to reject a string.8.2.2 Jokinen, Tarhio, and Ukkonen 1991.In 1991, Jokinen, Tarhio and UkkonenJokinen et al. 1996 adapted a previous filter for the kmismatches problemGrossi and Luccio 1989. The filter isbased on the simple fact that inside anymatch with at most k errors there mustbe at least m  k letters belonging to thepattern. The filter does not care aboutthe order of those letters. This is a simpleversion of Chang and Lawler 1994 seeSection 8.3, with less filtering efficiencybut simpler implementation.The search algorithm slides a windowof length m over the text9 and keeps countof the number of window characters thatbelong to the pattern. This is easily donewith a table that, for each character a,stores a counter of as in the pattern whichhas not yet been seen in the text window.The counter is incremented when an a enters the window and decremented whenit leaves the window. Each time a positive counter is decremented, the windowcharacter is considered as belonging to thepattern. When there are m k such characters, the area is verified with a classicalalgorithm.The algorithm was analyzed by Navarro1997a using a model of urns and balls. Heshows that the algorithm is On time for  em . Some possible extensions arestudied in Navarro 1998.The resulting algorithm is competitivein practice for short patterns, but it worsens for long ones. It is simple to adapt toother distance functions by just determining how many characters must match inan approximate occurrence.8.2.3 Wu and Manber 1992. In 1992, avery simple filter was proposed by Wu andManber 1992b among many other ideasin that work. The basic idea is in fact very9 The original version used a variable size window.This simplification is from Navarro 1997a.ACM Computing Surveys, Vol. 33, No. 1, March 2001.64 G. Navarroold Rivest 1976 if a pattern is cut in k 1pieces, then at least one of the pieces mustappear unchanged in an approximateoccurrence. This is evident, since k errorscannot alter the k 1 pieces. The proposalwas then to split the pattern in k 1approximately equal pieces, search thepieces in the text, and check the neighborhood of their matches of length m 2k.They used an extension of ShiftOrBaezaYates and Gonnet 1992 to searchall the pieces simultaneously in Omnwtime. In the same year, 1992, BaezaYates and Perleberg 1996 suggestedbetter algorithms for the multipatternsearch an AhoCorasick machine Ahoand Corasick 1975 to guarantee Onsearch time excluding verifications, orCommentzWalter 1979.Only in 1996 was the improvementreally implemented BaezaYates andNavarro 1999, by adapting the BoyerMooreSunday algorithm Sunday 1990to multipattern search using a trie ofpatterns and a pessimistic shift table.The resulting algorithm is surprisinglyfast in practice for low error levels.There is no closed expression for theaverage case cost of this algorithm BaezaYates and Regnier 1990, but we show inAppendix A.2 that a gross approximationis Okn log m . Two independentproofs in BaezaYates and Navarro 1999and BaezaYates and Perleberg 1996show that the cost of the search dominatesfor  13 log m. A simple way to seethis is to consider that checking a textarea costs Om2 and is done when any ofthe k 1 pieces of length mk 1 match,which happens with probability neark 1. The result follows from requiringthe average verification cost to be O1.This filter can be adapted, with somecare, to other distance functions. The mainissue is to determine how many pieces anedit operation can destroy and how manyedit operations can be made before surpassing the error threshold. For example,a transposition can destroy two pieces inone operation, so we would need to splitthe pattern in 2k 1 pieces to ensure thatone is unaltered. A more clever solution forthis case is to leave a hole of one characterbetween each pair of pieces, so that thetransposition cannot alter both.8.2.4 BaezaYates and Navarro 1996.The bitparallel algorithms presented inSection 7 BaezaYates and Navarro 1999were also the basis for novel filteringtechniques. As the basic algorithm islimited to short patterns, the algorithmssplit longer patterns in j parts, makingthem short enough to be searchable withthe basic bitparallel automaton usingone computer word.The method is based on a more generalversion of the partition into k 1 piecesMyers 1994a BaezaYates and Navarro1999. For any j , if we cut the pattern in jpieces, then at least one of them appearswith bkj c errors in any occurrence of thepattern. This is clear, since if each pieceneeds more than kj errors to match,then the complete match needs more thank errors.Hence, the pattern was split in j piecesof length mj  which were searched withkj errors using the basic algorithm. Eachtime a piece was found, the neighborhoodwas verified to check for the complete pattern. Notice that the error level  for thepieces is kept unchanged.The resulting algorithm is Onmkwon average. Its maximum  value is1 emO1w , smaller than 1 eand worsening as m grows. This may besurprising since the error level  is thesame for the subproblems. The reasonis that the verification cost keeps Om2but the matching probability is Omj ,larger than Om see Section 4.In 1997, the technique was enrichedwith superimposition BaezaYatesand Navarro 1999. The idea is to avoidperforming one separate search foreach piece of the pattern. A multipattern approximate searching is designedusing the ability of bitparallelism tosearch for classes of characters. Assume that we want to search surveyand secret. We search the patternsuercvreyt,where ab meansa, b. In the NFA of Figure 15, the horizontal arrows are traversable by morethan one letter. Clearly, any match of eachACM Computing Surveys, Vol. 33, No. 1, March 2001.A Guided Tour to Approximate String Matching 65Fig. 20. The hierarchical verification method fora pattern split in four parts. The boxes leaves arethe elements which are really searched, and the rootrepresents the whole pattern. At least one patternat each level must match in any occurrence of thecomplete pattern. If the bold box is found, all thebold lines may be verified.of the two patterns is also a match of thesuperimposed pattern, but not viceversae.g. servet matches zero errors. Sothe filter is weakened but the search ismade faster. Superimposition allowedlowering the average search time to Onfor  1 emO1wmw and toOnmkw for the maximum  ofthe 1996 version. By using a j valuesmaller than the one necessary to putthe automata in single machine words,an intermediate scheme was obtainedthat softly adapted to higher error levels.The algorithm was Okn logmw for  1 e .8.2.5 Navarro and BaezaYates 1998. Thefinal twist in the previous scheme was theintroduction of hierarchical verificationin 1998 Navarro and BaezaYates 1998a.For simplicity assume that the pattern ispartitioned in j  2r pieces, although thetechnique is general. The pattern is splitin two halves, each one to be searchedwith bk2c errors. Each half is recursivelysplit in two and so on, until the pattern isshort enough to make its NFA fit in a computer word see Figure 20. The leaves ofthis tree are the pieces actually searched.When a leaf finds a match, instead ofchecking the whole pattern as in theprevious technique, its parent is checkedin a small area around the piece thatmatched. If the parent is not found, theverification stops, otherwise it continueswith the grandparent until the root i.e.the whole pattern is found. This is correctbecause the partitioning scheme appliesto each level of the tree the grandparentcannot appear if none of its childrenappear, even if a grandchild appeared.Figure 20 shows an example. If onesearches the pattern aaabbbcccddd withfour errors in the text xxxbbxxxxxxx,and splits the pattern in four pieces to besearched with one error, the piece bbbwill be found in the text. In the originalapproach, one would verify the completepattern in the text area, while with thenew approach one verifies only its parentaaabbb and immediately determinesthat there cannot be a complete match.An orthogonal hierarchical verificationtechnique is also presented in Navarroand BaezaYates 1998a to includesuperimposition in this scheme. If thesuperimposition of four patterns matches,the set is split in two sets of two patternseach, and it is checked whether some ofthem match instead of verifying all thefour patterns one by one.The analysis in Navarro 1998 andNavarro and BaezaYates 1998a showsthat the average verification cost drops toOmj 2. Only now the problem scaleswell i.e. Omj  verification probabilityand Omj 2 verification cost. Withhierarchical verification, the verificationcost stays negligible for   1 e . Allthe simple extensions of bitparallel algorithms apply, although the partition intoj pieces may need some redesign for otherdistances. Notice that it is very difficultto break the barrier of   1  e forany filter because, as shown in Section 4,there are too many real matches, and eventhe best filters must check real matches.In the same year, 1998, the sameauthors Navarro and BaezaYates 1999cNavarro 1998 added hierarchical verification to the filter that splits the patternin k 1 pieces and searches them withzero errors. The analysis shows thatwith this technique the verification costdoes not dominate the search time for 1 log m. The resulting filter is thefastest for most cases of interest.8.2.6 Navarro and Raffinot 1998. In 1998Navarro and Raffinot Navarro andRaffinot 2000 Navarro 1998 presented anovel approach based on suffix automataACM Computing Surveys, Vol. 33, No. 1, March 2001.66 G. NavarroFig. 21. The construction to search any reverse prefix of survey allowing 2 errors.see Section 3.2. They adapted an exactstring matching algorithm, BDM, to allowerrors.The idea of the original BDM algorithmis as follows Crochemore et al. 1994Crochemore and Rytter 1994. The deterministic suffix automaton of the reversepattern is built so that it recognizes thereverse prefixes of the pattern. Then thepattern is aligned with a text window, andthe window is scanned backwards withthe automaton this is why the patternis reversed. The automaton is active aslong as what it has read is a substringof the pattern. Each time the automatonreaches a final state, it has seen a patternprefix, so we remember the last timeit happened. If the automaton arriveswith active states at the beginning ofthe window then the pattern has beenfound, otherwise what is there is nota substring of the pattern and hencethe pattern cannot be in the window. Inany case the last window position thatmatched a pattern prefix gives the nextinitial window position. The algorithmBNDM Navarro and Raffinot 2000 isa bitparallel implementation usingthe nondeterministic suffix automaton,see Figure 3 which is much faster inpractice and allows searching for classesof characters, etc.A modification of Navarro and Raffinot2000 is to build a NFA to search the reversed pattern allowing errors, modify itto match any pattern suffix, and apply essentially the same BNDM algorithm using this automaton. Figure 21 shows theresulting automaton.This automaton recognizes any reverseprefix of P allowing k errors. The window will be abandoned when no patternsubstring matches what was read with kerrors. The window is shifted to the nextpattern prefix found with k errors. Thematches must start exactly at the initialwindow position. The window length ism  k, not m, to ensure that if there isan occurrence starting at the windowposition then a substring of the patternoccurs in any suffix of the window so thatwe do not abandon the window beforereaching the occurrence. Reaching thebeginning of the window does not guarantee a match, however, so we have to checkthe area by computing edit distance fromthe beginning of the window at mostm k text characters.In Appendix A.3 it is shown thatthe average complexity10 is Onlog mm1  and the filterworks well for   1 e 2 e ,which for large alphabets tends to 12.The result is competitive for low errorlevels, but the pattern cannot be very10 The original analysis of Navarro 1998 is inaccurate.ACM Computing Surveys, Vol. 33, No. 1, March 2001.A Guided Tour to Approximate String Matching 67Fig. 22. Algorithms LET and SET. LET covers all the text with pattern substrings, whileSET works only at block beginnings and stops when it finds k differences.long because of the bitparallel implementation. Notice that trying to do thiswith the deterministic BDM would havegenerated a very complex construction,while the algorithm with the nondeterministic automaton is simple. Moreover, adeterministic automaton would have toomany states, just as in Section 6.2. Allthe simple extensions of bitparallelismapply, provided the window length m  kis carefully reconsidered.A recent software program, calledem nrgrep, capable of fast, exact, andapproximate searching of simple andcomplex patterns has been built with thismethod Navarro 2000b.8.3 Very Long Patterns8.3.1 Chang and Lawler 1990. In 1990,Chang and Lawler 1994 presented twoalgorithms better analyzed in Giegerichet al. 1997. The first one, called LETfor linear expected time, works asfollows the text is traversed linearly, andat each time the longest pattern substringthat matches the text is maintained.When the substring cannot be extendedfurther, it starts again from the currenttext position Figure 22 illustrates.The crucial observation is that, if lessthan m  k text characters have beencovered by concatenating k longest substrings, then the text area does not matchthe pattern. This is evident because amatch is formed by k 1 correct strokesrecall Section 5.2 separated by k errors.Moreover, the strokes need to be ordered,which is not required by the filter.The algorithm uses a suffix treeon the pattern to determine in a linearpass the longest pattern substring thatmatches the text seen up to now. Noticethat the article is from 1990, the sameyear that Ukkonen and Wood 1993 didthe same with a suffix automaton seeSection 5.2. Therefore, the filtering is inOn time. The authors use Landau andVishkin 1989 as the verifying algorithmand therefore the worst case is Okn.The authors show that the filtering timedominates for   1 log mO1. Theconstants are involved, but practicalfigures are   0.35 for   64 or   0.15for   4.The second algorithm presented iscalled SET for sublinear expectedtime. The idea is similar to LET, exceptthat the text is split in fixed blocks of sizem k2, and the check for k contiguousstrokes starts only at block boundaries.Since the shortest match is of lengthm k, at least one of these blocks isalways contained completely in a match.If one is able to discard the block, nooccurrence can contain it. This is alsoillustrated in Figure 22.The sublinearity is clear once it isproven that a block is discarded on average in Ok log m comparisons. Since2nm  k blocks are considered, theaverage time is O n log m1  .The maximum  level stays the same as inLET, so the complexity can be simplifiedto O n log m. Although the proof thatlimits the comparisons per block is quiteinvolved, it is not hard to see intuitivelywhy it is true the probability of finding astroke of length  in the pattern is limitedby m , and the detailed proof showsthat   log m is on average the longeststroke found. This contrasts with theresult of Myers 1986a Section 5.3, thatshows that k strokes add up Ok length.ACM Computing Surveys, Vol. 33, No. 1, March 2001.68 G. NavarroFig. 23. Qgram algorithm. The left one Ukkonen 1992 counts the number of pattern qgramsin a text window. The right one Sutinen and Tarhio 1995 finds sequences of pattern qgrams inapproximately the same text positions we have put in bold a text sample and the possible qgramsto match it.The difference is that here we can takethe strokes from anywhere in the pattern.Both LET and SET are effective forvery long patterns only, since their overhead does not pay off on short patterns.Different distance functions can be accommodated after rereasoning the adequatek values.8.3.2 Ukkonen 1992. In 1992, Ukkonen1992 independently rediscovered someof the ideas of Chang and Lampe. Hepresented two filtering algorithms, one ofwhich based on what he called maximalmatches is similar to the LET of Changand Lawler 1994 in fact Ukkonenpresents it as a new block distancecomputable in linear time, and shows thatit serves as a filter for the edit distance.The other filter is the first reference toqgrams for online searching there aremuch older ones in indexed searchingUllman 1977.A qgram is a substring of length q.A filter was proposed based on countingthe number of qgrams shared betweenthe pattern and a text window this ispresented in terms of a new qgramdistance which may be of interest onits own. A pattern of length m hasmq 1 overlapping qgrams. Eacherror can alter q qgrams of the pattern,and therefore mq 1  kq patternqgrams must appear in any occurrenceFigure 23 illustrates.Notice that this is a generalizationof the counting filter of Jokinen et al.1996 Section 8.2, which correspondsto q  1. The search algorithm is similaras well, although of course keeping atable with a counter for each of the  qqgrams is impractical especially because only m q 1 of them are present.Ukkonen uses a suffix tree to keep countof the last qgram seen in linear time therelevant information can be attached tothe m q 1 important nodes at depth qin the suffix tree.The filter therefore takes linear time.There is no analysis to show which isthe maximum error level tolerated bythe filter, so we attempt a gross analysisin Appendix A.4, valid for large m. Theresult is that the filter works well for  O1 log m, and that the optimalq to obtain it is q  log m. The searchalgorithm is more complicated than thatof Jokinen et al. 1996. Therefore, usinglarger q values only pays off for largerpatterns. Different distance functionsare easily accommodated by recomputingthe number of qgrams that must bepreserved in any occurrence.8.3.3 Takaoka 1994. In 1994, Takaoka1994 presented a simplification ofChang and Lawler 1994. He consideredhsamples of the text which are nonoverlapping qgrams of the text taken eachh characters, for hq. The idea is that ifone hsample is found in the pattern, thena neighborhood of the area is verified.By using h  bmkq 1k 1c onecannot miss a match. The easiest way tosee this is to start with k  0. Clearly, weneed h  mq 1 to not lose any matches.ACM Computing Surveys, Vol. 33, No. 1, March 2001.A Guided Tour to Approximate String Matching 69For larger k, recall that if the pattern issplit in k 1 pieces some of them mustappear with no errors. The filter dividesh by k 1 to ensure that any occurrence ofthose pieces will be found we are assuming q  mk 1.Using a suffix tree of the pattern, thehsample can be found in Oq time.Therefore the filtering time is Oqnh,which is On log m1   if theoptimal q  log m is used. The errorlevel is again   O1 log m, whichmakes the time On log m.8.3.4 Chang and Marr 1994. It looks likeOn log m is the best complexity achievable by using filters, and that it will workonly for   O1 log m. But in 1994,Chang and Marr obtained an algorithmwhich wasOk log mmnfor    , where  depends only on and it tends to 1  e for very large  .At the same time, they proved that thiswas a lower bound for the average complexity of the problem and therefore theiralgorithm was optimal on average. Thisis a major theoretical breakthrough.The lower bound is obtained by takingthe maximum or sum of two simple factsthe first one is the On log mm boundof Yao 1979 for exact string matching,and the second one is the obvious factthat in order to discard a block of m textcharacters, at least k characters should beexamined to find the k errors and henceOknm is a lower bound. Also, themaximum error level is optimal accordingto Section 4. What is impressive is thatan algorithm with such complexity wasfound.The algorithm is a variation of SETChang and Lawler 1994. It is of polynomial space in m, i.e. Omt space forsome constant t which depends on  . It isbased on splitting the text in contiguoussubstrings of length   t log m. Insteadof finding in the pattern the longest exactmatches starting at the beginning ofblocks of size m  k2, it searches thetext substrings of length  in the patternallowing errors.The algorithm proceeds as follows. Thebest matches allowing errors inside P areprecomputed for every tuple hence theOmt space. Starting at the beginning ofthe block, it searches consecutive tuplesin the pattern each in O time, untilthe total number of errors made exceedsk. If by that time it has not yet coveredm k text characters, the block can besafely skipped.The reason why this works is a simpleextension of SET. We have found an areacontained in the possible occurrence whichcannot be covered with k errors even allowing the use of unordered portions of thepattern for the match. The algorithm isonly practical for very long patterns, andcan be extended for other distances withthe same ideas as the other filtration andqgram methods.It is interesting to notice that 1 e is the limit we have discussed in Section 4, which is a firmbarrier for any filtering mechanism.Chang and Lawler proved an asymptoticresult, while a general bound is provedin BaezaYates and Navarro 1999. Thefilters of Chang and Marr 1994 andNavarro and BaezaYates 1998a reducethe problem to fewer errors instead of tozero errors. An interesting observation isthat it seems that all the filters that partition the problem into exact search canbe applied for   O1 log m, and thatin order to improve this to 1  e wemust partition the problem into smallerapproximate searching subproblems.8.3.5 Sutinen and Tarhio 1995. Sutinenand Tarhio 1995 generalized theTakaoka filter in 1995, improving itsfiltering efficiency. This is the first filterthat takes into account the relative positions of the pattern pieces that match inthe text all the previous filters matchedpieces of the pattern in any order. Thegeneralization is to force s qgrams of thepattern to match not just one. The piecesmust conserve their relative ordering inthe pattern and must not be more thank characters away from their correctACM Computing Surveys, Vol. 33, No. 1, March 2001.70 G. Navarroposition otherwise we need to make morethan k errors to use them. This methodis also illustrated in Figure 23.In this case, the sampling step is reduced to hbm k  q 1k sc. Thereason for this reduction is that, to ensurethat s pieces of the pattern match, weneed to cut the pattern into k s pieces.The pattern is divided in k s pieces anda hashed set is created for each pieceso that the pieces are forced not to betoo far away from their correct positions.The set contains the qgrams of the pieceand some neighboring ones too becausethe sample can be slightly misaligned. Atsearch time, instead of a single hsample,they consider text windows of contiguoussequences of k s hsamples. Each ofthese hsamples is searched in the corresponding set, and if at least s are foundthe area is verified. This is a sort ofHamming distance, and the authorsresort to an efficient algorithm for thatdistance BaezaYates and Gonnet 1992to process the text.The resulting algorithm is On log mon average using optimal q log m, andworks well for  1 log m. The algorithm is better suited for long patterns,although with s 2 it can be reasonablyapplied to short ones as well. In fact theanalysis is done for s  2 only in Sutinenand Tarhio 1995.8.3.6 Shi 1996. In 1996 Shi 1996 proposed to extend the idea of the k 1 piecesexplained in Section 8.2 to k s pieces,so that at least s pieces must match. Thisidea is implicit in the filter of Sutinen andTarhio but had not been explicitly writtendown. Shi compared his filter againstthe simple one, finding that the filteringefficiency was improved. However, thisimprovement will be noticeable onlyfor long patterns. Moreover, the onlinesearching efficiency is degraded becausethe pieces are shorter which affects anyBoyerMoorelike search, and becausethe verification logic is more complex. Noanalysis is presented in the paper, butwe conjecture that the optimum s is O1and therefore the same complexity andtolerance to errors is maintained.8.3.7 Giegerich, Kurtz, Hischke, and Ohlebusch 1996. Also in 1996, a generalmethod to improve filters was developedGiegerich et al. 1997. The idea is tomix the phases of filtering and checking,so that the verification of a text areais abandoned as soon as the combinedinformation from the filter number ofguaranteed differences left and theverification in progress number of actualdifferences seen shows that a match isnot possible. As they show, however, theimprovement occurs in a very narrowarea of . This is a consequence of thestatistics of this problem that we havediscussed in Section 4.9. EXPERIMENTSIn this section we make empirical comparisons among the algorithms described inthis work. Our goal is to show the bestoptions at hand depending on the case.Nearly 40 algorithms have been surveyed,some of them without existing implementations and many of them already knownto be impractical. To avoid excessively longcomparisons among algorithms known notto be competitive, we have left many ofthem aside.9.1 Included and Excluded AlgorithmsA large group of excluded algorithms isfrom the theoretical side based on thedynamic programming matrix. Althoughthese algorithms are not competitive inpractice, they represent or representedat their time a valuable contributionto the development of the algorithmicaspect of the problem. The dynamicprogramming algorithm Sellers 1980 isexcluded because the cutoff heuristic ofUkkonen 1985b is known to be fastere.g. in Chang and Lampe 1992 andin our internal tests the Masek andPaterson algorithm 1980 is argued inthe same paper to be worse than dynamicprogramming which is quite bad forn  40 GB Landau and Vishkin 1988has bad complexity and was improvedlater by many others in theory andpractice Landau and Vishkin 1989 isACM Computing Surveys, Vol. 33, No. 1, March 2001.A Guided Tour to Approximate String Matching 71implemented with a better LCA algorithmin Chang and Lampe 1992 and foundtoo slow Myers 1986a is consideredslow in practice by the same author inWu et al. 1996 Galil and Giancarlo1988 is clearly slower than Landau andVishkin 1989 Galil and Park 1990, oneof the fastest among the Okn worst casealgorithms, is shown to be extremely slowin Ukkonen and Wood 1993, Chang andLampe 1992, and Wright 1994 and ininternal tests done by ourselves Ukkonenand Wood 1993 is shown to be slow inJokinen et al. 1996 the Okn algorithmimplemented in Chang and Lawler 1994is in the same paper argued to be thefastest of the group and shown to be notcompetitive in practice Sahinalp andVishkin 1997 and Cole and Hariharan1998 are clearly theoretical, their complexities show that the patterns have tobe very long and the error level too lowto be of practical application. To give anidea of how slow is slow, we found Galiland Park 1990 10 times slower thanUkkonens cutoff heuristic a similarresult is reported by Chang and Lampe1992. Finally, other Okn average timealgorithms are proposed in Myers 1986aand Galil and Park 1990, and they areshown to be very similar to Ukkonenscutoff Ukkonen 1985b in Chang andLampe 1992. Since the cutoff heuristicis already not very competitive we leaveaside the other similar algorithms. Therefore, from the group based on dynamicprogramming we consider only the cutoffheuristic mainly as a reference andChang and Lampe 1992, which is theonly one competitive in practice.From the algorithms based on automata we consider the DFA algorithmUkkonen 1985b, but prefer its lazyversion implemented in Navarro 1997b,which is equally fast for small automataand much faster for large automata. Wealso consider the Four Russians algorithmof Wu et al. 1996. From the bitparallelalgorithms we consider Wu and Manber1992b, BaezaYates and Navarro 1999,and Myers 1999, leaving aside Wright1994. As shown in the 1996 versionof BaezaYates and Navarro 1999, thealgorithm of Wright 1994 was competitive only on binary text, and this wasshown to not hold anymore in Myers1999.From the filtering algorithms, we haveincluded Tarhio and Ukkonen 1993the counting filter proposed in Jokinenet al. 1996 as simplified in Navarro1997a the algorithm of Navarro andRaffinot 2000 and those of Sutinen andTarhio 1995 and Takaoka 1994 thislast seen as the case s  1 of Sutinen andTarhio 1995, since this implementationworked better. We have also includedthe filters proposed in BaezaYates andNavarro 1999, Navarro and BaezaYates1998a, and Navarro 1998, preferringto present only the last version whichincorporates all the twists of superimposition, hierarchical verification and mixedpartitioning. Many previous versionsare outperformed by this one. We havealso included the best version of thefilters that partition the pattern in k 1pieces, namely the one incorporatinghierarchical verification Navarro andBaezaYates 1999c Navarro 1998. Inthose publications it is shown that thisversion clearly outperforms the previousones proposed in Wu and Manber 1992b,BaezaYates and Perleberg 1996, andBaezaYates and Navarro 1999. Finally,we are discarding some filters Chang andLawler 1994 Ukkonen 1992 Chang andMarr 1994 Shi 1996 which are applicable only to very long patterns, since thiscase is excluded from our experimentsas explained shortly. Some comparisonsamong them were carried out by Changand Lampe 1992, showing that LET isequivalent to the cutoff algorithm withk  20, and that the time for SET is 2times that of LET. LET was shown to bethe fastest with patterns of a hundredletters long and a few errors in Jokinenet al. 1996, but we recall that manymodern filters were not included in thatcomparison.We now list the included algorithmsand the relevant comments about them.All the algorithms implemented by usrepresent our best coding effort andhave been found similar or faster thanACM Computing Surveys, Vol. 33, No. 1, March 2001.72 G. Navarroother implementations found elsewhere.The implementations coming from otherauthors were checked with the samestandards and in some cases their codewas improved with better register usage and IO management. The numberin parenthesis following the name ofeach algorithm is the number of linesof the C implementation we use. Thisgives a rough idea of how complex theimplementation of each algorithm is.CTF 239 The cutoff heuristic ofUkkonen 1985b implemented by us.CLP 429 The column partitioning algorithm of Chang and Lampe 1992, implemented by them. We replaced theirIO by ours, which is faster.DFA 291 The lazy deterministic automaton of Navarro 1997b, implemented byus.RUS 304 The FourRussians algorithmof Wu et al. 1996, implemented bythem. We tried different r values related to the timespace tradeoff andfound that the best option is alwaysr  5 in our machine.BPR 229 The NFA bitparallelizedby rows Wu and Manber 1992b,implemented by us and restricted tom  w. Separate code is used for k  1,2, 3 and k  3. We could continue writing separate versions but decided thatthis is reasonable up to k  3, as at thatpoint the algorithm is not competitiveanyway.BPD 249  1,224 The NFA bitparallelized by diagonals BaezaYates andNavarro 1999, implemented by us.Here we do not include any filteringtechnique. The first number 249corresponds to the plain techniqueand the second one 1,224 to handlingpartitioned automata.BPM 283  722 The bitparallel implementation of the dynamic programmingmatrix Myers 1999, implemented bythat author. The two numbers have thesame meaning as in the previous item.BMH 213 The adaptation of Horspoolto allow errors Tarhio and Ukkonen1993, implemented by them. We usetheir algorithm 2 which is faster, improve some register usage and replacetheir IO by ours, which is faster.CNT 387 The counting filter of Jokinenet al. 1996, as simplified in Navarro1997a and implemented by us.EXP 877 Partitioning in k 1 piecesplus hierarchical verification Navarroand BaezaYates 1999c Navarro 1998,implemented by us.BPP 3,466 The bitparallel algorithmsof BaezaYates and Navarro 1999,Navarro and BaezaYates 1998a, andNavarro 1998 using pattern partitioning, superimposition, and hierarchicalverification. The implementation isours and is packaged software that canbe downloaded from the Web page ofthe author.BND 375 The BNDM algorithm adaptedto allow errors in Navarro and Raffinot2000 and Navarro 1998 implementedby us and restricted to m  w. Separatecode is used for k  1, 2, 3 and k  3.We could continue writing separate versions but decided that this is reasonableup to k  3.QG2 191 The qgram filter of Sutinenand Tarhio 1995, implemented bythem and used with s 2 since s 1is the algorithm Takaoka 1994, seenext item and s 2 worked well onlyfor very long patterns. The code isrestricted to k  w2  3, and it is alsonot run when q is found to be 1 since theperformance is very poor. We improvedregister usage and replaced the IOmanagement by our faster versions.QG1 191 The qgram algorithm ofTakaoka 1994, run as the special cases  1 of the previous item. The samerestrictions on the code apply.We did our best to uniformize thealgorithms. The IO is the same in allcases the text is read in chunks of 64 KBto improve locality this is the optimumin our machine and care is taken to notlose or repeat matches in the bordersopen is used instead of fopen because itis slower. We also uniformize internalconventions only a final special characterACM Computing Surveys, Vol. 33, No. 1, March 2001.A Guided Tour to Approximate String Matching 73zero is used at the end of the bufferto help algorithms recognize it andonly the number of matches found isreported.In the experiments we separate the filtering and nonfiltering algorithms. This isbecause the filters can in general use anynonfilter to check for potential matches, sothe best algorithm is formed by a combination of both. All the filtering algorithms inthe experiments use the cutoff algorithmUkkonen 1985b as their verification engine, except for BPP whose very essenceis to switch smoothly to BPD and BNDwhich uses a reverse BPR to search in thewindow and a forward BPR for the verifications.9.2 Experimental SetupApart from the algorithms and their details, we describe our experimental setup.We measure CPU times and show the results in tenths of seconds per megabyte.Our machine is a Sun UltraSparc1 with167 MHz and 64 MB in main memory, werun Solaris 2.5.1 and the texts are on a local disk of 2 GB. Our experiments were runon texts of 10 MB and repeated 20 timeswith different search patterns. The samepatterns were used for all the algorithms.In the applications, we have selectedthree types of texts.DNA This file is formed by concatenatingthe 1.34 MB DNA chain of h.influenzaewith itself until 10 MB is obtained.Lines are cut at 60 characters. Thepatterns are selected randomly fromthe text, avoiding line breaks if possible.The alphabet size is four, save for a fewexceptions along the file, and the results are similar to a random fourlettertext.Natural language This file is formed by1.29 MB from the work of BenjaminFranklin filtered to lowercase andseparators converted to a space exceptline breaks which are respected. Thismimics common information retrievalscenarios. The text is replicated toobtain 10 MB and search patterns arerandomly selected from the same textat word beginnings. The results areroughly equivalent to a random textover 15 characters.SpeechWe obtained speech files fromdiscussions of U.S. law from IndianaUniversity, in PCM format with 8 bitsper sample. Of course, the standard editdistance is of no use here, since it has totake into account the absolute values ofthe differences between two characters.We simplified the problem in order touse edit distance we reduced the rangeof values to 64 by quantization, considering two samples that lie in the samerange as equal. We used the first 10 MBof the resulting file. The results aresimilar to those on a random text of 50letters, although the file shows smoothchanges from one letter to the next.We present results using different pattern lengths and error levels in two flavors we fix m and show the effect of increasing k, or we fix  and show the effectof increasing m. A given algorithm maynot appear at all in a plot when its timesare above the y range or its restrictionson m and k do not intersect with the xrange. In particular, filters are shown onlyfor   12. We remind readers that inmost applications the error levels of interest are low.9.3 ResultsFigure 24 shows the results for shortpatterns m 10 and varying k. In nonfiltering algorithms BPD is normally thefastest, up to 30 faster than the nextone, BPM. The DFA is also quite closein most cases. For k 1, a specializedversion of BPR is slightly faster thanBPD recall that for k 3 BPR starts touse a nonspecialized algorithm, hencethe jump. An exception occurs in DNAtext, where for k 4 and k 5, BPDshows a nonmonotonic behavior and BPMbecomes the fastest. This behavior comesfrom its Okm knw complexity,1111 Another reason for this behavior is that there areinteger roundoff effects that produce nonmonotonicresults.ACM Computing Surveys, Vol. 33, No. 1, March 2001.74 G. NavarroFig. 24. Results for m  10 and varying k. The left plots show nonfiltering and the right plotsshow filtering algorithms. Rows 13 show DNA, English, and speech files, respectively.ACM Computing Surveys, Vol. 33, No. 1, March 2001.A Guided Tour to Approximate String Matching 75which in texts with larger alphabets is notnoticeable because the cutoff heuristickeeps the cost unchanged. Indeed, thebehavior of BPD would have been totallystable if we had chosen m 9 instead ofm 10, because the problem would fitin a computer word all the time. BPM,on the other hand, handles much longerpatterns, maintaining stability, althoughit takes up to 50 more time than BPD.With respect to filters, EXP is thefastest for low error levels. The value oflow increases for larger alphabets. Atsome point, BPP starts to dominate. BPPadapts smoothly to higher error levelsby slowly switching to BPD, so BPP is agood alternative for intermediate errorlevels, where EXP ceases to work untilit switches to BPD. However, this rangeis void on DNA and English text form 10. Other filters competitive withEXP are BND and BMH. In fact, BNDis the fastest for k 1 on DNA, althoughno filter works very well in that case.Finally, QG2 does not appear because itonly works for k 1 and it was worse thanQG1.The best choice for short patterns seemsto be EXP while it works and switchingto the best bitparallel algorithm forhigher errors. Moreover, the verificationalgorithm for EXP should be BPR orBPD which are the fastest where EXPdominates.Figure 25 shows the case of longer patterns m 30. Many of the observationsare still valid in this case. However, inthis case the algorithm BPM shows itsadvantage over BPD, since the entireproblem still fits in a computer word forBPM and it does not for BPD. Hence inthe left plots the best algorithm is BPMexcept for low k, where BPR or BPD arebetter. With respect to filters, EXP orBND are the fastest, depending on thealphabet, until a certain error level isreached. At that point BPP becomes thefastest, in some cases still faster thanBPM. Notice that for DNA a specializedversion of BND for k 4 and even 5 couldbe the fastest choice.In Figure 26 we consider the case offixed  0.1 and growing m. The resultsrepeat somewhat those for nonfilteringalgorithms BPR is the best for k 1 i.e.m 10, then BPD is the best until acertain pattern length is reached whichvaries from 30 on DNA to 80 on speech,and finally BPM becomes the fastest. Notethat for such a low error level the numberof active columns is quite small, whichpermits algorithms like BPD and BPMto keep their good behavior for patternsmuch longer than what they could handlein a single machine word. The DFA isalso quite competitive until its memoryrequirements become unreasonable.The real change, however, is in thefilters. In this case PEX becomes the starfilter in English and speech texts. The situation for DNA, on the other hand, is quitecomplex. For m 30, BND is the fastest,and indeed an extended implementationallowing longer patterns could keep it being the fastest for a few more points. However, that case would have to handle fourerrors, and only a specialized implementation for fixed k 4, 5,. . . . could maintaina competitive performance. We havedetermined that such specialized code isworthwhile up to k 3 only. When BNDceases to be applicable, PEX becomes thefastest algorithm, and finally QG2 beatsit for m 60. However, notice that form 30, all the filters are beaten by BPMand therefore make little sense on DNA.There is a final phenomenon thatdeserves mention with respect to filters.The algorithms QG1 and QG2 improve asm grows. These algorithms are the mostpractical, and the only ones we tested inthe family of algorithms suitable for verylong patterns. Thus, although all thesealgorithms would not be competitive inour tests where m 100, they shouldbe considered in scenarios where thepatterns are much longer and the errorlevel is kept very low. In such a scenario,those algorithms would finally beat allthe algorithms we consider here.The situation becomes worse for thefilters when we consider   0.3 andvarying m Figure 27. On DNA, no filtercan beat the nonfiltering algorithms, andamong them the tricks to maintain a fewactive columns do not work well. ThisACM Computing Surveys, Vol. 33, No. 1, March 2001.76 G. NavarroFig. 25. Results for m  30 and varying k. The left plots show nonfiltering and the right plotsshow filtering algorithms. Rows 13 show DNA, English and speech files, respectively.ACM Computing Surveys, Vol. 33, No. 1, March 2001.A Guided Tour to Approximate String Matching 77Fig. 26. Results for   0.1 and varying m. The left plots show nonfiltering and the right plotsshow filtering algorithms. Rows 13 show DNA, English and speech files, respectively.ACM Computing Surveys, Vol. 33, No. 1, March 2001.78 G. NavarroFig. 27. Results for   0.3 and varying m. The left plots show nonfiltering and the right plotsshow filtering algorithms. Rows 13 show DNA, English and speech files, respectively.ACM Computing Surveys, Vol. 33, No. 1, March 2001.A Guided Tour to Approximate String Matching 79Fig. 28. The areas where each algorithm is the best gray is that of filtering algorithms.favors the algorithms that pack more information per bit, which makes BPM thebest in all cases except for m 10 whereBPD is better. The situation is almostthe same on English text, except thatBPP works reasonably well and becomesquite similar to BPM the periods whereeach one dominates are interleaved. Onspeech, on the other hand, the scenario issimilar to that for nonfiltering algorithms,but the PEX filter still beats all of them,as 30 of errors is low enough on thespeech files. Note in passing that the errorlevel is too high for QG1 and QG2, whichcan only be applied in a short range andyield bad results.To give an idea of the areas where eachalgorithm dominates, Figure 28 shows thecase of English text. There is more information in Figure 28 than can be inferredfrom previous plots, such as the areawhere RUS is better than BPM. We haveshown the nonfiltering algorithms and superimposed in gray the area where the filters dominate. Therefore, in the gray areathe best choice is to use the correspondingfilter using the dominating nonfilter as itsverification engine. In the nongray area itis better to use the dominating nonfiltering algorithm directly, with no filter.A code implementing such a heuristicincluding EXP, BPD and BPP only ispublicly available from the authors Webpage.12 This combined code is fasterthan each isolated algorithm, although ofcourse it is not really a single algorithmbut the combination of the best choices.10. CONCLUSIONSWe reach the end of this tour on approximate string matching. Our goal has beento present and explain the main ideasbehind the existing algorithms, to classifythem according to the type of approachproposed, and to show how they performin practice in a subset of possible practical scenarios. We have shown that theoldest approaches, based on the dynamicprogramming matrix, yield the mostimportant theoretical developments, butin general the algorithms have beenimproved by modern developments basedon filtering and bitparallelism. In particular, the fastest algorithms combine afast filter to discard most of the text witha fast nonfilter algorithm to check forpotential matches.We show some plots summarizing thecontents of the survey. Figure 29 showsthe historical order in which the algorithms appeared in the different areas.12 httpwww.dcc.uchile.clgnavarropubcode.To apply EXP the optionep must be used.ACM Computing Surveys, Vol. 33, No. 1, March 2001.80 G. NavarroFig. 29. Historical development of the different areas. References are shortened to first letterssingle authors or initials multiple authors, and to the last two digits of years.Key Sel80  Sellers 1980, MP80  Masek and Paterson 1980, LV88  Landau and Vishkin1988, Ukk85b Ukkonen 1985b, LV89 Landau and Vishkin 1989, Mye86a Myers 1986a,GG88  Galil and Giancarlo 1988, GP90  Galil and Park 1990, CL94  Chang and Lawler1994, UW93 Ukkonen and Wood 1993, TU93 Tarhio and Ukkonen 1993, JTU96 Jokinenet al. 1996, CL92  Chang and Lampe 1992, WMM96  Wu et al. 1996, WM92b  Wu andManber 1992b, BYP96  BaezaYates and Perleberg 1996, Ukk92  Ukkonen 1992, Wri94 Wright 1994, CM94  Chang and Marr 1994, Tak94  Takaoka 1994, Mel96  Melichar1996, ST95  Sutinen and Tarhio 1995, Kur96  Kurtz 1996, BYN99  BaezaYates andNavarro 1999, Shi96  Shi 1996, GKHO97  Giegerich et al. 1997, SV97  Sahinalp andVishkin 1997, Nav97a  Navarro 1997a, CH98  Cole and Hariharan 1998, Mye99  Myers1999, NBY98a  NBY99b  Navarro and BaezaYates 1998a 1999b, and NR00  Navarro andRaffinot 2000.Figure 30 shows a worst case timespacecomplexity plot for the nonfiltering algorithms. Figure 31 considers filtrationalgorithms, showing their average casecomplexity and the maximum error level for which they work. Some practicalassumptions have been made to order thedifferent functions of k, m,  , w, and n.Approximate string matching is avery active research area, and it shouldcontinue in that status in the foreseeablefuture strong genome projects in computational biology, the pressure for oralhumanmachine communication and theheterogeneity and spelling errors presentin textual databases are just a sample ofthe reasons that drive researchers to lookfor faster and more flexible algorithms forapproximate pattern matching.It is interesting to point out theoretical and practical questions that are stillopen.ACM Computing Surveys, Vol. 33, No. 1, March 2001.A Guided Tour to Approximate String Matching 81Fig. 30. Worst case time and space complexity of nonfiltering algorithms. We replaced w by2log n. References are shortened to first letters single authors or initials multiple authors,and to the last two digits of years.Key Sel80  Sellers 1980, LV88  Landau and Vishkin 1988, WM92b  Wu and Manber1992b, GG88  Galil and Giancarlo 1988, UW93  Ukkonen and Wood 1993, GP90  Galiland Park 1990, CL94  Chang and Lawler 1994, Mye86a  Myers 1986a, LV89  Landauand Vishkin 1989, CH98  Cole and Hariharan 1998, BYN99  BaezaYates and Navarro1999, Mye99  Myers 1999, WMM96  Wu et al. 1996, MP80  Masek and Paterson 1980,and Ukk85a  Ukkonen 1985a.The exact matching probability and average edit distance between two randomstrings is a difficult open question. Wefound a new bound in this survey, butthe problem is still open.A worstcase lower bound of theproblem is clearly On, but the onlyalgorithms achieving it have space andpreprocessing cost exponential in m ork. The only improvements to the worstcase with polynomial space complexityare the Okn algorithms and, for verysmall k, On1 k4m. Is it possibleto improve the algorithms or to find abetter lower bound for this caseThe previous question also has apractical side Is it possible to find analgorithm which is Okn in the worstcase and efficient in practice Using bitparallelism, there are good practical algorithms that achieve Oknw on average and Omnw in the worst case.The lower bound of the problem forthe average case is known to beOnk log mm, and there existsan algorithm achieving it, so from thetheoretical point of view that problemis closed. However, from the practicalside, the algorithms approaching thoselimits work well only for very long patterns, while a much simpler algorithmEXP is the best for moderate andshort patterns. Is it possible to find aunified approach, good in practice andwith that theoretical complexityACM Computing Surveys, Vol. 33, No. 1, March 2001.82 G. NavarroFig. 31. Average time and maximum tolerated error level for the filtration algorithms. Referencesare shortened to first letters single authors or initials multiple authors, and to the last two digitsof years.Key BYN99  BaezaYates and Navarro 1999, NBY98a  Navarro and BaezaYates 1998a,JTU96 Jokinen et al. 1996, Ukk92 Ukkonen 1992, CL94 Chang and Lawler 1994, WM92bWu and Manber 1992b, TU93  Tarhio and Ukkonen 1993, Tak94  Takaoka 1994, Shi96  Shi1996, ST95  Sutinen and Tarhio 1995, NR00  Navarro and Raffinot 2000, and CM94  Changand Marr 1994.Another practical question on filteringalgorithms is Is it possible in practice toimprove over the current best existingalgorithmsFinally, there are many other openquestions related to offline approximate searching, which is a much lessmature area needing more research.APPENDIX A. SOME ANALYSESSince some of the source papers lack ananalysis, or they do not analyze exactlywhat is of interest to us, we provide a simple analysis. This is not the purpose ofthis survey, so we content ourselves withrough figures. In particular, our analysesare valid for  m. All refer to filtersand are organized according to the originalorder, so the reader should first read thealgorithm description to understand theterminology.A.1 Tarhio and Ukkonen 1990First, the probability of a text character being bad is that of not matching2k 1 pattern positions, i.e. Pbad1 1 2k 1 e2k 1 , so we try onaverage 1Pbad characters until we finda bad one. Since k 1 bad charactersACM Computing Surveys, Vol. 33, No. 1, March 2001.A Guided Tour to Approximate String Matching 83have to be found, we make OkPbadleave the window. On the other hand, theprobability of verifying a text windowis that of reaching its beginning. Weapproximate that probability by equatingm to the average portion of the traversedwindow kPbad, to obtain   e2k 1 .A.2 Wu and Manber 1992The Sunday algorithm can be analyzed asfollows. To see how far we can verify inthe current window, consider that k 1patterns have to fail. Each one failson average in log mk 1 charactercomparisons, but the time for all them tofail is longer. By Yaos bound Yao 1979,this cannot be less than log m. Otherwisewe could split the test of a single patterninto k 1 tests of subpatterns, and all ofthem would fail in less than log m time,breaking the lower bound. To compute theaverage shift, consider that k charactersmust be different from the last windowcharacter, and therefore the average shiftis k. The final complexity is thereforeOkn log m . This is optimistic, butwe conjecture that it is the correct complexity. An upper bound is obtained byreplacing k by k2 i.e. adding the times forall the pieces to fail.A.3 Navarro and Raffinot 1998The automaton matches the text windowwith k errors until almost surely kcharacters have been inspected so thatthe error level becomes lower than .From there on, it becomes exponentiallydecreasing on  , which can be made1 in Ok total steps. From that pointon, we are in a case of exact stringmatching and then log m characters areinspected, for a total of Ok  log m.When the window is shifted to the lastprefix that matched with k errors, thisis also at k distance from the endof the window, on average. The windowlength is m k, and therefore we shiftthe window in m k k on average.Therefore, the total amount of work isOn log mm1. Thefilter works well unless the probabilityof finding a pattern prefix with errors atthe beginning of the window is high. Thisis the same as saying that k m k,which gives   1 e 2 e .A.4 Ukkonen 1992The probability of finding a given qgramin the text window is 1  1  1 qm 1  em q . So the probability of verifyingthe text position is that of findingmq 1kq qgrams of the pattern, i.e.mq 1kq1  em q mq 1kq . This mustbe O1m2 in order not to interfere withthe search time. Taking logarithms andapproximating the combinatorials usingStirlings n  nen2n1O1n,we arrive at2 log mkq  m q  1 log1 em qlog1 em q log kq log m q  1from which, by replacing q  log m, weobtain 1log mlog   log log m O1log ma quite common result for this type of filter. The q  log m is chosen because theresult improves as q grows, but it is necessary that q  log m holds, since otherwise log 1 em q  becomes zero and theresult worsens.ACKNOWLEDGMENTSThe author thanks the many researchers in thisarea for their willingness to exchange ideas andorshare their implementations Amihood Amir,Ricardo BaezaYates, William Chang, Udi Manber,Gene Myers, Erkki Sutinen, Tadao Takaoka, JormaTarhio, Esko Ukkonen, and Alden Wright. Thereferees also provided important suggestions thatimproved the presentation.ACM Computing Surveys, Vol. 33, No. 1, March 2001.84 G. NavarroREFERENCESAHO, A. AND CORASICK, M. 1975. Efficient stringmatching an aid to bibliographic search. Commun. ACM 18, 6, 333340.AHO, A., HOPCROFT, J., AND ULLMAN, J. 1974. TheDesign and Analysis of Computer Algorithms.AddisonWesley, Reading, MA.ALTSCHUL, S., GISH, W., MILLER, W., MYERS, G., ANDLIPMAN, D. 1990. Basic local alignment searchtool. J. Mol. Biol. 215, 403410.AMIR, A., LEWENSTEIN, M., AND LEWENSTEIN, N. 1997a.Pattern matching in hypertext. In Proceedingsof the 5th International Workshop on Algorithms and Data Structures WADS 97. LNCS,vol. 1272, SpringerVerlag, Berlin, 160173.AMIR, A., AUMANN, Y., LANDAU, G., LEWENSTEIN, M.,AND LEWENSTEIN, N. 1997b. Pattern matchingwith swaps. In Proceedings of the Foundationsof Computer Science FOCS97, 1997, 144153.APOSTOLICO, A. 1985. The myriad virtues of subwordtrees. In Combinatorial Algorithms on Words.SpringerVerlag, Barlin, 8596.APOSTOLICO, A. AND GALIL, Z. 1985. Combinatorial Algorithms on Words. NATO ISI Series.SpringerVerlag, Berlin.APOSTOLICO, A. AND GALIL, Z. 1997. Pattern Matching Algorithms. Oxford University Press, Oxford, UK.APOSTOLICO, A. AND GUERRA, C. 1987. The LongestCommon Subsequence problem revisited. Algorithmica 2, 315336.ARAUJO, M., NAVARRO, G., AND ZIVIANI, N. 1997. Largetext searching allowing errors. In Proceedings ofthe 4th South American Workshop on String Processing WSP 97, Carleton Univ. Press. 220.ARLAZAROV, V., DINIC, E., KONROD, M., AND FARADZEV,I. 1975. On economic construction of the transitive closure of a directed graph. Sov. Math.Dokl. 11, 1209, 1210. Original in Russian inDokl. Akad. Nauk SSSR 194, 1970.ATALLAH, M., JACQUET, P., AND SZPANKOWSKI, W. 1993.A probabilistic approach to pattern matchingwith mismatches. Random Struct. Algor. 4, 191213.BAEZAYATES, R. 1989. Efficient Text Searching.Ph.D. thesis, Dept. of Computer Science, University of Waterloo. Also as Res. Rep. CS8917.BAEZAYATES, R. 1991. Some new results on approximate string matching. In Workshop on DataStructures, Dagstuhl, Germany. Abstract.BAEZAYATES, R. 1992. Text retrieval Theory andpractice. In 12th IFIP World Computer Congress.Elsevier Science, Amsterdam. vol. I, 465476.BAEZAYATES, R. 1996. A unified view of stringmatching algorithms. In Proceedings of the Theory and Practice of Informatics SOFSEM 96.LNCS, vol. 1175, SpringerVerlag, Berlin, 115.BAEZAYATES, R. AND GONNET, G. 1992. A new approach to text searching. Commun. ACM 35, 10,7482. Preliminary version in ACM SIGIR89.BAEZAYATES, R. AND GONNET, G. 1994. Fast stringmatching with mismatches. Information andComputation 108, 2, 187199. Preliminaryversion as Tech. Rep. CS8836, Data Structuring Group, Univ. of Waterloo, Sept. 1988.BAEZAYATES, R. AND NAVARRO, G. 1997. Multiple approximate string matching. In Proceedings of the5th International Workshop on Algorithms andData Structures WADS 97. LNCS, vol. 1272,1997, SpringerVerlag, Berlin, 174184.BAEZAYATES, R. AND NAVARRO, G. 1998. New andfaster filters for multiple approximate stringmatching. Tech. Rep. TRDCC9810, Dept.of Computer Science, University of Chile.Random Struct. Algor. to appear. ftpftp.dcc.ptuchile.clpubusersgnavarromulti.ps.gz.BAEZAYATES, R. AND NAVARRO, G. 1999. Faster approximate string matching. Algorithmica 23, 2,127158. Preliminary versions in Proceedings ofCPM 96 LNCS, vol. 1075, 1996 and in Proceedings of WSP96, Carleton Univ. Press, 1996.BAEZAYATES, R. AND NAVARRO, G. 2000. Blockaddressing indices for approximate text retrieval. J. Am. Soc. Inf. Sci. JASIS 51, 1 Jan.,6982.BAEZAYATES, R. AND PERLEBERG, C. 1996. Fast andpractical approximate pattern matching. Information Processing Letters 59, 2127. Preliminary version in CPM 92 LNCS, vol. 644. 1992.BAEZAYATES, R. AND REGNIER, M. 1990. Fast algorithms for two dimensional and multiple patternmatching. In Proceedings of Scandinavian Workshop on Algorithmic Theory SWAT 90. LNCS,vol. 447, SpringerVerlag, Berlin, 332347.BAEZAYATES, R. AND RIBEIRONETO, B. 1999. ModernInformation Retrieval. AddisonWesley, Reading, MA.BLUMER, A., BLUMER, J., HAUSSLER, D., EHRENFEUCHT,A., CHEN, M., AND SEIFERAS, J. 1985. The smallest automaton recognizing the subwords of atext. Theor. Comput. Sci. 40, 3155.BOYER, R. AND MOORE, J. 1977. A fast string searching algorithm. Commun. ACM 20, 10, 762772.CHANG, W. AND LAMPE, J. 1992. Theoretical andempirical comparisons of approximate stringmatching algorithms. In Proceedings of the 3dAnnual Symposium on Combinatorial PatternMatching CPM 92. LNCS, vol. 644, SpringerVerlag, Berlin, 172181.CHANG, W. AND LAWLER, E. 1994. Sublinear approximate string matching and biological applications. Algorithmica 12, 45, 327344. Preliminary version in FOCS 90.CHANG, W. AND MARR, T. 1994. Approximate stringmatching and local similarity. In Proceedings ofthe 5th Annual Symposium on CombinatorialPattern Matching CPM 94. LNCS, vol. 807,SpringerVerlag, Berlin, 259273.ACM Computing Surveys, Vol. 33, No. 1, March 2001.For Evaluation Only.Copyright c by Foxit Software Company, 2004Edited by Foxit PDF EditorA Guided Tour to Approximate String Matching 85CHVATAL, V. AND SANKOFF, D. 1975. Longest common subsequences of two random sequences.J. Appl. Probab. 12, 306315.COBBS, A. 1995. Fast approximate matching usingsuffix trees. In Proceedings of the 6th AnnualSymposium on Combinatorial Pattern MatchingCPM 95, 4154.COLE, R. AND HARIHARAN, R. 1998. Approximatestring matching a simpler faster algorithm. InProceedings of the 9th ACMSIAM Symposiumon Discrete Algorithms SODA 98, 463472.COMMENTZWALTER, B. 1979. A string matching algorithm fast on the average. In Proc. ICALP 79.LNCS, vol. 6, SpringerVerlag, Berlin, 118132.CORMEN, T., LEISERSON, C., AND RIVEST, R. 1990. Introduction to Algorithms. MIT Press, Cambridge,MA.CROCHEMORE, M. 1986. Transducers and repetitions. Theor. Comput. Sci. 45, 6386.CROCHEMORE, M. AND RYTTER, W. 1994. Text Algorithms. Oxford Univ. Press, Oxford, UK.CROCHEMORE, M., CZUMAJ, A., GASIENIEC, L., JAROMINEK,S., LECROQ, T., PLANDOWSKI, W., AND RYTTER, W.1994. Speeding up two stringmatching algorithms. Algorithmica 12, 247267.DAMERAU, F. 1964. A technique for computer detection and correction of spelling errors. Commun.ACM 7, 3, 171176.DAS, G., FLEISHER, R., GASIENIEK, L., GUNOPULOS,D., AND KARKAINEN, J. 1997. Episode matching.In Proceedings of the 8th Annual Symposiumon Combinatorial Pattern Matching CPM 97.LNCS, vol. 1264, SpringerVerlag, Berlin, 1227.DEKEN, J. 1979. Some limit results for longest common subsequences. Discrete Math. 26, 1731.DIXON, R. AND MARTIN, T. Eds. 1979. AutomaticSpeech and Speaker Recognition. IEEE Press,New York.EHRENFEUCHT, A. AND HAUSSLER, D. 1988. A new distance metric on strings computable in lineartime. Discrete Appl. Math. 20, 191203.ELLIMAN, D. AND LANCASTER, I. 1990. A review of segmentation and contextual analysis techniquesfor text recognition. Pattern Recog. 23, 34, 337346.FRENCH, J., POWELL, A., AND SCHULMAN, E. 1997. Applications of approximate word matching in information retrieval. In Proceedings of the 6thACM International Conference on Informationand Knowledge Management CIKM 97, 915.GALIL, Z. AND GIANCARLO, R. 1988. Data structuresand algorithms for approximate string matching. J. Complexity 4, 3372.GALIL, Z. AND PARK, K. 1990. An improved algorithm for approximate string matching. SIAMJ. Comput. 19, 6, 989999. Preliminary versionin ICALP 89 LNCS, vol. 372, 1989.GIEGERICH, R., KURTZ, S., HISCHKE, F., AND OHLEBUSCH,E. 1997. A general technique to improve filteralgorithms for approximate string matching. InProceedings of the 4th South American Workshop on String Processing WSP 97. CarletonUniv. Press. 3852. Preliminary version asTech. Rep. 9601, Universitat Bielefeld, Germany, 1996.GONNET, G. 1992. A tutorial introduction to Computational Biochemistry using Darwin. Tech. rep.,Informatik E. T. H., Zuerich, Switzerland.GONNET, G. AND BAEZAYATES, R. 1991. Handbook ofAlgorithms and Data Structures, 2d ed. AddisonWesley, Reading, MA.GONZALEZ, R. AND THOMASON, M. 1978. Syntactic Pattern Recognition. AddisonWesley, Reading, MA.GOSLING, J. 1991. A redisplay algorithm. In Proceedings of ACM SIGPLANSIGOA Symposium onText Manipulation, 123129.GROSSI, R. AND LUCCIO, F. 1989. Simple and efficientstring matching with k mismatches. Inf. Process.Lett. 33, 3, 113120.GUSFIELD, D. 1997. Algorithms on Strings, Trees andSequences. Cambridge Univ. Press, Cambridge.HALL, P. AND DOWLING, G. 1980. Approximate stringmatching. ACM Comput. Surv. 12, 4, 381402.HAREL, D. AND TARJAN, E. 1984. Fast algorithmsfor finding nearest common ancestors. SIAM J.Comput. 13, 2, 338355.HECKEL, P. 1978. A technique for isolating differences between files. Commun. ACM 21, 4, 264268.HOLSTI, N. AND SUTINEN, E. 1994. Approximatestring matching using qgram places. In Proceedings of 7th Finnish Symposium on ComputerScience. Univ. of Joensuu. 2332.HOPCROFT, J. AND ULLMAN, J. 1979. Introduction toAutomata Theory, Languages and Computation.AddisonWesley, Reading, MA.HORSPOOL, R. 1980. Practical fast searching instrings. Software Practice Exper. 10, 501506.JOKINEN, P. AND UKKONEN, E. 1991. Two algorithmsfor approximate string matching in static texts.In Proceedings of the 2nd Mathematical Foundations of Computer Science MFCS 91. SpringerVerlag, Berlin, vol. 16, 240248.JOKINEN, P., TARHIO, J., AND UKKONEN, E. 1996. A comparison of approximate string matching algorithms. Software Practice Exper. 26, 12, 14391458. Preliminary version in Tech. Rep. A19917, Dept. of Computer Science, Univ. of Helsinki,1991.KARLOFF, H. 1993. Fast algorithms for approximately counting mismatches. Inf. Process.Lett. 48, 5360.KECECIOGLU, J. AND SANKOFF, D. 1995. Exact andapproximation algorithms for the inversiondistance between two permutations. Algorithmica 13, 180210.KNUTH, D. 1973. The Art of Computer Programming, Volume 3 Sorting and Searching.AddisonWesley, Reading, MA.ACM Computing Surveys, Vol. 33, No. 1, March 2001.For Evaluation Only.Copyright c by Foxit Software Company, 2004Edited by Foxit PDF Editor86 G. NavarroKNUTH, D., MORRIS, J., JR, AND PRATT, V. 1977. Fastpattern matching in strings. SIAM J. Comput. 6, 1, 323350.KUKICH, K. 1992. Techniques for automatically correcting words in text. ACM Comput. Surv. 24, 4,377439.KUMAR, S. AND SPAFFORD, E. 1994. A patternmatching model for intrusion detection. In Proceedings of the National Computer Security Conference, 1121.KURTZ, S. 1996. Approximate string searching under weighted edit distance. In Proceedings of the3rd South American Workshop on String Processing WSP 96. Carleton Univ. Press. 156170.KURTZ, S. AND MYERS, G. 1997. Estimating the probability of approximate matches. In Proceedingsof the 8th Annual Symposium on CombinatorialPattern Matching CPM 97. LNCS, vol. 1264,SpringerVerlag, Berlin, 5264.LANDAU, G. AND VISHKIN, U. 1988. Fast string matching with k differences. J. Comput. Syst. Sci. 37,6378. Preliminary version in FOCS 85.LANDAU, G. AND VISHKIN, U. 1989. Fast parallel andserial approximate string matching. J. Algor. 10,157169. Preliminary version in ACM STOC 86.LANDAU, G., MYERS, E., AND SCHMIDT, J. 1998. Incremental string comparison. SIAM J. Comput. 27, 2, 557582.LAWRENCE, S. AND GILES, C. L. 1999. Accessibility ofinformation on the web. Nature 400, 107109.LEE, J., KIM, D., PARK, K., AND CHO, Y. 1997. Efficient algorithms for approximate string matching with swaps. In Proceedings of the 8th AnnualSymposium on Combinatorial Pattern MatchingCPM 97. LNCS, vol. 1264, SpringerVerlag,Berlin, 2839.LEVENSHTEIN, V. 1965. Binary codes capable of correcting spurious insertions and deletions of ones.Probl. Inf. Transmission 1, 817.LEVENSHTEIN, V. 1966. Binary codes capable of correcting deletions, insertions and reversals. Sov.Phys. Dokl. 10, 8, 707710. Original in Russianin Dokl. Akad. Nauk SSSR 163, 4, 845848,1965.LIPTON, R. AND LOPRESTI, D. 1985. A systolic array for rapid string comparison. In Proceedingsof the Chapel Hill Conference on VLSI, 363376.LOPRESTI, D. AND TOMKINS, A. 1994. On the searchability of electronic ink. In Proceedings of the4th International Workshop on Frontiers inHandwriting Recognition, 156165.LOPRESTI, D. AND TOMKINS, A. 1997. Block edit models for approximate string matching. Theor.Comput. Sci. 181, 1, 159179.LOWRANCE, R. AND WAGNER, R. 1975. An extension of the stringtostring correction problem.J. ACM 22, 177183.LUCZAK, T. AND SZPANKOWSKI, W. 1997. A suboptimallossy data compression based on approximatepattern matching. IEEE Trans. Inf. Theor. 43,14391451.MANBER, U. AND WU, S. 1994. GLIMPSE A tool tosearch through entire file systems. In Proceedings of USENIX Technical Conference. USENIXAssociation, Berkeley, CA, USA. 2332. Preliminary version as Tech. Rep. 9334, Dept. of Computer Science, Univ. of Arizona, Oct. 1993.MASEK, W. AND PATERSON, M. 1980. A faster algorithm for computing string edit distances. J.Comput. Syst. Sci. 20, 1831.MASTERS, H. 1927. A study of spelling errors. Univ.of Iowa Studies in Educ. 4, 4.MCCREIGHT, E. 1976. A spaceeconomical suffix treeconstruction algorithm. J. ACM 23, 2, 262272.MELICHAR, B. 1996. String matching with k differences by finite automata. In Proceedings of theInternational Congress on Pattern RecognitionICPR 96. IEEE CS Press, Silver Spring, MD.256260. Preliminary version in Computer Analysis of Images and Patterns LNCS, vol. 970,1995.MORRISON, D. 1968. PATRICIAPractical algorithm to retrieve information coded in alphanumeric. J. ACM 15, 4, 514534.MUTH, R. AND MANBER, U. 1996. Approximate multiple string search. In Proceedings of the 7thAnnual Symposium on Combinatorial PatternMatching CPM 96. LNCS, vol. 1075, SpringerVerlag, Berlin, 7586.MYERS, G. 1994a. A sublinear algorithm for approximate keyword searching. Algorithmica 12, 45,345374. Perliminary version in Tech. Rep.TR9025, Computer Science Dept., Univ. of Arizona, Sept. 1991.MYERS, G. 1994b. Algorithmic Advances for Searching Biosequence Databases. Plenum Press, NewYork, 121135.MYERS, G. 1986a. Incremental alignment algorithms and their applications. Tech. Rep. 8622,Dept. of Computer Science, Univ. of Arizona.MYERS, G. 1986b. An ON D difference algorithmand its variations. Algorithmica 1, 251266.MYERS, G. 1991. An overview of sequence comparison algorithms in molecular biology. Tech. Rep.TR9129, Dept. of Computer Science, Univ. ofArizona.MYERS, G. 1999. A fast bitvector algorithm for approximate string matching based on dynamicprogamming. J. ACM 46, 3, 395415. Earlier version in Proceedings of CPM 98 LNCS, vol. 1448.NAVARRO, G. 1997a. Multiple approximate stringmatching by counting. In Proceedings of the 4thSouth American Workshop on String ProcessingWSP 97. Carleton Univ. Press, 125139.NAVARRO, G. 1997b. A partial deterministic automaton for approximate string matching. In Proceedings of the 4th South American Workshopon String Processing WSP 97. Carleton Univ.Press, 112124.ACM Computing Surveys, Vol. 33, No. 1, March 2001.Very relevantVery relevantFor Evaluation Only.Copyright c by Foxit Software Company, 2004Edited by Foxit PDF EditorMyers Alg.A Guided Tour to Approximate String Matching 87NAVARRO, G. 1998. Approximate Text Searching.Ph.D. thesis, Dept. of Computer Science, Univ. ofChile. Tech. Rep. TRDCC9814. ftpftp.dcc.uchile.clpubusersgnavarrothesis98.ps.gz.NAVARRO, G. 2000a. Improved approximate patternmatching on hypertext. Theor. Comput. Sci.,237, 455463. Previous version in Proceedingsof LATIN 98 LNCS, vol. 1380.NAVARRO, G. 2000b. Nrgrep A fast and flexible pattern matching tool, Tech. Rep. TRDCC20003.Dept. of Computer Science, Univ. of Chile, Aug.ftpftp.dcc.uchile.clpubusersgnavarronrgrep.ps.gz.NAVARRO, G. AND BAEZAYATES, R. 1998a. Improving an algorithm for approximatepattern matching. Tech. Rep. TRDCC985, Dept. of Computer Science, Univ.of Chile. Algorithmica, to appear. ftpftp.dcc.uchile.clpubusersgnavarrodexp.ps.gz.NAVARRO, G. AND BAEZAYATES, R. 1998b. A practicalqgram index for text retrieval allowing errors.CLEI Electron. J. 1, 2. httpwww.clei.cl.NAVARRO, G. AND BAEZAYATES, R. 1999a. Fast multidimensional approximate pattern matching. InProceedings of the 10th Annual Symposiumon Combinatorial Pattern Matching CPM 99.LNCS, vol. 1645, Springerverlag, Berlin, 243257. Extended version to appear in J. Disc. Algor.JDA.NAVARRO, G. AND BAEZAYATES, R. 1999b. A new indexing method for approximate string matching.In Proceedings of the 10th Annual Symposiumon Combinatorial Pattern Matching CPM 99,LNCS, vol. 1645, Springerverlag, Berlin, 163185. Extended version to appear in J. DiscreteAlgor. JDA.NAVARRO, G. AND BAEZAYATES, R. 1999c. Very fastand simple approximate string matching. Inf.Process. Lett. 72, 6570.NAVARRO, G. AND RAFFINOT, M. 2000. Fast and flexiblestring matching by combining bitparallelismand suffix automata. ACM J. Exp. Algor. 5, 4.Previous version in Proceedings of CPM 98.Lecture Notes in Computer Science, SpringerVerlag, New York.NAVARRO, G., MOURA, E., NEUBERT, M., ZIVIANI, N., ANDBAEZAYATES, R. 2000. Adding compression toblock addressing inverted indexes. Kluwer Inf.Retrieval J. 3, 1, 4977.NEEDLEMAN, S. AND WUNSCH, C. 1970. A generalmethod applicable to the search for similaritiesin the amino acid sequences of two proteins. J.Mol. Biol. 48, 444453.NESBIT, J. 1986. The accuracy of approximate stringmatching algorithms. J. Comput.Based Instr. 13, 3, 8083.OWOLABI, O. AND MCGREGOR, R. 1988. Fast approximate string matching. Software Practice Exper. 18, 4, 387393.REGNIER, M. AND SZPANKOWSKI, W. 1997. On theapproximate pattern occurrence in a text. InProceedings of Compression and Complexity ofSEQUENCES 97. IEEE Press, New York.RIVEST, R. 1976. Partialmatch retrieval algorithms.SIAM J. Comput. 5, 1.SAHINALP, S. AND VISHKIN, U. 1997. Approximate pattern matching using locally consistent parsing. Manuscript, Univ. of Maryland Institute forAdvanced Computer Studies UMIACS.SANKOFF, D. 1972. Matching sequences under deletioninsertion constraints. In Proceedings of theNational Academy of Sciences of the USA, vol. 69,46.SANKOFF, D. AND KRUSKAL, J., Eds. 1983. Time Warps,String Edits, and Macromolecules The Theoryand Practice of Sequence Comparison. AddisonWesley, Reading, MA.SANKOFF, D. AND MAINVILLE, S. 1983. Common Subsequences and Monotone Subsequences. AddisonWesley, Reading, MA, 363365.SCHIEBER, B. AND VISHKIN, U. 1988. On finding lowest common ancestors simplification and parallelization. SIAM J. Comput. 17, 6, 12531262.SELLERS, P. 1974. On the theory and computation ofevolutionary distances. SIAM J. Appl. Math. 26,787793.SELLERS, P. 1980. The theory and computation ofevolutionary distances pattern recognition. J.Algor. 1, 359373.SHI, F. 1996. Fast approximate string matchingwith qblocks sequences. In Proceedings of the3rd South American Workshop on String Processing WSP96. Carleton Univ. Press. 257271.SUNDAY, D. 1990. A very fast substring search algorithm. Commun. ACM 33, 8, 132142.SUTINEN, E. 1998. Approximate Pattern Matchingwith the qGram Family. Ph.D. thesis, Dept. ofComputer Science, Univ. of Helsinki, Finland.Tech. Rep. A19983.SUTINEN, E. AND TARHIO, J. 1995. On using qgram locations in approximate string matching. In Proceedings of the 3rd Annual European Symposium on Algorithms ESA 95. LNCS, vol. 979,SpringerVerlag, Berlin, 327340.SUTINEN, E. AND TARHIO, J. 1996. Filtration with qsamples in approximate string matching. In Proceedings of the 7th Annual Symposium on Combinatorial Pattern Matching CPM 96. LNCS,vol. 1075, SpringerVerlag, Berlin, 5061.TAKAOKA, T. 1994. Approximate pattern matchingwith samples. In Proceedings of ISAAC 94.LNCS, vol. 834, SpringerVerlag, Berlin, 234242.TARHIO, J. AND UKKONEN, E. 1988. A greedy approximation algorithm for constructing shortest common superstrings. Theor. Comput. Sci. 57, 131145.ACM Computing Surveys, Vol. 33, No. 1, March 2001.For Evaluation Only.Copyright c by Foxit Software Company, 2004Edited by Foxit PDF EditorText search Dyn Prog. alg.88 G. NavarroTARHIO, J. AND UKKONEN, E. 1993. ApproximateBoyerMoore string matching. SIAM J. Comput. 22, 2, 243260. Preliminary version inSWAT90 LNCS, vol. 447, 1990.TICHY, W. 1984. The stringtostring correctionproblem with block moves. ACM Trans. Comput.Syst. 2, 4, 309321.UKKONEN, E. 1985a. Algorithms for approximatestring matching. Information and Control 64,100118. Preliminary version in Proceedingsof the International Conference Foundationsof Computation Theory LNCS, vol. 158,1983.UKKONEN, E. 1985b. Finding approximate patternsin strings. J. Algor. 6, 132137.UKKONEN, E. 1992. Approximate string matchingwith qgrams and maximal matches. Theor.Comput. Sci. 1, 191211.UKKONEN, E. 1993. Approximate string matchingover suffix trees. In Proceedings of the 4thAnnual Symposium on Combinatorial PatternMatching CPM 93, 228242.UKKONEN, E. 1995. Constructing suffix trees online in linear time. Algorithmica 14, 3, 249260.UKKONEN, E. AND WOOD, D. 1993. Approximatestring matching with suffix automata. Algorithmica 10, 353364. Preliminary version in Rep.A19904, Dept. of Computer Science, Univ. ofHelsinki, Apr. 1990.ULLMAN, J. 1977. A binary ngram technique for automatic correction of substitution, deletion, insertion and reversal errors in words. Comput.J. 10, 141147.VINTSYUK, T. 1968. Speech discrimination by dynamic programming. Cybernetics 4, 5258.WAGNER, R. AND FISHER, M. 1974. The string to stringcorrection problem. J. ACM 21, 168178.WATERMAN, M. 1995. Introduction to ComputationalBiology. Chapman and Hall, London.WEINER, P. 1973. Linear pattern matching algorithms. In Proceedings of IEEE Symposium onSwitching and Automata Theory, 111.WRIGHT, A. 1994. Approximate string matching using withinword parallelism. Software PracticeExper. 24, 4, 337362.WU, S. AND MANBER, U. 1992a. Agrepa fast approximate patternmatching tool. In Proceedings ofUSENIX Technical Conference. USENIX Association, Berkeley, CA, USA. 153162.WU, S. AND MANBER, U. 1992b. Fast text searchingallowing errors. Commun. ACM 35, 10, 8391.WU, S., MANBER, U., AND MYERS, E. 1995. A subquadratic algorithm for approximate regular expression matching. J. Algor. 19, 3, 346360.WU, S., MANBER, U., AND MYERS, E. 1996. A subquadratic algorithm for approximate limitedexpression matching. Algorithmica 15, 1, 5067. Preliminary version as Tech. Rep. TR2936,Computer Science Dept., Univ. of Arizona, 1992.YAO, A. 1979. The complexity of pattern matchingfor a random string. SIAM J. Comput. 8, 368387.YAP, T., FRIEDER, O., AND MARTINO, R. 1996. High Performance Computational Methods for BiologicalSequence Analysis. Kluwer Academic Publishers, Dordrecht.ZOBEL, J. AND DART, P. 1996. Phonetic string matching lessons from information retrieval. In Proceedings of the 19th ACM International Conference on Information Retrieval SIGIR 96, 166172.Received August 1999 revised March 2000 accepted May 2000ACM Computing Surveys, Vol. 33, No. 1, March 2001.For Evaluation Only.Copyright c by Foxit Software Company, 2004Edited by Foxit PDF Editor
