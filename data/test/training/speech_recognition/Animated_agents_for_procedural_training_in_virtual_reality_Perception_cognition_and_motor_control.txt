Applied Artificial Intelligence 13343382, 1999.Received December 1997 revised May 1998.Animated Agents for Procedural Training in Virtual RealityPerception, Cognition, and Motor ControlJe Rickel and W. Lewis JohnsonInformation Sciences Institute  Computer Science DepartmentUniversity of Southern California4676 Admiralty Way, Marina del Rey, CA 902926695rickelisi.edu, johnsonisi.eduhttpwww.isi.eduisdVETvet.htmlAbstractThis paper describes Steve, an animated agent that helps students learn to perform physical, procedural tasks. The student and Steve cohabit a threedimensional,simulated mockup of the students work environment. Steve can demonstrate how toperform tasks and can also monitor students while they practice tasks, providing assistance when needed. This paper describes Steves architecture in detail, includingperception, cognition, and motor control. The perception module monitors the state ofthe virtual world, maintains a coherent representation of it, and provides this information to the cognition and motor control modules. The cognition module interprets itsperceptual input, chooses appropriate goals, constructs and executes plans to achievethose goals, and sends out motor commands. The motor control module implementsthese motor commands, controlling Steves voice, locomotion, gaze, and gestures, andallowing Steve to manipulate objects in the virtual world.1 IntroductionTo master complex tasks, such as operating complicated machinery, people need handsonexperience facing a wide range of situations. They also need a mentor that can demonstrate procedures, answer questions, and monitor their performance, and they may needteammates if their task requires multiple people. Since it is often impractical to providesuch training on real equipment, we are exploring the use of virtual reality instead trainingtakes place in a threedimensional, interactive, simulated mockup of the students workenvironment. Since mentors and teammates are often unavailable when the student needsthem, we are developing an autonomous, animated agent that can play these roles. Theagents name is Steve Soar Training Expert for Virtual Environments.Steve integrates methods from three primary research areas intelligent tutoring systems, computer graphics, and agent architectures. This novel combination results in aunique set of capabilities. Steve has many pedagogical capabilities one would expect of anintelligent tutoring system. For example, he can answer questions such as What shouldI do next and Why. However, because he has an animated body, and cohabits thevirtual world with students, he can provide more humanlike assistance than previous disembodied tutors. For example, he can demonstrate actions, use gaze and gestures to directthe students attention, and guide the student around the virtual world. Virtual reality is an1important application area for articial intelligence because it allows more humanlike interactions among synthetic agents and humans than desktop interfaces can. Finally, Stevesagent architecture allows him to robustly handle a dynamic virtual world, potentially populated with people and other agents he continually monitors the state of the virtual world,always maintaining a plan for completing his current task, and revising the plan to handleunexpected events.Steve consists of a set of domainindependent capabilities that utilize a declarative representation of domain knowledge. To teach students about the tasks in a new domain,someone must provide the appropriate domain knowledge. We assume that this person willbe a course author, a person with enough domain knowledge to create a course for teachingothers. Importantly, we do not assume that this person has any programming skills. Ensuring that Steve only relies on types of knowledge that a course author can provide imposesstrong constraints on Steves design.Steve is designed to coexist with other people and agents in a virtual world. Our goal isto support team training, where teams of people, possibly at dierent locations, can inhabitthe same virtual world and learn to perform tasks as a team. Agents like Steve can playtwo roles in such training they can serve as tutors for individual team members, and theycan play the role of missing team members. We have recently extended Steve to understandteam tasks and function as a team member. We will not address those issues in this paperhere, we focus primarily on Steves ability to work with a single student on a onepersontask. However, as will become clear, ensuring that Steve can function in an environmentwith other people and agents has placed important constraints on Steves design.This paper describes Steves architecture in detail, including perception, cognition, andmotor control. First, Section 2 illustrates Steves capabilities via an example of Steve anda student working together on a task. Next, as background, Section 3 briey describesthe larger software architecture for virtual worlds of which Steve is a part more detail isavailable in an earlier paper Johnson et al. 1998. Finally, Section 4 gives an overview ofSteves architecture, and the remainder of the paper provides the details.2 Steves CapabilitiesTo illustrate Steves capabilities, suppose Steve is demonstrating how to inspect a highpressure air compressor aboard a ship. The students headmounted display gives her athreedimensional view of her shipboard surroundings, which include the compressor infront of her and Steve at her side. As she moves or turns her head, her view changesaccordingly. Her headmounted display is equipped with a microphone to allow her tospeak to Steve.After introducing the task, Steve begins the demonstration. I will now check the oillevel, Steve says, and he moves over to the dipstick. Steve looks down at the dipstick,points at it, looks back at the student, and says First, pull out the dipstick. Steve pullsit out see Figure 1. Pointing at the level indicator, Steve says Now we can check the oillevel on the dipstick. As you can see, the oil level is normal. To nish the subtask, Stevesays Next, insert the dipstick and he pushes it back in.Continuing the demonstration, Steve says Make sure all the cutout valves are open.Looking at the cutout valves, Steve sees that all of them are already open except one.Pointing to it, he says Open cutout valve three, and he opens it.Next, Steve says I will now perform a functional test of the drain alarm light. First,2Figure 1 Steve pulling out a dipstick3Figure 2 Steve describing a power lightcheck that the drain monitor is on. As you can see, the power light is illuminated, sothe monitor is on see Figure 2. The student, realizing that she has seen this procedurebefore, says Let me nish. Steve acknowledges that she can nish the task, and he shiftsto monitoring her performance.The student steps forward to the relevant part of the compressor, but is unsure of whatto do rst. What should I do next she asks. Steve replies I suggest that you pressthe function test button. The student asks Why Steve replies That action is relevantbecause we want the drain monitor in test mode. The student, wondering why the drainmonitor should be in test mode, asks Why again. Steve replies That goal is relevantbecause it will allow us to check the alarm light. Finally, the student understands, but sheis unsure which button is the function test button. Show me how to do it she requests.Steve moves to the function test button and pushes it see Figure 3. The alarm light comeson, indicating to Steve and the student that it is functioning properly. Now the studentrecalls that she must extinguish the alarm light, but she pushes the wrong button, causinga dierent alarm light to illuminate. Flustered, she asks Steve What should I do next4Figure 3 Steve pressing a buttonSteve responds I suggest that you press the reset button on the temperature monitor.She presses the reset button to extinguish the second alarm light, then presses the correctbutton to extinguish the rst alarm light. Steve looks at her and says That completes thetask. Any questionsThe student only has one question. She asks Steve why he opened the cutout valve.1That action was relevant because I wanted to dampen oscillation of the stage three gaugehe replies.This example illustrates a number of Steves capabilities. He can generate and recognize speech, demonstrate actions, use gaze and gestures, answer questions, adapt domainprocedures to unexpected events, and remember past actions. The remainder of the paperdescribes the technical details behind these capabilities.1Such afteraction review questions are posed via a desktop menu, not speech. Steve generates menu itemsfor all the actions he performed, and the student simply selects one. A speech interface for afteraction reviewwould require more sophisticated speech understanding.5  Human InterfaceSimulator Steve     Speech Recognition Message Dispatcher Visual Interface  Speech GenerationAudio EffectsFigure 4 An architecture for virtual worlds. Although the gure only shows componentsfor one agent and one human, other agents and humans can be added by simply connectingthem to the message dispatcher in the same way.3 Creating Virtual Worlds for People and AgentsBefore we can discuss Steves architecture, we must introduce a software architecture forcreating virtual worlds that people and agents can cohabit Figure 4. With our colleaguesfrom Lockheed Martin Corporation and the USC Behavioral Technologies Laboratory, wehave designed and implemented such an architecture Johnson et al. 1998. For purposesof modularity and eciency, the architecture consists of separate components running inparallel as separate processes, possibly on dierent machines. The components communicate by exchanging messages. Our current architecture includes the following types ofcomponentsSimulator The behavior of the virtual world is controlled by a simulator. Our current implementation uses the VIVIDS simulation engine Munro  Surmon 1997, developedat the USC Behavioral Technologies Laboratory.2Visual Interface Each human participant has a visual interface component that allowsthem to view and manipulate the virtual world. The person is connected to thiscomponent via several hardware devices their view into the world is provided by aheadmounted display, their movements are tracked by position sensors on their headand hands, and they interact with the world by touching virtual objects using adata glove. They can also pinch objects using a pinch glove or click on objects usinga 3D mouse these actions are all treated the same by the visual interface component,which supports all these alternative devices. The visual interface component playstwo primary roles It receives messages from the other components primarily the simulator describing changes in the appearance of the world, and it outputs a threedimensional2VIVIDS is a descendant of the RIDES and VRIDES systems mentioned in our earlier papers.6graphical representation through the persons headmounted display. It informs the other components when the person interacts with objects.Our current implementation uses Lockheed Martins Vista Viewer Stiles, McCarthy, Pontecorvo 1995 as the visual interface component.Audio Each human participant has an audio component. This component receives messages from the simulator describing the location and audible radius of various sounds,and it broadcasts appropriate sounds to the headphones on the persons headmounteddisplay.Speech Generation Each human participant has a speech generation component that receives text messages from other components primarily agents, converts the text tospeech, and broadcasts the speech to the persons headphones. Our current implementation uses Entropics TrueTalkTM texttospeech product.Speech Recognition Each human participant has a speech recognition component thatreceives speech signals via the persons microphone, recognizes the speech as a paththrough its grammar, and outputs a semantic token representing the speech to theother components. Steve agents do not have any natural language understandingcapabilities, so they have no need for the recognized sentence. Our current implementation uses Entropics GrapHViteTM product.Agent Each Steve agent runs as a separate component. The remainder of this paperfocuses on the architecture of these agents and how they communicate with the othercomponents.The various components do not communicate directly. Instead, all messages are sent toa central message dispatcher. Each component tells the dispatcher the types of messagesin which it is interested. Then, when a message arrives, the dispatcher forwards it to allinterested components. For example, each visual interface component registers interest inmessages that specify changes in the appearance of the virtual world e.g., a change in thecolor or location of an object. When the simulator sends such a message, the dispatcherbroadcasts it to every visual interface component. This approach increases modularity,since one component need not know the interface to other components. It also increasesextensibility, since new components can be added without aecting existing ones. Ourcurrent implementation uses Suns ToolTalkTM as the message dispatcher.4 Overview of Steves Architecture4.1 Perception, Cognition, and Motor ControlSteve consists of three main modules perception, cognition, and motor control Figure 5.The perception module monitors messages from the message dispatcher and identies eventsthat are relevant to Steve, such as actions taken in the virtual world by people and agentsand changes in the state of the virtual world. The cognition module interprets the inputit receives from the perception module, chooses appropriate goals, constructs and executesplans to achieve those goals, and sends out motor commands to control the agents body.The motor control module decomposes these motor commands into a sequence of lowerlevelcommands that are sent to other components via the message dispatcher. For example, upon7   Message DispatcherMotor ControlCognitionPerceptionSpatial informationPerception snapshot,   important eventsRelevant eventsAbstract motor commandsDetailed motor commandsSteveFigure 5 The three main modules in Steve and the types of information they send andreceive.8receiving a motor command to push a button in the virtual world, the motor control modulewould send animation primitives to cause Steves graphical nger to move to the buttonand would then send a message to the simulator to simulate the eects of the button beingpressed.In our current implementation, cognition runs as one process, and perception and motorcontrol run in a separate process. This split has two advantages. First, it allows eachmodule to be implemented in a suitable language. The cognition module is built on topof Soar Laird, Newell,  Rosenbloom 1987 Newell 1990, which is intended as a generalarchitecture for cognition most of Steves cognitive capabilities are implemented in Soarproduction rules. In contrast, the perception and motor control modules are implementedin procedural languages, namely TclTk and C. The second advantage of the split is thatcognition can run in parallel with perception and motor control. This is especially importantwhen there is a high volume of message trac arriving at the perception module, as wouldbe the case for a highly dynamic world we do not want the perceptual processing to slowdown cognition. If the motor control module were computationally expensive, it might payto run perception and motor control as separate, parallel processes as well, but this has notbeen the case so far.The perception, cognition, and motor control modules communicate directly, not via themessage dispatcher. The cognition module communicates with the other two by messagepassing. It sends a message to the perception module when it is ready for an update on thestate of the virtual world the perception module responds with a snapshot of the state ofthe world and a set of important events that occurred since the last snapshot it sent e.g.,actions taken by people and agents. The cognition module also sends motor commandmessages to the motor control module. The motor control module resides in the sameprocess as the perception module, so it accesses perceptual information freely via procedurecalls and shared variables.4.2 Domain KnowledgeTo allow Steve to operate in a variety of domains, his architecture has a clean separationbetween domainindependent capabilities and domainspecic knowledge. The code in theperception, cognition, and motor control modules provides a set of general capabilities thatare independent of any particular domain. To allow Steve to operate in a new domain, acourse author simply species the appropriate domain knowledge in a declarative language.This declarative language was designed to be used by people with domain expertise but notnecessarily any programming skills. Steves general capabilities draw on the knowledge toteach it to students. The domain knowledge that Steve requires falls in two categoriesPerceptual Knowledge This knowledge tells Steve about the objects in the virtual world,their relevant simulator attributes, and their spatial properties. It resides in theperception module and will be discussed in Section 5.Task Knowledge This knowledge tells Steve about the procedures for accomplishing domain tasks and provides text fragments so that he can talk about them. It resides inthe cognition module and will be discussed in Section 6.95 PerceptionThe role of the perception module is to receive messages from other components via themessage dispatcher, use these messages to maintain a coherent representation of the stateof the virtual world, and to provide this information to the cognition and motor controlmodules. This section describes the representation that the perception module maintainsand how it obtains the information, thus illustrating the types of information available toan agent in virtual reality.5.1 Representing the State of the Virtual World5.1.1 Representing the Simulator StateMost information about the state of the virtual world is maintained by the simulator. Theperception module represents the simulator state as a set of attributevalue pairs. Eachattribute represents a state variable in the simulator, and the attributes value representsthe value of the variable. For example, the state of an indicator light, say light1, mightbe represented with the attribute light1state with possible values on and off. Thissimple representation was chosen to allow Steve to operate with a variety of simulatorswhile some simulators allow more sophisticated objectoriented representations, nearly allof them support this simple attributevalue representation.The perception module tracks the simulator state by listening for messages from thesimulator via the message dispatcher. The perceptual knowledge provided to Steve bythe course author includes a list of all relevant attributes. When Steve starts up, theperception module asks the simulator for the current value of each one. It also informs themessage dispatcher that it is interested in messages describing changes in these attributes.The simulator broadcasts messages whenever the simulation state changes. Each messagespecies the name of an attribute that changed and its new value.The perception module uses these messages to maintain a snapshot of the simulationstate. The cognition module periodically asks for this snapshot, so the perception modulemust always have one ready to be sent. After the perception module initializes its snapshot,it can simply update it whenever it receives a message from the simulator, except for onecomplication some groups of messages from the simulator represent simultaneous changes.For example, suppose that a light should be illuminated whenever a button is depressed.When the button is pressed, the simulator will send two messages one specifying that thebutton is depressed, and another specifying that the light is on. If the perception modulewere to update the simulation snapshot after each message, the cognition module mightask for a snapshot before both messages have been received and processed, and hence itcould receive an inconsistent state of the world. This situation is analogous to a databasetransaction Korth  Silberschatz 1986 either the cognition module should see the eectsof all the simultaneous changes, or it should not see the eects of any of them.To avoid this possibility, the simulator must use start and end messages to delimitmessages representing simultaneous changes. After receiving a start message, the perceptionmodule stores subsequent simulator messages on a queue. When the end message arrives, theperception module updates the simulation snapshot by processing all the queued messages.This update is atomic the cognition module cannot ask for a snapshot during the update.Thus, if the cognition module asks for a snapshot before the end message arrives, it seesnone of the changes if it asks for a snapshot after the end message arrives, it sees all ofthem.105.1.2 Representing Spatial Properties of Objects and AgentsIn order to control Steves body in the virtual world, Steve needs to know the spatial properties of objects, such as their position, orientation, and spatial extent. In principle, thesimulator could maintain such properties and provide them to the perception module asdescribed in the previous section. In practice, however, this is often inconvenient. Thesimulator controls the appearance of the virtual world by instructing the visual interfacecomponents to load graphical models for objects and by sending messages to change properties of the objects, such as location and color. Therefore, the simulator itself may have norepresentation for the geometric properties of the objects these details are in the graphicalmodels themselves, which are typically created by a course author using a 3D modeling tooland stored in les. Moreover, the simulator may not even have simple information such asthe location of the objects. This is because graphics objects are typically organized into ahierarchy, where each object has its own coordinate system that is relative to its parent.For example, the simulator might know how to move a button in and out relative to itsgraphical parent, a console, but may not know the global world coordinates of the button,which is what Steve needs.Fortunately, the visual interface components can provide such information. Currently,the perception module queries a visual interface component for such information when it isneeded. When the motor control module needs to interact with an object e.g., point to it,it asks the perception module for its location and bounding sphere. The location speciesthe origin of the object in Cartesian coordinates, as an x, y, z point. The bounding sphereis specied by the smallest radius around that origin that encompasses the object.The perception module can get these properties of other agents as well. Each agent hasa graphical body in the virtual world. To the visual interface components, these bodies areno dierent than any other graphical object, so the perception module can query for thelocation and bounding spheres of any agent.In addition to keeping track of the location of agents in Cartesian coordinates, theperception module also keeps track of Steves location in terms of objects. To move toan object, the cognition component sends a motor command to that eect. The motorcontrol module converts this request into a location in Cartesian coordinates and sendsa message to move Steve there. When Steve arrives, the perception module receives amessage from the visual interface component, and it records his location as being at thedesired object. The cognition module works at this level of abstraction, ignoring the actualCartesian coordinates.To interact with objects, Steve needs other spatial information that is not provided bythe visual interface components. Therefore, we require the course author to provide thefollowing perceptual knowledge for each objectfront vector To interact with an object, Steve must know where its front side is. Wheninteracting with an object, Steve will use this knowledge to position himself in frontof the object. The course author species the front of an object by a vector in thexy plane that points to the front of the object from its origin. We currently assumethat this vector does not change dynamically.grasp vector If Steve may need to grasp the object, he needs to know the appropriateorientation for his hand. The course author species this as a vector in threespacepointing from the objects origin in the direction in which Steve would pull the object.11Even if Steve has no reason to pull the object, this provides an orientation with whichto grasp it.press vector If Steve may need to press the object e.g., a button, he also needs anappropriate orientation for his hand when doing so. The course author species thisas a vector in threespace pointing from the objects origin in the direction in whichSteve should press the object.agent location When interacting with an object, Steve stands in front of it and slightly tothe right to avoid blocking the students view. Using the objects location, boundingsphere, and front vector, Steve can choose his location. Typically, this approach workswell, because it ensures that Steve is out of the students way when the student isstanding in front of the object. However, if the object has an irregular shape, thebounding sphere might lead Steve to stand unnecessarily far from it. Or, if there areother objects surrounding the desired object, Steve might need to adjust his positionto avoid colliding with them. If Steves default location is not appropriate, the courseauthor can specify a more appropriate location, by specifying how far in front, above,and to the right of the object Steve should stand. Negative numbers can be used toforce Steve to stand behind, below, or to its left when necessary.5.1.3 Representing Properties of Human ParticipantsThe perception module also keeps track of human participants. The visual interface component for a person uses the position sensor on their headmounted display to track theirlocation in Cartesian coordinates specically, the point between their eyes and their lineof sight, and the perception module can request this information when it is needed by themotor control module e.g., to look at a person.If Steve is working with a student on a task, the perception module also keeps track ofthe students eld of view. More specically, it keeps track of which objects in the virtualworld lie within the students eld of view. For each object, the perception module asks thestudents visual interface component whether that object is in the students eld of view.Subsequently, the visual interface component broadcasts a message when an object entersor leaves the students eld of view, so the perception module can maintain a snapshot ofwhich objects the student can see.5.1.4 Representing Perceptual Knowledge for Path PlanningSteve must navigate through the virtual world from object to object, avoiding collisions.There are several approaches to collisionfree navigation, most of them originally developedby robotics researchers and later adapted for graphical worlds. Steve follows one standardapproach he carves the virtual world into a graph, where the nodes of the graph are places,and there is an edge between two nodes if Steve can move directly between the places withoutcolliding into anything. As his set of places, Steve uses the objects in the virtual world, or,more specically, the places he stands when interacting with each object. Currently, ourwork focuses on relatively static environments, so we assume the graph does not changeover time. By default, there is an edge between any two nodes places. However, if thereis something blocking the path between two objects e.g., a wall, the course author canspecify that there is no direct path between the objects, eectively removing that edge fromthe graph. For sparse graphs or subgraphs, the author can alternatively just specify the12permissible edges. The resulting adjacency graph serves as Steves perceptual knowledgefor navigation using it, the motor control module can plan a path between any two nodes,as described in Section 7.2.5.2 Representing and Handling EventsWhenever the perception module passes a snapshot of the state of the world to the cognitionmodule, it also passes a list of important events that occurred since the last snapshot. If thecognition module could only see periodic snapshots of the state of the world, it might misssome events. For example, if a button were pressed and released in between snapshots, thecognition module would never know it had been pressed. By receiving both a snapshot ofthe world and a list of important events that occurred since the last snapshot, the cognitionmodule gets a complete view of the world and its changes.The perception module receives and forwards to the cognition module several types ofeventsstate changes As described earlier, the simulator sends messages whenever the state ofthe virtual world changes. The cognition module does not need most of these theyare summarized by the snapshot it receives. However, the perception module passes aselect few to the cognition module, specically those that provide feedback on Stevesobject manipulations. These important state changes are specied in Steves perceptual knowledge.actions on objects When a human participant interacts with an object e.g., touches itwith a data glove, that participants visual interface component broadcasts a messagespecifying the name of the participant and the object they touched. The meaningof this interaction depends on the object. For example, touching a button causesthe button to be pressed, while touching a valve allows the human participant toturn it. The result of the action is determined by the simulator the message fromthe visual interface component only species the participant and object. The visualinterface component also sends a message when the person stops touching the object.Agents interact with objects by sending these same messages, listing themselves asthe participant.humans speech Steve receives messages from a speech recognition component when ahuman participant begins talking and when they nish. The former message simplyspecies which person is speaking, while the latter additionally includes a semantictoken that represents the sentence that was recognized. If the speech recognizer didnot understand the sentence, it returns an unknown token.agents speech Steve agents can also tell when other agents are talking. An agent sendsout a message to the speech generation components to generate speech. Therefore, anagent can listen for such messages to detect when other agents begin speaking. When aspeech generation component nishes producing the speech for its human participant,it sends a message to this eect. Therefore an agent can also tell when other agentshave nished speaking. Moreover, an agent can use such messages to detect when itsown utterance is complete. Currently, these messages do not include a semantic token,like their corresponding messages representing human speech. Instead, agents sendseparate messages representing the semantic content of their speech these messagesare loosely based on speech acts, much like KQML Labrou  Finin 1994.136 Cognition6.1 The Layers of Steves CognitionThe cognition module is organized into three main layers domainspecic task knowledge domainindependent pedagogical capabilities SoarSteve is built on top of the Soar architecture Laird, Newell,  Rosenbloom 1987 Newell1990. Soar was designed as a general model of human cognition, so it provides a numberof features that support the construction of intelligent agents. This paper will not focus onSoar, since an understanding of Steve does not require an understanding of Soar. However,much of Steves design was inuenced by features of the Soar architecture.Soar is a general cognitive architecture, but it does not provide builtin support forparticular cognitive skills such as demonstration, explanation, and question answering. Ourmain task in building Steve was to design a set of domainindependent pedagogical capabilities such as these and layer them on top of the Soar architecture. These capabilities areimplemented as Soar production rules, and they will be discussed later in this section.To teach students how to perform procedural tasks in a particular domain, Steve needs arepresentation of the tasks. A course author must provide such knowledge to Steve. Givenappropriate task knowledge for a particular domain, Steve uses his general pedagogicalcapabilities to teach that knowledge to students. Thus, our layered approach to Stevescognition module allows Steve to be used in a variety of domains each new domain requiresonly new task knowledge, without any modication of Steves abilities as a teacher.6.2 Domain Task KnowledgeIntelligent tutoring systems typically represent procedural knowledge in one of two ways.Some, notably those of Anderson and his colleagues Anderson et al. 1995, use detailedcognitive models built from production rules. Such systems perform domain tasks by directly executing the rules. Other systems use a declarative representation of the knowledge,usually some variant of a procedural network representation Sacerdoti 1977 specifying thesteps in the procedure and their ordering. Such systems perform tasks by using a domainindependent interpreter to execute the procedural network i.e., walk through the steps.Production rule models provide a more exible ontology at a price they are laborious tobuild. The labor may be justied in domains like algebra and geometry, where a tutor, oncebuilt, can be used for many years by many people. In contrast, procedural network representations are more practical for domains like operation and maintenance of equipmentprocedures may change frequently in such domains, so it must be easy for domain expertsor course authors to represent procedures, examine them, and change them when necessary.For these reasons, Steve uses a procedural network plan representation of domain tasks.Steve represents domain tasks as hierarchical plans, using a relatively standard representation Russell  Norvig 1995. First, each task consists of a set of steps, each ofwhich is either a primitive action e.g., press a button or a composite action i.e., itself atask. Composite actions give tasks a hierarchical structure. Second, there may be orderingconstraints among the steps, each specifying that one step must precede another. These14Task functionaltestSteps pressfunctiontest, checkalarmlight, extinguishalarmCausal Linkspressfunctiontest achieves testmode for checkalarmlightcheckalarmlight achieves knowwhetheralarmfunctional for endtaskextinguishalarm achieves alarmoff for endtaskOrdering constraintspressfunctiontest before checkalarmlightcheckalarmlight before extinguishalarmFigure 6 An example task denitionconstraints dene a partial order over the steps. Finally, the role of the steps in the taskis represented by a set of causal links McAllester  Rosenblitt 1991. Each causal linkspecies that one step achieves a goal that is a precondition for another step or for termination of the task. For example, pulling out a dipstick achieves the goal of exposing thelevel indicator, which is a precondition for checking the oil level.Figure 6 shows an example of a task denition the task of performing a functional testof one of the subsystems of a highpressure air compressor aboard a ship. It consists of threesteps pressfunctiontest, in which the compressor operator presses the test button on thecontrol panel, checkalarmlight, in which the operator examines the light to make sure itis functional i.e., not burned out, and extinguishalarm, in which the operator presses thereset button to reset the light. In addition, every task has two dummy steps a begintaskthat precedes all other steps, and an endtask that follows all other steps. Several causallinks exist among the steps. For example, pressfunctiontest puts the device in testmodei.e., illuminates the alarm light, which is a precondition for checkalarmlight. In order forthe task to be complete, the operator must know whether the alarm light is functional, andthe alarm light must be o thus, these end goals are shown as preconditions for endtask.Similarly, if the task depended on conditions that must be established prior to starting thetask, these conditions would be represented as eects of begintask.The plan representation only denes the structure of a task, in terms of its goals andsteps. To complete the description, the course author must dene the goals and primitiveactions it references. Each goal is dened by an attributevalue pair. Steve can represent twotypes of goals attributes of the virtual world, and attributes of his own mental state. Forthe former, the attribute is one that will appear in Steves perception e.g., light1state,and the value is its desired value e.g., on. The goal is satised when that attributevalue pair is part of Steves current perceptual snapshot. For the latter, the attribute isone that will appear in Steves mental state. Such attributes are stored as the result ofcertain primitive actions that Steve executes, namely sensing actions Russell  Norvig1995. Sensing actions are used to record the state of some attribute of the virtual worldat a particular point during a task. For instance, in the functionaltest example above,checkalarmlight is a sensing action that causes Steve to record the resulting state of thelight as the value of a checkalarmlightresult attribute in mental state. A mental15state goal can optionally specify an attribute without any specic value for example, a goalspecied only as checkalarmlightresult is satised if Steve knows the result of thetest, regardless of the particular result. Thus, Steve can represent two types of goals goalsthat require putting the virtual world in some desired state, which Steve can evaluate usingperception, and goals of acquiring information, which Steve can evaluate by checking hismental state.Primitive actions require Steve to interact with the virtual world, typically via motorcommands. To simplify the course authors job of describing the primitive actions in a task,we are developing a library of primitive actions that are appropriate for a virtual world thecourse author denes each primitive action in a task as an instance of one in the library.The library is organized as a hierarchy of very general actions and their specializations. Forexample, one general action in the library is ManipulateObject. To dene a task step asan instance of ManipulateObject, the course author must specify the name of the objectin the virtual world to be manipulated e.g., button1, the name of the motor commandthat will perform the manipulation e.g., press, and the perceptual attributevalue pairthat will indicate that the manipulation has nished e.g., button1state depressed.Other actions in the library are dened as specializations of such general actions, to provide a shorthand for course authors. For example, the library includes PressButton as aspecialization of ManipulateObject a course author could dene the previous example asan instance of PressButton by merely specifying the name of the button. It is relativelyeasy to extend the action library, but it does require writing some simple Soar productions,so we would not expect course authors to extend it themselves.To complete the task knowledge, the course author must provide text fragments thatSteve can use for natural language generation. Steve does not currently include any sophisticated capabilities for natural language generation speech utterances are constructed byplugging domainspecic text fragments into text templates. Steve currently requires threetypes of text fragments He requires one fragment for each goal, in a form that would complete the sentenceI want .... He requires two fragments for each task step. The rst is a a simple imperativedescription of the step e.g., press the power button. The second has the sameform and purpose, but may include more elaboration. Steve uses the second fragmentwhen a verbose description of the step is appropriate. For sensing actions, he requires a fragment for each possible result e.g., the oil levelis low and the oil level is normal. Steve uses these fragments when describing theresults of sensing actions to a student.Our representation for domain task knowledge provides the information that Steve needswhile only requiring declarative knowledge that a course author can provide. In contrast tosimple partialorder plans, our hierarchical plan representation provides several benets itallows the course author to chunk complex procedures into subtasks, which may be reusedin multiple tasks, and it provides more structure to Steves demonstrations, allowing himto chunk complex procedures into subtasks to aid students comprehension. Our inclusionof causal links in the task representation diers from previous tutoring systems previoussystems that used a declarative representation of procedural knowledge, such as those ofBurton 1982, Munro et al. 1993, and Rickel 1988, only included steps and ordering16constraints. As we will discuss shortly, Steves knowledge of causal links allows him toautomatically generate explanations and to adapt procedures to unexpected circumstances,making him more robust than these previous systems.The central purpose of Steves task knowledge is to allow him to create a task model whenhe is required to demonstrate a task or monitor the student performing the task. He createsthe task model by simple topdown task decomposition Sacerdoti 1977. First, he initializesthe task model to contain the name of the task. Next, he adds the task representation steps,ordering constraints, and causal links for that task. Steve recursively repeats this processfor any composite step in the task representation, until the task has been fully decomposedinto primitive actions. The result is the full hierarchical representation of the given task.This task model includes all the steps that might be required to complete the task, even ifsome are not necessary given the current state of the world. As described shortly, this taskmodel is an important resource for Steves plan construction.6.3 Steves Decision CycleThe cognition module operates by continually looping through a decision cycle. In ourcurrent implementation, Steve executes about ten decision cycles per second. Once Steveis given a task and has created the task model, as described in the previous section, eachdecision cycle goes through ve phases31. Input phase Get the latest perceptual information from the perception module.2. Goal assessment Use the perceptual information to determine which goals of thecurrent task are satised. This includes the end goals of the task as well as anyintermediate goals i.e., preconditions of task steps.3. Plan Construction Based on the results of goal assessment, construct a plan tocomplete the task.4. Operator Selection Select the next operator. Each operator is represented by a setof production rules that implement one of Steves capabilities, such as answering aquestion or demonstrating an action. Steves operators serve as the building blocksfor his behavior.5. Operator Execution Execute the selected operator. In most cases, this will cause thecognition module to output one or more motor commands.The general notions of decision cycle, input phase, and operator selection and execution areprovided by Soar. The particulars of Steves decision cycle are unique to Steve.During the input phase, the cognition module asks the perception module for the stateof the virtual world. As discussed in Section 5, the cognition module receives three piecesof information the state of the simulator, represented as a set of attributevalue pairs as describedin Section 5.1.1 a set of important events that occurred since the last snapshot as described in Section 5.23Actually, Soar executes phase 5 concurrently with phases 13 of the next decision cycle.17 the students eld of view, represented as the set of objects that lie within it asdescribed in Section 5.1.3The remainder of this section discusses the rest of the decision cycle. First, we discussgoal assessment Section 6.4 and plan construction Section 6.5. Then, we discuss Stevesoperators i.e., his individual capabilities. The discussion of operators is organized aroundthree primary modes demonstrating a task to a student Section 6.6, monitoring a students performance and providing help Section 6.7, and answering questions about pastactions Section 6.8.6.4 Goal AssessmentIn order to construct a plan to complete the current task, Steve must know which of thetask goals are already satised. As described in Section 6.2, each goal in the task modelis associated with an attributevalue pair. Therefore, Steve can assess each goal by simply determining whether its associated attributevalue pair is satised given his currentperceptual input and mental state.Our implementation of this process exploits Soars truth maintenance system. Whenthe course author denes a goal, an associated Soar production rule is created. This rulesimply checks the current perceptual input or mental state, whichever is appropriate. Whenthe goal becomes satised, the rule res, marking the goal satised. As long as the goalis satised, this result will remain, without any further processing required. If the goalbecomes unsatised, Soar retracts the rule, along with its result. Thus, Steve need notevaluate every goal on every decision cycle each rule automatically res or retracts whenthe status of its goal changes.6.5 Plan ConstructionWhether demonstrating a task to a student or monitoring the students performance of thetask, Steve must maintain a plan for completing the task. The plan allows Steve to identifythe next appropriate action and, if asked, to explain the role of that action in completingthe task. As a teacher, Steves ability to rationalize the action is just as important as hisability to choose it.We faced conicting design criteria when designing Steves planner. To handle dynamicenvironments containing people and other agents, Steve must be able to adapt proceduresto unexpected events. This argues against a rote execution of domain procedures, in favorof a general planning and replanning capability. Thus, we might encode domain actionsas STRIPS operators Russell  Norvig 1995 and use a standard partialorder plannerWeld 1994 to construct plans. However, we also want Steve to follow standard procedures whenever possible. Thus, we would have to augment the partialorder planner withsubstantial control knowledge to discourage unusual plans. Moreover, Steve must be ableto construct and revise plans quickly, since he and the student are collaborating on tasksin real time. This can be a problem for general partialorder planners, which often requireexponential search. Finally, we must only require task knowledge that course authors caneasily provide, yet formulating STRIPS operators and control knowledge for a partialorderplanner is dicult even for AI researchers.To satisfy these criteria, Steve uses the task model, as described in Section 6.2, toguide his plan construction and revision. Recall that, when given a task to demonstrateor monitor, Steve uses topdown task decomposition to construct a task model. The task18model includes all the steps that might be required to complete the task, even if some arenot necessary given the current state of the world. Every decision cycle, after Steve gets anew perceptual snapshot and assesses the goals in the task model, he constructs a plan forcompleting the task. He does so by marking those elements of the task model that are stillrelevant to completing the task, as follows Every end goal of the task is relevant. A primitive step in the task model is relevant if it achieves a relevant, unsatised goal. Every precondition of a relevant step is a relevant goal.Thus, Steve starts by marking all the end goals as relevant i.e., in the plan. For eachone that is not already satised, he nds the step in the task model that achieves it and addsthat step to the plan. Each step that is added may have unsatised preconditions, and eachsuch precondition becomes a new goal that must likewise be achieved. This is exactly howa general partialorder planner operates. However, Steves use of the task model eliminatesmuch of the complexity that a partialorder planner must handle A partialorder planner may have multiple actions that could achieve each goal, so itmust search through alternative plans. In contrast, Steve uses the task model as anoracle for choosing the appropriate action to achieve each relevant, unsatised goal,so there is no search. Thus, Steves plan construction is predictably fast. A partialorder planner must identify threats i.e., two unordered steps that couldinteract undesirably if executed in the wrong order and add appropriate orderingconstraints. In contrast, Steve simply uses the ordering constraints in the task modelif two steps in the plan have an ordering constraint in the task model, that orderingconstraint is added to the plan. As long as there are no unresolved threats in the taskmodel, there will be no unresolved threats in the plan. A partialorder planner must create steps in the plan by instantiating STRIPS operators. Therefore, it must maintain a set of binding constraints, and it may haveto search when there are alternatives. In contrast, the steps in the task model areinstances of actions in the action library, so they have no variables. Hence, Steve neednot reason about binding constraints.This approach satises our design criteria. It is ecient, and it forces Steve to followstandard procedures as much as possible, yet it still allows him to adapt to unexpectedevents Steve reexecutes parts of his plan that get unexpectedly undone, and he skips overparts of the task that are unnecessary because their goals were serendipitously achieved.Thus, unlike videos or scripted demonstrations, Steve can adapt domain procedures to thestate of the virtual world, and he does so eciently.To execute the plan or evaluate the students actions, Steve must also determinewhich steps to do next. A plan step is ready for execution if it is applicable i.e., all itspreconditions are satised and not precluded i.e., no other plan step necessarily comesbefore it. Note that there may be a single next step, there may be multiple next stepssince this is a partiallyordered plan, and there may be no next steps if no subset of thetask model will get Steve from the current state to task completion.Steves plan construction exploits Soars truth maintenance system, making it even moreecient. Each of the three rules for determining relevance listed above is implemented as19a production rule. Depending on which goals in the task model are satised, instances ofthese production rules re, marking appropriate parts of the task model as relevant i.e., inthe current plan. As goals become satised or unsatised, only aected instances of theproduction rules re or retract, so only those parts of the plan that are aected by changesin the current state are revised.6.6 DemonstrationTo demonstrate a task to a student, Steve must perform the task himself, explaining whathe is doing along the way. First, he creates the task model. Then, in each decision cycle,he updates his plan for completing the task and determines the next appropriate steps, asdiscussed in the previous section. After determining the next appropriate steps, he mustchoose one and demonstrate it. First, we discuss how he chooses, and then we discuss howhe demonstrates.6.6.1 Choosing the Next Task Step to DemonstrateAt any point during a task, there may be multiple steps that could be executed next. Thatis, each of these steps may be applicable i.e., all their preconditions are satised and notprecluded i.e., no other step in the plan must necessarily come rst. From the standpointof completing the task, any of these steps could be chosen. However, from the standpointof communicating with the student, they may not be equally appropriate.Students will more easily follow the demonstration if Steve follows certain human conventions. For example, it is easier to follow a demonstration that focuses on one subtaskat a time. If two subtasks could be interleaved arbitrarily, Steve could alternately executeone step from each subtask until they are both complete, but this would be unnecessarilyconfusing. As another example, suppose that Steve were demonstrating a subtask e.g.,conguring a console when an unrelated, higherpriority task step suddenly became relevant e.g., acknowledging an alarm. After acknowledging the alarm, Steve could move onto an unrelated subtask, but the student will expect him to resume the interrupted subtaske.g., conguring the console. Researchers in computational linguistics have studied thisproblem of discourse focus for many years, and they have identied common conventionsin types of discourse as dierent as rhetorical persuasion and dialogues regarding tasks. Toensure coherent demonstrations, Steve must obey these conventions.Following Grosz and Sidner 1986, we represent the discourse focus as a stack. WhenSteve begins executing a step in the plan either primitive or composite, he pushes it ontothe stack. Therefore, the bottom element of the stack is the main task on which the studentand Steve are collaborating, and the topmost element is the one on which the demonstrationis currently focused. When the step at the top of the focus stack is complete, Steve popsit o the stack. A primitive action is complete when it is no longer in the current plan,while a composite step is complete when all its end goals are satised.Steve uses the focus stack to help choose the next step to demonstrate. When there aremultiple plan steps ready for execution, he prefers those that maintain the current focus orshift to a subtask of the current focus. To operationalize this intuition, Steve rst eshesout the list of candidates for demonstration Any step in the current plan that is ready for execution is a candidate. Each of theseis a primitive action, since the plan never includes any composite steps.20 If a step primitive or composite is a candidate, and its parent composite step inthe task model is not somewhere on the focus stack, that parent step is a candidate. The previous rule is applied recursively. That is, if a composite step is added as acandidate, and its parent in the task model is not somewhere on the focus stack, thatparent is added as a candidate.Having enumerated the candidates, Steve chooses among them as follows Executing a parent step next is preferable to executing any of its children. Intuitively,this means that Steve should shift focus to the subtask and introduce it before hebegins demonstrating its steps. A task step whose parent is the current focus i.e., the topmost element of the focusstack is preferable to one whose parent is not. If there are remaining candidates that are unordered by these preferences, Stevechooses one randomly.Lets illustrate these rules with a few examples Suppose Steve is beginning a new demonstration. Therefore, the focus stack is empty.Suppose the task is start the compressor, the rst subtask is check the oil, andthe rst step of that subtask is pull out the dipstick. Therefore, the rst step of theplan will be pull out the dipstick. Since that steps parent check the oil is noton the focus stack, it is a candidate for demonstration, and is preferable to pull outthe dipstick. Since the parent of check the oil, namely start the compressor, isnot on the focus stack, it is a candidate for demonstration, and is preferable to checkthe oil. Thus, start the compressor is added to the focus stack rst, and Steveexecutes it by introducing the task to the student. Next, Steve will push check theoil onto the stack and execute it by introducing this rst subtask. Finally, Steve canpush pull out the dipstick onto the stack and demonstrate it to the student at thispoint, Steve has introduced the appropriate hierarchical context for performing thisaction. Suppose Steve could perform two subtasks in any order, such as check the oil andcheck the coolant, and he randomly chooses to check the oil rst. Next, sincecheck the oil is the current focus, he will prefer pull out the dipstick to check thecoolant or any of its steps, so he will push it onto the focus stack and demonstrateit. When the dipstick is out, it will be removed from the plan and popped o thefocus stack, making check the oil the current focus again. This process will repeatfor each step of check the oil, until that subtask is completed and popped o thefocus stack. Suppose that Steve is performing one subtask e.g., congure console when anunrelated, higherpriority based on ordering constraints task step suddenly becomesrelevant e.g., acknowledge alarm. Steve will add acknowledge alarm to the plan,and it will be the only step ready for execution since it precludes the remaining stepsof congure console, so Steve will push it onto the focus stack and demonstrate it.When the alarm is acknowledged, it will be removed from the plan and popped othe focus stack, and Steve will resume congure console.216.6.2 Demonstrating a Task StepOnce Steve chooses the next task step and pushes it onto the focus stack, he demonstrates itto the student. If the step is a composite step, Steve simply introduces the subtask, usingits associated text fragment. If it is a primitive action, Steve demonstrates it as follows1. First, Steve moves to the location of the object he needs to manipulate by sending alocomotion motor command, along with the object to which he wants to move. Then,he waits for perceptual information to indicate that he has arrived. This typicallytakes multiple decision cycles during this period, Steve repeatedly executes a simplewait operator.2. Once Steve arrives at the desired object, he explains what he is going to do. This involves describing the step while pointing to the object to be manipulated. To describethe step, Steve outputs a speech specication with three pieces of information the name of the step  this will be used to retrieve the associated text fragment whether Steve has already demonstrated this step  this allows him to acknowledge the repetition a rhetorical relation indicating the relation in the task model between this stepand the last one Steve demonstrated  this is used to generate an appropriatecue phraseResearch has shown that human speakers often use cue phrases to indicate the rhetorical relation between one utterance and another Grosz  Sidner 1986 Moore 1993.Steve currently uses cue phrases to mark several types of rhetorical relations If the last step was to introduce a composite step, and the current step is a childof that step, Steve says First, .... If the previous step achieved a precondition of the current step, Steve says Nowwe can .... If there is an ordering constraint in the task model specifying that the last stepmust precede the current step, Steve says Next, .... This is used only whenthe previous cue phrase does not apply. If the current step precedes the last step in the task model, it represents aninterruption, so Steve says Oh, I need to ....These cue phrases help to structure the demonstration, hopefully aiding the studentscomprehension. Once Steve sends the motor command to generate the speech, hewaits for an event from the perception module indicating that the speech is complete.3. When his speech is complete, he performs the task step. This is done by sendingan appropriate motor command and waiting for evidence in his perception that thecommand was executed. For example, if he sends a motor command to press button1,he waits for his perception snapshot to include button1state depressed.4. If appropriate, he explains the results of the action, using the appropriate text fragments.22Actually, this sequence of events in demonstrating a primitive action is not hardwiredinto Steve. Rather, each item in the sequence is an independent capability, and each actiontype in the action library is associated with an appropriate suite of such items. Each suiteis essentially a nite state machine represented as Soar productions. By representing a suiteas a nite state machine rather than a xed sequence, Steves demonstration of an actioncan be more reactive and adaptive. Most of the actions in our current action library usethe sequence above, but our approach gives Steve the exibility to demonstrate dierenttypes of primitive actions dierently.Steve is sensitive to the student while demonstrating. For example, when Steve references an object and points to it, he checks whether the object is in the students eldof view. If not, he says Look over here and waits until the student is looking beforeproceeding with the demonstration.6.6.3 Let me nishSteves demonstrations can end in one of two ways. Typically, he completes the task andannounces his completion. However, we also allow the student to request Let me nish.In this case, Steve acknowledges that the student can nish the task, and he shifts tomonitoring the student.6.7 Monitoring a StudentOften, Steves role is to monitor a student performing a task, providing assistance whenneeded. For example, Steve might rst demonstrate a task and then suggest that thestudent try it. Or, as described in the previous section, the student might interrupt Stevesdemonstration and ask to nish the task. In either case, Steves role in monitoring a studentis to maintain his own plan for completing the task and to use it to assess the studentsactions and to answer questions.Steves ability to adapt to unexpected events is especially useful when monitoring astudent. Most tutoring systems require the student to follow the tutors plan, because thetutor would be unable to adapt to unexpected deviations. In contrast, we want to give thestudent the exibility to deviate from the standard procedure, make mistakes, and learnto recover from them. Such exibility is a prime advantage of simulationbased training itallows students to gain exposure to a wide variety of situations, and it encourages them tolearn from their own mistakes. Steves approach of repeatedly reevaluating and possiblyrevising his plan supports such exibility he can typically provide assistance to the studenteven when the student took unexpected actions and landed in an unusual state of the world.Steves approach to goal assessment and plan construction is the same for monitoringas it is for demonstration. The main dierence between monitoring and demonstration isthat, when monitoring, Steve allows the student to take the actions. There is one exceptionSteve must still perform any sensing actions in the plan e.g., checking whether a light comeson. Sensing actions do not cause observable changes in the virtual world they only changethe mental state of the student. In order to update his own plan, Steve must recognizewhen the goals of a sensing action are achieved. Therefore, whenever a sensing action isappropriate i.e., the next step in Steves plan, if the student is looking at the appropriateobject i.e., it is in the students eld of view, Steve performs the sensing action, recordsthe result, and assumes that the student did the same.In the remainder of this section, we outline Steves capabilities relevant to monitoring a23student. The details of these capabilities are not important additional sophistication could,and will, be added to each. The important point is to show how Steves domain knowledge,and his abilities to use the knowledge, allow him to assist the student in a variety of ways.6.7.1 Evaluating the students actionsUsing his own assessment of the task goals, and his plan for completing the task, Steve canevaluate the students actions. When the student performs an action, Steve must identifythe steps in the task model that match the action. If none of the matching steps is anappropriate next step, the students action is incorrect. In this case, Steve could providefeedback to the student, ranging anywhere from a simple shake of his head or look ofdisapproval to an explanation of why the action is incorrect e.g., a precondition is notsatised or the step is precluded by another step. Currently, Steve simply says no andshakes his head, but we will be experimenting with dierent forms of feedback soon. Whenthe students action is correct, Steve nods his head in agreement.6.7.2 What should I do nextThe student can always ask Steve What should I do next To answer this question, Stevesimply suggests the next step in his own plan. Unlike most tutoring systems, Steve cansuggest appropriate steps even when the student deviates from the standard procedure, asmentioned earlier. This is a direct consequence of Steves ability to adapt the procedure tounexpected events, in this case the students unexpected actions.If there are multiple possible next steps, Steve currently enumerates them. In some cases,this is appropriate. However, in other cases, Steve could provide a more focused answer ifhe knew more about the students current focus e.g., the subtask on which the student iscurrently working. Plan recognition algorithms infer such information, so they could beused to maintain the discourse focus stack during monitoring. Steves plan representationprovides the information that most plan recognition algorithms require, but we have notyet added this capability.It is also possible that Steve does not know what to do next. This could happen if nosubset of the task model is sucient for completing the task. For example, the studentmay have permanently damaged the virtual equipment. In the domains where we havetested Steve, the simulator has not supported such irreversible actions. Nonetheless, Stevecurrently handles such situations by simply explaining that he does not know what to donext. In the future, we could extend Steve to explain the aws in his plan that he does notknow how to resolve i.e., the preconditions he does not know how to achieve.6.7.3 Show me what to doThe student may understand what to do but not how. In this case, the student can tell SteveShow me what to do. Steve responds to such questions by demonstrating the next step, asdescribed in the previous section. Clearly, this is a capability that traditional disembodiedtutors cannot provide.If there are multiple possible next steps, Steve currently chooses one of them randomly.As mentioned before, plan recognition could provide information about the students currentfocus, leading to a more informed choice.24Steve I suggest that you press the function test button.Student WhySteve That action is relevant because we want the drain monitor in test mode.Student WhySteve That goal is relevant because it will allow us to check the alarm light.Student WhySteve That action is relevant because we want to know whether the alarm lightis functional.Figure 7 Example explanations generated by Steve6.7.4 Explaining the relevance of a step or goalWhen Steve suggests that the student perform an action, we want to allow the student toask what the role of that action is in the task. Without an understanding of the rationalefor each step in a procedure, students are forced to simply memorize the steps. In contrast,an understanding of the causal structure of a task should help students remember theprocedure, adapt it when necessary, and apply their knowledge to related tasks.Figure 7 shows Steves ability to rationalize suggestions. In this example, Steve ismonitoring the student and suggests that the student press the function test button. Whenthe student asks why, Steve explains the goal of that action it will put the drain monitorin test mode. The example also illustrates Steves ability to answer followup questionswhen the student asks why that goal is relevant, Steve explains that it will enable anotherrelevant action. The student can continue asking such followup questions until, ultimately,the initial suggestion has been related to an end goal of the task that the student was given.Steve generates such explanations from the causal links in the plan. Recall from Section 6.5 that if a step or goal is relevant i.e., in the current plan, it is for one of threereasons1. It is an end goal of the toplevel task.2. It is a precondition of a relevant primitive plan step.3. It is a primitive plan step that achieves a relevant, unsatised goal.These connections between steps and goals are specied by the causal links in the plan.Thus, one advantage to having Steve maintain a plan is that he can use it to rationalize hissuggestions.Although our current approach to explanation simply follows causal links one by onedriven by followup questions, our plan representation supports many other explanationstrategies as well. For example, using a model of the students knowledge, Steve couldskip over causal links that the student is presumed to understand. Similarly, Steve couldpurposely skip over some causal links in order to motivate an action in terms of a moredistant goal, forcing the student to relate the action to that goal. Also, since plans arerepresented hierarchically, Steve could provide suggestions and explanations at various levels25of detail based on the students knowledge and Steves pedagogical style. Providing a richfoundation for explanation was a prime motivation for choosing hierarchical plans as therepresentation for tasks.6.8 Episodic Memory and AfterAction ReviewThe previous section described Steves ability to rationalize his suggestions. In that case,Steve can explain the relevance of a step or goal to completing the task by inspecting hiscurrent plan. In addition, we wanted Steve to be able to rationalize his own actions duringan afteraction review. When Steve completes a demonstration, he asks the student whetherthey have any questions. At this point, they can ask him to rationalize any one of his actionsduring the demonstration, and they can ask followup Why questions as described inthe previous section. To answer such questions, Steve cannot rely on his current plan, sincethe task is already complete and the step in question is no longer relevant.To support such questions, Steve employs the episodic memory capability of the Debriefsystem Johnson 1994. Debrief includes a set of production rules that enable Soar agents toremember their actions and the situations in which they occurred. It uses Soars chunkingcapability Laird, Newell,  Rosenbloom 1987 to represent and recall situations eciently.When the student asks why Steve performed an action, Steve triggers the Debrief productions to recall the situation in which the action was performed i.e., Steves perceptionsnapshot and mental state. Given the recalled situation, Steve uses his standard methodsfor goal assessment and plan construction to reconstruct his plan. Using this past plan,Steve rationalizes his action and answers followup questions as described in the previoussection.7 Motor Control7.1 OverviewThe motor control module receives motor commands from the cognition module and decomposes them into a sequence of lowerlevel commands that are sent to other componentsvia the message dispatcher. Therefore, this module controls Steves appearance and voice,and it allows Steve to cause changes in the virtual world.The motor control module accepts a variety of motor commands Speak a text string to a person, agent, or everyone Send a speech act to an agent this allows the agent to understand associated spokentext Move to an object Look at an object, agent, or person Nod the head in agreement or shake it in disagreement Point at an object Move the hand to a neutral position i.e., not manipulating or pointing at anything26 Manipulate an object. For each primitive action in the cognition modules actionlibrary, there is a corresponding motor command that the motor control module accepts. These are easy to add, since they are built on top of Steves lowerlevel bodycontrol capabilities, which are discussed below. Currently, Steve can press objectse.g., buttons, ip switches, turn valves, move objects short distances i.e., distancesthat do not require Steve to move also, and pull and push objects e.g., a dipstick.The motor control module maps these commands into messages that it sends to themessage dispatcher to cause changes in the virtual world. The messages it sends fall intothree categoriesactions Some messages inform the simulator of Steves actions. Steve takes actions bysending the same messages that would be sent by a visual interface component ifa person took the action he can touch and release objects. In addition, tomanipulate objects that a person would touch and drag e.g., a throttle, Steve sendsa message specifying the desired endpoint of the manipulation e.g., set the throttle at3000 rpm the simulator responds to such messages by moving the object graduallyto the specied endpoint.speech When the cognition module sends a motor command to generate speech, the motorcontrol module sends a corresponding message to the message dispatcher, which willcause the appropriate speech generation components to generate the speech. Whenstarting Steve, a user can congure his voice gender, speaking rate, vocal tract size,and pitch, and this voice will be used whenever he speaks.body animation Steve supports a set of primitive body control commands. The motor control module converts motor commands from the cognition module into somecombination of these primitive commands. Each primitive command causes Steve tobroadcast lowlevel messages to the visual interface components to move or rotateSteves body parts. To create a new body for Steve, one only has to redene theseprimitive commands, which include the following move to an object look at an object, agent, or person turn the head only look at an object, agent, or person focus the whole body nod the head in agreement or shake the head in disagreement point at an object press an object grasp an object move the hand to a neutral position switch to a speaking facial expression switch to a neutral nonspeaking facial expressionWe are currently extending this set to include a wider variety of facial expressions.The ability to completely replace Steves body by reimplementing a small set of bodyprimitives allows us to experiment with dierent bodies. Since Steve teaches physical tasks,some variant of a human form seems most appropriate. The question is how much detail is27needed. For simply demonstrating actions, a hand is often sucient. Adding a head opensup additional channels of communication for example, it allows the student to track Stevesgaze. Simple representations, such as a head and hand, are actually better than a full humangure in some respects. For example, a full human gure is more visually obtrusive, whichcan be a disadvantage since current headmounted displays oer a relatively narrow eld ofview. Nonetheless, a full human gure representation oers exciting possibilities it allowsmore realistic demonstrations of physical tasks and a richer use of gestures and other typesof nonverbal communication. Because our architecture makes it easy to plug in dierentbodies, we can evaluate the tradeos among them.We have experimented with several bodies for Steve. At the simple end of the spectrum,we tried a hand alone and then a hand and head. At the complex end, we tried a fullhuman gure, using the Jack software Badler, Phillips,  Webber 1993 developed at theUniversity of Pennsylvania. In the long run, Jack is an exciting prospect. However, our useof Jack was limited, since Jack comes with its own visual interface, and cannot run in others.Since his visual interface does not support our architecture for creating virtual worlds, ouruse of Jack was awkward we had to send him movement commands, then query him forthe resulting position and orientation of his body parts, then update our own graphicalrepresentation of Jacks body. Our most recent body for Steve was shown in Section 2. Itincludes the upper half of a full human gure, and the head includes movable eyes, eyelids,eyebrows, and lips.Regardless of which body we use, our approach to animation is the same the motorcontrol module sends out messages to move and rotate graphical models of Steves bodyparts. In contrast, some other researchers, such as Stone and Lester 1996 and Andre et al.1998, create a library of animation sequences, and they dynamically string these togetherto control their agents behavior. Our approach provides a ner granularity for behaviorand allows Steve to interact with new virtual worlds without requiring the course authorto build a domainspecic library of animation clips.The remainder of this section will discuss control of Steves body in more detail, specifically locomotion, gaze, and hand control.7.2 LocomotionTo control Steves locomotion, the cognition module sends a motor command to move Steveto a specied object. To implement this command, the motor control module performs several steps. First, it plans a collisionfree path from Steves current location to the speciedobject. Recall from Section 5 that the perception module maintains an adjacency graph forthe objects in the virtual world. An edge between two objects in the graph indicates thatSteve can move from one to the other without colliding with other objects e.g., a wall.Given Steves current location one object and his specied destination another object,the motor control module uses Dijkstras shortest path algorithm Cormen, Leiserson, Rivest 1989 to compute a path.Next, the motor control module moves Steve along this path, one leg at a time. Foreach leg of the path i.e., movement from one object to the next, it does the following1. It determines the location, in Cartesian coordinates, where Steve should end up. Todo this, it asks for a bounding sphere for the destination object from the perceptionmodule. Starting with the objects origin, it uses the objects radius and front vectorto determine a point at the front, right corner of the object. Finally, it uses a default28oset to move slightly farther in front of the object and to its right. If the courseauthor specied an agent location oset for the object, this is used instead of thedefault.2. Next, it sends a message to the visual interface components to cause Steves body andgaze to focus on the destination object.3. After waiting half a second for Steves shift of gaze to complete, it sends anothermessage to move Steve along a path from his current location to the specied location.When Steve arrives at the desired location, the visual interface components send a message.At this point, the perception module updates Steves location and the motor control modulesends him on the next leg of the path.7.3 GazeSteve shifts his gaze in many dierent situations. Some of these shifts are triggered explicitly by the cognition module. Others are triggered by the motor control module inperforming another motor command. In rare cases, gaze shifts can be triggered directlyby the perception module a sort of kneejerk reaction. Gaze shifts occur in the followingsituations When moving from location to location, he looks where he is going triggered by motorcontrol module. He looks at an object when manipulating it triggered by motor control module. He looks at an object before pointing at it triggered by motor control module. He looks at a person or agent when talking to them triggered by motor controlmodule. If someone other than he interacts with an object, he looks at the object triggeredby perception module. If he is waiting for someone, he looks at them triggered by cognition module. When he is monitoring a student performing a task, he looks at them triggered bycognition module. When executing a sensing action, he looks at the object being sensed triggered bycognition module. When someone informs him of something, he looks at them and nods triggered bycognition module.The code to control Steves gaze has recently become more autonomous. Previously,each movement of the head required the perception module to query the visual interfacecomponents for the position of the gazes target. After receiving this information, themotor control module sent a command to the visual interface components to rotate thehead towards the target. More recently, the visual interface components accept a commandto have Steves gaze track an object, person, or agent after animating the shift, the headis rotated automatically every frame to remain looking at the target. Moreover, the visual29interface components will recognize Steves limits of motion for example, if an object ismoving around Steve, he will track it over his left shoulder until it moves directly behindhim, at which point he will track it over his right shoulder.7.4 Hand ControlTo animate Steves hands, we dened four possible poses for each one resting, pointing,pressing, and grasping. When Steve is not doing something with his hands, they are restingat his sides. To manipulate or point at an object, the motor control module rst gets thebounding sphere for the object. Next, it sends commands to animate the movement of thehand to the object. The pressing and grasping hands are placed at the front side of theobject as specied by the objects front vector, and their orientation is determined bythe press and grasp vectors for the object, whichever is appropriate. The pointing handis placed at the point on the objects bounding sphere closest to Steves correspondingshoulder, oriented so that it points to the objects origin. The visual interface componentsanimate the movement of the hand from its initial position to its target position, controllingthe corresponding movements of the arms as needed.When Steves hand is in the proper position, the motor control module sends a command to tether it to the object i.e., sustain a constant position and orientation relativeto the object. This serves two purposes. First, it allows Steve to turn his body e.g., tospeak to the student without causing an undesired change in the hands position relativeto the object. Second, it supports the hand animation for Steves object manipulations.For example, after tethering Steves nger to a button, the motor control module sends acommand to the simulator to simulate the button being pressed. The simulator animatesthe movement of the button, and Steves nger and hence hand and arm tracks the movement of the button, providing the illusion that he is pushing it. This approach works wellwhen the objects movement is within the exibility of Steves arms and hands, which hasbeen the case so far.8 Status and EvaluationSteve has been tested on a variety of Naval operating procedures. He can perform tasks onseveral of the consoles that are used to control the gas turbine engines that propel Navalships, he can check and manipulate some of the valves that surround these engines, and hecan perform a handful of procedures on the highpressure air compressors that are part ofthese engines. We are continuing to extend his capabilities in these areas.We are planning a set of evaluations, both within USC and in collaboration with theAir Force Armstrong Laboratory. We plan to investigate experimentally which factorscontribute to the eectiveness of agentbased instruction. In particular, we are interestedin determining which of the following factors are critical a whether or not the agents cancohabit the virtual world with students, b the type of embodiment graphical realizationof the agent, c whether or not the agents have pedagogical capabilities, and d the degreeof delity and believability of the agents behavior.While this paper focuses on training a single student to perform a oneperson task, wehave extended Steve to support team training. This required extensions to Steves taskknowledge to represent the various team members and the task steps for which they areresponsible, extensions that allow Steve to make use of such knowledge, and extensions toallow Steve to generate and understand taskspecic communication with teammates. A30short paper by Johnson et al. 1998 provides a brief overview, and the details will appearin a future paper. In our most complicated team scenario to date, ve team membersmust work together to handle a loss of fuel oil pressure in one of the ships gas turbineengines. This task involves a number of subtasks, some of which are individual tasks whileothers involve subteams. All together, the task consists of about three dozen actions bythe various team members. We have tested this scenario with two students and ve agentsthree of the agents serve as the students team members, and two of the agents serve astheir tutors.9 Related WorkThe most closely related pedagogical agent for virtual reality was developed by Billinghurstand his colleagues Billinghurst  Savage 1996 Billinghurst et al. 1996. Their agent inhabits a threedimensional, simulated nasal cavity, providing assistance in sinus surgery tomedical students. The agent can demonstrate surgical steps, monitor students performingsurgery, intervene when a student skips a step, and tell a student what to do next whenasked. However, their agent does not have an animated form it communicates with studentsvia a disembodied voice, and it demonstrates surgical steps by moving virtual instrumentsaround and controlling the students viewpoint. Unlike Steve, their agent is also capableof natural language understanding and gesture recognition. Their agent represents domaintasks as hierarchical scripts Schank  Abelson 1977, which are similar to Steves hierarchical plans. However, whereas Steve continually reevaluates his plans against the currentstate of the virtual world, their agent merely keeps track of which steps have been executed,so it cannot adapt to unexpected events or allow the student exibility in performing tasksas Steve can.Lester and his colleagues are developing two animated pedagogical agents, Herman theBug Stone  Lester 1996 and Cosmo Lester et al. 1998. These agents do not inhabitthreedimensional virtual worlds they appear as twodimensional characters oating ontop of a twodimensional image of a simulated world. The agents are notable for theirapproach to behavior control they control their behavior by dynamically selecting audioand visual segments from a large, domainspecic library. This approach is quite laborintensive, requiring considerable eort by artists and animators in building up the library,but it results in highquality animation. Unlike Steve, Herman and Cosmo do not interactwith a simulator, nor do they have any abilities to plan or replan procedural tasks.Several people have developed animated agents that can generate presentations. ThePPP Persona Andre  Rist 1996 Andre, Rist,  Mueller 1998 is an animated agentthat combines speech and gestures to describe procedures for operating physical devices.The agents body is controlled by ipping between dierent bitmap images of the agent indierent poses. The agent cannot interact with a simulation, and it has no pedagogicalcapabilities except the ability to describe a procedure. However, it is notable for its abilityto plan and schedule a sequence of presentation acts e.g., speech and gestures. Anotheragent, Presenter Jack Noma  Badler 1997, is a full human gure that uses speech,gestures, and shortrange locomotion to give presentations. The human gure animationis provided by the Jack software Badler, Phillips,  Webber 1993. Unlike Steve, thepresentations are not interactive they are scripted by a human. The work is notable for itsuse of a full human gure and its analysis of how gestures and gaze are used in presentations.A variety of researchers have studied control of animated human gures. Several projects31at the University of Pennsylvania are most relevant to our work. Although none of theseprojects has focused on pedagogical or presentation capabilities, they are notable for theirsophisticated control of animated humans. Trias et al. 1996 developed an agent thatcan play hideandseek with other virtual agents. The agent uses a hierarchical plannerfor some complex actions, incorporates a separate search planner for nding objects inthe environment, and can move around in the virtual environment. Geib et al. 1994developed an agent that integrates a highlevel planner with a search planner for ndingobjects and another planner for manipulating objects. The ability to realistically graspobjects in a taskdependent manner, as described by Douville et al. 1996, would bean especially valuable extension to Steve. Cassell et al. 1994 developed an agent thatintegrates speech, gestures, and facial expressions in the context of a dialogue. Their agentuses a greater variety of nonverbal communicative acts than Steve, and these acts are alsomore tightly integrated with spoken utterances such close coupling of verbal and nonverbalcommunication is crucial to achieving humanlike conversational abilities in Steve.In addition to improving Steves conversational abilities, we must improve the studentsability to communicate with Steve. The most critical problem is that Steve is not capableof understanding natural language, so the student is limited to prespecied speech utterances. The TRAINS system Allen et al. 1996 Ferguson, Allen,  Miller 1996 supportsa robust spoken dialogue between a computer agent and a person working together on atask. However, their agent has no animated form, and does not cohabit a virtual worldwith users. Because TRAINS and Steve carry on similar types of dialogues with users, yetfocus on dierent aspects of such conversations, a combination of the two systems seemspromising. Ultimately, we must allow students to use the full range of nonverbal communicative acts that people employ in facetoface communication. For example, the Gandalfagent Thorisson 1996 Cassell  Thorisson 1998 supports full multimodal conversationbetween human and computer. Like other systems, Gandalf combines speech, gesture, intonation and facial expression. Unlike most other systems, Gandalf also perceives thesecommunicative signals in humans people talking with Gandalf wear a suit that tracks theirupper body movement, an eye tracker that tracks their gaze, and a microphone that allowsGandalf to hear their words and intonation. Although it may be some time before technology like Gandalf is practical, the system points the way towards an exciting future forhumancomputer interaction.10 ConclusionSteve illustrates the enormous potential in combining work in agent architectures, intelligenttutoring, and graphics. Steve draws on work in agent architectures by sensing the state ofthe world, assessing task goals, constructing and revising plans, and sending out motorcommands to control the virtual world, all in a decision cycle that is executed multipletimes per second. He draws on work in intelligent tutoring by explaining tasks, monitoringstudents, and answering questions. He draws on work in computer graphics to control hisanimated body, including locomotion, gaze, gestures, and demonstrations of actions. Whencombined, these technologies result in a new breed of computer tutor a humanlike agentthat can interact with students in a virtual world to help them learn.3211 AcknowledgmentsThis work is funded by the Oce of Naval Research, grant N0001495C0179. We aregrateful for the contributions of our many collaborators Randy Stiles and his colleagues atLockheed Martin developed the visual interface component Allen Munro and his colleaguesat Behavioral Technologies Laboratory developed the simulator and Richard Angros, BenMoore, Behnam Salemi, Erin Shaw, and Marcus Thiebaux at ISI contributed to Steve. Weare especially grateful to Marcus, who developed the 3D model of Steves current body andthe code in the visual interface component that controls its animation.ReferencesAllen, J. F. Miller, B. W. Ringger, E. K. and Sikorski, T. 1996. Robust understandingin a dialogue system. In Proceedings of the 34th Annual Meeting of the Association forComputational Linguistics, 6270.Anderson, J. R. Corbett, A. T. Koedinger, K. R. and Pelletier, R. 1995. Cognitivetutors Lessons learned. Journal of the Learning Sciences 42167207.Andre, E., and Rist, T. 1996. Coping with temporal constraints in multimedia presentationplanning. In Proceedings of the Thirteenth National Conference on Articial IntelligenceAAAI96, 142147. Menlo Park, CA AAAI PressMIT Press.Andre, E. Rist, T. and Mueller, J. 1998. Employing AI methods to control the behaviorof animated interface agents. Applied Articial Intelligence. This issue.Badler, N. I. Phillips, C. B. and Webber, B. L. 1993. Simulating Humans. New YorkOxford University Press.Billinghurst, M., and Savage, J. 1996. Adding intelligence to the interface. In Proceedingsof the IEEE Virtual Reality Annual International Symposium VRAIS 96, 168175. LosAlamitos, CA IEEE Computer Society Press.Billinghurst, M. Savage, J. Oppenheimer, P. and Edmond, C. 1996. The expert surgicalassistant An intelligent virtual environment with multimodal input. In Proceedings ofMedicine Meets Virtual Reality IV.Burton, R. R. 1982. Diagnosing bugs in a simple procedural skill. In Sleeman, D., andBrown, J., eds., Intelligent Tutoring Systems. Cambridge, MA Academic Press. 157183.Cassell, J., and Thorisson, K. R. 1998. The power of a nod and a glance Envelope vs.emotion in animated conversational agents. Applied Articial Intelligence. This issue.Cassell, J. Pelachaud, C. Badler, N. Steedman, M. Achorn, B. Becket, T. Douville,B. Prevost, S. and Stone, M. 1994. Animated conversation Rulebased generation offacial expression, gesture and spoken intonation for multiple conversational agents. InProceedings of ACM SIGGRAPH 94.Cormen, T. H. Leiserson, C. E. and Rivest, R. L. 1989. Introduction to Algorithms. NewYork McGrawHill.Douville, B. Levison, L. and Badler, N. I. 1996. Tasklevel object grasping for simulatedagents. Presence Teleoperators and Virtual Environments 54416430.33Ferguson, G. Allen, J. and Miller, B. 1996. Trains95 Towards a mixedinitiativeplanning assistant. In Proceedings of the Third Conference on AI Planning Systems.Geib, C. Levison, L. and Moore, M. B. 1994. Sodajack An architecture for agentsthat search and manipulate objects. Technical Report MSCIS9416LINC LAB 265,Department of Computer and Information Science, University of Pennsylvania.Grosz, B. J., and Sidner, C. L. 1986. Attention, intentions, and the structure of discourse.Computational Liguistics 123175204.Johnson, W. L. Rickel, J. Stiles, R. and Munro, A. 1998. Integrating pedagogical agentsinto virtual environments. Presence Teleoperators and Virtual Environments 76523546.Johnson, W. L. Marsella, S. and Rickel, J. 1998. Pedagogical agents in virtual teamtraining. In Proceedings of the Virtual Worlds and Simulation Conference, volume 30 ofSimulation Series. San Diego, CA Society for Computer Simulation International.Johnson, W. L. 1994. Agents that learn to explain themselves. In Proceedings of the TwelfthNational Conference on Articial Intelligence AAAI94, 12571263. Menlo Park, CAAAAI Press.Korth, H. F., and Silberschatz, A. 1986. Database System Concepts. New York McGrawHill.Labrou, Y., and Finin, T. 1994. A semantics approach for KQML  a general purposecommunication language for software agents. In Proceedings of the Third InternationalConference on Information and Knowledge Management. ACM Press.Laird, J. E. Newell, A. and Rosenbloom, P. S. 1987. Soar An architecture for generalintelligence. Articial Intelligence 331164.Lester, J. C. Voerman, J. L. Towns, S. G. and Callaway, C. B. 1998. Deictic believability Coordinating gesture, locomotion, and speech in lifelike pedagogical agents. AppliedArticial Intelligence. This issue.McAllester, D., and Rosenblitt, D. 1991. Systematic nonlinear planning. In Proceedingsof the Ninth National Conference on Articial Intelligence AAAI91, 634639. MenloPark, CA AAAI Press.Moore, J. D. 1993. What makes human explanations eective In Proceedings of the 15thAnnual Conference of the Cognitive Science Society, 131136.Munro, A., and Surmon, D. 1997. Primitive simulationcentered tutor services. In Proceedings of the AIED Workshop on Architectures for Intelligent SimulationBased LearningEnvironments.Munro, A. Johnson, M. Surmon, D. and Wogulis, J. 1993. Attributecentered simulationauthoring for instruction. In Proceedings of the World Conference on Articial Intelligencein Education AIED 93, 8289. Association for the Advancement of Computing inEducation.Newell, A. 1990. Unied Theories of Cognition. Cambridge, MA Harvard UniversityPress.Noma, T., and Badler, N. I. 1997. A virtual human presenter. In Proceedings of the IJCAIWorkshop on Animated Interface Agents Making Them Intelligent, 4551.34Rickel, J. 1988. An intelligent tutoring framework for taskoriented domains. In Proceedingsof the International Conference on Intelligent Tutoring Systems.Russell, S., and Norvig, P. 1995. Articial Intelligence A Modern Approach. EnglewoodClis, NJ Prentice Hall.Sacerdoti, E. 1977. A Structure for Plans and Behavior. New York Elsevier NorthHolland.Schank, R., and Abelson, R. 1977. Scripts, Plans, Goals and Understanding. Hillsdale,NJ Lawrence Erlbaum Associates.Stiles, R. McCarthy, L. and Pontecorvo, M. 1995. Training studio A virtual environmentfor training. In Workshop on Simulation and Interaction in Virtual Environments SIVE95. Iowa City, IW ACM Press.Stone, B. A., and Lester, J. C. 1996. Dynamically sequencing an animated pedagogicalagent. In Proceedings of the Thirteenth National Conference on Articial IntelligenceAAAI96, 424431. Menlo Park, CA AAAI PressMIT Press.Thorisson, K. R. 1996. Communicative Humanoids A Computational Model of Psychosocial Dialogue Skills. Ph.D. Dissertation, Massachusetts Institute of Technology.Trias, T. S. Chopra, S. Reich, B. D. Moore, M. B. Badler, N. I. Webber, B. L. andGeib, C. W. 1996. Decision networks for integrating the behaviors of virtual agentsand avatars. In Proceedings of the IEEE Virtual Reality Annual International SymposiumVRAIS 96, 156162. Los Alamitos, CA IEEE Computer Society Press.Weld, D. S. 1994. An introduction to least commitment planning. AI Magazine 1542761.35
