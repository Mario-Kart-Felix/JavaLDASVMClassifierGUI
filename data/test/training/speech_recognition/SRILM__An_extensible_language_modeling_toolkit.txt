SRILM  AN EXTENSIBLE LANGUAGE MODELING TOOLKITAndreas StolckeSpeech Technology and Research LaboratorySRI International, Menlo Park, CA, U.S.A.httpwww.speech.sri.comABSTRACTSRILM is a collection of C libraries, executable programs, andhelper scripts designed to allow both production of and experimentation with statistical language models for speech recognition andother applications. SRILM is freely available for noncommercialpurposes. The toolkit supports creation and evaluation of a variety of language model types based on Ngram statistics, as wellas several related tasks, such as statistical tagging and manipulation of Nbest lists and word lattices. This paper summarizesthe functionality of the toolkit and discusses its design and implementation, highlighting ease of rapid prototyping, reusability, andcombinability of tools.1. INTRODUCTIONStatistical language modeling is the science and often art ofbuilding models that estimate the prior probabilities of wordstrings. Language modeling has many applications in natural language technology and other areas where sequences of discrete objects play a role, with prominent roles in speech recognition andnatural language tagging including specialized tasks such as partofspeech tagging, word and sentence segmentation, and shallowparsing. As pointed out in 1, the main techniques for effective language modeling have been known for at least a decade, although one suspects that important advances are possible, and indeed needed, to bring about significant breakthroughs in the application areas cited abovesuch breakthroughs just have been veryhard to come by 2, 3.Various software packages for statistical language modelinghave been in use for many yearsthe basic algorithms are simpleenough that one can easily implement them with reasonable effortfor research use. One such package, the CMUCambridge LMtoolkit 1, has been in wide use in the research community andhas greatly facilitated the construction of language models LMsfor many practitioners.This paper describes a fairly recent addition to the set ofpublicly available LM tools, the SRI Language Modeling ToolkitSRILM. Compared to existing LM tools, SRILM offers a programming interface and an extensible set of LM classes, severalnonstandard LM types, and more a comprehensive functionalitythat goes beyond language modeling to include tagging, Nbestrescoring, and other applications. This paper describes the designphilosophy and key implementation choices in SRILM, summarizes its capabilities, and concludes by discussing deficiencies andplans for future development. For lack of space we must refer toother publications for an introduction to language modeling and itsrole in speech recognition and other areas 3, 4.2. DESIGN GOALS AND HISTORYSRILM grew out of a dissatisfaction with previously available LMtools at SRI, and a desire to design an LM toolkit from the groundup, with the following goals in mind Efficient and careful implementation of stateoftheart LMalgorithms, to support development of competitive systems,mainly in speech recognition. Flexibility and extendibility, so as to facilitate research intonew types of LMs, while being able to reuse existing components. A rational, clean software design, providing both an application programming interface API and a convenient toolbox ofcommands for LM building and testing.The design was influenced by other related software implementations. The CMUCambridge toolkit 1, and discussionswith its original author, Roni Rosenfeld, served as a general inspiration and reference point. The HTK Lattice Toolkit 5 to whichSRILM has an interface provided many good ideas for a viable andefficient API for language models. The decision to explore objectoriented design was based on a prior project, an implementation ofvarious types of statistical grammars in the Common Lisp ObjectSystem 6. The software build system was borrowed from SRIsDecipherTM speech recognition system 7.A first implementation with minimal functionality for standardNgram models was created prior to the 1995 Johns Hopkins Language Modeling Summer Workshop 8. By the end of the workshop, support for dynamic LM interpolation and Nbest rescoringhad been added, and a small community of users outside SRI withan associated mailing list existed. Over the next four years a series of alpha versions were made available to this small group,while much of the current functionality described below was being added. In July 1999 a beta version was released for general distribution under an open source license, followed about a year laterby version 1.0. As of this writing, the latest released version is 1.3,which added a word graph rescoring tool, a test suite, and support for Windows platforms previous versions were Unixonly.Most ongoing governmentfunded LM research and developmentat SRI is based on SRILM we therefore expect a steady stream offunctionality enhancements as well as bug fixes to continue.3. FUNCTIONALITY3.1. Basic LM operationsThe main purpose of SRILM is to support language model estimation and evaluation. Estimation means the creation of a modelfrom training data evaluation means computing the probabilityof a test corpus, conventionally expressed as the test set perplexity.Since most LMs in SRILM are based on Ngram statistics, the toolsto accomplish these two purposes are named ngramcount andngram, respectively. A standard LM trigram with GoodTuringdiscounting and Katz backoff for smoothing would be created byngramcount text TRAINDATA lm LMThe resulting LM may then be evaluated on a test corpus usingngram lm LM ppl TESTDATA debug 2The ngram debug option controls the level of detail ofdiagnostic output. A value of 2 means that probabilities are tobe reported at the word level, including the order of Ngram used,in addition to the standard log probabilities and perplexities. Someadditional statistics that also help gauge LM quality are the numberof outofvocabulary OOV words and the hit rates of variouslevels of Ngrams in LMs based on Ngrams 1 these are eithercomputed by ngram itself or as in the case of hit rates tallied byauxiliary scripts that analyze the ngram output.SRILM by itself performs no text conditioning, and treats everything between whitespace as a word. Normalization and tokenization of text are highly corpusdependent, and are typicallyaccomplished with filters that preprocess the data.3.2. Bells and whistlesThe programs ngramcount and ngram have a rather largenumber of options to control the many parameters of LM estimation and testing. The most important parameters for LM trainingare the order of Ngrams to use e.g., unigram, bigram. There isno builtin limit on the length of Ngrams. the type of discounting algorithm to use. Currently supported methods include GoodTuring, absolute, WittenBell,and modified KneserNey 9. Each of these discounting methods requires its own set of parameters, as well as a choice ofwhether higher and lowerorder estimates are to be combinedvia backoff or interpolation. an optional predefined vocabulary to limit or augment the setof words from the training data. whether unknown words are to be discarded or treated as aspecial unknown word token. whether to collapse case distinctions in the input text.Beyond LM estimation, ngramcount performs useful Ngramcount manipulations, such as generating counts from text, summing count files, and recomputing lowerorder counts from higherorder counts. ngramcount handles integer or fractional counts,although only a subset of the smoothing algorithms supports thelatter generally speaking, those that rely on countsofcountsstatistics do not.The main parameters controlling LM evaluation are the orderof Ngram to use which can be lower than what the LM includes,so that a 4gram model may conveniently be used also as a bigramor trigram model, and the variant of Ngram model to useforexample, a wordbased, classbased, or interpolated Ngram, aswell as any additional parameters associated with that type of LM.The types of models supported are described further in Section 3.3.Beyond LM evaluation, ngram is really a tool to manipulateLMs in a variety of ways. Besides computing test set log probabilities from text or counts, it can renormalize a model recomputing backoff weights approximate a classbased or interpolated Ngram with a standard wordbased backoff LM prune Ngram parameters, using an entropy criterion 10 prepare LMs for conversion to finitestate graphs by removingNgrams that would be superseded by backoffs generate random sentences from the distribution embodied bythe LM.The ability to approximate classbased and interpolated Ngram LMs by a single word Ngram model deserves some discussion. Both of these operations are useful in situations whereother software e.g., a speech recognizer supports only standardNgrams. Class Ngrams are approximated by expanding class labels into their members which can contain multiword strings andthen computing the marginal probabilities of word Ngram strings.This operation increases the number of Ngrams combinatorially,and is therefore feasible only for relatively small models.An interpolated backoff model is obtained by taking the unionof Ngrams of the input models, assigning each Ngram theweighted average of the probabilities from those models in someof the models this probability might be computed by backoff, andthen renormalizing the new model. We found that such interpolated backoff models consistently give slightly lower perplexitiesthan the corresponding standard wordlevel interpolated models.The reason could be that the backoff distributions are themselvesobtained by interpolation, unlike in standard interpolation, whereeach component model backs off individually.3.3. Language model typesBesides the standard wordbased Ngram backoff models, SRILMimplements several other LM types, most of them based on Ngrams as basic building blocks.Classbased models  Ngrams over word classes are an effective way to increase the robustness of LMs and to incorporatedomain knowledge, e.g., by defining word classes reflecting thetask semantics. SRILM allows class members to be multiwordstrings e.g., san francisco can be a member of class CITYNAME. This, and the fact that words can belong to more thanone class, requires the use of dynamic programming to evaluatea class Ngram. Word classes may be defined manually or by aseparate program, ngramclass, which induces classes frombigram statistics using the Brown algorithm 11.Cache models  This wellknown LM technique assignsnonzero probability to recent words, thus modeling the tendencyof words to reoccur over short spans 12. They are usually interpolated with a standard model to obtain an adaptive LM.Disfluency and hidden event language models  Hidden eventLMs incorporate special words that appear in the models Ngrams, but are not overt in the observed word stream. Instead, theycorrespond to the states of a hidden Markov model, and can beused to model linguistic events such as unmarked sentence boundaries. Optionally, these events can be associated with nonlexicallikelihoods to condition the LM on other knowledge sources e.g.,prosody 13. A special type of hidden event LM can modelspeech disfluencies by allowing the hidden events to modify theword history for example, a word deletion event would erase oneor more words to model a false start 14.Skip language models  In this LM, words in the history areprobabilistically skipped, allowing more distant words to take theirplaces. The skipping probabilities associated with each word areestimated using expectation maximization.HMMs of Ngrams  This LM consists of a hidden Markovmodel HMM where each state is associated with its own Ngramdistribution. The model generates from a certain state until the local Ngram issues an endofsentence, at which point it transitionsprobabilistically to a neighboring state. HMMs of Ngrams provide a general framework that can encode a variety of LM typesproposed in the literature, such as sentencelevel mixtures 15 andpivot LMs 16.Dynamically interpolated LMs  Two or more LMs can beinterpolated linearly at the word level such that the interpolationweights reflect the likelihoods of the models given the recent Ngram history 8. With a null history, we obtain the usual staticLM interpolation approach that is often used to combine differentsources of training material into a single model.3.4. Other applications of language modelsOver the years SRILM has evolved to include tools that go beyondsimple LM construction and evaluation, covering mainly LM applications arising in speech recognition.disambig  an HMMbased tagger that uses Ngram LMs ofarbitrary order to model the prior on tag sequences.hiddenngram  a word boundary tagger, based on hiddenevent Ngram models.segmentnbest  a rescoring tool that applies a languagemodel over a sequence of adjoining Nbest lists, thereby overcoming sentence segmentation mismatch 17.latticetool  a tool to rescore and expand word lattices.nbestlattice  a tool to perform word error minimization on Nbest lists 18 or construct confusion networkssausages 19. Together with a helper script, this tool alsoimplements a word posteriorbased Nbest generalization ofthe ROVER algorithm 20, 21.nbestscripts  a collection of wrapper scripts that manipulate and rescore Nbest lists.pfsgscripts  for converting LMs to word graphs.nbestoptimize  optimizes log linear score combinationfor word posteriorbased sausage decoding.4. DESIGN AND IMPLEMENTATIONSRILM is designed and implemented in three layers.1. At the core are libraries comprising about 50 C classesfor language models and miscellaneous objects such as vocabulary symbol tables, Nbest lists, word graphs, DP trellises, which in turn make use of a library of efficient containerclasses e.g., arrays, hash tables.2. The 14 main executable tools such as ngramcount,ngram, and taggers, are written in C on top of the APIprovided by the libraries.3. Numerous helper and wrapper scripts perform miscellaneoustasks that are more conveniently implemented in the gawk andBourne shell scripting languages.We summarize the characteristics of each implementation layer.4.1. Class librariesC class libraries implement the API of SRILM. Objectorientedprogramming turns out to be an excellent match for LM implementation, for several reasons. A class hierarchy naturally reflects thespecialization relation between different LM types e.g., a class Ngram is a special case of an Ngram, which is a special case of anLM. Inheritance allows new LM variants to be derived from existing ones with minimal effort. A new LM class minimally needsto define only a wordProb function, the method used to computeconditional probabilities given a word and its history.1 Most LMfunctions are defined generically, and need not be reimplementedfor a new derived LM class. For example, sentenceProb isdefined in terms of wordProb and typically inherited from thegeneric LM class however, a given LM can define its own version of sentenceProb, for efficiency or to change the standardbehavior.Hash tables, arrays, tries, and other basic data structures havebeen implemented from scratch, for speed and compactness underthe types of uses associated with LM data structures.2 Templatizeddata structures and functions are very useful beyond the lowlevelcontainers Ngram statistics and estimation functions, for example, are templatized to support both integer and fractional counts.4.2. Executable toolsThe executable tools are designed to be both selfcontained andcombinable in useful ways. Thus, as shown earlier, a standard LMcan be built from a text file in a single invocation. More complexmanipulations are possible by chaining together tools, using theUnix standard inputoutput and pipe mechanism. Thus, a classbased Ngram model can be trained, pruned, expanded into a wordtrigram model, and interpolated with another model using the following pipeline some options have been omitted to save spacereplacewordswithclasses TRAINDATA  ngramcount text  lm   ngram lm  prune 1e5 writelm   ngram lm  expandclasses 3 writelm   ngram lm  mixlm LM2 writelm MIXLM4.3. Helpers and wrappersMiscellaneous other tools are implemented in gawk and shellscripts, either because they involve simple text manipulations that are more conveniently done this way such asreplacewordswithclasses in the example above, orbecause they only require a wrapper that combines the basic tools.An example of the latter is changelmvocab, a script thatmodifies the vocabulary of an existing Ngram LM. The scripteliminates Ngrams that have become OOV using the textual LMformat and then lets ngram fill in new unigrams and renormalizethe backoff weights. Other scripts parse the diagnostic output ofthe tools, such as computebestmix, which computes optimal interpolation weights from ngram ppl output.1Often a read function is also needed, but can be borrowed from anexisting class if the same external representation is used, as is frequentlythe case with Ngram based models.2We considered switching to the Standard Template Library STL forcontainers, but found that this would incur a significant loss of both speedand compactness.4.4. File formatsSRILM uses standard file formats where possiblein particular,the ARPA format for Ngram backoff LMs. Word graphs use SRIsprobabilistic finitestate grammar PFSG format, which can beconverted to and from that used by ATTs finite state machinetoolkit 22. Where new file formats were needed we chose easytoparse textual representations. All the main tools can read andwrite compressed files, as large amounts of data and models areoften associated with LM work. We avoided binary file formatsbecause of their lack of portability and flexibility, and prefer to usecompressed textual formats instead.5. SHORTCOMINGS AND FUTURE DEVELOPMENTSMany wellestablished LM techniques are not implemented inSRILM, such as deleted interpolation or maximum entropy modeling, mainly because these have not proven essential in our work sofar. One candidate for future addition is a more flexible classbasedmodel, since refinements of classbased LMs seem to provide aneffective and efficient way to incorporate grammatical informationinto the LM 23. The lowlevel implementation of data structures is currently biased toward speed and convenience rather thanmemory usage it might be worthwhile to reevaluate this choiceto accommodate everlarger training corpora. SRILM currently assumes singlebyte character encoding and uses only whitespace fortokenization it would be desirable to include support for multibyte character sets and SGMLtagged formats at some point. Ultimately, however, development of the toolkit will continue to bedriven by research needs, and is therefore hard to predict.Availability. SRILM is freely available for noncommercial users under an Open Source Community License, designedto ensure that enhancements by others find their way backinto the user community. Licensing for commercial purposesis also available. Documentation and software are online athttpwww.speech.sri.comprojectssrilm.6. ACKNOWLEDGMENTSFuliang Weng wrote the initial version of the lattice rescoring tool inSRILM Dimitra Vergyri developed the score combination optimizer basedon simplex search Anand Venkataraman contributed Nbest decodingand other enhancements to the statistical tagging tools. Development ofSRILM has benefited greatly from its use and constructive criticism bymany colleagues at SRI, the Johns Hopkins summer workshops, and thelarger research community. Barbara Peskin helped improve this paperwith valuable suggestions. The work described here was in part supportedby DARPA under contract N6600197C8544 and by NSFSTIMULATEgrant IRI9619921. The views herein are those of the author and do notreflect the policies of the funding agencies.7. REFERENCES1 P. Clarkson and R. Rosenfeld, Statistical language modeling usingthe CMUCambridge toolkit, in G. Kokkinakis, N. Fakotakis, andE. Dermatas, editors, Proc. EUROSPEECH, vol. 1, pp. 27072710,Rhodes, Greece, Sep. 1997.2 F. Jelinek, Up from trigrams The struggle for improved languagemodels, in Proc. EUROSPEECH, pp. 10371040, Genova, Italy,Sep. 1991.3 R. Rosenfeld, Two decades of statistical language modeling Wheredo we go from here, Proceedings of the IEEE, vol. 88, 2000.4 D. Jurafsky and J. H. Martin, Speech and Language Processing AnIntroduction to Natural Language Processing, Computational Linguistics, and Speech Recognition, PrenticeHall, Upper Saddle River,NJ, 2000.5 J. J. Odell, Lattice and Language Model Toolkit Reference Manual,Entropic Cambridge Research Laboratories, Inc., 1995.6 A. Stolcke, Bayesian Learning of Probabilistic Language Models,PhD thesis, University of California, Berkeley, CA, July 1994.7 H. Murveit, J. Butzberger, V. Digalakis, and M. Weintraub, Largevocabulary dictation using SRIs DECIPHER speech recognition system Progressive search techniques, in Proc. ICASSP, vol. II, pp.319322, Minneapolis, Apr. 1993.8 M. Weintraub, Y. Aksu, S. Dharanipragada, S. Khudanpur, H. Ney,J. Prange, A. Stolcke, F. Jelinek, and E. Shriberg, LM95 ProjectReport Fast training and portability, Research Note 1, Center forLanguage and Speech Processing, Johns Hopkins University, Baltimore, Feb. 1996.9 S. F. Chen and J. Goodman, An empirical study of smoothing techniques for language modeling, Technical Report TR1098, Computer Science Group, Harvard University, Aug. 1998.10 A. Stolcke, Entropybased pruning of backoff language models,in Proceedings DARPA Broadcast News Transcription and Understanding Workshop, pp. 270274, Lansdowne, VA, Feb. 1998. Morgan Kaufmann.11 P. F. Brown, V. J. Della Pietra, P. V. deSouza, J. C. Lai, and R. L.Mercer, Classbased ngram models of natural language, Computational Linguistics, vol. 18, pp. 467479, 1992.12 R. Kuhn and R. de Mori, A cachebase natural language model forspeech recognition, IEEE PAMI, vol. 12, pp. 570583, June 1990.13 A. Stolcke, E. Shriberg, D. HakkaniTur, and G. Tur, Modelingthe prosody of hidden events for improved word recognition, inProc. EUROSPEECH, vol. 1, pp. 307310, Budapest, Sep. 1999.14 A. Stolcke and E. Shriberg, Statistical language modeling for speechdisfluencies, in Proc. ICASSP, vol. 1, pp. 405408, Atlanta, May1996.15 R. Iyer, M. Ostendorf, and J. R. Rohlicek, Language modeling withsentencelevel mixtures, in Proc. ARPA HLT Workshop, pp. 8286,Plainsboro, NJ, Mar. 1994.16 K. W. Ma, G. Zavaliagkos, and M. Meteer, Subsentence discoursemodels for conversational speech recognition, in Proc. ICASSP,vol. II, pp. 693696, Seattle, WA, May 1998.17 A. Stolcke, Modeling linguistic segment and turn boundaries for Nbest rescoring of spontaneous speech, in G. Kokkinakis, N. Fakotakis, and E. Dermatas, editors, Proc. EUROSPEECH, vol. 5, pp.27792782, Rhodes, Greece, Sep. 1997.18 A. Stolcke, Y. Konig, and M. Weintraub, Explicit word error minimization in Nbest list rescoring, in G. Kokkinakis, N. Fakotakis,and E. Dermatas, editors, Proc. EUROSPEECH, vol. 1, pp. 163166,Rhodes, Greece, Sep. 1997.19 L. Mangu, E. Brill, and A. Stolcke, Finding consensus in speechrecognition Word error minimization and other applications of confusion networks, Computer Speech and Language, vol. 14, pp. 373400, Oct. 2000.20 J. G. Fiscus, A postprocessing system to yield reduced word error rates Recognizer output voting error reduction ROVER, inProceedings IEEE Automatic Speech Recognition and Understanding Workshop, pp. 347352, Santa Barbara, CA, 1997.21 A. Stolcke, H. Bratt, J. Butzberger, H. Franco, V. R. Rao Gadde,M. Plauche, C. Richey, E. Shriberg, K. Sonmez, F. Weng, andJ. Zheng, The SRI March 2000 Hub5 conversational speech transcription system, in Proceedings NIST Speech Transcription Workshop, College Park, MD, May 2000.22 M. Mohri, F. Pereira, and M. Riley, FSM Librarygeneralpurpose finitestate machine software tools, version 3.6,httpwww.research.att.comswtoolsfsm, 1998.23 W. Wang, Y. Liu, and M. P. Harper, Rescoring effectiveness oflanguage models using different levels of knowledge and their integration, in Proc. ICASSP, Orlando, FL, May 2002.
