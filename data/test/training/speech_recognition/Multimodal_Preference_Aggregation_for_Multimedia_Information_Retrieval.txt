Multimodal Preference Aggregation forMultimedia Information RetrievalEric Bruno, Stephane MarchandMailletViper group  Computer Vision and Multimedia Lab  University of Geneva, SwitzerlandEmail name.surnameunige.chAbstractRepresenting and fusing multimedia information is a key issue to discover semantics in multimedia.In this paper we address more specifically the problemof multimedia content retrieval through the joint designof an original multimodal information representation andof a machine learningbased fusion algorithm. We firstdefine a novel preferencebased representation particularlyadapted to the retrieval problem, and then, we investigatethe RankBoost algorithm to combine those preferences tofullfill a users query. Interestingly, it ends up being a flexibleretrieval model that only manipulates ranking informationand is blind to the intrinsic properties of the multimodalinformation input. The approach is tested on annotatedimages and on the complete TRECVID 2005 corpus andcompared with SVMbased fusion strategies. The resultsshow that our approach equals SVM performance but,contrary to SVM, is parameter free and faster.I. INTRODUCTIONDetermining semantic concepts by allowing users toiteratively and interactively refine their queries is a keyissue in multimedia contentbased retrieval. The Relevance Feedback loop allows us to build complex queriesmade out of documents marked as positive and negativeexamples. From this training set, a learning process hasto create a model of the sought concept from a set ofdata features to finally provide relevant documents to theuser. The success of this search strategy relies mainlyon the representation spaces where data is embeddedas well as on the learning algorithm operating in thosespaces. These two issues are also intrinsically related tothe problem of adequately fusing information arising fromdifferent sources. Various aspects of these problems havebeen studied with success for the last few years. Thisincludes works on machine learning strategies such asactive learning 1, imbalance classification algorithms2, automatic kernel setting 3 or automatic labellingof training data 4. Theoretical and experimental investigations have been achieved to determine optimalstrategies for multimodal fusion Kittler et al and R. Duinstudied different rules for classifier combination 5, 6Wu et al propose the superkernel fusion to determineThis work is partially supported by the Swiss NCCR IM2 Interactive Multimodal Information Management and the European Community under the Information Society Technologies IST programme ofthe 6th FP for RTD  project MultiMATCH contract IST20052.5.10.The author is solely responsible for the content of this paper. It doesnot represent the opinion of the European Community, and the EuropeanCommunity is not responsible for any use that might be made of dataappearing therein the IST European project Multimatch.optimal combination of features for video retrieval 7. In8, Maximum Entropy, Boosting and SVM algorithmsare compared to fuse audiovisual features. Multigraphlearning approaches 9 and latent semantic fusion 10have been proposed recently for image and video retrievaland annotation. A number of further relevant referencesmay be found into the Lecture Notes series on MultipleClassifier Systems 11.The diversity of the features involved is a difficultywhen dealing with fusion and learning. The multimediadescriptors may indeed be extracted from visual, audioor transcript streams using various operators providingoutputs such as histograms, filter responses, statisticalmeasures or symbolic labels. This heterogeneity imposesbuilding complex learning setup that need to take intoaccount all the variety of the features mathematical andsemantic properties 1213.We advocate for the definition of an homogeneousrepresentation to store multimodal signals regardless theirintrasic dimensionality and scale. The fusion complexitywould be then dramatically alleviated since a uniquelearning model can be indistinctly applied on multimodalinformation to determine documents semanticrelevance.It would allow to setup fast and flexible multimediainformation retrieval systems. In our context, fast meansthat online learning is possible and flexible means that thesystem could handle any modalities blindly as far as theycan be embedded into the homogeneous representation.A first attempt for designing an homogeneous representation is to index documents according to their similaritiesrelated to one or several features to the other documentsrather than to a feature vector. Considering a collection ofdocuments, the similaritybased representation, stored indissimilarity matrices or some distancebased indexingstructures 14, characterizes the content of an elementof the collection relatively to a part of or the wholecollection. Studies have been published for documentretrieval and collection browsing by using precomputedsimilarities. In 15, Boldareva et al proposed to indexelements relatively to their closest neighbors, i.e. thosewho have the best probabilities to belong to the sameclass. This provides them with a sparse association graphstructuring the multimedia collection and allowing fastretrieval of data. In 16, the idea of nearest neighbornetworks is extended by creating edges for every combination of features. The resulting graph, called NNk, allowsto browse the data collection from various viewpointscorresponding to the multiple features. In 17, we designa dissimilarity space 18 where elements are no longerrepresented by their multimodal features but by theirrelative dissimilarities with respect to a set of positiveprototypes provided by users. As pointed out by authors,the similarity approach provides a convenient way formultimodal data fusion, since adding new features simplyconsists in adding new distances to the same representation framework. However, it still leaves open the problemof how to properly scale the similarity values to makethem really comparable.Following the similaritybased representation idea, wepropose to simplify the similaritybased representationby retaining only ordering information from the distancemeasurements. The result is a preference space whereevery item is indexed through its relative ranking positions to a set of prototypes. The scaling issue is thuscompletely alleviated and we effectively obtain a unifiedrepresentation of multimodal content, but at the price oflosing an important amount of the initial information. Inthe following we address more specifically the problemof multimedia information retrieval using query by example and relevance feedback search paradigms. Problemposition and terminologies are defined in section II.In section III we consider three multimodal informationrepresentations, namely feature space, dissimilarity spaceand the proposed preference space. Retrieving items fromthe preference space is then a ranking problem sectionIV that can be addressed using the RankBoost algorithmsection V. We end up with a multimedia search enginethat a builds its retrieval model upon multimodal information, b is parameter free and c is fast. Experiments onartificial and real data annotated images and videos, seesection VI show that the preference space associated toRankBoost competes with SVMbased approaches in termof accuracy but speed up the retrieval by a factor greaterthan 10. Moreover, contrary to the SVM, our approachdoes not require to set querysensitive parameters a priori.It is therefore a valid approach for online retrieval ofmultimedia information.II. PROBLEM DEFINITIONA multimedia document is composed of multimodalcontents for instance visual, audio and textual contentand multimedia information retrieval will consist to determine the relevance of each document relatively to agiven query. This relevance will reflect the adequation ofthe multimodal content to the query.In the following, we consider a collection X containingl multimedia documents x. The terms item, element orobject are also used to refer to x. The query by examplesearch paradigm consists in gathering users judgementsindicating, for some objects, whether they are relevantor irrelevant to the user request. This set, denoted Q, iscalled the query and is composed of positive and negativesubsets, respectivelyP  xi pi1 and N  xi ni1.The query Q is then used to train a machine that willproduce a decision function ranking documents accordingto their relevance to the query.This paradigm might be embedded in the RelevanceFeedback RF strategy, where these two steps userjudgement and ranking estimation are iterated until thesearch converges to a satisfactory result.III. MULTIMODAL CONTENT REPRESENTATIONSExpressing multimodal content involves first to extractvarious descriptors from the multimedia objects. Ideally,each descriptor depicts an appropriate aspect of themultimodal features of the documents. Assuming suchdescriptors are available, we discuss in the followinghow efficient representations may be derived to storedescriptors and to facilitate their fusion.A. Featurebased representationAssuming m distinct descriptors are designed andextraction procedures implemented, the multimodal represention of an object x is the set of m feature vectorsxkmk1 living respectively in feature spaces Fkmk1.The dimension of each feature space intrinsically dependsof the descriptor they express. The featurebased representation is rather straightforward, but not really convenientsince it mixes heterogeneous vectors of various dimensions and scales. Fusion and ranking algorithms need tomanage the diversity of the representation, thus makingthem more dependent on complex parameter setting procedures and less flexible to handle new descriptors.To avoid this situation, modalityindependent representations are desirable. For that purpose, dissimilaritybased representations have been recently proposed 15,17, 19, 20. As pointed out by these authors, similarities are convenient to manipulate multimodal informationsince they form a homogeneous representation of thecontent. Moreover, similarity representations are generallymade such as their dimensionality remain much lowerthan their feature counterparts.B. Dissimilaritybased representationIn 17, we proposed a Querybased DissimilaritySpace QDS, derived from the dissimilarity spaces introduced by Pekalska et al 21. For a given feature spaceFk, the corresponding QDS, denoted DkP , is defined relatively to the positive set P by the mapping dkx,P  Rpdkx,P  dkx, x1 , dkx, x2 , . . . dkx, xp T , 1where dkx, xi   R is the dissimilarity from anyobject x  X to the prototype xi when the measure isdone in Fk. Using QDS, an object x is thus representedwith a set of m dissimilarity vectors dkmk1 living inpdimensional dissimilarity spaces DkPmk1,DkP dkx1, x1  dkx2, x1  . . . dkxl, x1 dkx1, x2  dkx2, x2  . . . dkxl, x2 ...dkx1, xp  dkx2, xp  . . . dkxl, xp  .2The QDS presents two decisive advantages relatively tofeature spaces 1 It provides a unified representation ofmultimodal information channels, and 2 is particularlyadapted to the class asymmetry typically exhibited bythe positive and negative classes. This asymmetry corresponds to a 1  x class setup where the one class, presumably wellclustered in the feature space, encompassesthe sought documents positive class, while an unknownnumber x of classes, partially represented by negativeexamples, is supposed to model all irrelevant documents.Classical learning approaches, by applying a symmetrictreatment to all classes are not really efficient for such asetup. Learning the negative classes, while being feasibleusing traditional nonlinear learning machines, becomeschallenging when only few samples are available. Nevertheless, we show in 17 how a builtin property of DPis to transform the asymmetric classification setup suchthat it becomes linearly separable.However, the issue of how properly scaling dissimilarity spaces so that modalities become easily comparablestill remains. This problem might be left out to the fusionand ranking algorithms 17, but a more elegant solutionwould be to end up with a fully homogeneous multimodalrepresentation.C. Preferencebased representationWe propose to simplify the QDS representation byreplacing the dissimilarity components dkx, xi  withthe ranking position kx, xi   N of an object x withrespect to the prototype xi according to the dissimilaritymeasure dk and the collection X ,kx, xi  xjXdkxj , xi   dkx, xi . 3The notation  is defined to be 1 if predicate  holds and0 otherwise. Considering the p positive prototypes and them dissimilarity measures, the multimodal representationof an object x may be represented as a unique p mdimensional vector of preferencesx  1x, x1 , 21x, x1  . . . , mx, x1 ,1x, x2 , 21x, x2  . . . , mx, x2 ,...1x, xp , 21x, xp , . . . .mx, xp T .For the sake of readability, the notation k, xi  issimplified to j, j  k m  i 1, j  1, p m,with i iterating over all objects xi  P and k over the mmodalities. The multimodal preference space embeddingall objects x  X is thereforeP 1x1 1x2 . . . 1xl2x1 2x2 . . . 2xl. . .pmx1 pmx2 . . . pmxl . 4It consists in a unique pmdimensional natural numberspace providing a fully homogeneous representation ofmultimodal information. Reading P columnwise givesthe preference vectors x of every object x  X , whilereading rowwise yields the complete ordering of X relatively to a given positive example xi and a given modalityk i and j are given by the relation j  km  i 1.Similarly to the QDS approach, P represents the twoclasses P and N asymmetrically since every element isevaluated relatively to the positive instances only.It is worth noting however that we obtain this modalityindependent representation at the price of losing mostinformation about the initial feature distributions onlyordering information is actually preserved. Our objectivenow is to define a machine learning effectively able tolearn from preferences as efficiently as learning directlyin feature spaces or in dissimilarity spaces.IV. THE RANKING PROBLEMThe ranking problem could be formulated as follows For each item x  X , it exists ranking features1, . . . , pm, where each j defines a linear ordering ofthe instances x  X . In our formulation, j  N andjx1  jx0 means x1 preferred to x0.Additionally to the ranking features, there exists afeedback function   X  X which provides to thelearner the desired form of the final ranking. Formallyx1, x0  0 means that x1 should be ranked above x0while x1, x0  0 means the opposite. x1, x0  0means no preferences between x0 and x1 and the magnitude of x1, x0 indicates how important is to rankx1 above or below x0. The bipartite feedback functionis special but common case in document retrieval thefunction is said bipartite if there exists two disjoint set X1and X0 such that  ranks all instances x1 of X1 aboveinstances x0 of X0. These subsets are respectively thepositive and negative subsets P and N we defined insection II.Learning such a feedback function implies estimatinga ranking H  X  R through the optimization of aranking loss function penalizing every missordered pairof items. We consider the loss proposed in 22x  Nx  Px, xHxHx. 5The function Hx is a ranking of items x stating that xis ranked higher than x whenever Hx  Hx.Interestingly, in case of bipartite feedback, the problembecomes separable and the ranking loss simplifies to 22xQwxsxHx, 6where the user feedback is carried by bothsx 1 if x  P1 if x  N , 7and wx a weight giving the importance of the rank ofthe item x.Given two disjoint subsets P and N and labels sxover P N as defined in 7Initializew1x 1p if x  P1n if x  NFor t  1, . . . , T Train weak learner using wt Get weak ranking ht  X  R Compute r x wtxsxhtx Choose t  R Updatewt1x  1Ztwtxetsxhtxwhere Zt is a normalization factorOutput the final ranking Hx Tt1 thtxFig. 1. The RankBoost algorithm for bipartite feedbackV. RANKBOOSTFollowing the boosting principle, the final ranking Hresults from a weighted sum of weak rankings ht  X RHx Tt1thtx, 8which is estimated through an Adaboostlike algorithm,namely RankBoost 22 see Figure 1. This greedycoordinatewise search algorithm aims at iteratively minimizing the normalization factor Zt by choosing at eachround an appropriate pair t, ht. For a given weakhypothesis ht  1, 1, it has been shown 23 that Ztis minimized fort 12ln1  rt1 rt, 9where r is the weighted classification ratert xQwtxsxhtx. 10The algorithm is run over a number T of iterationswhich is predefined or may depend on the training error.In our implementation, the loop is stopped whenever thetraining error is equal to 0, with a maximum of 2pmiterations.A. Weak rankingThe weak ranking ht is produced through a weaklearner. It has to provide a new ranking from rankingfeatures i conforming the best to the bipartite feedback.For example, the weak learner proposed in 22 selectsat each iteration the ranking feature i minimizing thetraining error. The output preserves only relativeorderinginformation so as to be independent of specific preferencevalues,hx 1 if ix  1 if ix  . 11As illustrated in Figure 2, this weak learner consists infitting a step function to the user feedbacks sxjqj111sxixFig. 2. Binary weak ranking. The ixs are ordered in increasingorder.101ixsxFig. 3. Soft weak ranking. The ixs are ordered increasing order.sorted by increasing order of ixj. The best weakranking is the one maximizing equation 10 over the qcandidate weak rankings for the pm preferences i. Theevaluation of all candidates is done in Oqpm.As defined in 11, the function hx provides at eachiteration a binary ranking. The final ranking H eq.8 is thus an injection on X whose image has as atmost cardinality 2T , ie Hx  X  v1, . . . , v2T .Typically when the training set is small or when theranking problem is simple, RankBoost converges in a fewiterations T small and consequently provides a coarseranking partitioning the collection X in few blocks. Toget a finer ranking, we propose to use the a soft rankingfunction,hx  2e2i x  1. 12Learning this weak ranking consists of choosing the pairi,  that maximize the classification rate rt 10. Givena ranking feature i, a grid search on  is achieved ratherthan a timeconsuming nonlinear regression. The gridvertices are positioned at the middle of the q rankingintervals see Figure 3. With this approximation, theweak learner complexity remains Oqpm.VI. EXPERIMENTSThe behavior and performance of ranking data inthe three representation spaces feature, dissimilarity andpreference are studied here. As stated before, RankBoostsoft and binary weak ranking will be used to learnpreferences. As far as feature and dissimilarity spaces areconcerned, ranking are produced with the SVM algorithmas it is considered as an effective and standart techniquefor multimedia retrieval 24, 25, 26. Depending ofexperiment, linear or nonlinear eg using RBF kernelSVM is used.RankBoost smooth weak rankingaRankBoost binary weak rankingbQDS, linear SVMcFeature space, rbf kerneldFig. 4. Cross toy exampleA. Toy examplesArtificial data allows us to concretely illustrate howrankings are learned in the various representation spaces.The following toy examples are made so as to be representative of the class asymmetry we generally meetin real applications. For every learning technique, thelearned ranking is superimposed to the items white areascorrespond to top ranks and black areas to the last rank.Moreover, prototypes selected by Rankboost are indicatedwith the  marker.The first example Figure 4 corresponds to an idealseparable case, where all the positive instances crossmarker belong to the same cluster, while the negativesamples are distributed around circle marker. The corresponding dissimilarity space is built using pairwiseEuclidean distances while the preference space is derivedby ordering dissimilarities. Linear SVM is used to learnin QDS while a RBFSVM with an appropriate scaleparameter operates in feature space.In each case preferences, dissimilarities and features,respectively in Figure 4.a, b, c and d, a perfect rankinghas been estimated. As the class setup is simple, onlyone weak ranking indicated by the selected prototypeis necessary for RankBoost Figure 4.a and b. It impliesthat the final ranking is binary when using the binaryweak ranking function, while learning with the soft weakranking provides us with a more convenient continuousranking. As mention in section IIIB, a linear function isalso enough to catch the positive class within dissimilarityspace, while a nonlinear RBFbased ranking function isneeded in feature space.The second example Figure 5 depicts a less obviousproblem, the XOR configuration. The classes are nolonger linearly separable neither in preference space norin dissimilarity space. In that case, the linear SVM usedin QDS failed in estimating the ranking. On contrary,RankBoost succeeds in finding the two positive clustersand selects one prototype per cluster.RankBoost smooth weak rankingaRankBoost binary weak rankingbQDS, linear SVMcFeature space, rbf kerneldFig. 5. XOR toy exampleB. Real data1 Corel image collection The studied image collection is a subset of the Corel collection. It contains1159 images annotated with 1 to 10 keywords per imageincluding some nonsense descriptions. The images arecategorized into 49 classes. Textual and visual featuresare considered for fusing experiments The vector spacemodel F text containing tfidf weights is built from keywords 2035 terms. The color space F color contains 166bins HSV histograms and the texture space F texture ismade of Gabor filter bank outputs 120 dimensions.Cosine distance is considered for textual features, whileEuclidean is used in visual feature spaces.Fusion is operated in feature space, dissimilarity spaceand preference space. In feature and dissimilarity spacewe have considered a stateoftheart hierarchical fusionscheme 17, 7. At the first level, base classifiers aretrained in each monomodal space. At the second level,a super classifier is used to fuse softoutputs of all baseclassifiers. Base classifiers and super classifier are RBFSVM. Optimal classifier parameters have been determinedthrough a leaveoneout cross validation.Retrieval performance is given in terms of Mean Average Precision MAP. Average Precision AP is the sumof the precision at each relevant hit in the retrieved list,divided by the minimum between the number of relevantdocuments in the collection and the length of the list.The MAP is simply the AP averaged over several classes.Additionally to the algorithm performance, a baselineconsisting in retrieving randomly documents is alwaysprovided. All results are displayed in Figure 6.Multimodal retrieval Figure 6.b and textonly searchFigure 6.a are studied. In both cases we observe thatfor RankBoost, soft ranking outperforms largely binaryranking. Moreover, the soft ranking performs similarlyto the SVM approaches whereas it uses only a degradedversion of the original features. The second observationwe can make is that the multimodal retrieval outperformsonly very slightly the keywordonly search, whatever2 4 6 8 10 12 14 160   0.040.080.120.160.20.240.280.32positive examples n20MAP  Soft rankingBinary rankingSVM featureSVM dissirandom baselinea2 4 6 8 10 12 14 160   0.040.080.120.160.20.240.280.32positive examples n20MAP  Soft rankingBinary rankingSVM featureSVM dissirandom baselinebFig. 6. Image retrieval results with a multimodal fusion and b keywordsonly searchthe approach considered. This result seems to indicatethat keywords bring much of the category information,and that color or texture lowlevel information are oflittle help in that case. This observation is confirmed byanalyzing the ranking features selected by RankBoost tobuild retrieval models among all retrieval instances, textinformation is used for 93 of them, while color andtexture features are only used for respectively 36 and17 of the cases.Concerning the computational time on this particularexample Table I, we observe that soft ranking is slightlyfaster than binary ranking. It is also interesting to note thatRankBoost is around 20 times faster than the hierarchicalSVM approaches.2 TRECVID video corpus We now consider theTRECVID 2005 benchmark. In our setup, videos aresegmented into around 89500 segments using the common shot reference 27. These shots are considered asindividual and independent documents. This means thatno contextual information is taken into account and thatshot description is restricted to its audiovisual content egTABLE ICOMPUTATIONAL TIMEIN SECOND, INTEL XEON 2.80GHZpn SVM in F SVM in DP binary rkg soft rkg20 0.46 0.36 0.009 0.0130 0.80 0.68 0.028 0.02460 2.95 2.75 0.17 0.12100 9.43 9.23 0.56 0.52visual, audio and speech1 information.The Search Task, as defined in TRECVID05, consistsin retrieving shots that are relevant to some predefinedqueries called topics. There are 24 topics concerningpeople personX queries, objects specific or generic,locations, sports and combinations of the former. Foreach topic, keywords, pictures and several video shots410 are provided as positive examples. Further detailsabout the Search Task may be found in 28. During theexperiments, we only considered video shots as positiveexamples. The positive examples are completed with ten1the speech transcripts extracted by Automatic Speech RecognitionASR are also available.negative examples randomly selected within the test set.Starting with this initial query, a relevance feedback loopis initiated by adding to the query up to 10 new positiveand negative examples returned in the 1000entries hitlist. The process is repeated ten times. Following theTRECVID evaluation protocol, the performance was measured at each iteration by MAP at 1000. Additionallyto the algorithm performance, a baseline consisting ofretrieving randomly documents is always provided.The multimodal features are derived from the sixfollowing text and audiovisual descriptors Color histogram, 4 4 4 bins in YCbCr space Motion vector histogram, 66 bins quantization of theMPEG block motion vectors 29 Local features, SIFT descriptors extracted around theLowe salient points 30, Face detection 31, Word occurrence histogram vector space modelcomputed from ASR, Dominant audio features 32 extracted from theaudio stream.The distance measures used are Euclidean for color andmotion histograms. An approximation of the minimalmatching distance is applied on local features to determine partial similarities 33. Euclidean distance inthe 30dimensional eigenface space gives the similaritybetween the detected faces. Cosine distance is used forthe vector space model and finally the audio similaritymeasure proposed in 32 is used for audio features.The fusion strategies remain the hierarchical RBF SVMapproach in feature spaces and dissimilarity spaces. Forfeature space however, we adapt the RBFkernel to thedistances used, kdx, y  edx,y22 it is worth notingthat kd is strictly a RBFkernel when d is an Euclideandistance. Optimal classifier parameters have been crossvalidated using the TRECVID development set.MAP results are given in Figure 7.a. We comparemultimodal retrieval techniques with the best monomodalsearch hierarchical SVM in DASRP . The overall RankBoost performance remains very close to the best retrievalresult provided by the hierarchical SVM in dissimilarityspace. Soft ranking and binary ranking have now similarperformance and the latter is even slightly better whenthe training set becomes large. However in that case, thesoft ranking is around three times faster than the binaryranking and ten times faster the SVM learning2 Figure7.b. This rapidity is explained by the fact that soft rankingsystematically selects less features than binary ranking toproduce the final ranking Figure 7.c and thus convergesfaster and provides simpler retrieval models. In all cases,the retrieval accuracy benefits from multimodal fusion andlargely outperforms the ASRonly search. On the contraryto the Corel experiment, we observe now that the retrievalmodels produced by RankBoost soft ranking are fullymultimodal. As shown in Figure 8, the modality usage,2The computational complexity to learn the SVM in F and in DP isequivalent. Only the computational time for SVM in DP is thus reportedin Figure 7.bie the frequency of the selection of each descriptor tobuild the final ranking over the 24 queries, is almost100 for every modality. This indicate that all multimodalinformation sources are needed to fulfil the semantic levelrequired by the TRECVID queries.VII. CONCLUSIONThe preference space we introduced in this paper isa degraded but lightweight representation of the originalfeature space where all information relative to multimediacontent is stored. The preferences have the strong advantage to completely abstract multimodal content fromdimensionality and scaling issues, and thus to facilitatefusion of heterogeneous descriptors. The challenge isthen how to implement retrieval algorithms in preference space that are as effective as techniques based onmore traditional representations eg feature space. TheRankBoost algorithm offers us a very convenient solution,especially when considering the soft ranking function asa weak ranking. The performance is very close to state ofthe art SVMbased fusion algorithm operating in featureor dissimilarity spaces. The algorithm is parameter freeand thus avoid any lengthy and hazardous parametersestimation. Finally, RankBoost is really fast compared toSVMbased approaches which is a crucial argument foronline retrieval systems.REFERENCES1 E. Y. Chang, B. Li, G. Wu, and K. Go, Statistical learning foreffective visual information retrieval, in Proceedings of the IEEEInternational Conference on Image Processing, 2003.2 X. Zhou and T. Huang, Small sample learning during multimediaretrieval using biasmap, in Proceedings of the IEEE Conferenceon Pattern Recognition and Computer Vision, CVPR01, vol. I,Hawaii, 2004, pp. 1117.3 X. Zhou, A. Garg, and T. Huang, A discussion of nonlinearvariants of biased discriminant for interactive image retrieval,in Proc. of the 3rd Conference on Image and Video Retrieval,CIVR04, 2004, pp. 353364.4 R. Yan, A. Hauptmann, and R. Jin, Negative pseudorelevancefeedback in contentbased video retrieval, in Proceedings of ACMMultimedia MM2003, Berkeley, USA, 2003.5 J. Kittler, M. Hatef, R. Duin, and J. Matas, On combiningclassifiers, IEEE Transactions on Pattern Analysis and MachineIntelligence, vol. 20, no. 3, pp. 226239, 1998.6 R. Duin, The combining classifier To train or not to trainin Proceedings of the 16th International Conference on PatternRecognition, ICPR02, vol. II. Quebec City IEEE ComputerSocity Press, 2004, pp. 765770.7 Y. Wu, E. Y. Chang, K.C. Chang, and J. Smith, Optimalmultimodal fusion for multimedia data analysis, in Proceedingsof ACM Int, Conf. on Multimedia, New York, 2004.8 W. H. Hsu and S.F. Chang, Generative, discriminative, andensemble learning on multimodal perceptual fusion toward newsvideo story segmentation, in ICME, Taipei, Taiwan, June 2004.9 M. Wang, X.S. Hua, X. Yuan, Y. Song, and L.R. Dai, Optimizing multigraph learning towards a unified video annotationscheme, in MULTIMEDIA 07 Proceedings of the 15th international conference on Multimedia. New York, NY, USA ACM,2007, pp. 862871.10 T.T. Pham, N. E. Maillot, J.H. Lim, and J.P. Chevallet, Latentsemantic fusion model for image retrieval and annotation, inCIKM 07 Proceedings of the sixteenth ACM conference onConference on information and knowledge management. NewYork, NY, USA ACM, 2007, pp. 439444.11 M. Haindl, J. Kittler, and F. Roli, Multiple classifier systems,in Series Lecture Notes in Computer Science, vol. 44722007.Springer, 2007.1 2 3 4 5 6 7 8 9 1000.050.10.150.20.250.30.350.4 feedback loopsMAP  Soft rankingBinary rankingSVM featureSVM dissirandom baselineASRonly  baselinea1 2 3 4 5 6 7 8 9 100102030405060708090 feedback loopsComputational time s  Soft rankingBinary rankingSVM dissib10 20 30 40 50 600510152025positive examples of selected ranking features  Soft rankingBinary rankingcFig. 7. Multimodal video retrieval using a Relevance Feedback strategy with a Mean Average Precision, b Computational time for Hard rankingand soft ranking, c percentage of ranking features selected by RankBoost, and d modality usage for the 24 queries.ASR Color Motion Audio SIFT Faces0102030405060708090100 of queriesFig. 8. Modality usage Selected ranking feature frequencies over the 24 TRECVID queries in .12 J. R. Smith, A. Jaimes, C.Y. Lin, M. Naphade, A. Natsev, andB. Tseng, Interactive search fusion methods for video databaseretrieval, in IEEE International Conference on Image ProcessingICIP, 2003.13 J. Yang and A. Hauptmann, Multimodality analysis for persontype classification in news video, in Electronic Imaging05 Conference on Storage and Retrieval Methods and Applicationsfor Multimedia, San Jose, USA, Jan 2005.14 E. Chavez, G. Navarro, R. BaezaYates, and J. Marroquin, Searching in metric spaces, ACM Computing Surveys, vol. 33, no. 3, pp.273321, Sep. 2001.15 L. Boldareva and D. Hiemstra, Interactive contentbased retrievalusing precomputed objectobject similarities, in Conference onImage and Video Retrieval, CIVR04, Dublin, Ireland, 2004, pp.308316.16 D. Heesch, A. Yavlinsky, and S. Ruger, Nnk networks andautomated annotation for browsing large image collections fromthe World Wide Web, in Proc ACM Intl Conference Multimedia,2006, pp. 220224.17 E. Bruno, N. MoenneLoccoz, and S. MarchandMaillet, Designof multimodal dissimilarity spaces for retrieval of multimediadocuments, To appear in IEEE Transaction on Pattern Analysisand Machine Intelligence, 2008.18 E. Pekalska, R. P. W. Duin, and P. Paclk, Prototype selection fordissimilaritybased classifiers, Pattern Recogn., vol. 39, no. 2, pp.189208, 2006.19 D. Heesch and S. Rueger, NNk networks for contentbased imageretrieval, in 26th European Conference on Information Retrieval,Sunderland, UK, 2004.20 G. P. Nguyen, M. Worring, and A. W. M. Smeulders, Similaritylearning via dissimilarity space in CBIR, inMIR 06 Proceedingsof the 8th ACM international workshop on Multimedia informationretrieval. New York, NY, USA ACM Press, 2006, pp. 107116.21 E. Pekalska, P. Paclk, and R. Duin, A generalized kernel approachto dissimilaritybased classification, Journal of Machine LearningResearch, vol. 2, pp. 175211, December 2001.22 Y. Freund, R. Iyer, R. E. Schapire, and Y. Singer, An efficientboosting algorithm for combining preferences, Journal of Machine Learning Research, vol. 4, pp. 933969, November 2003.23 R. E. Schapire and Y. Singer, Improved boosting algorithmsusing confidencerated predictions. Journal of Machine Learning,vol. 37, no. 3, pp. 297336, 1999.24 J. Cheng and K. Wang, Active learning for image retrieval withcosvm, vol. 40, no. 1, pp. 330334, January 2007.25 S. Hoi, R. Jin, J. Zhu, and M. Lyu, Semisupervised svm batchmode active learning for image retrieval, 2008, pp. 17.26 R. Liu, Y. Wang, T. Baba, D. Masumoto, and S. Nagata, Svmbased active feedback in image retrieval using clustering andunlabeled data, vol. 41, no. 8, pp. 26452655, August 2008.27 C. Petersohn, Fraunhofer HHI at TRECVID 2004 Shot boundarydetection system, in TREC Video Retrieval Evaluation OnlineProceedings, 2004.28 A. F. Smeaton, P. Over, and W. Kraaij, Evaluation campaigns andtrecvid, in MIR 06 Proceedings of the 8th ACM internationalworkshop on Multimedia information retrieval. New York, NY,USA ACM Press, 2006, pp. 321330.29 A. Jain, A. Vailaya, and X. Wei, Query by video clip,MultimediaSyst., vol. 7, no. 5, pp. 369384, 1999.30 D. Lowe, Object recognition fron local scale invariant features,in Proceedings of the International Conference in Computer Vision, ICCV99, Corfu, 1999, pp. 11501157.31 P. Viola and M. Jones, Robust realtime face detection, International Journal of Computer Vision IJCV, vol. 57, no. 2, pp.137154, 2004.32 J. Gu, L. Lu, H. Zhang, and J. Yang, Dominant feature vectorsbased audio similarity measure, in PCM, no. 2, 2004, pp. 890897.33 N. MoenneLoccoz, E. Bruno, and S. MarchandMaillet, Interactive partial matching of video sequences in large collections,in IEEE International Conference on Image Processing, Genova,Italy, 1114 September 2005.Eric Bruno received his M.S. degree from the Engineers Schoolof Physics in Strasbourg, France in 1995, and his Ph.D in signalprocessing from the Joseph Fourier University, Grenoble, Francein 2001. Since 2002, he is working at the Computer Vision andMultimedia Laboratory, University of Geneva, Switzerland, as aresearch associate. His research interests focus on multimedia information retrieval, machine learning and statistical approachesfor fusion.Stephane MarchandMaillet received his PhD on theoreticalimage processing from Imperial College, London in 1997. Hethen joined the Institut Eurecom at SophiaAntipolis Francewhere he worked on automatic video indexing techniques basedon human face localization and recognition. Since 1999, he isAssistant Professor in the Computer Vision and MultimediaLab at the University of Geneva, where he is working oncontentbased multimedia retrieval as head of the Viper researchgroup. He has authored several publications on image analysisand information retrieval, including a book on lowlevel imageanalysis. His current research interests are in the study ofsemantic extraction from collaborative interaction.
