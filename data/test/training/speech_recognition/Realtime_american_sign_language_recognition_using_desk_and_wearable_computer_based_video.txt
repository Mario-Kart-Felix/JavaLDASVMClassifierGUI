IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,  VOL.  20,  NO.  12,  DECEMBER  1998 1371RealTime American Sign LanguageRecognition Using Desk and WearableComputer Based VideoThad Starner, Student Member, IEEE, Joshua Weaver,and Alex Pentland, Member, IEEE Computer SocietyAbstractWe present two realtime hidden Markov modelbasedsystems for recognizing sentencelevel continuous American SignLanguage ASL using a single camera to track the users unadornedhands. The first system observes the user from a desk mountedcamera and achieves 92 percent word accuracy. The second systemmounts the camera in a cap worn by the user and achieves 98 percentaccuracy 97 percent with an unrestricted grammar. Both experimentsuse a 40word lexicon.Index TermsGesture recognition, hidden Markov models, wearablecomputers, sign language, motion and pattern analysis.   F   1 INTRODUCTIONWHILE there are many different types of gestures, the most structured sets belong to the sign languages. In sign language, eachgesture already has assigned meaning, and strong rules of contextand grammar may be applied to make recognition tractable.American Sign Language ASL is the language of choice for mostdeaf in the United States. ASL uses approximately 6,000 gesturesfor common words and finger spelling for communicating obscurewords or proper nouns. However, the majority of signing is withfull words, allowing signed conversations to proceed at about thepace of spoken conversation. ASLs grammar allows more flexibility in word order than English and sometimes uses redundancy foremphasis. Another variant, Signed Exact English SEE, has morein common with spoken English but is not as widespread inAmerica.Conversants in ASL may describe a person, place, or thing andthen point to a place in space to store that object temporarily forlater reference 14. For the purposes of this experiment, this aspectof ASL will be ignored. Furthermore, in ASL the eyebrows areraised for a question, relaxed for a statement, and furrowed for adirective. While we have also built systems that track facial features 4, 9, this source of information will not be used to aid recognition in the task addressed here.1.1 Related WorkFollowing a similar path to early speech recognition, many previous attempts at machine sign language recognition concentrate onisolated signs or fingerspelling. Space does not permit a thoroughreview 19, but, in general, most attempts either relied on instrumented gloves or a desktopbased camera system and used a formof template matching or neural nets for recognition. However,current extensible systems are beginning to employ hiddenMarkov models HMMs.Hidden Markov models are used prominently and successfully in speech recognition and, more recently, in handwritingrecognition. Consequently, they seem ideal for visual recognitionof complex, structured hand gestures as are found in sign languages. Explicit segmentation on the word level is not necessaryfor either training or recognition. Language and context modelscan be applied on several different levels, and much related development of this technology has been done by the speech recognition community 6.When the authors first reported this project in 1995 15, 18,very few uses of HMMs were found in the computer vision literature 22, 13. At the time, continuousdensity HMMs were beginning to appear in the speech community continuousgesture recognition was scarce gesture lexicons were very small and automatic training through BaumWelch reestimation was uncommon.Results were not reported with the standard accuracy measuresaccepted in the speech and handwritingrecognition communities,and training and testing databases were often identical or dependent in some manner.Since this time, HMMbased gesture recognizers for othertasks have appeared in the literature 21, 2, and, last year, several HMMbased continuous sign language systems were demonstrated. In a submission to UIST97, Liang and Ouhyoungswork in Taiwanese Sign Language 8 shows very encouragingresults with a glovebased recognizer. This HMMbased systemrecognizes 51 postures, eight orientations, and eight motionprimitives. When combined, these constituents can form a lexicon of 250 words which can be continuously recognized in realtime with 90.5 percent accuracy. At ICCV98, Vogler and Metaxasdescribed a deskbased 3D camera system that achieves 89.9 percent word accuracy on a 53 word lexicon 20. Since the visionprocess is computationally expensive in this implementation, anelectromagnetic tracker is used interchangeably with the threemutually orthogonal calibrated cameras for collecting experimental data.1.2 The TaskIn this paper, we describe two extensible systems which use onecolor camera to track unadorned hands in real time and interpretAmerican Sign Language using hidden Markov models. Thetracking stage of the system does not attempt a fine description ofhand shape, instead concentrating on the evolution of the gesturethrough time. Studies of human sign readers suggest that surprisingly little hand detail is necessary for humans to interpret signlanguage 10, 14. In fact, in movies shot from the waist up ofisolated signs, Sperling et al. 14 show that the movies retain 85percent of their full resolution intelligibility when subsampled to24  16 pixels For this experiment, the tracking process producesonly a coarse description of hand shape, orientation, and trajectory.The resulting information is input to a HMM for recognition of thesigned words.While the scope of this work is not to create a user independent, full lexicon system for recognizing ASL, the system is extensible toward this goal. The continuous sign language recognitionof full sentences demonstrates the feasibility of recognizing complicated series of gestures. In addition, the realtime recognitiontechniques described here allow easier experimentation, demonstrate the possibility of a commercial product in the future, andsimplify archival of test data. For this recognition system, sentences of the form personal pronoun, verb, noun, adjective, thesame personal pronoun are to be recognized. This structure allows a large variety of meaningful sentences to be generated usingrandomly chosen words from each class as shown in Table 1. Sixpersonal pronouns, nine verbs, twenty nouns, and five adjectivesare included for a total lexicon of forty words. The words werechosen by paging through Humphries et al. 7 and selecting thosewords which would generate coherent sentences given the grammar constraint. Words were not chosen based on distinctiveness or016288289810.00  1998 IEEE The authors are with the Massachusetts Institute of Technology, The MediaLaboratory, Room E15383, 20 Ames Street, Cambridge, MA  02139.Email thad, joshw, sandymedia.mit.edu.Manuscript received 19 Apr. 1996 revised 17 Sept. 1998. Recommended for acceptance by K. Boyer.For information on obtaining reprints of this article, please send email totpamicomputer.org, and reference IEEECS Log Number 108037.1372 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,  VOL.  20,  NO.  12,  DECEMBER  1998lack of detail in the finger positioning. Note that finger positionplays an important role in several of the signs pack vs. car, foodvs. pill, red vs. mouse, etc.2 HIDDEN MARKOV MODELINGDue to space limitations, the reader is encouraged to refer to theexisting literature on HMM evaluation, estimation, and decoding1, 6, 11, 23. A tutorial relating HMMs to sign language recognition is provided in the first authors Masters thesis 15.The initial topology for an HMM can be determined by estimating how many different states are involved in specifying asign. Fine tuning this topology can be performed empirically. Inthis case, an initial topology of five states was considered sufficientfor the most complex sign. To handle less complicated signs, skiptransitions were specified which allowed the topology to emulate astrictly three or four state HMM. While different topologies can bespecified per sign explicitly, the above method allows training toadapt the HMM automatically without human intervention. However, after testing several different topologies, a four state HMMwith one skip transition was determined to be appropriate for thistask Fig. 1.3 FEATURE EXTRACTION AND HAND AMBIGUITYPrevious systems have shown that, given strong constraints onviewing, relatively detailed models of the hands can be recoveredfrom video images 3, 12. However, many of these constraintsconflict with recognizing ASL in a natural context, since they eitherrequire simple, unchanging backgrounds unlike clothing do notallow occlusion require carefully labelled gloves or are difficult torun in real time.In this project, we track the hands using a single camera in realtime without the aid of gloves or markings. Only the natural colorof the hands is needed. For visionbased sign recognition, the twopossible mounting locations for the camera are in the position ofan observer of the signer or from the point of view of the signerhimself. These two views can be thought of as secondperson andfirstperson viewpoints, respectively.Training for a secondperson viewpoint is appropriate in therare instance when the translation system is to be worn by a hearing person to translate the signs of a mute or deaf individual.However, such a system is also appropriate when a signer wishesto control or dictate to a desktop computer as is the case in the firstexperiment. Fig. 2 demonstrates the viewpoint of the deskbasedexperiment.The firstperson system observes the signers hands from muchthe same viewpoint as the signer himself. Fig. 3 shows the placement of the camera in the cap used in the second experiment anddemonstrates the resulting viewpoint. The camera was attached toan SGI for development however, current hardware allows for theentire system to be unobtrusively embedded in the cap itself as awearable computer. A matchsticksized camera such as the ElmoQN401E can be embedded in front seam above the brim. The brimcan be made into a relatively good quality speaker by lining it with aPVDF transducer used in thin consumergrade stereo speakers.Finally a PC104based CPU, digitizer, and batteries can be placed atthe back of the head. See Starner et al. 17 and the MIT WearableComputing Site httpwearables.www.media.mit.eduprojectswearables formore detailed information about wearable computing and relatedtechnologies.A wearable computer system provides the greatest utility for anASL to spoken English translator. It can be worn by the signerwhenever communication with a nonsigner might be necessary,such as for business or on vacation. Providing the signer with aselfcontained and unobtrusive firstperson view translation system is more feasible than trying to provide secondperson translation systems for everyone whom the signer might encounter during the day.For both systems, color NTSC composite video is captured andanalyzed at 320  243 pixel resolution. This lower resolution avoidsvideo interlace effects. A Silicon Graphics 200MHz R4400 Indyworkstation maintains hand tracking at 10 frames per second, aframe rate which Sperling et al. 14 found sufficient for humanrecognition. To segment each hand initially, the algorithm scansTABLE 1ASL TEST LEXICONpart of speech vocabularypronoun I, you, he, we, youpl, theyverb want, like, lose, dontwant, dontlike,love, pack, hit, loannoun box, car, book, table, paper, pants,bicycle, bottle, can, wristwatch,umbrella, coat, pencil, shoes, food,magazine, fish, mouse, pill, bowladjective red, brown, black, gray, yellowFig. 1. The fourstate HMM used for recognition.Fig. 2. View from the deskbased tracking camera. Images are analyzed at 320  240 resolution.Fig. 3. The hatmounted camera, pointed downward toward the hands,and the corresponding view.IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,  VOL.  20,  NO.  12,  DECEMBER  1998 1373the image until it finds a pixel of the appropriate color, determinedby an a priori model of skin color. Given this pixel as a seed, theregion is grown by checking the eight nearest neighbors for theappropriate color. Each pixel checked is considered part of thehand. This, in effect, performs a simple morphological dilationupon the resultant image that helps to prevent edge and lightingaberrations. The centroid is calculated as a byproduct of thegrowing step and is stored as the seed for the next frame. Since thehands have the same skin tone, the labels left hand and righthand are simply assigned to whichever blob is leftmost andrightmost.Note that an a priori model of skin color may not be appropriate in some situations. For example, with a mobile system, lightingcan change the appearance of the hands drastically. However, theimage in Fig. 3 provides a clue to addressing this problem, at leastfor the firstperson view. The smudge on the bottom of the imageis actually the signers nose. Since the camera is mounted on a cap,the nose always stays in the same place relative to the image. Thus,the signers nose can be used as a calibration object for generatinga model of the hands skin color for tracking. While this calibrationsystem has been prototyped, it was not used in these experiments.After extracting the hand blobs from the scene, second momentanalysis is performed on each blob. A sixteenelement featurevector is constructed from each hands x and y position, change inx and y between frames, area in pixels, angle of axis of least inertia found by the first eigenvector of the blob 5, length of thiseigenvector, and eccentricity of bounding ellipse.When tracking skin tones, the above analysis helps to modelsituations of hand ambiguity implicitly. When a hand occludeseither the other hand or the face or the nose in the case of thewearable version, color tracking alone cannot resolve the ambiguity. Since the face remains in the same area of the frame, its position can be determined and discounted. However, the handsmove rapidly and occlude each other often. When occlusion occurs, the hands appear to the above system as a single blob oflarger than normal area with significantly different moments thaneither of the two hands in the previous frame. In this implementation, each of the two hands is assigned the features of this singleblob whenever occlusion occurs. While not as informative astracking each hand separately, this method still retains a surprisingamount of discriminating information. The occlusion event itself isimplicitly modeled, and the combined position and moment information are retained. This method, combined with the timecontext provided by hidden Markov models, is sufficient to distinguish between many different signs where hand occlusion occurs.4 THE SECONDPERSON VIEW A DESKBASEDRECOGNIZERThe first experimental situation explored was the second personview a deskbased recognizer. In this experiment, 500 sentenceswere obtained, but 22 sentences were eliminated due to subjecterror or outlier signs. In general, each sign is one to three seconds long. No intentional pauses exist between signs within asentence, but the sentences themselves are distinct. For testingpurposes, 384 sentences were used for training, and 94 were reserved for testing. The test sentences are not used in any portionof the training process.For training, the sentences are divided automatically in fiveequal portions to provide an initial segmention into componentsigns. Then, initial estimates for the means and variances of theoutput probabilities are provided by iteratively using Viterbialignment on the training data and then recomputing the meansand variances by pooling the vectors in each segment. EntropicsHidden Markov Model ToolKit HTK is used as a basis for thisstep and all other HMM modeling and training tasks. The resultsfrom the initial alignment program are fed into a BaumWelch reestimator, whose estimates are, in turn, refined in embeddedtraining which ignores any initial segmentation. For recognition,HTKs Viterbi recognizer is used both with and without the partofspeech grammar based on the known form of the sentences.Contexts are not used since they would require significantly moredata to train. However, a similar effect can be achieved with thestrong grammar in this data set. Recognition occurs five timesfaster than real time.Word recognition accuracy results are shown in Table 1 whendifferent, the percentage of words correctly recognized is shown inparentheses next to the accuracy rates. Accuracy is calculated byAccN D S IN  where N is the total number of words in the test set, D is the number of deletions, S is the number of substitutions, and I is the number of insertions. Note that, since all errors are accounted againstthe accuracy rate, it is possible to get large negative accuraciesand corresponding error rates of over 100 percent. When usingthe partofspeech grammar pronoun, verb, noun, adjective, pronoun, insertion and deletion errors are not possible since thenumber and class of words allowed is known. Thus, all errors arevocabulary substitutions when this grammar is used and accuracyis equivalent to percent correct. Assuming independence, randomchance would result in a percent correct of 13.9 percent, calculatedby averaging over the likelihood of each partofspeech being correct. Without the grammar, the recognizer is allowed to match theobservation vectors with any number of the 40 vocabulary wordsin any order. In fact, the number of words produced by the recognizer can be up to the number of samples in the sentence Thus,deletion D, insertion I, and substitution S errors are possible inthe unrestricted grammar tests, and a comparison to randomchance becomes irrelevant. The absolute number of errors of eachtype are listed in Table 2. Many of the insertion errors correspondto signs with repetitive motion.An additional relative features test is provided in the results.For this test, absolute x, y position is removed from the featurevector. This provides a sense of how the recognizer performs whenonly relative features are available. Such may be the case in dailyuse the signer may not place himself in the same location eachtime the system is used.The 94.1 percent and 91.9 percent accuracies using the partofspeech grammar show that the HMM topologies are sound andthat the models generalize well. However, the subjects variabilityin body rotation and position is known to be a problem with thisdata set. Thus, signs that are distinguished by the hands positions inrelation to the body were confused since the absolute positions ofthe hands in screen coordinates were measured. With the relativeTABLE 2WORD ACCURACY OF DESKBASED SYSTEMexperiment training set Independent testsetall features 94.1 percent 91.9 percentrelative features 89.6 percent 87.2 percentall features unrestrictedgrammar81.0 percent87 percentD  31, S  287,I  137, N  239074.5 percent83 percentD  3, S  76,I  41, N  470Word accuracies percent correct in parentheses where different. The first testuses the strong partofspeech grammar and all feature elements. The second testremoves absolute position from the feature vector. The last test again uses allfeatures but only requires that the hypothesized output be composed of wordsfrom the lexicon. Any word can occur at any time and any number of times.1374 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,  VOL.  20,  NO.  12,  DECEMBER  1998feature set, the absolute positions of the hands are removed fromthe feature vector. While this change causes the error rate to increase slightly, it demonstrates the feasibility of allowing the subject to vary his location in the room while signing, possibly removing a constraint from the system.The error rates of the unrestricted experiment better indicatewhere problems may occur when extending the system. Withoutthe grammar, signs with repetitive or long gestures were ofteninserted twice for each actual occurrence. In fact, insertions causedmore errors than substitutions. Thus, the sign shoes might berecognized as shoes shoes, which is a viable hypothesis withouta language model. However, a practical solution to this problem isto use context training and a statistical grammar.5 THE FIRST PERSON VIEW A WEARABLEBASEDRECOGNIZERFor the second experiment, the same 500 sentences were collectedby a different subject. Sentences were resigned whenever a mistakewas made. The full 500 sentence database is available fromanonymous ftp at whitechapel.media.mit.edu under pubasl. The subjecttook care to look forward while signing so as not to confound thetracking with head rotation, though variations can be seen. Often,several frames at the beginning and ending of a sentences datacontain the hands at a resting position. To take this in account,another token, silence in deference to the speech convention,was added to the lexicon. While this sign is trained with the rest,it is not included when calculating the accuracy measurement.The resulting word accuracies from the experiment are listed inTable 3. In this experiment, 400 sentences were used for training, andan independent 100 sentences were used for testing. A new grammar was added for this experiment. This grammar simply restrictsthe recognizer to five word sentences without regard to part ofspeech. Thus, the percent correct words expected by chance usingthis fiveword grammar would be 2.5 percent. Deletions and insertions are possible with this grammar since a repeated word can bethought of as a deletion and an insertion instead of two substitutions.Interestingly, for the partofspeech, fiveword, and unrestrictedtests, the accuracies are essentially the same, suggesting that all thesigns in the lexicon can be distinguished from each other using thisfeature set and method. As in the previous experiment, repeatedwords represent 25 percent of the errors. In fact, if a simple repeatedword filter is applied post process to the recognition, the unrestrictedgrammar test accuracy becomes 97.6 percent, almost exactly that of themost restrictive grammar Looking carefully at the details of the partofspeech and fiveword grammar tests indicate that the same beginning and ending pronoun restriction may have hurt the performance of the partofspeech grammar Thus, the strong grammars aresuperfluous for this task. In addition, the close accuracies betweenfairtest and testontraining cases indicate that the HMMs trainingconverged and generalized extremely well for the task.The main result is the high accuracies themselves, which indicate that harder tasks should be attempted. However, why is thewearable system so much more accurate than the desk systemThere are several possible factors. First, the wearable system hasless occlusion problems, both with the face and between the hands.Second, the wearable data set did not have the problem with bodyrotation that the first data set experienced. Third, each data set wascreated and verified by separate subjects, with successively betterdata recording methods. Controlling for these various factors requires a new experiment, described in the next section.6 DISCUSSION AND FUTURE WORKWe have shown a highaccuracy computer visionbased method ofrecognizing sentencelevel American Sign Language selected froma 40word lexicon. The first experiment shows how the system canbe used to communicate with a deskbased computer. The secondexperiment demonstrates how a wearable computer might use thismethod as part of an ASL to English translator. Both experimentsargue that HMMs will be a powerful method for sign languagerecognition, much as they have been for speech and handwritingrecognition. In addition, the experiments suggest that the firstperson view provides a valid perspective for creating a wearableASL translator.While it can be argued that sign evolved to have maximum intelligibility from a frontal view, further thought reveals that signalso may have to be distinguishable by the signer himself, both forlearning and to provide control feedback. To determine whichview is superior for recognition, we have begun a new experiment.Native signers will be given a task to complete. The task will bedesigned to encourage a small vocabulary e.g., a few hundredwords and to encourage natural sign. Four views of the signerswill be recorded simulaneously a stereo pair from the front, aview from the side, and the wearable computer view. Thus, both3D and 2D tracking from various views can be compared directly.Head motion and facial gestures also have roles in sign whichthe wearable system would seem to have trouble addressing. Infact, uncompensated head rotation would significantly impair thecurrent system. However, as shown by the effects in the first experiment, bodyhead rotation is an issue from either viewpoint.Simple fiducials, such as a belt buckle or lettering on a tshirt maybe used to compensate tracking or even provide additional features. Another option for the wearable system is to add inertialsensors to compensate for head motion. In addition, EMGs maybe placed in the caps head band along the forehead to analyzeeyebrow motion as has been discussed by Picard 9. In this way,facial gesture information may be recovered.As the system grows in lexicon size, finger and palm trackinginformation may be added. This may be as simple as countinghow many fingers are visible along the contour of the hand andwhether the palm is facing up or down. In addition, trisign context models and statistical grammars may be added which mayreduce error up to a factor of eight if speech and handwritingtrends hold true for sign 16.These improvements do not address user independence. Just asin speech, making a system which can understand different subjects with their own variations of language involves collecting datafrom many subjects. Until such a system is tried, it is hard to estimate the number of subjects and the amount of data that wouldcomprise a suitable training database. Independent recognitionoften places new requirements on the feature set as well. While themodifications mentioned above may be initially sufficient, thedevelopment process is highly empirical.TABLE 3WORD ACCURACY OF WEARABLE COMPUTER SYSTEMGrammar training set independenttest setpartofspeech 99.3 percent 97.8 percent5word sentence 98.2 percent98.4 percentD  5, S  36,I  5, N  250097.8 percentunrestricted 96.4 percent97.8 percentD  24, S  32,I  35, N  250096.8 percent98.0 percentD  4, S  6,I  6, N  500Word accuracies percent correct in parentheses where different. Thefiveword grammar limits the recognizer output to five words selected from the vocabulary. The other grammars are as before.IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,  VOL.  20,  NO.  12,  DECEMBER  1998 1375Similarly, we have not yet addressed the problem of fingerspelling. Changes to the feature vector to address finger information are vital, but adjusting the context modeling is also of importance. With finger spelling, a closer parallel can be made to speechrecognition. Trisign context occurs at the subword level whilegrammar modeling occurs at the word level. However, this is atodds with context across word signs. Can trisign context be usedacross finger spelling and signing Is it beneficial to switch to aseparate mode for finger spelling recognition Can natural language techniques be applied, and if so, can they also be used toaddress the spatial positioning issues in ASL The answers to thesequestions may be key to creating an unconstrained sign languagerecognition system.ACKNOWLEDGMENTSThe authors would like to thank Tavenner Hall for her help editingthis document. This work is supported by BT and the Things ThatThink consortium at the MIT Media Laboratory.REFERENCES1 L. Baum, An Inequality and Associated Maximization Techniquein Statistical Estimation of Probabilistic Functions of Markov Processes, Inequalities, vol. 3, pp. 18, 1972.2 L. Campbell, D. Becker, A. Azarbayejani, A. Bobick, and A. Pentland, Invariant Features for 3D Gesture Recognition, SecondIntl Conf. Face and Gesture Recognition, pp. 157162, 1996.3 B. Dorner, Hand Shape Identification and Tracking for Sign Language Interpretation, IJCAI Workshop on Looking at People, 1993.4 I. Essa, T. Darrell, and A. Pentland, Tracking Facial Motion,Proc. Workshop on Motion of NonRigid and Articulated Objects,Austin, Tex., Nov. 1994.5 B. Horn, Robot Vision. Cambridge, Mass. MIT Press, 1986.6 X.D. Huang, Y. Ariki, and M.A. Jack, Hidden Markov Models forSpeech Recognition. Edinburgh Univ. Press, 1990.7 T. Humphries, C. Padden, and T. ORourke, A Basic Course inAmerican Sign Language. Silver Spring, Md. T. J. Publ., Inc., 1990.8 R. Liang and M. Ouhyoung, A RealTime Continuous GestureInterface for Taiwanese Sign Language, Submitted to UIST, 1997.9 R. Picard, Toward Agents That Recognize Emotion, Imagina98,1998.10 H. Poizner, U. Bellugi, and V. LutesDriscoll, Perception ofAmerican Sign Language in Dynamic PointLight Displays, J.Exp. Pyschol. Human Perform., vol. 7, pp. 430440, 1981.11 L.R. Rabiner and B.H. Juang, An Introduction to Hidden MarkovModels, IEEE ASSP Magazine, pp. 416, Jan. 1986.12 J. M. Rehg and T. Kanade, DigitEyes VisionBased Human HandTracking, School of Computer Science Technical Report CMUCS93220, Carnegie Mellon Univ., Dec. 1993.13 J. Schlenzig, E. Hunter, and R. Jain, Recursive Identification ofGesture Inputs Using Hidden Markov Models, Proc. Second Ann.Conf. Applications of Computer Vision, pp. 187194, Dec. 1994.14 G. Sperling, M. Landy, Y. Cohen, and M. Pavel, Intelligible Encoding of ASL Image Sequences at Extremely Low InformationRates, Comp. Vision, Graphics, and Image Processing, vol. 31, pp. 335391, 1985.15 T. Starner, Visual Recognition of American Sign Language UsingHidden Markov Models, Masters thesis, MIT, Media Laboratory, Feb. 1995.16 T. Starner, J. Makhoul, R. Schwartz, and G. Chou, OnLine Cursive Handwriting Recognition Using Speech Recognition Methods, ICASSP, pp. 125128, 1994.17 T. Starner, S. Mann, B. Rhodes, J. Levine, J. Healey, D. Kirsch, R.Picard, and A. Pentland, Augmented Reality Through WearableComputing, Presence, vol. 6, no. 4, pp. 386398, 1997.18 T. Starner and A. Pentland, RealTime American Sign LanguageRecognition From Video Using Hidden Markov Models, Technical Report 375, MIT Media Lab, Perceptual Computing Group,1995. Earlier version appeared ISCV95.19 T. Starner, J. Weaver, and A. Pentland, RealTime American SignLanguage Recognition Using Desktop and Wearable ComputerBased Video, Technical Report 466, Perceptual Computing, MITMedia Laboratory, July 1998.20 C. Vogler and D. Metaxas, ASL Recognition Based on a CouplingBetween HMMs and 3D Motion Analysis, ICCV, Bombay, 1998.21 A.D. Wilson and A.F. Bobick, Learning Visual Behavior for Gesture Analysis, Proc. IEEE Intl Symp. Computer Vision, Coral Gables, Fla., Nov. 1995.22 J. Yamato, J. Ohya, and K. Ishii, Recognizing Human Action inTimeSequential Images Using Hidden Markov Models, Proc.Computer Vision and Pattern Recognition, pp. 379385, 1992.23 S. Young, HTK Hidden Markov Model Toolkit V1.5, Cambridge Univ. Eng. Dept., Speech Group, and Entropic ResearchLab., Inc., Washington D.C., 1993.
