Learning realistic human actions from moviesIvan Laptev Marcin Marszaek Cordelia Schmid Benjamin RozenfeldINRIA Rennes, IRISA INRIA Grenoble, LEAR  LJK BarIlan Universityivan.laptevinria.fr marcin.marszalekinria.fr cordelia.schmidinria.fr grurgrurgmail.comAbstractThe aim of this paper is to address recognition of naturalhuman actions in diverse and realistic video settings. Thischallenging but important subject has mostly been ignoredin the past due to several problems one of which is the lackof realistic and annotated video datasets. Our first contribution is to address this limitation and to investigate theuse of movie scripts for automatic annotation of human actions in videos. We evaluate alternative methods for actionretrieval from scripts and show benefits of a textbased classifier. Using the retrieved action samples for visual learning, we next turn to the problem of action classification invideo. We present a new method for video classificationthat builds upon and extends several recent ideas includinglocal spacetime features, spacetime pyramids and multichannel nonlinear SVMs. The method is shown to improvestateoftheart results on the standard KTH action datasetby achieving 91.8 accuracy. Given the inherent problemof noisy labels in automatic annotation, we particularly investigate and show high tolerance of our method to annotation errors in the training set. We finally apply the methodto learning and classifying challenging action classes inmovies and show promising results.1. IntroductionIn the last decade the field of visual recognition had anoutstanding evolution from classifying instances of toy objects towards recognizing the classes of objects and scenesin natural images. Much of this progress has been sparkedby the creation of realistic image datasets as well as by thenew, robust methods for image description and classification. We take inspiration from this progress and aim totransfer previous experience to the domain of video recognition and the recognition of human actions in particular.Existing datasets for human action recognition e.g. 15,see figure 8 provide samples for only a few action classesrecorded in controlled and simplified settings. This standsin sharp contrast with the demands of real applications focused on natural video with human actions subjected to inFigure 1. Realistic samples for three classes of human actionskissing answering a phone getting out of a car. All samples havebeen automatically retrieved from scriptaligned movies.dividual variations of people in expression, posture, motionand clothing perspective effects and camera motions illumination variations occlusions and variation in scene surroundings. In this paper we address limitations of currentdatasets and collect realistic video samples with human actions as illustrated in figure 1. In particular, we consider thedifficulty of manual video annotation and present a methodfor automatic annotation of human actions in movies basedon script alignment and text classification see section 2.Action recognition from video shares common problemswith object recognition in static images. Both tasks haveto deal with significant intraclass variations, backgroundclutter and occlusions. In the context of object recognition in static images, these problems are surprisingly wellhandled by a bagoffeatures representation 17 combinedwith stateoftheart machine learning techniques like Support Vector Machines. It remains, however, an open question whether and how these results generalize to the recognition of realistic human actions, e.g., in feature films orpersonal videos.Building on the recent experience with image classification, we employ spatiotemporal features and generalizespatial pyramids to spatiotemporal domain. This allowsus to extend the spatiotemporal bagoffeatures representation with weak geometry, and to apply kernelbased learning techniques cf. section 3. We validate our approachon a standard benchmark 15 and show that it outperformsthe stateoftheart. We next turn to the problem of actionclassification in realistic videos and show promising resultsfor eight very challenging action classes in movies. Finally,we present and evaluate a fully automatic setup with actionlearning and classification obtained for an automatically labeled training set.1.1. Related workOur scriptbased annotation of human actions is similar in spirit to several recent papers using textual information for automatic image collection from the web 10, 14and automatic naming of characters in images 1 andvideos 4. Differently to this work we use more sophisticated text classification tools to overcome action variability in text. Similar to ours, several recent methodsexplore bagoffeatures representations for action recognition 3, 6, 13, 15, 19, but only address human actions incontrolled and simplified settings. Recognition and localization of actions in movies has been recently addressedin 8 for a limited dataset, i.e., manual annotation of twoaction classes. Here we present a framework that scales toautomatic annotation for tens or more visual action classes.Our approach to video classification borrows inspirationfrom image recognition methods 2, 9, 12, 20 and extendsspatial pyramids 9 to spacetime pyramids.2. Automatic annotation of human actionsThis section describes an automatic procedure for collecting annotated video data for human actions frommovies. Movies contain a rich variety and a large number of realistic human actions. Common action classessuch as kissing, answering a phone and getting out of acar see figure 1, however, often appear only a few timesper movie. To obtain a sufficient number of action samplesfrom movies for visual training, it is necessary to annotatetens or hundreds of hours of video which is a hard task toperform manually.To avoid the difficulty of manual annotation, we makeuse of movie scripts or simply scripts. Scripts are publicly available for hundreds of popular movies1 and providetext description of the movie content in terms of scenes,characters, transcribed dialogs and human actions. Scriptsas a mean for video annotation have been previously used1We obtained hundreds of movie scripts from www.dailyscript.com,www.moviepage.com and www.weeklyscript.com.for the automatic naming of characters in videos by Everingham et al. 4. Here we extend this idea and apply textbased script search to automatically collect video samplesfor human actions.Automatic annotation of human actions from scripts,however, is associated with several problems. Firstly,scripts usually come without time information and have tobe aligned with the video. Secondly, actions described inscripts do not always correspond with the actions in movies.Finally, action retrieval has to cope with the substantial variability of action expressions in text. In this section we address these problems in subsections 2.1 and 2.2 and use theproposed solution to automatically collect annotated videosamples with human actions, see subsection 2.3. The resulting dataset is used to train and to evaluate a visual actionclassifier later in section 4.2.1. Alignment of actions in scripts and videoMovie scripts are typically available in plain text formatand share similar structure. We use line indentation as asimple feature to parse scripts into monologues, characternames and scene descriptions see figure 2, right. To alignscripts with the video we follow 4 and use time information available in movie subtitles that we separately download from the Web. Similar to 4 we first align speechsections in scripts and subtitles using word matching anddynamic programming. We then transfer time informationfrom subtitles to scripts and infer time intervals for scenedescriptions as illustrated in figure 2. Video clips used foraction training and classification in this paper are definedby time intervals of scene descriptions and, hence, maycontain multiple actions and nonaction episodes. To indicate a possible misalignment due to mismatches betweenscripts and subtitles, we associate each scene descriptionwith the alignment score a. The ascore is computed bythe ratio of matched words in the nearby monologues asa  matched wordsall words.Temporal misalignment may result from the discrepancybetween subtitles and scripts. Perfect subtitle alignmenta  1, however, does not yet guarantee the correct actionannotation in video due to the possible discrepancy between1172012017,240  012020,437Why werent you honest with meWhyd you keep your marriage a secret1173012020,640  012023,598lt wasnt my secret, Richard.Victor wanted it that way.1174012023,800  012026,189Not even our closest friendsknew about our marriage.RICKWhy werent you honest with me Whydid you keep your marriage a secretRick sits down with Ilsa.ILSAOh, it wasnt my secret, Richard. Victor wanted it that way. Not even our closest friends knew about our marriage.012017012023subtitles movie scriptFigure 2. Example of matching speech sections green in subtitlesand scripts. Time information blue from adjacent speech sectionsis used to estimate time intervals of scene descriptions yellow.50 100 150 200 250 300 350 40000.20.40.60.81a1.0 a0.5precisionnumber of samplesEvaluation of retrieved actions on visual ground truth11341  11345A black car pulls up. Twoarmy officers get out.Figure 3. Evaluation of scriptbased action annotation. Left Precision of action annotation evaluated on visual ground truth. RightExample of a visual false positive for get out of a car.scripts and movies. To investigate this issue, we manuallyannotated several hundreds of actions in 12 movie scriptsand verified these on the visual ground truth. From 147 actions with correct text alignment a1 only 70 did matchwith the video. The rest of samples either were misalignedin time 10, were outside the field of view 10 or werecompletely missing in the video 10. Misalignment ofsubtitles a  1 further decreases the visual precision asillustrated in figure 3 left. Figure 3 right shows a typicalexample of a visual false positive for the action get outof a car occurring outside the field of view of the camera.2.2. Text retrieval of human actionsExpressions for human actions in text may have a considerable withinclass variability. The following examplesillustrate variations in expressions for the GetOutCar action Will gets out of the Chevrolet., A black car pullsup. Two army officers get out., Erin exits her new truck..Furthermore, false positives might be difficult to distinguish from positives, see examples for the SitDown action About to sit down, he freezes., Smiling, he turnsto sit down. But the smile dies on his face when he findshis place occupied by Ellie.. Textbased action retrieval,hence, is a nontrivial task that might be difficult to solveby a simple keyword search such as commonly used for retrieving images of objects, e.g. in 14.To cope with the variability of text describing human actions, we adopt a machine learning based text classificationapproach 16. A classifier labels each scene descriptionin scripts as containing the target action or not. The implemented approach relies on the bagoffeatures model, whereeach scene description is represented as a sparse vector in ahighdimensional feature space. As features we use words,adjacent pairs of words, and nonadjacent pairs of words occurring within a small window of N words where N variesbetween 2 and 8. Features supported by less than threetraining documents are removed. For the classification weuse a regularized perceptron 21, which is equivalent to asupport vector machine. The classifier is trained on a manually labeled set of scene descriptions, and the parametersregularization constant, window size N, and the acceptance0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 100.10.20.30.40.50.60.70.80.91recallprecisionRegularized Perceptron action retrieval from scriptsAllActionsAnswerPhoneGetOutCarHandShakeHugPersonKissSitDownSitUpStandUp0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 100.10.20.30.40.50.60.70.80.91recallprecisionKeywords action retrieval from scriptsAllActionsAnswerPhoneGetOutCarHandShakeHugPersonKissSitDownSitUpStandUpFigure 4. Results of retrieving eight classes of human actions fromscripts using regularized perceptron classifier left and regular expression matching right.threshold are tuned using a validation set.We evaluate textbased action retrieval on our eightclasses of movie actions that we use throughout this paper AnswerPhone, GetOutCar, HandShake, HugPerson, Kiss,SitDown, SitUp, StandUp. The text test set contains 397 actionsamples and over 17K nonaction samples from 12 manually annotated movie scripts. The text training set was sampled from a large set of scripts different from the test set.We compare results obtained by the regularized perceptronclassifier and by matching regular expressions which weremanually tuned to expressions of human actions in text. Theresults in figure 4 very clearly confirm the benefits of thetext classifier. The average precisionrecall values for allactions are prec. 0.95  rec. 0.91 for the text classifier versus prec. 0.55  rec. 0.88 for regular expression matching.2.3. Video datasets for human actionsWe construct two video training sets, a manual and anautomatic one, as well as a video test set. They containvideo clips for our eight classes of movie actions see toprow of figure 10 for illustration. In all cases we first applyautomatic script alignment as described in section 2.1. Forthe clean, manual dataset as well as the test set we manually select visually correct samples from the set of manually textannotated actions in scripts. The automatic datasetcontains training samples that have been retrieved automatically from scripts by the text classifier described in section 2.2. We limit the automatic training set to actions withan alignment score a  0.5 and a video length of less than1000 frames. Our manual and automatic training sets contain action video sequences from 12 movies 2 and the testset actions from 20 different movies 3. Our datasets, i.e., the2 American Beauty, Being John Malkovich, Big Fish,Casablanca, The Crying Game, Double Indemnity, Forrest Gump,The Godfather, I Am Sam, Independence Day, Pulp Fiction andRaising Arizona.3 As Good As It Gets, Big Lebowski, Bringing Out The Dead,The Butterfly Effect, Dead Poets Society, Erin Brockovich, Fargo,Gandhi, The Graduate, Indiana Jones And The Last Crusade, ItsA Wonderful Life, Kids, LA Confidential, The Lord of the RingsFellowship of the Ring, Lost Highway, The Lost Weekend, MissionTo Mars, Naked City, The Pianist and Reservoir Dogs.5 6 9 7 10 21 5 33 9615 6 14 8 34 30 7 29 14320 12 23 15 44 51 12 62 239 233automatically labeled training setAnswerPhoneGetOutCarHandShakeHugPersonKissSitDownSitUpStandUpTotal labelsTotal samplesFalseCorrectAll22 13 20 22 49 47 11 47 231 219manually labeled training set            23 13 19 22 51 30 10 49 217 211test set            Table 1. The number of action labels in automatic training settop, cleanmanual training set middle and test set bottom.video clips and the corresponding annotations, are availableat httpwww.irisa.frvistaactions.The objective of having two training sets is to evaluate recognition of actions both in a supervised setting andwith automatically generated training samples. Note thatno manual annotation is performed neither for scripts norfor videos used in the automatic training set. The distribution of action labels for the different subsets and actionclasses is given in table 1. We can observe that the number of correctly labeled videos in the automatic set is 60.Most of the wrong labels result from the scriptvideo misalignment and a few additional errors come from the textclassifier. The problem of classification in the presence ofwrong training labels will be addressed in section 4.3.3. Video classification for action recognitionThis section presents our approach for action classification. It builds on existing bagoffeatures approaches forvideo description 3, 13, 15 and extends recent advances instatic image classification to videos 2, 9, 12. Lazebnik etal. 9 showed that a spatial pyramid, i.e., a coarse description of the spatial layout of the scene, improves recognition.Successful extensions of this idea include the optimizationof weights for the individual pyramid levels 2 and the useof more general spatial grids 12. Here we build on theseideas and go a step further by building spacetime grids.The details of our approach are described in the following.3.1. Spacetime featuresSparse spacetime features have recently shown goodperformance for action recognition 3, 6, 13, 15. They provide a compact video representation and tolerance to background clutter, occlusions and scale changes. Here we follow 7 and detect interest points using a spacetime extension of the Harris operator. However, instead of performingscale selection as in 7, we use a multiscale approach andextract features at multiple levels of spatiotemporal scales2i , 2j  with i  21i2, i  1, ..., 6 and j  2j2, j 1, 2 . This choice is motivated by the reduced computationalFigure 5. Spacetime interest points detected for two video frameswith human actions hand shake left and get out car right.complexity, the independence from scale selection artifactsand the recent evidence of good recognition performanceusing dense scale sampling. We also eliminate detectionsdue to artifacts at shot boundaries 11. Interest points detected for two frames with human actions are illustrated infigure 5.To characterize motion and appearance of local features,we compute histogram descriptors of spacetime volumesin the neighborhood of detected points. The size of eachvolume x,y,t is related to the detection scales byx,y  2k, t  2k . Each volume is subdivided intoa nx, ny, nt grid of cuboids for each cuboid we computecoarse histograms of oriented gradient HoG and optic flowHoF. Normalized histograms are concatenated into HoGand HoF descriptor vectors and are similar in spirit to thewell known SIFT descriptor. We use parameter values k 9, nx, ny  3, nt  2.3.2. Spatiotemporal bagoffeaturesGiven a set of spatiotemporal features, we build aspatiotemporal bagoffeatures BoF. This requires theconstruction of a visual vocabulary. In our experiments wecluster a subset of 100k features sampled from the trainingvideos with the kmeans algorithm. The number of clustersis set to k  4000, which has shown empirically to givegood results and is consistent with the values used for staticimage classification. The BoF representation then assignseach feature to the closest we use Euclidean distance vocabulary word and computes the histogram of visual wordoccurrences over a spacetime volume corresponding eitherto the entire video sequence or subsequences defined by aspatiotemporal grid. If there are several subsequences thedifferent histograms are concatenated into one vector andthen normalized.In the spatial dimensions we use a 1x1 gridcorresponding to the standard BoF representation, a 2x2gridshown to give excellent results in 9, a horizontalh3x1 grid 12 as well as a vertical v1x3 one. Moreover, weimplemented a denser 3x3 grid and a centerfocused o2x2grid where neighboring cells overlap by 50 of their widthand height. For the temporal dimension we subdivide thevideo sequence into 1 to 3 nonoverlapping temporal bins,xy t1x1 t1 1x1 t2 h3x1 t1 o2x2 t1Figure 6. Examples of a few spatiotemporal grids.resulting in t1, t2 and t3 binnings. Note that t1 representsthe standard BoF approach. We also implemented a centerfocused ot2 binning. Note that for the overlapping grids thefeatures in the center obtain more weight.The combination of six spatial grids with four temporalbinnings results in 24 possible spatiotemporal grids. Figure 6 illustrates some of the grids which have shown to beuseful for action recognition. Each combination of a spatiotemporal grid with a descriptor, either HoG or HoF, is in thefollowing called a channel.3.3. Nonlinear Support Vector MachinesFor classification, we use a nonlinear support vector machine with a multichannel 2 kernel that robustly combineschannels 20. We use the multichannel Gaussian kerneldefined byKHi,Hj  expcC1AcDcHi,Hj1where Hi  hin and Hj  hjn are the histograms forchannel c and DcHi,Hj is the 2 distance defined asDcHi,Hj 12Vn1hin  hjn2hin  hjn2with V the vocabulary size. The parameter Ac is the meanvalue of the distances between all training samples for achannel c 20. The best set of channels C for a given training set is found based on a greedy approach. Starting withan empty set of channels all possible additions and removalsof channels are evaluated until a maximum is reached. Inthe case of multiclass classification we use the oneagainstall approach.4. Experimental resultsIn the following we first evaluate the performance of thedifferent spatiotemporal grids in section 4.1. We then compare our approach to the stateoftheart in section 4.2 andevaluate the influence of noisy, i.e., incorrect, labels in section 4.3. We conclude with experimental results for ourmovie datasets in section 4.44.1. Evaluation of spatiotemporal gridsIn this section we evaluate if spatiotemporal grids improve the classification accuracy and which grids performbest in our context. Previous results for static image classification have shown that the best combination depends onthe class as well as the dataset 9, 12. The approach wetake here is to select the overall most successful channelsand then to choose the most successful combination for eachclass individually.As some grids may not perform well by themselves,but contribute within a combination 20, we search forthe most successful combination of channels descriptor spatiotemporal grid for each action class with a greedyapproach. To avoid tuning to a particular dataset, we findthe best spatiotemporal channels for both the KTH actiondataset and our manually labeled movie dataset. The experimental setup and evaluation criteria for these two datasetsare presented in sections 4.2 and 4.4. We refer the reader tothese sections for details.Figure 7 shows the number of occurrences for each ofour channel components in the optimized channel combinations for KTH and movie actions. We can see that HoGdescriptors are chosen more frequently than HoFs, but bothare used in many channels. Among the spatial grids thehorizontal 3x1 partitioning turns out to be most successful.The traditional 1x1 grid and the centerfocused o2x2 perform also very well. The 2x2, 3x3 and v1x3 grids occur lessoften and are dropped in the following. They are either redundant 2x2, too dense 3x3, or do not fit the geometryof natural scenes v1x3. For temporal binning no temporalsubdivision of the sequence t1 shows the best results, butt3 and t2 also perform very well and complement t1. Theot2 binning turns out to be rarely used in practiceit oftenduplicates t2and we drop it from further experiments.Table 2 presents for each datasetaction the performanceof the standard bagoffeatures with HoG and HoF descriptors, of the best channel as well as of the best combinationof channels found with our greedy search. We can observethat the spatiotemporal grids give a significant gain over thestandard BoF methods. Moreover, combining two to three 0 5 10 15 20 25hoghofh3x11x1o2x22x23x3v1x3t1 t3 t2 ot2KTH actionsMovie actionsFigure 7. Number of occurrences for each channel componentwithin the optimized channel combinations for the KTH actiondataset and our manually labeled movie dataset.Task HoG BoF HoF BoF Best channel Best combinationKTH multiclass 81.6 89.7 91.1 hof h3x1 t3 91.8 hof 1 t2, hog 1 t3Action AnswerPhone 13.4 24.6 26.7 hof h3x1 t3 32.1 hof o2x2 t1, hof h3x1 t3Action GetOutCar 21.9 14.9 22.5 hof o2x2 1 41.5 hof o2x2 t1, hog h3x1 t1Action HandShake 18.6 12.1 23.7 hog h3x1 1 32.3 hog h3x1 t1, hog o2x2 t3Action HugPerson 29.1 17.4 34.9 hog h3x1 t2 40.6 hog 1 t2, hog o2x2 t2, hog h3x1 t2Action Kiss 52.0 36.5 52.0 hog 1 1 53.3 hog 1 t1, hof 1 t1, hof o2x2 t1Action SitDown 29.1 20.7 37.8 hog 1 t2 38.6 hog 1 t2, hog 1 t3Action SitUp 6.5 5.7 15.2 hog h3x1 t2 18.2 hog o2x2 t1, hog o2x2 t2, hog h3x1 t2Action StandUp 45.4 40.0 45.4 hog 1 1 50.5 hog 1 t1, hof 1 t2Table 2. Classification performance of different channels and their combinations. For the KTH dataset the average class accuracy isreported, whereas for our manually cleaned movie dataset the perclass average precision AP is given.channels further improves the accuracy.Interestingly, HoGs perform better than HoFs for all realworld actions except for answering the phone. The inverseholds for KTH actions. This shows that the context and theimage content play a large role in realistic settings, whilesimple actions can be very well characterized by their motion only. Furthermore, HoG features also capture motioninformation up to some extent through their local temporalbinning.In more detail, the optimized combinations for sittingdown and standing up do not make use of spatial grids,which can be explained by the fact that these actions canoccur anywhere in the scene. On the other hand, temporalbinning does not help in the case of kissing, for which a highvariability with respect to the temporal extent can be observed. For getting out of a car, handshaking and hugging acombination of a h3x1 and a o2x2 spatial grid is successful.This could be due to the fact that those actions are usuallypictured either in a wide setting where a scenealigned gridshould work or as a closeup where a uniform grid shouldperform well.The optimized combinations determined in this section,cf. table 2, are used in the remainder of the experimentalsection.4.2. Comparison to the stateoftheartWe compare our work to the stateoftheart on the KTHactions dataset 15, see figure 8. It contains six types ofhuman actions, namely walking, jogging, running, boxing,hand waving and hand clapping, performed several timesby 25 subjects. The sequences were taken for four different scenarios outdoors, outdoors with scale variation, outdoors with different clothes and indoors. Note that in allcases the background is homogeneous. The dataset conMethod Schuldt Niebles Wong ourset al. 15 et al. 13 et al. 18Accuracy 71.7 81.5 86.7 91.8Table 3. Average class accuracy on the KTH actions dataset.Walking Jogging Running Boxing Waving ClappingFigure 8. Sample frames from the KTH actions sequences. All sixclasses columns and scenarios rows are presented.tains a total of 2391 sequences. We follow the experimentalsetup of Schuldt et al. 15 with sequences divided into thetrainingvalidation set 88 people and the test set 9 people. The best performing channel combination, reportedin the previous section, was determined by 10fold crossvalidation on the combined trainingvalidation set. Resultsare reported for this combination on the test set.Table 3 compares the average class accuracy of ourmethod with results reported by other researchers. Compared to the existing approaches, our method shows significantly better performance, outperforming the stateoftheart in the same setup. The confusion matrix for our methodis given in table 4. Interestingly, the major confusion occursbetween jogging and running.WalkingJoggingRunningBoxingWavingClappingWalking .99 .01 .00 .00 .00 .00Jogging .04 .89 .07 .00 .00 .00Running .01 .19 .80 .00 .00 .00Boxing .00 .00 .00 .97 .00 .03Waving .00 .00 .00 .00 .91 .09Clapping .00 .00 .00 .05 .00 .95Table 4. Confusion matrix for the KTH actions.Note that results obtained by Jhuang et al. 6 and Wonget al. 19 are not comparable to ours, as they are basedon nonstandard experimental setups they either use moretraining data or the problem is decomposed into simplertasks.4.3. Robustness to noise in the training dataTraining with automatically retrieved samples avoids thehigh cost of manual data annotation. Yet, this goes in handwith the problem of wrong labels in the training set. In thissection we evaluate the robustness of our action classification approach to labeling errors in the training set.Figure 9 shows the recognition accuracy as a function ofthe probability p of a label being wrong. Training for p  0is performed with the original labels, whereas with p  1 alltraining labels are wrong. The experimental results are obtained for the KTH dataset and the same setup as describedin subsection 4.2. Different wrong labelings are generatedand evaluated 20 times for each p the average accuracy andits variance are reported.The experiment shows that the performance of ourmethod degrades gracefully in the presence of labeling errors. Up to p  0.2 the performance decreases insignificantly, i.e., by less than two percent. At p  0.4 the performance decreases by around 10. We can, therefore, predicta very good performance for the proposed automatic training scenario, where the observed amount of wrong labels isaround 40.Note that we have observed a comparable level of resistance to labeling errors when evaluating an image classification method on the naturalscene images of the PASCALVOC07 challenge dataset. 0 0.2 0.4 0.6 0.8 1 0  0.2  0.4  0.6  0.8  1Average class accuracyProbability of a wrong labelKTH actionsFigure 9. Performance of our video classification approach in thepresence of wrong labels. Results are report for the KTH dataset.4.4. Action recognition in realworld videosIn this section we report action classification results forrealword videos, i.e., for our test set with 217 videos.Training is performed with a clean, manual dataset as wellas an automatically annotated one, see section 2.3 for deClean Automatic ChanceAnswerPhone 32.1 16.4 10.6GetOutCar 41.5 16.4 6.0HandShake 32.3 9.9 8.8HugPerson 40.6 26.8 10.1Kiss 53.3 45.1 23.5SitDown 38.6 24.8 13.8SitUp 18.2 10.4 4.6StandUp 50.5 33.6 22.6Table 5. Average precision AP for each action class of our testset. We compare results for clean annotated and automatic training data. We also show results for a random classifier chance.tails. We train a classifier for each action as being presentor not following the evaluation procedure of 5. The performance is evaluated with the average precision AP of theprecisionrecall curve. We use the optimized combinationof spatiotemporal grids from section 4.1. Table 5 presentsthe AP values for the two training sets and for a randomclassifier referred to as chance AP.The classification results are good for the manual training set and lower for the automatic one. However, for allclasses except HandShake the automatic training obtainsresults significantly above chance level. This shows thatan automatically trained system can successfully recognizehuman actions in realworld videos. For kissing, the performance loss between automatic and manual annotationsis minor. This suggests that the main difficulty with our automatic approach is the low number of correctly labeled examples and not the percentage of wrong labels. This problem could easily be avoided by using a large database ofmovies which we plan to address in the future.Figure 10 shows some example results obtained by ourapproach trained with automatically annotated data. Wedisplay key frames of test videos for which classificationobtained the highest confidence values. The two top rowsshow true positives and true negatives. Note that despitethe fact that samples were highly scored by our method,they are far from trivial the videos show a large variabilityof scale, viewpoint and background. The two bottom rowsshow wrongly classified videos. Among the false positivesmany display features not unusual for the classified action,for example the rapid getting up is typical for GetOutCaror the stretched hands are typical for HugPerson. Most ofthe false negatives are very difficult to recognize, see for example the occluded handshake or the hardly visible persongetting out of the car.5. ConclusionThis paper has presented an approach for automaticallycollecting training data for human actions and has shownthat this data can be used to train a classifier for action recognition. Our approach for automatic annotationAnswerPhone GetOutCar HandShake HugPerson Kiss SitDown SitUp StandUpTPTNFPFNFigure 10. Example results for action classification trained on the automatically annotated data. We show the key frames for test movieswith the highest confidence values for truefalse positivesnegatives.achieves 60 precision and scales easily to a large number of action classes. It also provides a convenient semiautomatic tool for generating action samples with manualannotation. Our method for action classification extendsrecent successful image recognition methods to the spatiotemporal domain and achieves best up to date recognitionperformance on a standard benchmark 15. Furthermore, itdemonstrates high tolerance to noisy labels in the trainingset and, therefore, is appropriate for action learning in automatic settings. We demonstrate promising recognition results for eight difficult and realistic action classes in movies.Future work includes improving the scripttovideoalignment and extending the video collection to a muchlarger dataset. We also plan to improve the robustness of ourclassifier to noisy training labels based on an iterative learning approach. Furthermore, we plan to experiment with alarger variety of spacetime lowlevel features. In the longterm we plan to move away from bagoffeatures based representations by introducing detector style action classifiers.Acknowledgments. M. Marszaek is supported by theEuropean Community under the MarieCurie project VISITOR. This work was supported by the European researchproject CLASS. We would like to thank J. Ponce and A. Zisserman for discussions.References1 T. Berg, A. Berg, J. Edwards, M. Maire, R. White, Y. Teh,E. Learned Miller, and D. Forsyth. Names and faces in the news.In CVPR, 2004.2 A. Bosch, A. Zisserman, and X. Munoz. Representing shape with aspatial pyramid kernel. In CIVR, 2007.3 P. Dollar, V. Rabaud, G. Cottrell, and S. Belongie. Behavior recognition via sparse spatiotemporal features. In VSPETS, 2005.4 M. Everingham, J. Sivic, and A. Zisserman. Hello my name is...Buffy  automatic naming of characters in TV video. In BMVC,2006.5 M. Everingham, L. van Gool, C. Williams, J. Winn, and A. Zisserman. Overview and results of classification challenge, 2007. ThePASCAL VOC07 Challenge Workshop, in conj. with ICCV.6 H. Jhuang, T. Serre, L. Wolf, and T. Poggio. A biologically inspiredsystem for action recognition. In ICCV, 2007.7 I. Laptev. On spacetime interest points. IJCV, 6423107123,2005.8 I. Laptev and P. Perez. Retrieving actions in movies. In ICCV, 2007.9 S. Lazebnik, C. Schmid, and J. Ponce. Beyond bags of featuresspatial pyramid matching for recognizing natural scene categories.In CVPR, 2006.10 L. Li, G. Wang, and L. Fei Fei. Optimol automatic online picturecollection via incremental model learning. In CVPR, 2007.11 R. Lienhart. Reliable transition detection in videos A survey andpractitioners guide. IJIG, 13469486, 2001.12 M. Marszaek, C. Schmid, H. Harzallah, and J. van de Weijer. Learning object representations for visual object class recognition, 2007.The PASCAL VOC07 Challenge Workshop, in conj. with ICCV.13 J. C. Niebles, H. Wang, and L. FeiFei. Unsupervised learning ofhuman action categories using spatialtemporal words. In BMVC,2006.14 F. Schroff, A. Criminisi, and A. Zisserman. Harvesting imagedatabases from the web. In ICCV, 2007.15 C. Schuldt, I. Laptev, and B. Caputo. Recognizing human actions Alocal SVM approach. In ICPR, 2004.16 F. Sebastiani. Machine learning in automated text categorization.ACM Computing Surveys, 341147, 2002.17 J. Willamowski, D. Arregui, G. Csurka, C. R. Dance, and L. Fan.Categorizing nine visual classes using local appearance descriptors.In IWLAVS, 2004.18 S.F. Wong and R. Cipolla. Extracting spatiotemporal interest pointsusing global information. In ICCV, 2007.19 S.F. Wong, T. K. Kim, and R. Cipolla. Learning motion categoriesusing both semantic and structural information. In CVPR, 2007.20 J. Zhang, M. Marszalek, S. Lazebnik, and C. Schmid. Local features and kernels for classification of texture and object categoriesA comprehensive study. IJCV, 732213238, 2007.21 T. Zhang. Large margin winnow methods for text categorization. InKDD2000 Workshop on Text Mining, 2000.
