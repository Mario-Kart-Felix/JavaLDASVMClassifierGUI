Natural Language Processing for MediaWiki: First major release of the Semantic Assistants Wiki-NLP Integration | semanticsoftware.info Skip navigation . semanticsoftware.info Semantic Software Lab Concordia University Montréal, Canada Research Projects Tools & Resources Publications Blogs Forums Semantic Assistants Durm Wiki Open Positions Contact Search this site: Site Menu Home Research SSL for Students Projects Tools & Resources Publications Blogs Forums Contact User login Log in using OpenID: What is OpenID? Username: * Password: * Log in using OpenID Cancel OpenID login To prevent automated spam submissions leave this field empty. Request new password Upcoming events No upcoming events available more Popular content Today's: The Durm Corpus Semantic Assistants Zeeva: A Collaborative Semantic Literature Management System All time: Using OwlExporter: various questions The Durm Project The Durm German Lemmatizer Last viewed: Interpretation of Reported Speech Semantic Assistants Architecture Durm Indexer - Small Current weather Montréal, Canada Overcast, mist, light rain Temperature: 3 °C , feels like 0.4 °C Wind: Northwest, 9.3 km/h Pressure: 1008 hPa Rel. Humidity: 93 % Visibility: 8 km Sunrise: 06:23 -0500 Sunset: 17:50 -0500 Reported on: Wed, 2017-03-08 02:21 Natural Language Processing for MediaWiki: First major release of the Semantic Assistants Wiki-NLP Integration Semantic Assistants Semantic Wiki NLP Text Mining Printer-friendly version PDF version Table of Contents   1. Introduction 2. Features 3. Application: NLP Wikis in Use 4. More Information 5. In the News 1. Introduction We are happy to announce the first major release of our Semantic Assistants Wiki-NLP integration . This is the first comprehensive open source solution for bringing Natural Language Processing (NLP) to wiki users, in particular for wikis based on the well-known MediaWiki engine and its Semantic MediaWiki (SMW) extension. It can run any NLP pipeline deployed in the General Architecture for Text Engineering (GATE) , brokered as web services through the Semantic Assistants server. This allows you to bring novel text mining assistants to wiki users, e.g., for automatically structuring wiki pages, answering questions in natural language, quality assurance, entity detection, summarization, among others. The results of the NLP analysis are written back to the wiki, allowing humans and AI to work collaboratively on wiki content. Additionally, semantic markup understood by the SMW extension can be automatically generated from NLP output, providing semantic search and query functionalities. 2. Features The current release includes the following features: Light-weight MediaWiki Extension The Wiki-NLP integration is introduced to an existing MediaWiki engine through installing a light-weight extension. Without requiring modifications on the wiki engine, the extension adds a link to the wiki toolbox menu through which users can load the Wiki-NLP interface. Using this interface, users can then inquire about and invoke NLP services through the dynamically generated Wiki-NLP interface within the wiki environment. Therefore, no context switching is needed by the wiki users in order to use the NLP services. NLP Pipeline Independent Architecture The Wiki-NLP integration is backed by the Semantic Assistants server, which provides a service-oriented solution to offer NLP capabilities in a wiki system. Therefore, any NLP service available in a given Semantic Assistants server can be invoked through the Wiki-NLP integration on a wiki's content. Flexible Wiki Input Handling At times, a user's information need is scattered across multiple pages in the wiki. To address this problem, our Wiki-NLP integration allows wiki users to collect one or multiple pages of the wiki in a so-called "collection" and run an NLP service on the collected pages at once. This feature allows batch-processing of wiki pages, as well as gathering multiple input pages for pipelines analyzing multi-documents. Flexible NLP Result Handling The Wiki-NLP integration is also flexible in terms of where the NLP pipelines' output can be written. Upon a user's request, the pipeline results can be appended to an existing page body or its associated discussion page, create a new page, as well as writing to a wiki page in an external wiki, provided that it is supported by the Wiki-NLP integration architecture. Based on the type of results generated by the NLP pipeline, e.g., annotations or new files, the Wiki-NLP integration offers a simple template-based visualization capability that can be easily customized. Upon each successful NLP service execution, the Wiki-NLP integration automatically updates the existing results on the specified wiki page, where applicable. Semantic Markup Generation Where semantic metadata is generated by an NLP pipeline, the Wiki-NLP integration takes care of representing it in a formal language using the Semantic MediaWiki special markup. For generated metadata, the Wiki-NLP integration enriches the text with its equivalent markup and makes it permanent in the wiki repository. Therefore, for each generated result, both a user-friendly and machine-processable representation of the result is made available in the page. These markups are, in turn, transformed to RDF triples by the Semantic MediaWiki parsing engine, making them available for querying purposes as well as externalization to other applications. Semantic enrichment of wiki text with the Wiki-NLP integration For example, when the sentence "Mary won the first prize." is contained in a wiki page and processed by a Named Entity Detection pipeline, an XML document is generated by the Semantic Assistants server and returned back to the Wiki-NLP integration, which indicates "Mary" as an entity of type "Person". This XML document is then processed by our integration and transformed for Semantic MediaWiki into a formal representation in the form of markup. In our example, [[hasType::Person|Mary]] markup is generated and written into the wiki page. The generated markup can then be queried using Semantic MediaWiki's inline queries . For example, a simple query like {{#ask: [[hasType::Person]]}} can be used to retrieve all the entities in wiki content with the type "Person". Wiki-independent Architecture The Wiki-NLP integration was developed from the ground up with extensibility in mind. Although the provided examples show how the Wiki-NLP integration can be used within a MediaWiki instance, it has an extensible architecture, where support for other wiki engines can be added to the architecture with a reasonable amount of effort. Both the Semantic Assistants server and the Wiki-NLP integration have a semantic-based architecture that allows adding new services and wiki engines without major modifications of their base code. 3. Application: NLP Wikis in Use Our open source Wiki-NLP solution is the result of more than 5 years of research [1] , [2] , [3] on the technical and social aspects of combining natural language processing with collaborative wiki systems. We developed a number of real-world wiki-based solutions that demonstrate how text mining assistants can effectively collaborate with humans on wiki content [4] . As part of this research, we investigated (i) the software engineering aspects of Wiki-NLP integrations, (ii) the usability for wiki users with different backgrounds, in particular those unfamiliar with NLP; and (iii) its effectiveness for helping users to develop and improve wiki content in a number of domains and tasks. To help you build your own Wiki-NLP solution, we documented a number of successful Wiki-NLP patterns in our Semantic Assistants Wiki-NLP Showcase. In the DurmWiki project, we investigate the application of NLP to wikis in cultural heritage data management, helping wiki users finding relevant information through NLP pipelines for automatic index generation, question-answering, and summarization on wiki content [5] . Our Wiki-NLP approach allows to transform historical documents into a semantic knowledge base that can be queried through state-of-the-art semantic technologies [6] . For biomedical research, Intelli GenWiki is our solution for helping curators dealing with the large amount of publications in this area. Text mining assistants can aid humans in deciding which papers to curate (triage task) and extract entities (database curation task) through biomedical entity recognition, e.g., for organisms or mutations . Experiments measuring the time for manual vs. NLP/Wiki supported curation in a real-world project demonstrate the effectiveness of this idea [7] . With ReqWiki , we developed the first semantic open source platform for collaborative software requirements engineering [8] . Here, semantic assistants provide users with tools for entity extraction on domain documents and quality assurance services for improving the content of a software requirements specification (SRS). User studies confirmed the usability for software engineers unfamiliar with NLP and its effectiveness for improving requirements documents [9] . 4. More Information For further technical information, please see our Wiki-NLP Integration page . For a number of application examples, check out our Semantic Assistants Wiki-NLP Showcase . For commercial support or consulting requests, please contact us . 5. In the News Concordia NOW Newsletter: Information overload? There's a solution for that References Witte, R. , and T. Gitzinger , " Connecting Wikis and Natural Language Processing Systems ", WikiSym '07: Proceedings of the 2007 International Symposium on Wikis , Montréal, Canada : ACM, pp. 165–176, October 21--23, 2007. Sateli, B. , " A General Architecture to Enhance Wiki Systems with Natural Language Processing Techniques ", Department of Computer Science and Software Engineering , M.Sc. Software Engineering, Montreal : Concordia University, 04/2012. Sateli, B. , and R. Witte , " Natural Language Processing for MediaWiki: The Semantic Assistants Approach ", The 8th International Symposium on Wikis and Open Collaboration (WikiSym 2012) , Linz, Austria : ACM, 08/2012. Sateli, B. , and R. Witte , " Supporting Wiki Users with Natural Language Processing ", The 8th International Symposium on Wikis and Open Collaboration (WikiSym 2012) , Linz, Austria : ACM, 08/2012. Sporleder, C. , A. van den Bosch , and K. Zervanou (Eds.), Witte, R. , T. Kappler , R. Krestel , and P. C. Lockemann , " Integrating Wiki Systems, Natural Language Processing, and Semantic Technologies for Cultural Heritage Data Management ", Language Technology for Cultural Heritage Springer Berlin Heidelberg, pp. 213--230, 2011. Witte, R. , R. Krestel , T. Kappler , and P. C. Lockemann , " Converting a Historical Architecture Encyclopedia into a Semantic Knowledge Base ", IEEE Intelligent Systems , vol. 25, no. 1, Los Alamitos, CA, USA : IEEE Computer Society, pp. 58--66, January/February, 2010. Sateli, B. , M. - J. Meurs , G. Butler , J. Powlowski , A. Tsang , and R. Witte , " IntelliGenWiki: An Intelligent Semantic Wiki for Life Sciences ", NETTAB 2012 , vol. 18 (Supplement B), Como, Italy : EMBnet.journal, pp. 50–52, 11/2012. Sateli, B. , S. S. Rajivelu , E. Angius , and R. Witte , " ReqWiki: A Semantic System for Collaborative Software Requirements Engineering ", The 8th International Symposium on Wikis and Open Collaboration (WikiSym 2012) , Linz, Austria : ACM, 08/2012. Sateli, B. , E. Angius , S. S. Rajivelu , and R. Witte , " Can Text Mining Assistants Help to Improve Requirements Specifications? ", Mining Unstructured Data (MUD 2012) , Kingston, Ontario, Canada, October 17, 2012. » Google Plus One Share on Facebook Facebook Like Linkedin Share Button Tweet Widget See also Tutorial: Adding Natural Language Processing Support to your (Semantic) MediaWiki Semantic Assistants for Wiki Systems Semantic Assistants Wiki-NLP Showcase Wiki-NLP Integration Research in Concordia NOW Newsletter Semantic Assistants Project Tag Cloud AI Bioinformatics Coreference Resolution Corpora Cultural Heritage Data Fuzzy Sets & Systems German Information Systems Lemmatization Linked Open Data Literature Management NLP Noun Phrase Chunking Ontology Process Modeling Reported Speech Requirements Engineering Semantic Computing Semantic Desktop Semantic Publishing Semantic Web Semantic Wiki Software Engineering Software Evolution Summarization System Architecture Teaching Text Mining Textual Entailment Recognition Traceability more tags @SemSoft's tweets RT @GateAcUk : Early bird registration to GATE training course still available: https://t.co/3zmjIX28E3 — 44 weeks 5 days ago Link up with us during @www2016ca #www2016 https://t.co/xw43mCrzjR — 46 weeks 6 days ago RT @alegonbel : #savesd2016 #www2016 I've created a #storify story for the workshop https://t.co/fpNpSiP2UN — 47 weeks 11 hours ago RT @savesdworkshop : #savesd2016 #www2016 Bahar Sateli on "Semantic User Profiles: Learning Scholars’ Competences by Analyzing [...] " https://t.co/uLs0rWgjqa — 47 weeks 1 day ago RT @shawnmjones : #savesd2016 #www2016 @BaharSateli presents open source workflow for semantic pub experiments https://t.co/t68r1kYRTz https://t.co/INWWpxLu93 — 47 weeks 1 day ago   1 of 23 ›› New Publications Semantic User Profiles: Learning Scholars' Competences by Analyzing their Publications From Papers to Triples: An Open Source Workflow for Semantic Publishing Experiments Semantic representation of scientific literature: bringing claims, contributions and named entities onto the Linked Open Data cloud Automated Quality Assurance of Non-Functional Requirements for Testability Automatic Construction of a Semantic Knowledge Base from CEUR Workshop Proceedings More... Recent blog posts OpenTrace Showcased at the WCRE'12 Conference Wiki-NLP Integration at the WikiSym'12 Conference mycoMINE au 80ème congrès de l'Acfas Text Mining Assistants in Wikis at Biocuration2012 DTMBIO2011 Talk Semantic Assistants at IBM CASCON 2011 mycoMINE at DTMBIO/CIKM 2011 Glasgow NL
