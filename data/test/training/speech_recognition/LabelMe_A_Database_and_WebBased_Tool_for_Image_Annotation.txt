LabelMe a database and webbased tool for imageannotationBryan C. Russell, Antonio Torralba,Computer Science and Artificial Intelligence Laboratory,Massachusetts Institute of Technology, Cambridge, MA 02139, USAbrussellcsail.mit.edu, torralbacsail.mit.eduKevin P. MurphyDepartments of computer science and statistics,University of British Columbia, Vancouver, BC V6T 1Z4murphykcs.ubc.caWilliam T. FreemanComputer Science and Artificial Intelligence Laboratory,Massachusetts Institute of Technology, Cambridge, MA 02139, USAbillfcsail.mit.eduMIT AI LAB MEMO AIM2005025, SEPTEMBER 2005REVISED APRIL 23, 2007AbstractWe seek to build a large collection of images with ground truth labels to be used for objectdetection and recognition research. Such data is useful for supervised learning and quantitativeevaluation. To achieve this, we developed a webbased tool that allows easy image annotation0and instant sharing of such annotations. Using this annotation tool, we have collected a largedataset that spans many object categories, often containing multiple instances over a wide variety of images. We quantify the contents of the dataset and compare against existing state of theart datasets used for object recognition and detection. Also, we show how to extend the datasetto automatically enhance object labels with WordNet, discover object parts, recover a depth ordering of objects in a scene, and increase the number of labels using minimal user supervisionand images from the web.1 IntroductionThousands of objects occupy the visual world in which we live. Biederman 4 estimates thathumans can recognize about 30000 entrylevel object categories. Recent work in computervision has shown impressive results for the detection and recognition of a few different objectcategories 42, 16, 22. However, the size and contents of existing datasets, among other factors,limit current methods from scaling to thousands of object categories. Research in object detection and recognition would benefit from large image and video collections with ground truthlabels spanning many different object categories in cluttered scenes. For each object present inan image, the labels should provide information about the objects identity, shape, location, andpossibly other attributes such as pose.By analogy with the speech and language communities, history has shown that performanceincreases dramatically when more labeled training data is made available. One can argue thatthis is a limitation of current learning techniques, resulting in the recent interest in Bayesianapproaches to learning 10, 35 and multitask learning 38. Nevertheless, even if we can learneach class from just a small number of examples, there are still many classes to learn.Large image datasets with ground truth labels are useful for supervised learning of object categories. Many algorithms have been developed for image datasets where all training exampleshave the object of interest wellaligned with the other examples 39, 16, 42. Algorithms thatexploit context for object recognition 37, 17 would benefit from datasets with many labeledobject classes embedded in complex scenes. Such datasets should contain a wide variety ofenvironments with annotated objects that cooccur in the same images.When comparing different algorithms for object detection and recognition, labeled data is necessary to quantitatively measure their performance the issue of comparing object detection1algorithms is beyond the scope of this paper see 2, 20 for relevant issues. Even algorithmsrequiring no supervision 31, 28, 10, 35, 34, 27 need this quantitative framework.Building a large dataset of annotated images with many objects is a costly and lengthy enterprise. Traditionally, datasets are built by a single research group and are tailored to solvea specific problem. Therefore, many currently available datasets only contain a small number of classes, such as faces, pedestrians, and cars. Notable exceptions are the Caltech 101dataset 11, with 101 object classes this was recently extended to 256 object classes 15, thePASCAL collection 8, and the CBCLstreetscenes database 5.We wish to collect a large dataset of annotated images. To achieve this, we consider webbased data collection methods. Webbased annotation tools provide a way of building largeannotated datasets by relying on the collaborative effort of a large population of users 43, 30,29, 33. Recently, such efforts have had much success. The Open Mind Initiative 33 aimsto collect large datasets from web users so that intelligent algorithms can be developed. Morespecifically, common sense facts are recorded e.g. red is a primary color, with over 700K factsrecorded to date. This project is seeking to extend their dataset with speech and handwritingdata. Flickr 30 is a commercial effort to provide an online image storage and organizationservice. Users often provide textual tags to provide a caption of depicted objects in an image.Another way lots of data has been collected is through an online game that is played by manyusers. The ESP game 43 pairs two random online users who view the same target image.The goal is for them to try to read each others mind and agree on an appropriate namefor the target image as quickly as possible. This effort has collected over 10 million imagecaptions since 2003, with the images randomly drawn from the web. While the amount of datacollected is impressive, only caption data is acquired. Another game, Peekaboom 44 has beencreated to provide location information of objects. While location information is provided for alarge number of images, often only small discriminant regions are labeled and not entire objectoutlines.In this paper we describe LabelMe, a database and an online annotation tool that allows thesharing of images and annotations. The online tool provides functionalities such as drawingpolygons, querying images, and browsing the database. In the first part of the paper we describethe annotation tool and dataset and provide an evaluation of the quality of the labeling. In thesecond part of the paper we present a set of extensions and applications of the dataset. In thissection we see that a large collection of labeled data allows us to extract interesting informationthat was not directly provided during the annotation process. In the third part we compare2the LabelMe dataset against other existing datasets commonly used for object detection andrecognition.2 LabelMeIn this section we describe the details of the annotation tool and the results of the online collection effort.2.1 Goals of the LabelMe projectThere are a large number of publically available databases of visual objects 38, 2, 21, 25, 9,11, 12, 15, 7, 23, 19, 6. We do not have space to review them all here. However, we give abrief summary of the main features that distinguishes the LabelMe dataset from other datasets. Designed for object class recognition as opposed to instance recognition. To recognizean object class, one needs multiple images of different instances of the same class, aswell as different viewing conditions. Many databases, however, only contain differentinstances in a canonical pose. Designed for learning about objects embedded in a scene. Many databases consist ofsmall cropped images of object instances. These are suitable for training patchbasedobject detectors such as sliding window classifiers, but cannot be used for training detectors that exploit contextual cues. High quality labeling. Many databases just provide captions, which specify that the object is present somewhere in the image. However, more detailed information, such asbounding boxes, polygons or segmentation masks, is tremendously helpful. Many diverse object classes. Many databases only contain a small number of classes,such as faces, pedestrians and cars a notable exception is the Caltech 101 database,which we compare against in Section 4. Many diverse images. For many applications, it is useful to vary the scene type e.g.nature, street, and office scenes, distances e.g. landscape and closeup shots, degree ofclutter, etc. Many noncopyrighted images. For the LabelMe database most of the images were takenby the authors of this paper using a variety of handheld digital cameras. We also have3many video sequences taken with a headmounted web camera. Open and dynamic. The LabelMe database is designed to allow collected labels to beinstantly shared via the web and to grow over time.2.2 The LabelMe webbased annotation toolThe goal of the annotation tool is to provide a drawing interface that works on many platforms,is easy to use, and allows instant sharing of the collected data. To achieve this, we designed aJavascript drawing tool, as shown in Figure 1. When the user enters the page, an image is displayed. The image comes from a large image database covering a wide range of environmentsand several hundred object categories. The user may label a new object by clicking controlpoints along the objects boundary. The user finishes by clicking on the starting control point.Upon completion, a popup dialog bubble will appear querying for the object name. The userfreely types in the object name and presses enter to close the bubble. This label is recorded onthe LabelMe server and is displayed on the presented image. The label is immediately availablefor download and is viewable by subsequent users who visit the same image.The user is free to label as many objects depicted in the image as they choose. When they aresatisfied with the number of objects labeled in an image, they may proceed to label anotherimage from a desired set or press the Show Next Image button to see a randomly chosen image. Often, when a user enters the page, labels will already appear on the image. These arepreviously entered labels by other users. If there is a mistake in the labeling either the outlineor text label is not correct, the user may either edit the label by renaming the object or deleteand redraw along the objects boundary. Users may get credit for the objects that they labelby entering a username during their labeling session. This is recorded with the labels that theyprovide. The resulting labels are stored in the XML file format, which makes the annotationsportable and easy to extend.The annotation tool design choices emphasizes simplicity and ease of use. However, there aremany concerns with this annotation collection scheme. One important concern is quality control. Currently quality control is provided by the users themselves, as outlined above. Anotherissue is the complexity of the polygons provided by the users i.e. do users provide simple orcomplex polygon boundaries. Another issue is what to label. For example, should one labelthe entire body, just the head, or just the face of a pedestrian What if it is a crowd of peopleShould all of the people be labeled We leave these decisions up to each user. In this way, we4Figure 1. A screenshot of the labeling tool in use. The user is shown an image along withpossibly one or more existing annotations, which are drawn on the image. The user has theoption of annotating a new object by clicking along the boundary of the desired object andindicating its identity, or editing an existing annotation. The user may annotate as manyobjects in the image as they wish.5hope the annotations will reflect what various people think are natural ways of segmenting animage. Finally, there is the text label itself. For example, should the object be labeled as a person, pedestrian, or manwoman An obvious solution is to provide a dropdown menu ofstandard object category names. However, we prefer to let people use their own descriptionssince these may capture some nuances that will be useful in the future. In Section 3.1, we describe how to cope with the text label variability via WordNet 13. All of the above issues arerevisited, addressed, and quantified in the remaining sections.A Matlab toolbox has been developed to manipulate the dataset and view its contents. Examplefunctionalities that are implemented in the toolbox allow dataset queries, communication withthe online tool this communication can in fact allow one to only download desired parts of thedataset, image manipulations, and other dataset extensions see Section 3.The images and annotations are organized online into folders, with the folder names providinginformation about the image contents and location of the depicted scenesobjects. The foldersare grouped into two main categories static pictures and sequences extracted from video. Notethat the frames from the video sequences are treated as independent static pictures and thatensuring temporally consistent labeling of video sequences is beyond the scope of this paper.Most of the images have been taken by the authors using a variety of digital cameras. A smallproportion of the images are contributions from users of the database or come from the web.The annotations come from two different sources the LabelMe online annotation tool andannotation tools developed by other research groups. We indicate the sources of the images andannotations in the folder name and in the XML annotation files. For all statistical analyses thatappear in the remaining sections, we will specify which subset of the database subset was used.2.3 Content and evolution of the LabelMe databaseWe summarize the content of the LabelMe database as of December 21, 2006. The databaseconsists of 111490 polygons, with 44059 polygons annotated using the online tool and 67431polygons annotated offline. There are 11845 static pictures and 18524 sequence frames with atleast one object labeled.As outlined above, a LabelMe description corresponds to the raw string entered by the user todefine each object. Despite the lack of constraint on the descriptions, there is a large degree ofconsensus. Online labelers entered 2888 different descriptions for the 44059 polygons thereare a total of 4210 different descriptions when considering the entire dataset. Figure 2a shows6a sorted histogram of the number of instances of each object description for all 111490 polygons1. Notice that there are many object descriptions with a large number of instances. Whilethere is much agreement among the entered descriptions, object categories are nonetheless fragmented due to plurals, synonyms, and description resolution e.g. car, car occluded, andcar side all refer to the same category. In section 3.1 we will address the issue of unifyingthe terminology to properly index the dataset according to real object categories.Figure 2b shows a histogram of the number of annotated images as a function of the percentage of pixels labeled per image. The graph shows that 11571 pictures have less than 10of the pixels labeled and around 2690 pictures have more than 90 of labeled pixels. Thereare 4258 images with at least 50 of the pixels labeled. Figure 2c shows a histogram of thenumber of images as a function of the number of objects in the image. There are, on average,3.3 annotated objects per image over the entire dataset. There are 6876 images with at least5 objects annotated. Figure 3 shows images depicting a range of scene categories, with thelabeled objects colored to match the extent of the recorded polygon. For many images, a largenumber of objects are labeled, often spanning the entire image.The webtool allows the dataset to continuously grow over time. Figure 4 depicts the evolutionof the dataset since the annotation tool went online. We show the number of new polygons andtext descriptions entered as a function of time. For this analysis, we only consider the 44059polygons entered using the webbased tool. The number of new polygons increased steadilywhile the number of new descriptions grew at a slower rate. To make the latter observationmore explicit, we also show the probability of a new description appearing as a function oftime we analyze the raw text descriptions.2.4 Quality of the polygonal boundariesFigure 5 illustrates the range of variability in the quality of the polygons provided by differentusers for a few object categories. For the analysis in this section, we only use the 44059polygons provided online. For each object category, we sort the polygons according to thenumber of control points. Figure 5 shows polygons corresponding to the 25th, 50th, and 75thpercentile with respect to the range of control points clicked for each category. Many objects1A partial list of the most common descriptions for all 111490 polygons in the LabelMe dataset, with countsin parenthesis person walking 25330, car 6548, head 5599, tree 4909, window 3823, building 2516,sky 2403, chair 1499, road 1399, bookshelf 1338, trees 1260, sidewalk 1217, cabinet 1183, sign 964,keyboard 949, table 899, mountain 823, car occluded 804, door 741, tree trunk 718, desk 656.7100101102103100101102103104aDescription rankNumber of polygonsNumber of images1 2 3 4 5 6 7 8 9 10 11 12 13 14 150200040006000800010000120001400016000cNumber of imagesNumber of objects per image0 20 40 60 100bPercentage of pixels labeled02000400060008000100001200010 30 50 70 80 90Figure 2. Summary of the database content. a Sorted histogram of the number of instances of each object description. Notice that there is a large degree of consensus withrespect to the entered descriptions. b Histogram of the number of annotated images as afunction of the area labeled. The first bin shows that 11571 images have less than 10 ofthe pixels labeled. The last bin shows that there are 2690 pictures with more than 90 ofthe pixels labeled. c Histogram of the number of labeled objects per image.Figure 3. Examples of annotated scenes. These images have more than 80 of their pixelslabeled and span multiple scene categories. Notice that many different object classes arelabeled per image.8Aug 2005 May 2006 Jan 2007x 104 Dataset growthTimeCounts  Aug 2005 May 2006 Jan 200700.050.10.150.20.250.3TimeProbability of new description appearing How many new descriptions appearPolygonsDescriptions01234521Dec2006Figure 4. Evolution of the online annotation collection over time. Left total number of polygons blue and descriptions green in the LabelMe dataset as a function of time. Rightthe probability of a new description being entered into the dataset as a function of time.Note that the graph plots the evolution through March 23rd, 2007 but the analysis in thispaper corresponds to the state of the dataset as of December 21, 2006, as indicated bythe star. Notice that the dataset has steadily increased while the rate of new descriptionsentered has decreased.9Person7 12 21Dog16 28 52Bird13 37 168Chair7 10 15Streetlamp5 9 15House5 7 12Motorbike12 22 36Boat6 9 14Tree11 20 36Mug6 8 11Bottle7 8 11Car8 15 22Figure 5. Illustration of the quality of the annotations in the dataset. For each object weshow three polygons depicting annotations corresponding to the 25th, 50th, and 75th percentile of the number of control points recorded for the object category. Therefore, themiddle polygon corresponds to the average complexity of a segmented object class. Thenumber of points recorded for a particular polygon appears near the topleft corner of eachpolygon. Notice that, in many cases, the objects identity can be deduced from its silhouette, often using a small number of control points.can already be recognized from their silhouette using a small number of control points. Notethat objects can vary with respect to the number of control points to indicate its boundary. Forinstance, a computer monitor can be perfectly described, in most cases, with just four controlpoints. However, a detailed segmentation of a pedestrian might require 20 control points.Figure 6 shows some examples of cropped images containing a labeled object and the corresponding recorded polygon.2.5 Distributions of object location and sizeAt first, one would expect objects to be uniformly distributed with respect to size and imagelocation. For this to be true, the images should come from a photographer who randomly pointstheir camera and ignores the scene. However, most of the images in the LabelMe dataset weretaken by a human standing on the ground and pointing their camera towards interesting partsof a scene. This causes the location and size of the objects to not be uniformly distributed inthe images. Figure 7 depicts, for a few object categories, a density plot showing where in theimage each instance occurs and a histogram of object sizes, relative to the image size. Given10Paper cupRockStatueChairFigure 6. Image crops of labeled objects and their corresponding silhouette, as given bythe recorded polygonal annotation. Notice that, in many cases, the polygons closely followthe object boundary. Also, many diverse object categories are contained in the dataset.how most pictures were taken, many of the cars can be found in the lower half region of theimages. Note that for applications where it is important to have uniform prior distribitions ofobject locations and sizes, we suggest cropping and rescaling each image randomly.3 Extending the datasetWe have shown that the LabelMe dataset contains a large number of annotated images, withmany objects labeled per image. The objects are often carefully outlined using polygons insteadof bounding boxes. These properties allow us to extract from the dataset additional information that was not provided directly during the labeling process. In this section we providesome examples of interesting extensions of the dataset that can be achieved with minimal userintervention. Code for these applications is available as part of the Matlab toolbox.3.1 Enhancing object labels with WordNetSince the annotation tool does not restrict the text labels for describing an object or region, therecan be a large variance of terms that describe the same object category. For example, a usermay type any of the following to indicate the car object category car, cars, red car,11person0500car010002000tree01000building0500table0100200chair0200400road0500bookshelf0500sidewalk0100200mountain0200400keyboard0100window050010000.03 12 1001.70.20.03 12 1001.70.20.03 12 1001.70.20.03 12 1001.70.20.03 12 1001.70.20.03 12 1001.70.20.03 12 1001.70.20.03 12 1001.70.20.03 12 1001.70.20.03 12 1001.70.20.03 12 1001.70.20.03 12 1001.70.2Figure 7. Distributions of object location and size for a number of object categories in theLabelMe dataset. The distribution of locations are shown as a 2D histogram of the objectcentroid location in the different images coordinates are normalized with respect to theimage size. The size histogram illustrates what is the typical size that the object has in theLabelMe dataset. The horizontal axis is in logarithmic units and represents the percentageof the image area occupied by the object.12car frontal, automobile, suv, taxi, etc. This makes analysis and retrieval of the labeledobject categories more difficult since we have to know about synonyms and distinguish betweenobject identity and its attributes. A second related problem is the level of description providedby the users. Users tend to provide basiclevel labels for objects e.g. car, person, tree,pizza. While basiclevel labels are useful, we would also like to extend the annotations toincorporate superordinate categories, such as animal, vehicle, and furniture.We use WordNet 13, an electronic dictionary, to extend the LabelMe descriptions. WordNetorganizes semantic categories into a tree such that nodes appearing along a branch are ordered,with superordinate and subordinate categories appearing near the root and leaf nodes, respectively. The tree representation allows disambiguation of different senses of a word polysemyand relates different words with similar meanings synonyms. For each word, WordNet returns multiple possible senses, depending on the location of the word in the tree. For instance,the word mouse returns four senses in WordNet, two of which are computer mouse androdent2. This raises the problem of sense disambiguation. Given a LabelMe description andmultiple senses, we need to decide what the correct sense is.WordNet can be used to automatically select the appropriate sense that should be assigned toeach description 18. However, polysemy can prove challenging for automatic sense assignment. Polysemy can be resolved by analyzing the context i.e. which other objects are presentin the same image. To date, we have not found instances of polysemy in the LabelMe dataseti.e. each description maps to a single sense. However, we found that automatic sense assignment produced too many errors. To avoid this, we allow for offline manual intervention todecide which senses correspond to each description. Since there are fewer descriptions thanpolygons c.f. Figure 4, the manual sense disambiguation can be done in a few hours for theentire dataset.We extended the LabelMe annotations by manually creating associations between the differenttext descriptions and WordNet tree nodes. For each possible description, we queried WordNetto retrieve a set of senses, as described above. We then chose among the returned senses theone that best matched the description. Despite users entering text without any quality control,2The WordNet parents of these terms are i computer mouse electronic device device instrumentality, instrumentation artifact, artifact whole, unit object, physical object physical entity entity and ii rodent rodent,gnawer, gnawing animal placental, placental mammal, eutherian, eutherian mammal mammal, mammalian vertebrate, craniate chordate animal, animate being, beast, brute, creature, fauna organism, being living thing,animate thing object, physical object physical entity entity.13person 27719 polygons car 10137 polygonsLabel Polygon count Label Polygon countperson walking 25330 car 6548person 942 car occluded 804person standing 267 car rear 584person occluded 207 car side 514person sitting 120 car crop 442pedestrian 121 car frontal 169man 117 taxi 8woman 75 suv 4child 11 cab 3girl 9 automobile 2Table 1. Examples of LabelMe descriptions returned when querying for the objects personand car after extending the labels with WordNet not all of the descriptions are shown.For each description, the counts represents the number of returned objects that have thecorresponding description. Note that some of the descriptions do not contain the querywords.3916 out of the 4210 93 unique LabelMe descriptions found a WordNet mapping, whichcorresponds to 104740 out of the 111490 polygon descriptions. The cost of manually specifyingthe associations is negligible compared to the cost of entering the polygons and must be updatedperiodically to include the newest descriptions. Note that it may not be necessary to frequentlyupdate these associations since the rate of new descriptions entered into LabelMe decreasesover time c.f. Figure 4.We show the benefit of adding WordNet to LabelMe to unify the descriptions provided by thedifferent users. Table 1 shows examples of LabelMe descriptions that were returned whenquerying for person and car in the WordNetenhanced framework. Notice that many ofthe original descriptions did not contain the queried word. Figure 8 shows how the number ofpolygons returned by one query after extending the annotations with WordNet are distributedacross different LabelMe descriptions. It is interesting to observe that all of the queries seem tofollow a similar law linear in a loglog plot.Table 2 shows the number of returned labels for several object queries before and after applyingWordNet. In general, the number of returned labels increases after applying WordNet. For14100101102100101102103104  personcarplanttreebuildingtablechairroadbookshelfSynonym description rankCountsFigure 8. How the polygons returned by one query in the WordNetenhanced frameworkare distributed across different descriptions. The distributions seem to follow a similar lawa linear decay in a loglog plot with the number of polygons for each different descriptionon the vertical axis and the descriptions sorted by number of polygons on the horizontalaxis. Table 1 shows the actual descriptions for the queries person and car.many specific object categories this increase is small, indicating the consistency with whichthat label is used. For superordinate categories, the number of returned matches increasesdramatically. The object labels shown in Table 2 are representative of the most frequentlyoccurring labels in the dataset.One important benefit of including the WordNet hierarchy into LabelMe is that we can nowquery for objects at various levels of the WordNet tree. Figure 9 shows examples of queries forsuperordinate object categories. Very few of these examples were labeled with a descriptionthat matches the superordinate category, but nonetheless we can find them.While WordNet handles most ambiguities in the dataset, errors may still occur when queryingfor object categories. The main source of error arises when text descriptions get mapped to anincorrect tree node. While this is not very common, it can be easily remedied by changing thetext label to be more descriptive. This can also be used to clarify cases of polysemy, which oursystem does not yet account for.3.2 Objectparts hierarchiesWhen two polygons have a high degree of overlap, this provides evidence of either i an objectpart hierarchy or ii an occlusion. We investigate the former in this section and the latter in15Category Original description WordNet descriptionperson 27019 27719car 10087 10137tree 5997 7355chair 1572 2480building 2723 3573road 1687 2156bookshelf 1588 1763animal 44 887plant 339 8892food 11 277tool 0 90furniture 7 6957Table 2. Number of returned labels when querying the original descriptions entered into thelabeling tool and the WordNetenhanced descriptions. In general, the number of returnedlabels increases after applying WordNet. For entrylevel object categories this increase isrelatively small, indicating the consistency with which the corresponding description wasused. In contrast, the increase is quite large for superordinate object categories. These descriptions are representative of the most frequently occurring descriptions in the dataset.16Animalseagull squirrel bull horse elephantPlantflower cactus tree potted plant bushes palm treeFooddish with food orange mustard applepizzaTooltoolbox knife scissors corkscrewFigure 9. Queries for superordinate object categories after incorporating WordNet. Very fewof these examples were labeled with a description that matches the superordinate categorythe original LabelMe descriptions are shown below each image. Nonetheless, we are ableto retrieve these examples.17Section 3.3.We propose the following heuristic to discover semantically meaningful objectpart relationships. Let IO denote the set of images containing a query object e.g. car and IP  IO denotethe set of images containing part P e.g. wheel. Intuitively, for a label to be considered as apart, the labels polygons must consistently have a high degree of overlap with the polygonscorresponding to the object of interest when they appear together in the same image. Let theoverlap score between an object and part polygons be the ratio of the intersection area to thearea of the part polygon. Ratios exceeding a threshold of 0.5 get classified as having high overlap. Let IO,P  IP denote the images where object and part polygons have high overlap. Theobjectpart score for a candidate label is NO,PNP  where NO,P and NP are the number ofimages in IO,P and IP respectively and  is a concentration parameter, set to 5. We can think of as providing pseudocounts and allowing us to be robust to small sample sizes.The above heuristic provides a list of candidate part labels and scores indicating how wellthey cooccur with a given object label. In general, the scores give good candidate parts andcan easily be manually pruned for errors. Figure 10 shows examples of objects and proposedparts using the above heuristic. We can also take into account viewpoint information and findparts, as demonstrated for the car object category. Notice that the objectparts are semanticallymeaningful.Once we have discovered candidate parts for a set of objects, we can assign specific part instances to their corresponding object. We do this using the intersection overlap heuristic, asabove, and assign parts to objects where the intersection ratio exceeds the 0.5 threshold. Forsome robustness to occlusion, we compute a depth ordering of the polygons in the image seeSection 3.3 and assign the part to the polygon with smallest depth that exceeds the intersectionratio threshold. Figure 11 gives some quantitative results on the number of parts per object andthe probability with which a particular objectpart is labeled.3.3 Depth orderingFrequently, an image will contain many partially overlapping polygons. This situation ariseswhen users complete an occluded boundary or when labeling large regions containing smalloccluding objects. In these situations we need to know which polygon is on top in order toassign the image pixels to the correct object label. One solution is to request depth orderinginformation while an object is being labeled. Instead, we wish to reliably infer the relative18car sidewheeltirecar windowcar doorcar rearlicense platewheeltail light mirrorcar windowpersonheadfacehandnoseneck moutheyehairbuildingdoorwindowshopwindowbalconydouble  doorpatioawning text rmarqueepillarentrancepassageairconditionerskysuncloudmoon birdrainbowmountainsnowtreewaterfallfog bankFigure 10. Objects and their parts. Using polygon information alone, we automatically discover objectpart relationships. We show example parts for the building, person, mountain,sky, and car object classes, arranged as constellations, with the object appearing in thecenter of its parts. For the car object class, we also show parts when viewpoint is considered.190 0.05 0.1 0.15 0.2beachshrubroadcrosswalkhousechimneyhousestairwaywallpaintingmountaintreesofapillowbrushtrunksofacushionlaptopc r tbuildingdoorbuildingentrancehousedoorbuilding windowhousewindowPercentage occurrenceObjectPart0 5 10 15 20 25 30 35sofatablewindowplantheadsidewalktreemountainstreetskyroadpersonhousecarbuildingNumber of partsObjectsa bFigure 11. Quantitative results showing a how many parts an object has and b the likelihood that a particular part is labeled when an object is labeled. Note that there are 29objects with at least one discovered part only 15 are shown here. We are able to discovera number of objects having parts in the dataset. Also, a part will often be labeled when anobject is labeled.depth ordering and avoid user input.The problem of infering depth ordering for overlaping regions is a simpler problem than segmentation. In this case we only need to infer who owns the region of intersection. We summarize a set of simple rules to decide the relative ordering of two overlapping polygons Some objects are always on the bottom layer since they cannot occlude any objects. Forinstance, objects that do not own any boundaries e.g. sky and objects that are on thelowest layer e.g. sidewalk and road. An object that is completely contained in another one is on top. Otherwise, the objectwould be invisible and, therefore, not labeled. Exceptions to this rule are transparent orwiry objects. If two polygons overlap, the polygon that has more control points in the region of intersection is more likely to be on top. To test this rule we handlabeled 1000 overlappingpolygon pairs randomly drawn from the dataset. This rule produced only 25 errors, with31 polygon pairs having the same number of points within the region of intersection. We can also decide who owns the region of intersection by using image features. For20 Figure 12. Each image pair shows an example of two overlapping polygons and the finaldepthordered segmentation masks. Here, white and black regions indicate near and farlayers, respectively. A set of rules see text were used to automatically discover the depthordering of the overlapping polygon pairs. These rules provided correct assignments for97 of 1000 polygon pairs tested. The bottom right example shows an instance where theheuristic fails. The heuristic sometimes fails for wiry or transparent objects.instance, we can compute color histograms for each polygon and the region of intersection. Then, we can use histogram intersection 36 to assign the region of intersection tothe polygon with the closest color histogram. This strategy achieved 76 correct assignments over the 1000 handlabeled overlapping polygon pairs. We use this approach onlywhen the previous rule could not be applied i.e. both polygons have the same number ofcontrol points in the region of intersection.Combining these heuristics resulted in 29 total errors out of the 1000 overlapping polygonpairs. Figure 12 shows some examples of overlapping polygons and the final assignments.The example at the bottom right corresponds to an error. In cases in which objects are wiryor transparent, the rule might fail. Figure 13 shows the final layers for scenes with multipleoverlapping objects.3.4 Semiautomatic labelingOnce there are enough annotations of a particular object class, one could train an algorithm toassist with the labeling. The algorithm would detect and segment additional instances in newimages. Now, the user task would be to validate the detection 41. A successful instance ofthis idea is the Seville project 1 where an incremental, boostingbased detector was trained.They started by training a coarse detector that was good enough to simplify the collection of21 Figure 13. Decomposition of a scene into layers given the automatic depth ordering recovery of polygon pairs. Since we only resolve the ambiguity between overlapping polygonpairs, the resulting ordering may not correspond to the real depth ordering of all the objects in the scene.additional examples. The user provides feedback to the system by indicating when a boundingbox was a correct detection or a false alarm. Then, the detector was trained again with theenlarged dataset. This process was repeated until a satisfactory number of images were labeled.We can apply a similar procedure to LabelMe to train a coarse detector to be used to labelimages obtained from online image indexing tools. For instance, if we want more annotatedsamples of sailboats, we can query both LabelMe 18 segmented examples of sailboats werereturned and online image search engines e.g. Google, Flickr, and Altavista. The onlineimage search engines will return thousands of unlabeled images that are very likely to contain asailboat as a prominent object. We can use LabelMe to train a detector and then run the detectoron the retrieved unlabeled images. The user task will be to select the correct detections in orderto expand the amount of labeled data.Here, we propose a simple object detector. Although objects labeled with bounding boxes haveproven to be very useful in computer vision, we would like the output of the automatic objectdetection procedure to provide polygonal boundaries following the object outline wheneverpossible. Find candidate regions instead of running the standard sliding window, we propose cre22a Sailboats from the LabelMe dataset b Detection and segmentationFigure 14. Using LabelMe to automatically detect and segment objects depicted in imagesreturned from a web search. a Sailboats in the LabelMe dataset. These examples are usedto train a classifier. b Detection and segmentation of a sailboat in an image download fromthe web using Google. First, we segment the image upper left, which produces around10 segmented regions upper right. Then we create a list of candidate bounding boxes bycombining all of the adjacent regions. Note that we discard bounding boxes whose aspectratios lie outside the range of the LabelMe sailboat crops. Then we apply a classifier toeach bounding box. We depict the bounding boxes with the highest scores lower left,with the best scoring one colored in red. The candidate segmentation is the outline of theregions inside the selected bounding box lower right. After this process, a user may thenselect the correct detections to augment the dataset.23a Images returned from online search engines with the query sailboat   Images sorted after training with LabelMeb Images returned from online search engines with the query dog   Images sorted after training with LabelMe100 500 1000707580859095100707580859095100100 500 1000RankRankPrecisionPrecisiondetectorqueryquerydetectorFigure 15. Enhancing webbasd image retrieval using labeled image data. Each pair of rowsdepict sets of sorted images for a desired object category. The first row in the pair is theordering produced from an online image search using Google, Flickr and Altavista theresults of the three search engines are combined respecting the ranking of each image.The second row shows the images sorted according to the confidence score of the objectdetector trained with LabelMe. To better show how the performance decreases with rank,each row displays one out of every ten images. Notice that the trained classifier returnsbetter candidate images for the object class. This is quantified in the graphs on the right,which show the precision percentage correct as a function of image rank.24ating candidate bounding boxes for objects by first segmenting the image to produce1020 regions. Bounding boxes are proposed by creating all the bounding boxes that correspond to combinations of these regions. Only the combinations that produce contiguousregions are considered. We also remove all candidate bounding boxes with aspect ratiosoutside the range defined by the training set. This results in a small set of candidates foreach image around 30 candidates. Compute features resize each candidate region to a normalized size 96 96 pixels.Then, represent each candidate region with a set of features e.g. bag of words 28, edgefragments 26, multiscaleoriented filters 24. For the experiments presented here, weused the Gist features 24 code available online to represent each region. Perform classification train a support vector machine classifier 40 with a Gaussiankernel using the available LabelMe data and apply the classifier to each of the candidatebounding boxes extracted from each image. The output of the classifier will be a score forthe bounding boxes. We then choose the bounding box with the maximum score and thesegmentation corresponding to the segments that are inside the selected bounding box.For the experiments presented here, we queried four object categories sailboats, dogs, bottles,and motorbikes. Using LabelMe, we collected 18 sailboat, 41 dog, 154 bottle, and 49 motorbikeimages. We used these images to train four classifiers. Then, we downloaded 4000 images foreach class from the web using Google, Flickr and Altavista. Not all of the images containedinstances of the queried objects. It has been shown that image features can be used to improvethe quality of the ranking returned by online queries 14, 3. We used the detector trained withLabelMe to sort the images returned by the online query tools.Figure 15 shows the results and compares the images sorted according to the ranking given bythe output of the online search engines and the ranking provided by the score of the classifier.For each image we have two measures i the rank in which the image was returned and ii thescore of the classifier corresponding to the maximum score of all the candidate bounding boxesin the image. In order to measure performance, we provided ground truth for the first 1000 images downloaded from the web for sailboats and dogs. The precisionrecall graphs show thatthe score provided by the classifier provides a better measure of probability of presence of thequeried object than the ranking in which the images are returned by the online tools. However,for the automatic labeling application, good quality labeling demands very good performanceon the object localization task. For instance, in current object detection evaluations 9, an ob25Figure 16. Examples of automatically generated segmentations and bounding boxes forsailboats, motorbikes, bottles, and dogs.ject is considered correctly detected when the area of overlap between the ground truth bounding box and the detected bounding box is above 50 of the object size. However, this degreeof overlap will not be considered satisfactory for labeling. Correct labeling requires above 90overlap to be satisfactory.After running the detectors on the 4000 images of each class collected from the web, we wereable to select 162 sailboats, 64 dogs, 40 bottles, and 40 motorbikes that produced good annotations. This is shown in Figure 16. The user had the choice to validate the segmentation or justthe bounding box. The selection process is very efficient. Therefore, semiautomatic labelingmay offer an interesting way of efficiently labeling images.However, there are several drawbacks to this approach. First, we are interested in labeling fullscenes with many objects, making the selection process less efficient. Second, in order fordetection to work with a reasonable level of accuracy with current methods, the object needs tooccupy a large portion of the image or be salient. Third, the annotated objects will be biasedtoward being easy to segment or detected. Note that despite semiautomatic labeling not beingdesirable for creating challenging benchmarks for evaluating object recognition algorithms, itcan still be useful for training. There are also a number of applications that will benefit fromhaving access to large amounts of labeled data, including image indexing tools e.g. Flickr andphotorealistic computer graphics 32. Therefore, creating semiautomatic algorithms to assistimage labeling at the object level is an interesting area of application on its own.26Dataset  categories  images  annotations Annotation typeLabelMe 183 30369 111490 PolygonsCaltech101 12 101 8765 8765 PolygonsMSRC 45 23 591 1751 Region masksCBCLStreetscenes 5 9 3547 27666 PolygonsPascal2006 9 10 5304 5455 Bounding boxesTable 3. Summary of datasets used for object detection and recognition research. For theLabelMe dataset, we provide the number of object classes with at least 30 annotated examples. All the other numbers provide the total counts.4 Comparison with existing datasets for object detection andrecognitionWe compare the LabelMe dataset against four annotated datasets currently used for objectdetection and recognition Caltech101 12, MSRC 45, CBCLStreetscenes 5, and PASCAL2006 9. Table 3 summarizes these datasets. The Caltech101 and CBCLstreetscenesprovide location information for each object via polygonal boundaries. PASCAL2006 providesbounding boxes and MSRC provides segmentation masks.For the following analysis with the LabelMe dataset, we only include images that have at leastone object annotated and object classes with at least 30 annotated examples, resulting in atotal of 183 object categories. We have also excluded, for the analysis of the LabelMe dataset,contributed annotations and sequences.Figure 17a shows, for each dataset, the number of object categories and, on average, howmany objects appear in an image. Notice that currently the LabelMe dataset contains moreobject categories than the existing datasets. Also, observe that the CBCLStreetscenes andLabelMe datasets often have multiple annotations per image, indicating that the images correspond to scenes and contain multiple objects. This is in contrast with the other datasets, whichprominently feature a small number of objects per image.Figure 17b is a scatter plot where each point corresponds to an object category and showsthe number of instances of each category and the average size, relative to the image. Noticethat the LabelMe dataset has a large number of points, which are scattered across the entireplot while the other datasets have points clustered in a small region. This indicates the range27of the LabelMe dataset some object categories have a large number of examples close to10K examples and occupy a small percentage of the image size. Contrast this with the otherdatasets where there are not as many examples per category and the objects tend to occupya large portion of the image. Figure 17c shows the number of labeled instances per objectcategory for the five datasets, sorted in decreasing order by the number of labeled instances.Notice that the line corresponding to the LabelMe dataset is higher than the other datasets,indicating the breadth and depth of the dataset.We also wish to quantify the quality of the polygonal annotations. Figure 17d shows thenumber of polygonal annotations as a function of the number of control points. The LabelMedataset has a wide range of control points and the number of annotations with many controlpoints is large, indicating the quality of the dataset. The PASCAL2006 and MSRC datasetsare not included in this analysis since their annotations consist of bounding boxes and regionmasks, respectively.5 ConclusionWe described a webbased image annotation tool that was used to label the identity of objects and where they occur in images. We collected a large number of high quality annotations,spanning many different object categories, for a large set of images, many of which are high resolution. We presented quantitative results of the dataset contents showing the quality, breadth,and depth of the dataset. We showed how to enhance and improve the quality of the datasetthrough the application of WordNet, heuristics to recover object parts and depth ordering, andtraining of an object detector using the collected labels to increase the dataset size from imagesreturned by online search engines. We finally compared against other existing state of the artdatasets used for object detection and recognition.Our goal is not to provide a new benchmark for computer vision. The goal of the LabelMeproject is to provide a dynamic dataset that will lead to new research in the areas of objectrecognition and computer graphics, such as object recognition in context and photorealisticrendering.28Number of labeled instancesAverage percentage of image occupied1021031040.1110100Number of object categories0 50 100 150 20002468101214Number of objects per image CBCLStreetscenesPASCAL 06MSRCLabelMeCaltech101a b100101102101102103104101102100101102103104Object categoriesNumber of labeled instancesc dNumber of control pointsNumber of polygonsCBCLLabelMeCaltechPASCALMSRCFigure 17. Comparison of five datasets used for object detection and recognition Caltech101 10, MSRC 45, CBCLStreetscenes 38, PASCAL2006 9, and LabelMe. a Numberof object categories versus number of annotated objects per image. b Scatter plot ofnumber of object category instances versus average annotation size relative to the imagesize, with each point corresponding to an object category. c Number of labeled instancesper object category, sorted in decreasing order based on the number of labeled instances.Notice that the LabelMe dataset contains a large number of object categories, often withmany instances per category, and has annotations that vary in size and number per image.This is in contrast to datasets prominently featuring one object category per image, makingLabelMe a rich dataset and useful for tasks involving scene understanding. d Depiction ofannotation quality, where the number of polygonal annotations are plotted as a function ofthe number of control points we do not show the PASCAL2006 and MSRC datasets sincetheir annotations correspond to bounding boxes and region masks, respectively.296 AcknowledgementsFinancial support was provided by the National GeospatialIntelligence Agency, NEGI1582040004, and a grant from BAE Systems. Kevin Murphy was supported in part by a CanadianNSERC Discovery Grant.References1 Y. Abramson and Y. Freund. Semiautomatic visual learning seville a tutorial on active learning for visual object recognition. In Intl. Conf. on Computer Vision and Pattern RecognitionCVPR05, San Diego, 2005.2 Shivani Agarwal, Aatif Awan, and Dan Roth. Learning to detect objects in images via asparse, partbased representation. IEEE Trans. on Pattern Analysis and Machine Intelligence,261114751490, 2004.3 T. L. Berg and D. A. Forsyth. Animals on the web. In CVPR, volume 2, pages 14631470, 2006.4 I. Biederman. Recognition by components a theory of human image interpretation. Pyschologicalreview, 94115147, 1987.5 S. Bileschi. CBCL streetscenes. Technical report, MIT CBCL, 2006. The CBCLStreetscenesdataset can be downloaded at http  cbcl.mit.edusoftwaredatasets.6 J. Burianek, A. Ahmadyfard, and J. Kittler. Soil47, the Surrey object image library.httpwww.ee.surrey.ac.ukResearchVSSPdemoscoloursoil47.7 Owen Carmichael and Martial Hebert. Word Wiry object recognition database.www.cs.cmu.eduowencword.htm, January 2004. Carnegie Mellon University.8 M. Everingham, A. Zisserman, C. Williams, L. Van Gool, M. Allan, C. Bishop, O. Chapelle,N. Dalal, T. Deselaers, G. Dorko, S. Duffner, J. Eichhorn, J. Farquhar, M. Fritz, C. Garcia, T. Griffiths, F. Jurie, D. Keysers, M. Koskela, J. Laaksonen, D. Larlus, B. Leibe, H. Meng, H. Ney,B. Schiele, C. Schmid, E. Seemann, J. ShaweTaylor, A. Storkey, S. Szedmak, B. Triggs, I. Ulusoy, V. Viitaniemi, and J. Zhang. The 2005 pascal visual object classes challenge. In First PASCALChallenges Workshop. SpringerVerlag, 2005.9 M. Everingham, A. Zisserman, C.K.I. Williams, and L. Van Gool. The pascal visual object classeschallenge 2006 voc 2006 results. Technical report, September 2006. The PASCAL2006 datasetcan be downloaded at http  www.pascalnetwork.orgchallengesVOCvoc2006.10 L. FeiFei, R. Fergus, and P. Perona. A bayesian approach to unsupervised oneshot learning ofobject categories. In IEEE Intl. Conf. on Computer Vision, 2003.11 L. FeiFei, R. Fergus, and P. Perona. Learning generative visual models from few training exam30ples an incremental bayesian approach tested on 101 object categories. In IEEE. CVPR 2004,Workshop on GenerativeModel Based Vision, 2004.12 L. FeiFei, R. Fergus, and P. Perona. Oneshot learning of object categories. IEEE Trans. PatternRecognition and Machine Intelligence, In press. The Caltech 101 dataset can be downloaded athttp  www.vision.caltech.eduImage DatasetsCaltech101Caltech101.html.13 C. Fellbaum. Wordnet An Electronic Lexical Database. Bradford Books, 1998.14 R. Fergus, L. FeiFei, P. Perona, and A. Zisserman. Learning object categories from googlesimage search. In Proceedings of the 10th International Conference on Computer Vision, Beijing,China, volume 2, pages 18161823, October 2005.15 G. Griffin, A.D. Holub, and P. Perona. The Caltech256. Technical report, California Institute ofTechnology, 2006.16 B. Heisele, T. Serre, S. Mukherjee, and T. Poggio. Feature reduction and hierarchy of classifiersfor fast object detection in video images. In CVPR, 2001.17 D. Hoiem, A. Efros, and M. Hebert. Putting objects in perspective. In CVPR, 2006.18 N. Ide and J. Vronis. Introduction to the special issue on word sense disambiguation the state ofthe art. Computational Linguistics, 241140, 1998.19 Yann LeCun, FuJie Huang, and Leon Bottou. Learning methods for generic object recognitionwith invariance to pose and lighting. In Proceedings of CVPR04. IEEE Press, 2004.20 B. Leibe. Interleaved object categorization and segmentation. PhD thesis, 2005.21 B. Leibe, A. Leonardis, and B. Schiele. Combined object categorization and segmentation with animplicit shape model. In ECCV, 2004.22 B. Leibe and B. Schiele. Analyzing appearance and contour based methods for object categorization. In IEEE Conference on Computer Vision and Pattern Recognition CVPR03, Madison, WI,June 2003.23 Y. Li and L. G. Shapiro. Consistent line clusters for building recognition in cbir. In Proceedingsof the International Conference on Pattern Recognition, 2002.24 A. Oliva and A. Torralba. Modeling the shape of the scene a holistic representation of the spatialenvelope. Intl. J. Computer Vision, 423145175, 2001.25 A. Opelt, A. Pinz, M.Fussenegger, and P.Auer. Generic object recognition with boosting. IEEETransactions on Pattern Recognition and Machine Intelligence PAMI, 283, 2006.26 A. Opelt, A. Pinz, and A. Zisserman. A boundaryfragmentmodel for object detection. In ECCV,2006.27 P. Quelhas, F. Monay, J. M. Odobez, D. GaticaPerez, T. Tuytelaars, and L. Van Gool. Modelingscenes with local descriptors and latent aspects. In IEEE Intl. Conf. on Computer Vision, 2005.28 B. C. Russell, A. A. Efros, J. Sivic, W. T. Freeman, and A. Zisserman. Using multiple segmentations to discover objects and their extent in image collections. In CVPR, 2006.3129 B. C. Russell, A. Torralba, K. P. Murphy, and W. T. Freeman. Labelme a database and webbasedtool for image annotation. Technical Report AIM2005025, MIT AI Lab Memo, September,2005.30 Flickr Photo Sharing Service. httpwww.flickr.com.31 J. Sivic, B. C. Russell, A. A. Efros, A. Zisserman, and W. T. Freeman. Discovering objects andtheir location in images. In IEEE Intl. Conf. on Computer Vision, 2005.32 N. Snavely, S. M. Seitz, and R. Szeliski. Photo tourism Exploring photo collections in 3d. ACMTransactions on Graphics, 253137154, 2006.33 D. G. Stork. The open mind initiative. IEEE Intelligent Systems and Their Applications, 1431920, 1999.34 E. Sudderth, A. Torralba, W. T. Freeman, and W. Willsky. Describing visual scenes using transformed dirichlet processes. In Advances in Neural Info. Proc. Systems, 2005.35 E. Sudderth, A. Torralba, W. T. Freeman, and W. Willsky. Learning hierarchical models of scenes,objects, and parts. In IEEE Intl. Conf. on Computer Vision, 2005.36 M. J. Swain and D. H. Ballard. Color indexing. International Journal of Computer Vision, 71,1991.37 A. Torralba. Contextual priming for object detection. Intl. J. Computer Vision, 532153167,2003.38 A. Torralba, K. Murphy, and W. Freeman. Sharing features efficient boosting procedures formulticlass object detection. In CVPR, 2004.39 M. Turk and A. Pentland. Eigenfaces for recognition. J. of Cognitive Neuroscience, 317186,1991.40 V. Vapnik. The Nature of Statistical Learning Theory. SpringerVerlag, 1999.41 T. Vetter, M. Jones, and T. Poggio. A bootstrapping algorithm for learning linear models of objectclasses. In CVPR, 1997.42 P. Viola and M. Jones. Rapid object detection using a boosted cascade of simple classifiers. InCVPR, 2001.43 L. von Ahn and L. Dabbish. Labeling images with a computer game. In Proc. SIGCHI conferenceon Human factors in computing systems, 2004.44 L. von Ahn, R. Liu, and M. Blum. Peekaboom A game for locating objects in images. In In ACMCHI, 2006.45 J. Winn, A. Criminisi, and T. Minka. Object categorization by learned universal visual dictionary. In IEEE Intl. Conf. on Computer Vision, 2005. The MSRC dataset can be downloaded athttp  research.microsoft.comvisioncambridgerecognitiondefault.htm.32
