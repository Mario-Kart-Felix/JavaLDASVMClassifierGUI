The TREC Spoken Document Retrieval Track A Success StoryJohn S. Garofolo, Cedric G. P. Auzanne, Ellen M. VoorheesNational Institute of Standards and Technology100 Bureau Drive, Mail Stop 8940Gaithersburg, MD 208998940USAjohn.garofolo, cedric.auzanne, ellen.voorheesnist.govAbstractThis paper describes work within the NIST Text REtrieval Conference TREC over the last three years in designingand implementing evaluations of Spoken Document Retrieval SDR technology within a broadcast news domain.SDR involves the search and retrieval of excerpts from spoken audio recordings using a combination of automaticspeech recognition and information retrieval technologies. The TREC SDR Track has provided an infrastructure forthe development and evaluation of SDR technology and a common forum for the exchange of knowledge betweenthe speech recognition and information retrieval research communities. The SDR Track can be declared a success inthat it has provided objective, demonstrable proof that this technology can be successfully applied to realistic audiocollections using a combination of existing technologies and that it can be objectively evaluated.  The design andimplementation of each of the SDR evaluations are presented and the results are summarized.  Plans for the 2000TREC SDR Track are presented and thoughts about how the track might evolve are discussed. 1.0 TRECThe National Institute of Standards and Technology sponsors an annual Text REtrieval ConferenceTREC that is designed to encourage research on text retrieval for realistic applications by providinglarge test collections, uniform scoring procedures, and a forum for organizations interested in comparingresults Voorhees, et al., 2000. The conference, however, is only the tip of the iceberg.  TREC isprimarily an evaluationtaskdriven research program.  Each TREC research task culminates in a commonevaluation just prior to the conference.  The results of the evaluations are published by NIST in the TRECworkshop notebook and conference proceedings.  The sites participating in the evaluations meet at TRECto discuss their approaches and evaluation results and plan for future TREC research tasks.In recent years the conference has contained one main task and a set of additional tasks called tracks. Themain task investigates the performance of systems that search a static set of documents using newquestions. This task is similar to how a researcher might use a librarythe collection is known but thequestions likely to be asked are not known. The tracks focus research on problems related to the maintask, such as retrieving documents written in a variety of languages using questions in a single languagecrosslanguage retrieval, retrieving documents from very large 100GB document collections, andretrieval performance with humans in the loop interactive retrieval. Taken together, the tracks representthe majority of the research performed in the most recent TRECs, and they keep TREC a vibrant researchprogram by encouraging research in new areas of information retrieval. The three most recent TRECsTREC6  TREC8 have also included a Spoken Document Retrieval SDR track.2.0 Spoken Document RetrievalThe motivation for developing technology that can provide access to nontextual information is fairlyobvious.  Large multimedia collections are already being assembled.  The explosive growth of theInternet has enabled access to a wealth of textual information.  However, access to audio information, andspecifically spoken audio archives is pitifully limited to audio which has been manually indexed ortranscribed.  It is true that commerical humangenerated transcripts are now available for many radio andtelevision broadcasts, but a much greater body of spoken audio recordings untranscribed legacy radioand television broadcasts, recordings of meetings and conferences, classes and seminars, etc. remainsvirtually inaccessible.  The TREC Spoken Document Retrieval SDR track has been created to begin toaddress these problems.SDR provides contentbased retrieval of excerpts from archives of recordings of speech.  It was chosen asan area of interest for TREC because of its potential use in navigating large multimedia collections of thenear future and because it was believed that the component speech recognition and information retrievaltechnologies would work well enough for usable SDR in some domains.  SDR technology opens up thepossibility of access to large stores of previously unsearchable audio archives and paves the way for thedevelopment of access technologies to multimedia collections containing audio, video, image, and otherdata formats.  Voorhees et. al., 1997aIn practice, SDR is accomplished by using a combination of automatic speech recognition andinformation retrieval technologies. A speech recognizer is applied to an audio stream and generates atimemarked transcription of the speech. The transcription may be phone or wordbased in either a latticeprobability network, nbest list multiple individual transcriptions, or more typically, a 1best transcriptthe most probable transcription as determined by the recognizer.  The transcript is then indexed andsearched by a retrieval system. The result returned for a query is a list of temporal pointers to the audiostream ordered by decreasing similarity between the content of the speech being pointed to and the queryGarofolo et al., 1997b.  A typical SDR process is shown in Figure 1.RecognizedTranscriptsBroadcast NewsSpeech RecognitionEngineBroadcast NewsAudio RecordingCorpusIR SearchEngineRankedDocumentListTopicQueryTemporalIndexFigure 1 Typical SDR Process3.0 TREC SDR BackgroundIn 1996, an evaluation of retrieval using the output of an optical character recognizer OCR was run as aconfusion track in TREC5 to explore the effect of OCR errors on retrieval Kantor, et al., 2000.  Thistrack showed that it was possible to implement and evaluate retrieval on corrupted text.  Afterimplementing this track, NIST and members of the TREC community thought it would be interesting toimplement a similar experiment using automatic speech recognition ASR.During the 1996 TREC5 workshop, researchers from NIST and the TREC community led by KarenSp rck Jones from the University of Cambridge met to discuss the possibility of applying informationretrieval techniques to the output of speech recognizers.  While the NIST Natural Language Processingand Information Retrieval Group had been supporting the evaluation of retrieval technologies under theauspices of TREC, the NIST Spoken Natural Language Processing Group had been working with theDARPA automatic speech recognition ASR community in evaluating speech recognition technology onradio and television broadcast news.  The broadcast news evaluation task had accelerated progress in therecognition of real data and it seemed that the technology was producing transcripts with reasonableenough accuracy for investigation of downstream application uses such as SDR.  The DARPA ASRcommunity also had access to a 100hour corpus of broadcast news recordings collected by the LinguisticData Consortium LDC for ASR training Graff et al., 1996 that for the first time provided a datacollection which might be sufficiently large for SDR.The NIST Spoken Natural Language Processing Group and Natural Language Processing and InformationRetrieval Group joined forces to develop a plan for the creation of a research track within TREC toinvestigate the new hybrid technology. The primary goal of the track would be to bring the speech andinformation retrieval communities together to promote the development of SDR technologies and to trackprogress in their development. The track would also foster research on the development of largescale,nearrealtime, continuous speech recognition technology as well as on retrieval technology that is robustin the face of input errors. More importantly, the track would provide a venue for investigating hybridsystems that may be more effective than simple stovepipe combinations.  Thus, the track would alsoencourage cooperation and synergy between groups with complementary speech recognition andinformation retrieval expertise.4.0 TREC6 SDR Known Item Retrieval4.1 Evaluation DesignThe first year for the SDR Track was truly one of getting the speech and IR communities together andexploring the feasibility of implementing and evaluating SDR technology.  Toward that end, the TREC6SDR evaluation was designed for easy entry and straightforward implementation.  Since it would be thefirst common evaluation of SDR technology, the evaluation itself was also considered to be experimental.While the main TREC task was focussing on adhoc retrieval of multiple relevant documents from singletopics, we decided that the first SDR Track should employ a knownitem retrieval task which simulates auser seeking a particular, halfremembered document in a collection.  The goal in a knownitem retrievaltask is to generate a single correct document for each topic rather than a set of relevant topics as in an adhoc task.  This approach simplified the topic selection process and eliminated the need for expensiverelevance assessments.  It was also thought at the time that an SDR adhoc retrieval task might produceresults too poor to evaluate and would discourage participation Voorhees, et al., 1997a.Early on we decided that the evaluation should measure not only the endtoend effectiveness of SDRsystems, but the individual ASR and IR components as well.  To that end, the evaluation included severalcomplementary runs  all using the same set of topics, but with different sets of transcriptions of thebroadcast news recordings in the test collectionReference retrieval using perfect1 humantranscribed reference transcriptionsBaseline retrieval using given IBM ASRgenerated transcriptionsSpeech retrieval using the recordings themselves, requiring both ASR and IR componentsThe Reference run permitted the evaluation of the overall effectiveness of the retrieval algorithms on aspoken language collection while removing ASR as a factor.  Likewise, the Baseline condition permittedthe comparison of the effectiveness of retrieval algorithms on the same errorful ASRproducedtranscripts.  Finally, the Speech run permitted the evaluation of full endtoend SDR performance.The Reference transcripts which were contributed by the LDC were formatted in Hub4style UTF formatfiles  one for each broadcast Garofolo, et al., 1997a. The Baseline recognizer transcripts werecontributed by IBM Dharanipragada et al., 1998. The Baseline and shared recognized transcripts were                                                     1 Human transcripts are not actually perfect. Hub4 training quality transcripts are generally believed to contain 3  4 WER.stored in SGMLformatted files which included  story boundaries and a record for each word includingstart and end times. The broadcast recordings were digitally sampled 16bit samples, linearPCMencoded, 16KHz. sampling rate using a single monophonic channel and stored in NIST SPHEREformatted files.This componentized approach served two purposes  First, it allowed different ASR and IR sites to jointogether to create pipelined systems in which the components could be mixed, matched, and separatelyevaluated.  It also permitted retrieval sites without access to ASR systems to participate in a limited wayby implementing only the Reference and Baseline retrieval tasks.  The participation level for sitesimplementing both recognition and retrieval was deemed Full SDR and the participation level for sitesimplementing retrieval only was deemed QuasiSDR.  Although artificial, to simplify implementation andevaluation, sites would be given humanannotated story boundaries with story IDs for all test conditions.This permitted a simplified documentbased approach to implementation and evaluation.NIST developed 47 test topics  half designed by the NIST NLPIR Group to exercise classic IRchallenges.  The other half were designed by the SNLP Group to exercise challenges in the speechrecognition part of the problem.  Half of the speech topics were designed to target stories with easytorecognize speech scripted speech recorded in studio conditions with native speakers and no noise ormusic in the background.  The other half of the speech topics were designed to target stories withdifficulttorecognize speech unscripted speech, speech over telephone channels, nonnative speakers,and speech with noise or music in the background.  The variety of topics would permit us to examine inmore detail the effect of speech recognition accuracy on retrieval performance.We found several important differences between broadcast news stories and documentbased IRcollections.  First, the broadcast news stories were extremely short with regard to number of words.  TheTREC6 SDR collection had an average number of 276 words per story with most stories containing 100words or less.  Fulltext IR collections tend to have documents with many more words  usually an orderof magnitude larger. Further about 13 of the stories in the SDR collection were annotated as filler nontopical transitional material.  We filtered the collection to remove commercials, sports summaries,weather reports, and untranscribed stories.  However, we decided to leave the filler segments in the testcollection to keep it as large as possible.  The final filtered broadcast news collection had only 1,451stories.  Although the collection represented a sizable corpus for speech recognition previous test corporawere less than 3 hours, it was pitifully small for retrieval testing  at least 2 orders of magnitude smallerthan current IR test collections.The test specifications and documentation for the TREC6 SDR track are archived athttpwww.nist.govspeechsdr97.txt.4.2 Test ResultsThe test participants were given 3 months to complete the evaluation.  Thirteen sites or site combinationsparticipated in the first SDR Track.  Nine of these performed Full SDR  ATT, Carnegie MellonUniversity, Claritech with CMU ASR, ETH Zurich, Glasgow University with Sheffield UniversityASR, IBM, Royal Melbourne Institute of Technology, Sheffield University, and University ofMassachusetts with Dragon Systems ASR.  The remaining 4 sites performed Quasi SDR CityUniversity of London, Dublin City University, National Security Agency, and University of Maryland.See TREC6 SDR participant papersSince the goal of the track was to evaluate retrieval performance, there was no formal evaluation ofrecognition performance.  However, Full SDR sites were encouraged to submit their 1best transcripts sothat NIST could examine the relationship between recognition performance and retrieval accuracy. Theword error rate for the IBM Baseline recognizer was 50.0 Dharanipragada et al., 1998.  The meanstory word error rate was a bit lower at 40.  The mean story word error rate for the other measuredrecognizers fell between 35 and 40.  These error rates were substantially higher than those obtained inthe Hub4 ASR tests.  This difference was primarily due to three factors The transcriptions used forscoring SDR ASR performance were created as ASR training material and had not been put through therigorous verification that NIST employs for its Hub4 evaluation test data.  Likewise, a generic SCLITEorthographic mapping file was used.  The orthographic mapping file maps alternate representations ofcertain words and contractions to a common format prior to scoring.  A custom version of this file iscreated for each Hub4 test set to minimizes the number of alternative representation confusion errors.Finally, in order to process the 50hour collection, several sites chose to use faster, less accuraterecognizers than were used in the Hub4 tests.Initially, we believed that the retrieval results for the SDR Track would be quite poor.  Therefore, wedevised scoring metrics such as Mean Rank When Found and Mean Reciprocal Rank which gave systemspartial credit for finding target stories at lower ranks Voorhees, et al, 1997a. However, we were happilysurprised to find that the systems performed quite well.  So well, in fact, that we chose to use PercentRetrieved at Rank 1 as our primary metric Garofolo, et al, 1997b. Retrieval rates were very high for theReference transcript condition and most sites showed only a small degradation for retrieval using theirown recognizers.  There was generally higher degradation in retrieval using the Baseline recognizertranscripts due to its high error rate and high number of outofvocabulary OOV words.  The results ofthe evaluation for all three retrieval conditions are shown in Figure 2.Figure 2 TREC6 SDR Retrieval rate at rank 1 for all systems and modes best runFor Percent Retrieved at Rank 1, the best performance for all three test conditions was achieved by theUniversity of Massachusetts System with Dragon Systems recognition for Full SDR which obtained aretrieval rate of 78.7 for the Reference condition, 63.8 for the Baseline recognizer condition, and76.6 for the Speech condition Allan et al, 1997. In fact, the UMass system missed only one more topicon the Speech condition than it did on the Reference condition.An analysis of errors across systems for particular topics Figure 3 showed that, in general, the Easy toRecognize topic set yielded the best performance for all 3 evaluation conditions while the Difficult toRecognize topic set yielded substantially degraded performance.  However, the Difficult  Query topicsubset yielded even greater performance degradation.  It is interesting to note that systems also haddifficulty in retrieving stories for the Difficult to Recognize topic subset from the Referencetranscriptions  an indication that factors in transcribed speech other than recognition errors mightinfluence retrieval performance.  However, there was far too much variance from the topic effect to makeany sweeping conclusions.Figure 3  TREC6 SDR Percent Retrieval at Rank 1 averaged across systems by topic subsetTo further examine the effect of recognition error rate on retrieval, we examined performance using theBaseline recognizer results.  For each topic, we sorted the mean rank at which the retrieval systems foundthe target story against the word error rate for that story Figure 4.  The sorting appears to show anincreasing trend toward poorer retrieval performance as recognition errors increase.Figure 4  TREC6 Baseline condition mean retrieval rank sorted by Baseline Recognizer story word errorrateInterestingly, the same plot for retrieval for the Reference transcripts shows a similar trend Figure 5indicating that stories that are difficult to recognize may also be innately difficult to retrieve  even whenrecognized perfectly.  One hypothesis is that the complexity of the language within the more difficulttorecognize stories is greater than that of the more easytorecognize stories.Figure 5  TREC6 Reference condition mean retrieval rank sorted by Baseline Recognizer story worderror rateA statistical analysis of variance showed that we had too little data to eliminate a large proportion ofconfounding unexplained factors Garofolo, et al., 1997b. A future evaluation which would providemultiple recognizer transcript sets which all retrieval sites would run against would help to clarify therelationship between recognition and retrieval performance.4.3 ConclusionsThe first SDR evaluation showed us that we could successfully implement an evaluation of SDRtechnology and that existing component technologies worked well on a knownitem task with a smallaudio collection.  However, the test participants all agreed that the test collection would have to beenlarged by at least an order of magnitude before any real performance issues would surface.  It wasalso agreed that the knownitem task provided insufficient evaluation granularity.  For this evaluation, itseemed that retrieval performance played a much more significant role in overall SDR performance thanrecognition performance.  However, it was difficult to make any conclusions given the limited evaluationparadigm and collection.5.0 TREC7 SDR  Ad Hoc Retrieval5.1 Evaluation DesignIn 1998, for TREC7, we set out to address some of the inadequacies in the TREC6 SDR Track.  We stilldid not have access to a large enough audio collection for true retrieval evaluation, but we were able todouble the size of the SDR collection using an additional broadcast news corpus collected by the LDC forHub4 ASR training.  More importantly, though, we decided to give up the known item retrievalparadigm and implement a classic TREC adhoc retrieval task.In an ad hoc retrieval test, systems are posed with topics and attempt to return a list of documents rankedby decreasing similarity to the topic.  The documents are then evaluated for relevance by a team of humanassessors.  In TREC, to keep the evaluation tractable, NIST pools the top N documents output by all ofthe evaluated systems and judges only those documents.  Therefore, systems get evaluated over alldocuments, but only some documents are judged.  Although not exhaustive, this approach assumes thatwith enough different systems, all of the relevant documents will be included in the pool.  The traditionalTREC adhoc track provided several forms of information for each topic A title, a short query form usually a single sentence or phrase, and a descriptive narrative giving rules for judging relevance.  Giventhe limited size of the SDR collection, we decided to simplify the SDR topics to a single short form.  Wealso required that all runs had to be fully automatic.The TREC7 SDR test collection contained 87 hours of audio with 2,866 usable stories after filtering anda similar mean and median story length as compared to the TREC6 collection.  As in TREC6,participants were given humanannotated story boundaries and story IDs.  This removed storyboundarydetection from the technical challenge, but permitted NIST to use the standard TREC documentbasedTRECEVAL scoring software to evaluate the results of the test.  A team of 3 NIST TREC assessorscreated 23 test topics averaging 14.7 words in length for the collection.   The following are two of thetest topics they createdFind reports of fatal air crashes. Topic 62What economic developments have occurred in Hong Kong since its incorporation in the ChinesePeoples Republic Topic 63To more accurately examine the effect of recognition performance on retrieval, we decided to add a newoptional evaluation condition, Cross Recognizer Retrieval, in which retrieval systems would run on othersites recognized transcripts.  This would permit us to more tightly control for the recognizer effect in ouranalyses as well as provide us with more information regarding the relationship between recognizerperformance and retrieval performance.  We therefore encouraged all sites running 1best recognition tosubmit their recognizer transcripts to NIST for sharing with other participants. To permit sites to explorethe effect of using different recognizers, we permitted each Full SDR site to run retrieval on both aprimary S1 and secondary S2 recognizer.For the Baseline recognizer, NIST created a local instantiation of the Carnegie Mellon UniversitySPHINXIII recognizer. Since SPHINXIII ran in nearly 200 times real time on NISTs UNIXbasedworkstations, NIST realized that it would take nearly two years of computation to complete a singlerecognition pass over the 87hour collection.  NIST learned of inexpensive clusters of PCLINUXbasedsystems being used by NASA in its BEOWULF project BEOWULF, 1997 and set out to create aclusterbased recognition system.  The final system incorporated a scheduling server and 40computational nodes.  Given the clusters enormous computational power, to further enrich the spectrumof recognizers in the evaluation, NIST chose to create two Baseline recognizer transcript sets.  One setB1 was created using an optimal version of the SPHINX recognizer and benchmarked at 27.1 worderror rate on the Hub4 97 test set Pallett, et al., 1998 and at 33.8 on the SDR test collection.  Thisenabled us to for the first time benchmark the difference in performance for the same recognizer runningboth Hub4 and SDR ASR tests. A second set B2 was created using lowered pruning thresholds andbenchmarked at 46.6 word error rate for the SDR collection.As in TREC6, Full SDR sites were required to implement the Reference, Baseline, and Speech inputretrieval conditions and the Quasi SDR sites were required to implement only the Reference and Baselineretrieval conditions.The test specifications and documentation for the TREC7 SDR track are archived athttpwww.nist.govspeechsdr98sdr98.htm.5.2 Test ResultsThe TREC7 SDR participants were given 4 months to implement the recognition portion of the task.They were then given one month to implement the required retrieval tasks and an additional month toimplement the optional Cross Recognizer retrieval task.  The sites were not restricted in the hardware ornumber of processors they could apply in implementing the evaluation.Eleven sites or site combinations participated in the second SDR Track.  Eight of these performed FullSDR ATT ATT, Carnegie Mellon University Group 1 CMU1, University of Cambridge CUHTK,DERA DERA,  Royal Melbourne Institute of Technology MDS, Sheffield University SHEF, TheNetherlands Organization  TPD TUDelft TNO, and University of Massachusetts with DragonSystems ASR UMass.  The remaining 3 sites performed Quasi SDR  Carnegie Mellon UniversityGroup 2 CMU2, National Security Agency NSA, and the University of Maryland UMD. SeeTREC7 SDR participant papersIn addition to the two NIST Baseline recognizers, 1best transcripts for 6 additional recognizers weresubmitted to NIST for scoring and sharing in the Cross Recognizer retrieval condition. The recognizerscovered a wide range of error rates and provided a spectrum of material for the Cross Recognizer retrievalcondition.  Figure 6 shows the word error rate and mean story word error rate for each of the submittedrecognizer transcripts.SDR98 Traditional Recognition Metrics20304050607080cuhtks1dragon98s1atts1nistb1shefs1nistb2derasrus2derasrus1Word Error RateWord Error Rate WERMean Story WERFigure 6 TREC7 SDR Test set word error rate WER and mean story word error rate SWER forsubmitted recognized transcripts with crosssystem significance at 95 for SWERThe best recognition results were obtained by the University of Cambridge HTK recognition system witha 24.6 test set word error rate and a 22.2 mean story word error rate Johnson, et al., 1998. Thecircled mean story word error rate points were not considered to have statistically different performance.While the SDR ASR error rates were still significantly higher than Hub4, in general, error rates weresignificantly improved from the previous year  even at the faster speeds required to recognize the largertest collection.Each retrieval run was required to produce a rankordered list of the IDs for the top 1000 stories for eachtopic.  The top 100 IDs from each of these lists were then merged to create the pools for humanassessment.  The 3 TREC assessors read the reference transcriptions for each of the topic pool stories toevaluate the stories for relevance.  All of the retrieval runs were then scored using the standardTRECEVAL text retrieval scoring software.  As in other TREC ad hoc tasks, the primary retrievalmetric for the SDR evaluation was mean average precision MAP which is the mean of the averageprecision scores for each of the topics in the run.  The average precision is equivalent to the areaunderneath the uninterpolated recallprecision graph Voorhees, et al., 1998.In all, the TREC7 SDR Track contained 6 retrieval conditions Reference R1  retrieval using Human closedcaptionquality reference transcriptsBaseline1 B1 retrieval using NIST CMU SPHINX ASR transcriptsBaseline2 B2 retrieval using NIST CMU SPHINX suboptimal ASR transcriptsSpeech1 S1 retrieval using participants own recognizerSpeech2 S2 retrieval using participants own secondary recognizerCross Recognizer CR  retrieval using other participants recognizer transcriptsThe results for each of the required test conditions Reference R1, Baseline1 B1, Baseline2 B2,Speech1 S1 and Speech2 S2 are shown in Figure 7. Full SDR participants were required toimplement the R1, B1, B2, and S1 retrieval conditions. Quasi SDR participants were required toimplement the R1, B1, and B2 retrieval conditions.00.10.20.30.40.50.6r1 b1 b2 s1 s2Retrieval ConditionMAPattcmu1cmu2cuhtkderamdsnsasheftnoumassumdMean Average Precision by Retrieval ConditionFigure7 TREC7 SDR Mean Average Precision MAP for required retrieval conditionsFor all retrieval conditions except S2, the University of Massachusetts system Allan, et al., 1998achieved the best mean average precision.  Most systems performed surprisingly well for the recognizerbased conditions.  Even more surprising, ATTs S2 run the best recognizerbased run in the evaluationoutperformed its R1 run.  ATT attributed this excellent performance to a new approach theyimplemented for document expansion using contemporaneous newswire texts which they employed fortheir S1S2 runs but not for their R1 run Singhal, et al., 1998.The most interesting condition for TREC7 SDR was the cross recognizer retrieval CR condition inwhich participating systems ran retrieval on the 6 submitted recognizerproduced transcript sets inaddition to the human Reference and B1B2 recognizer transcript sets.  This experiment gave us 9recognitionretrieval data points to examine the effect of recognition performance on retrievalperformance.  Four sites University of Cambridge, DERA, Royal Melbourne Institute of TechnologyMDS, and Sheffield University participated in the CR experiment.  Using the mean story word errorrate SWER ASR metric and the mean average precision MAP retrieval metric, we plotted therecognitionretrieval performance curve for each of the four systems Figure 8.Retrieval Vs. Recognition Mean Story WER0.000.050.100.150.200.250.300.350.400.450.500.550 5 10 15 20 25 30 35 40 45 50 55 60 65SWERMean Average PrecisionCUHTKDERAMDSSHEFMean Corr Coef  .87REFCUHTKATTDragonShefB1 B2 DERA2DERA1Figure 8 TREC7 SDR Cross Recognizer results mean average precision vs. mean story word error rateThe figure shows a gentle, but fairly linear dropoff in MAP for recognition transcripts with increasingSWER.  We calculated the correlation coefficient for the metrics to determine how well SWER correlatedwith retrieval performance.  The average correlation coefficient for the 4 systems was .87  a significantcorrelation.We explored several other worderrorratebased metrics to see if we could find an even better predictorfor retrieval performance.  Our hypothesis was that such a metric would be useful in developing ASRsystems for retrieval purposes.  We explored metrics which used IR methods to filter out unimportantwords for retrieval stopwordfiltered word error rate and stemmed stopwordfiltered word error rateGarofolo, et al., 1998. Surprisingly, however, these metrics turned out to be only slightly morecorrelated with mean average precision than word error rate. Other effective approaches to IRcustomizedASR scoring using the TREC SDR data have been explored and reported by Johnson 1999 and Singhal1999.While we were implementing the TREC7 SDR track, we were also administering a first evaluation inNamed Entity NE tagging using broadcast news.  The NE evaluation involved identification of people,locations, and organizations in broadcast news ASR transcripts Przybocki, et al., 1999.  To our fortune,GTEBBN had handannotated the same data we used in the SDR evaluation with Named Entity tagsMiller, et al., 1999.  Our hypothesis was that these named entities would identify most of the keycontentcarrying words in our spoken documents and that if we focussed our ASR metric on these words,we would obtain a better predictor of retrieval performance than by measuring the error rate of all words.We rescored the ASR systems using the named entity word error rate and plotted the ASR metric againstthe mean average precision as we had done with mean story word error rate Figure 9.Retrieval Vs. RecognitionNamed Entity Mean Story WER0.000.050.100.150.200.250.300.350.400.450.500.550 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85NESWERMean Average PrecisionCUHTKDERAMDSSHEFMean Corr Coef  .91REFCUHTKDragon DERA1DERA2B2ShefB1ATTFigure 9 TREC7 SDR Cross Recognizer results mean average precision vs. named entity mean storyword error rateThe plot showed a nearly linear relationship between named entity ASR performance and retrievalperformance with a mean correlation coefficient of .91 across the systems.  Most significantly, the plotmore accurately positioned the problematic NIST B2 recognizer which had systematicallyincreasederrors in longer probably morecontentcarrying words.  For all the systems, the namedentitybasedmetric showed a higher correlation with mean average precision than word error rate alone Garofolo, etal., 1998. Other things being equal, this finding tells us that an ASR system which recognizes namedentities most accurately will provide the best input for retrieval.5.3 ConclusionsFor TREC7, we learned that we could successfully implement and evaluate an ad hoc SDR task.  Withthe new Cross Recognizer condition, we were able to begin to investigate the relationship betweenrecognition performance and retrieval performance.  We found a nearlinear relationship between worderror rate and mean average precision and we found that recognition contentwordbased word errormetrics such as named entity word error rate provided even better predictors of  retrieval performancethan word error rate alone.  Although twice the size of its predecessor in number of stories, our 87hourcollection was still too far too small to make conclusions about the usefulness of the technology.  Further,we were still evaluating systems using artificial humanannotated story boundaries.6.0 TREC8 SDR  Large Audio Collection6.1 Evaluation DesignIn 1998, the Linguistic Data Consortium began collecting a large radio and television corpus for theDARPA Topic Detection and Tracking TDT program.  In contrast to most TREC tracks2, the TDTprogram, is concerned with detecting and processing information from a continuous stream as it occurs inan online manner Fiscus, et al., 1999.  The TDT2 corpus, collected to support the TDT program in199899, contains news recordings from ABC, CNN, Public Radio International, and the Voice ofAmerica.  With the exception of the VOA broadcasts, which began in early March, these sources weresampled evenly over a 6month period between January and June 1998.  The corpus also contains acontemporaneous newswire corpus containing articles from the New York Times and Associated PressCieri, et al., 1999.With its timesampled broadcast news sources and parallel text corpus, the 600hour TDT2 corpus wasalso almost perfectly suited for use in the SDR Track.  Unfortunately, it had no highquality humanreference transcriptions  only closedcaption quality transcriptions. Since the transcription qualityprevented us from reasonably evaluating recognition performance over the entire collection, we selected a10hour randomlyselected story subset of the collection for detailed transcription by the LDC.  Thesehighquality transcripts would permit us to perform a sampled evaluation of the ASR performance.  Theyalso permitted us to evaluate the error rate in the closedcaptionquality transcriptions themselves whichwe found to have roughly 14.5 WER for television closedcaption sources and 7.5 WER for radiosources which had been quickly transcribed by commercial transcription services Fisher, 1999.  Theseerror rates are significant and the television closed caption error rates approach the error rates for stateoftheart broadcast news recognizers.Several SDR participants were also Hub4 participants and intended to use their Hub4 ASR systemswhich contained training data from January 1998 which overlapped with the first month of the TDT2corpus.  To eliminate the possibility of trainingtest crosscontamination, we eliminated the January datafrom the SDR collection.  The final collection contained 557 hours of audio collected between February                                                     2 The TREC Filtering track works on an online retrieval task similar to TDT.1, 1998 and June 30, 1998.  The collection contained 21,754 stories  an order of magnitude larger thanthe 87hour TREC7 SDR collection.3We believe that deployed SDR systems will operate in an archive search modality.  The most efficientmeans to implement such a system is to employ online recognition in which recognition is performed ona continuous basis as audio is recorded and retrospective retrieval in which the entire collection isqueried after it is formed.  This is in contrast to a TDTtype system which performs online retrieval as theaudio is recognized. In both modalities, recognition should use adaptation techniques to adjust to changesin the collection language over time. Traditional Hub4style broadcast news recognizers employed onlystatic pretrained language models.  If such a recognizer was used in a real timelongitudinal application,the language in the news and the fixed language model used in the recognizer would diverge, resulting inincreasing error rates over time.  Such recognizers are incapable of recognizing new words  words likelyto be important for retrieval. Conversely, given the computational expense of performing recognition,retrospective recognition at the time of retrieval is impossible for realistically large collections. So, in areal SDR application where audio would be recorded over many months or years, the recognizer wouldhave to be retrained periodically to accommodate changes in the language and new words.  To supportthis modality, we defined an online recognition mode which supported the use of evolving rollinglanguage models in which the recognition systems could be periodically retrained over the test epoch.Full SDR sites were permitted to use either a traditional pretrained recognition system or a continuouslyadaptive recognition system which used the contemporaneous newswire text from days prior to the daybeing recognized for adaptation.   Sites were free to choose whatever retraining period or strategy theyliked as long as they didnt look ahead in time as they performed recognition Garofolo, et al., 1999.Realizing that the CMU SPHINX recognizer was far too slow to recognize the TREC8 collection, NISTset out to find a faster baseline recognizer.  During 1998, NIST added a spoke to its Hub4 broadcastnews ASR evaluation in which systems had to run in 10 times real time or fast on a single processor.This spoke, dubbed 10Xrt, encouraged the development of fast broadcast news recognizers whichsuffered little degradation in recognition accuracy over their 150Xrt cousins Pallett, et al., 1999.GTEBBN offered NIST a LINUX instantiation of their fast BYBLOS Rough N Ready recognizerwhich now operated at 4Xrt to use as a baseline in the SDR and TDT tests Kubala, et al., 2000.  BBNalso gave NIST a basic language modeling toolkit to work with.  Given the computational power ofNISTs recognition cluster and the speed of the BBN recognizer, NIST set out to create 2 complementarybaseline recognizer transcript sets.  The first set B1 used a traditional Hub4 fixed language model.  TheB1 recognizer benchmarked at 24.7 WER on the Hub4 97 test set, 23.4 WER on the Hub4 98 testset, and 27.5 WER on the SDR99 10hour subset.  NIST then created an adaptive rolling languagemodel version B2 that used the SDR contemporaneous newswire texts for periodic lookback languagemodel training.  Details regarding the B2 recognizer are provided in Auzanne, et al. 2000.  The B2system benchmarked at 26.7 WER on 10hour SDR99 subset.  This difference in performance mightseem insignificant.  However, NIST statistical tests showed that it is significantly different than the B1recognizer.  Further, the small decrease in word error belies a more significant decrease in the outofvocabulary OOV rate of the recognizer.  The OOV rate is the percentage of test set words which are notincluded in the recognizers vocabulary and which, therefore, can never be correctly recognized.  TheOOV rate for the fixed B1 recognizer was 2.54.  The OOV rate for the adaptive B2 recognizer was1.97  a 22.4 relative improvement.In addition to the Reference, Baseline, Speech, and Cross Recognizer retrieval conditions used in TREC7, an optional story boundaries unknown SU condition was added for TREC8.  This condition                                                     3 The difference in story density is explained by the large proportion of short CNN stories in the TREC8 collection.  The averagestory length in the TREC8 collection is only 169 words.permitted sites to explore SDR where they had to operate on whole broadcasts with no knowledge ofhumanannotated topical boundaries.  This condition more accurately represented the real SDRapplication challenge.  A new adhoc paradigm had to be created to support the SU condition since it wasnot document based as in previous evaluations.  The natural unit for audio recordings is time rather thandocuments or words.  Therefore, it was decided that SU systems would output a ranked list of timepointers.  Given that the TDT program was already investigating technology for story segmentation, wedid not want to require SDR systems to find the topical boundaries in the audio recordings.  Rather, wedecided to require them to emit only a single time pointing to a hot spot or midpoint of a topicalsection.  This approach allowed us to map the emitted times to known stories and make use of ourtraditional document retrieval evaluation software.  Thus, this approach focussed on a new and interestingproblem while making use of the existing evaluation infrastructure and permitting some comparisonbetween runs where story boundaries were known and runs where they werent known.  To keep the taskclean, we required that Full SDR sites implementing the SU option would also be required to run theirrecognizers without knowledge of story boundaries.  However, to make maximal use of the recognizersfor the CR task, NIST devised a script to backfill the story boundaries into the SU ASR transcripts.The new SU condition did pose some challenges for scoring.  The biggest issue was how time pointerswhich mapped to the commercials, fillers, or the same stories should be treated.  NIST decided toimplement a mapping algorithm that would severely penalize the overgeneration of time pointers. Thepointers were first mapped to known story IDs.  Duplicate story IDs, commercials, and fillers were thenmapped to dummy IDs which would be automatically scored as nonrelevant.  The results were thenscored as usual with TRECEVAL. Since the story boundary known SK collection excludedcommercials and other untranscribed segments that were included in the SU collection, directcomparisons between the two conditions would not be possible.  However, this first SU evaluation wouldgive us an idea of how difficult a technical challenge the SU condition would pose.A team of 6 NIST assessors created the ad hoc topics for the evaluation.  The goal in creating TRECtopics is to devise topics with a few but not too many relevant documents in the collection toappropriately challenge retrieval systems.  Prior to coming together at NIST, the assessors were told toreview the news for the first half of 1998 and to come up with 10 possible topics each.  The assessors thentested their putative topics against the Reference transcripts in the TREC8 SDR collection using theNIST PRISE search engine.  If a topic was found to retrieve 1 to 20 documents in the top 25, it wasconsidered for inclusion in the test.  Otherwise, the assessors were required to refine broaden or narrowor replace the topic to retrieve an appropriate number of relevant documents using PRISE.  The assessorscreated approximately 60 topics.  Topics with similar subjects or which were considered malformed werethen excluded to yield the final test set containing 49 topics.The test specifications and documentation for the TREC8 SDR track are archived athttpwww.nist.govspeechsdr99sdr99.htm.6.2 Test ResultsThe TREC8 SDR participants were given approximately three and a half months to implement therecognition portion of the task and a month and a half to implement the required retrieval tasks.  In orderto give the participants the maximum possible amount of time to run recognition, the retrieval periodoverlapped the recognition period by one month.  After the sites recognized transcripts were submitted toNIST, they were checked, filtered, formatted and distributed for the Cross Recognizer retrieval condition.The retrieval sites were then given 3 weeks to perform the CR task.  Since NIST had limited time forassessment, only the preCR retrieval results were used to construct the pools for assessment, which tookplace in parallel with the CR test.  As in TREC7, the sites were not restricted in the hardware or numberof processors they could apply in implementing the evaluation.Ten sites or site combinations participated in the third SDR Track.  Six of these performed Full SDRATT ATT, Carnegie Mellon University CMU, University of Cambridge CUHTK, LIMSILIMSI,  Sheffield University SHEFFIELD, and Twenty One Consortium TNO. The remaining 4sites performed Quasi SDR The State University of NY at Buffalo CEDAR, IBM IBM, The RoyalMelbourne Institute of Technology MDS, and the University of Massachusetts UMASS. See theTREC8 participant publicationsIn all, the TREC8 SDR Track contained 11 retrieval conditionsReference R1 retrieval using Human closedcaptionquality reference transcriptsBaseline1 B1 retrieval using NIST BBN Byblos fixed language model ASR transcriptsBaseline2 B2 retrieval using NIST BBN Byblos adaptive language model ASR transcriptsSpeech1 S1 retrieval using sites own recognizerSpeech2 S2 retrieval using sites own secondary recognizerCross Recognizer CR retrieval using other sites recognizer transcriptsBaseline1 boundaries unknown B1UBaseline2 boundaries unknown B2USpeech1 boundaries unknown S1USpeech2 boundaries unknown S2UCross Recognizer boundaries unknown CRUFull SDR sites were required to run the R1, B1, and S1 retrieval conditions.  QuasiSDR sites wererequired to run only the R1 and B1 retrieval conditions.  The B2, CR and all story boundaries unknownconditions U were optional.We benchmarked the performance of the speech recognizer transcripts contributed by Full SDR sites forsharing in the Cross Recognizer condition using the 10hour Hub4style transcribed subset of the SDRcollection.  The summary results are shown in Figure 10.SDR 99  Speech Recognition Results based on 10hr. Hub4style transcribed subset010203040506070ReferenceCUHTKS1LIMSIS1CUHTKS1P1NISTB2NISTB1ATTS1ShefS1CMUS1 WERWERSW ERFigure 10  TREC8 SDR Speech Recognition Performance Results Test Set Word Error Rate and MeanStory Word Error Rate with crosssystem significance for Word Error RateThe word error rates were surprisingly low considering the enormous size of the test collection which wasover 2 orders of magnitude larger than test sets used in Hub4 ASR tests.  The graph shows the results forboth testset word error rate and mean story word error rate.  Most of the systems produced transcriptswith word error rates of less than 30.  This is fairly impressive considering the speed at which thesystems had to be run to process the large collection.  It is also interesting to note that these scores aregenerally lower than the comparable scores from TREC7 in which ASR systems were not run at suchfast speeds.  The best ASR results were obtained by the University of Cambridge HTK recognizer with a20.5 WER Johnson, et al., TREC8 1999.  With the exception of the alternative firstpassonlyCambridge System and the NIST B2 system, none of the recognizer transcripts were found to besignificantly similar in performance with respect to WER by the NIST statistical significance software.The figure also shows the results of scoring the original closedcaptionstyle Reference transcripts againstthe more scrupulously transcribed Hub4style transcripts.As with the speech recognition performance, overall retrieval performance was quite good.  As with allTREC ad hoc tests, there was quite a bit of variation in performance for particular topics.  The followingsample TREC8 SDR test topics illustrate the variationTopic 105 How and where is nuclear waste stored in New Mexico.85 average MAP across all systemsruns, 7 relevant stories.Topic 117 If we get more income, will we save more or spend more.34 average MAP across all systemsruns, 28 relevant storiesTopic 94 What percentage of the population is in prison in the U. S. A. and in the E. C. countries.01 average MAP across all systemsruns, 7 relevant storiesFigure 11 shows the results for each of the nonCrossRecognizer retrieval conditions.  The best resultsfor the Reference and Baseline1 recognizer retrieval conditions were obtained by the ATT system, witha MAP of .5598 and .5539 respectively Singhal, et al., TREC8 1999.  The best result for the Speechinput retrieval condition was obtained by the University of Cambridge system with a MAP of .5529Johnson, et al., TREC8 1999.  Sheffield University achieved the best performance for the Baseline andSpeech input story boundary unknown conditions with a MAP of .4301 and .4250 respectively Abberleyet al.,1999.SDR99  Withinsite retrieval results00.10.20.30.40.50.6R1 B1 B1U B2 B2U S SUTest ConditionMAPATTCedarCMUCUHTKIBMLIMSIMDS08SheffieldTNO8bUmassFigure 11 TREC8 SDR Mean Average Precision MAP for required and noncrossrecognizer retrievalconditionsThe individual test conditions were useful in contrasting the effect of binary variables such as humantranscripts vs. ASR transcripts and story boundaries known vs. story boundaries unknown.  However,even more interesting results are found in the CrossRecognizer retrieval conditions which containmultiple recognition performanceretrieval performance data points with which we can examine the effectof recognition performance on retrieval performance.Four sites participated in the story boundaries known CrossRecognizer CR retrieval condition ATT,University of Cambridge, LIMSI, and Sheffield University.  Each of these sites ran retrieval on the 8 setsof submitted recognizer transcripts.  Adding the retrieval results for the closedcaptionquality Referencetranscripts, this gives us 9 recognitionretrieval data points for each system.  Figure 12 shows a graph ofretrieval performance vs. recognition performance for the story boundaries known CrossRecognizerretrieval condition.  The CMU recognizer data point was removed since it was an extreme outlier. Thegraph shows that retrieval performance degrades very little for transcripts with increasing word error ratesand that retrieval is fairly robust to recognition errors.  Our hypothesis is that the redundancy of keywords in the spoken documents permits the relevant documents to be retrieved  even when a substantialnumber of words are misrecognized.  For TREC7, we assumed that this robustness was due to the smallcollection size and expected the recognitionretrieval performance dropoff to be much steeper for thelarger TREC8 collection.  However, this does not appear to be the case.  When we compare the averagecrosssystem slope for the recognitionretrieval performance curve for TREC7 and TREC8, we find thatthey are almost identical .0016 for TREC8 vs. .0014 for TREC7.  Although the individual systems haddifferent relative retrieval performance, all of the systems slopes appears to be relatively flat.  The ATTsystem achieved the best CR performance and also had the most shallow recognitionretrievalperformance slope Singhal, et al., TREC8 1999.SDR 99  Retrieval vs RecognitionSK condition00.10.20.30.40.50.60 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85WERMAPATT CUHTK LIMSI Sheffieldatt slope 0.0008cuhtk slope 0.0005limsi slope 0.0033sheffield slope 0.00181999 mean slope 0.00161998 mean slope 0.0014Figure 12  TREC8 SDR Story Boundaries Known Cross Recognizer Retrieval condition results showingMean Average Precision vs. Word Error RateThree sites participated in the story boundaries unknown Cross Recognizer CRU retrieval conditionUniversity of Cambridge, Sheffield University, and The Twenty One Consortium.  The results of theCRU condition are shown in Figure 13.SDR 99  Retrieval vs RecognitionSU condition00.10.20.30.40.50.60 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85WERMAPCUHTK Sheffield TNO8bcuhtk slope 0.0024sheffield slope 0.0017tno8b slope 0.0015mean slope 0.0018Figure 13  TREC8 SDR Story Boundaries Unknown Cross Recognizer Retrieval condition resultsshowing Mean Average Precision vs. Word Error RateAs in the story boundaries known CR condition, although the relative performance of the retrievalsystems differed, their recognitionretrieval performance slopes were relatively flat with an average crosssystem slope of .0018.  The University of Cambridge system achieved the best CRU performanceJohnson, et al., TREC8 1999.  The CRU retrieval scores are significantly lower than the comparableCR scores, which indicates that the unknown story boundaries pose greater difficulties for the retrievalsystems.  Part of this difficulty is explained by the difference in test data.  The story boundaries knownsystems used transcripts in which commercials, filler, and untranscribed segments were removed, whereasthe story boundaries unknown systems had to process the entire broadcasts.   It is even more difficult tocompare the results given the penalization for duplicates in scoring.6.3 ConclusionsThe recognition results for the TREC8 were extremely encouraging.  We saw recognition error rates falleven as recognition systems were made faster to tackle the large TREC8 collection.  The results for theretrieval systems were also quite good.  Given these factors, we can conclude that not only is thetechnology robust to larger spoken document collections, but that it has also improved significantly sinceTREC7.  We found that adaptive recognition systems can be used to more effectively recognize speechdata collected over time than comparable static systems.  The Cross Recognizer retrieval conditions withits multiple recognitionretrieval data points showed us that there is a nearlinear relationship betweenrecognition errors and retrieval accuracy and that the retrieval performance degradation slope forincreasing recognition errors is relatively gentle.  Finally, we found that SDR technology can be appliedto, and evaluated for conditions in which story boundaries are unknown.7.0 TREC9 SDR PlansAfter much discussion, the TREC SDR community has decided to stabilize the SDR track for theupcoming year with only a few minor changes.  The most significant of these is that the story boundariesunknown condition will be mandatory for all participants.  The same test collection will be used as in1999, but a new set of 50 test topics will be developed.  Since the story boundaries unknown conditioncan make effective use of audiosignal information not found in the transcriptions such as speakerchanges, noise changes, volume changes, music, prosody, etc., we will encourage the development of acommon nonlexical information exchange format which can be used to store and share such information.We will also encourage SDR participants this year to share this data in addition to their ASR transcriptsfor the cross recognizer retrieval condition.The test specifications and documentation for the TREC9 SDR track will be made available athttpwww.nist.govspeechsdr2000sdr2000.htm.8.0 TREC SDR Track Conclusions and FutureThe SDR Track has been an enormous success with regard to its primary goals of bringing the speechrecognition and information retrieval research communities together to explore the feasibility ofimplementing and evaluating retrieval from spoken audio recordings.  Certainly, we have shown that thetechnology can be implemented and evaluated for TREC known item and ad hoc tasks.  Weve also foundthat it can be implemented and evaluated for reasonably large audio collections and for conditions wherestory boundaries are unknown.  In fact, progress has occurred so quickly, that one might conclude thatSDR is a solved problem.  However, there is still much useful nonlexical information to be harnessedfrom the audio signal.  Further, while we have explored traditional text retrieval modalities usingautomatically transcribed speech, we havent yet tackled such challenging problems as questionanswering or spoken queries in which the misrecognition of a single word could cause catastrophicfailure of the technology.  In our traditional SDR task, the redundancy of words in the collection hasprotected us from truly facing these issues. Finally, there are still many more issues to explore andconquer with regard to the more general problem of multimedia information retrieval.There has been much discussion regarding the future of the TREC SDR Track and several suggestions forfuture evaluations revolving around an audioonly domain have been circulated including passageretrieval, multilingual or crosslingual SDR, SDR with question answering, interactive SDR, to name afew.  However, most of these problems are already being tackled on a textonly basis within TREC and,with the possible exception of question answering, the additional information to be learned from them foraudio collections might be somewhat limited.  We now have a fairly good idea of the kinds of problemsthat ASR introduces for text retrieval and we can most likely model the behavior of other text retrievaldomains using ASR without running fullblown evaluations.It seems to us that the next challenge is, rather, a broadening to a true multimedia information retrievalMMIR domain which will require not only text retrieval and speech recognition, but video and stillimage processing as well.4 Further, these multimedia sources will come in many different forms whichwill need to be integrated and threaded.  Such threading will no doubt require natural language processingand knowledge engineering.  This is an enormous problem and will require collaboration among manydifferent technology communities.  For SDR, we brought together two research communities.  MMIR willrequire the involvement of many more.  Taken at once, this task seems virtually impossible. So, it willmake sense to break it down into its constituent components or component combinations that can beincrementally integrated.  Accordingly, we believe that several binary or ternary technology developmentand evaluation projects should be undertaken to explore the more tractable lowerlevel challenges beforewe undertake full MMIR.  With this approach, core signal processing technologies such as speechrecognition, speaker identification, face and object identification, scene tracking, etc. can beincrementally integrated with higherlevel information processing technologies. Eventually, the capabilityto create robust multimedia information system technologies will emerge.For next year, NIST is interested in creating a retrieval track that would begin to explore the informationcontained in the video signal.  If a video corpus including audio is used, we can also begin to explore theintegration of speech recognition and video processing into retrieval applications.                                                     4 Actually, weve only scratched the surface of audio processing with speech recognition, since a great deal more informationthan words are encoded in the audio signal.These new domains and integrated technologies will, of course, require the development of newevaluation methods, formats, and tools.  This is perhaps one of the greatest challenges to overcome indeveloping a new technology research task.  For each of the research tasks that NIST has createdevaluation programs for, there has been significant and sometimes lengthy discussion and debateregarding the development of metrics and scoring protocols.  Metrics which are taken for granted today,such as mean average precision and word error rate, were once hotbeds of discussion.  Further, we willneed to build not only component technology measures, but endtoend system measures as multimediasystems technologies take shape.  The possibilities are quite exciting, but there is much work to be done.AcknowledgementsNIST work in the TREC SDR tracks was sponsored in part by the Defense Advanced Research ProjectsAgency DARPA.The authors would like to thank Karen Sp rck Jones at the University of Cambridge for her guidance inthe development of the SDR Track.  Wed like to thank Donna Harman and David Pallett at NIST fortheir support for the SDR track and Vince Stanford at NIST for his help in implementing the baselinespeech recognition systems.  Wed like to thank Sue Johnson at the University of Cambridge for her helpin refining the test specifications and evaluation protocols. Wed like to thank IBM for their contributionof the baseline speech recognizer transcripts for TREC6 SDR, Carnegie Mellon University for theircontribution of the SPHINXIII recognizer for TREC7 SDR, and a special thanks to GTEBBN for thecontribution and support of their LINUXbased BYBLOS Rough N Ready fast recognizer for use inTREC8 SDR.  Finally, wed like all the TREC SDR participants without whose participation this trackwould not have been such a success.DisclaimerAny mention of commercial products or reference to commercial organizations is for information only itdoes not imply recommendation or endorsement by the National Institute of Standards and Technologynor does it imply that the products mentioned are necessarily the best available for the purpose.Bibliographical ReferencesBEOWULF Project, NASA Center of Excellence in Space Data and Information Sciences,httpcesdis.gsfc.nasa.govlinuxbeowulf, reviewed in 1997.Cieri, C., Graff, D., Liberman, M., Martey, N., Strassel, S, TDT2 Text and Speech Corpus, Proc. 1999DARPA Broadcast News Workshop, March 1999.Fiscus, J.G., Doddington, G., Garofolo, J.S., NISTs 1998 Topic Detection and Tracking Evaluation, Proc.1999 DARPA Broadcast News Workshop, February 1999.Fisher, re investigation of TDT2 transcription error rates, personal conversation, 1999.Garofolo, J., Fiscus, J., and Fisher, W., Design and preparation of the 1996 Hub4 Broadcast NewsBenchmark Test Corpora, Proc. DARPA Speech Recognition Workshop, February 1997.Garofolo, J., Voorhees, E., Stanford, V., and Sp rck Jones, K., TREC6 1997 Spoken Document RetrievalTrack Overview and Results, Proc. TREC6, 1997 and 1998 DARPA Speech Recognition Workshop,February 1998.Garofolo, J. S., Voorhees, E. M., Auzanne, C.G.P. , Stanford, V.M., Lund, B.A., 1998 TREC7 SpokenDocument Retrieval Task Overview and Results, Proc. TREC7, Nov. 1998.Garofolo, John S., Auzanne, Cedric G. P., Voorhees, Ellen M., 1999 Trec8 Spoken Document RetrievalTrack Overview and Results, Proc. TREC8, Nov. 1999.  Due to time constraints, the referenced paperwas not created.  Instead, this RIAO 2000 paper was also used as the TREC8 SDR overview in theTREC8 ProceedingsGraff, D., Wu, Z., MacIntyre, R., and Liberman, M., The 1996 Broadcast News Speech and LanguageModel Corpus, Proc. DARPA Speech Recognition Workshop, February 1997.Johnson, S.E., Jourlin, P., Moore, G.L., Sp rck Jones, K., and Woodland, P.C., The Cambridge UniversitySpoken Document Retrieval System, Proc ICASSP 99, Vol. 1, pp 4952, March 1999Kantor, P., and Voorhees, E.M., The TREC5 Confusion Track Comparing Retrieval Methods forScanned Text, Information Retrieval, In press  2000.Kubala, F., Colbath, S., Liu, D., Srivastava, A., Makhoul, J. Integrated technologies for indexing spokenlanguage, Communications of the ACM, Volume 43, page 48, Feb. 2000.Miller, D., Schwartz, R., Weischedel, R., Stone, R., Named Entity Extraction from Broadcast News, Proc.1999 DARPA Broadcast News Workshop, March 1999.Pallett, D., Fiscus, J., and Przybocki, M., 1996 Preliminary Broadcast News Benchmark Tests, Proc.DARPA Speech Recognition Workshop, February 1997.Pallett, D.S., Fiscus, J.G., Martin, A., Przybocki, M.A., 1997 Broadcast News Benchmark Test ResultsEnglish and NonEnglish, Proc. DARPA Broadcast News Transcription and Understanding Workshop,February 1998.Pallett, D.S., Fiscus, J.G., Garofolo, J.S., Martin, A., Przybocki, M., 1998 Broadcast News BenchmarkTest Results English and NonEnglish Word Error Rate Performance Measures, Proc. DARPABroadcast News Workshop, February 1999.Przybocki, M.A., Fiscus, J.G., Garofolo, J.S., Pallett, D.S., 1998 Hub4 Information ExtractionEvaluation, Proc. 1999 DARPA Broadcast News Workshop, March 1999.Singhal, A., Pereira, F., Document Expansion for Speech Retrieval,  Proc. SIGIR 99, 1999.Voorhees, E., Garofolo, J., and Sp rck Jones, K., The TREC6 Spoken Document Retrieval Track, Proc.DARPA Speech Recognition Workshop, February 1997.Voorhees, E., Garofolo, J., and Sp rck Jones, K., The TREC6 Spoken Document Retrieval Track, TREC6 Notebook, Nov. 1997.Voorhees, E.M., Harman, D., Overview of the Seventh Text REtrieval Conference TREC7, Proc. TREC7, November 1998.Voorhees, E.M., Harman, D., Overview of the Sixth Text REtrieval Conference TREC6, InformationProcessing and Management, Vol. 36, No. 1, pp 335, January 2000.TREC6 SDR Participant Publications httptrec.nist.govpubstrec6t6proceedings.htmlAbberley, D., Renals, S., The THISL Spoken Document Retrieval System, University of Sheffield, UK, G.Cook, T. Robinson, Proc. TREC6, Nov. 1997.Allan, J., Callan, J., Croft, W.B., Ballesteros, L, Byrd, D., Swan, R., Xu, J., INQUERY Does Battle WithTREC6,  Proc. TREC6, Nov. 1997.Crestani, F., Sanderson, M., Theophylactou, M., Lalmas, M., Short Queries, Natural Language andSpoken Document Retrieval Experiments at Glasgow University, Proc. TREC6, Nov. 1997.Fuller, M., Kaszkiel, M., Ng, C.L., Vines, P., Wilkinson, R., Zobel, J. MDS TREC6 Report, Proc. TREC6, Nov. 1997.Mateev, B., Munteanu, E., Sheridan, P., Wechsler, M., Sch uble, P., ETH TREC6 Routing, Chinese,CrossLanguage and Spoken Document Retrieval, Proc. TREC6, Nov. 1997.Oard, D.W., Hackett, P., Document Translation for CrossLanguage Text Retrieval at the University ofMaryland, Proc. TREC6, Nov. 1997.Siegler, M.A., Slattery, S.T., Seymore, K., Jones, R.E., Hauptmann, A.G., Witbrock, M.J., Experiments inSpoken Document Retrieval at CMU,  Proc. TREC6, Nov. 1997.Singhal, A., Choi, J., Hindle, D., Pereira, F., ATT at TREC6 SDR Track, Proc. TREC6, Nov. 1997.Smeaton, A.F., Quinn. G., Kelledy, F., Ad hoc Retrieval Using Thresholds, WSTs for French Monolingual Retrieval, DocumentataGlance for High Precision and Triphone Windows for SpokenDocuments, Proc. TREC6, Nov. 1997.Walker, S., Robertson, S.E., Boughanem, M., Jones, G.J.F., Sp rck Jones, K., Okapi at TREC6Automatic ad hoc, VLC, routing, filtering and QSDR, Proc. TREC6, Nov. 1997.TREC7 SDR  Participant Publications httptrec.nist.govpubstrec7t7proceedings.htmlAbberley, D., Renals, S., Cook, G., Robinson, T., Retrieval Of Broadcast News Documents With theTHISL System , Proc. TREC7, Nov. 1998.Allan, J, Callan, J., Sanderson, Xu, J., INQUERY and TREC7, Proc. TREC7, Nov. 1998.Dharanipragada, S., Franz, M., Roukos, S., AudioIndexing For Broadcast News reference to TREC6SDR, Proc. TREC7, Nov. 1998.Ekkelenkamp, R., Kraaij, W., van Leeuwen, D., TNO TREC7 site report SDR and filtering, Proc. TREC7, Nov. 1998.Fuller,. M., Kaszkiel, M., Ng, C., Wu, M., Zobel, J., Kim, D., Robertson, J., Wilkinson, R., TREC 7 AdHoc, Speech, and Interactive tracks at MDSCSIRO, Proc. TREC7, Nov. 1998.Henderson, G.D., Schone, P., Crystal, T.H., Text Retrieval via Semantic Forests TREC7, Proc. TREC7,Nov. 1998.Johnson, S.E., Jourlin, P., Moore, G.L., Sp rck Jones, K., Woodland, P.C., Spoken Document Retrievalfor TREC7, Proc. TREC7, Nov. 1998.Nowell, P., Experiments in Spoken Document Retrieval at DERASRU, Proc. TREC7, Nov. 1998.Oard, D.W., TREC7 Experiments at the University of Maryland, Proc. TREC7, Nov. 1998.Siegler, M., Berger, A., Hauptmann, A., Witbrock, M., Experiments in Spoken Document Retrieval atCMU, Proc. TREC7, Nov. 1998.Singhal, A., Choi, J., Hindle, D., Lewis, D.D., Pereira, F., ATT at TREC7, Proc. TREC7, Nov. 1998.TREC8 SDR Participant Publications httptrec.nist.govpubstrec8t8proceedings.htmlAbberley, D., Ellis, D., Renals, S., Robinson, T., The THISL SDR System At TREC8, Proc. TREC8,Nov. 1999.Allan, J., Callan, J., Feng, FF., Malin, D., INQUERY and TREC8, Proc. TREC8, Nov. 1999.Franz, M., McCarley, J.S., Ward, R.T., Ad hoc, Crosslanguage and Spoken Document InformationRetrieval at IBM, Proc. TREC8, Nov. 1999.Fuller, M., Kaszkiel, M., Kimberley, S., Ng, C., Wilkinson, R., Wu, M., Zobel, J., The RMITCSIRO AdHoc, QA, Web, Interactive, and Speech Experiments at TREC 8, Proc. TREC8, Nov. 1999.Gauvain, JL., de Kercadio, Y., Lamel, L., Adda, G., The LIMSI SDR System for TREC8, Proc. TREC8,Nov. 1999.Han, B., Nagarajan, R., Srihari, R., Srikanth, M., TREC8 Experiments at SUNY Buffalo, Proc. TREC8,Nov. 1999.Kraaij, W., Pohlmann, R., Hiemstra, D., TwentyOne at TREC8 using Language Technology forInformation Retrieval, Proc. TREC8, Nov. 1999.S.E. Johnson, P. Jourlin, K. Spark Jones, P.C. Woodland, Spoken Document Retrieval for TREC8 atCambridge University, Proc. TREC8, Nov. 1999.Siegler, M., Jin, R., Hauptmann, A., CMU Spoken Document Retireval in TREC8 Analysis of the role ofTerm Frequency TF, Proc. TREC8, Nov. 1999.Singhal, A., Abney, S., Bacchiani, M., Collins, M., Hindle, D., Pereira, F., ATT at TREC8, Proc.TREC8, Nov. 1999.
