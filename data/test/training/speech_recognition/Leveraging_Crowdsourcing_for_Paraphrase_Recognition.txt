Proceedings of the 7th Linguistic Annotation Workshop  Interoperability with Discourse, pages 205213,Sofia, Bulgaria, August 89, 2013. c2013 Association for Computational LinguisticsLeveraging Crowdsourcing for Paraphrase RecognitionMartin TschirsichDepartment of Computer Science,TU Darmstadtm.tschirsichgmx.deGerold HintzDepartment of Computer Science,TU Darmstadtgerold.hintzgooglemail.comAbstractCrowdsourcing, while ideally reducingboth costs and the need for domain experts, is no allpurpose tool. We reviewhow paraphrase recognition has benefitedfrom crowdsourcing in the past and identify two problems in paraphrase acquisition and semantic similarity evaluationthat can be solved by employing a smartcrowdsourcing strategy. First, we employthe CrowdFlower platform to conduct anexperiment on subsentential paraphraseacquisition with early exclusion of lowaccuracy crowdworkers. Second, we compare two human intelligence task designsfor evaluating phrase pairs on a semanticsimilarity scale. While the first experimentconfirms our strategy successful at tackling the problem of missing gold in paraphrase generation, the results of the second experiment suggest that, for both semantic similarity evaluation on a continuous and a binary scale, querying crowdworkers for a semantic similarity value ona multigrade scale yields better resultsthan directly asking for a binary classification.1 IntroductionParaphrase recognition1 means to analysewhether two texts are paraphrastic, i.e. a pairof units of text deemed to be interchangeableDras, 1999. It has numerous applications ininformation retrieval, information extraction,machine translation and plagiarism detection.For instance, an internet search provider couldrecognize murder of the 35th U.S. presidentand assassination of John F. Kennedy to be1the terms paraphrase detection and paraphrase identification might be used insteadparaphrases of each other and thus yield the sameresult. Paraphrase recognition is an open researchproblem and, even though having progressedimmensely in recent years Socher et al., 2011,state of the art performance is still below thehuman reference.In this research, we analyse how crowdsourcingcan contribute to paraphrase recognition. Crowdsourcing is the process of outsourcing a vastnumber of small, simple tasks, so called HITs2, toa distributed group of unskilled workers, so calledcrowdworkers3. Reviewing current literature onthe topic, we identify two problems in paraphraseacquisition and semantic similarity evaluation thatcan be solved by employing a smart crowdsourcing strategy. First, we propose how to reduceparaphrase generation costs by early exclusion oflowaccuracy crowdworkers. Second, we comparetwo HIT designs for evaluating phrase pairs on acontinuous semantic similarity scale. In order toevaluate our crowdsourcing strategies, we conductour own experiments via the CROWDFLOWER4platform.The rest of the paper is structured as follows.Section 2 first gives an overview of related workand lines out current approaches. We then proceed to our own experiments on crowdsourcingparaphrase acquisition 3.3 and semantic similarity evaluation 3.4. Section 4 and 5 conclude thestudy and propose future work in the area of paraphrase recognition and crowdsourcing.2 Literature ReviewMany research fields rely on paraphrase recognition and contribute to it, as there are many relatedconcepts. These include inference rule discoveryfor questionanswering and information retrievalLin and Pantel, 2001, idiom or multiword ex2Human Intelligence Tasks3often referred to as turkers4httpcrowdflower.com205pression acquisition Fellbaum et al., 2006 andidentification Boukobza and Rappoport, 2009,machine translation evaluation Snover et al.,2009, textual entailment recognition, and manymore.2.1 Paraphrase DefinitionThe notion of a paraphrase is closely related to theconcepts of semantic similarity and word ontology and an exact definition is not trivial. Often,complex annotation guidelines and aggregated expert agreements decide whether phrases are to beconsidered paraphrastic or not Dolan and Brockett, 2005. Formal definitions based e.g. on a domain theory and derivable facts Burrows et al.,2013 have little practical relevance in paraphraserecognition. In terms of the semantic similarityrelations equals, restates, generalizes, specifies and intersects Marsi and Krahmer, 2010,paraphrase is equated with restates.It is important to note that in the context ofcrowdsourcing, we, as well as most authors, relyon the crowdworkers intuition of what a paraphrase is. Usually, only a limited list of examplesof desired valid paraphrases is given to the crowdworker as a reference.2.2 Paraphrase RecognitionAccording to Socher et al. 2011, paraphraserecognition determines whether two phrases ofarbitrary length and form capture the same meaning. Paraphrase recognition is mostly understood as a binary classification process, althoughrecently, some authors proposed a continuous semantic similarity measure Madnani et al., 2012.Competing paraphrase recognition approachesare often compared by their performance on theMicrosoft Research Paraphrase Corpus MSRPC.Until 2011, simple features such as ngram overlap, dependency tree overlap as well as dependency tree edit distance produced the best resultsin terms of accuracy and Fmeasure values. However, algorithms based solely on such features cannot identify semantic equivalence of synonymouswords or phrases. Therefore, some authors subsequently integrated Wordnet synonyms as wellas other corpusbased semantic similarity measures. The work of Madnani et al. 2012 basedon the TERP machine translation evaluation metric Snover et al., 2009 using synonyms and subsentential paraphrases presents the current state ofthe art for paraphrase detection on the MSRPCFigure 1 Highest ranking accuracy and Fmeasure over time for paraphrase recognition onthe MSRPC with an interrater agreement amongsthuman annotators of 84with an accuracy of 77.4 and Fmeasure of84.1. The interrater agreement amongst humanannotators of 84 on the MSRPC can be considered as an upper bound for the accuracy that couldbe obtained using automatic methods Fernandoand Stevenson, 2008.As has become apparent, modern paraphraserecognition algorithms are evaulated on and incorporate semantic similarity measures trained on acquired paraphrases. Therefore, we subsequentlygive an overview over established paraphrase acquisition approaches.2.3 Paraphrase AcquisitionParaphrase acquisition5 is the process of collectingor generating phraseparaphrase pairs, often for agiven set of phrases. All strategies require a subsequent verification of the acquired paraphrases,either done by experts or trusted crowdworkers.2.3.1 Sentential ParaphrasesMost literature on paraphrase acquisition dealswith sentential or sentencelevel paraphrases.Bouamor et al. 2012 identify five strategiessuch as the translation based methods Zhou etal., 2006 using parallel corpora or alignment oftopicclustered news articles Dolan and Brockett,2005.Via Crowdsourcing In an outstanding approach, Chen and Dolan 2011 collected paraphrases by asking crowdworkers to describe short5also referred to as paraphrase generation206videos. A more costeffective multistage crowdsourcing framework was presented by Negri et al.2012 with the goal to increase lexical divergenceof the collected paraphrases.2.3.2 SubSentential ParaphrasesIncorporating subsentential paraphrases in machine translation metrics also used for paraphrasedetection has proven effective Madnani et al.,2012. A large corpus consisting of more than15 million subsentential paraphrases was assembled by Bannard and CallisonBurch 2005 usinga pivotbased paraphrase acquisition method.Via Crowdsourcing Buzek et al. 2010 acquired paraphrases of sentence parts problematicfor translation systems using AMAZON MECHANICAL TURK. Bouamor et al. 2012 collected subsentential paraphrases in the context of a webbased game.2.3.3 Passagelevel paraphrasesPassagelevel paraphrase acquisition has beentreated within the context of the evaluation labon uncovering plagiarism, authorship, and socialsoftware misuse PAN Potthast et al., 2010Burrows et al. 2013 acquired passagelevelparaphrases for the WEBISCPC11 corpus viacrowdsourcing.2.4 Semantic Similarity EvaluationParaphrase verification can be said to be a manual semantic similarity evaluation done by expertsor trusted crowdworkers, most often on a binaryscale. However, Madnani et al. 2012 believethat binary indicators of semantic equivalenceare not ideal and a continuous value . . .  indicating the degree to which two pairs are paraphrastic is more suitable for most approaches.They propose averaging a large number of binary crowdworker judgements or, alternatively, asmaller number of judgements on an ordinal scaleas in the SEMEVAL2012 Semantic Textual Similarity STS task Agirre et al., 2012. A continuous semantic similarity score is also used to weighthe influence of subsentential paraphrases used bythe TERP metric.3 Our Experiments3.1 The CrowdFlower PlatformCROWDFLOWER is a web service for HITproviders, abstracting from the actual platform onwhich these tasks are run. A web interface, incorporating a graphical editor as well as the CROWDFLOWER MARKUP LANGUAGE6 CML, can beused to model these tasks. CROWDFLOWER provides finegrained controls over how these tasksare executed, for instance, by restricting crowdworkers to live in specific countries or by limitingthe number of HITs a single worker is allowed tocomplete.Furthermore, CROWDFLOWER provides a sophisticated system to verify the correctness of thecollected data, aiming at early detection and exclusion of spammers and lowaccuracy workersfrom the job gold items. Gold items consist of aHIT, e.g. a pair of paraphrases together with one ormore possible valid answers. Once gold items arepresent in the dataset, workers are prompted to answer these correctly before being eligible to workon the actual data. Additionally, during the run ofa job, CROWDFLOWER uses hidden gold items torevise the trustworthiness of a human worker.3.2 Human Intelligence Task DesignApart from gold items, the actual HIT design hasthe biggest impact on the quality of the collecteddata. Correct instructions as well as good examples have a great influence on data quality. By using CML validation features, bad user input can beprevented from being collected in the first place.Care must also be taken not to introduce an artificial bias by offering answer choices of differenttimecomplexity. Within our experiments, wefollowed common human interface design principles such as colour coding answer options.3.3 Crowdsourcing SubSententialParaphrase AcquisitionThe biggest challenge in paraphrase acquisitionvia crowdsourcing is the low and varying accuracy of the crowdworkers The challenge . . .  isautomatic quality assurance without such meansthe crowdsourcing paradigm is not effective, andwithout crowdsourcing the creation of test corpora is unacceptably expensive for realistic orderof magnitudes Burrows et al., 2013.We propose a new crowdsourcing strategy thatallows for early detection of lowaccuracy workers during the generation stage. This preventsthese unwanted crowdworkers from completing6CML documentation httpcrowdflower.comdocscml207HITs that would almost certainly not be validatedlater on. We focus on the acquisition of subsentential paraphrases for a given set of phrases,where pivotbased paraphrase acquisition methodsmight not be applicable. Transferring our observations to other types of paraphrases should be unproblematic.3.3.1 PhraseParaphrase GenerationFor this simple baseline strategy, we asked thecrowdworker to generate a short phrase along withits paraphrase p1, p2 while providing a small setof examples.3.3.2 TwoStaged Paraphrase GenerationThis is the traditional crowdsourcing strategy. Ina first generation stage, we presented the crowdworker with a phrase p1 and asked for its paraphrase p2. In a second validation stage, two orthree workers were asked to verify each generated phraseparaphrase pair until an unambiguous agreement was reached. As the answers inthe validation stage are binary, golditems wereadded to improve the accuracy of the collected validation judgements. Negri et al. 2012 showedthat after such a validation stage, expert ratersagreed in 92 of the cases with the aggregatedcrowdworker judgements. However, the generation stage is without gold and we cannot excludelow accuracy workers early enough not to costmoney. We used the regular expression verifierprovided by CROWDFLOWER to ensure that thegenerated paraphrases contain at least one wordand are not equal to the given phrases. Other thanthis however, the worker could enter any text.Input Phrases As input data, we required meaningful chunks. For this, any constituent of a sentence can be used. A small number of examplessuggested that verb phrases have a high potentialof yielding interesting paraphrases, as they oftenhave to be replaced as an isolated unit get a flu catch a cold. Therefore, we extracted verbphrases of two to five words from a source corpus. For this, we used the POS tagger of NLTK7A Maxent Treebank POS tagger trained on PennTreebank and a simple chunking grammar parser.Offering a Choice of Input Phrase A crowdworker might not always be able to come up with aparaphrase for a given phrase. If a worker receives7NATURAL LANGUAGE TOOLKIT NLTK httpnltk.orgphrasesverifyParaphrasesphrasesphrasesphrasesgenerategenerateParaphrasesphrasesgenerateverifyverifygenerategenerategenerateverifyFigure 2 Illustration of the multistage paraphrasegeneration processone chunk at a time, he has to deal with it no matter how unfeasible it is for paraphrasing. One solution to this problem would be to offer a backoutoption, in which a worker could declare a unit asunsolvable and possibly explain why. This however could easily be exploited by human workers,resulting in many unsolved items. An alternativesolution is to offer workers a choice of the inputphrase they want to paraphrase. We designed aHIT with a set of three different input phrases ofwhich they have to pick one to paraphrase. If oneof these options is repeatedly declined by multipleworkers, we can declare it as bad, without havinga worker pass on a unit. However, it turned out thatless than 1 proved unsolvable and we thereforedeemed such measures unnecessary.3.3.3 MultiStaged Paraphrase GenerationWe improved the traditional twostage approachby combining the generation and verificationsteps. The task to decide whether a given pair is aparaphrase is combined with the task of paraphrasing a chunk. The matching of verification andgeneration items is arbitrary. Figure 2 illustratesthis approach. After an initial generate stage, subsequent stages are combined verifygenerate jobs.The benefit of this approach is that verification of208phrase pairs allows the usage of golditems. Wecan now assess the trustworthiness of a crowdworker through gold, and we indirectly infer theirability to paraphrase from their ability to decide iftwo items are paraphrases. The aim of this processis to reduce the number of incorrect paraphrasesbeing generated in the first place, and thus improvethe efficiency of the CROWDFLOWER task.In contrast to Negri et al. 2012, we did not restrict access to the later stages of this job to highaccuracy workers of previous stages since our intermingled golditems are expected to filter outlowaccuracy workers in each succeeding stage.Therefore, we expect to attract contributors froma bigger pool of possibly cheaper workers.3.3.4 EvaluationWhile only 28 of the collected pairs were validated after the traditional twostaged paraphrasegeneration, this percentage increased to 80 inthe second validation stage belonging to the multistage approach. Although the experiment wasconducted on a small number of phrases, this result is a good indicator that our hypothesis is correct and that a combined generation and verification stage with gold items can reduce costs byearly exclusion of lowaccuracy workers.Lexical divergence measures TERP decline,but this is expected after filtering out possibly highly divergent nonparaphrastic pairs.While our generation costs per nonvalidated subsentential paraphrase were around the same asthose reported by Buzek et al. 2010 0.024, thecosts for validated subsentential paraphrases werenot much higher 0.06. Negri et al. 2012 reportcosts of 0.27 per sentential paraphrase, howeverthese costs are difficult to compare, also becausewe did not optimize for lexical divergence.3.4 Crowdsourcing Semantic SimilarityEvaluationWe conducted an experiment in order to determinehow to optimally query continuous semantic similarity scores from crowdworkers. The two different examined methods originally proposed byMadnani et al. 2012 are binary and senary8 semantic similarity evaluation. Paraphrases weretaken from the MSRPC. Optimality was definedby two different criteria First, we analysed howwell the binary paraphrase classification by domain experts on the MSRPC can be reproduced8senary 0, 1, 2, 3, 4, 5 as opposed to binary 0, 1.from our collected judgements. Second, we analysed how consistent our collected judgements are.Since we could not find any reference corpusfor semantic similarity evaluation apart from theSEMEVAL2012 STS gold that was also acquiredvia crowdsourcing, we resorted to training a machine learning classifier and comparing relativeperformance on the collected training data.3.4.1 Binary Semantic SimilarityCrowdworkers were asked to give a binary classification of two phrases as either paraphrastic ornonparaphrastic. Binary decisions were enforcedsince no third option was given. Three examplesof valid paraphrases were given.A minimum of 20 judgements each for 207phrase pairs were collected for 0.01 per judgement. In order to deter spammers and the most inaccurate workers, we converted 14 of the phrasepairs  those with high expected interrater agreement  to gold items. Low interrater agreementon a phrase pair hinted at medium, high interrateragreement hinted at low or high semantic similarity. Trusted crowdworkers had an average gold accuracy of 93 on these gold items.3.4.2 Senary Semantic SimilarityCrowdworkers were asked to give a senary classification of two phrases. The six classes wereequivalent to those defined by the SemEval STStask. A short annotation guide consisting of oneexample per category was provided.A minimum of 8 judgements each for 667phrase pairs were collected for 0.02 per judgement. In order to deter spammers and the most inaccurate workers, we converted 13 of the phrasepairs to gold items. Gold items were accepted aslong as the judgement lay within an acceptablerange of an expected similarity value.3.4.3 Input Aggregation and NormalizationThe following two phrase pairs demonstrate therelationship between binary interrater agreementand aggregated senary semantic similarity1. It appears that many employers accused ofworkplace discrimination will be consideredguilty until they can prove themselves innocent, he said.Employers accused of workplace discrimination now are considered guilty untilthey can prove themselves innocent.209Name Stage  Phrase Pairs TERPPhraseParaphrase Generation Generation 100 0.89TwoStaged Generation1. Generation 378 0.852. Validation 109 28 0.68MultiStaged Generation3. Generation  Gold 165 0.724. Validation 134 80 0.64Table 1 Twostaged 1.  2. and multistaged 1.  4. paraphrase generation results. Percentage valuesdenote the amount of validated pairs relative to the preceding generation stage.2. Sixteen days later, as superheated air fromthe shuttles reentry rushed into the damagedwing, there was no possibility for crewsurvival, the board said.Sixteen days later, as superheated airfrom the shuttles reentry rushed into thedamaged wing, there was no possibility forcrew survival, the board said.The binary interrater agreement for the firstphrase pair is low 10, so crowdworkers seemingly could not decide between paraphrastic andnonparaphrastic. Accordingly, the averagedsenary semantic similarity takes an intermediatevalue 3.4.The binary interrater agreement for the second phrase pair however is very high 100, sowe expect the sentences to be either clearly nonparaphrastic or clearly paraphrastic. A maximalaveraged senary semantic similarity value of 5.0confirms this intuition.In order to make aggregated binary and senaryinput comparable, we scaled the binary judgements so that the sampled average and variancematched that of the senary judgements. Thesesemantic similarities are strongly correlated 3awith Pearson coefficient of 0.81 and seem to respect the MSRPC expert annotator rating withpositive correlation between aggregated semanticsimilarity and binary MSRPC classification.With reference to Denkowski and Lavie 2010,we used the following aggregation and normalization techniquesStraight Average The aggregated semantic similarity is the average of all collected judgements. This is our baseline approach.Judge Normalization To compensate for different evaluation standards, each judges judgements are scaled so that its sample averageand variance matches that of the average 3b.Judge Outlier Removal Removing judgeswhose interrater agreement with the averageis less than 0.5 motivated by Agirre etal. 2012 Given the high quality of theannotations among the turkers, we couldalternatively use the correlation betweenthe turkers itself to detect poor qualityannotators.Weighted Voting Each judges judgements areweighted by its interrater agreement with theaverage.We also wanted to know whether limiting theamount of possible HITs or judgements percrowdworker could increase the quality of thecollected judgements. However, while highthroughput crowdworkers showed lower variancein their agreement compared to crowdworkerswith a small number of completed HITs, correlation between the number of completed HITs andagreement was very weak 3c with Pearson coefficient of 0.01.3.4.4 Machine Learning EvaluationWe trained the UKP machine learning classifier originally developed for the Semantic TextualSimilarity STS task at SemEval2012 Br et al.,2012 on the averaged binary and senary judgements for 207 identical phrase pairs. Since wewere not interested in the performance of the machine learning classifier but in the quality of thecollected data, we measured the relative performance of the learned model on the training data.The number of training examples remained constant. This was repeated multiple times whilevarying the number of judgements used in the aggregation of the semantic similarity values. Weobserved that with increasing number of judgements, the correlation coefficient converges seemingly against an upper bound binary 0.68 for 20judgements, senary 0.741 for 8 judgements. The2101 2 3 4 512345Senary Semantic SimilarityBinary Semantic Similaritya Correlation between aggregatedsenary and binary semantic similarity black  paraphrases according toMSRPC0 5 10 15012345Phrase PairJudgementJudgeAverageNormalizedb Judge normalization0 200 400 6000.200.20.40.60.811.2Number of HITsAgreementc The activity of a crowdworker doesnot correlate with agreement to the averageFigure 3 Input aggregation and normalizationmachine learning classifier performs best whentrained on semantic similarity data collected on asenary scale 4. Even if we only take the firstthree senary judgements per phrase pair into account, it is still superior to 20 binary judgementsalthough the total amount of information queriedfrom the crowdworkers is much smaller.In a second step, we compared the performance while employing different input normalization techniques on the whole set of 667 phrasepairs with senary judgements. While all techniques increased the trained classifiers performance, weighted voting performed best 2.0 5 10 15 200.550.60.650.70.75 Judgements 0 2 4 6 8AgreementBinarySenaryFigure 4 Machine learning results agreement correlation with training data3.4.5 MSRPC EvaluationIn addition to the machine learning evaluation, wecompared our results to the binary semantic similarity classification given by the MSRPC expertannotators. In order to do so, we had to findan optimal threshold in 0, 5 splitting our semantic similarity range in two, dividing paraphrasTechnique CorrelationStraight Average 0.716Judge Outlier Removal 0.719Judge Normalization 0.721Weighted Voting 0.722Table 2 Input normalization resultstic from nonparaphrastic phrase pairs. Again,this was repeated multiple times while varyingthe number of judgements used in the aggregation of the semantic similarity values. However,this time we did not simply take the first n judgements each, but averaged over different possiblesampling combinations. We measured percentageagreement with MSRPC and the optimal threshold for nonweighted and weighted judgements,since weighted voting performed best in the machine learning evaluation 5c.Surprisingly, even for binary paraphrasticnonparaphrastic classification, querying a senary semantic similarity value from crowdworkers yieldsbetter results than directly asking for a binary classification. However, the results also indicate thatin both cases, input normalization plays an important role and agreement could be improved bymore sophisticated or combined input normalization techniques as well as by collecting additionaljudgements.A semantic similarity of 3.1 senary 5a respectively 3.5 binary 5b corresponds optimally to the paraphrasticnonparaphrastic threshold chosen by the MSRPC expert annotators.Costs per evaluated phrase pair were at 0.162110 1 2 3 4 5020406080a Optimal threshold for senarysemantic similarity is 3.10 1 2 3 4 5050100150b Optimal threshold for binarysemantic similarity is 3.50 4 8 12 16 200.780.80.820.840.860.88 JudgementsAgreementBinarySenaryWeightedWeighted0 2 4 6 8 10c Average agreement with MSRPCwith optimal threshold per number ofjudgementsFigure 5 MSRPC evaluation agreement  percentual agreement with aggregated judgementssenary, 8 judgements compared to 0.20 for theSEMEVAL2012 STS task senary, 5 judgements.However, we did not examine how this and possible further cost reduction impacts agreement withMSRPC.4 ConclusionWe presented a multistage crowdsourcing approach tackling the problem of missing gold inparaphrase generation. This approach has shownto work very well for subsentential paraphrasegeneration and we strongly believe that it willwork equally well for sentential paraphrase generation, resulting in significantly reduced costs ofparaphrase corpus creation.We also compared different crowdsourcing approaches towards semantic similarity evaluation,showing that for both semantic similarity evaluation on a continuous and a binary scale, queryingan ordinal senary semantic similarity value fromcrowdworkers yields better results than directlyasking for a binary classification.5 Future WorkOur goal to subsentential paraphrase generationwas cost minimization by early removal of lowaccuracy workers. Apart from being grammaticaland paraphrastic, we did not enforce other quality constraints on the collected data. A combination of our multistage approach with that of Negri et al. 2012 could prove successful if bothcost and quality, i.e. lexical divergence betweenphraseparaphrase pairs, are to be optimized.There is also room for reducing the cost ofthe verification stage e.g. by automatically filtering out paraphrases before presenting them to acrowdworker using e.g. lexical divergence, lengthof the sentence or other measures as it was doneby Burrows et al. 2013.Another interesting question we could not answer due to budget constraints is Can the crowdreplace the expert and if yes, how many crowdworkers are needed to do so reliably One possible way to answer this question for paraphraseevaluation would be to collect semantic similarityjudgements for the whole MSRPC and to see howmany judgements per phrase are needed to reliablyreproduce the MSRPC classification results withan interrater agreement of 84 for the whole corpus.AcknowledgementsThe authors would like to thank Chris Biemannof TU Darmstadt, Germany, for pointing us to theproblem of paraphrase evaluation via crowdsourcing leading to this research as well as his supervision and helpful suggestions. We also thank ourreviewers for their feedback.ReferencesEneko Agirre, Daniel Cer, Mona Diab, and AitorGonzalezAgirre. 2012. Semeval2012 task 6 Apilot on semantic textual similarity. In SEM 2012The First Joint Conference on Lexical and Computational Semantics  Volume 1 Proceedings of themain conference and the shared task, and Volume 2Proceedings of the Sixth International Workshop onSemantic Evaluation SemEval 2012, pages 385393, Montral, Canada, 78 June. Association forComputational Linguistics.212Colin Bannard and Chris CallisonBurch. 2005. Paraphrasing with bilingual parallel corpora. In Proceedings of the 43rd Annual Meeting on Associationfor Computational Linguistics, ACL 05, pages 597604, Stroudsburg, PA, USA. Association for Computational Linguistics.Daniel Br, Chris Biemann, Iryna Gurevych, andTorsten Zesch. 2012. Ukp Computing semantic textual similarity by combining multiple contentsimilarity measures. In Proceedings of the 6th International Workshop on Semantic Evaluation, heldin conjunction with the 1st Joint Conference on Lexical and Computational Semantics, pages 435440,Montreal, Canada, Jun.Houda Bouamor, Aurlien Max, Gabriel Illouz, andAnne Vilnat. 2012. A contrastive review ofparaphrase acquisition techniques. In Proceedings of the Eight International Conference on Language Resources and Evaluation LREC12, Istanbul, Turkey, may.Ram Boukobza and Ari Rappoport. 2009. Multiwordexpression identification using sentence surface features. In Proceedings of the 2009 Conference onEmpirical Methods in Natural Language Processing, pages 468477, Singapore, August. Associationfor Computational Linguistics.Steven Burrows, Martin Potthast, and Benno Stein.2013. Paraphrase Acquisition via Crowdsourcingand Machine Learning. Transactions on IntelligentSystems and Technology ACM TIST to appear.Olivia Buzek, Philip Resnik, and Benjamin B. Bederson. 2010. Error driven paraphrase annotation usingmechanical turk. In Proceedings of the NAACL HLT2010 Workshop on Creating Speech and LanguageData with Amazons Mechanical Turk, CSLDAMT10, pages 217221, Stroudsburg, PA, USA. Association for Computational Linguistics.David L. Chen and William B. Dolan. 2011. Collecting highly parallel data for paraphrase evaluation. InProceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 190200, Portland, Oregon, USA, June.Michael Denkowski and Alon Lavie. 2010. Exploring normalization techniques for human judgments of machine translation adequacy collectedusing amazon mechanical turk. In Proceedingsof the NAACL HLT 2010 Workshop on CreatingSpeech and Language Data with Amazons Mechanical Turk, CSLDAMT 10, pages 5761, Stroudsburg, PA, USA. Association for Computational Linguistics.William B. Dolan and Chris Brockett. 2005. Automatically constructing a corpus of sentential paraphrases. In Third International Workshop on Paraphrasing IWP2005. Asia Federation of NaturalLanguage Processing.Mark Dras. 1999. Tree Adjoining Grammar and theReluctant Paraphrasing of Text. Ph.D. thesis, Macquarie University.Christiane Fellbaum, Alexander Geyken, Axel Herold,Fabian Koerner, and Gerald Neumann. 2006.Corpusbased Studies of German Idioms and LightVerbs. International Journal of Lexicography,194349360, December.Samuel Fernando and Mark Stevenson. 2008. A semantic similarity approach to paraphrase detection.In Proceedings of the 11th Annual Research Colloquium of the UK Special Interest Group for Computational Linguistics.Dekang Lin and Patrick Pantel. 2001. Discovery ofinference rules for questionanswering. Nat. Lang.Eng., 74343360, December.Nitin Madnani, Joel Tetreault, and Martin Chodorow.2012. Reexamining machine translation metricsfor paraphrase identification. In Proceedings of the2012 Conference of the North American Chapterof the Association for Computational LinguisticsHuman Language Technologies, NAACL HLT 12,pages 182190, Stroudsburg, PA, USA. Associationfor Computational Linguistics.Erwin Marsi and Emiel Krahmer. 2010. Automaticanalysis of semantic similarity in comparable textthrough syntactic tree matching. In Proceedingsof the 23rd International Conference on Computational Linguistics Coling 2010, pages 752760,Beijing, China, August. Coling 2010 OrganizingCommittee.Matteo Negri, Yashar Mehdad, Alessandro Marchetti,Danilo Giampiccolo, and Luisa Bentivogli. 2012.Chinese whispers Cooperative paraphrase acquisition. In Proceedings of the Eight InternationalConference on Language Resources and EvaluationLREC12, Istanbul, Turkey, may.Martin Potthast, Alberto BarrnCedeo, AndreasEiselt, Benno Stein, and Paolo Rosso. 2010.Overview of the 2nd international competition onplagiarism detection. Notebook Papers of CLEF, 10.Matthew G Snover, Nitin Madnani, Bonnie Dorr, andRichard Schwartz. 2009. Terplus Paraphrase, semantic, and alignment enhancements to translationedit rate. Machine Translation, 232117127.Richard Socher, Eric H. Huang, Jeffrey Pennington,Andrew Y. Ng, and Christopher D. Manning. 2011.Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection. In Advances inNeural Information Processing Systems 24.Liang Zhou, ChinYew Lin, and Eduard Hovy. 2006.Reevaluating machine translation results with paraphrase support. In Proceedings of the 2006 Conference on Empirical Methods in Natural LanguageProcessing, EMNLP 06, pages 7784, Stroudsburg,PA, USA. Association for Computational Linguistics.213
