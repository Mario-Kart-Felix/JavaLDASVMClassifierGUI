Proc. 2002 Conf. on Empirical Methods in Natural Language Processing EMNLP Thumbs up Sentiment Classification using Machine LearningTechniquesBo Pang and Lillian LeeDepartment of Computer ScienceCornell UniversityIthaca, NY 14853 USApabo,lleecs.cornell.eduShivakumar VaithyanathanIBM Almaden Research Center650 Harry Rd.San Jose, CA 95120 USAshivalmaden.ibm.comAbstractWe consider the problem of classifying documents not by topic, but by overall sentiment, e.g., determining whether a reviewis positive or negative. Using movie reviews as data, we find that standard machine learning techniques definitively outperform humanproduced baselines. However, the three machine learning methodswe employed Naive Bayes, maximum entropy classification, and support vector machines do not perform as well on sentimentclassification as on traditional topicbasedcategorization. We conclude by examiningfactors that make the sentiment classification problem more challenging.1 IntroductionToday, very large amounts of information are available in online documents. As part of the effort tobetter organize this information for users, researchershave been actively investigating the problem of automatic text categorization.The bulk of such work has focused on topical categorization, attempting to sort documents according to their subject matter e.g., sports vs. politics. However, recent years have seen rapid growthin online discussion groups and review sites e.g.,the New York Times Books web page where a crucial characteristic of the posted articles is their sentiment, or overall opinion towards the subject matter for example, whether a product review is positive or negative. Labeling these articles with theirsentiment would provide succinct summaries to readers indeed, these labels are part of the appeal andvalueadd of such sites as www.rottentomatoes.com,which both labels movie reviews that do not contain explicit rating indicators and normalizes thedifferent rating schemes that individual reviewersuse. Sentiment classification would also be helpful inbusiness intelligence applications e.g. MindfulEyesLexant system1 and recommender systems e.g.,Terveen et al. 1997, Tatemura 2000, where userinput and feedback could be quickly summarized indeed, in general, freeform survey responses given innatural language format could be processed usingsentiment categorization. Moreover, there are alsopotential applications to message filtering for example, one might be able to use sentiment informationto recognize and discard flamesSpertus, 1997.In this paper, we examine the effectiveness of applying machine learning techniques to the sentimentclassification problem. A challenging aspect of thisproblem that seems to distinguish it from traditionaltopicbased classification is that while topics are often identifiable by keywords alone, sentiment can beexpressed in a more subtle manner. For example, thesentence How could anyone sit through this moviecontains no single word that is obviously negative.See Section 7 for more examples. Thus, sentimentseems to require more understanding than the usualtopicbased classification. So, apart from presentingour results obtained via machine learning techniques,we also analyze the problem to gain a better understanding of how difficult it is.2 Previous WorkThis section briefly surveys previous work on nontopicbased text categorization.One area of research concentrates on classifyingdocuments according to their source or source style,with statisticallydetected stylistic variation Biber,1988 serving as an important cue. Examples include author, publisher e.g., the New York Times vs.The Daily News, nativelanguage background, andbrow e.g., highbrow vs. popular, or lowbrowMosteller and Wallace, 1984 ArgamonEngelson et1httpwww.mindfuleye.comaboutlexant.htmal., 1998 Tomokiyo and Jones, 2001 Kessler et al.,1997.Another, more related area of research is that ofdetermining the genre of texts subjective genres,such as editorial, are often one of the possiblecategories Karlgren and Cutting, 1994 Kessler etal., 1997 Finn et al., 2002. Other work explicitlyattempts to find features indicating that subjectivelanguage is being used Hatzivassiloglou and Wiebe,2000 Wiebe et al., 2001. But, while techniques forgenre categorization and subjectivity detection canhelp us recognize documents that express an opinion, they do not address our specific classificationtask of determining what that opinion actually is.Most previous research on sentimentbased classification has been at least partially knowledgebased.Some of this work focuses on classifying the semanticorientation of individual words or phrases, using linguistic heuristics or a preselected set of seed wordsHatzivassiloglou and McKeown, 1997 Turney andLittman, 2002. Past work on sentimentbased categorization of entire documents has often involvedeither the use of models inspired by cognitive linguistics Hearst, 1992 Sack, 1994 or the manual orsemimanual construction of discriminantword lexicons Huettner and Subasic, 2000 Das and Chen,2001 Tong, 2001. Interestingly, our baseline experiments, described in Section 4, show that humansmay not always have the best intuition for choosingdiscriminating words.Turneys 2002 work on classification of reviewsis perhaps the closest to ours.2 He applied a specific unsupervised learning technique based on themutual information between document phrases andthe words excellent and poor, where the mutual information is computed using statistics gathered by a search engine. In contrast, we utilize several completely priorknowledgefree supervised machine learning methods, with the goal of understanding the inherent difficulty of the task.3 The MovieReview DomainFor our experiments, we chose to work with moviereviews. This domain is experimentally convenientbecause there are large online collections of such reviews, and because reviewers often summarize theiroverall sentiment with a machineextractable rating indicator, such as a number of stars hence, wedid not need to handlabel the data for supervisedlearning or evaluation purposes. We also note thatTurney 2002 found movie reviews to be the most2Indeed, although our choice of title was completelyindependent of his, our selections were eerily similar.difficult of several domains for sentiment classification, reporting an accuracy of 65.83 on a 120document set randomchoice performance 50.But we stress that the machine learning methods andfeatures we use are not specific to movie reviews, andshould be easily applicable to other domains as longas sufficient training data exists.Our data source was the Internet Movie DatabaseIMDb archive of the rec.arts.movies.reviewsnewsgroup.3 We selected only reviews where the author rating was expressed either with stars or somenumerical value other conventions varied too widelyto allow for automatic processing. Ratings wereautomatically extracted and converted into one ofthree categories positive, negative, or neutral. Forthe work described in this paper, we concentratedonly on discriminating between positive and negative sentiment. To avoid domination of the corpusby a small number of prolific reviewers, we imposeda limit of fewer than 20 reviews per author per sentiment category, yielding a corpus of 752 negativeand 1301 positive reviews, with a total of 144 reviewers represented. This dataset will be availableonline at httpwww.cs.cornell.edupeoplepabomoviereviewdata the URL contains hyphens onlyaround the word review.4 A Closer Look At the ProblemIntuitions seem to differ as to the difficulty of the sentiment detection problem. An expert on using machine learning for text categorization predicted relatively low performance for automatic methods. Onthe other hand, it seems that distinguishing positivefrom negative reviews is relatively easy for humans,especially in comparison to the standard text categorization problem, where topics can be closely related.One might also suspect that there are certain wordspeople tend to use to express strong sentiments, sothat it might suffice to simply produce a list of suchwords by introspection and rely on them alone toclassify the texts.To test this latter hypothesis, we asked two graduate students in computer science to independentlychoose good indicator words for positive and negative sentiments in movie reviews. Their selections,shown in Figure 1, seem intuitively plausible. Wethen converted their responses into simple decisionprocedures that essentially count the number of theproposed positive and negative words in a given document. We applied these procedures to uniformlydistributed data, so that the randomchoice baselineresult would be 50. As shown in Figure 1, the3httpreviews.imdb.comReviewsProposed word lists Accuracy TiesHuman 1 positive dazzling, brilliant, phenomenal, excellent, fantastic 58 75negative suck, terrible, awful, unwatchable, hideousHuman 2 positive gripping, mesmerizing, riveting, spectacular, cool, 64 39awesome, thrilling, badass, excellent, moving, excitingnegative bad, cliched, sucks, boring, stupid, slowFigure 1 Baseline results for human word lists. Data 700 positive and 700 negative reviews.Proposed word lists Accuracy TiesHuman 3  stats positive love, wonderful, best, great, superb, still, beautiful 69 16negative bad, worst, stupid, waste, boring, , Figure 2 Results for baseline using introspection and simple statistics of the data including test data.accuracy  percentage of documents classified correctly  for the humanbased classifiers were 58and 64, respectively.4 Note that the tie rates percentage of documents where the two sentimentswere rated equally likely  are quite high5 we chosea tie breaking policy that maximized the accuracy ofthe baselines.While the tie rates suggest that the brevity ofthe humanproduced lists is a factor in the relativelypoor performance results, it is not the case that sizealone necessarily limits accuracy. Based on a verypreliminary examination of frequency counts in theentire corpus including test data plus introspection,we created a list of seven positive and seven negativewords including punctuation, shown in Figure 2.As that figure indicates, using these words raised theaccuracy to 69. Also, although this third list is ofcomparable length to the other two, it has a muchlower tie rate of 16. We further observe that someof the items in this third list, such as  or still,would probably not have been proposed as possiblecandidates merely through introspection, althoughupon reflection one sees their merit the questionmark tends to occur in sentences like What was thedirector thinking still appears in sentences likeStill, though, it was worth seeing.We conclude from these preliminary experimentsthat it is worthwhile to explore corpusbased techniques, rather than relying on prior intuitions, to select good indicator features and to perform sentimentclassification in general. These experiments also provide us with baselines for experimental comparisonin particular, the third baseline of 69 might actually be considered somewhat difficult to beat, sinceit was achieved by examination of the test data although our examination was rather cursory we do4Later experiments using these words as features formachine learning methods did not yield better results.5This is largely due to 00 ties.not claim that our list was the optimal set of fourteen words.5 Machine Learning MethodsOur aim in this work was to examine whether it suffices to treat sentiment classification simply as a special case of topicbased categorization with the twotopics being positive sentiment and negative sentiment, or whether special sentimentcategorizationmethods need to be developed. We experimentedwith three standard algorithms Naive Bayes classification, maximum entropy classification, and support vector machines. The philosophies behind thesethree algorithms are quite different, but each hasbeen shown to be effective in previous text categorization studies.To implement these machine learning algorithmson our document data, we used the following standard bagoffeatures framework. Let f1, . . . , fm bea predefined set of m features that can appear ina document examples include the word still orthe bigram really stinks. Let nid be the number of times fi occurs in document d. Then, eachdocument d is represented by the document vectord  n1d, n2d, . . . , nmd.5.1 Naive BayesOne approach to text classification is to assign to agiven document d the class c  argmaxc P c  d.We derive the Naive Bayes NB classifier by firstobserving that by Bayes rule,P c  d P cP d  cP d,where P d plays no role in selecting c. To estimatethe term P d  c, Naive Bayes decomposes it by assuming the fis are conditionally independent givends classPNBc  d P cmi1 P fi  cnidP d.Our training method consists of relativefrequencyestimation of P c and P fi  c, using addonesmoothing.Despite its simplicity and the fact that its conditional independence assumption clearly does nothold in realworld situations, Naive Bayesbased textcategorization still tends to perform surprisingly wellLewis, 1998 indeed, Domingos and Pazzani 1997show that Naive Bayes is optimal for certain problemclasses with highly dependent features. On the otherhand, more sophisticated algorithms might and often do yield better results we examine two suchalgorithms next.5.2 Maximum EntropyMaximum entropy classification MaxEnt, or ME,for short is an alternative technique which hasproven effective in a number of natural language processing applications Berger et al., 1996.Nigam et al. 1999 show that it sometimes, but notalways, outperforms Naive Bayes at standard textclassification. Its estimate of P c  d takes the following exponential formPMEc  d 1Zdexpii,cFi,cd, c,where Zd is a normalization function. Fi,c is a featureclass function for feature fi and class c, definedas follows6Fi,cd, c 1, nid  0 and c  c0 otherwise.For instance, a particular featureclass functionmight fire if and only if the bigram still hate appears and the documents sentiment is hypothesizedto be negative.7 Importantly, unlike Naive Bayes,MaxEnt makes no assumptions about the relationships between features, and so might potentially perform better when conditional independence assumptions are not met.The i,cs are featureweight parameters inspection of the definition of PME shows that a large i,cmeans that fi is considered a strong indicator for6We use a restricted definition of featureclass functions so that MaxEnt relies on the same sort of featureinformation as Naive Bayes.7The dependence on class is necessary for parameterinduction. See Nigam et al. 1999 for additional motivation.class c. The parameter values are set so as to maximize the entropy of the induced distribution hencethe classifiers name subject to the constraint thatthe expected values of the featureclass functionswith respect to the model are equal to their expectedvalues with respect to the training data the underlying philosophy is that we should choose the modelmaking the fewest assumptions about the data whilestill remaining consistent with it, which makes intuitive sense. We use ten iterations of the improvediterative scaling algorithm Della Pietra et al., 1997for parameter training this was a sufficient number of iterations for convergence of trainingdata accuracy, together with a Gaussian prior to preventoverfitting Chen and Rosenfeld, 2000.5.3 Support Vector MachinesSupport vector machines SVMs have been shown tobe highly effective at traditional text categorization,generally outperforming Naive Bayes Joachims,1998. They are largemargin, rather than probabilistic, classifiers, in contrast to Naive Bayes andMaxEnt. In the twocategory case, the basic idea behind the training procedure is to find a hyperplane,represented by vector w, that not only separatesthe document vectors in one class from those in theother, but for which the separation, or margin, is aslarge as possible. This search corresponds to a constrained optimization problem letting cj  1,1corresponding to positive and negative be the correct class of document dj , the solution can be writtenasw jjcj dj , j  0,where the j s are obtained by solving a dual optimization problem. Those dj such that j is greaterthan zero are called support vectors, since they arethe only document vectors contributing to w. Classification of test instances consists simply of determining which side of ws hyperplane they fall on.We used Joachims 1999 SVM light package8 fortraining and testing, with all parameters set to theirdefault values, after first lengthnormalizing the document vectors, as is standard neglecting to normalize generally hurt performance slightly.6 Evaluation6.1 Experimental SetupWe used documents from the moviereview corpusdescribed in Section 3. To create a data set with uniform class distribution studying the effect of skewed8httpsvmlight.joachims.orgFeatures  of frequency or NB ME SVMfeatures presence1 unigrams 16165 freq. 78.7 NA 72.82 unigrams  pres. 81.0 80.4 82.93 unigramsbigrams 32330 pres. 80.6 80.8 82.74 bigrams 16165 pres. 77.3 77.4 77.15 unigramsPOS 16695 pres. 81.5 80.4 81.96 adjectives 2633 pres. 77.0 77.7 75.17 top 2633 unigrams 2633 pres. 80.3 81.0 81.48 unigramsposition 22430 pres. 81.0 80.1 81.6Figure 3 Average threefold crossvalidation accuracies, in percent. Boldface best performance for a givensetting row. Recall that our baseline results ranged from 50 to 69.class distributions was out of the scope of this study,we randomly selected 700 positivesentiment and 700negativesentiment documents. We then divided thisdata into three equalsized folds, maintaining balanced class distributions in each fold. We did notuse a larger number of folds due to the slowness ofthe MaxEnt training procedure. All results reportedbelow, as well as the baseline results from Section 4,are the average threefold crossvalidation results onthis data of course, the baseline algorithms had noparameters to tune.To prepare the documents, we automatically removed the rating indicators and extracted the textual information from the original HTML document format, treating punctuation as separate lexical items. No stemming or stoplists were used.One unconventional step we took was to attemptto model the potentially important contextual effectof negation clearly good and not very good indicate opposite sentiment orientations. Adapting atechnique of Das and Chen 2001, we added the tagNOT to every word between a negation word not,isnt, didnt, etc. and the first punctuationmark following the negation word. Preliminary experiments indicate that removing the negation taghad a negligible, but on average slightly harmful, effect on performance.For this study, we focused on features based onunigrams with negation tagging and bigrams. Because training MaxEnt is expensive in the number offeatures, we limited consideration to 1 the 16165unigrams appearing at least four times in our 1400document corpus lower count cutoffs did not yieldsignificantly different results, and 2 the 16165 bigrams occurring most often in the same data theselected bigrams all occurred at least seven times.Note that we did not add negation tags to the bigrams, since we consider bigrams and ngrams ingeneral to be an orthogonal way to incorporate context.6.2 ResultsInitial unigram results The classification accuracies resulting from using only unigrams as features are shown in line 1 of Figure 3. As a whole,the machine learning algorithms clearly surpass therandomchoice baseline of 50. They also handily beat our two humanselectedunigram baselinesof 58 and 64, and, furthermore, perform well incomparison to the 69 baseline achieved via limitedaccess to the testdata statistics, although the improvement in the case of SVMs is not so large.On the other hand, in topicbased classification,all three classifiers have been reported to use bagofunigram features to achieve accuracies of 90and above for particular categories Joachims, 1998Nigam et al., 19999  and such results are for settings with more than two classes. This providessuggestive evidence that sentiment categorization ismore difficult than topic classification, which corresponds to the intuitions of the text categorization expert mentioned above.10 Nonetheless, we stillwanted to investigate ways to improve our sentiment categorization results these experiments arereported below.Feature frequency vs. presence Recall that werepresent each document d by a featurecount vectorn1d, . . . , nmd. However, the definition of the9Joachims 1998 used stemming and stoplists insome of their experiments, Nigam et al. 1999, like us,did not.10We could not perform the natural experiment of attempting topicbased categorization on our data becausethe only obvious topics would be the film being reviewedunfortunately, in our data, the maximum number of reviews per movie is 27, too small for meaningful results.MaxEnt featureclass functions Fi,c only reflects thepresence or absence of a feature, rather than directlyincorporating feature frequency. In order to investigate whether reliance on frequency information couldaccount for the higher accuracies of Naive Bayes andSVMs, we binarized the document vectors, settingnid to 1 if and only feature fi appears in d, andreran Naive Bayes and SVM light on these new vectors.11As can be seen from line 2 of Figure 3,better performance much better performance forSVMs is achieved by accounting only for feature presence, not feature frequency. Interestingly,this is in direct opposition to the observations ofMcCallum and Nigam 1998 with respect to NaiveBayes topic classification. We speculate that this indicates a difference between sentiment and topic categorization  perhaps due to topic being conveyedmostly by particular content words that tend to berepeated  but this remains to be verified. In anyevent, as a result of this finding, we did not incorporate frequency information into Naive Bayes andSVMs in any of the following experiments.Bigrams In addition to looking specifically fornegation words in the context of a word, we alsostudied the use of bigrams to capture more contextin general. Note that bigrams and unigrams aresurely not conditionally independent, meaning thatthe feature set they comprise violates Naive Bayesconditionalindependence assumptions on the otherhand, recall that this does not imply that NaiveBayes will necessarily do poorly Domingos and Pazzani, 1997.Line 3 of the results table shows that bigraminformation does not improve performance beyondthat of unigram presence, although adding in the bigrams does not seriously impact the results, even forNaive Bayes. This would not rule out the possibilitythat bigram presence is as equally useful a featureas unigram presence in fact, Pedersen 2001 foundthat bigrams alone can be effective features for wordsense disambiguation. However, comparing line 4to line 2 shows that relying just on bigrams causesaccuracy to decline by as much as 5.8 percentagepoints. Hence, if context is in fact important, as ourintuitions suggest, bigrams are not effective at capturing it in our setting.11Alternatively, we could have tried integrating frequency information into MaxEnt. However, featureclassfunctions are traditionally defined as binary Berger etal., 1996 hence, explicitly incorporating frequencieswould require different functions for each count or countbin, making training impractical. But cf. Nigam et al.,1999.Parts of speech We also experimented with appending POS tags to every word via Oliver MasonsQtag program.12 This serves as a crude form of wordsense disambiguation Wilks and Stevenson, 1998for example, it would distinguish the different usagesof love in I love this movie indicating sentimentorientation versus This is a love story neutralwith respect to sentiment. However, the effect ofthis information seems to be a wash as depicted inline 5 of Figure 3, the accuracy improves slightlyfor Naive Bayes but declines for SVMs, and the performance of MaxEnt is unchanged.Since adjectives have been a focus of previous workin sentiment detection Hatzivassiloglou and Wiebe,2000 Turney, 200213, we looked at the performanceof using adjectives alone. Intuitively, we might expect that adjectives carry a great deal of information regarding a documents sentiment indeed, thehumanproduced lists from Section 4 contain almostno other parts of speech. Yet, the results, shown inline 6 of Figure 3, are relatively poor the 2633adjectives provide less useful information than unigram presence. Indeed, line 7 shows that simplyusing the 2633 most frequent unigrams is a betterchoice, yielding performance comparable to that ofusing the presence of all 16165 line 2. This mayimply that applying explicit featureselection algorithms on unigrams could improve performance.Position An additional intuition we had was thatthe position of a word in the text might make a difference movie reviews, in particular, might beginwith an overall sentiment statement, proceed witha plot discussion, and conclude by summarizing theauthors views. As a rough approximation to determining this kind of structure, we tagged each wordaccording to whether it appeared in the first quarter, last quarter, or middle half of the document14.The results line 8 didnt differ greatly from usingunigrams alone, but more refined notions of positionmight be more successful.7 DiscussionThe results produced via machine learning techniques are quite good in comparison to the humangenerated baselines discussed in Section 4. In termsof relative performance, Naive Bayes tends to do theworst and SVMs tend to do the best, although the12httpwww.english.bham.ac.ukstaffoliversoftwaretaggerindex.htm13Turneys 2002 unsupervised algorithm uses bigrams containing an adjective or an adverb.14We tried a few other settings, e.g., first third vs. lastthird vs middle third, and found them to be less effective.differences arent very large.On the other hand, we were not able to achieve accuracies on the sentiment classification problem comparable to those reported for standard topicbasedcategorization, despite the several different types offeatures we tried. Unigram presence informationturned out to be the most effective in fact, none ofthe alternative features we employed provided consistently better performance once unigram presence wasincorporated. Interestingly, though, the superiorityof presence information in comparison to frequencyinformation in our setting contradicts previous observations made in topicclassification work McCallumand Nigam, 1998.What accounts for these two differences  difficulty and types of information proving useful between topic and sentiment classification, and howmight we improve the latter To answer these questions, we examined the data further. All examplesbelow are drawn from the full 2053document corpus.As it turns out, a common phenomenon in the documents was a kind of thwarted expectations narrative, where the author sets up a deliberate contrastto earlier discussion for example, This film shouldbe brilliant. It sounds like a great plot, the actors arefirst grade, and the supporting cast is good as well, andStallone is attempting to deliver a good performance.However, it cant hold up or I hate the Spice Girls....3 things the author hates about them... Why I sawthis movie is a really, really, really long story, but Idid, and one would think Id despise every minute ofit. But... Okay, Im really ashamed of it, but I enjoyedit. I mean, I admit its a really awful movie ...the ninthfloor of hell...The plot is such a mess that its terrible.But I loved it. 15In these examples, a human would easily detectthe true sentiment of the review, but bagoffeaturesclassifiers would presumably find these instances difficult, since there are many words indicative of theopposite sentiment to that of the entire review. Fundamentally, it seems that some form of discourseanalysis is necessary using more sophisticated tech15This phenomenon is related to another commontheme, that of a good actor trapped in a bad movieAN AMERICAN WEREWOLF IN PARIS is a failed attempt... Julie Delpy is far too good for this movie. She imbues Serafine with spirit, spunk, and humanity. This isntnecessarily a good thing, since it prevents us from relaxing and enjoying AN AMERICAN WEREWOLF IN PARISas a completely mindless, campy entertainment experience.Delpys injection of class into an otherwise classless production raises the specter of what this film could have beenwith a better script and a better cast ... She was radiant,charismatic, and effective ....niques than our positional feature mentioned above,or at least some way of determining the focus of eachsentence, so that one can decide when the author istalking about the film itself. Turney 2002 makesa similar point, noting that for reviews, the wholeis not necessarily the sum of the parts. Furthermore, it seems likely that this thwartedexpectationsrhetorical device will appear in many types of textse.g., editorials devoted to expressing an overallopinion about some topic. Hence, we believe that animportant next step is the identification of featuresindicating whether sentences are ontopic which isa kind of coreference problem we look forward toaddressing this challenge in future work.AcknowledgmentsWe thank Joshua Goodman, Thorsten Joachims, JonKleinberg, Vikas Krishna, John Lafferty, Jussi Myllymaki, Phoebe Sengers, Richard Tong, Peter Turney, and the anonymous reviewers for many valuablecomments and helpful suggestions, and Hubie Chenand Tony Faradjian for participating in our baselineexperiments. Portions of this work were done whilethe first author was visiting IBM Almaden. This paper is based upon work supported in part by the National Science Foundation under ITRIM grant IIS0081334. Any opinions, findings, and conclusions orrecommendations expressed above are those of theauthors and do not necessarily reflect the views ofthe National Science Foundation.ReferencesShlomo ArgamonEngelson, Moshe Koppel, andGalit Avneri. 1998. Stylebased text categorization What newspaper am I reading In Proc. ofthe AAAI Workshop on Text Categorization, pages14.Adam L. Berger, Stephen A. Della Pietra, and Vincent J. Della Pietra. 1996. A maximum entropyapproach to natural language processing. Computational Linguistics, 2213971.Douglas Biber. 1988. Variation across Speech andWriting. Cambridge University Press.Stanley Chen and Ronald Rosenfeld. 2000. A surveyof smoothing techniques for ME models. IEEETrans. Speech and Audio Processing, 813750.Sanjiv Das and Mike Chen. 2001. Yahoo forAmazon Extracting market sentiment from stockmessage boards. In Proc. of the 8th Asia PacificFinance Association Annual Conference APFA2001.Stephen Della Pietra, Vincent Della Pietra, and JohnLafferty. 1997. Inducing features of random fields.IEEE Transactions on Pattern Analysis and Machine Intelligence, 194380393.Pedro Domingos and Michael J. Pazzani. 1997. Onthe optimality of the simple Bayesian classifier under zeroone loss. Machine Learning, 2923103130.Aidan Finn, Nicholas Kushmerick, and Barry Smyth.2002. Genre classification and domain transferfor information filtering. In Proc. of the European Colloquium on Information Retrieval Research, pages 353362, Glasgow.Vasileios Hatzivassiloglou and Kathleen McKeown.1997. Predicting the semantic orientation of adjectives. In Proc. of the 35th ACL8th EACL, pages174181.Vasileios Hatzivassiloglou and Janyce Wiebe. 2000.Effects of adjective orientation and gradability onsentence subjectivity. In Proc. of COLING.Marti Hearst. 1992. Directionbased text interpretation as an information access refinement. InPaul Jacobs, editor, TextBased Intelligent Systems. Lawrence Erlbaum Associates.Alison Huettner and Pero Subasic. 2000. Fuzzytyping for document management. In ACL2000 Companion Volume Tutorial Abstracts andDemonstration Notes, pages 2627.Thorsten Joachims. 1998. Text categorization withsupport vector machines Learning with many relevant features. In Proc. of the European Conference on Machine Learning ECML, pages 137142.Thorsten Joachims. 1999. Making largescale SVMlearning practical. In Bernhard Scholkopf andAlexander Smola, editors, Advances in KernelMethods  Support Vector Learning, pages 4456.MIT Press.Jussi Karlgren and Douglass Cutting. 1994. Recognizing text genres with simple metrics using discriminant analysis. In Proc. of COLING.Brett Kessler, Geoffrey Nunberg, and HinrichSchutze. 1997. Automatic detection of text genre.In Proc. of the 35th ACL8th EACL, pages 3238.David D. Lewis. 1998. Naive Bayes at forty Theindependence assumption in information retrieval.In Proc. of the European Conference on MachineLearning ECML, pages 415. Invited talk.Andrew McCallum and Kamal Nigam. 1998. A comparison of event models for Naive Bayes text classification. In Proc. of the AAAI98 Workshop onLearning for Text Categorization, pages 4148.Frederick Mosteller and David L. Wallace. 1984. Applied Bayesian and Classical Inference The Caseof the Federalist Papers. SpringerVerlag.Kamal Nigam, John Lafferty, and Andrew McCallum. 1999. Using maximum entropy for text classification. In Proc. of the IJCAI99 Workshop onMachine Learning for Information Filtering, pages6167.Ted Pedersen. 2001. A decision tree of bigrams is anaccurate predictor of word sense. In Proc. of theSecond NAACL, pages 7986.Warren Sack. 1994. On the computation of point ofview. In Proc. of the Twelfth AAAI, page 1488.Student abstract.Ellen Spertus. 1997. Smokey Automatic recognition of hostile messages. In Proc. of Innovative Applications of Artificial Intelligence IAAI,pages 10581065.Junichi Tatemura. 2000. Virtual reviewers for collaborative exploration of movie reviews. In Proc.of the 5th International Conference on IntelligentUser Interfaces, pages 272275.Loren Terveen, Will Hill, Brian Amento, David McDonald, and Josh Creter. 1997. PHOAKS A system for sharing recommendations. Communications of the ACM, 4035962.Laura Mayfield Tomokiyo and Rosie Jones. 2001.Youre not from round here, are you Naive Bayesdetection of nonnative utterance text. In Proc. ofthe Second NAACL, pages 239246.Richard M. Tong. 2001. An operational system fordetecting and tracking opinions in online discussion. Workshop note, SIGIR 2001 Workshop onOperational Text Classification.Peter D. Turney and Michael L. Littman. 2002. Unsupervised learning of semantic orientation froma hundredbillionword corpus. Technical ReportEGB1094, National Research Council Canada.Peter Turney. 2002. Thumbs up or thumbs downSemantic orientation applied to unsupervised classification of reviews. In Proc. of the ACL.Janyce M. Wiebe, Theresa Wilson, and MatthewBell. 2001. Identifying collocations for recognizingopinions. In Proc. of the ACLEACL Workshopon Collocation.Yorick Wilks and Mark Stevenson. 1998. The grammar of sense Using partofspeech tags as a firststep in semantic disambiguation. Journal of Natural Language Engineering, 42135144.
