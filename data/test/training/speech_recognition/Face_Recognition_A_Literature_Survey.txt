Face Recognition A Literature SurveyW. ZHAOSarnoff CorporationR. CHELLAPPAUniversity of MarylandP. J. PHILLIPSNational Institute of Standards and TechnologyANDA. ROSENFELDUniversity of MarylandAs one of the most successful applications of image analysis and understanding, facerecognition has recently received significant attention, especially during the pastseveral years. At least two reasons account for this trend the first is the wide range ofcommercial and law enforcement applications, and the second is the availability offeasible technologies after 30 years of research. Even though current machinerecognition systems have reached a certain level of maturity, their success is limited bythe conditions imposed by many real applications. For example, recognition of faceimages acquired in an outdoor environment with changes in illumination andor poseremains a largely unsolved problem. In other words, current systems are still far awayfrom the capability of the human perception system.This paper provides an uptodate critical survey of still and videobased facerecognition research. There are two underlying motivations for us to write this surveypaper the first is to provide an uptodate review of the existing literature, and thesecond is to offer some insights into the studies of machine recognition of faces. Toprovide a comprehensive survey, we not only categorize existing recognition techniquesbut also present detailed descriptions of representative methods within each category.In addition, relevant topics such as psychophysical studies, system evaluation, andissues of illumination and pose variation are covered.Categories and Subject Descriptors I.5.4 Pattern Recognition ApplicationsGeneral Terms AlgorithmsAdditional Key Words and Phrases Face recognition, person identificationAn earlier version of this paper appeared as Face Recognition A Literature Survey, Technical Report CARTR948, Center for Automation Research, University of Maryland, College Park, MD, 2000.Authors addresses W. Zhao, Vision Technologies Lab, Sarnoff Corporation, Princeton, NJ 085435300email wzhaosarnoff.com R. Chellappa and A. Rosenfeld, Center for Automation Research, University ofMaryland, College Park, MD 207423275 email rama,arcfar.umd.edu P. J. Phillips, National Instituteof Standards and Technology, Gaithersburg, MD 20899 email jonathonnist.gov.Permission to make digitalhard copy of part or all of this work for personal or classroom use is granted without fee provided that the copies are not made or distributed for profit or commercial advantage, the copyrightnotice, the title of the publication, and its date appear, and notice is given that copying is by permission ofACM, Inc. To copy otherwise, to republish, to post on servers, or to redistribute to lists requires prior specificpermission andor a fee.c2003 ACM 036003000312000399 5.00ACM Computing Surveys, Vol. 35, No. 4, December 2003, pp. 399458.400 Zhao et al.1. INTRODUCTIONAs one of the most successful applicationsof image analysis and understanding, facerecognition has recently received significant attention, especially during the pastfew years. This is evidenced by the emergence of face recognition conferences suchas the International Conference on Audioand VideoBased Authentication AVBPAsince 1997 and the International Conference on Automatic Face and GestureRecognition AFGR since 1995, systematic empirical evaluations of face recognition techniques FRT, including theFERET Phillips et al. 1998b, 2000 Rizviet al. 1998, FRVT 2000 Blackburn et al.2001, FRVT 2002 Phillips et al. 2003,and XM2VTS Messer et al. 1999 protocols, and many commercially availablesystems Table II. There are at least tworeasons for this trend the first is the widerange of commercial and law enforcementapplications and the second is the availability of feasible technologies after 30years of research. In addition, the problem of machine recognition of human facescontinues to attract researchers from disciplines such as image processing, patternrecognition, neural networks, computervision, computer graphics, and psychology.The strong need for userfriendly systems that can secure our assets and protect our privacy without losing our identity in a sea of numbers is obvious. Atpresent, one needs a PIN to get cash froman ATM, a password for a computer, adozen others to access the internet, andso on. Although very reliable methods ofbiometric personal identification exist, forTable I. Typical Applications of Face RecognitionAreas Specific applicationsVideo game, virtual reality, training programsEntertainment Humanrobotinteraction, humancomputerinteractionDrivers licenses, entitlement programsSmart cards Immigration, national ID, passports, voter registrationWelfare fraudTV Parental control, personal device logon, desktop logonInformation security Application security, database security, file encryptionIntranet security, internet access, medical recordsSecure trading terminalsLaw enforcement Advanced video surveillance, CCTV controland surveillance Portal control, postevent analysisShoplifting, suspect tracking and investigationexample, fingerprint analysis and retinalor iris scans, these methods rely on thecooperation of the participants, whereasa personal identification system based onanalysis of frontal or profile images of theface is often effective without the participants cooperation or knowledge. Some ofthe advantagesdisadvantages of differentbiometrics are described in Phillips et al.1998. Table I lists some of the applications of face recognition.Commercial and law enforcement applications of FRT range from static,controlledformat photographs to uncontrolled video images, posing a wide rangeof technical challenges and requiring anequally wide range of techniques from image processing, analysis, understanding,and pattern recognition. One can broadlyclassify FRT systems into two groups depending on whether they make use ofstatic images or of video. Within thesegroups, significant differences exist, depending on the specific application. Thedifferences are in terms of image quality, amount of background clutter posingchallenges to segmentation algorithms,variability of the images of a particularindividual that must be recognized, availability of a welldefined recognition ormatching criterion, and the nature, type,and amount of input from a user. A listof some commercial systems is given inTable II.A general statement of the problem ofmachine recognition of faces can be formulated as follows given still or videoimages of a scene, identify or verifyone or more persons in the scene using a stored database of faces. AvailableACM Computing Surveys, Vol. 35, No. 4, December 2003.Face Recognition A Literature Survey 401Table II. Available Commercial Face Recognition Systems Some of these Web sitesmay have changed or been removed. The identification of any company, commercialproduct, or trade name does not imply endorsement or recommendation by the NationalInstitute of Standards and Technology or any of the authors or their institutions.Commercial products WebsitesFaceIt from Visionics httpwww.FaceIt.comViisage Technology httpwww.viisage.comFaceVACS from Plettac httpwww.plettacelectronics.comFaceKey Corp. httpwww.facekey.comCognitec Systems httpwww.cognitecsystems.deKeyware Technologies httpwww.keywareusa.comPassfaces from IDarts httpwww.idarts.comImageWare Sofware httpwww.iwsinc.comEyematic Interfaces Inc. httpwww.eyematic.comBioID sensor fusion httpwww.bioid.comVisionsphere Technologies httpwww.visionspheretech.commenu.htmBiometric Systems, Inc. httpwww.biometrica.comFaceSnap Recoder httpwww.facesnap.dehtdocsenglishindex2.htmlSpotIt for face composite httpspotit.itc.itSpotIt.htmlFig. 1. Configuration of a generic face recognitionsystem.collateral information such as race, age,gender, facial expression, or speech may beused in narrowing the search enhancingrecognition. The solution to the probleminvolves segmentation of faces face detection from cluttered scenes, feature extraction from the face regions, recognition,or verification Figure 1. In identificationproblems, the input to the system is an unknown face, and the system reports backthe determined identity from a databaseof known individuals, whereas in verification problems, the system needs to confirmor reject the claimed identity of the inputface.Face perception is an important part ofthe capability of human perception system and is a routine task for humans,while building a similar computer system is still an ongoing research area. Theearliest work on face recognition can betraced back at least to the 1950s in psychology Bruner and Tagiuri 1954 and tothe 1960s in the engineering literatureBledsoe 1964. Some of the earliest studies include work on facial expressionof emotions by Darwin 1972 see alsoEkman 1998 and on facial profilebasedbiometrics by Galton 1888. But research on automatic machine recognition of faces really started in the 1970sKelly 1970 and after the seminal workof Kanade 1973. Over the past 30years extensive research has been conducted by psychophysicists, neuroscientists, and engineers on various aspectsof face recognition by humans and machines. Psychophysicists and neuroscientists have been concerned with issuessuch as whether face perception is adedicated process this issue is still being debated in the psychology communityBiederman and Kalocsai 1998 Ellis 1986Gauthier et al. 1999 Gauthier and Logothetis 2000 and whether it is done holistically or by local feature analysis.Many of the hypotheses and theoriesput forward by researchers in these disciplines have been based on rather smallsets of images. Nevertheless, many of theACM Computing Surveys, Vol. 35, No. 4, December 2003.402 Zhao et al.findings have important consequences forengineers who design algorithms and systems for machine recognition of humanfaces. Section 2 will present a concise review of these findings.Barring a few exceptions that use rangedata Gordon 1991, the face recognitionproblem has been formulated as recognizing threedimensional 3D objects fromtwodimensional 2D images.1 Earlier approaches treated it as a 2D pattern recognition problem. As a result, during theearly and mid1970s, typical pattern classification techniques, which use measuredattributes of features e.g., the distancesbetween important points in faces or faceprofiles, were used Bledsoe 1964 Kanade1973 Kelly 1970. During the 1980s, workon face recognition remained largely dormant. Since the early 1990s, research interest in FRT has grown significantly. Onecan attribute this to several reasons an increase in interest in commercial opportunities the availability of realtime hardware and the increasing importance ofsurveillancerelated applications.Over the past 15 years, research hasfocused on how to make face recognitionsystems fully automatic by tackling problems such as localization of a face in agiven image or video clip and extractionof features such as eyes, mouth, etc.Meanwhile, significant advances havebeen made in the design of classifiersfor successful face recognition. Amongappearancebased holistic approaches,eigenfaces Kirby and Sirovich 1990Turk and Pentland 1991 and Fisherfaces Belhumeur et al. 1997 Etemadand Chellappa 1997 Zhao et al. 1998have proved to be effective in experimentswith large databases. Featurebasedgraph matching approaches Wiskottet al. 1997 have also been quite successful. Compared to holistic approaches,featurebased methods are less sensitive to variations in illumination andviewpoint and to inaccuracy in face local1There have been recent advances on 3D face recognition in situations where range data acquired throughstructured light can be matched reliably Bronsteinet al. 2003.ization. However, the feature extractiontechniques needed for this type of approach are still not reliable or accurateenough Cox et al. 1996. For example,most eye localization techniques assumesome geometric and textural models anddo not work if the eye is closed. Section 3will present a review of stillimagebasedface recognition.During the past 5 to 8 years, much research has been concentrated on videobased face recognition. The still imageproblem has several inherent advantagesand disadvantages. For applications suchas drivers licenses, due to the controllednature of the image acquisition process,the segmentation problem is rather easy.However, if only a static picture of an airport scene is available, automatic locationand segmentation of a face could pose serious challenges to any segmentation algorithm. On the other hand, if a videosequence is available, segmentation of amoving person can be more easily accomplished using motion as a cue. But thesmall size and low image quality of facescaptured from video can significantly increase the difficulty in recognition. Videobased face recognition is reviewed inSection 4.As we propose new algorithms and buildmore systems, measuring the performanceof new systems and of existing systemsbecomes very important. Systematic datacollection and evaulation of face recognition systems is reviewed in Section 5.Recognizing a 3D object from its 2D images poses many challenges. The illumination and pose problems are two prominentissues for appearance or imagebased approaches. Many approaches have beenproposed to handle these issues, with themajority of them exploring domain knowledge. Details of these approaches are discussed in Section 6.In 1995, a review paper Chellappa et al.1995 gave a thorough survey of FRTat that time. An earlier survey Samaland Iyengar 1992 appeared in 1992. Atthat time, videobased face recognitionwas still in a nascent stage. During thepast 8 years, face recognition has receivedincreased attention and has advancedACM Computing Surveys, Vol. 35, No. 4, December 2003.Face Recognition A Literature Survey 403technically. Many commercial systems forstill face recognition are now available.Recently, significant research efforts havebeen focused on videobased face modelingtracking, recognition, and system integration. New datasets have been createdand evaluations of recognition techniquesusing these databases have been carriedout. It is not an overstatement to say thatface recognition has become one of themost active applications of pattern recognition, image analysis and understanding.In this paper we provide a critical reviewof current developments in face recognition. This paper is organized as follows inSection 2 we briefly review issues that arerelevant from a psychophysical point ofview. Section 3 provides a detailed reviewof recent developments in face recognitiontechniques using still images. In Section 4face recognition techniques based on videoare reviewed. Data collection and performance evaluation of face recognition algorithms are addressed in Section 5 with descriptions of representative protocols. InSection 6 we discuss two important problems in face recognition that can be mathematically studied, lack of robustness toillumination and pose variations, and wereview proposed methods of overcomingthese limitations. Finally, a summary andconclusions are presented in Section 7.2. PSYCHOPHYSICSNEUROSCIENCEISSUES RELEVANT TO FACERECOGNITIONHuman recognition processes utilize abroad spectrum of stimuli, obtained frommany, if not all, of the senses visual,auditory, olfactory, tactile, etc.. In manysituations, contextual knowledge is alsoapplied, for example, surroundings playan important role in recognizing faces inrelation to where they are supposed tobe located. It is futile to even attempt todevelop a system using existing technology, which will mimic the remarkable facerecognition ability of humans. However,the human brain has its limitations in thetotal number of persons that it can accurately remember. A key advantage of acomputer system is its capacity to handlelarge numbers of face images. In mostapplications the images are available onlyin the form of single or multiple views of2D intensity data, so that the inputs tocomputer face recognition algorithms arevisual only. For this reason, the literaturereviewed in this section is restricted tostudies of human visual perception offaces.Many studies in psychology and neuroscience have direct relevance to engineersinterested in designing algorithms or systems for machine recognition of faces. Forexample, findings in psychology Bruce1988 Shepherd et al. 1981 about the relative importance of different facial featureshave been noted in the engineering literature Etemad and Chellappa 1997. Onthe other hand, machine systems providetools for conducting studies in psychologyand neuroscience Hancock et al. 1998Kalocsai et al. 1998. For example, a possible engineering explanation of the bottom lighting effects studied in Johnstonet al. 1992 is as follows when the actuallighting direction is opposite to the usuallyassumed direction, a shapefromshadingalgorithm recovers incorrect structural information and hence makes recognition offaces harder.A detailed review of relevant studies inpsychophysics and neuroscience is beyondthe scope of this paper. We only summarize findings that are potentially relevantto the design of face recognition systems.For details the reader is referred to thepapers cited below. Issues that are of potential interest to designers are2Is face recognition a dedicated processBiederman and Kalocsai 1998 Ellis1986 Gauthier et al. 1999 Gauthier andLogothetis 2000 It is traditionally believed that face recognition is a dedicated process different from other object recognition tasks. Evidence for theexistence of a dedicated face processing system comes from several sourcesEllis 1986. a Faces are more easily remembered by humans than other2Readers should be aware of the existence of diverseopinions on some of these issues. The opinions givenhere do not necessarily represent our views.ACM Computing Surveys, Vol. 35, No. 4, December 2003.404 Zhao et al.objects when presented in an uprightorientation. b Prosopagnosia patientsare unable to recognize previously familiar faces, but usually have no otherprofound agnosia. They recognize people by their voices, hair color, dress, etc.It should be noted that prosopagnosiapatients recognize whether a given object is a face or not, but then have difficulty in identifying the face. Sevendifferences between face recognitionand object recognition can be summarized Biederman and Kalocsai 1998based on empirical evidence 1 configural effects related to the choice ofdifferent types of machine recognitionsystems, 2 expertise, 3 differencesverbalizable, 4 sensitivity to contrastpolarity and illumination direction related to the illumination problem in machine recognition systems, 5 metricvariation, 6 Rotation in depth relatedto the pose variation problem in machine recognition systems, and 7 rotation in planeinverted face. Contraryto the traditionally held belief, some recent findings in human neuropsychology and neuroimaging suggest that facerecognition may not be unique. According to Gauthier and Logothetis 2000,recent neuroimaging studies in humansindicate that level of categorization andexpertise interact to produce the specification for faces in the middle fusiformgyrus.3 Hence it is possible that the encoding scheme used for faces may alsobe employed for other classes with similar properties. On recognition of familiar vs. unfamiliar faces see Section 7.Is face perception the result of holisticor feature analysis Bruce 1988 Bruceet al. 1998 Both holistic and featureinformation are crucial for the perception and recognition of faces. Studiessuggest the possibility of global descriptions serving as a front end for finer,featurebased perception. If dominantfeatures are present, holistic descrip3The fusiform gyrus or occipitotemporal gyrus, located on the ventromedial surface of the temporaland occipital lobes, is thought to be critical for facerecognition.tions may not be used. For example, inface recall studies, humans quickly focus on odd features such as big ears, acrooked nose, a staring eye, etc. One ofthe strongest pieces of evidence to support the view that face recognition involves more configuralholistic processing than other object recognition hasbeen the face inversion effect in whichan inverted face is much harder to recognize than a normal face first demonstrated in Yin 1969. An excellent example is given in Bartlett and Searcy1993 using the Thatcher illusionThompson 1980. In this illusion, theeyes and mouth of an expressing faceare excised and inverted, and the result looks grotesque in an upright facehowever, when shown inverted, the facelooks fairly normal in appearance, andthe inversion of the internal features isnot readily noticed.Ranking of significance of facial featuresBruce 1988 Shepherd et al. 1981 Hair,face outline, eyes, and mouth not necessarily in this order have been determined to be important for perceiving and remembering faces Shepherdet al. 1981. Several studies have shownthat the nose plays an insignificant rolethis may be due to the fact that almost all of these studies have been doneusing frontal images. In face recognition using profiles which may be important in mugshot matching applications, where profiles can be extractedfrom side views, a distinctive noseshape could be more important than theeyes or mouth Bruce 1988. Anotheroutcome of some studies is that bothexternal and internal features are important in the recognition of previously presented but otherwise unfamiliar faces, but internal features are moredominant in the recognition of familiarfaces. It has also been found that theupper part of the face is more usefulfor face recognition than the lower partShepherd et al. 1981. The role of aesthetic attributes such as beauty, attractiveness, andor pleasantness has alsobeen studied, with the conclusion thatACM Computing Surveys, Vol. 35, No. 4, December 2003.Face Recognition A Literature Survey 405the more attractive the faces are, thebetter is their recognition rate the leastattractive faces come next, followed bythe midrange faces, in terms of ease ofbeing recognized.Caricatures Brennan 1985 Bruce 1988Perkins 1975 A caricature can be formally defined Perkins 1975 as a symbol that exaggerates measurements relative to any measure which varies fromone person to another. Thus the lengthof a nose is a measure that varies fromperson to person, and could be usefulas a symbol in caricaturing someone,but not the number of ears. A standard caricature algorithm Brennan1985 can be applied to different qualities of image data line drawings andphotographs. Caricatures of line drawings do not contain as much informationas photographs, but they manage to capture the important characteristics of aface experiments based on nonordinaryfaces comparing the usefulness of linedrawing caricatures and unexaggeratedline drawings decidedly favor the formerBruce 1988.Distinctiveness Bruce et al. 1994 Studies show that distinctive faces are better retained in memory and are recognized better and faster than typicalfaces. However, if a decision has to bemade as to whether an object is a faceor not, it takes longer to recognize anatypical face than a typical face. Thismay be explained by different mechanisms being used for detection and foridentification.The role of spatial frequency analysisGinsburg 1978 Harmon 1973 Sergent1986 Earlier studies Ginsburg 1978Harmon 1973 concluded that information in low spatial frequency bandsplays a dominant role in face recognition. Recent studies Sergent 1986have shown that, depending on the specific recognition task, the low, bandpass and highfrequency componentsmay play different roles. For examplegender classification can be successfullyaccomplished using lowfrequency components only, while identification requires the use of highfrequency components Sergent 1986. Lowfrequencycomponents contribute to global description, while highfrequency components contribute to the finer detailsneeded in identification.Viewpointinvariant recognition Biederman 1987 Hill et al. 1997 Tarrand Bulthoff 1995 Much work in visual object recognition e.g. Biederman1987 has been cast within a theoretical framework introduced in Marr1982 in which different views of objects are analyzed in a way whichallows access to largely viewpointinvariant descriptions. Recently, therehas been some debate about whether object recognition is viewpointinvariantor not Tarr and Bulthoff 1995. Someexperiments suggest that memory forfaces is highly viewpointdependent.Generalization even from one profileviewpoint to another is poor, thoughgeneralization from one threequarterview to the other is very good Hill et al.1997.Effect of lighting change Bruce et al.1998 Hill and Bruce 1996 Johnstonet al. 1992 It has long been informallyobserved that photographic negativesof faces are difficult to recognize. However, relatively little work has exploredwhy it is so difficult to recognize negative images of faces. In Johnston et al.1992, experiments were conducted toexplore whether difficulties with negative images and inverted images of facesarise because each of these manipulations reverses the apparent direction oflighting, rendering a toplit image of aface apparently lit from below. It wasdemonstrated in Johnston et al. 1992that bottom lighting does indeed make itharder to identity familiar faces. In Hilland Bruce 1996, the importance of toplighting for face recognition was demonstrated using a different task matching surface images of faces to determinewhether they were identical.Movement and face recognition OTooleet al. 2002 Bruce et al. 1998 Knight andJohnston 1997 A recent study KnightACM Computing Surveys, Vol. 35, No. 4, December 2003.406 Zhao et al.and Johnston 1997 showed that famous faces are easier to recognize whenshown in moving sequences than instill photographs. This observation hasbeen extended to show that movementhelps in the recognition of familiar facesshown under a range of different typesof degradationsnegated, inverted, orthresholded Bruce et al. 1998. Evenmore interesting is the observationthat there seems to be a benefitdue to movement even if the information content is equated in the moving and static comparison conditions.However, experiments with unfamiliarfaces suggest no additional benefit fromviewing animated rather than staticsequences.Facial expressions Bruce 1988 Basedon neurophysiological studies, it seemsthat analysis of facial expressions is accomplished in parallel to face recognition. Some prosopagnosic patients, whohave difficulties in identifying familiar faces, nevertheless seem to recognize expressions due to emotions. Patients who suffer from organic brainsyndrome suffer from poor expressionanalysis but perform face recognitionquite well.4 Similarly, separation of facerecognition and focused visual processing tasks e.g., looking for someone witha thick mustache have been claimed.3. FACE RECOGNITION FROMSTILL IMAGESAs illustrated in Figure 1, the problem of automatic face recognition involvesthree key stepssubtasks 1 detection andrough normalization of faces, 2 featureextraction and accurate normalization offaces, 3 identification andor verification.Sometimes, different subtasks are not totally separated. For example, the facialfeatures eyes, nose, mouth used for facerecognition are often used in face detection. Face detection and feature extractioncan be achieved simultaneously, as indi4From a machine recognition point of view, dramaticfacial expressions may affect face recognition performance if only one photograph is available.cated in Figure 1. Depending on the natureof the application, for example, the sizes ofthe training and testing databases, clutterand variability of the background, noise,occlusion, and speed requirements, someof the subtasks can be very challenging.Though fully automatic face recognitionsystems must perform all three subtasks,research on each subtask is critical. Thisis not only because the techniques usedfor the individual subtasks need to be improved, but also because they are criticalin many different applications Figure 1.For example, face detection is needed toinitialize face tracking, and extraction offacial features is needed for recognizinghuman emotion, which is in turn essentialin humancomputer interaction HCI systems. Isolating the subtasks makes it easier to assess and advance the state of theart of the component techniques. Earlierface detection techniques could only handle single or a few wellseparated frontalfaces in images with simple backgrounds,while stateoftheart algorithms can detect faces and their poses in clutteredbackgrounds Gu et al. 2001 Heisele et al.2001 Schneiderman and Kanade 2000 Viola and Jones 2001. Extensive research onthe subtasks has been carried out and relevant surveys have appeared on, for example, the subtask of face detection Hjelmasand Low 2001 Yang et al. 2002.In this section we survey the state of theart of face recognition in the engineeringliterature. For the sake of completeness,in Section 3.1 we provide a highlightedsummary of research on face segmentationdetection and feature extraction. Section 3.2 contains detailed reviews of recentwork on intensity imagebased face recognition and categorizes methods of recognition from intensity images. Section 3.3summarizes the status of face recognitionand discusses open research issues.3.1. Key Steps Prior to Recognition FaceDetection and Feature ExtractionThe first step in any automatic facerecognition systems is the detection offaces in images. Here we only provide asummary on this topic and highlight a fewACM Computing Surveys, Vol. 35, No. 4, December 2003.Face Recognition A Literature Survey 407very recent methods. After a face has beendetected, the task of feature extraction isto obtain features that are fed into a faceclassification system. Depending on thetype of classification system, features canbe local features such as lines or fiducialpoints, or facial features such as eyes,nose, and mouth. Face detection may alsoemploy features, in which case featuresare extracted simultaneously with facedetection. Feature extraction is also akey to animation and recognition of facialexpressions.Without considering feature locations,face detection is declared successful if thepresence and rough location of a face hasbeen correctly identified. However, without accurate face and feature location, noticeable degradation in recognition performance is observed Martinez 2002 Zhao1999. The close relationship between feature extraction and face recognition motivates us to review a few feature extractionmethods that are used in the recognitionapproaches to be reviewed in Section 3.2.Hence, this section also serves as an introduction to the next section.3.1.1. SegmentationDetection Summary.Up to the mid1990s, most work onsegmentation was focused on singlefacesegmentation from a simple or complexbackground. These approaches includedusing a wholeface template, a deformablefeaturebased template, skin color, and aneural network.Significant advances have been madein recent years in achieving automaticface detection under various conditions.Compared to featurebased methods andtemplatematching methods, appearanceor imagebased methods Rowley et al.1998 Sung and Poggio 1997 that trainmachine systems on large numbers ofsamples have achieved the best results.This may not be surprising since faceobjects are complicated, very similar toeach other, and different from nonface objects. Through extensive training, computers can be quite good at detecting faces.More recently, detection of faces underrotation in depth has been studied. Oneapproach is based on training on multipleview samples Gu et al. 2001 Schneiderman and Kanade 2000. Compared toinvariantfeaturebased methods Wiskottet al. 1997, multiviewbased methods offace detection and recognition seem to beable to achieve better results when the angle of outofplane rotation is large 35.In the psychology community, a similardebate exists on whether face recognitionis viewpointinvariant or not. Studies inboth disciplines seem to support the ideathat for small angles, face perception isviewindependent, while for large angles,it is viewdependent.In a detection problem, two statisticsare important true positives also referredto as detection rate and false positivesreported detections in nonface regions.An ideal system would have very hightrue positive and very low false positiverates. In practice, these two requirementsare conflicting. Treating face detection asa twoclass classification problem helpsto reduce false positives dramaticallyRowley et al. 1998 Sung and Poggio 1997while maintaining true positives. This isachieved by retraining systems with falsepositive samples that are generated bypreviously trained systems.3.1.2. Feature Extraction Summary andMethods3.1.2.1. Summary. The importance of facial features for face recognition cannotbe overstated. Many face recognition systems need facial features in addition tothe holistic face, as suggested by studiesin psychology. It is well known that evenholistic matching methods, for example,eigenfaces Turk and Pentland 1991 andFisherfaces Belhumeur et al. 1997, needaccurate locations of key facial featuressuch as eyes, nose, and mouth to normalize the detected face Martinez 2002 Yanget al. 2002.Three types of feature extraction methods can be distinguished 1 generic methods based on edges, lines, and curves2 featuretemplatebased methods thatare used to detect facial features suchas eyes 3 structural matching methodsACM Computing Surveys, Vol. 35, No. 4, December 2003.408 Zhao et al.that take into consideration geometricalconstraints on the features. Early approaches focused on individual featuresfor example, a templatebased approachwas described in Hallinan 1991 to detect and recognize the human eye in afrontal face. These methods have difficultywhen the appearances of the featureschange significantly, for example, closedeyes, eyes with glasses, open mouth. To detect the features more reliably, recent approaches have used structural matchingmethods, for example, the Active ShapeModel Cootes et al. 1995. Compared toearlier methods, these recent statisticalmethods are much more robust in termsof handling variations in image intensityand feature shape.An even more challenging situation forfeature extraction is feature restoration,which tries to recover features that areinvisible due to large variations in headpose. The best solution here might be tohallucinate the missing features either byusing the bilateral symmetry of the face orusing learned information. For example, aviewbased statistical method claims to beable to handle even profile views in whichmany local features are invisible Cooteset al. 2000.3.1.2.2. Methods. A templatebased approach to detecting the eyes and mouth inreal images was presented in Yuille et al.1992. This method is based on matching a predefined parameterized templateto an image that contains a face region.Two templates are used for matching theeyes and mouth respectively. An energyfunction is defined that links edges, peaksand valleys in the image intensity tothe corresponding properties in the template, and this energy function is minimized by iteratively changing the parameters of the template to fit the image. Compared to this model, which ismanually designed, the statistical shapemodel Active Shape Model, ASM proposed in Cootes et al. 1995 offers moreflexibility and robustness. The advantagesof using the socalled analysis throughsynthesis approach come from the factthat the solution is constrained by a flexible statistical model. To account for texture variation, the ASM model has beenexpanded to statistical appearance models including a Flexible Appearance ModelFAM Lanitis et al. 1995 and an ActiveAppearance Model AAM Cootes et al.2001. In Cootes et al. 2001, the proposed AAM combined a model of shapevariation i.e., ASM with a model of theappearance variation of shapenormalizedshapefree textures. A training set of 400images of faces, each manually labeledwith 68 landmark points, and approximately 10,000 intensity values sampledfrom facial regions were used. The shapemodel mean shape, orthogonal mappingmatrix Ps and projection vector bs is generated by representing each set of landmarks as a vector and applying principalcomponent analysis PCA to the data.Then, after each sample image is warpedso that its landmarks match the meanshape, texture information can be sampled from this shapefree face patch. Applying PCA to this data leads to a shapefree texture model mean texture, Pgand bg . To explore the correlation between the shape and texture variations,a third PCA is applied to the concatenated vectors bs and bg  to obtain thecombined model in which one vector cof appearance parameters controls boththe shape and texture of the model. Tomatch a given image and the model, anoptimal vector of parameters displacement parameters between the face regionand the model, parameters for linear intensity adjustment, and the appearanceparameters c are searched by minimizing the difference between the syntheticimage and the given one. After matching, a bestfitting model is constructedthat gives the locations of all the facialfeatures and can be used to reconstructthe original images. Figure 2 illustratesthe optimizationsearch procedure forfitting the model to the image. To speed upthe search procedure, an efficient methodis proposed that exploits the similaritiesamong optimizations. This allows the direct method to find and apply directionsof rapid convergence which are learnedoffline.ACM Computing Surveys, Vol. 35, No. 4, December 2003.Face Recognition A Literature Survey 409Fig. 2. Multiresolution search from a displaced position using a face model. Courtesy of T. Cootes,K. Walker, and C. Taylor.3.2. Recognition from Intensity ImagesMany methods of face recognition havebeen proposed during the past 30 years.Face recognition is such a challengingyet interesting problem that it has attracted researchers who have differentbackgrounds psychology, pattern recognition, neural networks, computer vision,and computer graphics. It is due to thisfact that the literature on face recognitionis vast and diverse. Often, a single system involves techniques motivated by different principles. The usage of a mixtureof techniques makes it difficult to classifythese systems based purely on what typesof techniques they use for feature representation or classification. To have a clearand highlevel categorization, we insteadfollow a guideline suggested by the psychological study of how humans use holistic and local features. Specifically, we havethe following categorization1 Holistic matching methods. Thesemethods use the whole face region asthe raw input to a recognition system.One of the most widely used representations of the face region is eigenpictures Kirby and Sirovich 1990Sirovich and Kirby 1987, which arebased on principal component analysis.2 Featurebased structural matchingmethods. Typically, in these methods,local features such as the eyes, nose,and mouth are first extracted and theirlocations and local statistics geometric andor appearance are fed into astructural classifier.3 Hybrid methods. Just as the humanperception system uses both local features and the whole face region to recognize a face, a machine recognitionsystem should use both. One can argue that these methods could potentially offer the best of the two types ofmethods.Within each of these categories, furtherclassification is possible Table III. Usingprincipalcomponent analysis PCA,many face recognition techniques havebeen developed eigenfaces Turk andPentland 1991, which use a nearestneighbor classifier featurelinebasedmethods, which replace the pointtopointdistance with the distance between a pointand the feature line linking two storedsample points Li and Lu 1999 Fisherfaces Belhumeur et al. 1997 Liu andWechsler 2001 Swets and Weng 1996bZhao et al. 1998 which use linearFisherdiscriminant analysis FLDLDA Fisher1938 Bayesian methods, which use aprobabilistic distance metric Moghaddamand Pentland 1997 and SVM methods,which use a support vector machine as theclassifier Phillips 1998. Utilizing higherorder statistics, independentcomponentACM Computing Surveys, Vol. 35, No. 4, December 2003.410 Zhao et al.Table III. Categorization of Still Face Recognition TechniquesApproach Representative workHolistic methodsPrincipalcomponent analysis PCAEigenfaces Direct application of PCA Craw and Cameron 1996 Kirbyand Sirovich 1990 Turk and Pentland 1991Probabilistic eigenfaces Twoclass problem with prob. measure Moghaddam andPentland 1997Fisherfacessubspace LDA FLD on eigenspace Belhumeur et al. 1997 Swets and Weng1996b Zhao et al. 1998SVM Twoclass problem based on SVM Phillips 1998Evolution pursuit Enhanced GA learning Liu and Wechsler 2000aFeature lines Pointtoline distance based Li and Lu 1999ICA ICAbased feature analysis Bartlett et al. 1998Other representationsLDAFLD LDAFLD on raw image Etemad and Chellappa 1997PDBNN Probabilistic decision based NN Lin et al. 1997Featurebased methodsPure geometry methods Earlier methods Kanade 1973 Kelly 1970 recentmethods Cox et al. 1996 Manjunath et al. 1992Dynamic link architecture Graph matching methods Okada et al. 1998 Wiskott et al.1997Hidden Markov model HMM methods Nefian and Hayes 1998 Samaria 1994Samaria and Young 1994Convolution Neural Network SOM learning based CNN methods Lawrence et al. 1997Hybrid methodsModular eigenfaces Eigenfaces and eigenmodules Pentland et al. 1994Hybrid LFA Local feature method Penev and Atick 1996Shapenormalized Flexible appearance models Lanitis et al. 1995Componentbased Face region and components Huang et al. 2003analysis ICA is argued to have morerepresentative power than PCA, andhence may provide better recognition performance than PCA Bartlett et al. 1998.Being able to offer potentially greatergeneralization through learning, neuralnetworkslearning methods have alsobeen applied to face recognition. One example is the Probabilistic DecisionBasedNeural Network PDBNN method Linet al. 1997 and the other is the evolutionpursuit EP method Liu and Wechsler2000a.Most earlier methods belong to the category of structural matching methods, using the width of the head, the distancesbetween the eyes and from the eyes to themouth, etc. Kelly 1970, or the distancesand angles between eye corners, mouthextrema, nostrils, and chin top Kanade1973. More recently, a mixturedistancebased approach using manually extracteddistances was reported Cox et al. 1996.Without finding the exact locations offacial features, Hidden Markov ModelHMM based methods use strips of pixels that cover the forehead, eye, nose,mouth, and chin Nefian and Hayes 1998Samaria 1994 Samaria and Young 1994.Nefian and Hayes 1998 reported better performance than Samaria 1994 byusing the KL projection coefficients instead of the strips of raw pixels. One ofthe most successful systems in this category is the graph matching system Okadaet al. 1998 Wiskott et al. 1997, whichis based on the Dynamic Link Architecture DLA Buhmann et al. 1990 Ladeset al. 1993. Using an unsupervised learning method based on a selforganizing mapSOM, a system based on a convolutionalneural network CNN has been developedLawrence et al. 1997.In the hybrid method category, wewill briefly review the modular eigenfacemethod Pentland et al. 1994, a hybridrepresentation based on PCA and localfeature analysis LFA Penev and Atick1996, a flexible appearance modelbasedmethod Lanitis et al. 1995, and a recentdevelopment Huang et al. 2003 alongthis direction. In Pentland et al. 1994,ACM Computing Surveys, Vol. 35, No. 4, December 2003.Face Recognition A Literature Survey 411Fig. 3. Electronically modified images which were correctly identified.the use of hybrid features by combiningeigenfaces and other eigenmodules is explored eigeneyes, eigenmouth, and eigennose. Though experiments show slightimprovements over holistic eigenfaces oreigenmodules based on structural matching, we believe that these types of methodsare important and deserve further investigation. Perhaps many relevant problemsneed to be solved before fruitful resultscan be expected, for example, how to optimally arbitrate the use of holistic and localfeatures.Many types of systems have been successfully applied to the task of face recognition, but they all have some advantagesand disadvantages. Appropriate schemesshould be chosen based on the specific requirements of a given task. Most of thesystems reviewed here focus on the subtask of recognition, but others also include automatic face detection and featureextraction, making them fully automaticsystems Lin et al. 1997 Moghaddam andPentland 1997 Wiskott et al. 1997.3.2.1. Holistic Approaches3.2.1.1. PrincipalComponent Analysis.Starting from the successful lowdimensional reconstruction of facesusing KL or PCA projections Kirby andSirovich 1990 Sirovich and Kirby 1987,eigenpictures have been one of the majordriving forces behind face representation, detection, and recognition. It iswell known that there exist significantstatistical redundancies in natural images Ruderman 1994. For a limited classof objects such as face images that arenormalized with respect to scale, translation, and rotation, the redundancy iseven greater Penev and Atick 1996 Zhao1999. One of the best global compactrepresentations is KLPCA, which decorrelates the outputs. More specifically,sample vectors x can be expressed as linear combinations of the orthogonal basisi x ni1 aii mi1 aii typicallym  n by solving the eigenproblemC  , 1where C is the covariance matrix for inputx.An advantage of using such representations is their reduced sensitivity to noise.Some of this noise may be due to small occlusions, as long as the topological structure does not change. For example, goodperformance under blurring, partial occlusion and changes in background hasbeen demonstrated in many eigenpicturebased systems, as illustrated in Figure 3.This should not come as a surprise, sincethe PCA reconstructed images are muchbetter than the original distorted images in terms of their global appearanceFigure 4.For better approximation of face imagesoutside the training set, using an extendedtraining set that adds mirrorimaged faceswas shown to achieve lower approximation error Kirby and Sirovich 1990. Using such an extended training set, theeigenpictures are either symmetric or antisymmetric, with the most leading eigenpictures typically being symmetric.ACM Computing Surveys, Vol. 35, No. 4, December 2003.412 Zhao et al.Fig. 4. Reconstructed images using 300 PCA projection coefficients for electronically modified images Figure 3. From Zhao 1999.The first really successful demonstration of machine recognition of faces wasmade in Turk and Pentland 1991 usingeigenpictures also known as eigenfacesfor face detection and identification. Giventhe eigenfaces, every face in the databasecan be represented as a vector of weightsthe weights are obtained by projecting theimage into eigenface components by a simple inner product operation. When a newtest image whose identification is requiredis given, the new image is also representedby its vector of weights. The identificationof the test image is done by locating theimage in the database whose weights arethe closest to the weights of the test image.By using the observation that the projection of a face image and a nonface imageare usually different, a method of detecting the presence of a face in a given imageis obtained. The method was demonstrated using a database of 2500 face images of 16 subjects, in all combinations ofthree head orientations, three head sizes,and three lighting conditions.Using a probabilistic measure of similarity, instead of the simple Euclideandistance used with eigenfaces Turk andPentland 1991, the standard eigenfaceapproach was extended Moghaddam andPentland 1997 to a Bayesian approach.Practically, the major drawback of aBayesian method is the need to estimate probability distributions in a highdimensional space from very limited numbers of training samples per class. To avoidthis problem, a much simpler twoclassproblem was created from the multiclassproblem by using a similarity measurebased on a Bayesian analysis of image differences. Two mutually exclusive classeswere defined I , representing intrapersonal variations between multiple imagesof the same individual, and E , representing extrapersonal variations due to differences in identity. Assuming that bothclasses are Gaussiandistributed, likelihood functions P I  and P E  wereestimated for a given intensity difference  I1  I2. Given these likelihood functions and using the MAP rule, two face images are determined to belong to the sameindividual if P I   P E . A largeperformance improvement of this probabilistic matching technique over standard nearestneighbor eigenspace matching was reported using large face datasetsincluding the FERET database Phillipset al. 2000. In Moghaddam and Pentland1997, an efficient technique of probability density estimation was proposed by decomposing the input space into two mutually exclusive subspaces the principalsubspace F and its orthogonal subspace Fa similar idea was explored in Sung andPoggio 1997. Covariances only in theprincipal subspace are estimated for usein the Mahalanobis distance Fukunaga1989. Experimental results have been reported using different subspace dimensionalities MI and ME for I and E .For example, MI  10 and ME  30were used for internal tests, while MI ME  125 were used for the FERET test.In Figure 5, the socalled dual eigenfacesseparately trained on samples from Iand E are plotted along with the standard eigenfaces. While the extrapersonalACM Computing Surveys, Vol. 35, No. 4, December 2003.Face Recognition A Literature Survey 413Fig. 5. Comparison of dual eigenfaces and standard eigenfaces a intrapersonal, b extrapersonal, c standard Moghaddam and Pentland 1997.Courtesy of B. Moghaddam and A. Pentland.eigenfaces appear more similar to thestandard eigenfaces than the intrapersonal ones, the intrapersonal eigenfacesrepresent subtle variations due mostlyto expression and lighting, suggestingthat they are more critical for identification Moghaddam and Pentland 1997.Face recognition systems usingLDAFLD have also been very successful Belhumeur et al. 1997 Etemadand Chellappa 1997 Swets and Weng1996b Zhao et al. 1998 Zhao et al. 1999.LDA training is carried out via scattermatrix analysis Fukunaga 1989. Foran M class problem, the within andbetweenclass scatter matrices Sw, Sb arecomputed as followsSw Mi1PriCi,2Sb Mi1Primi  m0mi  m0T ,where Pri is the prior class probability,and is usually replaced by 1M in practicewith the assumption of equal priors. HereSw is the withinclass satter matrix, showing the average scatter5 Ci of the sample vectors x of different classes i around5These are also conditional covariance matrices theFig. 6. Different projection bases constructed froma set of 444 individuals, where the set is augumentedvia adding noise and mirroring. The first row showsthe first five pure LDA basis images W  the secondrow shows the first five subspace LDA basis imagesW the average face and first four eigenfaces  areshown on the third row Zhao et al. 1998.their respective means mi Ci  Exmix  miT   i. Similarly, Sb isthe Betweenclass Scatter Matrix, representing the scatter of the conditional meanvectors mi around the overall mean vectorm0. A commonly used measure for quantifying discriminatory power is the ratioof the determinant of the betweenclassscatter matrix of the projected samples tothe determinant of the withinclass scatter matrix J T   T T SbT T T SwT .The optimal projection matrix W whichmaximizes J T  can be obtained by solving a generalized eigenvalue problemSbW  SwWW . 3It is helpful to make comparisonsamong the socalled linear projection algorithms. Here we illustrate the comparison between eigenfaces and Fisherfaces. Similar comparisons can be madefor other methods, for example, ICA projection methods. In all these projection algorithms, classification is performed by 1projecting the input x into a subspace viaa projectionbasis matrix Proj 6total covariance C used to compute the PCA projection is C  Mi1 PriCi .6Proj is  for eigenfaces, W for Fisherfaces withpure LDA projection, and W for Fisherfaces withACM Computing Surveys, Vol. 35, No. 4, December 2003.414 Zhao et al.z  Proj x 42 comparing the projection coefficientvector z of the input to all the prestoredprojection vectors of labeled classes todetermine the input class label. Thevector comparison varies in differentimplementations and can influence thesystems performance dramatically Moonand Phillips 2001. For example, PCAalgorithms can use either the angle orthe Euclidean distance weighted or unweighted between two projection vectors.For LDA algorithms, the distance can beunweighted or weighted.In Swets and Weng 1996b, discriminant analysis of eigenfeatures is appliedin an image retrieval system to determinenot only class human face vs. nonfaceobjects but also individuals within theface class. Using treestructure learning,the eigenspace and LDA projectionsare recursively applied to smaller andsmaller sets of samples. Such recursivepartitioning is carried out for every nodeuntil the samples assigned to the nodebelong to a single class. Experiments onthis approach were reported in Swets andWeng 1996. A set of 800 images wasused for training the training set camefrom 42 classes, of which human facesbelong to a single class. Within the singleface class, 356 individuals were includedand distinguished. Testing results onimages not in the training set were 91for 78 face images and 87 for 38 nonfaceimages based on the top choice.A comparative performance analysiswas carried out in Belhumeur et al. 1997.Four methods were compared in this paper 1 a correlationbased method, 2 avariant of the linear subspace method suggested in Shashua 1994, 3 an eigenfacemethod Turk and Pentland 1991, and 4a Fisherface method which uses subspaceprojection prior to LDA projection toavoid the possible singularity in Sw asin Swets and Weng 1996b. Experimentswere performed on a database of 500images created by Hallinan 1994 and asequential PCA and LDA projections these threebases are shown for visual comparison in Figure 6.database of 176 images created at Yale.The results of the experiments showedthat the Fisherface method performedsignificantly better than the other threemethods. However, no claim was madeabout the relative performance of thesealgorithms on larger databases.To improve the performance of LDAbased systems, a regularized subspaceLDA system that unifies PCA and LDAwas proposed in Zhao 1999 and Zhaoet al. 1998. Good generalization abilityof this system was demonstrated by experiments that carried out testing on newclassesindividuals without retraining thePCA bases , and sometimes the LDAbases W . While the reason for not retraining PCA is obvious, it is interestingto test the adaptive capability of the system by fixing the LDA bases when images from new classes are added.7 Thefixed PCA subspace of dimensionality 300was trained from a large number of samples. An augmented set of 4056 mostlyfrontalview images constructed from theoriginal 1078 FERET images of 444 individuals by adding noise and mirroringwas used in Zhao et al. 1998. At leastone of the following three characteristicsseparates this system from other LDAbased systems 1 the unique selectionof the universal face subspace dimension,2 the use of a weighted distance measure, and 3 a regularized procedure thatmodifies the withinclass scatter matrixSw. The authors selected the dimensionality of the universal face subspace basedon the characteristics of the eigenvectorsfacelike or not instead of the eigenvalues Zhao et al. 1998, as is commonlydone. Later it was concluded in Penev andSirovich 2000 that the global face subspace dimensionality is on the order of400 for large databases of 5,000 images.A weighted distance metric in the projection space z was used to improve performance Zhao 1999.8 Finally, the LDA7This makes sense because the final classification iscarried out in the projection space z by comparisonwith prestored projection vectors.8Weighted metrics have also been used in the pureLDA approach Etemad and Chellappa 1997 and theACM Computing Surveys, Vol. 35, No. 4, December 2003.Face Recognition A Literature Survey 415Fig. 7. Two architectures for performing ICA on images. Left architecture forfinding statistically independent basis images. Performing source separation onthe face images produces independent images in the rows of U . Right architecturefor finding a factorial code. Performing source separation on the pixels produces afactorial code in the columns of the output matrix U Bartlett et al. 1998. Courtesyof M. Bartlett, H. Lades, and T. Sejnowski.training was regularized by modifying theSw matrix to SwI , where  is a relativelysmall positive number. Doing this solvesa numerical problem when Sw is close tobeing singular. In the extreme case whereonly one sample per class is available, thisregularization transforms the LDA problem into a standard PCA problem with Sbbeing the covariance matrix C. Applyingthis approach, without retraining the LDAbasis, to a testingprobe set of 46 individuals of which 24 were trained and 22 werenot trained a total of 115 images including19 untrained images of nonfrontal views,the authors reported the following performance based on a frontviewonly gallerydatabase of 738 images 85.2 for all images and 95.1 for frontal views.An evolution pursuit EP based adaptive representation and its application toface recognition were presented in Liu andWechsler 2000a. In analogy to projectionpursuit methods, EP seeks to learn an optimal basis for the dual purpose of datacompression and pattern classification. Inorder to increase the generalization abilityof EP, a balance is sought between minimizing the empirical risk encounteredduring training and narrowing the confidence interval for reducing the guaranteed risk during future testing on unseendata Vapnik 1995. Toward that end, EPimplements strategies characteristic of genetic algorithms GAs for searching thesocalled enhanced FLD EFM approach Liu andWechsler 2000b.space of possible solutions to determinethe optimal basis. EP starts by projectingthe original data into a lowerdimensionalwhitened PCA space. Directed random rotations of the basis vectors in this spaceare then searched by GAs where evolutionis driven by a fitness function defined interms of performance accuracy empiricalrisk and class separation confidence interval. The feasibility of this method hasbeen demonstrated for face recognition,where the large number of possible basesrequires a greedy search algorithm. Theparticular face recognition task involves1107 FERET frontal face images of 369subjects there were three frontal imagesfor each subject, two for training and theremaining one for testing. The authors reported improved face recognition performance as compared to eigenfaces Turkand Pentland 1991, and better generalization capability than FisherfacesBelhumeur et al. 1997.Based on the argument that for taskssuch as face recognition much of theimportant information is contained inhighorder statistics, it has been proposed Bartlett et al. 1998 to use ICAto extract features for face recognition.Independentcomponent analysis is a generalization of principalcomponent analysis, which decorrelates the highordermoments of the input in addition to thesecondorder moments. Two architectureshave been proposed for face recognitionFigure 7 the first is used to find a setof statistically independent source imagesACM Computing Surveys, Vol. 35, No. 4, December 2003.416 Zhao et al.Fig. 8. Comparison of basis images using two architectures for performing ICA a 25 independent components of Architecture I, b 25 independent components of Architecture II Bartlettet al. 1998. Courtesy of M. Bartlett, H. Lades, and T. Sejnowski.that can be viewed as independent imagefeatures for a given set of training images Bell and Sejnowski 1995, and thesecond is used to find image filters thatproduce statistically independent outputs a factorial code method Bell and Sejnowski 1997. In both architectures, PCAis used first to reduce the dimensionality of the original image size 60  50.ICA is performed on the first 200 eigenvectors in the first architecture, and is carriedout on the first 200 PCA projection coefficients in the second architecture. The authors reported performance improvementof both architectures over eigenfaces inthe following scenario a FERET subsetconsisting of 425 individuals was usedall the frontal views one per class wereused for training and the remaining upto three frontal views for testing. Basisimages of the two architectures are shownin Figure 8 along with the correspondingeigenfaces.3.2.1.2. Other Representations. In additionto the popular PCA representation and itsderivatives such as ICA and EP, other features have also been used, such as raw intensities and edges.A fully automatic face detectionrecognition system based on aneural network is reported in Lin et al.1997. The proposed system is basedon a probabilistic decisionbased neural network PDBNN, an extendedDBNN Kung and Taur 1995 whichconsists of three modules a face detector,an eye localizer, and a face recognizer.Unlike most methods, the facial regionscontain the eyebrows, eyes, and nose,but not the mouth.9 The rationale ofusing only the upper face is to build arobust system that excludes the influenceof facial variations due to expressionsthat cause motion around the mouth.To improve robustness, the segmentedfacial region images are first processedto produce two features at a reducedresolution of 1410 normalized intensityfeatures and edge features, both in therange 0, 1. These features are fed intotwo PDBNNs and the final recognitionresult is the fusion of the outputs of thesetwo PDBNNs. A unique characteristic ofPDBNNs and DBNNs is their modularstructure. That is, for each classperson9Such a representation was also used in Kirby andSirovich 1990ACM Computing Surveys, Vol. 35, No. 4, December 2003.Face Recognition A Literature Survey 417Fig. 9. Structure of the PDBNN face recognizer. Each class subnet isdesigned to recognize one person. All the network weightings are in probabilistic format Lin et al. 1997. Courtesy of S. Lin, S. Kung, and L. Lin.to be recognized, PDBNNDBNN devotesone of its subnets to the representation ofthat particular person, as illustrated inFigure 9. Such a oneclassinonenetworkOCON structure has certain advantages over the allclassesinonenetworkACON structure that is adopted bythe conventional multilayer perceptronMLP. In the ACON structure, all classesare lumped into one supernetwork,so large numbers of hidden units areneeded and convergence is slow. Onthe other hand, the OCON structureconsists of subnets that consist of smallnumbers of hidden units hence it notonly converges faster but also has bettergeneralization capability. Compared tomost multiclass recognition systems thatuse a discrimination function betweenany two classes, PDBNN has a lowerfalse acceptancerejection rate because ituses the full density description for eachclass. In addition, this architecture isbeneficial for hardware implementationsuch as distributed computing. However,it is not clear how to accurately estimatethe full density functions for the classeswhen there are only limited numbers ofsamples. Further, the system could haveproblems when the number of classesgrows exponentially.3.2.2. FeatureBased Structural Matching Approaches. Many methods in the structuralmatching category have been proposed,including many early methods based ongeometry of local features Kanade 1973ACM Computing Surveys, Vol. 35, No. 4, December 2003.418 Zhao et al.Fig. 10. The bunch graph representation of faces used in elastic graph matching Wiskott et al.1997. Courtesy of L. Wiskott, J.M. Fellous, and C. von der Malsburg.Kelly 1970 as well as 1D Samaria andYoung 1994 and pseudo2D Samaria1994 HMM methods. One of the mostsuccessful of these systems is the Elastic Bunch Graph Matching EBGM system Okada et al. 1998 Wiskott et al.1997, which is based on DLA Buhmannet al. 1990 Lades et al. 1993. Wavelets,especially Gabor wavelets, play a buildingblock role for facial representation in thesegraph matching methods. A typical localfeature representation consists of waveletcoefficients for different scales and rotations based on fixed wavelet bases calledjets in Okada et al. 1998. These locallyestimated wavelet coefficients are robustto illumination change, translation, distortion, rotation, and scaling.The basic 2D Gabor function and itsFourier transform areg x, y  u0, v0  expx22 2x  y22 2y 2iu0x  vo y,Gu, v  exp   22 2x u  u02  2y v  v02, 5where x and  y represent the spatialwidths of the Gaussian and u0, v0 is thefrequency of the complex sinusoid.DLAs attempt to solve some of the conceptual problems of conventional artificialneural networks, the most prominent ofthese being the representation of syntactical relationships in neural networks.DLAs use synaptic plasticity and areable to form sets of neurons grouped intostructured graphs while maintainingthe advantages of neural systems. BothBuhmann et al. 1990 and Ladeset al. 1993 used Gaborbased waveletsFigure 10a as the features. As described in Lades et al. 1993 DLAs basicmechanism, in addition to the connectionparameter Tij betweeen two neurons i,j , is a dynamic variable Jij . Only theJ variables play the roles of synapticweights for signal transmission. TheT parameters merely act to constrain theJ variables, for example, 0  Jij  Tij .The T parameters can be changed slowlyby longterm synaptic plasticity. Theweights Jij are subject to rapid modification and are controlled by the signalcorrelations between neurons i and j .Negative signal correlations lead to adecrease and positive signal correlationslead to an increase in Jij . In the absenceof any correlation, Jij slowly returns to aresting state, a fixed fraction of Tij . Eachstored image is formed by picking a rectangular grid of points as graph nodes. Thegrid is appropriately positioned over theimage and is stored with each grid pointslocally determined jet Figure 10a, andserves to represent the pattern classes.Recognition of a new image takes place bytransforming the image into the grid ofjets, and matching all stored model graphsto the image. Conformation of the DLAis done by establishing and dynamicallymodifying links between vertices in themodel domain.The DLA architecture was recently extended to Elastic Bunch Graph Matching Wiskott et al. 1997 Figure 10. Thisis similar to the graph described above,but instead of attaching only a single jetto each node, the authors attached a setACM Computing Surveys, Vol. 35, No. 4, December 2003.Face Recognition A Literature Survey 419of jets called the bunch graph representation, Figure 10b, each derived from adifferent face image. To handle the posevariation problem, the pose of the face isfirst determined using prior class information Kruger et al. 1997, and the jettransformations under pose variation arelearned Maurer and Malsburg 1996a.Systems based on the EBGM approachhave been applied to face detection andextraction, pose estimation, gender classification, sketchimagebased recognition,and general object recognition. The success of the EBGM system may be due toits resemblance to the human visual system Biederman and Kalocsai 1998.3.2.3. Hybrid Approaches. Hybrid approaches use both holistic and localfeatures. For example, the modular eigenfaces approach Pentland et al. 1994uses both global eigenfaces and localeigenfeatures.In Pentland et al. 1994, the capabilities of the earlier system Turk andPentland 1991 were extended in severaldirections. In mugshot applications, usually a frontal and a side view of a personare available in some other applications,more than two views may be appropriate.One can take two approaches to handlingimages from multiple views. The firstapproach pools all the images and constructs a set of eigenfaces that representall the images from all the views. Theother approach uses separate eigenspacesfor different views, so that the collection ofimages taken from each view has its owneigenspace. The second approach, knownas viewbased eigenspaces, performsbetter.The concept of eigenfaces can beextended to eigenfeatures, such aseigeneyes, eigenmouth, etc. Using alimited set of images 45 persons, twoviews per person, with different facialexpressions such as neutral vs. smiling,recognition performance as a function ofthe number of eigenvectors was measuredfor eigenfaces only and for the combinedrepresentation. For lowerorder spaces,the eigenfeatures performed better thanFig. 11. Comparison of matching a testviews, b eigenface matches, c eigenfeature matches Pentland et al. 1994.the eigenfaces Pentland et al. 1994when the combined set was used, onlymarginal improvement was obtained.These experiments support the claim thatfeaturebased mechanisms may be usefulwhen gross variations are present in theinput images Figure 11.It has been argued that practical systems should use a hybrid of PCA andLFA Appendix B in Penev and Atick1996. Such view has been long held inthe psychology community Bruce 1988.It seems to be better to estimate eigenmodeseigenfaces that have large eigenvalues and so are more robust againstnoise, while for estimating higherordereigenmodes it is better to use LFA. To support this point, it was argued in Penevand Atick 1996 that the leading eigenpictures are global, integrating, or smoothing filters that are efficient in suppressing noise, while the higherorder modesare ripply or differentiating filters that arelikely to amplify noise.LFA is an interesting biologically inspired feature analysis method Penevand Atick 1996. Its biological motivationcomes from the fact that, though a hugearray of receptors more than six millioncones exist in the human retina, only aACM Computing Surveys, Vol. 35, No. 4, December 2003.420 Zhao et al.Fig. 12. LFA kernels K xi , y at different grids xi Penev and Atick 1996.small fraction of them are active, corresponding to natural objectssignals thatare statistically redundant Ruderman1994. From the activity of these sparselydistributed receptors, the brain has todiscover where and what objects are inthe field of view and recover their attributes. Consequently, one expects to represent the natural objectssignals in a subspace of lower dimensionality by findinga suitable parameterization. For a limited class of objects such as faces whichare correctly aligned and scaled, this suggests that even lower dimensionality canbe expected Penev and Atick 1996. Onegood example is the successful use of thetruncated PCA expansion to approximatethe frontal face images in a linear subspace Kirby and Sirovich 1990 Sirovichand Kirby 1987.Going a step further, the whole face region stimulates a full 2D array of receptors, each of which corresponds to a location in the face, but some of these receptors may be inactive. To explore thisredundancy, LFA is used to extract topographic local features from the globalPCA modes. Unlike PCA kernels i whichcontain no topographic information theirsupports extend over the entire grid ofimages, LFA kernels Figure12 K xi, yat selected grids xi have local support.1010These kernels Figure 12 indexed by grids xi aresimilar to the ICA kernels in the first ICA systemarchitecture Bartlett et al. 1998 Bell and Sejnowski1995.The search for the best topographic set ofsparsely distributed grids xo based on reconstruction error is called sparsificationand is described in Penev and Atick 1996.Two interesting points are demonstratedin this paper 1 using the same numberof kernels, the perceptual reconstructionquality of LFA based on the optimal setof grids is better than that of PCA themean square error is 227, and 184 for aparticular input 2 keeping the secondPCA eigenmodel in LFA reconstruction reduces the mean square error to 152, suggesting the hybrid use of PCA and LFA. Noresults on recognition performance basedon LFA were reported. LFA is claimed tobe used in Visionicss commercial systemFaceIt Table II.A flexible appearance model basedmethod for automatic face recognition waspresented in Lanitis et al. 1995. To identify a face, both shape and graylevel information are modeled and used. The shapemodel is an ASM these are statisticalmodels of the shapes of objects which iteratively deform to fit to an example ofthe shape in a new image. The statistical shape model is trained on example images using PCA, where the variables are the coordinates of the shapemodel points. For the purpose of classification, the shape variations due to interclass variation are separated from thosedue to withinclass variations such assmall variations in 3D orientation and facial expression using discriminant analysis. Based on the average shape of theACM Computing Surveys, Vol. 35, No. 4, December 2003.Face Recognition A Literature Survey 421Fig. 13. The face recognition scheme based on flexible appearancemodel Lanitis et al. 1995. Courtesy of A. Lanitis, C. Taylor, and T.Cootes.shape model, a global shapefree graylevel model can be constructed, again using PCA.11 To further enhance the robustness of the system against changesin local appearance such as occlusions,local graylevel models are also builton the shape model points. Simple local profiles perpendicular to the shapeboundary are used. Finally, for an inputimage, all three types of information,including extracted shape parameters,shapefree image parameters, and localprofiles, are used to compute a Mahalanobis distance for classification as illustrated in Figure 13. Based on training 10and testing 13 images for each of 30 individuals, the classification rate was 92 forthe 10 normal testing images and 48 forthe three difficult images.The last method Huang et al. 2003 thatwe review in this category is based on recent advances in componentbased detectionrecognition Heisele et al. 2001 and3D morphable models Blanz and Vetter1999. The basic idea of componentbasedmethods Heisele et al. 2001 is to decompose a face into a set of facial componentssuch as mouth and eyes that are intercon11Recall that in Craw and Cameron 1996 andMoghaddam and Pentland 1997 these shapefreeimages are used as the inputs to the classifier.ntected by a flexible geometrical model.Notice how this method is similar to theEBGM system Okada et al. 1998 Wiskottet al. 1997 except that grayscale components are used instead of Gabor wavelets.The motivation for using components isthat changes in head pose mainly lead tochanges in the positions of facial components which could be accounted for by theflexibility of the geometric model. However, a major drawback of the system isthat it needs a large number of trainingimages taken from different viewpointsand under different lighting conditions. Toovercome this problem, the 3D morphableface model Blanz and Vetter 1999 is applied to generate arbitrary synthetic images under varying pose and illumination.Only three face images frontal, semiprofile, profile of a person are needed to compute the 3D face model. Once the 3D modelis constructed, synthetic images of size58  58 are generated for training boththe detector and the classifer. Specifically,the faces were rotated in depth from 0 to34 in 2 increments and rendered withtwo illumination models the first modelconsists of ambient light alone and thesecond includes ambient light and a rotating point light source at each pose.Fourteen facial components were used forface detection, but only nine componentsACM Computing Surveys, Vol. 35, No. 4, December 2003.422 Zhao et al.that were not strongly overlapped and contained grayscale structures were used forclassification. In addition, the face regionwas added to the nine components to forma single feature vector a hybrid method,which was later trained by a SVM classifer Vapnik 1995. Training on three images and testing on 200 images per subject led to the following recognition rateson a set of six subjects 90 for the hybridmethod and roughly 10 for the globalmethod that used the face region only thefalse positive rate was 10.3.3. Summary and DiscussionFace recognition based on still images orcaptured frames in a video stream canbe viewed as 2D image matching andrecognition range images are not available in most commerciallaw enforcementapplications. Face recognition based onother sensing modalities such as sketchesand infrared images is also possible. Eventhough this is an oversimplification of theactual recognition problem of 3D objectsbased on 2D images, we have focused onthis 2D problem, and we will address twoimportant issues about 2D recognition of3D face objects in Section 6. Significantprogress has been achieved on various aspects of face recognition segmentation,feature extraction, and recognition of facesin intensity images. Recently, progress hasalso been made on constructing fully automatic systems that integrate all thesetechniques.3.3.1. Status of Face Recognition. Aftermore than 30 years of research and development, basic 2D face recognition hasreached a mature level and many commercial systems are available Table II forvarious applications Table I.Early research on face recognition wasprimarily focused on the feasibility question, that is is machine recognition offaces possible Experiments were usuallycarried out using datasets consisting ofas few as 10 images. Significant advanceswere made during the mid1990s, withmany methods proposed and tested ondatasets consisting of as many as 100images. More recently, practical methods have emerged that aim at more realistic applications. In the recent comprehensive FERET evaluations Phillipset al. 2000 Phillips et al. 1998b Rizviet al. 1998, aimed at evaluating different systems using the same largedatabase containing thousands of images,the systems described in Moghaddam andPentland 1997 Swets and Weng 1996bTurk and Pentland 1991 Wiskott et al.1997 Zhao et al. 1998, as well asothers, were evaluated. The EBGM system Wiskott et al. 1997, the subspaceLDA system Zhao et al. 1998, and theprobabilistic eigenface system Moghaddam and Pentland 1997 were judged tobe among the top three, with each methodshowing different levels of performance ondifferent subsets of sequestered images.A brief summary of the FERET evaluations will be presented in Section 5. Recently, more extensive evaluations usingcommercial systems and thousands of images have been performed in the FRVT2000 Blackburn et al. 2001 and FRVT2002 Phillips et al. 2003 tests.3.3.2. Lessons, Facts and Highlights. During the development of face recognitionsystems, many lessons have been learnedwhich may provide some guidance in thedevelopment of new methods and systems.Advances in face recognition have comefrom considering various aspects of thisspecialized perception problem. Earliermethods treated face recognition as astandard pattern recognition problemlater methods focused more on the representation aspect, after realizing itsuniqueness using domain knowledgemore recent methods have been concerned with both representation andrecognition, so a robust system withgood generalization capability can bebuilt. Face recognition continues toadopt stateoftheart techniques fromlearning, computer vision, and patternrecognition. For example, distributionmodeling using mixtures of Gaussians,and SVM learning methods, have beenused in face detectionrecognition.ACM Computing Surveys, Vol. 35, No. 4, December 2003.Face Recognition A Literature Survey 423Among all face detectionrecognitionmethods, appearanceimagebased approaches seem to have dominated upto now. The main reason is the strongprior that all face images belong to a faceclass. An important example is the useof PCA for the representation of holisticfeatures. To overcome sensitivity to geometric change, local appearancebasedapproaches, 3D enhanced approaches,and hybrid approaches can be used.The most recent advances toward fast3D data acquisition and accurate 3Drecognition are likely to influence futuredevelopements.12The methodological difference betweenface detection and face recognition maynot be as great as it appears to be. Wehave observed that the multiclass facerecognition problem can be convertedinto a twoclass detection problem byusing image differences Moghaddamand Pentland 1997 and the face detection problem can be converted into amulticlass recognition problem by using additional nonface clusters of negative samples Sung and Poggio 1997.It is well known that for face detection,the image size can be quite small. Butwhat about face recognition Clearly theimage size cannot be too small for methods that depend heavily on accuratefeature localization, such as graphmatching methods Okada et al. 1998.However, it has been demonstrated thatthe image size can be very small forholistic face recognition 12  11 for thesubspace LDA system Zhao et al. 1999,1410 for the PDBNN system Lin et al.1997, and 18  24 for human perception Bachmann 1991. Some authorshave argued that there exists a universal face subspace of fixed dimensionhence for holistic recognition, image sizedoes not matter as long as it exceedsthe subspace dimensionality Zhao et al.1999. This claim has been supportedby limited experiments using normalized face images of different sizes, for12Early work using range images was reportedin Gordon 1991.example, from 12  11 to 48  42, toobtain different face subspaces Zhao1999. Indeed, slightly better performance was observed when smaller images were used. One reason is that thesignaltonoise ratio improves with thedecrease in image size.Accurate feature location is critical forgood recognition performance. This istrue even for holistic matching methods,since accurate location of key facial features such as eyes is required to normalize the detected face Yang et al. 2002Zhao 1999. This was also verified in Linet al. 1997 where the use of smaller images led to slightly better performancedue to increased tolerance to location errors. In Martinez 2002, a systematicstudy of this issue was presented.Regarding the debate in the psychologycommunity about whether face recognition is a dedicated process, the recent success of machine systems thatare trained on large numbers of samplesseems to confirm recent findings suggesting that human recognition of facesmay be not uniquededicated, but needsextensive training.When comparing different systems, weshould pay close attention to implementation details. Different implementations of a PCAbased face recognition algorithm were compared in Moonand Phillips 2001. One class of variations examined was the use of seven different distance metrics in the nearestneighbor classifier, which was found tobe the most critical element. This raisesthe question of what is more important in algorithm performance, the representation or the specifics of the implementation. Implementation detailsoften determine the performance of asystem. For example, input images arenormalized only with respect to translation, inplane rotation, and scale inBelhumeur et al. 1997, Swets andWeng 1996b, Turk and Pentland1991, and Zhao et al. 1998, whereasin Moghaddam and Pentland 1997the normalization also includes masking and affine warping to align theACM Computing Surveys, Vol. 35, No. 4, December 2003.424 Zhao et al.shape. In Craw and Cameron 1996,manually selected points are used towarp the input images to the meanshape, yielding shapefree images. Because of this difference, PCA was agood classifier in Moghaddam and Pentland 1997 for the shapefree representations, but it may not be as goodfor the simply normalized representations. Recently, systematic comparisonsand independent reevaluations of existing methods have been publishedBeveridge et al. 2001. This is beneficial to the research community. However, since the methods need to be reimplemented, and not all the details in theoriginal implementation can be takeninto account, it is difficult to carry outabsolutely fair comparisons.Over 30 years of research has providedus with a vast number of methods andsystems. Recognizing the fact that eachmethod has its advantages and disadvantages, we should select methods andsystems appropriate to the application.For example, local feature based methods cannot be applied when the inputimage contains a small face region, say15  15. Another issue is when to usePCA and when to use LDA in building asystem. Apparently, when the number oftraining samples per class is large, LDAis the best choice. On the other hand,if only one or two samples are availableper class a degenerate case for LDA,PCA is a better choice. For a more detailed comparison of PCA versus LDA,see Beveridge et al. 2001 Martinezand Kak 2001. One way to unify PCAand LDA is to use regularized subspaceLDA Zhao et al. 1999.3.3.3. Open Research Issues. Thoughmachine recognition of faces from stillimages has achieved a certain level ofsuccess, its performance is still far fromthat of human perception. Specifically, wecan list the following open issuesHybrid face recognition systems thatuse both holistic and local features resemble the human perceptual system.While the holistic approach provides aquick recognition method, the discriminant information that it provides maynot be rich enough to handle very largedatabases. This insufficiency can becompensated for by local feature methods. However, many questions need tobe answered before we can build such acombined system. One important question is how to arbitrate the use of holisticand local features. As a first step, a simple, naive engineering approach wouldbe to weight the features. But how todetermine whether and how to use thefeatures remains an open problem.The challenge of developing face detection techniques that report not only thepresence of a face but also the accuratelocations of facial features under largepose and illumination variations still remains. Without accurate localization ofimportant features, accurate and robustface recognition cannot be achieved.How to model face variation under realistic settings is still challengingforexample, outdoor environments, natural aging, etc.4. FACE RECOGNITION FROM IMAGESEQUENCESA typical videobased face recognition system automatically detects face regions, extracts features from the video, and recognizes facial identity if a face is present. Insurveillance, information security, and access control applications, face recognitionand identification from a video sequenceis an important problem. Face recognitionbased on video is preferable over using stillimages, since as demonstrated in Bruceet al. 1998 and Knight and Johnston1997, motion helps in recognition of familiar faces when the images are negated,inverted or threshold. It was also demonstrated that humans can recognize animated faces better than randomly rearranged images from the same set. Thoughrecognition of faces from video sequenceis a direct extension of stillimagebasedrecognition, in our opinion, true videobased face recognition techniques that coherently use both spatial and temporalinformation started only a few years agoACM Computing Surveys, Vol. 35, No. 4, December 2003.Face Recognition A Literature Survey 425and still need further investigation. Significant challenges for videobased recognition still exist we list several of themhere.1 The quality of video is low. Usually,video acquisition occurs outdoors orindoors but with bad conditions forvideo capture and the subjects are notcooperative hence there may be largeillumination and pose variations in theface images. In addition, partial occlusion and disguise are possible.2 Face images are small. Again, due tothe acquisition conditions, the face image sizes are smaller sometimes muchsmaller than the assumed sizes inmost stillimagebased face recognition systems. For example, the validface region can be as small as 15 15 pixels,13 whereas the face imagesizes used in featurebased still imagebased systems can be as large as 128128. Smallsize images not only makethe recognition task more difficult, butalso affect the accuracy of face segmentation, as well as the accurate detection of the fiducial pointslandmarksthat are often needed in recognitionmethods.3 The characteristics of faceshumanbody parts. During the past 8 years,research on human actionbehaviorrecognition from video has been veryactive and fruitful. Generic descriptionof human behavior not particular to anindividual is an interesting and usefulconcept. One of the main reasons forthe feasibility of generic descriptions ofhuman behavior is that the intraclassvariations of human bodies, and in particular faces, is much smaller than thedifference between the objects insideand outside the class. For the same reason, recognition of individuals withinthe class is difficult. For example, detecting and localizing faces is typicallymuch easier than recognizing a specificface.13Notice this is totally different from the situationwhere we have images with large face regions butthe final face regions feed into a classifier is 15  15.Before we examine existing videobasedface recognition algorithms, we brieflyreview three closely related techniquesface segmentation and pose estimation,face tracking, and face modeling. Thesetechniques are critical for the realizationof the full potential of videobased facerecognition.4.1. Basic Techniques of VideoBased FaceRecognitionIn Chellappa et al. 1995, four computervision areas were mentioned as being important for videobased face recognitionsegmentation of moving objects humansfrom a video sequence structure estimation 3D models for faces and nonrigid motion analysis. For example, in Jebara et al.1998 a face modeling system which estimates facial features and texture froma video stream was described. This system utilizes all four techniques segmentation of the face based on skin color toinitiate tracking use of a 3D face modelbased on laserscanned range data to normalize the image by facial feature alignment and texture mapping to generate afrontal view and construction of an eigensubspace for 3D heads use of structurefrom motion SfM at each feature pointto provide depth information and nonrigid motion analysis of the facial features based on simple 2D SSD sum ofsquared differences tracking constrainedby a global 3D model. Based on the currentdevelopment of videobased face recognition, we think it is better to review threespecific facerelated techniques instead ofthe above four general areas. The threevideobased facerelated techniques areface segmentation and pose estimation,face tracking, and face modeling.4.1.1. Face Segmentation and Pose Estimation. Early attempts Turk and Pentland1991 at segmenting moving faces from animage sequence used simple pixelbasedchange detection procedures based on difference images. These techniques may runinto difficulties when multiple moving objects and occlusion are present. More sophisticated methods use estimated flowACM Computing Surveys, Vol. 35, No. 4, December 2003.426 Zhao et al.fields for segmenting humans in motion Shio and Sklansky 1991. More recent methods Choudhury et al. 1999McKenna and Gong 1998 have used motion andor color information to speed upthe process of searching for possible faceregions. After candidate face regions arelocated, stillimagebased face detectiontechniques can be applied to locate thefaces Yang et al. 2002. Given a face region, important facial features can be located. The locations of feature points canbe used for pose estimation, which is important for synthesizing a virtual frontalview Choudhury et al. 1999. Newly developed segmentation methods locate theface and estimate its pose simultaneouslywithout extracting features Gu et al.2001 Li et al. 2001b. This is achieved bylearning multiview face examples whichare labeled with manually determinedpose angles.4.1.2. Face and Feature Tracking. Afterfaces are located, the faces and their features can be tracked. Face tracking andfeature tracking are critical for reconstructing a face model depth throughSfM, and feature tracking is essentialfor facial expression recognition and gazerecognition. Tracking also plays a keyrole in spatiotemporalbased recognitionmethods Li and Chellappa 2001 Li et al.2001a which directly use the tracking information.In its most general form, tracking isessentially motion estimation. However,general motion estimation has fundamental limitations such as the aperture problem. For images like faces, some regionsare too smooth to estimate flow accurately,and sometimes the change in local appearances is too large to give reliable flow.Fortunately, these problems are alleviatedthanks to face modeling, which exploitsdomain knowledge. In general, trackingand modeling are dual processes tracking is constrained by a generic 3D modelor a learned statistical model under deformation, and individual models are refined through tracking. Face tracking canbe roughly divided into three categories1 head tracking, which involves trackingthe motion of a rigid object that is performing rotations and translations 2 facialfeature tracking, which involves trackingnonrigid deformations that are limited bythe anatomy of the head, that is, articulated motion due to speech or facial expressions and deformable motion due to muscle contractions and relaxations and 3complete tracking, which involves tracking both the head and the facial features.Early efforts focused on the first twoproblems head tracking Azarbayejaniet al. 1993 and facial feature tracking Terzopoulos and Waters 1993 Yuilleand Hallinan 1992. In Azarbayejani et al.1993, an approach to head tracking usingpoints with high Hessian values was proposed. Several such points on the head aretracked and the 3D motion parameters ofthe head are recovered by solving an overconstrained set of motion equations. Facialfeature tracking methods may make useof the feature boundary or the feature region. Feature boundary tracking attemptsto track and accurately delineate theshape of the facial feature, for example, totrack the contours of the lips and mouthTerzopoulos and Waters 1993. Featureregion tracking addresses the simplerproblem of tracking a region such as abounding box that surrounds the facialfeature Black et al. 1995.In Black et al. 1995, a tracking system based on local parameterized models is used to recognize facial expressions.The models include a planar model forthe head, local affine models for the eyes,and local affine models and curvature forthe mouth and eyebrows. A face tracking system was used in Maurer and Malsburg 1996b to estimate the pose of theface. This system used a graph representation with about 2040 nodeslandmarksto model the face. Knowledge about facesis used to find the landmarks in thefirst frame. Two tracking systems described in Jebara et al. 1998 and Stromet al. 1999 model faces completely withtexture and geometry. Both systems usegeneric 3D models and SfM to recoverthe face structure. Jebara et al. 1998 relied fixed feature points eyes, nose tip,ACM Computing Surveys, Vol. 35, No. 4, December 2003.Face Recognition A Literature Survey 427Table IV. Categorization of VideoBased Face Recognition TechniquesApproach Representative workStillimage methods Basic methods Turk and Pentland 1991 Lin et al. 1997Moghaddam and Pentland 1997 Okada et al. 1998 Penev andAtick 1996 Wechsler et al. 1997 Wiskott et al. 1997Trackingenhanced Edwards et al. 1998 McKenna and Gong1997, 1998 Steffens et al. 1998Multimodal methods Video and audiobased Bigun et al. 1998 Choudhury et al. 1999Spatiotemporal methods Feature trajectorybased Li and Chellappa 2001 Li et al. 2001aVideoto video methods Zhou et al. 2003while Strom et al. 1999 tracked onlypoints with high Hessian values. Also, Jebara et al. 1998 tracked 2D features in3D by deforming them, while Strom et al.1999 relied on direct comparison of a 3Dmodel to the image. Methods have beenproposed in Black et al. 1998 and Hagerand Belhumeur 1998 to solve the varying appearance both geometry and photometry problem in tracking. Some of thenewest modelbased tracking methods calculate the 3D motions and deformationsdirectly from image intensities Brandand Bhotika 2001, thus eliminating theinformationlossy intermediate representations.4.1.3. Face Modeling. Modeling of facesincludes 3D shape modeling and texturemodeling. For large texture variations dueto changes in illumination, we will addressthe illumination problem in Section 6.Here we focus on 3D shape modeling. 3Dmodels of faces have been employed in thegraphics, animation, and modelbased image compression literature. More complicated models are used in applications suchas forensic face reconstruction from partial information.In computer vision, one of the mostwidely used methods of estimating 3Dshape from a video sequence is SfM, whichestimates the 3D depths of interestingpoints. The unconstrained SfM problemhas been approached in two ways. In thedifferential approach, one computes sometype of flow field optical, image, or normal and uses it to estimate the depthsof visible points. The difficulty in this approach is reliable computation of the flowfield. In the discrete approach, a set of features such as points, edges, corners, lines,or contours are tracked over a sequenceof frames, and the depths of these features are computed. To overcome the difficulty of feature tracking, bundle adjustment Triggs et al. 2000 can be used toobtain better and more robust results.Recently, multiview based 2D methodshave gained popularity. In Li et al. 2001b,a model consisted of a sparse 3D shapemodel learned from 2D images labeledwith pose and landmarks, a shapeandposefree texture model, and an affine geometrical model. An alternative approachis to use 3D models such as the deformablemodel of DeCarlo and Metaxas 2000 orthe linear 3D object class model of Blanzand Vetter 1999. In Blanz and Vetter1999 a morphable 3D face model consisting of shape and texture was directlymatched to singlemultiple input imagesas a consequence, head orientation, illumination conditions, and other parameterscould be free variables subject to optimization. In Blanz and Vetter 1999, realtime3D modeling and tracking of faces wasdescribed a generic 3D head model wasaligned to match frontal views of the facein a video sequence.4.2. VideoBased Face RecognitionHistorically, video face recognition originated from stillimagebased techniquesTable IV. That is, the system automatically detects and segments the face fromthe video, and then applies stillimage facerecognition techniques. Many methods reviewed in Section 3 belong to this categoryeigenfaces Turk and Pentland 1991,probabilistic eigenfaces Moghaddamand Pentland 1997, the EBGMmethod Okada et al. 1998 Wiskottet al. 1997, and the PDBNN method Linet al. 1997. An improvement over thesemethods is to apply tracking this can helpACM Computing Surveys, Vol. 35, No. 4, December 2003.428 Zhao et al.in recognition, in that a virtual frontalview can be synthesized via pose anddepth estimation from video. Due to theabundance of frames in a video, anotherway to improve the recognition rate is theuse of voting based on the recognitionresults from each frame. The voting canbe deterministic, but probabilistic votingis better in general Gong et al. 2000McKenna and Gong 1998. One drawbackof such voting schemes is the expense ofcomputing the deterministicprobabilisticresults for each frame.The next phase of videobased facerecognition will be the use of multimodalcues. Since humans routinely use multiple cues to recognize identities, it is expected that a multimodal system will dobetter than systems based on faces only.More importantly, using multimodal cuesoffers a comprehensive solution to the taskof identification that might not be achievable by using face images alone. For example, in a totally noncooperative environment, such as a robbery, the face of therobber is typically covered, and the onlyway to perform faceless identificationmight be to analyize body motion characteristics Klasen and Li 1998. Excludingfingerprints, face and voice are the mostfrequently used cues for identification.They have been used in many multimodalsystems Bigun et al. 1998 Choudhuryet al. 1999. Since 1997, a dedicated conference focused on video and audiobasedperson authentication has been held everyother year.More recently, a third phase of videoface recognition has started. These methods Li and Chellappa 2001 Li et al.2001a coherently exploit both spatial information in each frame and temporal information such as the trajectories of facial features. A big difference betweenthese methods and the probabilistic votingmethods McKenna and Gong 1998 is theuse of representations in a joint temporaland spatial space for identification.We first review systems that applystillimagebased recognition to selectedframes, and then multimodal systems. Finally, we review systems that use spatialand temporal information simultaneously.In Wechsler et al. 1997, a fully automatic person authentication system wasdescribed which included video break, facedetection, and authentication modules.Video skimming was used to reduce thenumber of frames to be processed. Thevideo break module, corresponding to keyframe detection based on object motion,consisted of two units. The first unit implemented a simple optical flow method itwas used when the image SNR level waslow. When the SNR level was high, simplepairwise frame differencing was used todetect the moving object. The face detection module consisted of three units facelocalization using analysis of projectionsalong the x and yaxes face region labeling using a decision tree learned from positive and negative examples taken from 12images each consisting of 2759 windowsof size 88 and face normalization basedon the numbers of face region labels. Thenormalized face images were then usedfor authentication, using an RBF network.This system was tested on three image sequences the first was taken indoors withone subject present, the second was takenoutdoors with two subjects, and the thirdwas taken outdoors with one subject understormy conditions. Perfect results were reported on all three sequences, as verifiedagainst a database of 20 still face images.An access control system based onperson authentication was describedin McKenna and Gong 1997. The systemcombined two complementary visual cuesmotion and facial appearance. In orderto reliably detect significant motion, spatiotemporal zero crossings computed fromsix consecutive frames were used. Thesemotions were grouped into moving objectsusing a clustering algorithm, and Kalmanfilters were employed to track the groupedobjects. An appearancebased face detection scheme using RBF networks similarto that discussed in Rowley et al. 1998was used to confirm the presence of aperson. The face detection scheme wasbootstrapped using motion and objectdetection to provide an approximate headregion. Face tracking based on the RBFnetwork was used to provide feedback tothe motion clustering process to help dealACM Computing Surveys, Vol. 35, No. 4, December 2003.Face Recognition A Literature Survey 429Fig. 14. Varying the most significant identity parameters top andmanipulating residual variation without affecting identity bottomEdwards et al. 1998.with occlusions. Good tracking resultswere demonstrated. In McKenna andGong 1998, this work was extended toperson authentication using PCA or LDA.The authors argued that recognitionbased on selected frames is not adequatesince important information is discarded.Instead, they proposed a probabilisticvoting scheme that is, face identificationwas carried out continuously. Though theygave examples demonstrating improvedperformance in identifying 8 or 15 peopleby using sequences, no performancestatistics were reported.An appearance model based method forvideo tracking and enhancing identification was proposed in Edwards et al. 1998.The appearance model is a combinationof the active shape model ASM Cooteset al. 1995 and the shapefree texturemodel after warping the face into a meanshape. Unlike Lanitis et al. 1995, whichused the two models separately, the authors used a combined set of parametersfor both models. The main contributionwas the decomposition of the combinedmodel parameters into an identity subspace and an orthogonal residual subspaceusing linear discriminant analysis. SeeFigure 14 for an illustration of separating identity and residue. The residualsubspace would ideally contain intraperson variations caused by pose, lighting,and expression. In addition, they pointedout that optimal separation of identityand residue is classspecific. For example, the appearance change of a personsnose depends on its length, which is apersonspecific quantity. To correct thisclassspecific information, a sequence ofimages of the same class was used. Specifically, a linear mapping was assumed tocapture the relation between the classspecific correction to the identity subspace and the intraperson variation in theresidual subspace. Examples of face tracking and visual enhancement were demonstrated, but no recognition experimentswere reported. Though this method is believed to enhance tracking and make it robust against appearance change, it is notclear how efficient it is to learn the classspecific information from a video sequencethat does not present much residualvariation.In De Carlo and Metaxas 2000, a system called PersonSpotter was described.This system is able to capture, track,and recognize a person walking towardor passing a stereo CCD camera. It hasseveral modules, including a head tracker,preselector, landmark finder, and identifier. The head tracker determines the image regions that are changing due to objectmotion based on simple image differences.A stereo algorithm then determines thestereo disparities of these moving pixels.The disparity values are used to compute histograms for image regions. Regions within a certain disparity intervalare selected and referred to as silhouettes.Two types of detectors, skin color basedand convex region based, are applied tothese silhouette images. The outputs ofthese detectors are clustered to form regions of interest which usually correspondto heads. To track a head robustly, temporal continuity is exploited in the form ofACM Computing Surveys, Vol. 35, No. 4, December 2003.430 Zhao et al.the thresholds used to initiate, track, anddelete an object.To find the face region in an image, thepreselector uses a generic sparse graphconsisting of 16 nodes learned from eightexample face images. The landmark finderuses a dense graph consisting of 48 nodeslearned from 25 example images to findlandmarks such as the eyes and the nosetip. Finally, an elastic graph matchingscheme is employed to identify the face.A recognition rate of about 90 wasachieved the size of the database is notknown.A multimodal person recognition system was described in Choudhury et al.1999. This system consists of a facerecognition module, a speaker identification module, and a classifier fusion module. It has the following characteristics1 the face recognition module can detect and compensate for pose variationsthe speaker identification module can detect and compensate for changes in theauditory background 2 the most reliable video frames and audio clips are selected for recognition 3 3D informationabout the head obtained through SfM isused to detect the presence of an actualperson as opposed to an image of thatperson.Two key parts of the face recognitionmodule are face detectiontracking andeigenface recognition. The face is detected using skin color information usinga learned model of a mixture of Gaussians.The facial features are then located usingsymmetry transforms and image intensitygradients. Correlationbased methods areused to track the feature points. The locations of these feature points are used toestimate the pose of the face. This poseestimate and a 3D head model are usedto warp the detected face image into afrontal view. For recognition, the featurelocations are refined and the face is normalized with eyes and mouth in fixed locations. Images from the face tracker areused to train a frontal eigenspace, andthe leading 35 eigenvectors are retained.Face recognition is then performed usinga probabilistic eigenface approach wherethe projection coefficients of all images ofeach person are modeled as a Gaussiandistribution.Finally, the face and speaker recognition modules are combined using a Bayesnet. The system was tested in an ATMscenario, a controlled environment. AnATM session begins when the subject enters the cameras field of view and thesystem detects hisher face. The systemthen greets the user and begins the banking transaction, which involves a seriesof questions by the system and answersby the user. Data for 26 people were collected the normalized face images were40  80 pixels and the audio was sampled at 16 kHz. These experiments onsmall databases and wellcontrolled environments showed that the combinationof audio and video improved performance,and that 100 recognition and verificationwere achieved when the imageaudio clipswith highest confidence scores were used.In Li and Chellappa 2001, a face verification system based on tracking facialfeatures was presented. The basic idea ofthis approach is to exploit the temporalinformation available in a video sequenceto improve face recognition. First, the feature points defined by Gabor attributes ona regular 2D grid are tracked. Then, thetrajectories of these tracked feature pointsare exploited to identify the person presented in a short video sequence. The proposed trackingforverification scheme isdifferent from the pure tracking schemein that one template face from a databaseof known persons is selected for tracking. For each template with a specificpersonal ID, tracking can be performedand trajectories can be obtained. Basedon the characteristics of these trajectories, identification can be carried out. According to the authors, the trajectories ofthe same person are more coherent thanthose of different persons, as illustratedin Figure 15. Such characteristics can alsobe observed in the posterior probabilitiesover time by assuming different classes.In other words, the posterior probabilitiesfor the true hypothesis tend to be higherthan those for false hypotheses. This inturn can be used for identification. Testing results on a small databases of 19ACM Computing Surveys, Vol. 35, No. 4, December 2003.Face Recognition A Literature Survey 431Fig. 15. Corresponding feature points obtained from 20 frames a resultof matching the same person to a video, b result of matching a differentperson to the video, c trajectories of a, d trajectories of b Li andChellappa 2001.individuals have suggested that performance is favorable over a frametoframematching and voting scheme, especiallyin the case of large lighting changes. Thetesting result is based on comparison withalternative hypotheses.Some details about the tracking algorithm are as follows Li and Chellappa2001. The motion of facial feature pointsis modeled as a global twodimensional2D affine transformation accounting forhead motion plus a local deformation accounting for residual motion that is dueto inaccuracies in the 2D affine modeling and other factors such as facial expression. The tracking problem has beenformulated as a Bayesian inference problem and sequential importance samplingSIS Liu and Chen 1998 one form of SISis called Condensation Isard and Blake1996 in the computer vision literatureproposed as an empirical solution to theinference problem. Since SIS has difficultyin highdimensional spaces, a reparameterization that captures essentially onlythe difference was used to facilitate thecomputation.While most face recognition algorithmstake still images as probe inputs, a videobased face recognition approach that takesvideo sequences as inputs has recentlybeen developed Zhou et al. 2003. Sincethe detected face might be moving in thevideo sequence, one has to deal with uncertainty in tracking as well as in recognition.Rather than resolving these two uncertainties separately, Zhou et al. 2003 performed simultaneous tracking and recognition of human faces from a videosequence.In stilltovideo face recognition, wherethe gallery consists of still images, a timeseries state space model is proposed tofuse temporal information in a probevideo, which simultaneously characterizes the kinematics and identity using amotion vector and an identity variable,respectively. The joint posterior distribution of the motion vector and the identityvariable is first estimated at each timeinstant and then propagated to the nexttime instant. Marginalization over themotion vector yields a robust estimate ofthe posterior distribution of the identityvariable and marginalization over theidentity variable yields a robust estimateof the posterior distribution of the motionvector, so that tracking and recognitionare handled simultaneously. A computationally efficient sequential importancesampling SIS algorithm is used to estimate the posterior distribution. Empiricalresults demonstrate that, due to the propagation of the identity variable over time,degeneracy in the posterior probabilityof the identity variable is achieved togive improved recognition. The galleryis generalized to videos in order to realize videotovideo face recognition.An exemplarbased learning strategy isemployed to automatically select videorepresentatives from the gallery, servingas mixture centers in an updated likelihood measure. The SIS algorithm is usedto approximate the posterior distributionof the motion vector, the identity variable,and the exemplar index. The marginaldistribution of the identity variable produces the recognition result. The modelformulation is very general and allows aACM Computing Surveys, Vol. 35, No. 4, December 2003.432 Zhao et al.Fig. 16. Identity surface Li et al. 2001a.Courtesy of Y. Li, S. Gong, and H. Liddell.variety of image representations andtransformations. Experimental results using imagesvideos collectedat UMD, NISTUSF, and CMU withposeillumination variations have illustrated the effectiveness of this approachin both stilltovideo and videotovideoscenarios with appropriate model choices.In Li et al. 2001a, a multiview basedface recognition system was proposed torecognize faces from videos with largepose variations. To address the challenging pose issue, the concept of an identity surface that captures joint spatial andtemporal information was used. An identity surface is a hypersurface formed byprojecting all the images of one individual onto the discriminating feature spaceparameterized on head pose Figure 16.14To characterize the head pose, two angles, yaw and tilt, are used as basis coordinates in the feature space. As plotted inFigure 16, the other basis coordinates represent discriminating feature patterns offaces this will be discussed later. Based onrecovered pose information, a trajectoryof the input feature pattern can be constructed. The trajectories of features fromknown subjects arranged in the same temporal order can be synthesized on their respective identity surfaces. To recognize aface across views over time, the trajectoryfor the input face is matched to the trajectories synthesized for the known subjects.This approach can be thought of as a generalized version of face recognition basedon single images taken at different poses.14Notice that this viewbased idea has already beenexplored, for example, in Pentland et al. 1994.Experimental results using twelve training sequences, each containing one subject, and new testing sequences of thesesubjects were reported. Recognition rateswere 100 and 93.9, using 10 and 2 KDAkernel discriminant analysis vectors, respectively.Other techniques have also been usedto construct the discriminating basis inthe identity surface kernel discriminantanalysis KDA Mika et al. 1999 wasused to compute a nonlinear discriminating basis, and a dynamic face model isused to extract a shapeandposefree facial texture pattern. The multiview dynamic face model Li et al. 2001b consists of a sparse Point Distribution ModelPDM Cootes et al. 1995, a shapeandposefree texture model, and an affine geometrical model. The 3D shape vector ofa face is estimated from a set of 2D faceimages in different views using landmarkpoints. Then a face image fitted by theshape model is warped to the mean shapein a frontal view, yielding a shapeandposefree texture pattern.15 When part ofa face is invisible in an image due to rotation in depth, the facial texture is recovered from the visible side of the face usingthe bilateral symmetry of faces. To obtaina lowdimensional statistical model, PCAwas applied to the 3D shape patterns andshapeandposefree texture patterns separately. To further suppress withinclassvariations, the shapeandposefree texture patterns were further projected intoa KDA feature space. Finally, the identity surface can be approximated and constructed from discrete samples at fixedposes using a piecewise planar model.4.3. SummaryThe availability of videoimage sequencesgives videobased face recognition a distinct advantage over stillimagebasedface recognition the abundance of temporal information. However, the typicallylowquality images in video present asignificant challenge the loss of spatial15Notice that this procedure is very similar toAAM Cootes et al. 2001.ACM Computing Surveys, Vol. 35, No. 4, December 2003.Face Recognition A Literature Survey 433information. The key to building a successful videobased system is to use temporal information to compensate for the lostspatial information. For example, a highresolution frame can in principle be reconstructed from a sequence of lowresolutionvideo frames and used for recognition. Afurther step is to use the image sequenceto reconstruct the 3D shape of the trackedface object via SfM and thus enhance facerecognition performance. Finally, a comprehensive approach is to use spatial andtemporal information simultaneously forface recognition. This is also supported byrelated psychological studies.However, many issues remain for existing systemsSfM is a common technique used incomputer vision for recovering 3D information from video sequences. However, a major obstacle exists to applying this technique in face recognitionthe accuracy of 3D shape recovery. Faceimages contain smooth, textureless regions and are often acquired under varying illumination,16 resulting in significant difficulties in accurate recovery of3D information. The accuracy issue maynot be very important for face detection,but it is for face recognition, which mustdifferentiate the 3D shapes of similarobjects. One possible solution is the complementary use of shapefromshading,which can utilize the illumination information. A recent paper on using flowbased SfM techniques for face modelingis A. K. R. Chowdhury, and R. Chellappa2003.Up to now, the databases used inmany systems have been very small,say 20 subjects. This is partiallydue to the tremendous amount ofstorage space needed for video sequences. Fortunately, relatively largevideo databases exist, for example,the XM2TV database Messer et al.1999, the BANCA database BaillyBailliere et al. 2003, and the additionof video into the FERET and FRVT200216Stereo is less sensitive to illumination change butstill has difficulty in handling textureless regions.databases. However, largescale systematic evaluations are still lacking.Although we argue that it is best touse both temporal and spatial information for face recognition, existingspatiotemporal methods have not yetshown their full potential. We believethat these types of methods deserve further investigation.During the past 8 years, recognition ofhuman behavior has been actively studied facial expression recognition, handgesture recognition, activity recognition,etc. As pointed out earlier, descriptions ofhuman behavior are useful and are easier to obtain than recognition of faces. Often they provide complementary information for face recognition or additional cuesuseful for identification. In principle, bothgender classification and facial expressionrecognition can assist in the classificationof identity. For recent reviews on facialexpression recognition, see Donato et al.1999 and Pantic and Rothkrantz 2000.We also believe that analysis of body movements such as gait or hand gestures canhelp in person recognition.5. EVALUATION OF FACE RECOGNITIONSYSTEMSGiven the numerous theories and techniques that are applicable to face recognition, it is clear that evaluation and benchmarking of these algorithms is crucial.Previous work on the evaluation of OCRand fingerprint classification systems provided insights into how the evaluation ofalgorithms and systems can be performedefficiently. One of the most important factslearned in these evaluations is that largesets of test images are essential for adequate evaluation. It is also extremely important that the samples be statistically assimilar as possible to the images that arisein the application being considered. Scoring should be done in a way that reflectsthe costs of errors in recognition. Rejecterror behavior should be studied, not justforced recognition.In planning an evaluation, it is important to keep in mind that the operationACM Computing Surveys, Vol. 35, No. 4, December 2003.434 Zhao et al.of a pattern recognition system is statistical, with measurable distributions ofsuccess and failure. These distributionsare very applicationdependent, and notheory seems to exist that can predictthem for new applications. This stronglysuggests that an evaluation should bebased as closely as possible on a specificapplication.During the past 5 years, several large,publicly available face databases havebeen collected and corresponding testingprotocols have been designed. The series of FERET evaluations Phillips et al.2000b, 1998 Rizvi et al. 199817 attractednine institutions and companies to participate. They were succeeded by the seriesof FRVT vendor tests. We describe herethe most important face databases andtheir associated evaluation methods, including the XM2VTS and BANCA BaillyBailliere et al. 2003 database.5.1. The FERET ProtocolUntil recently, there did not exist a common FRT evaluation protocol that included large databases and standard evaluation methods. This made it difficult toassess the status of FRT for real applications, even though many existing systems reported almost perfect performanceon small databases.The first FERET evaluation test wasadministered in August 1994 Phillipset al. 1998b. This evaluation establisheda baseline for face recognition algorithms,and was designed to measure performanceof algorithms that could automatically locate, normalize, and identify faces. Thisevaluation consisted of three tests, eachwith a different gallery and probe set. Agallery is a set of known individuals, whilea probe is a set of unknown faces presented for recognition. The first test measured identification performance from agallery of 316 individuals with one image per person the second was a falsealarm test and the third measured the effects of pose changes on performance. Thesecond FERET evaluation was adminis17httpwww.itl.nist.goviadhumanidferet.tered in March 1995 it consisted of a single test that measured identification performance from a gallery of 817 individuals, and included 463 duplicates in theprobe set Phillips et al. 1998b. A duplicate is a probe for which the correspondinggallery image was taken on a different daythere were only 60 duplicates in the Aug94evaluation. The third and last evaluationSep96 was administered in September1996 and March 1997.5.1.1. Database. Currently, the FERETdatabase is the only large database that isgenerally available to researchers withoutcharge. The images in the database wereinitially acquired with a 35mm cameraand then digitized.The images were collected in 15 sessionsbetween August 1993 and July 1996. Eachsession lasted 1 or 2 days, and the locationand setup did not change during the session. Sets of 5 to 11 images of each individual were acquired under relatively unconstrained conditions see Figure 17. Theyincluded two frontal views in the first ofthese fa a neutral facial expression wasrequested and in the second fb a different facial expression was requested theserequests were not always honored. For200 individuals, a third frontal view wastaken using a different camera and different lighting this is referred to as the fcimage. The remaining images were nonfrontal and included right and left profiles,right and left quarter profiles, and rightand left half profiles. The FERET databaseconsists of 1564 sets of images 1199 original sets and 365 duplicate setsa total of 14,126 images. A development setof 503 sets of images were released toresearchers the remaining images weresequestered for independent evaluation.In late 2000 the entire FERET databasewas released along with the Sep96 evaluation protocols, evaluation scoring code,and baseline PCA algorithms.5.1.2. Evaluation. For details of the threeFERET evaluations, see Phillips et al.2000, 1998b and Rizvi et al. 1998.The results of the most recent FERETACM Computing Surveys, Vol. 35, No. 4, December 2003.Face Recognition A Literature Survey 435Fig. 17. Images from the FERET dataset these images are of size 384256.evaluation Sep96 will be briefly reviewedhere. Because the entire FERET data sethas been released, the Sep96 protocol provides a good benchmark for performance ofnew algorithms. For the Sep96 evaluation,there was a primary gallery consisting ofone frontal image fa per person for 1196individuals. This was the core gallery usedto measure performance for the followingfour different probe setsfb probesgallery and probe images ofan individual taken on the same daywith the same lighting 1195 probesfc probesgallery and probe images ofan individual taken on the same daywith different lighting 194 probesDup I probesgallery and probe images of an individual taken on differentdaysduplicate images 722 probesandDup II probesgallery and probe images of an individual taken over a yearapart the gallery consisted of 894 images 234 probes.Performance was measured using twobasic methods. The first measured identification performance, where the primaryperformance statistic is the percentageof probes that are correctly identified bythe algorithm. The second measured verification performance, where the primaryperformance measure is the equal errorrate between the probability of false alarmand the probability of correct verification.A more complete method of reportingidentification performance is a cumulativematch characteristic for verification performance it is a receiver operating characteristic ROC.The Sep96 evaluation tested the following 10 algorithmsan algorithm from Excalibur Corporation Carlsbad, CASept. 1996two algorithms from MIT Media Laboratory Sept. 1996 Moghaddam et al.1996 Turk and Pentland 1991three linear discriminant analysisbased algorithms from Michigan StateUniversity Swets and Weng 1996bSept. 1996 and the University of Maryland Etemad and Chellappa 1997 Zhaoet al. 1998 Sept. 1996 and March1997a grayscale projection algorithm fromRutgers University Wilder 1994 Sept.1996an Elastic Graph Matching algorithmfrom the University of Southern California Okada et al. 1998 Wiskott et al.1997 March 1997a baseline PCA algorithm Moon andPhillips 2001 Turk and Pentland 1991anda baseline normalized correlationmatching algorithm.Three of the algorithms performedvery well probabilistic eigenface fromMIT Moghaddam et al. 1996, subspace LDA from UMD Zhao et al. 1998,1999, and Elastic Graph Matching fromUSC Wiskott et al. 1997.A number of lessons were learned fromthe FERET evaluations. The first is thatperformance depends on the probe category and there is a difference between bestand average algorithm performance.Another lesson is that the scenariohas an impact on performance. ForACM Computing Surveys, Vol. 35, No. 4, December 2003.436 Zhao et al.identification, on the fb and duplicateprobes, the USC scores were 94 and 59,and the UMD scores were 96 and 47.However, for verification, the equal errorrates were 2 and 14 for USC, and 1and 12 for UMD.5.1.3. Summary. The availability of theFERET database and evaluation technology has had a significant impact onprogress in the development of face recognition algorithms. The series of testshas allowed advances in algorithm development to be quantifiedfor example, the performance improvements in theMIT algorithms between March 1995 andSeptember 1996, and in the UMD algorithms between September 1996 andMarch 1997.Another important contribution of theFERET evaluations is the identification ofareas for future research. In general thetest results revealed three major problemareas recognizing duplicates, recognizingpeople under illumination variations, andrecognizing them under pose variations.5.1.4. FRVT 2000. The Sep96 FERETevaluation measured performance on prototype laboratory systems. After March1997 there was rapid advancement in thedevelopment of commercial face recognition systems. This advancement represented both a maturing of face recognitiontechnology, and the development of thesupporting system and infrastructure necessary to create commercial offtheshelfCOTS systems. By the beginning of 2000,COTS face recognition systems were readily available.To assess the state of the art in COTSface recognition systems the Face Recognition Vendor Test FRVT 200018 was organized Blackburn et al. 2001. FRVT 2000was a technology evaluation that used theSep96 evaluation protocol, but was significantly more demanding than the Sep96FERET evaluation.Participation in FRVT 2000 was restricted to COTS systems, with companies18httpwww.frvt.org.from Australia, Germany, and the UnitedStates participating. The five companiesevaluated were BanqueTec InternationalPty. Ltd., CVIS Computer Vision und Automation GmbH, Miros, Inc., Lau Technologies, and Visionics Corporation.A greater variety of imagery was usedin FRVT 2000 than in the FERET evaluations. FRVT 2000 reported results in eightgeneral categories compression, distance,expression, illumination, media, pose, resolution, and temporal. There was no common gallery across all eight categories thesizes of the galleries and probe sets variedfrom category to category.We briefly summarize the results ofFRVT 2000. Full details can be found inBlackburn et al. 2001, and include identification and verification performancestatistics. The media experiments showedthat changes in media do not adverselyaffect performance. Images of a personwere taken simultaneously on conventional film and on digital media. Thecompression experiments showed thatcompression does not adversely affect performance. Probe images compressed up to401 did not reduce recognition rates. Thecompression algorithm was JPEG.FRVT 2000 also examined the effect ofpose angle on performance. The resultsshow that pose does not significantly affectperformance up to 25, but that performance is significantly affected when thepose angle reaches 40.In the illumination category, two keyeffects were investigated. The first waslighting change indoors. This was equivalent to the fc probes in FERET. For thebest system in this category, the indoorchange of lighting did not significantlyaffect performance. A second experimenttested recognition with an indoor galleryand an outdoor probe set. Moving from indoor to outdoor lighting significantly affected performance, with the best systemachieving an identification rate of only0.55.The temporal category is equivalent tothe duplicate probes in FERET. To compare progress since FERET, dup I anddup II scores were reported. For FRVT2000 the dup I identification rate was 0.63ACM Computing Surveys, Vol. 35, No. 4, December 2003.Face Recognition A Literature Survey 437compared with 0.58 for FERET. The corresponding rates for dup II were 0.64 forFRVT 2000 and 0.52 for FERET. These results showed that there was algorithmicprogress between the FERET and FRVT2000 evaluations. FRVT 2000 showed thattwo common concerns, the effects of compression and recording media, do not affect performance. It also showed thatfuture areas of interest continue to be duplicates, pose variations, and illuminationvariations generated when comparing indoor images with outdoor images.5.1.5. FRVT 2002. The Face Recognition Vendor Test FRVT 2002 Phillipset al. 200318 was a largescale evaluation of automatic face recognition technology. The primary objective of FRVT 2002was to provide performance measures forassessing the ability of automatic facerecognition systems to meet realworld requirements. Ten participants were evaluated under the direct supervision of theFRVT 2002 organizers in July and August2002.The heart of the FRVT 2002 was thehigh computational intensity test HCInt.The HCInt consisted of 121,589 operational images of 37,437 people. The images were provided from the U.S. Department of States Mexican nonimmigrantVisa archive. From this data, realworldperformance figures on a very large dataset were computed. Performance statisticswere computed for verification, identification, and watch list tasks.FRVT 2002 results showed that normal changes in indoor lighting do not significantly affect performance of the topsystems. Approximately the same performance results were obtained using two indoor data sets, with different lighting, inFRVT 2002. In both experiments, the bestperformer had a 90 verification rate ata false accept rate of 1. On comparableexperiments conducted 2 years earlier inFRVT 2000, the results of FRVT 2002 indicated that there has been a 50 reductionin error rates. For the best face recognition systems, the recognition rate for facescaptured outdoors, at a false accept rate of1, was only 50. Thus, face recognitionfrom outdoor imagery remains a researchchallenge area.A very important question for realworld applications is the rate of decreasein performance as time increases betweenthe acquisition of the database of imagesand new images presented to a system.FRVT 2002 found that for the top systems,performance degraded at approximately5 per year.One open question in face recognitionis how does database and watch list sizeeffect performance Because of the largenumber of people and images in the FRVT2002 data set, FRVT 2002 reported thefirst largescale results on this question.For the best system, the toprank identification rate was 85 on a database of800 people, 83 on a database of 1,600,and 73 on a database of 37,437. For every doubling of database size, performancedecreases by two to three overall percentage points. More generally, identificationperformance decreases linearly in the logarithm of the database size.Previous evaluations have reported facerecognition performance as a function ofimaging properties. For example, previousreports compared the differences in performance when using indoor versus outdoorimages, or frontal versus nonfrontal images. FRVT 2002, for the first time, examined the effects of demographics on performance. Two major effects were found.First, recognition rates for males werehigher than females. For the top systems,identification rates for males were 6 to9 points higher than that of females.For the best system, identification performance on males was 78 and for females it was 79. Second, recognitionrates for older people were higher thanfor younger people. For 18 to 22yearoldsthe average identification rate for the topsystems was 62, and for 38 to 42yearolds it was 74. For every 10year increase in age, performance increased onthe average by approximately 5 throughage 63.FRVT 2002 looked at two of thesenew techniques. The first was the threedimensional morphable models techniqueACM Computing Surveys, Vol. 35, No. 4, December 2003.438 Zhao et al.of Blanz and Vetter 1999. Morphablemodels are a technique for improvingrecognition of nonfrontal images. FRVT2002 found that Blanz and Vetters technique significantly increased recognitionperformance. The second technique isrecognition from video sequences. UsingFRVT 2002 data, recognition performanceusing video sequences was the same as theperformance using still images.In summary, the key lessons learnedin FRVT 2002 were 1 given reasonable controlled indoor lighting, the current state of the art in face recognitionis 90 verification at a 1 false acceptrate. 2 Face Recognition in outdoor images is a research problem. 3 The useof morphable models can significantly improve nonfrontal face recognition. 3 Identification performance decreases linearlyin the logarithm of the size of the gallery.4 In face recognition applications, accommodations should be made for demographic information since characteristicssuch as age and sex can significantly affectperformance.5.2. The XM2VTS ProtocolMultimodal methods19 are a very promising approach to userfriendly hence acceptable, highly secure personal verification. Recognition and verification systemsneed training the larger the training set,the better the performance achieved. Thevolume of data required for training a multimodal system based on analysis of videoand audio signals is on the order of TBytestechnology that allows manipulation andeffective use of such volumes of data hasonly recently become available in the formof digital video. The XM2VTS multimodaldatabase Messer et al. 1999 contains fourrecordings of 295 subjects taken over a period of 4 months. Each recording containsa speaking head shot and a rotating headshot. Available data from this databaseinclude highquality color images, 32kHz16bit sound files, video sequences, and a3D model.19httpwww.ee.surrey.ac.ukResearchVSSPxm2vtsdb.The XM2VTS database is an expansion of the earlier M2VTS databasePigeon and Vandendorpe 1999. TheM2VTS project Multimodal Verificationfor Teleservices and Security Applications, a European ACTS Advanced Communications Technologies and Servicesproject, deals with access control by multimodal identification of human faces.The goal of the project was to improverecognition performance by combining themodalities of face and voice. The M2VTSdatabase contained five shots of each of37 subjects. During each shot, the subjectswere asked to count from 0 to 9 in theirnative language most of the subjects wereFrenchspeaking and rotate their headsfrom 0 to 90, back to 0, and then to90. They were then asked to rotate theirheads again with their glasses off, if theywore any. Three subsequences were extracted from these video sequences voicesequences, motion sequences, and glassesoff motion sequences. The voice sequencescan be used for speech verification, frontalview face recognition, and speechlips correlation analysis. The other two sequencesare intended for face recognition only.It was found that the subjects were relatively difficult to recognize in the fifthshot because it varied significantly infacevoicecamera setup from the othershots. Several experiments have been conducted using the first four shots with thegoals of investigatingtextdependent speaker verificationfrom speech,textindependent speaker verificationfrom speech,facial feature extraction and trackingfrom moving images,verification from an overall frontal view,verification from lip shape,verification from depth information obtained using structured light,verification from a profile, andsynchronization of speech and lip movement.5.2.1. Database. The XM2VTS databasediffers from the M2VTS databaseACM Computing Surveys, Vol. 35, No. 4, December 2003.Face Recognition A Literature Survey 439primarily in the number of subjects 295rather than 37. The M2VTS databasecontains five shots of each subject takenat sessions over a period of 3 months theXM2VTS database contains eight shots ofeach subject taken at four sessions over aperiod of 4 months so that each sessioncontains two repetitions of the sequence.The XM2VTS database was acquiredusing a Sony VX1000E digital camcorderand a DHR1000UX digital VCR.In the XM2VTS database, the first shotis a speaking head shot. Each subject, whowore a clipon microphone, was asked toread three sentences that were writtenon a board positioned just below the camera. The subjects were asked to read thethree simple sentences twice at their normal pace and to pause briefly at the end ofeach sentence.The second shot is a rotating head sequence. Each subject was asked to rotate hisher head to the left, to the right,up, and down, and finally to return tothe center. The subjects were told thata full profile was required and wereasked to repeat the entire sequence twice.The same sequence was used in all foursessions.An additional dataset containing a 3Dmodel of each subjects head was acquiredduring each session using a highprecisionstereobased 3D camera developed by theTuring Institute.205.2.2. Evaluation. The M2VTS Lausanne protocol was designed to evaluatethe performance of vision and speechbased person authentication systems onthe XM2VTS database. This protocol wasdefined for the task of verification. Thefeatures of the observed person are compared with stored features correspondingto the claimed identity, and the systemdecides whether the identity claim is trueor false on the basis of a similarity score.The subjects whose features are stored inthe systems database are called clients,whereas persons claiming a false identityare called imposters.20Turing Institute Web address httpwww.turing.gla.ac.uk.The database is divided into three partsa training set, an evaluation set, and a testset. The training set is used to build clientmodels. The evaluation set is used to compute client and imposter scores. On thebasis of these scores, a threshold is chosen that determines whether a person isaccepted or rejected. In multimodal classification, the evaluation set can also beused to optimally combine the outputs ofseveral classifiers. The test set is selectedto simulate a real authentication scenario.295 subjects were randomly divided into200 clients, 25 evaluation imposters, and70 test imposters. Two different evaluation configurations were used with different distributions of client training andclient evaluation data. For more details,see Messer et al. 1999.In order to collect face verification results on this database using the Lausanne protocol, a contest was organizedin conjuction with ICPR 2000 the International Conference on Pattern Recognition. There were twelve algorithms fromfour partipicants in this contest Mataset al. 2000 an EBGM algorithm fromIDIAP Daller Molle Institute for Perceptual Artificial Intelligence, a slightlymodified EBGM algorithm from Aristotle University of Thessaloniki, a FNDbased Fractal Neighbor Distance algorithm from the University of Sydney, andeight variants of LDA algorithms and oneSVM algorithm from the University ofSurrey. The performance measures of averification system are the false acceptance rate FA and the false rejection rateFR. Both FA and FR are influenced byan acceptance threshold. According to theLausanne protocol, the threshold is set tosatisfy certain performance levels on theevaluation set. The same threshold is applied to the test data and FA and FR onthe test data are computed. The best results of FA and FR on the test data FAFR2.32.5 and 1.21.0 for evaulationconfigurations I and II, respectively wereobtained using an LDA algorithm with anonEuclidean metric University of Surrey when the threshold was set so thatFA was equal to FB on the evaulation result. This result seems to concur with theACM Computing Surveys, Vol. 35, No. 4, December 2003.440 Zhao et al.equal error rates reported in the FERETprotocol. In addition, FA and FR on thetest data were reported when the threshold was set set so that FA or FB was zeroon the evaulation result. For more detailson the results, see Matas et al. 2000.5.2.3. Summary. The results of theM2VTSXM2VTS projects can be usedfor a broad range of applications. Inthe telecommunication field, the resultsshould have a direct impact on networkservices where security of informationand access will become increasinglyimportant. Telephone fraud in the U.S.has been estimated to cost several billiondollars a year.6. TWO ISSUES IN FACE RECOGNITIONILLUMINATION AND POSE VARIATIONIn this section, we discuss two importantissues that are related to face recognition. The best face recognition techniquesreviewed in Section 3 were successful interms of their recognition performance onlarge databases in wellcontrolled environments. However, face recognition inan uncontrolled environment is still verychallenging. For example, the FERETevaluations and FRVTs revealed thatthere are at least two major challengesthe illumination variation problem andthe pose variation problem. Though manyexisting systems build in some sort ofperformance invariance by applying preprocessing methods such as histogramequalization or pose learning, significantillumination or pose change can cause serious performance degradation. In addition, face images can be partially occluded,or the system may need to recognize a person from an image in the database thatwas acquired some time ago referred toas the duplicate problem in the FERETtests.These problems are unavoidable whenface images are acquired in an uncontrolled, uncooperative environment, as insurveillance video clips. It is beyond thescope of this paper to discuss all these issues and possible solutions. In this sectionwe discuss only two welldefined problemsand review approaches to solving them.Pros and cons of these approaches arepointed out so an appropriate approachcan be applied to a specific task. The majority of the methods reviewed here aregenerative approaches that can synthesize virtual views under desired illumination and viewing conditions. Many ofthe reviewed methods have not yet beenapplied to the task of face recognition,at least not on large databases.21 Thismay be for several reasons some methods may need many sample images perperson, pixelwise accurate alignment ofimages, or highquality images for reconstruction or they may be computationallytoo expensive to apply to recognition tasksthat process thousands of images in nearrealtime.To facilitate discussion and analysis,we adopt a varyingalbedo Lambertian reflectance model that relates the image Iof an object to the object p, q Horn andBrooks 1989I   1  pPs  qQs1  p2  q21  P2s  Q2s , 6where p, q,  are the partial derivativesand varying albedo of the object, respectively. Ps, Qs, 1 represents a single distant light source. The light source can alsobe represented by the illuminant slant andtilt angles slant  is the angle betweenthe opposite lighting direction and the positive zaxis, and tilt  is the angle betweenthe opposite lighting direction and the xzplane. These angles are related to Ps andQs by Ps  tan  cos  , Qs  tan  sin  .To simplify the notation, we replace theconstant1  P2s  Q2s by K . For easieranalysis, we assume that frontal face objects are bilaterally symmetric about thevertical midlines of the faces.21One exception is a recent report Blanz and Vetter2003 where faces were represented using 4448 images from the CMUPIE databases and 1940 imagesfrom the FERET database.ACM Computing Surveys, Vol. 35, No. 4, December 2003.Face Recognition A Literature Survey 441Fig. 18. In each row, the same face appears differently under different illuminations from the Yale face database.6.1. The Illumination Problem in FaceRecognitionThe illumination problem is illustratedin Figure 18, where the same face appears different due to a change in lighting. The changes induced by illuminationare often larger than the differences between individuals, causing systems basedon comparing images to misclassify inputimages. This was experimentally observedin Adini et al. 1997 using a dataset of 25individuals.In Zhao 1999, an analysis was carriedout of how illumination variation changesthe eigensubspace projection coefficientsof images under the assumption of a Lambertian surface. Consider the basic expression for the subspace decomposition of aface image I  I  IA mi1 aii, where IAis the average image, i are the eigenimages, and ai are the projection coefficients.Assume that for a particular individual wehave a prototype image Ip that is a normally lighted frontal view Ps  0, Qs  0in Equation 6 in the database, and wewant to match it against a new image I ofthe same class under lighting Ps, Qs, 1.The corresponding subspace projection coefficient vectors a  a1, a2, . . . , amT forIp and a  a1, a2, . . . , amT for I  arecomputed as followsai  Ip  i  IA  i,ai  I  i  IA  i,7where  denotes the sum of all elementwise products of two matrices vectors. Ifwe divide the images and the eigenimagesinto two halves, for example, left and right,we haveai  I Lp  Li  I Rp  Ri  IA  i,ai  I L  Li  I R  Ri  IA  i.8Based on Equation 6, the symmetricproperty of eigenimages and face objects,we haveai  2I Lp x, y  Li x, y  IA  i,ai 2KI Lp x, y  I Lp x, yqLx, yQs Li x, y  IA  i,9leading to the following relationa 1Ka QsKf a1 , fa2 , . . . , famT10 K  1KaA.,where f ai  2I Lp x, yqLx, yLi x, yand aA is the projection coefficient vectorof the average image IA IA 1, . . . , IA m. Now let us assume that the trainingset is extended to include mirror imagesas in Kirby and Sirovich 1990. A similaranalysis can be carried out, since in such acase the eigenimages are either symmetric for most leading eigenimages or antisymmetric.In general, Equation 11 suggests thata significant illumination change canseriously degrade the performance ofACM Computing Surveys, Vol. 35, No. 4, December 2003.442 Zhao et al.Fig. 19. Changes of projection vectors due to class variation a and illumination change b is of thesame order Zhao 1999.subspacebased methods. Figure 19 plotsthe projection coefficients for the sameface under different illuminations  00, 400,   00, 1800 and comparesthem against the variations in the projection coefficient vectors due to pure differences in class.In general, the illumination problem isquite difficult and has received considerable attention in the image understanding literature. In the case of face recognition, many approaches to this problemhave been proposed that make use of thedomain knowledge that all faces belong toone face class. These approaches can bedivided into four types Zhao 1999 1heuristic methods, for example, discardingthe leading principal components 2 image comparison methods in which appropriate image representations and distancemeasures are used 3 classbased methods using multiple images of the sameface in a fixed pose but under differentlighting conditions and 4 modelbasedapproaches in which 3D models are employed.6.1.1. Heuristic Approaches. Many existing systems use heuristic methods to compensate for lighting changes. For example, in Moghaddam and Pentland 1997simple contrast normalization was usedto preprocess the detected faces, whilein Sung and Poggio 1997 normalizationin intensity was done by first subtracting a bestfit brightness plane and thenapplying histogram equalization. In theface eigensubspace domain, it was suggested and later experimentally verified inBelhumeur et al. 1997 that by discarding a few most significant principal components, variations due to lighting canbe reduced. The plot in Figure 19b alsosupports this observation. However, in order to maintain system performance fornormally illuminated images, while improving performance for images acquiredunder changes in illumination, it mustbe assumed that the first three principalcomponents capture only variations due tolighting. Other heuristic methods basedon frontalface symmetry have also beenproposed Zhao 1999.6.1.2. Image Comparison Approaches.In Adini et al. 1997, approaches basedon image comparison using differentimage representations and distancemeasures were evaluated. The imagerepresentations used were edge maps,derivatives of the gray level, imagesfiltered with 2D Gaborlike functions,and a representation that combines alog function of the intensity with theserepresentations. The distance measuresused were pointwise distance, regionaldistance, affineGL gray level distance, local affineGL distance, and logACM Computing Surveys, Vol. 35, No. 4, December 2003.Face Recognition A Literature Survey 443pointwise distance. For more detailsabout these methods and about the evaluation database, see Adini et al. 1997.It was concluded that none of theserepresentations alone can overcome theimage variations due to illumination.A recently proposed image comparisonmethod Jacobs et al. 1998 used a newmeasure robust to illumination change.The rationale for develop such a methodof directly comparing images is the potential difficulty of building a complete representation of an objects possible images assuggested in Belhumeur and Kriegman1997. The authors argued that it is notclear whether it is possible to constructthe complete representation using a smallnumber of training images taken under uncontrolled viewing conditions andcontaining multiple light sources. It wasshown that given two images of an object with unknown structure and albedo,there is always a large family of solutions.Even in the case of given light sources,only two out of three independent components of the Hessian of the surface can bedetermined. Instead, the authors arguedthat the ratio of two images of the sameobject is simpler than if the images arefrom different objects. Based on this observation, the complexity of the ratio oftwo aligned images was proposed as thesimilarity measure. More specifically, wehaveI1I2K2K1 1  pI Ps,1  qI Qs,11  pI Ps,2  qI Qs,211for images of the same object, andI1J2K2K1IJ1  pI Ps,1  qI Qs,11  pJ Ps,2  qJ Qs,2121  p2J  q2J1  p2I  q2Ifor images of different objects. They chosethe integral of the magnitude of thegradient of the function ratio imageas the measure of complexity and proposed the following symmetric similaritymeasuredGI, J   minI, J  IJ 13 JI dx dy.They noticed the similarity between thismeasure and the measure that simplycompares the edges. It is also clear thatthe measure is not strictly illuminationinvariant because it changes for a pairof images of the same object when theillumination changes. Experiments onface recognition showed improved performance over eigenfaces, which were somewhat worse than the illumination conebased method Georghiades et al. 1998 onthe same set of data.6.1.3. ClassBased Approaches. Underthe assumptions of Lambertian surfaces and no shadowing, a 3D linearillumination subspace for a personwas constructed in Belhumeur andKriegman 1997, Hallinan 1994,Murase and Nayar 1995, RicklinRaviv and Shashua 1999, and Shashua1994 for a fixed viewpoint, using threealigned facesimages acquired underdifferent lighting conditions. Underideal assumptions, recognition based onthis subspace is illuminationinvariant.More recently, an illumination cone hasbeen proposed as an effective methodof handling illumination variations, including shadowing and multiple lightsources Belhumeur and Kriegman 1997Georghiades et al. 1998. This method isan extension of the 3D linear subspacemethod Hallinan 1994 Shashua 1994and has the same drawback, requiringat least three aligned training imagesacquired under different lighting conditions per person. A more detailedreview of this approach and its extensionto handle the combined illuminationand pose problem will be presented inSection 6.2.ACM Computing Surveys, Vol. 35, No. 4, December 2003.444 Zhao et al.Fig. 20. Testing the invariance of the quotient image Qimage to varying illumination. a Original images of a novel face taken under five different illuminations. b The Qimages corresponding to the novel images, computed with respectto the bootstrap set of ten objects RiklinRaviv and Shashua 1999. Courtesy ofT. RiklinRaviv and A. Shashua.More recently, a method based on quotient images was introduced RiklinRavivand Shashua 1999. Like other classbasedmethods, this method assumes that thefaces of different individuals have thesame shape and different textures. Giventwo objects a, b, the quotient image Qis defined to be the ratio of their albedofunctions ab, and hence is illuminationinvariant. Once Q is computed, the entire illumination space of object a can begenerated by Q and a linear illuminationsubspace constructed from three imagesof object b. To make this basic idea workin practice, a training set called the bootstrap set in the paper is needed that consists of images of N objects under variouslighting conditions, and the quotient image of a novel object y is defined relativeto the average object of the bootstrap set.More specifically, the bootstrap set consists of 3N images taken from three fixed,linearly independent light sources s1, s2,and s3 that are not known. Under this assumption, any light source s can be expressed as a linear combination of the sis  x1s1  x2s2  x3s3. The authors further defined the normalized albedo function  of the bootstrap set as the squaredsum of the i, where i is the albedo function of object i. An interesting energycostfunction is defined that is quite different from the traditional bilinear form. LetA1, A2, . . . , AN be m  3 matrices whosecolumns are images of object i from thebootstrap set that contain the same mpixels then the bilinear energycost function Freeman and Tenenbaum 2000 foran image ys of object y under illuminations is ys Ni1i Aix2, 14which is a bilinear problem in the N unknowns i and 3 unknowns x. For comparison, the proposed energy function isNi1i ys  Aix2. 15This formation of the energy function isa major reason why the quotient imagemethod works better than reconstruction methods based on Equation 14 interms of smaller size of the bootstrap setand less requirement for pixelwise imagealignment. As pointed out by the authors,another factor contributing to the successof using only a small bootstrap set is thatthe albedo functions occupy only a smallsubspace. Figure 20 demonstrates the invariance of the quotient image againstchange in illumination conditions theimage synthesis results are shown inFigure 21.6.1.4. ModelBased Approaches. Inmodelbased approaches, a 3D face modelis used to synthesize the virtual imagefrom a given image under desired illumination conditions. When the 3D modelis unknown, recovering the shape fromACM Computing Surveys, Vol. 35, No. 4, December 2003.Face Recognition A Literature Survey 445Fig. 21. Image synthesis example. Original image a and its quotient image bfrom the N  10 bootstrap set. The quotient image is generated relative to theaverage object of the bootstrap set, shown in c, d, and e. Images f through kare synthetic images created from b and c, d, e RiklinRaviv and Shashua1999. Courtesy of T. RiklinRaviv and A. Shashua.the images accurately is difficult withoutusing any priors. ShapefromshadingSFS can be used if only one image isavailable stereo or structure from motioncan be used when multiple images of thesame object are available.Fortunately, for face recognition the differences in the 3D shapes of different faceobjects are not dramatic. This is especiallytrue after the images are aligned and normalized. Recall that this assumption wasused in the classbased methods reviewedabove. Using a statistical representationof the 3D heads, PCA was suggested as atool for solving the parametric SFS problem Atick et al. 1996. An eigenhead approximation of a 3D head was obtainedafter training on about 300 laserscannedrange images of real human heads. Theillposed SFS problem is thereby transformed into a parametric problem. The authors also demonstrated that such a representation helps to determine the lightsource. For a new face image, its 3D headcan be approximated as a linear combination of eigenheads and then used to determine the light source. Using this complete 3D model, any virtual view of the faceimage can be generated. A major drawback of this approach is the assumptionof constant albedo. This assumption doesnot hold for most real face images, eventhough it is the most common assumptionused in SFS algorithms.To address the issue of varying albedo,a direct 2Dto2D approach was proposedbased on the assumption that frontviewfaces are symmetric and making use of ageneric 3D model Zhao et al. 1999. Recallthat a prototype image Ip is a frontal viewwith Ps  0, Qs  0. Substituting this intoEquation 6, we haveIpx, y   11  p2  q2. 16Comparing Equations 6 and 16, we obtainIpx, y  K21  qQs I x, y  I x, y.17This simple equation relates the prototype image Ip to I x, y  I x,  y, whichis already available. The two advantagesof this approach are 1 there is no needto recover the varying albedo x, y 2there is no need to recover the full shapegradients p, q q can be approximatedby a value derived from a generic 3Dshape. As part of the proposed automaticmethod, a modelbased light source identification method was also proposed toimprove existing sourcefromshading algorithms. Figure 22 shows some comparisons between rendered images obtained using this method and using aACM Computing Surveys, Vol. 35, No. 4, December 2003.446 Zhao et al.Fig. 22. Image rendering comparison. The original images are shownin the first column. The second column shows prototype images renderedusing the local SFS algorithm Tsai and Shah 1994. Prototype imagesrendered using symmetric SFS are shown in the third column. Finally,the fourth column shows real images that are close to the prototypeimages Zhao and Chellappa 2000.local SFS algorithm Tsai and Shah 1994.Using the Yale and Weizmann databasesTable V, significant performance improvements were reported when the prototype images were used in a subspaceLDA system in place of the original input images Zhao et al. 1999. In these experiments, the gallery set contained about500 images from various databases andthe probe set contained 60 images fromthe Yale database and 96 images from theWeizmann database.Recently, a general method of approximating Lambertian reflectance using secondorder spherical harmonics hasbeen reported Basri and Jacobs 2001.Assuming Lambertian objects under distant, isotropic lightng, the authors wereable to show that the set of all reflectance functions can be approximatedusing the surface spherical harmonic expansion. Specifically, they have provedthat using a secondorder nine harmonics, i.e., ninedimensional 9Dspace approximation, the accuracy for any lightfunction exceeds 97.97. They then extended this analysis to image formation,which is a much more difficult problem dueto possible occlusion, shape, and albedovariations. As indicated by the authors,worstcase image approximation can bearbitrarily bad, but most cases are good.Using their method, an image can be decomposed into socalled harmonic images,which are produced when the object isilluminated by harmonic functions. Thenine harmonic images of a face are plottedin Figure 23. An interesting comparisonwas made between the proposed methodand the 3D linear illumination subspacemethods Hallinan 1994 Shashua 1994the 3D linear methods are just firstorderharmonic approximations without the DCcomponents.Assuming precomputed object pose andknown color albedotexture, the authorsreported an 86 correct recognition ratewhen applying this technique to the taskof face recognition using a probe setof 10 people and a gallery set of 42people.6.2. The Pose Problem in Face RecognitionIt is not surprising that the performanceof face recognition systems drops significantly when large pose variations arepresent, in the input images. This difficulty was documented in the FERET andFRVT test reports Blackburn et al. 2001Phillips et al. 2002b, 2003, and was suggested as a major research issue. When illumination variation is also present, thetask of face recognition becomes even moredifficult. Here we focus on the outofplanerotation problem, since inplane rotationis a pure 2D problem and can be solvedmuch more easily.ACM Computing Surveys, Vol. 35, No. 4, December 2003.Face Recognition A Literature Survey 447Table V. Internet Resources for Research and DatabasesResearch pointersFace recognition homepage www.cs.rug.nlpeterkrFACEfrhp.htmlFace detection homepage home.tonline.dehomeRobert.Frischholzface.htmFacial analysis homepage mambo.ucsc.edupslfanl.htmlFacial animiation homepage mambo.ucsc.edupslfan.htmlFace databasesFERET database httpwww.itl.nist.goviadhumanidferetXM2TVS database httpwww.ee.surrey.ac.ukResearchVSSPxm2vtsdbUT Dallas database httpwww.utdallas.edudeptbbsFACULTY PAGESotooledatabase.htmNotre Dame database httpwww.nd.educvrlHIDdata.htmlMIT face databases ftpwhitechapel.media.mit.edupubimagesShimon Edelmans face database ftpftp.wisdom.weizmann.ac.ilpubFaceBaseCMU face detection database www.ius.cs.cmu.eduIUSdylan usr0harfacestestCMU PIE database www.ri.cmu.eduprojectsproject 418.htmlStirling face database pics.psych.stir.ac.ukM2VTS multimodal database www.tele.ucl.ac.beM2VTSYale face database cvc.yale.eduprojectsyalefacesyalefaces.htmlYale face database B cvc.yale.eduprojectsyalefacesByalefacesB.htmlHarvard face database hrl.harvard.edupubfacesWeizmann face database www.wisdom.weizmann.ac.ilyaelUMIST face database images.ee.umist.ac.ukdannydatabase.htmlPurdue University face database rvl1.ecn.purdue.edualeixaleix face DB.htmlOlivetti face database www.camorl.co.ukfacedatabase.htmlOulu physicsbased face database www.ee.oulu.firesearchimagcolorpbfd.htmlFig. 23. The first nine harmonic images of a faceobject from left to right, top to bottom Basri andJacobs 2001. Courtesy of R. Basri and D. Jacobs.Earlier methods focused on constructinginvariant features Wiskott et al. 1997 orsynthesizing a prototypical view frontalview after a full model is extracted fromthe input image Lanitis et al. 1995.22Such methods work well for small rotation angles, but they fail when the angleis large, say 60, causing some importantfeatures to be invisible. Most proposedmethods are based on using large num22One exception is the multiview eigenfaces ofPentland et al. 1994.bers of multiview samples. This seems toconcur with the findings of the psychologycommunity face perception is believed tobe viewindependent for small angles, butviewdependent for large angles.To assess the pose problem more systematically, an attempt has been made toclassify pose problems Zhao 1999 Zhaoand Chellappa 2000b. The basic idea ofthis analysis is to use a varyingalbedo reflectance model Equation 6 to synthesize new images in different poses from areal image, thus providing a tool for simulating the pose problem. More specifically, the 2Dto2D image transformationunder 3D pose change has been studied.The drawback of this analysis is the restriction of using a generic 3D model nodeformation of this 3D shape was carriedout, though the authors suggested doingso.Researchers have proposed variousmethods of handling the rotation problem. They can be divided into threeclasses Zhao 1999 1 multiview image methods, when multiview databaseimages of each person are available 2hybrid methods, when multiview trainingimages are available during trainingbut only one database image per personACM Computing Surveys, Vol. 35, No. 4, December 2003.448 Zhao et al.is available during recognition and3 singleimageshapebased methods where no training is carried out.Akamatsu et al. 1992, Beymer 1993,Georghiades et al. 1999, 2001, andUllman and Basri 1991 are examples ofthe first class and Beymer 1995, Beymerand Poggio 1995, Cootes et al. 2000,Maurer and Malsburg 1996a, Sali andUllman 1998, and Vetter and Poggio1997 of the second class. Up to now,the second type of approach has beenthe most popular. The third approachdoes not seem to have received muchattention.6.2.1. MultiviewBased Approaches. Oneof the earliest examples of the first class ofapproaches is the work of Beymer 1993,which used a templatebased correlationmatching scheme. In this work, poseestimation and face recognition werecoupled in an iterative loop. For eachhypothesized pose, the input image wasaligned to database images correspondingto that pose. The alignment was firstcarried out via a 2D affine transformationbased on three key feature points eyesand nose, and optical flow was then usedto refine the alignment of each template.After this step, the correlation scores ofall pairs of matching templates were usedfor recognition. The main limitations ofthis method, and other methods belongingto this type of approach, are 1 manydifferent views per person are needed inthe database 2 no lighting variations orfacial expressions are allowed and 3 thecomputational cost is high, since iterativesearching is involved.More recently, an illuminationconebased Belhumeur and Kriegman 1997image synthesis method Georghiadeset al. 1999 has been proposed to handle both pose and illumination problemsin face recognition. It handles illumination variation quite well, but not posevariation. To handle variations due torotation, it needs to completely resolvethe GBR generalizedbasrelief ambiguity and then reconstruct the Euclidean 3Dshape. Without resolving this ambiguity,images from nonfrontal viewpoints synthesized from a GBR reconstruction willdiffer from a valid image by an affinewarp of the image coordinates.23 To address GBR ambiguity, the authors proposed exploiting face symmetry to correcttilt and the fact that the chin and the forehead are at about the same height to correct slant, and requiring that the rangeof heights of the surface be about twicethe distance between the eyes to correctscale Georghiades et al. 2001. They propose a pose and illuminationinvariantface recognition method based on building illumination cones at each pose foreach person. Though conceptually this is agood idea, in practice it is too expensive toimplement. The authors suggested manyways of speeding up the process, includingfirst subsampling the illumination coneand then approximating the subsampledcone with a 11D linear subspace. Experiments on building illumination cones andon 3D shape reconstruction based on seventraining images per class were reported.To visualize illuminationcone based image synthesis, see Figure 24. Figure 25demonstrates the effectiveness of imagesynthesis under variable pose and lightingafter the GBR ambiguity is resolved. Almost perfect recognition results on ten individuals were reported using nine posesand 45 viewing conditions.6.2.2. Hybrid Approaches. Numerous algorithms of the second type have beenproposed. These methods, which makeuse of prior class information, are themost successful and practical methods upto now. We review several representative methods here 1 a viewbased eigenface method Pentland et al. 1994, 2a graph matchingbased method Wiskottet al. 1997, 3 a linear classbasedmethod Blanz and Vetter 1999 Vetterand Poggio 1997, 4 a vectorized image representation based method Beymer1995 Beymer and Poggio 1995, and 5a viewbased appearance model Cootes23GBR is a 3D affine transformation with three parameters scale, slant, and tilt. A weakperspectiveimaging model is assumed.ACM Computing Surveys, Vol. 35, No. 4, December 2003.Face Recognition A Literature Survey 449Fig. 24. The process of constructing the illumination cone. a The seven training images from Subset 1 near frontal illumination in frontal pose. b Images corresponding to the columns of B. c Reconstruction up to a GBR transformation. On the left,the surface was rendered with flat shading, that is, the albedo was assumed to be constant across the surface, while on the right the surface was texturemapped using thefirst basis image of B shown in Figure 24b. d Synthesized images from the illumination cone of the face under novel lighting conditions but fixed pose. Note the large variations in shading and shadowing as compared to the seven training images. Courtesy ofA. Georghiades, P. Belhumeur, and D. Kriegman.et al. 2000. Some of the reviewed methods are very closely relatedfor example,methods 3, 4, and 5. Despite their popularity, these methods have two common drawbacks 1 they need many example imagesto cover the range of possible views 2 theillumination problem is not explicitly addressed, though in principle it can be handled if images captured under the samepose but different illumination conditionsare available.The popular eigenface approach Turkand Pentland 1991 to face recognition hasbeen extended to a viewbased eigenfacemethod in order to achieve poseinvariantrecognition Pentland et al. 1994. Thismethod explicitly codes the pose information by constructing an individual eigenface for each pose. More recently, a unified framework called the bilinear modelwas proposed in Freeman and Tenenbaum2000 that can handle either pure posevariation or pure class variation. A bilinear example is given in Equation 14 forthe illumination problem.In Wiskott et al. 1997, a robust facerecognition scheme based on EBGM wasproposed. The authors assumed a planarsurface patch at each feature point landmark, and learned the transformationsACM Computing Surveys, Vol. 35, No. 4, December 2003.450 Zhao et al.Fig. 25. Synthesized images undervariable pose and lighting generated from the training images shownin Figure 24 and 25. Courtesy ofA. Georghiades, P. Belhumeur, andD. Kriegman.of jets under face rotation. Their results demonstrated substantial improvement in face recognition under rotation.Their method is also fully automatic, including face localization, landmark detection, and flexible graph matching. Thedrawback of this method is its requirement for accurate landmark localization, which is not an easy task, especially when illumination variations arepresent.The image synthesis method in Vetterand Poggio 1997 is based on the assumption of linear 3D object classes and the extension of linearity to images both shapeand texture that are 2D projections of the3D objects. It extends the linear shapemodel which is very similar to the active shape model of Cootes et al. 1995from a representation based on featurepoints to full images of objects. To implement this method, a correspondence between images of the input object and areference object is established using optical flow. Correspondences between the reference image and other example imageshaving the same pose are also computed.Finally, the correspondence field for theinput image is linearly decomposed intothe correspondence fields for the examples. Compared to the parallel deformation scheme in Beymer and Poggio 1995,Fig. 26. The best fit to a profile model is projectedto the frontal model to predict new views Cooteset al. 2000. Courtesy of T. Cootes, K. Walker, andC. Taylor.this method reduces the need to computecorrespondences between images of different poses. On the other hand, parallel deformation was able to preserve some peculiarities of texture that are nonlinearand that could be erased by linear methods. This method was extended in Saliand Ullman 1998 to include an additiveerror term for better synthesis. In Blanzand Vetter 1999, a morphable 3D facemodel consisting of shape and texture wasdirectly matched to singlemultiple inputimages. As a consequence, head orientation, illumination conditions, and otherparameters could be free variables subjectto optimization.In Cootes et al. 2000, a viewbasedstatistical method was proposed based ona small number of 2D statistical models AAM. Unlike most existing methodsthat can handle only images with rotation angles up to, say 45, the authors argued that their method can handle evenprofile views in which many features areinvisible. To deal with such large posevariations, they needed sample views at90 full profile, 45 quasiprofile, and0 frontal view. A key element that isunique to this method is that for eachpose, a different set of features is used.Given a single image of a new person,all the models are used to match the image, and estimation of the pose is achievedby choosing the best fit. To synthesizea new view from the input image, therelationship between models at differentACM Computing Surveys, Vol. 35, No. 4, December 2003.Face Recognition A Literature Survey 451views are learned. More specifically, thefollowing steps are needed 1 removing the effects of orientation, 2 projecting into the identity subspace Edwards et al. 1998, 3 projecting acrossinto the subspace of the target model,and 4 adding the appropriate orientation. Figure 26 demonstrates the synthesis of a virtual view of a novel faceusing this method. Results of tracking a face across large pose variationsand predicting novel views were reportedon a limited dataset of about 15 shortsequences.Earlier work on multiviewbased methods Beymer 1993 was extended to explore the prior class information that isspecific to a face class and can be learnedfrom a set of prototypes Beymer 1993,1995. The key idea of these methods isthe vectorized representation of the images at each pose this is similar to viewbased AAM Cootes et al. 2000. A vectorized representation at each pose consists of both shape and texture, which aremapped into the standardaverage reference shape. The reference shape is computed offline by averaging shapes consisting of manually defined line segments surrounding the eyes, eyebrows, nose, mouth,and facial outline. The shapefree textureis represented either by the original geometrically normalized prototype images orby PCA bases constructed from these images. Given a new image, a vectorizationprocedure similar to the iterative energyminimization procedure in AAM Cooteset al. 2001 is invoked that iterates between a shape step and a texture step.In the texture step, the input image iswarped onto a previously computed alignment with the reference shape and thenprojected into the eigensubspace. In theshape step, the PCAreconstructed imageis used to compute the alignment for nextiteration. In both methods Beymer 1995Beymer and Poggio 1995, an optical flowalgorithm is used to compute a dense correspondence between the images. To synthesize a virtual view at pose 2 of a novelimage at pose 1, the flow between theseposes of the prototype images is computedand then warped to the novel image afFig. 27. View synthesis by parallel deformation.First A the prototype flow is measured between theprototype image and the novel image at the samepose, then B the flow is mapped onto the novel face,and finally C the novel face is 2Dwarped to the virtual view Beymer and Poggio 1995.ter the correspondence between the newimage and the prototype image at pose1 is computed using the warped flow, avirtual view can be generated by warpingthe novel image. Figure 27 illustrates aparticular procedure adopted in Beymerand Poggio 1995 the parallel deformation needed to compute the flow betweenthe prototype image and the novel image. An obvious drawback of this approachis the difficulty of computing flow whenthe prototype image and novel image aredramatically different. To handle this issue, Beymer 1995 proposed first subsampling the estimated dense flow to locatelocal features line segments based onprior knowledge about both images, andthen matching the local features. Feeding the virtual views into a simple recognizer based on templates of eyes, nose, andmouth, a recognition rate of 85 was reported on a test set of 620 images 62 people, 10 views per person given one single real view. Apparently this method isnot adequate, since it needs to synthesize all virtual views. A better strategy isto detect the pose of the novel face andsynthesize only the prototype say frontalview.ACM Computing Surveys, Vol. 35, No. 4, December 2003.452 Zhao et al.6.2.3. SingleImageBased Approaches.Finally, the third class of approachesincludes lowlevel featurebased methods,invariantfeaturebased methods, and 3Dmodelbased methods. In Manjunath et al.1992, a Gabor waveletbased featureextraction method is proposed for facerecognition which is robust to smallanglerotations. In these methods, face shape isusually represented by either a polygonalmodel or a mesh model which simulatestissue. Due to its complexity and computational cost, no serious attempt to applythis approach to face recognition has beenmade, except for Gordon 1991, where3D range data was available. In Zhao andChellappa 2000b, a unified approachwas proposed to solving both the pose andillumination problems. This method is anatural extension of the method proposedin Zhao and Chellappa 2000 to handlethe illumination problem. Using a generic3D model, they approximately solved thecorrespondence problem involved in a3D rotation, and performed an inputtoprototype image computation. To addressthe varying albedo issue in the estimationof both pose and light source, the useof a selfratio image was proposed. Theselfratio image rI x, y was defined asrI x, y  I x, y  I x, yI x, y  I x, y18 px, yPs1  qx, yQs ,where I x, y is the original image andI x, y is the mirrored image.Using the selfratio image, which isalbedofree, the authors formulated thefollowing combined estimation problemfor pose  and light source ,  , ,   arg ,, minrIm,    rI  , ,  2,19where rI  ,,  is the selfratio image forthe virtual frontal view synthesized fromthe original rotated image IR via imagewarping and texture mapping, and rIm isthe selfratio image generated from the 3Dface model. Improved recognition resultsbased on subspace LDA Zhao et al. 1999were reported on a small database consisting of frontal and quasiprofile images of115 novel objects size 4842. In these experiments, the frontal view images servedas the gallery images and nonfrontal viewimages served as the probe images. Unfortunately, estimation of a single pose valuefor all the images was done manually. Formany images, this estimate was not good,negating the performance improvement.7. SUMMARY AND CONCLUSIONSIn this paper we have presented an extensive survey of machine recognition ofhuman faces and a brief review of relatedpsychological studies. We have consideredtwo types of face recognition tasks onefrom still images and the other from video.We have categorized the methods used foreach type, and discussed their characteristics and their pros and cons. In addition to a detailed review of representativework, we have provided summaries of current developments and of challenging issues. We have also identified two important issues in practical face recognitionsystems the illumination problem and thepose problem. We have categorized proposed methods of solving these problemsand discussed the pros and cons of thesemethods. To emphasize the importance ofsystem evaluation, three sets of evaluations were described FERET, FRVT, andXM2VTS.Getting started in performing experiments in face recognition is very easy. TheColorado State Universitys Evaluation ofFace Recognition Algorithms Web site,http www.cs.colostate.eduevalfacerec,has an archive of baseline face recognition algorithms. Baseline algorithmsavailable are PCA, LDA, elastic bunchgraph matching, and Bayesian IntrapersonalExtrapersoanl Image DiffferenceClassifier. Source code, and scripts for running the algorithms can be downloaded.The Web site includes scripts for runningthe FERET Sep96 evaluation protocol theFERET data set needs to be obtained fromthe FERET Web site. The baseline algorithms and FERET Sep96 protocol provideACM Computing Surveys, Vol. 35, No. 4, December 2003.Face Recognition A Literature Survey 453a framework for benchmarking new algorithms. The scripts can be modified to rundifferent sets of images against the baseline. For online resources related to facerecognition, such as research papers anddatabases, see Table V.We give below a concise summary of ourdiscussion, followed by our conclusions,in the same order as the topics have appeared in this paperMachine recognition of faces hasemerged as an active research areaspanning disciplines such as imageprocessing, pattern recognition, computer vision, and neural networks.There are numerous applications ofFRT to commercial systems such asface verificationbased ATM and accesscontrol, as well as law enforcementapplications to video surveillance, etc.Due to its userfriendly nature, facerecognition will remain a powerful toolin spite of the existence of very reliablemethods of biometric personal identification such as fingerprint analysis andiris scans.Extensive research in psychophysicsand the neurosciences on human recognition of faces is documented in the literature. We do not feel that machinerecognition of faces should strictly follow what is known about human recognition of faces, but it is beneficial for engineers who design face recognition systems to be aware of the relevant findings. On the other hand, machine systems provide tools for conducting studies in psychology and neuroscience.Numerous methods have been proposedfor face recognition based on image intensities Chellappa et al. 1995. Manyof these methods have been successfully applied to the task of face recognition, but they have advantages anddisadvantages. The choice of a methodshould be based on the specific requirements of a given task. For example,the EBGMbased method Okada et al.1998 has very good performance, butit requires an image size, for example, 128  128, which severely restrictsits possible application to videobasedsurveillance where the image size of theface area is very small. On the otherhand, the subspace LDA method Zhaoet al. 1999 works well for both large andsmall images, for example, 96  84 or12  11.Recognition of faces from a video sequence especially a surveillance videois still one of the most challenging problems in face recognition because video isof low quality and the images are small.Often, the subjects of interest are not cooperative, for example, not looking intothe camera. One particular difficultyin these applications is how to obtaingoodquality gallery images. Nevertheless, videobased face recognition systems using multiple cues have demonstrated good results in relatively controlled environments.A crucial step in face recognition is theevaluation and benchmarking of algorithms. Two of the most important facedatabases and their associated evaluation methods have been reviewed theFERET, FRVT, and XM2VTS protocols.The availability of these evaluations hashad a significant impact on progress inthe development of face recognition algorithms.Although many face recognition techniques have been proposed and haveshown significant promise, robust facerecognition is still difficult. There areat least three major challenges illumination, pose, and recognition in outdoorimagery. A detailed review of methodsproposed to solve these problems hasbeen presented. Some basic problems remain to be solved for example, pose discrimination is not difficult but accuratepose estimation is hard. In addition tothese two problems, there are other evenmore difficult ones, such as recognitionof a person from images acquired yearsapart.The impressive face recognition capability of the human perception system hasone limitation the number and types offaces that can be easily distinguished.Machines, on the other hand, canstore and potentially recognize as manyACM Computing Surveys, Vol. 35, No. 4, December 2003.454 Zhao et al.people as necessary. Is it really possiblethat a machine can be built that mimicsthe human perceptual system withoutits limitations on number and typesTo conclude our paper, we present a conjecture about face recognition based onpsychological studies and lessons learnedfrom designing algorithms. We conjecturethat different mechanisms are involved inhuman recognition of familiar and unfamiliar faces. For example, it is possiblethat 3D head models are constructed, byextensive training for familiar faces, butfor unfamiliar faces, multiview 2D imagesare stored. This implies that we have fullprobability density functions for familiarfaces, while for unfamiliar faces we onlyhave discriminant functions.REFERENCESADINI, Y., MOSES, Y., AND ULLMAN, S. 1997. Facerecognition The problem of compensating forchanges in illumination direction. IEEE Trans.Patt. Anal. Mach. Intell. 19, 721732.AKAMATSU, S., SASAKI, T., FUKAMACHI, H., MASUI, N.,AND SUENAGA, Y. 1992. An accurate and robustface identification scheme. In Proceedings, International Conference on Pattern Recognition.217220.ATICK, J., GRIFFIN, P., AND REDLICH, N. 1996. Statistical approach to shape from shading Reconstruction of threedimensional face surfacesfrom single twodimensional images. NeuralComputat. 8, 13211340.AZARBAYEJANI, A., STARNER, T., HOROWITZ, B., ANDPENTLAND, A. 1993. Visually controlled graphics. IEEE Trans. Patt. Anal. Mach. Intell. 15,602604.BACHMANN, T. 1991. Identification of spatiallyquantized tachistoscopic images of faces Howmany pixels does it take to carry identity European J. Cog. Psych. 3, 87103.BAILLYBAILLIERE, E., BENGIO, S., BIMBOT, F., HAMOUZ,M., KITTLER, J., MARIETHOZ, J., MATAS, J., MESSER,K., POPOVICI, V., POREE, F., RUIZ, B., AND THIRAN,J. P. 2003. The BANCA database and evaluation protocol. In Proceedings of the InternationalConference on Audio and VideoBased BiometricPerson Authentication. 625638.BARTLETT, J. C. AND SEARCY, J. 1993. Inversion andconfiguration of faces. Cog. Psych. 25, 281316.BARTLETT, M. S., LADES, H. M., AND SEJNOWSKI, T.1998. Independent component representationfor face recognition. In Proceedings, SPIE Symposium on Electronic Imaging Science and Technology. 528539.BASRI, R. AND JACOBS, D. W. 2001. Lambertian refelectances and linear subspaces. In Proceedings,International Conference on Computer Vision.Vol. II. 383390.BELHUMEUR, P. N., HESPANHA, J. P., AND KRIEGMAN, D.J. 1997. Eigenfaces vs. Fisherfaces Recognition using class specific linear projection. IEEETrans. Patt. Anal. Mach. Intell. 19, 711720.BELHUMEUR, P. N. AND KRIEGMAN, D. J. 1997. Whatis the set of images of an object under all possiblelighting conditions In Proceedings, IEEE Conference on Computer Vision and Pattern Recognition. 5258.BELL, A. J. AND SEJNOWSKI, T. J. 1995. An information maximisation approach to blind separationand blind deconvolution. Neural Computation 7,11291159.BELL, A. J. AND SEJNOWSKI, T. J. 1997. The independent components of natural scenes are edge filters. Vis. Res. 37, 33273338.BEVERIDGE, J. R., SHE, K., DRAPER, B. A., ANDGIVENS, G. H. 2001. A nonparametric statisical comparison of principal component andlinear discriminant subspaces for face recognition. In Proceedings, IEEE Conference onComputer Vision and Pattern Recognition.An updated version can be found onlineat httpwww.cs.colostate.eduevalfacerecnews.html.BEYMER, D. 1995. Vectorizing face images by interleaving shape and texture computations. MITAI Lab memo 1537. Massachusetts Institute ofTechnology, Cambridge, MA.BEYMER, D. J. 1993. Face recognition under varying pose. Tech. Rep. 1461. MIT AI Lab, Massachusetts Institute of Technology, Cambridge,MA.BEYMER, D. J. AND POGGIO, T. 1995. Face recognitionfrom one example view. In Proceedings, International Conference on Computer Vision. 500507.BIEDERMAN, I. 1987. Recognition by components Atheory of human image understanding. Psych.Rev. 94, 115147.BIEDERMAN, I. AND KALOCSAI, P. 1998. Neural andpsychophysical analysis of object and face recognition. In Face Recognition From Theory to Applications, H. Wechsler, P. J. Phillips, V. Bruce, F.F. Soulie, and T. S. Huang, Eds. SpringerVerlag,Berlin, Germany, 325.BIGUN, J., DUC, B., SMERALDI, F., FISCHER, S., ANDMAKAROV, A. 1998. Multimodal person authentication. In Face Recognition From Theory to Applications, H. Wechsler, P. J. Phillips,V. Bruce, F. F. Soulie, and T. S. Huang, Eds.SpringerVerlag, Berlin, Germany, 2650.BLACK, M., FLEET, D., AND YACOOB, Y. 1998. AFramework for modelling appearance change inimage sequences. In Proceedings, InternationalConference on Computer Vision, 660667.BLACK, M. AND YACOOB, Y. 1995. Tracking and recognizing facial expressions in image sequencesACM Computing Surveys, Vol. 35, No. 4, December 2003.Face Recognition A Literature Survey 455using local parametrized models of image motion. Tech. rep. CSTR3401. Center for Automation Research, Unversity of Maryland, CollegePark, MD.BLACKBURN, D., BONE, M., AND PHILLIPS, P. J. 2001.Face recognition vendor test 2000. Tech. rep.httpwww.frvt.org.BLANZ, V. AND VETTER, T. 1999. A Morphable modelfor the synthesis of 3D faces. In Proceedings,SIGGRAPH99, 187194.BLANZ, V. AND VETTER, T. 2003. Face recognitionbased on fitting a 3D morphable model. IEEETrans. Patt. Anal. Mach. Intell. 25, 10631074.BLEDSOE, W. W. 1964. The model method in facialrecognition. Tech. rep. PRI15, Panoramic research Inc., Palo Alto, CA.BRAND, M. AND BHOTIKA, R. 2001. Flexible flow for3D nonrigid tracking and shape recovery. In Proceedings, IEEE Conference on Computer Visionand Pattern Recognition.BRENNAN, S. E. 1985. The caricature generator.Leonardo, 18, 170178.BRONSTEIN, A., BRONSTEIN, M., GORDON, E., AND KIMMEL,R. 2003. 3D face recognition using geometricinvariants. In Proceedings, International Conference on Audio and VideoBased Person Authentication.BRUCE, V. 1988. Recognizing faces, Lawrence Erlbaum Associates, London, U.K.BRUCE, V., BURTON, M., AND DENCH, N. 1994. Whatsdistinctive about a distinctive face Quart. J.Exp. Psych. 47A, 119141.BRUCE, V., HANCOCK, P. J. B., AND BURTON, A. M. 1998.Human face perception and identification. InFace Recognition From Theory to Applications,H. Wechsler, P. J. Phillips, V. Bruce, F. F. Soulie,and T. S. Huang, Eds. SpringerVerlag, Berlin,Germany, 5172.BRUNER, I. S. AND TAGIURI, R. 1954. The perception of people. In Handbook of Social Psychology,Vol. 2, G. Lindzey, Ed., AddisonWesley, Reading,MA, 634654.BUHMANN, J., LADES, M., AND MALSBURG, C. V. D. 1990.Size and distortion invariant object recognitionby hierarchical graph matching. In Proceedings,International Joint Conference on Neural Networks. 411416.CHELLAPPA, R., WILSON, C. L., AND SIROHEY, S. 1995.Human and machine recognition of faces A survey. Proc. IEEE, 83, 705740.CHOUDHURY, T., CLARKSON, B., JEBARA, T., AND PENTLAND,A. 1999. Multimodal person recognition using unconstrained audio and video. In Proceedings, International Conference on Audio andVideoBased Person Authentication. 176181.COOTES, T., TAYLOR, C., COOPER, D., AND GRAHAM, J.1995. Active shape modelstheir training andapplication. Comput. Vis. Image Understand. 61,1823.COOTES, T., WALKER, K., AND TAYLOR, C. 2000. Viewbased active appearance models. In Proceedings,International Conference on Automatic Face andGesture Recognition.COOTES, T. F., EDWARDS, G. J., AND TAYLOR, C. J. 2001.Active appearance models. IEEE Trans. Patt.Anal. Mach. Intell. 23, 681685.COX, I. J., GHOSN, J., AND YIANILOS, P. N. 1996.Featurebased face recognition using mixturedistance. In Proceedings, IEEE Conference onComputer Vision and Pattern Recognition. 209216.CRAW, I. AND CAMERON, P. 1996. Face recognition bycomputer. In Proceedings, British Machine Vision Conference. 489507.DARWIN, C. 1972. The Expression of the Emotionsin Man and Animals. John Murray, London, U.K.DECARLO, D. AND METAXAS, D. 2000. Optical flowconstraints on deformable models with applications to face tracking. Int. J. Comput. Vis. 38,99127.DONATO, G., BARTLETT, M. S., HAGER, J. C., EKMAN, P.,AND SEJNOWSKI, T. J. 1999. Classifying facialactions. IEEE Trans. Patt. Anal. Mach. Intell. 21,974989.EDWARDS, G. J., TAYLOR, C. J., AND COOTES, T. F.1998. Learning to identify and track facesin image sequences. In Proceedings, International Conference on Automatic Face and GestureRecognition.EKMAN, P. Ed., 1998. Charles Darwins The Expression of the Emotions in Man and Animals,Third Edition, with Introduction, Afterwordsand Commentaries by Paul Ekman. HarperCollinsOxford University Press, New York,NYLondon, U.K.ELLIS, H. D. 1986. Introduction to aspects of faceprocessing Ten questions in need of answers. InAspects of Face Processing, H. Ellis, M. Jeeves,F. Newcombe, and A. Young, Eds. Nijhoff, Dordrecht, The Netherlands, 313.ETEMAD, K. AND CHELLAPPA, R. 1997. Discriminantanalysis for recognition of human face images.J. Opt. Soc. Am. A 14, 17241733.FISHER, R. A. 1938. The statistical utilization ofmultiple measuremeents. Ann. Eugen. 8, 376386.FREEMAN, W. T. AND TENENBAUM, J. B. 2000. Separating style and contents with bilinear models.Neural Computat. 12, 12471283.FUKUNAGA, K. 1989. Statistical Pattern Recognition, Academic Press, New York, NY.GALTON, F. 1888. Personal identification and description. Nature, June 21, 173188.GAUTHIER, I., BEHRMANN, M., AND TARR, M. J. 1999.Can face recognition really be dissociated fromobject recognition J. Cogn. Neurosci. 11, 349370.GAUTHIER, I. AND LOGOTHETIS, N. K. 2000. Is facerecognition so unique after All J. Cogn. Neuropsych. 17, 125142.ACM Computing Surveys, Vol. 35, No. 4, December 2003.456 Zhao et al.GEORGHIADES, A. S., BELHUMEUR, P. N., AND KRIEGMAN,D. J. 1999. Illuminationbased image synthesis Creating novel images of human faces under differing pose and lighting. In Proceedings,Workshop on MultiView Modeling and Analysisof Visual Scenes, 4754.GEORGHIADES, A. S., BELHUMEUR, P. N., AND KRIEGMAN,D. J. 2001. From few to many Illuminationcone models for face recognition under variablelighting and pose. IEEE Trans. Patt. Anal. Mach.Intell. 23, 643660.GEORGHIADES, A. S., KRIEGMAN, D. J., AND BELHUMEUR,P. N. 1998. Illumination cones for recognitionunder variable lighting Faces. In Proceedings,IEEE Conference on Computer Vision and Pattern Recognition, 5258.GINSBURG, A. G. 1978. Visual information processing based on spatial filters constrained by biological data. AMRL Tech. rep. 78129.GONG, S., MCKENNA, S., AND PSARROU, A. 2000. Dynamic Vision From Images to Face Recognition.World Scientific, Singapore.GORDON, G. 1991. Face recognition based on depthmaps and surface curvature. In SPIE Proceedings, Vol. 1570 Geometric Methods in ComputerVision. SPIE Press, Bellingham, WA 234247.GU, L., LI, S. Z., AND ZHANG, H. J. 2001. Learningprobabilistic distribution model for multiviewface dectection. In Proceedings, IEEE Conferenceon Computer Vision and Pattern Recognition.HAGER, G. D., AND BELHUMEUR, P. N. 1998. Efficientregion tracking with parametri models of geometry and illumination. IEEE Trans. Patt. Anal.Mach. Intell. 20, 115.HALLINAN, P. W. 1991. Recognizing human eyes. InSPIE Proceedings, Vol. 1570 Geometric MethodsIn Computer Vision. 214226.HALLINAN, P. W. 1994. A lowdimensional representation of human faces for arbitrary lightingconditions. In Proceedings, IEEE Conference onComputer Vision and Pattern Recognition. 995999.HANCOCK, P., BRUCE, V., AND BURTON, M. 1998. Acomparison of two computerbased face recognition systems with human perceptions of faces.Vis. Res. 38, 22772288.HARMON, L. D. 1973. The recognition of faces. Sci.Am. 229, 7182.HEISELE, B., SERRE, T., PONTIL, M., AND POGGIO, T.2001. Componentbased face detection. In Proceedings, IEEE Conference on Computer Visionand Pattern Recognition.HILL, H. AND BRUCE, V. 1996. Effects of lighting onmatching facial surfaces. J. Exp. Psych. HumanPercept. Perform. 22, 9861004.HILL, H., SCHYNS, P. G., AND AKAMATSU, S. 1997.Information and viewpoint dependence in facerecognition. Cognition 62, 201222.HJELMAS, E. AND LOW, B. K. 2001. Face detectionA Survey. Comput. Vis. Image Understand. 83,236274.HORN, B. K. P. AND BROOKS, M. J. 1989. Shape fromShading. MIT Press, Cambridge, MA.HUANG, J., HEISELE, B., AND BLANZ, V. 2003.Componentbased face recognition with 3D morphable models. In Proceedings, InternationalConference on Audio and VideoBased PersonAuthentication.ISARD, M. AND BLAKE, A. 1996. Contour tracking bystochastic propagation of conditional density. InProceedings, European Conference on ComputerVision.JACOBS, D. W., BELHUMEUR, P. N., AND BASRI, R.1998. Comparing images under variable illumination. In Proceedings, IEEE Conference onComputer Vision and Pattern Recognition. 610617.JEBARA, T., RUSSEL, K., AND PENTLAND, A. 1998.Mixture of eigenfeatures for realtime structure from texture. Tech. rep. TR440, MIT Media Lab, Massachusetts Institute of Technology,Cambridge, MA.JOHNSTON, A., HILL, H., AND CARMAN, N. 1992. Recognizing faces Effects of lighting direction, inversion and brightness reversal. Cognition 40,119.KALOCSAI, P. K., ZHAO, W., AND ELAGIN, E. 1998.Face similarity space as perceived by humansand artificial systems. In Proceedings, International Conference on Automatic Face and GestureRecognition. 177180.KANADE, T. 1973. Computer recognition of human faces. Birkhauser, Basel, Switzerland, andStuttgart, Germany.KELLY, M. D. 1970. Visual identification of people by computer. Tech. rep. AI130, Stanford AIProject, Stanford, CA.KIRBY, M. AND SIROVICH, L. 1990. Application of theKarhunenLoeve procedure for the characterization of human faces. IEEE Trans. Patt. Anal.Mach. Intell. 12.KLASEN, L. AND LI, H. 1998. Faceless identification.In Face Recognition From Theory to Applications, H. Wechsler, P. J. Phillips, V. Bruce, F. F.Soulie, and T. S. Huang, Eds. SpringerVerlag,Berlin, Germany, 513527.KNIGHT, B. AND JOHNSTON, A. 1997. The role ofmovement in face recognition. Vis. Cog. 4, 265274.KRUGER, N., POTZSCH, M., AND MALSBURG, C. V. D. 1997.Determination of face position and pose with alearned representation based on labelled graphs.Image Vis. Comput. 15, 665673.KUNG, S. Y. AND TAUR, J. S. 1995. Decisionbasedneural networks with signalimage classificationapplications. IEEE Trans. Neural Netw. 6, 170181.LADES, M., VORBRUGGEN, J., BUHMANN, J., LANGE, J.,MALSBURG, C. V.D., WURTZ, R., AND KONEN, W.1993. Distortion invariant object recognitionin the dynamic link architecture. IEEE Trans.Comput. 42, 300311.ACM Computing Surveys, Vol. 35, No. 4, December 2003.Face Recognition A Literature Survey 457LANITIS, A., TAYLOR, C. J., AND COOTES, T. F. 1995.Automatic face identification system using flexible appearance models. Image Vis. Comput. 13,393401.LAWRENCE, S., GILES, C. L., TSOI, A. C., AND BACK,A. D. 1997. Face recognition A convolutionalneuralnetwork approach. IEEE Trans. NeuralNetw. 8, 98113.LI, B. AND CHELLAPPA, R. 2001. Face verificationthrough tracking facial features. J. Opt. Soc. Am.18.LI, S. Z. AND LU, J. 1999. Face recognition using thenearest feature line method. IEEE Trans. Neural Netw. 10, 439443.LI, Y., GONG, S., AND LIDDELL, H. 2001a. Constructing facial identity surfaces in a nonlinear discriminating space. In Proceedings, IEEE Conference on Computer Vision and Pattern Recognition.LI, Y., GONG, S., AND LIDDELL, H. 2001b. Modellingface dynamics across view and over time. In Proceedings, International Conference on ComputerVision.LIN, S. H., KUNG, S. Y., AND LIN, L. J. 1997. Facerecognitiondetection by probabilistic decisionbased neural network. IEEE Trans. NeuralNetw. 8, 114132.LIU, C. AND WECHSLER, H. 2000a. Evolutionary pursuit and its application to face recognition. IEEETrans. Patt. Anal. Mach. Intell. 22, 570582.LIU, C. AND WECHSLER, H. 2000b. Robust codingscheme for indexing and retrieval from large facedatabases. IEEE Trans. Image Process. 9, 132137.LIU, C. AND WECHSLER, H. 2001. A shape andtexturebased enhanced fisher classifier for facerecognition. IEEE Trans. Image Process. 10,598608.LIU, J. AND CHEN, R. 1998. Sequential Monte Carlomethods for dynamic systems. J. Am. Stat. Assoc.93, 10311041.MANJUNATH, B. S., CHELLAPPA, R., AND MALSBURG, C. V. D.1992. A feature based approach to face recognition. In Proceedings, IEEE Conference on Computer Vision and Pattern Recognition. 373378.MARR, D. 1982. Vision. W. H. Freeman, San Francisco, CA.MARTINEZ, A. 2002. Recognizing imprecisely localized, partially occluded and expression variantfaces from a single sample per class. IEEE Trans.Patt. Anal. Mach. Intell. 24, 748763.MARTINEZ, A. AND KAK, A. C. 2001. PCA versusLDA. IEEE Trans. Patt. Anal. Mach. Intell. 23,228233.MAURER, T. AND MALSBURG, C. V. D. 1996a. Singleview based recognition of faces rotated in depth.In Proceedings, International Workshop on Automatic Face and Gesture Recognition. 176181.MAURER, T. AND MALSBURG, C. V. D. 1996b. Tracking and learning graphs and pose on imagesequences of faces. In Proceedings, International Conference on Automatic Face and GestureRecognition. 176181.MCKENNA, S. J. AND GONG, S. 1997. Nonintrusiveperson authentication for access control by visual tracking and face recognition. In Proceedings, International Conference on Audio andVideoBased Person Authentication. 177183.MCKENNA, S. AND GONG, S. 1998. Recognising moving faces. In Face Recognition From Theory toApplications, H. Wechsler, P. J. Phillips, V. Bruce,F. F. Soulie, and T. S. Huang, Eds. SpringerVerlag, Berlin, Germany, 578588.MATAS, J. ET. AL., 2000. Comparison of face verification results on the XM2VTS database. In Proceedings, International Conference on PatternRecognition, Vol. 4, 858863.MESSER, K., MATAS, J., KITTLER, J., LUETTIN, J., ANDMAITRE, G. 1999. XM2VTSDB The ExtendedM2VTS Database. In Proceedings, InternationalConference on Audio and VideoBased PersonAuthentication. 7277.MIKA, S., RATSCH, G., WESTON, J., SCHOLKOPF, B.,AND MULLER, K.R. 1999. Fisher discriminantanalysis with kernels. In Proceedings, IEEEWorkshop on Neural Networks for Signal Processing.MOGHADDAM, B., NASTAR, C., AND PENTLAND, A. 1996.A Bayesian similarity measure for direct imagematching. In Proceedings, International Conference on Pattern Recognition.MOGHADDAM, B. AND PENTLAND, A. 1997. Probabilistic visual learning for object representation.IEEE Trans. Patt. Anal. Mach. Intell. 19, 696710.MOON, H. AND PHILLIPS, P. J. 2001. Computationaland performance aspects of PCAbased facerecognition algorithms. Perception, 30, 301321.MURASE, H. AND NAYAR, S. 1995. Visual learningand recognition of 3D objects from appearances.Int. J. Comput. Vis. 14, 525.NEFIAN, A. V. AND HAYES III, M. H. 1998. Hidden Markov models for face recognition. In Proceedings, International Conference on Acoustics,Speech and Signal Processing. 27212724.OKADA, K., STEFFANS, J., MAURER, T., HONG, H., ELAGIN,E., NEVEN, H., AND MALSBURG, C. V. D. 1998. TheBochumUSC Face Recognition System and howit fared in the FERET Phase III Test. In FaceRecognition From Theory to Applications, H.Wechsler, P. J. Phillips, V. Bruce, F. F. Soulie,and T. S. Huang, Eds. SpringerVerlag, Berlin,Germany, 186205.OTOOLE, A. J., ROARK, D., AND ABDI, H. 2002.Recognitizing moving faces. A psychological andneural synthesis. Trends Cogn. Sci. 6, 261266.PANTIC, M. AND ROTHKRANTZ, L. J. M. 2000. Automatic analysis of facial expressions The state ofthe art. IEEE Trans. Patt. Anal. Mach. Intell. 22,14241446.ACM Computing Surveys, Vol. 35, No. 4, December 2003.458 Zhao et al.PENEV, P. AND SIROVICH, L. 2000. The global dimensionality of face space. In Proceedings, International Conference on Automatic Face and GestureRecognition.PENEV, P. AND ATICK, J. 1996. Local feature analysis A general statistical theory for objecct representation. Netw. Computat. Neural Syst. 7, 477500.PENTLAND, A., MOGHADDAM, B., AND STARNER, T. 1994.Viewbased and modular eigenspaces for facerecognition. In Proceedings, IEEE Conference onComputer Vision and Pattern Recognition.PERKINS, D. 1975. A definition of caricature andrecognition. Stud. Anthro. Vis. Commun. 2, 124.PHILLIPS, P. J., GROTHER, P. J., MICHEALS, R. J., BLACKBURN, D. M., TABASSI, E., AND BONE, J. M. 2003.Face recognition vendor test 2002 Evaluationreport. NISTIR 6965, 2003. Available online athttpwww.frvt.org.PHILLIPS, P. J. 1998. Support vector machines applied to face fecognition. Adv. Neural Inform.Process. Syst. 11, 803809.PHILLIPS, P. J., MCCABE, R. M., AND CHELLAPPA, R.1998. Biometric image processing and recognition. In Proceedings, European Signal Processing Conference.PHILLIPS, P. J., MOON, H., RIZVI, S., AND RAUSS, P. 2000.The FERET evaluation methodology for facerecognition algorithms. IEEE Trans. Patt. Anal.Mach. Intell. 22.PHILLIPS, P. J., WECHSLER, H., HUANG, J., AND RAUSS,P. 1998b. The FERET database and evaluation procedure for facerecognition algorithms.Image Vis. Comput. 16, 295306.PIGEON, S. AND VANDENDORPE, L. 1999. The M2VTSmultimodal face database Release 1.00. InProceedings, International Conference on Audioand VideoBased Person Authentication. 403409.RIKLINRAVIV, T. AND SHASHUA, A. 1999. The quotient image Class based rerendering and recognition with varying illuminations. In Proceedings, IEEE Conference on Computer Vision andPattern Recognition. 566571.RIZVI, S. A., PHILLIPS, P. J., AND MOON, H. 1998. Averification protocol and statistical performanceanalysis for face recognition algorithms. In Proceedings, IEEE Conference on Computer Visionand Pattern Recognition. 833838.ROWLEY, H. A., BALUJA, S., AND KANADE, T. 1998.Neural network based face detection. IEEETrans. Patt. Anal. Mach. Intell. 20.CHOUDHURY, A. K. R. AND CHELLAPPA, R. 2003. Facereconstruction from monocular video using uncertainty analysis and a generic model. Comput.Vis. Image Understand. 91, 188213.RUDERMAN, D. L. 1994. The statistics of natural images. Netw. Comput. Neural Syst. 5, 598605.SALI, E. AND ULLMAN, S. 1998. Recognizing novel3D objects under new illumination and viewingposition using a small number of example viewsor even a single view. In Proceedings, IEEE Conference on Computer Vision and Pattern Recognition. 153161.SAMAL, A. AND IYENGAR, P. 1992. Automatic recognition and analysis of human faces and facialexpressions A survey. Patt. Recog. 25, 6577.SAMARIA, F. 1994. Face recognition using hiddenmarkov models. Ph.D. dissertation. Universityof Cambridge, Cambridge, U.K.SAMARIA, F. AND YOUNG, S. 1994. HMM based architecture for face identification. Image Vis. Comput. 12, 537583.SCHNEIDERMAN, H. AND KANADE, T. 2000. Probabilistic modelling of local Appearance and spatialreationships for object recognition. In Proceedings, IEEE Conference on Computer Vision andPattern Recognition. 746751.SERGENT, J. 1986. Microgenesis of face perception.In Aspects of Face Processing, H. D. Ellis, M. A.Jeeves, F. Newcombe, and A. Young, Eds. Nijhoff,Dordrecht, The Netherlands.SHASHUA, A. 1994. Geometry and photometry in3D visual recognition. Ph.D. dissertation. Massachusetts Institute of Technology, Cambridge,MA.SHEPHERD, J. W., DAVIES, G. M., AND ELLIS, H. D. 1981.Studies of cue saliency. In Perceiving and Remembering Faces, G. M. Davies, H. D. Ellis, andJ. W. Shepherd, Eds. Academic Press, London,U.K.SHIO, A. AND SKLANSKY, J. 1991. Segmentation ofpeople in motion. In Proceedings, IEEE Workshop on Visual Motion. 325332.SIROVICH, L. AND KIRBY, M. 1987. Lowdimensionalprocedure for the characterization of humanface. J. Opt. Soc. Am. 4, 519524.STEFFENS, J., ELAGIN, E., AND NEVEN, H. 1998.PersonSpotterfast and robust system for human detection, tracking and recognition. In Proceedings, International Conference on AutomaticFace and Gesture Recognition. 516521.STROM, J., JEBARA, T., BASU, S., AND PENTLAND, A.1999. Real time tracking and modeling offaces An EKFbased analysis by synthesis approach. Tech. rep. TR506, MIT Media Lab, Massachusetts, Institute of Technology, Cambridge,MA.SUNG, K. AND POGGIO, T. 1997. Examplebasedlearning for viewbased human face detection.IEEE Trans. Patt. Anal. Mach. Intell. 20, 3951.SWETS, D. L. AND WENG, J. 1996b. Using discriminant eigenfeatures for image retrieval.IEEE Trans. Patt. Anal. Mach. Intell. 18, 831836.SWETS, D. L. AND WENG, J. 1996. Discriminantanalysis and eigenspace partition tree for faceand object recognition from views. In Proceedings, International Conference on AutomaticFace and Gesture Recognition. 192197.ACM Computing Surveys, Vol. 35, No. 4, December 2003.Face Recognition A Literature Survey 459TARR, M. J. AND BULTHOFF, H. H. 1995. Is human object recognition better described by geonstructural descriptions or by multiple viewscomment on Biederman and Gerhardstein1993. J. Exp. Psych. Hum. Percep. Perf. 21, 7186.TERZOPOULOS, D. AND WATERS, K. 1993. Analysisand synthesis of facial image sequences usingphysical and anatomical models. IEEE Trans.Patt. Anal. Mach. Intell. 15, 569579.THOMPSON, P. 1980. Margaret ThatcherA new illusion. Perception, 9, 483484.TSAI, P. S. AND SHAH, M. 1994. Shape from shadingusing linear approximation. Image Vis. Comput.12, 487498.TRIGGS, B., MCLAUCHLAN, P., HARTLEY, R., ANDFITZGIBBON, A. 2000. Bundle adjustmentamodern synthesis. In Vision Algorithms Theoryand Practice, SpringerVerlag, Berlin, Germany.TURK, M. AND PENTLAND, A. 1991. Eigenfaces forrecognition. J. Cogn. Neurosci. 3, 7286.ULLMAN, S. AND BASRI, R. 1991. Recognition by linear combinations of models. IEEE Trans. Patt.Anal. Mach. Intell. 13, 9921006.VAPNIK, V. N. 1995. The Nature of StatisticalLearning Theory. SpringerVerlag, New York,NY.VETTER, T. AND POGGIO, T. 1997. Linear objectclasses and image synthesis from a single example image. IEEE Trans. Patt. Anal. Mach. Intell.19, 733742.VIOLA, P. AND JONES, M. 2001. Rapid object detection using a boosted cascade of simple features.In Proceedings, IEEE Conference on ComputerVision and Pattern Recognition.WECHSLER, H., KAKKAD, V., HUANG, J., GUTTA, S., ANDCHEN, V. 1997. Automatic videobased personauthentication using the RBF network. In Proceedings, International Conference on Audioand VideoBased Person Authentication. 8592.WILDER, J. 1994. Face recognition using transformcoding of gray scale projection and the neural tree network. In Artificial Neural Networkswith Applications in Speech and Vision, R. J.Mammone, Ed. Chapman Hall, New York, NY,520536.WISKOTT, L., FELLOUS, J.M., AND VON DER MALSBURG, C.1997. Face recognition by elastic bunch graphmatching. IEEE Trans. Patt. Anal. Mach. Intell.19, 775779.YANG, M. H., KRIEGMAN, D., AND AHUJA, N. 2002. Detecting faces in images A survey. IEEE Trans.Patt. Anal. Mach. Intell. 24, 3458.YIN, R. K. 1969. Looking at upsidedown faces.J. Exp, Psych. 81, 141151.YUILLE, A. L., COHEN, D. S., AND HALLINAN, P. W.1992. Feature extractiong from faces using deformable templates. Int. J. Comput. Vis. 8, 99112.YUILLE, A. AND HALLINAN, P. 1992. Deformable templates. In Active vision, A. Blake, and A. Yuille,Eds., Cambridge, MA, 2138.ZHAO, W. 1999. Robust Image Based 3D FaceRecognition, Ph.D. dissertation. University ofMaryland, College Park, MD.ZHAO, W. AND CHELLAPPA, R. 2000b. SFS BasedView synthesis for robust face recognition. InProceedings, International Conference on Automatic Face and Gesture Recognition.ZHAO, W. AND CHELLAPPA, R. 2000. Illuminationinsensitive face recognition using symmetricshapefromshading. In Proceedings, Conferenceon Computer Vision and Pattern Recognition.286293.ZHAO, W., CHELLAPPA, R., AND KRISHNASWAMY, A. 1998.Discriminant analysis of principal componentsfor face recognition. In Proceedings, International Conference on Automatic Face and GestureRecognition. 336341.ZHAO, W., CHELLAPPA, R., AND PHILLIPS, P. J. 1999.Subspace linear discriminant analysis for facerecognition. Tech. rep. CARTR914, Center forAutomation Research, University of Maryland,College Park, MD.ZHOU, S., KRUEGER, V., AND CHELLAPPA, R. 2003.Probabilistic recognition of human faces fromvideo. Comput. Vis. Image Understand. 91, 214245.Received July 2002 accepted June 2003ACM Computing Surveys, Vol. 35, No. 4, December 2003.
