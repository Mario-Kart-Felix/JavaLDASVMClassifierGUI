Maximum Entropy Markov Models
for Information Extraction and Segmentation
Andrew McCallum MCCALLUMJUSTRESEARCHCOM
Dayne Freitag DAYNEJUSTRESEARCHCOM
Just Research, 4616 Henry Street, Pittsburgh, PA 15213 USA
Fernando Pereira PEREIRARESEARCHATTCOM
AT&T Labs - Research, 180 Park Ave, Florham Park, NJ 07932 USA
Abstract
Hidden Markov models (HMMs) are a powerful
probabilistic tool for modeling sequential data
and have been applied with success to many
text-related tasks, such as part-of-speech tagging
text segmentation and information extraction. In
these cases, the observations are usually mod
eled as multinomial distributions over a discrete
vocabulary, and the HMM parameters are set
to maximize the likelihood of the observations
This paper presents a new Markovian sequence
model, closely related to HMMs, that allows ob
servations to be represented as arbitrary overlap
ping features (such as word, capitalization, for
matting, part-of-speech), and defines the condi
tional probability of state sequences given ob
servation sequences. It does this by using the
maximum entropy framework to fit a set of expo
nential models that represent the probability of a
state given an observation and the previous state
We present positive experimental results on the
segmentation of FAQs
1. Introduction
The large volume of text available on the Internet is caus
ing an increasing interest in algorithms that can automati
cally process and mine information from this text. Hidden
Markov models (HMMs) are a powerful tool for represent
ing sequential data, and have been applied with significant
success to many text-related tasks, including partofspeech
tagging (Kupiec, 1992), text segmentation and event track
ing (Yamron, Carp, Gillick, Lowe, & van Mulbregt,
named entity recognition (Bikel, Schwartz, & Weischedel
1999) and information extraction (Leek, 1997; Freitag amp
McCallum,
HMMs are probabilistic finite state models with parameters
for state-transition probabilities and state-specific observa
tion probabilities. Greatly contributing to their popularity
is the availability of straightforward procedures for train
ing by maximum likelihood (Baum-Welch) and for using
the trained models to find the most likely hidden state se
quence corresponding to an observation sequence Viterbi
In text-related tasks, the observation probabilities are typ
ically represented as a multinomial distribution over a dis
crete, finite vocabulary of words, and Baum-Welch training
is used to learn parameters that maximize the probability of
the observation sequences in the training data
There are two problems with this traditional approach
First, many tasks would benefit from a richer representa
tion of observations—in particular a representation that de
scribes observations in terms of many overlapping features
such as capitalization, word endings, part-of-speech, for
matting, position on the page, and node memberships in
WordNet, in addition to the traditional word identity. For
example, when trying to extract previously unseen com
pany names from a newswire article, the identity of a word
alone is not very predictive; however, knowing that the
word is capitalized, that is a noun, that it is used in an
appositive, and that it appears near the top of the article
would all be quite predictive (in conjunction with the con
text provided by the state-transition structure). Note that
these features are not independent of each other
Furthermore, in some applications the set of all possible
observations is not reasonably enumerable. For example
it may beneficial for the observations to be whole lines of
text. It would be unreasonable to build a multinomial distri
bution with as many dimensions as there are possible lines
of text. Consider the task of segmenting the questions and
answers of a frequently asked questions list (FAQ). The fea
tures that are indicative of the segmentation are not just the
individual words themselves, but features of the line as a
whole, such as the line length, indentation, total amount of
whitespace, percentage of non-alphabetic characters, and
grammatical features. We would like the observations to
be parameterized with these overlapping features
The second problem with the traditional approach is that
it sets the HMM parameters to maximize the likelihood of
the observation sequence; however, in most text applica
tions, including all those listed above, the task is to predict
the state sequence given the observation sequence. In other
words, the traditional approach inappropriately uses a gen
erative joint model in order to solve a conditional problem
in which the observations are given
This paper introduces maximum entropy Markov models
(MEMMs), which address both of these concerns. To al
low for non-independent, difficult to enumerate observa
tion features, we move away from the generative, joint
probability parameterization of HMMs to a conditional
model that represents the probability of reaching a state
given an observation and the previous state. These con
ditional probabilities are specified by exponential models
based on arbitrary observation features. The exponen
tial models follow from a maximum entropy argument, and
are trained by generalized iterative scaling (GIS) Darroch
& Ratcliff, 1972), which is similar in form and compu
tational cost to the expectation-maximization (EM) algo
rithm (Dempster, Laird, & Rubin, 1977). The “three classic
problems” (Rabiner, 1989) of HMMs can all be straightfor
wardly solved in this new model with new variants of the
forward-backward, Viterbi and Baum-Welch algorithms
The remainder of the paper describes our alternative model
in detail, explains how to fit the parameters using GIS, for
both known and unknown state sequences), and presents
the variant of the forward-backward procedure, out of
which solutions to the “classic problems” follow naturally
We also give experimental results for the problem of ex
tracting the question-answer pairs in lists of frequently
asked questions (FAQs), showing that our model increases
both precision and recall, the former by a factor of two
2. Maximum-Entropy Markov Models
A hiddenMarkov model (HMM) is a finite state automaton
with stochastic state transitions and observations Rabiner
1989). The automaton models a probabilistic generative
process whereby a sequence of observations is produced
by starting in some state, emitting an observation selected
by that state, transitioning to a new state, emitting another
observation—and so on until a designated final state is
reached. More formally, the HMM is given by a finite set of
states , a set of possible observations , two conditional
probability distributions: a state transition probability from
to , for and an observation probability
States as well as observations could be represented by fea
tures, but we will defer the discussion of that refinement to later
s
o
t
t
t-1s s
o
t-1s t
t
(a) b

Figure 1. (a) The dependency graph for a traditional HMM; b
for our conditional maximum entropy Markov model
distribution, for , and an initial state
distribution . A run of the HMM pairs an observation
sequence with a state sequence . In text
based tasks, the set of possible observations is typically a
finite character set or vocabulary
In a supervised task, such as information extraction, there is
a sequence of labels attached to each training ob
servation sequence . Given a novel observation
the objective is to recover the most likely label sequence
Typically, this is done with models that associate one or
more states with each possible label. If there is a onetoone
mapping between labels and states, the sequence of states
is known for any training instance; otherwise, the state se
quence must be estimated. To label an unlabeled observa
tion sequence, the Viterbi path is calculated, and the labels
associated with that path are returned
2.1 The New Model
As an alternative to HMMs, we propose maximum entropy
Markov models (MEMMs), in which the HMM transition
and observation functions are replaced by a single function
that provides the probability of the current state
given the previous state and the current observation
In this model, as in most applications of HMMs, the ob
servations are given—reflecting the fact that we don’t actu
ally care about their probability, only the probability of the
state sequence (and hence label sequence) they induce. In
contrast to HMMs, in which the current observation only
depends on the current state, the current observation in an
MEMMmay also depend on the previous state. It can then
be helpful to think of the observations as being associated
with state transitions rather than with states. That is, the
model is in the form of probabilistic finite-state acceptor
(Paz, 1971), in which is the probability of the
transition from state to state on input
In what follows, we will split into separately
trained transition functions . Each
of these functions is given by an exponential model, as de
scribed later in Section
Next we discuss how to solve the state estimation problem
in the new framework
2.2 State Estimation from Observations
Despite the differences between the new model and
HMMs, there is still an efficient dynamic programming so
lution the classic problem of identifying the most likely
state sequence given an observation sequence. The Viterbi
algorithm for HMMs fills in a dynamic programming ta
ble with forward probabilities , defined as the prob
ability of producing the observation sequence up to time
and being in state at time . The recursive Viterbi step is

In the new model, we redefine to be the probability
of being in state at time given the observation sequence
up to time . The recursive Viterbi step is then

The corresponding backward probability (used for
Baum-Welch, which is discussed later) is the probabil
ity of starting from state at time given the observa
tion sequence after time . Its recursive step is simply
. Space limitations pre
vent a full description here of Viterbi and Baum-Welch; see
Rabiner (1989) for an excellent tutorial
2.3 An Exponential Model for Transitions
The use of state-observation transition functions rather than
the separate transition and observation functions in HMMs
allows us to model transitions in terms of multiple, non
independent features of observations, which we believe to
be the most valuable contribution of the present work. To
do this, we turn to exponential models fit by maximum en
tropy
Maximum entropy is a framework for estimating probabil
ity distributions from data. It is based on the principle that
the best model for the data is the one that is consistent with
certain constraints derived from the training data, but other
wise makes the fewest possible assumptions. In our prob
abilistic framework, the distribution with the “fewest pos
sible assumptions” is that which is closest to the uniform
distribution, that is, the one with the highest entropy
Each constraint expresses some characteristic of the train
ing data that should also be present in the learned distribu
tion. Our constraints will be based on binary features
Examples of such features might be “the observation is the
word apple” or “the observation is a capitalized word” or
if the observations are whole lines of text at time, “the ob
servation is a line of text that has two noun phrases.” As
in other conditional maximum entropy models, features do
We use binary features in this paper, but the maximum en
tropy framework can in general handle real-valued features
not depend only on the observation but also on the outcome
predicted by the function being modeled. Here, that func
tion is the -specfic transition function , and the
outcome is the new current state . Thus, each feature
gives a function of two arguments, a current obser
vation and a possible new current state
In this paper, each such is a pair , where is a
binary feature of the observation alone and is a destina
tion state
if is true and
otherwise
The algorithm description that follows can be expressed in
terms of the generic feature without reference to this par
ticular feature decomposition. Furthermore, we will sug
gest later that more general features may be useful
The constraints we apply are that the expected value of each
feature in the learned distribution be the same as its average
on the training observation sequence (with corre
sponding state sequence ). Formally, for each pre
vious state and feature , the transition function
must have the property that

where are the time steps with , i.e., the
time steps that involve the transition function
The maximum entropy distribution that satisfies those con
straints (Della Pietra, Della Pietra, & Lafferty, 1997) is
unique, agrees with the maximum-likelihood distribution
and has the exponential form

where the are parameters to be learned and is
the normalizing factor that makes the distribution sum to
one across all next states
2.4 Parameter Estimation by Generalized Iterative
Scaling
GIS (Darroch & Ratcliff, 1972) finds iteratively the val
ues that form the maximum entropy solution for each tran
sition function (Eq. 4). It requires that the values of the
features sum to the same (arbitrary) constant for each
context . If this is not already true, we make it true by
adding a new ordinal-valued “correction” feature , where
, such that and
is chosen to be large enough that for all and

Inputs: An observation sequence , a correspond
ing sequence of labels , a certain number of states
each with a label, and potentially having a restricted transi
tion structure. Also, a set of observation-state features
Determine the state sequence associated with
the observation-label sequence. (When this is ambiguous
it can be determined probabilistically by iterating the next
two steps with EM
Deposit state-observation pairs into their correspond
ing previous states as training data for each state’s transi
tion function
Find the maximium entropy solution for each state’s dis
criminative function by running GIS
Output: A maximum-entropy-based Markov model that
takes an unlabeled sequence of observations and predicts
their corresponding labels
Table 1. An outline of the algorithm for estimating the parameters
of a Maximum-Entropy Markov model
The application of GIS to learning the transition function
for state consists of the following steps
1. Calculate the training data average of each feature

2. Start iteration 0 of GIS with some arbitrary parameter
values, say
3. At iteration , use the current values in
(Eq. 4) to calculate the expected value of each feature

4. Make a step towards satisfying the constraints by
changing each to bring the expected value of each
feature closer to corresponding training data average
5. Until convergence is reached, return to step
To summarize the overall MEMM training procedure, we
first split the training data into the eventsobservation
destination state pairs—relevant to the transitions from
each state . (Let us assume for the moment that, given
the labels in the training sequence, the state sequence is
unambiguously known.) We then apply GIS using the fea
ture statistics for the events assigned to each in order to
induce the transition function for . The set of these
functions defines the desired maximum-entropy Markov
model. Table 2.4 contains an overview of the maximum
entropy Markov model training algorithm
2.5 Parameter Estimation with Unknown State
The procedure described above assumes that the state se
quence of the training observation sequence is known; that
is, the states have to be predicted at test time but not train
ing time. Often, it is useful to be able to train when the
state sequence is not known. For example, there may be
more than one state with the same label, and for a given
label sequence it may be ambiguous which state produced
which label instance
We can use a variant of the Baum-Welch algorithm for this
The E-step calculates state occupancies using the forward
backward algorithm with the current transition functions
The M-step uses the GIS procedure with feature frequen
cies based on the E-step state occupancies to compute new
transition functions. This will maximize the likelihood of
the label sequence given the observations. Note that GIS
does not have to be run to convergence in each Mstep
not doing so would make this an example of Generalized
Expectation-Maximization (GEM), which is also guaran
teed to converge to a local maximum
Notice also that the same Baum-Welch variant can be
used with unlabeled or partially labeled training sequences
where, not only is the state unknown, but the label itself
is missing. These models could be trained with a combina
tion of labeled and unlabeled data, which is often extremely
helpful when labeled data is sparse
2.6 Variations
We have thus far described one particular method for max
imum entropy Markov models, but there are several other
possibilities
Factored state representation
One difficulty that MEMMs share with HMMs is that there
are transition parameters, making data sparseness
a serious problem as the number of states increases. Recall
that in our model observations are associated with transi
tions instead of states. This has advantages for expressive
power, but comes at the cost of having many more param
eters. For HMMs and related graphical models, tied pa
rameters and factored state representations (Ghahramani amp
Jordan, 1996; Kanazawa, Koller, & Russell, 1995) have
been used to alleviate this difficulty
We can achieve a similar effect in MEMMs by not split
ting into different functions . Instead
we would use a distributed representation for the previous
state as a collection of features with weights set by max
imum entropy, just as we have done for the observations
For example, state features might include “we have already
consumed the start-time extraction field,” “we haven’t yet
exited the preamble of the document,” “the subject of the
previous sentence is female” or “the last paragraph was an
answer.” One could also have second-order features link
ing observation and state features. With such a representa
tion, information would be shared among different source
states, reducing the number of parameters and thus improv
ing generalization. Furthermore, this proposal does not re
quire the difficult step of hand-crafting a parametertying
scheme or graphical model for the state transition function
as is required in HMMs and other graphical models
Observations in states instead of transitions
Rather than combining the transition and emission param
eters in a single function, the transition probabilities could
be represented as a traditional multinomial, , and
the influence of the observations could be repre
sented by a maximum-entropy exponential

This method of “correcting” a simple multinomial or prior
by adding extra features with maximum entropy has been
used previously in various statistical language modeling
problems. These include the combination of traditional tri
grams with “trigger word” features (Rosenfeld, 1994) and
the combination of arbitrary features of sentences with tri
gram models (Chen & Rosenfeld,
Note here that the observation and the previous state are
treated as independent evidence for the current state. This
approach would put the observations back in the states in
stead of the transitions. It would reduce the number of pa
rameters, and thus might be useful when training data is
especially sparse
An Environmental Model for Reinforcement Learning
The transition function can also include an action, , re
sulting in —a model suitable for representing
the envinronment of a reinforcement agent. The depen
dency on the action could be modeled either with separate
functions for each action, or with a factored represention of
actions in terms of arbitrary overlapping features, such as
“steer left,” “beep,” and “raise arm.” Certain particular ac
tions, states and observations with strong interactions can
be modeled as features that represent their conjunction
3. Experimental Results
We tested our method on a collection of 38 files belonging
to 7 Usenet multi-part FAQs downloaded from the Internet
All documents in this data set are organized according to
the same basic structure: each contains a header, which
See http://www.cs.cmu.edu/ mccallumfaqdata
Table 2. Excerpt from a labeled FAQ. Lines have been truncated
for reasons of space. The tags at the beginnings of lines were
inserted manually
<head>X-NNTP-Poster: NewsHound v
ltheadgt
<head>Archive-name: acornfaqpart
<head>Frequency: monthly
ltheadgt
<question>2.6) What configuration of serial cable should I use
ltanswergt
<answer> Here follows a diagram of the necessary connections
<answer>programs to work properly. They are as far as I know t
<answer>agreed upon by commercial comms software developers fo
ltanswergt
<answer> Pins 1, 4, and 8 must be connected together inside
<answer>is to avoid the well known serial port chip bugs. The
Table 3. Line-based features used in these experiments
begins-with-number containsquestionmark
begins-with-ordinal containsquestionword
begins-with-punctuation endswithquestionmark
begins-with-question-word firstalphaiscapitalized
begins-with-subject indented
blank indentedto
contains-alphanum indentedto
contains-bracketed-number morethanonethirdspace
contains-http onlypunctuation
contains-non-space previsblank
contains-number prevbeginswithordinal
contains-pipe shorterthan
includes text in Usenet header format and occasionally a
preamble or table of contents; a series of one or more ques
tion/answer pairs; and a tail, which typically includes items
such as copyright notices and acknowledgments, and var
ious artifacts reflecting the origin of the document. There
are also some formatting regularities, such as indentation
numbered questions and styles of paragraph breaks. The
multiple documents belonging to a single FAQ are format
ted in a consistent manner, but there is considerable varia
tion between different FAQs
We labeled each line in this document collection into one of
four categories, according to its role in the document: head
question, answer, tail, corresponding to the parts of docu
ments described in the previous paragraph. Table 2 shows
an excerpt from a labeled FAQ. The object of the task is
to recover these labels. This excerpt demonstrates the dif
ficulty of recovering line classifications by only looking at
the tokens that occur in the line. In particular, the numerals
in the answer might easily confuse a token-based classifier
We defined 24 Boolean features of lines, shown in Table
which we believed would be useful in determining the class
of a line. No effort was made to control statistical depen
dence between pairs of features. Although the set contains
a few feature pairs which are mutually disjoint, the features
represent partitions of the data that overlap to varying de
grees. Note also that the usefulness of a particular feature
such as indented, depends on the formatting conventions
of a particular FAQ
The results presented in this section are meant to answer
the question of how well can a MEMM trained on a single
manually labeled document label novel documents format
ted according to the same conventions. Our experiments
treat each group of documents belonging to the same FAQ
as a separate dataset. We train a model on a single doc
ument in such a group and test it on the remaining docu
ments in the group. In other words, we perform leave
-minus- -out” evaluation. Each group of documents
yields results. Scores are the average performance
across all FAQs in the collection
Given a sequence of lines (a test document) and a MEMM
we use the Viterbi algorithm to compute the most likely
state sequence. We consider three metrics in evaluating the
predicted sequences. The first is the co-occurrence agree
ment probability (COAP), proposed by Beeferman, Berger
and Lafferty
act pred act pred
where is a probability distribution over the set of
distances between lines; act is 1 if lines and are
in the same actual segment, and 0 otherwise; pred
is a similar indicator function for the predicted segmenta
tion; and is the XNOR function. This metric gives the
empirical probability that the actual and predicted segmen
tations agree on the placement of two lines drawn accord
ing to . In computing the COAP we define a segment to
be any unbroken sequence of lines with the same label. In
Beeferman et al. (1999), is an exponential distribution
depending on , a parameter calculated on features of the
dataset, such as average document length. For simplicity
we set to a uniform distribution of width 10. In other
words, our COAP measures the probability that any two
lines within 10 lines of each other are placed correctly by
the predicted segmentation
In constrast with the COAP, (which reflects the probabil
ity that segment boundaries are properly identified by the
learner, but ignores the labels assigned to the segments
themselves), the other two metrics only count as correct
those predicted segments that have the right labels. A seg
ment is counted as correct if it has the same boundaries and
label (e.g., question) as an actual segment. The segmen
tation precision (SP) is the number of correctly identified
segments divided by the number of segments predicted
The segmentation recall (SR) is the number of correctly
identified segments divided by the number of actual seg
ments
We tested four different models on this dataset
Table 4. Co-occurrence agreement probability (COAP), segmen
tation precision (SegPrec) and segmentation recall (SegRecall) of
four learners on the FAQ dataset. All these averages have
confidence intervals of 0.01 or less
Learner COAP SegPrec SegRecall
ME-Stateless 0.520 0.038
TokenHMM 0.865 0.276
FeatureHMM 0.941 0.413
MEMM 0.965 0.867
ME-Stateless: A single maximum entropy classifier
trained on and applied to each line independently, us
ing the 24 features shown in Table 3. MEStateless
can be considered typical of any approach that treats
lines in isolation from their context
TokenHMM: A traditional, fully connected HMM
with four states, one for each of the line categories
The states in the HMM emit individual tokens groups
of alphanumeric characters and individual punctuation
characters). The observation distribution at a given
state is a smoothed multinomial over possible tokens
The label assigned to a line is that assigned to the
state responsible for emitting the tokens in the line
In computing a state sequence for a document, the
model is allowed to switch states only at line bound
aries, thereby ensuring that all tokens in a line share
the same label. This model was used in previous work
on information extraction with HMMs (e.g. Freitag amp
McCallum,
FeatureHMM: Identical to TokenHMM, only the
lines in a document are first converted to sequences of
features from Table 3. For every feature that tests true
for a line, a unique symbol is inserted into the corre
sponding line in the converted document. The HMM
is trained to emit these symbols. Notice that the emis
sion model for each state is in this case a naı̈ve Bayes
model
MEMM: The maximum entropy Markov model de
scribed in this paper. As in the other HMMs, the
model contains four labeled states and is fully con
nected
Note that because training is fully supervised, the sequence
of states a training document passes through is unambigu
ous. Consequently, training does not involve BaumWelch
reestimation
Table 4 shows the performance of the four models on FAQ
data. It is clear from the table that MEMM is the best of
the methods tested. What is more, the results support two
claims that underpin our research into this problem. First
representing lines in terms of features salient to the prob
lem at hand is far more effective than a token-level rep
resentation, which is the essential difference between To
kenHMM and FeatureHMM. FeatureHMM does as well as
it does (surprisingly well) because it has access to mean
ingful features of the segmentation problem
The performance ofME-Stateless show that it is not possi
ble to classify lines unambiguously from the features alone
Our second claim is that structural regularities such as if
you have left the header, do not classify any further lines as
header lines” are critical. Markov models provide a conve
nient way to model such regularities. For the purposes of
segmentation, the results suggest that it is more important
to model structure than to have access to line features
The scores of the three Markov model methods on the
COAP metric indicate that they all do reasonably well at
segmenting FAQs into their constituent parts. In particu
lar, FeatureHMM segments almost as well as MEMM. The
more stringent metrics, SP and SR, which punish any mis
classified line in a predicted segment, hint at the main short
coming of the two non-maximum entropy Markov models
they occasionally interpolate bad predictions into otherwise
correctly handled segments
The precision (SP score) of MEMM has significant prac
tical implications. If the results of the segmentation are
to be used in an automatic system, then precision is crit
ical. In the case of FAQs, at least one such system
a question-answering system, has been described in the
literature (Burke, Hammond, Kulyukin, Lytinen, & To
muro, 1997). Whereas the segmentation returned by Fea
tureHMM is probably not of high enough quality for such
a use—not without manual intervention or rule-based post
processing—MEMM segmentation might be
4. Related Work
A wide range of machine-learning techniques have been
used in information extraction and text segmentation. In
this paper we focus exclusively on techniques based on
probabilistic models. Non-probabilistic methods such
as memory-based techniques (Argamon, Dagan, & Kry
molowski, 1998), transformation-based learning Brill
1995), and Winnow-based combinations of linear classi
fiers (Roth, 1998), do not give normalized scores to each
decision that can be combined into overall scores for de
cision sequences. Therefore, they do not support the stan
dard dynamic programming methods for finding the best
segmentation (Viterbi), and must thus resort to suboptimal
methods to label the observation sequence. Furthermore
they do not support hidden-variable reestimation Baum
Welch) methods, which are required for missing or incom
plete training labels
Exponential models derived by maximum entropy have
been applied with considerable success to many natural
language tasks, including language modeling for speech
recognition (Rosenfeld, 1994; Chen & Rosenfeld,
segmentation of newswire stories (Beeferman et al.,
part-of-speech tagging, prepositional phrase attachment
and parsing (Ratnaparkhi,
HMMs have also been successful in similar natural
language tasks, including part-of-speech tagging Kupiec
1992), named-entity recognition (Bikel et al., 1999) and
information extraction (Leek, 1997; Freitag & McCallum

However, we know of no previous general method that
combines the rich state representation of Markov models
with the flexible feature combination of exponential mod
els. The MENE named-entity recognizer (Borthwick, Ster
ling, Agichtein, & Grishman, 1998) uses an exponential
model to label each word with a label indicating the po
sition of the word in a labeled-entity class (start, inside
end or singleton), but the conditioning information does
not include the previous label, unlike our model. There
fore, it is closer to our ME-Stateless model. It is possible
that its inferior performance compared to an HMMbased
named-entity recognizer (Bikel et al., 1999) may have sim
ilar causes to the corresponding weakness of MEStateless
relative to FeatureHMM in our experiments—the lack of
representation of sequential dependencies
The model closest to our proposal is the part-of-speech tag
ger of Ratnaparkhi (1998). He starts with a maximum
entropy model of the joint distribution of word sequences
and the corresponding part-of-speech tags, but the practical
form of his model is a conditional Markov model whose
states encode the past two parts-of-speech and features of
the previous two and next two words. While our model
splits the transition functions for different source states
Ratnaparkhi’s combines all of them into a single exponen
tial model, which is more complex but may handle sparse
data better. Note, however, that it does not allow for arbi
trary state-transition structures and the relatively more ex
pressive context representation they allow
Nevertheless, the most direct inspiration for our model was
the work on Markov processes on curves (MPCs) (Saul amp
Rahim, 1999), which defines a class of conditional Markov
models mapping (continuous) segments of a trajectory in
acoustic space to states representing phonetic distinctions
Our model is a simpler, discrete time version of the same
observation-conditionalMarkovian architecture
5. Conclusions and Further Work
We have shown that it is possible to combine the advan
tages of HMMs and maximum-entropy models into a gen
eral model that allows state transitions to depend on non
independent features of the sequence under analysis. The
newmodel performs considerably better than either HMMs
or stateless maximum-entropy models on the task of seg
menting FAQs into questions and answers, and we believe
that the same technique can be advantageously applied to
many other text-related applications, for example named
entity recognition
We believe that a distributed state representation and non
fully connected topologies may facilitate applications to
more demanding tasks, such as information extraction with
large vocabulary and many features, as well as automatic
feature generation and selection following Della Pietra
et al. (1997). We also believe it would be worth investi
gating training with partially labeled data using the combi
nation of Baum-Welch and GIS discussed earlier. In the
longer term, the combination of maximum entropy and
conditional parameterization may be useful for a wider
range of graphical models than finite-state networks
Acknowledgements
Many thanks to John Lafferty for helpful discussions on
training with unknown state and on associating observa
tions with states instead of transitions, to Kamal Nigam for
help with GIS, and to Michael Collins for guidance on re
lated work
References
Argamon, S., Dagan, I., & Krymolowski, Y. (1998). A memory
based approach to learning shallow natural language pat
terns. In COLING-ACL 98, pp. 67–73 New Brunswick
New Jersey. Association for Computational Linguistics
Beeferman, D., Berger, A., & Lafferty, J. (1999). Statistical mod
els for text segmentation. Machine Learning,

Bikel, D. M., Schwartz, R. L., & Weischedel, R. M. (1999). An
algorithm that learns what’s in a name. Machine Learning
Journal, 34,
Borthwick, A., Sterling, J., Agichtein, E., & Grishman, R.
Exploiting diverse knowledge sources via maximum en
tropy in named entity recognition. In Proceedings of the
Sixth Workshop on Very Large Corpora New Brunswick
New Jersey. Association for Computational Linguistics
Brill, E. (1995). Transformation-based error-driven learning and
natural language processing: a case study in part of speech
tagging. Computational Linguistics, 21(4),
Burke, R., Hammond, K., Kulyukin, V., Lytinen, S., & Tomuro
N. (1997). Question answering from frequentlyasked
question files: Experiences with the FAQ Finder system
AI Magazine, 18,
Chen, S., & Rosenfeld, R. (1999). Efficient sampling and feature
selection in whole sentence maximum entropy language
models. In Proceedings of ICASSP’99. IEEE
Darroch, J. N., & Ratcliff, D. (1972). Generlized iterative scaling
for log-linear models. The Annals of Mathematical Statis
tics, 43(5),
Della Pietra, S., Della Pietra, V., & Lafferty, J. (1997). Inducing
features of random fields. IEEE Transactions on Pattern
Analysis and Machine Intelligence,
Dempster, A. P., Laird, N. M., & Rubin, D. B. (1977). Max
imum likelihood from incomplete data via the EM algo
rithm. Journal of the Royal Statistical Society, Series B
39(1),
Freitag, D., & McCallum, A. K. (1999). Information extraction
using hmms and shrinkage. In Papers from the AAAI
Workshop on Machine Learning for Information Extration
pp. 31–36 Menlo Park, California. AAAI
Ghahramani, Z., & Jordan, M. I. (1996). Factorial hidden Markov
models. In Mozer, M., Touretzky, D., & Perrone, M
(Eds.), Advances in Neural Information Processing Sys
tems 8. MIT Press
Kanazawa, K., Koller, D., & Russell, S. (1995). Stochastic simu
lation algorithms for dynamic probabilistic networks. In
Proceedings of the Eleventh Conference on Uncertainty
in Artificial Intelligence Montreal, Canada. Morgan Kauf
mann
Kupiec, J. (1992). Robust part-of-speech tagging using a hidden
Markov model. Computer Speech and Language, 6,

Leek, T. R. (1997). Information extraction using hidden Markov
models. Master’s thesis, UC San Diego
Paz, A. (1971). Introduction to Probabilistic Automata. Academic
Press
Rabiner, L. R. (1989). A tutorial on hidden Markov models and
selected applications in speech recognition. Proceedings
of the IEEE, 77(2),
Ratnaparkhi, A. (1998). Maximum Entropy Models for Natural
Language Ambiguity Resolution. Ph.D. thesis, University
of Pennsylvania
Rosenfeld, R. (1994). Adaptive Statistical Language Modeling: A
Maximum Entropy Approach. Ph.D. thesis, Carnegie Mel
lon University
Roth, D. (1998). Learning to resolve natural language ambigui
ties: a unified approach. In Proceedings of the Fifteenth
National Conference on Artificial Intelligence, pp.
813 Menlo Park, California. AAAI Press
Saul, L., & Rahim, M. (1999). Markov processes on curves for au
tomatic speech recognition. In Kearns, M. S., Solla, S. A
& Cohn, D. A. (Eds.), Advances in Neural Information
Processing Systems, Vol. 11 Cambridge, Massachusetts
MIT Press
Yamron, J., Carp, I., Gillick, L., Lowe, S., & van Mulbregt, P
(1998). A hidden Markov model approach to text segmen
tation and event tracking. In Proceedings of ICASSP
IEEE

