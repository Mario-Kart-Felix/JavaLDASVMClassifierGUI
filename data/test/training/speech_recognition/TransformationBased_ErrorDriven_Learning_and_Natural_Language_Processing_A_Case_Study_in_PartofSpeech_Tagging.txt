TransformationBased ErrorDrivenLearning and Natural LanguageProcessing A Case Study in Part ofSpeech TaggingEric BrillThe Johns Hopkins UniversityRecently there has been a rebirth of empiricism in the eld of natural languageprocessing Manual encoding of linguistic information is being challenged by automatedcorpusbased learning as a method of providing a natural language processing system withlinguistic knowledge Although corpusbased approaches have been successful in many different areas of natural language processing it is often the case that these methods capturethe linguistic information they are modelling indirectly in large opaque tables of statistics This can make it dicult to analyze understand and improve the ability of theseapproaches to model underlying linguistic behavior In this paper we will describe a simple rulebased approach to automated learning of linguistic knowledge This approach hasbeen shown for a number of tasks to capture information in a clearer and more directfashion without a compromise in performance We present a detailed case study of thislearning method applied to part of speech tagging IntroductionIt has recently become clear that automatically extracting linguistic information from asample text corpus can be an extremely powerful method for overcoming the linguisticknowledge acquisition bottleneck inhibiting the creation of robust and accurate naturallanguage processing systems A number of part of speech taggers are readily availableand widely used all trained and retrainable on text corpora Church  Cutting etal  Brill  Weischedel et al  Endemic structural ambiguity which canlead to such diculties as trying to cope with the many thousands of possible parsesthat a grammar can assign to a sentence can be greatly reduced by adding empiricallyderived probabilities to grammar rules Fujisaki et al  Sharman Jelinek and Mer Department of Computer Science Baltimore Md  Email brillcsjhueduc  Association for Computational LinguisticsComputational Linguistics Volume  Number cer  Black et al  and by computing statistical measures of lexical associationHindle and Rooth  Word sense disambiguation a problem which once seemed outof reach for systems without a great deal of hand crafted linguistic and world knowledgecan now in some cases be done with high accuracy when all information is derived automatically from corpora Brown Lai and Mercer  Yarowsky  Gale Churchand Yarowsky  Bruce and Wiebe  An eort has recently been undertaken tocreate automated machine translation systems where the linguistic information neededfor translation is extracted automatically from aligned text corpora Brown et al These are just a few of the many recent applications of corpusbased techniques in naturallanguage processingAlong with great research advances the infrastructure is in place for this line ofresearch to grow even stronger with online corpora the grist of the corpusbased naturallanguage processing grindstone getting bigger and better and becoming more readilyavailable There are a number of eorts worldwide to manually annotate large corporawith linguistic information including parts of speech phrase structure and predicateargument structure eg the Penn Treebank and the British National Corpus MarcusSantorini and Marcinkiewicz  Leech Garside and Bryant  A vast amountof online text is now available with much more becoming available in the future Usefultools such as large aligned corpora eg the aligned Hansards Gale and Church and semantic word hierarchies eg Wordnet Miller  have also recently becomeavailableCorpusbased methods are often able to succeed while ignoring the true complexitiesof language banking on the fact that complex linguistic phenomena can often be indirectly observed through simple epiphenomena For example one could accurately assigna part of speech tag to the word race in  without any reference to phrase structureor constituent movement simply by realizing that it is usually the case that a word thatis one or two words to the right of a modal is a verb and not a noun an exception beingwhen the word is also one word to the right of a determinerEric Brill TransformationBased ErrorDriven Learning He will raceVERB the car He will not raceVERB the car When will the raceNOUN endIt is an exciting discovery that simple stochastic ngram taggers can obtain veryhigh rates of tagging accuracy simply by observing xedlength word sequences withoutrecourse to the underlying linguistic structure However in order to make progress incorpusbased natural language processing we must become better aware of just whatcues to linguistic structure are really being captured and where the failings are in theseapproximations to the true underlying phenomena With many of the current corpusbased approaches to natural language processing this is a nearimpossible task to doFor example in the part of speech tagging example above this information about wordsthat follow modals would be hidden deeply in a stochastic ngram tagger in the thousandsor tens of thousands of contextual probabilities P TagijTagiTagi and the resultof multiplying dierent combinations of these probabilities togetherBelow we describe a new approach to corpusbased natural language processingcalled transformationbased errordriven learning This algorithm has been applied toa number of natural language problems including part of speech tagging prepositionalphrase attachment disambiguation and syntactic parsing Brill  Brill a Brillb Brill and Resnik  Brill  We have also recently begun exploring the useof this technique for letter to sound generation and for building pronunciation networksfor speech recognition In this approach the learned linguistic information is representedin a concise and easy to understand form This property should make transformationbased learning amenable to further exploring linguistic modelling and attempting todiscover ways of more tightly coupling the underlying linguistic systems and our approximating modelsComputational Linguistics Volume  Number UNANNOTATEDTEXTINITIAL STATEANNOTATEDTEXTTRUTHLEARNER RULESFigure TransformationBased ErrorDriven Learning TransformationBased ErrorDriven LearningFigure  illustrates how transformationbased errordriven learning works First unannotated text is passed through an initialstate annotator The initialstate annotatorcan range in complexity from assigning random structure to assigning the output ofa sophisticated manually created annotator In part of speech tagging various initialstate annotators that have been used include the output of a stochastic ngram taggerlabelling all words with their most likely tag as indicated in the training corpus andnaively labelling all words as nouns For syntactic parsing we have explored initial stateannotations ranging from the output of a sophisticated parser to random tree structurewith random nonterminal labelsOnce text has been passed through the initialstate annotator it is then comparedto the truth A manually annotated corpus is used as our reference for truth An orderedlist of transformations is learned that can be applied to the output of the initial stateEric Brill TransformationBased ErrorDriven Learningannotator to make it better resemble the truth There are two components to a transformation a rewrite rule and a triggering environment An example of a rewrite rule forpart of speech tagging isChange the tag from modal to nounand an example of a triggering environment isThe preceding word is a determinerTaken together the transformation with this rewrite rule and triggering environmentwhen applied to the word can would correctly change the mistaggedThedeterminer canmodal rustedverb toThedeterminer cannoun rustedverb An example bracketing rewrite rule is change the bracketing of a subtree from HHA HB CtoHHHA BCwhere A B and C can be either teminals or nonterminals One possible set of triggeringenvironments is any combination of words part of speech tags and nonterminal labelswithin and adjacent to the subtree Using this rewrite rule and the triggering environmentA  the the bracketingComputational Linguistics Volume  Number  the  boy ate  would become  the boy  ate In all of the applications we have examined to date the following greedy search isapplied for deriving a list of transformations at each iteration of learning the transformation is found whose application results in the best score according to the objectivefunction being used that transformation is then added to the ordered transformationlist and the training corpus is updated by applying the learned transformation Learningcontinues until no transformation can be found whose application results in an improvement to the annotated corpus Other more sophisticated search techniques could be usedsuch as simulated annealing or learning with a lookahead window but we have not yetexplored these alternativesFigure  shows an example of learning transformations In this example we assumethere are only four possible transformations T through T and that the objectivefunction is the total number of errors The unannotated training corpus is processed bythe initial state annotator and this results in an annotated corpus with  errorsdetermined by comparing the output of the initial state annotator with the manuallyderived annotations for this corpus Next we apply each of the possible transformationsin turn and score the resulting annotated corpus In this example we see that applyingtransformation T results in the largest reduction of errors and so T is learned as therst transformation T is then applied to the entire corpus and learning continues Atthis stage of learning transformation T results in the largest reduction of error and soit is learned as the second transformation After applying the initial state annotator thenT and then T we see that no further reduction in errors can be obtained from applyingany of the transformations and so learning stops To annotate fresh text this text is rst In the real implementation the search is data driven and therefore all transformations need not beexaminedEric Brill TransformationBased ErrorDriven LearningUnannotatedCorpusInitial    StateAnnotatorAnnotatedCorpusErrors  5,100AnnotatedAnnotatedAnnotatedAnnotatedAnnotatedAnnotatedAnnotatedAnnotatedAnnotatedAnnotatedAnnotatedAnnotatedCorpusCorpusCorpusCorpusCorpusCorpusCorpusCorpusCorpusCorpusCorpusCorpusErrors  5,100Errors  3,145Errors  3,910Errors  6,300Errors  3,310Errors  2,110Errors  1,231Errors  4,255Errors  1,410Errors  1,251Errors  1,231Errors  1,231T1T3 T3T4T4T2T4T1T1T2T2T3Figure An Example of TransformationBased ErrorDriven Learningannotated by the initial state annotator followed by the application of transformationT and then by the application of TTo dene a specic application of transformationbased learning one must specifythe following The initial state annotator The space of allowable transformations rewrite rules and triggering environments The objective function for comparing the corpus to the truth and choosing a transformationIn cases where the application of a particular transformation in one environmentcould aect its application in another environment two additional parameters must bespecied the order in which transformations are applied to a corpus and whether atransformation is applied immediately or only after the entire corpus has been examinedComputational Linguistics Volume  Number for triggering environments For example take the sequenceA A A A A Aand the transformation change the label from A to B if the previous label is A If theeect of the application of a transformation is not written out until the entire le hasbeen processed for that one transformation then regardless of the order of processingthe output will be B B B B B B since the triggering environment of a transformationis always checked before that transformation is applied to any surrounding objects inthe corpus If the eect of a transformation is recorded immediately then processing thestring left to right would result in A B A B A B whereas processing right to left wouldresult in B B B B B B A Comparison With Decision TreesThe technique employed by the learner is somewhat similar to that used in decision treesBreiman et al  Quinlan  Quinlan and Rivest  A decision tree is trainedon a set of preclassied entities and outputs a set of questions that can be asked aboutan entity to determine its proper classication Decision trees are built by nding thequestion whose resulting partition is the purest splitting the training data according tothat question and then recursively reapplying this procedure on each resulting subsetWe will rst show that the set of classications that can be provided via decisiontrees is a proper subset of those that can be provided via transformation lists an orderedlist of transformationbased rules given the same set of primitive questions We thengive some practical dierences between the two learning methods One possible measure for purity is entropy reductionEric Brill TransformationBased ErrorDriven Learning Decision Trees  Transformation ListsWe prove here that for a xed set of primitive queries any binary decision tree can beconverted into a transformation list Extending the proof beyond binary trees is straightforwardProof by inductionBase CaseGiven the following primitive decision tree where the classication is A if the answerto the query X is yes and the classication is B if the answer is noXB AYESNOthis tree can be converted into the following transformation list Label with S  Start State Annotation  If X then S  A S  B  Empty Tagging Environment  Always Applies To Entities Currently Labelled With S InductionAssume that two decision trees T and T have corresponding transformation lists Land L Assume that the arbitrary label names chosen in constructing L are not usedin L and that those in L are not used in L Given a new decision tree T constructedfrom from T and T as followsComputational Linguistics Volume  Number XYESNOT2 T1we construct a new transformation list L Assume the rst transformation in L isLabel with S and the rst transformation in L is Label with S The rst threetransformations in L will then be Label with S If X then S  S S  Sfollowed by all of the rules in L other than the rst rule followed by all of the rules in Lother than the rst rule The resulting transformation list will rst label an item as S ifX is true or as S is X is false Since S is the initialstate label for L the tranformationsfrom L will then be applied if X is true and likewise the transformations from L willbe applied if X is false Decision Trees  Transformation ListsWe show here that there exist transformation lists for which no equivalent decision treesexist for a xed set of primitive queries The following classication problem is oneexample given a sequence of characters classify a character based on whether the positionindex of a character is divisible by  querying only using a context of two characters tothe left of the character being classied Assuming transformations are applied left toright on the sequence the above classication problem can be solved for arbitrary lengthsequences if the eect of a transformation is written out immediately or for sequencesup to any prespecied length if a transformation is carried out only after all triggeringEric Brill TransformationBased ErrorDriven Learningenvironments in the corpus are checked We present the proof for the former case Giventhe input sequenceA A A A A A A A A A         the underlined characters should be classied as true because their indices are   and To see why a decision tree could not perform this classication regardless of orderof classication note that both the characters and classication of the two charactersbefore both A and A are the same although these two characters should be classieddierently Below is a transformation list for performing this classication Once again weassume transformations are applied lefttoright and that the result of a transformationis written out immediately so that the result of applying transformation x to characterai will always be known when applying transformation x to ai Label with SRESULT AS AS AS AS AS AS AS AS AS AS AS If there is no previous character then S  FRESULT AF AS AS AS AS AS AS AS AS AS AS If the character two to the left is labelled with F then S  FRESULT AF AS AF AS AF AS AF AS AF AS AF If the character two to the left is labelled with F then F  SRESULT AF AS AS AS AF AS AS AS AF AS AS F  yes S  noRESULT Ayes Ano Ano Ano Ayes Ano Ano Ano Ayes Ano AnoThe extra power of transformation lists comes from the fact that intermediate resultsfrom the classication of one object are reected in the current label of that objectthereby making this intermediate information available to be used in classifying otherComputational Linguistics Volume  Number objects This is not the case for decision trees where the outcome of questions asked issaved implicitly by the current location within the tree Some Practical Dierences Between Decision Trees and TransformationListsThere are a number of practical dierences between transformationbased errordrivenlearning and learning decision trees One dierence is that when training a decision treeeach time the depth of the tree is increased the average amount of training materialavailable per node at that new depth is halved for a binary tree In transformationbased learning the entire training corpus is used for nding all transformations andtherefore this method is more resistant to sparse data problems that arise as the depthof the decision tree being learned increasesTransformations are ordered with later transformations being dependent upon theoutcome of applying earlier transformations This allows for intermediate results in classifying one object to be available in classifying other objects For instance whether theprevious word is tagged as toinnitival or toprepositionmay be a good cue for determining the part of speech of a word If initially the word to is not reliably tagged everywherein the corpus with its proper tag or not tagged at all then this cue will be unreliableThe transformationbased learner will delay positing a transformation triggered by thetag of the word to until other transformations have resulted in a more reliable taggingof this word in the corpus For a decision tree to take advantage of this information thedecision tree path for a word whose outcome is dependent upon the tagging of to wouldhave to have the entire decision tree structure for properly classifying each occurrenceof to built into the classication for that word If the classication of to were dependentupon the classication of yet another word this would have to be built into the decisiontree as well Unlike decision trees intermediate classication results in transformation The original tagged Brown Corpus Francis and Kucera  makes this distinction the PennTreebank Marcus Santorini and Marcinkiewicz  does notEric Brill TransformationBased ErrorDriven Learningbased learning are available and can be used as classication progresses Even if decisiontrees are applied to a corpus in a lefttoright fashion they only are allowed one pass inwhich to properly classifySince a transformation list is a processor and not a classier it can readily be usedas a postprocessor to any annotation system In addition to annotating from scratchrules can be learned to improve the performance of a mature annotation system by usingthe mature system as the initialstate annotator This can have the added advantagethat the list of transformations learned using a mature annotation system as the startstate annotator provide us with a readable description or classication of the errors themature system makes thereby aiding in the renement of that system Being a processor gives exibility beyond the classierbased decision tree For example in applyingtransformationbased learning to parsing a rule can apply any structural change to atree In tagging a rule such asChange the tag of the current word to X and of the previous word to Y if Z holdscan easily be handled in the processorbased system while it would be dicult to do thisin a classication systemIn transformationbased learning the objective function used in training is the sameas that used for evaluation whenever this is feasible In a decisiontree using systemaccuracy as an objective function for training typically results in poor performance andsome measure of node purity such as entropy reduction is used instead The direct correlation between rules and performance improvement in transformationbased learning canmake the learned rules more readily interpretable than decision tree rules for increasingpopulation purity For a discussion on why this is the case see Breiman et al  pages  For a discussion of other issues regarding these two learning algorithms see Ramshaw and MarcusComputational Linguistics Volume  Number  Part of Speech Tagging A Case Study in TransformationBased ErrorDriven LearningIn this section we describe the practical application of transformationbased learning topart of speech tagging Part of speech tagging is a good application to test the learner fora number of reasons There are a number of large tagged corpora available allowing fora variety of experiments to be run Part of speech tagging is an active area of researchwith a great deal of work having been done in this area over the past few years egJelinek  Church  DeRose  Hindle  DeMarcken  Merialdo Brill  Black et al  Cutting et al  Kupiec  Charniak et al Weischedel et al  Schutze and Singer Part of speech tagging is also a very practical application with uses in many areasincluding speech recognition and generation machine translation parsing informationretrieval and lexicography Insofar as tagging can be seen as a prototypical problem inlexical ambiguity advances in part of speech tagging could readily translate to progressin other areas of lexical and perhaps structural ambiguity such as word sense disambiguation and prepositional phrase attachment disambiguation Also it is possible tocast a number of other useful problems as part of speech tagging problems such as letterto sound translation Huang SonBell and Baggett  and building pronunciationnetworks for speech recognition Recently a method has been proposed for using part ofspeech tagging techniques as a method for parsing with lexicalized grammars Joshi andSrinivas When automated part of speech tagging was initially explored Klein and Simmons Harris  people manually engineered rules for tagging sometimes with theaid of a corpus As large corpora became available it became clear that simple Markov All of the programs described herein are freely available with no restrictions on use orredistribution For information on obtaining the tagger contact the author In Brill and Resnik  we describe an approach to prepositional phrase attachmentdisambiguation that obtains highly competitive performance compared to other corpusbasedsolutions to this problem This system was derived in under two hours from thetransformationbased part of speech tagger described in this paperEric Brill TransformationBased ErrorDriven Learningmodel based stochastic taggers that were automatically trained could achieve high rates oftagging accuracy Jelinek  Markovmodel based taggers assign to a sentence the tagsequence that maximizes ProbwordjtagProbtagjprevious n tags These probabilitiescan be estimated directly from a manually tagged corpus These stochastic taggershave a number of advantages over the manually built taggers including obviating theneed for laborious manual rule construction and possibly capturing useful informationthat may not have been noticed by the human engineer However stochastic taggershave the disadvantage that linguistic information is only captured indirectly in largetables of statistics Almost all recent work in developing automatically trained part ofspeech taggers has been on further exploring Markovmodel based tagging Jelinek Church  DeRose  DeMarcken  Merialdo  Cutting et al Kupiec  Charniak et al  Weischedel et al  Schutze and Singer  Transformationbased Errordriven Part of Speech TaggingTransformationbased part of speech tagging works as follows The initial state annotatorassigns each word its most likely tag as indicated in the training corpus The methodused for initially tagging unknown words will be described in a later section An orderedlist of transformations is then learned to improve tagging accuracy based on contextualcues These transformations alter the tagging of a word from X to Y i either The word was not seen in the training corpus OR The word was seen tagged with Y at least once in the training corpusIn taggers based on Markov models the lexicon consists of probabilities of the somewhat counterintuitive but proper form P WORDjTAG In the transformationbasedtagger the lexicon is simply a list of all tags seen for a word in the training corpus with One can also estimate these probabilities without a manually tagged corpus using a hidden Markovmodel However it appears to be the case that directly estimating probabilities from even a verysmall manually tagged corpus gives better results than training a hidden Markov model on a largeuntagged corpus see Merialdo  Earlier versions of this work were reported in Brill  Brill Computational Linguistics Volume  Number one tag labelled as the most likely Below we show a lexical entry for the word half inthe transformationbased tagger A description of the part of speech tags is provided inAppendix Ahalf CD DT JJ NN PDT RB VBThis entry lists the seven tags seen for half in the training corpus with NN marked asthe most likely Below are the lexical entries for half in a Markov model tagger extractedfrom the same corpusP half jCD  P half jDT   P half jJJ  P half jNN   P half jPDT   P half jRB  P half jV B  It is dicult to make much sense of these entries in isolation they have to be viewed inthe context of the many contextual probabilitiesFirst we will describe a nonlexicalized version of the tagger where transformationtemplates do not make reference to specic words In the nonlexicalized tagger thetransformation templates we use areChange tag a to tag b when The preceding following word is tagged z The word two before after is tagged z One of the two preceding following words is tagged zEric Brill TransformationBased ErrorDriven Learning One of the three preceding following words is tagged z The preceding word is tagged z and the following word is tagged w The preceding following word is tagged z and the word two before afteris tagged wwhere abz and w are variables over the set of parts of speech To learn a transformation the learner in essence applies every possible transformation counts the numberof tagging errors after that transformation is applied and chooses that transformationresulting in the greatest error reduction Learning stops when no transformations can befound whose application reduces errors beyond some prespecied thresholdIn the experiments described below processing was done left to right For eachtransformation application all triggering environments are rst found in the corpus andthen the transformation triggered by each triggering environment is carried outThe search is datadriven so only a very small percentage of possible transformationsreally need be examined In gure  we give pseudocode for the learning algorithm inthe case where there is only one transformation template Change the tag from X to Yif the previous tag is Z In each learning iteration the entire training corpus is examinedonce for every pair of tags X and Y nding the best transformation whose rewrite changestag X to tag Y For every word in the corpus whose environment matches the triggeringenvironment if word has tag X and X is the correct tag then making this transformationwill result in an additional tagging error and so we increment the number of errors causedwhen making the transformation given the part of speech tag of the previous word lines and  If X is the current tag and Y is the correct tag then the transformation willresult in one less error and so we increment the number of improvements caused whenmaking the transformation given the part of speech tag of the previous word lines  and All possible instantiations of transformation templatesComputational Linguistics Volume  Number  apply initial state annotator to corpus while transformations can still be found do for from tag  tag to tagn for to tag  tag to tagn for corpus position   to corpus size if correct tagcorpus position  to tag current tagcorpus position  from tag num good transformationstagcorpus position  else if correct tagcorpus position  from tag current tagcorpus position  from tag num bad transformationstagcorpus position  nd maxT num good transformationsT  num bad transformationsT if this is the best scoring rule found yet then store as best ruleChange tag from from tag to to tag if previous tag is T apply best rule to training corpus append best rule to ordered list of transformationsFigure Pseudocode for learning transformationsEric Brill TransformationBased ErrorDriven LearningIn certain cases a signicant speedup for training the transformationbased taggercan be obtained by indexing in the corpus where dierent transformations can and doapply For a description of a fast indexbased training algorithm see Ramshaw andMarcus In gure  we list the rst twenty transformations learned from training on the PennTreebank Wall Street Journal Corpus Marcus Santorini and Marcinkiewicz The rst transformation states that a noun should be changed to a verb if the previous tagis TO as in toTO conictNNVB with The second transformation xes a taggingsuch as mightMD vanishVBPVB The third xes mightMD not replyNNVBThe tenth transformation is for the token s which is a separate token in the PennTreebank s is most frequently used as a possessive ending but after a personal pronounit is a verb John s compared to he s The transformations changing IN to WDT arefor tagging the word that to determine in which environments that is being used as asynonym of which Lexicalizing the TaggerIn general no relationships between words have been directly encoded in stochastic ngram taggers In the Markov model typically used for stochastic tagging state transitionprobabilities P TagijTagi   T agin express the likelihood of a tag immediatelyfollowing n other tags and emit probabilities P WordjjTagi express the likelihoodof a word given a tag Many useful relationships such as that between a word and theprevious word or between a tag and the following word are not directly captured byMarkovmodel based taggers The same is true of the nonlexicalized transformationbased tagger where transformation templates do not make reference to wordsTo remedy this problem we extend the transformationbased tagger by adding contextual transformations that can make reference to words as well as part of speech tags Version  of the Penn Treebank was used in all experiments reported in this paper In Kupiec  a limited amount of lexicalization is introduced by having a stochastic taggerwith word states for the  most frequent words in the corpusComputational Linguistics Volume  Number Change Tag From To Condition NN VB Previous tag is TO VBP VB One of the previous three tags is MD NN VB One of the previous two tags is MD VB NN One of the previous two tags is DT VBD VBN One of the previous three tags is VBZ VBN VBD Previous tag is PRP VBN VBD Previous tag is NNP VBD VBN Previous tag is VBD VBP VB Previous tag is TO POS VBZ Previous tag is PRP VB VBP Previous tag is NNS VBD VBN One of previous three tags is VBP IN WDT One of next two tags is VB VBD VBN One of previous two tags is VB VB VBP Previous tag is PRP IN WDT Next tag is VBZ IN DT Next tag is NN JJ NNP Next tag is NNP IN WDT Next tag is VBD JJR RBR Next tag is JJFigure The rst  nonlexicalized transformationsEric Brill TransformationBased ErrorDriven LearningThe transformation templates we add areChange tag a to tag b when The preceding following word is w The word two before after is w One of the two preceding following words is w The current word is w and the preceding following word is x The current word is w and the preceding following word is tagged z The current word is w The preceding following word is w and the preceding following tag is t The current word is w the preceding following word is w and thepreceding following tag is twhere w and x are variables over all words in the training corpus and z is a variableover all parts of speech Below we list two lexicalized transformations that were learnedtraining once again on the Wall Street JournalChange the tag From IN to RB if the word two positions to the right is as From VBP to VB if one of the previous two words is ntThe Penn Treebank tagging style manual species that in the collocation as   asthe rst as is tagged as an adverb and the second is tagged as a preposition Since asis most frequently tagged as a preposition in the training corpus the initial state tagger In the Penn Treebank nt is treated as a separate token so dont becomes doVBP ntRBComputational Linguistics Volume  Number  RB VBP RB VBRB  VBPRB  VBFigure Trigram Tagger Probability Tableswill mistag the phrase as tall as asasIN tallJJ asINThe rst lexicalized transformation corrects this mistagging Note that a bigram tagger trained on our training set would not correctly tag the rst occurrence of as Although adverbs are more likely than prepositions to follow some verb form tags the factthat P asjIN  is much greater than P asjRB and P JJ jIN  is much greater thanP JJ jRB lead to as being incorrectly tagged as a preposition by a stochastic tagger Atrigram tagger will correctly tag this collocation in some instances due to the fact thatP IN jRB JJ is greater than P IN jIN JJ but the outcome will be highly dependentupon the context in which this collocation appearsThe second transformation arises from the fact that when a verb appears in a contextsuch as We do nt eat or We did nt usually drink the verb is in base form A stochastictrigram tagger would have to capture this linguistic information indirectly from frequencycounts of all trigrams of the form shown in gure  where a star can match any part ofspeech tag and from the fact that P ntjRB is fairly highIn Weischedel et al  results are given when training and testing a Markovmodel based tagger on the Penn Treebank Tagged Wall Street Journal Corpus They citeresults making the closed vocabulary assumption that all possible tags for all words inthe test set are known When training contextual probabilities on  million words anaccuracy of  was achieved Accuracy dropped to  when contextual probabilEric Brill TransformationBased ErrorDriven Learningities were trained on  words We trained the transformationbased tagger on thesame corpus making the same closed vocabulary assumption When training contextual rules on  words an accuracy of  was achieved on a separate word test set When the training set was reduced to  words accuracy dropped to The transformationbased learner achieved better performance despite the factthat contextual information was captured in a small number of simple nonstochasticrules as opposed to  contextual probabilities that were learned by the stochastictagger See table  When training on  words a total of  transformations werelearned However transformations toward the end of the list contribute very little toaccuracy So only applying the rst  learned transformations to the test set achievesan accuracy of  Applying the rst  gives an accuracy of  To match the accuracy achieved by the stochastic tagger when it was trained on one millionwords only the rst  transformations are neededTo see whether lexicalized transformations were contributing to the transformationbased tagger accuracy rate we ran the exact same test using the tagger trained using thenonlexical transformation template subset Accuracy of that tagger was  Addinglexicalized transformations resulted in a  decrease in the error rate These resultsare summarized in table We found it a bit surprising that the addition of lexicalized transformations did notresult in a much greater improvement in performance When transformations are allowedto make reference to words and word pairs some relevant information is probably misseddue to sparse data We are currently exploring the possibility of incorporating wordclasses into the rulebased learner in hopes of overcoming this problem The idea is quite In both Weischedel et al  and here the test set was incorporated into the lexicon but wasnot used in learning contextual information Testing with no unknown words might seem like anunrealistic test We have done so for three reasons We show results when unknown words areincluded later in the paper  to allow for a comparison with previously quoted results  toisolate known word accuracy from unknown word accuracy and  in some systems such as aclosed vocabulary speech recognition system the assumption that all words are known is valid The training we did here was slightly suboptimal in that we used the contextual rules learned withunknown words described in the next section and lled in the dictionary rather than training ona corpus without unknown wordsComputational Linguistics Volume  Number Training  of RulesCorpus or Context AccMethod Size Words Probs Stochastic  K  Stochastic  Million  RuleBasedWith Lex Rules  K  RuleBasedWith Lex Rules  K  RuleBasedwo Lex Rules  K  Table Comparison of Tagging Accuracy With No Unknown WordsEric Brill TransformationBased ErrorDriven Learningsimple Given any source of word class information such as WordNet Miller  thelearner is extended such that a rule is allowed to make reference to parts of speech wordsand word classes allowing for rules such as Change the tag from X to Y if the followingword belongs to word class Z This approach has already been successfully applied to asystem for prepositional phrase attachment attachment disambiguation Brill and Resnik Tagging Unknown WordsSo far we have not addressed the problem of unknown words As stated above theinitial state annotator for tagging assigns all words their most likely tag as indicated ina training corpus Below we show how a transformationbased approach can be taken fortagging unknown words by automatically learning cues to predict the most likely tag forwords not seen in the training corpus If the most likely tag for unknown words can beassigned with high accuracy then the contextual rules can be used to improve accuracyas described aboveIn the transformationbased unknownword tagger the initial state annotator naivelylabels the most likely tag for unknown words as proper noun if capitalized and commonnoun otherwiseBelow we list the set of allowable transformationsChange the tag of an unknown word from X to Y if Deleting the prex sux x jxj   results in a word x is any string oflength  to  The rst last  characters of the word are x Adding the character string x as a prex sux results in a wordjxj   If we change the tagger to tag all unknown words as common nouns then a number of rules arelearned of the form change tag to proper noun if the prex is E A B etc sincethe learner is not provided with the concept of upper case in its set of transformation templatesComputational Linguistics Volume  Number  Word W ever appears immediately to the left right of the word Character Z appears in the wordAn unannotated text can be used to check the conditions in all of the above transformation templates Annotated text is necessary in training to measure the eect oftransformations on tagging accuracy Since the goal is to label each lexical entry for newwords as accurately as possible accuracy is measured on a per type and not a per tokenbasisFigure  shows the rst  transformations learned for tagging unknown words inthe Wall Street Journal corpus As an example of how rules can correct errors generatedby prior rules note that applying the rst transformation will result in the mistaggingof the word actress The th learned rule xes this problem This rule states change atag from plural common noun to singular common noun if the word has sux ssKeep in mind that no specic axes are prespecied A transformation can makereference to any string of characters up to a bounded length So while the rst rulespecies the English sux s the rule learner was not constrained from consideringsuch nonsensical rules as change a tag to adjective if the word has sux xhqr Alsoabsolutely no Englishspecic information such as an ax list need be prespecied inthe learnerWe then ran the following experiment using  million words of the Penn TreebankTagged Wall Street Journal Corpus  words were used for training and words were used for testing Annotations of the test corpus were not used in any wayto train the system From the  word training corpus  words were used tolearn rules for tagging unknown words and  words were used to learn contextualrules  rules were learned for tagging unknown words and  contextual tagging This learner has also been applied to tagging Old English See Brill b Although thetransformations are not Englishspecic the set of transformation templates would have to beextended to process languages with dramatically dierent morphologyEric Brill TransformationBased ErrorDriven LearningChange Tag From To Condition NN NNS Has sux s NN CD Has character  NN JJ Has character  NN VBN Has sux ed NN VBG Has sux ing  RB Has sux ly  JJ Adding sux ly results in a word NN CD The word  can appear to the left NN JJ Has sux al NN VB The word would can appear to the left NN CD Has character  NN JJ The word be can appear to the left NNS JJ Has sux us NNS VBZ The word it can appear to the left NN JJ Has sux ble NN JJ Has sux ic NN CD Has character  NNS NN Has sux ss  JJ Deleting the prex un results in a word NN JJ Has sux iveFigure The rst  transformations for unknown wordsComputational Linguistics Volume  Number rules were learned Unknown word accuracy on the test corpus was  and overalltagging accuracy on the test corpus was  To our knowledge this is the highestoverall tagging accuracy ever quoted on the Penn Treebank Corpus when making the openvocabulary assumption Using the tagger without lexicalized rules an overall accuracyof  and an unknown word accuracy of  is obtained A graph of accuracy as afunction of transformation number on the test set for lexicalized rules is shown in gure Before applying any transformations test set accuracy is  So the transformationsreduce the error rate by  over the baseline The high baseline accuracy is somewhatmisleading as this includes the tagging of unambiguous words Baseline accuracy whenthe words that are unambiguous in our lexicon are not considered is  However it isdicult to compare taggers using this gure as then the accuracy of the system dependson the particular lexicon used For instance in our training set the word the was taggedwith a number of dierent tags and so according to our lexicon the is ambiguous If weinstead used a lexicon where the is listed unambiguously as a determiner the baselineaccuracy would be For tagging unknown words each word is initially assigned a part of speech based onword and worddistribution features Then the tag may be changed based on contextualcues via contextual transformations that are applied to the entire corpus both knownand unknown words When the contextual rule learner learns transformations it doesso in an attempt to maximize overall tagging accuracy and not unknown word taggingaccuracy Unknown words account for only a small percentage of the corpus in our experiments typically two to three percent Since the distributional behavior of unknownwords is quite dierent from that of known words and since a transformation that doesnot increase unknown word tagging accuracy can still be benecial to overall taggingaccuracy the contextual transformations learned are not optimal in the sense of leadingto the highest tagging accuracy on unknown words Better unknown word accuracy maybe possible by training and using two sets of contextual rules one maximizing knownword accuracy and the other maximizing unknown word accuracy and then applying theEric Brill TransformationBased ErrorDriven LearningTransformation NumberTest Set Accuracy0 100 200 300 40093949596Test Set Accuracy93949596Figure Accuracy vs Transformation NumberComputational Linguistics Volume  Number Corpus AccuracyPenn WSJ Penn Brown Orig Brown Table Tagging Accuracy on Dierent Corporaappropriate transformations to a word when tagging depending upon whether the wordappears in the lexicon We are currently experimenting with this ideaIn Weischedel et al  a statistical approach to tagging unknown words isshown In this approach a number of suxes and important features are prespeciedThen for unknown wordspW jT   punknown wordjT pCapitalizefeaturejT psuffixes hyphenationjT Using this equation for unknown word emit probabilities within the stochastic taggeran accuracy of  was obtained on the Wall Street Journal corpus This portion of thestochastic model has over  parameters with  possible unique emit probabilitiesas opposed to a small number of simple rules that are learned and used in the rulebasedapproach In addition the transformationbased method learns specic cues instead ofhaving them prespecied allowing for the possibility of uncovering cues not apparent tothe human language engineer We have obtained comparable performance on unknownwords while capturing the information in a much more concise and perspicuous mannerand without prespecifying any information specic to English or to a specic corpusIn table  we show tagging results obtained on a number of dierent corpora in eachcase training on roughly  x  words total and testing on a separate test set of x  words Accuracy is consistent across these corpora and tag setsIn addition to obtaining high rates of accuracy and representing relevant linguisticinformation in a small set of rules the part of speech tagger can also be made to runextremely fast Roche and Schabes  shows a method for converting a list of taggingEric Brill TransformationBased ErrorDriven Learningtransformations into a deterministic nite state transducer with one state transition takenper word of input the result being a transformationbased tagger whose tagging speedis about ten times that of the fastest Markovmodel tagger KBest TagsThere are certain circumstances where one is willing to relax the one tag per wordrequirement in order to increase the probability that the correct tag will be assigned toeach word In DeMarcken  Weischedel et al  kbest tags are assigned withina stochastic tagger by returning all tags within some threshold of probability of beingcorrect for a particular wordWe can modify the transformationbased tagger to return multiple tags for a wordby making a simple modication to the contextual transformations described aboveThe initialstate annotator is the tagging output of the previously described onebesttransformationbased tagger The allowable transformation templates are the same asthe contextual transformation templates listed above but with the rewrite rule changetag X to tag Y modied to add tag X to tag Y or add tag X to word W Instead ofchanging the tagging of a word transformations now add alternative taggings to a wordWhen allowing more than one tag per word there is a tradeo between accuracyand the average number of tags for each word Ideally we would like to achieve as largean increase in accuracy with as few extra tags as possible Therefore in training we ndtransformations that maximize the functionNumber of corrected errorsNumber of additional tagsIn table  we present results from rst using the onetagperword transformationbased tagger described in the previous section and then applying the kbest tag transformations These transformations were learned from a separate  word corpus Asa baseline we did kbest tagging of a test corpus as follows Each known word in thetest corpus was tagged with all tags seen with that word in the training corpus and theComputational Linguistics Volume  Number  of Rules Accuracy Avg  of tags per word            Table Results from kbest taggingve most likely unknown word tags were assigned to all words not seen in the trainingcorpus This resulted in an accuracy of  with an average of  tags per wordThe transformationbased tagger obtained the same accuracy with  tags per wordone third the number of additional tags as the baseline tagger ConclusionsIn this paper we have described a new transformationbased approach to corpusbasedlearning We have given details of how this approach has been applied to part of speechtagging and have demonstrated that the transformationbased approach obtains competitive performance with stochastic taggers on tagging both unknown and known wordsThe transformationbased tagger captures linguistic information in a small number ofsimple nonstochastic rules as opposed to large numbers of lexical and contextual probabilities This learning approach has also been applied to a number of other tasks includingprepositional phrase attachment disambiguation Brill and Resnik  bracketing textBrill a and labeling nonterminal nodes Brill c Recently we have begun toexplore the possibility of extending these techniques to other problems including learningpronunciation networks for speech recognition and learning mappings between syntactic Thanks to Fred Jelinek and Fernando Pereira for suggesting this baseline experiment Unfortunately it is dicult to nd results to compare these kbest tag results to In DeMarcken the test set is included in the training set and so it is dicult to know how this systemwould do on fresh text In Weischedel et al  a kbest tag experiment was run on the WallStreet Journal corpus They quote the average number of tags per word for various thresholdsettings but do not provide accuracy resultsEric Brill TransformationBased ErrorDriven Learningand semantic representationsA Penn Treebank Part of Speech Tags Excluding Punctuation CC Coordinating conjunction CD Cardinal number DT Determiner EX Existential there FW Foreign word IN Preposition or subordinating conjunction JJ Adjective JJR Adjective comparative JJS Adjective superlative LS List item marker MD Modal NN Noun singular or mass NNS Noun plural NNP Proper noun singular NNPS Proper noun plural PDT Predeterminer POS Possessive ending PP Personal pronoun PP Possessive pronoun RB Adverb RBR Adverb comparative RBS Adverb superlative RP Particle SYM Symbol TO to UH Interjection VB Verb base form VBD Verb past tense VBG Verb gerund or present participle VBN Verb past participle VBP Verb nonrd person singular present VBZ Verb rd person singular present WDT Whdeterminer WP Whpronoun WP Possessive whpronoun WRB WhadverbAcknowledgmentsThis work was funded in part by NSF grantIRI In addition this work wasdone in part while the author was in theSpoken Language Systems Group atMassachusetts Institute of Technology underARPA grant NJ and byDARPAAFOSR grant AFOSR atComputational Linguistics Volume  Number the University of Pennsylvania Thanks toMitch Marcus Mark Villain and theanonymous reviewers for many usefulcomments on earlier drafts of this paperReferencesBlack Ezra Fred Jelinek John LaertyDavid Magerman Robert Mercer andSalim Roukos  Towardshistorybased grammars Using richermodels for probabilistic parsing InProceedings of the st Annual Meeting ofthe Association for ComputationalLinguistics Columbus OhioBlack Ezra Fred Jelinek John LaertyRobert Mercer and Salim Roukos Decision tree models applied to thelabeling of text with partsofspeech InDarpa Workshop on Speech and NaturalLanguage Harriman NYBreiman Leo Jerome Friedman RichardOlshen and Charles Stone Classication and regression treesWadsworth and BrooksBrill Eric  A simple rulebased partof speech tagger In Proceedings of theThird Conference on Applied NaturalLanguage Processing ACL Trento ItalyBrill Eric a Automatic grammarinduction and parsing free text Atransformationbased approach InProceedings of the st Meeting of theAssociation of Computational LinguisticsColumbus OhBrill Eric b A CorpusBasedApproach to Language Learning PhDthesis Department of Computer andInformation Science University ofPennsylvaniaBrill Eric c Transformationbasederrordriven parsing In Proceedings of theThird International Workshop on ParsingTechnologies Tilburg The NetherlandsBrill Eric  Some advances inrulebased part of speech tagging InProceedings of the Twelfth NationalConference on Articial IntelligenceAAAI Seattle WaBrill Eric and Philip Resnik  Atransformationbased approach toprepositional phrase attachmentdisambiguation In Proceedings of theFifteenth International Conference onComputational LinguisticsCOLING Kyoto JapanBrown Peter John Cocke Stephen DellaPietra Vincent Della Pietra Fred JelinekJohn Laerty Robert Mercer and PaulRoossin  A statistical approach tomachine translation ComputationalLinguistics Eric Brill TransformationBased ErrorDriven LearningBrown Peter Jennifer Lai and RobertMercer  Wordsense disambiguationusing statistical methods In Proceedingsof the th Annual Meeting of theAssociation for ComputationalLinguistics Berkeley CaBruce Rebecca and Janyce Wiebe Wordsense disambiguation usingdecomposable models In Proceedings ofthe nd Annual Meeting of theAssociation for ComputationalLinguistics Las Cruces NMCharniak Eugene Curtis Hendrickson NeilJacobson and Michael Perkowitz Equations for part of speech tagging InProceedings of the Conference of theAmerican Association for ArticialIntelligence AAAI Washington DCChurch Kenneth  A stochastic partsprogram and noun phrase parser forunrestricted text In Proceedings of theSecond Conference on Applied NaturalLanguage Processing ACL Austin TxCutting Doug Julian Kupiec JanPedersen and Penelope Sibun  Apractical partofspeech tagger InProceedings of the Third Conference onApplied Natural Language ProcessingACL Trento ItalyDeMarcken Carl  Parsing the LOBcorpus In Proceedings of the Conference of the Association forComputational Linguistics PittsburghPaDeRose Stephen  Grammaticalcategory disambiguation by statisticaloptimization Computational LinguisticsFrancis Winthrop Nelson and HenryKucera  Frequency analysis ofEnglish usage Lexicon and grammarHoughton Miin BostonFujisaki Tetsu Fred Jelinek John Cockeand Ezra Black  Probabilisticparsing method for sentencedisambiguation In Proceedings of theInternational Workshop on ParsingTechnologiesGale William and Kenneth Church A program for aligning sentences inbilingual corpora In Proceedings of theth Annual Meeting of the Associationfor Computational Linguistics BerkeleyCaGale William Kenneth Church and DavidYarowsky  A method fordisambiguating word senses in a largecorpus Computers and the HumanitiesHarris Zellig  String Analysis ofLanguage Structure Mouton and CoComputational Linguistics Volume  Number The HagueHindle D and M Rooth  Structuralambiguity and lexical relationsComputational Linguistics Hindle Donald  Acquiringdisambiguation rules from text InProceedings of the th Annual Meeting ofthe Association for ComputationalLinguistics Vancouver BCHuang Caroline Mark SonBell and DavidBaggett  Generation ofpronunciations from orthographies usingtransformationbased errordrivenlearning In International Conference onSpeech and Language Processing ICSLPYokohama JapanJelinek Fred  SelfOrganizedLanguage Modelling for SpeechRecognition Dordrecht In Impact ofProcessing Techniques onCommunication J Skwirzinski edJoshi Aravind and B Srinivas Disambiguation of super parts of speechor supertags Almost parsing InProceedings of the th InternationalConference on Computational LinguisticsKyoto JapanKlein Sheldon and Robert Simmons A computational approach togrammatical coding of English wordsJACM Kupiec Julian  Robust partofspeechtagging using a hidden Markov modelComputer Speech and Language Leech Georey Roger Garside and MichaelBryant  Claws The tagging of theBritish National Corpus In Proceedingsof the th International Conference onComputational Linguistics Kyoto JapanMarcus Mitchell Beatrice Santorini andMaryann Marcinkiewicz  Building alarge annotated corpus of English thePenn Treebank ComputationalLinguistics Merialdo Bernard  Tagging englishtext with a probabilistic modelComputational LinguisticsMiller George  Wordnet an onlinelexical database International Journal ofLexicography Quinlan J Ross  Induction ofdecision trees Machine LearningQuinlan J Ross and Ronald Rivest Inferring decision trees using theminimum description length principleInformation and Computation Ramshaw Lance and Mitchell Marcus Exploring the statistical derivationof transformational rule sequences forEric Brill TransformationBased ErrorDriven Learningpartofspeech tagging In The BalancingAct Proceedings of the ACL Workshop onCombining Symbolic and StatisticalApproaches to Language New MexicoState UniversityRoche Emmanuel and Yves Schabes Deterministic part of speech tagging withnite state transducers ComputationalLinguistics Schutze Hinrich and Yoram Singer Part of speech tagging using a variablememory Markov model In Proceedings ofthe Association for ComputationalLinguistics Las Cruces New MexicoSharman Robert Fred Jelinek and RobertMercer  Generating a grammar forstatistical training In Proceedings of the Darpa Speech and Natural LanguageWorkshopWeischedel Ralph Marie Meteer RichardSchwartz Lance Ramshaw and JePalmucci  Coping with ambiguityand unknown words through probabilisticmodels Computational LinguisticsYarowsky David  Wordsensedisambiguation using statistical models ofRogets categories trained on largecorpora In Proceedings of COLINGpages  Nantes France July
