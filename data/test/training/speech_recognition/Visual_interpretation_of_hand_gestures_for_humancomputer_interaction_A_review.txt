IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,  VOL.  19,  NO.  7,  JULY  1997 677Visual Interpretation of Hand Gesturesfor HumanComputer Interaction A ReviewVladimir I. Pavlovic, Student Member, IEEE,Rajeev Sharma, Member, IEEE,and Thomas S. Huang, Fellow, IEEEAbstractThe use of hand gestures provides an attractive alternative to cumbersome interface devices for humancomputerinteraction HCI. In particular, visual interpretation of hand gestures can help in achieving the ease and naturalness desired for HCI.This has motivated a very active research area concerned with computer visionbased analysis and interpretation of hand gestures.We survey the literature on visual interpretation of hand gestures in the context of its role in HCI. This discussion is organized on thebasis of the method used for modeling, analyzing, and recognizing gestures. Important differences in the gesture interpretationapproaches arise depending on whether a 3D model of the human hand or an image appearance model of the human hand is used.3D hand models offer a way of more elaborate modeling of hand gestures but lead to computational hurdles that have not beenovercome given the realtime requirements of HCI. Appearancebased models lead to computationally efficient purposiveapproaches that work well under constrained situations but seem to lack the generality desirable for HCI. We also discussimplemented gestural systems as well as other potential applications of visionbased gesture recognition. Although the currentprogress is encouraging, further theoretical as well as computational advances are needed before gestures can be widely used forHCI. We discuss directions of future research in gesture recognition, including its integration with other natural modes of humancomputer interaction.Index TermsVisionbased gesture recognition, gesture analysis, hand tracking, nonrigid motion analysis, humancomputerinteraction.      1 INTRODUCTIONITH the massive influx of computers in society, humancomputer interaction, or HCI, has become an increasingly important part of our daily lives. It is widely believedthat as the computing, communication, and display technologies progress even further, the existing HCI techniquesmay become a bottleneck in the effective utilization of theavailable information flow. For example, the most popularmode of HCI is based on simple mechanical deviceskeyboards and mice. These devices have grown to be familiar but inherently limit the speed and naturalness withwhich we can interact with the computer. This limitationhas become even more apparent with the emergence ofnovel display technology such as virtual reality 2, 78,41. Thus in recent years there has been a tremendous pushin research toward novel devices and techniques that willaddress this HCI bottleneck.One longterm attempt in HCI has been to migrate thenatural means that humans employ to communicate witheach other into HCI. With this motivation automatic speechrecognition has been a topic of research for decades. Tremendous progress has been made in speech recognition,and several commercially successful speech interfaces havebeen deployed 75. However, it has only been in recentyears that there has been an increased interest in trying tointroduce other humantohuman communication modalitiesinto HCI. This includes a class of techniques based on themovement of the human arm and hand, or hand gestures.Human hand gestures are a means of nonverbal interaction among people. They range from simple actions of using our hand to point at and move objects around to themore complex ones that express our feelings and allow usto communicate with others.To exploit the use of gestures in HCI it is necessary toprovide the means by which they can be interpreted bycomputers. The HCI interpretation of gestures requires thatdynamic andor static configurations of the human hand,arm, and even other parts of the human body, be measurable by the machine. First attempts to solve this problemresulted in mechanical devices that directly measure handandor arm joint angles and spatial position. This group isbest represented by the socalled glovebased devices 9, 32,88, 70, 101. Glovebased gestural interfaces require theuser to wear a cumbersome device, and generally carry aload of cables that connect the device to a computer. Thishinders the ease and naturalness with which the user caninteract with the computer controlled environment. Eventhough the use of such specific devices may be justified by ahighly specialized application domain, for example simulation of surgery in a virtual reality environment, theeveryday user will certainly be deterred by such cumbersome interface tools. This has spawned active research toward more natural HCI techniques.016288289710.00  1997 IEEE V. Pavlovic and T.S. Huang are with The Beckman Institute and Department of Electrical and Computer Engineering, University of Illinois, Urbana, IL 61801. Email vladimirifp.uiuc.edu. R. Sharma is with the Department of Computer Science and Engineering,Pennsylvania State University, University Park, PA 16802. Email rsharmacse.psu.edu.Manuscript received 17 Nov. 1995 revised 5 May 1997. Recommended for acceptance by R. Kasturi.For information on obtaining reprints of this article, please send email totranspamicomputer.org, and reference IEEECS Log Number 105027.W678 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,  VOL.  19,  NO.  7,  JULY  1997Potentially, any awkwardness in using gloves and otherdevices can be overcome by using videobased noncontactinteraction techniques. This approach suggests using a setof video cameras and computer vision techniques to interpret gestures. The nonobstructiveness of the resulting visionbased interface has resulted in a burst of recent activityin this area. Other factors that may have contributed to thisincreased interest include the availability of fast computingthat makes realtime vision processing feasible, and recentadvances in computer vision techniques. Numerous approaches have been applied to the problem of visual interpretation of gestures for HCI, as will be seen in the following sections. Many of those approaches have been chosenand implemented so that they focus on one particular aspect of gestures, such as, hand tracking, hand posture estimation, or hand pose classification. Many studies havebeen undertaken within the context of a particular application, such as using a finger as a pointer to control a TV, orinterpretation of American Sign Language.Until recently, most of the work on visionbased gesturalHCI has been focused on the recognition of static handgestures or postures. A variety of models, most of themtaken directly from general object recognition approaches,have been utilized for that purpose. Images of hands, geometric moments, contours, silhouettes, and 3D hand skeleton models are a few examples. In recent year, however,there has been an interest in incorporating the dynamiccharacteristics of gestures. The rationale is that hand gestures are dynamic actions and the motion of the hands conveys as much meaning as their posture does. Numerousapproaches, ranging from global hand motion analysis toindependent fingertip motion analysis, have been proposedfor gesture analysis. There has thus been rapid growth ofvarious studies related to visionbased gesture analysis fueled by a need to develop more natural and efficient humancomputer interfaces. These studies are reported indisparate literature and are sometimes confusing in theirclaims and their scope. Thus there is a growing need tosurvey the stateoftheart in visionbased gesture recognition and to systematically analyze the progress toward visionbased gestural humancomputer interface. This paperattempts to bring together the recent progress in visualgesture interpretation within the context of its role in HCI.We organize the survey by breaking the discussion intothe following main components based on the general viewof a gesture recognition system as shown in Fig. 1 Gesture Modeling Section 2 Gesture Analysis Section 3 Gesture Recognition Section 4 GestureBased Systems and Applications Section 5The first phase of a recognition task whether consideredexplicitly or implicitly in a particular study is choosing amodel of the gesture. The mathematical model may consider both the spatial and temporal characteristic of thehand and hand gestures. We devote Section 2 to an indepth discussion of gesture modeling issues. The approachused for modeling plays a pivotal role in the nature andperformance of gesture interpretation.Once the model is decided upon, an analysis stage isused to compute the model parameters from the imagefeatures that are extracted from single or multiple videoFig. 1. Visionbased gesture interpretation system. Visual images of gesturers are acquired by one or more video cameras. They are processed inthe analysis stage where the gesture model parameters are estimated. Using the estimated parameters and some higher level knowledge, theobserved gestures are inferred in the recognition stage.PAVLOVIC ET AL.    VISUAL INTERPRETATION OF HAND GESTURES FOR HUMANCOMPUTER INTERACTION A REVIEW 679input streams. These parameters constitute some description of the hand pose or trajectory and depend on themodeling approach used. Among the important problemsinvolved in the analysis are that of hand localization,hand tracking, and selection of suitable image features.We discuss these and other issues of gesture analysis inSection 3.The computation of model parameters is followed bygesture recognition. Here, the parameters are classified andinterpreted in the light of the accepted model and perhapsthe rules imposed by some grammar. The grammar couldreflect not only the internal syntax of gestural commandsbut also the possibility of interaction of gestures with othercommunication modes like speech, gaze, or facial expressions. Evaluation of a particular gesture recognition approach encompasses both accuracy, robustness, and speed,as well as the variability in the number of different classesof handarm movements it covers. We survey the variousgesture recognition approaches in Section 4.A major motivation for the reported studies on gesturerecognition is the potential to use hand gestures in variousapplications aiming at a natural interaction between thehuman and various computercontrolled displays. Some ofthese applications have been used as a basis for defininggesture recognition, using a purposive formulation of theunderlying computer vision problem. In Section 5 we survey the reported as well as other potential applications ofvisual interpretation of hand gestures.Although the current progress in gesture recognition isencouraging, further theoretical as well as computationaladvances are needed before gestures can be widely used forHCI. We discuss some of the directions of research forgesture recognition, including its integration with othernatural modes of humancomputer interaction in Section 6.This is followed by concluding remarks in Section 7.2 GESTURE MODELINGIn order to systematically discuss the literature on gestureinterpretation, it is important to first consider what modelthe authors have used for the hand gesture. In fact, thescope of a gestural interface for HCI is directly related tothe proper modeling of hand gestures. How to model handgestures depends primarily on the intended applicationwithin the HCI context. For a given application, a verycoarse and simple model may be sufficient. However, if thepurpose is a naturallike interaction, a model has to be established that allows many if not all natural gestures to beinterpreted by the computer. The following discussion addresses the question of modeling of hand gestures for HCI.2.1 Definition of GesturesOutside the HCI framework, hand gestures cannot be easilydefined. The definitions, if they exist, are particularly related to the communicational aspect of the human handand body movements. Websters Dictionary, for example,defines gestures as ...the use of motions of the limbs orbody as a means of expression a movement usually of thebody or limbs that expresses or emphasizes an idea, sentiment, or attitude. Psychological and social studies tend tonarrow this broad definition and relate it even more tomans expression and social interaction 48. However, inthe domain of HCI the notion of gestures is somewhat different. In a computer controlled environment one wants touse the human hand to perform tasks that mimic both thenatural use of the hand as a manipulator, and its use inhumanmachine communication control of computermachine functions through gestures. Classical definitions of gestures, on the other hand, are rarely, if ever,concerned with the former mentioned use of the humanhand so called practical gestures 48.Fig. 2. Production and perception of gestures. Hand gestures originateas a mental concept G, are expressed Thg through arm and handmotion H, and are perceived Tvh as visual images V.Hand gestures are a means of communication, similar tospoken language. The production and perception of gestures can thus be described using a model commonly foundin the field of spoken language recognition 85, 100. Aninterpretation of this model, applied to gestures, is depictedin Fig. 2. According to the model, gestures originate as agesturers mental concept, possibly in conjunction withspeech. They are expressed through the motion of arms andhands, the same way speech is produced by air streammodulation through the human vocal tract. Also, observersperceive gestures as streams of visual images which theyinterpret using the knowledge they possess about thosegestures. The production and perception model of gesturescan also be summarized in the following formH  ThgG 1V  TvhH 2V  TvhThgG  TvgG 3Transformations T. can be viewed as different models Thg isa model of hand or arm motion given gesture G, Tvh is amodel of visual images given hand or arm motion H, andTvg describes how visual images V are formed given somegesture G. The models are parametric, with the parametersbelonging to their respective parameter spaces 07.. In lightof this notation, one can say that the aim of visual interpretation of hand gestures is to infer gestures G from their visual images V using a suitable gesture model Tvg, orG T Vvg1 4In the context of visual interpretation of gestures, it may thenbe useful to consider the following definition of gesturesA hand gesture is a stochastic process in the gesture model parameterspace 0T over a suitably defined time interval ,.680 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,  VOL.  19,  NO.  7,  JULY  1997Each realization of one gesture can then been seen as a trajectory in the model parameter space. For example, in performing a gesture the human hands position in 3D spacedescribes a trajectory in such space, Fig. 3. The stochasticproperty in the definition of gestures affirms their naturalcharacter no two realizations of the same gesture will result in the same hand and arm motion or the same set ofvisual images. The presence of the time interval , suggeststhe gestures dynamic nature.Fig. 3. Gesture as a stochastic process. Gestures can be viewed asrandom trajectories in parameter spaces which describe hand or armspatial states. In this example, two different gestures are shown in athree dimensional parameter space. One realization of Gesture 1 is atrajectory in that space solid line.The gesture analysis and gesture recognition problemscan then be posed in terms of the parameters involved inthe above definition. For example, the problem of constructing the gestural model T. over the parameter set 0T,or the problem of defining the gesture interval ,.2.2 Gestural TaxonomySeveral alternative taxonomies have been suggested in theliterature that deal with psychological aspects of gestures.Kendon 48 distinguishes autonomous gestures thatoccur independently of speech from gesticulationgestures that occur in association with speech. McNeilland Levy 65 recognize three groups of gestures iconicand metaphoric gestures, and beats. The taxonomy thatseems most appropriate within the context of HCI was recently developed by Quek 71, 72. A slightly modifiedversion of the taxonomy is given in Fig. 4.All handarm movements are first classified into twomajor classes gestures and unintentional movements.Unintentional movements are those handarm movementsthat do not convey any meaningful information. Gesturesthemselves can have two modalities communicative and manipulative.Manipulative gestures are the ones used to act on objects inan environment object movement, rotation, etc. Communicative gestures, on the other hand, have an inherentcommunicational purpose. In a natural environment theyare usually accompanied by speech. Communicative gestures can be either acts or symbols. Symbols are those gestures that have a linguistic role. They symbolize some referential action for instance, circular motion of index fingermay be a referent for a wheel or are used as modalizers,often of speech Look at that wing and a modalizinggesture specifying that the wing is vibrating, for example. InHCI context these gesture are, so far, one of the most commonly used gestures since they can often be represented bydifferent static hand postures, as we will discuss further inSection 5. Finally, acts are gestures that are directly related tothe interpretation of the movement itself. Such movementsare classified as either mimetic which imitate some actions ordeictic pointing acts.Taxonomy of gestures largely influences the way parameter space 0T and gesture interval , are determined. Arelated issue is the classification of gestural dynamics,which we consider next.2.3 Temporal Modeling of GesturesSince human gestures are a dynamic process, it is importantto consider the temporal characteristics of gestures. This mayhelp in the temporal segmentation of gestures from otherunintentional handarm movements. In terms of our generaldefinition of hand gestures, this is equivalent to determiningthe gesture interval ,. Surprisingly, psychological studies arefairly consistent about the temporal nature of hand gestures.Kendon 48 calls this interval a gesture phrase. It has beenestablished that three phases make a gesture preparation, nucleus peak or stroke 65, and retraction.The preparation phase consists of a preparatory movementthat sets the hand in motion from some resting position.Fig. 4. A taxonomy of hand gestures for HCI. Meaningful gestures aredifferentiated from unintentional movements. Gestures used for manipulation examination of objects are separated from the gestureswhich possess inherent communicational character.PAVLOVIC ET AL.    VISUAL INTERPRETATION OF HAND GESTURES FOR HUMANCOMPUTER INTERACTION A REVIEW 681The nucleus of a gesture has some definite form and enhanced dynamic qualities 48. Finally, the hand either returns to the resting position or repositions for the new gesture phase. An exception to this rule is the so called beatsgestures related to the rhythmic structure of the speech.The above discussion can guide us in the process oftemporal discrimination of gestures. The three temporalphases are distinguishable through the general handarmmotion Preparation and retraction are characterizedby the rapid change in position of the hand, while thestroke, in general, exhibits relatively slower hand motion.However, as it will be seen in Section 4, the complexity ofgestural interpretation usually imposes more stringent constraints on the allowed temporal variability of hand gestures. Hence, a work in visionbased gesture HCI sometimes reduces gestures to their static equivalents, ignoringtheir dynamic nature.2.4 Spatial Modeling of GesturesGestures are observed as hand and arm movements, actionsin 3D space. The description of gestures, hence, also involves the characterization of their spatial properties. In aHCI domain this characterization has so far been mainlyinfluenced by the kind of application for which the gesturalinterface is intended. For example, some applications requiresimple models like static image templates of the human handin TV set control in 35, while some others require more sophisticated ones 3D hand model used by 56, for instance.If one considers the gesture production and perceptionmodel suggested in Section 2.1, two possible approaches togesture modeling may become obvious. One approach maybe to try to infer gestures directly from the visual imagesobserved, as stated by 4. This approach has been oftenused to model gestures, and is usually denoted as appearancebased modeling. Another approach may result if theintermediate tool for gesture production is considered thehuman hand and arm. In this case, a two step modelingprocess may be followedH T Vvh1 5 G T Hhg1 6In other words, one can first model the motion and postureof the hand and arm H  and then infer gestures G  from themotion and posture model parameters. A group of modelswhich follows this approach is known as 3Dmodelbased.Fig. 5 shows the two major approaches used in the spatial modeling of gestures. We examine the two approachesmore closely in the following subsections.2.4.1 3D HandArm ModelThe 3D hand and arm models have often been a choice forhand gesture modeling. They can be classified in two largegroups volumetric models and skeletal models.Volumetric models are meant to describe the 3D visualappearance of the human hand and arms. They are commonly found in the field of computer animation 64, buthave recently also been used in computer vision applications. In the field of computer vision volumetric models ofthe human body are used for analysisbysynthesis trackingand recognition of the bodys posture 52, 105. Briefly, theidea behind the analysisbysynthesis approach is to analyze the bodys posture by synthesizing the 3D model of thehuman body in question and then varying its parametersuntil the model and the real human body appear as thesame visual images. Most of the volumetric models used incomputer animation are complex 3D surfaces NURBS ornonuniform rational Bsplines which enclose the parts ofthe human body they model 64. Even though such modelshave become quite realistic, they are too complex to be rendered in realtime. A more appealing approach, suitable torealtime computer vision, lies in the use of simple 3D geometric structures to model the human body 68. Structureslike generalized cylinders and superquadrics which encompass cylinders, spheres, ellipsoids and hyperrectangles areoften used to approximate the shape of simple body parts,like finger links, forearm, or upperarm 6, 20, 29, 31,37. The parameters of such geometric structures are quitesimple. For example, a cylindrical model is completely described with only three parameters height, radius, andcolor. The 3D models of more complex body parts, likehands, arms, or legs, are then obtained by connecting together the models of the simpler parts 46. In addition tothe parameters of the simple models, these structures contain the information on connections between the basicparts. The information may also include constraints whichdescribe the interaction between the basic parts in thestructure. There are two possible problems in using suchelaborate hand and arm models. First, the dimensionality ofthe parameter space is high more than 23  3 parametersper hand. Second, and more importantly, obtaining theparameters of those models via computer vision techniquesmay prove to be quite complex.Instead of dealing with all the parameters of a volumetric hand and arm model, models with a reduced set ofequivalent joint angle parameters together with segmentlengths are often used. Such models are known as skeletalmodels. Skeletal models are extensively studied in the human hand morphology and biomechanics 92, 95. Webriefly describe the basic notions relevant to our discussion.The human hand skeleton consists of 27 bones, divided inthree groupsFig. 5. Spatial models of gestures. 3D hand modelbased models ofgestures use articulated models of the human hand and arm to estimate the hand and arm movement parameters. Such movements arelater recognized as gestures. Appearancebased models directly linkthe appearance of the hand and arm movements in visual images tospecific gestures.682 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,  VOL.  19,  NO.  7,  JULY  1997 carpals wrist boneseight, metacarpals palm bonesfive, and phalanges finger bones14.The joints connecting the bones naturally exhibit differentdegrees of freedom DoF. Most of the joints connecting carpalshave very limited freedom of movement. The same holds forthe carpalmetacarpal joints except for the TM, see Fig. 7.Finger joints show the most flexibility For instance, the MCPand the TM joint have two DoFs one for extensionflexionand one for adductionabduction, while the PIP and the DIPjoints have one DoF extensionflexion. Equally importantto the notion of DoF is the notion of dependability betweenthe movements in neighboring joints. For instance, it isnatural to most people to bend flexextend their fingerssuch that both PIP and DIP joints flexextend. Also, there isonly a certain range of angles that the hand joints can naturally assume. Hence, two sets of constraints can be placedon the joint angle movements static range and dynamicdependencies. One set of such constraints was used byKuch 55 in his 26 DoF hand modelStatic ConstraintsFingers Thumb0 90  MCP sy,o  15 15o o MCP sx,Dynamic Constraints PIPyDIPy 32 IPyMCPy MCPyPIPy 12 TMyMCPy 13 MCPxMCP convergexMCP sxMCP sxMCPy 90 , ,,e j TMxMCPx 12where superscripts denote flexionsextensions y or adductionabduction x movements in local, joint centeredcoordinate systems. In another example, Lee and Kunii 59,60 developed a 27 degree of freedom hand skeleton modelwith an analogous set of constraints. Similar skeletonbasedmodels of equal or lesser complexity have been used byother authors 4, 66, 76, 77, 97.2.4.2 AppearanceBased ModelThe second group of models is based on appearance ofhandsarms in the visual images. This means that themodel parameters are not directly derived from the 3Dspatial description of the hand. The gestures are modeledby relating the appearance of any gesture to the appearanceof the set of predefined, template gestures.A large variety of models belong to this group. Some arebased on deformable 2D templates of the human hands,arms, or even body 18, 21, 45, 49, 58. Deformable 2Dtemplates are the sets of points on the outline of an object,used as interpolation nodes for the object outline approximation. The simplest interpolation function used is apiecewise linear function. The templates consist of the average point sets, point variability parameters, and socalledexternal deformations. Average point sets describe theaverage shape within a certain group of shapes. Pointvariability parameters describe the allowed shape deformation variation within that same group of shapes. Thesetwo types of parameters are usually denoted as internal.For instance, the human hand in open position has one shapeon the average, and all other instances of any open posture ofthe human hand can be formed by slightly varying the average shape. Internal parameters are obtained through principalcomponent analysis PCA of many of the training sets of data.External parameters or deformations are meant to describe               a b c d eFig. 6. Hand models. Different hand models can be used to represent the same hand posture. a 3D Textured volumetric model. b 3D wireframevolumetric model. c 3D skeletal model. d Binary silhouette. e Contour.Fig. 7. Skeletonbased model of the human hand. The human handskeleton consists of 27 bones. This model, on the other hand, approximates the anatomical structure using five serial link chainswith 19 links.PAVLOVIC ET AL.    VISUAL INTERPRETATION OF HAND GESTURES FOR HUMANCOMPUTER INTERACTION A REVIEW 683the global motion of one deformable template. Rotationsand translations are used to describe such motion. Templatebased models are used mostly for handtracking purposes 18, 49. They can also be used for simple gestureclassification based on the multitude of classes of templates58. Trajectories of external parameters of deformable templates have also been used for simple gesture recognition45. Extensions of the 2D template approach to 3D deformable models have also been recently explored. For example,3D point distribution model has been employed for gesturetracking 42.A different group of appearancebased models uses 2Dhand image sequences as gesture templates. Each gesturefrom the set of allowed gestures is modeled by a sequenceof representative image ntuples. Furthermore, each element of the ntuple corresponds to one view of the samehand or arm. In the most common case, only onemonoscopic or two stereoscopic views are used. Parameters of such models can be either images themselves orsome features derived from the images. For instance, complete image sequences of the human hands in motion canbe used as templates per se for various gestures 25, 26.Images of fingers only can also be employed as templates22 in a finger tracking application. Another recently pursued approach has been to model different gestural actionsby motion history images or MHIs 12. MHIs are 2D imagesformed by accumulating the motion of every single pixel inthe visual image over some temporal window. This way theintensity of the pixel in the MHI relates to how much prolonged motion is observed at that pixel.The majority of appearancebased models, however, useparameters derived from images in the templates. We denote this class of parameters as hand image property parameters. They include contours and edges, image moments,and image eigenvectors, to mention a few. Many of theseparameters are also used as features in the analysis of gestures see Section 3. Contours as a direct model parameterare often used simple edgebased contours 17, 81 orsignatures contours in polar coordinates 14 are somepossible examples. Contours can also be employed as thebasis for further eigenspace analysis 23, 67. Other parameters that are sometimes used are image moments 80,86. They are easily calculated from handarm silhouettesor contours. Finally, many other parameters have beenused Zernike moments 79 and orientation histograms34, for example.Another group of models uses fingertip positions as parameters. This approach is based on the assumption thatthe position of fingertips in the human hand, relative to thepalm, is almost always sufficient to differentiate a finitenumber of different gestures. The assumption holds in 3Dspace under several restrictions some of them were notedby Lee and Kunii 59, 60 The palm must be assumed tobe rigid, and the fingers can only have a limited number ofDoFs. However, most of the models use only 2D locationsof fingertips and the palm 3, 28, 57. Applications thatare concerned with deictic gestures usually use only a single index fingertip and some other reference point on thehand or body 36, 57, 73.3 GESTURE ANALYSISIn the previous section, we discussed different approachesfor modeling gestures for HCI. In this section we considerthe analysis phase where the goal is to estimate the parameters of the gesture model using measurements fromthe video images of a human operator engaged in HCI.Two generally sequential tasks are involved in the analysissee Fig. 8. The first task involves detecting or extractingrelevant image features from the raw image or image sequence. The second task uses these image features for computing the model parameters. We discuss the different approaches used in this analysis.Fig. 8. Analysis and recognition of gestures. In the analysis stage,features F are extracted from visual images V. Model parameters Pare estimated and possibly predicted. Gestures G  are recognized inthe recognition stage. Recognition may also influence the analysisstage by predicting the gesture model at the next time instance.3.1 Feature DetectionFeature detection stage is concerned with the detection offeatures which are used for the estimation of parameters ofthe chosen gestural model. In the detection process it is firstnecessary to localize the gesturer. Once the gesturer is localized, the desired set of features can be detected.3.1.1 LocalizationGesturer localization is a process in which the person whois performing the gestures is extracted from the rest of thevisual image. Two types of cues are often used in the localization process color cues and motion cues.Color cues are applicable because of the characteristic colorfootprint of the human skin. The color footprint is usuallymore distinctive and less sensitive to illumination changesin the huesaturation space than in the standard cameracapture RGB color space. Most of the color segmentationtechniques rely on histogram matching 4 or employ asimple lookup table approach 51, 73 based on thetraining data for the skin and possibly its surrounding areas. The major drawback of colorbased localization techniques is the variability of the skin color footprint in differentlighting conditions. This frequently results in undetected skin684 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,  VOL.  19,  NO.  7,  JULY  1997regions or falsely detected nonskin textures. The problemcan be somewhat alleviated by considering only the regionsof a certain size scale filtering or at certain spatial positionpositional filtering. Another common solution to theproblem is the use of restrictive backgrounds and clothinguniform black background and long dark sleeves, for example. Finally, many of the gesture recognition applications resort to the use of uniquely colored gloves or markers on handsfingers 19, 28, 57, 60, 62. The use ofbackground restriction or colored gloves makes it possibleto localize the hand efficiently and even in realtime, but imposes the obvious restriction on the user and the interfacesetup. On the other hand, without these restrictions some ofthe colorbased localization techniques such as the ones thatuse histogram matching are computationally intensive andcurrently hard to implement in realtime.Motion cue is also commonly applied for gesturer localization and is used in conjunction with certain assumptionsabout the gesturer. For example, in the HCI context, it is usually the case that only one person gestures at any given time.Moreover, the gesturer is usually stationary with respect tothe also stationary background. Hence, the main component of motion in the visual image is usually the motion ofthe armhand of the gesturer and can thus be used to localizeherhim. This localization approach is used in 35, 72. Thedisadvantage of the motion cue approach is in its assumptions. While the assumptions hold over a wide spectrum ofcases, there are occasions when more than one gesturer isactive at a time active role transition periods or the background is not stationary.To overcome the limitations of the individual cues forlocalization, several approaches have been suggested. Oneapproach is the fusion of color, motion and other visual cues7 or the fusion of visual cues with nonvisual cues likespeech or gaze 83. The potential advantage of the socalled multimodal approach has not yet been fully exploitedfor hand localization though it has been explored for facelocalization in video 38. We discuss the multimodal approach further in Section 6. Another way in which the localization problem can be substantially eased is by the useof prediction techniques. These techniques provide estimatesof the future feature locations based on the model dynamicsand the previously known locations. We will discuss thisfurther in Section 3.2.3.1.2 Features and DetectionEven though different gesture models are based on different types of parameters, the image features employed tocompute those parameters are often very similar. For example, some 3D handarm models and models that usefinger trajectories all require fingertips to be extracted first.Color or gray scale images which encompass hands andarms or gesturers themselves are often used as the features.This choice of features is very common in the appearancebased models of gestures where sequences of images areused to form temporal templates of gestures 25. The computational burden of the detection of these features is relatively low and is associated mostly with the gesturer localization phase. Another approach to using whole images asfeatures is related to building of the socalled motion energyhistory images or MEI MHI. MEIs are 2D images whichunify the motion information of a sequence of 2D images byaccumulating the motion of some characteristic imagepoints over the sequence 30. One simple yet effectivechoice of characteristic points is the whole image itself 12.As discussed in Section 3.2, such features can represent avalid choice for the recognition of communicative gestures.However, their applicability to hand and arm tracking andrecognition of manipulative gestures seems to be limited.Hand and arm silhouettes are among the simplest, yetmost frequently used features. Silhouettes are easily extractedfrom local hand and arm images in the restrictive background setups. In the case of complex backgrounds, techniques that employ color histogram analyses, as described inthe gesturer localization phase, can be used. Examples of theuse of silhouettes as features are found in both 3D handmodelbased analyses 56 as well as in the appearancebasedtechniques as in 54, 69. Naturally, the use of such binaryfeatures results in a loss of information which can effect theperformance especially for 3D hand posture estimators. Forexample, in the 3D hand posture estimation problem of 56,the binary silhouette prevents the accurate estimation of thepositions of some fingers.Contours represent another group of commonly usedfeatures. Several different edge detection schemes can beused to produce contours. Some are extracted from simplehandarm silhouettes, and thus, are equivalent to them,while the others come from color or graylevel images.Contours are often employed in 3D modelbased analyses.In such cases, contours can be used to select finger and armlink candidates through the clustering of the sets of paralleledges 29, 31, or through imagecontourtomodelcontour matching 37, for example. In appearancebasedmodels, on the other hand, many different parameters canbe associated with contours for instance signaturesdescription in polar coordinates of the points on the contour 14 and size functions 96.A frequently used feature in gesture analysis is the fingertip. Fingertip locations can be used to obtain parametersof both the 3D hand models and the 2D appearancebasedgestural models see Section 3.2. However, the detection offingertip locations in either 3D or 2D space is not trivial. Asimple and effective solution to the fingertip detectionproblem is to use marked gloves or color markers to designate the characteristic fingertips see 19, 28, 57, 60,93, for instance. Extraction of fingertip location is thenfairly simplified and can be performed using color histogrambased techniques. A different way to detect fingertipsis to use pattern matching techniques templates can be images of fingertips 22 or fingers 77 or generic 3D cylindrical models 27. Such pattern matching techniques can beenhanced by using additional image features, like contours76. Some fingertip extraction algorithms are based on thecharacteristic properties of fingertips in the image. For instance, curvature of a fingertip outline follows a characteristic pattern lowhighlow which can be used for the feature detection 63, 97. Other heuristics can be used aswell. For example, for deictic gestures it can be assumedthat the finger represents the foremost point of the hand 63,73. Finally, many other indirect approaches in detection ofPAVLOVIC ET AL.    VISUAL INTERPRETATION OF HAND GESTURES FOR HUMANCOMPUTER INTERACTION A REVIEW 685fingertips have be employed in some instances, like imageanalysis using specially tuned Gabor kernels 66. The mainhindrance in the use of fingertips as features is their susceptibility to occlusions. Very often one or more fingers areoccluded by the palm from a given camera viewpoint anddirection. The most obvious solution to this occlusionproblem involves the use of multiple cameras 60, 76.Other solutions are based on the estimation of the occludedfingertip positions based on the knowledge of the 3D modelof the gesture in question 77. More often, however, restrictions are placed on the user to posture herhis hand sothat the occlusions are minimized.3.2 Parameter EstimationComputation of the model parameters is the last stage ofthe gesture analysis phase. In the gesture recognition systems, this is followed by the recognition stage, as shown inFig. 8. For hand or arm tracking systems, however, the parameter computation stage usually produces the final output. The type of computation used depends on both themodel parameters and the features that were selected.3.2.1 Estimation of 3D Model ParametersAs mentioned in Section 2.4.1, two sets of parameters areused in 3D hand modelsangular joint angles and linear phalangae lengths and palm dimensions. The estimation of these kinematic parameters from the detectedfeatures is a complex and cumbersome task. The processinvolves two steps the initial parameter estimation and the parameter update as the hand gesture evolves intime.All of the 3D hand models employed so far assume that allthe linear parameters are known a priori. This assumptionreduces the problem of finding the hand joint angles to aninverse kinematics problem. Given a 3D position of the endeffectors and the base of a kinematic chain, the inversekinematics task is to find the joint angles between the linksin the chain. The 3D model of the hand can then be viewedas a set of five serial kinematic chains finger links attachedto a common base palm. The finger tips now play the roleof the endeffectors in the chains. Inverse kinematic problems are in general illposed, allow for multiple solutions,and are computationally expensive. The use of constraintson parameter values see Section 2.4.1 somewhat alleviatesthose problems. Nevertheless, alternative approaches to 3Dhand parameter estimation have been often sought. Oneautomated solution to the initial parameter estimationproblem was proposed by 59 through a two phase procedure using the accumulated displacement torque approach.The first phase involves the initial wrist positioning whilethe second phase deals with palmfinger adjustment. Theprocedure is applied recursively until the accumulatedtorque excerpted on all links reaches a local minimum, constrained on a set of static and dynamic joint angle constraints. Even though this approach produces accurate parameter estimates, it is computationally very expensive andthus not applicable to realtime problems. Some simplersolutions involve a user interactive model parameter initialization 56. Another approach is to use interpolation ofthe discretized forward kinematics mappings to approximatethe inverse kinematics 4. Given a table of the discrete valuesof the joint angles and the resulting fingertip positions it ispossible to estimate the values of the joint angles for a nontablevalue of the fingertip position.Once the hand model parameters are initially estimated,the parameter estimates can be updated using some kind ofpredictionsmoothing scheme. A commonly used schemeis Kalman filtering and prediction. This scheme works underthe assumption of small motion displacements and aknown parameter update motion model. Such a modelcan be derived from a known hand kinematics model, using the inverse Jacobian mapping from the space of measurable linear displacements into the space of desired angular displacements. A variation of this approach was used by76 in a realtime 27 degree of freedom hand tracker. Onthe other hand, when the dynamics are not explicitly available a simple scheme like the one reported in 56 may beemployed. In this scheme, a simple silhouette matchingbetween the 3D hand model and the real hand image wasused to obtain satisfactory parameter estimation and update.It is necessary to stress three major drawbacks associated with the mentioned 3D hand model parameter estimation approach. One has to do with the obvious computational complexity of any task involving the inverse kinematics. The other, potentially more serious problem, is due toocclusions of the fingertips used as the model features. Anobvious, yet expensive, solution is to use multiple cameras.Another possible solution was developed by 77, and involves the use of finger links as features built upon a set ofrules designed to resolve the finger occlusions. The lastdrawback stems from the employed assumption that thelinear dimensions of the hand are known, which is necessary in the inverse kinematics problems. Thus, any changein scale of the hand images always results in inaccurateestimates of the hand joint angles. Finally, it should bepointed out that the knowledge of the exact hand postureparameters seem unnecessary for the recognition of communicative gestures 71 although the exact role of 3D handparameter in gesture recognition is not clear.The motion of the arm and hand also plays a role ingesture recognition although again the exact nature of thisrole is controversial 71. The estimates of such motion canbe made using either 3D space or 2D space. The 3D armparameters are similar to the ones used in the 3D handmodel descriptionjoint angles and links. Hence, similartechniques could be used for the 3D arm parameter estimation. However, because of the simpler macro structure ofthe arm the arm can be viewed as a serial kinematic chainwith only three links and fewer occlusions, it is possible touse less complex approaches to the arm parameter estimation. Most of the approaches match simplified geometrical3D models of the arm see Section 2.4.1 to the visual images of a real arm. The commonly used features are edgesand contours which are used to estimate the link axes. Forexample, 29, 31 used sets of symmetry axes of line segments to estimate the axes of generalized cylinders whichmodeled the arm links and the upper body. In another example, 37 used chamfer matching to align the 3D taperedsuperquadrics model of the upper body to two camera686 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,  VOL.  19,  NO.  7,  JULY  1997visual images. In a slightly different approach 105 usedfusion of color blob features and contours to detect elements of the blob representation of the human body.As is the case in the 3D hand model parameter estimation, a good initialization of arm parameters is crucial formany of these techniques to work. This is because thesetechniques often rely on dynamic updates of the parameters through a Kalmanbased filteringprediction schemerather than a global initial search. Also, the introduction ofconstraints on the position and motion of the arm links, asin 37, can greatly improve the estimation process.3.2.1 Estimation of Appearance ParametersMany different appearancebased models have been reported. The estimation of the parameters of such modelsusually coincides with the estimation of some compact description of the image or image sequence.Appearance models based on the visual images per seare often used to describe gestural actions. These modelsare often known as the temporal models. Various differentparameters of such models are used. In the simplest casethe parameters can be selected as the sets of key visualframes, as in 25. Another possibility is to use the eigendecomposition representation of visual images in the sequence with respect to an average image 104. A promisingdirection has recently been explored accumulation of spatiotemporal information of a sequence of visual images intoa single 2D image, a socalled motion history image MHI12. Such a 2D image can then be easily parameterized using one of 2D image description techniques, such as the geometric moment description or eigendecomposition. A majoradvantage of using these appearance models is the inherentsimplicity of their parameter computation. However, thisadvantage may be outweighed by the loss of precise spatialinformation which makes them especially less suited for manipulative gestures.Deformable 2D templatebased models are often employed as the spatial models of hand and arm contours oreven the whole human body 45. They are usually specified through a pair of mean values of the template nodes mand their covariances v 21, 49. The parameter estimatesare obtained through principal component analysis PCA onsets of training data. Different parameters are then used todescribe individual gestures. The variation of the node parameters allows for the same gesture to be recognized despite the fact that it takes on slightly different appearancewhen performed by different gesturers. An extension ofthis approach to 3D deformable templates or point distribution models PDM was recently suggested in 42. Associated with the deformable template model parameters arealso the so called external deformations or global motionparameters rotation and translation of the hand or body inthe workspace. The updates of the model parameters canthen be estimated in a framework similar to the one usedfor rigid motion estimation. The main difference is that inthe case of deformable templates an additional displacement due to the template variability dv also needs to beestimated 42, 49. While the parameter computation forsuch deformable models is not extensive in the parameterupdate phase, it can be overwhelming during the initialization. On the other hand, deformable models can providesufficient information for the recognition of both classes ofgestures manipulative and communicative.Finally, a wide class of appearance models uses silhouettes or gray level images of the hands. In such cases, themodel parameters attempt to capture a description of theshape of the hand while being relatively simple. A verycommonly employed technique is built upon the geometricmoment description of hand shapes 14, 69, 86. Usually,moments of up to the second order are used. Some othertechniques use Zernike moments 79 whose magnitudesare invariant to rotation, thus allowing for rotation invariant shape classification. Many other shape descriptors havealso been testedorientation histograms 34, for example,represent summary information of small patch orientationsover the whole image. This parameter tends to be invariantunder changes in the lighting conditions which often occurduring the hand motion. Even though the parameters of theabove mentioned models are easy to estimate, they are alsovery sensitive to the presence of other, nonhand objects inthe same visual image. This means that tight boundingboxes around the hand need to be known at all times duringthe hand motion. This in turn implies either the use of goodmotion prediction or restriction to the hand postures. Likethe other parameter estimation tasks, the reported estimationof motion parameters are usually based on simple Newtonian dynamics models and Kalmanbased predictors.4 GESTURE RECOGNITIONGesture recognition is the phase in which the data analyzedfrom the visual images of gestures is recognized as a specific gesture. Analogously, using the notation we established in Section 2, the trajectory in the model parameterspace obtained in the analysis stage is classified as amember of some meaningful subset of that parameterspace. Two tasks are commonly associated with the recognition process Optimal partitioning of the parameter space and Implementation of the recognition procedure.The task of optimal partitioning is usually addressedthrough different learningfromexamples training procedures. The key concern in the implementation of the recognition procedure is computational efficiency. We discusseach of the above issues in more detail.The task of optimal partitioning of the model parameterspace is related to the choice of the gestural models andtheir parameters, as mentioned in Section 2. However, mostof the gestural models are not implicitly designed with therecognition process in mind. This is especially true for themodels of static gestures or hand postures. For example,most of the static models are meant to accurately describethe visual appearance of the gesturers hand as they appearto a human observer. To perform recognition of those gestures, some type of parameter clustering technique stemming from vector quantization VQ is usually used. Briefly,in vector quantization, an ndimensional space is partitioned into convex sets using ndimensional hyperplanes,based on training examples and some metric for determiningPAVLOVIC ET AL.    VISUAL INTERPRETATION OF HAND GESTURES FOR HUMANCOMPUTER INTERACTION A REVIEW 687the nearest neighbor. If the parameters of the model arechosen especially to help with the recognition, as for example in 23, 90, the separation of classes belonging to different gestures can be done easily. However, if the modelparameters are not chosen to properly describe the desiredclasses, the separation of the classes, and thus, accurate recognition in that parameter space may not be possible. Forexample, with contour descriptors, several hand postureswould be confused during classification and recognition.Therefore, contours are often used for hand or arm trackingrather than for the recognition of hand postures. Parameters of other appearancebased static hand models oftensuffer from the same problem. For example, it is knownthat geometric moment parameters are not rotationally invariant. Thus, a small change in rotation of the same handposture can cause it to be classified as a different posture.This problem can be somewhat alleviated if the chosentraining hand postures classes are either very distinct orsomehow normalized with respect to rotation. Another approach is to introduce different model parameters, such asZernike moments 79 or orientation histograms 34, whichposses 2D rotational invariance property. Some other models, based on eigenspace decompositions, are more discriminant and hence produce higher recognition accuraciesunder classical clustering techniques 103. The problem ofaccurate recognition of postures which use model parameters that cluster in nonconvex sets can also be solved byselecting nonlinear clustering schemes. Neural networksare one such option, although their use for gesture recognition has not been fully explored 50. Such nonlinearschemes are often sensitive to training and may be computationally expensive. Further, there is an inherent limitationin the discrimination capability by considering a 2D projection or appearance of a 3D hand when trying to capture awide class of natural gestures. On the other hand, the use of3D hand and gesture models offers the possibility of improving recognition, but because of the complexity ofmodel parameter computation they are not often used forhand posture recognition.Gestural actions, as opposed to static gestures, involveboth the temporal and the spatial context 16. As in thecase of static posture recognition, the recognition of gestural actions depends on the choice of gestural models.Most of the gestural models, as seen in Section 2, producetrajectories in the models parameter space. Since gesturalaction possess temporal context, the main requirement forany clustering technique used in their classification is that itbe time instance invariant and time scale invariant. For example, a clapping gesture should be recognized as suchwhether it is performed slowly or quickly, now, or in 10minutes. Numerous signal recognition techniques deal withsuch problems, the most prominent of these being automaticspeech recognition ASR. Since both speech as well as gestures are a means of natural human communication, an analogy is drawn between them and computational tools developed for ASR are frequently used in gesture recognition.In speech recognition problems, a long standing task hasbeen to recognize spoken words independent of their duration and variation in pronunciation. A tool called the Hidden Markov Models or HMM 74 has shown tremendoussuccess is such tasks. HMM is a doubly stochastic process, aprobabilistic network with hidden and observable states. Thehidden states drive the model dynamicsat each timeinstance the model is in one of its hidden states. Transitionsbetween the hidden states are governed by probabilisticrules. The observable states produce outcomes during hidden state transitions or while the model is in one of its hidden states. Such outcomes are measurable by an outsideobserver. The outcomes are governed by a set of probabilistic rules. Thus, an HMM can be represented as a tripletA, b, p, where A is called the hidden state transition matrix, b describes the probabilities of the observation states,and p is the initial hidden state distribution. It is common toassume that the hidden state space is discrete, and that theobservables are allowed to assume a continuum of values.In such cases, b is usually represented as a mixture of Gaussian MOG probability density functions. In automaticspeech recognition, one HMM is associated with each different unit of speech phoneme or sometimes word. Analogously, in the recognition of gestural actions, one HMM canbe associated with each different gesture. In speech, the observables take on values of the linear prediction cepstrumcoefficients LPC cepstrum. In gestures, the observable is avector of the spatial model parameters, like geometric moments 69, Zernike moments 79, or eigen image coefficients103. The process of association of different HMMs with different gestures speech units is denoted as training. In thisprocess the parameters of the HMM A, b, p are modified sothat the chosen model best describes the spatiotemporaldynamics of the desired gestural action. The training is usually achieved by optimizing the maximum likelihood measurelogProbservationmodel over a set of training examplesfor the particular gesture associated with the model. Suchoptimization involves the use of computationally expensiveexpectationmaximization or EM procedures, like the BaumWelch algorithm 74. However, any such training procedure involves a step based on dynamic programming or DPwhich in turn has a dynamic time warping or DTW property.This means that the variability in duration of training samples is accounted for in the model. The same is true for therecognition or model evaluation process. In that process, agesture trajectory is tested over the set of trained HMMs inorder to decide which one it belongs to. A probability of thegesture being produced by each HMM is evaluated usingthe Viterbi algorithm 74. Obviously, the larger the numberof trained HMMs gestures is, the more computationallydemanding the recognition procedure. Problems like thisone have successfully been solved by imposing an externalset of rules or grammar which describes the language sentence structure or how the trained units gestures or spoken can be connected in time 69, 86. Several problemsare related to the use of the HMM as a recognition tool. Forexample, in its original formulation, an HMM is a first order stochastic process. This implies that the hidden stateof the model at time instance i depends only on the state attime i  1. While this model may be sufficient for someprocesses, it often results in lower recognition rates for theprocesses which do not follow the first order Markov property. As in speech, such problems can be somewhat reduced by extending the parameter vectors with the time688 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,  VOL.  19,  NO.  7,  JULY  1997derivatives of the original parameters 15. It is also possibleto derive a higher order HMMs, however such models donot share the computational efficiency of the first ordermodels 75. Another possible drawback of classical HMMsis the assumption that probability distribution functions orpdfs of the observables can be modeled as mixtures ofGaussians. The main reason for modeling the observablesas MOGs is in training. In such cases, the HMM parameterscan be efficiently computed using the BaumWelch algorithm. Extensions in this direction have been achieved inASR by using neural networks to model the observationpdfs 13. Unfortunately, the training procedure in that caseis computationally overwhelming. Also, in the originalformulation a HMM is assumed to be stationary. This meansthat the observation probabilities do not vary in time. Suchassumption may hold over short time intervals. However,since a complete gestural action is often modeled as a singleHMM, the stationarity of observation pdfs may not holdtrue in this case. Nonstationary HMMs have been formulated for ASR 89, but have not yet been used for gesturerecognition. Finally, it is interesting to note that hiddenstates of the HMM may possibly be viewed as the temporalphases known from the psychological studies of gestureSection 2.3.Another approach to recognition of gestural actions proposed recently is based on temporal templates, so called motionenergy 30 or motion history images MHIs 12 see Section 3.2. Such motion templates accumulate the motion history of a sequence of visual images into a single 2D image.Each MHI is parameterized by the length of the time historywindow that was used for its computation. To achieve timeduration invariance, the templates are calculated for a set ofhistory windows of different durations, ranging between twopredefined values. The recognition is then simply achievedusing any of the 2D image clustering techniques, based onthe sets of trained templates. An advantage of a such temporal template approach is in its extreme computational simplicity. However, the fact that the motion is accumulatedover the entire visual image can result in artifacts being introduced by motions of unrelated objects or body parts presentin the images.A successful recognition scheme should also considerthe timespace context of any specific gesture. This can beestablished by introducing a grammatical element into recognition procedure. The grammar should reflect the linguistic character of communicative gestures as well as spatial character of manipulative gestures. In other words, onlycertain subclasses of gestural actions with respect to thecurrent and previous states of the HCI environment arenaturally plausible. For example, if a user reachesperforms a valid manipulative gesture for the coffee cuphandle and the handle is not visible from the users point ofview, the HCI system should discard such a gesture. Still,only a small number of the systems so far exploits this. Thegrammars are simple and usually introduce artificial linguisticstructures they build their own languages that have to belearned by the user 36, 50, 73, 80.The computational complexity of a recognition approachis important in the context of HCI. The tradeoffs involvedacross various approaches is a classical onemodel complexity, versus richness of the gesture classes, versus recognition time. The more complex the model is, the wider classof gestures to which it can be applied. The computationalcomplexity increases, and, hence, the recognition time.Most of the 3D modelbased gesture models are characterized by more than 10 parameters. Their parameter calculation gesture analysis requires computationally expensivesuccessive approximation procedures the price of which issomewhat lowered using predictiontype analysis. Thesystems based on such models rarely show close to realtime performance. For example, the time performanceranges from 45 minutes per single frame in 59 although itdoes not use any prediction element to 10 frames per second in 76. Yet potentially, the 3D models can be used tocapture the richest sets of hand gestures for HCI. The appearancebased models are usually restricted in their applicability to a narrower subclass of HCI applications, enhancements of the computer mouse concept 22, 35, 36,50, 73, or hand posture classification 43, 63, 66, 67,81, 86. On the other hand, because of the lower complexity of the appearancebased models they are easier toimplement in realtime and more widely used.5 APPLICATIONS AND SYSTEMSRecent interest in gestural interface for HCI has been drivenby a vast number of potential applications Fig. 9. Handgestures as a mode of HCI can simply enhance the interaction in classical desktop computer applications by replacing the computer mouse or similar handheld devices.They can also replace joysticks and buttons in the control ofcomputerized machinery or be used to help the physicallyimpaired to communicate more easily with others. Nevertheless, the major impulse to the development of gesturalinterfaces has come from the growth of applications situated in virtual environments VEs 2, 53.Hand gestures in natural environments are used for bothmanipulative actions and communication see Section 2.However, the communicative role of gestures is subtle,since hand gestures tend to be a supportive element ofspeech with the exception of deictic gestures, which play amajor role in human communication. Manipulative aspectof gestures also prevails in their current use for HCI. However, some applications have emerged recently which takeadvantage of the communicative role of gestures. We present a brief overview of several application driven systemswith interfaces based on hand gestures.Most applications of hand gestures portray them as themanipulators of virtual objects VOs. This is depicted inFig. 9. VOs can be computer generated graphics, like simulated 2D and 3D objects 14, 22, 44, 36 or windows50, 73, or abstractions of computercontrolled physicalobjects, such as device control panels 4, 35, 36 or robotic arms 18, 43, 47, 94. To perform manipulations ofsuch objects through HCI a combination of coarse trackingand communicative gestures is currently being used. Forexample, to direct the computer to rotate an object a user ofsuch an interface may issue a twostep command select object rotate object. The first action uses coarse hand tracking to move a pointer in the VE to the vicinity of the object.PAVLOVIC ET AL.    VISUAL INTERPRETATION OF HAND GESTURES FOR HUMANCOMPUTER INTERACTION A REVIEW 689To rotate the object, the user rotates hisher hand back andforth producing a metaphor for rotational manipulation 14.One may then pose the question Why use the communicative gestures for manipulative actions Communicativegestures imply a finite and usually small vocabulary ofgestures that has to be learned, whereas the manipulativeones are natural handarm movements. To answer thisquestion, one has to consider the complexity of analysis andrecognition of each type of gestural models Section 3 andSection 4. The 3D hand modelbased gestural models arewell suited for modeling of both manipulative and communicative gestures, while the appearancebased models ofgestures are mostly applicable to the communicative ones.However, the use of 3D hand modelbased gesture modelsare computationally more expensive than that of the appearancebased models see Section 4. Therefore, toachieve a usable realtime performance one has to usuallyresort to the less desirable appearancebased models ofgestures. Recently, however, with the increase in computing power, simplified handhead blob models 6, 105have been considered for applications which use communicative gesture recognition 10. Such models are simpleenough to be analyzed in realtime and are used for recognition of a small set of communicative gestures. For example, 10 used such a model followed by a HMM classifier torecognize eighteen Tai Chi gestures. The system was intended to provide a virtual environment for the relaxationof cancer patients.A brief summary of characteristics of some of the systems aimed at the application of hand gestures for HCI isgiven in Table 1. It summarizes the basic modeling technique used for the gestures, the class of gesture commandsthat are interpreted, and the reported performance in termsof the speed of processing.Not all of the applications of hand gestures for HCI aremeant to yield manipulative actions. Gestures for HCI canalso be used to convey messages for the purpose of theiranalysis, storage or transmission. VideoteleconferencingVTC and processing of American sign language ASL provide such opportunities. In VTC applications, reduction ofbandwidth is one of the major issues. A typical solution is touse different coding techniques. One such technique ismodelbased coding where image sequences are described bythe states e.g., position, scale, and orientation of all physicalobjects in the scene human participants in the case of VTC1, 40. Only the updates of descriptors are sent while at thereceiving end a computer generated model of physical objects is driven using the received data. Modelbased codingfor VTC, therefore, requires that the human bodies be modeled appropriately. Depending on the amount of detail desired, this can be achieved by only coarse models of the upper body and limbs 20, or finely tuned models of humanfaces or hands. Modeling of handarm gestures can then beof substantial value for such applications.Recognition of ASL is often considered as another application that naturally employs human gestures as means ofcommunication. Such applications could play a vital role incommunication with people with a communication impairment like deafness. A device which could automaticallytranslate ASL hand gestures into speech signals would undoubtedly have a positive impact on such individuals. However, the more practical reason for using the ASL as a test bedfor the present hand gesture recognition systems is its welldefined structure compared to other natural gestures humansuse. This fact implies that the appearancebased modelingtechniques are particularly suited for such ASL interpretation, as was proven in several recent applications 86, 96.There are numerous prospective applications of visionbased hand gesture analysis. The applications mentioned sofar are only the first steps toward using hand gestures inHCI. The need for further development is thus quite clear.We discuss several important research issues that need tobe addressed toward incorporating natural hand gesturesinto the HCI.6 FUTURE DIRECTIONSTo fully exploit the potential of gestures in HCI environments, the class of recognizable gestures should be as broadas possible. Ideally, any and every gesture performed bythe user should be unambiguously interpretable, thus allowing for naturalness of the interface. However, the state ofthe art in visionbased gesture recognition does not providea satisfactory solution for achieving this goal. Most of thegesturebased HCI systems at the present time address avery narrow group of applications mostly symbolic commands based on hand postures or 3Dmouse type ofpointing see Section 5. The reason for this is the complexity associated with the analysis Section 3 and recognitionSection 4 of gestures. Simple gesture models make it possible to build realtime gestural interfacesfor example,pointing direction can be quickly found from the silhouFig. 9. Applications of gestural interface for HCI. Unlike the gestures in a natural environment, both manipulative and communicative gestures inHCI can be employed to direct manipulations of objects or to convey messages.690 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,  VOL.  19,  NO.  7,  JULY  1997ettes of the human hand in relatively nonrestrictive environments 36,73. However, as it can be seen from Fig. 10to find the hand posture and thus distinguish between thetwo gestures from the simple image appearance silhouettes is sometimes quite difficult.Realtime interaction based on 3D hand modelbasedgesture analysis is yet to be demonstrated. The use of 3Dmodels are mostly confined to hand tracking and handposture analysis. Yet, the analysis of the parameters of the3D hand modelbased models can result in a wider class ofhand gestures that can be identified than the analysislinked with the appearancebased models. This leads us tothe conclusion that, from the point of the naturalness ofHCI, the 3D hand modelbased approaches offer morepromise than the appearancebased models. However, thisprospect is presently hindered by a lack of speed and therestrictiveness of the background in the 3D hand modelbased approaches. The first problem is associated with thecomplexity of the model and the feature extraction. Fingertip positions seem to be a very useful feature see Section 3.1.2, yet sometimes difficult to extract. A possiblesolution to this problem may employ the use of skin andnail texture to distinguish the tips of the fingers. Additionally, the computational complexity of estimating theTABLE  1SYSTEMS THAT EMPLOY HAND GESTURES FOR HCIApplication Gestural Modeling Technique Gestural Commands ComplexitySpeedCD Player Control Panel 4 Hand silhouette moments Tracking only 30 fps1Staying Alive 10 3D hand  head blob model 6 Tracking  HMMbased recognition real timeVirtual Squash 14 Hand silhouette moments  contoursignatureTracking  three metaphors 10.6 fpsFingerPaint 22 Fingertip template Tracking only n.a.2ALIVE 26 Template correlation Tracking combined with recognitionof facial expressionsrealtimeComputer Game Control 33 Image moments using dedicatedhardwareHand  body posture recognition realtimeTV Display Control 35 Template correlation Tracking only 5 fpsFingerPointer 36 Heuristic detection of pointing action Tracking and one metaphor combined with speechrealtimeWindow Manager 50 Hand pose recognition using neuralnetworksTracking  four metaphors realtimeGestureComputer 63 Image moments  fingertip position Tracking and six metaphors 1025 fpsFingerMouse 73 Heuristic detection of pointing action Tracking only realtimeDigitEyes 76 27 DoF 3D hand model Tracking only 10 fpsRobot manipulator guidance 18 active contour pointing realtimeROBOGEST 43 Silhouette Zernike moments Six metaphors 12 fpsAutomatic robot instruction Fingertip position in 2D Grasp tracking n.a.Robot manipulator control 94 Fingertip positions in 3D Six metaphors realtimeHand sign recognition 24 Most expressive featur cameresMEF of images40 signs n.a.ASL recognition 86 Silhouette moments  grammar 40 words 5 fps1. Frames per second.2. Not available.We choose speed as the measure of complexity of interpretation given the lack of any other accurate measure. Note, however, that different applications may beimplemented on different computer systems with different levels of optimization.              a   bFig. 10. Silhouettes and grayscale images of two different hand postures. The silhouette in a can also be interpreted as the reflection about thevertical axes of the silhouette in b. Hence, the two silhouettes do not unambiguously define the hand posture.PAVLOVIC ET AL.    VISUAL INTERPRETATION OF HAND GESTURES FOR HUMANCOMPUTER INTERACTION A REVIEW 691model parameters Section 3.2 can be reduced by choosing an optimal number of parameters that satisfies a particular level of naturalness and employing parallelizationof the computations involved.Several other aspects that pertain to the construction of anatural HCI need to be adequately addressed in the future.One of the aspects involves the twohanded gestures. Human gestures, especially communicative, naturally employactions of both hands. Yet, many of the visionbased gesture systems focus their attention on singlehand gestures.Until recently, the singlehand gesture approach has beenalmost inevitable. First, many analysis techniques requirethat the hands be extracted from global images. If the twohanded gestures are allowed, several ambiguous situationsthat do not occur in singlehand case may occur that haveto be dealt with occlusion of hands, distinction between orindexing of leftright hand. Second, the most versatilegesture analysis techniques namely, 3D modelbased techniques currently exhibit one major drawback speed. Some3D modelbased techniques which use coarse upper bodyand arm models 6 have reached the near realtime speedsand have been utilized for basic twohand gesture analysis.Appearancebased techniques can, in principle, handle twohanded gestures. Nevertheless, their applicability has beenusually restricted to simple symbolic gestures that do notrequire two hands. A more recent work on appearancebased motion templates 12 has indirectly addressed theissue of twohanded gestures. Another notable exception isan early system developed by Krueger 54. Thus, to adequately address the issue of twohanded gestures in thefuture, more effective analysis techniques should be considered. These techniques should not only rely on the improvements of the classical techniques used in singlehand gestures, but also exploit the interdependence between the two hands performing a gesture since in manycase the two hands performing a single gesture assumesymmetrical postures.An issue related to twohanded gestures is the one ofmultiple gesturers. Successful interaction in HCIbased environments has to consider multiple users. For example, avirtual modeling task can benefit enormously if several designers simultaneously participate in the process. However,the implementation of the multiuser interface has severaldifficult issues to face, the foremost one being the analysisof gestures. The analysis at the present assumes that there isa welldefined workspace associated with the gesturer.However, in the case of multiple users the intersection ofworkspaces is a very probable event. The differentiationbetween the users can then pose a serious problem. The useof active computer vision 11, 82, 5, 8, in which thecameras adaptively focus on some area of interest, mayoffer a solution to this problem. Another approach wouldbe to optimize the parameters of the stationary camerasfor a given interface related issues are studied under sensorplanning 39, 91.Hand gestures are, like speech, body movement, andgaze, a means of communication see Section 2.1. Moreover, almost any natural communication among humansconcurrently involves several modes of communicationthat accompany each other. For instance, the come heregesture is usually accompanied by the words Comehere. Another example is the sentence Notice this control panel. and a deictic gesture involving an index fingerpointing at the particular control panel and a gaze directed at the panel. As seen from the above examples, thecommunicative gestures can be used both to affirm and tocomplement the meaning of a speech message. In fact, inthe literature that reports psychological studies of humancommunication, the interaction between the speech andgestures as well as the other means of communication isoften explored 48, 61, 87. This leads to the conclusionthat any such multimodal interaction can also be rendereduseful for HCI see Fig. 11 and 84. The affirmative handgesture speech can be used to reduce the uncertainty inspeech hand gesture recognition and, thus, provide amore robust interface. Gestures that complement speech,on the other hand, carry a complete communicationalmessage only if they are interpreted together with speechand, possibly, gaze. The use of such multimodal messagescan help reduce the complexity and increase the naturalness of the interface for HCI see Fig. 12. For example,instead of designing a complicated gestural command forthe object selection which may consist of a deictic gesturefollowed by a symbolic gesture to symbolize that the object that was pointed at by the hand is supposed to be selected a simple concurrent deictic gesture and verbalcommand this can be used 84. The number of studiesthat explore the use of multimodality in HCI has beensteadily increasing over the past couple of years 36, 83,98, 99, 102. At the present time, the integration ofcommunication modes in such systems is performed afterthe commands portions of different modes have been independently recognized. Although the interface structureis simplified in this way, the information pertaining to theinteraction of the modes at lower levels is probably lost.To utilize the multimodal interaction at all levels, newapproaches that fuse the multimodal input analysis aswell as recognition should be considered in the future.Fig. 11. A possible situation where speechgesture integration may beparticularly effective A 3D visualization facility for structural biologistwhere researchers could be examining and discussing the results of asimulation.Photograph courtesy of Rich Saal, Illinois State JournalRegister, Springfield, Ill.692 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,  VOL.  19,  NO.  7,  JULY  19977 CONCLUSIONSHumancomputer interaction is still in its infancy. Visualinterpretation of hand gestures would allow the development of potentially natural interfaces to computer controlled environments. In response to this potential, thenumber of different approaches to videobased hand gesture recognition has grown tremendously in recent years.Thus there is a growing need for systematization andanalysis of many aspects of gestural interaction. This papersurveys the different approaches to modeling, analysis, andrecognition of hand gestures for visual interpretation. Thediscussion recognizes two classes of models employed inthe visual interpretation of hand gestures. The first relies on3D models of the human hand, while the second utilizes theappearance of the human hand in the image. The 3D handmodels offer a rich description and discrimination capability that would allow a wide class of gestures to be recognized leading to natural HCI. However, the computation of3D model parameters from visual images under realtimeconstraints remains an elusive goal. Appearancebasedmodels are simpler to implement and use for realtimegesture recognition, but suffer from inherent limitationswhich could be a drawback for natural HCI.Several simple HCI systems have been proposed thatdemonstrate the potential of visionbased gestural interfaces. However, from a practical standpoint, the development of such systems is in its infancy. Though most currentsystems employ hand gestures for the manipulation of objects, the complexity of the interpretation of gestures dictates the achievable solution. For example, the gesturesused to convey manipulative actions today are usually ofthe communicative type. Further, hand gestures for HCI aremostly restricted to singlehanded and produced only by asingle user in the system. This consequently downgradesthe effectiveness of the interaction. We suggest several directions of research for raising these limitations towardgestural HCI. For example, integration of hand gestureswith speech, gaze and other naturally related modes ofcommunication in a multimodal interface. However, substantial research effort that connects advances in computervision with the basic study of humancomputer interactionwill be needed in the future to develop an effective andnatural hand gesture interface.ACKNOWLEDGMENTSThis work was supported in part by the U. S. Army Research Laboratory under Cooperative AgreementNo. DAAL019620003 and in part by the National ScienceFoundation Grant IRI9634618. The authors would like toacknowledge Yusuf Azoz and Lalitha Devi for their helpwith the references, and Karin Pavlovic for her help in reviewing the manuscript. They would also like to thank theanonymous reviewers for their comments which greatlyhelped in improving this survey.REFERENCES1 J.F. Abramatic, P. Letellier, and M. Nadler, A NarrowBandVideo Communication System for the Transmission of Sign Language Over Ordinary Telephone Lines, Image Sequences Processing and Dynamic Scene Analysis, T.S. Huang, ed., pp. 314336. Berlin and Heidelberg SpringerVerlag, 1983.2 J.A. Adam, Virtual Reality, IEEE Spectrum, vol. 30, no. 10,pp. 2229, 1993.3 S. Ahmad and V. Tresp, Classification With Missing and Uncertain Inputs, Proc. Intl Conf. Neural Networks, vol. 3, pp. 1,9491,954, 1993.4 S. Ahmad, A Usable RealTime 3D Hand Tracker, IEEE Asilomar Conf., 1994.5 J. Aloimonos, I. Weiss, and A. Bandyopadhyay, Active Vision,Intl J. Computer Vision, vol. 1, pp. 333356, 1988.6 A. Azarbayejani, C. Wren, and A. Pentland, RealTime 3DTracking of the Human Body, Proc. IMAGECOM 96, Bordeaux,France, 1996.7 Y. Azoz, L. Devi, and R. Sharma, VisionBased Human ArmTracking for Gesture Analysis Using Multimodal Constraint Fusion, Proc. 1997 Advanced Display Federated Laboratory Symp.,Adelphi, Md., Jan. 1997.8 R. Bajcsy, Active Perception, Proc. IEEE, vol. 78, pp. 9961,005,1988.9 T. Baudel and M. BaudouinLafon, Charade Remote Control ofObjects Using FreeHand Gestures, Comm. ACM, vol. 36, no. 7,pp. 2835, 1993.10 D.A. Becker and A. Pentland, Using a Virtual Environment toTeach Cancer Patients Tai Chi, Relaxation, and SelfImagery,Proc. Intl Conf. Automatic Face and Gesture Recognition, Killington,Vt. , Oct. 1996.11 A. Blake and A. Yuille, Active Vision. Cambridge, Mass. MITPress, 1992.12 A.F. Bobick and J.W. Davis, RealTime Recognition of ActivityUsing Temporal Templates, Proc. Intl Conf. Automatic Face andGesture Recognition, Killington, Vt., Oct. 1996.13 H.A. Boulard and N. Morgan, Connectionnist Speech Recognition. AHybrid Approach. Norwell, Mass. Kluwer Academic Publishers,1994.14 U. BrcklFox, RealTime 3D Interaction With Up to 16 Degreesof Freedom From Monocular Image Flows, Proc. Intl Workshopon Automatic Face and Gesture Recognition, Zurich, Switzerland,pp. 172178, June 1995.15 L.W. Campbell, D.A. Becker, A. Azarbayejani, A.F. Bobick, andA. Pentland, Invariant Features for 3D Gesture Recognition,Proc. Intl Conf. Automatic Face and Gesture Recognition, Killington,Vt., pp. 157162, Oct. 1996.16 C. Cedras and M. Shah, MotionBased Recognition A Survey,Image and Vision Computing, vol. 11, pp. 129155, 1995.17 K. Cho and S.M. Dunn, Learning Shape Classes, IEEE Trans.Pattern Analysis and Machine Intelligence, vol. 16, pp. 882888, Sept.1994.18 R. Cipolla and N.J. Hollinghurst, HumanRobot Interface byPointing With Uncalibrated Stereo Vision, Image and Vision Computing, vol. 14, pp. 171178, Mar. 1996.19 R. Cipolla, Y. Okamoto, and Y. Kuno, Robust Structure FromMotion Using Motion Parallax, Proc. IEEE Intl Conf. ComputerVision, pp. 374382, 1993.20 E. Clergue, M. Goldberg, N. Madrane, and B. Merialdo, Automatic Face and Gestural Recognition for Video Indexing, Proc.Intl Workshop on Automatic Face and Gesture Recognition, Zurich,Switzerland, pp. 110115, June 1995.21 T. F. Cootes, C.J. Taylor, D.H. Cooper, and J. Graham, ActiveShape ModelsTheir Training and Application, Computer Visionand Image Understanding, vol. 61, pp. 3859, Jan. 1995.22 J.L. Crowley, F. Berard, and J. Coutaz, Finger Tacking As anInput Device for Augmented Reality, Proc. Intl Workshop onAutomatic Face and Gesture Recognition, Zurich, Switzerland,pp. 195200, June 1995.23 Y. Cui and J. Weng, LearningBased Hand Sign Recognition,Proc. Intl Workshop on Automatic Face and Gesture Recognition, Zurich, Switzerland, pp. 201206, June 1995.24 Y. Cui and J. J. Weng, Hand Segmentation Using LearningBasedPrediction and Verification for Hand Sign Recognition, Proc. IntlConf. Automatic Face and Gesture Recognition, Killington, Vt.,pp. 8893, Oct. 1996.PAVLOVIC ET AL.    VISUAL INTERPRETATION OF HAND GESTURES FOR HUMANCOMPUTER INTERACTION A REVIEW 69325 T. Darrell, I. Essa, and A. Pentland, TaskSpecific Gesture Analysis in RealTime Using Interpolated Views, IEEE Trans. PatternAnalysis and Machine Intelligence, vol. 18, no. 12, pp. 1,2361,242,Dec. 1996.26 T. Darrell and A.P. Pentland, AttentionDriven Expression andGesture Analysis in an Interactive Environment, Proc. Intl Workshop on Automatic Face and Gesture Recognition, Zurich, Switzerland, pp. 135140, June 1995.27 J. Davis and M. Shah, Determining 3D Hand Motion, Proc. 28thAsilomar Conf. Signals, Systems, and Computer, 1994.28 J. Davis and M. Shah, Recognizing Hand Gestures, Proc. European Conf. Computer Vision, Stockholm, Sweden, pp. 331340, 1994.29 A. C. Downton and H. Drouet, Image Analysis for ModelBasedSign Language Coding, Progress in Image Analysis and Processing II Proc. Sixth Intl Conf. Image Analysis and Processing, pp. 637644, 1991.30 I. Essa and S. Pentland, Facial Expression Recognition Using aDynamic Model and Motion Energy, Proc. IEEE Intl Conf. Computer Vision, 1995.31 M. Etoh, A. Tomono, and F. Kishino, StereoBased Descriptionby Generalized Cylinder Complexes From Occluding Contours,Systems and Computers in Japan, vol. 22, no. 12, pp. 7989, 1991.32 S.S. Fels and G.E. Hinton, GloveTalk A Neural Network Interface Between a DataGlove and a Speech Synthesizer, IEEETrans. Neural Networks, vol. 4, pp. 28, Jan. 1993.33 W.T. Freeman, K. Tanaka, J. Ohta, and K. Kyuma, ComputerVision for Computer Games, Proc. Intl Conf. Automatic Face andGesture Recognition, Killington, Vt., pp. 100105, Oct. 1996.34 W.T. Freeman and M. Roth, Orientation Histograms for HandGesture Recognition, Proc. Intl Workshop on Automatic Face andGesture Recognition, Zurich, Switzerland, June 1995.35 W.T. Freeman and C.D. Weissman, Television Control by HandGestures, Proc. Intl Workshop on Automatic Face and Gesture Recognition, Zurich, Switzerland, pp. 179183, June 1995.36 M. Fukumoto, Y. Suenaga, and K. Mase, FingerPointer Pointing Interface by Image Processing, Computers and Graphics,vol. 18, no. 5, pp. 633642, 1994.37 D.M. Gavrila and L.S. Davis, Towards 3D ModelBased Trackingand Recognition of Human Movement A MultiView Approach,Proc. Intl Workshop on Automatic Face and Gesture Recognition, Zurich, Switzerland, pp. 272277, June 1995.38 H.P. Graf, E. Cosatto, D. Gibbon, M. Kocheisen, and E. Petajan,MultiModal System for Locating Heads and Faces, Proc. IntlConf. Automatic Face and Gesture Recognition, Killington, Vt.,pp. 8893, Oct. 1996.39 G. D. Hager, Task Directed Sensor Fusion and Planning. KluwerAcademic Publishers, 1990.40 H. Harashima and F. Kishino, Intelligent Image Coding andCommunications With Realistic SensationsRecent Trends,IEICE Trans., vol. E 74, pp. 1,5821,592, June 1991.41 A.G. Hauptmann and P. McAvinney, Gesture With Speech forGraphics Manipulation, Intl J. ManMachine Studies, vol. 38,pp. 231249, Feb. 1993.42 T. Heap and D. Hogg, Towards 3D Hand Tracking Using a Deformable Model, Proc. Intl Conf. Automatic Face and Gesture Recognition, Killington, Vt., pp. 140145, Oct. 1996.43 E. Hunter, J. Schlenzig, and R. Jain, Posture Estimation in ReducedModel Gesture Input Systems, Proc. Intl Workshop onAutomatic Face and Gesture Recognition, June 1995.44 K. Ishibuchi, H. Takemura, and F. Kishino, Real Time HandGesture Recognition Using 3D Prediction Model, Proc. 1993 IntlConf. Systems, Man, and Cybernetics, Le Touquet, France, pp. 324328, Oct. 1720, 1993.45 S.X. Ju, M.J. Black, and Y.Y. oob, Cardboard People A Parameterized Model of Articulated Image Motion, Proc. Intl Conf.Automatic Face and Gesture Recognition, Killington, Vt., pp. 3843,Oct. 1996.46 I.A. Kakadiaris, D. Metaxas, and R. Bajcsy, Active PartDecomposition, Shape and Motion Estimation of Articulated Objects A PhysicsBased Approach, Proc. IEEE C.S. Conf. ComputerVision and Pattern Recognition, pp. 980984, 1994.47 S.B. Kang and K. Ikeuchi, Toward Automatic Robot Instructionfor PerceptionRecognizing a Grasp From Observation, IEEETrans. Robotics and Automation, vol. 9, pp. 432443, Aug. 1993.48 A. Kendon, Current Issues in the Study of Gesture, The Biological Foundations of Gestures Motor and Semiotic Aspects, J.L.Nespoulous, P. Peron, and A. R. Lecours, eds., pp. 2347. Lawrence Erlbaum Assoc., 1986.49 C. Kervrann and F. Heitz, Learning Structure and DeformationModes of Nonrigid Objects in Long Image Sequences, Proc.Intl Workshop on Automatic Face and Gesture Recognition, June1995.50 R. Kjeldsen and J. Kender, Visual Hand Gesture Recognition forWindow System Control, Proc. Intl Workshop on Automatic Faceand Gesture Recognition, Zurich, Switzerland, pp. 184188, June1995.51 R. Kjeldsen and J. Kender, Finding Skin in Color Images, Proc.Intl Conf. Automatic Face and Gesture Recognition, Killington, Vt.,pp. 312317, Oct. 1996.52 R. Koch, Dynamic 3D Scene Analysis Through Synthetic Feedback Control, IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 15, no. 6, pp. 556568, 1993.53 M.W. Krueger, Artificial Reality II. AddisonWesley, 1991.54 M.W. Krueger, Environmental Technology Making the RealWorld Virtual, Comm. ACM, vol. 36, pp. 3637, July 1993.55 J.J. Kuch, VisionBased Hand Modeling and Gesture Recognitionfor Human Computer Interaction, masters thesis, Univ. of Illinois at UrbanaChampaign, 1994.56 J.J. Kuch and T.S. Huang, VisionBased Hand Modeling andTracking, Proc. IEEE Intl Conf. Computer Vision, Cambridge,Mass., June 1995.57 Y. Kuno, M. Sakamoto, K. Sakata, and Y. Shirai, VisionBasedHuman Computer Interface With User Centered Frame, Proc.IROS94, 1994.58 A. Lanitis, C.J. Taylor, T.F. Cootes, and T. Ahmed, AutomaticInterpretation of Human Faces and Hand Gestures Using FlexibleModels, Proc. Intl Workshop on Automatic Face and Gesture Recognition, Zurich, Switzerland, pp. 98103, June 1995.59 J. Lee and T.L. Kunii, ConstraintBased Hand Animation, Models and Techniques in Computer Animation, pp. 110127. TokyoSpringerVerlag, 1993.60 J. Lee and T.L. Kunii, ModelBased Analysis of Hand Posture,IEEE Computer Graphics and Applications, pp. 7786, Sept. 1995.61 E.T. Levy and D. McNeill, Speech, Gesture, and Discourse,Discourse Processes, no. 15, pp. 277301, 1992.62 C. Maggioni, A Novel Gestural Input Device for Virtual Reality,1993 IEEE Annual Virtual Reality Intl Symp., pp. 118124, IEEE,1993.63 C. Maggioni, GestureComputerNew Ways of Operating aComputer, Proc. Intl Workshop on Automatic Face and Gesture Recognition, Zurich, Switzerland, pp. 166171, June 1995.64 N. MagnenatThalmann and D. Thalman, Computer AnimationTheory and Practice. New York SpringerVerlag, 2nd rev. ed., 1990.65 D. McNeill and E. Levy, Conceptual Representations in Language Activity and Gesture, Speech, Place and Action Studies inDeixis and Related Topics, J. Jarvella and W. Klein, eds. Wiley, 1982.66 A. Meyering and H. Ritter, Learning to Recognize 3DHandPostures From Perspective Pixel Images, Artificial Neural Networks 2, I. Alexander and J. Taylor, eds. NorthHolland ElsevierScience Publishers B.V., 1992.67 B. Moghaddam and A. Pentland, Maximum Likelihood Detection of Faces and Hands, Proc. Intl Workshop on Automatic Faceand Gesture Recognition, Zurich, Switzerland, pp. 122128, June1995.68 ORourke and N.L. Badler, ModelBased Image Analysis of Human Motion Using Constraint Propagation, IEEE Trans. PatternAnalysis and Machine Intelligence, vol. 2, pp. 522536, 1980.69 V.I. Pavlovic, R. Sharma, and T.S. Huang, Gestural Interface to aVisual Computing Environment for Molecular Biologists, Proc.Intl Conf. Automatic Face and Gesture Recognition, Killington, Vt.,pp. 3035, Oct. 1996.70 D.L. Quam, Gesture Recognition With a DataGlove, Proc. 1990IEEE National Aerospace and Electronics Conf., vol. 2, 1990.694 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,  VOL.  19,  NO.  7,  JULY  199771 F.K.H. Quek, Toward a VisionBased Hand Gesture Interface,Virtual Reality Software and Technology Conf., pp. 1731, Aug. 1994.72 F.K.H. Quek, Eyes in the Interface, Image and Vision Computing,vol. 13, Aug. 1995.73 F.K.H. Quek, T. Mysliwiec, and M. Zhao, Finger Mouse A Freehand Pointing Interface, Proc. Intl Workshop on Automatic Faceand Gesture Recognition, Zurich, Switzerland, pp. 372377, June1995.74 L.R. Rabiner, A Tutorial on Hidden Markov Models and SelectedApplications in Speech Recognition, Proc. IEEE, vol. 77, pp. 257286, Feb. 1989.75 L.R. Rabiner and B. Juang, Fundamentals of Speech Recognition.Englewood Cliffs, N.J. Prentice Hall, 1993.76 J.M. Rehg and T. Kanade, DigitEyes VisionBased Human HandTracking, Technical Report CMUCS93220, School of ComputerScience, Carnegie Mellon Univ., 1993.77 J.M. Rehg and T. Kanade, ModelBased Tracking of SelfOccluding Articulated Objects, Proc. IEEE Intl Conf. ComputerVision, Cambridge, Mass., pp. 612617, June 2023 1995.78 H. Rheingold, Virtual Reality. Summit Books, 1991.79 J. Schlenzig, E. Hunter, and R. Jain, VisionBased Hand GestureInterpretation Using Recursive Estimation, Proc. 28th AsilomarConf. Signals, Systems, and Computer, 1994.80 J. Schlenzig, E. Hunter, and R. Jain, Recursive Identification ofGesture Inputs Using Hidden Markov Models, Proc. Second IEEEWorkshop on Applications of Computer Vision, Sarasota, Fla., pp. 187194, Dec. 57, 1994.81 J. Segen, Controlling Computers With Gloveless Gestures, Proc.Virtual Reality Systems, Apr. 1993.82 R. Sharma, Active Vision for Visual Servoing A Review, IEEEWorkshop on Visual Servoing Achievements, Applications and OpenProblems, May 1994.83 R. Sharma, T.S. Huang, and V.I. Pavlovic, A MultimodalFramework for Interacting With Virtual Environments, HumanInteraction With Complex Systems, C.A. Ntuen and E.H. Park, eds.,pp. 5371. Kluwer Academic Publishers, 1996.84 R. Sharma, T.S. Huang, V.I. Pavlovic, Y. Zhao, Z. Lo, S. Chu,K. Schulten, A. Dalke, J. Phillips, M. Zeller, and W. Humphrey,SpeechGesture Interface to a Visual Computing Environmentfor Molecular Biologists, Proc. Intl Conf. Pattern Recognition,1996.85 K. Shirai and S. Furui, Special Issue on Spoken Dialogue, SpeechCommunication, vol. 15, pp. 34, 1994.86 T.E. Starner and A. Pentland, Visual Recognition of AmericanSign Language Using Hidden Markov Models, Proc. Intl Workshop on Automatic Face and Gesture Recognition, Zurich, Switzerland, pp. 189194, June 1995.87 J. Streeck, Gesture as Communication I Its Coordination WithGaze and Speech, Communication Monographs, vol. 60, pp. 275299, Dec. 1993.88 D.J. Sturman and D. Zeltzer, A Survey of GloveBased Input,IEEE Computer Graphics and Applications, vol. 14, pp. 3039, Jan. 1994.89 D.X. Sun and L. Deng, Nonstationary Hidden Markov Modelsfor Speech Recognition, Image Models and Their Speech ModelCousins, S.E. Levinson and L. Shepp, eds., pp. 161182. NewYork SpringerVerlag, 1996.90 D.L. Swets and J. Weng, Using Discriminant Eigenfeatures forImage Retrieval, IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 18, no. 8, pp. 831836, 1996.91 K.A. Tarabanis, P.K. Allen, and R.Y. Tsai, A Survey of SensorPlanning in Computer Vision, IEEE Trans. Robotics and Automation, vol. 11, pp. 86104, 1995.92 D. Thompson, Biomechanics of the Hand, Perspectives in Computing, vol. 1, pp. 1219, Oct. 1981.93 Y.A. Tijerino, K. Mochizuki, and F. Kishino, Interactive 3DComputer Graphics Driven Through Verbal Instructions Previous and Current Activities at ATR, Computers and Graphics,vol. 18, no. 5, pp. 621631, 1994.94 A. Torige and T. Kono, HumanInterface by Recognition of Human Gestures With Image Processing. Recognition of Gesture toSpecify Moving Directions, IEEE Intl Workshop on Robot and Human Communication, pp. 105110, 1992.95 R. Tubiana, ed., The Hand, vol. 1. Philadelphia, Penn. Sanders,1981.96 C. Uras and A. Verri, Hand Gesture Recognition From EdgeMaps, Proc. Intl Workshop on Automatic Face and Gesture Recognition, Zurich, Switzerland, pp. 116121, June 1995.97 R. Vaillant and D. Darmon, VisionBased Hand Pose Estimation, Proc. Intl Workshop on Automatic Face and Gesture Recognition, Zurich, Switzerland, pp. 356361, June 1995.98 M.T. Vo, R. Houghton, J. Yang, U. Bub, U. Meier, A. Waibel, andP. Duchnowski, Multimodal Learning Interfaces, ARPA SpokenLanguage Technology Workshop 1995, Jan. 1995.99 M.T. Vo and A. Waibel, A MultiModal HumanComputer Interface Combination of Gesture and Speech Recognition, AdjunctProc. InterCHI93, Apr. 2629 1993.100 A. Waibel and K.F. Lee, Readings in Speech Recognition. MorganKaufmann, 1990.101 C. Wang and D.J. Cannon, A Virtual EndEffector Pointing System in PointandDirect Robotics for Inspection of Surface FlawsUsing a Neural NetworkBased Skeleton Transform, Proc. IEEEIntl Conf. Robotics and Automation, vol. 3, pp. 784789, May 1993.102 K. Watanuki, K. Sakamoto, and F. Togawa, Multimodal Interaction in Human Communication, IEICE Trans. Information and Systems, vol. E78D, pp. 609614, June 1995.103 A.D. Wilson and A.F. Bobick, Configuration States for the Representation and Recognition of Gesture, Proc. Intl Workshop onAutomatic Face and Gesture Recognition, Zurich, Switzerland,pp. 129134, June 1995.104 A.D. Wilson and A.F. Bobick, Recovering the Temporal Structureof Natural Gestures, Proc. Intl Conf. Automatic Face and GestureRecognition, Killington, Vt., pp. 6671, Oct. 1996.105 C. Wren, A. Azarbayejani, T. Darrell, and A. Pentland, PfinderRealTime Tracking of the Human Body, Proc. Intl Conf. Automatic Face and Gesture Recognition, Killington, Vt., pp. 5156, Oct.1996.Vladimir I. Pavlovic received the Dipl Eng degree in electrical engineering from the Universityof Novi Sad, Yugoslavia, in 1991. In 1993, hereceived the MS degree in electrical engineeringand computer science from the University ofIllinois at Chicago. He is currently a doctoralstudent in electrical engineering at the BeckmanInstitute and the Department of Electrical andComputer Engineering, University of Illinois atUrbanaChampaign. His research interests include visionbased computer interaction, multimodal signal fusion, and image coding.Rajeev Sharma is an assistant professor in theDepartment of Computer Science and Engineering at the Pennsylvania State University,University Park. After receiving a PhD in computer science from the University of Maryland,College Park, in 1993, he spent three years atthe University of Illinois, UrbanaChampaign asa Beckman Fellow and adjunct assistant professor in the Department of Electrical and Computer Engineering.He is a recipient of the Association of Computing Machinery Samuel Alexander Doctoral Dissertation Award andthe IBM predoctoral fellowship.His research interests lie in studying the role of computer vision inrobotics and advanced humancomputer interfaces.PAVLOVIC ET AL.    VISUAL INTERPRETATION OF HAND GESTURES FOR HUMANCOMPUTER INTERACTION A REVIEW 695T.S. Huang received his B.S. Degree in electrical engineering from National Taiwan University,Taipei, Taiwan, China and his MS and ScDdegrees in Electrical Engineering from the Massachusetts Institute of Technology, Cambridge,Massachusetts. He was on the faculty of theDepartment of Electrical Engineering at MIT from1963 to 1973 and on the faculty of the School ofElectrical Engineering and director of its Laboratory for Information and Signal Processing atPurdue University from 1973 to 1980. In 1980, he joined the Universityof Illinois at UrbanaChampaign, where he is now William L. EverittDistinguished Professor of Electrical and Computer Engineering, Research Professor at the Coordinated Science Laboratory, and head of theImage Formation and Processing Group at the Beckman Institute forAdvanced Science and Technology.During his sabbatical leaves Dr. Huang has worked at the MIT Lincoln Laboratory, the IBM Thomas J. Watson Research Center, and theRheinishes Landes Museum in Bonn, West Germany, and held visitingprofessor positions at the Swiss Institutes of Technology in Zurich andLausanne, University of Hannover in West Germany, INRSTelecommunications of the University of Quebec in Montreal, Canada, andUniversity of Tokyo, Japan. He has served as a consultant to numerous industrial firms and government agencies both in the US andabroad.Dr. Huangs professional interests lie in the broad area of information technology, especially the transmission and processing of multidimensional signals. He has published 11 books, and over 300 papers innetwork theory, digital filtering, image processing, and computer vision.He is a Fellow of the International Association of Pattern Recognition,IEEE and the Optical Society of America and has received a Guggenheim Fellowship, an A.V. Humboldt Foundation Senior U.S. ScientistAward, and a Fellowship from the Japan Association for the Promotionof Science. He received the IEEE Acoustics, Speech, and Signal Processing Societys Technical Achievement Award in 1987 and the Society Award in 1991. He is a Founding Editor of the International JournalComputer Vision, Graphics, and Image Processing and Editor of theSpringer Series in Information Sciences, published by Springer Verlag.
