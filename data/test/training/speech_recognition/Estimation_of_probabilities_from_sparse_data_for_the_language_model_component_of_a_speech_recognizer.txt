400 IEEE TRANSACTIONS ON ACOUSTICS, SPEECH,  AND SIGNAL PROCESSING, VOL. ASP35, NO. 3, MARCH 1987 Estimation of Probabilities from  Sparse  Data for the Language  Model  Component of a Speech Recognizer SLAVA M. KATZ AbstractThe  description of a novel  type of rngram  language  model is  given.  The  model offers, via a nonlinear  recursive procedure,  a com putation and space  efficient  solution to the problem of estimating prob abilities from  sparse  data. This  solution compares favorably  to other proposed  methods.  While the method  has  been  developed for  and suc cessfully  implemented in the IBM Real  Time  Speech  Recognizers, its generality  makes it applicable  in other  areas where the problem of es timating  probabilities from  sparse  data arises. Sparseness  of  data  is  an  inherent  property  of  any  real  text,  and it is a problem that one always encounters while collecting fre quency  statistics on words  and  word  sequences mgrams  from a text  of  finite  size.  This means  that even  for a  very  large  data  col lection,  the  maximum  likelihood  estimation  method  does  not  allow us to  adequately  estimate  probabilities  of  rare  but  nevertheless  pos sible  word sequencesmany sequences  occur only  once  single tons many  more do not occur  at  all.  Inadequacy of  the  maximum likelihood  estimator  and  the  necessity to  estimate  the  probabilities of  mgrams  which  did  not  occur  in the text  constitute  the  essence of  the  problem. The main  idea  of  the  proposed  solution to  the  problem is  to  re duce unreliable probability estimates given by the observed fre quencies and redistribute the freed probability mass among mgrams which  never  occurred  in  the  text. The reduction is achieved by replacing maximum likelihood estimates for mgrams having low counts with renormalized Turings estimates l, and the re distribution  is  done via the  recursive  utilization of lower  level  con ditional distributions. We found Turings method attractive be cause  of  its  simplicity  and  its  characterization  as  the  optimal empirical  Bayes  estimator  of  a  multinomial  probability.  Robbins in 2  introduces the empirical Bayes methodology and Nadas in 3 gives  various  derivations  of the  Turings  formula. Let N be  a  sample  text  size  and  let n, be the  number  of  words mgrams which  occurred  in  the  text  exactly r times, so that N  C rn,. 1 Turings  estimate PT for  a  probability  of  a  word mgram which occurred  in  the  sample r times  is r r PT  where We  call  a  procedure of replacing  a  count r with  a  modified  count r discounting  and  a  ratio r t  r  a  discount  coefficient dr. When r  r  ,  we  have  Turings  discounting. Let us denote the mgram w l ,   . . , w, as wy and the number of times  it  occurred  in  the  sample  text as c  w T  .  Then  the  maxi mum likelihood  estimate  is Manuscript  received  May 1, 1986  revised  September 2 ,  1986. The author  is  with  the  Continuous  Speech  Recognition  Group,  Depart ment of Computer  Sciences, IBM T. J .  Watson  Research  Center,  Yorktown Heights, NY 10598. IEEE Log  Number  8611734. and  the  Turings  estimate  is where .x   . x    1 n r o  l   f k x   It  follows  from 13 that  the  total  probability  estimate,  using  5  ,  for the set of words mgram that actually occurred in the sample  is c P T  W      1   nl w c  w f    0 N This, in turn,  leads  to  the  estimate  for  the  probability  of  observing some previously unseen mgram as a fraction nl  N  of single tons  in  the  text On the  other  hand, where Thus, Our method is based on  the interpretation  of 6, in 1 1 as a  con tribution  of  an mgram w with a count c  w y   to  the probability of unseen  mgrams.  We  go  further with this interpretation for the  estimating of conditional  probabilities P  w ,  I w y    . Assum ing  that  the m  1gram w has  been  observed  in  the  sample text,  we  introduce  an  entity ijynd analogous to 6,, given by 10 We now define our  estimator P, w, 1 w  inductively as  follows. Assume  that  the  lower  level  conditional  estimator P, w, I w    has been defined. Then, when c  w 7  l    0 ,  gives the conditional probability estimate for words w, observed in the sample texl after w7I c w7   0  .  It is convenient to define  a  function 3 by This  gives an estimate  of  the  sum of conditional  probabilities of all words w, which  never  followed w Y    c  w y    0  .  We  distribute the probability mass 0 ,  defined by 14, among w, for which c  w 7    0 using a previously defined by induction  lower  level conditional  distribution P, w,  1 W T   I  P s  w J W   I    .P,w,Iwy 15 00963518870300040001 .OO 0 1987 IEEE IEEE TRANSACTIONS ON ACOUSTICS, SPEECH, AND  SIGNAL  PROCESSING, VOL. ASSP35, NO. 3, MARCH 1987 40 1 where 1  em Pw,jwy w m  C  w ,   O   1  em Pw,wy w ,  c  w ,   O  is  a  normalizing  constant.  When cw7I  0, then we define P,w,JwT  Psw,wy. 17 Complementing P and definitions, given by 13 and 14, for the  case  when cwy  0, with Pwmlwyl  0 B  w y  1    1 ,  and we finally combine  estimates  13,  15,  and  17  in  the  following recursive  expression  for  the  conditional  probability  distribution Pswrnwy  PwrnlwT  B    w ,  w        aWy1  PswrnwT 18 where We now propose  a  somewhat  modified  version of the  distribu tion  given by 18.  We  shall  leave  intact  the  estimate nl  N for  the probability of all  unseen mgrams  and  we  shall  not  discount  high values of counts c  k ,  considering them as reliable. To achieve this, we redefine d,  1, for r  k 20 and we shall  correspondingly  adjust  Turings  original  discount  coef ficients d, for r 5 k so that the equation expressing the balance between  the  contributions  and  the  probability of unseen mgrams is satisfied. Equation 21 is analogous to 9. We obtain an ad justment of the d, looking  for  a  solution of 21  in  the  form where p is  a  constant.  The  unique  solution  is  given by    r  k   1    I   k   1     I  r nl d,  , for 1 5 r 5 k. 23 1   n1 Equality  22  is  equivalent  to  the  requirement  that  newly  defined count  contributions  r   N  r   N   are  proportional  to  the  Tur ings contributions  r  N   r   N  .  As for the value for the parameter k ,  in practice, k  5 or so is  a  good  choice.  Other  ways of computing  discount  coefficients  could  be  suggested  for, in gen eral,  the mode1,is not very  sensitive  to  the  actual  values  of d,. The latter  holds  as  long  as  an  estimate  for  the  probability of observing some  previously  unseen mgram is reasonable.  When  data  are  very TABLE 1 PERPLEXITY RESULTS ON 100  TEST SENTENCES Model 1 2 3 2gram 118  119  117 3gram 89  91  88 sparse,  an  estimate n,  N  is  well  justifiedthere  is  not  too much of a  difference in seeing  an mgram once  or  not  seeing it at all. Numerical values for as can be precomputed, and that pro vides  the  methods  computational  efficiency. A 3gram model  con structed  in  accordance  with  formulas  18,  16,  and  23  was  im plemented  as a  language  model  component of the  RealTime  Speech Recognizer  4.  On  the  other  hand,  as  our  experiments  show,  set ting dl  0, which is equivalent to discarding all singletons, does not affect  the  model  performance,  and  thus  provides  substan tial  saving in space  needed for  the  language  model.  We  took  ad vantage of it in  constructing  a  compact  language  model  for  the PC based  Speech  Recognizer 5 .  The  approach  described  compares  favorably  to  other  proposed methods.  Table I gives  the  perplexity  computation  results  for  three models  being  compared.  The first column  gives  the  results  for  the deleted  estimation  method by Jelinek  and  Mercer  6,  the  second column  for  the  parametric  empirical Bayes  method by  NBdas 7, and  the  third one  for  our  back of f   estimation  method. Perplexity, defined as 2 where 1  I .  WI,    , w, is the  test word  sequence  and m  2,  3  for  a  2gram and 3gram model, respectively, is used here to characterize the performance of the modelthe lower  the  perplexity,  the  better  the model. The statistics used for constructing the models were ob tained  from  approximately  750 000 words of an office  correspon dence  database 100 sentences  were  used  for  testing.  This  and  other experiments  showed  that  our  method  consistently  yields  perplexity results  which  are  at  least  as  good  as  those  obtained by other  meth ods. Meanwhile, our model is much easier to construct, imple ment,  and  use. In closing,  we  wish  to  emphasize  that  the  novelty of our  approach  lies  in  the  nonlinear  backoff  procedure which utilizes  an  explicit  estimation of the  probability of unseen mgrams and  not in the  details of computation  such  as  the  use of Turings formulas. REFERENCES l I. J .  Good,  The  population  frequencies  of  species  and  the  estimation of  population  parameters, Biometrika, vol. 40, no.  3  and 4, pp. 237 264, 1953. 2 H. E. Robbins,  An  empirical  Bayes  approach  to  statistics,  in Proc. 3rd Berkeley Symp. Math. Statist. Prob., vol. 1, 1956, pp. 157164. 3  A.  Nadas,  On  Turings  formula for  word  probabilities, ZEEE Trans. Acoust., Speech, Signal Processing, vol. ASSP33, pp. 14141416, Dec. 1985. 4 A. Averbuch et al., A realtime isolatedword, speech recognition system  for  dictation  transcription,  in Proc. IEEE Znt. Con Acoust.,  Speech, Signal Processing, Tampa, FL, 1985, pp. 858861. 5 A. Averbuch e t  a l . ,  An IBM PC based largevocabulary isolated utterance  speech  recognizer,  in Proc. ZEEE  Znt. Con5 Acoust., Speech,  Signal  Processing, Tokyo,  Japan, Apr. 1986. 6 F. Jelinek  and R. L. Mercer,  Interpolated  estimation  of  Markov  source parameters  from  sparse data, in Pattern  Recognition in Practice, E. S.  Gelsema and L. N. Kana, Eds. Amsterdam NorthHolland, 1980. 7 A. NAdas, Estimation of probabilities in the language model of the IBM  Speech  Recognition  System, ZEEE Trans.  Acoust.,  Speech,  Sig nal Processing, vol.  ASSP32,  pp.  859861,  Aug.  1984.
