Discriminative Training and Maximum Entropy Models for StatisticalMachine TranslationFranz Josef Och and Hermann NeyLehrstuhl fur Informatik VI, Computer Science DepartmentRWTH Aachen  University of TechnologyD52056 Aachen, Germanyoch,neyinformatik.rwthaachen.deAbstractWe present a framework for statisticalmachine translation of natural languagesbased on direct maximum entropy models, which contains the widely used sourcechannel approach as a special case. Allknowledge sources are treated as featurefunctions, which depend on the sourcelanguage sentence, the target languagesentence and possible hidden variables.This approach allows a baseline machinetranslation system to be extended easily byadding new feature functions. We showthat a baseline statistical machine translation system is significantly improved using this approach.1 IntroductionWe are given a source French sentence fJ1 f1, . . . , fj , . . . , fJ , which is to be translated into atarget English sentence eI1  e1, . . . , ei, . . . , eI .Among all possible target sentences, we will choosethe sentence with the highest probability1eI1  argmaxeI1PreI1fJ1  1The argmax operation denotes the search problem,i.e. the generation of the output sentence in the targetlanguage.1The notational convention will be as follows. We use thesymbol Pr to denote general probability distributions withnearly no specific assumptions. In contrast, for modelbasedprobability distributions, we use the generic symbol p.1.1 SourceChannel ModelAccording to Bayes decision rule, we can equivalently to Eq. 1 perform the following maximizationeI1  argmaxeI1PreI1  PrfJ1 eI1 2This approach is referred to as sourcechannel approach to statistical MT. Sometimes, it is also referred to as the fundamental equation of statistical MT Brown et al., 1993. Here, PreI1 isthe language model of the target language, whereasPrfJ1 eI1 is the translation model. Typically, Eq. 2is favored over the direct translation model of Eq. 1with the argument that it yields a modular approach.Instead of modeling one probability distribution,we obtain two different knowledge sources that aretrained independently.The overall architecture of the sourcechannel approach is summarized in Figure 1. In general, asshown in this figure, there may be additional transformations to make the translation task simpler forthe algorithm. Typically, training is performed byapplying a maximum likelihood approach. If thelanguage model PreI1  peI1 depends on parameters  and the translation model PrfJ1 eI1 pfJ1 eI1 depends on parameters , then the optimal parameter values are obtained by maximizingthe likelihood on a parallel training corpus fS1 , eS1Brown et al., 1993  argmaxSs1pfses 3  argmaxSs1pes 4                Computational Linguistics ACL, Philadelphia, July 2002, pp. 295302.                         Proceedings of the 40th Annual Meeting of the Association forSourceLanguage TextPreprocessingPreI1 Language ModelooGlobal SearcheI1  argmaxeI1PreI1  PrfJ1 eI1PrfJ1 eI1 Translation ModelooPostprocessingTargetLanguage TextFigure 1 Architecture of the translation approach based on sourcechannel models.We obtain the following decision ruleeI1  argmaxeI1peI1  pfJ1 eI1 5Stateoftheart statistical MT systems are based onthis approach. Yet, the use of this decision rule hasvarious problems1. The combination of the language model peI1and the translation model pfJ1 eI1 as shownin Eq. 5 can only be shown to be optimal if thetrue probability distributions peI1  PreI1and pfJ1 eI1  PrfJ1 eI1 are used. Yet,we know that the used models and trainingmethods provide only poor approximations ofthe true probability distributions. Therefore, adifferent combination of language model andtranslation model might yield better results.2. There is no straightforward way to extend abaseline statistical MT model by including additional dependencies.3. Often, we observe that comparable results areobtained by using the following decision ruleinstead of Eq. 5 Och et al., 1999eI1  argmaxeI1peI1  peI1fJ1  6Here, we replaced pfJ1 eI1 by peI1fJ1 .From a theoretical framework of the sourcechannel approach, this approach is hard to justify. Yet, if both decision rules yield the sametranslation quality, we can use that decisionrule which is better suited for efficient search.1.2 Direct Maximum Entropy TranslationModelAs alternative to the sourcechannel approach, wedirectly model the posterior probability PreI1fJ1 .An especially wellfounded framework for doingthis is maximum entropy Berger et al., 1996. Inthis framework, we have a set of M feature functions hmeI1, fJ1 ,m  1, . . . , M . For each featurefunction, there exists a model parameter m,m 1, . . . , M . The direct translation probability is givenSourceLanguage TextPreprocessing1  h1eI1, fJ1 ooGlobal SearchargmaxeI1 Mm1mhmeI1, fJ1 2  h2eI1, fJ1 oo. . .ooPostprocessingTargetLanguage TextFigure 2 Architecture of the translation approach based on direct maximum entropy models.byPreI1fJ1   pM1 eI1fJ1  7expMm1 mhmeI1, fJ1 eI1expMm1 mhmeI1, fJ1 8This approach has been suggested by Papineni etal., 1997 Papineni et al., 1998 for a natural language understanding task.We obtain the following decision ruleeI1  argmaxeI1PreI1fJ1  argmaxeI1 Mm1mhmeI1, fJ1 Hence, the timeconsuming renormalization in Eq. 8is not needed in search. The overall architecture ofthe direct maximum entropy models is summarizedin Figure 2.Interestingly, this framework contains as specialcase the source channel approach Eq. 5 if we usethe following two feature functionsh1eI1, fJ1   log peI1 9h2eI1, fJ1   log pfJ1 eI1 10and set 1  2  1. Optimizing the correspondingparameters 1 and 2 of the model in Eq. 8 is equivalent to the optimization of model scaling factors,which is a standard approach in other areas such asspeech recognition or pattern recognition.The use of an inverted translation model in theunconventional decision rule of Eq. 6 results if weuse the feature function log PreI1fJ1  instead oflog PrfJ1 eI1. In this framework, this feature canbe as good as log PrfJ1 eI1. It has to be empiricallyverified, which of the two features yields better results. We even can use both features log PreI1fJ1 and log PrfJ1 eI1, obtaining a more symmetrictranslation model.As training criterion, we use the maximum classposterior probability criterionM1  argmaxM1Ss1log pM1 esfs11This corresponds to maximizing the equivocationor maximizing the likelihood of the direct translation model. This direct optimization of the posterior probability in Bayes decision rule is referred toas discriminative training Ney, 1995 because wedirectly take into account the overlap in the probability distributions. The optimization problem hasone global optimum and the optimization criterionis convex.1.3 Alignment Models and MaximumApproximationTypically, the probability PrfJ1 eI1 is decomposedvia additional hidden variables. In statistical alignment models PrfJ1 , aJ1 eI1, the alignment aJ1 is introduced as a hidden variablePrfJ1 eI1 aJ1PrfJ1 , aJ1 eI1The alignment mapping is j  i  aj from sourceposition j to target position i  aj .Search is performed using the socalled maximumapproximationeI1  argmaxeI1PreI1 aJ1PrfJ1 , aJ1 eI1 argmaxeI1PreI1 maxaJ1PrfJ1 , aJ1 eI1Hence, the search space consists of the set of all possible target language sentences eI1 and all possiblealignments aJ1 .Generalizing this approach to direct translationmodels, we extend the feature functions to include the dependence on the additional hidden variable. Using M feature functions of the formhmeI1, fJ1 , aJ1 ,m  1, . . . , M , we obtain the following modelPreI1, aJ1 fJ1  expMm1 mhmeI1, fJ1 , aJ1 eI1,aJ1expMm1 mhmeI1, fJ1 , aJ1 Obviously, we can perform the same step for translation models with an even richer structure of hiddenvariables than only the alignment aJ1 . To simplifythe notation, we shall omit in the following the dependence on the hidden variables of the model.2 Alignment TemplatesAs specific MT method, we use the alignment template approach Och et al., 1999. The key elementsof this approach are the alignment templates, whichare pairs of source and target language phrases together with an alignment between the words withinthe phrases. The advantage of the alignment template approach compared to single wordbased statistical translation models is that word context andlocal changes in word order are explicitly considered.The alignment template model refines the translation probability PrfJ1 eI1 by introducing two hidden variables zK1 and aK1 for the K alignment templates and the alignment of the alignment templatesPrfJ1 eI1 zK1 ,aK1PraK1 eI1 PrzK1 aK1 , eI1  PrfJ1 zK1 , aK1 , eI1Hence, we obtain three different probabilitydistributions PraK1 eI1, PrzK1 aK1 , eI1 andPrfJ1 zK1 , aK1 , eI1. Here, we omit a detailed description of modeling, training and search, as this isnot relevant for the subsequent exposition. For further details, see Och et al., 1999.To use these three component models in a directmaximum entropy approach, we define three different feature functions for each component of thetranslation model instead of one feature function forthe whole translation model pfJ1 eI1. The featurefunctions have then not only a dependence on fJ1and eI1 but also on zK1 , aK1 .3 Feature functionsSo far, we use the logarithm of the components ofa translation model as feature functions. This is avery convenient approach to improve the quality ofa baseline system. Yet, we are not limited to trainonly model scaling factors, but we have many possibilities We could add a sentence length featurehfJ1 , eI1  IThis corresponds to a word penalty for eachproduced target word. We could use additional language models byusing features of the following formhfJ1 , eI1  heI1 We could use a feature that counts how manyentries of a conventional lexicon cooccur inthe given sentence pair. Therefore, the weightfor the provided conventional dictionary can belearned. The intuition is that the conventionaldictionary is expected to be more reliable thanthe automatically trained lexicon and thereforeshould get a larger weight. We could use lexical features, which fire if acertain lexical relationship f, e occurshfJ1 , eI1 Jj1f, fj Ii1e, ei We could use grammatical features that relatecertain grammatical dependencies of sourceand target language. For example, using a function k that counts how many verb groups exist in the source or the target sentence, we candefine the following feature, which is 1 if eachof the two sentences contains the same numberof verb groupshfJ1 , eI1  kfJ1 , keI1 12In the same way, we can introduce semanticfeatures or pragmatic features such as the dialogue act classification.We can use numerous additional features that dealwith specific problems of the baseline statistical MTsystem. In this paper, we shall use the first three ofthese features. As additional language model, weuse a classbased fivegram language model. Thisfeature and the word penalty feature allow a straightforward integration into the used dynamic programming search algorithm Och et al., 1999. As this isnot possible for the conventional dictionary feature,we use nbest rescoring for this feature.4 TrainingTo train the model parameters M1 of the direct translation model according to Eq. 11, we use the GISGeneralized Iterative Scaling algorithm Darrochand Ratcliff, 1972. It should be noted that, aswas already shown by Darroch and Ratcliff, 1972,by applying suitable transformations, the GIS algorithm is able to handle any type of realvalued features. To apply this algorithm, we have to solve various practical problems.The renormalization needed in Eq. 8 requires asum over a large number of possible sentences,for which we do not know an efficient algorithm.Hence, we approximate this sum by sampling thespace of all possible sentences by a large set ofhighly probable sentences. The set of consideredsentences is computed by an appropriately extendedversion of the used search algorithm Och et al.,1999 computing an approximate nbest list of translations.Unlike automatic speech recognition, we do nothave one reference sentence, but there exists a number of reference sentences. Yet, the criterion as itis described in Eq. 11 allows for only one referencetranslation. Hence, we change the criterion to allow Rs reference translations es,1, . . . , es,Rs for thesentence esM1  argmaxM1Ss11RsRsr1log pM1 es,rfsWe use this optimization criterion instead of the optimization criterion shown in Eq. 11.In addition, we might have the problem that nosingle of the reference translations is part of the nbest list because the search algorithm performs pruning, which in principle limits the possible translations that can be produced given a certain input sentence. To solve this problem, we define for maximum entropy training each sentence as referencetranslation that has the minimal number of word errors with respect to any of the reference translations.5 ResultsWe present results on the VERBMOBIL task, whichis a speech translation task in the domain of appointment scheduling, travel planning, and hotel reservation Wahlster, 1993. Table 1 shows the corpus statistics of this task. We use a training corpus, which is used to train the alignment templatemodel and the language models, a development corpus, which is used to estimate the model scaling factors, and a test corpus.Table 1 Characteristics of training corpus Train,manual lexicon Lex, development corpus Dev,test corpus Test.German EnglishTrain Sentences 58 073Words 519 523 549 921Singletons 3 453 1 698Vocabulary 7 939 4 672Lex Entries 12 779Ext. Vocab. 11 501 6 867Dev Sentences 276Words 3 159 3 438PP trigr. LM  28.1Test Sentences 251Words 2 628 2 871PP trigr. LM  30.5So far, in machine translation research does notexist one generally accepted criterion for the evaluation of the experimental results. Therefore, we usea large variety of different criteria and show that theobtained results improve on most or all of these criteria. In all experiments, we use the following sixerror criteria SER sentence error rate The SER is computed as the number of times that the generatedsentence corresponds exactly to one of the reference translations used for the maximum entropy training. WER word error rate The WER is computedas the minimum number of substitution, insertion and deletion operations that have to be performed to convert the generated sentence intothe target sentence. PER positionindependent WER A shortcoming of the WER is the fact that it requiresa perfect word order. The word order of anacceptable sentence can be different from thatof the target sentence, so that the WER measure alone could be misleading. To overcomethis problem, we introduce as additional measure the positionindependent word error ratePER. This measure compares the words in thetwo sentences ignoring the word order. mWER multireference word error rate Foreach test sentence, there is not only used a single reference translation, as for the WER, buta whole set of reference translations. For eachtranslation hypothesis, the edit distance to themost similar sentence is calculated Nieen etal., 2000. BLEU score This score measures the precisionof unigrams, bigrams, trigrams and fourgramswith respect to a whole set of reference translations with a penalty for too short sentencesPapineni et al., 2001. Unlike all other evaluation criteria used here, BLEU measures accuracy, i.e. the opposite of error rate. Hence,large BLEU scores are better. SSER subjective sentence error rate For amore detailed analysis, subjective judgmentsby test persons are necessary. Each translated sentence was judged by a human examiner according to an error scale from 0.0 to 1.0Nieen et al., 2000. IER information item error rate The test sentences are segmented into information items.For each of them, if the intended informationis conveyed and there are no syntactic errors,the sentence is counted as correct Nieen etal., 2000.In the following, we present the results of this approach. Table 2 shows the results if we use a directtranslation model Eq. 6.As baseline features, we use a normal word trigram language model and the three component models of the alignment templates. The first row showsthe results using only the four baseline features with1      4  1. The second row shows theresult if we train the model scaling factors. We see asystematic improvement on all error rates. The following three rows show the results if we add theword penalty, an additional classbased fivegramTable 2 Effect of maximum entropy training for alignment template approach WP word penalty feature,CLM classbased language model fivegram, MX conventional dictionary.objective criteria  subjective criteria SER WER PER mWER BLEU SSER IERBaselinem  1 86.9 42.8 33.0 37.7 43.9 35.9 39.0ME 81.7 40.2 28.7 34.6 49.7 32.5 34.8MEWP 80.5 38.6 26.9 32.4 54.1 29.9 32.2MEWPCLM 78.1 38.3 26.9 32.1 55.0 29.1 30.9MEWPCLMMX 77.8 38.4 26.8 31.9 55.2 28.8 30.90.740.760.780.80.820.840.860.880.90 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000sentence error rate SERnumber of iterationsMEMEWPMEWPCLMMEWPCLMMXFigure 3 Test error rate over the iterations of theGIS algorithm for maximum entropy training ofalignment templates.language model and the conventional dictionary features. We observe improved error rates for using theword penalty and the classbased language model asadditional features.Figure 3 show how the sentence error rate SERon the test corpus improves during the iterations ofthe GIS algorithm. We see that the sentence errorrates converges after about 4000 iterations. We donot observe significant overfitting.Table 3 shows the resulting normalized modelscaling factors. Multiplying each model scaling factor by a constant positive value does not affect thedecision rule. We see that adding new features alsohas an effect on the other model scaling factors.6 Related WorkThe use of direct maximum entropy translation models for statistical machine translation has been sugTable 3 Resulting model scaling factors of maximum entropy training for alignment templates 1trigram language model 2 alignment templatemodel, 3 lexicon model, 4 alignment modelnormalized such that4m1 m  4.ME WP CLM MX1 0.86 0.98 0.75 0.772 2.33 2.05 2.24 2.243 0.58 0.72 0.79 0.754 0.22 0.25 0.23 0.24WP  2.6 3.03 2.78CLM   0.33 0.34MX    2.92gested by Papineni et al., 1997 Papineni et al.,1998. They train models for natural language understanding rather than natural language translation.In contrast to their approach, we include a dependence on the hidden variable of the translation modelin the direct translation model. Therefore, we areable to use statistical alignment models, which havebeen shown to be a very powerful component forstatistical machine translation systems.In speech recognition, training the parameters ofthe acoustic model by optimizing the average mutual information and conditional entropy as they aredefined in information theory is a standard approachBahl et al., 1986 Ney, 1995. Combining variousprobabilistic models for speech and language modeling has been suggested in Beyerlein, 1997 Petersand Klakow, 1999.7 ConclusionsWe have presented a framework for statistical MTfor natural languages, which is more general than thewidely used sourcechannel approach. It allows abaseline MT system to be extended easily by addingnew feature functions. We have shown that a baseline statistical MT system can be significantly improved using this framework.There are two possible interpretations for a statistical MT system structured according to the sourcechannel approach, hence including a model forPreI1 and a model for PrfJ1 eI1. We can interpret it as an approximation to the Bayes decision rulein Eq. 2 or as an instance of a direct maximum entropy model with feature functions log PreI1 andlog PrfJ1 eI1. As soon as we want to use modelscaling factors, we can only do this in a theoreticallyjustified way using the second interpretation. Yet,the main advantage comes from the large number ofadditional possibilities that we obtain by using thesecond interpretation.An important open problem of this approach isthe handling of complex features in search. An interesting question is to come up with features thatallow an efficient handling using conventional dynamic programming search algorithms.In addition, it might be promising to optimize theparameters directly with respect to the error rate ofthe MT system as is suggested in the field of patternand speech recognition Juang et al., 1995 Schluterand Ney, 2001.ReferencesL. R. Bahl, P. F. Brown, P. V. de Souza, and R. L. Mercer. 1986. Maximum mutual information estimationof hidden markov model parameters. In Proc. Int.Conf. on Acoustics, Speech, and Signal Processing,pages 4952, Tokyo, Japan, April.A. L. Berger, S. A. Della Pietra, and V. J. DellaPietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics,2213972, March.P. Beyerlein. 1997. Discriminative model combination. In Proc. of the IEEE Workshop on AutomaticSpeech Recognition and Understanding, pages 238245, Santa Barbara, CA, December.P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, andR. L. Mercer. 1993. The mathematics of statisticalmachine translation Parameter estimation. Computational Linguistics, 192263311.J. N. Darroch and D. Ratcliff. 1972. Generalized iterative scaling for loglinear models. Annals of Mathematical Statistics, 4314701480.B. H. Juang, W. Chou, and C. H. Lee. 1995. Statistical and discriminative methods for speech recognition.In A. J. R. Ayuso and J. M. L. Soler, editors, SpeechRecognition and Coding  New Advances and Trends.Springer Verlag, Berlin, Germany.H. Ney. 1995. On the probabilisticinterpretation ofneuralnetwork classifiers and discriminative trainingcriteria. IEEE Trans. on Pattern Analysis and MachineIntelligence, 172107119, February.S. Nieen, F. J. Och, G. Leusch, and H. Ney. 2000.An evaluation tool for machine translation Fast evaluation for MT research. In Proc. of the Second Int.Conf. on Language Resources and Evaluation LREC,pages 3945, Athens, Greece, May.F. J. Och, C. Tillmann, and H. Ney. 1999. Improvedalignment models for statistical machine translation.In Proc. of the Joint SIGDAT Conf. on Empirical Methods in Natural Language Processing and Very LargeCorpora, pages 2028, University of Maryland, College Park, MD, June.K. A. Papineni, S. Roukos, and R. T. Ward. 1997.Featurebased language understanding. In EuropeanConf. on Speech Communication and Technology,pages 14351438, Rhodes, Greece, September.K. A. Papineni, S. Roukos, and R. T. Ward. 1998. Maximum likelihood and discriminative training of directtranslation models. In Proc. Int. Conf. on Acoustics,Speech, and Signal Processing, pages 189192, Seattle, WA, May.K. A. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2001.Bleu a method for automatic evaluation of machinetranslation. Technical Report RC22176 W0109022,IBM Research Division, Thomas J. Watson ResearchCenter, Yorktown Heights, NY, September.J. Peters and D. Klakow. 1999. Compact maximum entropy language models. In Proc. of the IEEE Workshopon Automatic Speech Recognition and Understanding,Keystone, CO, December.R. Schluter and H. Ney. 2001. Modelbased MCE boundto the true Bayes error. IEEE Signal Processing Letters, 85131133, May.W. Wahlster. 1993. Verbmobil Translation of facetoface dialogs. In Proc. of MT Summit IV, pages 127135, Kobe, Japan, July.
