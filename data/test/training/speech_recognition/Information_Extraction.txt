Information ExtractionJim Cowie and Yorick Wilks1. IntroductionInformation Extraction IE is the name given to any process which selectively structures andcombines data which is found, explicitly stated or implied, in one or more texts. The final outputof the extraction process varies in every case, however, it can be transformed so as to populatesome type of database. Information analysts working long term on specific tasks already carryout information extraction manually with the express goal of database creation.One reason for interest in IE is its role in evaluating, and comparing, different Natural LanguageProcessing technologies. Unlike other NLP technologies, MT for example, the evaluationprocess is concrete and can be performed automatically. This, plus the fact that a successfulextraction system has   immediate applications, has encouraged research funders to support bothevaluations of and research into IE. It seems at the moment that this funding will continue andwill bring about the existence of working systems. Applications of IE are still scarce. A few wellknown examples exist and other classified systems may also be in operation. It is certainly nottrue that the level of the technology is such that it is easy to build systems for new tasks, or thatthe levels of performance are sufficiently high for use in fully automatic systems. The effect onlong term research on NLP is debatable and this is considered in the final section whichspeculates on future directions in IE.We begin our examination of IE by considering a specific example from the Fourth MessageUnderstanding Conference MUC4 DARPA 92 evaluation. An examination of the prognosisfor this relatively new, and as yet unproven, language technology follows together with a briefhistory of how IE has evolved is given. The related problems of evaluation methodology and taskdefinition are examined. The current methods used for building IE extraction systems are outlined. The term IE can be applied to a range of tasks, and we consider three generic applications.1. An Example The MUC4 Terrorism TaskThe task given to participants in the MUC4 evaluation 1991 was to extract specificinformation on terrorist incidents from newspaper and newswire texts relating to South America.Human analysts in this case the participants in the evaluation prepared training and test data byperforming human extraction from a set of texts. The templates to be completed, either byhumans, or by computers, consisted of slot labels, and rules as to how the slot was to be filled.For MUC4 a flat record structure was used, slots which had no information being left empty.Without further commentary we give a short text and its associated template    SANTIAGO, 10 JAN 90  TEXT POLICE ARE CARRYING OUT INTENSIVE OPERATIONS IN THE TOWN OF MOLINA IN THE SEVENTH REGION IN SEARCH OF A GANGOF ALLEGED EXTREMISTS WHO COULD BE LINKED TO A RECENTLY DISCOVEREDARSENAL. IT HAS BEEN REPORTED THAT CARABINEROS IN MOLINA RAIDED THEHOUSE OF 25YEAROLD WORKER MARIO MUNOZ PARDO, WHERE THEY FOUND AFAL RIFLE, AMMUNITION CLIPS FOR VARIOUS WEAPONS, DETONATORS, ANDMATERIAL FOR MAKING EXPLOSIVES.   IT SHOULD BE RECALLED THAT A GROUP OF ARMED INDIVIDUALS WEARING SKIMASKS ROBBED A BUSINESSMAN ON A RURAL ROAD NEAR MOLINA ON 7 JANUARY. THE BUSINESSMAN, ENRIQUE ORMAZABAL ORMAZABAL, TRIED TO RESISTTHE MEN SHOT HIM AND LEFT HIM SERIOUSLY WOUNDED. HE WAS LATER HOSPITALIZED IN CURICO. CARABINEROS CARRIED OUT SEVERAL OPERATIONS,INCLUDING THE RAID ON MUNOZ HOME. THE POLICE ARE CONTINUING TOPATROL THE AREA IN SEARCH OF THE ALLEGED TERRORIST COMMAND.FIGURE 1. Extracted Terrorism TemplateTemplate Slot ID Fill Value0.  MESSAGE ID DEVMUC30017 NCCOSC1.  MESSAGE TEMPLATE 12.  INCIDENT DATE 07 JAN 903.  INCIDENT LOCATION CHILE MOLINA CITY4.  INCIDENT TYPE ROBBERY5.  INCIDENT STAGE OF EXECUTION ACCOMPLISHED6.  INCIDENT INSTRUMENT ID 7.  INCIDENT INSTRUMENT TYPE GUN 8.  PERP INCIDENT CATEGORY TERRORIST ACT9.  PERP INDIVIDUAL ID ARMED INDIVIDUALS  GROUP OF ARMED INDIVIDUALS WEARING SKI MASKS  MEN10. PERP ORGANIZATION ID 11. PERP ORGANIZATION CONFIDENCE    12. PHYS TGT ID 13. PHYS TGT TYPE 14. PHYS TGT NUMBER 15. PHYS TGT FOREIGN NATION 16. PHYS TGT EFFECT OF INCIDENT 17. PHYS TGT TOTAL NUMBER 18. HUM TGT NAME ENRIQUE ORMAZABAL ORMAZABAL19. HUM TGT DESCRIPTION BUSINESSMAN ENRIQUE ORMAZABAL ORMAZABAL20. HUM TGT TYPE CIVILIAN ENRIQUE ORMAZABAL ORMAZABAL21. HUM TGT NUMBER 1 ENRIQUE ORMAZABAL ORMAZABAL22. HUM TGT FOREIGN NATION 23. HUM TGT EFFECT OF INCIDENT INJURY ENRIQUE ORMAZABAL ORMAZABAL24. HUM TGT TOTAL NUMBERThe template illustrates the two of the basic types of slot strings from the text e.g. ENRIQUEORMAZABAL ORMAZABAL, and set fills in which one of a set of predeterminedcategories must be selected e.g. ROBBERY, GUN, ACCOMPLISHED. On the surface theproblem appears reasonably straightforward. The reader should bear in mind, however, that thedefinition of a template must be precise enough to allow human analysts to produce consistentfilled templates keys and also give clear guidelines to the builders of automatic systems. Wereturn to these problems in Section2. Information Extraction A core language technology IE technology has not yet reached the market but it could be of great significance to informationenduser industries of all kinds, especially finance companies, banks, publishers and governments. For instance, finance companies want to know facts of the following sort and on a largescale what company takeovers happened in a given time span they want widely scattered textinformation reduced to a simple data base. Lloyds of London need to know of daily ship sinkingsthroughout the world and pay large numbers of people to locate them in newspapers in a widerange of languages. All these are potential uses for   IE.Computational linguistic techniques and theories are playing a strong role in this emerging technology, which should not be confused with the more mature technology of Information RetrievalIR, which selects a relevant subset of documents from a larger set. IE extracts information fromthe actual text of documents. Any application of this technology is usually preceded by an IRphase, which selects a set of documents relevant to some querynormally a string of features orterms that appear in the documents. So, IE is interested in the structure of the texts, whereas onecould say that, from an IR point of view, texts are just bags of words.You can contrast these two ways of envisaging text information and its usefulness by thinkingabout finding, from the World Wide Web, what TV programs you might want to watch in thenext week there is already a web site in operation with text descriptions of the programs on 25or more British TV channels, more text than most people can survey easily at a single session.On this web site you can input the channels or genre e.g. musicals, news etc. that interest youand the periods when you are free to watch. You can also specify up to twelve words that canhelp locate programs for you, e.g. stars or film directors names. The web site has aconventional IR engine behind it, a standard boolean function of the words and genrechannelnames you use. The results are already usefuland currently free and treat the programdescriptions as no more than bags of words.Now suppose you also wanted to know what programs your favorite TV critic liked and supposethe web site also had access to the texts of recent newspapers. An IR system cannot answer thatquestion because it requires searching review texts for films and seeing which ones are describedin favorable terms. Such a task would require IE and some notion of text structure. In fact, such asearch for program evaluations is not a best case for IE, and we mention it only because it is anexample of the kind of leisure and entertainment application that will be so important in futureinformatics developments. To see that one only has to think of the contrast between the designeduses and the actual uses of the French Minitel systemdesigned for phone number informationbut actually used largely as an adult dating service.Some extraction tasks push out the limits of extracting structured information in a standard form,In fact any task with an evaluative component e.g. one can search movie reviews for directorsand actorseven for films where an individual appears in the nonstandard role, such as Mel Gibson as a director, and that is a difficult task for an IR systemthose are potentially matchable totemplates, but a much harder task is to decide if a movie review is positive or not. It is said thatUS Congressmen, who receive vast amounts of email that they almost certainly cannot read,would welcome any IE system that could tell them simply, of the content of each emailmessage. The result of such a component could clearly be expressed as a templatewhat isunclear is how one could fill it in a reliable manner.An important insight, even after accepting our argument that IE is a new, emergent technology,is that what may seem to be wholly separate information technologies are really not so MT andIE, for example, are just two ways of producing information to meet peoples needs and can becombined in differing ways for example, one could translate a document and then extractinformation from the result or viceversa, which would mean just translating the contents of theresulting templates. Which of these one chose to do might depend on the relative strengths of thetranslation systems available a simpler one might only be adequate to translate the contents oftemplates, and so on. This last observation emphasizes that the product of an IE systemthefilled templates can be seen either as a compressed, or summarized, text itself, or as a form ofdata base with the fillers of the template slots corresponding to conventional database fields.One can then imagine new, learning, techniques like data mining being done as a subsequentstage on the results of IE itself.3. Information Extraction A Recent EnthusiasmExtracting information from text as a demonstration of understanding goes back to the earlydays of NLP. Early work by DeJong 79 at Yale University was on searching texts with a computer to fill predetermined slots in structures, called scripts by his advisor Schank 77, butwhich were close to what would now more usually be called templates structures withpredetermined slots to be filled in specified ways, as a Film Director slot would be filled with aname and a ShipSinkingName slot would be filled with a ships name. Film evaluations are notvery script like, but the scenario of ships sinking needed by Lloyds of London, or the patternsof company takeovers, are much more templatescenario like and suitable for IE techniques.Early commercially used systems like JASPER from Carnegie Group Andersen et al 86,built for Reuters depended on very complex handcrafted templates, made up by analysts and avery specific extraction task. However, the IE movement has grown by exploiting, and joining,the recent trend towards a more empirical and text based computational linguistics, that is to sayby putting less emphasis on linguistic theory and trying to derive structures and various levels oflinguistic generalization from the large volumes of text data that machines can now manipulate.Information Extraction, particularly in the context of automatic evaluation against human produced results, is a relatively new phenomenon. The early Message Understanding Conferences,in 1987 and 1989, processed naval shipto shore messages. A move was then made to extractterrorism information from general newspaper texts. The task of developing the human producedkeys template structures filled with data for specific texts was shared among the MUCparticipants themselves. The combination of an evaluation methodology and a task which hasdefinite applicability, and appears practicable, attracted the attention of various U.S. governmentagencies, who were prepared to pay for the development of large numbers of keys usingprofessional information analysts. IE as a subject and standards of evaluation and success up toMUC5 were surveyed in Lehnert  Cowie 1996, and broadly one can say that the field grewvery rapidly when ARPA, the US defense agency, funded competing research groups to pursueIE,   based initially on scenarios like the MUC4 terrorism events. To this task were added thedomains of joint ventures and microelectronics fabrication developments, with extractionsystems for two languages, English and Japanese. All these tasks represent domains where thefunders want to replace the government analysts who read the newspapers and then filltemplates when and where, a terrorist event took place, how many casualties etc. Automatingthis painful human activity is the goal of IE.A fairly stable RD community has arisen around the Message Understanding Conferences. Aswell as the U.S. participants, a few groups from Europe, Canada, and Japan have also beeninvolved. The idea of a common task as a stimulus to research is a useful one, but it also has dangers. In particular, getting so focused on performing well in the evaluation may actually forcepeople to follow avenues which are only short term solutions. The other drawback is that theamount of software development needed to produce the specific requirements of an extractionsystem are very large. A common plea at the MUC organizing committee is lets not have thenext one next year, that way well get some time to do research. On the other hand someactually usable technologies are appearing as a result of the focus on IE and the visibility of theevaluations to funders, both government and commercial. Recognizing and classifying names intext, not a task of particular interest to the NLP community, now proves to be possible at highlevels of accuracy. That IE provides a good focus for NLP research is debatable. One keyrequirement for making IE a usable technology is developing the ability to produce IE systemsrapidly without using the full resources of an NLP research laboratory. The most recent MUCshave introduced a task, coreference evaluation, with the goal of stimulating more fundamentalNLP research.The trend inside the ARPA Tipster Text Initiative, which provides funding for research on IEand IR, is to attempt to standardize NLP around a common architecture for annotatingdocuments Grishman 96. This has proved useful for building multicomponent NLP systemswhich share this common representation. CRLs Temple machine translation system Zajac 96,and Oleada language training system Ogden 96 both use this representation system as does theSheffield GATE system described later in this article. This quest for some kind ofstandardization is now extending to specifying the kind of information patterns basically whichdrive IE systems. More formally the idea is to have a common representation language thatdifferent developers can use to share pieces of an extraction system. Thus if someone hasexpended a lot of effort recognizing information on people in texts this can be incorporated intosomeone elses system to recognize changes in holders of particular jobs. Reusable componentsof this type would certainly reduce the duplication of effort which is currently occurring in theMUC evaluations.4. Evaluation and Template DesignEvaluation is carried out for IE by comparing the templates produced automatically by an extraction program with templates for the same texts produced by humans. The evaluation can be fullyautomatic. Thus analysts produce a set of filled out templates or keys using a computer tool toensure correct formatting and selection of fields. The automatic system produces its templates inthe same form and a scoring program then produces sets of results see Figure 1, Overview ofthe Development of an Extraction System, on page 6 below for every slotMost of the MUC evaluations have been based on giving a one point score for every slotcorrectly filled Correct. Spurious slots S are also counted, these are slots that are generated,and filled, despite there being no information in the text, and slots with incorrect fills I. Thetotal number of correct slots TC in a template or key is also known. These numbers allow twobasic scores to be calculated PRECISION, a measure of the percentage correctness of theinformation produced, and RECALL, a measure of the percentage of information availablewhich is actually found. These measures are adapted from information retrieval, and are not so appropriate for IE. Forexample in an objectstyle template, if a pointer to a person slot is filled, this counts as a correctfill, then if the name is filled in the person object, this counts as a second correct fill. Anotherproblem comes when there are multiple objects filling a slot how should the scoring systemmatch these up with the multiple objects generated by the human analyst For example an objectmay contain a company name and a location. If two of these objects are created one with anempty name slot and the location slot filled, and the other with data in the company name slotand in the location slot. There are also two objects in the key if they are aligned in one order thecompany slot is correct, but both locations are incorrect. Aligned in the opposite order thecompany name is incorrect, but both locations are correct. The method of counting correct slotscan produce some paradoxical results for a systems scores. In MUC3 one single key which wasfilled with information about the killing of Jesuit priests was used as the extracted informationfor each of the test documents. This gave scores as good as many systems which were genuinelytrying to extract information. Similarly a set of keys with only pointers to objects and no stringsin any other slots was submitted by George Krupka GE at that time in MUC5. This scoredabove the median of system performance. The point really is that the details of how a score wereachieved is important. A 100 recall IR system is easy to build too, just retrieve all thedocuments.FIGURE 2. A Partial View of System Summary Scores  MicroElectronics TemplateSLOT POS ACT COR PAR INC SPU MIS REC PREtemplate 100 100 100 0 0 0 0 100 100content 123 134 94 0 2 38 27 76 70subtotals 123 134 94 0 2 38 27 76 70entity 121 131 91 0 9 31 21 75 69name 121 131 77 3 20 31 21 65 60location 58 47 25 4 4 14 25 46 57nationality 36 19 14 0 4 1 18 39 74type 121 131 91 0 9 31 21 75 69subtotals 336 328 207 7 37 77 85 63 64microprocess 124 134 94 0 2 38 28 76 70process 124 134 84 0 12 38 28 68 63developer 63 91 23 0 9 59 31 36 25manufacturer 83 141 43 0 15 83 25 52 30distributor 80 138 45 0 9 84 26 56 33purchaser 25 36 13 0 1 22 11 52 36subtotals 375 540 208 0 46 286 121 55 38layering 44 57 36 0 1 20 7 82 63type 44 57 32 2 3 20 7 75 58film13 2 0 0 1 1 12 0 0temperature 5 5 2 0 0 3 3 40 40device 13 9 6 0 0 3 7 46 67equipment 39 57 20 0 13 24 6 51 35subtotals 114 130 60 2 17 51 35 54 47lithography 51 47 35 0 1 11 15 69 74subtotals 161 165 90 5 12 58 54 57 56etching 17 15 9 0 1 5 7 53 60subtotals 39 37 16 2 5 14 16 44 46packaging 12 15 10 0 0 5 2 83 67subtotals 35 40 25 0 0 15 10 71 62To give a flavor of what an IE system developer faces during an evaluation we present a muchreduced set of summary scores for the MUC5 microelectronics task Figure 2, A PartialView of System Summary Scores  MicroElectronics Template, on page 8. This presents totalscores for a batch of documents. Individual scores by document are also produced by the scoringprogram. The first column shows the names of the slots in the template objects. New objects aremarked by delimiting angle brackets. The next columns are the number of correct fills for theslot, the number of fills produced by the system, the number correct, the number partiallymatched i.e. part, but not all, of a noun phrase recognized, the number incorrect, the numbergenerated which have no equivalent in the human produced key SPUrious, the numbermissing, and finally the recall and precision scores for this slot. At the end of the report arescores total scores for all slots, total scores for only slots in matched objects, a line showing howmany texts had templates correctly generated. Finally a score is given, the F measure, whichcombines retrieval and precision into one number. This can be weighted to favor recall orprecision PR, 2PR,P2R.The Effects of EvaluationThe aim of evaluation is to highlight differences between NLP methods to show improvement inthe technology over time, and to push research in certain directions. These goals are somewhat inconflict as we will now show. One result of the whole evaluation process has been to push mostof the successful groups into very similar methods based on finite state automata and partialparsing Appelt 95, Grishman 96a. One key factor in the development process is to be able toscore test runs rapidly and evaluate changes to the system. Slower more complex systems are notwell suited for this rapid test development cycle. The demonstration of improvement over timeimplies that the same tasks be attempted repeatedly, year, after year. This is an extremely boringprospect for system developers and the MUC evaluations have moved to new tasks in everyother evaluation. Making a comparison of performance between old and new tasks is extremelydifficult.The whole scoring system, coupled with the public evaluation process, can actually result indecisions being made in system development which are incorrect in terms of languageprocessing, but which happen to give better scores.One novel focus produced by IE is in what Donald Walker once called the Ecology of Language. Most NLP research was concerned with problems which were most easily tested withsentences containing no proper nouns. Why bother then with the idiosyncratic complexities ofproper nouns Walker observed that this ecology would have to be addressed before realistictext processing on general text could be undertaken. The IE evaluations have forced people toaddress this issue and as a result highly accurate name recognition and classification systemshave been developed. A separate Tipster evaluation was in fact set up to find if accurate namerecognition technology better than 90 precision and recall could be produced for languagesother than English. The Multilingual Named Entity Task MET Merchant 96 was set up in avery short period of time and showed that scores of between 80 and 90 precision and recallwere easily achievable for Spanish, Chinese, and Japanese. Markup here was carried out usingSGML.Template DefinitionThe evaluation methodology depends on a detailed task specification. Without a clear specification the training and test keys produced by the human analysts have low consistency. Often thereis a cycle of discovery with new areas of divergence between template designers and human template fillers regularly having to be resolved.  This task involves complex decisions, which canhave serious implications for the builders of extraction systems.Defining templates is a difficult task involving the selection of the information elementsrequired, and the definition of their relationships. This applied task has been further complicatedin the evaluations by the attempt to define slots which provided NLP challenges. For exampledetermining if a contract is being planned, under execution, or has terminated.Often theseslots became very low priority for the extraction system builders as an attempt to fill them oftenhad seriously prejudicial effects on the system score. Often the best approach is to simply selectthe most common option.The actual structure of the templates used has varied from the flat record structure of MUC4 to amore complex object oriented definition which was used for Tipster and MUC5 and MUC6.This groups related information into a single object. For example a person object might containthree strings name, title, age, and an employer slot, which is a pointer to an organization object.The information contained in both types of representation is equivalent. The newer object styletemplates make it easier to handle multiple entities which share one slot, as it groups together theinformation related to each entity in the corresponding object. The readability of the key inprinted form suffers as much of it consists of pointers.The definition consists of two parts a syntactic description of the structure of the template oftengiven in a standard form known as BNF  Backus Naur Form, and a written description of howto determine whether a template should be filled and detailed instructions on determining thecontent of the slots. The description of the Tipster joint venture task extended to more than 40pages. For the simple task of name recognition described below seven pages are used. To see thatthis detail is necessary consider the following short extract  4.1 Expressions Involving ElisionMultiname expressions containing conjoined modifiers with elision of the head of one conjunct should be marked up asseparate expressions.North and South America ENAMEX TYPELOCATIONNorthENAMEXand ENAMEX TYPELOCATIONSouth AmericaENAMEXA similar case involving elision with number expressions10 and 20dollar bills NUMEX TYPEMONEY10NUMEX andNUMEX TYPEMONEY20dollarNUMEX billsIn contrast, there is no elision in the case of singlenameexpressions containing conjoined modifiers such expressionsshould be marked up as a single expression.U.S. Fish and Wildlife Service ENAMEX TYPEORGANIZATIONU.S.Fish and Wildlife ServiceENAMEXThe subparts of range expressions should be marked up as separateexpressions.175 to 180 million Canadian dollars NUMEX TYPEMONEY175NUMEX to NUMEX TYPEMONEY180 million Canadian dollarsNUMEXthe 198687 academic year the TIMEX TYPEDATE1986TIMEXTIMEX TYPEDATE ALT8787 academic yearTIMEX   A short sample of the BNF for the microelectronics task is given below. It should be notedthat while the texts provided for this task included many on the packaging of microchips theyalso included a few on the packaging of potato chips.MICROELECTRONICSCAPABILITY PROCESS  LAYERING  LITHOGRAPHY  ETCHING PACKAGING DEVELOPER  ENTITY MANUFACTURER  ENTITY DISTRIBUTOR  ENTITY PURCHASERORUSER  ENTITY COMMENT   ENTITY NAME ENTITY NAMELOCATION LOCATION NATIONALITY LOCATIONCOUNTRYONLY TYPE COMPANY, PERSON, GOVERNMENT,OTHERCOMMENT  PACKAGING TYPE PACKTYPE PITCH NUMBERPITCH UNITS MIL, IN, MMPACKAGEMATERIAL CERAMIC, PLASTIC, EPOXY, GLASS, CERAMICGLASS,  OTHER PLCOUNT NUMBER UNITSPERPACKAGENUMBER BONDING BONDTYPES DEVICE DEVICE EQUIPMENT EQUIPMENT COMMENT  New Types of TaskThree qualitatively different tasks are now being evaluated at the Message UnderstandingConferences Name recognition and classification see above Template element creation  simple structures linking information on one particularentity Scenario template creation  more complex structures linking template elements.The first two tasks are intended to be domain independent, and the third domain specific. Thedegree of difficulty ranges from easy for the first through most difficult for the last. It wasintended that each task would provide a base support for its successors, however, the undecomposed output required for the names task may not provide sufficient information to support thetemplate element creation task. The scenario template creation task is distinguished by the factthat a time constraint is placed on the system developers. The specifics of the task are announceda mere month before the evaluation. Thus groups possessing systems which can be rapidlyadapted should do well at this task. Groups possessing people with insomnia may also do relatively well5. Methods and ToolsPractically every known NLP technique has been applied to this problem. Currently the mostsuccessful systems use a finite state automata approach, with patterns being derived fromtraining data and corpora, or specified by computational linguists. The simplicity of this type ofsystem design allows rapid testing of patterns using feedback from the scoring system. Theexperience of the system developers in linguistics, and in the development of IE systemsremains, however, an important factor.When IE has been attempted for languages other than English the problem appears to be no moredifficult. In fact for the Japanese tasks in MUC and Tipster it seemed that the structure of thetexts made IE actually easier.Systems relying solely on training templates and heuristic methods of combination have beenattempted with some success for the microelectronics domain. This means the system is usingno NLP whatsoever. In the earlier MUC evaluations there was a definite bias against using thetraining data to help build the system. By MUC3 groups were using the training data to findpatterns in the text, and to extract lists of organizations and locations. This approach, althoughsuccessful, has the drawback that only in the early MUC evaluations were hundreds of trainingkeys available.Much work on learning and statistical methods have been applied to the IE task. This has givenrise to a number of independent components which can be applied to the IE task. A conspicuoussuccess has been partofspeech taggers, systems that assign one and only one part ofspeechsymbol like Proper noun, or Auxiliary verb to a word in a running text and do so on the basisusually of statistical generalizations across very large bodies of text. Recent research Church96 has shown that a number of quite independent modules of analysis of this kind can be builtup independently from data, usually very large electronic texts, rather than coming from eitherintuition or some dependence on other parts of a linguistic theory.These independent modules, each with reasonably high levels of performance in blind tests,include partofspeech tagging, aligning texts sentencebysentence in different languages,syntax analysis, attaching word sense tags to words in texts to disambiguate them in context andso on. That these tasks can be done relatively independently is very surprising to those whobelieved them all contextually dependent subtasks within a larger theory. These modules havebeen combined in various ways to perform tasks like IE as well as more traditional ones likemachine translation MT. The modules can each be evaluated separately but they are not in theend real human tasks that people actually do, as MT and IE are.One can call the former intermediate tasks and the latter real or final tasksand it is reallyonly the latter that can be firmly evaluated against human needs by people who know whata translation, say, is and what it is for. The intermediate tasks are evaluated internally to improveperformance but are only, in the end, stages on the way to some larger goal. Moreover, it is notpossible to have quite the same level of confidence in them since what is, or is not, a correct syntactic structure for a sentence is clearly more dependent on ones commitments to a linguistictheory of some sort, and such matters are in constant dispute. What constitutes proper extractionof peoples names from texts, or a translation of it, can be assessed more consistently by manypeople with no such subjective commitments.The empirical movement, basing, as it does, linguistic claims on text data, has another streamthe use in language processing of large language dictionaries of single languages and bilingualforms that became available about ten years ago in electronic forms from publishers tapes.These are not textual data in quite the sense above, since they are large sets of intuitions aboutmeaning set out by teams of lexicographers or dictionary makers. Sometimes they are actuallywrong, but they have nevertheless proved a useful resource for language processing bycomputer, and lexicons derived from them have played a role in actual working MT and IEsystems Cowie et al 93.What such lexicons lack is a dynamic view of a language they are inevitably fossilizedintuitions. To use a well known example dictionaries of English normally tell you that the first,or main, sense of television is as a technology or a TV set, although it is mainly used now tomean the medium itself. Modern texts are thus out of step with dictionarieseven modern ones.It is this kind of evidence that shows that, for tasks like IE, lexicons must be adapted or tunedto the texts being analyzed which has led to a new, more creative wave, in IE research the neednot just to use large textual and lexical resources, but to adapt them as automatically as possible,to enable systems to support new domains and corpora.  This means both dealing with theirobsolescent vocabulary  and extending the lexicon with the specialized vocabulary of the newdomain.6. Assembling a Generic IE systemIEs brief history is tightly tied to the recent advances in empirical NLP, in particular to thedevelopment and evaluation of relatively independent modules for a range of linguistic tasks,many of which had been traditionally seen as inseparable, or only achievable within somegeneral knowledgebased AI program. It has been something of a surprise to many that suchstriking results have been achieved in tasks as various as word sense tagging, syntactic parsing,word sense tagging, sentence alignment between parallel corpora, and so on. Striking heremeans over 95 accuracy, and those who do not find this striking should remember the manyyears of argument in linguistics and AI that such tasks, however apparently lowlevel, could notbe performed without access to strong theories or knowledge representations.All this is of strong relevance to IE and to the question of which of such modules, if any, an IEsystem should consist of, since it now hard to conceive of IE except as some combination of suchmodules, usually within an overall management architecture such as GATE. Hobbs has arguedHobbs 95 that most IE systems will draw their modules from a fairly predictable set and hasspecified a Generic IE System that anyone can construct like a tinkertoy from an inventory ofthe relevant modules, cascaded in an appropriate manner. The original purpose of thisdescription was to allow very brief system presentations at the MUC conferences to highlighttheir differences from the generic system.   Most systems contain most of the functionalitiesdescribed below, but where exactly they occur and how they are linked together variesimmensely. Many systems, at least in the early days were fairly monolithic Lisp programs.External forces, such as a requirement for speed, which has meant reimplementation in C orC, the necessary reuse of external components, such as Japanese segementors, and the desireto have standalone modules for proper name recognition, which is reaching the status of a usefulcommercial product, have imposed new modularity on the IE system. We will retain Hobbsdivision of the generic system for a brief exploration of the functionalities required for an IEsystem. Hobbs system consists of ten modules1. a Text Zoner, which turns a text into a set of segments.2. a Preprocessor which turns a text or text segment into a sequence of sentences, each of whichbeing a sequence of lexical items.3. a Filter, which turns a sequence of sentences into a smaller set of sentences by filtering outirrelevant ones.4. a Preparser, which takes a sequence of lexical items and tries to identify reliably determinablesmallscale structures.5. a Parser, which takes a set of lexical items words and phrases and outputs a set of parsetreefragments, which may or may not be complete.6.  a Fragment Combiner, which attempts to combine parsetree or logicalform fragments into astructure of the same type for the whole sentence.7. a Semantic Interpreter, which generates semantic structures or logical forms from parsetreefragments.8. a Lexical Disambiguator, which indexes lexical items to one and only one lexical sense, orcan be viewed as reducing the ambiguity of the predicates in the logical form fragments.9.  a Coreference Resolver which identifies different descriptions of the same entity in differentparts of a text.10.  a Template Generator which fills the IE templates from the semantic structures.We consider in some more detail the functionality of each of these components.Text ZonerThe zoner uses whatever format information is available from markup information and textlayout to select those parts of a text which will actually go through the remainder of theprocesses. It isolates the rest of the system from the differences in possible text formats. Markuplanguages such as HTML and SGML Goldfarb 90 provide the most explicit and well definedstructure. Most newswires too support some sort of convention for indicating fielded data in atext. Special fields such as a dateline, giving the location and date of an article, can berecognized and stored in separate internal field. Problematic portions of a text, such as headlinesusing uppercase, can be isolated for separate treatment. If paragraph boundaries, or tables arealso flagged then the zoner is the place to recognize them.PreprocessorSentences are not normally marked, even in SGML documents, so special techniques arerequired to recognize sentence boundaries. For most languages the main problem here isdistinguishing the use of the full stop as a sentence terminator from its use as an abbreviationmarker Dr., Mr., etc. and also other idiosyncratic uses e.g. .... Paradoxically languageswhich appear to be more difficult as they dont use spaces to separate lexical units Japanese,Chinese do not have this stop ambiguity problem and can have sentences identified relativelyeasily.Once the sentences are identified, or as a part of this process, it is necessary to identify lexicalitems and possibly convert them to an appropriate form for lexical lookup. Thus, we may converteach word in an English text to uppercase, while still retaining information about its original caseusage. Recognizing words is relatively easy in most languages due to the use of spaces. Japaneseand Chinese now provide problems and a special purpose segmentation program is normallyused to identify with the ever popular 90 accuracy. The Juman program produced at KyotoUniversity has been used by many sites as their preprocessor for Japanese. Juman also providespart of speech information and typically this type of lexical information is extracted at this stageof processing.FilterThe filter process can serve several purposes. Following our argument that IE and IR are naturalpartners we would normally assume that texts processed by IE have already come through theimplicit filter of the IR system. Therefore the assumption is they do contain appropriate information. Many do, but a side effect of the retrieval process is to supply some bogus articles whichmay pass through an IE system producing incorrect data. A popular example from the microelectronics domain in Tipster was some article on packaging potato chips these were not aboutpackaging microelectronic chips, the actual topic for the IE system. Thus a filter may attemptto block these texts which are artifacts of the IR process.The main objective of a filter is to reduce the load on the rest of the system. If relevantparagraphs can be identified then the others can be abandoned. This is particularly important forsystems which do extensive parsing on every sentence. The risk here is that paragraphscontaining relevant information may be lost.Normally a filter process will rely on identifying either supporting vocabulary, or patterns, tosupport its operation. This may be in the inform of simple word counting or more elaboratestatistical processing.PreparserThis stage handles the ecology of natural language described earlier and contains what is arguably the most successful of the results of IE so far proper name identification and classification.Typically numbers in text or numeric form, dates, and other regularly formed constructions arealso recognized here. This may involve the use of case information, special lexicons, and contextfree patterns, which can be processed rapidly. Often a second pass may be required to confirmshortened forms of names which can not be reliably identified by the patterns, but which can beflagged more reliably once fuller forms of the names are identified. Truly accurate name classification may require some examination of context and usage. Although not common it is possibleto provide many instances where simple methods will fail Tuesday Morning  a chain of US stores Ms. Washington  a government staffer nCube  a company which does not follow normal capitalization conventions China  a town in MexicoA sophisticated system will pass all possible options to the next stages of processing, possiblyincreasing their complexity.ParserMost systems perform some type of partial parsing. The necessity of processing actualnewspaper sentences, which are often long and very complex, means the development of acomplete grammar is impossible. Accurate identification of the structure of noun phrases, andsubordinate clauses is, however, possible. This stage may be combined with the process ofsemantic interpretation described below.Fragment CombinerThe fragment combiner attempts to produce a complete structure for a sentence. It operates onthe components identified in the parser and will use a variety of heuristics to produce arelationship between the fragments. Up to this point it can be argued that the process is domainindependent.Semantic InterpreterA mapping from the syntactic structures to semantic structures related to the templates to befilled has to be carried out. Systems relate the structures found in a sentence to a specifictemplate using semantic information. Semantic processing may use verb subcategorizationinformation to check if appropriate types are found in the context around a verb or noun phrase.Simple techniques like identifying the semantic types of an apposition may be used to producecertain structures. For example Jim Smith human name, chairman occupation of XYZ CorpCompany name can produce two template objects a person, employed by a company, and acompany, which has an employee. The imposition of semantic restrictions also produces adisambiguation effect as if inappropriate fillers are found the template elements may not beproduced.At the end of this stage structures will be available which contain fillers for some of the slots in atemplate.Lexical DisambiguatorThis process can occur either as a side effect of other processes for example the semantic interpreter, or an even earlier filtering stage. It can also be a standalone stage prior even to parsing.The process does have to occur somewhere in the system.Coreference ResolverCoreference is an important component in further combining fragments to produce fewer, butmore completely filled templates. It can be carried out both in the early stages, when pronounsand noun phrases can be linked to proper names using a variety of cues, both syntactic andsemantic. It can also be delayed to the final stages when semantic structures can be merged. Bothidentity, meronymy partof relationships, and event coreference are required by an IE system.Reference to the original text, as well as to the semantic structures may be required forsuccessful processing. Strong merging may have the unfortunate effect of merging distinctevents as one. Just as unfortunate is the lack of merging identifying two events when in fact onlyone occurred.Template GeneratorFinally the semantic structures have to be unwound into a structure which can be evaluated automatically or fed into a database system. This stage is fairly automatic, but may actually absorb asignificant degree of effort to ensure that the correct formats are produced and that the stringsfrom the original text are used.The System as a WholeHobbs is surely right that most or all of these functions will be found somewhere in an IEsystem the last by definition. However, the description we give of module 8 shows that it is aprocess that can be performed early, on lexical items, or later, on semantic structures. There is agreat deal of dispute about what appears under module number 5, since some systems use a formof syntactic parser but the majority now prefer some form of direct application of corpusderivedfinitestate patterns to the lexical sequences, which is a process that would once have been calledsemantic parsing Cowie 93.Other such contrasts could be drawn, and it is probably not correct to assert as some do that,because there is undoubtedly much genericness in IE, there is only one standard IE system andeveryone uses it. Another important point to make is that IE is not, as many persist in believing,wholly superficial and theoryfree, with no consequences for broader NLP and CL, no matterwhat the level of success achieved, by the whole technology or by individual modules. many ofthe major modules encapsulate highly traditional CLNLP tasks and preoccupations e.g. syntactic parsing, wordsense disambiguation, coreference resolution etc. and their optimization, individually or in combination, to very high levels of accuracy, by whatever heuristics, is asubstantial success for the traditional concerns of the field.7. The Sheffield GATE SystemThe system designed at the University of Sheffield has been evaluated in two MUCs and it hasdone particularly well at the named entity task. It incorporates aspects of the earlier NMSUDIDEROT TIPSTER system Cowie et al 93, and the POETIC system Mellish et al 92 fromthe University of Sussex, since some members of those teams joined forces to do IE at Sheffield.There are two aspects of the Sheffield system first, a software environment called GATE  General Architecture for Text Engineering Cunningham 95  which attempts to meet thefollowing objectives support information interchange between LE modules at the   highest common levelpossible without prescribing a theoretical   approach though it allows modules which sharetheoretical   presuppositions to pass data in a mutually accepted common form support the integration of modules written in any source   language, available either insource or binary form, and be   available on any common platform support the evaluation and refinement of LE component modules,   and of systems builtfrom them, via a uniform, easytouse graphical   interface which in addition offers facilitiesfor managing test   corpora and ancillary linguistic resources.GATE owes a great deal to collaboration with the TIPSTER architecture. Secondly, they havebuilt VIE a vanilla Extraction System within GATE, one version of which LaSie has enteredtwo MUC evaluations Gaizauskas 95.GATE DesignGATE comprises three principal elements GDM, the GATE Document Manager, based on theTIPSTER document manager CREOLE, a Collection of REusable Objects for Language Engineering a set of LE modules integrated with the system and GGI, the GATE GraphicalInterface, a development tool for LE RD, providing integrated access to the services of theother components and adding visualization and debugging tools.Working with GATE the researcher will from the outset reuse existing components, and thecommon APIs of GDM and CREOLE mean only one integration mechanism must be learned.And as CREOLE expands, more and more modules will be available from external sources.VIE An Application In GATEFocussing on IE within the context of the ARPA Message Understanding Conferences, hasmeant fully implementing a system that processes unrestricted real world1 text containing large numbers of proper   names,idiosyncratic punctuation, idioms, etc. processes relatively large volumes of text in a reasonable time needs to achieve only a relatively shallow level of   understanding in a predefined domainarea  can be ported to a new domain area relatively rapidly a few   weeks at most.Given these features of the IE task, many developers of IE systems have opted for robust,shallow processing approaches which do not employ a general framework for knowledgerepresentation, as that term is generally understood. That is, there may be no attempt to build ameaning representation of the overall text, nor to represent and use world and domain knowledgein a general way to help in resolving ambiguities of attachment, word sense, quantifier scope, coreference, and so on. Such shallow approaches typically rely on collecting large numbers oflexically triggered patterns for partially filling templates, as well as domainspecific heuristicsfor merging partially filled templates to yield a final, maximally filled template. This approach isexemplified in systems such as the SRI FASTUS system Appelt 95 and the SRA and MITREMUC6 systems Kru95,Abe95.However, this is not the approach that we have taken in VIE. While still not attempting fullunderstanding whatever that might mean, we do attempt to derive a richer meaning representation of the text than do many IE systems, a representation that goes beyond the template itself.Our approach is motivated by the belief, which may be controverted if shallower approachesprove consistently more successful, that high levels of precision in the IE task simply will not beachieved without attempting a deeper understanding of at least parts of the text. Such an understanding requires, given current theories of natural language understanding, both the translationof the individual sentences of the text into an initial, canonical meaning representation formalismand also the availability of general and domain specific world knowledge together with a reasoning mechanism that allows this knowledge to be used to resolve ambiguities in the initial textrepresentation and to derive information implicit in the text.The key difference between the VIE approach and shallower approaches to IE is that the discourse model and intermediate representations used to derive it in VIE are less task andtemplatespecific than those used in other approaches. However, while committed to derivingricher representations than many IE systems, we are still attempting to achieve only limited,domaindependent understanding, and hence the representations and mechanisms adopted stillmiss much meaning. The approach we have adopted to KR does, nevertheless, allow us toaddress in a general way the problems of presupposition, coreference resolution, robust parsingand inferencedriven derivation of template fills. Results from the MUC6 evaluation show thatsuch an approach does no worse overall than shallower approaches and we believe that itsgenerality will, in the long run, lead to the significantly higher levels of precision which will beneeded to make IE a genuinely usable NL technology. Meanwhile, we are developing within                                                1Well, the Wall Street Journal.GATE and using many LaSIe modules, a simpler finite state pattern matcher of the, now, classictype. We will then, within GATE, be able to compare the performances of the two sets ofmodules.8. The FutureIf we think along these lines we see that the first distinction of this paper, between traditional IRand the newer IE, is not totally clear everywhere but can itself become a question of degree. Suppose parsing systems that produce syntactic and logical representations were so good, as somenow believe, that they could process huge corpora in an acceptably short time. One can thenthink of the traditional task of computer question answering in two quite different ways. The oldway was to translate a question into a formalized language like SQL and use it to retrieveinformation from a database as   in Tell me all the IBM executives over 40 earning under 50Ka year. But with a full parser of large corpora one could now imagine transforming in the queryto form an IE template and searching the WHOLE TEXT not a data base for all examples ofsuch employeesboth methods should produce exactly the same result starting from differentinformation sources  a text versus a formalized database.What we have called an IE template can now be seen as a kind of frozen query that one can reusemany times on a corpus and is therefore only important when one wants stereotypical, repetitive,information back rather than the answer to oneoff questions.Tell me the height of Everest, as a question addressed to a formalized text corpus is then neither IR nor IE but a perfectly reasonable single request for an answer. Tell me about fungi,addressed to a text corpus with an IR system, will produce a set of relevant documents but noparticular answer. Tell me what films my favorite movie critics likes, addressed to the righttext corpus, is undoubtedly IE as we saw, and will produce an answer also. The needs and theresources available determine the techniques that are relevant, and those in turn determine whatit is to answer a question as opposed to providing information in a broader sense.At Sheffield we are working on two applications of IE systems funded as European CommissionLRE projects one, AVENTINUS is in the classic IE tradition, seeking information onindividuals about security, drugs and crime, and using classic templates. the other ECRAN, amore research orientated project, searches movie and financial databases and exploits the notionwe mentioned of tuning a lexicon so as to have the right contents, senses and so on to deal withnew domains and relations unseen before.In all this, and with the advent of speech research products and the multimedia associated withthe Web, it is still important to keep in mind how much of our cultural, political and businesspatrimony is still bound up with texts, from manuals for machines, to entertainment news tonewspapers themselves. The text world is vast and growing exponentially one should never beseduced by multimedia fun into thinking that text and how to deal with it, how to extract itscontent, is going to go away.ReferencesP. M. Andersen, P. J. Hayes, A. K. Heuttner, L. M. Schmandt, and I. B. Nirenberg. Automatic extraction InProceedings of the Conference of   the Association for Artificial Intelligence, pages 10891093, Philadelphia, 1986.C. Aone, H. Blejer, S. Flank, D McKee, S Shinn. The Murasaki Project Multilingual natural languageunderstanding. In Proceedings of the DARPA Spoken and Written Language Workshop,1993 D. Appelt, J. Bear, J. Hobbs, D. Israel, and M. Tyson. 1992 SRI International FASTUS system MUC4 evaluationresults. In Proceedings of the Fourth Message Understanding Conference MUC4,pages 143147. Morgan Kaufmann, 1992.D. Appelt, J. Hobbs, J. Bear, D. Israel, M. Kanneyama, and M. Tyson. 1993 SRI Description of the JVFASTUSSystem used for MUC5. In Proceedings of the Fifth Message Understanding ConferenceMUC5. Morgan Kaufmann.ARPA. The Tipster Extraction Corpus available only to MUC participants at present. 1992.R. Basili, M. Pazienza,   P. Velardi, 1993 Acquisition of selectional patterns in sub languages. Machine Translation, 8.Communications of the ACM Special Issue on Text Filtering, 3512, 1992.L. M. Carlson et al. The Tipster Extraction Corpus A resource for evaluating natural language processing systems. In preparation.K. Church, S. Young and G Bloothcroft ed. 1996 CorpusBased  Methods in Language and Speech, Dordrecht,Kluwer Academic Publishers.F. Ciravegna, P. Campia and A. Colognese. 1992 Knowledge extraction from texts by SINTESI, In Proceedings ofthe 14th International Conference on Computational Linguistics COLING92, pages12441248, Nantes, France. J. Cowie, T. Wakao, L. Guthrie, W. Jin, J. Pustejovsky and S. Waterman. 1993 The Diderot informationextraction system, In Proceedings of the First Conference of the Pacific Association forComputational Linguistics PACLING, Vancouver.J. Cowie,  W. Lehnert 1996 Information Extraction, in Y. Wilks, ed. Special NLP Issue of the Comm. ACM.H. Cunningham, R. Gaizauskas  Y. Wilks, 1995 GATE a general architecture for text extraction, University ofSheffield, Computer Science Dept. Technical memorandum.DARPA. Proceedings of the Third Message Understanding Conference MUC3, San Diego, California, 1991.Morgan Kaufmann.DARPA. Proceedings of the Fourth Message Understanding Conference MUC4, McLean, Virginia, 1992.Morgan Kaufmann.B, Dorr,  D. Jones 1996 The role of wordsense disambiguation in lexical acquisition predicting semantics fromsyntactic cues, Proc. COLING96.Gaizauskas, T. Wakao, K. Humphreys. H. Cunningham, and Y. Wilks  1995 Description of the LaSIE System asUsed for MUC6. In Proceedings of the Sixth Message Understanding ConferenceMUC6. DARPA.R. R. Granger 1977 FOULUP a program that figures out meanings of words from context. Proc. Fifth JointInternational Conference on AI.R. Grishman  et al., 1996 Tipster Text Phase II Architecture Design, Proceedings of the Tipster Text Phase IIWorkshop, Vienna, Virginia, DARPAC.F. Goldfarb 1990 The SGML Handbook, Clarendon Press, Oxford.J. Hobbs, D. Appelt, M. Tyson, J. Bear, and D. Israel. 1992 SRI International Description of the FASTUS system.In Proceedings of the Fourth Message Understanding Conference MUC4, pages 268275. Morgan Kaufmann.A. Kilgarriff  1993 Dictionary wordsense distinctions an enquiry into their nature. Computers and theHumanities,G. F. DeJong. Prediction and substantiation A new approach to natural language processing. Cognitive Science,3251273, 1979.G. F. DeJong. An overview of the FRUMP system. In W. G. Lehnert and M. H. Ringle, editors, Strategies forNatural P. S. Jacobs and L. F. Rau. SCISOR Extracting information from online news.Communications of the ACM, 33118897, 1990.W. Lehnert, C. Cardie, D. Fisher, J. McCarthy, E. Riloff, and S. Sonderland. University of MassachusettsDescription of the CIRCUS system. In Proceedings of the Fourth MessageUnderstanding Conference MUC4, pages 282288. Morgan Kaufmann, 1992.W. Lehnert and B. Sundheim. A performance evaluation of text analysis technologies. AI Magazine, 1238194,1991.Language Processing, pages 149176. Erlbaum, Hilldsale, N.J., 1982.B. Levin 1993 English verb classes and alternations, Chicago, IL.C. Mellish, A. Allport, R. Evans, L. J. Cahill, R. Gaizauskas, and J. Walker. The TIC message analyser. TechnicalReport CSRP 225, University of Sussex, 1992.R. Merchant, M. E. Okurowski and N Chichor 1996 The MultiLingual Entity Task MET Overview,Proceedings of the Tipster Text Phase II Workshop, Vienna, Virginia, DARPAW. Ogden and P. Bernick 1996 OLEADA UserCentered TIPSTER Technology for Language Instruction,Proceedings of the Tipster Text Phase II Workshop, Vienna, Virginia, DARPAW. Paik, E.D. Liddy, E. Yu and M McKenna. Interpretation of proper nouns for information retrieval. InProceedings of the DARPA Spoken and Written Language Workshop, 1993.P. Procter  et al. 1994 The Cambridge Language Survey Semantic Tagger. Technical Report, CambridgeUniversity PressP. Procter, editor. Longman Dictionary, of Contemporary English. Longman, Harlow,1978.J. Pustejovsky  and P. Anick  1988 On the semantic interpretation of nominals. Proc. COLING88.L. Rau. Extracting company names from text. In Proceedings of the Seventh Conference on Artificial IntelligenceApplications, Miami Beach, Florida, 1991.E. Riloff and W. Lehnert. Automated Dictionary Construction for Information Extraction from Text, In Proceedingsof the Ninth IEEE Conference on Artificial Intelligence for Applications, pages. 9399.IEEE Computer Society Press. 1993.E. Riloff, and J. Shoen 1995 Automatically acquiring conceptual patterns without an annotated corpus, Proc. ThirdWorkshop on Very Large Corpora.N. Sager. Natural Language Information Processing A Computer Grammar of English and its Application.AddisonWesley, Reading, Mass., 1981.R. C. Schank and R.P. Abelson. Scripts, Plans, Goals and Understanding. Lawrence Erlbaum Associates,Hillsdales, NJ, 1977.B. M. Sundheim and N. A. Chinchor. Survey of the Message Understanding Conferences. In Proceedings of theDARPA Spoken and Written Language Workshop, 1993.T. Wakao, R.  Gaizauskas and Y. Wilks  1996, Evaluation of an algorithm for the recognition and classification ofproper names. Proc. COLING96.Y. Wilks  1978 Making preferences more active, Artificial Intelligence, 11.Y. Wilks, B.  Slator, B.  L. Guthrie  1996 Electric Words dictionaries, computers and meanings. MIT Press.Y. Wilks  in press Senses and Texts, Computational Linguistics.R. Weischedel. Studies in the statistical analysis of text, Proceedings of the DARPA Spoken and Written LanguageWorkshop, pages 331. Morgan Kaufmann, 1991.R. Zajac  and M. Vanni 1996 The Temple Translators Workstation Project, Proceedings of the Tipster Text PhaseII Workshop, Vienna, Virginia, DARPA
