Deep Learning in Visual Computing and Signal Processing Home Journals About Us Applied Computational Intelligence and Soft Computing Indexed in Web of Science About this Journal Submit a Manuscript Table of Contents Journal Menu About this Journal · Abstracting and Indexing · Aims and Scope · Article Processing Charges · Articles in Press · Author Guidelines · Bibliographic Information · Citations to this Journal · Contact Information · Editorial Board · Editorial Workflow · Free eTOC Alerts · Publication Ethics · Reviewers Acknowledgment · Submit a Manuscript · Subscription Information · Table of Contents Open Special Issues · Published Special Issues · Special Issue Resources Abstract Full-Text PDF Full-Text HTML Full-Text ePUB Full-Text XML Linked References How to Cite this Article Complete Special Issue Views 197 Citations 0 ePub 1 PDF 67 Applied Computational Intelligence and Soft Computing Volume 2017 (2017), Article ID 1320780, 13 pages https://doi.org/10.1155/2017/1320780 Review Article Deep Learning in Visual Computing and Signal Processing Danfeng Xie ,  Lei Zhang , and Li Bai Department of Electrical and Computer Engineering, Temple University, Philadelphia, PA 19121, USA Correspondence should be addressed to Danfeng Xie Received 21 October 2016; Revised 15 December 2016; Accepted 15 January 2017; Published 19 February 2017 Academic Editor: Francesco Carlo Morabito Copyright © 2017 Danfeng Xie et al. This is an open access article distributed under the Creative Commons Attribution License , which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited. Abstract Deep learning is a subfield of machine learning, which aims to learn a hierarchy of features from input data. Nowadays, researchers have intensively investigated deep learning algorithms for solving challenging problems in many areas such as image classification, speech recognition, signal processing, and natural language processing. In this study, we not only review typical deep learning algorithms in computer vision and signal processing but also provide detailed information on how to apply deep learning to specific areas such as road crack detection, fault diagnosis, and human activity detection. Besides, this study also discusses the challenges of designing and training deep neural networks. 1. Introduction Deep learning methods are a group of machine learning methods that can learn features hierarchically from lower level to higher level by building a deep architecture. The deep learning methods have the ability to automatically learn features at multiple levels, which makes the system be able to learn complex mapping function directly from data, without help of the human-crafted features. This ability is crucial for high-level feature abstraction since high-level features are difficult to be described directly from raw training data. Moreover, with the sharp growth of data, the ability to learn high-level features automatically will be even more important. The most characterizing feature of deep learning methods is that their models all have deep architectures. A deep architecture means it has multiple hidden layers in the network. In contrast, a shallow architecture has only few hidden layers (1 to 2 layers). Deep architectures are loosely inspired by mammal brain. When given an input percept, mammal brain processes it using different area of cortex which abstracts different levels of features. Researchers usually describe such concepts in hierarchical ways, with many levels of abstraction. Furthermore, mammal brains also seem to process information through many stages of transformation and representation. A very clear example is that the information in the primate visual system is processed in a sequence of stages: edge detection, primitive shapes, and more complex visual shapes. Inspired by the deep architecture of mammal brain, researchers investigated deep neural networks for two decades but did not find effective training methods before 2006: researchers only obtained good experimental results of neural network with one or two hidden layers but could not get good results of neural network with more hidden layers. In 2006, Hinton et al. proposed deep belief networks (DBNs) [ 1 ], with a learning algorithm that uses unsupervised learning algorithm to greedily train deep neural network layer by layer. This training method, which is called deep learning, turns out to be very effective and efficient in training deep neural networks. Many other deep architectures, that is, autoencoder, deep convolutional neural networks, and recurrent neural networks, are successfully applied in various areas. Regression [ 2 ], classification [ 3 – 9 ], dimensionality reduction [ 10 , 11 ], modeling motion [ 12 , 13 ], modeling textures [ 14 ], information retrieval [ 15 – 17 ], natural language processing [ 18 – 20 ], robotics [ 21 ], fault diagnosis [ 22 ], and road crack detection [ 23 ] have seen increasing deep learning-related research studies. There are mainly three crucial reasons for the rapid development of deep learning applications nowadays: the big leap of deep learning algorithms, the significantly increased computational abilities, and the sharp drop of price in hardware. This survey provides an overview of several deep learning algorithms and their emerging applications in several specific areas, featuring face recognition, road crack detection, fault diagnosis, and falls detection. As complementarity to existing review papers [ 24 , 25 ], we not only review the state-of-the-art deep learning methods but also provide detailed information on how to apply deep learning to specific problems. The reminder of this paper is organized as follows. In Section 2 , the two categories of deep learning algorithms are introduced: restricted Boltzmann machines (RBMs) and convolutional neural networks (CNNs). The training strategies are discussed in Section 3 . In Section 4 , we describe several specific deep learning applications, that is, face recognition, road crack detection, fault diagnosis, and human activity detection. In Section 5 , we discuss several challenges of training and using the deep neural networks. In Section 6 , we conclude the paper. 2. Deep Learning Algorithms Deep learning algorithms have been extensively studied in recent years. As a consequence, there are a large number of related approaches. Generally speaking, these algorithms can be grouped into two categories based on their architectures: restricted Boltzmann machines (RBMs) and convolutional neural networks (CNNs). In the following sections, we will briefly review these deep learning methods and their developments. 2.1. Deep Neural Network This section introduces how to build and train RBM-based deep neural networks (DNNs). The building and training procedures of a DNN contain two steps. First, build a deep belief network (DBN) by stacking restricted Boltzmann machines (RBMs) and feed unlabeled data to pretrain the DBN. The pretrained DBN provides initial parameters for the deep neural network. In the second step, labeled data is fed to train the DNN using back-propagation. After two steps of training, a trained DNN is obtained. This section is organized as follows. Section 2.1.1 introduces RBM, which is the basic component of DBN. In Section 2.1.2 , RBM-based DNN is introduced. 2.1.1. Restricted Boltzmann Machines RBM is an energy-based probabilistic generative model [ 26 – 29 ]. It is composed of one layer of visible units and one layer of hidden units. The visible units represent the input vector of a data sample and the hidden units represent features that are abstracted from the visible units. Every visible unit is connected to every hidden unit, whereas no connection exists within the visible layer or hidden layer. Figure 1 illustrates the graphical model of restricted Boltzmann machine. Figure 1: Restricted Boltzmann machine. As a result of the lack of hidden-hidden and input-input interactions, the energy function of a RBM is where are the parameters of RBM and they need to be learned during the training procedure; denotes the weights between the visible layer and hidden layer; and are the bias of the visible layer and hidden layer, respectively; this model is called binary RBM because the vectors and only contain binary values (0 or 1). We can obtain a tractable expression for the conditional probability [ 30 ]: For binary RBM, where , the equation for a hidden unit’s output given its input is
