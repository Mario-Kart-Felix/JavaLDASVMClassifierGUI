334 IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICSPART C APPLICATIONS AND REVIEWS, VOL. 34, NO. 3, AUGUST 2004A Survey on Visual Surveillance ofObject Motion and BehaviorsWeiming Hu, Tieniu Tan, Fellow, IEEE, Liang Wang, and Steve MaybankAbstractVisual surveillance in dynamic scenes, especially forhumans and vehicles, is currently one of the most active researchtopics in computer vision. It has a wide spectrum of promisingapplications, including access control in special areas, humanidentification at a distance, crowd flux statistics and congestion analysis, detection of anomalous behaviors, and interactivesurveillance using multiple cameras, etc. In general, the processingframework of visual surveillance in dynamic scenes includes thefollowing stages modeling of environments, detection of motion,classification of moving objects, tracking, understanding anddescription of behaviors, human identification, and fusion ofdata from multiple cameras. We review recent developments andgeneral strategies of all these stages. Finally, we analyze possibleresearch directions, e.g., occlusion handling, a combination of twoand threedimensional tracking, a combination of motion analysisand biometrics, anomaly detection and behavior prediction,contentbased retrieval of surveillance videos, behavior understanding and natural language description, fusion of informationfrom multiple sensors, and remote surveillance.Index TermsBehavior understanding and description, fusionof data from multiple cameras, motion detection, personal identification, tracking, visual surveillance.I. INTRODUCTIONAS AN ACTIVE research topic in computer vision, visualsurveillance in dynamic scenes attempts to detect, recognize and track certain objects from image sequences, and moregenerally to understand and describe object behaviors. The aimis to develop intelligent visual surveillance to replace the traditional passive video surveillance that is proving ineffective as thenumber of cameras exceeds the capability of human operators tomonitor them. In short, the goal of visual surveillance is not onlyto put cameras in the place of human eyes, but also to accomplishthe entire surveillance task as automatically as possible.Visual surveillance in dynamic scenes has a wide range ofpotential applications, such as a security guard for communities and important buildings, traffic surveillance in cities andManuscript received April 14, 2003 revised September 26, 2003 and January8, 2004. This work was supported in part by the National Science Foundationof China NSFC under Grants 60105002, 60335010, 60121302, and 60373046,by the Natural Science Foundation of Beijing under Grant 4031004 and Grant4041004, by the National 863 HighTech RD Program of China under Grant2002AA11701011 and Grant 2002AA142100, and by the International Cooperation Project of Beijing, the LIAMA Project. This paper was recommendedby Associate Editor D. Zhang.W. Hu, T. Tan, and L. Wang are with the National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences,Beijing 100080, China email wmhunlpr.ia.ac.cn tntnlpr.ia.ac.cnlwangnlpr.ia.ac.cn.S. Maybank is with the School of Computer Science and InformationSystems, Birkbeck College, London WC1E 7HX, U.K. email sjmaybankdcs.bbk.ac.uk.Digital Object Identifier 10.1109TSMCC.2004.829274expressways, detection of military targets, etc. We focus in thispaper on applications involving the surveillance of people or vehicles, as they are typical of surveillance applications in general,and include the full range of surveillance methods. Surveillanceapplications involving people or vehicles include the following.1 Access control in special areas. In some securitysensitive locations such as military bases and important governmental units, only people with a special identity areallowed to enter. A biometric feature database includinglegal visitors is built beforehand using biometric techniques. When somebody is about to enter, the systemcould automatically obtain the visitors features, such asheight, facial appearance and walking gait from imagestaken in real time, and then decide whether the visitor canbe cleared for entry.2 Personspecific identification in certain scenes. Personal identification at a distance by a smart surveillancesystem can help the police to catch suspects. The policemay build a biometric feature database of suspects, andplace visual surveillance systems at locations where thesuspects usually appear, e.g., subway stations, casinos,etc. The systems automatically recognize and judgewhether or not the people in view are suspects. If yes,alarms are given immediately. Such systems with facerecognition have already been used at public sites, butthe reliability is too low for police requirements.3 Crowd flux statistics and congestion analysis. Usingtechniques for human detection, visual surveillance systems can automatically compute the flux of people at important public areas such as stores and travel sites, andthen provide congestion analysis to assist in the management of the people. In the same way, visual surveillancesystems can monitor expressways and junctions of theroad network, and further analyze the traffic flow and thestatus of road congestion, which are of great importancefor traffic management.4 Anomaly detection and alarming. In some circumstances, it is necessary to analyze the behaviors of peopleand vehicles and determine whether these behaviors arenormal or abnormal. For example, visual surveillancesystems set in parking lots and supermarkets could analyze abnormal behaviors indicative of theft. Normally,there are two ways of giving an alarm. One way is toautomatically make a recorded public announcementwhenever any abnormal behavior is detected. The otheris to contact the police automatically.5 Interactive surveillance using multiple cameras. Forsocial security, cooperative surveillance using multiple109469770420.00  2004 IEEEHU et al. A SURVEY ON VISUAL SURVEILLANCE OF OBJECT MOTION AND BEHAVIORS 335cameras could be used to ensure the security of an entirecommunity, for example by tracking suspects over a widearea by using the cooperation of multiple cameras. Fortraffic management, interactive surveillance using multiple cameras can help the traffic police discover, track,and catch vehicles involved in traffic offences.It is the broad range of applications that motivates the interests of researchers worldwide. For example, the IEEE has sponsored the IEEE International Workshop on Visual Surveillanceon three occasions, in India 1998, the U.S. 1999, and Ireland 2000. In 68 and 1, a special section on visual surveillance was published in June and August of 2000, respectively.In 78, a special issue on visual analysis of human motion waspublished in March 2001. In 69, a special issue on thirdgeneration surveillance systems was published in October 2001. In130, a special issue on understanding visual behavior was published in October 2002. Recent developments in human motionanalysis are briefly introduced in our previous paper 75. It isnoticeable that, after the 911 event, visual surveillance has received more attention not only from the academic community,but also from industry and governments.Visual surveillance has been investigated worldwide underseveral large research projects. For example, the DefenseAdvanced Research Projection Agency DARPA supportedthe Visual Surveillance and Monitoring VSAM project3 in 1997, whose purpose was to develop automatic videounderstanding technologies that enable a single human operatorto monitor behaviors over complex areas such as battlefieldsand civilian scenes. Furthermore, to enhance protection fromterrorist attacks, the Human Identification at a Distance HIDprogram sponsored by DARPA in 2000 aims to develop a fullrange of multimodal surveillance technologies for successfully detecting, classifying, and identifying humans at greatdistances. The European Unions Framework V Programmesponsored Advisor, a core project on visual surveillance inmetrostations.There have been a number of famous visual surveillance systems. The realtime visual surveillance system W4 4 employsa combination of shape analysis and tracking, and constructsmodels of peoples appearances in order to detect and trackgroups of people as well as monitor their behaviors even in thepresence of occlusion and in outdoor environments. This systemuses the single camera and grayscale sensor. The VIEWS system87 at the University of Reading is a threedimensional 3Dmodel based vehicle tracking system. The Pfinder system developed by Wren et al. 8 is used to recover a 3D description of aperson in a large room. It tracks a single nonoccluded personin complex scenes, and has been used in many applications.As a singleperson tracking system, TI, developed by Olsen etal. 9, detects moving objects in indoor scenes using motiondetection, tracks them using firstorder prediction, and recognizes behaviors by applying predicates to a graph formed bylinking corresponding objects in successive frames. This systemcannot handle small motions of background objects. The systemat CMU 10 can monitor activities over a large area using multiple cameras that are connected into a network. It can detectand track multiple persons and vehicles within cluttered scenesand monitor their activities over long periods of time. The abovecomments on 810 are derived from 4. Please see 4 formore details.As far as hardware is concerned, companies like Sony andIntel have designed equipment suitable for visual surveillance,e.g., active cameras, smart cameras 76, omnidirectional cameras 23, 77, etc.All of the above activities are evidence of a great and growinginterest in visual surveillance in dynamic scenes. The primarypurpose of this paper is to give a general review on the overallprocess of a visual surveillance system. Fig. 1 shows the general framework of visual surveillance in dynamic scenes. Theprerequisites for effective automatic surveillance using a singlecamera include the following stages modeling of environments,detection of motion, classification of moving objects, tracking,understanding and description of behaviors, and human identification. In order to extend the surveillance area and overcomeocclusion, fusion of data from multiple cameras is needed. Thisfusion can involve all the above stages. In this paper we reviewrecent developments and analyze future open directions in visual surveillance in dynamic scenes. The main contributions ofthis paper are as follows. Lowlevel vision, intermediatelevel vision, andhighlevel vision are discussed in a clearly organizedhierarchical manner according to the general frameworkof visual surveillance. This, we believe, can help readers,especially newcomers to this area, not only to obtain anunderstanding of the stateoftheart in visual surveillance, but also to appreciate the major components ofa visual surveillance system and their intercomponentlinks. Instead of detailed summaries of individual publications,our emphasis is on discussing various methods for different tasks involved in a general visual surveillancesystem. Each issue is accordingly divided into subprocesses or categories of various methods to examine thestate of the art. Only the principles of each group ofmethods are described. The merits and demerits of avariety of different algorithms, especially for motiondetection and tracking, are summarized. We give a detailed review of the state of the art in personalidentification at a distance and fusion of data from multiple cameras. We provide detailed discussions on future researchdirections in visual surveillance, e.g., occlusion handling, combination of twodimensional 2D trackingand 3D tracking, combination of motion analysis andbiometrics, anomaly detection and behavior prediction,behavior understanding and nature language description,contentbased retrieval of surveillance videos, fusion ofinformation from multiple sensors, and remote surveillance.The remainder of this paper is organized as follows. Section IIreviews the work related to motion detection including modeling of environments, segmentation of motion, classificationof moving objects. Section III discusses tracking of objects, andSection IV details understanding and description of behaviors.Sections V and VI cover, respectively, personal identification at336 IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICSPART C APPLICATIONS AND REVIEWS, VOL. 34, NO. 3, AUGUST 2004Fig. 1. General framework of visual surveillance.a distance and fusion of data from multiple cameras. Section VIIanalyzes some possible directions for future research. The lastsection summarizes the paper.II. MOTION DETECTIONNearly every visual surveillance system starts with motiondetection. Motion detection aims at segmenting regions corresponding to moving objects from the rest of an image. Subsequent processes such as tracking and behavior recognition aregreatly dependent on it. The process of motion detection usuallyinvolves environment modeling, motion segmentation, and object classification, which intersect each other during processing.A. Environment ModelingThe active construction and updating of environmentalmodels are indispensable to visual surveillance. Environmentalmodels can be classified into 2D models in the image plane and3D models in real world coordinates. Due to their simplicity,2D models have more applications. For fixed cameras, the key problem is to automaticallyrecover and update background images from a dynamicsequence. Unfavorable factors, such as illumination variance, shadows and shaking branches, bring many difficulties to the acquirement and updating of background images. There are many algorithms for resolving these problems including temporal average of an image sequence15, 82, adaptive Gaussian estimation 70, and parameter estimation based on pixel processes 79, 80, etc.Ridder et al. 81 model each pixel value with a KalmanFilter to compensate for illumination variance. Stauffer etal. 12, 80 present a theoretic framework for recoveringand updating background images based on a process inwhich a mixed Gaussian model is used for each pixel valueand online estimation is used to update background imagesin order to adapt to illumination variance and disturbancein backgrounds. Toyama et al. 83 propose the Wallfloweralgorithm in which background maintenance and background subtraction are carried out at three levels the pixellevel, the region level, and the frame level. Haritaoglu etal. 4 build a statistical model by representing each pixelwith three values its minimum and maximum intensityvalues, and the maximum intensity difference betweenconsecutive frames observed during the training period.These three values are updated periodically. McKenna etal. 11 use an adaptive background model with color andgradient information to reduce the influences of shadowsand unreliable color cues. For pure translation PT cameras, an environment modelcan be made by patching up a panorama graph to acquirea holistic background image 84. Homography matricescan be used to describe the transformation relationshipbetween different images. For mobile cameras, motion compensation is needed toconstruct temporary background images 85.Regarding 3D environmental models 86, current work is stilllimited to indoor scenes because of the difficulty of 3D reconstructions of outdoor scenes.HU et al. A SURVEY ON VISUAL SURVEILLANCE OF OBJECT MOTION AND BEHAVIORS 337B. Motion SegmentationMotion segmentation in image sequences aims at detectingregions corresponding to moving objects such as vehicles andhumans. Detecting moving regions provides a focus of attention for later processes such as tracking and behavior analysisbecause only these regions need be considered in the later processes. At present, most segmentation methods use either temporal or spatial information in the image sequence. Several conventional approaches for motion segmentation are outlined inthe following.1 Background subtraction. Background subtraction isa popular method for motion segmentation, especiallyunder those situations with a relatively static background.It detects moving regions in an image by taking the difference between the current image and the referencebackground image in a pixelbypixel fashion. It issimple, but extremely sensitive to changes in dynamicscenes derived from lighting and extraneous events etc.Therefore, it is highly dependent on a good backgroundmodel to reduce the influence of these changes 4, 11,12, as part of environment modeling.2 Temporal differencing. Temporal differencing makesuse of the pixelwise differences between two or threeconsecutive frames in an image sequence to extractmoving regions. Temporal differencing is very adaptiveto dynamic environments, but generally does a poorjob of extracting all the relevant pixels, e.g., there maybe holes left inside moving entities. As an example ofthis method, Lipton et al. 10 detect moving targets inreal video streams using temporal differencing. Afterthe absolute difference between the current and theprevious frame is obtained, a threshold function is usedto determine changes. By using a connected componentanalysis, the extracted moving sections are clustered intomotion regions. An improved version uses threeframeinstead of twoframe differencing.3 Optical flow. Opticalflowbased motion segmentationuses characteristics of flow vectors of moving objects overtime to detect moving regions in an image sequence. Forexample, Meyer et al. 13, 21 compute the displacementvector field to initializeacontourbased trackingalgorithm,called active rays, for the extraction of articulated objects.The results are used for gait analysis. Opticalflowbasedmethods can be used to detect independently movingobjects even in the presence of camera motion. However,most flow computation methods are computationallycomplex and very sensitive to noise, and cannot be appliedto video streams in real time without specialized hardware.More detailed discussion of optical flow can be found inBarrons work 14.Of course, besides the basic methods described above, thereare some other approaches for motion segmentation. Using theextended expectation maximization EM algorithm, Friedmanet al. 15 implement a mixed Gaussian classification model foreach pixel. This model classifies the pixel values into three separate predetermined distributions corresponding to background,foreground and shadow. It also updates the mixed componentautomatically for each class according to the likelihood of membership. Hence, slowly moving objects are handled perfectly,while shadows are eliminated much more effectively. VSAM 3has successfully developed a hybrid algorithm for motion segmentation by combining an adaptive background subtraction algorithm with a threeframe differencing technique. This hybridalgorithm is very fast and surprisingly effective for detectingmoving objects in image sequences. In addition, Stringa 16proposes a novel morphological algorithm for detecting motionin scenes. This algorithm obtains stable segmentation resultseven under varying environmental conditions.C. Object ClassificationDifferent moving regions may correspond to different movingtargets in natural scenes. For instance, the image sequences captured by surveillance cameras mounted in road traffic scenesprobably include humans, vehicles and other moving objectssuch as flying birds and moving clouds, etc. To further trackobjects and analyze their behaviors, it is essential to correctlyclassify moving objects. Object classification can be consideredas a standard pattern recognition issue. At present, there are twomain categories of approaches for classifying moving objects.1 Shapebased classification. Different descriptions ofshape information of motion regions such as points,boxes, silhouettes and blobs are available for classifying moving objects. VASM 3 takes image blobdispersedness, image blob area, apparent aspect ratioof the blob bounding box, etc, as key features, andclassifies movingobject blobs into four classes singlehuman, vehicles, human groups, and clutter, using aviewpointspecific threelayer neural network classifier.Lipton et al. 10 use the dispersedness and area of imageblobs as classification metrics to classify all movingobject blobs into humans, vehicles and clutter. Temporalconsistency constraints are considered so as to makeclassification results more precise. Kuno et al. 17 usesimple shape parameters of human silhouette patterns toseparate humans from other moving objects.2 Motionbased classification. In general, nonrigid articulated human motion shows a periodic property, so this hasbeen used as a strong cue for classification of moving objects. Cutler et al. 18 describe a similaritybased technique to detect and analyze periodic motion. By trackingan interesting moving object, its selfsimilarity is computed as it evolves over time. As we know, for periodicmotion, its selfsimilarity measure is also periodic. Therefore timefrequency analysis is applied to detect and characterize the periodic motion, and tracking and classification of moving objects are implemented using periodicity.In Liptons work 19, residual flow is used to analyzerigidity and periodicity of moving objects. It is expectedthat rigidobjectspresent little residual flow,whereasanonrigid moving object such as a human being has a higher average residual flowandevendisplayaperiodiccomponent.Based on this useful cue, human motion is distinguishedfrom motion of other objects, such as vehicles.338 IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICSPART C APPLICATIONS AND REVIEWS, VOL. 34, NO. 3, AUGUST 2004The two common approaches mentioned above, namelyshapebased and motionbased classification, can also beeffectively combined for classification of moving objects. Furthermore, Stauffer 20 proposes a novel method based on a timecooccurrence matrix to hierarchically classify both objects andbehaviors. It is expected that more precise classification resultscan be obtained by using extra features such as color and velocity.III. OBJECT TRACKINGAfter motion detection, surveillance systems generally trackmoving objects from one frame to another in an image sequence.The tracking algorithms usually have considerable intersectionwith motion detection during processing. Tracking over timetypically involves matching objects in consecutive frames usingfeatures such as points, lines or blobs. Useful mathematical toolsfor tracking include the Kalman filter, the Condensation algorithm, the dynamic Bayesian network, the geodesic method,etc. Tracking methods are divided into four major categoriesregionbased tracking, activecontourbased tracking, featurebased tracking, and modelbased tracking. It should be pointedout that this classification is not absolute in that algorithms fromdifferent categories can be integrated together 169.A. RegionBased TrackingRegionbased tracking algorithms track objects according tovariations of the image regions corresponding to the moving objects. For these algorithms, the background image is maintaineddynamically 90, 91, and motion regions are usually detectedby subtracting the background from the current image. Wren etal. 8 explore the use of small blob features to track a singlehuman in an indoor environment. In their work, a human bodyis considered as a combination of some blobs respectively representing various body parts such as head, torso and the fourlimbs. Meanwhile, both human body and background scene aremodeled with Gaussian distributions of pixel values. Finally,the pixels belonging to the human body are assigned to thedifferent body parts blobs using the loglikelihood measure.Therefore, by tracking each small blob, the moving human issuccessfully tracked. Recently, McKenna et al. 11 proposean adaptive background subtraction method in which color andgradient information are combined to cope with shadows andunreliable color cues in motion segmentation. Tracking is thenperformed at three levels of abstraction regions, people, andgroups. Each region has a bounding box and regions can mergeand split. A human is composed of one or more regions groupedtogether under the condition of geometric structure constraintson the human body, and a human group consists of one or morepeople grouped together. Therefore, using the region tracker andthe individual color appearance model, perfect tracking of multiple people is achieved, even during occlusion. As far as regionbased vehicle tracking is concerned, there are some typical systems such as the CMS mobilizer system supported by theFederal Highway Administration FHWA, at the Jet PropulsionLaboratory JPL 92, and the PATH system developed by theBerkeley group 93.Although they work well in scenes containing only a fewobjects such as highways, regionbased tracking algorithmscannot reliably handle occlusion between objects. Furthermore,as these algorithms only obtain the tracking results at the regionlevel and are essentially procedures for motion detection, theoutline or 3D pose of objects cannot be acquired. The 3D poseof an object consists of the position and orientation of the object. Accordingly, these algorithms cannot satisfy the requirements for surveillance against a cluttered background or withmultiple moving objects.B. Active ContourBased TrackingActive contourbased tracking algorithms track objects byrepresenting their outlines as bounding contours and updatingthese contours dynamically in successive frames 6, 71, 72,74. These algorithms aim at directly extracting shapes of subjects and provide more effective descriptions of objects than regionbased algorithms. Paragios et al. 30 detect and track multiple moving objects in image sequences using a geodesic activecontour objective function and a level set formulation scheme.Peterfreund 31 explores a new active contour model based ona Kalman filter for tracking nonrigid moving targets such aspeople in spatiovelocity space. Isard et al. 32 adopt stochasticdifferential equations to describe complex motion models, andcombine this approach with deformable templates to cope withpeople tracking. Malik et al. 82, 94 have successfully appliedactive contourbased methods to vehicle tracking.In contrast to regionbased tracking algorithms, active contourbased algorithms describe objects more simply and moreeffectively and reduce computational complexity. Even underdisturbance or partial occlusion, these algorithms may track objects continuously. However, the tracking precision is limitedat the contour level. The recovery of the 3D pose of an objectfrom its contour on the image plane is a demanding problem. Afurther difficulty is that the active contourbased algorithms arehighly sensitive to the initialization of tracking, making it difficult to start tracking automatically.C. FeatureBased TrackingFeaturebased tracking algorithms perform recognition andtracking of objects by extracting elements, clustering them intohigher level features and then matching the features between images. Featurebased tracking algorithms can further be classifiedinto three subcategories according to the nature of selected features global featurebased algorithms, local featurebased algorithms, and dependencegraphbased algorithms. The features used in global featurebased algorithms include centroids, perimeters, areas, some orders of quadratures and colors 100, 101, etc. Polana et al. 33 provide a good example of global featurebased tracking. Aperson is bounded with a rectangular box whose centroidis selected as the feature for tracking. Even when occlusion happens between two persons during tracking, as longas the velocity of the centroids can be distinguished effectively, tracking is still successful. The features used in local featurebased algorithms include line segments, curve segments, and corner vertices98, 99, etc.HU et al. A SURVEY ON VISUAL SURVEILLANCE OF OBJECT MOTION AND BEHAVIORS 339 The features used in dependencegraphbased algorithmsinclude a variety of distances and geometric relations between features 97.Theabovethreemethodscanbecombined.IntherecentworkofJang et al. 34, an active template that characterizes regional andstructuralfeaturesofanobjectisbuiltdynamicallybasedontheinformation of shape, texture, color, and edge featuresof the region.UsingmotionestimationbasedonaKalmanfilter,thetrackingofanonrigid moving object is successfully performed by minimizinga feature energy function during the matching process.In general, as they operate on 2D image planes, featurebasedtracking algorithms can adapt successfully and rapidly to allowrealtime processing and tracking of multiple objects which arerequired in heavy thruway scenes, etc. However, dependencegraphbased algorithms cannot be used in realtime trackingbecause they need timeconsuming searching and matching ofgraphs. Featurebased tracking algorithms can handle partial occlusion by using information on object motion, local featuresand dependence graphs. However, there are several serious deficiencies in featurebased tracking algorithms. The recognition rate of objects based on 2D image features is low, because of the nonlinear distortion duringperspective projection and the image variations with theviewpoints movement. These algorithms are generally unable to recover 3D poseof objects. The stability of dealing effectively with occlusion, overlapping and interference of unrelated structures is generally poor.D. ModelBased TrackingModelbased tracking algorithms track objects by matchingprojected object models, produced with prior knowledge, toimage data. The models are usually constructed offline withmanual measurement, CAD tools or computer vision techniques.As modelbased rigid object tracking and modelbased nonrigid object tracking are quite different, we review separatelymodelbased human body tracking nonrigid object trackingand modelbased vehicle tracking rigid object tracking.1 ModelBased Human Body Tracking The generalapproach for modelbased human body tracing is known asanalysisbysynthesis, and it is used in a predictmatchupdatestyle. Firstly, the pose of the model for the next frame ispredicted according to prior knowledge and tracking history.Then, the predicted model is synthesized and projected into theimage plane for comparison with the image data. A specificpose evaluation function is needed to measure the similaritybetween the projected model and the image data. Accordingto different search strategies, this is done either recursively orusing sampling techniques until the correct pose is finally foundand is used to update the model. Pose estimation in the firstframe needs to be handled specially. Generally, modelbasedhuman body tracking involves three main issues construction of human body models representation of prior knowledge of motion models andmotion constraints prediction and search strategies.Previous work on these three issues is briefly and respectivelyreviewed as follows.a Human body models Construction of human bodymodels is the base of modelbased human body tracking24. Generally, the more complex a human body model, themore accurate the tracking results, but the more expensive thecomputation. Traditionally, the geometric structure of humanbody can be represented in the following four styles. Stick figure. The essence of human motion is typicallycontained in the movements of the torso, the head and thefour limbs, so the stickfigure method is to represent theparts of a human body as sticks and link the sticks withjoints. Karaulova et al. 25 use a stick figure representation to build a novel hierarchical model of human dynamics encoded using hidden Markov models HMMs,and realize viewindependent tracking of a human bodyin monocular image sequences. 2D contour. This kind of human body model is directlyrelevant to human body projections in an image plane.The human body segments are modeled by 2D ribbonsor blobs. For instance, Ju et al. 26 propose a cardboardhuman body model, in which the human limbs are represented by a set of jointed planar ribbons. The parameterized image motion of these patches is constrained to enforce the articulated movement of human limbs. Niyogiet al. 27 use the spatialtemporal pattern in XYT spaceto track, analyze and recognize walking figures. They examine the characteristic braided pattern produced by thelower limbs of a walking human, the projections of headmovements are then located in the spatiotemporal domain, followed by the identification of the joint trajectories The contour of a walking figure is outlined by utilizing these joint trajectories, and a more accurate gaitanalysis is carried out using the outlined 2D contour forthe recognition of the specific human. Volumetric models. The main disadvantage of 2Dmodels is that they require restrictions on the viewingangle. To overcome this disadvantage, many researchersuse 3D volumetric models such as elliptical cylinders, cones 102, 103, spheres, superquadrics 104,etc. Volumetric models require more parameters thanimagebased models and lead to more expensive computation during the matching process. Rohr 28 makes useof fourteen elliptical cylinders to model a human body in3D volumes. Wachter et al. 29 establish a 3D bodymodel using connected elliptical cones. Hierarchical model. Plankers et al. 105 present a hierarchical human model for achieving more accurate results. It includes four levels skeleton, ellipsoid meatballssimulating tissues and fats, polygonal surface representingskin, and shaded rendering.b Motion models Motion models of human limbs andjoints are widely used in tracking. They are effective because themovements of the limbs are strongly constrained. These motionmodels serve as prior knowledge to predict motion parameters106, 107, to interpret and recognize human behaviors 108,or to constrain the estimation of lowlevel image measurements340 IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICSPART C APPLICATIONS AND REVIEWS, VOL. 34, NO. 3, AUGUST 2004109. For instance, Bregler 108 decomposes a human behavior into multiple abstractions, and represents the highlevelabstraction by HMMs built from phases of simple movements.This representation is used for both tracking and recognition.Zhao et al. 106 learn a highly structured motion model forballet dancing under the minimum description length MDLparadigm. This motion model is similar to a finitestate machineFSM. The multivariate principal component analysis MPCAis used to train a walking model in Sidenbladh et al.s work109. Similarly, Ong et al. 110 employ the hierarchical PCAto learn their motion model which is based on the matrices oftransition probabilities between different subspaces in a globaleigensapce and the matrix of transition probabilities betweenglobal eigenspaces. Ning et al. 7 learn a motion model fromsemiautomatically acquired training examples and represent itusing Gaussian distributions.c Search strategies Pose estimation in a highdimensional body configuration space is intrinsically difficult, so,search strategies are often carefully designed to reduce thesolution space. Generally, there are four main classes of searchstrategies dynamics, Taylor models, Kalman filtering, andstochastic sampling. Dynamical strategies use physical forcesapplied to each rigid part of the 3D model of the tracked object.These forces, as heuristic information, guide the minimizationof the difference between the pose of the 3D model andthe pose of the real object 102. The strategy based on theTaylor models incrementally improves an existing estimation,using differentials of motion parameters with respect to theobservation to predict better search directions 112. It at leastfinds local minima, but cannot guarantee finding the globalminimum. As a recursive linear estimator, Kalman filteringcan thoroughly deal with the tracking of shape and positionover time in the relatively clutterfree case in which the densityof the motion parameters can be modeled satisfactorily asGaussian 29, 114. To handle clutter that causes the probability density function for motion parameters to be multimodaland nonGaussian, stochastic sampling strategies, such asMarkov Chain Monte Carlo 115, Genetic Algorithms, andCONDENSATION 116, 117, are designed to representsimultaneous alternative hypotheses. Among the stochasticsampling strategies in visual tracking, CONDENSATION isperhaps the most popular.2 ModelBased Vehicle Tracking As to modelbased vehicle tracking, 3D wireframe vehicle models are mainly used95. The research groups at the University of Reading 87,88, the National Laboratory of Pattern Recognition NLPR111, 174 and the University of Karlsruhe 124126 havemade important contributes to 3D modelbased vehicle localization and tracking.The research group at the University of Reading adopts3D wireframe vehicle models. Tan et al. 87, 119 proposethe groundplane constraint GPC, under which vehicles arerestricted to move on the ground plane. Thus the degrees offreedom of vehicle pose are reduced to three from six. Thisgreatly decreases the computational cost of searching for theoptimal pose. Moreover, under the weak perspective assumption, the pose parameters are decomposed into two independentsets translation parameters and rotation parameters. Tan et al.120 propose a generalized Hough transformation algorithmbased on a single characteristic line segment matching toestimate vehicle pose. Further, Tan et al. 121 analyze theonedimensional 1D correlation of image gradients and determine the vehicle pose by voting. As to the refinement of thevehicle pose, the research group in the University of Readinghave utilized an independent 1D searching method 121 intheir past work. Recently, Pece et al. 122, 123 introduce astatistical Newton method for estimating the vehicle pose.The NLPR group has extended the work of the research groupat the University of Reading. Yang et al. 111 propose a new3D modelbased vehicle localization algorithm, in which theedge points in the image are directly used as features, and thedegree of matching between the edge points and the projectedmodel is measured by a pose evaluation function. Lou et al.174 present an algorithm for vehicle tracking based on animproved extended Kalman filter. In the algorithm, the turn ofthe steering wheel and the distance between the front and rearwheels are taken into account. As there is a direct link betweenthe behavior of the driver who controls the motion of the vehicle and the assumed dynamic model, the improved extendedKalman filter outperforms the traditional extended Kalman filterwhen the vehicle carries out a complicated maneuver.The Karlsruhe group 124 uses the 3D wireframe vehiclemodel. The image features used in the algorithm are edges. Theinitial values for the vehicle pose parameters are obtained fromthe correspondence between the segments in an image and thosein the projection model. The correspondence is calculated usingviewpoint consistent constraints and some clustering rules. Themaximum a posterior MAP estimate of the vehicle positionis obtained using the LevenbergMarquardt optimization technique. The algorithm is datadriven and dependent on the accuracy of edge detection. Kollnig et al. 125 also propose analgorithm based on image gradients, in which virtual gradientsin an image are produced by spreading the Gaussian distribution around line segments. Under the assumption that the realgradient at each point in an image is the sum of a virtual gradient and a Gaussian white noise, the pose parameters can beestimated using the extended Kalman filter EKF. Furthermore,Haag et al. 126 integrate Kollnig et al.s algorithm based onimage gradients with that based on optic flow. The method usesimage gradients evaluated in the neighborhoods of the imagefeatures. However, the optic flow uses global information onimage features, integrated across the whole region of interestROI. So the gradients and the optic flow are complementarysources of information.The above reviews modelbased human body tracking andmodelbased vehicle tracking. Compared with other tracking algorithms, modelbased tracking algorithms have the followingmain advantages. By making use of the prior knowledge of the 3D contoursor surfaces of objects, the algorithms are intrinsically robust. The algorithms can obtain better results even underocclusion including selfocclusion for humans or interference between nearby image motions. As far as modelbased human body tracking is concerned,the structure of human body, the constraint of human motion, and other prior knowledge can be fused.HU et al. A SURVEY ON VISUAL SURVEILLANCE OF OBJECT MOTION AND BEHAVIORS 341 As far as 3D modelbased tracking is concerned, after setting up the geometric correspondence between 2D imagecoordinates and 3D world coordinates by camera calibration, the algorithms naturally acquire the 3D pose of objects. The 3D modelbased tracking algorithms can be appliedeven when objects greatly change their orientations duringthe motion.Ineluctably, modelbased tracking algorithms have some disadvantages such as the necessity of constructing the models,high computational cost, etc.IV. UNDERSTANDING AND DESCRIPTION OF BEHAVIORSAfter successfully tracking the moving objects from oneframe to another in an image sequence, the problem of understandingobject behaviors from image sequences followsnaturally. Behavior understanding involves the analysis andrecognition of motion patterns, and the production of highleveldescription of actions and interactions.A. Behavior UnderstandingUnderstanding of behaviors may simply be thought as theclassification of time varying feature data, i.e., matching anunknown test sequence with a group of labeled referencesequences representing typical behaviors. It is then obviousthat a fundamental problem of behavior understanding is tolearn the reference behavior sequences from training samples,and to devise both training and matching methods for copingeffectively with small variations of the feature data within eachclass of motion patterns. Some efforts have been made in thisdirection 176 and the major existing methods for behaviorunderstanding are outlined in the following.a Dynamic time warping DTW DTW is a templatebased dynamic programming matching technique widelyused in the algorithms for speech recognition. It has the advantage of conceptual simplicity and robust performance, and hasbeen used recently in the matching of human movement patterns 127, 128. For instance, Bobick et al. 128 use DTWto match a test sequence to a deterministic sequence of statesto recognize human gestures. Even if the time scale between atest sequence and a reference sequence is inconsistent, DTWcan still successfully establish matching as long as the timeordering constraints hold.b Finitestate machine FSM The most important feature of a FSM is its statetransition function. The states are usedto decide which reference sequence matches with the test sequence. Wilson et al. 129 analyze the explicit structure of natural gestures where the structure is implemented by an equivalent of a FSM but with no learning involved. Statemachinerepresentations of behaviors have also been employed in higherlevel description. For instance, Bremond et al. 131 use handcrafted deterministic automata to recognize airborne surveillance scenarios describing vehicle behaviors in aerial imagery.c HMMs A HMM is a kind of stochastic state machines35. It allows a more sophisticated analysis of data withspatiotemporal variability. The use of HMMs consists oftwo stages training and classification. In the training stage,the number of states of a HMM must be specified, and thecorresponding state transition and output probabilities areoptimized in order that the generated symbols can correspondto the observed image features of the examples within a specificmovement class. In the matching stage, the probability withwhich a particular HMM generates the test symbol sequencecorresponding to the observed image features is computed.HMMs generally outperform DTW for undivided time seriesdata, and are therefore extensively applied to behavior understanding. For instance, Starner et al. 132 propose HMMs forthe recognition of sign language. Oliver et al. 133 proposeand compare two different statebased learning architectures,namely, HMMs and coupled hidden Markov models CHMMsfor modeling people behaviors and interactions such as following and meeting. The CHMMs are shown to work muchmore efficiently and accurately than HMMs. Brand et al. 134show that, by the use of the entropy of the joint distributionto learn the HMM, a HMMs internal state machine can bemade to organize observed behaviors into meaningful states.This technique has found applications in video monitoring andannotation, in low bitrate coding of scene behaviors, and inanomaly detection.d Timedelay neural network TDNN TDNN is also aninteresting approach to analyzing timevarying data. In TDNN,delay units are added to a general static network, and some ofthe preceding values in a timevarying sequence are used to predict the next value. As larger data sets become available, moreemphasis is being placed on neural networks for representingtemporal information. TDNN has been successfully applied tohand gesture recognition 135 and lipreading 136.e Syntactic techniques 137 The syntactic approach inmachine vision has been studied mostly in the context of pattern recognition in static images. Recently the grammatical approach has been used for visual behavior recognition. Brand138 uses a simple nonprobabilistic grammar to recognize sequences of discrete behaviors. Ivanov et al. 137 describe aprobabilistic syntactic approach to the detection and recognition of temporally extended behaviors and interactions betweenmultiple agents. The fundamental idea is to divide the recognition problem into two levels. The lower level is performed usingstandard independent probabilistic temporal behavior detectors,such as HMMs, to output possible lowlevel temporal features.These outputs provide the input stream for a stochastic contextfree parser. The grammar and parser provide longer rangetemporal constraints, disambiguate uncertain lowlevel detection, and allow the inclusion of a priori knowledge about thestructure of temporal behaviors in a given domain.f Nondeterministic finite automaton NFA Wada et al.139 employ NFA as a sequence analyzer, because it is a simpleexample satisfying the following properties instantaneousnessand purenondeterminism. They present an approach for multiobject behavior recognition based on behavior driven selectiveattention.g Selforganizing neural network The methods discussed in af all involve supervised learning. They areapplicable for known scenes where the types of object motionsare already known. The selforganizing neural networks aresuited to behavior understanding when the object motions342 IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICSPART C APPLICATIONS AND REVIEWS, VOL. 34, NO. 3, AUGUST 2004are unrestricted. Johnson et al. 140 describe the movementof an object in terms of a sequence of flow vectors, each ofwhich consists of 4 components representing the positions andvelocities of the object in the image plane. A statistical modelof object trajectories is formed with two competitive learningnetworks that are connected with leaky neurons. Sumpter et al.141 introduce feedback to the second competitive networkin 140 giving a more efficient prediction of object behaviors.Hu et al. 175 improve the work in 140 by introducing anew neural network structure that has smaller scale and fasterlearning speed, and is thus more effective. Owens et al. 142apply the Kohonen selforganizing feature map to find the flowvector distribution patterns. These patterns are used to determine whether a point on a trajectory is normal or abnormal.B. Natural Language Description of BehaviorsIn many applications it is important to describe object behaviors in natural language suitable for nonspecialist operatorof visual surveillance 22, 147. For example, Herzog et al.143 have developed the VITRA project that uses natural language to describe visual scenes. In 1995, MIT 147 convened aworkshop to discuss how to link natural language and computervision. Generally, there are two main categories of behavior description methods statistical models and formalized reasoning.a Statistical models A representative statistical modelis the Bayesian network model 144, 145. This modelinterprets certain events and behaviors by analysis of timesequences and statistical modeling. For example, Remagninoet al. 148 describe interactions between objects using atwolayer agentbased Bayesian network. These methods reston lowerlevel recognition based on motion concepts, anddo not yet involve highlevel concepts, such as events andscenarios, and the relationships between these concepts. Theseconcepts need highlevel reasoning based on a large amount ofprior knowledge.b Formalized reasoning Formalized reasoning 146uses symbol systems to represent behavior patterns, andreasoning methods such as predication logic to recognize andclassify events. Recently, Kojima et al. 36, 37 propose anew method for generating natural language descriptions ofhuman behaviors appearing in real image sequences. First, ahead region of a human is extracted from each image frame,and the 3D pose and position of the head are estimated usinga modelbased approach. Next, the head motion trajectoryis divided into the segments of monotonous movement. Theconceptual features for each segment, such as degrees ofchanges of pose and position and the relative distances fromother objects in the surroundings, are evaluated. Meanwhile, themost suitable verbs and other syntactic elements are selected.Finally, the natural language text for interpreting human behaviors is generated by machine translation technology. Kollniget al. 118 use fuzzy membership functions to associateverbs with quantitative details obtained by automatic imagesequence analysis for generating natural language descriptionsof a traffic scene. In their scheme, each occurrence is definedby three predicates a precondition, monotonicity conditionand postcondition. The most significant disadvantage ofthe formalized reasoning methods is that they cannot handleuncertainty of events 96.Although there is some progress in description of behaviors,some key problems remain open, for example how to properlyrepresent semantic concepts, how to map motion characteristicsto semantic concepts and how to choose efficient representations to interpret the scene meanings.V. PERSONAL IDENTIFICATION FOR VISUAL SURVEILLANCEThe problem of who is now entering the area under surveillance is of increasing importance for visual surveillance. Suchpersonal identification can be treated as a special behaviorunderstanding problem. Human face and gait are now regardedas the main biometric features that can be used for personalidentification in visual surveillance systems 2. In recent years,great progress in face recognition 4650 has been achieved.The main steps in the face recognition for visual surveillanceare face detection, face tracking, face feature detection andface recognition 5155. Usually, these steps are studiedseparately. Therefore, developing an integrated face recognitionsystem involving all of the above steps is critical for visualsurveillance. As the length of this paper is restricted, we reviewhere only recent researches on the major existing methods forgait recognition.A. ModelBased MethodsIn modelbased methods, parameters, such as joint trajectories, limb lengths, and angular speeds, are measured156162, 180, 181.Cunado et al. 156, 157 model gait as the movement ofan articulated pendulum and use the dynamic Hough transform158 to extract the lines representing the thigh in each frame.The least squares method is used to smooth the inclination dataof the thigh and to fill the missing points caused by selfocclusion of the legs. Phaseweighted magnitude spectra are used asgait features for recognition.Yam et al. 159, 160 propose a new modelbased gaitrecognition algorithm. Biomechanical models of walking andrunning are used to form a type of new anatomical model calleda dynamically coupled oscillator, for the hip motion, and thestructure and motion of the thigh and the lower leg. Temporaltemplate matching 161 is used to extract the rotation angles ofthe thigh and the lower legs. Then gait signatures are obtainedfrom the lowerorder phaseweighted magnitude spectra.Another recent paper 162 uses dynamic features from trajectories of lowerbody joint angles such as the hip and the kneeto recognize individuals. This work first projects the 3D positions of markers attached to the body into the walking plane.Then a simple method is applied to estimate the planar offsets between the markers and the underlying skeleton or joints.Finally, given these offsets, the trajectories of joint angles arecomputed.Tracking and localizing the human body accurately in 3Dspace is still difficult despite the recent work on structurebasedmethods. In theory, joint angles are sufficient for recognition ofpeople by their gait. However, accurately recovering joint anglesHU et al. A SURVEY ON VISUAL SURVEILLANCE OF OBJECT MOTION AND BEHAVIORS 343from a walking video is still an unsolved or not wellsolvedproblem. In addition, the computational cost of the modelbasedapproaches is quite high.B. Statistical MethodsStatistical recognition techniques usually characterize the statistical description of motion image sets, and have been well developed in automatic gait recognition 60, 163168, 182,184.Murase et al. 163 use a parametric eigenspace representation to reduce computational cost and to improve the robustnessof gait estimation. Huang et al. 164166 have successfullyextended Murase et al.s work by adding canonical space analysis to obtain better discrimination. The eigenspace transformation EST has the advantage of reducing the dimensionality, butit cannot optimize class discrimination. Therefore, Huang et al.165 describe an integrated gait recognition system using ESTand canonical space analysis CSA.Shutler et al. 167 develop a velocitymomentbased methodfor describing the object motion in image sequences. Similarly,Lee et al. 60, 168 use the moment features of image regionsto recognize individuals. Assuming that people walk frontalparallel toward a fixed camera, the silhouette region is dividedinto seven subregions. A set of momentbased region featuresis used to recognize people and to predict the gender of an unknown person by hisher walking appearance.Statistical methods are relatively robust to noise and changeof time interval in input image sequences. Compared withmodelbased approaches, the computational cost of statisticalmethods is low.C. PhysicalParameterBased MethodsPhysicalparameterbased methods make use of geometricstructural properties of a human body to characterize a personsgait pattern. The parameters used include height, weight, stridecadence and length, etc. 56, 170173, 183.For example, a gait recognition technique using specific behavior parameters is recently proposed by Bobick et al. 170,171. This method does not directly analyze the dynamics ofgait patterns, but uses walking activities to recover the staticbody parameters of walking such as the vertical distance between head and feet, the distance between head and pelvis, thedistance between feet and pelvis, and the distance between theleft and right feet. The method is assessed using an expectedconfusion metric 172 to predict how well a given feature vectorcan identify an individual in a large population.Some recent work 56, 173 also uses human stature, stridelength and cadence as the input features for parametric gait classification. Given the calibration parameters of the camera andthe walking plane, the method uses the walking periodicity toaccurately estimate cadence and stride 56.Physicalparameterbased methods are intuitively understandable, and independent of viewing angles because theseparameters usually are recovered in the 3D space. However,they depend greatly on the vision techniques used to recover therequired parameters, e.g., bodypart labeling, depth compensation, camera calibration, shadow removal, etc. In addition, theparameters used for recognition may be not effective enoughacross a large population.D. SpatioTemporal MotionBased MethodsFor motion recognition based on spatiotemporal analysis, theaction or motion is characterized via the entire 3D spatiotemporal data volume spanned by the moving person in the imagesequence. These methods generally consider motion as a wholeto characterize its spatiotemporal distributions 27, 58, 59,61, 62, 177, 178, 185.Perhaps the earliest approach to recognizing people is to obtain gait features from the spatiotemporal pattern of a walkingfigure 27. In translation and time XT space, the motions ofthe head and legs have different patterns. These patterns are firstprocessed to determine the bounding box of a moving body, andthen fitted to a fivestick model. Gait signatures could be acquired from the velocitynormalized fitted model. Later, Niyogiet al. 58 extend their own work by using the spatiotemporalsurface to analyze gait. After motion detection, the XYT pattern2D space and 1D time is fitted with a smooth spatiotemporalsurface. This surface is represented as a combination of a standard parametric surface and a difference surface that can be usedto recognize some simple actions.Using the image selfsimilarity in XYT, BenAbdelkader et al.59, 177 propose a motionbased gaitrecognition technique.The similarity plots SPs of the image sequence of a movingobject are projections of its planar dynamics 61. Hence, theseSPs include much information of gait motion.Kale et al. 62 propose a HMMbased method for representing and recognizing gait. First, a set of key frames that occurduring a walk cycle is chosen. The widths of the walking figuresbinary silhouettes, in such a set of key frames, are chosen as theinput features. Then, a lowdimensional measurement vector isproduced using the Euclidean distance between a given imageand the set of key frames. These measurement vector sequencesare used to train the HMMs.Spatiotemporal motionbased methods are able to better capture both spatial and temporal information of gait motion. Theiradvantage is low computational complexity and a simple implementation. However, they are susceptible to noise and to variations of the timings of movements.E. Fusion of Gait With Other BiometricsThe fusion of gait information with other biometricscan further increase recognition robustness and reliability.Shakhnarovich et al. 64 develop a viewnormalized methodfor solving the problem of integrated face and gait recognitionfrom multiple views. For optimal face recognition, they set avirtual camera to capture the frontal face. For gait recognition,they set a virtual camera to capture the sideview walkingsequence. Results show that the integrated face and gaitrecognition outperforms recognition which only uses a singlemode. In extended work, Shakhnarovich et al. 65 evaluatethe recognition performances of several different probabilisticcombinations for fusing viewnormalized face and gait.Although many researchers have been working on gait recognition, current research of gait recognition is still in its infancy.344 IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICSPART C APPLICATIONS AND REVIEWS, VOL. 34, NO. 3, AUGUST 2004First, most experiments are carried out under constrained circumstances, e.g., no occlusion happens while objects are usually moving, the background is simple, etc. Second, existing algorithms are evaluated on small databases. Future work on gaitrecognition will focus on handling these two problems.VI. FUSION OF DATA FROM MULTIPLE CAMERASMotion detection, tracking, behavior understanding, andpersonal identification at a distance discussed above can berealized by single camerabased visual surveillance systems.Multiple camerabased visual surveillance systems can beextremely helpful because the surveillance area is expandedand multiple view information can overcome occlusion.Tracking with a single camera easily generates ambiguity dueto occlusion or depth. This ambiguity may be eliminated fromanother view. However, visual surveillance using multicamerasalso brings problems such as camera installation how to coverthe entire scene with the minimum number of cameras, cameracalibration, object matching, automated camera switching, anddata fusion.A. InstallationThe deployment of the cameras has a great influence on therealtime performance and the cost of the system. Camerascannot be employed arbitrarily due to factors such as thetopography of the area. Redundant cameras increase not onlyprocessing time and algorithmic complexity, but also theinstallation cost. In contrast, a lack of cameras may cause someblind angles, which reduce the reliability of a surveillancesystem. So the question of how to cover the entire scene withthe minimum number of cameras is important. Pavlidis et al.149 provide an optimum algorithm for solving the problemof multiplecamera installation in parking lots. The basic ideais to place camera 1 on a certain position at first, then to searchthe rest of the space to place the second camera at a point wherethere is a 2550 overlap region between the fields of viewof camera 1 and camera 2. The other cameras are added one byone, subject to the constraint that the field of view of each newcamera should have a 2550 overlap with the combinedfields of view of all the previous cameras.B. CalibrationTraditional calibration methods use the 3D coordinates andthe image coordinates of some known points to compute the parameters of a camera. Calibration is more complex when multiple cameras are concerned. Current multiple camera selfcalibration methods use temporal information. Stein and Lee et al.38, 39 use the motion trajectory and the ground plane constraint to determine the projection transformation matrix, andthen such matrix is decomposed to obtain the extrinsic parameters of the camera. However, this method is inaccurate, andcannot be used if there is no ground plane.C. Object MatchingObject matching among multiple cameras involves findingthe correspondences between the objects in different imagesequences taken by different cameras. There are two popularmethods one is the geometrybased method that establishescorrespondence according to geometric features transformed tothe same space and the other is the recognitionbased method.As an example of the geometrybased method, Cai et al. 41,42 use features of location, intensity and geometry to matchbetween images taken by different cameras. As an exampleof recognitionbased methods, Krumm et al. 57 use colorhistograms to match regions. In general, the methods for objectmatching need camera calibration. However, some researchersalso develop methods without calibration. For example, Javedet al. 150 use the spatial relationships between view fields ofcameras to establish the corresponding relationships of images.D. SwitchingWhen an object moves out of the view field of an activecamera, or the camera cannot give a good view of the movingobject, then the system should switch to another camera thatmay give a better view of the object. The key problems are howto find the better camera and how to minimize the number ofswitches during tracking. Cai et al. 41, 42 establish a trackingconfidence for each object. When the tracking confidence isbelow a certain threshold, the system begins a global search andselects the camera with the highest tracking confidence as theactive camera.E. Data FusionData fusion is important for occlusion handling and continuous tracking. Dockstader et al. 151 use a Bayesian networkto fuse 2D state vectors acquired from various image sequencesto obtain a 3D state vector. Collins et al. 152 design an algorithm that obtains an integrated representation of an entire sceneby fusing information from every camera into a 3D geometriccoordinate system. Kettnaker et al. 43 synthesize the trackingresults of different cameras to obtain an integrated trajectory.F. Occlusion HandlingIn practice, selfocclusion, and occlusions between differentmoving objects or between moving objects and the backgroundare inevitable. Multiple camera systems offer efficient andpromising methods for coping with occlusion. Utsumi et al.40 utilize multiple cameras to track people, successfullyresolving the mutualocclusion and selfocclusion by choosingthe best view. Dockstader et al. 151 describe a multiplecamera surveillance system that is used to track partly occludedpeople. Tsutsui et al. 153 apply the multiple camera surveillance system to optical flowbased human tracking. Whena static object in one camera occludes an object, the systempredicts the 3D coordinate position and moving speed of theoccluded object according to information from other cameras.Mittal et al. 154 resolve human tracking in complex scenesusing multiple cameras. First, using the Bayesian classificationrule, images are segmented according to the human model andthe estimated position of each person. Then, data from multiplecameras are fused to estimate the positions of the humans onthe ground plane. Finally, a Kalman filter is used for tracking.HU et al. A SURVEY ON VISUAL SURVEILLANCE OF OBJECT MOTION AND BEHAVIORS 345VII. FUTURE DEVELOPMENTSIn Sections IIVI, we have reviewed the stateoftheart ofvisual surveillance for humans and vehicles sorted by a general framework of visual surveillance systems. Although a largeamount of work has been done in visual surveillance for humansand vehicles, many issues are still open and deserve further research, especially in the following areas.A. Occlusion HandlingOcclusion handing is a major problem in visual surveillance.Typically, during occlusion, only portions of each object are visible and often at very low resolution. This problem is generally intractable, and motion segmentation based on backgroundsubtraction may become unreliable. To reduce ambiguities dueto occlusion, better models need be developed to cope with thecorrespondence between features and body parts, and thus eliminate correspondence errors that occur during tracking multipleobjects. When objects are occluded by fixed objects such asbuildings and street lamps, some resolution is possible throughmotion region analysis and partial matching. However, whenmultiple moving objects occlude each other, especially whentheir speeds, directions and shapes are very close, their motionregions coalesce, which makes the location and tracking of objects particularly difficult. The selfocclusion of a human bodyis also a significant and difficult problem. Interesting progressis being made using statistical methods to predict object pose,position, and so on, from available image information. Perhapsthe most promising practical method for addressing occlusionis through the use of multiple cameras.B. Fusion of 2D and 3D TrackingTwodimensional tracking is simple and rapid, and it hasshown some early successes in visual surveillance, especiallyfor lowresolution application areas where the precise posturereconstruction is not needed, e.g., pedestrian and vehicletracking in a traffic surveillance setting. However, the majordrawback of the 2D approach is its restriction of the cameraangle.Compared with 2D approaches, 3D approaches are moreeffective for accurate estimation of position in space, moreeffective handling of occlusion, and highlevel judgments aboutcomplex object movements such as wandering around, shakinghands, dancing, and vehicle overtaking. However, applying3D tracking requires more parameters and more computationduring the matching process. Also, visionbased 3D trackingbrings a number of challenges such as the acquisition of objectmodels, occlusion handling, parameterized object modeling,etc.In fact, the combination of 2D tracking and 3D trackingis a significant research direction that few researches have attempted. This combination is expected to fuse the merits of the2D tracking algorithms and those of the 3D tracking algorithms. The main difficulties of this combination are deciding when 2D tracking should be used and when 3Dtracking should be used how to initialize pose parameters for 3D tracking according to the results from 2D tracking, when the trackingalgorithm is switched from 2D to 3D.C. ThreeDimensional Modeling of Humans and VehiclesWe think that it is feasible to build 3D models for humans andvehicles. As far as vehicles are concerned, they can be treated asrigid objects, drawn from only a few classes and with invariable3D shapes during normal usage. It is possible to establish 3Dmodels of vehicles using CAD tools, etc. A generic and parametric model can be established for each class 125, 155. Asfar as human beings are concerned, the shapes of human bodiesare similar, so it is possible to build a uniform parametric modelfor human bodies. The parametric models and their applicationsin tracking and identification are important research directionsin visual surveillance. 3D modeling deserves more attention infuture work.D. Combination of Visual Surveillance and PersonalIdentificationAs mentioned in Section V, visionbased human identification at a distance has become increasingly important. Gait is amost attractive modality used for this purpose. Generally, futurework on gait recognition will focus on the following directions.1 Establishing a large common database and a standardtest protocol. The database with an independent subdatabase for the test just like the FERET protocol 113is necessary for convincing test. Any realistic databaseshould include the factors affecting gait perception, e.g.,clothing, environments, distance, carried objects such asbriefcases 179, and viewing angle. Such a database allows one to explore the limitations of the extracted gaitsignatures as well as the confidence estimation associatedwith the use of gait to buttress other biometric measures66.2 Combining dynamic features and static features. Gaitincludes both individual appearances and the dynamicsof walking. Developing the underlying static parametersof a human body and the dynamic characteristics of jointangles is helpful to recognition.3 Developing multiple biometric featurebased systemsin which gait is a basic biometric feature. A multiplebiometric system either fuses multiple biometric featuresor automatically switches among different biometric features according to operational conditions. For example, ata distance, gait can be used for recognition when an individual is near to the camera, the face image provides apowerful cue at intermediate distances, the informationfrom both face and gait can be fused to improve the recognition accuracy.4 Obtaining the viewinvariant gait signatures from thetracked image sequences 67. To extract and localize arbitrarily articulated shapes, viewinvariant gait signaturesfrom the tracked image sequences need to be obtained infuture recognition systems.346 IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICSPART C APPLICATIONS AND REVIEWS, VOL. 34, NO. 3, AUGUST 2004E. Behavior UnderstandingOne of the objectives of visual surveillance is to analyze andinterpret individual behaviors and interactions between objectsto decide for example whether people are carrying, depositing orexchanging objects, whether people are getting on or getting offa vehicle, or whether a vehicle is overtaking another vehicle, etc.Recently, related research has still focused on some basic problems like recognition of standard gestures and simple behaviors.Some progress has been made in building the statistical modelsof human behaviors using machine learning. Behavior recognition is complex, as the same behavior may have several differentmeanings depending upon the scene and task context in whichit is performed. This ambiguity is exacerbated when several objects are present in a scene 130. The following problems withinbehavior understanding are challenging statistical learning formodeling behaviors, contextdependent learning from exampleimages, realtime performance required by behavior interpretation, classification and labeling of motion trajectories of trackedobjects, automated learning of the priori knowledge 63 implied in object behaviors, visually mediated interaction, and attention mechanisms.F. Anomaly Detection and Behavior PredictionAnomaly detection and behavior prediction are significant inpractice. In applications of visual surveillance, not only shouldvisual surveillance systems detect anomalies such as traffic accidents and car theft etc, according to requirements of functions,but also predict what will happen according to the current situation and raise an alarm for a predicted abnormal behavior. Implementations are usually based on one or other of the followingtwo methods.1 Probability reasoning and prior rules combinedmethods. A behavior with small probability, or againstthe prior rules would be regarded as an anomaly.2 Behaviorpatternbased methods. Based on learnedpatterns of behaviors, we can detect anomalies andpredict object behaviors. When a detected behaviordoes not match the learned patterns, it is classed as ananomaly. We can predict an object behavior by matchingthe observed subbehavior of the object with the learnedpatterns. Generally, patterns of behaviors in a scene canbe constructed by supervised or unsupervised learning ofeach objects velocities and trajectories, etc. Supervisedlearning is used for known scenes where objects movein predefined ways. For unknown scenes, patterns ofbehaviors should be constructed by selforganizing andselflearning of image sequences. Fernyhough et al. 5establish the spatiotemporal region by learning resultsof tracking objects in a image sequence, and construct aqualitative behavior model by qualitative reasoning andstatistical analysis.G. ContentBased Retrieval of Surveillance VideosThe task in contentbased retrieval of surveillance videosis to retrieve video clips from surveillance video databasesaccording to video contents, based on automatic image andvideo understanding. At present, research on video retrievalfocuses on the lowlevel perceptively meaningful representations of pictorial data such as color, texture, shape, etc andsimple motion information. These retrieval techniques cannotaccurately and effectively search the videos for sequencesrelated to specified behaviors. Semanticbased video retrievalSBVR aims to bridge the gap between lowlevel features andhighlevel semantic meanings. Based on automatic interpretation of contents in surveillance videos, SBVR may classifyand further access the surveillance video clips that are relatedto specific behaviors, and supply a more highlevel, moreintuitive and more humanistic retrieval mode. Semanticbasedretrieval of surveillance videos brings the following difficultproblems automatic extraction of semantic behavior features,combination between lowlevel visual features and behaviorfeatures, hierarchical organization of image and video features,semantic video indexing, inquire interface, etc.H. Natural Language Description of Object BehaviorsDescribing object behaviors by natural language in accordwith human habits is a challenging research subject. The keytask is to obtain the mapping relationships between object behaviors in image sequences and the natural language. Thesemapping relationships are related to the following two problems.1 Relationships between behaviors and semantic concepts. Each semantic concept of motion describes a classof behaviors, but each behavior may be related to multiplesemantic concepts. After the mapping has been clearly defined, we could construct the relationship between the results of lowlevel image processing and object behaviors.The key problems include the modeling of semantic concepts of motions, and the automatic learning of semanticconcepts of behaviors.2 Semantic recognition and natural language description of object behaviors. People usually describedevelopments and transformations of objects with concepts at different levels. The higher level concepts requiregreater background knowledge. It is a key problem to analyze the behaviors of moving objects using the trackingresults from lowlevel systems, and further recognize themore abstract semantic concepts at higher layers. We canuse the corresponding relationships between semanticconcepts and object behaviors, semantic networks withdifferent layers and reasoning theory to explore thisproblem. Natural language is the most convenient andnatural way for humans to communicate each other.Organizing recognized concepts and further representingobject behaviors in brief and clear natural languageis one of the ultimate goals of visual surveillance. Inaddition, the synchronous description, i.e., giving the description before a behavior finishes during the behavioris progressing, is also a challenge. We should design anincremental description method which is able to predictobject behaviors.HU et al. A SURVEY ON VISUAL SURVEILLANCE OF OBJECT MOTION AND BEHAVIORS 347I. Fusion of Data From Multiple SensorsIt is obvious that future visual surveillance systems willgreatly benefit from the use of multiple cameras 44, 45,73. The cooperation between multiple cameras relies greatlyon fusion of data from each camera. Data fusion is primarilyfeaturelevel based rather than imagelevel based or decisionmakinglevel based. It happens in single view tracking,correspondence of crosscameras, automatic camera switchingi.e., best view selection, etc. The main problems involvehow to fuse different types of features, e.g., color, geometricfeatures, into one group to track and recognize objects, and further understand their behaviors how to fuse features extractedfrom different viewpoints to correspond objects and how tocommunicate data about the same object between multiplecameras.Besides video, sensors for surveillance include audio,infrared, ultrasonic, and radar, etc. Each of these sensors hasits own characteristics. Surveillance using multiple differentsensors seems to be a very interesting subject. The mainproblem is how to make use of their respective merits and fuseinformation from such kinds of sensors.J. Remote SurveillanceRemote surveillance becomes more and more important formany promising applications, e.g., military combat, preventionof forest fires, etc. Video data are acquired from distributed sensors and transmitted to a remote control center. The transmissionprocess must satisfy the following requirements. The upload bandwidth from sensors to the control centershould be much wider than the download bandwidth fromthe control center to sensors. The security of transmission must be guaranteed. Becausesome surveillance data involve privacy, commercial secrets and even national security, and nevertheless are transmitted through public networks, information security becomes a key problem. This needs the developments of thetechniques such as digital watermarking and encryption89.The demand for remote surveillance and surveillance usingmultiple cameras and multiple sensors motivates the combination of network and visual surveillance, which brings new challenges in intelligent surveillance.VIII. CONCLUSIONSVisual surveillance in dynamic scenes is an active andimportant research area, strongly driven by many potentialand promising applications, such as access control in specialareas, personspecific identification in certain scenes, crowdflux statistics and congestion analysis, and anomaly detectionand alarming, etc.We have presented an overview of recent developments invisual surveillance within a general processing framework forvisual surveillance systems. The stateoftheart of existingmethods in each key issue is described with the focus on thefollowing tasks detection, tracking, understanding and description of behaviors, personal identification for visual surveillance,and interactive surveillance using multiple cameras. As forthe detection of moving objects, it involves environmentalmodeling, motion segmentation and object classification.Three techniques for motion segmentation are addressedbackground subtraction, temporal differencing, and opticalflow. We have discussed four intensively studied approachesto tracking region based, activecontour based, feature based,and model based. We have reviewed several approaches tobehavior understanding, including DTW, FSM, HMMs, andTDNN. In addition, we examine the stateoftheart of behaviordescription. As to personal identification at a distance, wehave divided gait recognition methods into four classes modebased, statistics, physicalparameter based, and spatiotemporalmotion based. As to fusion of data from multiple cameras, wehave reviewed installation, object matching, switching, anddata fusion.At the end of this survey, we have given some detailed discussions on future directions, such as occlusion handling, fusion of 2D tracking and 3D tracking, 3D modeling of humansand vehicles, combination of visual surveillance and personalidentification, anomaly detection and behavior prediction, contentbased retrieval of surveillance videos, natural language description of object behaviors, fusion of data from multiple sensors, and remote surveillance.ACKNOWLEDGMENTThe authors thank J. Lou, Q. Liu, H. Ning, M. Hu, D. Xie,and G. Xu from the NLPR for their valuable suggestions andassistance in preparing this paper.REFERENCES1 R. T. Collins, A. J. Lipton, and T. Kanade, Introduction to the specialsection on video surveillance, IEEE Trans. Pattern Anal. Machine Intell., vol. 22, pp. 745746, Aug. 2000.2 J. Steffens, E. Elagin, and H. Neven, Person spotterfast and robustsystem for human detection, tracking and recognition, in Proc. IEEEInt. Conf. Automatic Face and Gesture Recognition, 1998, pp. 516521.3 R. T. Collins, A. J. Lipton, T. Kanade, H. Fujiyoshi, D. Duggins, Y.Tsin, D. Tolliver, N. Enomoto, O. Hasegawa, P. Burt, and L. Wixson, Asystem for video surveillance and monitoring, Carnegie Mellon Univ.,Pittsburgh, PA, Tech. Rep., CMURITR0012, 2000.4 I. Haritaoglu, D. Harwood, and L. S. Davis, W  Realtime surveillance of people and their activities, IEEE Trans. Pattern Anal. MachineIntell., vol. 22, pp. 809830, Aug. 2000.5 J. Fernyhough, A. G. Cohn, and D. C. Hogg, Constructing qualitativeevent models automatically from video input, Image Vis. Comput., vol.18, no. 9, pp. 81103, 2000.6 A. Baumberg and D. C. Hogg, Learning deformable models fortracking the human body, in MotionBased Recognition, M. Shah andR. Jain, Eds. Norwell, MA Kluwer, 1996, pp. 3960.7 H. Z. Ning, L. Wang, W. M. Hu, and T. N. Tan, Articulated model basedpeople tracking using motion models, in Proc. Int. Conf. MultiModelInterfaces, 2002, pp. 115120.8 C. R. Wren, A. Azarbayejani, T. Darrell, and A. P. Pentland, Pfinderrealtime tracking of the human body, IEEE Trans. Pattern Anal. Machine Intell., vol. 19, pp. 780785, July 1997.9 T. Olson and F. Brill, Moving object detection and event recognitionalgorithms for smart cameras, in Proc. DARPA Image UnderstandingWorkshop, 1997, pp. 159175.10 A. J. Lipton, H. Fujiyoshi, and R. S. Patil, Moving target classificationand tracking from realtime video, in Proc. IEEE Workshop Applications of Computer Vision, 1998, pp. 814.11 S. McKenna, S. Jabri, Z. Duric, A. Rosenfeld, and H. Wechsler,Tracking groups of people, Comput. Vis. Image Understanding, vol.80, no. 1, pp. 4256, 2000.348 IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICSPART C APPLICATIONS AND REVIEWS, VOL. 34, NO. 3, AUGUST 200412 C. Stauffer and W. Grimson, Adaptive background mixture models forrealtime tracking, in Proc. IEEE Conf. Computer Vision and PatternRecognition, vol. 2, 1999, pp. 246252.13 D. Meyer, J. Denzler, and H. Niemann, Model based extraction of articulated objects in image sequences for gait analysis, in Proc. IEEEInt. Conf. Image Processing, 1998, pp. 7881.14 J. Barron, D. Fleet, and S. Beauchemin, Performance of optical flowtechniques, Int. J. Comput.Vis., vol. 12, no. 1, pp. 4277, 1994.15 N. Friedman and S. Russell, Image segmentation in video sequencesa probabilistic approach, in Proc. 13th Conf. Uncertainty in ArtificialIntelligence, 1997, pp. 13.16 E. Stringa, Morphological change detection algorithms for surveillanceapplications, in Proc. British Machine Vision Conf., 2000, pp. 402412.17 Y. Kuno, T. Watanabe, Y. Shimosakoda, and S. Nakagawa, Automateddetection of human for visual surveillance system, in Proc. Int. Conf.Pattern Recognition, 1996, pp. 865869.18 R. Cutler and L. S. Davis, Robust realtime periodic motion detection,analysis, and applications, IEEE Trans. Pattern Anal. Machine Intell.,vol. 22, pp. 781796, Aug. 2000.19 A. J. Lipton, Local application of optic flow to analyze rigid versusnonrigid motion, in Proc. Int. Conf. Computer Vision WorkshopFrameRate Vision, Corfu, Greece, 1999.20 C. Stauffer, Automatic hierarchical classification using timebasecooccurrences, in Proc. IEEE Conf. Computer Vision and PatternRecognition, vol. 2, 1999, pp. 335339.21 D. Meyer, J. Psl, and H. Niemann, Gait classification with HMMs fortrajectories of body parts extracted by mixture densities, in Proc. BritishMachine Vision Conf., 1998, pp. 459468.22 J. G. Lou, Q. F. Liu, W. M. Hu, and T. N. Tan, Semantic interpretationof object activities in a surveillance system, in Proc. Int. Conf. PatternRecognition, 2002, pp. 777780.23 T. Boult, Framerate multibody tracking for surveillance, in Proc.DARPA Image Understanding Workshop, Monterey, CA, Nov. 1998, pp.305308.24 J. K. Aggarwal, Q. Cai, W. Liao, and B. Sabata, Nonrigid motion analysis articulated  elastic motion, Comput.Vis. Image Understanding,vol. 70, no. 2, pp. 142156, 1998.25 I. A. Karaulova, P. M. Hall, and A. D. Marshall, A hierarchical modelof dynamics for tracking people with a single video camera, in Proc.British Machine Vision Conf., 2000, pp. 262352.26 S. Ju, M. Black, and Y. Yaccob, Cardboard people a parameterizedmodel of articulated image motion, in Proc. IEEE Int. Conf. AutomaticFace and Gesture Recognition, 1996, pp. 3844.27 S. A. Niyogi and E. H. Adelson, Analyzing and recognizing walkingfigures in XYT, in Proc. IEEE Conf. Computer Vision and PatternRecognition, 1994, pp. 469474.28 K. Rohr, Toward modelbased recognition of human movements inimage sequences, CVGIP Image Understanding, vol. 59, no. 1, pp.94115, 1994.29 S. Wachter and H.H. Nagel, Tracking persons in monocular imagesequences, Comput. Vis. Image Understanding, vol. 74, no. 3, pp.174192, 1999.30 N. Paragios and R. Deriche, Geodesic active contours and level setsfor the detection and tracking of moving objects, IEEE Trans. PatternAnal. Machine Intell., vol. 22, pp. 266280, Mar. 2000.31 N. Peterfreund, Robust tracking of position and velocity with Kalmansnakes, IEEE Trans. Pattern Anal. Machine Intell., vol. 22, pp.564569, June 2000.32 M. Isard and A. Blake, Contour tracking by stochastic propagation ofconditional density, in Proc. European Conf. Computer Vision, 1996,pp. 343356.33 R. Polana and R. Nelson, Low level recognition of human motion,in Proc. IEEE Workshop Motion of NonRigid and Articulated Objects,Austin, TX, 1994, pp. 7782.34 D.S. Jang and H.I. Choi, Active models for tracking moving objects,Pattern Recognit., vol. 33, no. 7, pp. 11351146, 2000.35 M. Brand, N. Oliver, and A. Pentland, Coupled hidden Markov modelsfor complex action recognition, in Proc. IEEE Conf. Computer Visionand Pattern Recognition, 1997, pp. 994999.36 M. Izumi and A. Kojima, Generating natural language description ofhuman behaviors from video images, in Proc. Int. Conf. Pattern Recognition, 2000, pp. 728731.37 A. Kojima, T. Tamura, and K. Fukunaga, Natural language descriptionof human activities from video images based on concept hierarchy ofactions, Int. J. Comput. Vis., vol. 50, no. 2, pp. 171184, 2002.38 G. P. Stein, Tracking from multiple view points selfcalibration ofspace and time, in Proc. IEEE Conf. Computer Vision and PatternRecognition, vol. I, 1999, pp. 521527.39 L. Lee, R. Romano, and G. Stein, Monitoring activities from multiplevideo streams establishing a common coordinate frame, IEEE Trans.Pattern Anal. Machine Intell., vol. 22, pp. 758767, Aug. 2000.40 A. Utsumi, H. Mori, J. Ohya, and M. Yachida, Multipleviewbasedtracking of multiple humans, in Proc. Int. Conf. Pattern Recognition,1998, pp. 197601.41 Q. Cai and J. K. Aggarwal, Tracking human motion using multiplecameras, in Proc. Int. Conf. Pattern Recognition, Vienna, Austria, 1996,pp. 6872.42 , Tracking human motion in structured environments using a distributedcamera system, IEEE Trans. Pattern Anal. Machine Intell., vol.21, no. 11, pp. 12411247, 1999.43 V. Kettnaker and R. Zabih, Bayesian multicamera surveillance, inProc. IEEE Conf. Computer Vision and Pattern Recognition, 1999, pp.253259.44 T. H. Chang, S. Gong, and E. J. Ong, Tracking multiple people underocclusion using multiple cameras, in Proc. British Machine VisionConf., 2000, pp. 566576.45 Y. Caspi and M. Irani, Spatiotemporal alignment of sequences, IEEETrans. Pattern Anal. Machine Intell., vol. 24, pp. 14091424, Nov. 2002.46 A. Samal and P. A. Iyengar, Automatic recognition and analysis ofhuman faces and facial expressions a survey, Pattern Recognit., vol.25, no. 1, pp. 6577, 1992.47 R. Chellappa, C. L. Wilson, and S. Sirohey, Human and machine recognition of faces A survey, Proc. IEEE, vol. 83, pp. 705741, May 1995.48 D. Swets and J. Weng, Discriminant analysis and eigenspace partitiontree for face and object recognition from views, in Proc. Int. Conf. Automatic Face and Gesture Recognition, 1996, pp. 182187.49 B. Moghaddam, W. Wahid, and A. Pentland, Beyond eigenfaces probabilistic matching for face recognition, in Proc. IEEE Int. Conf. Automatic Face and Gesture Recognition, 1998, pp. 3035.50 G. Guo, S. Li, and K. Chan, Face recognition by support vector machines, in Proc. Int. Conf. Automatic Face and Gesture Recognition,2000, pp. 196201.51 H. Rowley, S. Baluja, and T. Kanade, Neural network based face detection, IEEE Trans. Pattern Anal. Machine Intell., vol. 20, pp. 2338,Jan. 1998.52 C. Garcia and G. Tziritas, Face detection using quantified skin colorregions merging and wavelet packet analysis, IEEE Trans. Multimedia,vol. 1, pp. 264277, Sept. 1999.53 B. Menser and M. Wien, Segmentation and tracking of facial regionsin color image sequences, in Proc. SPIE Visual Communications andImage Processing, vol. 4067, Perth, Australia, 2000, pp. 731740.54 A. Saber and A. M. Tekalp, Frontalview face detection and facial feature extraction using color, shape and symmetry based cost functions,Pattern Recognit. Lett., vol. 19, no. 8, pp. 669680, 1998.55 G. Xu and T. Sugimoto, A softwarebased system for realtime face detection and tracking using pantiltzoom controllable camera, in Proc.Int. Conf. Pattern Recognition, 1998, pp. 11941197.56 C. BenAbdelkader, R. Culter, and L. Davis, Stride and cadence as abiometric in automatic person identification and verification, in Proc.Int. Conf. Automatic Face and Gesture Recognition, Washington, DC,2002, pp. 372377.57 J. Krumm, S. Harris, B. Meyers, B. Brumitt, M. Hale, and S. Shafer,Multicamera multiperson tracking for EasyLiving, in Proc. IEEEInt. Workshop Visual Surveillance, Dublin, Ireland, July 2000, pp. 310.58 S. A. Niyogi and E. H. Adelson, Analyzing gait with spatiotemporalsurface, in Proc. IEEE Workshop Motion of NonRigid and ArticulatedObjects, 1994, pp. 6469.59 C. BenAbdelkader, R. Cutler, H. Nanda, and L. Davis, EigenGait motionbased recognition of people using image selfsimilarity, in Proc.Int. Conf. Audio and VideoBased Biometric Person Authentication,2001, pp. 312317.60 L. Lee and W. Grimson, Gait analysis for recognition and classification, in Proc. Int. Conf. Automatic Face and Gesture Recognition,Washington, DC, 2002, pp. 155162.61 R. Culter and L. Davis, Robust realtime periodic motion detection,analysis and applications, IEEE Trans. Pattern Recognit. Machine Intell., vol. 13, pp. 129155, Feb. 2000.62 A. Kale, A. Rajagopalan, N. Cuntoor, and V. Kruger, Gaitbasedrecognition of humans using continuous HMMs, in Proc. Int. Conf.Automatic Face and Gesture Recognition, Washington, DC, 2002, pp.336341.HU et al. A SURVEY ON VISUAL SURVEILLANCE OF OBJECT MOTION AND BEHAVIORS 34963 D. Makris and T. Ellis, Path detection in video surveillance, Image Vis.Comput., vol. 20, no. 12, pp. 895903, 2002.64 G. Shakhnarovich, L. Lee, and T. Darrell, Integrated face and gaitrecognition from multiple views, in Proc. IEEE Conf. Computer Visionand Pattern Recognition, 2001, pp. I439I446.65 G. Shakhnarovich and T. Darrell, On probabilistic combination of faceand gait cues for identification, in Proc. Int. Conf. Automatic Face andGesture Recognition, Washington, DC, 2002, pp. 176181.66 M. S. Nixon, J. N. Carter, D. Cunado, P. S. Huang, and S. V. Stevenage,Automatic gait recognition, in BIOMETRICS Personal Identificationin Networked Society, A. K. Jain, Ed. Norwell, MA Kluwer, 1999, ch.11.67 N. Spencer and J. Carter, Viewpoint invariance in automatic gait recognition, in BMVA British Machine Vision Association Symp. AdvancingBiometric Techniques at the Royal Statistical Society, London, U.K.,Mar. 6, 2002, pp. 16.68 S. J. Maybank and T. N. Tan, Special section on visual surveillanceintroduction, Int. J. Comput. Vis., vol. 37, no. 2, pp. 173174,2000.69 C. Regazzoni and V. Ramesh, Special issue on video communications, processing, and understanding for third generation surveillancesystems, Proc. IEEE, vol. 89, pp. 13551367, Oct. 2001.70 M. Khle, D. Merkl, and J. Kastner, Clinical gait analysis by neural networks Issues and experiences, in Proc. IEEE Symp. ComputerBasedMedical Systems, 1997, pp. 138143.71 A. Mohan, C. Papageorgiou, and T. Poggio, Examplebased object detection in images by components, IEEE Trans. Pattern Recognit. Machine Intell., vol. 23, pp. 349361, Apr. 2001.72 A. Galata, N. Johnson, and D. Hogg, Learning variablelength Markovmodels of behavior, Comput. Vis. Image Understanding, vol. 81, no. 3,pp. 398413, 2001.73 L. Z. Manor and M. Irani, Multiview constraints on homographies,IEEE Trans. Pattern Anal. Machine Intell., vol. 24, pp. 214223, Feb.2002.74 Y. Wu and T. S. Huang, A coinference approach to robust visualtracking, in Proc. Int. Conf. Computer Vision, vol. II, 2001, pp. 2633.75 L. Wang, W. Hu, and T. Tan, Recent developments in human motionanalysis, Pattern Recognit., vol. 36, no. 3, pp. 585601, 2003.76 S. E. Kemeny, R. Panicacci, B. Pain, L. Matthies, and E. R. Fossum,Multiresolution image sensor, IEEE Trans. Circuits Syst. VideoTechnol., vol. 7, no. Aug., pp. 575583, 1997.77 A. Basu and D. Southwell, Omnidirectional sensors for pipe inspection, in Proc. IEEE Int. Conf. Systems, Man and Cybernetics, vol. 4,1995, pp. 31073112.78 A. Hilton and P. Fua, Foreword modeling people toward visionbasedunderstanding of a persons shape, appearance, and movement,Comput. Vis. Image Understanding, vol. 81, no. 3, pp. 227230, 2001.79 H. Z. Sun, T. Feng, and T. N. Tan, Robust extraction of moving objectsfrom image sequences, in Proc. Asian Conf. Computer Vision, Taiwan,R.O.C., 2000, pp. 961964.80 W. E. L. Grimson, C. Stauffer, R. Romano, and L. Lee, Using adaptivetracking to classify and monitor activities in a site, in Proc. IEEE Conf.Computure Vision and Pattern Recognition, Santa Barbara, CA, 1998,pp. 2231.81 C. Ridder, O. Munkelt, and H. Kirchner, Adaptive background estimation and foreground detection using Kalmanfiltering, in Proc. Int.Conf. Recent Advances in Mechatronics, 1995, pp. 193199.82 D. Koller, J. Weber, T. Huang, J. Malik, G. Ogasawara, B. Rao, and S.Russel, Toward robust automatic traffic scene analysis in realtime, inProc. Int. Conf. Pattern Recognition, Israel, 1994, pp. 126131.83 K. Toyama, J. Krumm, B. Brumitt, and B. Meyers, Wallflower principles and practice of background maintenance, in Proc. Int. Conf. Computer Vision, 1999, pp. 255261.84 H.Y. Shum, M. Han, and R. Szeliski, Interactive construction of 3Dmodels from panoramic mosaics, in Proc. IEEE Conf. Computer Visionand Pattern Recognition, Santa Barbara, CA, 1998, pp. 427433.85 T. Tian and C. Tomasi, Comparison of approaches to egomotion computation, in Proc. IEEE Conf. Computer Vision and Pattern Recognition, 1996, pp. 315320.86 Z. Y. Zhang, Modeling geometric structure and illumination variationof a scene from real images, in Proc. Int. Conf. Computer Vision,Bombay, India, 1998, pp. 47.87 T. N. Tan, G. D. Sullivan, and K. D. Baker, Modelbased localizationand recognition of road vehicles, Int. J. Comput. Vis., vol. 29, no. 1, pp.2225, 1998.88 , Recognizing objects on the groundplane, Image Vis. Comput.,vol. 12, no. 3, pp. 164172, 1994.89 F. Bartoleni, A. Tefas, M. Barni, and I. Pitas, Image authenticationtechniques for surveillance applications, Proc. IEEE, vol. 89, pp.14031417, Oct. 2001.90 K. Karmann and A. Brandt, Moving object recognition using anadaptive background memory, in TimeVarying Image Processingand Moving Object Recognition, V. Cappellini, Ed. Amsterdam, TheNetherlands Elsevier, 1990, vol. 2.91 M. Kilger, A shadow handler in a videobased realtime traffic monitoring system, in Proc. IEEE Workshop Applications of Computer Vision, Palm Springs, CA, 1992, pp. 1118.92 JPL, Traffic surveillance and detection technology development,Sensor Development Final Rep., Jet Propulsion Laboratory Publicationno. 9710, 1997.93 J. Malik, S. Russell, J. Weber, T. Huang, and D. Koller, A machinevision based surveillance system for Californaia roads, Univ. of California, PATH project MOU83 Final Rep., Nov. 1994.94 J. Malik and S. Russell, Traffic Surveillance and Detection TechnologyDevelopment New Traffic Sensor Technology, Univ. of California,Berkeley, California PATH Research Final Rep., UCBITSPRR976,1997.95 W. F. Gardner and D. T. Lawton, Interactive modelbased vehicletracking, IEEE Trans. Pattern Anal. Machine Intell., vol. 18, pp.11151121, Nov. 1996.96 Z. Q. Liu, L. T. Bruton, J. C. Bezdek, J. M. Keller, S. Dance, N. R.Bartley, and C. Zhang, Dynamic image sequence analysis using fuzzymeasures, IEEE Trans. Syst., Man, Cybern. B, vol. 31, pp. 557571,Aug. 2001.97 T. J. Fan, G. Medioni, and G. Nevatia, Recognizing 3D objects usingsurface descriptions, IEEE Trans. Pattern Recognit. Machine Intell.,vol. 11, pp. 11401157, Nov. 1989.98 B. Coifman, D. Beymer, P. McLauchlan, and J. Malik, A realtime computer vision system for vehicle tracking and traffic surveillance, Transportation Res. Part C, vol. 6, no. 4, pp. 271288, 1998.99 J. Malik and S. Russell, Traffic surveillance and detection technologydevelopment new traffic sensor technology, Univ. of California,Berkeley, 1996.100 C. A. Pau and A. Barber, Traffic sensor using a color vision method,in Proc. SPIETransportation Sensors and Controls Collision Avoidance, Traffic Management, and ITS, vol. 2902, 1996, pp. 156165.101 B. Schiele, Vodelfree tracking of cars and people based on colorregions, in Proc. IEEE Int. Workshop Performance Evaluation ofTracking and Surveillance, Grenoble, France, 2000, pp. 6171.102 Q. Delamarre and O. Faugeras, 3D articulated models and multiviewtracking with physical forces, Comput. Vis. Image Understanding, vol.81, no. 3, pp. 328357, 2001.103 , 3D articulated models and multiview tracking with silhouettes, in Proc. Int. Conf. Computer Vision, Kerkyra, Greece, 1999, pp.716721.104 C. Sminchisescu and B. Triggs, Covariance scaled sampling for monocular 3D body tracking, in Proc. IEEE Conf. Computer Vision and Pattern Recognition, Kauai, HI, 2001, pp. I447I454.105 R. Plankers and P. Fua, Articulated soft objects for videobased bodymodeling, in Proc. Int. Conf. Computer Vision, Vancouver, BC, Canada,2001, pp. 394401.106 T. Zhao, T. S. Wang, and H. Y. Shum, Learning a highly structuredmotion model for 3D human tracking, in Proc. Asian Conf. ComputerVision, Melbourne, Australia, 2002, pp. 144149.107 J. C. Cheng and J. M. F. Moura, Capture and representation of humanwalking in live video sequence, IEEE Trans. Multimedia, vol. 1, pp.144156, June 1999.108 C. Bregler, Learning and recognizing human dynamics in video sequences, in Proc. IEEE Conf. Computer Vision and Pattern Recognition, San Juan, Puerto Rico, 1997, pp. 568574.109 H. Sidenbladh and M. Black, Stochastic tracking of 3D human figuresusing 2D image motion, in Proc. European Conf. Computer Vision,Dublin, Ireland, 2000, pp. 702718.110 E. Ong and S. Gong, A dynamic human model using hybrid 2D3Drepresentation in hierarchical PCA space, in Proc. British Machine Vision Conf., U.K., 1999, pp. 3342.111 H. Yang, J. G. Lou, H. Z. Sun, W. M. Hu, and T. N. Tan, Efficient androbust vehicle localization, in Proc. IEEE Int. Conf. Image Processing,2001, pp. 355358.112 D. G. Lowe, Fitting parameterized 3D models to images, IEEE Trans.Pattern Anal. Machine Intell., vol. 13, pp. 441450, May 1991.113 J. Phillips, H. Moon, S. Rizvi, and P. Rause, The FERET evaluationmethodology for face recognition algorithms, IEEE Trans. PatternAnal. Machine Intell., vol. 22, pp. 10901104, Oct. 2000.350 IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICSPART C APPLICATIONS AND REVIEWS, VOL. 34, NO. 3, AUGUST 2004114 J. Hoshino, H. Saito, and M. Yamamoto, A match moving technique formerging CG cloth and human video, J. Visualiz. Comput. Animation,vol. 12, no. 1, pp. 2329, 2001.115 J. E. Bennett, A. RacinePoon, and J. C. Wakefield, MCMC for nonlinear hierarchical models, in Markov Chain Monte Carlo in Practice, W. R. Gilks, S. Richardson, and D. J. Spiegelhalter, Eds. London,U.K. Chapman and Hall, 1996, pp. 339357.116 M. Isard and A. Blake, CONDENSATIONConditional density propagation for visual tracking, Int. J. Comput. Vis., vol. 29, no. 1, pp. 528,1998.117 , Condensation unifying lowlevel and highlevel tracking in astochastic framework, in Proc. European Conf. Computer Vision, vol.1, 1998, pp. 893909.118 H. Kollnig, H. H. Nagel, and M. Otte, Association of motion verbs withvehicle movements extracted from dense optical flow fields, in Proc.European Conf. Computer Vision, 1994, pp. 338347.119 T. N. Tan and K. D. Baker, Efficient image gradient based vehicle localization, IEEE Trans. Image Processing, vol. 9, pp. 13431356, Aug.2000.120 T. N. Tan, G. D. Sullivan, and K. D. Baker, Pose determination andrecognition of vehicles in traffic scenes, in European Conf. ComputerVisionLecture Notes in Computer Science, vol. 1, J. O. Eklundh, Ed.,Stockholm, Sweden, 1994, pp. 501506.121 , Fast vehicle localization and recognition without line extraction,in Proc. British Machine Vision Conf., 1994, pp. 8594.122 A. E. C. Pece and A. D. Worrall, Tracking without feature detection,in Proc. IEEE Int. Workshop Performance Evaluation of Tracking andSurveillance, Grenoble, France, 2000, pp. 2937.123 , A statisticallybased Newton method for pose refinement,Image Vis. Comput., vol. 16, no. 8, pp. 541544, June 1998.124 D. Koller, K. Daniilidis, and H.H. Nagel, Modelbased object trackingin monocular image sequences of road traffic scenes, Int. J. Comput.Vis., vol. 10, no. 3, pp. 257281, 1993.125 H. Kollnig and H.H. Nagel, 3D pose estimation by directly matchingpolyhedral models to gray value gradients, Int. J. Comput. Vis., vol. 23,no. 3, pp. 283302, 1997.126 M. Haag and H.H. Nagel, Combination of edge element and opticalflow estimates for 3Dmodelbased vehicle tracking in traffic image sequences, Int. J. Comput. Vis., vol. 35, no. 3, pp. 295319, 1999.127 K. Takahashi, S. Seki, H. Kojima, and R. Oka, Recognition of dexterousmanipulations from time varying images, in Proc. IEEE Workshop Motion of NonRigid and Articulated Objects, Austin, TX, 1994, pp. 2328.128 A. F. Bobick and A. D. Wilson, A statebased technique to the representation and recognition of gesture, IEEE Trans. Pattern Anal. MachineIntell., vol. 19, pp. 13251337, Dec. 1997.129 A. D. Wilson, A. F. Bobick, and J. Cassell, Temporal classification ofnatural gesture and application to video coding, in Proc. IEEE Conf.Computer Vision and Pattern Recognition, 1997, pp. 948954.130 S. G. Gong and H. Buxton, Editorial understanding visual behavior,Image Vis. Comput., vol. 20, no. 12, pp. 825826, 2002.131 F. Bremond and G. Medioni, Scenario recognition in airborne videoimagery, in Proc. Int. Workshop Interpretation of Visual Motion, 1998,pp. 5764.132 T. Starner, J. Weaver, and A. Pentland, Realtime American signlanguage recognition using desk and wearable computerbased video,IEEE Trans. Pattern Anal. Machine Intell., vol. 20, pp. 13711375,Dec. 1998.133 N. M. Oliver, B. Rosario, and A. P. Pentland, A Bayesian computervision system for modeling human interactions, IEEE Trans. PatternAnal. Machine Intell., vol. 22, pp. 831843, Aug. 2000.134 M. Brand and V. Kettnaker, Discovery and segmentation of activities invideo, IEEE Trans. Pattern Anal. Machine Intell., vol. 22, pp. 844851,Aug. 2000.135 M. Yang and N. Ahuja, Extraction and classification of visual motionpattern recognition, in Proc. IEEE Conf. Computer Vision and PatternRecognition, 1998, pp. 892897.136 U. Meier, R. Stiefelhagen, J. Yang, and A. Waibel, Toward unrestrictedlip reading, Int. J. Pattern Recognit. Artificial Intell., vol. 14, no. 5, pp.571585, Aug 2000.137 Y. A. Ivanov and A. F. Boblic, Recognition of visual activities and interactions by stochastic parsing, IEEE Trans. Pattern Anal. MachineIntell., vol. 22, pp. 852872, Aug. 2000.138 M. Brand, Understanding manipulation in video, in Proc. Int. Conf.Automatic Face and Gesture Recognition, 1996, pp. 9499.139 T. Wada and T. Matsuyama, Multiobject behavior recognition by eventdriven selective attention method, IEEE Trans. Pattern Anal. MachineIntell., vol. 22, pp. 873887, Aug. 2000.140 N. Johnson and D. Hogg, Learning the distribution of object trajectoriesfor event recognition, Image Vis. Comput., vol. 14, no. 8, pp. 609615,1996.141 N. Sumpter and A. Bulpitt, Learning spatiotemporal patterns forpredicting object behavior, Image Vis. Comput., vol. 18, no. 9, pp.697704, 2000.142 J. Owens and A. Hunter, Application of the selforganizing map to trajectory classification, in Proc. IEEE Int. Workshop Visual Surveillance,2000, pp. 7783.143 G. Herzog and P. Wazinski, Visual translator linking perceptions andnatural language descriptions, Artific. Intell. Rev., vol. 8, no. 2, pp.175187, 1994.144 T. Huang, D. Koller, J. Malik, G. Ogasawara, B. Rao, S. Russell,and J. Weber, Automatic symbolic traffic scene analysis using beliefnetworks, in Proc. National Conf. Artificial Intelligence, 1994, pp.966972.145 P. Remagnino, T. Tan, and K. Baker, Agent orientated annotation inmodel based visual surveillance, in Proc. IEEE Int. Conf. ComputerVision, 1998, pp. 857862.146 M. Mohnhaupy and B. Neumann, On the use of motion concepts fortopdown control in traffic scene, in Proc. European Conf. ComputerVision, 1990, pp. 542550.147 Proc. AAAI Fall Symp. Computational Models for Integrating Languageand Vision, R. K. Srihari, Ed.. Cambridge, MA, November 1995.148 P. Remagnino, T. N. Tan, A. D. Worrall, and K. D. Baker, Multiagentvisual surveillance of dynamic scenes, Image Vis. Comput., vol. 16, no.8, pp. 529532, 1998.149 I. Pavlidis, V. Morellas, P. Tsiamyrtzis, and S. Harp, Urban surveillancesystem from the laboratory to the commercial world, Proc. IEEE, vol.89, pp. 14781497, Oct. 2001.150 O. Javed, S. Khan, Z. Rasheed, and M. Shah, Camera handoff trackingin multiple uncalibrated stationary cameras, in Proc. IEEE WorkshopHuman Motion HUMO00, Austin, TX, 2000, pp. 113118.151 S. L. Dockstader and A. M. Tekalp, Multiple camera tracking ofinteracting and occluded human motion, Proc. IEEE, vol. 89, pp.14411455, Oct. 2001.152 R. T. Collins, A. J. Lipton, H. Fujiyoshi, and T. Kanade, Algorithmsfor cooperative multisensor surveillance, Proc. IEEE, vol. 89, pp.14561477, Oct. 2001.153 H. Tsutsui, J. Miura, and Y. Shirai, Optical flowbased person trackingby multiple cameras, in Proc. IEEE Conf. Multisensor Fusion and Integration in Intelligent Systems, 2001, pp. 9196.154 A. Mittal and L. S. Davis, M2 tracker a multiview approach to segmenting and tracking people in a cluttered scene, in Proc. EuropeanConf. Computer Vision, vol. 1, 2002, pp. 1836.155 J. M. Ferryman, A. D. Worrall, G. D. Sullivan, and K. D. Baker, Ageneric deformable model for vehicle recognition, in Proc. British Machine Vision Conf., 1995, pp. 127136.156 D. Cunado, M. S. Nixon, and J. N. Carter, Using gait as a biometricvia phaseweighted magnitude spectra, in Proc. Int. Conf. Audio andVideoBased Biometric Person Authentication, 1997, pp. 95102.157 , Extracting a human gait model for use as a biometric, in Proc.Inst. Elect. Eng. IEE Colloq. Computer Vision for Virtual Human Modeling, 1998, pp. 111114.158 J. M. Nash, J. N. Carter, and M. S. Nixon, Dynamic feature extractionvia the velocity Hough transform, Pattern Recognit. Lett., vol. 18, no.10, pp. 10351047, 1997.159 C. Y. Yam, M. S. Nixon, and J. N. Carter, Extended modelbasedautomatic gait recognition of walking and running, in Proc. Int. Conf.Audio and VideoBased Biometric Person Authentication, 2001, pp.278283.160 , Gait recognition by walking and running a modelbased approach, in Proc. Asia Conf. Computer Vision, Melbourne, Australian,2002, pp. 16.161 D. Cunado, J. Nash, M. S. Nixon, and J. N. Carter, Gait extraction anddescription by evidence gathering, in Proc. Int. Conf. Audio and VideoBased Biometric Person Authentication, 1999, pp. 4348.162 R. Tanawongsuwan and A. Bobick, Gait recognition from timenormalized jointangle trajectories in the walking plane, in Proc. IEEE Conf.Computer Vision and Pattern Recognition, 2001, pp. II726II731.163 H. Murase and R. Sakai, Moving object recognition in eigenspace representation gait analysis and lip reading, Pattern Recognit. Lett., vol.17, no. 2, pp. 155162, 1996.164 P. S. Huang, C. J. Harris, and M. S. Nixon, Human gait recognition incanonical space using temporal templates, Proc. Inst. Elect. Eng. IEEVision Image and Signal Processing, vol. 146, no. 2, pp. 93100, 1999.HU et al. A SURVEY ON VISUAL SURVEILLANCE OF OBJECT MOTION AND BEHAVIORS 351165 , Comparing different template features for recognizing people bytheir gait, in Proc. British Machine Vision Conf., 1998, pp. 639643.166 , Canonical space representation for recognizing humans by gaitor face, in Proc. IEEE Southwest Symp. Image Analysis and Interpretation, 1998, pp. 180185.167 J. D. Shutler, M. S. Nixon, and C. J. Harris, Statistical gait recognitionvia temporal moments, in Proc. IEEE Southwest Symp. Image Analysisand Interpretation, 2000, pp. 291295.168 L. Lee, Gait Dynamics for Recognition and Classification, MIT AILab, Cambridge, MA, Tech. Rep. AIM2001019, 2001.169 O. Javed and M. Shah, Tracking and object classification for automatedsurveillance, in Proc. European Conf. Computer Vision, vol. 4, 2002,pp. 343357.170 A. Bobick and A. Johnson, Gait recognition using static, activityspecific parameters, in Proc. IEEE Conf. Computer Vision and PatternRecognition, 2001, pp. I423I430.171 A. Johnson and A. Bobick, A multiview method for gait recognitionusing static body parameters, in Proc. Int. Conf. Audio and VideoBased Biometric Person Authentication, 2001, pp. 301311.172 A. Bobick and A. Johnson. 2001 Expected Confusion as a Methodof Evaluating Recognition Techniques. OnlineGVU Tech. Rep.GITGVU0110173 C. BenAbdelkader, R. Culter, and L. Davis, Person identification usingautomatic height and stride estimation, in Proc. Int. Conf. PatternRecognition, vol. 4, Qubec, Canada, 2002, pp. 377380.174 J. G. Lou, H. Yang, W. M. Hu, and T. N. Tan, Visual vehicle trackingusing an improved EKF, in Proc. Asian Conf. Computer Vision, 2002,pp. 296301.175 W. M. Hu, D. Xie, and T. N. Tan, A hierarchical selforganizingapproach for learning the patterns of motion trajectories, Chin. J.Comput., vol. 26, no. 4, pp. 417426, 2003.176 A. Bobick and J. Davis, The recognition of human movement usingtemporal templates, IEEE Trans. Pattern Recognit. Machine Intell., vol.23, pp. 257267, Mar. 2001.177 C. BenAbdelkader, R. Culter, and L. Davis, Motionbased recognitionof people in eigengait space, in Proc. Int. Conf. Automatic Face andGesture Recognition, Washington, DC, 2002, pp. 267274.178 R. Collins, R. Gross, and J. Shi, Silhouettebased human identificationfrom body shape and gait, in Proc. Int. Conf. Automatic Face and Gesture Recognition, Washington, DC, 2002, pp. 366371.179 C. BenAbdelkader and L. Davis, Detection of loadcarrying people forgait and activity recognition, in Proc. Int. Conf. Automatic Face andGesture Recognition, Washington, DC, USA, 2002, pp. 378383.180 B. Bhanu and J. Han, Individual recognition by kniematicbased gaitanalysis, in Proc. Int. Conf. Pattern Recognition, vol. 3, Qubec,Canada, 2002, pp. 343346.181 V. Laxmi, J. Carter, and R. Damper, Biologicallymotivated human gaitclassifiers, in Proc. IEEE Workshop Automatic Identification AdvancedTechnologies, 2002, pp. 1722.182 I. Robledo and S. Sarkar, Experiments on gait analysis by exploitingnonstationaryity in the distribution of feature relationships, in Proc. Int.Conf. Pattern Recognition, vol. 1, Qubec, Canada, 2002, pp. 14.183 C. BenAbdelkader, R. Cutler, and L. Davis, Viewinvariant estimationof height and stride for gait recognition, in Proc. Workshop BiometricAuthentication at European Conf. Computer Vision, 2002, pp. 155167.184 L. Wang, W. M. Hu, and T. N. Tan, A new attempt to gaitbasedhuman identification, in Proc. Int. Conf. Pattern Recognition, 2002,pp. 115118.185 L. Wang, H. Z. Ning, and W. M. Hu, Gait recognition based on procrustes statistical shape analysis, in Proc. IEEE Int. Conf. Image Processing, 2002, pp. III433III436.Weiming Hu received the Ph.D. degree from the Department of Computer Science and Engineering, Zhejiang University, Hangzhou, China.From April 1998 to March 2000, he was a Postdoctoral Research Fellow with the Institute of ComputerScience and Technology, Founder Research and Design Center, Peking University, Peking, China. SinceApril 1998, he has been with the National Laboratoryof Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China, as an Associate Professor. His research interests are in visualsurveillance and monitoring of dynamic scenes, neural networks, and filteringof Internet objectionable images. He has published more than 50 papers in national and international journals and international conferences.Tieniu Tan M92SM97F03 received the B.Sc.degree in electronic engineering from Xian JiaotongUniversity, China, in 1984 and the M.Sc., DIC, andPh.D. degrees in electronic engineering from Imperial College of Science, Technology and Medicine,London, U.K., in 1986, 1986, and 1989, respectively.He joined the Computational Vision Group,Department of Computer Science, The Universityof Reading, Reading, U.K., in October 1989, wherehe worked as Research Fellow, Senior ResearchFellow, and Lecturer. In January 1998, he returnedto China to join the National Laboratory of Pattern Recognition, the Instituteof Automation, Chinese Academy of Sciences, Beijing, China. He is currentlya Professor and Director of the National Laboratory of Pattern Recognition,as well as President of the Institute of Automation. He has published widelyon image processing, computer vision, and pattern recognition. His currentresearch interests include speech and image processing, machine and computervision, pattern recognition, multimedia, and robotics.Dr. Tan is an Associate Editor of Pattern Recognition and of the IEEETRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE and isthe Asia Editor of Image and Vision Computing. He serves as a referee for manymajor national and international journals and conferences. He was an electedmember of the Executive Committee of the British Machine Vision Associationand Society for Pattern Recognition 19961997 and is a Founding CoChairof the IEEE International Workshop on Visual Surveillance.Liang Wang received the B.Sc. degree in electricalengineering and the M.Sc. degree in video processingand multimedia communication from Anhui University, Hefei, China, in 1997 and 2000, respectively,and the Ph.D. degree in pattern recognition and intelligent system from the National Laboratory of Pattern Recognition, Institute of Automation, ChineseAcademy of Sciences, Beijing, China, in 2003.He has published more than ten papers in majorinternational journals and conferences. His currentresearch interests include computer vision, patternrecognition, digital image processing and analysis, multimedia, and visualsurveillance.352 IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICSPART C APPLICATIONS AND REVIEWS, VOL. 34, NO. 3, AUGUST 2004Steve Maybank received the B.A. degree in mathematics from Kings College, Cambridge, U.K., in1976 and the Ph.D. degree in computer science fromBirkbeck College, University of London, London,U.K., in 1988.He joined the Pattern Recognition Group, MarconiCommand and Control Systems, Frimley, U.K., in1980 and moved to the GEC Hirst Research Centre,Wembley, U.K., in 1989. During 19931995, he wasa Royal SocietyEPSRC Industrial Fellow in the Department of Engineering Science, University of Oxford, Oxford, U.K. In 1995, he joined the University of Reading, Reading, U.K.,as a Lecturer in the Department of Computer Science. In 2004, he became a Professor in the School of Computer Science and Information Systems, BirkbeckCollege. His research interests include the geometry of multiple images, cameracalibration, visual surveillance, information geometry, and the applications ofstatistics to computer vision.Dr. Maybank is a Fellow of the Royal Statistical Society and the Instituteof Mathematics and its Applications, and is a member of the British MachineVision Association and the Societe Mathematique de France.
