DISCOVERING CUES TO ERROR DETECTION IN SPEECH RECOGNITION OUTPUT     237Journal of Management Information Systems  Spring 2006, Vol. 22, No. 4, pp. 237270. 2006 M.E. Sharpe, Inc.07421222  2006 9.50  0.00.Discovering Cues to Error Detection inSpeech Recognition OutputA UserCentered ApproachLINA ZHOU, YONGMEI SHI, DONGSONG ZHANG, ANDANDREW SEARSLINA ZHOU is an Assistant Professor in the Department of Information Systems at theUniversity of Maryland, Baltimore County. She received a Ph.D. in Computer Science from Peking University, Beijing, China. Her current research interests center onontology learning, deception detection, natural language processing, and online community. Her work has been published in the Journal of Management InformationSystems, Communications of the ACM, IEEE Transactions on Speech and Audio Processing, IEEE Transactions on Systems, Man, and Cybernetics, IEEE Transactionson Professional Communication, Decision Support Systems, Information  Management, Group Decision and Negotiation, and others.YONGMEI SHI is a Ph.D. Candidate in Computer Science at the University of Maryland, Baltimore County, where she received an M.S. in Computer Science. Her research interests include natural language processing, speech recognition, informationretrieval, and machine learning.DONGSONG ZHANG is an Assistant Professor in the Department of Information Systems at the University of Maryland, Baltimore County. He received a Ph.D. in Management Information Systems from the University of Arizona. His current researchfocuses on mobile computing, multimediabased elearning, computermediated communication, and intelligent systems. His work has been published in the Communications of the ACM, IEEE Transactions on Multimedia, IEEE Transactions on Systems,Man, and Cybernetics, IEEE Transactions on Professional Communication, Decision Support Systems, Information  Management, Information Systems Frontier,Communications of the AIS, Journal of the American Society for Information Scienceand Technology, and others.ANDREW SEARS is a Professor of Information Systems and the Chair of the Information Systems Department at University of Maryland, Baltimore County. He receivedhis B.S. in Computer Science from Rensselaer Polytechnic Institute, Troy, New York,in 1988 and his Ph.D. in Computer Science from the University of Maryland, CollegePark, in 1993. He is a coeditor of The HumanComputer Interaction Handbook Fundamentals, Evolving Technologies and Emerging Applications Mahwah, NJ LawrenceErlbaum, 2003. His research explores issues related to humancomputer interactionwith recent projects investigating issues associated with mobile computing, speechrecognition, IT accessibility, and the difficulties IT users experience as a result oftheir work environment or tasks. Dr. Sears is currently serving as the Adjunct Chairfor Education for the Special Interest Group on ComputerHuman Interaction andthe Treasurer of the Special Interest Group on Accessible Computing.238     ZHOU, SHI, ZHANG, AND SEARSABSTRACT The great potential of speech recognition systems in freeing users handswhile interacting with computers has inspired a variety of promising applications.However, given the performance of the stateoftheart speech recognition technology today, widespread acceptance of speech recognition technology would not berealistic without designing and developing new approaches to detecting and correcting recognition errors effectively. In seeking solutions to the above problem, identifying cues to error detection CERD is central. Our survey of the extant literature onthe detection and correction of speech recognition errors reveals that the systeminitiated, datadriven approach is dominant, but that heuristics from human users havebeen largely overlooked. This may have hindered the advance of speech technology.In this research, we propose a usercentered approach to discovering CERD. Userstudies are carried out to implement the approach. Content analysis of the collectedverbal protocols lends itself to a taxonomy of CERD. The CERD discovered in thisstudy can improve our knowledge on CERD by not only validating CERD from ausers perspective but also suggesting promising new CERD for detecting speechrecognition errors. Moreover, the analysis of CERD in relation to error types andother CERD provides new insights into the context where specific CERD are effective. The findings of this study can be used to not only improve speech recognitionoutput but also to provide contextaware support for error detection. This will helpbreak the barrier for mainstream adoption of speech technology in a variety of information systems and applications.KEY WORDS AND PHRASES cues to error detection, speech recognition, taxonomy,verbal protocol analysis.SPEECH RECOGNITION IS ONE OF THE MAIN information technologies that provideusers with handsfree interaction with computers. The power of speech control promises to help many individuals who might otherwise not be able to interact with acomputer through the conventional mouse or keyboard interface. Particularly, userswho are physically challenged, visually impaired, or mobility impaired are offerednew opportunities by speech recognitionenabled applications.Although many interesting applications have emerged or enhanced with the advance of speech technology, the current use of such a technology reflects only a tipof the iceberg of the full power that the technology could potentially offer cf. 8, p.69. However, some fundamental and practical limitations of the technology resultin unsatisfactory performance of speech recognition systems. The convenience andefficiency promised by speech technology in interacting with computers is seriouslycompromised by the laborious efforts and frustration experienced in detecting andcorrecting recognition errors 40. Widespread acceptance of speech as a primaryinput modality for computers will not be possible unless the underlying recognitiontechnology can produce sufficiently robust and lowerror output 8.To bridge the gap between what people expect from speech recognition and whatthe technology can achieve, it is desirable to find effective ways to detect and evenDISCOVERING CUES TO ERROR DETECTION IN SPEECH RECOGNITION OUTPUT     239correct recognition errors. Error detection is the premise for error correction. Nomatter whether it is systems or users who are responsible for error detection, they relyon cues to error detection CERD. In the systeminitiated approach, CERD are extracted automatically from internal parameters or the output of speech recognition,and then used to build machine learning models using the training data 5, 47, 50.The machine learning models are then used to detect possible recognition errors andeven to improve speech recognition output e.g., 21, 47, 49. Thus, this approach ischaracterized as datadriven because CERD are selected primarily based on their impacton the training data and their automation potentials. This type of approach is efficient however, its effectiveness may be undermined by overlooking other significantCERD based on user experience and heuristics knowledge, which are neither automatically generated nor predefined.According to our survey of the related literature, the datadriven approach is dominant in identifying CERD. The ultimate goal of speech recognition is to approximatenative speakers recognition capability. Given the unsatisfactory performance of systemsinitiated error detection and the impact of human factors on information technology implementation 23, there is a strong need to learn CERD from users in orderto improve the usefulness of speech recognition systems. However, the extant studieshave failed to consider users as an important source for CERD. Therefore, the research question that we aim to address in this study is What types of CERD do usersapply in detecting speech recognition errorsWe propose a knowledgedriven, usercentered approach to discovering CERD inthis paper. Instead of relying on trialanderror, as shown in the systemsinitiated,datadriven approach, the proposed method uncovers important CERD from usersknowledge and experience. To the best of our knowledge, this is the first study todiscover CERD via a usercentered approach. An empirical study was designed andconducted to elicit verbal explanations from participants about strategies and cuesused to detect speech recognition errors, which were then interpreted and encodedvia verbal protocol analysis. This type of analysis lends itself to a taxonomy of CERD,consisting of linguistic, hypothesesbased, and other information. Moreover, the association between CERD and three types of speech recognition errorsinsertion,deletion, and substitution errorswas analyzed to gain insights into the effectivenessof CERD for specific types of errors. Furthermore, the result of correlation analysisbetween different CERD suggests that CERD strongly associated with one anothercan be used together to enhance the performance of error detection.The developed CERD taxonomy and related findings of this research can makemultifold contributions to the detection and correction of speech recognition errors.First, the taxonomy of CERD is the first of its kind. It helps advance our knowledgeon CERD and allows future expansion and refinement. Second, it complements CERDdiscovered by the traditional datadriven approach. Third, the analysis of CERD, inrelation to error types, allows us to contextualize CERD to maximize their utility.Fourth, the findings enable the development of supportive and contextsensitive environments to facilitate users in their detection of errors. They enhance the performanceof error detection systems with additional knowledge.240     ZHOU, SHI, ZHANG, AND SEARSBackground and Related WorkSPEECH RECOGNITION TECHNOLOGY HAS ADVANCED significantly and has had a tremendous impact on individuals and businesses in the past decade. However, a fundamental challenge that has been taking center stage is the detection and correction ofrecognition errors. Error detection aims to predict whether there is any mismatchbetween the output of a speech recognition system and the corresponding reference.Effective error detection relies on knowledge support, which is referred to as cues toerror detection.Benefits and Challenges of the Speech Recognition TechnologySpeech technology affects business and human life in ways that go far beyond offering an alternative input mode. It can serve as a tool to support contentbased retrievalof audio and audiovisual data 35 and information extraction 13. Business benefits derived from speech recognition include significantly lowered operational costsdecreased dropped calls improved business agility, customer satisfaction, and loyalty and increased productivity 8, 19, 30. Customer services supported by speechrecognition technology allow customers calls to be directed quickly and seamlessly.In addition, the speech recognition technology is especially attractive to the following groups of users People with physical or situationinduced e.g., onthemove impairment. Thosewho suffer from workplaceassociated repetitive stress maladies 2, carpal tunnel syndrome, spinal cord injuries 40, and other similar problems can greatlybenefit from this technology by improving their accessibility to computers throughthe use of their voices rather than a keyboardmouse. People who are deaf orhard of hearing are enabled to process information presented orally 3. Moreover, senior citizens, who experience reduction in their mobility, can access computers and applications at the rate of their speaking rather than typing. Students with learning disabilities. It is challenging for students with learningdisabilities to keep pace with other students in regular classroom settings. Several research projects have incorporated the speech recognition technology intoclassrooms 3, 18, 25. The spoken lecture could be simultaneously processedby a speech recognizer, which can serve as an alternative to traditional notetaking. Speech recognition has been proven to have a remedial effect on letterrecognition, word recognition, spelling, reading comprehension, phonologicalprocessing, and writing fluency 18, 25, which may otherwise be very laborious and frustrating. Doctors, lawyers, transcriptionists, and office personnel. Specialized speech recognition software enables more efficient generation and maintenance of the paperwork required in medical and legal fields 22. Users of mobile handheld devices. Interaction with handheld devices through asmall physical or soft keyboard is slow and error prone. Moreover, handhelddevices are often used while traveling, in which typing may be difficult due toDISCOVERING CUES TO ERROR DETECTION IN SPEECH RECOGNITION OUTPUT     241influences such as shaking 16. Multimodal mobile applications combine voiceand touch as input in order to enhance users experience 48. Operators of transportation systems. By preventing attention from being divertedby button or touch screen interaction with handsfree control, speech recognition technologies can improve the safety of vehicle drivers.Despite the numerous potential benefits of speech recognition technology and continuous research efforts, the accuracy of current speech recognition systems is still farinferior to humans recognition performance. Speech recognition is challenged bycooccurring ambient noise, continuous utterance, diversity in individual speakerspronunciation, outofvocabulary words, and so on. This has seriously hindered thewide adoption of the technology and its impact on society. No user would like to usea speech recognizer that does not guarantee acceptable levels of recognition accuracy. Given the above challenges in advancing fundamental speech technology, weidentified two promising directions for improving the usefulness and adoption of thistechnology 1 empowering a speech recognition system with the ability to automatically detect and correct errors, and 2 developing an information system to support users in error detection and correction, which would otherwise be a cumbersomeand timeconsuming process. Both directions can be pursued by advancing the stateoftheknowledge on CERD.Related WorkMinimizing recognition errors remains as a longstanding goal in speech recognitionresearch. CERD are essential to any solutions for detecting recognition errors, whichis shown in the extant work related to CERD e.g., 5, 34, 47, 50. Moreover, oursurvey of related studies reveals that the systeminitiated, datadriven paradigm iswidely adopted.Confidence MeasuresConfidence measures are referred to as a method for detecting hypothesized wordsthat are likely to be erroneous by estimating word and sentence correctness 14. Theresult of a confidence measure is denoted by confidence scores. They enable a speechrecognition system and subsequent modules to spot the positions of possible errors inthe system output automatically 46.In order to design a confidence measure, one should consider four factorsthelevel of confidence measure, error definition, predictors used and combination mechanism, and evaluation 6. A confidence measure can be computed at different levelssuch as phone 6, word 6, 26, concept 33, and sentenceutterance 32. It commonly takes into account the values of an array of predictors or CERD. CERD can beextracted from original models in a recognizer or from additional models, which arethen combined with either probabilistic or nonprobabilistic mechanism 46. In orderto compute posterior probabilities in the probabilistic approach, word lattice, a compact representation of alternative hypotheses 26, and nbest list of top hypotheses242     ZHOU, SHI, ZHANG, AND SEARShave been used 43. The majority of nonprobabilistic methods for confidence measures use selected features or CERD to build classifiers for predicting the correctnessof a hypothesis. Therefore, CERD are a key component of confidence measures.Cues to Error DetectionA variety of CERD have been exploited and incorporated into confidence measures inthe datadriven approach. According to their generality, CERD can be classified intotwo categoriesrecognizer independent and recognizer dependent 50. Recognizerindependent CERD are generic to different types of speech recognizers. For example,based on the speech recognition output, partofspeech information 41 can be generated via linguistic analysis. Other types of linguistic information that have served asCERD include syllable, content words 41, parsing mode i.e., whether a word can beparsed by the grammar and the specific slot position of a parsed word 49, probability of nodesarcs assigned in a parse tree, scores from semantic structured languagemodels 37, discourse information 41, and so on. Moreover, linguistic informationin the local context has been employed to detect errors 9, 37. In Sarikaya et al. 37,for example, various context lengths were factored into the computation of semanticstructured language model scores. Another group of generic cues come from intermediate parameters generated by a speech recognizer such as acoustic model scores 36,49, language model scores 36, 44, and posterior word probabilities 20, 44, 46.Acoustic models are developed to represent audio signals and language models areused to predict the probability of a word based on previous words. Posterior wordprobability is found to be the best single feature among the internal parameters of aspeech recognizer 46. Moreover, confidence scores of words in the immediate neighborhood influence the probability of the current word being an error 17.The availability of recognizerdependent CERD is contingent upon specific speechrecognizers. For example, speech recognition output can be presented in several alternative formatsbest hypothesis, nbest list, and word lattice. Alternative hypotheses e.g., quickly, quietly, quirkily are available in an nbest list and a word latticebut not in the best hypothesis. If an output word appears rarely in topn alternativeutterance hypotheses, that word is likely to be an error. Thus, path ratios the ratiobetween number of paths containing a word and the total number of paths 7, 15, 36are also employed as CERD.The above work contributes to our understanding of possible CERD as well as ourdesign of an empirical study for discovering CERD from users.A UserCentered Approach to Discovering CERDDespite the notable effect of CERD on improving speech recognition output, as discussed in the previous section, error detection and correction remains as a bottleneckin improving the productivity of speech recognition technology. Two avenues foradvancing error detection or correction are identified through our survey of relatedwork. One is to advance the body of knowledge on CERD by learning from humanDISCOVERING CUES TO ERROR DETECTION IN SPEECH RECOGNITION OUTPUT     243users. The other is to improve the underlying approaches to confidence measures. Abetter understanding of CERD is conducive to the effectiveness of confidence measures. Therefore, we focus on the first issue in this research.As discussed in the introduction, the usercentered approach can potentially overcome the limitations of the systemsinitiated, datadriven approach. Although somestudies involved human users in error detection, the focal interest of those studies wasto either improve the usability of speech recognition systems by building multimodalinterfaces 42 or compare user performance in error detection in different settings41. Tacit CERD that users apply in error detection and correction remainunderexplored. The above situations are accounted for by several factors 1 it islabor intensive to elicit knowledge from users, 2 the rapid advance in powerfulcomputer technologies has cultivated the tendency to ignore users roles, and 3 thereis a gap between what each research community e.g., speech technology and humancomputer interaction wants and what it can deliver.Previous work that bears close relevance to this research was done by Brill et al. 4,who aimed to improve the stateoftheart language modeling by incorporating moresophisticated linguistic and world knowledge from people. Several major limitationsof the study were 1 the scope of CERD was constrained to linguistic knowledgeonly 2 a list of possible CERD was precompiled for participants to choose from,which was essentially a CERD validation process rather than an elicitation process,possibly limiting the type of information that users apply in identifying errors and3 the CERD obtained were at a coarse granularity e.g., argument structure, whichare difficult to directly apply in practice.To advance our knowledge of CERD, we designed a usercentered approach andexecuted it in an empirical study. In a knowledgedriven, usercentered approach,CERD are elicited from users when they detect and interpret speech recognition errors in real time. Supplementary data e.g., alternative hypotheses were also provided to increase the chance of discovering useful knowledge. Based on the relatedwork on CERD, we selected the following types of data to support error detection inthis study Speech recognition output is a rich source of CERD. It enables the applicationof linguistic knowledge and contextual information in error detection as in database applications 10. Alternative hypotheses and associated information serve as additional referencesfor judging the correctness of a recognized word. In light of the merit of discourse information in error detection, backgroundscenario information about the original speech is likely to be useful in errordetection. Nonetheless, existing discourse CERD e.g., previous dialog act 41are designed specifically for spoken dialogue rather than monologue applications such as dictation. New discourse CERD are required to support error detection in dictation.We did not include parameters of internal models of a speech recognizer to supportuser error detection because 1 they have already been incorporated while generating244     ZHOU, SHI, ZHANG, AND SEARSthe recognition output, 2 they are rarely accessible in a commercial recognition system, and 3 they are difficult for nonexpert users to use. This study aims to not onlytest the effectiveness of extant CERD for user error detection but also discover newCERD that can improve error detection by both users and systems.Research MethodologyWE CONDUCTED LABORATORY EXPERIMENTS to discover CERD that users apply indetecting speech recognition errors. In order to collect data about participants decisions and reasoning, the thinkaloud protocol 11 was employed during the experiments to let participants provide explicit explanations about their decisions on errors29. A pilot test involving four participants was conducted to ensure correctness andclarity of experiment instructions and procedure. Minor research design modifications were made based on our observations and the results of the pilot study, as wellas interviews with those participants.ParticipantsTen undergraduate students were recruited for this study from a midsized universityon the east coast of the United States. They were all native English speakers and noneof them was a professional editor. These participants were sophomores, juniors, andseniors from eight different majors. Sixty percent of the participants were female.They were told that the research study would take about two hours and each participant would be paid 20 for his or her participation.TaskThe experimental task mainly consisted of two partsdetecting speech recognitionerrors and providing possible explanations for every error detected. The goal of theerror detection was to identify the discrepancy between words in the transcripts generated by a speech recognition system and the original speech rather than polish thelanguage.Eight transcripts were selected and presented in three different formatstext actual speech recognition outputonly, text with alternative hypotheses, and text withboth alternative hypotheses and background information. This helped us evaluate themerit of supplementary information to error detection. Alternative hypotheses included topthree alternative word hypotheses and topnine alternative utterance hypotheses along with their confidence scores. Background information was representedwith task scenarios that were originally used to elicit the speech. The transcript distribution for three different formats was 332. The text transcripts were randomly sortedfor each participant to counterbalance the potential ordering effect. Moreover, transcripts for the background information setting were extracted from different task scenarios to avoid carryover effects.DISCOVERING CUES TO ERROR DETECTION IN SPEECH RECOGNITION OUTPUT     245Each participant was asked to provide more than one piece of evidence to supporteach of the identified errors. It was recommended for each participant to seek evidence from multiple perspectives without the assistance of specific CERD. This wouldallow us to acquire knowledge that was actually applied by participants to detecterrors and, more importantly, that can help us discover new CERD.Transcript DataThe transcripts were randomly extracted from a dictation corpus that was generatedby a commercial speech recognition system under highquality conditions from thespontaneous speech of 27 speakers 12, 39. All of the speakers were native, but notprofessional, English speakers.We used two criteria for selecting transcript datarecognition accuracy and transcript length. The recognition accuracy of the speech corpus, 84 percent, was chosenas the empirical error rate. A qualified text transcript should be neither too short toprovide necessary context information for error detection, nor too long to be completed within the given amount of time. Based on our experience and observations, 90words were selected as the target length. The descriptive statistics of selected transcripts are shown in Table 1. The two columns in the middle, number of words in theoutput and number of words in the reference, represent actual speech recognitionoutputs and corresponding manually corrected reference transcripts, respectively. Eachtranscript was prepared in all three formats, as described in the previous section.ProcedureBefore their arrival, participants were asked to preview two online documents introducing error annotation schemes and examples of text transcripts and other relatedinformation e.g., alternative hypotheses and confidence scores. During the experiment, each participant first took a pretest to assess his or her knowledge of the annotation schemes and comprehension of data that would be presented in the transcripts.If a participant made mistakes on one or more questions, he or she would be asked toredo the questions after reviewing the related online documents. Then, the participantmoved on to read instructions and analyze the text transcripts in a given sequence. Allthe text transcripts were presented in hard copy workbooks, on which a participantcould mark and annotate errors. In addition, the participant was asked to think andexplain aloud by providing justification explanations whenever an error was detected,which were recorded with a digital voice recorder. After completing all the texttranscripts presented in each formats, each participant was asked to fill out a shortquestionnaire about his or her perception of the process and the results of errordetection.A verbal protocol analysis relies on collecting information about the course andmechanisms of cognitive processes of the internal states of the problem solvers 11,29. The thinkaloud instruction used in the experiment was Say out loud everyjustification that passed through your mind for explaining each error as you detect it.246     ZHOU, SHI, ZHANG, AND SEARSTable 1. Statistics of the Selected TranscriptsNumber ofNumber of wordsNumber of wordsNumber ofRecognitionTranscriptssentencesin the outputin the referenceerrorsaccuracyA471691584.1B690882080.7C385753673.3D577831878.3E41111092085.3F51001092280.7G51051071884.1H483811682.7DISCOVERING CUES TO ERROR DETECTION IN SPEECH RECOGNITION OUTPUT     247Participants were encouraged to think of more than one justification to acquire morethorough protocols. The contents obtained from the thinkaloud method and immediate retrospective responses are valid.Data Analyses and ResultsIN THE FIRST STEP OF DATA ANALYSIS, verbal protocols recorded from each participant during the error detection process were segmented into the units of text transcripts. Content analysis was carried out on the segmented verbal protocols by twoindependent coders to interpret and encode them. An interrater comparison was conducted to validate the encoding results. Finally, CERD that emerged from the verbalprotocol analysis were illustrated and their effectiveness was reported.Data EncodingTwo coders were recruited to encode the recorded explanation independently. Theirtasks were to listen to participants explanations recorded in the audio files and interpret them while referring to the errors annotated in the completed workbooks. Basedon their interpretation results, the coders filled out a coding worksheet in Excel seeTable 2 to record coding results separately. Particularly, coders were asked to fill outthe parts formatted in italics in Table 2 by specifying transcript AH, sentence,error, error type I insertion D deletion S substitution, a list of cues cue 1 andcue 2 were placeholders for specific cues, and their presence i.e., Y for specificerrors. Such a process was repeated for every text transcript completed by each participant. The average independent coding time was five times of the audio length pertext transcript per coder. Thus, the data encoding was a timeconsuming and laborintensive process.Interrater ComparisonsTo examine potential bias, the reliability of cues generated by the two independentcoders was tested. There were a total of 1,275 errors for which cues were independently encoded by each coder. The employed thinkaloud method was conducive toeliciting multiple cues for detecting each error from the participants. The number ofcues per error ranged from 1 to 7, with a mean of 2.22 and a standard deviation of 1.0.The text descriptions of cues in the encoding results were first normalized by reducing nouns to their singular forms, removing articles, transforming verb phrases to nounphrases, and so on. Then, all the cues were sorted in alphabetical order. The same cueswere merged, and each distinctive cue was assigned with a unique identifier.Given that there could be more than one cue category encoded for each error andsome cues encoded by coder A did not appear in the list of cues encoded by coder B,and vice versa, Cohens kappa, a popular measure for interrater reliability, was notappropriate for this type of qualitative data. Thus, we developed metrics to measure248     ZHOU, SHI, ZHANG, AND SEARSthe overall agreement on the separately encoded cues between coders A and B, whichis shown in formulas 1 and 2 or 1 and 2. p t s ip t s ieN, , ,consensusoverall agreement1 p t s i p t s iA Bp t s i C Ce, , , , , ,, , ,1 if ,consensus0 otherwise  2 p t s i p t s iA Bp t s i p t s i p t s iA BC CeC C, , , , , ,, , , , , , , , ,consensus ,2where N is the total number of errors identified by all the participants, ep,t,s,i uniquelydenotes error i identified by participant p in sentence s of transcript t, and CAp,t,s,i orCBp,t,s,i denotes a set of cues encoded for error ep,t,s,i by coder A or B. It is noted thattwo alternative formulas were provided for computing the consensus on the codingresults for ep,t,s,i. Formula 2 concerns whether there are any overlapping cues to ep,t,s,ibetween two coders, whereas formula 2 takes into account the percentage of cuesshared by the two coders. The latter is more conservative than the former.The resulting overall agreement was about 62.0 percent and 43.5 percent based onformulas 2 and 2, respectively. The relatively low agreement rates were attributable to several reasons 1 each coder had his or her own preference in choosingwords to represent specific cues, 2 the similarity between cues that were represented at different levels of granularity were not factored into the comparison, and3 different cues were encouraged because the goal of this research was to discovernew knowledge from users rather than validate existing knowledge. After mappingleaf node cues to an upper level according to the hierarchy of CERD, which will beintroduced in the next section, the overall intercoder agreement reached 71.5 percent and 55.0 percent based on formulas 2 and 2, respectively. Given the complexity of the encoding situation, especially the large number of categories and thehigh frequency of multicategory encodings, we believe that the agreement rates werereasonable.Table 2. A Sample Coding WorksheetTranscript A A . . .Sentence 1 1 . . .Error 1 2 . . .Error type D I . . .Cue 1 Y . . .Cue 2 Y . . .. . . . . .DISCOVERING CUES TO ERROR DETECTION IN SPEECH RECOGNITION OUTPUT     249The disagreements in the encoded cues for individual errors are further investigatedby the first author and addressed by either consolidation or discussion with the coders. The following two scenarios were resolved by consolidation If two cues had overlap, the nature of their relationship was further analyzed.For example, if it was a genericspecific relationship e.g., incompatible semantics between two constituents versus incompatible semantics between subjectand predicate, the more generic cue would be discarded. If two cues were disjointed and contradictory with each other e.g., partofspeech confusion versus openclass word choice, the corresponding verbal protocol would be examined by the first author and one of the cues would beeliminated.For the remaining inconsistent results, two coders would revisit the original audiofiles, discuss their discrepancy under the facilitation of the first author, and reach aconsensus or choose the majority opinion. As a result, cues that were found to becomplementary to each other were kept, different expressions of the same cues weremerged, and cues resulting from the coders misinterpretation were dropped. Finally,53 cues were selected for the final list of CERD.A Taxonomy of CERDAn analysis of the discovered CERD revealed that some of cues were relevant to eachother. Drawing upon the literature from linguistics, natural language processing, andspeech recognition, we clustered different CERD based on the closeness of their relationships and developed a taxonomy of CERD using the bottomup approach. Thetop two levels of the CERD hierarchy are shown in Figure 1. CERD were first groupedinto three categorieslinguisticsbased, hypothesesbased, and others. The linguisticsbased CERD were further divided into phonological, morphological, syntactic,semantic, and discourse types. The hypothesesbased CERD included both word andutterance hypotheses. Others type of CERD contained adjacent errors, language style,and unnecessary repetitions.Each leaf node in Figure 1 can be further expanded into subcategories. The fulltaxonomy of CERD is provided in the Appendix. For the sake of space, we only usedthe syntactic node for illustration purpose. According to the fully expanded hierarchyshown in Figure 2, the syntactic CERD consist of phrase structure and sentence structure, and the phrasestructure CERD are further decomposed into parallel structure,modifier, and so on.To uniquely identify CERD in the taxonomy, we represented the top two levels ofCERD with boldface letters in Figure 1 and delineated the lower levels of CERD withsequential numbers in Figure 2. Moreover, the identifier of a CERD at a lower level iscreated by attaching the corresponding delineating letter or number to the identifier ofits parent CERD using a period as the delimiter. For example, LG stands for linguisticsbased syntactic CERD and LG.1.2 stands for parallel structure, where 1 denotes the first child of LG, and 2 denotes the second child of the phasestructure250     ZHOU, SHI, ZHANG, AND SEARSCERD. Due to space constraints, the rest of the discussion focuses on the CERD in thethird layer. Detailed descriptions and examples of these CERD are given in Table 3.Since all the languagestyle CERD were mainly induced by userintroduced rather thansystemgenerated errors, they were grouped together and referred to as OS in Table 3.Descriptive Statistics of CERDOur analysis of the encoding results showed that some CERD were used more frequently than others. To evaluate the effectiveness of individual CERD, we neededmetrics that could take two factors into consideration 1 the measurement should bepositively proportional to the usage frequency of CERD, and 2 the measurementFigure 1. A Taxonomy of CERD Top Two LevelsFigure 2. The Hierarchy of Syntactic CERDDISCOVERING CUES TO ERROR DETECTION IN SPEECH RECOGNITION OUTPUT     251Table 3. Descriptions and Examples of CERDIDsCERDDescriptionsExamples1HU.1Utterance hypothesesAlternative hypotheses at the utterance level.easier than I had expected 14 6 5 5 40An utterance is a continuous acoustic sequence.easier than I have expected 14 6 5 3 40HU.2Utterance lengthThe length of utterance in which an output word appears.easier than I had expected 14 6 11 5 40HU.3Path ratioRatio between number of paths containing a word andThe lengths of the above three utterancetotal number of paths.hypotheses are equal to 5, and the path ratio ofhad is 67 percent 23.HW.1Word hypothesesAlternative word hypotheses.quickly 33 quietly 9 quirkily 1HW.2Confidence scoresWord confidence scores.There are three alternative word hypotheses inHW.3Highest confidenceWhether a word has the highest confidence scorethe above list. The confidence score of quickly isscoreamong all the alternatives.33, which is the highest among the three.LD.1Out of contextIncompatibility between the current word and theThis game is likely held in inland because that issurrounding discourse.where John is from....Also, John Wild lived in London for most of his life.LD.2CoreferenceInconsistency between the reference with the referentEven though I do not know very much about truck,in the discourse.I feel that I have a good idea of what kind ofemployee he is.LD.3ContradictoryContradiction between the meaning of a word andI have many questions and often can decideinformationthe discourse.between two conflicting ideas and he was one ofthe few people always available to give ideas.LD.4Background scenarioBackground scenarios used to elicit the original speech.Describe a book that you are going to write. continues252     ZHOU, SHI, ZHANG, AND SEARSTable 3. ContinuedIDsCERDDescriptionsExamples1LG.1Phrase structureUngrammatical phrase structure, including phrase head,Even though Paul received his Ph.D. in computerparallel structure, incomplete phrase, modifier, andscience, he is really interested in sell issues ofcommon term.information systems.LG.2Sentence structureUngrammatical sentence structure, including illegalHe pick up the kids from school everyday.beginningending of a sentence, numbertensevoiceI reserved a 2 for my mom for a Broadway show.aspectmodalperson disagreement, dangling words,subject, object, or incomplete sentence.LM.1PartofspeechMisuse of a word of a different partofspeechThe fact that they all have common subjectsconfusione.g., determiner, preposition, pronoun, conjunction,makes them close related.and verb.LM.2Openclass wordThe choice of a wrong content word such as noun,I am able to add very personable with clientschoiceverb, adjective, or adverb.and develop a trusting relationship with them.LM.3Closedclass wordThe choice of a wrong function word from partsofI would like to continue working on this productchoicespeech such as proposition and determiner.and the coming year.LM.4Nonword choiceProblem with choosing punctuation marks, includingI would like to give feedback about Susan andmissing, extraneous, or wrong punctuation marks this name is fictitious because... would indicatesuch as commas and parentheses.the specific individual.LP.1DisfluencySpeech disfluency such as false start, repetitions,I went to medical school in 1970s but a I did notrepairs, and filler words.finish it.LP.2Phonetic similarityA word mistakenly used due to sharing similar soundPeter is a very sociable parson, however, he triesto another word.to focus on too many things at the same time.LP.3Word splitOne word being split into more than one word.The conference that I would like to attend is the WorldFinancial conference held in London and when.DISCOVERING CUES TO ERROR DETECTION IN SPEECH RECOGNITION OUTPUT     253LS.1Making no senseA word does not make sense in the sentence.In particular, Janet kilns and move will be travelingwith me.LS.2IncompatibleIncompatibility in the meaning between two sentenceI could purchase a new pressure.semanticsconstituents, including subjectobject, prepositionobject,Some of diary food must be refrigerated such asmodifierhead, subjectpredicate, and predicateobject.eggs that can go back.OE.1Preceding errorThe preceding word is an error.In reviewing one of my peers, and cited to choosePeter because it was only until this week that Ireally got to know him.OE.2Following errorThe following word is an error.The travel arrangements 2 couldnt for thisconference is I would like to take a cruise linerfrom Baltimore to London.OR.1RedundancyRepeat the same word more than once in a row.I was very happy to to meet an old friend who justvisited Baltimore.OSLanguage styleLack of conformance to traditional writing style suchit causes my entire paper to be lost.as using wrong letter cases, contractions, anddouble negations.Notes 1  words shown in italics are recognition errors 2 a deletion error.254     ZHOU, SHI, ZHANG, AND SEARSshould reflect the chance that certain CERD were applicable. For example, alternative hypotheses were available in the alternative hypotheses format but not in the textonly format in our experiment. As a result, we adapted the concept of support fromthe association rule mining to measure the effectiveness of CERD. Support was defined as the ratio of the number of errors for which a CERD was actually used dividedby the total number of errors for which that CERD could be used. The levels ofsupport for the CERD are reported in Table 4 in descending order.There were two types of support valuesall and correct. The former was based onall the detected errors, whereas the latter was based on the errors detected correctly. Itis shown in Table 4 that the rankings of CERD appeared to be consistent betweenboth types of support for the majority of CERD except OS.3 and LM.4, which wereranked much lower for correct support than for all support. This is because we did notconsider letter case and punctuation as the sole explanation for a recognition error.Therefore, discussion in the next section focuses on all support.DiscussionOUR IMPLEMENTATION OF THE USERCENTERED APPROACH to discovering CERD resulted in a taxonomy of CERD. The taxonomy provides broad implications and benefits to both research and practice in error detection, including generic error detection,contextsensitive error detection and correction, and knowledge support for error detection.Implications to Generic Error DetectionThe taxonomy allows researchers to investigate CERD in a systematic way and guidesfuture research and practice in error detection. As shown in Table 4, LS.1 making nosense received the highest support among all the CERD. LS.2 incompatible semantics was also well supported. The former represents the cases in which a word doesnot fit in a sentence or is irrelevant to the meaning of the sentence. The latter represents semantic mismatches between two sentence constituents e.g., subjectobjectand modifierhead. Both CERD suggest that semantic analysis, a process to determine what each word means and what a sentence means when individual words arecombined with each other, is crucial to detecting semantic anomalies caused by speechrecognition errors. Moreover, word cooccurrence analysis could be helpful to determine whether or not a word makes sense in a sentence.LG.1 phrase structure and LG.2 sentence structure were next after LS.1 in Table 4.They suggest that syntactic information, including parallel structures modifierheaderrelationships sentence completeness and number, person, tense, and voiceagreements and so on, is indispensable to detecting speech recognition errors. Whether ornot a word can be parsed and the probability of a word to be parsed are good syntacticsemantic indicators 37, 49. The stateoftheart technologies for syntactic parsingDISCOVERING CUES TO ERROR DETECTION IN SPEECH RECOGNITION OUTPUT     255Table 4. CERD and Their Levels of SupportSupportSupportIDsCERDAllCorrectIDsCERDAllCorrectLS.1Making no sense0.3160.360LP.2Phonetic similarity0.0210.026LG.1Phrase structure0.2590.263LD.4Background scenario0.0190.022LG.2Sentence structure0.1640.186HU.1Utterance hypotheses0.0180.019HW.1Word hypotheses0.1540.180HU.2Utterance length0.0150.015HW.2Confidence scores0.1490.175OE.1Preceding error0.0130.008LM.1Partofspeech confusion0.1010.121LP.3Word split0.0120.015LS.2Incompatible semantics0.0820.102LM.4Nonword choice0.0120.003HW.3Highest confidence score0.0800.092OS.4Misplacement0.0090.006LP.1Disfluency0.0610.059OE.2Following error0.0060.004OS.3Letter case0.0570.013LD.2Coreference0.0050.006LD.1Out of context0.0540.057HU.3Path ratio0.0040.005LM.2Openclass word choice0.0510.058OS.2Double negation0.0020.003OR.1Redundancy0.0460.032LD.3Contradictory information0.0020.002LM.3Closedclass word choice0.0350.029OS.1Contraction0.0010.000Note Boldface figures refer to lowered rankings for the CERD when switching from all support based on all the detected error to correct support based on correctlydetected errors only.256     ZHOU, SHI, ZHANG, AND SEARSare relatively mature, which could play a significant role in the detection of recognition errors.HW.1 hypotheses, HW.2 confidence scores, and HW.3 highest confidence scorewere among the best supported hypothesesbased CERD. They reveal that, in general, alternative word hypotheses and their confidence scores are helpful to users indetecting speech recognition errors. They can be used to discriminate top hypothesesand to infer that a word is possibly wrong if its confidence score is much lower thanthose of other alternative words 44. This implies that enhancing confidence measures is important to improving error detection.LM.1 partofspeech confusion follows HW.2 in terms of the level of support.Partofspeech confusion may lead to illformed sentences. Consequently, traditionalparsing technologies become less effective in dealing with sentences containing sucherrors and can even fail to produce an output. Therefore, a robust natural languageparser that can indicate where a sentence breaks would be extremely valuable.Among all the phonological CERD, LP.1 disfluency received the highest support.During spontaneous dictation, a participant must plan for the next sentence whilespeaking. Thus, disfluencies such as false start become useful CERD to identify errors in continuous speech such as dictation, meeting conversation, and presentation.The approaches to automatic disfluency detection have utilized prosodic information, statistical word language models, syntactic structures, textual information, andlexical features 24. Nevertheless, this line of research is still at an early stage. Giventhat acoustic processing is insufficient to solve disfluencies, postprocessing would benecessary to reduce the ambiguity inherent in a single knowledge source. For example, repairs and false start may be detected via linguistic means.OS.3 letter case was best supported among others type of CERD. Many speechrecognizers either generate words only in lowercase or only partially address theletter case problem. This is largely attributed to the difficulty associated with detecting sentence boundaries. Although, in this study, the support of letter case was muchlower for correct detection than all detection, it is shown that the letter case affects theusability of a speech recognition system. For example, it is uncommon to start asentence with a lowercase and capitalize some common words within a sentence.Such problems may be addressed by sentence boundary identification and name entity recognition techniques, respectively.LD.1 out of context was the best supported discourselevel CERD. It indicatesthat other adjacent sentences provide a useful context for detecting errors in the current sentence. Sometimes it is necessary to look for useful CERD beyond one sentence. Unlike domainspecific dialogue systems or taskoriented recognition systems,the discourse information obtained from large vocabulary dictation recognition isless structured and predictable. Nonetheless, models and theories on the rhetoric structure of text 28 from discourse processing and computational linguistics fields canguide the effort on incorporating contextual information into error detection.It is shown in Table 4 that OR.1 redundancy turned out to be a concern for aspeech recognition system. This is partly due to the nature of dictation, in which aspeaker tends to repeat previous words when a recognition system does not transcribeDISCOVERING CUES TO ERROR DETECTION IN SPEECH RECOGNITION OUTPUT     257them immediately or when the output is wrong. Moreover, repetition of the samepunctuation marks in a consecutive sequence in dictation appeared to be atypical tothe participants.LM.2LM.4 refer to choosing a wrong 1 function word e.g., conjunction, 2 content word e.g., noun, or 3 nonword string e.g., punctuation, respectively. Thefirst two were supported better than average, but the last one was not supported well.In all three cases, a wrong word was chosen from the same grammatical category ora wrong punctuation mark was selected. Different techniques are required to dealwith different kinds of words. For example, to detect the problem of wrong contentword choice, semantic or discourse processing could help examine whether the wordfits in a phrase, a sentence, or even a discourse. In addition, the problem of wrongfunction word choice may be addressed with corpusbased analysis of the collocationof function words and content words or dictionarybased pattern matching.Those CERD whose levels of support were greater than 0.01 and less than 0.03included LP.2 phonetic similarity, LD.4 background scenario, HU.1 utterancehypotheses, HU.2 utterance length, OE.1 preceding error, and LP.3 word split.Differentiating homonyms consisting of similar phones is a fundamental challenge inspeech recognition. The problem of phonetic similarity is exacerbated in recognitionerrors that cut across word boundaries. This is rooted in incorrect segmentation ofacoustic signal sequence of continuous utterance as well as ngram language modelsthat are widely adopted in speech recognition engines. A speech recognition errormay generate a domino effect and result in further errors in subsequent words 7. Itimplies that correctly detecting one recognition error may lead to the successful detection of other errors in adjacent words. In our study, background scenarios werefound to play a notable role in error detection by providing discourse informationabout the text transcript, which can be especially useful in situations where the thirdparty is involved. Like word hypotheses, utterance hypotheses and utterance lengthcan also signal recognition errors. Longer utterances provide more context for detecting an erroneous word.The next group of CERD whose support was at least 0.005 but no more than 0.01included OS.4 misplacement, OE.2 following error, and LD.2 coreference. Theremaining CERD that were least supported in this study included HU.3 path ratio,OS.2 double negation, LD.3 contradictory information, and OS.1 contraction.Most of the above CERD deal with writing style issues or involve deep understandingof discourse or hypotheses information, and thus were not well supported.Implications to ContextSensitive Error Detection andCorrectionIntuitively, different types of speech recognition errors i.e., deletion, insertion, andsubstitution require different CERD. Possible associations between error types andCERD can support contextaware error detection and correction by users or systems.On one hand, if specific CERD are applicable to certain words, they will not onlyfacilitate detecting possible errors but also suggest how to correct errors. On the other258     ZHOU, SHI, ZHANG, AND SEARShand, once an error type is specified, error correction can be better guided by a recommended subset of relevant CERD.An analysis of error distribution in the text transcripts used in this study showedthat substitution, insertion, and deletion errors accounted for 62.9 percent, 18.3 percent, and 18.8 percent, respectively, as shown in Figure 3. This distribution was veryclose to the error distribution in the entire speech corpus substitution 63.7 percentdeletion 18.7 percent insertion 17.6 percent. Apparently, substitution was the predominant type of error.The error type distribution is echoed in the distribution of CERD in terms of errortypes, as shown in Table 5. For example, CERD such as HU.3, LD.2LD.4, LM.2,LP.3, and OS.1OS.3 were used solely to detect substitution errors CERD such asHU.1, HW.1, HW.3, LM.3, LP.2, and LS.2 were used predominantly  80 percent todetect substitution errors. These findings suggest that hypothesesbased, stylerelated,and most discourse CERD are particularly useful for detecting substitution errors.They also confirm our anticipation that CERD concerning word choice issues e.g.,LM.2 and LM.3, phonetic similarity i.e., LP.2, and semantic incompatibility i.e.,LS.2 are effective for handling substitution errors.Some CERD, including HU.2, HW.2, LD.1, LS.1, and OS.4, were used primarilyfor substitution errors and occasionally for insertion errors. This corroborates theprevious finding that hypothesesrelated CERD are helpful to detect substitution errors. In addition, when an output word makes no sense or is perceived to be out ofcontext or misplaced, the word can be corrected mostly likely by replacing it with adifferent word or sometimes by deleting it. LG.1, LG.2, and LM.1 were mostly usedto address substitution errors and sometimes deletion and insertion errors.It was interesting to observe that, although both OE.1 and OE.2 were associatedwith substitution and deletion errors, OE.1 was more commonly used in detectingsubstitution errors and OE.2 was more popular in detecting deletion errors. The formerFigure 3. Distribution of Error TypesDISCOVERING CUES TO ERROR DETECTION IN SPEECH RECOGNITION OUTPUT     259Table 5. Results of Association of CERD with Error TypesAssociationsSubstitutionInsertionDeletionStrongHU.3, LD.2, LD.3, LM.2, LP.3, LD.4,LM.4, OR.1OE.2  60 percentOS.1, OS.2, OS.3HU.1, HW.1, HW.3, LM.3, LP.2, LS.2HU.2, HW.2, LD.1, LS.1, OS.4LG.1, LG.2, LM.1MediumOE.1, LP.1HU.2, HW.2, LD.1, OS.4, LS.1, LP.1LM.4, OE.1  30 percentWeak  10 percentOE.2LG.1, LG.2, LM.1, OE.1LG.1, LG.2, LM.1, LP.1  is the probability of the CERD used for the error type.260     ZHOU, SHI, ZHANG, AND SEARSwas also occasionally used for insertion errors. Insertion errors accounted for themajority of the cases handled with LM.4 nonword choice and OR.1 redundancy.As a result, nonwords and redundant words are most likely to be removed in errorcorrection. LM.4 was also used for deletion errors occasionally to indicate missingpunctuation. LP.1 was more likely to pinpoint substitution and insertion errors thandeletion errors. This indicates that the output due to disfluency noise is likely to becorrected by word replacement or elimination.In sum, more CERD were strongly associated with substitution errors than insertion and deletion errors. Since many CERD tend to be associated with a specific errortype, the analysis of the relationship between different CERD may provide additionalinsights into error detection, which will be discussed in the next section.Implications to KnowledgeSupported Error DetectionBy analyzing the associations between different CERD, we will be able to empowerusers or systems with knowledge to help them make more informed and better judgments in error detection. This will also lead to the development of knowledgebasedsupport systems 31 user error detection.We used the Jaccard coefficient 1, a popular metric for querydocument matchingin information retrieval, to measure the association between different CERD mainlyfor two reasons 1 the Jaccard coefficient measures asymmetric information on binary variables CERD are represented as binary variables, and 2 a pair of CERDusually cooccur in a small number of errors and coabsent in a large number oferrors. Unlike traditional correlation coefficients, the Jaccard coefficient helps remove doubleabsence cases that make little contribution to the association betweentwo CERD. The results are reported in Table 6. Based on our observation, we empirically selected 0.05 as the threshold to distinguish strong and weak associations. TheOS type of CERD was excluded from the analysis because those CERD addresseduser errors rather than recognition errors.As shown in Table 6, LS.1 was most active, with strong associations with manyother CERD such as LD.1, LG.1, LM.1, LM.2, LS.2, HW.1, HW.2, and HW.3. Whenan output word is out of context, has incompatible semantics, or results in illformedphrase structures, it is unlikely the word will make any sense in a sentence. Choosinga wrong word from the same or a different grammatical category can suggest that theword does not make sense in the sentence. Moreover, alternative word hypothesesrelated CERD can facilitate the judgment of whether a word fits in a sentence or not.In this study, LS.2 also played an active role in error detection, and it was highlycorrelated with LM.1 and LM.2. In addition, word hypotheses and whether they havethe highest confidence scores can provide information about the semantic compatibility of an output word. Therefore, the judgment of whether a word makes sense ornot can be enhanced by using other types of linguistic information such as discourse,syntactic, and morphological CERD and hypotheses.LG.1 was highly correlated with LM.1, LP.1, HW.1, and LS.1. Specifically, illformed phrase structures may be caused by wrong partsofspeech, insertingdeletingDISCOVERING CUES TO ERROR DETECTION IN SPEECH RECOGNITION OUTPUT     261Table 6. Results of Associations Between CERDHU.1HU.2HU.3HW.1HW.2HW.3LD.1LD.2LD.3LD.4LG.1LG.2HU.20.040HU.30.0630.071HW.10.0150.0070.000HW.20.0150.0160.0080.169HW.30.0000.0130.0310.1540.096LD.10.0120.0000.0000.0430.0390.000LD.20.0000.0000.0000.0080.0000.0000.027LD.30.0000.0000.0000.0000.0000.0000.0000.000LD.40.0000.0000.0000.0000.0080.0000.0270.0000.000LG.10.0000.0060.0000.0680.0440.0290.0280.0000.0000.012LG.20.0050.0000.0050.0640.0480.0420.0180.0000.0000.0090.049LM.10.0000.0070.0000.0730.0510.0550.0210.0080.0000.0000.0930.073LM.20.0260.0000.0000.0440.0280.0240.0550.0000.0000.0430.0230.019LM.30.0000.0000.0000.0180.0120.0000.0090.0000.0000.0000.0140.012LM.40.0000.0000.0000.0000.0000.0000.0100.0000.0000.0000.0000.000LP.10.0000.0710.0000.0810.0260.0360.0070.0000.0000.0000.0540.032LP.20.0000.0260.0000.0200.0350.0000.0000.0000.0000.0000.0170.004LP.30.0740.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.000LS.10.0050.0050.0020.0690.0740.0520.0980.0150.0050.0050.0830.041LS.20.0000.0090.0000.0660.0280.0700.0180.0090.0000.0000.0360.013OE.10.0330.0360.0000.0070.0070.0250.0240.0000.0000.0430.0150.004OE.20.0000.0000.0000.0000.0000.0000.0130.0000.0000.0770.0060.005OR.10.0000.0000.0000.0060.0170.0000.0000.0000.0000.0000.0030.004continues262     ZHOU, SHI, ZHANG, AND SEARSTable 6. ContinuedLM.1LM.2LM.3LM.4LP.1LP.2LP.3LS.1LS.2OE.1OE.2HU.2HU.3HW.1HW.2HW.3LD.1LD.2LD.3LD.4LG.1LG.2LM.1LM.20.000LM.30.0000.000LM.40.0000.0000.000LP.10.0200.0210.0000.000LP.20.0470.0570.0000.0000.000LP.30.0000.0960.0000.0000.0000.000LS.10.0540.0660.0180.0020.0260.0210.000LS.20.0500.0700.0420.0070.0220.0400.0080.059OE.10.0070.0120.0000.0000.0330.0000.0000.0070.000OE.20.0000.0140.0000.0000.0240.0000.0000.0050.0000.200OR.10.0000.0080.0000.0000.0300.0000.0000.0130.0000.0000.000Note Boldface figures refer to strong associations  0.05 between two CERD.DISCOVERING CUES TO ERROR DETECTION IN SPEECH RECOGNITION OUTPUT     263a word, or speech disfluency. Alternative word hypotheses can also be a point ofreference for possible problems with phrase structures. For similar reasons, problemswith sentence structures LG.2 can be substantiated with morphological e.g., LM.1and word hypotheses i.e., HW.1 information.LM.1 was highly associated with HW.1HW.3 and some semantic and syntacticCERD e.g., LS.1, LS.2, LG.1, and LG.2. Alternative word hypotheses may containthe correct word, indicating possible confusion in the partofspeech of a recognizedword.In addition to semantic CERD e.g., LS.1 and LS.2, LM.2 was also highly associated with some phonological e.g., LP.2 and LP.3 and discourse CERD e.g., LD.1.The results suggest that a misrecognized content word is likely to sound similar to, orbe a part of, the content word being spoken or to be out of context.The strong associations between different word hypotheses CERD e.g., HW.1HW.3 in Table 6 show that they mutually reinforce each other. As discussed above,word hypotheses and related information are strongly associated with some morphological, syntactic, and semantic cues i.e., LM.1, LG.1, LG.2, LS.1, and LS.2. Furthermore, HW.1 was also found to correlate strongly with LP.1, implying that wordhypotheses are suggestive of disfluency.Like word hypotheses, utterance hypotheses CERD were highly correlated withone another except for the relatively weak relationship between HU.1 and HU.2. Theresults imply that HU.3 path ratio complements other utterance hypotheses CERD.The high correlation between HU.1 and LP.3 suggests that if two consecutive wordsare perceived to be the outcome of an inappropriate split of one word, there is a goodchance to catch such an error by referring to alternative utterance hypotheses. It isinteresting to find that HU.2 has a strong association with LP.1, which warrants afurther examination of the relationship between utterance length and disfluency.Finally, our analysis revealed that OE.1 and OE.2 were highly correlated with eachother. It infers that if there is an error with the preceding word, it would be helpful tocheck the following word in judging the correctness of the current word, and viceversa. This highlights the phenomenon of consecutive errors in the recognition output. Moreover, if the previous word is an error, background information LD.4 canbe scrutinized to help determine if the current word is possibly wrong.By applying the knowledge acquired from multiple correlated sources, we shouldreduce the ambiguity associated with a single knowledge source in error detection.Summary of CERD and DiscussionUsing a usercentered approach, this study discovered a variety of CERD, includingmorphological, syntactic, semantic, hypothesesrelated, and others, which were useful in detecting recognition errors. Some CERD received the best support in this study,including LS.1 making no sense, LG.1 phrase structure, LG.2 sentence structure, HW.1 word hypotheses, HW.2 confidence scores, and LM.1 partofspeechconfusion. Moreover, the utility of CERD was examined in relation to error typesand other CERD. By incorporating CERD into an error correction system, automatic264     ZHOU, SHI, ZHANG, AND SEARSerror detection and correction can be improved. The findings of this research alsocollectively lay the foundation for developing a decision support system that is ableto recommend a group of correlated and contextsensitive CERD to facilitate users indetecting speech recognition errors.The developed taxonomy of CERD not only provides new evidence to supportsome findings of previous studies but also discovers some promising new CERD toguide future error detection. For example, word hypotheses have been explored inerror detection or correction 20, 43, 44, 45, 49. The highest confidence score, approximated by the difference in the confidence scores between the best and the secondbest hypotheses, was used to develop confidence measures 44. In addition, thesecondbest hypothesis was used by some systems to replace the best hypothesis thatwas possibly wrong in error correction 27. Preceding error echoes the observationsof other studies 7, 12, 17.Some linguistic CERD, such as LM.1, LG.1, and LG.2, were incorporated as features to derive the scores of confidence measures and showed promising results 37,41, 49. Openclass word choice was used to detect errors at the word level 41.However, the encoding of syntactic information in previous studies was restricted toeither a probabilistic structured language model 37 or a probabilistic ornonprobabilistic representation of whether a word can be parsed 37, 49. It is rare toencode specific syntactic knowledge explicitly. Such knowledge is found to be beneficial to error detection in this study.Semanticsinduced CERD were suggested by several other studies. For example,cooccurrence analysis was used to detect words that were incompatible with thesurrounding words 38. Features extracted by a semantic parser were helpful to exposing semantic incompatibility among different components 49. The discourseCERD were incorporated into a dialogue system to indicate whether a word wasalready mentioned in the previous dialogue 41. Cooccurrence analysis could alsomake use of longrange contextual features beyond those in the current sentence 38.Nonetheless, this study represents the first effort to specify concrete types of semantic incompatibility and discourse incongruity caused by speech recognition errors.Some CERD that were selected by previous studies did not emerge from this study.For example, path ratio 7, 15, 36, 44, 49, 51 is a complex measure, which may notbe intuitive for general users to employ immediately.It is encouraging to discover some new CERD in this study. For example, wordsplit, coreference, background scenario, and redundancy are promising CERD thathave potential in speech error detection and correction. Moreover, some syntactic andsemantic CERD obtained from the experiment remain to be fully explored in improving the speech recognition output. The syntactic and semantic information that hasbeen previously applied is restricted to whether words can be parsed or cooccurred. Semantic and pragmatic knowledge can potentially address a key factor liable for recognition errorslack of commonsense understanding of what is beingsaid 8. Therefore, automatic error detection will greatly benefit from the advance ofnatural language processing. Meantime, challenges faced in improving related linguistic techniques, such as coreference resolution, should never be underestimated.DISCOVERING CUES TO ERROR DETECTION IN SPEECH RECOGNITION OUTPUT     265Verbal protocol analysis, as we conducted in this study, was both timeconsumingand labor intensive. Participants data had to be reviewed several times by each encoder in order to avoid possible misinterpretation of their intentions. Moreover, reexamination was followed to consolidate and normalize the encoding results for differentparticipants and to resolve disagreements between coders. We hope that the taxonomycreated in this study provides a jumpstart and general guidance for future studiesalong this line.The findings on CERD in this study were based on dictation speech recognition.They do not depend on dictation but can be extended to other applications of speechrecognition. Nevertheless, the effectiveness of CERD may vary as the speech contextchanges. For example, sentence hypotheses may become an important feature in dialogue recognition. As a result, the CERD reported here need to be reevaluated inother types of speech applications.ConclusionDETECTING AND CORRECTING RECOGNITION ERRORS are important and challengingissues in achieving widespread adoption of speech technology. In this research, weembarked on a quest to discover CERD in speech recognition output using a usercentered approach. A taxonomy of CERD was created based on content analysis ofverbal protocols collected from a user experiment. The findings of this study canguide future research efforts to improve recognition output and aid users in detectingspeech recognition errors.This research makes multifold contributions to improving the usefulness of speechtechnologies. First, the developed CERD taxonomy is the first taxonomy in this field,which not only advances our knowledge on CERD but also provides a systematicorganization of CERD for future reference. Second, this is the first study to apply ausercentered approach to discovering CERD, which overcomes the limitations ofand complements the traditional datadriven approach. Third, we propose a new measure i.e., support to assess the effectiveness of CERD. Fourth, to the best of ourknowledge, this is the first attempt to analyze CERD in relation to error types i.e.,substitution, insertion, and deletion. These findings, coupled with the ability to learnfrom users behavior during error correction, will enable the development of knowledgebased, contextaware, and personalized systems to ease users effort in errordetection and correction.The current work can be extended in several directions, such as implementing CERDand assessing their validity for automatic error detection, evaluating the impact ofCERD on the users performance in error detection, and investigating and comparingCERD in other types of speech applications such as dialogue systems.Acknowledgments This material is based upon work supported by the National Science Foundation under grant number 0328391. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the viewsof the National Science Foundation.266     ZHOU, SHI, ZHANG, AND SEARSREFERENCES1. Anderberg, M.R. Cluster Analysis for Applications. New York Academic Press, 1973.2. Arnold, S.C. Mark, L. and Goldthwaite, J. Programming by voice, VocalProgramming.In M. Tremaine, E. Cole, and E. Mynatt eds., Proceedings of the Fourth International ACMConference on Assistive Technologies. New York ACM Press, 2000, pp. 149155.3. Bain, K. Basson, S.H. and Wald, M. Speech recognition in university classrooms Liberated learning project. In V.L. Hanson and J.A. Jacko eds., Proceedings of the Fifth International ACM Conference on Assistive Technologies. New York ACM Press, 2002, pp. 192196.4. Brill, E. Florian, R. Henderson, J.C. and Mangu, L. Beyond ngrams Can linguisticsophistication improve language modeling In C. Boitet and P. Whitelock eds., Proceedingsof the ThirtySixth Annual Meeting on Association for Computational Linguistics. Morristown,NJ Association for Computational Linguistics, 1998, pp. 186190.5. Carpenter, P. Jin, C. Wilson, D. Zhang, R. Bohus, D. and Rudnicky, A.I. Is thisconversation on track In P. Dalsgaard, B. Lindberg, H. Benner, and Z. Tan eds., Proceedingsof the Seventh European Conference on Speech Communication and Technology. Bonn, Germany International Speech Communication Association, 2001, pp. 21212124.6. Chase, L. ErrorResponsive Feedback Mechanisms for Speech Recognizers. Ph.D. dissertation, School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, 1997.7. Chase, L. Word and acoustic confidence annotation for large vocabulary speech recognition. In G. Kokkinakis, N. Fakotakis, and E. Dermatas eds., Proceedings of the Fifth European Conference on Speech Communication and Technology. Bonn, Germany InternationalSpeech Communication Association, 1997, pp. 815818.8. Deng, L., and Huang, X. Challenges in adopting speech recognition. Communications ofthe ACM, 47, 1 January 2004, 6975.9. Duchateau, J. Demuynck, K. and Wambacq, P. Confidence scoring based on backwardlanguage models. In F.J. Taylor, J. Principe, and H. Bourlard eds., 2002 IEEE InternationalConference on Acoustics, Speech, and Signal Processing, vol. 1. Los Alamitos, CA IEEEComputer Society Press, 2002, pp. 221224.10. EinDor, P., and Spiegler, I. Natural language access to multiple databases A model anda prototype. Journal of Management Information Systems, 12, 1 Summer 1995, 171197.11. Ericsson, K.A., and Simon, H.A. Protocol Analysis Verbal Reports as Data. Cambridge,MA MIT Press, 1993.12. Feng, J., and Sears, A. Using confidence scores to improve handsfree speech basednavigation in continuous dictation systems. ACM Transactions on ComputerHuman Interaction, 11, 4 December 2004, 329356.13. Furui, S. Automatic speech recognition and its application to information extraction. InR. Dale and K. Church eds., Proceedings of the ThirtySeventh Annual Meeting of the Association for Computational Linguistics. Morristown, NJ Association for Computational Linguistics, 1999, pp. 1120.14. Gauvain, J.L., and Lamel, L. Large vocabulary speech recognition based on statisticalmethods. In W. Chou and B.H. Juang eds., Pattern Recognition in Speech and LanguageProcessing. Boca Raton, FL CRC Press, 2003, pp. 149189.15. Gillick, L. Ito, Y. and Young, J. A probabilistic approach to confidence estimation andevaluation. In M.K. Lang and H. Hoge eds., 1997 IEEE International Conference on Acoustics, Speech, and Signal Processing, vol. 2. Los Alamitos, CA IEEE Computer Society Press,1997, pp. 879882.16. Hagen, A. Connors, D.A. and Pellom, B.L. The analysis and design of architecturesystems for speech recognition on modern handheldcomputing devices. In R. Gupta and Y.Nakamura eds., Proceedings of the First IEEEACMIFIP International Conference on HardwareSoftware Codesign and System Synthesis. New York ACM Press, 2003, pp. 6570.17. HernandezAbrego, G., and Marino, J.B. Contextual confidence measures for continuous speech recognition. In H. Abut and L. Onural eds., 2000 IEEE International Conferenceon Acoustics, Speech, and Signal Processing, vol. 3. Los Alamitos, CA IEEE Computer Society Press, 2000, pp. 18031806.18. Higgins, E.L., and Raskind, M.H. Speaking to read The effects of continuous vs. discrete speech recognition systems on the reading and spelling of children with learning disabiliDISCOVERING CUES TO ERROR DETECTION IN SPEECH RECOGNITION OUTPUT     267ties. Journal of Special Education Technology, 15, 1 Winter 2000 available at jset.unlv.edu15.1higginsfirst.html.19. Hoffman, T. Speech recognition powers utilitys customer service. ComputerWorld, September 12, 2005 available at www.computerworld.commanagementtopicsmanagementhelpdeskstory0,10801,104535,00.html.20. Kemp, T., and Schaaf, T. Estimating confidence using word lattices. In G. Kokkinakis,N. Fakotakis, and E. Dermatas eds., Proceedings of the Fifth European Conference on SpeechCommunication and Technology. Bonn, Germany International Speech Communication Association, 1997, pp. 827830.21. Krahmer, E. Swerts, M. Theune, M. and Weegels, M. Error detection in spoken humanmachine interaction. International Journal of Speech Technology, 4, 1 March 2001, 1930.22. Lai, J., and Vergo, J. MedSpeak Report creation with continuous speech recognition. InS. Pemberton ed., Proceedings of the SIGCHI Conference on Human Factors in ComputingSystems. New York ACM Press, 1997, pp. 431438.23. Levine, H.G., and Rossmoore, D. Diagnosing the human threats to information technology implementation A missing factor in systems analysis illustrated in a case study. Journal ofManagement Information Systems, 10, 2 Fall 1993, 5574.24. Liu, Y. Structural Event Detection for Rich Transcription of Speech. Ph.D. dissertation,School of Electrical and Computer Engineering, Purdue University, West Lafayette, IN, 2004.25. Lubert, J. Kotler, A. Shein, F. and Tam, C. Speech recognition. SNOW, Toronto, ON,1998 available at snow.utoronto.cabestspecialspeechrecognition.html.26. Maison, B., and Gopinath, R. Robust confidence annotation and rejection for continuousspeech recognition. In V.J. Mathews and A. Swindlehurst eds., 2001 IEEE InternationalConference on Acoustics, Speech, and Signal Processing, vol. 1. Los Alamitos, CA IEEEComputer Society Press, 2001, pp. 389392.27. Mangu, L., and Padmanabhan, M. Error corrective mechanisms for speech recognition.In V.J. Mathews and A. Swindlehurst eds., 2001 IEEE International Conference on Acoustics, Speech, and Signal Processing, vol. 1. Los Alamitos, CA IEEE Computer Society Press,2001, pp. 2932.28. Mann, W.C., and Thompson, S.A. Rhetorical structure theory A theory of text organization. In L. Polanyi ed., The Structure of Discourse. Norwood, NJ Ablex, 1987, pp. 8596.29. Mao, J.Y., and Benbasat, I. The use of explanations in knowledgebased systems Cognitive perspectives and a processtracing analysis. Journal of Management Information Systems, 17, 2 Fall 2000, 153180.30. McTear, M.F. Spoken dialogue technology Enabling the conversational user interface.ACM Computing Surveys, 34, 1 March 2002, 90169.31. Nunamaker, J.F., Jr. Konsynski, B.R. Chen, M. Vinze, A.S. King, D.R. and Heltne,M.M. Knowledgebased systems support for information centers. Journal of Management Information Systems, 5, 1 Summer 1988, 624.32. Pao, C. Schmid, P. and Glass, J. Confidence scoring for speech understanding systems.In R.H. Mannell and J. RobertRibes eds., Proceedings of the Fifth International Conferenceon Spoken Language Processing. Canberra Australian Speech Science and Technology Association, 1998, pp. 815818.33. Pradhan, S.S., and Ward, W.H. Estimating semantic confidence for spoken dialoguesystems. In F.J. Taylor, J. Principe, and H. Bourlard eds., 2002 IEEE International Conference on Acoustics, Speech, and Signal Processing, vol. 1. Los Alamitos, CA IEEE ComputerSociety Press, 2002, pp. 233236.34. Ringger, E.K., and Allen, J.F. Error correction via a postprocessor for continuous speechrecognition. In M.H. Hayes and M.A. Clements eds., 1996 IEEE International Conferenceon Acoustics, Speech, and Signal Processing, vol. 1. Los Alamitos, CA IEEE Computer Society Press, 1996, pp. 427430.35. Robertson, J. Wong, W.Y. Chung, C. and Kim, D.K. Automatic speech recognition forgeneralised time based media retrieval and indexing. In W. Effelsberg and B.C. Smith eds.,Proceedings of the Sixth ACM International Conference on Multimedia. New York ACM Press,1998, pp. 241246.36. SanSegundo, R. Pellom, B. Hacioglu, K. Ward, W. and Pardo, J.M. Confidence measures for spoken dialogue systems. In V.J. Mathews and A. Swindlehurst eds., 2001 IEEE268     ZHOU, SHI, ZHANG, AND SEARSInternational Conference on Acoustics, Speech, and Signal Processing, vol. 1. Los Alamitos,CA IEEE Computer Society Press, 2001, pp. 393396.37. Sarikaya, R. Gao, Y. and Picheny, M. Word level confidence measurement using semantic features. In W. Siu, A.G. Constantinides, and Y. Chan eds., 2003 IEEE InternationalConference on Acoustics, Speech, and Signal Processing, vol. 1. Los Alamitos, CA IEEEComputer Society Press, 2003, pp. 604607.38. Sarma, A., and Palmer, D.D. Contextbased speech recognition error detection and correction. In J.B. Hirschberg, S. Dumais, D. Marcu, and S. Roukos eds., Human LanguageTechnology Conference of the North American Chapter of the Association for ComputationalLinguistics 2004 Short Papers. East Stroudsburg, PA Association for Computational Linguistics, 2004, pp. 8588.39. Sears, A. Feng, J. Oseitutu, K. and Karat, C.M. Handsfree speechbased navigationduring dictation Difficulties, consequences, and solutions. HumanComputer Interaction, 18,3 2003, 229257.40. Sears, A. Karat, C.M. Oseitutu, K. Karimullah, A. and Feng, J. Productivity, satisfaction, and interaction strategies of individuals with spinal cord injuries and traditional usersinteracting with speech recognition software. Universal Access in the Information Society, 1, 1June 2001, 415.41. Skantze, G., and Edlund, J. Early error detection on word level. In B. Milner ed.,Proceedings of COST278 and ISCA Tutorial and Research Workshop on Robustness Issues inConversational Interaction. Bonn, Germany International Speech Communication Association, 2004 available at www.iscaspeech.orgarchiverobust2004rob417.html.42. Suhm, B. Myers, B. and Waibel, A. Multimodal error correction for speech user interfaces. ACM Transactions on ComputerHuman Interaction, 8, 1 March 2001, 6098.43. Weintraub, M. Beaufays, F. Rivlin, Z. Konig, Y. and Stolcke, A. Neuralnetwork basedmeasures of confidence for word recognition. In M.K. Lang and H. Hoge eds., 1997 IEEEInternational Conference on Acoustics, Speech, and Signal Processing, vol. 2. Los Alamitos,CA IEEE Computer Society Press, 1997, pp. 887890.44. Wendemuth, A. Rose, G. and Dolfing, J.G.A. Advances in confidence measures forlarge vocabulary. In D. Cochran and A. Spanias eds., 1999 IEEE International Conference onAcoustics, Speech, and Signal Processing, vol. 2. Los Alamitos, CA IEEE Computer SocietyPress, 1999, pp. 705708.45. Wessel, F. Schluter, R. and Ney, H. Using posterior probabilities for improved speechrecognition. In H. Abut and L. Onural eds., 2000 IEEE International Conference on Acoustics, Speech, and Signal Processing, vol. 3. Los Alamitos, CA IEEE Computer Society Press,2000, pp. 15871590.46. Wessel, F. Schluter, R. Macherey, K. and Ney, H. Confidence measures for large vocabulary continuous speech recognition. IEEE Transactions on Speech and Audio Processing,9, 3 March 2001, 288298.47. Young, S.R. Detecting misrecognitions and outofvocabulary words. In 1994 IEEE International Conference on Acoustics, Speech, and Signal Processing, vol. 2. Los Alamitos,CA IEEE Computer Society Press, 1994, pp. 2124.48. Zhang, D., and Adipat, B. Challenges, methodologies, and issues in the usability testingof mobile applications. International Journal of HumanComputer Interaction, 18, 3 July2005, 293308.49. Zhang, R., and Rudnicky, A.I. Word level confidence annotation using combinations offeatures. In P. Dalsgaard, B. Lindberg, H. Benner, and Z. Tan eds., Proceedings of the Seventh European Conference on Speech Communication and Technology. Bonn, Germany International Speech Communication Association, 2001, pp. 21052108.50. Zhou, L. Shi, Y. Feng, J. and Sears, A. Data mining for detecting errors in dictationspeech recognition. IEEE Transactions on Speech and Audio Processing, 13, 5 September2005, 681688.51. Zhou, Z., and Meng, H. A twolevel schema for detecting recognition errors. In S.H.Kim, S. Lee, Y. Oh, and Y. Lee eds., Proceedings of the Eighth International Conference onSpoken Language Processing. Bonn, Germany International Speech Communication Association, 2004, pp. 449452.DISCOVERING CUES TO ERROR DETECTION IN SPEECH RECOGNITION OUTPUT     269Appendix. A Taxonomy of Cues to Error Detection CERDLinguisticsBased L Phonological P Disfluency LP.1 False start LP.1.1 Repetition LP.1.2 Phonetic similarity LP.2 Word split LP.3 Morphological M Partofspeech confusion LM.1 Conjunction LM.1.1 Openclass word choice LM.2 Closedclass word choice LM.3 Preposition LM.3.1 Determiner LM.3.2 Nonword choice LM.4 Syntactic G Phrase structure LG.1 Phrase head LG.1.1 Parallel structure LG.1.2 Incomplete phrase LG.1.3 Modifier LG.1.4 Common phrase LG.1.5 Sentence structure LG.2 Beginningending LG.2.1 Tensevoiceaspectmodal LG.2.2 Number disagreement LG.2.3 Dangling words LG.2.4 Incomplete sentence LG.2.5 Subjectobject LG.2.6 Person disagreement LG.2.7 Semantic S Making no sense LS.1 Incompatible semantics LS.2 Subjectobject LS.2.1 Prepositionobject LS.2.2 Modifierhead LS.2.3 Subjectpredicate LS.2.4 Predicateobject LS.2.5 Two constituents LS.2.6 Discourse D Out of context LD.1270     ZHOU, SHI, ZHANG, AND SEARS Preceding LD.1.1 Following LD.1.2 Coreference LD.2 Contradictory information LD.3 Background scenario LD.4HypothesesBased H Word W Word hypotheses HW.1 Confidence scores HW.2 Highest confidence score HW.3 Utterance U Utterance hypotheses HU.1 Utterance length HU.2 Path ratio HU.3Others O Error E Preceding error OE.1 Following error OE.2 Style S Contraction OS.1 Double negation OS.2 Letter case OS.3 Misplacement OS.4 Repetition R Redundancy OR.1
