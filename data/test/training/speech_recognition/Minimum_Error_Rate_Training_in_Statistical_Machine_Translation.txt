Minimum Error Rate Training in Statistical Machine TranslationFranz Josef OchInformation Sciences InstituteUniversity of Southern California4676 Admiralty Way, Suite 1001Marina del Rey, CA 90292ochisi.eduAbstractOften, the training procedure for statistical machine translation models is based onmaximum likelihood or related criteria. Ageneral problem of this approach is thatthere is only a loose relation to the finaltranslation quality on unseen text. In thispaper, we analyze various training criteriawhich directly optimize translation quality. These training criteria make use of recently proposed automatic evaluation metrics. We describe a new algorithm for efficient training an unsmoothed error count.We show that significantly better resultscan often be obtained if the final evaluation criterion is taken directly into accountas part of the training procedure.1 IntroductionMany tasks in natural language processing haveevaluation criteria that go beyond simply counting the number of wrong decisions the systemmakes. Some often used criteria are, for example,FMeasure for parsing, mean average precision forranked retrieval, and BLEU or multireference worderror rate for statistical machine translation. The useof statistical techniques in natural language processing often starts out with the simplifying often implicit assumption that the final scoring is based onsimply counting the number of wrong decisions, forinstance, the number of sentences incorrectly translated in machine translation. Hence, there is a mismatch between the basic assumptions of the usedstatistical approach and the final evaluation criterionused to measure success in a task.Ideally, we would like to train our model parameters such that the endtoend performance in someapplication is optimal. In this paper, we investigatemethods to efficiently optimize model parameterswith respect to machine translation quality as measured by automatic evaluation criteria such as worderror rate and BLEU.2 Statistical Machine Translation withLoglinear ModelsLet us assume that we are given a source Frenchsentence , which isto be translated into a target English sentence Among all possibletarget sentences, we will choose the sentence withthe highest probability1   Pr 1The argmax operation denotes the search problem,i.e. the generation of the output sentence in the target language. The decision in Eq. 1 minimizes thenumber of decision errors. Hence, under a socalledzeroone loss function this decision rule is optimalDuda and Hart, 1973. Note that using a different loss functionfor example, one induced by theBLEU metrica different decision rule would beoptimal.1The notational convention will be as follows. We use thesymbol Pr , . to denote general probability distributions withnearly no specific assumptions. In contrast, for modelbasedprobability distributions, we use the generic symbol 0, . .As the true probability distribution Pr is unknown, we have to develop a model  that approximates Pr . We directly model the posteriorprobability Pr by using a loglinear model. Inthis framework, we have a set of  feature functions . For each feature function,there exists a model parameter  .The direct translation probability is given byPr  2 exp   exp    3In this framework, the modeling problem amountsto developing suitable feature functions that capturethe relevant properties of the translation task. Thetraining problem amounts to obtaining suitable parameter values  . A standard criterion for loglinear models is the MMI maximum mutual information criterion, which can be derived from themaximum entropy principle    ,4The optimization problem under this criterion hasvery nice properties there is one unique global optimum, and there are algorithms e.g. gradient descent that are guaranteed to converge to the globaloptimum. Yet, the ultimate goal is to obtain goodtranslation quality on unseen test data. Experienceshows that good results can be obtained using thisapproach, yet there is no reason to assume that anoptimization of the model parameters using Eq. 4yields parameters that are optimal with respect totranslation quality.The goal of this paper is to investigate alternative training criteria and corresponding training algorithms, which are directly related to translationquality measured with automatic evaluation criteria.In Section 3, we review various automatic evaluation criteria used in statistical machine translation.In Section 4, we present two different training criteria which try to directly optimize an error count. InSection 5, we sketch a new training algorithm whichefficiently optimizes an unsmoothed error count. InSection 6, we describe the used feature functions andour approach to compute the candidate translationsthat are the basis for our training procedure. In Section 7, we evaluate the different training criteria inthe context of several MT experiments.3 Automatic Assessment of TranslationQualityIn recent years, various methods have been proposed to automatically evaluate machine translationquality by comparing hypothesis translations withreference translations. Examples of such methodsare word error rate, positionindependent word errorrate Tillmann et al., 1997, generation string accuracy Bangalore et al., 2000, multireference worderror rate Nieen et al., 2000, BLEU score Papineni et al., 2001, NIST score Doddington, 2002.All these criteria try to approximate human assessment and often achieve an astonishing degree of correlation to human subjective evaluation of fluencyand adequacy Papineni et al., 2001 Doddington,2002.In this paper, we use the following methods multireference word error rate mWERWhen this method is used, the hypothesis translation is compared to various reference translations by computing the edit distance minimumnumber of substitutions, insertions, deletionsbetween the hypothesis and the closest of thegiven reference translations. multireference position independent error ratemPER This criterion ignores the word orderby treating a sentence as a bagofwords andcomputing the minimum number of substitutions, insertions, deletions needed to transformthe hypothesis into the closest of the given reference translations. BLEU score This criterion computes the geometric mean of the precision of . grams ofvarious lengths between a hypothesis and a setof reference translations multiplied by a factorBP 0that penalizes short sentencesBLEUBP 01325476889 Here  8 denotes the precision of . grams in thehypothesis translation. We use9. NIST score This criterion computes aweighted precision of . grams between a hypothesis and a set of reference translations multiplied by a factor BP 0that penalizes shortsentencesNISTBP 0688Here8 denotes the weighted precision of . grams in the translation. We use9.Both, NIST and BLEU are accuracy measures,and thus larger values reflect better translation quality. Note that NIST and BLEU scores are not additive for different sentences, i.e. the score for a document cannot be obtained by simply summing overscores for individual sentences.4 Training Criteria for Minimum ErrorRate TrainingIn the following, we assume that we can measurethe number of errors in sentence  by comparing itwith a reference sentence  using a function E    .However, the following exposition can be easilyadapted to accuracy metrics and to metrics that makeuse of multiple references.We assume that the number of errors for a setof sentences  is obtained by summing the errors for the individual sentences .Our goal is to obtain a minimal error count on arepresentative corpus with given reference translations and a set of  different candidate translations  for each input sentence.    ,5    ,with   6The above stated optimization criterion is not easyto handle It includes an argmax operation Eq. 6. Therefore, it is not possible to compute a gradientand we cannot use gradient descent methods toperform optimization. The objective function has many different localoptima. The optimization algorithm must handle this.In addition, even if we manage to solve the optimization problem, we might face the problem of overfitting the training data. In Section 5, we describe anefficient optimization algorithm.To be able to compute a gradient and to make theobjective function smoother, we can use the following error criterion which is essentially a smoothederror count, with a parameter  to adjust the smoothness   7In the extreme case, for   , Eq. 7 convergesto the unsmoothed criterion of Eq. 5 except in thecase of ties. Note, that the resulting objective function might still have local optima, which makes theoptimization hard compared to using the objectivefunction of Eq. 4 which does not have different local optima. The use of this type of smoothed errorcount is a common approach in the speech community Juang et al., 1995 Schluter and Ney, 2001.Figure 1 shows the actual shape of the smoothedand the unsmoothed error count for two parameters in our translation system. We see that the unsmoothed error count has many different local optima and is very unstable. The smoothed error countis much more stable and has fewer local optima. Butas we show in Section 7, the performance on ourtask obtained with the smoothed error count doesnot differ significantly from that obtained with theunsmoothed error count.5 Optimization Algorithm forUnsmoothed Error CountA standard algorithm for the optimization of theunsmoothed error count Eq. 5 is Powells algorithm combined with a gridbased line optimization method Press et al., 2002. We start at a random point in the  dimensional parameter space 9400 9410 9420 9430 9440 9450 9460 9470 94804 3 2 1  0  1  2  3  4error count unsmoothed error countsmoothed error rate alpha3 9405 9410 9415 9420 9425 9430 9435 9440 9445 94504 3 2 1  0  1  2  3  4error count unsmoothed error countsmoothed error rate alpha3Figure 1 Shape of error count and smoothed error count for two different model parameters. These curveshave been computed on the development corpus see Section 7, Table 1 using alternatives per sourcesentence. The smoothed error count has been computed with a smoothing parameter .and try to find a better scoring point in the parameter space by making a onedimensional line minimization along the directions given by optimizingone parameter while keeping all other parametersfixed. To avoid finding a poor local optimum, westart from different initial parameter values. A majorproblem with the standard approach is the fact thatgridbased line optimization is hard to adjust suchthat both good performance and efficient search areguaranteed. If a finegrained grid is used then thealgorithm is slow. If a large grid is used then theoptimal solution might be missed.In the following, we describe a new algorithm forefficient line optimization of the unsmoothed errorcount Eq. 5 using a loglinear model Eq. 3 whichis guaranteed to find the optimal solution. The newalgorithm is much faster and more stable than thegridbased line optimization method.Computing the most probable sentence out of aset of candidate translation seeEq. 6 along a line  with parameter results in an optimization problem of the followingfunctional form      8Here,0and  0are constants with respect to  .Hence, every candidate translation in  correspondsto a line. The function   9is piecewise linear Papineni, 1999. This allows usto compute an efficient exhaustive representation ofthat function.In the following, we sketch the new algorithmto optimize Eq. 5 We compute the ordered sequence of linear intervals constitutingfor every sentencetogether with the incremental changein error count from the previous to the next interval. Hence, we obtain for every sentencea sequence     6which denote theinterval boundaries and a corresponding sequencefor the change in error count involved at the corresponding interval boundary   6.Here, 8 denotes the change in the error count atposition   8    8to the error count at position 8 8. By merging all sequences   and for all different sentences of our corpus, thecomplete set of interval boundaries and error countchanges on the whole corpus are obtained. The optimal  can now be computed easily by traversingthe sequence of interval boundaries while updatingan error count.It is straightforward to refine this algorithm toalso handle the BLEU and NIST scores instead ofsentencelevel error counts by accumulating the relevant statistics for computing these scores ngramprecision, translation length and reference length .6 Baseline Translation ApproachThe basic feature functions of our model are identical to the alignment template approach Och andNey, 2002. In this translation model, a sentenceis translated by segmenting the input sentence intophrases, translating these phrases and reordering thetranslations in the target language. In addition to thefeature functions described in Och and Ney, 2002,our system includes a phrase penalty the numberof alignment templates used and special alignmentfeatures. Altogether, the loglinear model includesdifferent features.Note that many of the used feature functions arederived from probabilistic models the feature function is defined as the negative logarithm of the corresponding probabilistic model. Therefore, the feature functions are much more informative than forinstance the binary feature functions used in standard maximum entropy models in natural languageprocessing.For search, we use a dynamic programmingbeamsearch algorithm to explore a subset of all possible translations Och et al., 1999 and extract . best candidate translations using A search Ueffinget al., 2002.Using an . best approximation, we might face theproblem that the parameters trained are good for thelist of . translations used, but yield worse translation results if these parameters are used in the dynamic programming search. Hence, it is possiblethat our new search produces translations with moreerrors on the training corpus. This can happen because with the modified model scaling factors the. best list can change significantly and can includesentences not in the existing . best list. To avoidthis problem, we adopt the following solution First,we perform search using a manually defined set ofparameter values and compute an . best list, anduse this . best list to train the model parameters.Second, we use the new model parameters in a newsearch and compute a new . best list, which is combined with the existing . best list. Third, using thisextended . best list new model parameters are computed. This is iterated until the resulting . best listdoes not change. In this algorithm convergence isguaranteed as, in the limit, the . best list will contain all possible translations. In our experiments,we compute in every iteration about 200 alternativetranslations. In practice, the algorithm converges after about five to seven iterations. As a result, errorrate cannot increase on the training corpus.A major problem in applying the MMI criterionis the fact that the reference translations need to bepart of the provided . best list. Quite often, none ofthe given reference translations is part of the . bestlist because the search algorithm performs pruning,which in principle limits the possible translationsthat can be produced given a certain input sentence.To solve this problem, we define for the MMI training new pseudoreferences by selecting from the . best list all the sentences which have a minimal number of word errors with respect to any of the true references. Note that due to this selection approach, theresults of the MMI criterion might be biased towardthe mWER criterion. It is a major advantage of theminimum error rate training that it is not necessaryto choose pseudoreferences.7 ResultsWe present results on the 2002 TIDES ChineseEnglish small data track task. The goal is the translation of news text from Chinese to English. Table 1 provides some statistics on the training, development and test corpus used. The system we usedoes not include rulebased components to translatenumbers, dates or names. The basic feature functions were trained using the training corpus. The development corpus was used to optimize the parameters of the loglinear model. Translation results arereported on the test corpus.Table 2 shows the results obtained on the development corpus and Table 3 shows the results obtainedTable 2 Effect of different error criteria in training on the development corpus. Note that better resultscorrespond to larger BLEU and NIST scores and to smaller error rates. Italic numbers refer to results forwhich the difference to the best result indicated in bold is not statistically significant.error criterion used in training mWER  mPER  BLEU  NIST  wordsconfidence intervals  2.4  1.8  1.2  0.2 MMI 70.7 55.3 12.2 5.12 10382mWER 69.7 52.9 15.4 5.93 10914smoothedmWER 69.8 53.0 15.2 5.93 10925mPER 71.9 51.6 17.2 6.61 11671smoothedmPER 71.8 51.8 17.0 6.56 11625BLEU 76.8 54.6 19.6 6.93 13325NIST 73.8 52.8 18.9 7.08 12722Table 1 Characteristics of training corpus Train,manual lexicon Lex, development corpus Dev,test corpus Test.Chinese EnglishTrain Sentences 5 109Words 89 121 111 251Singletons 3 419 4 130Vocabulary 8 088 8 807Lex Entries 82 103Dev Sentences 640Words 11 746 13 573Test Sentences 878Words 24 323 26 489on the test corpus. Italic numbers refer to resultsfor which the difference to the best result indicatedin bold is not statistically significant. For all errorrates, we show the maximal occurring 95 confidence interval in any of the experiments for that column. The confidence intervals are computed usingbootstrap resampling Press et al., 2002. The lastcolumn provides the number of words in the produced translations which can be compared with theaverage number of reference words occurring in thedevelopment and test corpora given in Table 1.We observe that if we choose a certain error criterion in training, we obtain in most cases the best results using the same criterion as the evaluation metric on the test data. The differences can be quitelarge If we optimize with respect to word error rate,the results are mWER68.3, which is better thanif we optimize with respect to BLEU or NIST andthe difference is statistically significant. BetweenBLEU and NIST, the differences are more moderate,but by optimizing on NIST, we still obtain a largeimprovement when measured with NIST comparedto optimizing on BLEU.The MMI criterion produces significantly worseresults on all error rates besides mWER. Note that,due to the redefinition of the notion of referencetranslation by using minimum edit distance, the results of the MMI criterion are biased toward mWER.It can be expected that by using a suitably defined . gram precision to define the pseudoreferences forMMI instead of using edit distance, it is possible toobtain better BLEU or NIST scores.An important part of the differences in the translation scores is due to the different translation lengthlast column in Table 3. The mWER and MMI criteria prefer shorter translations which are heavily penalized by the BLEU and NIST brevity penalty.We observe that the smoothed error count givesalmost identical results to the unsmoothed errorcount. This might be due to the fact that the numberof parameters trained is small and no serious overfitting occurs using the unsmoothed error count.8 Related WorkThe use of loglinear models for statistical machinetranslation was suggested by Papineni et al. 1997and Och and Ney 2002.The use of minimum classification errortraining and using a smoothed error count iscommon in the pattern recognition and speechTable 3 Effect of different error criteria used in training on the test corpus. Note that better results correspond to larger BLEU and NIST scores and to smaller error rates. Italic numbers refer to results for whichthe difference to the best result indicated in bold is not statistically significant.error criterion used in training mWER  mPER  BLEU  NIST  wordsconfidence intervals  2.7  1.9  0.8  0.12 MMI 68.0 51.0 11.3 5.76 21933mWER 68.3 50.2 13.5 6.28 22914smoothedmWER 68.2 50.2 13.2 6.27 22902mPER 70.2 49.8 15.2 6.71 24399smoothedmPER 70.0 49.7 15.2 6.69 24198BLEU 76.1 53.2 17.2 6.66 28002NIST 73.3 51.5 16.4 6.80 26602recognition community Duda and Hart, 1973Juang et al., 1995 Schluter and Ney, 2001.Paciorek and Rosenfeld 2000 use minimum classification error training for optimizing parametersof a wholesentence maximum entropy languagemodel.A technically very different approach that has asimilar goal is the minimum Bayes risk approach, inwhich an optimal decision rule with respect to anapplication specific riskloss function is used, whichwill normally differ from Eq. 3. The loss function iseither identical or closely related to the final evaluation criterion. In contrast to the approach presentedin this paper, the training criterion and the statistical models used remain unchanged in the minimumBayes risk approach. In the field of natural languageprocessing this approach has been applied for example in parsing Goodman, 1996 and word alignmentKumar and Byrne, 2002.9 ConclusionsWe presented alternative training criteria for loglinear statistical machine translation models whichare directly related to translation quality an unsmoothed error count and a smoothed error counton a development corpus. For the unsmoothed error count, we presented a new line optimization algorithm which can efficiently find the optimal solution along a line. We showed that this approach obtains significantly better results than using the MMItraining criterion with our method to define pseudoreferences and that optimizing error rate as part ofthe training criterion helps to obtain better error rateon unseen test data. As a result, we expect that actual true translation quality is improved, as previous work has shown that for some evaluation criteria there is a correlation with human subjectiveevaluation of fluency and adequacy Papineni et al.,2001 Doddington, 2002. However, the differentevaluation criteria yield quite different results on ourChineseEnglish translation task and therefore weexpect that not all of them correlate equally well tohuman translation quality.The following important questions should be answered in the future How many parameters can be reliably estimated using unsmoothed minimum error ratecriteria using a given development corpus sizeWe expect that directly optimizing error rate formany more parameters would lead to seriousoverfitting problems. Is it possible to optimizemore parameters using the smoothed error ratecriterion Which error rate should be optimized duringtraining This relates to the important questionof which automatic evaluation measure is optimally correlated to human assessment of translation quality.Note, that this approach can be applied to anyevaluation criterion. Hence, if an improved automatic evaluation criterion is developed that has aneven better correlation with human judgments thanBLEU and NIST, we can plug this alternative criterion directly into the training procedure and optimize the model parameters for it. This means thatimproved translation evaluation measures lead directly to improved machine translation quality. Ofcourse, the approach presented here places a highdemand on the fidelity of the measure being optimized. It might happen that by directly optimizing an error measure in the way described above,weaknesses in the measure might be exploited thatcould yield better scores without improved translation quality. Hence, this approach poses new challenges for developers of automatic evaluation criteria.Many tasks in natural language processing, for instance summarization, have evaluation criteria thatgo beyond simply counting the number of wrongsystem decisions and the framework presented heremight yield improved systems for these tasks aswell.AcknowledgementsThis work was supported by DARPAITO grant660010019814.ReferencesSrinivas Bangalore, O. Rambox, and S. Whittaker. 2000.Evaluation metrics for generation. In Proceedingsof the International Conference on Natural LanguageGeneration, Mitzpe Ramon, Israel.George Doddington. 2002. Automatic evaluation of machine translation quality using ngram cooccurrencestatistics. In Proc. ARPA Workshop on Human Language Technology.Richhard O. Duda and Peter E. Hart. 1973. Pattern Classification and Scene Analysis. John Wiley, New York,NY.Joshua Goodman. 1996. Parsing algorithms and metrics.In Proceedings of the 34th Annual Meeting of the ACL,pages 177183, Santa Cruz, CA, June.B. H. Juang, W. Chou, and C. H. Lee. 1995. Statistical and discriminative methods for speech recognition.In A. J. Rubio Ayuso and J. M. Lopez Soler, editors,Speech Recognition and Coding  New Advances andTrends. Springer Verlag, Berlin, Germany.Shankar Kumar and William Byrne. 2002. Minimumbayesrisk alignment of bilingual texts. In Proc. ofthe Conference on Empirical Methods in Natural Language Processing, Philadelphia, PA.Sonja Nieen, Franz J. Och, G. Leusch, and HermannNey. 2000. An evaluation tool for machine translation Fast evaluation for machine translation research.In Proc. of the Second Int. Conf. on Language Resources and Evaluation LREC, pages 3945, Athens,Greece, May.Franz Josef Och and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In Proc. of the 40th AnnualMeeting of the Association for Computational Linguistics ACL, Philadelphia, PA, July.Franz J. Och, Christoph Tillmann, and Hermann Ney.1999. Improved alignment models for statistical machine translation. In Proc. of the Joint SIGDAT Conf.on Empirical Methods in Natural Language Processing and Very Large Corpora, pages 2028, Universityof Maryland, College Park, MD, June.Chris Paciorek and Roni Rosenfeld. 2000. Minimumclassification error training in exponential languagemodels. In NISTDARPA Speech Transcription Workshop, May.Kishore A. Papineni, Salim Roukos, and R. T. Ward.1997. Featurebased language understanding. In European Conf. on Speech Communication and Technology, pages 14351438, Rhodes, Greece, September.Kishore A. Papineni, Salim Roukos, Todd Ward, andWeiJing Zhu. 2001. Bleu a method for automatic evaluation of machine translation. TechnicalReport RC22176 W0109022, IBM Research Division, Thomas J. Watson Research Center, YorktownHeights, NY, September.Kishore A. Papineni. 1999. Discriminative training vialinear programming. In Proceedings of the 1999 IEEEInternational Conference on Acoustics, Speech  Signal Processing, Atlanta, March.William H. Press, Saul A. Teukolsky, William T. Vetterling, and Brian P. Flannery. 2002. Numerical Recipesin C. Cambridge University Press, Cambridge,UK.Ralf Schluter and Hermann Ney. 2001. ModelbasedMCE bound to the true Bayes error. IEEE Signal Processing Letters, 85131133, May.Christoph Tillmann, Stephan Vogel, Hermann Ney, AlexZubiaga, and Hassan Sawaf. 1997. AcceleratedDP based search for statistical translation. In European Conf. on Speech Communication and Technology, pages 26672670, Rhodes, Greece, September.Nicola Ueffing, Franz Josef Och, and Hermann Ney.2002. Generation of word graphs in statistical machine translation. In Proc. Conference on Empirical Methods for Natural Language Processing, pages156163, Philadelphia, PE, July.
