International journal of scientific and engineering research IJSER Volume 2, Issue 5, May  2011 ISSN 22295518 Research Publication Email ijser.editorijser.org IJSER httpwww.ijser.org httpwww.ijser.orgxplore.html httpwww.ijser.orgforum   International Journal of Scientific and Engineering Research IJSER  Journal Information  SUBSCRIPTIONS  The International Journal of Scientific and Engineering Research Online at www.ijser.org is published monthly by IJSER Publishing, Inc., FranceUSAIndia  Subscription rates  Print 50 per issue. To subscribe, please contact Journals Subscriptions Department, Email subijser.org  SERVICES  Advertisements Advertisement Sales Department, Email serviceijser.org  Reprints minimum quantity 100 copies Reprints Coordinator, IJSER Publishing. Email subijser.org   COPYRIGHT  Copyright2011 IJSER Publishing, Inc.  All Rights Reserved. No part of this publication may be reproduced, stored in a retrieval system, or transmitted, in any form or by any means, electronic, mechanical, photocopying, recording, scanning or otherwise, except as described below, without the permission in writing of the Publisher.  Copying of articles is not permitted except for personal and internal use, to the extent permitted by national copyright law, or under the terms of a license issued by the national Reproduction Rights Organization.  Requests for permission for other kinds of copying, such as copying for general distribution, for advertising or promotional purposes, for creating new collective works or for resale, and other enquiries should be addressed to the Publisher.  Statements and opinions expressed in the articles and communications are those of the individual contributors and not the statements and opinion of IJSER Publishing, Inc. We assumes no responsibility or liability for any damage or injury to persons or property arising out of the use of any materials, instructions, methods or ideas contained herein. We expressly disclaim any implied warranties of merchantability or fitness for a particular purpose. If expert assistance is required, the services of a competent professional person should be sought.      PRODUCTION INFORMATION For manuscripts that have been accepted for publication, please contact Email ijser.secretaryijser.org  Contents  1. Translation of Software Requirements Hanan Elazhary.....................................................................................................................................17  2. Tuning of FOPID Controller Using Taylor Series Expansion Ali Akbar Jalali, Shabnam khosravi..................................................................................................812  3. Adaptive Classification of Web Mining Methods and Challenges of Customer Relationship Management Domain Siavash Emtiyaz, MohammadReza Keyvanpour........................................................................ 1317  4. A Study of Carcinogenic Components Removal in Chemical Industry R.Uma Mythili,K.Kanthavel,R.Krishna Raj........................................................................ ..........1822  5. Effects of SocioDemographic Covariates on Blood Glucose Level of the Diabetic Patients A Multiple Classification Analysis MCA Md. Behzad Noor, Dr. Mir Mohammad Azad, Dr. J.A.M. Shoquilur Rahman....................... 2331  6. Optimization of Power Consumption in Wireless Sensor Networks Surendra bilouhan, Prof.Roopam Gupta........................................................................ ..............3236  7. A Robust H Speed Tracking Controller for Underwater Vehicles via Particle Swarm Optimization MohammadPourmahmood Aghababa, Mohammd Esmaeel Akbari....................................... 3743  8. CrankNicolson scheme for numerical solutions of twodimensional coupled Burgers equations Vineet Kumar Srivastava, Mohammad Tamsir, Utkarsh Bhardwaj, YVSS Sanyasiraju.........4450  9. Implementation of Adaptive Modulation and Coding Technique using Matlab Part I Physical Layer Design Sami H. O. SALIH, Mamoun M. A. SULIMAN........................................................................ ...5154  10. A QoS Based Web Service Selection through Delegation G. Vadivelou, E. IIavarasan, R. Manoharan, P. Praveen..............................................................5563  11. Iatrogenic effects of Orthodontic treatment  Review on white spot lesions Sangamesh B., Amitabh Kallury........................................................................ .............................6470   Contents  12. A Study Of Effects Of Disease Caused Death In A Simple Epidemic Model Dr. Sunil Kumar Singh, Dr. Shekh Aqeel.......................................................................................7173  13. Autonomous Room Air Cooler Using Fuzzy Logic Control System M. Abbas, M. Saleem Khan, Fareeha Zafar....................................................................................7481  14. Handwritten Character Recognition using Neural Network Chirag I Patel, Ripal Patel, Palak Patel...........................................................................................8287  15. A New Approach For Data Encryption Using Genetic Algorithms And Brain Mu Waves Gove Nitinkumar Rajendra, Bedi Rajneeshkaur...........................................................................8791  16. Use of Natural Compounds from Plant Sources as AchE Inhibitors for the Treatment of Early Stage Alzheimers diseaseAn Insilico Approach Amrendar Kumar, Abhilasha Singh, Biplab Bhattacharjee........................................................ 9297  17. Hybrid Mechanical Charger Ayush R Jain, Chinmay V Harmalkar..........................................................................................98100  18. A Novel Realtime Intelligent Tele Cardiology System Using Wireless Technology To Detect Cardiac Abnormalities S.Kohila, K.Gowri..........................................................................................................................101107  19. A Novel Dynamic Key Management Scheme Based On Hamming Distance for Wireless Sensor Networks R.Divya, T.Thirumurugan............................................................................................................108114  20. Speedy Deconvolution using Vedic Mathematics Rashmi K. Lomte, Prof.Bhaskar P.C...........................................................................................115118  21. Different Approaches of Spectral Subtraction method for Enhancing the Speech Signal in Noisy Environments Anuradha R. Fukane, Shashikant L. Sahare..............................................................................119124  22. Economic Power Dispatch using Artificial Immune System R.Behera, B.B.Pati, B.P.Panigrahi................................................................................................125130  23. Mining Knowledge Using Decision Tree Algorithm S.V. Kulkarni........................................................................ .........................................................131136 Contents  24. New Approach To Weighted Pattern Sequential MiningDataset D.Saravana Kumar, N.Ananthi, D.Yadavaram.........................................................................137141  25. Decreasing Inventory Levels Fluctuations by Moving Horizon Control Method and Move Suppression in the Demand Network Mohammad Miranbeigi, Aliakbar Jalali.....................................................................................142146  26. Receding Horizon Control on Large Scale Supply Chain Mohammad Miranbeigi, Aliakbar Jalali.....................................................................................147151  27. Load Forecasting Using New Error Measures In Neural Networks Devesh Pratap Singh, Mohd. Shiblee, Deepak Kumar Singh.................................................152157  28. Effect of Temperature on Deformation Characteristics of Gold Ball Bond in AuAl Thermosonic Wire Bonding Gurbinder Singh, Othman Mamat..............................................................................................158162  29. Sorption Of Cr Vi  AsV On Hdtma  Modified Zeolites Vandana Swarnkar, Nishi Agrawal, Radha Tomar..................................................................163171  30. Analysis Of A Population Of Diabetic Patients Databases In Weka Tool P.Yasodha, M.Kannan...................................................................................................................172176  31. A Study and Application on CrossDisciplinary Proficiency Learning of Artificial Intelligence Prof.Pavan .G.P, Dr.G.Lavanya Devi, Dr.P.Srinivasa Rao.......................................................177180  32. A Navel Approach for Evaluating Completeness of Business Rules using First Order Logic M.Thirumaran, E. IIavarasan, R. Manoharan, Thanigaivel.K.................................................181191  33. Implementation of RSA Cryptosystem Using Verilog Chiranth E, Chakravarthy H.V.A, Nagamohanareddy P, Umesh T.H, Chethan Kumar M........................................................................ .............................................................................192198  34. Metrics For Component Based Measurement Tools P. Edith Linda, V. Manju Bashini, S. Gomathi...........................................................................199204  35. Reactive Power Management For Wind Electric Generator Er. V. Karunakaran, Er. R. Karthikeyan.....................................................................................205212 Contents  36. Security Challenges  Preventions in Wireless Communications Kashif Laeeq...................................................................................................................................213220  37. Compost Adsorption Desdorption of Picloram in the Presence of Surfactant on Six Agricultural Soils Rounak M. Shariff..........................................................................................................................221229  38. A Few Aspects of Power Quality Improvement Using Shunt Active Power Filter Subhransu Sekhar Dash, C.Nalini Kiran, S.Prema Latha....................................................... 230240  39. Tidal Power An Effective Method of Generating Power Shaikh Md. Rubayiat Tousif, Shaiyek Md. Buland Taslim..................................................... 241245  40. Design and Development of Fault Tolerent Control system for an Infant Incubator Suswetha Parisineti, Eswaran.P...................................................................................................246251  41. Behavior of Eight Bus System with TCIPC V.V.Satyanarayana Rao.R, S.Rama Reddy.................................................................................252256  42. Implementing Anti Collision Algorithm for Multiple Tag Identification Sindhura kodali, Eswaran.P.........................................................................................................257261  43. Face Recognition using Neural Network and Eigenvalues with Distinct Block Processing Amit Kumar, Prashant Sharma, Shishir Kumar........................................................................262277  44. Molecular Biocoding of Insulin  Amino Acid Gly Lutvo Kuri.................................................................................................................................... 278294  45. Electrical Power Generation Using Piezoelectric Crystal Anil Kumar.................................................................................................................................... 295297  International Journal of Scientific  Engineering Research, Volume 2, Issue 5, May2011      ISSN 22295518  IJSER  2011 httpwww.ijser.org  Translation of Software Requirements  Hanan Elazhary AbstractStakeholders typically speak and express software requirements in their native languages. On the other hand, software engineers typically express software requirements in English to programmers who program using Englishlike programming languages. Translation of software requirements between the native languages of the stakeholders and English introduces further ambiguities. This calls for a system that simplifies translation of software requirements while minimizing ambiguities. Unfortunately, this problem has been overlooked in the literature. This paper introduces a system designed to facilitate translation of requirements between English and Arabic. The system can also facilitate the analysis of software requirements written in Arabic. This is achieved through enforcing writing software requirements statements using templates. Templates are selected such that they enforce following best practices in writing requirements documents.  Index Terms Requirements, Software Engineering, Translation         1 INTRODUCTIONOFTWARE requirements engineering is concerned with understanding and specifying the services and constraints of a given software system. It involves software requirements elicitation and specification 1. Elicitation of software requirements from stakeholders typically results in user requirements, which are natural language statements that describe the highlevel goals of a given software system 2. Analysis of natural language user requirements is an important activity since imprecision in this stage causes errors in later stages. Requirements imprecision is at least an order of magnitude more expensive to correct when undetected until late software engineering stages 3. Thus, focusing on improving the precision of the elicited user requirements in the first cycle is one of the ambitious aims of software engineering 4. One of the main causes of imprecision is the ambiguity of natural languages used to express the user requirements 5. To minimize ambiguity, a number of best practices in writing requirements documents have been proposed by experts 69. These practices include  Maintain terminological consistency and clarity by restricting action and actor descriptions to terms that are clearly defined in a glossary.  Do not use different phrases to refer to the same entity For example, do not use Order Processing System and Order Entry System to refer to the same system.  Avoid using phrases, such as easy to use, whose meaning is subjective and leads to ambiguity.  Write each requirement as a single separate sentence.  Write complete sentences rather than bulleted buzz phrases.  Write complete activevoice sentences which clearly specify the actoragent and the action.  Write requirements sentences in a consistent fashion using a standard set of syntaxes with each syntaxtype corresponding to and signaling different kinds of requirements.  Associate a unique identifier with each requirement. While these practices are relatively easy to state and understand, it seems fairly difficult for requirements engineers to consistently apply them throughout requirements documents with thousands of requirements. Thus, some tools in the literature have been developed to help users adhere to best practices in writing requirements documentation. This also simplifies the automatic analysis of requirements documents written in natural language and allows generating warning messages when the requirements do not conform to best practices 9. One of the problems that have been overlooked in the literature is the problem of software requirements translation. Stakeholders typically speak their native languages, while requirements documents are typically written in English and software programs are typically developed in Englishlike programming languages. The problem is that the translation of software requirements from the native language of stakeholders to English introduces further ambiguities. Whenever errors are discovered in later stages of the software engineering process, suggested modifications of the software requirements result. To negotiate these modifications with the stakeholders, modified requirements need to be translated between English and the native language of the stakeholders back and forth. This can introduce more ambiguities that complicate the problem even more. To address this problem, we suggest implementing systems that helps users adhere to best practices in writing requirements documents using different natural languages. This simplifies analyzing requirements documents in the natural language of the stakeholders. By specifying the mappings between the different developed systems, we allow translation of software requirements between different natural languages while minimizing ambiguities. In this paper, we introduce the Arabic Requirements Analysis Tool ARAT system that has been designed to handle software requirements in Arabic. The reason for selecting the Arabic language is that it is the official language of hundreds of millions of people in the Middle East and North Africa. It is expected that a large number of these targeted users would benefit from ARAT. We also specify mappings between our system and a similar system called Requirements Analysis Tool RAT 9. RAT handles requirements written in English. Mappings simplify translation of natural language requirements between English and Arabic, while minimizing ambiguities. S1International Journal of Scientific  Engineering Research, Volume 2, Issue 5, May2011      ISSN 22295518   IJSER  2011 httpwww.ijser.org  The paper is organized as follows Section 2 describes related research in the literature. Section 3 describes the RAT system and Section 4 describes the ARAT system and how the Arabic requirements are analyzed using it. Section 5 provides examples that illustrate the mappings between the English syntaxes in the RAT system and the Arabic syntaxes in the ARAT system. The examples also illustrate how translation is performed between English and Arabic requirements accordingly while minimizing ambiguities. Finally, Section 6 provides conclusions and directions for future research. 2 REALTED WORK   Many tools in the literature have been developed to automatically analyze natural language software requirements. Lami 10 and Hussain et al. 11 developed systems that can automatically detect potential imprecision in natural language software requirements through indicators such as weak verbs. But, these systems dont assist in correcting any imprecision.  Another approach in the literature attempts to avoid the introduction of imprecision while the software requirements are being written by imposing the use of natural language patterns. Some of these have focused on developing natural language patterns for specific domains such as database systems 12, scenarios 13, and embedded systems 5. General purpose systems include Raven 14, which can analyze use cases. Jain et al. 9 developed the generalpurpose RAT system that imposes the use of specific natural language patterns that help users adhere to best practices in writing software requirements in different situations and can provide useful advices.  The REAS system 15 attempts to integrate these two approaches intelligently to exploit their advantages and avoid their disadvantages, but cannot help in the translation of requirements. Thus, the prposed system emulates the RAT system and uses the analogy ti simplify the translation of requirements between Arabic and English while minimizing ambiguities.  3 THE RAT SYSTEM In this section, we describe the structure of an English requirements document according to the RAT system and discuss how the requirements document is analyzed accordingly. 3.1 UserDefined Glossaries Userdefined glossaries should be created to define valid entities and actions in the requirements document. This helps requirements engineers adhere to one of the best practices in writing requirements documents that is using entities and actions consistently. As will be explained later in Section 3.3, the terms in the glossaries will also be used as placeholders that help in the analysis of the requirements.  The entity glossary defines all entities in the requirements document. It also specifies whether each entity is an agent that can perform actions or not. Agents and nonagents will be referred to as agents and entities respectively throughout the paper. Snapshots of an entity glossary and an action glossary are shown in Tables 1 and 2 respectively. 3.2 Best Practices Syntaxes A set of syntaxes has been defined in the RAT system to help requirements engineers adhere to best practices in writing requirements documents as explained in Section 1. These syntaxes are as follows a The syntax Agent Phrase shall  must  will Action Phrase is used to express a requirement that an agent is responsible for carrying out some action. For example The Web server must inform administrator of failed login attempts. b The syntax Agent Phrase shall  must  will be able to Action Phrase is used to express a requirement that an agent should have the ability to perform an action. For example The payroll system shall be able to deduct loan amounts from paychecks. c The syntax Agent Phrase shall  must will allow  permit Agent Phrase to Action Phrase is used to express a requirement that an agent should provide another agent with the capability to perform an action. For example The order processing system must permit administrator to view daily transactions. d The syntax Agent Phrase shall  will  may only  not Action Phrase when  if condition is used to express imposed conditions or constraints on actions performed by agents. For example The account management system shall only close an account if the current balance is zero.  e The syntax Only Agent Phrase may  maybe Action Phrase is used to express imposed conditions on agents who may perform an action or to whom an action may be performed. For example Only the payroll employees may access the payroll database. TABLE 1 A SNAPSHOT OF AN ENTITY GLOSSARY  Entity Descriptor Explanation Is Agent order processing system System for processing    orders Yes Web server HTTP Web Server Yes finance department user User from finance        department Yes chemical containers Containers that store acids No customer standing The status of the customer No TABLE 2 A SNAPSHOT OF AN ACTION GLOSSARY  Action Descriptor Explanation process payroll Action for processing of payroll. inform administrator Action for sending email            notification to the administrator. send contracts data Action for transfer of contract   data. Display Rendering an item on screen.  2International Journal of Scientific  Engineering Research, Volume 2, Issue 5, May2011      ISSN 22295518   IJSER  2011 httpwww.ijser.org  f The syntax Entity Phrase  Agent Phrase must always  never  not be  have Value Phrase is used to express imposed constraints on attributes or values of attributes of entities or agents. For example The customer standing must always be gold, silver, or bronze.  g The syntax Entity Phrase  Agent Phrase is defined as  classified as  Entity Phrase is used to express a definition of an entity or an agent. For example The total sales value is defined as total item value plus sales tax.   h The syntax Entity Phrase  Agent Phrase is  is not Action Phrase is used to express policies that should be adhered to. For example The sales tax is computed on instate shipments.  It is clear that each category of requirements is expressed by a specific syntax type with different keywords. This simplifies understanding the intent of each requirement and its analysis as explained in Section 3.3. It should be noted that these syntaxes can be written in the format of the conditional syntax d. 3.3 Analyzing Requirements RAT uses a two phased approach for the analysis of the requirements document 1 lexical analysis and 2 syntactic analysis.    Fig. 1. A state machine for syntax type a of the RAT system 9. 3.3.1 Lexical analysis A lexical analyzer breaks down a given requirement into a set of tokens agent phrases, entity phrases, action phrases, or modal phrases formed of keywords such as shall, will and shall be able to. For example, the following statement The SAP system shall send the vendor data to the order processing system is tokenized into the agent phrase The SAP system, the modal phrase shall, the action phrase send, the entity phrase the vendor data, the unknown phrase to and the agent phrase the order processing system. After tokenization, the requirement is classified into one of the syntax types based on the modal phrases. Thus, according to the above modal phrase shall, the requirement follows syntax type a.  3.3.2 Syntactic analysis The syntactic analyzer has a different sate machine to validate each syntax type. The tokenized requirement is run through the corresponding state machine. A requirement is treated as syntactically correct when the state machine successfully transitions from the start state to a valid final state. In the above example, the state machine shown in Figure 1 is used. The token stream for the requirement will end up in Action State and so will be treated as a valid requirement.   For every error state, there is a predefined warning message that is displayed to the user. The statement of each warning message explains why the requirement deviates from best practices in writing requirements documents. Table 3 shows the warning messages corresponding to different error states in the above state machine. For example, the statement shall display error messages in new window halts in Missing Agent State since it lacks an agent phrase in the beginning. The generated warning message is This requirement lacks an agent before shall. It can be confusing to leave the agent implicit.  3.3.3 Early Deployment Results Eleven real industrial software projects have been used to assess the tool. They used RAT to make changes to the requirements based on the warning messages generated by RAT. This resulted in   1030 reduction in time required to transform notes taken in interview sessions to well formed requirements.  3050 reduction in time needed to review requirements.  5 estimated reduction in overall budget, due to expected reduction in requirements defects and associated reduction in rework.  TABLE 3 ERROR STATES AND THE CORRESPONDING WARNING MESSAGES  Error State Warning Message Missing Agent State This requirement lacks an agent before variable at which error occurs. It can be confusing to leave the agent implicit. Unknown Action State This requirement contains variable at which error occurs where an action is expected, but variable at which fault occurs is not in the action glossary. Unknown Agent State This requirement contains variable at which error occurs where an agent is expected, but variable at which fault occurs is not in the entity glossary. Non Agent Entity State This requirement contains variable at which error occurs where an agent is expected. variable at which error occurs is in the entity glossary but is not designated as an agent. Missing Action State This requirement lacks an action before variable at which error occurs. It can be confusing to leave the action implicit. 3International Journal of Scientific  Engineering Research, Volume 2, Issue 5, May2011      ISSN 22295518   IJSER  2011 httpwww.ijser.org  4 THE ARAT SYSTEM The ARAT system is an Arabic version of the RAT system. The advantage of this system is twofold. First, it has similar advantages as the RAT system but with respect to Arabic requirement documents. In other words, it helps requirements engineer adhere to best practices in writing Arabic requirements documents and simplifies the analysis of these documents. Second, due to the analogy between both systems, it allows the translation of software requirements between Arabic and English while resolving many ambiguities. Thus, an Arabic syntax and a corresponding state machine in the ARAT system has been designed corresponding to each English syntax and its corresponding state machine in the RAT system. While reading the Arabic syntaxes, it should be noted that  Arabic statements and thus Arabic syntaxes are written from the right to the left.   Some syntaxes in RAT are decomposed into two syntaxes in ARAT due to the nature of the Arabic language.   Some model phrases in RAT are represented by two modal phrases in the corresponding syntax in ARAT due to the nature of the Arabic language. For example be able to is represented by    and    for masculine and feminine respectively.   The meanings of the Arabic modal phrases in these syntaxes are provided in the modal phrases dictionary shown in Table 4.   The arrangements of the components of a given syntax in ARAT may be different from that of the corresponding syntax in RAT due to the nature of the Arabic language. The ARAT Arabic syntaxes corresponding to the RAT English syntaxes described in Section 3.2 are as follows      a Agent Phrase     Action Phrase b Agent Phrase                   Action Phrase  c Agent Phrase        Agent Phrase  Action Phrase. d Agent Phrase                                                                 Action Phrase     condition Agent Phrase                                                                                                                             Action Phrase     condition e  Agent Phrase  Action Phrase f Entity Phrase  Agent Phrase                            Value Phrase  g Entity Phrase  Agent Phrase      Entity Phrase   h Entity Phrase  Agent Phrase Action Phrase Entity Phrase  Agent Phrase   Action Phrase  As an example of analyzing Arabic requirements in ARAT, consider the following Arabic statement                         . This statement is tokenized into the agent phrase     , the modal phrase   , the action phrase   , the entity phrase      , the unknown phrase    , and the agent phrase        . According to the above modal phrase    , the requirement follows syntax type a. When this token stream is run through the corresponding state machine, it ends up in Action State and so is treated as a valid requirement. In the next section, we provide examples that illustrate the mappings between the English syntaxes in the RAT system and the Arabic syntaxes in the ARAT system. The examples also illustrate how translation is performed between English and Arabic requirements accordingly while resolving many ambiguities. 5 TRANSLATION BETWEEN ARABIC AND ENGLISH REQUIREMENTS Translating a given statement between RAT and ARAT systems is done by following the following steps  Break down the statement into a set of tokens.  Specify the corresponding syntax.  Translate each modal phrase according to the modal phrases dictionary shown in table 4.  Interpret the unknown phrases and rearrange the statement according to the corresponding goal syntax RAT syntax if translating from Arabic to English and ARAT syntax if translating from English to Arabic.  The rest is left to the software engineer to resolve. This is illustrated by few examples. The first example involves translating Arabic statements to English. Because the reader expects to read mainly English, the rest of the examples will involve translating English statements to Arabic.  5.1 Example 1 Consider the following Arabic statement                                          . This statement is tokenized into the agent phrase     , the modal phrase TABLE 4 THE MODAL PHRASES DICTIONARY  Modal phrases in RAT Corresponding modal phrases in ARAT shall  will  shall  will not  must   must not    may  maybe   may not  may not be    is  is not  be able to      allow  permit    to  only  when  if      always  never   be    have      defined as    classified as    4International Journal of Scientific  Engineering Research, Volume 2, Issue 5, May2011      ISSN 22295518   IJSER  2011 httpwww.ijser.org    , the action phrase   , the entity phrase      , the unknown phrase    , and the agent phrase         remember that the Arabic statements are written from the right to the left. According to the above modal phrase, the requirement follows syntax type a. The unknown phrases are interpreted as extensions of the action phrase. According to the modal phrases dictionary, this modal phrase is translated into shall  will. The statement is then rearranged as follows      shall  will                                               .  The software engineer can then translate    to The SAP system,   to send, and               to the vendor data to the order processing system withoutwith minimal ambiguity. Note that the words shall and will have the same meaning. The translated statement is   The SAP system shall  will send the vendor data to the order processing system.  5.2 Example 2 Consider the following English statement The Web server must inform administrator of failed login attempts. This statement is tokenized into the agent phrase The Web server, the modal phrase must, the action phrase inform administrator, the unknown phrase of, and the entity phrase failed login attempts. According to the above modal phrase, the requirement follows syntax type a. The unknown phrases are interpreted as extensions of the action phrase. According to the modal phrases dictionary, the modal phrase must is translated into    . The statement is then rearranged as follows remember that the Arabic statements are written from the right to the left   The Web server                                                                                                    inform administrator of failed login attempts The software engineer can then translate The Web server to      , inform administrator of failed login attempts to         withoutwith minimal ambiguity. The translated statement is              5.3 Example 3 Consider the following English statement The payroll system shall be able to deduct loan amounts from paychecks. This statement is tokenized into the agent phrase The payroll system, the modal phrase shall, the modal phrase be able to, the action phrase deduct loan amounts, the unknown phrase from, and the entity phrase paychecks. According to the above modal phrases, the requirement follows syntax type b. The unknown phrases are interpreted as extensions of the action phrase. According to the modal phrases dictionary, the modal phrase shall is translated into    and the modal phrase be able to is translated into         . The statement is then rearranged as follows   The payroll system                  deduct loan amounts from paychecks The software engineer can then translate The payroll system to         and deduct loan amounts from paychecks to          withoutwith minimal ambiguity. Since      is masculine in Arabic, the software engineer selects      rather than     . The translated statement is              5.4 Example 4 Consider the following English statement The order processing system must permit administrator to view daily transactions. This statement is tokenized into the agent phrase The order processing system, the modal phrase must, the modal phrase permit, and agent phrase administrator, the modal phrase to, and the action phrase view daily transactions. According to the above modal phrases, the requirement follows syntax type c. According to the modal phrases dictionary, the modal phrase must is translated into    , the modal phrase permit is translated into       , and the modal phrase to is translated into  . The statement is then rearranged as follows   The order processing system       administrator  view daily transactions The software engineer can then translate The order processing system to      , administrator to   , and view daily transactions to        withoutwith minimal ambiguity. Since    is masculine in Arabic, the software engineer selects     rather than    . The translated statement is             5.5 Example 5 Consider the following English statement The account management system shall only close an account if the current balance is zero. This statement is tokenized into the agent phrase The account management system, the modal phrase shall, the modal phrase only, the action phrase close an account, the modal phrase if, and the unknown phrase the current balance is zero. According to the above modal phrases, the requirement follows syntax type d. The unknown phrase is interpreted as a condition. According to the modal phrases dictionary, the modal phrase shall is translated into   , the modal phrase only is translated into   , and the modal phrase if is translated into         . The statement is then rearranged as follows   The account management system               close an account                                        the current balance is zero  The software engineer can then translate The account management system to       , close an account to      , and the current balance is zero to        withoutwith minimal ambiguity. Note that the words      ,  , and     have the same meaning. The translated statement is                 5.6 Example 6 Consider the following English statement Only the payroll 5International Journal of Scientific  Engineering Research, Volume 2, Issue 5, May2011      ISSN 22295518   IJSER  2011 httpwww.ijser.org  employees may access the payroll database. This statement is tokenized into the modal phrase only, the agent phrase the payroll employees, the modal phrase may, and the action phrase access the payroll database. According to the above modal phrases, the requirement follows syntax type e. According to the modal phrases dictionary, the modal phrase only is translated into     and the modal phrase may is translated into    . The statement is then rearranged as follows    the payroll employees  access the payroll database The software engineer can then translate the payroll employees to        and access the payroll database to        withoutwith minimal ambiguity. The translated statement is              5.7 Example 7 Consider the following English statement The customer standing must always be gold, silver, or bronze. This statement is tokenized into the entity phrase The customer standing, the modal phrase must, the modal phrase always, the modal phrase be, and the unknown phrase gold, silver, or bronze. According to the above modal phrases, the requirement follows syntax type f. The unknown phrase is interpreted as a value phrase. According to the modal phrases dictionary, the modal phrase must is translated into    , the modal phrase always is translated into    , and the modal phrase be is translated into    . The statement is then rearranged as follows   The customer standing       gold, silver, or bronze  The software engineer can then translate The customer standing to      and gold, silver, or bronze to                        withoutwith minimal ambiguity. Since       is masculine in Arabic, the software engineer selects   rather than  . The translated statement is            5.8 Example 8 Consider the following English statement The total sales value is defined as total item value plus sales tax. This statement is tokenized into the entity phrase The total sales value, the modal phrase is, the modal phrase defined as, and the entity phrase total item value plus sales tax. According to the above modal phrases, the requirement follows syntax type g. According to the modal phrases dictionary, the modal phrase is is translated into , and the modal phrase defined as is translated into    . The statement is then rearranged as follows     The total sales value                         total item value plus sales tax The software engineer can then translate The total sales value to     and total item value plus sales tax to          withoutwith minimal ambiguity. Since      is masculine in Arabic, the software engineer selects     rather than    . The translated statement is           5.9 Example 9 Consider the following English statement The sales tax is computed on instate shipments. This statement is tokenized into the entity phrase The sales tax, the modal phrase is, the action phrase computed on instate shipments. According to the above modal phrase, the requirement follows syntax type h. According to the modal phrases dictionary, the modal phrase is is translated into . The statement is then rearranged as follows   The sales tax computed on instate shipments  The software engineer can then translate The sales tax to      and computed on instate shipments to        withoutwith minimal ambiguity. The translated statement is         5.10 Results From the above examples, it is clear that many ambiguities have been resolved while translating software requirements between English and Arabic. These include specifying the agents, the entities, the actions, who performs each action, and to whom an action is performed. Besides, the intent of each statement is well understood. Few ambiguities are left to the software engineer to resolve, which dramatically simplifies the work of the software engineer. 6 CONCLUSION Stakeholders typically speak and express software requirements in their native language. On the other hand, software engineers typically express software requirements in English to programmers who write program using Englishlike programming languages. This requires translating requirements between the native language of the stakeholders and English back and forth. This introduces further ambiguities, which affects the length of the whole software process. Unfortunately, this problem has been overlooked in the literature. To tackle this problem, this paper introduces the ARAT system. The ARAT system is analogous to the RAT system 9 that uses templates to express English software requirements. As illustrated by many examples, specifying mappings between both systems facilitate translation of requirements between English and Arabic while resolving many ambiguities such as specifying agents and their actions. Since the RAT system uses templates that enforce following best practices in writing requirements documents in English, the ARAT system achieves the same while writing requirements documents in Arabic. Similarly, since the RAT system has been designed to facilitate the analysis of English software requirements, the ARAT system facilitates the analysis of Arabic software requirements. As future work, the tool should be tested against more requirements documents. Observings and suggestions should be taken into consideration in future versions of the system. We encourage 6International Journal of Scientific  Engineering Research, Volume 2, Issue 5, May2011      ISSN 22295518   IJSER  2011 httpwww.ijser.org  researchers to develop similar systems in their native languages with the aim of facilitating communication between stakeholders and software engineers using different languages. REFERENCES 1 I. Sommerville, Software Engineering. Addison Wesley, 8th edition, 2006. 2 A. Lamsweerde, R. Darimont, and E. Letier, Managing Conflicts in GoalDriven Requirements Engineering, IEEE Transactions on Software Engineering, vol. 24, no. 11, pp. 908926, 1998. 3 S. Schach, ObjectOriented and Classical Software Engineering. McGrawHill, 7th edition, 2006. 4 C. Rupp, Requirements and Psychology, IEEE Software, vol. 19, no. 3, pp.1618, 2002.  5 C. Denger, D. Berry, and E. Kamsties, Higher Quality Requirements Specifications through Natural Language Patterns, Proc. IEEE SwSTE03, 2003. 6 K. Wiegers, Software Requirements. Microsoft Press, 2003. 7 R. Young, Effective Requirements Practices. AddisonWesley Longman Publishing Co., 2000. 8 IEEE Recommended Practice for Software Requirements Specifications, IEEEANSI Standard 8301998, Institute of Electrical and Electronics Engineers, 1998.   9 P. Jain, K. Vema, A. Kass, and R. Vasquez, Automated Review of Natural Language Requirements Documents Generating Useful Warnings with Userextensible Glossaries Driving a Simple State Machine, Proc. Second India Software Engineering Conference, 2009. 10 G. Lami, QuARS A Tool for Analyzing Requirements, Technical Report CMUSEI2005TR014, Carnegie Mellon Software, Engineering Institute, PA, USA, 2005. 11 I. Hussain, O. Ormandjieva, and L. Kosseim, Automatic Quality Assessment of SRS Text by Means of a DecisionTreeBased Text Classifier, Proc. the 7th International Conference on Quality Software, 2007. 12 A. Ohnishi, Software Requirements Specification Database Based on Requirements Frame Model, Proc. the 2nd International Conference on Requirements Engineering, 1996. 13 C. Ben Achour, Guiding Scenario Authoring, Proc. the 8th EuropeanJapanese Conference on Information Modeling and Knowledge Bases, 1998. 14 Raven Software, www.ravensoft.com. 15 H. Elazhary, REAS An Interactive SemiAutomated System for Software Requirements Elicitation Assistance, Int. Journal of Engineering Science and Technology, vol. 2, no. 5, pp. 958962, 2010. 7International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                  ISSN 22295518   IJSER  2011 httpwww.ijser.org  Tuning of FOPID Controller Using Taylor Series Expansion Ali Akbar Jalali, Shabnam Khosravi   Abstract In this paper, a direct synthesis approach to fractional order controller design  Is investigated. The proposed algorithm makes use of Taylor series of both desired closedloop and actual closedloop transfer function which is truncated to the first five terms. FOPID Controller parameters are synthesized in order to match the closedloop response of the plant to the desired closedloop response. The standard and stable secondorder model is considered for both plant and the desired closedloop transfer functions. Therefore for a given plant with damping ratio 1  and natural frequency 1n . The tuned FOPID controller results in the desired closedloop response with damping ratio 2 and natural frequency 2n .  An example is presented that indicates the designed FOPID results in actual closedloop response very close to desired response rather than PID controller. It is shown that the proposed method performs better than Genetic Algorithm in obtaining the desired response. Index Terms FOPID controller, Taylor series expansion, second order model.          1 INTRODUCTION                                                                     or many decades, proportionalintegralderivative PID controllers have been very popular in industries for process control applications. The popularity and widespread use of PID controllers are attributed primarily to their simplicity and performance characteristics. Owing to the paramount importance of PID controllers, continuous efforts are being made to improve their quality and robustness 1, 2.  An elegant way of enhancing the performance of PID controllers is to use fractional order controllers where the integral and derivative operators have noninteger orders. Podlubny proposed the concept of fractional order control in 1999 3. In FOPID controller, despite of the proportional, integral and derivative constants, there are two more adjustable parameters the power of s in integral and derivative operators, ,   respectively. Therefore this type of controllers is generalizations of PIDs and consequently has a wider scope of design, while retaining the advantages of classical ones. Several methods have been reported for FOPID design. Vinagre, Podlubny, Dorcak, Feliu 4 proposed a frequency domain approach based on expected crossover frequency and phase margin. Petras came up with a method based on the pole distribution of the characteristic equation in the complex plane 5. In recent years evolutionary algorithms are used for FOPID tuning. YICAO, LIANG, CAO 6, presented optimization of FOPID controller parameters based on Genetic Algorithm. A method based on Particle Swarm Optimization was proposed 7. In this paper a tuning method for FOPID controller is proposed. Suppose a standard and stable second order plant such that desired response is not available. Tuning FOPID controller by the proposed method results in desired closedloop response. The standard second order is considered for desired response. It is shown that the proposed method performs better than Genetic Algorithm in obtaining the desired response. The rest of the paper is organized as follows In section 2 the tuning method for FOPID controller is described. An example is investigated in section 3 and finally Section 4 draws some conclusions. 2    OBTAINING THE TUNING METHOD FOR FOPID CONTROLLER  Consider the block diagram of feedback control system in fig. 1. The objective is design a FOPID controller,  ,cG s  such that for a given plant,  ,pG s  with standard second order model, the actual closedloop response results in desired closedloop response. Desired closedloop response denoted by  dG s and described by standard second order model as follows 22 2  2ndn nG ss s                                                            1 Where , n   are damping ratio and natural frequency of desired response.            F Fig. 1. Block diagram of feedback control system.  8International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org   According to fig. 1. the actual closedloop transfer function is given by     1    p cAclp cG s G sG sG s G s                                                          2                                        In 2, transfer function of FOPID controller is given by   1 ic dkG s k k ss                                                              3                                        Where proportional, integral, derivative constants are denoted by , ,c i dk k k respectively. The orders of integral and derivative actions, ,  , include noninteger values as 0 , 2                                                                                   4 M. Ramasy and Sundaramoorthy in 8 has used different structure for FOPID controller as    cc ckG s G ss                                                                         5 Where 11   ic dkG s s k ss                                                         6  Then for actual closedloop transfer function in 2, we have        c p cAclc p ck G s G sG ss k G s G s                                                    7                                               For designing FOPID controller, it is not possible to set both  AclG s and  dG s equal directly. For each closedloop transfer function, there exist an equal expression but in different structure. This equal expression is Taylor series expansion that can be represented for both  AclG s and  dG s in s  as follows 2           ......2AclAcl Acl Acls GG s G s G                 8  2           .......2dd d ds GG s G s G                        9        In 8, 9, expressions for derivatives of actual closedloop transfer function involve derivatives of FOPID controller.  1 1   c c i dG s k k s k s                                               10a 2 2    1  1 c c i dG s k k s k s                              10b  33    1 2 1 2 c c idG s k k sk s                                               10c 4 44    1 2 3 1 2 3 c c idG s k k sk s                                     10d 5 55    1 2 3 4 1 2 3 4 c c idG s k k sk s                               10e However derivatives of FOPID controller are not defined at 0s  . For convenience and avoiding complexity that causes by noninteger orders of the Laplace variable s, it is proposed to evaluate 8 and 9 at 1  . FOPID controller has five tuning parameters. Therefore five independent equations are needed for tuning FOPID controller.  1 1Acl dG G                                                                       11a 1 1Acl dG G                                                                       11b 1 1Acl dG G                                                                       11c 1 1Acl dG G                                                                       11d 4 41 1Acl dG G                                                                      11e 5 51 1Acl dG G                                                                      11f In obtaining design parameters of FOPID controller, the first terms in 8 and 9 are not considered. According to 1 and 7,  ,  Acl dG s G s  are equal in 0s  . Probably these values are nearly equal in 1s . Using 2, the equations in 11 can be written as 21 1 1 11 01 1 1c p c pdc pG G G GGG G                                     12a  23 1 1 2 1 1 1 111 1 2 1 1 1 11 01 1 1c p c p c pc p c p c pdc pG G G G G GG G G G G GGG G                 12b 234 1 1 3 1 1 3 1 1 111 1 1 6 1 1 1 1 1 1 2 1 1 1 1 6 1 1 1 1 11 1 1c p c p c p cp c p c p c pc p c p c p c pc p c pdc pG G G G G G GG G G G G G GG G G G G G G GG G G GGG G                 0   9International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org                                                                                               12c  44 322 1 1 4 1 1 6 1 14 1 1 1 11 1 16 1 1 2 1 1 1 11 1 1 8 1 1 1 1 1 1 3 1c p c p c pc p c p c pc p c p c pc p c p c pc p cG G G G G GG G G G G GG G G G G GG G G G G GG G G                22445 1 3 1 11 11 1 1 36 1 11 1  1 1 2 1 1 111 1 1 24 1 1 11 1 01 1 1p c pc p c p c pc p c p c p cp c p c p cpdc pG G GG G G G G GG G G G G G GG G G G G GGGG G                                                                                                            12d 5 44 54 1 1 5 1 1 10 1 110 1 1 5 1 1 1 11 1 1 20 1 1 2 1 11 1 1 1 3 1 1 3 11c p c p c pc p c p c pc p c p c pc p c p c p cpG G G G G GG G kG G G GG G G G G GG G G G G G GG                  3224 1 11 1 1 90 11 2 1 1 1 1  1 11 11 1 1 10 1 11 1 1 1 4 1 1 61 1 4 1c p c p cp c p c p c pc p c p c pc p c p c pc p cG G G G GG G G G G G GG G G G G GG G G G G GG G G                  43 2p2 31 1 11 11 60 1 1 1 1  1 13 1 1 3 1 G 1 1 111 1 240 1 1 1 1  11 2 1 1p c p cp c p c p c pc c p c c pc p c p c p cp c pG G G GG G G G G G Gk G G G G GG G G G G G GG G G                5561 11 1 1120 1 1 1 11 01 1 1c p c pc p c pdc pG G G GG G G GGG G    12e Where 1 1 c c i dG k k k                                                             13a 1  c c i dG k k k                                                             13b 1   1  1c c i dG k k k                                            13c 1   1 2  1 2c c i dG k k k                           13d 4 1   1 2 3  1 2 3c c i dG k k k                        13e 5 1   1 2 3 4 1 2 3 4c c idG k kk                                    13f The nonlinear equations in 12 are complicated to obtain design parameters, , , , , .c i dk k k    thus a nonlinear optimization problem must be solved. The fsolve command in optimization toolbox of Matlab is a sufficient tool for solving the set of nonlinear equations. The input arguments of this command are described as fun, x0 and options. Fsolve starts at an initial value for design parameters, x0, in order to solve the set of nonlinear equations described in fun. The argument of option determines the type of optimization algorithm which is used in solving the nonlinear equations. The output arguments are the solution of nonlinear equations, x, and the value of objective function at x. Consider the nonlinear equations in 12 as the objective functions. In this case the solution, x, is the design parameters of FOPID controller, , , , , .c i dk k k   The optimization algorithm of LevenbergMarquert and GaussNewton are considered. In the next section, the proposed tuning method is illustrated during an example.   3 EXAMPLE In this section, a process from 9 is considered. The example involves the speed control of a DC motor. Since the most basic requirement of a motor is that it should rotate at the desired speed, the steadystate error of the motor speed should be less than 1. We want to have settling time of 2s and an overshoot of 4. A desired closedloop transfer function that includes all of the design specifications, can be defined as follows  28.9401 4.2398 8.9401dG ss s                                                 14 The transfer function of the process is defined by   220.02  0.099912 20.02pG ss s                                             15 Applying the values of 41, 1, 1, 1, 1p p p p pG G G G G    4 55 1, 1, 1, 1, 1, 1, 1p d d d d d dG G G G G G G   in 12, five nonlinear equations are obtained. Using the fsolve command in optimization toolbox of Matlab, the fractional order controller is designed as 0.11.021.11  18.271 0.563 cG s ss                                              16 After designing the FOPID controller, the values of nonlinear equations in 12, are 4 4 4 4 310 , 10 , 10 , 10 ,10 .         It is obvious that the obtained values for design parameters, , , , , .c i dk k k   , are very close to roots of nonlinear eq10International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org  uations in 12. Furthermore the values of both actual and desired closedloop transfer functions at 1s   are nearly equal with difference of 310 . Thus the first six terms of Taylor series of actual closedloop transfer function are equal to same order terms for desired one. The accuracy of how much the actual closedloop response is close to desired response can be concluded from fig. 2. For comparison, the tuned FOPID controller using Genetic Algorithm is considered. Fig. 3 shows that the closedloop response under FOPID controller, which is tuned by both GA and proposed methods, in comparison to desired response. Data about specifications of desired closedloop response are collected in Table1.                                TABLE 1 SPECIFICATIONS OF DESIRED CLOSEDLOOP RESPONSE  Maximum overshoot Settling time Rise time Steadystate error 4 2 0.9 0  Table. 1. reports the maximum overshoot in , settling time and rise time in second and steadystate error in  for the closedloop step response. Data about the performance of closedloop system under FOPID and PID controllers against unit step, are collected in Table 2.   TABLE 2 SUMMERY OF THE PERFORMANCE OF CLOSEDLOOP SYSTEM UNDER FOPID AND PID CONTROLLER AGAINST UNIT STEP  Different controllers Maximum overshoot Settling time Rise time Steadystate error FOPID using proposed method 4 2.08 0.9 0 FOPID using GA method 4.5 5.3 1.26 0 PID using proposed method 2 1.08 0.86 0   Fig. 2. compares actual closedloop response under both PID and FOPID controllers and desired response. In fig. 2.  PID and FOPID controllers are tuned by proposed method. Due to Table. 2. the actual closedloop response under FOPID controller is very close to desired response rather than applying PID controller. For example the specifications of transient response, such as maximum overshoot, rise time, settling time and steadystate error when using FOPID controller, are very close to desired transient specifications. Using PID controller results in a behavior in t0, which is not appeared when applying FOPID controller. This behavior is because of existing a zero near the origin. Furthermore due to fig.3. and Table. 2 the FOPID controller, which is tuned by proposed method, results in better performance rather than FOPID controller which is tuned by GA method.  Fig. 4. shows the bode diagram of openloop systems, applying both FOPID and PID controllers.       Fig. 3. Step response of closedloop system using FOPID controller obtained by proposed method and Genetic Algorithm tuning method, step response of the desired closedloop system.  Fig. 2. Step response of closedloop system using both FOPID and PID controllers, step response of the desired closedloop system, step response of uncontrolled system. 11International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org                     According to fig.4. applying FOPID and PID controllers which is tuned by proposed method result in a same phase margin of 66 deg. However the difference in values of gain margin is significant. The gain margin of 30.7 dB and 17dB are obtained for FOPID and PID controllers respectively. The FOPID controller which is tuned by Genetic Algorithm results in gain margin and phase margin of 8.2dB and 59 deg respectively. It can be concluded that better performances can be obtained by using the proposed method.      4 CONCLUSION A design method for FOPID controller is proposed. This method is based on Taylor series of both actual and desired closedloop transfer function. The design parameters of controller are used for matching the same order terms of both desired and actual closedloop response. FOPID controller has two design parameters, , , more than PID controller. Thus two more terms in Taylor series are used to match closedloop response to desired response. This causes increasing accuracy in tracking the desired response rather than using PID controller. Furthermore the FOPID controller which is tuned by proposed method performs better than FOPID controller which is tuned by Genetic Algorithm. It can be concluded that that better performances can be obtained by the proposed method.  REFERENCES 1    Y.X. Su, Dong Sun, B.Y. Duan, Design of an enhanced nonlinear PID controller, Mechatronics, Vol 15, No. 8, pp. 10051024, October 2005. 2    Deepyaman Maiti, Ayan Acharya, Mithun Chakraborty, Amit Konar, Ramadoss Janarthanan, Tuning PID and DPI Controllers using the Integral Time Absolute Error Criterion, 4th IEEE International Conference on Information and Automation for Sustainability, Nov 2008.  3   Igor  Podlubny, Fractionalorder systems and DPI  controllers, IEEE Transactions on Automatic Control, Vol 44, No. 1, pp. 208213, JANUARY 1999. 4   Vinagre, B.M., Podlubny, I., Dorcak, L., Feliu, V., 2000. On fractional PID controllers a frequency domain approach. In Proceedings of IFAC Workshop on Digital ControlPID, 2000, Terrassa, Spain. 5    Petras, I., 1999. The fractional order controllers methods for their synthesis and application. Journal of Electrical Engineering 50 910, 284288, Apr 2000. 6     JUNYICAO, JIN LIANG, BINGGANG CAO, optimization of fractional order PID controllers based on genetic algorithms, proceeding of fourth International Conference on Machine Learning and Cybernetics, Guangzhou, Vol 9, pp. 56865689, 1821 August 2005. 7   Deepyaman Maiti, Sagnik Biswas, Amit Konar, Design of a Fractional Order PID Controller UsingParticle Swarm Optimization Technique, 2nd National Conference on Recent Trends in Information Systems Oct 2008. 8     M. Ramasamya, S. Sundaramoorthy, PID controller tuning for desired closedloop responses for SISO systems using impulse response, Computers and Chemical Engineering 32 2008 17731788. 9    Arijit Biswas, Swagatam Das, Ajith Abraham, Sambarta Dasgupta, Design of fractionalorder PID controllers with an improved differential evolution, Enginnering Application of Artificial Intelligence, Vol 22, No. 2, pp. 343350, March 2009. 12International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518   IJSER  2011 httpwww.ijser.org  Adaptive Classification of Web Mining Methods and Challenges of Customer Relationship Management Domain Siavash Emtiyaz, MohammadReza Keyvanpour  Abstract   In recent years, World Wide Web has been extended from research society to the most dominant and general way for communication and broadcasting of information. Web mining is responsible to discover the hidden knowledge, rules and patterns from web. Customers always play a key role for the establishment or mean of crisis for any organization. Web mining is going to be involved in every organization for extracting extra information which is not visible for everyone. The most important application of web mining is in the domain of the ecommerce and economy that leads to the detection of most facts and effective factors in the customer relationship management and efficient services to the customers through the behavior and communicating with system. CRM uses data mining one of the elements of CRM techniques to interact with customers. It is also used for web mining in web domain.Our Analysis provides a roadmap to guide creation concerning the adaptive classification based on webdata mining methods to solve challenges of CRM. For this, we comparison and analyze the application of web mining process to solution challenges of CRM dimensions. Index Terms CRM Challenges, Customer Relationship Management, Web Mining, Data Mining.         1 INTRODUCTION                                                                     HE ecommerce Rapid development Caused to business advancement in the Internet. The CRM became the main part of the work when ecommerce turned to customeroriented from productoriented and products and services were built and personalized according to customers recommendations. It was done because the internetbased technologies are the best framework for implementation of CRM 1. One of the goals of CRM is close relationship between customers and vendors, which is the simplest form of ecommerce. CRM uses data mining one of the elements of CRM techniques to interact with customers. It is also used for web mining in web domain. Web mining is one of the domains of Data mining. Web mining refers to the techniques that automatically retrieve, extract and evaluate information from documents and web services for knowledge discovery. This article proposes data mining methods in the commerce, understanding challenges and ways to manage customer communications with the related rules and classification provided in this area. 2 CRM CONCEPT CRM is collection of steps to create, develop, maintain and optimize longterm and valuable relationships      between Customers and organizations. Furthermore, it is part of the organizations strategy to identify customers, keep them satisfied and change them in to permanent customers. The main architecture includes several layers 3. Fig 1 Operational layer In this layer some tools are provided for company sales and marketing personnel that can control, manage and improve their contacts with customers, accounting and sales information.  Technologies used in the operational layer are responsible for collecting customer data through their contact points such as connection centers, connection management systems, mails, faxes, sale persons, web and etc. Fig 2  Data layer As it is obvious in Figure 1 the collected data from operational part are stored in a repository that is large in size. Analytical layer This layer is one of the most important layers that are responsible to obtain, store, process, interpret and report data to the users which use customers data. This layer provides capabilities for classification of customers to optimize the behavior of company, improve marketing activities and keeping customers. It is done by studying information in information storage.  In terms of ICT and ebusiness development, customers data are gathered more and more and day by day. Analytical layer capabilities can be used for better management of these data. Operational and analytical layers interact with each other. This means that data of operational layer go through to the analysis layer, then the data are analyzed and the results derived would have direct effect on the operational layer. By analyzing this section, Customers are classified and focusing on particular customers is provided for the organization.    T  Siavash Emtiyaz, Departments of Computer Engineering Qazvin Islamic Azad University, Qazvin, Iran. Email siavash1065gmail.com  MohammadReza Keyvanpour, Departments of Computer Engineering Qazvin Islamic Azad University, Qazvin, Iran.  Email keyvanpouralzahra.ac.ir 13International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org  This layer uses statistical analysis and predictive models for making decision, process improvement and other unperceived benefits. Generally, they have been equipped by two types of technology Online analytical processing, Data   mining 3. Online analytical processing is a process that uses one or more resources for collecting data. Data analysis and unification are done dynamically in different levels so they can help in finding, mapping and analyzing the customers patterns based on their past data values.  Application Layer When data are analyzed, and          customers profiles are created, this layer provides some statistics and analyzes the status of sales, marketing and support and it also measures the amount of satisfaction and loyalty of customers then the customers will be categorized based on this method. For example, we can give responsibility to the system for monitoring all of customers behaviors and when the amount of customers purchases exceeds of a certain limit, makes the organization informed to do special works for them, Such as sending email or phone call automatically with the buyer.          Fig 1 CRM Architecture 3                 Fig 2 Operation Layer CRM  3 CRM CHALLENGES Organization before implementing a CRM should be aware of the potential and possible problems, in order to deal with them when it is necessary.  In organizational levels, business should create common activity between different parts that have contacts with customer and all sections related to the CRM in order to be more effective. This established intergroup relationships with customers, have effect on the role of employee within the        organization also this issue can reduce emphasis on participation of the certain sections of the company.  Usually this type of policy changes faced with the opposition in the early stages of performances. One of the important points is that organizations should be maintaining the best employees of sales and service departments they must develop personal skills of talented employees and reward them. For example, in telephone centers, six months is needed to turn an armature person to an experienced one and six month is needed for an experienced person to become specialized and professional.  Employees, who work in the internal sections, should be involved in the discussions related to the customers needs. Customers feedbacks should be considered in the process of developing products and services. Because all parts of organization collaborate in order to gain customer satisfaction, they should create a reward system for injecting the necessary motivation to these domains. The main challenges can be divided into four important cases that organization might face with during the implementing of CRM 2, 6, 7  3.1 Executive Challenges  Initial launching Costs It is considered as one of the challenges of CRM. Organizations may spend large amount of investment on customer management application tools, but some of these tools may have a specific application that can be hard to share them in different parts.  Integrated application tools Organizations need some integrated application tools that have been created based on customers life cycles and related interactions. For example, organizations which need different languages and monetary units for managing customer interactions, cannot implement CRM through traditional technologies and this will be a severe problem, because data types are various and the noise of data is high and inevitable.  Collaboration of various segments CRM is an integrated approach and needs assisting of some parts of business that previously were working independently. Data that are collected in one portion should to share their data with be shared within all segments. It is possible that some sections would be unpleased others.  Challenges that should be noticed in this approach are       Customer Data Marketing Sales Service    Contact Media Integration 14International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org  1 Nonobvious results that often require a combination of the analysis layer techniques data mining, 2 Serious requests for collecting of data before using of analysis layer techniques data mining, 3 Not being aware that what data are available for mining and what kinds of   virtual activities should be done.  3.2 Stategy Challenges These are parts of interorganization challenges which provide forces that any decision should be made is based on it and Organization processes are managed from the starting point to the end within the rules of the             Organization. The raised Challenges in this regard are   1 output results must match the realities of the world,    2 good activities mechanism, 3 combination of the old knowledge.  3.3 Technology Challenges These are one of the key challenges for the realization of policies, processes, people and their interaction with each other. Organizations need an integrated approach to unify the technologies within it. The raised Challenges in this regard are 1 Reliable considerations dont exist for results of analysis and data because of lack of integrated approach in technologies. 2 Finding data for deeper understanding, which cause high accuracy and cost reduction, for example metric product evaluation.  3.4 Customer Dimention Challenges These Challenges that relate to the customer loyalty and maintenance are 1 Deeper models for identifying and developing customer behavior. 2 A framework for evaluating that leads to diagnosing the accuracy of customer understanding. 4 WEB MINING FOR CRM The research method can be divided into three steps     1 collecting and cleaning the information, store the information in the data warehouse.  2 An iterative discovering process by the data mining tools and analysts review of the extracted patterns to generate new set of questions to refine the search. After refine the search, the results of the mining process is be translated to association rules, which stored in the knowledge base.    3 The patterns are good predictors of purchasing behaviors, the CRM process uses the scores generated by the data mining process to sharpen the focus of targeted customers or prospects, thereby increasing response rates and campaign effectiveness. Fig 3 presents the applicable methods of web mining for CRM 1,9,10. A. Data Collection Process Suitable representation, in such a way that it is readable by the mining algorithm in the next step. This process ensures the data set is ready to use for specific mining technique and purpose. In this process, generalization can be done through attribute removal or attribute generalization and aggregation can be done by combines conformed records and add up total amount. B. Data mining process The Fig 4 process model for data mining provides an overview of the life cycle of a data mining project. It contains the corresponding phases of a project, their respective tasks, and relationships between these tasks. At this description level, it is not possible to identify all relationships. There possibly exist relationships between all data mining tasks depending on goals, background and interest of the user, and most importantly depending on the data.                   Fig 4 The CRISPDM process model 8 C. CRM process The CRM process is the most influential customer oriented strategy of the decade. Despite its humble origins it has evolved into a relatively complex strategy. The essentials of a CRM program include focus, commitment to CRM goals and above all a desire to be customer focused 8. Data warehouse, data mining, used to serve the purpose of supporting selection for acquisition, crosssales, and retention of real or potential customers. By running data mining algorithms on customer dataset, data mining can uncover important associations about what products are often purchased together. This knowledge can then be used for product recommendations and product bundling. This knowledge is then used to make a recommendation for a future customer. 4 CONCLUSION The most important results obtained from the above       techniques are reorganization of data and information for easier access, yielding more effective Efficiency and better data classification in order to obtain best results from them. Web mining and CRM integration has advantages and lots of benefits for companies that need to discover profitability of some customers than other customers. 15International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org  Web mining can   identify important customers in large databases.  Intelligent CRM system is created based on web mining analysis on important customers that leads to customers management ,sharing the obtained  information from different channels, reforming of  communications between different parts, organization and investigation of  operational activities and the appropriate understanding of trade. Therefore, customer chooses the connection channel with the company according to his or her own interest to receive the best services .In table 1 Proposed Adaptive Classification Based On WebData Mining Methods to Solve Challenges of CRM are shown.  References 1 ShiMing Huang, Irene Kwan and ShingHan Li Web Mining for CRM an Empirical Study of computer Game Service Company, International Journal of Electronic Business Management, Vol. 1, No. 1, pp. 3645 2003. 2 Jaideep SrivastavaData Mining  for Customer Relationship Management ,white paper. 3 Dezhen Feng, Zaimei Zhang, Fang Zhou, Jianheng Ji Application Study of Data Mining on Customer Relationship Management in ECommerce 97814244329120825.00 2008 IEEE. 4 Olaf Boon, Brian Corbitt, Craig ParkerConceptualizing the Requirements of CRM from an Organizational Prespective a Review of the LiteratureSchool of Information Systems Deakin University,2002. 5 E.W.T. Ngai , Li Xiu , D.C.K. Chau Application of data mining techniques in customer relationship management A literature review and classification Expert Systems with Applications journal, 2009 Elsevier.R. Nicole, The Last Word on Decision Theory, J. Computer Vision, submitted for publication. Pending publication 6 Research Team CRMnext Knowledge BaseCRM Challenges Building an Effective Strategy directory group                    7 Mirela Danubianu, Stanica Anca MariaStudy Of Improving The Customer Relationship Management By Data Mining Application a lecturer in Electrical Engineering and Computer Science, 2009.S.P. Bingulac, On the Compatibility of Adaptive Controllers, Proc. Fourth Ann. Allerton Conf. Circuits and Systems Theory, pp. 816, 1994. Conference proceedings. 8 A.S. Al Mudimigh, F. Saleem, Z. Ullah, F.N. AlAboud Implementation of Data Mining Engine On CRM Improve Customer Satisfaction King Saud University, Riyadh, Kingdom of  Saudi Arabia 97814244460940925.00 2009 IEEE. 9 R. Kosala,H. Blockeel Web Mining Research  A Survey, In SIGKDD Explorations, ACM SIGKDD,Volume2,Issue1, July 2000 10 Zdravko Markov, Daniel T. LAROSE Data Mining The Web Uncovering Patterns in Web Content, Structure, and   Usage, WILEYINTERSCIENCE, 2007. 11 Jaideep Srivastava, Robert Cooley, Mukund Deshpande, PangNing Tan Web usage mining discovery And applications of usage patterns from web data.SIGKDD Explorations, 121223, 2000.                                    Fig 3  Presents the Applicable Methods of Web Mining for CRM 1 Data Collecting Process  Data Source Database, Flat Files Operation CRM Preprocess Data  xtractionE ransformationT Load Data Warehouse Data Mining Process  Discovering Patterns Review Patterns Refine Patterns Knowledge  CRM Process   Marketing Sales Service 16International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org  Table 1.  Proposed Adaptive Classification Based On Web Data Mining Methods to Solve Challenges of CRM          Mining Types of Methods Main Idea Dimensions of CRM CRM Challenges             Data mining  Association Rules Statistical and former algorithms are used for diagnosing the behavior of an event or specific process that identifies the communication between elements in a specified record uses. Customer Development Integrated application tools Technology challenges Collaboration of various segments   Classification Classification is one of the most common learning models in data mining. It aims at building a model to predict future customer behaviors through classifying Database records into a number of predefined classes based on certain criteria. Common tools used for classification are neural networks, decision trees and if thenelse rules. Customer Identification Initial launching Costs Strategy challenges Customer Dimensions Challenges   Clustering Clustering is the task of segmenting a heterogeneous population into a number of more homogenous clusters. It is different to classification in that clusters are unknown at the time the algorithm starts. In other words, there are no predefined clusters. Common tools for clustering include neural networks and discrimination analysis Customer Identification Initial launching Costs Strategy challenges Customer Dimensions Challenges   Forecasting Forecasting estimates the future value based on a records patterns. It deals with continuously valued outcomes. It relates to modeling and the logical relationships of the model at some time in the future. Demand forecast is a typical example of a forecasting model. Common tools for forecasting include neural networks and survival analysis. Customer Attractions Integrated application tools Collaboration of various segments    Regression Regression is a kind of statistical estimation technique used to map each data object to a real value provide prediction value. Uses of regression include curve fitting, prediction including forecasting, modeling of causal relationships, and testing scientific hypotheses about relationships between variables. Common tools for regression include linear regression and logistic regression. Customer Retention Collaboration of various segments Strategy challenges Customer  Dimensions  challenges   Sequence Sequence discovery is the identification of associations or patterns over time. Its goal is to model the states of the process generating the sequence or to extract and report deviation and trends over time. Common tools for sequence discovery are statistics and set theory. Customer Retention Collaboration of various segments Strategy challenges Technology challenges    Visualization Visualization refers to the presentation of data so that users can view complex It is used in conjunction with other data mining models to provide a clearer understanding of the discovered patterns or relationships. Examples of visualization model are 3D graphs. Customer Development Integrated application tools Technology challenges Collaboration of various segments     Web Mining   Web Content Mining Active methods of Automatic discovery, retrieve, organization, and management of a huge volume of web information and resources. It helps in  improving  or filtering the search information that is usually based on the information derived from user profile or request so  provides more complex queries  from a simple keyword for more accuracy. Customer Identification Customers Primary Information Initial launching Costs Strategy challenges Customer Dimensions Challenges Web Structure Mining Web is a graph. Its nodes are web pages and hyperlinks create edges that provide the communication between the related pages. Customer Identification Customers Primary Information Initial launching Costs Strategy challenges Customer Dimensions Challenges   Web Usage Mining It focuses on techniques that can predict user behavior interactively while working with the web pages. Mostly, ecommerce is very important for web based companies and CRM can be an effective advantage of web usage mining. Customer  Attractions, Retention,  Development       Customers Secondary Information  Integrated application tools Technology challenges Collaboration of various segments Customer Dimensions Challenges 17International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518   IJSER  2011 httpwww.ijser.org  A Study of Carcinogenic Components Removal in Chemical Industry  R.Uma Mythili, K.Kanthavel, R.Krishna Raj AbstractThe chemical industries are involving with various surfaces coating process which develops pollutions like Carcinogenic components. In industry and environment inhalation of high doses have the potential to induce lung tumors in human and animals. The process of Carcinogenic components removal is a major real time problem in environmental toxicology in chemical processing industry. The standard analysis method determines and calculates the constituent of hexavalent chromium and trivalent chromium compounds in carcinogenic fumes. The research identifies that the presence of Carcinogenic components leads to the low generations of gastric juice, epithelial lining fluids to the human beings and animals. The segregation and analysis of Carcinogenic components was attracted several researcher for the past three decades. This paper discusses with Biological, Chemical and Mechanical analysis methods of identifying Carcinogenic components present in the effluent of the chemical industry in the form of sludge.  Index Terms Environmental Production Agency, Hexavalent chromium, Scrubber, Trivalent chromium        1 INTRODUCTION                                                                     N experiment was conducted by Abbasi and       Soni1984 thirty adult channel fish exposed to waterborne hexavalent chromium concentration of 50 to 100ppm and hardness of 3ppm resulted in alterations in swimming and balancing behaviors including loss of balance erratic and rapid twisting. Further research was proposed by James R Kastner et al 2002 for the determination of chromium compounds using Polarography.He embraced EPA method which is also named as ion chromatography to determine the dissolved hexavalent chromium CrO42 in ambient water .This method was developed by integrating analytical procedures.The promulgation of odour was controlled and determined using mass spectrometry and wet scrubber including the reducing agents 7.                 John C.Chang et al 2003 inquiers Ontario Hydro analysis technique coupled with FGD scrubber in laboratory scale inorder to remove the effulent by 70 using 10. More over Hyeon  Yeong Kim et al 2004 conducted experiment for Sprague Dawley rat, 90 day repeated dose inhalation toxicity study was carried out resulted in 0.55m decrease of activity,nasal hemorrhage.Later ChihCheng Wu et al2004 absorbed the effects of hexavalent chromium in municipal and hazardous waste and he adopted packed tower scrubber for the elimination3. Present investigation is aims at reduction in high percentage of trivalent chromium and hexavalent chromium accompanies with chemical using packed scrubber and spray technology at the exit. 2 THEORITICAL MODELS         Theoritical model was first adopted by S.Sakar et al 2007 for dust particle collection.In this model there are various paramaters which is used to calculate the percentage of the hexavalent chromium removal 15. It includes two major factors to be found, they are collection efficiency of the particle and the mass balance of the liquid drop.  . 2.1 Collection Efficiency    Dust collection efficiency is frequently expressed in terms of penetration which was adopted by Lim K.S et al 2006. Penetration is defined as the fraction of particles that passes through the scrubber uncollected.       Penetration is the opposite of the fraction of particles collected and is expressed as   Pt  1                                                                            1         Wet scrubbers usually have an efficiency curve that fits the relationship of    1ef system                                                               2                                        By substituting for efficiency, penetration can be expressed as   Pt       1                11ef system     Ptefsystem                                                                    3          The major equation to be included is SoudersBrown equation. They are as follows 2.2 The SoudersBrown equation           Trond Austrheim.et al., 2008 used the expression for sizing of gas scrubbers 16.This was the expression developed and given below as                              K  4gdd  3Cd                                                  4         Equation 4 involves an empirically quantified factor known as the SoudersBrown value, the Kvalue, or the Gas Load Factor. The basis of the SoudersBrown expression is a force balance resolved in the vertical direction on a spherical droplet in an upward flowing gas in a gravity field.             When the droplet is held stationary,  A18International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org   Fr0.5CdAdgU2g,setGd6dd2gig                         5             In practice Cd varies with the droplet Reynolds number,    RegUg,setdd                                                             6                                      Except for high values of Re, where Newtons law states that it is constant and about equal to 0.43.  At low Re, the wellknown relation of Stokes states that   Cd24Re                                                                   7                                                             Svrcek and Monnery said that decreasing the Kvalue with 25 for 85 bars pressure. They said while designing a column we have  to avoid the upwards velocity entrains droplets and the recommended Kvalue is K  0.1 ms for lowpressure applications often a safety margin of 50 is added for vessels without internals. Yarong Li,et al., 2002  said that for increasing pressures the critical Kvalue has been seen to decline. Increasing pressure in oilgas applications is often accompanied by a decrease in interfacial tension and thereby a decrease in the droplet sizes 16.  2.3 Mass balance of liquid drops         Sarkar.S.et al., 2007 considered the schematic of a horizontal cocurrent gasliquid scrubber Fig 2. In this scrubber air stream containing a trace amount of trivalent chromium. This trivalent chromium enters horizontally at the left face x  0 with a velocity Ugas along with a spray of water droplets whose number concentration at the inlet is Nin. Trivalent chromium and hexavalent chromium was removed from the gas stream by absorption of droplets as both travels through the chamber from left to right. They assumed that individual drop moves horizontally with a velocity equal to Ugas and falls vertically at its terminal settling velocity Uter.          They assumed that mixed flow in chamber has mixing in the vertical direction while no mixing in the horizontal direction. Consider the slice or control volume of thickness x. There will be a certain height h along the vertical direction in this slice below which all drops will reach the floor of the chamber all such drops will be collected 15.  t hUter  xUgas                                             8                                                                              dN   Uter       dx                                                 9                                                                           N        HUgas                     Integrating Eq. from the inlet to any location x gives    NNinexpUterx                                                    10                                                                     HUgas         Nin, Nx, and Nx x be the number concentration of water drops at the inlet x  0 and at locations x and  x x.     Nin Nout      1exp  Uter       x                       11                                                                 Nin                       HUgas  3 EXPERIMENTAL METHODS              Experimental methods have lots of categories. The major three main categories of experimental methods are Biological, Chemical and Mechanical methods. Each category has sub category based on the instruments and experiments. 3.1 Biological Technology             Graciela Rojas, Jorge Silva, Jaime A. et al., 2005 experimented using Flame atomic absorption spectrometry and colorimetric to determine Chromium in solution along with diphenylcarbazide. Crossinked chitosan is efficient in removing hexavalent chromium from aqueous solutions and at the same time less efficient in eliminating trivalent chromium owing to protonated active sites interacting mainly with metallic anions at acidic pH. He absorbed that the maximum adsorption capacity achieved in this experiment, as high as 215                      Fig. 1 Upward flowing spherical droplet                    Fig. 2 Horizontal cocurrent gasliquid scrubber  19International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org  mgg, is among the highest reported elsewhere 5.             Further Jinwook Chung,et al., 2010 adopted hydrogen based membrane biofilm reactor along with reducing bacteriafor reduction of hexavalent and trivalent chromium.Reduction occurred rapidly under normal membrane biofilm reactor denitrifying condition and Reduction is optimum near 7pH.  The results show that the hydrogen based membrane biofilm reactor is a treatment technology for treating hexavalent chromium   in wastewater.For effective hexavalent chromium removal, the critical operational parameters for maximizing reduction hexavalent chromium are hydrogen concentration, nitrate concentration and pH 9.Later Trond Austrheim.et al., 2008 Trametes Versicolor Polyporus fungi .The Absorption capacity of this trametes versicolor polyporus fungi was 125.0 mgg. The maximum uptake of hexavalent chromium ions was occurred at pH 4.By increasing the amount of the biosorbent which increase the percentage of metal ions removal 16.              Surfactant cetyltrimethylammonium bromide was used as an adsorbent by Sadaoui.Z.et al., 2009 to remove the hexavalent chromium from wastewaters.The adsorption performances of cetyltrimethylammonium bromide was studied.The adsorption capacity of hexavalent chromium was increased with initial metal concentration and in a lesser extent with solution pH. However, it decreases with initial cetyltrimethylammonium bromide mass and with concentration of other ions present in the solution.The maximum capacities of metal adsorption as calculated as 17.89 mgg1 and 13.85 mgg1 using Langmuir adsorption isotherm at 450C 13 .           Alok Prasad Das,Susmita Mishra, 2010 implemented Microbial remediation Peptone Yeast Exatract and Brevibacterium Casei for hexavalent chromium removal.These bacterium detoxify the chromate present in contaminated wastes of  industries and minings. This Brevibacterium Casei can effectively degrade hexavalent chromium upto 99 in 12hr at neutral pH and temperature of 30 1.           Futher Activated sludge with powdered Activated carbon technique was adopted by Ferro Orozco A.M. et al., 2010 4 .They observed that the hexavalent chromium removal in the combined activated sludge with powdered activated carbon system in comparison is high than individual Activated sludge treatments.Chromate removal was improved by increasing powdered activated carbon concentrations in both PAC and activated sludge with powdered activated carbon systems. 3.2 Chemical Technology             Ramajeevan Ganeshjeevan,et al., 2003 investicates hexavalent chromium detection in the presence of a high load of colourants using an online dialysis technique for ion chromatography.This method has been developed to remove watersoluble anionic dyes and particulate colourants and other substances to facilitate hexavalent chromium  quantification. They found that this method has a detection limit of 5 mg l, recovery rate of 100 and analysis time less than 20 min 11.            Later Venkata Subbaiah.M.et al., 2006 inquiries coupled plasma mass spectrometry along with dynamic reaction cell with ammonia for detection. This method is used for determining ultratrace levels of hexavalent chromium in ambient air. The method involves a 24h sampling of air into potassium hydroxide solution, followed by silica gel column separation of hexavalent chromium, then preconcentration by complexation and solvent extraction. The hexavalent chromium complex was dissolved in nitric acid. The resultant chromium ions were determined by inductively coupled plasma mass spectrometry using a dynamic reaction cell with ammonia 9.3 of trivalent chromium converted to hexavalent chromium within 6 hrs 17.     Complex matrices such as crude oil and sludge was adopted by Safavi.A,Maleki.N., 2006. They implemented sensitive method for determination of chromium ion VI in complex matrices such as crude oil and sludge was presented based on the decreasing effect of hexavalent chromium on cathodic adsorptive stripping peak height of Cuadenine complex.  Under the optimum experimental conditions, a linear decrease of the peak current of Cuadenine was observed, when the chromium VI concentration was increased from 5 gL1 to 120gL1. Trivalent chromium at concentrations10100fold excess of hexavalent chromium 14.  Turkish brown coals Ilgn, Beysehir, and Ermenek have been established for treating chromium by Gulsin Arslan, Erol Pehlivan, 2007. This method is simple, effective and economical means of wastewater treatment. The mechanism of hexavalent chromium ion binding to brown coals may include ion exchange, surface adsorption, chemisorption, complexation, and adsorptioncomplexation. Maximum adsorption capacity of 11.2 mM of hexavalent chromium g for Ilgn, 12.4 mM of Hexavalent chromium g for Beysehir, 7.4 mM of hexavalent chromium g for Ermenek  and 6.8 mM of Hexavalent chromium g for activated carbon was achieved at pH of 3.06.    Trichamber bath system, titanium plated ruthenium anode,nafion quaternary cation has been adopted by JIANG Xiaojun et al., 2009 . This technique has higher current efficiency 34.7 and no health damage to the operators 8.  3.3 Mechanical Technology  Mass spectrometry analysis and wet scrubber has been adopted for carcenogenic component removal by 20International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org  James R.Kastner and K.C.Das, 2002.Emission rate of Sulfur compounds is reduced nearly 90  7.              Wet Scrubber Simulator System and Ontario Hydro Analysis were used as technique for examination by John C.S Chang,S.Behrooz Ghorishi , 2003. The model can be used to predict Hg0 increase across wet FGD scrubbers when the Hg2 reduction and Hg0 emission in the scrubber is dominated by the mechanism. Pilot and fullscale tests are needed to validate the results from this current work which was conducted under simulated benchscale conditions.  This reduce S IV concentration enhances Hg0 and Hg2 emission 10.             Amornpon Chandsuphan et al., 2003 explorize oxidizing agent along with packed bed wet scrubberwas meant for treating the chemicals. 99.10 of carcenogenic components were removed. Surfactants with packed tower scrubber reduce C10E4  Naphthalene nearly 60. By using Pilot Scale Packed Bed scrubber along with oxidant 99.5H2S was removed 2.  Residence time distribution approach has been used by Sarkar.S.et al., 2007 for developing a theoretical model for predicting the SO2 removal efficiency in a horizontal cocurrent gasliquid scrubber by water spray. Experimental investigation shows that a very high percentage removal of SO2 can be achieved from airSO2 mixture without using any additives or pretreatment. Removal of sulfur compound efficiency lies in the range of 6097 15. 4 LIST OF SYMBOLS  TABLE1                    Symbol                                Quantity  Pt Penetration  Collection efficiency e Exponential function f system Function of the scrubbing system  variables  Cd Drag coefficient K Gas Load Factor g Gravity field dd Droplet diameter  Ad Projected area of the droplet, dd2 4  dd Droplet diameter i and g Densities of the gas and liquid U g,set Terminal velocity relative to the gas Fr Flow force G d Gravity force  Nout Number concentration of liquid drops at the outlet x  L of the scrubber   5 CONCLUSION              Various biological, chemical and mechanical method list different percentages of hexavalent chromium and trivalent chromium removal, among these Microbial remediation and Brevibacterium Casei removes hexavalent chromium  upto 99 in 12hr.This is the maximum percentages of chemical removal as examined among the biological techonolgy upto now,Maximum adsorption capacity of 11.2 mM of hexavalent chromium g for Ilgn , 12.4 mM of hexavalent chromium g for Beysehir , 7.4 mM of hexavalent chromium g for Ermenek and 6.8 mM of hexavalent chromium g  has been observed by Gulsin Arslan.Among the chemical technology this  Turkish brown coals produces maximum removal of chemicals. 99.10 of mercury is removed in the mechanical technoly using packed bed wet scrubber. The future work concentrates in the increase of trivalent and hexavalent chromium removal percentage. REFERENCES 1 Alok Prasad Das,Susmita Mishra, Biodegradation of the metallic carcinogen  hexavalent chromium Hexavalent chromium  by an indigenously isolated bacterial strain,Vol.9,Journal of carcinogenesis,2010. 2 Amornpon Chandsuphan,Somrat Kerdsuwan,Vladimir, Efficiency of Mercury Removal in Packed Bed Wet Scrubber from Infectious Waste Incinerator, Vol.13, No.4, Journal of KMITNB,2003. 3 ChihCheng Wu and Whel May Grace Lee, Control of Vaporous Naphthalene by Scrubbing with Surfactants, Vol.140, No.112,Journal of Environmental Engineeering,2004. 4  Ferro Orozco A.M,Contreras E.M , N.E. Zaritzky,  Effects of combining biological treatment and activated carbon on hexavalent chromium reduction, Vol.102,No.24952502,Bioresource Technology,2010. 5 Graciela Rojas, Jorge Silva, Jaime A. Flores, Angelica Rodriguez,Martha Ly, Holger Maldonado, Adsorption of chromium onto crosslinked chitosan, Vol.44 ,3136, Separation and Purification Technology,2005. 6 Gulsin Arslan , Erol Pehlivan ,Batch removal of chromiumVI from aqueous solution by Turkish brown coals, Vol.98 ,28362845, Bioresource Technology,2007. 7       James R.Kastner and K.C.Das, Wet Scrubber Analysis of Volatile Organic Compound Removal in the Rendering Industry, Vol.5, 459469,Journal of the air and waste management association,2002. 8  JIANG Xiaojun,CHEN Wenchao, XU Hongbo, Elimination of a pollution associated with chromic acid during the electro21International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org  deposition of Trivalent chromium using appropriate anodic and membrane materials in a double film bath,No.5255,Journal of Environmental Sciences Supplement,2009. 9 Jinwook Chung, Robert Nerenberg,Bruce.E.Rittmann, Bioreduction of soluble chromate using a hydrogenbased membrane biofilm reactor,Vol.41 Journal of water research,2010.      10        John C.S Chang,S.Behrooz Ghorishi ,Simulatiom and Evaluation of Elemetal Mercury Concentration Increase in Flue Gas Across e Wet Scrubber, Vol.37, 57635766, Environ. Sci. Technology,2003. 11        Ramajeevan Ganeshjeevan, Raghavan Chandrasekar, Subramanian Yuvaraj,Ganga Radhakrishnan ,Determination of hexavalent chromium by online dialysis ion chromatography in a matrix of strong colourants and trivalent chromium, Vol.988 151159,Journal of Chromatography ,2003. 12       Raj Mohan.B. Comprehensive analysis for prediction of dust removal efficiencyusing twinfluid atomization in a spray scrubber, Vol.63 269277, Separation and Purification Technology,2008. 13       Sadaoui.Z. , S. Hemidouche, O. Allalou , Removal of hexavalent chromium from aqueous solutions by micellar compounds, Vol.249 768773, Desalination,2009. 14       Safavi.A,Maleki.N. H.R. Shahbaazi, Indirect determination of hexavalent chromium ion in complex matrices by adsorptive stripping voltammetry at a mercury electrode, Vol.68 11131119,Talanta,2006. 15       Sarkar.S. B.C.Meikap,S.G .Chatterjee,Modeling of Removal of Sulfur dioxide from Flue Gases in a Horizontal Cocurrent Gas Liquid Scrubber, Vol.131 263271, Chemical Engineering Journal,2007. 16       Trond Austrheim,Lars H. Gjertsen,Alex C. Hoffmann,An experimental investigation of scrubber internals at conditions of low pressure, Vol.138, 95102, Chemical Engineering Journal,2008. 17       VenkataSubbaiah.MS.Kalyani,G.SankaraReddy,VeeraM.Boddu,A.Krishnaiah, Biosorption of Hexavalent chromium  from Aqueous Solution Using Trametes Versicolor Polyporus Fungi, Vol.5,No.3,499510,EJournal of chemistry,2006. 18       Yarong Li, Narayan K. Pradhan, Roy Foley, Gary K.C. Low, Selective determination of airborne hexavalent chromium using inductively coupled plasma mass spectrometry, Vol.57 11431153,Talanta,2002. 22International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518   IJSER  2011 httpwww.ijser.org  Effects of SocioDemographic Covariates on Blood Glucose Level of the Diabetic Patients  A Multiple Classification Analysis MCA Md. Behzad Noor, Dr. Mir Mohammad Azad, Dr. J.A.M. Shoquilur Rahman   Abstract This study has been undertaken to determine the sociodemographic related covariates affecting blood glucose level of the diabetic patients as well as to identify the extent of influences of the variables by the implied factors on blood glucose level. Data have been collected from two diabetic diagnostic centers in Rajshahi City. The result implies that insulin intake, blood pressure, tablet intake and mental suppression of the diabetic patients have the intense influence on blood glucose level. The proportion of variance explained adjusted by the respective variables are 2  0.422, 0.122 and 0.120 respectively. So, the diabetic patients are supposed to maintain their medication, control their blood pressure, become free from mental suppression, overall, giving up the sedentary life style. They should maintain a regulatory life to keep themselves away from having high blood glucose level.    Index Terms Glucose Level of Diabetic Patients.          1 INTRODUCTION                                                                     LUCOSE is a main source of energy for the cells that make up our muscles and other tissues. Blood glucose tests measure how well our body processes sugar glucose. A normal fasting blood glucose result is lower than 100 milligrams of glucose per deciliter of blood mgdl 1. Hyperglycemia, or high blood glucose levels, is the hallmark of diabetic and it is linked to the development of longterm diabetic complications. Healthy people without diabetic typically have blood glucose levels of 65110 mgdl and 120140 mgdl one to two hours after eating 2, 3.TypeI diabetes is an autoimmune disease. An autoimmune disease results when the bodys system for fighting infection the immune system turns against a part of the body. In diabetes, the immune system attacks and destroys the insulinproducing beta cells in the pancreas. The pancreas then produces little or no insulin. A person who has typeI diabetes must take insulin daily to live.   In typeII diabetic, the pancreas is usually producing enough insulin, but for unknown reasons the body cannot use the insulin effectively, a condition called insulin resistance. After several years, insulin production decreases. The result is the same as for typeI diabetes  glucose builds up in the blood and the body cannot make efficient use of its main source of fuel 4. Worldwide, typeII diabetes has soared to epidemic proportions and is at the forefront of current public and community health intervention in both Western and developing countries. Obesity, consanguinity, blood pressure, total cholesterol, HDLcholesterol and triglyceries are more prevalent in diabetic patients 5. It is estimated that 135 million people worldwide have diagnosed diabetes in 1995, and this number is expected to rise to at least 300 million by 2025 6. The prevalence data on diabetes were available from seven countries in the Region. The sources of information were surveys covering relatively small population groups. For the purpose of projection to the countries with no representative data, the data from the seven countries were used. it was estimated that the Region had about 40.9 million cases of diabetes 7. For this estimation, the weighted average prevalence of the countries with available data was used. Two other approaches were also used.       G Md. Behzad Noor1, Received M.Phil Fellow,M.Sc. Thesis First Class, First Department of Human Resource Development HRD, Major in HRM and B.Sc. Hons from University of Rajshahi, Working as Lecturer, Departmental of Business Administration, Shanto Mariam University of Creative Technology, , PH8801923051674, bnoorru08yahoo.com, Dhaka,   Bangladesh  Dr. Mir Mohammad Azad2, Received PhD in Computer Science from Golden State University. Received Masters of Computer Application MCA Degree from Bharath Institute of Higher Education and Research Deemed University Presently Bharath University and Bachelor of Computer Application BCA Degree from Bangalore University, India, Presently Working as an Assistant Professor and Departmental Head of Computer Science and Engineering, Shanto Mariam University of Creative Technology, PH8801191695195, azadrayhangmail.com, Dhaka, Bangladesh.  Dr. J.A.M. Shoquilur Rahman, Professor,Department of Pupolation Science and Human Resource Development,University of Rajshahi,Rajshahi,       Bangladesh  23International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org   Table 1  Estimated prevalence of diabetes mellitus in SEAR, 1998  Country Population 000 Estimated cases  of diabetes mellitus Bangladesh 124,000 2,700,000 DPR Korea 23,700 16,497 India 982,200 28,702,100 Indonesia 209,000 4,639,000 Maldives 282 7,000 Sri Lanka 18,700 1,500,000 Thailand 61,000 1,380,000 Total of above countries 1,418,882 38,944,597 Estimated cases from countries with no representative data  70,250 1,928,178 Estimated total cases in the Region 1,489,132 40,872,775 Source Based on prevalence estimate derived from the above seven countries.  First, as the data showed that two countries in the Region have either high Sri Lanka or very low DPR Korea prevalence, a revised estimate was made excluding these two countries, which came to 40.8 million. Secondly, the Indian prevalence estimate for the three countries from where data was not available, was used as it was felt that culturally and socio demographically, India was the closest to these countries. This gave an estimate of 41.0 million cases 7. In the Diabetes Atlas 2000, published by the International Diabetes Federation IDF, it was estimated that in 2000 report of IDF includes India, Bangladesh, Mauritius and Sri Lanka in the SEA Region of IDF an estimated 35 million cases of diabetes would be prevalent 7. The differences are in the year of estimate and the countries included. If we take only India, Bangladesh and Sri Lanka WHOSEAR countries included in the IDF report, then the comparison of the two estimates is shown in Table 2 below. Though the total estimate appears to be similar, the country specific estimates show a variation 7.  Table 2 Comparison of estimates of prevalence of diabetes mellitus of derived by IDF and the present report  Country IDF WHO SEARO Bangladesh 1,759,700 2,700,000 India 32,674,400 28,702,100 Sri Lanka 335,400 1,250,000 Source  Diabetes Atlas 2000, International Diabetes Federation   The ten Member Countries of WHOs SouthEast Asia Region Bangladesh, Bhutan, DPR Korea, India, Indonesia, Maldives, Myanmar, Nepal, Sri Lanka and Thailand are undergoing significant social, economic and demographic changes. Most of the noncommunicable diseases NCDs are a part of the degenerative diseases group and have higher prevalence in the older population. In addition, many of the lifestyle risk factors for NCDs like improper nutrition, sedentary life, alcohol and tobacco use are showing an upward trend in these countries. This has led to the emergence of NCDs as important causes of morbidity and mortality in the Region. In 1998 alone, NCDs are estimated to have contributed almost 60 of deaths in the world and accounted for 43 of the global burden of disease. Based on current trends, by 2020, these diseases are expected to account for 73 of deaths and 60 of the disease burden 7.             Fig. 1 Projected trends in the number of deaths by broad cause group in developing regions The development of typeII diabetes is the result of a complex interaction between genetic and environmental factors and has been researched extensively in the scientific and medical literature 8. The most common form of diabetic is typeII diabetic and this type of diabetic is most often associated with older age, obesity, physical inactivity, and ethnicities 9. Obesity is a wellknown risk factor for cardiovascular disease, including systemic hypertension, heart failure those in turn affect the blood glucose level. Obesity is a wellknown risk factor for cardiovascular disease, including systemic hypertension, heart failure those in turn affect the blood glucose level 10.  Investigating the prevalence of typeII Diabetic Mellitus DM, impaired glucose tolerance and the factors affecting blood glucoselevel, it has been found that the sex, hyperlipoidemia, hypertriglyceredemia and hypertension as independent factors for the abnormalities in glucose tolerance whereas, BMI and per day insulin and tablet intake affect blood glucoselevel for the both sexes and for both tablet or insulin users 11. It has also been revealed that aging, overweight and a sedentary lifestyle are the important determinants in the prevalence of the diabetic during this transition period in Vietnam 12. For noninsulindependent diabetic mellitus NIDDM, impaired glucose tolerance IGT and 0102030401985 1990 1995 2000 2005 2010 2015 2020 2025Year Communicable Diseases NonCommunicable DiseasesInjuries Number of Deaths 24International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org  hyper tension in rural community of Bangladesh, increased age has been considered as an important risk factor for all disorder related to diabetic whereas BMI associated risk have been significant with NIDDM and hypertension 13,14.  The main aim of this paper is to assess different covariates affecting blood glucose level as well as to intensify their effect on blood glucose level through other implied factors.   2 DATA SOURCE Out of 84 diagnostic and pathologies 15, there are only two well recognized diabetic diagnostic centers named Rajshahi Diabetic Association and Diagnostic Center at Laxmipur and Diabetic Welfare Center, Talaimary, Rajshahi in Rajshahi city. Theses two diabetic centers have been considered as our study areas. There are 5345 registered diabetic patients at both the diabetic diagnostic centers out of them 3772 registered patients are in Rajshahi Diabetic Association and Diagnostic Center and 1573 in Diabetic Welfare Center. We serially numbered all those registered diabetic patients from the two centers respective registered book and thereafter we selected 1069 diabetic patients through the process of linear systematic sampling procedure using the formula Nnk, where N5348, n1069 and k5 16. Among 1069 patients there are 504 males and 565 females of whom 454 are tablet users and 615 insulin users. 3 ANALYTICAL METHODOLOGY Multiple Classification Analysis MCA requires one dependent variable and two or more independent variables. The dependent variable can be either a continuous or a categorical variable, but all the independent variables must be categorical. MCA can equally handle the nominal and ordinal variables and can also deal with linear and nonlinear relationships of predictor variables with dependent variables. Mathematically, the model can be expressed by the following equation Yijk  y   ai  bj  ck          eijk . Where, Yijk is the value or score of an individual who falls in the i th category of the factor A, j th category of the factor B and k th category of the factor C. y  is the grand mean of  Y. ai is the effect due to the i th category of the factor A, which is equal to the difference between y  and the mean of its category of factor A. bj is the effect due to j th category of the factor B, which is equal to the difference between y  and the mean of its category of factor B. ck is the effect due to the k th category of the factor C, which is equal to the difference between y  and the mean of its category of factor C. eijk is the error term related with Yijk score of the    individals. The coefficients, which are estimated by solving the normal equation systems, are called the adjusted or net effect of the predictors. These effects measure those of the predictor alone after taking into account the effects of all other predictors. The unadjusted, etasquare 2 coefficient is a correlation ratio, which explains how well the predictor variable explains the variation in the dependent variables. Similarly, the betasquare 2 coefficient indicates the proportion of variation explained by the other predictor variables. 3.1 Variables Considered for the Analysis The multiple classification analysis is undertaken first to evaluate the contribution of socio demographic and the health related characteristics such as age, respondents education, occupation, Body Mass Index BMI, calorie intake, injection intake, obesity, patients mental suppression, smoking habit and walking habit on the blood glucose level. In this case blood glucose level is taken to be the dependent variable and socio demographic and health related variables as explanatory variables. All the socio demographic variables are the categorical variables. The categories of the selected variables have been disclosed in the following table              25International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org    Table 3 Variables and their Covariates used in the    Multiple Classification Analysis Variable Covariates Dependent Independent Y  Blood Glucose Level X1  Age of the Respondents     X2   Body Mass Index     X3   Calorie Intake    X4    Educational Qualification     X5  Occupation    X6   Sugar level increased in relation to suppression   X7  Injection Intake   X8  Blood Pressure    X9  Walking Distance Covered  X10   Tablet Intake 0  Early Age 1  Middle Age 2  Older Age  1  Under Weight 2  Over Weight 3 Obesity  0  Dont Take 1  Medium Intake 2 High Intake  1  Illiterate 2  Primary Level of Education 3Secondary Level of Education 4 H.S.C Level of Education 5Graduation Level of Education 6 Post Graduation Level of Education  0  Non Professional 2  Professional   0  N0 1  Yes   0   Dont take 1  Take  0  Normal 1   High 2  Low 0 Dont Walk 2  Walk  0  Dont Take 1 Take    4 INFLUENTIAL DETERMINANTS OF THE BLOOD GLUCOSE LEVEL There are a variety of sociodemographic and health related factors that influence blood glucose level. To examine the differential patterns of mean level of the blood glucose among the patients, the well known Multiple Classification Analysis MCA has been employed. Table 2 shows the mean level of blood glucose of both unadjusted and adjusted by different socio demographic and diabetes related characteristics with the values of 2 and 2 produced from multiple classification analysis.  As we can found from Table 2 that the mean level of blood glucose Adjusted for the early, middle and the older aged persons are 13.16, 12.39 and 12.83 respectively Table 2. At the early age due to have a sedentary life style blood glucose level remains uncontrolled, which might lead some sort of vulnerability to the young diabetic patients. Again, the mean level of blood glucose has been found to be a bit higher for the older aged people than that of the middle aged people. This refers that with the increase of the age patients blood glucose is increased. This finding is supported by other studies, where the prevalence of the typeII diabetes in urban slum of Dhaka, Bangladesh have been determined and it has been found that increasing age, sex, literacy and waist to hip ratio for men as significant risk factors for increasing the blood glucose level 17.                       Many researchers have found BMI as one of the most influential factors affecting blood glucose level. Some other studies shows that the risk factors for increasing blood glucose level are age, BMI and parity which are all positively associated with the prevalence of the typeII diabetes 18. From our study we have noticed that over weighted and obese peoples mean blood glucose level Adjusted are much higher 14.55 and 14.99 respectively Table 2 than that of the respondents with the normal weight 11.48 Table 2. Calorie intake per day is another important factor that often termed as the decider of the blood glucose level for the diabetes patients. Here, we could notice that the patients who used to follow the chart of medium and the high calorie have much lower mean level of blood glucose 12.52 and 13.32 respectively Table 2 than of those who dont follow any calorie chart referred by the physician from the diabetes 26International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org  book 15.54 Table 2.  The respondents with the post graduation level of education are found with a lowest mean glucose level 11.15 Table 2 than of the other educational category. This result focuses that with the advanced educational background a people comes to know with various knowledge regarding diabetes through various accessories through accessing with the latest news and views about the various invariable knowledge of the diabetes. So, education has its own part to play in developing the knowledge and awareness about diabetes.  The mean glucose level Adjusted for the respondents with the professional background is 12.59 Table 2 which is a bit higher than those of the respondents who are nonprofessional 12.56 Table 2. This might happen due to the matter of less physical work and more mental suppression on the professional respondents who are more used to be involved with various types of official jobs where the maximum part of the day have to be spent without doing any physical work as a result the weight gaining was ongoing that increase the blood glucose level. TypeII diabetes and the impaired fasting  glucose IFG in the rural area of the Bangladesh had been investigated by the Sayeed and group and found that the prevalence of the diabetes and IFG in the rural population was increased, where, older ages, highest obesity and reduced physical activities proved significant risk factors for the diabetes and IFG 19. Tension and the mental suppression play the pivotal role in increasing blood glucose level. From Table 2 we notice that the respondents whose blood glucose level increase at the time of being tensed are found to have the maximum mean blood glucose level Adjusted 13.0 Table 2 comparing to those whose blood glucose level is not influenced by the mental suppression 12.04 Table 2. Scientifically as we know that excessive tension or mental agony cause the pancreas to exert more insulin through urine that in turn cause the deficit of insulin which influence blood glucose level 19.  The respondents who are used to take tablet and insulin to control their blood glucose level are noticed to have more adjusted mean glucose level 14.82 for the insulin intake and 14.30 for the tablet intake Table 2 than of those who dont control their blood glucose level through either of these process 10.65 and 10.63 for the tablet and insulin users respectively Table 2. Here, more mean glucose level has been noticed for those who used to have the insulin as their medication of controlling diabetes.  Among 1,000 respondents the mean adjusted blood glucose level is high for the patients who have the high blood pressure 15.07 Table 2 comparing to the mean adjusted blood glucose level to those of the respondents with normal blood pressure 10.55 Table 2.  Here, from our study it has been found that the mean adjusted blood glucose level of the respondents who have regular walking habit is depicted as much lower 11.67 Table 2 than of those who dont have any habit to walk at a regular basis 13.08 Table 2.  4.1 Intensity of effects of the various implied factors on blood glucose level In this section an attempt has been made to observe the extent of influences of the variables on blood glucose level. Table 3 produces the results of zero order correlation coefficients of blood glucose level with various socio demographic and health related characteristics. These variables have to affect blood glucose level through one or more proximate determinants.  Table 2 shows that respondents habit of taking insulin has a significant contribution on blood glucose level which has been noticed to have a positive association with blood glucose level. The correlation coefficient is found to be r  0.343 Table 3. Among the included variables respondents insulin intake has the strongest influence on blood glucose level. The proportion of variance explained unadjusted by insulin intake is 2  0.335 Table 2 and the proportion of variance explained adjusted by this variable is 2 0.422 Table 2. The indirect effect of insulin intake through calorie intake, BMI, and walking distance on the blood glucose level are 0.010, 0.032 and 0.005 respectively.  Blood pressure of the respondents has a significant contribution on blood glucose level. Blood pressure has a positive significant association r 0.046 with blood glucose level Table 3. Among the included variables, blood pressure has the second strongest influence on blood glucose level. The proportion of variance explained unadjusted by blood pressure was 2 0.117 Table 2 and the proportion of variance explained adjusted by this variable is 2  0.122 Table 2. The effect of blood pressure on blood glucose level through mental suppression, calories intake and the walking habit of the respondents is 0.003, 0.007, 0.00012 and 0.0046 respectively.   Mental tension has a positive significant association r  0.083 with blood glucose level Table 3. The strength of explaining variability unadjusted is 2 0.083 Table 2 and the strength of explaining variability adjusted is 2  0.119 Table 2 which is the third most influential factors affected 27International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org  the blood glucose level. The indirect effect of mental suppression of the respondents suffering from the diabetes on blood glucose level through body mass index is 0.013 Table 2 and through education and age are 0.013 and 0.0009 respectively on the blood glucose level.  Daily tablet intake of the patients also has a significant contribution on blood glucose level. The tablet intake of the respondents has the negative significant association r 0.332 Table 3 with blood glucose level. The proportion of variance explained unadjusted by tablet intake of the respondents is 2 0.124 and the proportion of variance explained adjusted by this variable is 2  0.324 Table 2. Tablet intake by the diabetes patients affects blood glucose level through body mass index, blood pressure, calories intake and walking habit of the respondents with the value of  0.0096, 0.0043, 0.0026 and .0026 respectively. Calorie intake of the respondents has a vital role to control the blood glucose level and has shown a considerable positive association with blood glucose level.  The proportion of variance explained by the calorie intake is unadjusted 2 0.054 and the proportion of variance explained adjusted by this variable is 2  0.048 Table 2.                Fig.1 Decomposing and synthesizing the relative influences of different variables on blood glucose level of the diabetic patients. As there is no mechanical method for building a causal model Srinivsan, 1981, it primarily depends on the substantive reasoning of the cause effect relationship. With these statistics, this part attempts to decompose and synthesize relative influence of the variables on blood glucose level. The model shows that there is a positive significant correlation between education and occupation, r  0.364 Table 3. In addition, it can be mentioned that the educational qualification has the positive and significant association with the mental suppression, as we found from other study that the educated people are more likely to have high level of mental suppression 19. Again, from the Table 3 we observe that education has the positive association r0.048 Table 3 with the calorie intake and through the calorie intake education affect the blood glucose level that is the indirect effect of the education through calorie intake was 0.0019.       Walking habit of the respondents had four posterior factors age, calorie intake, occupation and BMI. The extent of correlation between walking habit and age is 0.084 Table 3. The result implies that as age increases the walking habit is reduced. For instance, in spite of having the desire, due to the physical inability an aged population couldnt be able to walk at a regular basis for a long distance. So walking habit also affects blood glucose level through age with the indirect effect of 0.0015.                  Table 4 Results of Multiple Classification Analysis MCA Affecting SocioDemographic Covariates on Blood Glucose level of Diabetic Patients in Rajshai City, Bangladesh.   Characteristics Unadjusted Adjusted 2 2 Age Early Age Middle Age Older Age  13.32 12.36 12.92  13.16 12.39 12.83 0.055 0.043  X1 1 X2 2 X3 3 X4 4 X5 5 X6 6 X77 X8 8 X9 9 X10 10      Y  28International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org  Body Mass Index BMI Under Weight Normal Weight Over Weight Obesity  13.08 11.08 14.44 14.71  13.17 11.48 14.55 14.98 0.039 0.016 Calorie Intake 1. Dont follow the calorie  Chart 2. Follow the Medium Calorie Chart 3. Follow the High Calorie Chart  15.14 12.49 13.80  15.54 12.52 13.32 0.054 0.048 Educational Qualification 1. Illiterate 2. Primary Level of Education 3. Secondary Level of Education Higher.             4. Secondary Level of Education 5. Graduate Level of Education 6. Post Graduate Level of Education  12.44 12.48 12.73 12.21 13.21 11.01  12.49 12.47 12.65 12.77 12.99 11.15 0.061 0.041 Occupation Professional NonProfessional  12.56 12.48  12.59 12.56 0.007 0.004 Sugar Level increased with Mental Suppression No Yes  12.04 12.96   11.99 13.00  0.083 0.119 Insulin Intake Dont take Take  10.63 14.35  10.13 14.82 0.335 0.422 Blood Pressure Normal High Low  10.03 14.88 11.88  10.55 15.07 12.04 0.117 0.122 Walking Habit Dont have regular Walking habit Having regular Walking habit   12.89 11.43  13.08 11.67 0.022 0.019 Tablet Intake Dont take Take  10.65 14.14  10.89 14.30 0.324 0.120 Grand Mean Proportion of Variance Explained 12.55 0.367 The correlation between walking habit and calorie intake shows an insignificant association r 0.091 Table 3. This implies that the respondents who are used to take high calorie are not used to walk at a regular basis that leads them to the obesity which in turn affects the blood glucose level 20. So it is clear that walking habit also affects blood glucose level through calorie intake and this time the indirect effect through calorie intake is 0.0017. The more professional personnel have to occupy their most of the time doing many official works that dont allow them to walk at a sufficient rate of time and distance. So walking habit also affects blood glucose level through occupation as well. The association between the walking habits and occupation is 0.143 Table 3 and has the significant association. The indirect effect of walking habit through occupation is 0.0027. The correlation between the walking habit and the BMI is0.016 Table 3 that reveals that the respondents who have the less regular habit of walking are more likely to have more BMI that produce the risk of gripping by the diabetes 21. Here, in this case the negative effect of the walking habit through BMI is 0.0003. According to some other study, diabetes is associated with the age, BMI, blood pressure, HDL, cholesterol, education levels, and the family history 21. Table 5. Zero order correlation coefficient of SocioDemographic variables on the blood glucose level.  Y X1 X2 X3 X4 X5 X6 X7 X8 X9 X10 Y 1 0.024 0.014 0.056 0.013 0.066 0.083 0.343 0.046 0.023 0.332 X1  1 0.077 0.095 0.046 0.011 0.008 0.013 0.010 0.084 0.008 X2   1 0.044 0.046 0.024 0.114 0.078 0.062 0.016 0.080 X3    1 0.048 0.048 0.027 0.024 0.001 0.091 0.022 X4     1 0.364 0.110 0.030 0.037 0.026 0.022 X5      1 0.21 0.011 0.037 0.143 0.036 X6       1 0.075 0.025 0.037 0.002 X7        1 0.033 0.012 0.974 X8         1 0.038 0.036 X9          1 0.022 X10           1 In our study the correlation between the age and blood glucose level is 0.004 Table 3 that implies that as age increase the blood glucose level is more likely to increase. Again age of the respondents has three posterior factors BMI, calorie intake and the walking distance. The correlation between age and BMI of the patients is observed to be significant r  0.077 Table 3 which implies that with the 29International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org  increase of age the BMI decrease. So age affects blood glucose level through BMI. The indirect effect of age through BMI was 0.035. Age also affects blood glucose level through calorie intake and walking distance. We have found that there is a significant and positive association between the age and calorie intake r 0.095 Table 3, that refers that with the increase of age patients of our study take more calorie and in previous we have detected that with the increase of age the patients are less used to walk at a regular basis, so, more calorie consumption and less walking habit always create a huge risk to the prevalence of obesity that is the most influencing and effective factor in the prevalence of diabetes 22. The effect of the age through calorie intake was 0.0036.  Again, the correlation between BMI and mental pressure and BMI and blood pressure are 0.114 and 0.062 respectively Table 3, where, the association between the BMI and mental suppression has the significant association. This result implies that with the increase of BMI both the mental suppression and blood pressure both are increased. Here, the negative effect of the BMI and the blood pressure on the blood glucose level are 0.0018 and 0.0009 respectively.  Education and mental suppression have a significant positive association r0.110 Table 3. This refers that with the advanced educational background people become more likely to grippe by the tension which is one of the most effective factor in the prevalence of the diabetes. Other study has reported as BMI associated risk of the diabetes was significant with NIDDM and hypertension 22. The education has the negative effect on the blood glucose level through mental tension and BMI of 0.00451 and 0.0018 respectively. CONCLUSION Insulin and Tablet intake and Blood pressure of the patients are most important influential factors that affect blood glucose level. As we can observe that mental tension has a positive and significant association with blood glucose level. we also have find that the early aged patients mean blood glucose Adjusted is much higher than that of the older aged people and the patients who dont follow the calorie chart at all have the highest mean blood glucose level than of those who follow the medium and high calorie chart from their diabetes book. Multiple Classification Analysis MCA results deserve considerations from the viewpoint of policy implication.  So, we could say that the early and middle aged population should be more and more conscious about controlling their weight and the older aged patients should maintain their blood glucose through dieting and doing physical work as well and also they should take care of their calorie intake to keep themselves in safer zone. REFERENCES 1 C. Snehalatha, A. Kapur, and V. Vijay, J. Assoc. Physicians India 51, 766 2001. 2 H. G. Allen, J. C. Allen, L. C. Boyd, and B. P. AlstonMills Can, Nutrition 19, 584 2003. 3 N. D. Kim, K. T. Mary, L. Karen, C. Gary, M. Neil, and Gsc. D. Francine, Obstet  Gynecol. 194, 339 2005. 4 Diabetes  A brief historical aspect A Handbook of Diabetes Mellitus, New Mediwave Publication LUPN 45 2001. 5 A. Bener, M. Zirie, and A. AlRikabi, Genetics, obesity, and environmental risk factors associated with typeII diabetic, Croat Med. J., 46, 302307 2005. 6 World Health Organization, World Diabetic A News Letter, 3 2002 7 World Health Organization, Regional Office for South East Asia, New Delhi, 2002. 8 J. Lopatynski, G. Mardarowicz, and G. Szczesniak, A comparative evaluation of waist circumference, waisttohip ratio, waisttoheight ratio and body mass index as indicator of impaired glucose tolerance and as risk factors for typeII. Ann. Univ. Mariae curie Skildowska, 58, 413419 2003. 9 A. M. Habori, A. M. Mamari and A. A. Meeri, Diabetic Res. Clin. Pract. 65, 275 2004. 10 N. D. Kim, K. T. Karen, C. Gary, M. Neil and Gs. D. Francine, Am. J. Obset. Gynecol. 194, 339 2006.   11 L. N. Duc Son, T. T. Hanh, K. Kusama, D. Kunni, T. Sakai, N. T. Hung, and S. Yamamoto, J. Am. Coll Nutr. 24, 229 2008. 12 C. G. Sonomon, F. B. Hu, A. Dunaif, J. RichEdwards, W. C. Willett, D. J. Hunter, G. A. Colditz, F. E. Speizer, and J. E. Manson, JAMA 286, 2421 2004. 13 M. A. Sayeed, A. Banu, A. R. Khan, and M. Z. Hussain, Diabetic Care 18, 555 1995. 14 H. King, R. E. Aubert, and W. H. Herman, Diabetic Care 16, 157 1998. 15 Register book of Rajshahi Civil Surgeon Office, June 2008 16 Daroga Singh, and F. S. Chaudhary, The Theory and Analysis of Sample Survey Designs, 3rd Ediation New Age Internatioal P Limited, New Delhi, 1997. 17 M. A. Rahim, S. Vaaler, S. M. Keramat Ali, A. K. Khan, A. Hussain, and Q. Nahar, Prevalence of typeII diabetic in urban slums of Dhaka, Bangladesh, Bangladesh Med. Res. Counc. Bull., 20, 6070 2004. 18  C. Snehalatha, A. Ramachandran, A. Kapur, and V. Vijay, Agespecific prevalence and risk associations for impaired glucose tolerance in urban southern Indian population, J. Assoc, Physicians India, 51, 766769 2003. 19 M. A. Sayeed, M. Hajera, A. K. Parvin, A. L. Zafar, S. M. A. Keramat, B.  Akhtar, B. Ahren, and A. K. Azad, Diabetic and impaired fasting glycemia in rural population of Bangladesh, Diabetic Care, 26, 10341039 2003. 20 T. Saito, Y. Shimazaki, Y. Kiyohara, I. Kato, M. Kubo, M. Iida and Y. Yamasita, J. Periodontal Res. 40, 346 2005. 30International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org  21 C. G. Sonomon, A. Dunaif, J. RichEdward, W. C. Willett, D. J. Hunter, G. A. Colditz, F. E. Speizer and J. E. Manson, J. A. M. A. 286, 2421 2001. 22 H. G. Allen, J. C. Allen, L. C. Boyd and B. P. AlstonMills, Nutrition 19, 584 2003.  31International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                               ISSN 22295518  IJSER  2011 httpwww.ijser.org  Optimization of Power Consumption in Wireless Sensor Networks                                                                     Surendra bilouhan,  Prof.Roopam Gupta  AbstractIn this paper, we consider the problem of discovery of information in a densely deployed Wireless Sensor Network WSN, where the initiator of search is unaware of the location of target information. We propose a protocol Increasing Ray Search IRS, an energy efficient and scalable search protocol. The priority of IRS is energy efficiency and sacrifices latency. The basic principle of this protocol is to route the search packet along a set of trajectories called rays that maximizes the likelihood of discovering the target information by consuming least amount of energy. The rays are organized such that if the search packet travels along all these rays, then the entire terrain area will be covered by its transmissions while minimizing the overlap of these transmissions. In this way, only a subset of total sensor nodes transmits the search packet to cover the entire terrain area while others listen. We believe that query resolution based on the principles of area coverage provides a new dimension for conquering the scale of WSN. We compare IRS with existing query resolution techniques for unknown target location such as Round Robin Search. We show by simulation that, performance improvement in total number of transmitted bytes, energy consumption, and latency with terrain size  Index TermsWireless sensor networks, energy efficiency, scalability, CSMA, Sensor Sim , SIR, Lowpower optimization , transmission strategy.         1. INTRODUCTION   sensor network is a group of specialized transducers with a communications infrastructure intended to monitor and record conditions at diverse locations. Commonly monitored parameters are temperature, humidity, pressure, wind direction and speed, illumination intensity, vibration intensity, sound intensity, powerline voltage, chemical concentrations, pollutant levels and vital body functions.  A sensor network consists of multiple detection stations called sensor nodes, each of which is small, lightweight and portable. Every sensor node is equipped with a transducer, microcomputer, transceiver and power source. The transducer generates electrical signals based on sensed physical effects and phenomena. The microcomputer processes and stores the sensor output. The transceiver, which can be hardwired or wireless, receives commands from a central computer and transmits data to that computer. The power for each sensor node is derived from the electric utility or from a battery. This paper provides an analytical model for the study of energy consumption in multihop wireless embedded and sensor networks where nodes are extremely power constrained. Low   AuthorSurendra bilouhan working on sensor technology for ph.d in computer science, working as a Asst.prof.in Sagar institute of Research and technology,emailedbsurendra2005gmail.com, phone 9893864675 Co.AuthorDr.Roopam gupta is a Head of Deptt.of I.T in UIT,RGPV,email idroopamguptargtu.net    power optimization techniques developed for conventional ad hoc networks are not sufficient as they do not properly address particular features of embedded and sensor networks. It is not enough to reduce overall energy consumption, it is also important to maximize the lifetime of the entire network, that is, maintain full network connectivity for as long as possible. This paper considers different multihop scenarios to compute the energy per bit, efficiency and energy consumed by individual nodes and the network as a whole. The analysis uses a  detailed model for the energy consumed by the radio at each node. Multihop topologies with equidistant and optimal node spacing are studied. Numerical computations illustrate the effects of packet routing, and explore the effects of coding and medium access control. These results show that always using a simple multihop message relay strategy is not always the best procedure. Fig1.Life cycle of sensor network  A32International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                               ISSN 22295518  IJSER  2011 httpwww.ijser.org     2.BACKGROUND Recent advances in sensor technology in terms of size, power consumption, wireless communication and manufacturing costs have enabled the prospect of deploying large quantities of sensor nodes to form Wireless Sensor Networks WSN. These networks are created by distributing large quantities of usually small, inexpensive sensor nodes over a geographical region of interest with a view to collect data relating to one or more variables. These nodes are primarily equipped with the means to sense, process and communicate data to other nodes and ultimately to a remote users. WSN nodes can also have mobility capabilities which enable them to move around and roam the region of interest to harvest information. Figure 1 shows a typical sensor node hardware architecture. Sensor nodes may cooperate with their neighbors within communication range to form an adhoc Network. WSN topologies are generally dynamic and decentralized. Figure 2 gives a general overview of a WSN.Fig2.Execution phase of sensor signal  Fig3.wireless area consist of various sensor nodes. BACKGROUND AND RELATED WORK Understanding the performance and behavior of sensor networks requires simulation tools that can scale to very large numbers of nodes. Traditional network simulation environments, such as ns2 6, are effective for understanding the behavior of network protocols, but generally do not capture the operation of endpoint nodes in detail. Also, while ns2 provides implementations of the 802.11 MACPHY layers, many sensor networks employ nonstandard     wireless protocols that are not implemented in ns2. A number of instructionlevel simulators for sensor  network nodes have been developed, including At emu 11 and Simulavr 20. These systems provide a  very detailed trace of node execution, although only At emu provides a simulation of multiple nodes in a networked environment. The overhead required to simulate sensor nodes at the instruction level considerably limits scalability. Other sensor network simulationenvironments include PROWLER 21, TOSSF 19 based on SWAN 14, Sensor Sim 17, and SENS 23. Each of these systems provides differing levels of scalability and realism, depending on the goals for the simulation environment. In some cases, the goal is to work at a very abstract level, while in others, the goal is to accurately simulate the behavior of sensor nodes. Few of these simulation environments have considered power consumption. Sensor Sim 17 and SENS 23 incorporate simple power usage and battery models, but they do not appear to have been validated against actual hardware and real applications 18. Also, SensorSim does not appear to be publically available. Several power simulation tools have also been developed for energy profiling in the embedded systems community. Performance Metrics Used Number of transmitted bytes Average number of bytes transmitted by all the nodes in the network for finding the target information. As the message formats are not uniform across protocols, we measured the number of bytes transmitted instead of the number of messages transmitted. Number of transmitted and received bytesAverage number of transmitted and received bytes by all the nodes in the network for finding the target information. Energy consumed The total energy consumed by all the nodes in the network for finding the target information. Latency Time taken to find the target information, i.e., the time difference between, the time at which the search is initiated by the sink node by transmitting the search packet, and the time at which the search packet is received by the target node. Probability of finding the target informationProbability of finding the target information is a measure of the success probability of 33International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                               ISSN 22295518  IJSER  2011 httpwww.ijser.org  the search protocols. It is also a measure of non determinism of the search protocols. 4.Method of Reducing  Power ConsumptionWSN A method for reducing power consumption in a wireless sensor network is provided. An optimized path destined for a sink node is set using a common channel in which first and second nodes use a CSMA scheme. A first channel is set and transmissionreception slots for packet transmissionreception arc allocated in the first channel. A packet is transmitted to the second node through a first transmission slot using a TDMA scheme. When a packet is not received from the second node through a first reception slot within a first set amount of time, the first reception slot is allowed to transition to an inactive state. The first node is one of the sink node, at least one parent node, and at least one child node of the parent node, and the second node is one of child nodes of the first node. 3.Sampling energy consumption in wireless sensor networks  Knowing available energy in each part of a wireless sensor networks WSN is undoubtedly essential information. A simple approach to achieve this would be for sensor nodes to periodically report to the sink node on their available energy. This approach is expensive however in terms of energy consumption. In this work we apply several sampling techniques for obtaining such information in WSNs. The results show our samples are representative and the energy map may be satisfactorily built using information from a single subgroup of nodes hence leading to important energy savings Software components embedded in wireless sensors are generally elaborated taking into account the limited available resources. Operating systems dedicated to the Wireless Sensor Networks WSN were thus developed. However, this design method can induce major problems if resource management policies associated to each software component are very different or opposite. The aim is to develop a global method in order to manage the energy consumption of the WSN. The first goal of this postdoc is to develop an energetic model of wireless sensor in order to virtually test various policies of management. If some generic models exist at the level of the energy consumption of the communication medium, models dedicated to the energy consumption of the memory and the processor are not or poorly addressed in the literature. Some complete models already exist but are only dedicated to a specific hardware architecture of wireless sensor. It highlights the need of a model that can be applied to various wireless sensors. This model will be used in a WSN platform jointly developed by the Cemagref and the LIMOS laboratory of ClermontFerrand. The second objective is to establish optimal action policies for a given sensor. Thus, a wireless sensor can be seen as a system that we would try to optimize the use of its resources such as energy, memory and computing power. The application of the viability theory to the energy management problem will be also studied during this year. The viability theory consists in maintaining a system in a space of constraints for example, to maintain a frequency of sending between a minimal value and a maximum value. To do that, a control is defined on which one can act and, which allows us to maintain this system  into a space of constraints. Thus the management of a wireless sensor can be done thanks to the use of control techniques such as viability. Some recommendations or policies applicable to a wireless sensor would be defined in order to achieve a lifetime long enough while performing assigned tasks. Some simulations will be done before an implementation into a real system. Finally, this work will be extended to the case of the whole WSN at the end of the postdoc.Salary nearly 2000 euros per month. An EnergyEfficient Routing Method of Wireless Sensor Networks  In wireless sensor networksWSNs, as sensor nodes are characterized by having specific requirements such as limited energy availability, low memory and reduced processing power, energy efficiency is a key issue in designing the network routing. In the existing clusteringbased routing protocols for the WSNs, the clusterheads are usually selected randomly and do a lot of work, which may cause unbalanced energy consumption and thus short network lifetime. Based on this approach, the paper proposes a method that a clusterhead distributes energy load evenly among its members based on their energy usage by chain. With the chain, in a cluster, different cycle. The new method balances the nodes power depletion. Simulation results show that it has better power management performance and can prolong the lifetime of the network. 4.OPTIMIZATION OF ENERGY CONSUMPTION IN WIRELESS SENSOR NETWORKS Multiple antenna systems are capable of providing high data rate transmissions in a fading environment without the need of increasing the signal bandwidth. This suggests the question whether they can be used to reduce the energy consumption inwireless networks. Before addressing this problem, we must first understand the problem of minimizing transmit powerssubject to some SIR requirements. In fact, note that for a fixed transmit time TA, a fixed number 34International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                               ISSN 22295518  IJSER  2011 httpwww.ijser.org  of antennas N and fixed beam formers Uk,Vk at each transmitterreceiver pair, the problem of energy minimization subject to the SIR requirements is equivalent to minimizing transmit powers subject to the same SIR requirements.   5.CONCLUSIONS  In wireless networks, a widely studied approach is to minimize transmit powers subject to some QoS constraints. However, minimizing the transmit powers is not equivalent to minimizing the energy consumption. In this paper we addressed the problem of minimizing the overall energy consumption in wireless networks including the energy consumption for hardware. Therefore, we first pointed out some basic properties of the optimal power allocation. In order to give some insights, we then discussed the general energy minimization problem that depends on system parameters as the number of antennas N, on the transmission strategy represented by beam formers and number of parallel data streams and on transmit powers. Due to the fact that the general problem is quite complex we focused on a restricted problem. Considering the relation between transmission time and transmit power, we optimized both jointly to find an energyoptimal powertime tradeoff. More precisely, we proposed an algorithm that determines the energyoptimal number of data streams per link for a certain SIR requirement. To gain further insights into the energy minimization problem, it has to be considered for assumptions that may be more general or give another perspective on the problem. Further, note that the notion of energy minimization is not restricted to sensor networks. Thus, in future work the optimization problemmay also include other aims and constraints.  6.Acknowledgement I am thankfull to my  mother,father ,brother and sister   I am thankful to Mr.jitendra Soni for its cooperation and thanks to Mr.Hasan Mehmood for some suggestions. also thanks Mrs Roopam Gupta for its continuous motivation. also thankfull to my best friends anjali and avani Sharma.  7.REFERENCES  1 Hosseingholizadeh, A. Abhari, A. A new AgentBased Solution for Wireless Sensor networks Management, 12th Communications and Networking Simulation Symposium CNS, San Diego, CA, USA, 22 27 March 2009. 2 Heinzelman, W. Chandrakasan, A. and Balakrishnan, H. Energy Efficient Communication Protocol for Wireless Microsensor Networks, Proceedings of the 33rd Hawaii International Conference on System Sciences HICSS 00, January 2000. 3 Akyildiz, I.F. Weilian Su Sankarasubramaniam, Y. Cayirci, E., A survey on sensor networks, Communications Magazine, IEEE, vol.40, no.8, pp. 102114, Aug 2002. 4 CheeYee Chong Kumar, S.P., Sensor networks evolution, opportunities, and challenges, Proceedings of the IEEE , vol.91, no.8, pp. 12471256, Aug. 2003. 5 Biswas, S. Morris, R., Opportunistic Routing in MultiHop Wireless Networks, Proceedings of the second workshop on hot topics in networking HotnetsII, Massachusetts, Nov. 2003. 6 Yeling Zhang Ramkumar, M. Memon, N., Information flow based routing algorithms for wireless sensor networks, Global Telecommunications Conference, 2004. GLOBECOM 04. IEEE, vol.2, pp. 742747, 29 Nov. 2004. 7 AlKaraki, J.N. Kamal, A.E., Routing techniques in wireless sensor networks a survey, Wireless Communications, IEEE , vol.11, no.6, pp. 628, Dec. 2004. 8 Tynan, R. Marsh, D. OKane, D. OHare, G.M.P., Agents for wireless sensor network power management, International Conference Workshops on Parallel Processing ICPP 2005, vol., pp.413418, June 2005. 9 Hock Beng Lim Bang Wang Cheng Fu Phull, A. Di Ma, A Middleware Services Simulation Platform for Wireless Sensor Networks, 28th International Conference on Distributed Computing Systems ICDCS 2008 , pp.168173, 1720 June 2008.  10 N.B. Chang and M. Liu, Controlled Flooding Search in a Large Network, IEEEACM Trans. Networking, vol. 15, no. 2, pp. 436449, Apr. 2007. 11 J. Hassan and S. Jha, Optimizing Expanding Ring Search for MultiHop Wireless Networks, Proc. 47th Ann. IEEE Global Telecomm. Conf. GLOBECOM 04, pp. 10611065, Nov. 2004. 12 L. Lima and J. Barros, Random Walks on Sensor Networks, Proc. Fifth Intl Symp. Modeling and Optimization in Mobile, Ad Hoc, and Wireless NetworksWiOpt 07, pp. 15, Apr. 2007. 35International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                               ISSN 22295518  IJSER  2011 httpwww.ijser.org  13 H. Tian, H. Shen, and T. Matsuzawa, Random Walk Routing for Wireless Sensor Networks, Proc. Sixth IntlConf. Parallel and Distributed Computing Applications and Technologies PDCAT 05, pp. 196200, Dec. 2005. 14 Z.J. Haas, J.Y. Halpern, and L. Li, GossipBased Ad Hoc Routing, IEEEACM Trans. Networking, vol. 14, no. 3, pp.479491, 2006. 15 A.V. Kini, V. Veeraraghavan, N. Singhal, and S. Weber, Smartgossip An Improved Randomized Broadcast Protocol for Sensor Networks, Proc. Fifth Intl Conf. Information Processing in Sensor Networks IPSN 06, pp. 210217, Apr. 2006. 16 S. Stanczak, M. Wiczanowski, and H. Boche, Theory and Algorithms for Resource Allocation in Wireless Networks, Lecture Notes in Computer Science LNCS. Springer, Berlin,2006.    36 International Journal of Scientific  Engineering Research, Volume 2, Issue 5, May2011      ISSN 22295518  A Robust H Speed Tracking Controller for Underwater Vehicles via Particle Swarm Optimization Mohammad Pourmahmood Aghababa, Mohammd Esmaeel Akbari   Abstract This paper presents an H controller designing method for robust speed tracking of underwater vehicles, using Particle Swarm Optimization PSO. Nonlinearity mapping of the underwater vehicles model to a nominal linear model, by employing a linear controller for a nonlinear model, is one of the main contributions of this paper. For reaching the linear H controller, the nonlinear models linearized around an operating point. Both nonlinear and linearized models are discussed. A brief explanation of H synthesis is given. Also frequency dependent weighting functions are used for penalizing tracking errors, setpoint commands and measured outputs noises using PSO. Obtained controller is reduced order to achieve a lower order controller. After simulating the reduced order H controller it is embedded into the nonlinear model. By nonlinear simulations, robustness and efficient performance of the H controller is shown. Control efforts of actuators revealed no saturation, therefore it is feasible to implement.  Index Terms H controller, underwater vehicles, particle swarm optimization, robustness.         1 INTRODUCTIONn the past two decades, underwater vehicles have become an intense area of oceanic researches because of their emerging applications, such as scientific inspection of deep sea, exploitation of underwater resources, long range survey, oceanographic mapping, underwater pipelines tracking and so on. Developing a control system that can achieve the aforementioned goal is challenging for a variety of reasons such as the nonlinearity of the dynamics, the multivariable character of the vehicle with coupling among different channels, the consistent amount of uncertainty due to the lack of precise knowledge of hydrodynamic drag coefficients and evaluation of external disturbance due to environmental interaction. Several control techniques have been proposed in literature to deal with uncertainty. Sliding mode controller for trajectory control of underwater vehicles, neglecting the cross coupling terms, is proposed in 2. Multivariable sliding mode control for diving, steering and speed control of underwater vehicles with decoupled design is used in 3.  An H autopilot for subzero II that had two subcontrollers, the longitudinal controller for the forward speed and depth, and the lateral controller for the heading angle is presented in 4. A reduced order H control   that had three SISO decoupled controllers for the forward speed, heading angle and depth control was applied to subzero III in 5. A time delay control law for robust trajectory control of underwater vehicles is proposed in 6.  In this paper, designing of an H controller for robust speed tracking is the major purpose. Position control cannot be performed without suitable speed tracking. Here, both linear and angular speeds are considered to be controlled. Using Particle Swarm Optimization PSO, weighting functions, that capture the disturbance characteristics and performance requirements are selected to take advantage of H design algorithm.  For designing the speed controller, the nonlinear model is linearzed around an operating point. Afterwards, parameters changing mapped to the linear model as uncertainties. Weighted noises are also added to measured outputs. Then weighting functions for setpoint commands and tracking errors are obtained. It is assumed that all states can be measured by sensors, so that the state estimator is not necessary. After designing the H controller and order reduction, it is embedded to full nonlinear model. Tracking robustness and efficiency of the proposed controller is shown by nonlinear simulation. The required control efforts of thrusters are possible to realize. The proposed method has the following characteristics a the problem of speed tracking is considered as a new work, b designed controller is MIMO Multi InputMulti Output without neglecting the cross coupling terms, c the H controller is designed by using PSO, and d the linear controller robustness is shown when it is embedded to the nonlinear model. IJSER  2011 I  Mohammad Pourmahmood Aghababa, Department of Electrical Engineering, Ahar Branch, Islamic Azad University, Ahar,Iran                              mPourmahmoodiauAhar.ac.ir  Mohammd Esmaeel AkbariDepartment of Electrical Engineering, Ahar Branch, Islamic Azad University, Ahar, Iran mAkbariiauAhar.ac.ir  37International Journal of Scientific  Engineering Research, Volume 2, Issue 5, May2011      ISSN 22295518  IJSER  2011  The rest of this paper is preceded as follows. Section 2 presents the nonlinear and linearized motion equations of underwater vehicles.  In Section 3, a general scheme of H control synthesis is firstly explained. Then, the main procedure of PSO method is given. In Section 4, an H controller is designed for speed tracking aim of the underwater vehicles and numerical simulations are performed.  Finally, some conclusions are given in section 5. 2 MOTION EQUATIONS AND DYNAMICS OF UNDERWATER VEHICLES 2.1 The Nonlinear Model of Underwater Vehicles Throughout the marine robotics literature a vehicles six degrees of freedom dynamic equations are expressed as 1      Mv C v v D v v g                                         1  J v  1 2    ,  J diag J J                         2 where s.sin., c.cos., t.tan.,   is the position and orientation of the vehicle in the Earth fixed frame, 6 1R  , v is linear and angular velocity of the vehicle in the body fixed frame, 6 1R  , M is the inertia matrix including added mass, 6 6R  , Cv is a matrix consisting Coriolis and centripetal terms, 6 6R  , Dv is a matrix consisting damping or drag terms, 6 6R  ,  g   is the vector of restoring forces and moments due to gravity and buoyancy, 6 1R  , and   is the vector of forces and moments of propulsion, 6 1R  .The matrix  J   converts velocity in a body fixed frame, v, to velocity in an earth fixed frame, , as shown in Fig. 1. In fact 1 J  and 2  J  convert linear and angular velocities in a body fixed frame, v, to velocities in an earth fixed frame, , respectively. A detailed derivation of these nonlinear equations of motion can be found in 1. Below a small summary of the modeled phenomena is given. 1 Mass and Inertia, M In matrix M, two inertial components are accounted for 1,  MMRBMA, MMT, M0                                         3 The rigid body inertial matrix, MRB, represents the mass and inertia terms due to the mass and other physical characteristics of the craft. However in a dense medium such as water, a considerable contribution to the mass originates from the medium. This so called added mass is accounted for by the matrix MA. 2 Coriolis and Centripetal forces, Cv For matrix Cx, a similar discourse can be held. Both the coriolis and centripetal forces are forces that are proportional to mass and inertia. Hence, the matrix consists of two matrices      RB AC v C v C v TRB RBC C                              4 where CRB represents forces and moments due to the mass and physical characteristics of the craft, CAx incorporates the terms originating from the added mass.  3 Damping terms, Dv In the damping matrix, Dx, four terms are combined 1  DxDp Dsx  Dw  DMx                                             5 where Dp is the potential damping, Dsx is linear and quadratic skin friction, Dw is wave drift damping and DMx is damping due to vortex shedding. Potential damping is introduced due to forces on the body when the latter is forced to oscillate. Skin friction effects can be shown to constitute both a linear and a quadratic term. Wave drift damping only plays a major role at the surface where it can be interpreted as added resistance due to incoming waves. Damping due to vortex shedding is a result of the nonconservative nature of a moving system in water with respect to energy. The viscous damping force due to this phenomenon is a function of the relative velocity of the craft, its physical characteristics and the density and viscosity of the water. 4 Gravitation and Buoyancy,  g   This term models the restoring forces which result from gravitation and buoyancy.  5 Thruster model,   Usually, propellers are used as propulsion devices for underwater vehicles. The load torque Q from the propeller, and the thrust force T, are then usually written as 1 nnJKDQ Q 5  ,  nnJKDT T  04                  6 where n is rotational velocity of the thruster,  is the mass density of water, D is the diameter of the propeller, KQ and KT are the torque and the thrust coefficients of the propeller, and J0 is the advance ratio. In this paper, the thrusters are assumed to be driven by DC motors. DC motors are usually controlled by velocity feedback. It is assumed that six propellers are erected in six freedom degrees. Therefore, ni will be the physical input related to thruster number i. It can be also shown that an algebraic relation exists between the thrust  of propeller i and the physical input. Therefore, the thrust will be chosen as input in the model ui  Ti. 2.2 The Linearzed Model for Underwater Vehicles The nonlinear speed system of the underwater vehicles can be described in state space form by defining a six dimensional state vector xu, v, w, p, q, r as follows.  uBuxfx ,                                                       7 11 ,   MBgxDxCMxf      8 For a linear controller design, it is necessary to extract the linearzed model from the nonlinear model around a representative operating point. In this paper, the nominal value of rotational speed of the propellers is considered 100 rpm. Using this assumption, the operating point is  Fig. 1. Inertial and body coordinate frames 38International Journal of Scientific  Engineering Research, Volume 2, Issue 5, May2011      ISSN 22295518  IJSER  2011  obtained  x01, 1, 1, 1, 1, 1                                                                   9 The linearized model is xCyuBxAx  ,  ,,,,,, 654321  ur q, p,  w,v, u,xTTr q, p,  w,v, u,y                                                        10             where A and B are 6 6 matrices and C is a 61 vector,  i  i1, 2, , 6 are the propeller forces, explained in the previous section. u, v, w and p, q, r are the linear and angular speeds of the underwater vehicle in a body fixed coordinate system, respectively.  The step response of linearized model is shown in Fig. 2. As seen in this figure, the step response is not tracked and system modes are not decoupled. 3 METHODOLOGIES 3.1 H Synthesis Approach Figure 3 shows a tracking problem, with disturbance rejection, measurement noise, and control input signal limitations. K is a controller to be designed, G is the system, as nonlinearity uncertainties modeled, to be controlled and Wnoise, Wcmd and Wperf are weighting functions for sensor noises, setpoint commands and tracking errors, respectively. A reasonable design objective would be to design K to keep tracking errors and control input signal small for all reasonable reference commands, sensor noises, and external force disturbances.  Hence, a natural performance objective is the closed loop gain from exogenous influences reference commands, sensor noise, and external force disturbances to regulated variables tracking errors and control input signal. Specifically, let T denote the closed loop mapping from the outside influences to the regulated variables. Good performance is associated with T being small. The mathematical objective of H control is to make the closed loop MIMO transfer function Ted to satisfy Ted1. The weighting functions are used to scale the inputoutput transfer functions such that when Ted  1, the relationship between e and d is suitable.  Without lack of generality, a mathematical overview of H synthesis is as follows. Figure 4 shows a standard feedback system, where w is the input vector of exogenous signals, e is the output vector of errors to be reduced, y is the vector of measurements that are available for feedback and u is the vector of external force signals. Let Tew denote the closed loop transfer matrix from w to e. The H synthesis problem is to find, among all controllers that yield a stable closed loop system, a controller K that minimizes the infinity norm Ted. Throughout this paper we assume that all states are available for measurement, that is, y equals the internal state of the generalized plant P.  Suppose that a state space realization for P can be written as  uBwBxCx 211                                                        11 xyuDxCe  ,121                                                     12 and assume that A, B2 is stabilizable, D12 has independent columns and the system with input u and output e has no zeros on the imaginary axis.  Theorem. Suppose   0 is a given positive number. Let 22211211HHHHH  denote the Hamiltonian matrix with entries H11AB2D12TD121D12TC1 H12 2 B1B1TB2D12TD121B2T H21C1TI D12 D12TD121 D12TC1 H22ATC1TD12 D12TD121 B2T                                           13 Then, there exists a stabilizing controller K such that Ted    if and only if i   H  has no eigenvalues on the imaginary axis and there exist a basis for the spectral subspace X H  of H  of the form X1T, X2T where X1 and X2 are square matrices of appropriate dimensions and X1 is invertible. ii X  X2X11 is positive 024u1u024v012w024p024q0 20 40012ru20 20 40 0 20 40u40 20 40u50 20 40u60 20 40 sec u3 Fig. 2. Step response of open loop linear model  Fig. 3. Generalized Performance Block Diagram  Fig. 4. Standard feedback configuration 39International Journal of Scientific  Engineering Research, Volume 2, Issue 5, May2011      ISSN 22295518  IJSER  2011  semi definite. In this case, one such controller is Ks  F, where  FD12TD121D12TC1B2TX                                           14 Existence and computation of  X   is a standard matrix algebra problem that can be solved using a standard technique for solving Riccati equations based on the real Schur decomposition 9.  3.2 Particle Swarm Optimization A particle swarm optimizer is a population based stochastic optimization algorithm modeled after the simulation of the social behavior of bird flocks. PSO is similar to genetic algorithm GA in the sense that both approaches are populationbased and each individual has a fitness function. Furthermore, the adjustments of the individuals in PSO are relatively similar to the arithmetic crossover operator used in GA. However, PSO is influenced by the simulation of social behavior rather than the survival of the fittest. Another major difference is that, in PSO each individual benefits from its history whereas no such mechanism exists in GA. In a PSO system, a swarm of individuals called particles fly through the search space. Each particle represents a candidate solution to the optimization problem. The position of a particle is influenced by the best position visited by itself i.e. its own experience and the position of the best particle in its neighborhood. When the neighborhood of a particle is the entire swarm, the best position in the neighborhood is referred to as the global best particle and the resulting algorithm is referred to as a gbest PSO. When smaller neighborhoods are used, the algorithm is generally referred to as a lbest PSO. The performance of each particle i.e. how much close the particle is to the global optimum is measured using a fitness function that varies depending on the optimization problem. The global optimizing model proposed by Shi and Eberhart 7  is as follows xGcrandxPcRANDvwvibest2ibest1i1i        15 1ii1i vxx                                                                          16  where vi is the velocity of particle i, xi is the particle position, w is the inertial weight. c1 and c2 are the positive constant parameters, Rand and rand are the random functions in the range 0,1, Pbest is the best position of the ith particle and Gbest is the best position among all particles in the swarm.  The inertia weight term, w, serves as a memory of previous velocities. The inertia weight controls the impact of the previous velocity a large inertia weight favors exploration, while a small inertia weight favors exploitation 7. As such, global search starts with a large weight and then decreases with time to favor local search over global search 7. It is noted that the second term in equation 15 represents cognition, or the private thinking of the particle when comparing its current position to its own best. The third term in equation 15, on the other hand, represents the social collaboration among the particles, which compares a particles current position to that of the best particle 8. Also, to control the change of particles velocities, upper and lower bounds for velocity change is limited to a userspecified value of Vmax. Once the new position of a particle is calculated using equation 16, the particle, then, flies towards it 7. As such, the main parameters used in the PSO technique are the population size number of birds number of generation cycles the maximum change of a particle velocity Vmax and w. Generally, the basic PSO procedure works as follows the process is initialized with a group of random particles solutions. The ith particle is represented by its position as a point in search space. Throughout the process, each particle moves about the cost surface with a velocity. Then the particles update their velocities and positions based on the best solutions. This process continues until stop conditions is satisfied e.g. a sufficiently good solution has been found or the maximum number of iterations has been reached.  4 H CONTROLLER DESIGNING PROCEDURE  4.1 Weight Selection and Building Model Uncertainty To take advantage of H design algorithm, we formulate the design as a closed loop gain minimization problem. So we select weighting functions that capture the disturbance characteristics and performance requirements to help normalize the corresponding frequency dependent gain constraints.  Wcmd is included in H control problems that require tracking of a reference command. Wcmd shapes the normalized reference command signals into the reference signals that we expect to occur. It describes the magnitude and the frequency dependence of the reference commands generated by the normalized reference signal. Reference commands for underwater vehicles linear and angular speeds are usually flat. This means that underwater vehicle speed does not change frequently and has no high oscillations. Therefore, Wcmd, is selected equal to BsAWcmd                                                                           17 where A and B are two constants that are determined using PSO. Wperf weights the difference between the response of the closed loop system and the ideal model. Often we might want accurate matching of the ideal model at low frequencies and require less accurate matching at higher frequencies, in which case Wperf is flat at low frequencies, rolls off at first or second order, and flattens out at a small, nonzero value at high frequencies. Therefore, the error weights penalize setpoint tracking errors on u, v, w, p, q and r. Hence, Wperf is considered as follows, for all of 40International Journal of Scientific  Engineering Research, Volume 2, Issue 5, May2011      ISSN 22295518  IJSER  2011  them. EDsCWperf                                                                   18 as A and B C, D and E are three constants that are found using PSO. Wnoise represents frequency domain models of sensor noise. Each sensor measurement feedback to the controller has some noise, which is often higher in one frequency range than another.  The weighting function for the sensors would be small at low frequencies, gradually increase in magnitude as a first order or second order system, and level out at high frequencies. Then a high pass filter is selected for weighting functions of measured states. GsFsWnoise                                                                          19 where F and G constants are found by PSO. To complete the uncertainty model, changing of the underwater vehicle speeds due to vehicle parameters changing, that can be produced by hydrodynamic drag coefficients and propellers rotational speeds and external disturbance, should be considered in controller designing procedure. In this paper it performed by evaluating the underwater vehicle nonlinear behavior, when the mentioned vehicle parameters are changed reasonably, and mapping it to the linear model. Therefore, we will build an uncertainty model that matches our estimate of uncertainty in the physical system as closely as possible. Because the amount of the model uncertainty or variability typically depends on frequency, our uncertainty model involves frequencydependent weighting functions to normalize modeling errors across frequency. The following frequency dependent weighting function for both linear and angular speeds is chosen.  JsIHsW yuncerta int                                                                20 where H, I and J constants are computed by PSO.  4.2 H Controller Design and Simulation results Now that all plant components, as illustrated in Figure 3, are described and nonlinearity uncertainties and the weighting functions are constructed. We can design a desired H controller. By using sysic function of MATLAB Robust Control Toolbox, the weighted uncertain model is built. Nonlinearity uncertainties are modeled by using ultidyn function.  The weighting functions unknown parameters are computed using PSO. Therefore, minimizing a cost function, determining the vector PA, B, C, D, E, F, G, H, I, J is the main purpose. For doing this, a performance index as a cost function that should be minimized must be selected. The performance criterion is defined based on some typical desired output specifications in the time domain such as overshoot Mp, rise time Tr, settling time Ts, and steadystate error Ess. Therefore, in this paper, a time domain performance criterion defined by       61611min,i jrijsijK TTePFRQ                                            ssijpij EMe                                                             21 is used for evaluating the H controller performance. where Mpij is the maximum overshoot, Tsij is the settling time, Trij is the rise time and Esij is the integral absolute error of step response i, j1, 2, , 6. Note that desired steady state of diagonal modes of the system i.e. ij is 1 while for nondiagonal modes i.e. ij it is desired to be 0.  4,0 is the weighting factor. The optimum selection of  depends on the designers requirement and the characteristics of the plant under control. We can set  to be smaller than 0.7 to reduce the overshoot and steadystate error. On the other hand, we can set  to be larger than 0.7 to reduce the rise time and settling time. If  is set to 0.7, then all performance criteria i.e. overshoot, rise time, settling time, and steadystate error will have the same worth.  The minimization process is performed using PSO algorithm. Step response of the plant is used to compute four performance criteria Mp, Ess, Tr and Ts in the time domain. At first, the lower and upper bounds of the parameters are specified. Then a population of particles and a velocity vector are initialized, randomly in the specified range. Each particle represents a solution i.e. weighting functions parameters P that its performance criterion should be evaluated. This work is performed by computing Mp, Ess, Tr, and Ts using the step response of the plant, iteratively. Then, by using the four computed parameters, the performance criterion is evaluated for each particle according. Then using equations 15 and 16 the next likely better particles solutions are determined. This process is repeated until a stopping condition is satisfied. In this stage, the particle corresponding to Gbest is the optimal vector P. The optimal P is obtained as P0.15, 1.23, 98.47, 0.95, 0.11, 0.2, 1.51, 5.73, 1.29, 10.33. After constructing the weights and the weighted plant, we have recast the control problem as a closed loop gain minimization. A gain minimizing controller for the uncertain plant can be computed by using hinfsyn function. By using this function, the desired H controller K in Figure 3 is obtained. The obtained controller has 12 inputs plant noisy outputs and weighted setpoint commands, 6 outputs for control forces of the plant, and 18 states, with nominal performance 65.0min  . For model order reduction, modred and balreal commands are used. Small Henkel singular values indicate that the associated states are weakly coupled. With discarding these negligible Henkel singular values, the controller order is reduced to 11. Figure 5 shows the reduced order H controller behavior step response, when it is engaged with the linear plant. As illustrated in this figure, the H controller can control the vehicle to track the desired speeds, efficiently. To assess the behavior of the designed H controller, it is embedded to the full nonlinear model of the underwa41International Journal of Scientific  Engineering Research, Volume 2, Issue 5, May2011      ISSN 22295518  IJSER  2011  ter vehicle as described in section 2.1 to form a closed loop system. Simulations are implemented in MATLAB Simulink. By the step response, the speed tracking quality is examined. Figure 6 shows the robust behavior of the designed controller against the nonlinearity of the nonlinear model. As shown in this figure, when a step is simultaneously commanded to the actuators, the proposed H   controller can follow the signal with small errors. Furthermore, steady state and amplitude errors are desirably small. This means that the designed controller can behave robustly against to the nonlinearities. Figures 7 and 8 show the control efforts of the H controller with the linearized and the nonlinear models, respectively. As illustrated in the figures, the control effort of actuators reveals no saturation and so it is feasible to implement. 5 CONCLUSIONS A robust H controller for underwater vehicles speed tracking is introduced, in this paper. Nonlinearity of nonlinear model is mapped onto the nominal linear model  as  uncertainties. Using  frequency dependent weighting functions that are determined by PSO, tracking errors and noise errors are eliminated, robustly. The designed controller order is reduced. Using nonlinear simulations, robust behavior of the proposed controller is shown. The actuator control efforts were at the suitable rang for implementation. The future work can focus on control of underwater vehicles using nonlinear methods hybrid with intelligent techniques.  REFERENCES 1 Fossen T. I., Guidance and Control of Ocean Vehicles, John Wiley  Sons Ltd, 1994. 2 D.R. Yoerger, and J. E. Slotine, Robust Trajectory Control of Underwater Vehicles, IEEE Journal of Ocean Engineering 10 4, 1985, pp. 462 470. 3 A.J. Healey and D. Lienard, Multivariable Sliding Mode Control for Autonomous Diving And Steering of Unmanned Underwater Vehicles, IEEE Journal of Ocean Engineering 18 3, 1993, pp. 327339. 4 Z. Feng, and R. Allen, H Autopilot Design for an Autonomous Underwater Vehicle, Proceedings of the 2002 IEEE CCACACSD, 2002, pp. 350 354. 5 Z. Feng, and R. Allen, Reduced Order H Control of an Autonomous Underwater Vehicle, Journal of Control Engineering Practice, 2004, pp. 15111520. 6 R. P. Kumar, A. Dasgupta and C.S. Kumar, Robust trajectory control of underwater vehicles using time delay control law, Ocean Engineering 34, 2007, pp. 842849. 7 Y. Shi, and A. R. Eberhart, Modified Particle Swarm Optimizer, Proceedings of the IEEE conference on evolutionary computation. Piscataway, NJ IEEE Press, 1998, pp. 6973 012From u1To u012To v00.51To w00.51To p00.51To q0 0.5 1 1.5012To rFrom u20 0.5 1 1.5From u30 0.5 1 1.5From u40 0.5 1 1.5From u50 0.5 1 1.5From u60 0.5 1 1.5Step ResponseTime secAmplitude Fig. 5. Step response of linear model with reduced order H controller 0 10 20 30 40 50012Response of u0.1 sec0 10 20 30012Response of v0.1 sec0 10 20 30 40012Response of w0.1 sec0 10 20 30012Response of p0.1 sec0 10 20 30012Response of q0.1 sec0 10 20 30 40012Response of r0.1 sec  Fig. 6. Step response of the reduced order H controller integrated in the nonlinear model  Fig. 7. Control efforts of the H controller embedded into the linear plant, time unit is 0.1s  Fig. 8. Control efforts of the H controller embedded into the nonlinear plant, time unit is 0.1s 42International Journal of Scientific  Engineering Research, Volume 2, Issue 5, May2011      ISSN 22295518  IJSER  2011  8 J. Kennedy and R. Eberhart, Particle Swarm Optimization, In Proceedings of IEEE International Conference on Neural Networks, Perth, Australia, vol.4,  1995, pp.19421948. 9 A. Laub, A Schur method for solving algebraic Riccati equations, IEEE Transactions on Automatic Control, AC246, 1979, pp. 913921. 43International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518       CrankNicolson Scheme for Numerical Solutions of Twodimensional Coupled Burgers Equations Vineet Kumar Srivastava, Mohammad Tamsir, Utkarsh Bhardwaj, YVSS Sanyasiraju Abstract The twodimensional Burgers equation is a mathematical model to describe various kinds of phenomena such as turbulence and viscous fluid. In this paper, CrankNicolson finitedifference method is used to handle such problem. The proposed scheme forms a system of nonlinear algebraic difference equations to be solved at each time step. To linearize the nonlinear system of equations, Newtons method is used. The obtained linear system is then solved by Gauss elimination with partial pivoting. The proposed scheme is unconditionally stable and second order accurate in both space and time. Numerical results are compared with those of exact solutions and other available results for different values of Reynolds number. The proposed method can be easily implemented for solving nonlinear problems evolving in several branches of engineering and science. Index Terms  Burgers equations CrankNicolson scheme finite difference Newtons method Reynolds number.         1 INTRODUCTIONHE nonlinear coupled Burgers equation is a special form of incompressible NavierStokes equation without having pressure term and continuity equation. Burgers equation is an important partial differential equation from fluid dynamics, and is widely used for various physical applications, such as modeling of gas dynamics and traffic flow, shock waves 1, investigating the shallow water waves2,3, in examining the chemical reactiondiffusion model of Brusselator4 etc. It is also used to test several numerical algorithms. The first attempt to solve Burgers equation analytically was given by Bateman 5, who derived the steady solution for a simple onedimensional Burgers equation, which was used by J.M. Burger in 6 to model turbulence. In the past several years, numerical solution to onedimensional Burgers equation and system of multidimensional Burgers equations have attracted a lot of attention and which has resulted in various finitedifference, finiteelement and boundary element methods. Finite difference methods can be classified in two broad categories, i.e. explicit and implicit. Chabak and Sharma 7 have executed the solution of two dimensional coupled wave eqution explicitly. An implicit approach has been utilized for solving two dimensional coupled Burgers equations. Since in this paper the focus is numerical solutions of the twodimensional Burgers equations, a detailed survey of the numerical schemes for solving the onedimensional Burgers equation is not necessary. Interested readers can refer to 814 for more details. Consider twodimensional coupled nonlinear Burgers equations   2 22 21   0,                             1Reu u u u uu vt x y x y             2 22 21   0,                             2Rev v v v vu vt x y x y              subject to the initial conditions  1 , , 0  ,    ,  ,u x y x y x y    2 , , 0  ,    ,  ,v x y x y x y     and boundary conditions   , ,   , ,    ,  ,  0,u x y t x y t x y t      , ,   , ,    ,  ,  0,v x y t x y t x y t      Where  ,   ,   and x y a x b c y d       is its boundary  , , u x y t and  , , v x y t are the velocity components to be determined, 1 , 2 ,  and  are known functions and Re is the Reynolds number. The analytic solution of eqns. 1 and 2 was proposed by IJSER  2011 httpwww.ijser.org     Vineet Kumar Srivastava is currently the faculty member at Faculty of Science  Technology, The ICFAI University, Dehradun, India , Email vineetsriiitmgmail.com.  Mohammad Tamsir  is currently the faculty member at Faculty of Science  Technology, The ICFAI University, Dehradun, India ,Email  tamsiriitmgmail.com.  Utkarsh Bhardwaj  is currently third year B.Tech. student in computer science  at the  Department of Computer Science, The ICFAI University, Dehradun, India ,Email haptorkhotmail.com.  YVSS Sanyasiraju is ,currently the professor at the Department of Mathematics, IIT Madras Chennai, India. Email sryedidaiitm.ac.in.  T44International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org   Fletcher using the HopfCole transformation 15. The numerical solutions of this system of equations have been solved by many researchers. Jain and Holla 16 developed two algorithms based on cubic spline method. Fletcher 17 has discussed the comparison of a number of different numerical approaches.Wubs and Goede 18 have applied an explicitimplicit method. Goyon 19 used several multilevel schemes with ADI.  Recently A. R. Bahadr 20 has applied a fully implicit method. In this paper, CrankNicolson finitedifference scheme is used for solving twodimensional coupled nonlinear Burgers equations. Two numerical examples have been carried out and their results are presented to illustrate the efficiency of the proposed method.  2 THE SOLUTION PROCEDURE The computational domain  is discretized with uniform grid. Denote the discrete approximation of  , , u x y t and  , , v x y t at the grid point  , , i x j y n t   by ,ni ju  and ,ni jv  respectively  0,1, 2......,  0,1,2....., x yi n j n   0,1,2......,n  where 1 xx n   is the grid size in xdirection, 1 yy n   is the grid size in ydirection, and k represents the increment in time.  CrankNicolson finitedifference approximation to 1 is given by  1 1 1, , 1, 1, 1, 1,1, ,1      2 2 2n n n n n ni j i j i j i j i j i jn ni j i ju u u u u uu ut x x           1 1, 1 , 1 , 1 , 11, ,1  v     2 2 2n n n ni j i j i j i jn ni j i ju u u uvy y         1 1 11, , 1, 1, , 1,2 22 21 1    Re 2    n n n n n ni j i j i j i j i j i ju u u u u ux x           1 1 1, 1 , , 1 , 1 , , 12 22 21     02    n n n n n ni j i j i j i j i j i ju u u u u uy y             Similarly, CrankNicolson approximation to 2 is given by  1 1 1, , 1, 1, 1, 1,1, ,1      2 2 2n n n n n ni j i j i j i j i j i jn ni j i jv v v v v vu ut x x           1 1, 1 , 1 , 1 , 11, ,1  v     2 2 2n n n ni j i j i j i jn ni j i jv v v vvy y         1 1 11, , 1, 1, , 1,2 22 21 1    Re 2    n n n n n ni j i j i j i j i j i jv v v v v vx x           1 1 1, 1 , , 1 , 1 , , 12 22 21     02    n n n n n ni j i j i j i j i j i jv v v v v vy y             Consider the above nonlinear system of equations in the form   0 ,                          3 w   Where, 1 2 2 , ,........,  ,tng g g 1 1 1 1 1 11 1 2 2 , , , ,........, , ,n n n n n nn nw u v u v u v       and 1 1.x yn n n     Newtons method when it is applied to 3 results in the following steps 1. set 0w , an initial approximation. 2. while for 0,1,.......k  until convergence do  solve         ,k k kJ w w w     set  1     ,k k kw w w      Where   kJ w is the Jacobian  matrix and  kw  is the correction vector. It is a n n  square matrix whose elements are blocks of size 2 2 . Newtons iteration at each timestep is stopped when   5    10kw   . By using Taylors series expansion one can easily see that the present scheme has accuracy of order 2 2 2      t x y      .  3 NUMERICAL EXAMPLES AND DISCUSSION 3.1 Problem 1 The exact solutions of Burgers equations 1 and 2 can be generated by using the HopfCole transformation 13 which are 3 1 , ,  ,4 41 exp  4 4 Re 32 u x y tx y t      3 1 , ,  ,4 41 exp  4 4 Re 32 v x y tx y t        Here the computational domain is taken as a square domain  ,   0 1,0 1x y x y      . The initial and boundary conditions for  , , u x y t and  , , v x y t are taken from the analytical solutions. The numerical computations are performed using uniform grid, with a mesh width 0.05x y    . From Tables 110, it is clear that the results from the present study are in good agreement with the exact solution for different values of Reynolds number. From Tables 110, it is also clear that that the present scheme is unconditionally stable as it is accurate for any time stepsize. Perspective views of u  and v  for  Re500 at 0.5t   with 0.001t   are given in Figs. 1 and 2.   3.2 Problem 2.  45International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                  ISSN 22295518   IJSER  2011 httpwww.ijser.org   Here the computational domain is taken as  ,   0 0.5,0 0.5x y x y      and Burgers equations 1 and 2 are taken with the initial conditions,   , , 0 sin  cos  0 0.5,  0 0.5, , , 0u x y x yx yv x y x y        and boundary conditions,  0, ,  cos ,    0.5, ,  1 cos  0 0.5,  0,0, ,  ,              0.5, ,  0.5u y t y u y t yy tv y t y v y t y          , 0,  1 sin ,     , 0.5,  sin  0 0.5,  0, , 0,  ,               , 0.5,  0.5u x t x u x t xx tv x t x v x t x         The numerical computations are performed using 20 20  grids and 0.0001t  .The steady state solutions for Re 50 and Re 500 are obtained at 0.625t  . Perspective views of u and v for Re 50  at 0.0001t   are given in Fig.s 3 and 4.The results given in Tables 11 14 demonstrate that the proposed scheme achieves similar results given by 16,20.  Table 1 The numerical results for u  in comparison with the exact solution at 0.5t   and 2t   with 0.0001t  ,and Re 10 . x, y t0.5 t2 Numerical   Exact Numerical       Exact 0.1, 0.1         0.61525       0.61525         0.58716         0.58716 0.5, 0.1         0.58540       0.58540         0.56129         0.56127 0.9, 0.1         0.56062       0.55984         0.54330         0.54113 0.3, 0.3         0.61526       0.61525         0.58718         0.58716 0.7, 0.3         0.58555       0.58540         0.56188         0.56127 0.1, 0.5         0.64628       0.64628         0.61721         0.61720 0.5, 0.5         0.61529       0.61525         0.58738         0.58716 0.9, 0.5         0.58707       0.58540         0.56654         0.56127 0.3, 0.7         0.64638       0.64628         0.61771         0.61720 0.7, 0.7         0.61562       0.61525         0.58900         0.58716 0.1, 0.9         0.67557       0.67481         0.65097         0.64817 0.5, 0.9         0.64773       0.64628         0.62264         0.61720 0.9, 0. 9        0.61802       0.61525          0.59642        0.58716  Table 2 The numerical results for v  in comparion with the exact solution at 0.5t   and 2t   with 0.0001t  , and Re 10 . x, y t0.5 t2 Numerical   Exact Numerical       Exact 0.1, 0.1       0.88475       0.88475          0.91284          0.91284 0.5, 0.1       0.91460       0.91460          0.93871          0.93873 0.9, 0.1       0.93938       0.94016          0.95670          0.95887 0.3, 0.3       0.88474       0.88475          0.91283          0.91284 0.7, 0.3       0.91445       0.91460          0.93812          0.93873 0.1, 0.5       0.85372       0.85373          0.88279          0.88280 0.5, 0.5       0.88471       0.88475          0.91262          0.91284 0.9, 0.5       0.91293       0.91460          0.93346          0.93873 0.3, 0.7       0.85362       0.85373          0.88229          0.88280 0.7, 0.7       0.88437       0.88475          0.91101          0.91284 0.1, 0.9       0.82443       0.82519          0.84903          0.85183 0.5, 0.9       0.85227       0.85373          0.87736          0.88280 0.9, 0.9       0.88198       0.88475          0.90358          0.91284  Table 3 The numerical results for u  in comparison with the exact solution at 0.5t   and 2t   with 0.0001t  , and Re 100 . x, y t0.5 t2 Numerical   Exact Numerical       Exact 0.1, 0.1       0.54300       0.54332          0.500470       0.50048 0.5, 0.1       0.50034       0.50035          0.500003       0.50000 0.9, 0.1       0.50000       0.50000          0.500000       0.50000 0.3, 0.3       0.54269       0.54332          0.500441       0.50048 0.7, 0.3       0.50032       0.50035          0.500003       0.50000 0.1, 0.5       0.74215       0.74221          0.555151       0.55568 0.5, 0.5       0.54250       0.54332          0.500415       0.50048 0.9, 0.5       0.50030       0.50035          0.500014       0.50000 0.3, 0.7       0.74212       0.74221          0.554811       0.55568 0.7, 0.7       0.54246       0.54332          0.500683       0.50048 0.1, 0.9       0.74995       0.74995          0.744215       0.74426 0.5, 0.9       0.74213       0.74221          0.559802       0.55568 0.9, 0.9       0.54640       0.54332          0.513409       0.50048  Table 4 The numerical results for v  in comparison with the exact solution at 0.5t  and 2t   with 0.0001t  , and Re 100 . x, y t0.5 t2 Numerical   Exact Numerical      Exact 0.1, 0.1       0.95700       0.95668           0.99953        0.99952 0.5, 0.1       0.99966       0.99965           1.00000        1.00000 0.9, 0.1       1.00000       1.00000           1.00000        1.00000 0.3, 0.3       0.95731       0.95668           0.99956        0.99952 0.7, 0.3       0.99968       0.99965           1.00000        1.00000 0.1, 0.5       0.75785       0.75779           0.94485        0.94433 0.5, 0.5       0.95750       0.95668           0.99959        0.99952 0.9, 0.5       0.99970       0.99965           0.99999        1.00000 0.3, 0.7       0.75789       0.75779           0.94519        0.94433 0.7, 0.7       0.95754       0.95668           0.99932        0.99952 0.1, 0.9       0.75006       0.75005           0.75579        0.75574 0.5, 0.9       0.75787       0.75779           0.94020        0.94433 0.9, 0.9       0.95360       0.95668           0.98659        0.99952                      Table 5 The numerical results for u  in comparison with the exact solution at 0.5t   and 2t   with 0.0001t  , and Re 500 . x, y t0.5 t2 Numerical   Exact Numerical       Exact 0.1, 0.1       0.48732       0.50010          0.49725          0.50000 0.5, 0.1       0.50002       0.50000          0.50024          0.50000 0.9, 0.1       0.50000       0.50000          0.49932          0.50000 0.3, 0.3       0.49531       0.50010          0.50687          0.50000 0.7, 0.3       0.50001       0.50000          0.49929          0.50000 0.1, 0.5       0.74990       0.75000          0.43945          0.50048 0.5, 0.5       0.49438       0.50010          0.49958          0.50000 46International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org   0.9, 0.5       0.49977       0.50000          0.51378          0.50000 0.3, 0.7       0.75001       0.75000          0.41654          0.50048 0.7, 0.7       0.49323       0.50010          0.51054          0.50000 0.1, 0.9       0.75000       0.75000          0.75003          0.75000 0.5, 0.9       0.75001       0.75000          0.42895          0.50048 0.9, 0.9       0.47390       0.50010          0.56293          0.50000   Table 6 The numerical results for v  in comparison with the exact solution at 0.5t   and 2t   with 0.0001t  , and Re 500 .  x, y t0.5 t2 Numerical   Exact Numerical      Exact 0.1, 0.1        1.01268        0.99990        1.00276         1.00000 0.5, 0.1        0.99999        1.00000        0.99975         1.00000 0.9, 0.1        1.00000        1.00000        1.00068         1.00000 0.3, 0.3        1.00469        0.99990        0.99313         1.00000 0.7, 0.3        0.99999        1.00000        1.00072         1.00000 0.1, 0.5        0.75010        0.75000        1.06055         0.99952 0.5, 0.5        1.00562        0.99990        1.00041         1.00000 0.9, 0.5        1.00023        1.00000        0.98622         1.00000 0.3, 0.7        0.74999        0.75000        1.08346         0.99952 0.7, 0.7        1.00677        0.99990        0.98946         1.00000 0.1, 0.9        0.75000        0.75000        0.74997         0.75000 0.5, 0.9        0.74999        0.75000        1.07105         0.99952 0.9, 0.9        1.02610        0.99990        0.93707         1.00000   Table 7 The numerical values for u  in comparison with the exact solution at 0.5t   and 2t   with 0.001t  , and Re 500 .  x, y t0.5 t2 Numerical   Exact Numerical       Exact 0.1, 0.1       0.48732       0.50010            0.49725       0.50000 0.5, 0.1       0.50002       0.50000            0.50025       0.50000 0.9, 0.1       0.50000       0.50000            0.49932       0.50000 0.3, 0.3       0.49531       0.50010            0.50687       0.50000 0.7, 0.3       0.50001       0.50000            0.49929       0.50000 0.1, 0.5       0.74990       0.75000            0.43945       0.50048 0.5, 0.5       0.49438       0.50010            0.49959       0.50000 0.9, 0.5       0.49977       0.50000            0.51376       0.50000 0.3, 0.7       0.75001       0.75000            0.41654       0.50048 0.7, 0.7       0.49324       0.50010            0.51050       0.50000 0.1, 0.9       0.75000       0.75000            0.75003       0.75000 0.5, 0.9       0.75001       0.75000            0.42895       0.50048 0.9, 0.9       0.47380       0.50010            0.56293       0.50000   Table 8 The numerical values for v  in comparison with the exact solution at 0.5t   and 2t   with 0.001t  , and Re 500 . x, y t0.5 t2 Numerical   Exact Numerical       Exact 0.1, 0.1        1.01286        0.99990          1.00276        1.00000 0.5, 0.1        1.00000        1.00000          0.99976        1.00000 0.9, 0.1        1.00000        1.00000          1.00068        1.00000 0.3, 0.3        1.00481        0.99990          0.99313        1.00000 0.7, 0.3        0.99999        1.00000          1.00072        1.00000 0.1, 0.5        0.75010        0.75000          1.06055        0.99952 0.5, 0.5        1.00571        0.99990          1.00041        1.00000 0.9, 0.5        1.00022        1.00000          0.98624        1.00000 0.3, 0.7        0.74999        0.75000          1.08346        0.99952 0.7, 0.7        1.00676        0.99990          0.98950        1.00000 0.1, 0.9        0.75000        0.75000          0.74997        0.75000 0.5, 0.9        0.74999        0.75000          1.07105        0.99952 0.9, 0.9        1.02620        0.99990          0.93707        1.00000  Table 9 The numerical values for u  in comparison with the exact solution at 0.5t   and 2t   with 0.01t  , and Re 500 . x, y t0.5 t2 Numerical   Exact Numerical       Exact 0.1,  0.1       0.48714       0.50010          0.49729          0.50000 0.5,  0.1       0.50002       0.50000          0.50024          0.50000 0.9,  0.1       0.50000       0.50000          0.49934          0.50000 0.3,  0.3       0.49519       0.50010          0.50690          0.50000 0.7,  0.3       0.50001       0.50000          0.49928          0.50000 0.1,  0.5       0.74990       0.75000          0.43939          0.50048 0.5,  0.5       0.49429       0.50010          0.49951          0.50000 0.9,  0.5       0.49978       0.50000          0.51355          0.50000 0.3,  0.7       0.75001       0.75000          0.41647          0.50048 0.7,  0.7       0.49325       0.50010          0.51008          0.50000 0.1,  0.9       0.75000       0.75000          0.75004          0.75000 0.5,  0.9       0.75001       0.75000          0.42909          0.50048 0.9,  0.9       0.47275       0.50010          0.56275          0.50000   Table 10 The numerical values for v  in comparison with the exact solution at 0.5t   and 2t   with 0.01t  , and Re 500 . x, y t0.5 t2 Numerical   Exact Numerical       Exact 0.1, 0.1        1.01286       0.99990         1.00271          1.00000 0.5, 0.1        0.99999       1.00000         0.99976          1.00000 0.9, 0.1        1.00000       1.00000         1.00066          1.00000 0.3, 0.3        1.00481       0.99990         0.99310          1.00000 0.7, 0.3        0.99999       1.00000         1.00072          1.00000 0.1, 0.5        0.75010       0.75000         1.06061          0.99952 0.5, 0.5        1.00571       0.99990         1.00049          1.00000 0.9, 0.5        1.00022       1.00000         0.98646          1.00000 0.3, 0.7        0.74999       0.75000         1.08353          0.99952 0.7, 0.7        1.00676       0.99990         0.98992          1.00000 0.1, 0.9        0.75000       0.75000         0.74996          0.75000 0.5, 0.9        0.74999       0.75000         1.07091          0.99952 0.9, 0.9        1.02725       0.99990         0.93725          1.00000   47International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                  ISSN 22295518   IJSER  2011 httpwww.ijser.org    Fig.1.The numerical value of u  for Re 500  at time level 0.5t   with 0.001t  .  Fig.2.The numerical value of v  for Re 500  at time level 0.5t   with 0.001t  .   Table 11 Comparison of computed values of u  for Re 50  at 0.625t  . x, y              Computed values of u  Present work     A.R.Bahadir     Jain and Holla N20                    N20                   N20 0.1, 0.1 0.97146 0.96688           0.97258 0.3, 0.1 1.15280 1.14827            1.16214 0.2, 0.2 0.86307            0.85911 0.86281 0.4, 0.2 0.97981 0.97637 0.96483 0.1, 0.3 0.66316 0.66019 0.66318 0.3, 0.3 0.77230 0.76932 0.77030 0.2, 0.4 0.4, 0.4         0.58180 0.75856 0.57966 0.75678 0.58070 0.74435       Table 12 Comparison of computed values of v for Re 50 at 0.625t   x, y              Computed values of v  Present work     A.R.Bahadir     Jain and Holla N20                    N20                   N20 0.1, 0.1 0.09869 0.09824           0.09773 0.3, 0.1 0.14158 0.14112 0.14039 0.2, 0.2 0.16754 0.16681 0.16660 0.4, 0.2 0.17110 0.17065 0.17397 0.1, 0.3 0.26378 0.26261 0.26294 0.3, 0.3 0.22654 0.22576 0.22463 0.2, 0.4 0.4, 0.4         0.32851 0.32500 0.32745 0.32441 0.32402 0.31822   Table 13 Comparison of computed values of u for Re 500 at 0.625t  . x, y               Computed values of u  Present work     A.R.Bahadir     Jain and Holla  N20                    N20                   N20 0.15, 0.1 0.96870 0.96650 0.95691 0.3, 0.1 1.03200 1.02970 0.95616 0.1, 0.2 0.86178 0.84449 0.84257 0.2, 0.2 0.87814 0.87631 0.86399 0.1, 0.3 0.67920 0.67809 0.67667 0.3, 0.3 0.79947 0.79792 0.76876 0.15, 0.4 0.2, 0.4         0.66036 0.58959 0.54601 0.58874 0.54408 0.58778   Table 14 Comparison of computed values of v  for Re 500  at 0.625t  . x, y              Computed values of v  Present work     A.R.Bahadir     Jain and Holla N20                    N20                   N20 0.15, 0.1 0.09043 0.09020 0.10177 0.3, 0.1 0.10728 0.10690 0.13287 0.1, 0.2 0.17295 0.17972 0.18503 0.2, 0.2 0.16816 0.16777 0.18169 0.1, 0.3 0.26268 0.26222 0.26560 0.3, 0.3 0.23550 0.23497 0.25142 0.15, 0.4 0.2, 0.4         0.29019 0.30419 0.31753 0.30371 0.32084 0.30927   48International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org    Fig.3.The computed value of u for Re 50  at time level 0.625t  .   Fig.4.The computed value of v for Re 50  at time level 0.625t  .  4 CONCLUSION CrankNicolson finitedifference method for twodimensional coupled nonlinear viscous Burgers equations has been presented.The advantage of the proposed method is that it is second order accurate in space and time. The efficiency and numerical accuracy of the present scheme are validated through two numerical examples. The test examples show that the present scheme is unconditionally stable as there is no constraint on time stepsize. Numerical results are compared well with those from the exact solutions and previous available results. ACKNOWLEDGEMENT The authors are highly grateful to the Vicechancellor, Dean FST, The ICFAI University, Dehradun, India, for providing facilities to carry out the present work. The authors would also like to thank Dr. S.K. Chabak, scientist, Wadia Institute of Himalayan Geology, Dehradun, India, for giving valueable suggestions.  REFERENCES 1 Cole JD, On a quasilinear parabolic equations occurring in aerodynamics, Quart Appl Math 1951 922536. 2 J.D. Logan, An introduction to nonlinear partial differential equations,  WilyInterscience, New York, 1994.  3 L. Debtnath, Nonlinear partial differential equations for scientist and engineers, Birkhauser, Boston, 1997. 4 G. Adomian,  The diffusionBrusselator equation, Comput. Math. Appl. 291995 13. 5 Bateman H., some recent researches on the motion of fluids, Monthly Weather Review 1915 43163170 6 Burger JM, A Mathematical Model Illustrating the Theory of Turbulence, Advances in Applied mathematics 1950 3201230 7 S.K. Chabak, P.K. Sharma, Numerical simulation of coupled wave equation, Int Review of Pure and Applied Mathematics 2007 215969. 8 Hon YC, Mao XZ,An efficient numerical scheme for Burgerslike equations,Applied Mathematics and Computation 1998 953750. 9 Aksan EN, Ozdes A, A numerical solution of Burgers equation,Applied Mathematics and Computation 2004 156395402. 10 Mickens R, Exact solutions to difference equation models of Burgers equation,Numerical Methods for Partial Differential Equations 1986 22123129. 11 Kutluay S, Bahadir AR, Ozdes A,Numerical solution of onedimensional Burgers equation explicit and exactexplicit finite difference methods, Journal of Computational and Applied Mathematics 1999 103251261. 12 Kutluay S, Rsen A,A linearized numerical scheme for Burgerslike equations, Applied Mathematics and Computation 2004 156295305. 13 Ozis T, Aslan Y, The semiapproximate approach for solving Burgers equation with high Reynolds number,Applied Mathematics and Computation 2005 163131145.  14 Wenyuan Liao, An implicit fourthorder compact finite difference scheme for onedimensional Burgers equation, Applied Mathematics and Computation 2008 206755764.  15 C.A.J. Fletcher, Generating exact solutions of the twodimensional Burgers equation, Int. J. Numer. Meth. Fluids 3 1983 213216. 16 P.C. Jain, D.N. Holla, Numerical solution of coupled Burgers equations, Int. J. Numer. Meth.Eng. 12 1978 213222. 17 C.A.J. Fletcher, A comparison of finite element and finite difference of the one and twodimensional Burgers equations, J. Comput. Phys., Vol. 51, 1983, 159188. 18 F.W. Wubs, E.D. de Goede, An explicitimplicit method for a class of timedependent partial differential equations, Appl. Numer. Math. 9 1992 157181. 19 O. Goyon, Multilevel schemes for solving unsteady equations, Int. J. Numer. Meth. Fluids 221996 937959. 49International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                  ISSN 22295518   IJSER  2011 httpwww.ijser.org   20 Bahadir AR. A fully implicit finitedifference scheme for twodimensional Burgers equation, Appl Math Comput 2003 1371317.  50International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518   IJSER  2011 httpwww.ijser.org  Implementation of Adaptive Modulation and Coding Technique using Sami H. O. Salih, Mamoun M. A. Suliman  AbstractDifferent order modulations combined with different coding schemes, allow sending more bits per symbol, thus achieving higher throughputs and better spectral efficiencies. However, it must also be noted that when using a modulation technique such as 64QAM with less overhead bits, better signaltonoise ratios SNRs are needed to overcome any Intersymbol Interference ISI and maintain a certain bit error ratio BER. The use of adaptive modulation allows wireless technologies to yielding higher throughputs while also covering long distances. The aim of this paper is to implement an Adaptive Modulation and Coding AMC features of the WiMAX and LTE access layer using SDR technologies in Matlab. This papper focusing on the physical layer design i.e. Modulation, here the various used modulation type will be implemented in a single Matlab function that can be called with the appropriate coefficients. A comparison with the hardware approaches will be made in terms of SNR vs. BER relation. Index Terms. Adaptive Modulation and Coding AMC, Cognitive Radio CR, LTE, Software Defined Radio SDR, WiMAX.        1 INTRODUCTION                                                                     The growth in the use of the information networks lead to the need for new communication networks with higher data rates. The telecommunication industry is also changing, with a demand for a greater range of services, such as video conferences, or applications with multimedia contents. The increased reliance on computer networking and the Internet has resulted in a wider demand for connectivity to be provided any where, any time, leading to a rise in the requirements for higher capacity and high reliability broadband wireless access Broadband wireless Access BWA telecommunication systems. BWA intensively focused in the last few years. Thus, various new technologies with high transmission abilities have been designed. The BWA has become the best way to meet escalating business demand for rapid Internet connection and integrated triple play services. That is the very base of the HSPA, WiMAX, and LTE concept a wireless transmission infrastructure that allows a fast deployment as well as low maintenance costs. The emergent demand of all types of services, not only voice and data but also multimedia services, aims for the design of increasingly more intelligent and agile communication systems, capable of providing spectrally efficient and flexible data rate access. These systems are able to adapt and adjust the transmission parameters based on the link quality, improving the spectrum efficiency of the system, and reaching, in this way, the capacity limits of the underlying wireless channel. Link adaptation techniques, often referred to as adaptive modulation and coding AMC, are a good way for reaching the cited requirements. They are designed to track the channel variations, thus changing the modulation and coding scheme to yield a higher throughput by transmitting with high information rates under favorable channel conditions and reducing the information rate in response to channel degradation.  2 BWA DEVELOPMENT ROADMAP 2.1 Preface The current WiMAX revision is based upon IEEE802.16e2005, approved in December 2005. It is a supplement to the IEEE802.162004. 1 Thus, IEEE 802.16e2005 improves by  Adding support for mobility  Scaling of the Fast Fourier transform FFT to the channel bandwidth in order to keep the carrier spacing constant across different channel bandwidths typically 1.25 MHz, 5 MHz, 10 MHz or 20 MHz  Advanced antenna diversity schemes, and hybrid automatic repeatrequest HARQ  Adaptive Antenna Systems AAS and MIMO technology  Denser subchannelization, thereby improving indoor penetration  Introducing Turbo Coding and LowDensity Parity Check LDPC  Introducing downlink subchannelization, allowing administrators to trade coverage for capacity or vice versa  Adding an extra QoS class for real time applications T  Sami H. O. Salih is a lecturer at the dept. of Electronics Engineering, Sudan University of Science and Technology SUST.Ccurrently he is PhD.student in telecommunication  engineering in SUST, P.O BOX 111112869, Emailsamisustech.edu  Mamoun M.A. Suliman is an Associate Professor at the dept. of Electronics Engineering, Sudan University of Science and Technology, Sudan, Emailmamounsulimanyahoo.com. 51International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org  In the other hand, Long Term Evolution LTE is the latest standard in the 3rd Generation Partnership Project 3GPP, mobile network technology tree that produced the GSMEDGE and UMTSHSPA network technologies.12 The LTE specification provides downlink peak rates of at least 100 Mbps, an uplink of at least 50 Mbps and RAN roundtrip times of less than 10 ms. LTE supports scalable carrier bandwidths, from 1.4 MHz to 20 MHz and supports both frequency division duplexing FDD and time division duplexing TDD. The main advantages with LTE are high throughput, low latency, plug and play, FDD and TDD in the same platform, an improved enduser experience and a simple architecture resulting in low operating costs. LTE will also support seamless passing to cell towers with older network technology such as GSM, cdmaOne, UMTS, and CDMA2000. The next step for LTE evolution is LTE Advanced and is currently being standardized in 3GPP Release 10. 3               The most important similarity between LTE and WiMAX is orthogonal frequency division multiplex OFDM signaling. Both technologies also employ Viterbi and turbo accelerators for forward error correction. From a chip designers perspective, that makes the extensive reuse of gates highly likely if one had to support both schemes in the same chip or chipset. From a software defined radio SDR perspective, the opportunity is even more enticing. Flexibility, gate reuse and programmability seem to be the answers to the WiMAXLTE multimode challenge. 2.2 Hypothesis of AMC In traditional communication systems, the transmission is designed for the worst case channel scenario thus, coping with the channel variations and still delivering an error rate below a specific limit. Adaptive transmission schemes, however, are designed to track the channel quality by adapting the channel throughput to the actual channel state. These techniques take advantage of the timevarying nature of the wireless channel to vary the transmitted power level, symbol rate, coding scheme, constellation size, or any combination of these parameters, with the purpose of improving the link average spectral efficiency bitssHz. 3 SYSTEM DESIGN 3.1 Preface Most BWA support variety of modulation and coding schemes and allows for the scheme to change on a burstbyburst basis per link, depending on channel conditions. Current systems contains separate hardware channel for each ModulationCoding scheme. The more intelligent approach is to design single soft defined circuit for BPSK, QPSK, 16QAM, and 64QAM based on SDR then design a cognitive engine CE to determine which profile to load and operate. Following is a list of the various modulation and coding schemes supported by WiMAX and LTE. Both WiMAX and LTE support a variety of modulation and coding schemes and allows for the scheme to change on a burstbyburst basis per link, depending on channel conditions. Using the channel quality feedback indicator, the mobile can provide the base station with feedback on the downlink channel quality. For the uplink, the base station can estimate the channel quality, based on the received signal quality. For example, when a user gets closer to a cell site, the number of channels will increase and the modulation can also change to increase bandwidth. At longer ranges, modulations like QPSK which offer robust links but lower bandwidth can give way at shorter ranges to 64 QAM which are more sensitive links, but offer much higher bandwidth. Each subscriber is linked to a number of subchannels that obviate multipath interference. The upshot is that cells should be much less sensitive to overload and cell size shrinkage during the load than before. Ideally, customers at any range should receive solid QoS without drops that 3G technology may experience.          TABLE 1 MODULATIONCODING SCHEMES SUPPORTED BY WIMAX 1   Fig. 1. BWA System Development.   Fig. 2. AMC Cell  52International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org  3.2 System Architecture 3.2.1 Basic Wimax The model for the WiMAX is build from the standard documents 1, 2 as follow               3.2.2 Cognitive Engin When the basic system successfully built and tested, a cognitive engine CE must develop to automatically direct the SDR to load and execute the appropriate profile. The CE refer to predefined polices, while continuously sensing the channel situation. Then, perform its logic to pick up the suitable configuration to execute it in the SDR system.          4 ADAPTIVE COMMUNICATION SYSTEM 4.1 AMC Architecture The function of AMC is based on SDRCR combination. The receiver evaluate received packets i.e. SNR or BER to estimate the Channel Quality Indictor CQI module, then feedback the transmitter to reconfigure itself for the next packet send.         4.2 AMC System Performance The performance of AMC highly depends on the accurate channel estimation at the receiver and the reliable feedback path between that estimator and the transmitter on which the receiver reports channel state information CSI. In order to assure a highquality implementation the next steps must be followed 4.2.1 Channel Quality Estimation The transmitter requires an estimate of the expected channel conditions for the next transmission interval. Since this knowledge can only be gained by prediction from past channel quality estimations, the adaptive system can only operate efficiently in an environment with relatively slowlyvarying channel conditions. Therefore, the delay between the quality estimation and the actual transmission in relation to the maximal Doppler frequency of the channel is crucial for the system implementation since poor system performance will result if the channel estimate is obsolete at the time of transmission. 4.2.2 Parameter adaptation The choice of the appropriate modulation and coding mode to be used in the next transmission is made by the transmitter, based on the prediction of the channel conditions for the next time interval. An SNR threshold such that it guarantees a BER below the target BER BER0, is defined by the system for each scheme whenever the SNR is above the SNR threshold. 4.2.3 Feedback Mechanism Once the receiver has estimated the channel SNR, converted it into BER information for each mode candidate, and, based on a target BER, selected the mode that yields the largest throughput while remaining within the BER target bounds, it has to feed back the selected mode to the transmitter in order that the adaptation can be performed. However, the challenge associated with adaptive modulation and coding is that the mobile channel is timevarying, and thus, the feedback of the channel information becomes a limiting factor. Therefore, the assumption of a slowlyvarying as well as a reliable feedback channel is necessary in order to achieve an accurate performance of the AMC scheme. In this way, no delay or transmission error can occur in the feedback channel so that no discrepancy between the predicted and the actual SNR of the next frame appears. Moreover, the receiver must also be informed of which demodulator and decoding parameters to employ for the next received packet. 5 SIMULATION RESULTS 5.1 Functionality A single function the can give different modulation order from BPSK to MQAM M 2n, where n  2,4,6,... implemented in Matlab. The function called with the modulation order and the SNR in dB as input, then its plot the constella Fig. 3. BWA System Components  Fig. 4. Cognitive Mechanism   Fig. 5. Adaptive System  53International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org  tion and calculates the BER.                      5.2 BER vs. SNR BER is the number of error bits occurs within one second in transmitted signal. BER defined mathematically as follow  BER               1               When the transmitter and receivers medium are good in a particular time and SignaltoNoise Ratio is high, and then Bit Error rate is very low. In our thesis simulation we generated random signal when noise occurs after that we got the value of Bit error rate.  SNR                     2 6 CONCLUSION The function implemented in this paper demonstrates the ability of converge AMC concepts in a single Matlab file. Tests show that all measured can be compared with the hardware model in terms of functionality and system performance. This component can be reused against a defined standard, IEEE 802.1 6e, LTE, or other BWA. The second part of the paper Part II will implement other system component related to coding both for source and channel. In the future this model can be expanded to include the components of the upper layers and a complete end to end BWA system could be built. REFERENCES 1 IEEE 802.162006 IEEE Standard for Local and Metropolitan Area Networks  Part 16 Air Interface for Fixed Broadband Wireless Access Systems. 2 ETSI TS 102 177 Version 1.3. 1, February 2006, Broadband Radio Access Networks BRAN HiperMAN Physical PHY Layer 3 Practical Applications for Wireless Networks, Paris, 10 October, 2006, lET Workshop 2006. 4 Douglas H. Morais, UMTSs LTE Webcast, Adroit Wireless Strategies, 16 Feb. 2010 5 Muhammad Nadeem Khan, Sabir Ghauri, The WiMAX 802.16e Physical Layer Model,University of West England. 6 Sami H. O., Mamoun M. A., Software Defined Radio Approaches on WiMAX Access Layer Design, SUST, September 2009. 7 Matthew Sherman, IEEE Standards Supporting Cognitive Radio and Networks, Dynamic Spectrum Access, and Coexistence, Electronics  Integrated Solutions, July, 2008. 8 KuoHui Li, PhD, IEEE 802.16e2005 Air Interface Overview, WiMAX Solutions Division, Intel Mobility Group, June 05, 2006.   Fig. 4. AMC Constalation Diagrams   Fig. 4. Simulation BER vs. SNR  54International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                           ISSN 22295518     IJSER  2011 httpwww.ijser.org  A QoS Based Web Service Selection                  Through Delegation  G. Vadivelou, E. IIavarasan, R. Manoharan, P. Praveen   Abstract Service selection is essential for fulfilling the requirements of service requestors. In the service oriented environment, Quality of Services QoS is one of the utmost concerns for consumers during service selection. Existing web service standards do not undertake the QoS issue efficiently and the load balancing is not performed to the maximum degree. In this paper we propose a new architecture called the Delegation Web Service DWS for selecting the web service more efficiently and with maximum load balancing. The load balancing is achieved by grouping the web services of similar type from the registry by the DWS for each consumers request and it is predestinated to each monitored web service. The monitoring of QoS parameters such as response time, efficiency, round trip time are done using the Web Service Distributed Management WSDM standard, since it has the better method and specifications.   Index Terms  Delegation Web Service, load balancing, Quality of Services, response time, Service selection, Service oriented architecture, WSDM          1. INTRODUCTION    Web service is a programmable Web application that is universally accessible through standard Internet protocols 1, such as Simple Object Access Protocol SOAP. Web service technology is becoming more and more popular in many practical application domains, such as electronic commerce, flow management, application integration, etc. It presents a promising solution for solving platform interoperability problems encountered by the application system integrators.   With the ever increasing number of functional similar web services being made available on the Internet, how to distinguish the best Web service from others becomes an urgent problem to be solved. Web Service Selection is a key component in serviceoriented architecture 2.      First Author Research Scholar in Computer Science and Engineering, Bharathiyar University Coimbatore, India.    CoAuthors Department of Computer Science  Engineering, Pondicherry Engineering College, Pondicherry, India.   The selection of web service is usually based on the functional requirements of the consumer but those web services may not able to provide the quality the consumer expects.  Consumer requirements may include not only functional aspects of very depends on the ability to describe and to match QoS offers and demands, in addition to functional capabilities 3, 4, 5, 6.The web services has to provide a good quality of service to the consumer. The Web Service Selection based on QoS parameters become the challenging task in current trend. Quality of Service is an aggregated metric for describing characteristics of systems in areas, such as networks and distributed systems. According to Liu Sha 7, QoS based web service selection mechanisms plays an essential role in serviceoriented architectures, because most of the applications want to use services that accurately meet their requirements.  To overcome the above mentioned drawbacks of previous works, a new approach has been proposed which offers a better solution for implementing web service load balancing. It also reduces the complexity of the work in selecting a particular web service and provide less components compared to the previous selector approaches. In this proposed work, each similar type of web service has one Delegation Web Service and it is predestinated thereby we can  A 55International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                           ISSN 22295518     IJSER  2011 httpwww.ijser.org  solve the problem of load balancing. It also simplifies the architecture as well as the Delegation Web Services interface. Moreover in this paper the manageability of resources can be performed efficiently. The standard called WSDM has been provided for the resource monitoring. WSDM includes Management using Web Services MUWS and Management of Web Services MOWS. The MUWS and MOWS of the WSDM standards are used to monitor the QoS of each of those similar web services consumed by the Delegation Web Service. The availability of the web service has been checked by using Web Service Ping operation which is called as a diagnostic tool. With help of these standards the accuracy of the web service can be maintained greatly.  The remainder of this paper is outlined as follows In section 2 we briefly present the framework to monitor the QoS parameters and also the processing states of the request , in Section 3 we present the related researches, in Section 4 we describe the Delegation Web Service DWS architectures concepts and the selection process steps, in Section 5 the selection algorithm based on strategy pattern is discussed, in section 6 the process to balance the load is discussed, in    section 7 the implementation and its results is discussed and Section 8 concludes the paper and presents the future work.  2. BACKGROUND  2.1 WSDM Framework       2.1.1 Framework Description        The WSDM standard specifies how the manageability of a resource is made available to manageability consumers via web services. Endpoints that support access to manageable resources are called manageability endpoints. The implementation behind manageability endpoints must be capable of retrieving and manipulating the information related to a manageable resource. The focus of the WSDM architecture is the manageable resource. The manageable resource must be represented as a Web service. In other words, management information regarding the resource must be accessible through a web service endpoint. To provide access to a resource, this endpoint must be able to be referenced by an endpointreference EPR. The EPR provides the target location to which a manageability consumer directs messages. The manageable resource may also direct notifications of significant events to a manageability consumer, provided the consumer has subscribed to receive notifications. Thus, WSDM covers three modes of interaction between a manageable resource and a manageability consumer.  These modes of interaction are as follows  A manageability consumer can retrieve management information about the manageable resource. For example, the consumer can retrieve the current operating status of the manageable resource or the current state of the process running on the manageable resource.  A manageability consumer may affect the state of some manageable resource by changing its management information.  A manageable resource may inform, or notify, a manageability consumer of a significant event.  This mode of interaction requires the manageability consumer to subscribe to receive events on a desired topic.                                                             Fig. 1 WSDM Framework  2.2 WSDM SPECIFICATIONS  56International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                           ISSN 22295518     IJSER  2011 httpwww.ijser.org  WSDM consists of two specifications Management of Web Service MOWS and Management using Web Service MUWS.    WSDMMUWS   MUWS enables management of distributed information technology IT resources using Web services. Many distributed IT resources use different management interfaces. By leveraging Web service technology, MUWS enables easier and more efficient management of IT resources. This is accomplished by providing a flexible, common framework for manageability interfaces that leverage key features of web services protocols. Universal management and interoperability across the many and various types of distributed IT resources can be achieved using MUWS.   The types of management capabilities exposed by MUWS are the management capabilities generally expected in systems that manage distributed IT resources. Examples of manageability functions that can be performed via MUWS include    monitoring the quality of a service   enforcing a service level agreement   controlling a task   managing a resource lifecycle   WSDMMOWS   MOWS provide mechanisms and methodologies that enable manageable web services applications to interoperate across enterprise and organizational boundaries. It is used to publish Web Services QoS parameters. WSDMs Management of Web Services specification extends MUWS to define how to specifically manage a resource that is a web service. Web services, like other resources, have identity, metrics, configuration, and other capabilities to enable management. For web services the compose ability characteristic is especially interesting because it allows the business function of a service   and the management function for a service to be composed together into a single service.    2.3 REQUEST PROCESSING STATES       A web service endpoint accepts and processes messages targeted at it requests. Every request goes through a number of states e.g. received, processing, completed or failed as defined by the WSLC and extended here.                      Fig. 2 Request processing states Following is a list of elements corresponding to the toplevel states of the request processing state model Fig 2. Request Received State This element corresponds to the Received toplevel state which means that the web service endpoint has accepted a request to perform one of the services functional responsibilities. This state represents the earliest point at which the manageability provider knows that the request was dispatched to the web service endpoint being managed.  Request Processing State This element corresponds to the Processing toplevel state which means that the web service endpoint is doing some internal processingexecution to fulfill the requested function. This state represents the earliest point at which the application module or business logic begins processing the request. For example, if the application server queues the request before dispatching it to the business logic, the time difference between request received and processing will include the duration the request was queued.  Request Completed State 57International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                           ISSN 22295518     IJSER  2011 httpwww.ijser.org  This element corresponds to the Completed toplevel state which means that the web service endpoint successfully completed requested function returning results to the requester.  Request Failed State This element corresponds to the failed toplevel state which means that the web service endpoint encountered an error and didnt complete the requested function, returning errorfault to the requester. The RequestProcessingStateType XML Schema type is declared as follows.       An instance of the request processing state information represented in XML may appear as shown in the following example          3. RELATED WORKS      According to Liu Sha 7, the Web service selection is usually driven only by functional requirements, which cant guarantee the realtime validity of the web services selected. So he proposed a QoS based Web Services Selection Model WSSMQ to provide QoS support for service publishing and selection. In the model, the QoS of web services is managed, including defining the QoS model, collecting the QoS information, computing and maintaining the QoS data. Upon the QoS management, the web services that match the requirements of consumers are ranked for selection according to the overall QoS utility. In 8, Web service architecture employs an extended UDDI registry to support service selection based on QoS, but only the certification approach is used to verify QoS and no information is provided about the QoS specification.   According to Diego Zuquim Guimares Garcia and  Maria Beatriz Felgar de Toledo 9, an extended web service architecture can be used to support QoS management. In this approach, QoS information derived from policies based on WSPolicy is encapsulated inside QoS Policy structures stored in UDDI registries. Each element of a QoS Policy structure can be associated with a Technical Model tModel structure, which allows specification, standardization and reuse of QoSrelated concepts. Furthermore, the extension allows the use of brokers to facilitate service selection according to functional and nonfunctional requirements, and monitors to verify QoS attributes. According to S.Ran 10, a component called certifier and a new extension of UDDI is modeled. The certifiers role is to verify service providers QoS claims and the new UDDI registry is a repository of registered web services with lookup facilities. The new registry differs from the current UDDI model by having information about the functional description of the web service as well as its associated quality of service registered in the repository. Lookup could be made by functional description of the desired web service, with the required quality of service attributes as lookup constraints. The new role in this model is the Web service QoS certifier that does not exist in the original UDDI model. The certifier verifies the claims of quality of service for a web service before its registration.    Y.Lee 11 introduces WSQMS Web Service quality Management System to measure QoS of web services. It further introduces WSQDL Web Service Quality Description Language. It advocates the use of the QoS enhanced UDDI for web service selection. Tao Yu et al. 12 design the service selection algorithms to meet the endtoend QoS constraints. Their works are not focused on the trustworthiness of QoS criteria of a service. Although the global quality constraints can be satisfied, service selection may not be locally optimized. Therefore, good component service xscomplexType nameRequestProcessingStateType xscomplexContent xsextension basemuwsStateType xscomplexContent xscomplexType myRequestProcessingStateInformationElement xsitypemowsRequestProcessingStateType mysoapSerializationState mowsRequestProcessingState mysoapSerializationState myRequestProcessingStateInformationElement 58International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                           ISSN 22295518     IJSER  2011 httpwww.ijser.org  often fails to exert its potential and embody its personality. The work was proposed in the reference 13, which presented a model of reputationenhanced QoSbased web service discovery that combines an augmented UDDI registry to publish the QoS information and a reputation manager to assign reputation scores to services. However, it only described abstract service matchmaking, ranking and selection algorithm. The references 14, 15 introduces WS QoS Web Service QoS, a framework for QoS based selection and monitoring of web services. It advocates the use of a Web Service Broker for QoS based web service selection. Frameworks to support QoS verification with the goal of guarantying quality levels are described in 16, 17, 18.   The works discussed above may have certain limitations and involves complexity in monitoring the QoS parameters efficiently and also the load balancing may not be provided to maximum degree. So a new approach has been proposed to overcome the limitations of the existing works.   4. DELEGATION WEB SERVICE ACHITECTURE  4.1 Concept     The new approach that is the Delegation Web Service as selector architecture is predestinated to implement web service load balancing. In this approach a Delegation Web Service for each web service type is made, as this simplifies the architecture as well as the Delegation Web Services interface. However, one Delegation Web Service can also be used for multiple web service types. The Delegation Web Service does not implement any functional parts. It delegates the functional request to corresponding web services. This DWS is used to perform the load balancing factor in an efficient way by grouping all the similar web services requested by the consumer into its register module and there by assigning the priority value to all the grouped web services and it will use the particular web service with the highest priority value. In case if that particular service gets overloaded then immediately it will use the next service in the prioritized order from the register module of DWS. Moreover it will consume functional as well as nonfunctional parts from the used web services. Nonfunctional parts will include MOWS and web service Ping, as well as MUWS from multiple MUWS web services. The Delegation Web Service will then decide which web service it delegates to. The decision is based on the consumers functional requirements and the nonfunctional requirement that is the QoS preferences.                                                      Fig. 3 Proposed Architecture 4.2 Selection Process Steps Following are the steps to select the best Web Service using the Delegation Web Service 1 In step 1 the consumer requests the Delegation Web Service DWS for the best web service by giving the functional requirements along with the QoS preference. 2 In step 2 the DWS look for similar web services according to the functional requirements of the consumer from the UDDI registry. 3 In step 3 the DWS send the request to publish the QoS parameters of each web services. 4 In step 4 the web services publish their QoS parameters to DWS. 5 In step 5 DWS request the WSDM to monitor the QoS parameters of the consumers preference of each web services by using the selection algorithm which is described below. 6 In step 6 the WSDM response with the QoS metrics value and report the best Web Service. 59International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                           ISSN 22295518     IJSER  2011 httpwww.ijser.org  7 In step 7 the request will be sent to the best web service that has been reported by the WSDM and that particular Web Service will then respond to the DWS. 8 In step 8 the best web service meeting the requirement and the QoS preference will be sent to the consumer.   5. SELECTION ALGORITHM                                The above algorithm is based on the strategy pattern to select the best Web Service by monitoring the QoS parameters specified by the user. The strategy pattern based selection algorithm yields easy to implement algorithms and to use multiple diverse selection algorithms. The decision context of the algorithm is to return the best web service to the Delegation Web Service. At first it checks for the service identification to see that particular service belongs to the service list if not it will added to the service list.  Then the end point reference will be assigned to it and also the corresponding MUWS address to which it should get monitored.  After that get the QoS parameters to be monitored for all the similar web services which have been sent by the Delegation Web Service and those corresponding parameters will be then monitored. Then the result of the monitored parameters of all the web services in the list will be compared and the decision will be made to select the best web service. After the decision the result will be sent to the decision context and then it will be responded to the Delegation Web Service. Multiple algorithms can be defined for the same QoS parameter depending on the constraints. 6. MANAGEMENT OF BALANCING THE LOAD  In the proposed work the load balancing factor is concerned greatly. For every consumer request the DWS look into the registry and group all the similar web services meeting the functional requirements. In this approach for each type of web services a Delegation Web Service is provided and also it is predestinated in sending the request to the web services to implement web service load balancing to maximum. Each web service will be provided with some threshold value and by reaching above the value will make the service to get overloaded. Since for each type a DWS is provided it is easy to balance the load when it gets overloaded by making the next subsequent monitored web service to be available.   From the consumer the functional requirement needed will be requested to DWS along with the QoS preferences. Then the DWS identifies the type of service based on the functional requirement requested by the consumer and will look into the registry by specifying the name of the service. The Find module of the UDDI registry will display all the services of the similar type. The DWS will then  Input Web Services WI, WI1Wn with QoS constraints Output Best performing Web Service Wk satisfying QoS constraints Begin DecisionContext  Best Web Service check for service identification If Wi    SLService List AddServiceWi to SL   adding service to the service list SL Assign Wi  EPR, IP, EPRMUWS  assigning endpoint reference, IP and the MUWS EPR to the added Web Service While  ServiceList  empty do ServcieListnull throw List is Empty Else getstrategyQoS  Getting the QoS parameter to be monitored with the constraint  DecisionStrategyQoS for all Wk SL   QoS monitoring of the Web Services in the list Compare Result QoS Wk SL     To compare all the web services result of the QoS from the list Select WkSL  Selecting the best Web Service Return Best WebServiceWk   returning the best web services satisfying the QoS constraints End 60International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                           ISSN 22295518     IJSER  2011 httpwww.ijser.org  group all the similar web services into its registry module. Then each of these similar types of web services will be monitored and assigned the priority value so that the DWS will be able to predestinate the request of the consumers to the web service. This mechanism provides the maximum degree of load balancing compared to the other approaches.  7. IMPLEMENTATION AND EXPERIEMENTS   The proposed system is implemented using the Eclipse IDE and Apache Tomcat server as the service deploying platform. Apache Muse a java based implementation is used to implement the WSDM standards. In the Apache Muse framework first create a WSDL specifying the Delegation Web Service interface and then create DelegationAsSelectorResource.rmd. Secondly add the Operation getBestWS, for returning the Address of the web service with the best QoS. The DelegationAsSelector.wsdl needs to contain all functional operations, of the web service it delegates to. The code is then imported to the Eclipse and then being tested with the TestServlet to be run on the server as shown in the Fig 4.    Fig. 4 Testing of the services using the Test Servlet   Then the result of the best web service for different QoS parameters in the service list based on the proposed selection algorithm will be displayed as shown in the Fig. 5   Fig. 5 Result of the Best Web Service in the Service List  For inquiry and publishing of the service UDDI browser and jUDDI registry is used. The jUDDI is the java implementation of the UDDI registry. The module publishes the service in the UDDI registry with the WSDL file and extracts the QoS information of the service. The service can be published and inquired by specifying the URL into the registry information module as shown in the Fig. 6     Fig. 6 The info window for publishinquiry of service   After the service has been published it can be inquired at any time for the functional implementation of the service. The customer queries the Find module for services with functional and QoS requirements. The Delegation Web Service gets functional matched services from the registry and then requests those web services for the QoS attributes of each Web services and then it will be monitored by the WSDM framework. The Find module of the registry provides the search with three options as shown in the Fig. 7 61International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                           ISSN 22295518     IJSER  2011 httpwww.ijser.org     Fig. 7 The Find window to search any published     services   The previous work 7 is taken for experiment and compared with the proposed work. Both the works have been experimented in their own form of selection process. Then the experimental result of both the works is calculated by evaluating the parameters such as responsiveness of the web service selection per request and efficiency. These parameters are defined as follows  Efficiency Refers to the overall quality of the service selection and the load balancing factor.  Responsiveness Defines the time taken to response for each consumers request second    The comparison graph of efficiency for the proposed architecture and the previous work 7 is shown below Fig. 8 in which for example the consumer 1 gets the efficiency of 94.5 percent with the proposed work whereas the same consumer gets the efficiency with 92.54 percent. The Fig. 9 shows the responsiveness of the web service selection by the selector to the consumers request. Fig. 8 Efficiency of the web service using proposed and previous techniques   Fig. 9 Responsiveness of web service selection to customers request  8. CONCLUSION    Web Service Selection based on QoS value is an important research area. The selection of web service are usually based on the functional requirements of the consumer but those web services are not able to provide the accuracy in their services The decision should always be based on QoS parameters important to the specific consumer. The proposed Delegation Web Service as selector architecture hides the web services and also it is predestinated to implement web service load balancing. For each type of Web Services a Delegation Web Service is provided for better load balancing. The QoS monitoring is done efficiently by using the WSDM and the best web service is selected based on the selection algorithm being proposed. Future and upcoming work is to include more QoS parameters to get monitored by the WSDM and to have one Delegation Web Services for multiple web services types.   9. REFERENCES  1 Ryman, Simple object access protocol SOAP and Web services, Proceedings of the 23rd International Conference on Software Engineering ICSE 2001, Toronto, Ontario, Canada, pp. 689, 2001.  2  Berbner, T. Grollius and N. Rep, An approach for the Management of Service oriented Architecture SoA based Application Systems, in Proceedings of the Enterprise Modelling and Information Systems Architectures, Oct.2005.  3 G. Alonso, F. Casati, H. Kuno, and V. Machiraju. WebServices Concepts, Architectures and Applications, Springer, 2004. 62International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                           ISSN 22295518     IJSER  2011 httpwww.ijser.org  4 M. P. Papazoglou and D. Georgakopoulos. Service oriented computing  guest editorial. Communications of theACM, 46102428, 2003.  5 H. Ludwig, Web services QoS external SLAs and internal policies or how do we deliver what we promise, In Proc. of the Intl Conf. on Web Information Systems Engineering Workshops, pages 115120. Springer, 2004.  6 E. Lee, W. Jung, W. Lee, Y. Park, B. Lee, H. Kim, and C. Wu, A framework to support QoSaware usage of Web services, In Proc. of the Intl Conf. on Web Eng., pages 318327.Springer, 2005..  7 Liu Sha, Guo Shaozhong, Chen Xin and Lan Mingjing, A QoS based Web Service Selection Model, in Proceedings of the IEEE International Forum on Information Technology and Applications, pp.353356, 2009. 8 M. A. Serhani, R. Dssouli, A. Hafid and H. Sahraoui, A QoS broker based architecture for Efficient web services selection, in Proceedings of the IEEE International Conference on Web Services ICWS, Proceedings, pp. 113120,2005.  9 Diego Zuquim Guimares Garcia and Maria Beatriz Felgar de Toledo, A Web Service Architecture Providing QoS Management, in Proceedings of the Fourth Latin American Web Congress, 2006.  10     S. Ran, A Model for Web Services Discovery with QoS, in Proceedings of the ACM SIGecom Exchanges, pp.110, 2003   11 Y. Lee, Quality Context Composition for Management of SOA Quality, 2008 IEEE International   Workshop on Semantic Computing and Applications, pp. 117122, Sept. 2008.  12 Tao Yu, KweiJy Lin, Service Selection Algorithms for Web Services with Endtoend QoS       Constraints, In Proc. Of the IEEE International Conference on ECommerce Technology, 2004.  13 Z. Xu, P. Martin, W. Powley and F. Zulkernine,Reputationenhanced QoSbased web services discovery,In Proc. of the IEEE Intl. Conf. on Web services, pp.249256, 2007.  14 M. Tian, A. Gramm, H. Ritter, J. Schiller, Efficient Selection and Monitoring of QoSaware Web services with the WSQoS framework, In proceedings of the 2004 IEEEWICACM International Conference on Web Intelligence, pp.152158, Sept. 2004.  15 D. A. DMello, V. S. Ananthanarayana and Santhi T, A QoS Broker Based Architecture for Dynamic Web Service Selection, in Proceedings of the IEEE Second Asia International Conference on Modelling  Simulation, pp.101106, 2008.   16 H. Ludwig, A. Keller, A. Dan, R. P. King, and R. Franck, Web Service Level Agreement Version 1.0Specification, IBM, 28thJan, 2003.   17 S. Ran., A framework for discovering Web services with desired quality of services attributes, In Proc. of the Intl Conf.on Web Services, pages 208213. CSREA Press, 2003.  18 Z. U. Singhera Extended Web services framework to meet nonfunctional requirements, In Proc. of the Symposium on Applications and Internet, pp334340, 2007.   63International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518   IJSER  2011 httpwww.ijser.org  Iatrogenic effects of Orthodontic treatment  Review on white spot lesions  Sangamesh B., Amitabh Kallury   Abstract Demineralization is an inevitable side effect associated with fixed appliance orthodontic treatment, especially associated with poor oral hygiene. Fixed orthodontic appliances create several retentive areas for the accumulation of bacterial plaque. The acidic byproducts of these bacteria are responsible for the subsequent enamel demineralization and formation of white spot lesions WSL, causing caries therefore leading to poor esthetics, patient dissatisfaction and legal complications. This highlights the need for assessing the saliva, oral hygiene status and caries rate before beginning of treatment and initiating preventive measures. Orthodontists must take up active responsibility to educate the patients about the importance of maintaining good dietary compliance and excellent oral hygiene regime. Depending on the oral environment, WSL can develop into cavities, stay stable for a long time, or heal to a certain extent. Thus, the prevention of WSL is crucial to prevent tooth decay as well as minimize tooth discoloration that could compromise the treatment results. Index Terms Iatrogenic effects, White spot lesion, Incidence, Orthodontic Treatment, Duration, Oral hygiene, Etiology, Prevention, Fixed Appliances, Demineralization.          1 INTRODUCTION                                                                     hite spot lesion WSL is a common iatrogenic effect seen in patients undergoing orthodontic treatment with fixed appliances Fig.1.1,2  Individuals with malocclusions often have many plaque retention sites due to the irregularities of their teeth Fig.2. Orthodontic treatment with fixed appliances and complex loop designs further increases the risk for development of WSL due to the creation of additional retention sites on surfaces generally not susceptible to caries.2 Hence a strong corelation exists between oral hygiene and caries incidence in orthodontic patients as compared to in non orthodontic individuals.3 Despite intensive efforts to educate patients about effective oral hygiene procedures, WSL associated with fixed orthodontic appliances remains a significant clinical problem Fig.3. This clinical problem has increased since the advent of directly bonded orthodontic brackets.4 Appearance of these spots after the completion of orthodontic treatment can lead to patient dissatisfaction and legal complication.5 The formation of WSL after completion of orthodontic therapy is discouraging to a speciality whose goal is to improve esthetics in the dentofacial region. Orthodontists should be proactive and take active responsibility to prevent the development of WSL by educating their patients about the importance of maintaining an excellent dietary compliance and oral hygiene regime. Oral hygiene regime must include topical fluoride agents such as fluoridated toothpaste, fluoridecontaining mouth rinse, gel and varnish to prevent or minimize the formation of WSL during orthodontic treatment.6  DEFINITION The term white spot lesion was defined as the first sign of a caries lesion on enamel that can be detected with the naked eye.7 The WSL has also been defined as subsurface enamel porosity from carious demineralization that presents itself as a milky white opacity when located on smooth surfaces.8   CLASSIFICATION OF WHITE LESIONS ON ENAMEL White discolorations of enamel can be classified as dental fluorosis, opacities, or WSL.6 A set of criteria has been developed to differentiate between fluorosis and opacities.9 Fluorosis Fig.4 is a whiteyellowish lesion that is not well defined, blends with normal enamel, and has symmetrical distribution in the mouth. Nonfluoride opacities have a more defined shape, are well differentiated from surrounding enamel, often located in the middle of the tooth, and randomly distributed Fig.5.  INCIDENCE Orthodontic patients have significantly more WSL than nonorthodontic patients and these WSL may present esthetic problems years after treatment.3,10 A recent review of literature11 showed variations ranging from 2 to 97, for WSL prevalence associated with orthodontic treatment3,10,1217. This high prevalence is attributed to the difficulties in performing oral hygiene procedures on bonded dental arches along with longtime accumulation and easier retention of bacterial plaque on tooth surfaces around fixed orthodontic appliances.15,18 The variation in WSL prevalence among studies could be attributed to differences in the number of teeth exW  Sangamesh B  Assistant Professor, Dept. of Orthodontics, SDM College of Dental Sciences, Dharwad. Email bsangameshgmail.com Amitabh Kallury  Professor and Head, Dept. of Orthodontics, Peoples Dental Academy, Bhopal. Email dr.amitabhkallurygmail.com 64International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org  amined, the methods and the standardizations in examinations, the location of the study sample cultural differences, time era of the study, age at the start of treatment, treatment duration, and materials banding vs bonding.19 In general, the prevalence of WSL in patients after orthodontic treatment varies from 15 to 8513, with most studies reporting 50 to 70.10,1517,2022 It is reported that any tooth in the mouth can be affected by the process with the common ones being maxillary lateral incisors, maxillary canines, and mandibular premolars.15 The incidence was highest in the labiogingival area of the maxillary lateral incisors Fig.6 and lowest in the maxillary posterior segment. The reported incidence and prevalence of WSL between males and females have been found to be inconclusive.10,11,14 No significant differences between the right and left sides of the maxilla and mandible were noted.10,14   MECHANISM OF FORMATION OF WSL WSL can occur on any tooth surface in the oral cavity where the plaque is allowed to develop and remain for a period of time Fig.7. The naturally occurring self cleansing mechanisms of the oral musculature and saliva are limited by the irregular surface of brackets, bands, and wires.23 The composition of the bacterial flora of the plaque shows a rapid shift following the placement of orthodontic appliances. Patients undergoing treatment with fixed orthodontic appliances have a rapid increase in the volume of dental plaque with a lower pH than that in nonorthodontic patients.24,25 The levels of acidogenic bacteria, especially Streptococcus mutans and lactobacillus, are significantly elevated.26 Both S. mutans and lactobacilli are often associated with caries development. Streptococcus mutans colonize over the retentive areas of orthodontic appliances and surrounding enamel surfaces. Lactobacillus is responsible for the progression of the carious lesion. Their presence in large numbers is indicative of the necessary condition for dental caries to exist.27 However, the association between caries and bacteria is not straightforward. The prediction of caries development based on bacterial counts is uncertain and of minor clinical significance.28 S. mutans and lactobacilli produce organic acids in the presence of fermentable carbohydrates and this is responsible for lowering the pH. Sucrose plays an important role in plaque formation inducing the formation of a cariogenic plaque.29 There is a  direct relationship between plaque pH and total plaque fluoride.  Total plaque fluoride levels are low in areas of low pH. The lowest pH as low as 4 during resting and fermenting conditions was observed in the plaque of the bonded upper incisors.30 After bonding, resting pH is lowered. In the patient with good oral hygiene, fluoride is able to prevent lesions to develop by increasing remineralization and inhibiting demineralization. With poor oral hygiene, plaque builds up around the appliance and the resting pH may reach the limit of the fluoride effect at pH 4.5. During an acid attack, caries and even erosions develop.29 Carious decalcification occurs when the pH drops below the threshold for remineralization and creates an alteration in the appearance of the enamel surface which is visualized as WSL.25,31 Such lesions have been clinically noticed within a short span of 4 weeks2. If these are not treated, they progress to a cavitated carious lesion.32 WSL makes the affected area softer than the surrounding sound enamel, making the tooth more prone to caries33. There is about 10 reduction in the mineral content of enamel in these incipient carious lesions. This leads to their increased abrasion in vivo.34  This makes the affected teeth more susceptible to enamel loss while debonding.35 Fast developing white spots may remineralize almost completely within a few weeks of the removal of the cariogenic challenge. However, lesions that develop slowly take a longer period to remineralize.36 Microleakage around orthodontic brackets can be another cause for the formation of WSLFig.8. 37 The teeth expand and contract when they are heated and cooled by the ingestion of hot or cold foods38. The linear thermal coefficient of expansion of enamel, ceramicmetal brackets and the adhesive systems do not match.39 This repeated expansion and contraction at different coefficients results in fluids being sucked in and pushed out at the margins of the bracket. In comparison with ceramic brackets, the metal brackets are associated with more microleakage Fig.9.40 Metal brackets contract and expand more than ceramic brackets, enamel, or the adhesive systems, producing microgaps between the bracket and the adhesive system causing leakage of oral fluids and bacteria beneath the brackets, leading to the formation of WSL.41  RISK FACTORS FOR WSL Formation of WSL is primarily due to the subsurface demineralization resulting in porosities and a change in the optical properties. If the surface of porous enamel remains intact, there is a possibility of arrestremineralization of the lesion due to the buffering action of the saliva. If the pH of plaque remains low for a prolonged period of time, the environment becomes conducive for long periods of demineralization with short periods of remineralization, resulting if frank carious lesions. Risk factors for the development of incipient caries during orthodontic treatment are young age preadolescents, number of poor oral hygiene citations during treatment, unfavorable clinical outcome score, white ethnic group, and inadequate oral hygiene at the initial pretreatment examination.42 Factors such as the patients medical history, dental history, medication history, diet salivary flow rate, levels of calcium, phosphate, and bicarbonate in saliva, fluoride levels and genetic susceptibility also play an important role.23,43,44 There is a poor correlation between length of treatment time and the incidence of number of white spot formations.14  PREVENTION OF WSL Studies have shown that decalcification is a significant risk during fixed orthodontic treatment.3,10,14,1745 Current evidence suggests that topical fluoride treatment TFT is beneficial in preventing the development of WSL during orthodontic treatment.46 When topical fluoride is applied on the tooth surface enameldentin, a calcium fluoridelike material CaF2 builds up in plaque, or in incipient lesions which acts as a reservoir and releases fluoride ions when the pH is lowered during a caries attack.47 Implementing a good oral hygiene regimen including proper tooth brushing with a fluoridated dentifrice is the most important prophylactic 65International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org  measure to prevent the occurrence of WSL in orthodontic patients. Fluoride concentrations of less than 0.05 are beneficial in reduction of the carious lesions.48 Evidence suggests that reduced demineralization and enhanced remineralization can occur with a toothpaste containing 5000ppm fluoride.49,50 When fluoride ions are incorporated into the surface of enamel, it forms a fluoroapatite crystal structure that has lower solubility in the oral environment compared with hydroxyapatite. Fluoroapatite helps in reducing tooth decay by remineralization of small decalcified areas and reduction in the formation of new lesions.51 In addition, stannous fluoride may have a plaqueinhibiting effect by interfering with the adsorption of plaque bacteria to the enamel surface.52,53 Atoms of tin in stannous products block the passage of sucrose into bacterial cells and thus inhibit acid production. The use of a fluoridated antiplaque dentifrice may reduce enamel demineralization around brackets more than the use of a fluoridated dentifrice alone.54 Enamel dissolution occurs rapidly around orthodontic brackets even during regular use of a fluoride dentifrice31. Thus, supplemental sources of fluoride are suggested. Fluoridated mouth rinses containing 0.05 sodium fluoride used daily have been shown to significantly reduce lesion formation beneath bands. Chemical agents such as chlorhexidine or benzydamine used in the form of mouth rinses or oral sprays are useful adjuncts in plaque and inflammation control.55 These mouth rinses have been combined with antibacterial agents such as chlorhexidene, triclosan, or zinc to improve their cariostatic effect.33 When patients have been noncompliant with other oral hygiene regimens, chlorhexidine mouthwashes might be beneficial in preventing white spot caries lesions as an intensive, shortterm regimen. Chlorhexidine mouthwash used as a complement to fluoride therapy has demonstrated demineralizationinhibiting tendencies in patients with fixed orthodontic appliances.56 The main goal of antimicrobial therapy is to achieve a shift from an ecologically unfavorable to an ecologically favorable biofilm.57 Patients are instructed to use chlorhexidine rinse available in nonalcohol formulations for patients with xerostomia or saliva dysfunction for 30 seconds once a day, preferably before bedtime, because saliva flow diminishes overnight and the concentration of the drug in the oral cavity remains high until morning.58 A 14day regimen is usually recommended.58 While these products provide the patient with increased caries protection, patient compliance is mandatory. A fluoride mouth rinse will work best if it is used regularly by the patient. Studies have showed that less than 15 of orthodontic patients rinsed daily as instructed but patients who were more compliant had fewer WSL.15  An inoffice application of a high concentration of fluoride in the form of a varnish can be beneficial for the less compliant patients. It eliminates the need for patient cooperation that is required with fluoride rinses. The American Dental Associations Council on Scientific Affairs recommends application of inoffice fluoride varnish at sixmonth intervals for moderate and highrisk patients. Although varnish application is associated with the temporary discoloration of the teeth and gingival tissue, it has been reported that the application of a fluoride varnish resulted in a 44.3 reduction in enamel demineralization in orthodontic patients.59 Acidresistant coatings of calcium fluoride or titanium fluoride on the enamel surface and the use of fluoride in combination with different antimicrobials have been suggested to improve the cariostatic effect of fluoride at low pH.60 Varnish forms of the other antibacterial solutions such as benzydamine, triclosan, and xylitol could be helpful for suppressing levels of oral mutans or the other microbes for long periods, when used before the placement of fixed orthodontic appliances. In contrast to onetime topical application in high doses, a longterm, lowdose fluoride availability might increase the cariesresistant fluorapatite concentration in enamel, helping the prevention and reduction of demineralization.48,51 Unfortunately, preventive and chemoprophylactic products, such as highfluoride toothpaste or gel, fluoride varnish, and chlorhexidine rinse, gel, or varnish, are rarely prescribed by orthodontists. It was reported that 95 of orthodontists provide oral hygiene instructions, while only 52 prescribe fluoride mouth rinse61.  Xylitol, a polyol a type of carbohydrate that does not act as a metabolizing substrate for Streptococcus mutans, can be used as a lowcalorie sugar substitute to prevent caries.62 Xylitol has been used as a caries preventive agent in form of gum and mints. It is noncariogenic and appears to have antimicrobial properties that help to inhibit S mutans attachment to the teeth. The salivary pH remains stable as there is no metabolism by bacteria, and the environment does not favor acidogenic bacteria.63 Additionally, the consumption of chewing gum and mints has been demonstrated to result in increased production of stimulated saliva containing more calcium and phosphate ionic concentrations when compared with nonstimulated saliva.63 The systematic use of xylitol chewing gum can significantly reduce the risk of caries compared with gums that contain sorbitol and sucrose.64 Chewing xylitol gum thrice a day for 5 minutes has shown positive results.65 However, longterm clinical trials with a standardized methodology are needed. Moderate and highrisk adult patients are recommended to chew 2 pieces of xylitol gum for 10minutes at least, 3 to 5 times a day.66 Therapeutically, 6gmday of xylitol is recommended for adults.67 However, xylitol can cause diarrhea if the recommended doses are exceeded.63 Enamel demineralization might be prevented by the application of products containing casein phosphopeptidesamorphous calcium phosphate CPPACP. CPPACP is a nanocluster that  binds calcium and phosphate ions in an amorphous form. CPPACP has been shown to adhere to the bacterial wall of microorganisms and tooth surfaces.68,69 When an intraoral acid attack occurs, the calcium and phosphate ions are released to produce a supersaturated concentration of ions in the saliva, which then precipitates a calciumphosphate compound onto the exposed tooth surface.69 However, there is insufficient clinical trial evidence to make a recommendation regarding its longterm effectiveness.63 The prolonged duration of orthodontic treatment places the patient at an increased caries risk. This risk can be 66International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org  minimized by a continuous fluoride release from the bonding system around the bracket base. The introduction of fluoridereleasing adhesive systems, resin composites, and glass ionomer cements for bracket bonding offered a means of fluoride delivery adjacent to bracketenamel interface independent of patient cooperation. However, the ability of these materials to reduce decalcification clinically remains equivocal.70 Glass ionomer cementsGIC do not provide complete caries protection under loose bands or in areas of missingdissolved cement.71 However the shear bond strength SBS of GIC was not adequate for bonding brackets. In an attempt to increase the bond strength of GICs, resin particles were added to their formulation to create resin modified Glass Ionomer RMGI bonding systems. These adhesives release fluoride like conventional GICs and can also be used successfully to bond orthodontic brackets because of their relatively higher SBS.7283 Additionally, in vivo studies have shown no significant differences in bracket failure rates between the RMGIs and composite adhesives.84 Because of the recent improvements in the fluoridereleasing capabilities and the SBS of RMGI, it has been suggested that these adhesives should be used more widely in bonding orthodontic brackets in the future.85 However a recent study concluded that it is impossible to make recommendations on the use of fluoridecontaining orthodontic adhesives during fixed orthodontic treatment.86 The  authors found sufficient evidence to suggest that GIC is more effective than composite resin in preventing white spot formation, but further research is required to determine the effectiveness of the various fluoridecontaining orthodontic adhesives. A fluoride releasing antibacterial bonding agent has been developed by combining the physical advantages of dental adhesive technology and antibacterial effect. The antibacterial activity of 12methacryloyloxydodecylpyridinium bromide MDPB incorporated in the antibacterial adhesive systems demonstrated inhibition of caries formation, especially along the enamel margins.87 Incorporating MDPB into selfetching primer and adhesive resin has demonstrated in vitro antibacterial activity, bonding ability, cytotoxicity, and pulpal response. It was confirmed that MDPBcontaining primer has got antibacterial effects in vivo when used in animal models.88,89  Other fluoriderelease mechanisms like fluoride releasing elastic ligature and power chains have been tried. Research has shown that fluoridereleasing elastomeric ligatures were effective in reducing plaque accumulation and decalcification around the brackets.16,90,91 However, later investigations reported that fluoridereleasing elastomeric ligatures did not reduce the amount disclosed plaque around the brackets.92 Research has shown that the fluoride release was high in the first week but decreased significantly in the subsequent weeks.93 Finally, use of argon laser to cure composite resins has demonstrated its ability to alter the enamel, rendering it less susceptible to demineralization. It was also shown that combining laser irradiation with fluoride treatment can have a synergistic effect on acid resistance preventing formation of WSL and dental caries.94 Research has shown that exposing the teeth to an argon laser for 60 seconds at the time of appliance placement reduced lesion depth by 91.4 and lesion area by 94.6 when compared with untreated control teeth.95   Clinical Significance The authors recommend the following measures to prevent WSL in orthodontic patients 1. Educate and motivate the patients at every visit to maintain optimal oral hygiene around the appliances to obtain the full effect of fluoride.29  2. Daily brushing with fluoride toothpaste 1500ppm or more twice a day.56 Use of    interdental brushes to remove plaque around the brackets.        3.  Daily use of a fluoride mouth rinse 0.05 NaF.29,31,55                                                                                                                            4.  Performing oral prophylaxis scaling and reinforcing instructions at each appointment in              noncompliant patients.        5. Use of chlorhexidine mouth rinse at night for 2 weeks in patients with poor oral hygiene.          6. Use of topical fluoride in the form of solutions, varnishes, or gels around the brackets of            noncompliant high risk patients at 6 months interval.        7.  Cementing the bands with good quality GIC. CONCLUSION WSL on the enamel surface adjacent to fixed orthodontic appliances is an important and prevalent iatrogenic effect of orthodontic therapy. The components of the appliance and the bonding materials create stagnation areas for plaque accumulation and bacterial colonization. The subsequent acid production by the acidogenic bacteria leads to enamel decalcification. The orthodontist must educate the patient regarding the importance of maintaining good oral hygiene and dietary regime. Fluoride is the most important agent to prevent decalcification and restrict lesions from progressing. Oral hygiene regime must include topical fluoride agents such as fluoridated toothpaste, fluoridecontaining mouth rinse, gel and varnish to prevent or minimize the formation of WSL during orthodontic treatment. REFERENCES 1 gaard B., Bishara S, Duschner H Enamel effects during bondingdebonding and treatment with fixed appliances, in Graber T, Eliades T, Athanasiou A, eds Risk Management in Orthodontics. Experts Guide to Malpractice. Quintessence, 2004, pp 1946.  2 Ogaard B, Rlla G, Arends J. Orthodontic appliances and enamel demineralization. Part 1. Lesion development. Am J Orthod Dentofacial Orthop 1988946873. 3 Zachrisson BU, Zachrisson S Caries incidence and oral hygiene during orthodontic treatment. Scand J Dent Res 197179394401. 4 Zachrisson BU A post treatment evaluation of direct bonding in orthodontics. Am J Orthod Dentofacial Orthop 1977  71173189.  5 Machen D E Legal aspects of orthodontic practice risk management concepts. Am J Orthod Dentofacial Orthop 1991,100 9394. 6 Bishara SE, Ostby AW. White Spot Lesions Formation, Prevention,and Treatment. Semin Orthod 200814174182. 67International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org  7 Fejerskov O, Nyvad B, Kidd EAM. Clinical and histological manifestations of dental caries. In Fejerskov O, Kidd EAM, editors Dental caries the disease and its clinical management. Copenhagen, Denmark Blackwell Munksgaard 2003.pp.7199. 8   Summitt JB, Robbins JW, Schwartz RS Fundamentals of Operative Dentistry A  Contemporary Approach, 3rd ed. Hanover Park, IL, Quintessence Publishing, 2006, Chapter 1,pp 24. 9  Russell AL The differential diagnosis of fluoride and nonfluoride enamel opacities. J Public Health Dent 196121143146. 10 Ogaard B. Prevalence of white spot lesions in 19yearolds a study on untreated and orthodontically treated persons 5 years after treatment. Am J Orthod Dentofacial Orthop 1989964237. 11 Boersma JG, van der Veen MH, Lagerweij MD, Bokhout B, PrahlAndersen B. Caries prevalence measured with QLF after treatment with fixed orthodontic appliances influencing factors. Caries Res 200539417. 12 Ogaard B, Larsson E, Henriksson T, Birkhed D, Bishara SE. Effects of combined application of antimicrobial and fluoride varnishes in orthodontic patients. Am J Orthod Dentofacial Orthop 20011202835. 13 Mitchell L. Decalcification during orthodontic treatment with fixed appliances. Br J Orthod 199219199205. 14 Gorelick L, Geiger AM, Gwinnett AJ. Incidence of white spot formation after bonding and banding. Am J Orthod 198281938. 15 Geiger AM, Gorelick L, Gwinnett AJ, Griswold PG. The effect of a fluoride program on white spot formation during orthodontic treatment. Am J Orthod Dentofacial Orthop 1988932937. 16 Banks PA, Chadwick SM, AsherMcDade C, Wright JL. Fluoride releasing elastomerics a prospective controlled clinical trial. Eur J Orthod 2000224017.  17 Artun J, Brobakken B. Prevalence of carious white spots after orthodontic treatment with multibonded appliances. Eur J Orthod 1986822934. 18 Zachrisson BU. Fluoride application procedures in orthodontic practice, current concepts. Angle Orthod 1975457281. 19 Joshua A. Chapman, W. Eugene Roberts, George J. Eckert, Katherine S. Kula, and Carlos Gonza lezCabezas Risk factors for incidence and severity of white spot lesions during treatment with fixed orthodontic appliances Am J Orthod Dentofacial Orthop 201013818894. 20 Mizrahi E. Surface distribution of enamel opacities following orthodontic treatment. Am J Orthod 19838432331. 21 Marcusson A, Norevall LI, Persson M. White spot reduction when using glass ionomer cement for bonding in orthodontics a longitudinal and comparative study. Eur J Orthod 19971923342. 22 Basdra EK, Huber H, Komposch G. Fluoride release from orthodontic bonding agents alters enamel surface and inhibits enamel demineralization in vitro. Am J Orthod Dentofacial Orthop 199610946672. 23 Mount GJ, Hume WR. Preservation and restoration of tooth structure. 2nd Edition. Queensland, Australia Knowledge Books and Software 20056182. 24 Chatterjee R, Kleinberg I. Effect of orthodontic band placement on the chemical composition human incisor plaque. Arch Oral Biol 1979 2497100. 25 Gwinnett JA, Ceen F. Plaque distribution on bonded brackets a scanning electron microscope study. Am J Orthod 197975667677. 26 Fournier A, Payant L, Bouchin R. Adherence of Streptococcus mutans to orthodontic brackets. Am J Orthod Dentofacial Orthop 1998114414417. 27 Klock B, Krasse B. A comparision between different methods of prediction of caries activity. Scand J Dent Res 197987129139. 28 Hausen H, Sepp L, Fejerskov O Can caries be predicted,in Thylstrup A, Fejerskov O, eds Textbook of Clinical Cariology. Copenhagen, Munksgaard, 1994, pp 393411. 29 gaard B. White Spot Lesions During Orthodontic Treatment Mechanisms and Fluoride Preventive Aspects Semin Orthod 200814183193. 30 Arneberg P, Giertsen E, Emberland H, et al Intraoral variations in total plaque fluoride related to plaque pH. A study in orthodontic patients. Caries Res 31451456, 1997. 31 OReilly MM, Featherstone JD. Demineralization and remineralization around orthodontic appliances an in vitro study. Am J Orthod Dentofacial Orthop 1987 923340. 32 Mitchell L. An investigation into the effect of fluoride releasing adhesive on the prevalence of enamel surface changes associated with directly bonded orthodontic attachments. Br J Orthod 199219207214. 33 Ogaard B Oral microbiological changes, longterm enamel alterations due to decalcification, and caries prophylactic aspects, in Bratley WA, Eliades T eds Orthodontic Materials Scientific and Clinical Aspects. Stutgard, Thieme, 2001, pp 127. 34 Linton JL. Quantitative measurements of remineralization of incipient caries. Am J Orthod Dentofacial Orthop 1996110590597.  35 Tufekci E, Mirrill TE, Pintado MR, et al. Enamel loss associated with orthodontic adhesive removal on teeth with white spot lesions an in vitro study. Am J Orthod Dentofacial Orthop 2004125733740.  36 Ogaard B, ten Bosch J Regression of white spot enamel lesions. A new optical method for quantitative longitudinal evaluation in vivo. Am J Orthod Dentofacial Orthop 1991106238242.  37 James JW, Miller BH, English JD, et al. Effects of high speed curing devices on shear bond strength and microleakage of orthodontic brackets. Am J Orthod Dentofacial Orthop 2003 123555561.  38 Gladwin M, Bagby M Clinical Aspects of Dental Materials Theory, Practice and Cases. Baltimore, MD, Lippincott Williams  Wilkins, 2004. 39 Van Noort R Introduction to Dental Materials. 1st ed. London, UK, Mosby, 1994, pp 5354. 40 Arhun N, Arman A, Cehreli SB, et al. Microleakage beneath metal and ceramic brackets bonded with a conventional and an antibacterial adhesive system. Angle Orthod 20067610281034. 41 Arhun N and Arman A  Effects of Orthodontic Mechanics on Tooth Enamel A Review Semin Orthod 200713281291. 42 Chapman et al Risk factors for incidence and severity of white spot lesions during treatment with fixed orthodontic appliances Am J Orthod Dentofacial Orthop 201013818894. 43 Chalmers JM. Minimal intervention dentistry strategies for the new caries challenge in our older patients. JCDA 20067232531. 44 Papas AS, Joshi A, MacDonald SL, MaravelisSplagounias L, PretaraSpanedda P, Curro FA. Caries prevalence in xerostomic individuals. J Can Dent Assoc 199359171179. 45 Ingervall B. The influence of orthodontic appliances on caries frequency. Odontol Revy 19621317590. 46 Suri L, Huang G, English JD Jr, Owen S, Nah HD, Riolo ML, Shroff B, Southard TE and Turpin DL. Topical fluoride treatment Readers Forum, Ask Us. Am J Orthod Dentofacial Orthop 2009,13556163. 47 gaard B Effects of fluoride on caries development and progression in vivo. J Dent Res 69Spec Issue813819,1990. 48 Margolis HC, Mareno EC, Murphy BJ. Effect of low levels of fluoride in solution on enamel demineralization in vitro. J Dent Res1986652329.  49 Baysan A, Lynch E, Ellwood R, Davies R, Petersson L, Borsboom P. Reversal of primary root caries using dentifrices containing 5000 and 1100 ppm fluoride. Caries Res 200135416. 50 Schirrmeister JF, Gebrande JP, Altenburger MJ, Monting JS, 68International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org  Hellwig E. Effect of dentifrice containing 5000 ppm fluoride on noncavitated fissure carious lesions in vivo after 2 weeks. Am J Dent 2007202126. 51 Mellberg JR, Mallon DE. Acceleration of remineralization in vitro by sodium mono fluoriphosphate and sodium fluoride. J Dent Res 1984 6311301155.  52 gaard B, Gjermo P, Rolla G. Plaqueinhibiting effect in orthodontic patients of a dentifrice containing stannous fluoride. Am J Orthod 198078266272.  53 Boyd RL, Chun YS. Eighteenmonth evaluation of the effects of a 0.4 stannous fluoride gel on gingivitis in orthodontic patients. Am J Orthod Dentofacial Orthop 19941053541. 54 de Moura MS, de Melo Simplicio AH, Cury JA. Invivo effects of fluoridated antiplaque dentifrice and bonding material on enamel demineralization adjacent to orthodontic appliances. Am J Orthod Dentofacial Orthop 2006130357363.  55 Zachrisson BU. Cause and prevention of injuries to teeth and supporting structures during orthodontic treatment. Am J Orthod 197669285300.  56 Derks A, Katsaros C, Frencken JE, Vant Hof MA, KuijpersJagtman AM. Cariesinhibiting effect on preventive measures during orthodontic treatment with fixed appliances a systematic review. Caries Res 20043841320. 57 Marsh PD. Are dental diseases examples of ecological catastrophes Microbiology 200314927994. 58 Emilson C, Linquist B,Wennerholm K. Recolonization of human tooth surfaces by streptococcus mutans after suppression by chlorhexidine treatment. J Dent Res 1987 6615038. 59 VivaldiRodrigues G, Demito CF, Bowman SJ, et al. The effectiveness of a fluoride varnish in preventing the development of white spot lesions. World J Orthod2006 7138144. 60 Buyukyilmaz T, gaard B. Cariespreventive effects of fluoridereleasing materials. Adv Dent Res1995 9377383.  61 Derks A, KuijpersJagtman AM, Frencken JE, Vant Hof MA, Katsaros C. Caries preventive measures used in orthodontic practice an evidencebased decision Am J Orthod Dentofacial Orthop 200713216570. 62 Scheinin A, Makinen KK, Ylitalo K. Turku sugar studies Final report on the effect of sucrose, fructose and xylitol diets on the caries incidence in man. Acta Odonto Scan 197634 4179216. 63 Sandra GuzmanArmstrong,Jane Chalmers, John J. Warren. White spot lesions Prevention and Treatment Readers Forum, Ask Us Am J Orthod Dentofacial Orthop 2010138690696. 64 Makinen KK, Bennett CA, Hujoel PP, Isotupa KP, Pape HR, Makinen KK. Xylitol chewing gums and caries rates a 40month cohort study. J Dent Res 199574190413. 65 Zimmer S, Robke FJ, Roulet JF. Caries prevention with fluoride varnish in a socially deprived community. Community Dent Oral Epidemiol  1999271038. 66 Isokangas P, Alanen P, Tiesko J, Makinen KK. Xylitol chewing gum in caries prevention a field study in children. J Am Dent Assoc 198411731520. 67 Dawes C, Macpherson LM. Effects of nine different chewinggums and lozenges on salivary flow rate and pH. Caries Res 19922617682.   68 Aimutis W. Bioactive properties of milk proteins with particularfocus on anticariogenesis. J Nutr 2004134989S95S. 69 Tung MS, Eichmiller FC. Dental applications of amorphous calcium phosphates. J Clin Dent 19991016. 70 Turner PJ. The clinical evaluation of a fluoride containing orthodontic bonding material. Br J Orthod 1993201307313.  71 RezkLega F, Ogaard B, Arends J. An in vivo study on the merits of two glass ionomers for the cementation of orthodontic bands. Am J Orthod Dentofacial Orthop199199162167. 72 DiazArnold AM, Holmes DC, Wistrom DW, et al. Shortterm fluoride releaseuptake of glass ionomer restoratives. Dent Mater 19951196101.  73 Forsten L. Resinmodified glass ionomer cements fluoride release and uptake. Acta Odontol Scand1995 53222225.  74 Forss H. Release of fluoride and other elements from lightcured glass ionomers in neutral and acidic conditions. J Dent Res 19937212571262.  75 Forsten L. Fluoride release of glass ionomers. J Esthet Dent 1994 6216222.  76 McCourt JW, Cooley RL, Barnwell S. Bond strength of lightcure fluoridereleasing baseliners as orthodontic bracket adhesives. Am J Orthod Dentofacial Orthop 1991100 4752.  77 Komori A, Ishikawa H. Evaluation of a resinreinforced glass ionomer cement for use as an orthodontic bonding agent. Angle Orthod 199767189195.  78 Rix D, Foley TF, Mamandras A. Comparison of bond strength of three adhesives composite resin, hybrid GIC, and glassfilled GIC. Am J Orthod Dentofacial Orthop 20011193642. 79 CoupsSmith KS, Rossouw PE, Titley KC. Glass ionomer cements as luting agents for orthodontic brackets. Angle Orthod 2003 73436444. 80 Bishara SE, VonWald L, Olsen ME, et al. Effect of time on the shear bond strength of glass ionomer and composite orthodontic adhesives. Am J Orthod Dentofacial Orthop1999 116616620. 81 Fricker JP. A 12month clinical evaluation of a lightactivated glass polyalkenoate ionomer cement for the direct bonding of orthodontic brackets. Am J Orthod Dentofacial Orthop 1994105502505. 82 Silverman E, Cohen M, Demke R, et al. A new lightcured glass ionomer cement that bonds brackets to teeth without etching in the presence of saliva. Am J Orthod Dentofacial Orthop 1995108231236.  83 Bishara SE, Ostby AW, Laffoon J, et al. Shear bond strength comparison of two adhesive systems following thermocyling a new selfetch primer and resinmodified glass ionomer. Angle Orthod2007 77337341.  84 Summers A, Kao E, Gilmore J, et al. Comparison of bond strength between a conventional resin adhesive and a resinmodified glass ionomer adhesive an in vitro and in vivo study. Am J Orthod Dentofacial Orthop 2004126200206.  85 Eliades T. Orthodontic materials research and applicationsPart 1. Current status and projected future developments in bonding and adhesives. Am J Orthod Dentofacial Orthop 2006130445451. 86 Rogers et al. Fluoridecontaining orthodontic adhesives and decalcification in patients with fixed appliances A systematic review Am J Orthod Dentofacial Orthop 2010138390.e1390.e8.  87 Han L, Edward C, Okamoto A, et al. A comparative study of fluoridereleasing adhesive resin materials. Dent Mater200221919.  88 Imazato S, Torii M, Tsuchitani Y, et al. Incorporation of bacterial inhibitor into resin composite. J Dent Res 19947314371443.  89 Imazato S, Kinomoto Y, Tarumi H, et al. Antibacterial activity and bonding characteristics of an adhesive resin containing antibacterial monomer MDPB. Dent Mater 200319313319. 90 Whitshire WA. In vitro and in vivo fluoride release from orthodontic elastomeric ligature ties. Am J Orthod Dentofacial Orthop 1999115288292.  91 Mattick CR, Mitchell L, Chadwick SM, Wright J. Fluoride releasing elastomeric modules reduce decalcification a randomized controlled trial. J Orthod 200128217219. 92 Benson PE, Shah AA, Campbell IF. Fluoride elastomers Effects on disclosed plaque J Orthod 2004314146. 93 Joseph VP,Grobler SR, Rossou PE. Fluoride release from orthodontic elastic chain. J Clin Orthod 199327101105. 69International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org  94 Hicks MJ, Flaitz CM, Westerman GH, et al. Enamel caries initiation and progression following low fluence energy argon laser and fluoride treatment. J Clin Pediatr Dent 199520913.  95 Anderson AM, Kao E, Gladwin M, Benli O, and Ngan P The effects of argon laser irradiation on enamel decalcification An in vivo study.Am J Orthod Dentofacial Orthop 20021222519. 70International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                  ISSN 22295518  IJSER  2011 httpwww.ijser.org  A Study of Effects of Disease Caused Death in A Simple Epidemic Model  Dr. Sunil Kumar Singh, Dr. Shekh Aqeel  Abstract This paper is concerned with various effects of disease caused death on the host population in an epidemic model of SIR type. Various effects of disease caused death on the host population are studied in this epidemic model. The basic problem discussed in this paper is to be describing the spread of an infection caused death within a population. It is further assumed that there is no substantial development of immunity and that removed infectious are in effect cured of disease. The rate of natural birth and death is assumed to be balanced.  Key Word  Mathematical modeling, Population size , Birth rate , Death rate  Infection rate.         INTORDUCTION nderson and May 7 studied the effects of disease caused death on the population size in model for a disease which spreads through direct infection within a population whose size is allowed to very in time. Two important new phenomena a was revealed by their study.   A threshold for the population size exists that determines whether the population can sustain an epidemic fatal diseases are found to have a regulating effect on the growth of the population. Many subsequent works have fallowed this line of research 5. Another characteristic of this body of research is that the emphasis is on the interplay between the net intrinsic growth rate r and the rate of disease caused death   it r   then the disease it likely to become endemic. To explain this phenomenon, potential mechanisms   to endemicity other then a large intrinsic growth rate r need to be studied. In recent study, we discovered that a long incubation period incorporated into a SIR model may provide an explanation for concurrence of high pathogenicity and long life span of infectiousness. We took different approach to study of epidemic models by assuming that the population a small intrinsic growth rate r so that disease caused mortality rate   is relatively large. This approach has following advantage. 1. If greatly reduces the technicality in mathematical analysis, One may start with the case r  0 and then consider the case of small positive r. 2. If enables us to isolate those effects on the population that directly   related to the disease e.g. we discovered our essential difference between a model that incorporates an incubation period and one that does not. Even in a simple model that does not contain an   Associate Professor, Rajarshi Rananjay Sinh   Institute of Management  Technology, Amethi,CSJM Nager.U.P. INDIA.  Email ID  drsunilsinghmathsgmail.com , drshekhaqeelgmail.com                  incubation period, this new approach leads  to the discovery of several interesting details not found in literature.   3. By keeping the mathematical technicalities at its minimum, this approach may allow our models more accessible to field  epidemiologist and hence encourage of application of  mathematics in epidemiological studies.  In the present work we demonstrate our approach through a very simple model. We assume that the disease spreads through direct contact among the hosts, the disease has no incubation period as considered in most of previous works 2,4 and that the intrinsic growth rate of host population is zero i.e r  0 so that in absence of disease, the population size remains constant. The mathematical analysis of model is very elementary, and it provides epidemiologically interesting details about an epidemic. Also, we demonstrate that the kind of phenomenon one many observe in the case of a small positive intrinsic growth is essentially the some as we obtained here.  In particular, this seems to suggest that, an SIR model is essentially a model for an epidemic it does not provide an epidemiologically relevant mechanism for disease endimicity. For other studies on epidemological models with varying population size closely related to the one we consider her, see Greenhalgh 1 and Mena Lorca and Hethcote 6 and references there on. Other models with varying population and disease caused death have been studies by Brauer 2, Bremerman and Thieme 3, Gao and Hethcote 4, and Hethcote 6, and Pugliese 7. A71International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                  ISSN 22295518  IJSER  2011 httpwww.ijser.org   FORMULATION OF MATHEMATICAL MODEL   The population is partitioned into classes of susceptible, infectious and immune individuals, with population Nt is             nt   xt  yt z t                1 Let us consider the per capital birth rate is a constant b and all newborns are susceptible. The per capital natural death rate assumed to be b so that total population remains constant in absence of disease. Suppose the disease spreads through direct contact between susceptible and infections individuals. We assume that the   transmission coefficient per unit time by  xtyt. This is equivalent to assuming that the contact rate between individual is  nt, proportional to n t.  The disease is assumed to causes death to infected individuals, with a death rate constant  . Let the average infectious period for an infectious individual be 1 so that transfer from infectious class to immune class is at a constant rate  . It is also assumed that the disease confers permanent immunity so that no transfer from infectious class to immune class exists. Since vaccination is one of the major means of control and prevention of many viral infections, the effect of a vaccination strategy is also considered. All susceptible individuals are vaccinated at a constant per capita rate p. Based on these modeling hypotheses, the following set of differential equations is derived.  x  bn xy bxpx y  xy  b  r   y z  ry  bz  px   2 and    nt      x t  yt  zt The model 2 was one of many model proposed and      studied in 1, in which only a local analysis is carried out. It is also related to a model studied in 5 which does not consider vaccination.  Adding the equations in 2 gives   x  y  z   bn xy  bx  px  xyb  r                              y  ry  bz  px n   bn  xy  bx  px  xy by  ry  y  ry  bz  px n  y             3  which implies that total population always decreases.    Observing that the variable z does not appear in first two equations in 1 we may simply study the following system          x  bn  xy  bx  px          y  xy  b  r   y            4           n  y  and determine the variable z from           zt   nt x t  y  t The feasible region for 4 is  G  x, y, n R        x  y  n    It can be checked that G is positively invariant under the dynamics of 4 which, together with the fact that the right hand side of 4 is analytic in x, y, n, shows that the model is well posed.  EQUILIBRIUM POINT Setting the right hand side of 4 equal to zero, we obtain the set of equilibrium Eo Eo   x, y, n G   y  0, x  bbp n          5 LOCAL STABILITIES  To study the local stability of each equilibrium,                P  bnbp, 0, n  Eo, we linearize the  vector field of 4 at P. The corresponding Jacobian matrix is       bnbp                b J P            0       bnbp  b  r          0         0                                             0           whose characteristic equation is J  I  0                                                b  p          bn bp                                 b           0            bn bp  b  r                0        0                                                   0               0     Or    bp bnbp  b  r     0    whose eigen values are                  1 b p  bp 72International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                  ISSN 22295518  IJSER  2011 httpwww.ijser.org               2  bnbp  b  r    3  0                      6    with corresponding eigen vetors   v1  1, 0, 0  v2  a1, a2, a3  v3  b, 0, bp                            7  where  a1 b bn a2  b p  bnbp  p  r   a2  bn  bp   b  r          8 set     n     b p b  r    b      9  The local stability of  P  x, y, n is determined by the sings of is. These are two cases arises   CASE I   1. P  E0 and n  n , then P always has a 1 dimensional manifold       given by E0.  2. If n  n, then P has a 1 dimensional stable and 1 dimensional     unstable manifold.  3. If n  n , then P has a 2 dimensional stable manifold.  CASE II  If PE0and n n then we observe that subspace y0 is invariant with respect to 6.2.4 and so is the line defined by y0 and n n for each n0. In each of these lines P is the global attractor in the line y0, n  n.   RESULT The parameter      n     b p b  r    b  b  p x  b is the threshold for the total population to sustain an epidemic. Where the total population nt is much below this value so that x  x , any outbreak of infection, no matter its initial momentum y0, will sustain itself  and immediately decreases and continues to decrease monotonically with time until it dies out. On other hand, if  the population above this threshold value, disease will sustain itself for certain time, reach its maximum extent while killing many of the infection and reducing the total population below the threshold n, and start to decline and eventually die out.  If we set, each solution xt , yt, nt to   4        y max  max yt  Then y max is achieved where yt  0, so   x t  b  r        x  where no vaccination is applied, namely P  0 then x  n. This is the loss from the infected class divided by the rate of transmission of disease caused death.  increases. Once again this implies that a faster killing disease has less change to cause a large scale epidemic. If y max increases as transmission coefficient  increases. This implies that diseases with greater transmission rate cause larger scale epidemics. If p increases. y max decreases and it takes less time for yt to achieve its minimum, the faster the susceptible are removed to the immune class, the less virulent the disease becomes. . Also x can used as an indicator as to whether the epidemic has reached its maximum extent. After the number of susceptible has decreased below x , the disease will start to die down and eventually die out. If y max decreases when the rate   REFERENCES  1 D. Greenhalgh, Vaccination in densitydependentepidemic models, Bull. Math. Biol. 54 1992, 733758. 2 F. Brauer, Models for the spread of universally fatal diseases, J. Math. Biol. 28 1990, 451452. 3 H.J. Bremermann and H.R. Thieme, A competitive exclusion  principal for pathogen virulence, J. Math. Biol. 27 1989, 171190. 4 L. Q. Gao and H. W. Hethcote, disease transmission models with densitydependent demograhpics, J. Math. Biol. 30 1992, 717731. 5 M. May and R.M. Anderson, population biology of infectious  diseases II, Nature 280 1979, 455461.                           6 MenaLorca and H.W. Hethcote, Dynamic Models of infectious  disease as regulator of    population sizes, J. Math. Biol. 30 1992  693716. 7 Pugliese, Population models for disease with no recovery, J.Math.Biol. 28 1990, 6582. 8 R.M. Anderson and R.M. May, Population biology of infectious diseases I, Nature 1801979, 361367.  9 S.N. Busenberg and K.P. Hadeler,demography and epidemics, Math.iosciences 101 1990, 4162. 10 S.N. busenberg and P. van den Driessche,Analysis of a disease transmission model in a population with varying size, J. Math.Biol.28 1990, 257270.  73International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518   IJSER  2011 httpwww.ijser.org  Autonomous Room Air Cooler Using Fuzzy Logic Control System M. Abbas, M. Saleem Khan, Fareeha Zafar Abstract This research paper describes the design and implementation of an autonomous room air cooler using fuzzy rule based control system. The rule base receives two crisp input values from temperature and humidity sensors, divides the universe of discourse into regions with each region containing two fuzzy variables, fires the rules, and gives the output singleton values corresponding to each output variable. Three defuzzifiers are used to control the actuators cooler fan, water pump and room exhaust fan. The results obtained from the simulation were found correct according to the design model. This research work will increase the capability of fuzzy logic control systems in process automation with potential benefits. MATLABsimulation is used to achieve the designed goal. Index Terms Fuzzy  Logic Control, Inference Engine, MATLAB simulation and Rule Selection.        1 INTRODUCTION                                                                     ODERN processing systems are heavily dependent on automatic control systems. The control automation has become essential for machines and processes to run successfully for the achievement of consistent operation, better quality, reduced operating costs, and greater safety. The control system design, development and implementation need the specification of plants, machines or processes to be controlled. A control system consists of controller and plant, and requires an actuator to interface the plant and controller. The behaviour and performance of a control system depend on the interaction of all the elements. The dynamical control systems design, modeling and simulation in local and distributed environment need to express the behaviour of quantitative control system of multiinput and multioutput variables control environment to establish the relation between actions and consequences of the control strategies 1. Computational Intelligence CI is a field of intelligent information processing related with different branches of computer sciences and engineering. The fuzzy systems are one paradigm of CI. The contemporary technologies in the area of control and autonomous processing are benefited using fuzzy sets 2. The user based processing capability is an important aspect of fuzzy systems taken into account in any design consideration of human centric computing systems. The human centricity plays a vital role in the areas of intelligent data analysis and system modeling 3. The elements of fuzzy sets belong to varying degrees of membership or belongingness. Fuzzy sets offer an important and unique feature of information granules. A membership function quantifies different degrees of membership. The higher the degree of membership A x, the stronger is the level of belongingness of this element to A. Fuzzy sets provide an ultimate mechanism of communication between humans and computing environment 4. The fuzzy logic and fuzzy set theory deal with nonprobabilistic uncertainties issues. The fuzzy control system is based on the theory of fuzzy sets and fuzzy logic 5. Previously a large number of fuzzy inference systems and defuzzification techniques were reported. These systemstechniques with less computational overhead are useful to obtain crisp output. The crisp output values are based on linguistic rules applied in inference engine and defuzzification techniques 67. The efficient industrial control with new techniques of fuzzy algorithm based on active rule selection mechanism to achieve less sampling time ranging from milliseconds in pressure control, and higher sampling time in case of temperature control of larger installations of industrial furnaces has been proposed 8. The development of an air condition control system based on fuzzy logic with two inputs and one output using temperature and humidity sensors for feedback control, and three control elements for heating, cooling, and humidity, and formulated fuzzy rules for temperature and humidity has been achieved. To control the room temperature, the controller reads the room temperature after every sampling period 9.  This proposed design work of Autonomous Room Cooling System is the application of fuzzy logic control system consisting of two input variables Temperature and Humidity, and three output variables Cooler fan speed, Water pump speed and Exhaust fan speed, used in a processing plant of room cooler to maintain the M  M. Abbas is working as Visiting Faculty in the Department of Electrical Engineering in National University of FAST Lahore and also Lecturer Internee in Physics, Government Islamia College, Lahore, Pakistan email shairabbasyahoo.com  Dr. M. Saleem Khan  is with the GC University Lahore Pakistan, working as Director Computer Science Department  email mskgcuyahoo.com, s.khaned.ac.uk  Dr. Fareeha Zafar  is an Assistant Professor in the  Department of  Computer Science, GC University, Lahore, Pakistan  email dr.f.zafargcu.edu.pk  74International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org  required cooling environment. The basic structure of the proposed model is described in Section 2. Section 3 gives the simplified design algorithm of fuzzy logic for room air cooler system. Section 4 describes the simulation results of this system. Conclusion and future work is given in Section 5. 2 BASIC STRUCTURE OF THE PROPOSED MODEL The basic structure of the proposed model of autonomous water room cooler consists of room air cooler with fuzzy logic control system. The room cooler mounted in a room has cooler fan, a water pump to spread water on its boundary walls of grass roots or wooden shreds. A room exhaust fan, humidity and temperature sensors used to monitor the environment of room are mounted in the room. The sensors with amplification and voltage adjustment unit are connected with the two fuzzifiers of the fuzzy logic control system. Three outputs of defuzzifiers cooler fan speed control, water pump speed control and room exhaust fan speed control are connected through actuators.          Fig.1. Block diagram of Autonomous Room Air Cooler System 3 SIMPLIFIED DESIGN ALGORITHM OF FUZZY LOGIC FOR ROOM AIR COOLER SYSTEM This simplified design algorithm is used to design the fuzzifier, inference engine, rule base and defuzzifier for the autonomous room air cooling system according to the control strategy of the processing plant to achieve the quantity and quality of the desire needs to maintain the room environment. This design work uses five triangular membership functions equally determined over a scale range of 0C to 40C for the temperature input and 0 to 100 relative humidity inputs. The five fuzzy membership functions for temperature input are termed as cold 010C, cool 020C, normal 1030C, warm 2040C, and hot 3040C. As for humidity input, the five fuzzy membership functions are dry 025, not too dry 050, moist 2575, not too wet 50100, and wet range 75100.This fuzzy logic model aims to determine the amplitude of the voltage signal 05v to be sent to the three actuators for Cooler fan speed, water pump speed and room exhaust fan speed to maintain a constant and desired environment. Here no time constrain is applied. Three outputs of this proposed system are Cooler fan speed, water pump speed and room exhaust fan speed. Each output variable consists of five membership functions Stop 05, Low 050, Medium 4060, Fast 5090 and Very Fast 90100. 3.1 Fuzzifier The set points of fuzzifiers use the data of two input variables, Temperature and, Humidity. Their occupied region description, membership functions and range are given in TABLE 1 and TABLE 2.    For each input variable, five membership functions are used as shown in Fig. 2 and in Fig. 3. The five membership functions, Cold, Cool, Normal, Warm, Hot are used to show the various ranges of input fuzzy variable TEMPERATURE in a plot consisting of four regions as shown in Fig. 2.   TABLE 1 MEMBERSHIP FUNCTIONS AND RANGES OF INPUT VARIABLE TEMPERATURE  TABLE 2 MEMBERSHIP FUNCTIONS AND RANGES OF INPUT VARIABLE HUMIDITY  75International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org  Fig.2. Plot of membership functions for input variable,  TEMPERATURE  The five membership functions, Dry, Not Too Dry, Moist, Not Too Wet, Wet are used to show the various ranges of input fuzzy variable HUMIDITY in a plot also consisting of four regions as shown in Fig. 3  Fig.3. Plot of membership functions for input variable,  HUMIDITY  The linguistic values are the mapping values of the fuzzy input variables with the membership functions occupied in the regions 10. As we are using two variables, therefore four linguistic values are shown in Fig.4. The mapping of input fuzzy variables with the functions in four regions is listed in TABLE 3.     TABLE 4 RULE MAPPING FOR REGIONS OCCUPIED Case No. Regions Occupied Rules fnm Membership value, where nNo. of input  variable,  mNo. of membership  funcTemperature Input variable 1  Humidity Input variable 2      1 1 1 R1  f1  f3   R2  f1  f4   R3  f2  f3   R4  f2  f4   f11  f21 f11  f22 f12  f21 f12  f22 2 1 2 R1  f1  f3   R2  f1  f4   R3  f2  f3   R4  f2  f4  f11  f22  f11  f23  f12  f22  f12  f23  3 1 3 R1  f1  f3   R2  f1  f4   R3  f2  f3   R4  f2  f4  f11  f23  f11  f24  f12  f23  f12  f24 4 1 4 R1  f1  f3   R2  f1  f4   R3  f2  f3   R4  f2  f4  f11  f24 f11  f25 f12  f24 f12  f25 5 2 1 R1  f1  f3   R2  f1  f4   R3  f2  f3   R4  f2  f4  f12  f21 f12  f22 f13  f21 f13  f22 6 2 2 R1  f1  f3   R2  f1  f4   R3  f2  f3   R4  f2  f4  f12  f22 f12  f23 f13  f22 f13  f23 7 2 3 R1  f1  f3   R2  f1  f4   R3  f2  f3   R4  f2  f4  f12  f23 f12  f24 f13  f23 f13  f24 8 2 4 R1  f1  f3   R2  f1  f4   R3  f2  f3   R4  f2  f4  f12  f24 f12  f25 f13  f24 f13  f25 9 3 1 R1  f1  f3   R2  f1  f4   R3  f2  f3   R4  f2  f4  f13  f21 f13  f22 f14  f21 f14  f22   10. 3 2 R1  f1  f3   R2  f1  f4   R3  f2  f3   R4  f2  f4  f13  f22 f13  f23 f14  f22 f14  f23   11. 3 3 R1  f1  f3   R2  f1  f4   R3  f2  f3   R4  f2  f4  f13  f23 f13  f24 f14  f23 f14  f24   12. 3 4 R1  f1  f3   R2  f1  f4   R3  f2  f3   R4  f2  f4  f13  f24 f13  f25 f14  f24 f14  f25   13. 4 1 R1  f1  f3   R2  f1  f4   R3  f2  f3   R4  f2  f4  f14  f21 f14  f22 f15  f21 f15  f22 TABLE 3 LINGUISTIC VALUES OF FUZZIFIERS OUTPUTS  IN ALL REGIONS  76International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org  Case No. Regions Occupied Rules fnm Membership value, where nNo. of input  variable,  mNo. of membership  funcTemperature Input variable 1  Humidity Input variable 2   14. 4 2 R1  f1  f3   R2  f1  f4   R3  f2  f3   R4  f2  f4  f14  f22 f14  f23 f15  f22 f15  f23   15. 4 3 R1  f1  f3   R2  f1  f4   R3  f2  f3   R4  f2  f4  f14  f23 f14  f24 f15  f23 f15  f24   16. 4 4 R1  f1  f3   R2  f1  f4   R3  f2  f3   R4  f2  f4  f14  f24 f14  f25 f15  f24 f15  f25  Fuzzifier converts the input crisp value into the linguistic fuzzy values. The output of fuzzifier gives the linguistic values of fuzzy set. For the two input variables, two fuzzifiers are used which are shown in Table 5.  Fig.4 Fuzzifier Block  Each fuzzifier consists of  a multiplier which converts the input voltage range 05volt into the crisp value 040 for temperature by multiplying the input with 10, and the crisp value 0100 for humidity by multiplying the input with 25, comparators used to decide the region occupied by the input variable value, subtractors used to find the difference of crisp value from the end value of each region, multiplexer using the address information from the region selection and inputs from the four subtractors, multiplex the four values because this system is designed for the four predefined regions, divider used to divide the difference value in each selected region by 10 to find the mapping value of membership function with the input variable value of temperature in that region, and to find the mapping value of membership function for input variable value of humidity, divide the difference value in each selected region by 25, a second fuzzy set subtractor used to find the active value of the second fuzzy set by subtracting the first active fuzzy set value from 1. The general internal hardware structural scheme of a fuzzifier for four regions is shown in Fig. 5  Fig. 6 and the results of fuzzification are shown in TABLE 5 for mathematical analysis 11.  Fig. 5 Internal Hardware Structural Scheme for Four Regions TEMPERATURE    Fig. 6 Internal Hardware Structural Scheme for Four Regions HUMIDITY  3.2  Inference Engine The inference engine consists of four AND operators, these are not the logical ANDs but select minimum value input for the output. This inference engine accepts four inputs from fuzzifier and applies the minmax composition to obtain the output R values. The minmax inference TABLE 5 RESULTS OF FUZZIFICATION  77International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org  method uses minAND operation between the four inputs. Fig. 7 shows this type of inference process. Number of active rules  mn, where m  maximum number of overlapped fuzzy sets and n  number of inputs. For this design, m  5 and n  2, so the total number of active rules are 25. The total number of rules is equal to the product of number of functions accompanied by the input variables in their working range 12. The two input variables described here consisted of five membership functions. Thus, 5 x 5  25 rules were required which are shown in TABLE 6.  TABLE 6 TOTAL NUMBER OF RULES INPUTS OUTPUTS Temperature C Humidity  Speed of Cooler Fan Speed of Water Pump Speed of Room Exhaust Fan Cold Dry Stop Low Stop Cold Not Too Dry Stop Low Slow Cold Moist Stop Low Slow Cold Not Too Wet Stop Low Slow Cold Wet Low Stop Medium Cool Dry Stop Medium Stop Cool Not Too Dry Stop Low Slow Cool Moist Stop Low Medium Cool Not Too Wet Low Low Medium Cool Wet Low Stop Medium Normal Dry Low Medium Stop Normal Not Too Dry Low Medium Slow Normal Moist Medium Medium Slow Normal Not Too Wet Medium Low Medium Normal Wet High Stop Fast Warm Dry High High Stop Warm Not Too Dry High High Slow Warm Moist High High Medium Warm Not Too Wet High Medium Slow Warm Wet Very High Low Stop Hot Dry High Very High Stop Hot Not Too Dry Very High Very High Slow Hot Moist Very High High Medium Hot Not Too Wet Very High Medium Medium Hot Wet Very High Low Fast  In this case only 4 rules are required for the particular values of two variables because each value of two  variables in a region corresponds to mapping of two  functions. The corresponding mapping values of f13, f14, f22, f23 were used to establish the 4 rules. Here f1 3 means the corresponding mapping value of membership function Normal of temperature in region3 and the similar definitions are for the others 13.  R1  f1  f3  f13  f22  0.2  0.4  0.2 R2  f1  f4  f13  f23   0.2  0.6  0.2 R3  f2  f3  f14  f22   0.8  0.4  0.4 R4  f2  f4  f14  f23   0.8  0.6  0.6  Fig. 7 Block Diagram of Inference Engine 3.3. Rule Selector The rule selector receives two crisp values of  temperature and humidity. It gives singleton values of output functions under algorithm rules applied on design model.  For two variables, four rules are needed to find the corresponding singleton values S1, S2, S3 and S4 for each variable according to these rules are listed in Table 7.  The rule base accepts two crisp input values,  distributes the universe of discourse into regions with each region containing two fuzzy variables, fires the rules, and gives the output singleton values corresponding to each output variable. Fig. 8 shows the main block diagram of the Rule Base.  TABLE 7 ILLUSTRATION OF RULES APPLIED MODEL  78International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org  Fig. 8 Rule Base 3.4  Deffuzifier In this system, three defuzzifiers control the actuators speed of cooler fan, speed of water pump, speed of room exhaust fan. The membership functions of the three output variables are shown in Fig. 9 to Fig. 11, and the detail of each plot is given in TABLE 8. TABLE 8 OUTPUT VARIABLES MEMBERSHIP FUNCTIONS             The defuzzification process provides the crisp  value outputs after estimating its inputs 14. In this  system 8 inputs are given to each of three defuzzifiers. Four values of R1, R2, R3, R4 from the outputs of inference engine and four values S1, S2, S3, S4 from the rule selector are shown in Fig. 10. Each defuzzifier estimates the crisp value output according to the center of average C.O.A method using the mathematical expression, i  Ri  Ri , where i  1 to 4. Each output variable membership function plot consists of five functions with the same range values for simplification. Fig.12 shows the design arrangement of a defuzzifier. One defuzzifier consists of  one adder for Ri , four multipliers for the product of Si  Ri , one adder for i  Ri , and one divider for i  Ri  Ri. Finally a defuzzifier gives the estimated crisp value output.             Fig.12 Defuzzifier Block   4. RESULTS AND DISCUSSION The designed values for three outputs cooler fan speed, water pump speed and room exhaust fan speed are given in the Tables 911.  According to the results of inference engine  Ri  R1R2R3R4  0.20.20.40.6  1.4  Fig. 9 Plot of Membership Functions for Output  Variable, Cooler Fan Speed Fig.10. Plot of Membership Functions for Output Variable, Water Pump Speed Fig.11.  Plot of Membership Functions for Output Variable, 79International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org  TABLE 9 DESIGNED VALUE FOR COOLER FAN SPEED           i  Ri  0.85 and iRi  Ri 0.851.40.6071   60.71 of Cooler Fan Speed  TABLE 10 DESIGNED VALUE FOR WATER PUMP SPEED         i  Ri  0.90 and i  Ri  Ri  0.901.4  0.6428        64.28  of Water Pump  Speed  TABLE 11 DESIGNED VALUE FOR EXHAUST FAN SPEED          i  Ri  0.50 and i  Ri  Ri  0.501.4 0.3571  35.71  of Exhaust Fan Speed. Using mathematical expression i  Ri  Ri the crisp values for output variables were determined and the results were found according to the MATLAB simulation as shown in Fig.13. These results are compared in TABLE 13 and found correct according to the design model. MATLAB simulation was adapted according to the arrangement of membership functions for four rules as given in TABLE 12.      TABLE 12 ARRANGEMENT OF MEMBERSHIP FUNCTIONS FOR SIMULATION  In Fig. 13 the same values of input variables, Temperature  28, and Humidity  40 are shown. Various values of input and output variables match the dependency scheme of the system design. The simulated values were checked using MATLABRule viewer as shown in Fig. 13.  Fig.13.  MATLABRule Viewer  The correctness of results shows the validity of the simplified design work for processing system using fuzzy control system. TABLE 13 COMPARISON OF SIMULATED AND CALCULATED RESULT 4.1. Simulation Graphs Discussion This system was simulated for the given range of input variables. The given value of Temperature  28 lies in region 3 of the range 2030 and Humidity  40 lies in region 2 of the range 2550. The four rules were applied for MATLAB simulation according to this range scheme. In this design model, the speed of cooler fan depends upon the selected value of temperature sensor, water pump and exhaust fan speeds depend on the value of humidity. The simulated and calculated results are according to the reliance scheme. 80International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org  Fig. 14 shows that the cooler fan speed is directly  proportional to temperature and it does not depend upon the humidity. Fig. 15 represents that the water pump speed is inversely proportional to humidity and it does not depend upon the temperature. Fig. 16 shows that the exhaust fan speed is directly proportional to humidity. 5.  CONCLUSION AND FUTURE WORK Design model of autonomous room air cooler fuzzy logic processing control system provided the results  effectively in agreement with the simulation results  during the testing of various parts of the control system. The algorithmic design approach makes the system  efficient and absolutely under control. This work builds up the control management without the complexity in a processing plant of room cooler to sustain the required cooling environment. The utility of the proposed system in such processing plants is being carried out and in  future it will help to design the advanced control system for the various industrial applications in environment monitoring and management systems using state of the art FPGAs based Microelectronics Chips.  Fig.14 Plot between TemperatureHumidity  Cooler Fan Speed  Fig.15. Plot between TemperatureHumidity  Water Pump Speed    Fig.16. Plot between TemperatureHumidity  Room Exhaust Fan Speed REFERENCES 1 E. FriasMartinez, G. Magoulas, S. Chen and R. Macredie, Modeling human behavior in useradaptive systems Recent advances using soft computing techniques  Expert Syst. Appl.292 2005 320329 2 M. Perkowitz and O. Etzioni, Adaptive web sites  Commun ACM 43 8 2000 152158 3 M. Spott and D. Nauck, Towards the automation of  intelligentdata analysis Appl. Soft Comput. 62006 348356 4 S. Gottwald, Mathematical fuzzy logic as a tool for the treatment of vague information Inf. Sci. 17212 2005 4171 5 H. J. Zimmermann, Fuzzy Set and Its Applications, 3rd ed. Kluwer Academic Publishers, Norwell, MA. 1996 6 Y.Y. Chen and T.C. Tsao, A Description of the Dynamic Behaviour of Fuzzy Systems, IEEE Trans. 19 1989745755 7 M. Sugeno Tanaka, Successive Identification of a Fuzzy Modeand its Application to Prediction of a Complex  System, Fuzzy Sets Syst. 42 1991315334 8 M.Y. Hassan and F. Waleed Sharif, Design of FPGA  Based PIDlike Fuzzy Controller for Industrial  Applications, IAENG, IJCS. 342 2005 9 Shakowat Zaman Sarkar, A proposed Airconditioning systemusing Fuzzy Algorithm for Industrial  Application ICSE IEEE Proc. 2006 832834 10 Shabiul Islam, Shakowat, Development of a Fuzzy Logic Controller Algorithm for Airconditioning System, ICSE2006Proc2006 IEEE 11 M. Saleem Khan and Khaled Benkrid, MultiDimensional Supervisory Fuzzy Logic Control DEV, Processing System forIndustrial Applications Lecture Notes in Engineering and Computer Science vol. 2175, p.p. 12081217, Directory of Open Access Journals DOAJ       12 M. Berthold, D. Hand, Intelligent Data Analysis, 2nd ed., SpringerVerlag, Heidelberg 2006 13 M. Saleem Khan and Khaled Benkrid, A proposed Grinding and Mixing System using Fuzzy Time Control Discrete Event Model for Industrial Application,  Lecture Notes in Engineering and Computer Science vol. 2175 2009, p.p. 12311236, Directory of Open Access Journals DOAJ  14 M. Saleem Khan and Khaled Benkrid, Design of Liquids Mixing Control System using Fuzzy Time Control  Discrete Event Model for Industrial Applications, World Academy of Science, Engineering and Technology vol. 72 2010, p.p. 545553, Directory of Open Access Journals DOAJ  81International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518   IJSER  2011 httpwww.ijser.org  Handwritten Character Recognition using Neural Network Chirag I Patel, Ripal Patel, Palak Patel Abstract Objective is this paper is recognize the characters in a given scanned documents and study the effects of changing the Models of ANN.Today Neural Networks are mostly used for Pattern Recognition task. The paper describes the behaviors of different Models of Neural Network used in OCR. OCR is widespread use of Neural Network. We have considered parameters like number of Hidden Layer, size of Hidden Layer and epochs. We have used Multilayer Feed Forward network with Back propagation. In Preprocessing we have applied some basic algorithms for segmentation of characters, normalizing of characters and Deskewing. We have used different Models of Neural Network and applied the test set on each to find the accuracy of the respective Neural Network.  Index Terms Optical Character Recognition,  Artificial Nueral Network, Backpropogation Network, Skew Detection.          1 INTRODUCTION                                                                     uch softwares are useful when we want to convert our Hard copies into soft copies. Such softwares reduces almost 80 of the conversion work while still some verification is always required.      Optical character recognition, usually abbreviated to OCR, involves computer software designed to translate images of typewritten text usually captured by a scanner into machineeditable text, or to translate pictures of characters into a standard encoding scheme representing them in ASCII or Unicode. OCR began as a field of research in artificial intelligence and machine vision. Though academic research in the field continues, the focus on OCR has shifted to implementation of proven techniques 4. 2 ARTIFICIAL NUERAL NETWORK Pattern recognition is extremely difficult to automate. Animals recognize various objects and make sense out of large amount of visual information, apparently requiring very little effort. Simulating the task performed by animals to recognize to the extent allowed by physical limitations will be enormously profitable for the system. This necessitates study and simulation of Artificial Neural Network. In Neural Network, each node perform some simple computation and each connection conveys a signal from one node to another labeled by a number called the connection strength or weight indicating the extent to  Fig. 1 A simple Neuron  which signal is amplified or diminished by the connection.   Different choices for weight results in different functions are being evaluated by the network. If in a given network whose weight are initial random and given that we know the task to be accomplished by the network , a learning algorithm must be used to determine the values of the   weight that will achieve the desired task. Learning Algorithm qualifies the computing system to be called Artificial Neural Network. The node function was predetermined to apply specific function on inputs imposing a fundamental limitation on the capabilities of the network.      Typical pattern recognition systems are designed using two pass. The first pass is a feature extractor that finds features within the data which are specific to the task being solved e.g. finding bars of pixels within an image for character recognition. The second pass is the classifier, which is more general purpose and can be trained using a neural network and sample data sets. Clearly, the feature extractor typically requires the most design effort, since it usually must be handcrafted based on what the applicaS  Chirag I Patel has been completed his M.Tech in Computer Science engineering in Nirma Institute of Technology, Ahmedabad, India, PH919979541227. Email chirag453gmail.com  Ripal Patel has been completed her M.E in Electronics  C ommunication engineering in Dharmsinh Desai Institute of Technology, Nadiad, India, PH919998389428.Email ripalpatel315gmail.com  Palak Patel is persuing her M.E in Electronics  C ommunication engineering in G.H.Patel College of Enginnering   Technology, Vallabh Vidyanagar, India, PH919998389428.Email ipalakecyahoo.com   82International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org  tion is trying to achieve.  One of the main contributions of neural networks to pattern recognition has been to provide an alternative to this design properly designed multilayer networks can learn complex mappings in highdimensional spaces without requiring complicated handcrafted feature extractors. Thus, rather than building complex feature detection algorithms, this paper focuses on implementing a standard backpropagation neural network. It also encapsulates the Preprocessing that is required for effective.  2.1 Backpropogation Backpropagation was created by generalizing the WidrowHoff learning rule to multiplelayer networks and nonlinear differentiable transfer functions. Input vectors and the corresponding target vectors are used to train a network until it can approximate a function, associate input vectors with specific output vectors, or classify input vectors in an appropriate way as defined by you. Networks with biases, a sigmoid layer, and a linear output layer are capable of approximating any function with a finite number of discontinuities. 3 ANALYSIS By analyzing the OCR we have found some parameter which affects the accuracy of OCR system 15. The parameters listed in these papers are skewing, slanting, thickening, cursive handwriting, joint characters. If all these parameters are taken care in the preprocessing phase then overall accuracy of the Neural Network would increase. 4 DESIGN AND IMPLEMENTATION  Initially we are making the Algorithm of Character Extraction. We are using MATLAB as tool for implementing the algorithm. Then we design neural network, we need to have a Neural Network that would give the optimum results 2. There is no specific way of finding the correct model of Neural Network. It could only be found by trial and error method. Take different models of Neural Network, train it and note the output accuracy. There are basically two main phases in our Paper Preprocessing and Character Recognition .  In first phase we have are preprocessing the given scanned document for separating   the Characters from it and normalizing each characters. Initially we specify an input image file, which is opened for reading and preprocessing. The image would be in RGB format usually so we convert it into binary format. To do this, it converts the input image to grayscale format if it is not already an intensity image, and then uses threshold to convert this grayscale image to binary i.e all the pixels above certain threshold as 1 and below it as 0.  Firstly we needed a method to extract a given character from the document. For this purpose we modified the graphics 8way connected algorithm which we call as EdgeDetection.   5 PREPROCESSING 5.1 Character Extraction Algorithm  1. Create a TraverseList  List of pixels which have been already traversed. This list is initially empty. 2. Scan row PixelbyPixel. 3. Whenever we get a black pixel check whether the pixel is already in the traverse list, if it is simply ignore and move on else apply Edgedetection Algorithm. 4. Add the List of Pixels returned by Edgedetection Algorithm to TraverseList. 5. Continue the steps 2  5  for all rows  5.2 Edge Detection Algorithm The Edge Detection Algorithm has a list called traverse list. It is the list of pixel already traversed by the algorithm.  EdgeDetectionx,y,TraverseList 1 Add the current pixel to TraverseList. The current position of pixel is x,y. 2 NewTraverseList TraverseList  current position x,y.  If  pixel at x1,y1 then Check if it is not in TraverseList. Edgedetectionx1,y1,NewTraverseList endif  If  pixel at x1,y then Check if it is not in TraverseList. Edgedetectionx1,y,NewTraverseList endif  If  pixel at x1,y then Check if it is not in TraverseList. Edgedetectionx1,y1,NewTraverseList endif  If  pixel at x,y1 then Check if it is not in TraverseList. Edgedetectionx,y1,NewTraverseList Endif  If  pixel at x,y1 then Check if it is not in TraverseList. Edgedetectionx,y1,NewTraverseList endif 83International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org   If  pixel at x1,y1 then Check if it is not in TraverseList. Edgedetectionx1,y1,NewTraverseList endif  If  pixel at x1,y then Check if it is not in TraverseList. Edgedetectionx1,y,NewTraverseList endif  If  pixel at x1,y1 then Check if it is not in TraverseList. Edgedetectionx1,y1,NewTraverseList endif  3 return  The EdgeDetection algorithm terminates when it has covered all the pixels of the character as every pixels position would be in TraverseList so any further call to EdgeDetection is prevented.               Fig 4a shows the traversing each scan lines.               4b shows the respective calls made to the all 8neighbouring pixels. The Edge detection is called when we hit a pixel i.e. encounter a pixel with value 1. As per the algorithm the current position is entered in TraverseList and recursive calls are made to all 8  neighboring pixels. Before the calls are made it is ensured that the corresponding neighboring pixels is having value 1 and is not already encountered before i.e. it should not be in the TraverseList. 5.3 Normalizing Now as we have extracted the character we need to normalize the size of the characters. There are large variations in the sizes of each Character hence we need a method to normalize the size. We have found a simple method to implement the normalizing. To understand this method considers an example that we have extracted a character of size 7 X 8. We want to convert it to size of 10 X 10. So we make a matrix of 70 X 80 by duplicating rows and columns. Now we divide this 70 X 80 into sub Matrix of 7 X 8. We extract each sub matrix and calculate the no. of ones in that sub matrix. If the no. of ones is greater than half the size of sub matrix we assign 1 to corresponding position in normalized matrix. Hence the output would be a 10 X 10 matrix.                                        Fig 5a shows original representation of the character.           Fig 5b shows the Normalized Character representation after Normalizing. The Fig 5a is shows a representation of character of 12 X 12 size. Using the above algorithm it is converted into a character of 8 X 8 as shown in the Fig 5b.  5.4 Skew Detection The Characters are often found to be skewed. This would impose problems on the efficient character recognition 3.  So to correct the effect of this skewedness we need counter rotate the image by an angle .  We use a very simple but effective technique for Skew Correction. We use Line Fitting i.e. Linear Regression to find the angle . Consider the Skewed character as a graph i.e. all the pixels that have value 1 are considered to be data points. Then we perform linear regression using the equation Y  MX C. Using the formulas for regres84International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org  sion we calculate M nxiyi  xiyi  nxi2xi2. This angle is equivalent to the skewed angle so by rotating the image by opposite of this angle will remove the skew ness. This is a very crude way of removing skew ness there are other highly efficient ways of removing skew ness. But for Characters that have very low Skew angles this gets the thing done.   The Characters are often found to be skewed. This would impose problems on the efficient character recognition.  So to correct the effect of this skewed ness we need counter rotate the image by an angle .   .We use a very simple but effective technique for Skew Correction. We use Line Fitting i.e. Linear Regression to find the angle . Consider the Skewed character as a graph i.e. all the pixels that have value 1 are considered to be data points. Then we perform linear regression using the equation Y  MX C. Using the formulas for regression we calculate M nxiyi  xiyi  nxi2xi2. This angle is equivalent to the skewed angle so by rotating the image by opposite of this angle will remove the skew ness. This is a very crude way of removing skew ness there are other highly efficient ways of removing skew ness. But for Characters that have very low Skew angles this gets the thing done.  Fig 6a Skewed Image        Fig 6b Corrected Image. 6 NUERAL NETWORK DESIGN For training and simulating purposes we have scanned certain documents. We have 2 types of documents train documents and test documents. The train documents are the images of the documents which we want to use for training. Similarly test documents are the images of documents which we want to use for test.  According to the characters in the documents we train the neural network and apply the test documents. We have different Models of Neural Network. Hence we record certain parameters like training time, accuracy etc. to find the effectiveness of the Neural Network. We have selected an image size of 10 X 10 as an input to the Neural Network. Hence we have taken a neural network that has 100 inputs. We are performing the test on only Capital characters so the outputs of the Neural Networks are 26. The no. of nodes of input layer are 100 and the no. of node of output layer are 26. The no. of hidden layer and the size of hidden layer vary.         Fig 7. The general Model of ANN used.            Fig 8. Ttraining automated character extraction Fig 9 recognition Fig 10 Training  User defined Character Extraction. 7 TEST AND RESULTS ANALYSIS 7.1 Test This section shows some implementation results. The 85International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org  training variables involved in the tests were the number of cycles, the size of the hidden layer, and the number of hidden layer. The dataset consisted of AZ typed characters of different size and type. Thus the input layer consisted of 100 neurons, and the output layer 26 neurons one for each character. Ideally, wed like our training and testing data to consist of thousands of samples, but this not feasible since this data was created from scratch.   Table 1. Model 1    Epochs   Number of Hidden Layer              Configuration No. of nodes in HL            Accuracy 300 1 26 20 600 1 26 65 1000 1 26 82 300 1 52 25 600 1 52 69 1000 1 52 88 300 1 78 27 600 1 78 71 1000 1 78 91                                   Table 2. Model 2       Table 3. Model 3  Table 4. Model 4     Epochs   Number of Hidden Layer             ConfigurationNo. of nodes in HL  Accuracy 300 3 265226 31 600 3 265226 65 1000 3 265226 82 300 3 265278 29 600 3 265278 74 1000 3 265278 92 300 3 782678 27 600 3 782678 71 1000 3 782678 91              Table 5. Model 5    Epochs   Number of Hidden Layer             Configuration No. of nodes in HL            Accuracy 300 2 2652 23 600 2 2652 67 1000 2 2652 81 300 2 5278 40 600 2 5278 78 1000 2 5278 96 300 2 2678 27 600 2 2678 77 1000 2 2678 89    Epochs   Number of Hidden Layer       Configuration No. of nodes in HL     Accuracy  300 3 265226 31 600 3 265226 65 1000 3 265226 82 300 3 265278 29 600 3 265278 74 1000 3 265278 92 300 3 782678 27 600 3 782678 71 1000 3 782678 91 86International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org     Epochs   Number of Hidden Layer             ConfigurationNo. of nodes in HL   Accuracy 300 4 265278104 35 600 4 265278104 79 1000 4 265278104 96 300 4 26527826 30 600 4 26527826 61 1000 4 26527826 86 300 4 782652104 43 600 4 782652104 82 1000 4 782652104 98 300 4 78267852 31 600 4 78267852 88 1000 4 78267852 94  Table 6. Model 6    Epochs   Number of Hidden Layer             ConfigurationNo. of nodes in HL   Accuracy 300 4 265278104 35 600 4 265278104 79 1000 4 265278104 96 300 4 26527826 30 600 4 26527826 61 1000 4 26527826 86 300 4 782652104 43 600 4 782652104 82 1000 4 782652104 98 300 4 78267852 31 600 4 78267852 88 1000 4 78267852 94  We have used sigmoid transfer function in all the layers. We have used same dataset for training all the different Models while testing character set was changed.  7.2 Result Analysis From the results, the following observations are made  A small number of nodes in the hidden layer eg. 26 lower the accuracy. A large number of neurons in the hidden layer help in increasing the accuracy however there is probably some upper limit to this which is dependent on the data being used. Additionally, high neuron counts in the hidden layers increase training time significantly. As number of hidden layer increases the accuracy increases initially and then saturates at certain rate probably due to the data used in training. Mostly Accuracy is increased by increasing the number of cycles. Accuracy could also be increased by increasing the training set.  7.3 Additional Formatting and Style Resources Additional information on formatting and style issues can be obtained in the IJSER Style Guide, which is posted online at httpwww.ijser.org. Click on the appropriate topic under the Special Sections link. 8 CONCLUSION The backpropagation neural network discussed and implemented in this paper can also be used for almost any general image recognition applications such as face detection and fingerprint detection. The implementation of the fully connected backpropagation network gave reasonable results toward recognizing characters.       The most notable is the fact that it cannot handle major variations in translation, rotation, or scale. While a few preprocessing steps can be implemented in order to account for these variances, as we did. In general they are difficult to solve completely. REFERENCES  1 S. Basavaraj Patil, N. V. Subbareddy Neural network based system for script identification in Indian documents in Sadhana Vol. 27, Part 1, February 2002, pp. 8397. 2 T. V. Ashwin, P. S. Sastry A font and sizeindependent OCR system for printed Kannada documents using support vector machines in Sadhana Vol. 27, Part 1, February 2002, pp. 3558. 3 Kavallieratou, E.   Fakotakis, N.   Kokkinakis, G., New algorithms for skewing correction and slant removal on wordlevel OCR in Proceedings of ICECS 99. 4 Simmon Tanner, Deciding whether Optical Character Recognition is Feasible. 5 Matthew Ziegler, Handwritten Numeral Recognition via Neural Networks with Novel Preprocessing Schemes. 87International Journal Of Scientific And Engineering Research Volume 2, Issue 5, May2011                                                                                                ISSN 22295518 IJSER  2011 httpwww.ijser.org  A New Approach for Data Encryption Using Genetic Algorithms and Brain Mu Waves Gove Nitinkumar Rajendra, Bedi Rajneesh kaur   Abstract In todays computer world security, integrity, confidentiality of the organizations data is the most important issue. This paper deals with the confidentiality of the data that organization manages and works with. This paper proposes a new approach to data security using the concept of genetic algorithm and brain mu waves with pseudorandom binary sequence to encrypt and decrypt the data. The feature of such an approach includes high data security and high feasibility for practical implementation. Index TermsMu waves, Genetic algorithms, Pseudorandom binary sequence, Encryption, Crossover operator, Data security, Confidentiality.         INTRODUCTION ecently, due to the big data losses from illegal data access, data security has become an important issue for public, private and defense organizations. In order to protect this valuable data or information from unauthorized readers and illegal modifications and reproductions various types of cryptographic techniques are used. There are two basic types of cryptographic techniques 1,2 symmetric and asymmetric cryptography. In symmetric cryptography, same key is used for encryption and decryption. While in asymmetric cryptography, two different keys are used, one for encryption called public key and another for decryption called private key. Symmetric key algorithms are typically fast and are suitable for processing large stream of data. Some of the popular and efficient symmetric algorithms include Twofish, Serpent, AES, Blowfish and IDEA etc. There are other encryption algorithms which are proposed. Genetic algorithms 4 are among such techniques. Generally, genetic algorithms contain three basic operators reproduction, crossover and mutation 5.  Reproductions and crossover together gives the genetic algorithms most of their power. This paper proposes a new approach for encrypting large volume of organization data and highly secret personal data or information. First, an 8 character long string is interpreted from the mu waves generated by the brain. Second, a pseudo random binary sequence is generated from the string obtained after processing above string. Third, the first character string and the pseudorandom sequence is applied to crossover operator which will output two keys which then are concatenated to get a final 512 bit key. This key is then used to encrypt and decrypt data. The rest of the paper is organized as follows. In section 2, the proposed method is introduced. Section 3 gives the analysis of proposed method. Section 4 concludes the paper. 1. THE PROPOSED METHOD The proposed method block diagram is shown in fig 1. It consists of key generation logic, encryption and decryption modules, which are explained in following subsections.   Fig 1.The Block Diagram of Proposed Method  R  Gove Nitinkumar Rajendra   is currently pursuing bachelors degree program in computer  engineering in Pune University, India, Email gove.nitinkumargmail.com  Rajneesh Kaur Bedi is Head of the Department of computer engineering in MITCOE, Pune University, India. Email meenubedihotmail.com 88International Journal Of Scientific And Engineering Research Volume 2, Issue 5, May2011                                                                                                ISSN 22295518 IJSER  2011 httpwww.ijser.org   1.1 The Key Generation Logic  Fig 2.Block Diagram for Key Generation Process Fig 2 shows the model of key generation logic. It consists of sensory input detection unit which is responsible for detecting the mu waves of the pass though of user pass thought is the thinking which is used as a key in latter processing. and interpreting appropriate characters that the user is thinking about to press. This involves signal acquisition, feature extraction, and finally translation. The other modules involved in this are character swapper, pseudorandom binary sequence generator and crossover operator. The pseudorandom binary sequence generator is explained in following subsections. 2.1.1 Character Swapper There are mainly two function performed by this unit. First is, separating the characters according their position i.e., odd or even. Then each two consecutive oddeven positioned characters are swapped with their next character. 2.1.2 Pseudorandom Binary Sequence Generator  Fig 3 shows a general model of PRBSG 3. It is a non linear forward feedback shift register with a feedback function f and non linear function g.  Fig 3.A General Model Of 4 bit NLFFSR When register is loaded with a nonzero value, a pseudo random sequence with very good randomness and statistical properties is generated. The only signal required for the operation of this module clock pulse. The balance, run and correlation properties of the sequence generated make it more useful for generating the private key. 2.1.3 Crossover Operator Crossover in simple words is a process in which two strings are mixed such that they match their desirable qualities in a random fashion. Crossover operator proceeds in three steps as given below 1. Two new strings are selected. 2. A random location from strings is selected.  3. The portions of strings on right side are swapped together. 89International Journal Of Scientific And Engineering Research Volume 2, Issue 5, May2011                                                                                                ISSN 22295518 IJSER  2011 httpwww.ijser.org   Fig 4. Illustration of crossover operator 2.1.4 Key Generation Process The key generation algorithm used here produces a very strong key which is very difficult to guess even with exhaustive search. The process of key generation is as given below 1. Scan the pass thought. Take the string generated after sensing, filtering and processing Mu waves. This is key1. 2. Pass this string to the character swapper. 3. Pass the non zero output of character swapper to the pseudo random binary sequence generator. 4. The output of PRBSG is key2. 5. Both key1 and key2 are 256 bit long. 6. Apply both these keys to crossover operator. 7. Finally, concatenate the two strings generated at after crossover operation. This whole process is depicted in fig 2. 2.2 The Encryption Process The encryption process emulates the operation of key generator and crossover operator. The encryption process comprises of following steps 1. Generate the key using the key generator logic as Kn. 2. Take mode 8 of the key generated to get decimal value ranging from 0 to 7. 3. Kn mod Kn, 8 4. Initialize i0 5. Take two consecutive bytes of the data file as A1 and A2 6. Crossover the two consecutive bytes of the data file as B1 and B 2 Using the number Ki. 7. Encrypt data as C1 and C2 .This is done as follows XiKi XOR Ki  4 Xi1ki1 XOR ki1 4 C1Bi XOR X1 C2B2 XOR Xi1 And ii2 Repeat steps 4 to 6 until end of the file. 2.3 The Decryption Process The steps for encryption are just reversal of the encryption. First extract the key1 from sensory output, then obtain key2 through character swapper, generate PRBS and then the key, apply the process using crossover operator decrypt the data. 3. PERFORMANCE ANALYSIS It should be checked that, if a data is encrypted by the proposed technique whether it can be easily decrypted or not. Since there are M combinations to encrypt 2 consecutive data bytes, thus the number of possible encryption results is M N2, where N is the total number of bytes in data to be encrypted and M is the length of one data byte. The speed of the algorithm is good. But, the initial key generation process takes some time which may decrease the throughput of the algorithm and may increase the execution time by some seconds. 4. CONCLUSION This paper proposes a new approach for data security. It uses the concept of brain Mu waves, genetic algorithms and pseudorandom binary sequence. This methodology of scurrying the confidential data is highly safe and reliable. So, without the secret thought of the person i.e., the key no one will be able to extract the data. Since, the 90International Journal Of Scientific And Engineering Research Volume 2, Issue 5, May2011                                                                                                ISSN 22295518 IJSER  2011 httpwww.ijser.org  pass thought is unpredictable is unpredictable it is very difficult to decrypt correctly without knowing the initial pass thought. The proposed method is very sensitive to the changes in pass thought. In the future work, we plan to implement a system implementing this methodology and provide security to highly confidential and secret data in defense and other institutions. ACKNOWLEDGMENT We wish to thank the Dept. of Computer Engineering, MIT COE for their important support. REFERENCES 1   Douglas, R. Stinson, Cryptography  Theory and Practice, CRC Press, 1995. 6 Goldberg D.E., Genetic algorithms in search optimization  Machine learning, Addison Wesley, 1989. 2   Menzes A. J., Paul, C., Van Dorschot, V., anstone, S.     A., Handbook of Applied Cryptography, CRS pess 5th Printing 2001. 3    Ahmad A., AlMusharafi M. J., AlBusaidi S., AlNaamany A., and Jervase J. A., An NLFSR Based Sequence Generation for Stream Ciphers, Proceedings of International Conference on Sequences and their Applications, pp. 1112, 2001. 4   Tragha A., Omary F., and A. Kriouile, Genetic Algorithms Inspired Cryptography A.M.S.E Association for the Advancement of Modeling  Simulation Techniques in Enterprises, Series D Computer Science and Statistics, 2005. 5 Tragha A., Omary F., Mouloudi A.,ICIGA Improved Cryptography Inspired by Genetic Algorithms, Proceedings of the International Conference on Hybrid Information Technology ICHIT06, pp. 335341, 2006. 91International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                                       ISSN 22295518  JSER  2011 httpwww.ijser.org  Use of Natural Compounds from Plant Sources as AchE Inhibitors for the Treatment of Early Stage  Alzheimers diseaseAn Insilico Approach  Amrendar Kumar, Abhilasha Singh, Biplab Bhattacharjee  Abstract Traditionally, drugs were discovered by testing compounds manufactured in time consuming multistep processes against a battery of in vivo biological screens. Promising compounds were then further studied in development, where their pharmacokinetic properties, metabolism and potential toxicity were investigated. Here we present a study on herbal lead compounds and their potential binding affinity to the effectors molecules of major disease like Alzheimers disease. Clinical studies demonstrate a positive correlation between the extent of Acetyl cholinesterase enzyme and Alzheimers disease. Therefore, identification of effective, welltolerated acetyl cholinesterase represents a rational chemo preventive strategy. This study has investigated the effects of naturally occurring nonprotein compounds polygala and Jatrorrhizine that inhibits acetylcholinesterase enzyme. The results reveal that these compounds use less energy to bind to acetylcholinesterase enzyme and inhibit its activity. Their high ligand binding affinity to acetylcholinesterase enzyme introduce the prospect for their use in chemopreventive applications in addition they are freely available natural compounds that can be safely used to prevent Alzheimers Disease.   Index Terms Alzheiemrs Disease, Acetylcholinesterase, Binding Affinity, Jatrorrhizine, Clinical Studies, Docking, Rational, Toxicity        1 INTRODUCTION lzheimers disease, most common form of dementia is incurable, degenerative, and terminal disease mostly diagnosed in people over 65 years of age. The disease advances with symptoms include confusion, irritability and aggression, mood swings, language breakdown, longterm memory loss, and the general withdrawal of the sufferer as their senses decline. Gradually, bodily functions are lost, ultimately leading to death. In advanced stages of the disease, all memory and mental functioning may be lost. The condition predominantly affects the cerebral cortex and hippocampus, which lose mass and shrink atrophy as the disease advances. These changes, occurring in the association area of the cerebral cortex, the hippocampus and the middle and temporal lobes, are accompanied by decreased concentrations of the neurotransmitter acetylcholine.Acetylcholinesterase is also known as AChE. An acetyl cholinesterase inhibitor or anticholinesterase is a chemical that inhibits the cholinesterase enzyme from breaking down acetylcholine, increasing both the level and duration of action of the neurotransmitter acetylcholine. 2 METHODOLOGY Some small molecules were taken as targeting agent which are responsible for inhibiting biological process in AD. The investigation drug Galantamine was used as a reference drug in the studies. 2W9I was taken as targeting protein and structure for the same was taken from PDB. Then Initial screening is done by Lipinskis rule of 5. The accepted compounds that were showing better interaction with the target protein and their energy were minimized using Marvins Sketch. Then the selected conformations are saved in three formats 1. SDF Format 2. PDB Format 3. MOL. Format Then docking is done between the protein and small molecules with QUANTUM. The best three results A  Author name is Amrendar kumar currently pursuing Bachelors degree program in Biotechnology, Amity University, Indiay, PH9453167284. Email amrendar2290gmail.com  CoAuthor name is Abhilasha singh currently pursuing Bachelorss degree program in Bioinformatics, Amity University, India, PH9532395556. Email abhilasha7489gmail.com  92International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                                       ISSN 22295518  JSER  2011 httpwww.ijser.org  obtained were analysed under HEX and ARGUS. IC50 value is taken by QUANTUM.Then graphs are plotted by seeing the values.  After this, ADME TOX analysis was performed. In this analysis ADME features were showing interaction   With 2W9I in both Argus and Quantum analysis were predicted under ADME test for toxicity prediction. TABLE 1   Natural compounds and Their Quantum results  S.no Name of compound Gbind Rms 1. Dichlorfop 17.66 69.85 2. Naringenin 20.45 72.55 3. Caffeine 13.02 80.61 4. Cla 20.00 66.34 5. Toluidine red 12.08 69.48 6. Polygala 22.82 73.28 7. Jatrorrhizine 25.95 96.90 8. Sterigmatocystin 22.54 73.39 9. Testosterone 20.03 74.21 10. Vitamin b6 21.39 72.23  TABLE 2   Drugs and Their Quantum results S.no Name of drug Gbind Rms 1. Cinacalcet  15.58 95.83 2. Penicillamine  16.18 90.59 3. Selegeline  21.29 90.18 4. Amantadine  23.79 94.72   AMES test is considering for initial screening of the molecule based on their ability to induce mutation. AMES test is used for determining if a chemical is mutagen. The molecules which showing ability to induce mutation were rejected in toxicity based screening.  Using ADME TOX analysis it was found that Jatrorrhizine was showing lower AMES test values than reference molecules. Further health effects of these molecules in blood, cardiovascular system, gastrointestinal system, kidney, liver and lungs were predicted. LD 50 values also were predicted for selecting reliable molecule for ADME analysis. After ADME analysis,a graph was plotted on ADMETOX values.                                  After all analysis, one compound Jatrorrhizine was the best molecule. This molecule is considered as better ligands for 2W9I based on interaction, Pharmacokinetics, and pharmacodynamic features and can be used for Chemotherapeutic use.  3 BIOCHEMISTRY OF ALZHEIMERS The biochemistry of Alzheimers disease AD, one of the most common causes of adult dementia, is as yet not well understood. It has been identified as a protein misfolding disease due to the accumulation of abnormally folded amyloid betaprotein in the brains of AD patients.  Amyloid beta, also written A, is a short peptide that is an abnormal proteolyticbyproduct of the transmembrane protein amyloid precursor protein APP, whose function is unclear but thought to be involved in neuronal development. The presenilins are components of proteolytic complex involved in APP processing and degradation. Amyloid beta monomers are soluble and contain short regions of beta sheet and polyproline II helix secondary structures in solution, though they are largely alpha helical in membranes however, at sufficiently high concentration, they undergo a dramatic conformational change to form a beta sheetrich tertiary structure that aggregates to form amyloid fibrils. These fibrils deposit outside neurons in dense formations known as senile plaques or neuritic plaques, in less dense aggregates as diffuse plaques, and sometimes in the walls of small blood vessels in the brain in a process called amyloid angiopathy orcongophilic angiopathy. AD is also considered a tauopathy due to abnormal aggregation of the tau protein, a microtubuleassociated proteinexpressed in neurons that normally acts to stabilize microtubules in the cell cytoskeleton. Like most microtubuleassociated proteins, tau is normally regulated by phosphorylation however, in AD patients, hyperphosphorylated tau accumulates as paired helical filaments that in turn aggregate into masses inside nerve cell bodies known as neurofibrillary tangles and as dystrophic neurites associated with amyloid plaques. Although little is known about the process of filament assembly, it has recently been shown that a depletion of a prolyl isomerase protein in the parvulin family accelerates the accumulation of abnormal tau. 93International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                                       ISSN 22295518  JSER  2011 httpwww.ijser.org  4 DISEASE MECHANISM Although the gross histological features of AD in the brain are well characterized, three major hypotheses have been advanced regarding the primary cause. The oldest hypothesis suggests that deficiency in cholinergic signaling initiates the progression of the disease. Two alternative misfolding hypotheses instead suggest that either tau protein or amyloid beta initiates the cascade. While researchers have not identified a clear causative pathway originating from any of the three molecular hypotheses to explain the gross anatomical changes observed in advanced AD, variants of the amyloid beta hypothesis of molecular initiation have become dominant among the three possibilities. 4.1 THE MECHANISM OF ACTION OF ACETYLCHOLINESTARASE Cholinergic nerve transmission is terminated by the enzyme acetylcholinesterase AchE. AchE is found both on the postsynaptic membrane of cholinergic synapses and in other tissues eg red blood cells. Acetylcholine Ach binds to AchE and is hydrolysed to acetate and choline. This inactivates the Ach and the nerve impulse is halted. AchE inhibitors eg rivastigmine prevent the hydrolysis of Ach, which increases the concentration of Ach in the synaptic cleft AchE inhibitors are widely used in the treatment of Alzheimers disease. 5 RESULTS AND ANALYSIS Alzheimers disease, the most common form of dementia, is a progressive disorder characterized by widespread loss of brain cells called neurons, betaamyloid deposits in the cerebral blood vessels, development of plaques and the presence of neurofibrillary tangles. Alzheimers disease AD is an irreversible, progressive disorder in which brain cellsneurons deteriorate, resulting in the loss of cognitive functions, primarily memory, judgment and reasoning, movement coordination, and pattern recognition. There is a very high genetic cause for Alzheimers disease. In addition to genetics as a cause of Alzheimers disease, many environmental factors including diet are to be considered. Older adults who smoke have an elevated risk of developing Alzheimers disease. Nerve signals travel across synapses with the help of chemicals known as neurotransmitters, including one called acetylcholine. Nerve cell destruction causes a reduction in acetylcholine, leading to impaired transmission of nerve signals and poor communication between nerve cells called neurons. In addition to acetylcholine, the brain of Alzheimers disease patients have areas of abnormal protein called plaques and tangles, the names reflecting what these abnormalities in the brain look like under the microscope.  The underlying cause of Alzheimers  what actually triggers the changes in the brain  is still not fully known but could partly be due oxidation and damage to nerve cells over time. It is likely that no single factor is responsible, but rather that it is due to a variety of factors, which may differ from person to person.  People whose parents or brothers and sisters develop the disease appear to be at greater risk of developing it themselves, so there may be a genetic component. However, no straightforward pattern of inheritance has been found. It is known that head injury is a risk factor, and also that Alzheimers disease often affects people with Downs syndrome. Some researchers have suggested that people who exercise their brains for example, doing crosswords and other mental agility exercises are less likely to develop the disease. And Omega 3 fatty acids, contained in oily fish such as mackerel and salmon may, also help to prevent dementia.  Australian scientists say they have identified a toxin that may play a key role as a potential cause of Alzheimers disease. The toxin, called quinolinic acid, kills nerve cells in the brain, leading to dysfunction and death. Quinolinic acid may not be the main cause of Alzheimers disease, but it plays a key role in its progression. Researchers have shown that a common anesthetic gas can cause fragments of a normal brain protein called amyloidbeta to clump together, which is thought to be the main problem underlying Alzheimers disease. Intravenous anesthetics have less of an effect, the team reports in the journal Biochemistry.  Anesthetics used in long surgery, such as inhaled anesthetics isoflurane and halothane, may be another cause of Alzheimers disease. Scientists conducted a series of lab experiments using nuclear magnetic resonance to investigate the reaction of amyloidbeta peptides to the inhaled anesthetic isoflurane and the intravenous anesthetics propofol and thiopental. They found that the peptides aggregated together after 10 to 30 hours exposure to isoflurane, depending on the concentration of the gas and the size of the protein fragments. The effect was seen with propofol after exposure for 48 hours, but no clumping was seen with thiopental. Biochemistry, January 23, 2007. Having frequent colds or flu or other infections increases the risk for this neurological deterioration due to increased overall inflammation in the body and brain.   There are certain natural and synthetic AchE inhibitors which will prevent the cause of AD by blocking the Biochemical pathway .Some number of natural compounds which are Inhibiting Ache were taken. 94International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                                       ISSN 22295518  JSER  2011 httpwww.ijser.org  Docking of these molecules was performed under QUANTUM 3.3.0. It was found that the natural compound Polygala, Sterigmatocystin and Jatrorrhizine were showing reliable pharmacokinetics and pharmacodynamics features than the commercial drugs. Hence they were taken out for work. After analysing the graphs, it was found that Bulbocapnie was the best Acetylcholinesterase Inhibitor. This molecule is considered as better ligands for Acetylcholinesterase based on ligandreceptor interaction,  Jatrorrhizine is a protoberberine alkaloid isolated from Enantia chlorantha Annonaceae and other species. Synonyms that may be encountered include jateorrhizine, neprotin, jatrochizine, jatrorhizine, or yatrorizine. It has been reported to have anti inflammatory effect, and to improve blood flow and mitotic activity in thioacetamidetraumatized rat livers. It was found to have antimicrobial and antifungal activity. It binds and noncompetitively inhibits monoamine oxidase IC50 4 micromolar for MAOA and 62 for MAOB.It interferes with multidrug resistance by cancer cells in vitro when exposed to a chemotherapeutic agent. Large doses 50100 mgkg reduced blood sugar levels in mice by increasing aerobic glycolysis.   Fig. 1 Graph showing QUANTUM Results of natural compounds and commercial drugs. Greens are for natural compounds and red is for commercial drugs.    Fig.2 Graph showing Hex results of best two natural compounds and best two commercial drugs based on quantum. Greens are for natural compounds and red is for commercial drugs. 6 CONCLUSION Alzheimers disease, the most common form of dementia, is a progressive disorder characterized by widespread loss of brain cells called neurons, betaamyloid deposits in the cerebral blood vessels, development of plaques and the presence of neurofibrillary tangles. Alzheimers disease AD is an irreversible, progressive disorder in which brain cells neurons deteriorate, resulting in the loss of cognitive functions, primarily memory, judgment and reasoning, movement coordination, and pattern recognition.There are certain natural and synthetic AchE inhibitors which will prevent the cause of AD by blocking the Biochemical pathway .Some number of natural compounds which are Inhibiting Ache were taken.Docking of these molecules was performed under QUANTUM. It was found that the natural compound POLYGALA, STERIGMATOCYSTIN AND JATRORRHIZINE was showing reliable pharmacokinetics and pharmacodynamics features than the commercial drugs. Hence they were taken out for work. 95International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                                       ISSN 22295518  JSER  2011 httpwww.ijser.org  Further, it was found that among three natural compounds JATRORRHIZINE is the best natural compound and better than commercial drugs and hence can be taken as AD chemopreventive agent and it can be effective inhibitors for AchE in AD pathway.     ACKNOWLEDGMENT The Author wishes to thanks Mr. Ajay Singh and Biplab Bhattacharjee sponsor and financial support. Researchers that contributed information or assistance to the article are Mayank Agarwal, Ajit Kumar pandey. REFERENCES 1. Ben AzizAloya R, Sternfeld M, Soreq H 1994. Promoter elements and alternative splicing in the human ACHE gene.. Prog. Brain Res. 98 14753. PMID 8248502. 2.  Craig SP, Buckle VJ, Lamouroux A, et al. 1986. Localization of the human tyrosine hydroxylase gene to 11p15 gene duplication and evolution of metabolic pathways.. Cytogenet. Cell Genet. 42 12 29 doi10.1159000132246. PMID 2872999. 3. Grima B, Lamouroux A, Boni C, et al. 1987. A single human gene encoding multiple tyrosine hydroxylases with different predicted functional characteristics.. Nature 326 6114 70711.doi10.1038326707a0. PMID 2882428. 4. Haycock JW, Ahn NG, Cobb MH, Krebs EG 1992. ERK1 and ERK2, two microtubuleassociated protein 2 kinases, mediate the phosphorylation of tyrosine hydroxylase at serine31 in situ.. Proc. Natl. Acad. Sci. U.S.A. 89 6 23659. doi10.1073pnas.89.6.2365. PMID 1347949. 5. Haycock JW 1990. Phosphorylation of tyrosine hydroxylase in situ at serine 8, 19, 31, and 40.. J. Biol. Chem. 265 20 1168291. PMID 1973163. 6. Momonoki YS May 1992. Occurrence of AcetylcholineHydrolyzing Activity at the SteleCortexInterface. Plant Physiol. 99 1 130133.doi10.1104pp.99.1.130. PMID 16668839. PMC 1080416. 7. Massoulie J, Pezzementi L, Bon S, Krejci E, Valette F 1993. Molecular and Cellular Biology of Cholinesterases.. Prog. Brain Res. 93 1 3191. PMID 8321908. 8. Masserano JM, Weiner N 1983. Tyrosine hydroxylase regulation in the central nervous system.. Mol. Cell. Biochem. 5354 12 12952. doi10.1007BF00225250. PMID 61377603 9. Meloni R, Biguet NF, Mallet J 2002. Postgenomic era and gene discovery for psychiatric diseases there is a new art of the trade The example of the HUMTH01 microsatellite in the Tyrosine Hydroxylase gene.. Mol. Neurobiol. 26 23 389403. doi10.1385MN2623389. PMID 12428766. 10. Purves, Dale, George J. Augustine, David Fitzpatrick, William C. Hall, AnthonySamuel LaMantia, James O. McNamara, and Leonard E. White 2008. Neuroscience. 4th ed.. Sinauer Associates.  11. Spring FA, Gardner B, Anstee DJ 1992. Evidence that the antigens of the Yt blood group system are located on humanerythrocyte acetylcholinesterase.. Blood 80 8213641. PMID 1391965. 12. Shafferman A, Kronman C, Flashner Y, et al. 1992. Mutagenesis of human acetylcholinesterase. Identification of residues involved in catalytic activity and in polypeptide folding.. J. Biol. Chem. 267 25 176408. PMID 1517212. 96International Journal of Scientific  Engineering Research Volume 2, Issue 4, April2011                                                                                  6 ISSN 22295518   IJSER  2011 httpwww.ijser.org                                            97International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518   IJSER  2011 httpwww.ijser.org  Hybrid Mechanical Charger  Ayush R Jain, Chinmay V Harmalkar  Abstract Mobile phone is our means to remain connected. While the phones have progressively got more powerful processors clocking 1GHz, huge amounts of memory and large touch screen interfaces, their power requirement has increased correspondingly. Unfortunately, battery technology has not been growing at a comparable pace. Hence, there is a need to frequently charge the batteries. While travelling people face a common problem of charging electronic appliances like mobile, mp3 player, camera, etc. Solution to this problem is Hybrid Mechanical Charger, extracts energy from some reliable renewable source. Hence in order to achieve this, Hybrid Mechanical Charger uses mechanical energy from both windmill and hand crank. To compensate the difference in rpm a gear shifting mechanism is used. Keywords Gear box, Generator, Handcrank, Windmill        1 INTRODUCTION                                                                     Existing battery chargers depend on the electricity supply. During travelling, avalibility of electricity supply is a problem. Hybrid Mechanical Charger is developed to solve this problem. Mechanical chargers have been previously developed but they either have hand crank or windmill mechanism to provide mechanical energy to the generator. These Mechanical chargers are not reliable and hence they are not prevalent. Hybrid Mechanical Charger harnesses energy by both mechanisms  handcrank and windmill using a single generator. This is achieved by using gear shifting mechanism. Hybrid Mechanical Charger increases the relaiblity for charging of electronic appliances. 2    DESIGN 2.1 Generator Generator is used to convert mechanical energy to electrical energy. Generator used is a permanent magnet geared dc motor. The stator consists of two magnets aligned with opposite poles facing each other and the rotor consists of three coils. When the shaft of the motor rotates, there is a relative motion between the permanent magnets and the coils which generates ac current in the coil. The flux associated with the coil is radial in nature. Commutator is used as a mechanical rectifier to convert AC current to DC current. The output of generator is 12v at 435rpm. The generator consists of a base motor and a gear box. The base motor rpm is quite large and to reduce the rpm it consists of a gear box. The gearbox consists of various gears such last gear is attached to the shaft. Gear 3 is mounted on gear 2 and gear 5 is mounted on gear 4. Gear 1 is attached to the shaft of the base motor. Refer to table 1  TABLE 1 Gear Box Assembly Gear No No of Teeth Base Motor Shaft Gear 9 Gear no 2 28 Gear no 3 8 Gear no 4 26 Gear no 5 14 Final Shaft Gear 20  2.2 Windmill and Handcrank Mechanical Enregy is required to produce electricity from generator, for which windmill and hand crank are used. The windmill has got three blades of length 117mm, which are attached to a hub of radius 23mm. Three blades are used to maintain proper rpm by reducing the effect of turbulence. It is a horizontal axis windmill which works on Bernoullis principle which says that as the velocity of the fluid increases, the pressure exerted by that fluid decreases. The shape of the windmill blade is aerofoil which is the main cause of the lift force which drives the windmill. Linear velocity at every point on blade is different because as radius changes at different point the linear velocity also changes.   v  r             1 v linear velocity r Radius of windmill  Angular velocity  At the tip the linear velocity is maximum. Therefore the drag force at the tip will be maximum which will give rise to shearing force. It can cause damage to the blade. To reduce the shearing force the area of the tip is reduced by making the tip narrow.  2.3 Gear Shifting Mechanism The windmill rotates at 400500 rpm which gives 910 V.   Ayush Jain is currently pursuing BE degree in electronics engineering in Mumbai University, India, PH91 9987418880. Email ayushjain1912yahoo.co.in  Chinmay Harmalkar is currently pursuing BE degree in electronics engineering in Mumbai University, India, PH 91 9819372211. Email chinmay.harmalkargmail.com  98International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org  The hand crank can be rotated with maximum of 120150 rpm. When hand crank is used, the rpm of the shaft is reduced and generator provides less power. To increases the rpm of the generator, an additional spur gear of 40mm diameter is used which is mounted on another shaft. This additional gear will internally increase the rpm by 14 ratio because 40mm gear is connected to the final gear of the geanerator 10mm.  2.4 Gearbox design The Gearbox consists of two aluminum plates constituting the top and the bottom surface. The bottom surface fig 2 has got a hole for generator and two holes for fitting the bearing. The top surface fig 1 has got a hole so that the shaft of the generator can come out and two holes for bearing. Both the surfaces have got a linear track so that the other shaft with 40mm gear can linearly move through it. Bearings are used to reduce friction and it also locks the shaft at proper position.  Fig 1. Top surface of the gearbox   Fig 2. Bottom surface of the gearbox     3   ANALYSIS AND CALCULATIONS TABLE 2 Motor Parameters Motor Parameters Symbol Value Rated Motor Voltage V 12 v Current at rated voltage I 60mA Rated RPM R 435 Idle Current Io 22mA Internal Resistance Ri 11    Various motor parameters have been calculated and listed in table 2 Pi  VI                                                                      2 Pi  Input electrical power  Pi 0.72 W  Po  Pi  loss Po  Output power  Loss includes both copper losses, mechanical loss which includes losses due to friction and winding. Copper loss will cause drop in voltage across the winding of the motor which is denoted by Vcu.  Vcu  I Ri                                                                     3 Vcu  0.66V  No load idle current Io is considered as the reason for mechanical loss in motor.  Po  Pi  V  Vcu I  Io                                               4 Po  0.43W    Po Pi                                                    5  Efficiency of electric motor   60  Kv  RPM  VVcu                                                                6 Kv  RPM Constant Kv  38.36  max  74     max  Maximum Efficiency  02004006000 5 10 15RPM Fsig 3. Voltage vs RPM characterictics 99International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org  Voltage is linearly related to the speed of rotations rpm as seen in fig 3.As the speed of rotation is increased more is the output voltage obtained. The black line shows the ideal characteristics.   Fig 4 Current vs Torque characteristics  Torque and Current are linearly related to each other as seen in fig 4. The black line shows the ideal characteristics. 4   COMPARISON OF STANDARD NOKIA CHARGER AND HYBRID MECHANICAL CHARGER Standard Nokia mobile charger  No load condition  Vout  7.25V Vout  Output voltage of the charger  With load mobile connected for charging  Vout  4V Iout  200  300mA Iout  Output current Hybrid Mechanical Charger  Minimum rpm required  45 Vout  3.9V Iout  150  250mA  Generator characteristics Vgen 5.5V Vgen  Output voltage of the generator  5 FUTURE INOVATION 5.1 Axial Flux generator The existing generator is a radial flux motor which consists of a drum shaped base motor and a motor gearbox which occupies a lot of space. Axial Flux Generators are disc shaped flat and have got high power density, can accommodate more no of poles.These generator have strong magnetic field due to neodymiumironboron magnets. The stator is made of thin coils or it is a printed circuited armature.  5.2 Worm wheel gear shift arrangement Worm wheel gear shifting arrangement will occupy very less space and gear shifting mechanism will become simple. A worm gear will be mounted on the shaft of the Axial Flux Generator which will be in touch with 20mm worm wheel which will result in a gear ratio of 110. Vertcical gear shift can be done which will consume very less space.  5.3 Ultracapacitor It can have a 1 Farad ultracapacitor which can store energy in the form of charge. In emergency situations rather than using mechanical source of energy we can just discharge the capacitor and use the energy. 2500Farads can provide energy of 1 rotation so 1 Farad can provide energy for 400 rotations. 6   CONCLUSION  A Hybrid Mechanical Charger was successfully implemeted on a Nokia phone. Charging of a mobile phone was done by both handcrank and windmill mechanism. It was found that the rate of charging from Hybrid Mechanical Charger is equivalent to the normal charger . ACKNOWLEDGMENT The authors wish to thank Mr. Dileep C. C, Mr. Anasta E.Rumao for their support and technical guidance.This work was supported in part by a grant from Fr. Conceicao Rodrigues College of Engineering. REFERENCES 1 B.L Theraja,A.K Theraja, A Textbook of Electrical Technology 2 Jacek F.Gieras, RongJia Wang, Maarten J.Kamper, Axial Flux Permanent Magnet Brushless Machines 3 Irving Gottlieb Practical Electrical Motor Handbook  4 David A Spera,  Wind Turbine Technology  5 D.P Kothari, I.J Nagrath, Electric Machines  6 J.R Davis,  Gear Materials, properties and manufacture  7 D.A Neamen, Electronic Circuit Analysis and Design              100International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                                    ISSN 22295518  IJSER  2011 httpwww.ijser.org  A Novel Realtime Intelligent Tele Cardiology System Using Wireless Technology to Detect Cardiac Abnormalities                                                                   S. Kohila, K. GowriAbstract  This study presents a novel wireless, ambulatory, realtime, and auto alarm intelligent telecardiology system to improve healthcare for cardiovascular disease, which is one of the most prevalent and costly health problems in the world. This system consists of a lightweight and powersaving wireless ECG device equipped with a builtin automatic warning expert system.  A temperature sensor is fixed to the users body, which senses temperature in the body, and delivers it to the ECG device. This device is connected to a microcontroller and ubiquitous realtime display platform. The acquired ECG signals which are transmitted to the microcontroller is then, processed by the expert system in order to detect the abnormality. An alert signal is sent to the remote database server, which can be accessed by an Internet browser, once an abnormal ECG is detected. The current version of the expert system can identify five types of abnormal cardiac rhythms in realtime, including sinus tachycardia, sinus bradycardia, wide QRS complex, atrial fibrillation AF, and cardiac asystole, which is very important for both the subjects who are being monitored and the healthcare personnel tracking cardiacrhythm disorders. The proposed system also activates an emergency medical alarm system when problems occur. We believe that in the future a businesscardlike ECG device, accompanied with a Personal Computer, can make universal cardiac protection service possible. Keywords  Atrial fibrillation AF, ECG, Temperature Sensor, Expert Systems, Personal Computer, Wireless.        1 INTRODUCTION 1.1 General Introduction ARDIO Vascular disease CVD is one of the most prevalent and serious health problems in the world. An Estimated 17.5 million people died from CVD in 2005, representing 30 of all deaths worldwide. Based on current trends, over 20 million people will die from CVD by 2015. In 2000, 56 of CVD deaths occurred before the age of 75. However, CVD is becoming more common in younger people, with most of the people affected now aged between 34 and 65 years 1. In addition to the fatal cases, at least 20 million people experience nonfatal heart attacks and strokes every year many requiring continuing costly medical care. Developed countries around the world continue to experience significant problems in providing healthcare services, which are as follows 1 The increasing proportion of elderly, whose lifestyle changes are increasing the demand for chronic disease Healthcare services 2 Demand for increased accessibility to hospitals and mobile healthcare services, as well as inhome care 2 3 Financial constraints in efficiently improving personalized and qualityoriented healthcare though the current trend of centralizing specialized clinics can certainly reduce clinical costs, decentralized healthcare allow the alternatives of inhospital and outhospital care, and even further, home healthcare 3. Rapid developments in information and communication technologies have made it possible to overcome the challenges mentioned earlier and to provide a changing society with an improved quality of life and medical services.   1.2 Sinus Tachycardia Sinus tachycardia also colloquially known as sinus tach or sinus tachy is a heart rhythm with elevated rate of impulses originating from the sinoatrial node, defined as a rate greater than 100 beatsmin in an average adult. The normal heart rate in the average adult ranges from 60100 beatsmin. Note that the normal heart rate varies with age, with infants having normal heart rate of 110150 bpm to the elderly, who have slower normals. Tachycardia is often asymptomatic. If the heart rate is too high, cardiac output may fall due to the markedly reduced ventricular filling time. Rapid rates, though they may be compensating for ischemia elsewhere, increase myocardial oxygen demand and reduce coronary blood flow, thus precipitating an ischemia heart or valvular disease. 1.3 Sinus Bradycardia Sinus bradycardia is a heart rhythm that originates from the sinus node and has a rate of under 60 beats per minute. The decreased heart rate can cause a decreased cardiac output resulting in symptoms such as lightheadedness, dizziness, hypotension, vertigo, and syncope. The slow heart rate may also lead to atrial, junctional, or ventricular ectopic rhythms. Sinus Bradycardia is not necessarily problematic. People who regularly practice sports may have sinus bradycardia, because their trained hearts can pump enough blood in each C101International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                                    ISSN 22295518  IJSER  2011 httpwww.ijser.org  contraction to allow a low resting heart rate. Sinus Bradycardia can aid in the sport of Free diving, which includes any of various aquatic activities that share the practice of breathhold underwater diving, Bradycardia aids in this process due to drop in blood rate pulse. These adaptations enable the human body to endure depth and lack of oxygen far beyond what would be possible without the mammalian diving reflex. Sinus bradycardia is a sinus rhythm of less than 60 bpm. It is a common condition found in both healthy individuals and those who are considered well conditioned athletes. Studies have found that 50  85 percent of conditioned athletes have benign sinus bradycardia, as compared to 23 percent of the general population studied. Trained athletes or young healthy individuals may also have a slow resting heart rate. 1.4 Wide QRS Complex A widened QRS 120 msec occurs when ventricular activation is abnormally slow, either because the arrhythmia originates outside of the normal conduction system e.g., ventricular tachycardia, or because of abnormalities within the HisPurkinje system e.g., supraventricular tachycardia with aberrancy. 1.5 Atrial Fibrillation Atrial fibrillation AF is the most common cardiac arrhythmia, affecting nearly 1 of the population. Its prevalence increases with age although relatively infrequent in those under 40 years old, it occurs in up to 5 of those over 80. Most people with a normal sinus rhythm have a resting heart rate of between 60 and 100 beats per minute. In AF patients, the atria contract rapidly and irregularly at rates between 400 to 800 beat per minute. Fortunately, the atrioventricular node compensates for this activity only about one or two out of three atrial beats pass to the ventricles 4. A typical ECG in AF shows a rapid irregular tachycardia in which recognizable P waves are sometimes absent. The ventricular rate in patients with untreated AF is generally 110 to 180 beats per minute. However, slower ventricular rates may occur in elderly patients with untreated AF. Data from the Framingham study demonstrates that chronic heart failure is associated with a 4.5fold increase in risk of AF in men and a 5.9fold increase in women. Apart from the epidemiological data, most evidence on the prevalence of AF in heart failure patients stems from analysis of a number of clinical trials conducted within the last 1015 years on populations with heart failure. AF might have no detectable CVD. Hemodynamic impairment and thromboembolic events related to AF patients included in these trials were selected for different purposes, which are reflected in the varying prevalence of AF. In addition, AF often associated with structural heart disease, causes significant morbidity, mortality, and healthcare cost in a substantial proportion of patients, thus making it a major global healthcare challenge. In this study, we attempted to develop an intelligent expert system with a builtin abnormal ECGdetection mechanism in the telecardiology healthcare service to facilitate diagnosis and management of patients with AF and other rhythm disorders 6. Simplicity, reliability, and universality are the main concepts behind this service. Therefore, this study constructs a ubiquitous and intelligent telecardiology healthcare network consisting of a miniature wireless ECG device embedded with an alert expert system for the early detection of cardiac disorders. 1.6 Cardiac Asystole Asystole is a state of no cardiac electrical activity hence no contractions of the myocardium and no cardiac output or blood flow. Asystole is one of the conditions required for a medical practitioner to certify death. While the heart is asystolic, there is no blood flow to the brain unless CPR or internal cardiac massage is performed, and even then it is a small amount. After many emergency treatments have been applied but the heart is still unresponsive, it is time to consider pronouncing the patient dead. Even in the rare case that a rhythm reappears, if asystole has persisted for fifteen minutes or more the brain will have been deprived of oxygen long enough to cause brain death. 2 OVERALL SYSTEM The system proposed in this study uses a Threelead wireless ECG device, a microcontroller expert system, and a Webbased monitoring platform to meet these objectives. A small, threelead ECG device is first set up using electrodes, affixed to areas on the users body. A temperature sensor is fixed to the users body, which senses temperature in the body and deliver it to the ECG device. This lightweight ECG can be connected to portable devices, such as a notebook or mobile phone, using ZigBee Module. The programming application installed on a microcontroller in ECG Device can then initiate data recording and data transmission. Successive data are transmitted to a portable device and processed in a period of 6 s by an expert system. The lead signal, coordinated with the direction of cardiac conduction pathway is extracted for signal analysis. As long as an abnormal ECG signal was detected, the system automatically transmitted data over a WiFi3G2G networks to a remote data server see Fig 1. At the same time, the system will send out an alert message to a nursing station in the cardiovascular ward for further examination. If necessary, the emergency ward or other departments can also access this data through an intranet. With the convenience of the worldwide Web protocol, anyone, including physicians, nurses, and family members, can access the data server and monitor realtime ECG plots using a Web browser, such as Internet Explorer IE. For patients admitted to a cardiovascular ward or intensive care unit ICU, the proposed cardiohealthcare system provides greater freedom of movement than products currently on the market 7. Paring lightweight wireless ECG devices with mobile phones offers continuous and reliable patient monitoring. A warning system is also activated when unstable ECGs appear.  102International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                                    ISSN 22295518  IJSER  2011 httpwww.ijser.org  Whenever the person moves his body temperature rises due the work done, and the degree of increase is based on the amount of work done. Also if a person got any fever his heart beat may increase which may be misrecognized as an abnormal heart condition on an average the heart beat increase for an average of 78beats for every single degree increase in temperature. Here in the proposed work the constraint has been considered and the problem has been faced better. In the proposed system Zigbee module is used, which is fully secured and provide full duplex transmission. This will enhance the  Fig 1 Flowchart of proposed system.The System uses a 5 lead ECG acquisition device ,the Abnormality detection Algorithm in the device detect the abnormal ECG signal and a Webbased monitoring window allowing access to the data server and plot in real time by using an IE browser. Step The Signal are acquired by the ECG Electrodes Step The Abnormality in signal is detected and transferred to the hospital intranet, StepThe information from intranet is moved to the emergency ward database, Step The information from the intranet is also moved to the hospital intranet server, Step The abnormality information can be accessed by anyone through web service. way of wireless communication used in the system. ZigBee targets the application domain of low power, low duty cycle and low data rate requirement devices. 3 ECG DEVICE WITH WIRELESS UNIT The Hardware Circuit Designed Using all the Required parts 5 ECG electrode, Pre Amplifier, Band Pass Filter, Amplifier, PIC Microcontroller, ADC, Zigbee Module 8. The proposed threelead ECG device contains two main parts The Analog unit and The Digital unit see Fig 2.  3.1 Analog Unit The DAQ unit integrates an analog preamplifier, filter, and an AD converter ADC into a small 20  18 mm2, lightweight, and batterypowered DAQ system. The ECG signal is sampled at 512 Hz with 12bit resolution, amplified by 100 times, and band pass filtered between 1 to 150 Hz. To reduce the number of wires for highdensity recordings, the  103International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                                    ISSN 22295518  IJSER  2011 httpwww.ijser.org   Fig 2 Block Diagram of ECG Device power clocks, and measured signals are daisychained from one node to another with bitserial output. Therefore, adjacent nodes electrodes are connected together to 1 share the power, reference voltage, and ADC clocks and 2 daisy chain the digital signal outputs.  3.2 Digital Unit The Digital unit consists of a wireless module and a microcontroller. This unit uses a Zigbee module to send the acquired ECG signals to a Zigbee Enabled PC, serving as a realtime signal processing unit. All modules included one wirelesstransmission unit and three DAQs, the theoretical maximum running time is estimated at about 33 h when using an 1100 mAh Liion battery with continuous acquisition and transmission of the physiological signal to the expert system9. 4 QRS WAVE DETECTION ALGORITHM The ECG signals are amplified and recorded with a sampling rate of 512 Hz and band pass filtered between 1 and 150 Hz. Artifacts were removed before R peak detection. A 50 Hz notch filter is used to eliminate the power line interference, producing highfrequency, noisefree, and smooth data. Two segments of the baseline signal are extracted to compute mean and standard deviation SD. Besides, the QRS detector requires the first and secondorder derivative of the preprocessed ECG signal.  The latter gives spikes at the fiducial points. There are also false spikes, but their relative magnitudes are lower than those of the spikes at the fiducial points. Accordingly, the R peak is clipped by higher magnitude negative peaks and high positive peaks in the first derivative plot. The procedure of defining the QRS complex onset is as follows after 256 ms of flat segment in the ECG, the first sample, where the slope becomes steeper high positive peaks than the higher slope threshold, is defined as the QRS onset. The lower slope threshold is used to detect the higher magnitude negative peaks. Both thresholds are updated to search for missing beats. After identifying the QRS onset, the R peak is labeled by searching for the maximal value of the ECG samples in the 36 ms following the QRS onset. When R peak is determined, the QRS detector searches forward and backward to identify the two most negative points on the ECG plot and labels them as the Q wave peak and the S wave peak, respectively. The black curve means the normal ECG signal which contains three waves. The dotted line shows the first deviation of the normal ECG signal see Fig 3. Results show that full squares wave peak  .  Fig 3 QRS detection in the normal ECG waves.  mark the R wave peaks, full circles mark the Q s, and full star mark the S wave peak. QRS onset is defined by the vertical line.The QRS complex duration is set from QRS onset time to 20 ms after the S wave peak 5 ABNORMAL ECG DETECTION  After defining the QRS complex and the Q, R, and S wave peak, we then sought to detect common and important rhythm disorders, including sinus tachycardia, sinus bradycardia, cardiac systole, AF, and wide QRS complex. Sinus tachycardia is detected by the condition of the heart rate 100 beats per minute. An asystole indicates the situation of no heart rate. Wide QRS complex occurred as the duration of QRS 104International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                                    ISSN 22295518  IJSER  2011 httpwww.ijser.org  complex was greater than or equal to 120 ms. C. AF Detection Since an irregular rhythm of the QRS complexes is the major feature of AF, the RR interval RRI, defined as the interval of neighboring QRS complexes, is an ideal parameter to identify AF. This study uses two different algorithms for AF detection. 5.1 Algorithm I for Abnormal ECG Detection Step 1 Detection of R waves and marking of R peaks. Step 2 Calculation of RRI the duration of adjoined R peaks. Step 3 Calculation of the variation of consecutive RRI RRI. Step 4 Activation of the alarm system when RRI150 ms occurs twice within each 6 s of computation. 5.2 Algorithm II for Abnormal ECG Detection Step1 Detection of R waves and marking of R peaks. Step2 Calculation of RRI the duration of adjoined R peaks. Step3 Counting the number of peaks to calculate the number of beats in each 6s. Step4 Check whether temperature ranges between 33370 continue the process, else go to    Step 8 Step5 Calculation of the variation of consecutive               RRIRRI. Step6 Calculation of the SD of RRI RRIstd in each 6s recording. Step7 Activation of the alarm system when RRI150 ms occurs twice and RRIstd  60 ms within 6 s of computation. Step8 Measuring the heartbeat, up to 180bpm is considerable during very high fever. Theoretically, Algorithm I is more accurate in detecting an irregular ventricular rhythm, though in detecting frequent pre mature beats during uncommon, it is difficult to differentiate the Atrial Fibrillation from Premature Beats. To overcome this problem, we formulated Algorithm II, which uses a cutoff value of RRIstd60 ms for AF detection. This condition shows that whenever the standard Deviation of RRI exceeds 60ms the system will produce an alert signal. The cutoff value of 60 ms was based on comparing 50 normal subjects and 50 patients having cardiac abnormality Also Algorithm II will produce an accurate detection considering the temperature of the patient Fig. 4 shows statistical results of the differences between normal and AF patients regarding RRI and RRIstd. Accordingly, the threshold level of RRI and RRIstd were given as 150 and 60 ms, respectively. 6 RESULTS AND DISCUSSIONS In order to analyze the performance of the abnormality detection algorithm, the device is fixed to 10 patients who underwent treatment at General Hospital.Analysis and Performance Calculation was performed according to the recommendations of the American National Standard for ambulatory ECG analyzers ANSIAAMI EC381994 11. A true positive TP shows that the algorithm successfully detected abnormality for abnormal subject during every 6 s of computation. On the other hand, a false negative FN shows a failed detection of abnormality for an abnormal patient. Finally, false positive FP represents a false detection of abnormality, whereas true negative TN means normal subjects have no abnormality detection. Accuracy, sensitivity and positive predictive values were used for further analysis. The recorded data were shown in Tables 1 and 2 for normal and abnormal patients under testing for different algorithms. The subjects tested for abnormalities were of age group greater than 50. Table 1 shows the abnormality detection in subjects using Algorithm I.   TABLE 1 Abnormality Detection in Subjects Using Algorithm I Subject Condition N10 Positive Test Negative Test Total Abnormal patients  187 8 195 Normal patients  5 0 5 Total no. of tests 200  TABLE 2 Abnormality Detection in Subjects Using Algorithm II Subject Condition N10 Positive Test Negative Test Total Abnormal patients  192 5 197 Normal patients  3 0 3 105International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                                    ISSN 22295518  IJSER  2011 httpwww.ijser.org  Total no. of tests 200    Fig 5 Analysis of Average Performance between two Algorithms.  Totally 10 subjects were tested and each were tested 20 times, hence there will be totally 200 tests undergone. Each test was recorded for duration of 6 sec. Hence the total duration for each subject will be 2 min. This algorithm shows 100 for normal subjects and with little difference in case of abnormal subjects. Similarly, Table 2 shows the abnormality detection in subjects using Algorithm II. While considering Algorithm I the average accuracy was 93.5, its sensitivity performance is 95.9, and the positive predictive value is 97.4 see Fig 5. The same 10 patients were also tested using Algorithm II, in which abnormality is detected only when three conditions were met. i.e.,  i  RRI150 ms occurs twice ii RRIstd  60 ms within 6 s of computation and iii Considering body temperature. The average accuracy, sensitivity, and positive predictive performance were 96, 97.5, and 98.5 see Fig 5 respectively. Comparing the performance of both Algorithms I and II, the performance in Algorithm II was much better. These results showed that combining the three conditions as the detection criteria in Algorithm II, will improve the abnormality detection performance, especially in terms of accuracy and sensitivity performance. Among a total of 10 patients, Algorithm II displays the stable and high impact results across subjects. The results suggest that our system can provide a reliable abnormality detection function in telecardiology healthcare services. 7 CONCLUSION AF, the most common sustained cardiac arrhythmia, causes significant mortality and morbidity, and remains a major healthcare challenge 11. Early detection is very important for providing appropriate therapeutic interventions and managing disease related complications, such as congestive heart failure and stroke. This study demonstrates that the proposed intelligent telecardiology system is capable of accurately detecting AF episodes and instantaneously alerting both the user and the healthcare personnel, facilitating early medical intervention. Furthermore, this intelligent telecardiology system is superior to conventional healthcare devices because it integrates all the key elements in one system. The following list describes the most important features of the proposed system  1 Wireless Communications between devices are all wireless Zigbee, reducing wire stock usage and allowing convenient operation. 2 Ambulatory The miniature ECG device is very lightweight, can easily be applied to the body, and can operate for a considerable length of time. The system can be run anywhere with a notebook or mobile phone, eliminating the problems of limited power or restricted areas. 3 Real time ECG signals can be transmitted to nearby mobile devices instantly and there is only a few seconds lag when the signals are transmitted to a remote database server, depending on network capacity. 4 Selfalarm The builtin expert system automatically detects abnormal ECG signals and alerts both the user and healthcare personnel using a Internet, or by sending a message to a remote database server installed in the hospital computer system and the emergency service system. This novel system cannot only be used for inpatients and outpatients, but also provides a longlasting health monitor to normal people. Patients wearing the lightweight threelimb lead wireless ECG device can hardly feel its presence, but still enjoy a sense of protection.  However, there are several limitations for the expert system. First, Abnormality detection is based on the RRI variation, when the user has frequent atrial or ventricular premature beats, which can be misdiagnosed as AF. Second, in patients with Cardiac Abnormality and markedly impaired AV nodal conduction, RRI variations may become too small for the system to diagnose Abnormality accurately. Lastly, there is still considerable motion noise during the recording, which might impair diagnostic accuracy. In conclusion, this novel intelligent telecardiology system 106International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                                    ISSN 22295518  IJSER  2011 httpwww.ijser.org  is capable of early Abnormality detection, and represents a successful first step toward improving efficiency and quality of care in CVD. Further researches aimed at improving both hardware and software designs are necessary to enhance the efficiency and accuracy in future models of this system. 8 ACKNOWLEDGEMENT The Authors would like to thank The Principal and Head of the Department of KSR College of Engineering for their constant support also they thank Dr.T.Gayathri M.B.B.S for her great help in providing clinical guidance.   9 REFERENCES 1. World Health Organization, The World Health Report       2008, World Health Organization,     Geneva,2008.Availablehttpwww.who.intwhr2008whr08en.pdf 2. Koch.S,2006 Home telehealthCurrent state and future trends, Int. J. Med. Inf., vol. 75, pp. 565576. 3. Romanow, R.J. 2002, Nov.. Building on values The future of healthcare in CanadaFinal report. Commission of the Future of Health Care, Ottawa, Canada Online. Available httpwww.cbc.cahealthcare finalreport.pdf  4. Waktare.J.E.P, 2002 Atrial fibrillation, Circulation, vol. 106, pp. 1416. 5. Hurst. J, Naming of the waves in the ECG, with a brief account of their Genesis. 6. Kannel.W.B and McGee.D.L,1979 Diabetes and cardiovascular disease. The Framingham study, J. Amer. Med. Assoc., vol. 241, no. 19, pp. 2035 2038. 7. Amer. Coll. Cardiol.J 2006 ACCAHAESC 2006 guidelines for the management of patients with atrial fibrillation, vol. 48, pp. 149246. 8. Vanagas.G, Zaliunas.R, Benetis.R, and Slapikas.R,2008 Factors affecting relevance of teleECG systems application to high risk for future ischemic heart disease events patients group, Telemed. J. eHealth, vol. 14, no. 4, pp. 345349. 9. Lin.C.T, Ko.L.W, Chang.C.J, Wang.Y.T, Chung.C.H, Yang.F.S, Duann.J.R, Jung.T.P, and ChiouJ.C,2009 Wearable and wireless brain computer interface and its applications, presented at the 13th Int. Conf. Human Computer Interface, San Diego, CA. 10. Pang.L, Techoudovski.I, Braecklein.M,                            Egorouchkina.K, Kellermann.W and Bolz.A,2005 Real time ischemia detection in the smart home care system, in Proc. 27th Annu. Conf. IEEE Eng. Med. Biol., Shanghai, China, Sep. 14, pp. 3703370 11. American National Standard for Ambulatory Electrocardiographs, ANSIAAMI EC381994, 1994.  12. Papaloukas.C, Fotiadis.D.I,  Likas.A,  Stroumbis.C.S, and Michalis.L.K,1998 Use of a novel rulebased expert system in the detection of changes in the ST segment and the T wave in long duration ECG, J. Electrocardiol., vol. 35, no. 1, pp. 2734, 2002, Circulation, vol. 98, pp. 19371942.     107International Journal of Scientific  Engineering Research Volume 2, Issue 5, May 2011                                                                                                                              ISSN 22295518  IJSER  2011 httpwww.ijser.org    A Novel Dynamic Key Management Scheme Based  On Hamming Distance for Wireless Sensor Networks R.Divya  , T.Thirumurugan  Abstract  Numerous key management schemes have been proposed for sensor networks. The objective of key management is to dynamically establish and maintain secure channels among communicating nodes. Many schemes, referred to as static schemes, have adopted the principle of key predistribution with the underlying assumption of a relatively static shortlived network node replenishments are rare, and keys outlive the network. An emerging class of schemes, dynamic key management schemes, assumes longlived networks with more frequent addition of new nodes, thus requiring network rekeying for sustained security and survivability. This paper proposes a dynamic key management scheme by combining the advantages of simple cryptography and random key distribution schemes. When the hamming distance between the two nodes is found high, the unique key is changed instead of changing the set of keys and the communication takes place by using any one of the set of key xoring with the new unique key. The security and performance of the proposed algorithm is compared with the existing dynamic key management scheme based on Exclusion Basis System and prove that the proposed scheme performs better when compared to existing scheme by considering the number of nodes colluded with time. The result obtained by simulation also shows that the proposed scheme provides security solution and performs better than the existing scheme.  Index Terms  WSNs, dynamic key management, collusion, hamming distance, security.         1. INTRODUCTION               HE envisioned growth in utilizing sensor networks in a wide variety of sensitive applications ranging from healthcare to warfare is stimulating numerous efforts to secure these networks. Sensor networks comprise a large number of tiny sensor nodes that collect and partially process data from the surrounding environment. The data is then communicated, using wireless links, to aggregation and forwarding nodes or gateways that may further process the data and communicate it to the outside world through one or more base stations or command nodes. Base stations are the entry points to the network where user requests begin and network responses are received. Typically, gateways and base stations are higherend nodes. It is to be noted, however, that various sensor, gateway, and base station functions can be performed by the same or different nodes. The sensitivity of collected data makes encryption keys essential to secure sensor networks.  1.1 Key Management  The term key may refer to a simple key e.g., 128bit string or a more complex key construct e.g., a symmetric bivariate key polynomial.A large number of keys need to be managed in order to encrypt and authenticate sensitive data exchanged. The objective of key management is to dynamically   establish and maintain secure channels among communicating parties.               Typically, key management schemes use administrative keys key encryption keys for the secure and efficient                      redistribution and, at times, generation of the secure channel communication keys data encryption keys to the communicating parties. Communication keys may be pairwise keys used to secure a communication channel between two nodes that are in direct or indirect communications, or they may be group keys shared by multiple nodes. Network keys both administrative and communication keys may need to be changed rekeyed to maintain secrecy and resilience to attacks, failures, or network topology changes. Numerous key management schemes have been proposed for sensor networks. Most existing schemes build on the seminal random key predistribution scheme introduced by Eschenauer and Gligor 1. Subsequent extensions to that scheme include using deployment knowledge 2 and key polynomials 3 to enhance scalability and resilience to attacks. These set of schemes is referred as static key management schemes since they do not update the administrative keys post network deployment.              An example of dynamic keying schemes is proposed by Jolly et al. 4 in which a key management scheme based on identity based symmetric keying is given. This scheme requires very few keys typically two to be stored at each sensor node and shared with the base station as well as the cluster gateways. Rekeying involves reestablishment of clusters and redistribution of keys. Although the storage requirement is very affordable, the rekeying procedure is inefficient due to the large number of messages exchanged for key renewals. Another emerging T     R.Divya is currently working in Dr.Pauls Engineering College         in Electronics  Communication Engineering, Anna              University, India.     Email div.1484gmail.com    T.Thirumurugan  is currently pursuing doctorate  degree from U.  Anna University, Coimbatore, India.    Email thiru0809gmail.com  108International Journal of Scientific  Engineering Research Volume 2, Issue 5, May 2011                                                                                                                              ISSN 22295518  IJSER  2011 httpwww.ijser.org    category of schemes employ a combinatorial formulation of the group key management problem to affect efficient rekeying      5, 6. These are examples of dynamic key management schemes.            While static schemes primarily assume that administrative keys will outlive the network and emphasize pair wise communication keys.   Dynamic schemes advocate rekeying to achieve resilience to attack in longlived networks and primarily emphasize group communication keys. Since the dynamic scheme has the advantage of long lived network and rekeying when compared to the static schemes, the dynamic key management is chosen as a security scheme for the WSNs.   2.      KEY MANAGEMENT SCHEMES IN SENSOR                   NETWORKS  The success of a key management scheme is determined in part by its ability to efficiently survive attacks on highly vulnerable and resource challenged sensor networks. Key management schemes in sensor networks can be classified broadly into dynamic or static solutions based on whether rekeying update of administrative keys is enabled post network deployment.  2.1   Static Key Management Schemes  The static schemes assume that once administrative keys are predeployed in the nodes, they will not be changed. Administrative keys are generated prior to deployment, assigned to nodes either randomly or based on some deployment information, and then distributed to nodes. For communication key management, most static schemes use the overlapping of administrative keys to determine the eligibility of neighboring nodes to generate a direct pairwise communication key. Communication keys are assigned to links rather than nodes. In order to establish and distribute a communication key between two non neighboring nodes andor a group of nodes, that key is propagated one link at a time using previously established direct communication keys.  2.2   Dynamic Key Management Schemes  Dynamic key management schemes may change administrative keys periodically, on demand or on detection of node capture. The major advantage of dynamic keying is enhanced network survivability, since any captured keys is replaced in a timely manner in a process known as rekeying. Another advantage of dynamic keying is providing better support for network expansion upon adding new nodes, unlike static keying, which uses a fixed pool of keys, the probability of network capture increase is prevented. The major challenge in dynamic keying is to design a secure yet efficient rekeying mechanism. A proposed solution to this problem is using exclusionbased systems EBSs a combinatorial formulation of the group key management problem  3.           SENSOR NETWORK MODEL  Both the proposed and the existing security algorithm are based on a wireless sensor network consisting of a command node and numerous sensor nodes which are grouped into clusters. The clusters of sensors can be formed based on various criteria such as capabilities, location, communication range, etc.  Each cluster is controlled by a cluster head, also known as gateway, which can broadcast messages to all sensors in the cluster. We assume that the sensor and gateway nodes are stationary and the physical location and communication range of all nodes in the network are known. Each gateway is assumed to be reachable to all sensors in its cluster, either directly or in multihop. Sensors perform two main functions sensing and relaying. The sensing component is responsible for probing their environment to track a target event. The collected data are then relayed to the gateway. Nodes that are more than one hop away from the gateway send their data through relaying nodes. Sensors communicate only via shorthaul radio communication. The gateway fuses reports from different sensors, processes the data to extract relevant information and transmits it to the command node via longhaul transmission.   Fig. 1. Clustered Sensor Network  109International Journal of Scientific  Engineering Research Volume 2, Issue 5, May 2011                                                                                                                              ISSN 22295518  IJSER  2011 httpwww.ijser.org                                                         The network architecture is depicted in Fig. 1. Each tier of the network possesses different capabilities. The command node is resourcerich. However, the amount of traffic flowing between the command node and gateways causes the communication channel between the command node and gateways to be restrained. Most often, the command node is situated at a considerable distance from the deployment region, and might only be reachable through slow satellite links. Larger communication distances also incur increased security vulnerability and packet loss during long haul transmissions.   4.         COLLUSION PROBLEM  The security scheme proposed in 6 is based on the Exclusion Basis System EBS to address the collusion problem in EBS that performs location based key assignment to minimize the number of keys revealed by capturing collocated nodes. The network model is similar to the model developed in 6 with clusters and gateways. It uses the EBS framework to perform rekeying within each cluster. Keys are distributed to nodes by the gateways. SHELL uses postdeployment location information in key assignment collocated nodes share more keys than nodes that are not collocated.  4.1         Exclusion Basis System EBS    EBS is a combinatorial formulation of the group key management problem in which each node is assigned k keys out of a pool of size P  k  m keys. Rekeying takes place either periodically or when one or more nodes are captured or suspected of being captured. Replacement keys are generated, then encrypted with all the m keys unknown to the captured nodes, and finally distributed to other nodes that collectively know the m keys. A drawback of the basic EBSbased solution is that a small number of nodes may collude and collectively reveal all the network keys. This is particularly true when the value of m is selected to be relatively small to make rekeying feasible in terms of number of messages. EBSbased key management can be prone to collusion attacks. Two nodes collude when they share their keys with each other. In other words, colluding nodes would grow their knowledge about the network security measures. In SHELL 6, keys are reused in multiple nodes and only key combinations are unique. Therefore, it is conceivable that few compromised nodes can collude and reveal all the keys employed in the network to an adversary. Such a scenario is considered as capturing the entire network since the adversary would be capable of revealing all encrypted communications in the network.  SHELL exploits the physical proximity of nodes so that a node would share most keys with reachable nodes. To avoid the assignment of same key combinations, swapping of keys is employed. If S1 is a neighbor of S2, S2 is a neighbor of S3, and S1 colludes with S2, the resultant keys known to both of them would be Keys of S1 U Keys of S2. Thereafter if S2 and S3 collude, the keys known to S2 and S3 would be Keys of S1 U Keys of S2 U Keys of S3.   Thus, it can be seen that if multiple nodes collude, it is likely to uncover all employed keys. The collusion of nodes is illustrated in Fig 2. Due to swapping of keys, the key combination gets repeated at a particular time instant such that the number of nodes that share the same key combination increases.              Fig.2. Collusion of Nodes    As a result the neighboring node gets colluded if it shares the same key combinations. When the number of nodes getting colluded increases, capturing few node will reveal all the keys that has been employed in the network which results in capturing the entire network. In order to address the collusion problem in 6, an efficient dynamic key management scheme is proposed.  5. DYNAMIC KEY MANAGEMENT  Due to swapping of keys the number of nodes getting colluded with the neighboring nodes is increased so capturing lesser nodes will reveal most of the keys and thus the whole network can be captured by the attacker. In order to reduce the number of colluding nodes a dynamic key assignment was chosen to employ the simple cryptography and random key distribution. Both the simple cryptography and random key distribution has its own advantages and limitations. Thus the dynamic key management scheme with the advantages of both the schemes and by taking the hamming distance into consideration is proposed as a security solution. Three methods are proposed for the dynamic key management namely  110International Journal of Scientific  Engineering Research Volume 2, Issue 5, May 2011                                                                                                                              ISSN 22295518  IJSER  2011 httpwww.ijser.org    1. Simple cryptography 2. Random key distribution 3. Dynamic key assignment  5.1 Simple Cryptography  A simple cryptography of xoring two keys is first tried as a dynamic key management. In this simple cryptographic scheme, each sensor node is assigned a set of keys and a unique key. Communication takes place through any one of the set of keys xoring with the unique key. Once the encryption is over, the decryption takes place through the unique key that is known to the gateway node. The major drawback in this scheme is that the security level is low i.e. when any two key is known the other key may be revealed which results in revealing the keys of that node.   5.2 Random Key Distribution  Since the security level is low in xoring of two keys, random distribution of keys is tried to enhance the security of the proposed method. In this random key distribution scheme, a set of keys is assigned to each sensor node. The communication takes place through any one of the set of keys. Once the hamming distance between any two nodes is found high the set of keys are randomly replaced and the new set of keys will be generated. Since all the keys are newly generated whenever the hamming distance is high the power consumption will be higher in this scheme and the security level is also enhanced since keys cannot be revealed by the reversing of xor operation.  5.3         Dynamic Key Assignment  The proposed dynamic key assignment takes the advantage of both the simple cryptography and random key distribution scheme to reduce the collusion of nodes. In this dynamic key management algorithm, each key combination can be represented in the form of bit strings of k 1s and m 0s, where k is the number of keys stored at each node and m is the number of rekey messages required. The Hamming distance between any two combinations is defined as the number of bits that the two key combinations differ in. Let d be the Hamming distance between a pair of key combinations.   The value of d is bounded by                                 2  d  2k        k  m 2  d  2m       m k 2  d  km     k  m  When two nodes collude, they both will know at least d keys, since d is the number of keys that they differ in. In addition, they will also know all the keys that are common to both nodes. The common keys are equal to k  d2. Thus, the number of keys known to the two colluding nodes as k  d2. This leads to the conclusion that the lower the Hamming distance the value of d fewer the total number of potentially revealed keys.   In this proposed dynamic key management, each sensor node is assigned a set of keys and a unique key as in the simple cryptography case. When the hamming distance between the two nodes is found high by the boundary condition, the unique key alone is changed instead of changing the set of keys and the communication takes place by using any one of the set of key xoring with the new unique key. This method provides enhanced security with less power consumption when compared to the other two schemes.   6. SIMULATION  The simulations were carried out in MATLAB 6.5. The number of nodes getting colluded with time has been analyzed for both the SHELL and the dynamic key assignment and it is observed that the proposed dynamic key assignment performs better when compared to the existing method SHELL.   6.1      Simulation Parameters              The simulation parameters that are considered for simulation of both dynamic key assignment and SHELL are listed below    i Transmission range                   55m ii  Deployment region                    850x500m iii Frequency of key renewals       5 x 102 sec iv Key size                   128 bits   By taking the above parameters, the simulation were performed  on a network of 2000 nodes deployed in an area of 850x500 meters size setting an transmission range of 55 meters. Each of the sensor nodes is assigned unique key and a set of keys. Once the keys are assigned, the keys of the neighboring nodes are verified whether the hamming distance is low based on the boundary condition given in section 5.3. If the hamming distance of any node is found to be high, the unique key of that node is dynamically changed and the communication takes place by xoring the new unique key with any one of the set of keys. The operation continues for the other nodes also.  The simulation were carried out and the results are obtained for the following cases  Case i    Number of nodes colluded with time  Case ii   Comparison of methods with static and   111International Journal of Scientific  Engineering Research Volume 2, Issue 5, May 2011                                                                                                                              ISSN 22295518  IJSER  2011 httpwww.ijser.org                      mobile nodes   6.2 Results and Discussion   6.2.1 Number of nodes colluded with time                 From the simulation results it is found that the number of nodes getting colluded by the dynamic key assignment scheme is reduced to a greater extent. The average number of nodes colluded is found out by varying the time and the results are plotted in Fig 3. From Fig 3 it is observed that as the number of nodes colluding with each other in the dynamic key assignment is reduced when compared to the other methods like simple cryptography, random key distribution and SHELL.  The dynamic key assignment out performs the simple cryptography and random key distribution scheme as expected since it is a combination of both the schemes. The random key distribution performs better than the simple cryptography which in turn performs better when compared to SHELL.     Fig. 3. Number of Nodes Colluded with Time        6.2.2 Comparison of methods with static and mobile               nodes  In this case, both the dynamic key assignment and the SHELL are compared with static and mobile nodes. The simulation setup is the same for static nodes and the mobile nodes are given mobility with a speed of 20 meter per sec with the same simulation set up. Again the number of nodes colluding with each other gets reduced in the dynamic key assignment. The number of nodes colluded when the nodes are static and mobile for both conventional and proposed scheme is obtained by simulation and plotted in fig 4. It is observed that when both the schemes are compared for static and mobile nodes, the number of colluded nodes for the mobile nodes is lesser and approaches nearly zero preventing the collusion of nodes when compared to the static nodes because the hamming distance remains the same when the nodes are static and it differs when the nodes are given mobility. Thus by preventing the collusion of nodes the dynamic key assignment provides enhanced security when compared to other existing dynamic key management schemes.  112International Journal of Scientific  Engineering Research Volume 2, Issue 5, May 2011                                                                                                                              ISSN 22295518  IJSER  2011 httpwww.ijser.org     Fig. 4. Comparison of methods with Static and Mobile Nodes  113International Journal of Scientific  Engineering Research Volume 2, Issue 5, May 2011                                                                                                                              ISSN 22295518  IJSER  2011 httpwww.ijser.org    7   CONCLUSION  The number of nodes getting colluded with each other in the dynamic key assignment scheme is greatly reduced when compared to the other dynamic key management schemes. From Figure 3, it is observed that the proposed dynamic key management performs far better than the SHELL and from Figure 4, it is observed that by providing mobility to the nodes the collusion can be prevented. Thus the proposed dynamic key assignment prevents the collusion of nodes and provides enhanced security to the cluster based sensor network.  REFERENCES  1  L. Eschenauer and V. Gligor, A Key Management Scheme for Distributed Sensor Networks, Proc. 9th ACM Conf. Comp. and Commun. Sec., Nov. 2002, pp. 4147.  2  W. Du et al., A Key Management Scheme for Wireless Sensor Networks Using Deployment Knowledge, Proc. IEEE INFOCOM 04, Mar. 2004.  3  D. Liu and P. Ning, Improving Key PreDistribution with Deployment Knowledge in Static Sensor Networks, ACM Trans. Sensor Networks, 2005, pp 20439.  4  G. Jolly et al., A LowEnergy Key Management Protocol for Wireless Sensor Networks, Proc. IEEE Symp. Comp. and Commun., June 2003, p. 335  5  M. Eltoweissy et al., Group Key Management Scheme for LargeScale Wireless Sensor Network, J. Ad Hoc Networks, Sept. 2005, pp. 796802.  6  M. Younis, K. Ghumman, and M. Eltoweissy, Location aware Combinatorial Key Management Scheme for Clustered Sensor Networks, to appear, IEEE Trans. Parallel and Distrib. Sys., 2006.  114International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518   IJSER  2011 httpwww.ijser.org  Speedy Deconvolution using Vedic Mathematics Rashmi K. Lomte Mrs.Rashmi R. Kulkarni, Prof.Bhaskar P.C   Abstract Deconvolution is a computationally intensive digital signal processing DSP function widely used in applications such as imaging, wireless communication, and seismology. In this paper deconvolution of two finite length sequences NXM, is implemented using direct method to reduce deconvolution processing time. Vedic multiplier is used to achieve high speed. Urdhava Triyakbhyam algorithm of ancient Indian Vedic Mathematics is utilized to improve its efficiency. For division operation nonrestoring algorithm is modified and used. The efficiency of the proposed convolution circuit is tested by embedding it on Spartan 3E FPGA. Simulation shows  that ,the circuit has a delay of 79.595 ns from input to output using 90nm process library. It also provides the necessary modularity, expandability, and regularity to form different deconvolutions for any number of bits. Index Terms Deconvolution, NonRestoring algorithm, Urdhva Tiryagbhyam          1 INTRODUCTION                                                                    HE concept of deconvolution is widely used in the techniques of signal processing and image processing. The concept of deconvolution has applications in reflection seismology, in reversing the optical distortion, to sharpen images etc. Faster additions, multiplications and divisions are of extreme importance in DSP for deconvolution. Speeding up deconvolution using a Hardware Description Language for design entry not only increases improves the level of abstraction, but also opens new possibilities for using programmable devices.  In this paper, a novel method for computing the linear deconvolution of two finite length sequences is used. Method is explained in detail in 1. This method is similar to computing longhand division and polynomial division. As a need of project, all required possible adders are studied. All these adders are synthesized using Xilinx9.2i. There delays and areas are compared. Adders which have highest speed and comparatively less area occupied, are selected for implementing deconvolution. Since 44 bit multiplier is need of this project, different 44 bit multipliers are studied and Urdhava Triyakbhyam algorithm which gives lowest delay among remaining all multipliers is used here. For division, different division algorithms are studied, by comparing drawbacks and advantages of each algorithm, Non restoring algorithm is modified according to need and then used.    This paper can be considered as extension of 2. where discrete linear convolution of two finite length sequences4 4 is implemented. That convolved output of 2. is input to this proposed design, impulse response of system  is known is  another input, this paper proposes design that carry out  high speed deconvolution and extracts input samples.    Paper is organized as follows section 2 gives brief introduction of novel method for deconvolution. Section 3 describes division algorithm. Section 4 discusses the Vedic mathematics and Urdhva Tiryagbhyam algorithm for multiplication. Section 5 presents selection of speedy adder. In section 6 design verification is given. Finally, the conclusion is obtained. 2 NOVEL METHOD FOR CALCULATING DECONVOLUTION     In general, the object of deconvolution is to find the solution of a convolution equation of the form f g  h                                                       1 Usually, h is some recorded signal, and  is some signal that wish to recover, but has been convolved with some other signal g before get  recorded. The function g might represent the transfer function of an instrument or a driving force that was applied to a physical system.If one know g or at least form of g,then one can perform deterministic deconvolution.     If the two sequences fn and  g  n  are causal, then the convolution sum is hn  k,                   n  0   2 Therefore, solving for fn given gn and yn results in fn   ,                     n  1   3 where     f0                                                         4 where solution requires that g0   0     This recursion can be carried out in a manner similar to long division. Lets take  example ,let hn   16  36  56  17  28  12  and gn   4  4  3  2  , solving for fn given gn and hn. The sequences are set up in a fashion similar to long division, as shown below, but where no carries are performed out of a column. T115International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org                           4         5          6       fn  4  4  3  2          16    36    56    47    28     12                              16    16    12     8                                 20    44     39   28                                 20    20     15   10                                         24     24   18    12                                         24     24   18    12                                                                  0 fig.1. Deconvolution by novel method    By observing figure 1. one can easily predict, for implementing speedy deconvolution by above method, divisor , multiplier  and adderto achieve subtraction in form of addition used in design must be speedy.    High speed division can be carried out by selecting proper division algorithm.Vedic multiplier is used to get speedy multiplication.  3 ALGORITHM SELECTION FOR DIVISION Division is most complex and time consuming operation for DSP.Basically division algorithm is classified as multiplicative and subtractive approaches. Multiplicative division algorithms do not compute the quotient directly, but use successive approximations to converge to the quotient. Normally, such algorithms only yield a quotient, but with an additional step the final remainder can be computed, if needed. Computations include several multiplications each iteration. The Goldschmidt division algorithm, binary search division algorithm, NewtonRaphson algorithm all these algorithms work by calculating estimates. Drawback of these algorithms is one has to compromise with exact result. Different errors need to be considered to calculate result. Though required number of iterations are less, steps required per iteration are more. The fastest implementation of a division function is a table lookup. A lookuptable implementation introduces a latency of just one clock cycle, but requires a tablesize that grows exponentially with the input data width. another method is shiftandsubtract division algorithm.The algorithm is based on the wellknown paperandpencil method of shifting the divisor from left to right of the dividend, and subtracting it when it is smaller then the remainder. Such an implementation scales linearly with the input data width, but requires a number of iterations equal to the input data width.    Hence here we concentrate ourselves to the subtractive approach. In the subtractive idea the algorithm is the same as the division methods that we were taught in the elementary school. Suppose that there are two ndigit numbers, X and D, which represent the dividend and divisor respectively. By the division operation we can find a ndigit quotient and a ndigit remainder denoted as Q and R respectively. The mathematical representations of X, D, Q, and R are as following .3.   r      D                                            5 Where j  0, 1, 2, , n1 is the iteration number.             R j is the partial remainder at iteration j.                      r is the radix number.     q  j 1 is the j1th digit of the quotient The final quotient is represented as Q       we use radix2, i.e., r2, for design. Therefore,equation 5 can be rewritten and represented in equation 6 as follows 3.               2      D                                          6 In the hardware design we have to check the subtraction at each step to decide the quotient in that digit. There are two ways to find the quotient of the current digit. One is the restoring method, and the other is the nonrestoring method.In the restoring approach, when the current partial remainder, R j1 , is positive, the current quotient bit is equal to 1. On the other hand, if the current partial remainder is less than 0, then the current quotient bit is set to 0, and then the partial remainder should be added with the divisor and restore back to the previous partial remainder, Rj and it is so called the restoring method. In the nonrestoring method,if the current partial remainder, R  j 1 , is positive,the current quotient bit is equal to 1. On the other hand, if the current partial remainder is less than 0, then the current quotient bit is set to 1, and at the next step we have to add the divisor to the current partial remainder, R  j 1 , to form the next partial remainder, R  j 2  . By this method, there is no need to add divisor to restore the previous partial remainder.However, the quotient in the nonrestoring scheme is represented in the signed bit digit format.Therefore, after we finish the division process, the nonrestoring method needs an additional step to convert the signed bit format to the binary number representation. Since we do not need to check the polarity of the partial remainder to do the restoring of the partial remainder. Therefore, the speed of the nonrestoring division algorithm is faster than the speed of the restoring division algorithm 3,4.     Here asper need of task,non restoring division algorithm is modified.Here dividend is 8 bit long number and divisor  is number represented by 4 bits .Hence quotient would have  maximum length of  4bit.     Table 1. Show delay and area utilized  by divisor prepared by using non restoring division algorithm.Xilinx 9.2i is used for synthesis and simulation.Family Spartan 3E90 nm process technology is selected,with speed grade 5. 116International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org   TABLE 1. SIMULATION OF 8BIT BY 4BIT DIVISION USING NON RESTORING ALGORITHM  Area Delay ns Slices LUTs IOBs 39 70 16 17.911 4 VEDIC MULTIPLIER A multiplier is one of the key hardware blocks in convolution system. With advances in technology, many researchers have tried and are trying to design multipliers which offer either of the following high speed, low power consumption, regularity of layout and hence less area or even combination of them in multiplier. . However area and speed are two conflicting constraints. So improving speed results always in larger areas. So here in this paper attempt is to find out the best trade off solution among the both of them. Depending upon the arrangement of the components, there are different types of multipliers available. A Particular multiplier architecture is chosen based on the application. Two most common multiplication algorithms followed in the digital hardware are array multiplication algorithm and Booth multiplication algorithm. The computation time taken by the array multiplier is comparatively less because the partial products are calculated independently in parallel. The delay associated with the array multiplier is the time taken by the signals to propagate through the gates that form the multiplication array. After comparison of 4 bit array multiplier and 4 bit Booth multiplier it is found that array multiplier is superior than Booth. 2. Among all these  multipliers, this paper proposes to use the multiplier based on an algorithm Urdhva Tiryagbhyam Vertical  Crosswise of ancient Indian Vedic Mathematics. Vedic mathematics is part of four Vedas books of wisdom. It is part of Sthapatya Veda book on civil engineering and architecture, which is an upaveda supplement of Atharva veda. His Holiness Jagadguru Shankaracharya Bharati Krishna Teerthaji Maharaja 18841960 comprised all this work together and gave its mathematical explanation while discussing it for various applications. Swamiji constructed 16 sutras formulae and 16 Upa sutras sub formulae after extensive research in Atharva Veda. Urdhva Tiryagbhyam Sutra is a general multiplication formula applicable to all cases of multiplication. It literally means Vertically and crosswise. It is based on a novel concept through which the generation of all partial products can be done and then, concurrent addition of these partial products can be done. Thus parallelism in generation of partial products and their summation is obtained using Urdhava Tiryagbhyam. The algorithm can be generalized for n x n bit number. Since the partial products and their sums are calculated in parallel, the multiplier is independent of the clock frequency of the processor. This design need 44 bit  multilplier.As per.1, 44 bit Array multiplier is fastest among Booth, Wallance tree etc. Here we proposed to use Vedic multiplier based on Urdhava Triyakbhyam algorithm as we found it is fastest of all multipliers. Comparison between Array and Vedic is given in table2 TABLE 2. SIMULATION OF 44 MULTIPLIERS FOR DELAY COMPARISION  Multiplier Vedic Array Spartan 3E 11.676 ns 13.773 ns Virtex 5 6.036 ns 6.2 ns . 5 ADDER SLECTION Choice of speedy and area saving adder is done by implementing and comparing 8 bit carry look ahead adder  performance with 8 bit ripple carry adder, below in table 3.It shows  delay and area utilized  by these  adders, for family Spartan 3E90 nm process technology and with speed grade 5. TABLE 3. SIMULATION OF DIFFERENT ADDERS FOR ADDITION OF TWO NUMBERS, EACH ONE IS 8 BIT LONG  Adder name Area Delay ns Slices LUTs IOBs Carry look ahead 9 15 25 12.025 Ripple carry 9 15 25 12.675  After observing results of comparisons, for two 8bit numbers addition carry look ahead adder is selected. 6 DESIGN VERIFICATION OF DECONVOLVER Verification is carried out by ISE simulator. Simulator results are shown in figure 2.RTL results are shown in figure 3, of another attached file named Results.We used these two arrays with samples First input array gn F h    A h   B h  9 h Second input array hn B4 h  F0 h  13Dh  1A0h  F9h  ADh  5Ah  Deconvolved output    C  8   7   A     117International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org    Area utilization summary is given below. Cell Usage      BELS                              605        LUT2                         27          LUT3                         142          LUT4                         397         MUXF5                      39  IO Buffers                        52         IBUF                         36                                OBUF                      16 7 CONCLUSION In this paper, we presented an optimized implementation of deconvolution. This particular model has the advantage of being fine tuned for signal processing. To accurately analyze our proposed system, we have coded our design using the VHDL hardware description language and have synthesized it for FPGA products using ISE. The proposed circuit uses less area and less power, and it takes 79.595 ns to complete on 90 nm process technology devices.Thus aim of high speed deconvolver implementation is achieved.  REFERENCES 1 John W. Pierre, A Novel Method for Calculating the Convolution Sum of Two Finite Length Sequences, IEEE transaction on education, VOL.39, NO. 1, 1996.  2 Khader Mohammad, Sos Agaian, Efficient FPGA implementation of convolution, Proceedings of the 2009 IEEE International Conference on Systems, Man, and Cybernetics,San Antonio, TX, USA  October 2009. 3 Koren,I. Computer Arithemetic Algorithms,PrenticeHall Inc1993 4 Hwang,K. Computer Arithmethmetic Principles,Arichitecture,and design,John Wiley  Sons.  5 Ramesh Pushpangadan, Vineeth Sukumaran, Rino Innocent, Dinesh Sasikumar, Vaisak Sundarv, High Speed Vedic Multiplier for Digital Signal Processors IETE, VOL.55,page 282286 118International Journal of Scientific  Engineering Research, Volume 2, Issue 5, May2011                                                                                   ISSN 22295518   IJSER  2011 httpwww.ijser.org  Different Approaches of Spectral Subtraction method for Enhancing the Speech Signal in Noisy Environments  Anuradha R. Fukane, Shashikant L. Sahare  AbstractEnhancement of speech signal degraded by additive background noise has received more attention over the past decade, due to wide range of applications and limitations of the available methods. Main objective of speech enhancement is to improve the perceptual aspects of speech such as overall quality, intelligibility and degree of listener fatigue. Among the all available methods the spectral subtraction algorithm is the historically one of the first algorithm, proposed for background noise reduction. The greatest asset of Spectral Subtraction Algorithm lies in its simplicity. The simple subtraction process comes at a price. More papers have been written describing variations of this algorithm that minimizes the shortcomings of the basic method than other algorithms. In this paper we present the review of basic spectral subtraction Algorithm, a short coming of basic spectral subtraction Algorithm, different modified approaches of Spectral Subtraction Algorithms such as Spectral Subtraction with over subtraction factor, Non linear Spectral Subtraction, Multiband Spectral Subtraction, Minimum mean square Error Spectral Subtraction, Selective Spectral Subtraction, Spectral Subtraction based on perceptual properties that minimizes the shortcomings of the basic method, then performance evaluation of various modified spectral subtraction Algorithms, and conclusion.  Index Terms speech enhancement additive noise  Spectral Subtraction intelligibility Discrete Fourier Transform, vad.        1 INTRODUCTION                                                                     peech signals from the uncontrolled environment may contain degradation components along with required speech components. The degradation components include background noise, speech from other speakers etc. Speech signal degraded by additive noise, this make the listening task difficult for a direct listener, gives poor performance in automatic speech processing tasks like speech recognition speaker identification, hearing aids, speech coders etc.  The degraded speech therefore needs to be processed for the enhancement of speech components. The aim of speech enhancement is to improve the quality and intelligibility of degraded speech signal. Main objective of speech enhancement is to improve the perceptual aspects of speech such as overall quality, intelligibility and degree of listener fatigue. Improving quality and intelligibility of speech signals reduces listeners fatigue improve the performance of hearing aids, cockpit communication, videoconferencing, speech coders and many other speech systems. Quality can be measured in terms of signal distortion but intelligibility and pleasantness are difficult to measure by any mathematical algorithm. Perceptual quality and intelligibility are two measures of speech signals and which are not corelated. In this study a speech signal enhancement using basic spec tral subtraction and modified versions of spectral subtraction methods such as  Spectral Subtraction with over subtraction, Non linear Spectral Subtraction, Multiband Spectral Subtraction, MMSE Spectral Subtraction, Selective Spectral Subtraction, Spectral Subtraction based on perceptual properties has been explained in detail with their performance evaluation.   2    METHODOLOGIES  2.1  Basic spectral subtraction algorithm  The speech enhancement algorithms based on theory from signal processing. The spectral  subtractive algorithm is historically one of the first algorithms proposed for noise reduction 4. Simple and easy to implement it is based on the principle that one can estimate and update the noise spectrum when speech signal is not present and subtract it from the noisy speech signal to obtain clean speech signal spectrum7. Assumption is noise is additive and its spectrum does not change with time, means noise is stationary or its slowly time varying signal. Whose spectrum does not change significantly between the updating periods. Let yn be the noise corrupted input speech signal, is composed of the clean speech signal   xn and the additive noise signal dn. In mathematical equation form one can   yn  xn dn                             1     S  Anuradha R. Fukane is currently pursuing masters degree program in signal processing in Electronics and Telecommunication Engg.branch in Cummins College of Engg. For Women Pune ,in Pune University, Maharastra,  India . Email anuraj110rediffmail.com  Shashikant L. Sahare is currently working as Asst.Professor in Cummins College of Engg. For Women in Pune University, Maharastra, India.  Emailshashikantsaharerediffmail.com 119International Journal of Scientific  Engineering Research, Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org  Many of speech enhancement algorithms operates in the Discrete Fourier Transform DFT domain 3 assume that the real and imaginary part of the clean speech DFT coefficients can be modeled by different speech enhancement algorithms. In Fourier domain, we can write yn as                Yw  xw Dw.                                 2  Yw can be expressed in terms of Magnitude and Phase as    Yw  Y w e j  y  Where Yw is the magnitude spectrum and  is the  phase spectra of the  corrupted noisy speech signal.Noise spectrum in terms of magnitude and phase spectra is                          Dw  Dw  e j  y  The Magnitude of noise spectrum Dw is unknown but can be replaced by its average value or estimated noise Dew  computed during non speech activity that is  during speech pauses. The noise phase is replaced by the noisy speech phase y that does not affect speech ineligibility 4. We can estimate the clean speech signal simply by subtracting noise spectrum from noisy speech spectrum in equation form              Xew  Yw  Dew  ejy            3  Where Xew is estimated clean speech signal. Many spectral subtractive algorithms are there depending on the parameters to be subtracted such as Magnitude spectral subtraction Power spectral subtraction, Autocorrelation subtraction. The estimation of clean speech Magnitude signal spectrum is             Xew  Yw  Dew  Similarly for Power spectrum subtraction is                        Xew2  Yw2  Dew2                                 4  The enhanced speech signal is finally obtained by computing the inverse Fourier Transform of the estimated clean speech Xew for magnitude. Spectrum subtractions and Xew2 for power spectrum substation subtraction, using the phase of the noisy speech signal. The more general version of the spectral subtraction algorithms is          Xe p  Yp  Dep                            5  Where P is the power exponent the general form of the spectral subtraction, when p1 yielding the magnitude spectral subtraction algorithm and p2 yielding the power spectral subtraction algorithm. The general form of the spectral subtraction algorithm is   shown in figure 1. 4              Figure1The general form of the spectral subtraction algorithm 4 2.2 Short comings of S. S. Algorithm   The subtraction process needs to be done carefully to avoid any speech distortion. If too little is subtracted than much of the interfering noise remains if too much is the subtracted then some speech information might be removed 1. It is clear that spectral subtraction method can lead to negative values, resulting from differences among the estimated noise and actual noise frame.  Simple solution is set the negative values to zero, to ensure a non negative magnitude spectrum.  This non linear processing of the negative values called negative rectification or halfwave rectification 4. This ensure a nonnegative magnitude spectrum given by equation 6   Xe  Y   De,  if Y  De                                else                    0                                                           6  This nonlinear processing of the negative values creates small, isolated peaks in the spectrum occurring at random frequency locations in each frame. Converted in the timedomain, these peaks sound like tones with frequencies that change randomly from frame to frame. That is, tones that are turned on and off at the analysis frame rate every 20 to 30 ms. This new type of noise introduced by the halfwave rectification process has been described as warbling and of tonal quality, and is commonly referred to in the literature as musical noise. Minor shortcoming of the spectral subtraction Algorithm is the use of noisy phase that produces a roughness in the quality of the synthesized speech 4. Estimating the phase of the clean speech is a difficult task and greatly increases the complexity of the enhancement algorithm.  The phases of the noise corrupted signal are not enhanced, because the presence of noise in the phase information does not contribute much to the degradation of speech quality 6. The distortion due to noisy phase information is not very significant compared to that of the Magnitude spectrum especially for high SNRs. Combating musical noise is much more critical than finding methods to preserve the original phase.  Due to that reason, much efforts has been fo120International Journal of Scientific  Engineering Research, Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org  cused on finding methods to reduce musical noise which are explained in next section  2.3 Spectral Subtraction with over subtraction   Modifications made to the original spectral subtraction method are subtracting an over estimate of the noise power spectrum and preventing the resultant spectrum from going below a preset minimum level spectral floor.This modifications lead to minimizing the perception of the narrow spectral peaks by decreasing the spectral excursions and thus lower the musical noise effect. Berouti 5 has taken a different approach that does not require access to future information. This Method consists of subtracting an overestimate of the noise power spectrum and presenting the resultant spectral components from going below a preset minimum spectral floor value. This algorithm is given in equation 7, where Xej denotes the enhanced spectrum estimated in frame i and De is the spectrum of the noise obtained during nonspeech activity  Xej  Yj De                                                      ifYj    De                                                           De       else       7  With   1 and 0    1 . Where  is over subtraction factor and  is the spectral floor parameter.  Parameter  controls the amount of residual noise and the amount of perceived Musical noise. If  is too small, the musical noise will became audible but the residual noise will be reduced .If  is too large, then the residual noise will be audible but the musical issues related to spectral subtraction reduces. Parameter  affects the amount of speech spectral distortion. If  is too large  then resulting signal will be severely distorted and intelligibility may suffer. If   is too small noise remains in enhanced speech signal. When   1, the subtraction can remove all of the broadband noise by eliminating most of wide peaks.  But the deep valleys surrounding the peaks still remain in the spectrum 1. The valleys between peaks are no longer deep when   0 compared to when   0 4 Berouti found that speech processed by equation 7 had less musical noise. Experimental results showed that for best noise reduction with the least amount of musical noise,  should be smaller for high SNR frames and large for low SNR frames.  The parameter  varies from frame to frame according to Burouti 5 as given below     o   320 SNR      5 dB  SNR   20dB   Where o is the desired value of  at 0 dB SNR is the short time SNR estimate in each frame.  It is an a posteriori estimate of the SNR computed based on the ratio of the noisy speech power to the estimated noise power.  Berouti 5 determine the optimum values of  and . For high noise levels SNR   5dB, the suggested  is in the range of 0.02 to 0.06 and for lower noise levels SNR  0dB,  in the range 0.005 to 0.02.  The parameter  suggested by Berouti 5 is in the range of 3 to 6. The influence of  also investigated by others  Martin4,15 suggest the range of  should lie between 1.3 and 2 for Low SNR conditions for high SNR conditions subtraction factor  less than one was suggested.  2.4 Nonlinear Spectral Subtraction NSS The NSS proposed by 8 Lockwood and Boudy.  NSS is basically a modification of the method suggested in 5 by making the over subtraction factor frequency dependent and the subtraction process nonlinear.  In case of NSS assumption is that noise does not affects all spectral components equally. Certain types of noise may affect the low frequency region of the spectrum more than high frequency region.  This suggests the use of a frequency dependent subtraction factor for different types of noise.  Due to frequency dependent subtraction factor, subtraction process becomes nonlinear.  Larger values are subtracted at frequencies with low SNR levels and smaller values are subtracted at frequencies with high SNR levels. The subtraction rule used in the NSS algorithm has the following form.  Xe   Y    N     if                                Y   N   De  else                Y                        8  Where  is the spectral floor set to 0.1 in 8 Y and De are the smoothed estimates of noisy speech and noise respectively,  is a frequency dependent subtraction factor and N is a nonlinear function of the noise spectrum  where  N  Max De                   9  The N term is obtained by computing the maximum of the noise magnitude spectra De over the part 40 frames  4.  The  given in 8 as    1r   p                   10  Where  is a scaling factor and P is the square root of the posteriori SNR estimate given as   P      Y   De             11  The NSS algorithm was successfully used in 8 as a preprocessor to enhance the performance of speech recognition systems in noise.  2.5 Multiband Spectral Subtraction MBSS  In MBSS approach 9,4 the speech spectrum is divided into N overlapping bands and spectral subtraction is per121International Journal of Scientific  Engineering Research, Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org  formed independently in each band. The processes of splitting the speech signal into different bands can be performed either in the time domain by using band pass filters or in the frequency domain by using appropriate windows. The estimate of the clean speech spectrum in the ith band is obtained by 9.  Xei k   Yi k   i i Di k        12                                 bi  k  ei  Where k  2pi k  N, k  0, 1 ... N  1 are the discrete frequencies Deik is the estimated noise power spectrum obtained during speech absent segment, i is the over subtraction factor of the ith band and i is an additional band. Subtraction factor can be individually set for each frequency band to customize the noise removal processor bi and ei are the beginning and ending frequency bins of the ith frequency band. The band specific over subtraction factor is a function of the segmented SNRi of the ith frequency band and is computed as follows 4                       4.75             SNRi  5      i      320 SNRi      5  SNRi  20                1             SNRi  20 The values for i are set to                 1              fi 1 KHz         i   2.5             1KHz  fi  Fs  2  2 KHz                1.5                          fi  Fs  2  2 KHz   Where fi is the upper frequency of the ith band and Fs is the sampling frequency in Hz. The main difference between the MB and the NSS algorithm is in the estimation of the over subtraction factors. The MB approach estimates one subtraction factor for each frequency band, whereas the NSS algorithm estimates one subtraction factor for each frequency bin 4 2.6 MMSE Spectral Subtraction Algorithm  Minimum Mean Square Error MMSE Spectral subtraction Algorithm is proposed by Sim 11. A method for optimally selecting the subtractive parameters in the mean error sense 17,18. Consider a general version of the spectral subtraction algorithm  X  P    p  Y P   p  De        13       Where p and p are the parameters of interest. P is the power exponent and De is the average noise spectrum obtained during non speech activity. The parameter p can be determined by minimizing the mean square error spectrum  ep   XpP  XeP                                                   14  Where Xp is the clean speech spectrum, assuring   an ideal spectral subtraction model and Xe is enhanced speech. Here assumption is that noisy speech spectrum consists of the sum of two independent spectra the XpP spectrum  and the true noise spectrum DeP .Where P is constant, considering P  1 and processing equation 13 by minimizing the mean  square error of the error spectrum giving equation 14 with respect to p and p, we get the following optimal subtractive parameters 4.  p   p  1   p                       15    p   p  1   p2                      16 Where       E Xp  E De                   17  2.7 Selective Spectral Subtraction Algorithm All previously mentioned methods treated all speech segments equally, making no distinction between voiced and unvoiced segments. Due to the spectral differences between vowels and consonants 4 several researchers have proposed algorithms that treated the voiced and unvoiced segment differently. The resulting spectral subtractive algorithms were therefore selective for different classes of speech sounds 4. The two band spectral subtraction algorithm was proposed in 13. The incoming speech frame was first classified into voiced or unvoiced by comparing the energy of the noisy speech to a threshold.  Voiced segments were then filtered into two bands, one above the determined cutoff frequency high pass speech and one below the determined cutoff frequency low pass speech.  Different algorithms were then used to enhance the low passed and high passed speech signals accordingly. The over subtraction algorithm was used for the low passed speech based on the short term FFT. The subtraction factor was set according to short term SNR as per 5. For high passed voiced speech as well as for unvoiced speech, the spectral subtraction algorithm was employed with a different spectral estimator 4. A dual excitation Model was proposed in 3 for speech enhancement.  In the proposed approach, speech was decomposed into two independent components voiced and unvoiced components. Voiced component analysis was performed first by extracting the fundamental frequency and the harmonic amplitudes. The noisy estimates of the harmonic amplitudes were adjusted according to some rule to account for any noise that might have leaked to the harmonics. Following that the unvoiced component spectrum was computed by subtracting the voiced spectrum from the noisy speech spectrum.  Then a two pass system, which included a modified Wiener Filter, was used to enhance the unvoiced spectrum.  Finally the enhanced speech consists of the sum of the enhanced voiced and unvoiced components.  Treating voiced and unvoiced segments differently can bring about substantial improvements in performance 4. The 122International Journal of Scientific  Engineering Research, Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org  major challenge with such algorithms is making accurate and reliable voiced, unvoiced decisions particularly at low SNR conditions. 2.8 Spectral Subtraction based on perceptual properties In the preceding methods, the subtractive parameters were computed experimentally, based on short term SNR levels 5 or obtained optimally in a mean square error sense 11.  No perceptual properties of the auditory system have been considered. An algorithm proposed by Virag 14 that incorporates psycho acoustical properties of speech signal, in the spectral subtraction process.  The main objective of this algorithm is to remove the residual noise perceptually inaudible and improve the intelligibilityof enhanced speech by taking into account the properties of the human auditory system 4.  Method proposed by Virag 14 was based on idea that, if the estimated masking threshold at a particular frequency is low, the residual noise level might be above.  The threshold and will therefore be audible.  The subtraction parameters should therefore attain their maximal values at that frequency. Similarly, if the masking threshold level is high at a certain frequency, the residual noise will be masked and will be inaudible. The subtraction parameters should attain their minimal values at that frequency. The subtraction parameters    are given as    Fa min, max, T                               18   Fb min, max, T    Where T was the masking threshold, min and max were set to 1 and 6 respectively and spectral floor constants min  max, were set to 0 and 0.02 respectively in 4. The Fa function had the following boundary conditions  Fa    amax                if    T  Tmin      amin                if    T  Tmax       19  Where Tmin and Tmax are the minimal and maximum values of masking thresholds estimated in each frame. Similarly the function Fb was computed using min and max as boundary conditions. The main advantage of Virags approach lies in the use of noise masking thresholds T rather than SNR levels for adjusting the parameters  and . The masking thresholds T provide a smoother evolution from frame to frame than the SNR. This algorithm requires accurate computation of the masking threshold. 3 PERFORMANCE OF SPECRAL SUBTRACTION ALGORITHMS The spectral subtraction algorithm was evaluated in many studies, primarily using objective measures such as SNR improvement and spectral distances and then subjective listening tests. The intelligibility and speech quality measures reflect the true performance of speech enhancement 4 algorithms in realistic scenarios.  Ideally, the SS algorithm should improve both intelligibility and quality of speech in noise. Results from the literature were mentioned as follows. Boll5 performed intelligibility and quality measurement tests using the Diagnostic Rhyme Test DRT. Result indicated that SS did not decrease speech intelligibility but improved speech quality particularly in the area of pleasantness and inconspicuousness of the background noise. Lim 4 evaluated the intelligibility of nonsense sentences in white noise at 5, 0, and 5dB SNR processed by a generalized SS algorithm eqa. No.5. the intelligibility of processed speech was evaluated for varies power exponents P ranging from P  0.25 to P  2. Results indicated that SS algorithm did not degrade speech intelligibility except when P  0.25. Kang and Fransen 4 evaluated the quality of noise processed by the SS algorithm and then fed to a 2400 bps LPC recorder. Here SS algorithm was used as a preprocessor to reduce the input noise level. The Diagnostic Acceptability Measure DAM test 19 was used to evaluate the speech quality of ten sets of noisy sentences, recorded actual military platforms containing helicopter, tank, and jeep noise results indicated that SS algorithm improved the quality of speech. The largest improvement in speech quality was noted for relatively stationary noise sources 4, 2. The NSS algorithm was successfully used in 8 as a preprocessor to enhance the performance of speech recognition systems in noisy environment. The performance of the multiband spectral subtraction algorithm 9 was evaluated by Hu Y. and Loizou 2, 19 using formal subjective listening tests conducted according to ITUT P.835 20. The ITU T P.835 methodology is designed to evaluate the speech quality along with three dimensions signal distortion, noise distortion and overall quality. Results indicated that the MBSS algorithm performed the best consistently across all noise conditions, 4 in terms of overall quality.  In terms of noise distortion the MBSS algorithms performed well, except in 5dB train and 10dB street conditions. The algorithm proposed by Virag was evaluated in 14 using objective measures and subjective tests, and found better quality than the NSS and standard SS algorithms. The low energy segments of speech are the first to be lost in the subtraction process particularly when over subtraction is used. Overall most studies confirmed that the SS algorithm improves speech quality but not speech intelligibility. 4 CONCLUSION  Various spectral subtraction algorithms proposed for speech enhancement were described in above sections.  These algorithms are computationally simple to implement as they involve a forward and an inverse Fourier transform.  The simple subtraction processing comes at a price. The subtraction of the noise spectra from the noisy 123International Journal of Scientific  Engineering Research, Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org  spectrum introduces a distortion in the signal known as Musical noise 4. We presented different techniques that mitigated the Musical noise distortion.  Different variations of spectral subtraction were developed over the years. The most common variation involved the use of an over subtraction factor that controlled to some amount of speech spectral distortion caused by subtraction process. Use of spectral floor parameter prevents the resultant spectral components from going below a preset minimum value. The spectral floor value controlled the amount of remaining residual noise and the amount of musical noise 4. Different methods were proposed for computing the over subtraction factor based on different criteria that included linear 5 and nonlinear functions 8 of the spectral SNR of individual frequency bins or bands 9 and psychoacoustic masking threshold 14. Evaluation of spectral subtractive algorithms revealed that these algorithms 4 improve speech quality and not affect much more on intelligibility of speech signals.    ACKNOWLEDGMENT Mrs. Anuradha R. Fukane wishes to thank Dr. Bhide S. D.  and Dr. Madhuri Khambete for their valuable guidance and support. REFERENCES 1 Yi Hu and Philipos C. Loizou, Subjective comparison and evaluation of speech enhancement algorithms   IEEE Trans. Speech Audio Proc.2007497 588601. 2 Gustafsson H., Nordhohm S, Claesson I2001 Spectral subtraction using reduced delay convolution and adaptive averaging . . IEEE. Trans. Speech Audio Process,98, 799805. 3 Kim W, Kang  S, and ko H.2000 Spectral subtraction based on phonetic dependancy and masking effects IEEE. Proc.vision image signal process, 1475,pp423427 4 Phillips C Loizou Speech enhancement theory and practice  1st ed. Boca Raton, FL. CRC, 2007.  Releases   Taylor  Francis 5 Berouti,M. Schwartz,R. and Makhoul,J.,Enhancement of Speech Corrupted by Acoustic Noise, Proc ICASSP 1979,  pp208211,. 6 Paliwal K. and Alsteris L.2005, On usefulness of STFT phase spectrum in human listening tests  Speech Commmun.452,153170 7 Boll,S.F.,Suppression of Acoustic Noise in Speech using Spectral Subtraction, IEEE Trans ASSP 272113120, April 1979 8 Lockwoord, P. and Boudy,J.,Experiments with a Nonlinear Spectral Subtractor NSS, Hidden Markov Models and the projection, for robust speech recognition in cars, Speech Communication, 11, pp215228, Elsevier 1992. 9 Kamath S. and Loizou P.2002 A multiband spectral subtraction methode for enhancing  speech currupted by colored noise Proc. IEEE Intl. Conf. Acoustics, Speech, Signal Processing  10 Hu Y., Bhatnager M. Loizou P.2001 A crosscorellation technique for enhancing speech currupted with correlated noise . Proc. IEEE Intl. Conf. Acoustics, Speech, Signal Processing1.pp 673676  11 Sim B, Tong Y, chang J., Tan C.1998 A parametric formulation of the generalized spectral subtraction method  IEEE. Trans. Speech Audio Process,64, 328337 12 Hardwick J., Yoo C. and Lim J 1998 speech enhancement using dual exitation model Proc. IEEE Intl. Conf. Acoustics, Speech, Signal Processing 2, pp 367370 13 He C, and Zweig G. 1999 Adaptive two band spectral subtraction with multiwindow spectral estmation Proc. IEEE Intl. Conf. Acoustics, Speech, Signal Processing ,2, pp 793796 14 Virag, N., 1999. Single channel speech enhancement based on masking properties of the human auditory system. IEEE. Trans. Speech Audio Process,73, 126137.. 15 Lebart K, Boucher J M,2001 A New method based on spectral subtraction for speech enhancement Acta acustica, Acustica vol. 87 pp359366.. 16 R. Martin, Spectral Subtraction Based on Minimum Statistics, in Proc. Euro. Signal Processing Conf. EUSIPCO, pp. 11821185, 1994 17 Martin, R2002 Speech Enhancement Using MMSE Short Time Spectral Estimation with Gamma Distributed Speech Priors,in Proc. IEEE Intl. Conf. Acoustics, Speech, Signal Processing ICASSP, vol. I, pp. 253256, 2002 18 Epraim Y. and malah D Speech Enhancement Using minimum mean squre error shorttime spectral amplitude estmator IEEE, Trans. on Audio, Speech, signal pross.vol 64pp 328337 19 Yi Hu and Philipos C. Loizou, Senior Member, IEEE Evaluation of Objective Quality Measures for Speech Enhancement  IEEE, Trans. on Audio, Speech, and  Language pross.vol 16, 2008 20 ITUT2003 subjective test Methodalogy for evaluating speech communication system that include noise supression slgorithm. ITUT recommendation p.835  124International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                              ISSN 22295518  IJSER  2011 httpwww.ijser.org  Economic Power Dispatch Problem using Artificial Immune System R.Behera, B.B.Pati, B.P.Panigrahi  Abstract This paper presents the genetic algorithm approach to adaptive optimal economic dispatch of Electrical Power Systems. Artificial Immune System Algorithms, also termed as the machine learning approach to Artificial Intelligence, are powerful stochastic optimization techniques with potential features of random search, hill climbing, statistical sampling and competition. Artificial immune system algorithmic approach to power system optimization, as reported here for a case of economic power dispatch, consists essentially of minimizing the objective function while gradually satisfying the constraint relations. The unique problem solving strategy of the genetic algorithm and their suitability for power system optimization is described. The advantages of the artificial immune system algorithm approach in terms of problem reduction, flexibility and solution methodology are also discussed. The suitability of the proposed approach is described for the case of a six generator thirty bus IEEE system.  Index TermsAntibody, Antigen, Cloning, Hypermutation, Optimization, Softcomputing                                                                     1   INTRODUCTION HE definition of economic dispatch provided in EP Act section 1234 is The operation of generation facilities to produce energy at the lowest cost to reliably serve consumers, recognizing any operational limits of generation and transmission facilities. Most electric power systems dispatch their own generating units and their own purchased power in a way that may be said to meet this definition. There are two fundamental components to economic dispatch I Planning for tomorrows dispatch II Dispatching the power system today I Planning for tomorrows dispatch a. Scheduling generating units for each hour of the next days dispatch b. Based on forecast load for the next day c. Select generating units to be running and available for dispatch the next day operating day d. Recognize each generating units operating limit, including its Ramp rate how quickly the generators output can be changed, Maximum and minimum generation levels, Minimum amount of time the generator must run,    Mr. R.Behera Currently persuing his Phd under  Sambalpur University Dr.B.B. Pati Professor VSSUT Burla Dr. B.P.Panigrahi Professor IGIT Sarang   Minimum amount of time the generator must stay off once turned off, Recognize generating unit characteristics  e.   Cost of generation, which depends on its efficiency heat rate, variable operating costs fuel and nonfuel, variable cost of environmental compliance, startup costs, next day scheduling is typically performed by a generation group or an independent market operator, reliability assessment. Analyze forecasted load and transmission conditions in the area to ensure that scheduled generation dispatch can meet load reliability. If the scheduled dispatch is not feasible within the limits of the transmission system, revise it. The reliability assessment is typically performed by a transmission operations group. II Dispatching the power system today  a Monitor load, generation and interchange importsexports to ensure balance of supply and load. b Monitor and maintain system frequency at 5060 Hz during dispatch according to NERC standards, using Automatic Generation Control AGC to change generation dispatch as needed. c  Monitor hourly dispatch schedules to ensure that dispatch for the next hour will be in balance. d  Monitor flows on transmission system. T125International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                              ISSN 22295518  IJSER  2011 httpwww.ijser.org  e  Keep transmission flows within reliability limits. f  Keep voltage levels within reliability ranges. g Take corrective action, when needed, by Limiting new power flow schedules, curtailing existing power flow schedules, changing the dispatch or shedding load. h This monitoring is typically performed by the transmission operator area factors limiting the effectiveness of dispatch in minimizing customer costs. Geographic area included the size of the geographic region over which the dispatch occurs affects the level of costs that is, which generation resources and which transmission facilities are considered in planning and economic dispatch. Generation resources included which generation resources in the area are included in the planning and economic dispatch, and whether they are included in the same manner, affects the level of costs. Transmission facilities included what transmission facilities are included in the planning and economic dispatch, and how the reliability security limits of the transmission facilities are incorporated into the economic dispatch. Implementation factors limiting effectiveness of dispatch in minimizing customer costs. Performing an economic dispatch more frequently e.g., 5 or 15 minutes rather than each hour affects the level of costs. Generation operators, transmission owners, and load serving entities must provide accurate and current information to those performing the planning and dispatch functions.  Those performing planning and dispatch must provide accurate and current dispatch instructions to generation operators, transmission operators and load serving entities. Inadequate or incomplete communications affects the level of costs of the economic dispatch. Reliable and secure computer software is essential for rapidly responding to system changes to maintain power system reliability, while selecting the lowest cost generators to dispatch. Obsolete software affects the level of costs achieved by the economic dispatch. Where there are multiple, independently performed, dispatches in a region, the effectiveness of coordination agreements and their implementation affect the level of costs of the economic dispatch.   Masashi Yoshimi Technical, Swamp, K. S. Yoshio Izui 1 had presented the genetic algorithm approach to adaptive optimal economic dispatch of electrical power systems. The advantages of the genetic algorithmic approach in terms of problem reduction, flexibility        and solution methodology are also discussed.  Mehrdad Salami and Greg Cain 2 had presented the application of a hardware genetic algorithm processor to handle the economic power dispatch problem. We have analyzed a variety of configurations including varying the string word length and the introduction of multiple processor configurations. Results show that multiple genetic algorithm processor configurations work better than other configurations with less complexity. It is possible to apply multiple configurations to larger numbers of generators in the problem.   Leandro Nunes de Castro  Fernando J. Von Zuben 3 had presented The clonal selection algorithm is used by the natural immune system to define the basic features of an immune response to an antigenic stimulus. It establishes the idea that only those cells that recognize the antigens are selected to proliferate. The selected cells are subject to an affinity maturation process, which improves their affinity to the selective antigens. In this paper, we propose a powerful computational implementation of the clonal selection principle that explicitly takes into account the affinity maturation of the immune response. The algorithm is shown to be an evolutionary strategy capable of solving complex machine learning tasks, like pattern recognition and multimodal optimization.   Leandro Nunes de Castro 4 had presented review the general algorithms of immune, swarm and evolutionary systems.  Leandro N. de Castro and Fernando J. Von Zuben 5 had presented the clonal selection principle used to explain the basic features of an adaptive immune response to an antigenic stimulus.  K Selvi, Dr N Ramaraj,  S P Umayal 6 had presented a genetic algorithm GA based effective method for the optimal scheduling of thermal generation incorporating the uncertainties in the 126International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                              ISSN 22295518  IJSER  2011 httpwww.ijser.org  system production cost data. The algorithm gives fairly accurate results. The effectiveness of the method has been demonstrated by analyzing sample systems and the results are presented.  Titik Khawa Abdul Rahman, Zuhaila Mal Yasin arid Wan Norani W.Abdullah 7 had presented Artificial Immune System based optimization approach for solving the economic dispatch problem in a power system. Economic Dispatch determines the electrical power to be generated by the committed generating units in a power system so that the generation cost is minimized, while satisfying the load demand simultaneously. The developed Artificial Immune System optimization technique used the total generation cost as the objective function and represented as the affinity measure. Though genetic evolution, the antibodies with high affinity measure are produced and become the solution .The simulation result reveal that the developed technique is easy to implement and converged within an acceptable execution time and highly optimal solution for economic dispatch with minimum generation cost can be achieved .The result also confirms that AIS based optimization technique can be a useful tool for solving optimization solution in economic dispatch problem ,which involves a large number of generating units and at the same time to comply with a large number of constraints  . 2  ECONOMIC LOAD DISPATCH The major component of generator operating cost is the fuel inputhour, while maintenance contributes only to a small extent. The fuel cost is meaningful in case of thermal and nuclear stations, but for hydro stations where the energy storage is apparently free the operating cost as such is not meaningful. Here we will concentrate on fuel fired stations. The input Output curve of a unit can be expressed in a million Kcalories per hour or directly in terms of Rshr vs. output in MW. The cost curve can be determined experimentally.  A typical curve is shown in Fig.2.1 where MW minis the minimum loading limit below which it is uneconomical or may be technically infeasible to operate the unit and MW max is the maximum output limit. The inputoutput curve has discontinuities steam valve openings which have not been indicated in the figure. By fitting a suitable degree polynomial, an analytical expression for operating cost can be written as CiPGi Rshr at out put  PGi Where the suffix i stand for the unit number. It generally suffices to fit a second degree polynomial that is   Ci 12 ai PGi2biPGidi Rshour                                                                                     1 The slope of the cost curve, i.e.  dCidPGi is called the incremental fuel cost IC and is expressed in units of RsMWh .A typical plot of incremental fuel cost versus power output is shown in Fig.2.1 .If the cost curve is approximated as a quadratic as in Eq.1, we have   ICi  aiPGi  bi                                                                                                                               2 i.e., a linear relationship. For better accuracy incremental fuel cost may be expressed by a number of short line segments. Alternatively, we can fit a polynomial of suitable degree to present  IC curve in inverse form PGi  i   iICi  iICi2                                                                                          3 Let us assume that it is known a priori which generators have to run to meet a particular load demand on the station. Obviously PGi,max   PD                                                                                                                     4 Where PGi,max  is the rated real power capacity of the ith  generator and PD is the total power demand on the station. Further, the load on each generator is to be constrained within the lower and upper limits, i.e.   127International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                              ISSN 22295518  IJSER  2011 httpwww.ijser.org   PGi,minPGiPGi,max,i1,2,3,,k                                                                                    5   We also require that                                           PGi,max   PD                                                                                                            6 by a proper margin i.e. eq.6 must be strict inequality. Since the operating cost is insensitive to reactive loading of a generator, the manner in which the reactive load of the stations shared among various online generators does not affect the operating economy. So the optimal manner in which the load demand PD must be shared by the generators on the bus by minimizing the operating cost i,e. C  CiPGi                                                                                                                       7 Under the inequality constraint of meeting the load demand i.e.  ki 1PGi,max  PD  0                                                                                                       8 Where k is the number generators on the bus The objective is to minimize the overall cost of generation  C ki 1CiPGi                                                                                               9 At anytime under equality constraint of meeting the load demand with transmission loss, i.e.   ki 1PGi    PD  PL                                                                                                       10 Where  PL km 1  kn 1PGmBmnPGn                                                                                  11 Where PGm, PGn   real power generation at m, nth plants Bmn  loss coefficients which are constraints under certain assumed operating conditions  3   GENETIC ALGORITHIMGA Genetic algorithms, first introduced by John Holland in the early  seventies, are becoming an important tool in machine learning and function optimization. The metaphor underlying GA is natural selection. To solve a learning task, a design, or an optimization problem, the GA maintains a population of chromosomes or bit strings and probabilistically modifies the population seeking a nearoptimal solution to the given task. Beneficial changes to the parents are combined in their offspring. The GA adapts itself to the problem being solved through syntactic operations on the bit strings. GA has been  tried on NPhard combinatorial optimization problems and may provide a more advantageous approach to machine learning  problems than neuromorphic networks and simulated annealing. The genetic algorithm consists of a string representation of  nodes in the search space, a set of genetic operators for generating new search nodes, a fitness function to evaluate the  search nodes and a stochastic assignment to control the genetic  operators. The stepwise procedure of GA for the optimization of generation cost can be outlined as follows i.  Initialization Random generation of the initial population of the search nodes. ii.   Evaluation Calculation of the fitness value of each node according to fitness function. iii. Evaluation Evaluation of strings or chromosomes of a population. iv.   Selection Selection of reproducing sets from the population based on relative fitness values. v.  Reproduction Generation of new strings from each reproducing set using various reproductive strategies. Replacing some or all of the original  vi. Population with new strings. The possible reproductive strategies are combinations of vii. Reproduction Generation of identical copies of some or all of the strings in the reproduction set. viii. Mating Construction of a new string by concatenating substrings chosen from members of the reproductive set. ix. Mutation Substitution of new symbols for selected positions in the new string.  4    ARTIFICIAL IMMUNE SYSTEMAIS The immune system IS is a complex of cells, molecules and organs that represent an identification mechanism capable of perceiving and combating dysfunction from our own cells infectious self and the action of exogenous infectious microorganisms infectious nonself. The interaction among the IS and several other systems and organs allows the regulation of the body, guaranteeing its stable functioning. Without the immune system, death from infection would be inevitable. Its cells and molecules maintain 128International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                              ISSN 22295518  IJSER  2011 httpwww.ijser.org  constant surveillance for infecting organisms. They recognize an almost limitless variety of infectious foreign cells and substances, known as nonself elements, distinguishing them from those native noninfectious cells, known as self molecules .When a pathogen infectious foreign element enters the body, it is detected and mobilized for elimination. The AIS can be defined as a computational system based upon metaphors of the biological immune system. The immune engineering IE is a metasynthesis process that uses the information contained in the problem itself to define the solution tool to a given problem, and then apply it to obtain the problem solution. It is not our intention to pose a strict limit between the AIS and the IE. Instead, we intend to make use of all immunological inspired phenomena and algorithm in order to solve complex problems. The topics involved in the definition and development of the artificial immune systems cover mainly a Hybrid structures and algorithms that take into account immunelike mechanisms. b Computational algorithms based on immunological principles, like distributed processing, clonal selection algorithms, and immune network theory. c Immunitybased optimization, learning, selforganization, artificial life, cognitive models, multiagent systems, design and scheduling, pattern recognition and anomaly detection. d Immune engineering tools. Potential applications of the artificial immune systems can be listed but are not limited to  Pattern recognition, function approximation and optimization, anomaly detection, computer and network security, generation of diversity and noise tolerance. The stepwise procedure of AIS for the optimization of generation cost can be outlined as follows i. Read the data which includes ai, bi,ci, maximum and minimum limits of generation of power of each generator and population size etc. ii. Generate random binary string iii. Decode them to actual value iv. Insert them in population pool v. Check for the satisfaction of constraints of the objective function if yes go to vi else go to i. vi. Evaluate fitness of each set of generation to meet the demand using      Where , n  number of generators. vii. Select the antigen and antibody from the fitness values viii. Calculate the Euclidean distance between antibody and antigen using Liii agabD12  ix. If  D is more select them for hyper mutation else simple mutation  by cloning the antibody. x. Enter the cloned population in new population pool. xi. Check for the satisfaction of constraints of the objective function. xii. Check for the convergence else go to clonal proliferation.  5   RESULTS AND DISCUSSION IEEE data of 6 generators 30 bus system TableI Generator Data Generator a b c Pmax Pmin G1 0.0037     2.0000 0 200 50 G2 0.0175     1.7500     0 80 20 G3 0.0625     1.0000     0 50 15 G4 0.0083     3.2500     0 35 10 G5 0.0250     3.0000     0 30 10 G6 0.0250 3.0000 0 40 12  6    REFERENCES 1  Masashi Yoshimi Technical, Swamp, K. S.Yoshio Izui , optimal economic power dispatch using genetic algorithms, IEEE  tran1993pp157162. 2 Mehrdad Salami and Greg Cain, Multiple Genetic Algorithm Processor for the Economic Bower Dispatch Problem ,Victoria University of 129International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                              ISSN 22295518  IJSER  2011 httpwww.ijser.org  Technology, Australia Conference Publication No. 41 4,O IEE, 1995. 3  Leandro Nunes de Castro  Fernando J. Von Zuben , The Clonal Selection Algorithm with Engineering Applications.In Workshop Proceedings of GECCO00, pp. 3637, Workshop on Artificial Immune Systems and Their Applications, Las Vegas, USA, July 2000. 4  Leandro Nunes de Castro, immune, swarm, and evolutionary algorithms part I basic models, proc. Of the ICONIP conference international conference on neural Information Processing, Workshop on Artificial Immune Systems, vol. 3, pp. 14641468, Singapore 1822 November, 2002. 5  Leandro N. de Castro and Fernando J. Von Zuben , Learning and Optimization Using the Clonal Selection Principle, IEEE Transactions on Evolutionary Computation, Special Issue on Artificial Immune Systems, vol. 6, n. 3, pp. 239251, 2002. 6  L. N. de Castro and J. Timmis , Artificial Immune Systems A Novel Paradigm to Pattern Recognition SOCO2002, University of Paisley, UK, pp. 6784, 2002. 7  Titik Khawa Abdul Rahman, Zuhaila Mal Yasin ArtificialImmuneBased  For Solving Economic Dispatch In Power System National Power  Energy Conference PECon 2004 Proceedings, Kuala Lumpur, Malaysia. 8  J. Tippayachai, W. Ongsakul and I. Nganiroo, Parallel Micro Genetic Algorithm for Constrained dispatch IEEE Trans.,Power System, vol. 17, pp. 790  797, Aug 2002. 9 J. G. Damousis, A.G. . Bakirtzis and P.S. Dokopoulos. NetworkConstrained Economic Dispatch Using RealCodcd Gcnetic Algorithm. IEEE Trans Power System,vol. 18, pp. I98  205,Feb 2003. 10  U. N. S. Rahimullah, E.I. Ramlan and T.K.A. Rahman. Evolutionary Approach for Solving Economic Dispatch in Power System Proceedings of the IEEEPES National Power Engineering, voI.1, pp. 32  36, Dec 2003. 11 D.Dasgupta and N.A.Okine ImmunityBased Systems A survey. in proceeding of International conference on systems, Man and Cybernetics Vol1,pp369374,Oct 1997.   .       130International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518   IJSER  2011 httpwww.ijser.org  Mining knowledge using Decision Tree Algorithm Mrs. Swati .V. Kulkarni   Abstract Industry is experiencing more and more competitions in recent years. The battle is over their most valuable customers. With massive industry deregulation across the world, each customer is facing an evergrowing number of choices in telecommunications and financial services. As a result, an increasing number of customers are switching from one service provider to another. This phenomenon is called customer churning or attrition, which is a major problem for these companies and makes it hard for them to stay profitable. The data sets are often cost sensitive and unbalanced. If we predict a valuable customer who will be an attritor as loyal, the cost is usually higher than the case when we classify a loyal customer as an attritor. Similarly, in direct marketing, it costs more to classify a willing customer as a reluctant one. Such information is usually given by a cost matrix, where the objective is to minimize the total cost. In addition, a CRM data set is often unbalanced the most valuable customers who actually churn can be only a small fraction of the customers who stay. The main focus is on the output of decision tree algorithms as the input to post processing algorithms. This algorithm relies on not only a prediction, but also a probability estimation of the classification, such as the probability of being loyal. Such information is available from decision trees.  Index Terms Attritor, Churning, CRM, cost matrix, decision tree, post processing algorithm, ROC curve.          1 INTRODUCTION                                                                 xtensive research in data mining1 has been done on discovering distributional knowledge about the underlying data. Models such as Bayesian models, decision trees, support vector machines, and association rules have been applied to various industrial applications such as customer relationship management CRM 2. Despite such phenomenal success, most of these techniques stop short of the final objective of data mining, which is to maximize the profit while reducing the costs, relying on such post processing techniques as visualization and interestingness ranking. While these techniques are essential to move the data mining result to the eventual applications, they nevertheless require a great deal of human manual work by experts. Often, in industrial practice, one needs to walk an extra mile to automatically extract the real nuggets of knowledge, the actions, in order to maximize the final objective functions. In this paper, a novel post processing technique is presented to extract actionable knowledge from decision trees. To illustrate my motivation, customer relationship management CRM is considered, in particular, the Mobile communications industry is taken as an example 3. This industry is experiencing more and more competitions in recent years. With massive industry deregulation across the world, each customer is facing an evergrowing number of choices in communications and financial services. As a result, an increasing number of customers are switching from one service provider to another. This phenomenon is called customer churning or attrition, which is a major problem for these companies and makes it hard for them to stay profitable. In addition, a CRM data set is often unbalanced the most valuable customers who actually churn can be only a small fraction of the customers who stay. In the past, many researchers have tackled the direct marketing problem as a classification problem, where the costsensitive and unbalanced nature of the problem is taken into account. In management and marketing sciences, stochastic models are used to describe the response behavior of customers. In the data mining area, a main approach is to rank the customers by the estimated likelihood to respond to direct marketing actions and compare the rankings using a lift chart or the area under curve measure from the ROC curve. Like most data mining algorithms today, a common problem in current applications of data mining in intelligent CRM is that people tend to focus on, and be satisfied with, building up the models and interpreting them, but not to use them to get profit explicitly.  In this paper, a novel algorithm for post processing decision trees is presented to obtain actions that are associated with attributevalue changes, in order to maximize the profitbased objective functions. This allows a large number of candidate actions to be considered, complicating the computation. More specifically, in this paper, two broad cases are considered. One case corresponds to the unlimited resource situation, which is only an approximaE131International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org  tion to the realworld situations, although it allows a clear introduction to the problem. Another more realistic case is the limitedresource situation, where the actions must be restricted to be below a certain cost level. In both cases, the aim is to maximize the expected net profit of all the customers as well as the industry. It can be shown that finding the optimal solution for the limited resource problem and designing a greedy heuristic algorithm to solve it efficiently is computationally hard 4. An important contribution of the paper is that it integrates data mining and decision making together, such that the discovery of actions is guided by the result of data mining algorithms decision trees in this case5. An  approach is  considered as a new step in this direction, which is to discover action sets from the attribute value changes in a non sequential data set through optimization. The rest of the paper is organized as follows  First a base algorithm is presented for finding unrestricted actions in Section 2. Then formulate two versions of actionset extraction problems, and show that finding the optimal solution for the problems is computationally difficult in the limited resources case Section 3. then greedy algorithms are efficient while achieving results very close to the optimal ones obtained by the exhaustive search which is exponential in time complexity. A case study for Mobile handset manufacturing and selling company is discussed in section 4. Conclusions and future work are presented in section 5.   This is the unlimitedresource case. The data set consists of descriptive attributes and a class attribute. For simplicity, discretevalue problem is considered in which the class values are discrete values. Some of the values under the class attribute are more desirable than others. For example, in the banking application, the loyal status of a customer stay is more desirable than not stay. The overall process of the algorithm can be briefly described in the following four steps  1. Import customer data with data collection, data Cleaning, data preprocessing, and so on. 2. Build customer profiles using an improved decision tree learning algorithm from the training data. In this case, a decision tree is built from the training data to predict if a customer is in the desired status or not. One improvement in the decision tree building is to use the area under the curve AUC of the ROC curve to evaluate probability estimation instead of the accuracy. Another improvement is to use Laplace correction to avoid extreme probability values.  3. Search for optimal actions for each customer see Section 2 for details.  4. Produce reports for domain experts to review the actions and selectively deploy the actions.  In the next section, we will discuss the leafnode Search algorithm used in Step 3 search for optimal actions in detail.   2   LEAFNODE SEARCH IN THE UNLIMITED RESOURCE CASE  First consider the simpler case of unlimited resources where the case serves to introduce computational problem in an intuitive manner. The leafnode search algorithm searches for optimal actions to transfer each leaf node to another leaf node with a higher probability of being in a more desirable class.                               Fig.  1   An example of customer profile.  After a customer profile is built, the resulting decision tree can be used to classify, and more importantly, provide the probability of customers in the desired status such as being loyal or highspending. When a customer, who can be either a training example used to build the decision tree or an unseen testing example, falls into a particular leaf node with a certain probability of being in the desired status, the algorithm tries to move the customer into other leaves with higher probabilities of being in the desired status. The probability gain can then be converted into an expected gross profit. However, moving a customer from one leaf to another means some attribute values of the customer must be changed. This change, in which an attribute As value is transformed from v1 to v2, corresponds to an action. These actions incur costs. The cost of all changeable attributes is defined in a cost matrix see Section 2.3 by a domain expert. The leafnode search algorithm searches all leaves in the tree 132International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org  so that for every leaf node, a best destination leaf node is found to move the customer to. The collection of moves is required to maximize the net profit, which equals the gross profit minus the cost of the corresponding actions. Based on a domainspecific cost matrix Section 2.3 for actions, we define the net profit of an action to be as follows  P Net  PE  X P gain  cost I  Where P Net denotes the net profit, PE denotes the total profit of the customer in the desired status, P gain denotes the probability gain, and cost I denotes the cost of each action involved.  In Section 3.1, this definition is extended to a formal definition of the computational problem. The leafnode search algorithm for searching the best actions can thus be described as follows Algorithm leafnode search 1. For each customer x, do 2. Let S be the source leaf node in which x falls into 3. Let D be a destination leaf node for x the max net profit P net 4. Output S,D, P net.  To illustrate, consider an example shown in Fig. 1, which represents an overly simplified, hypothetical decision tree as the customer profile of loyal customers built from a bank. The tree has five leaf nodes A, B, C, D, and E, each with a probability of customers being loyal. The probability of attritors is simply 1 minus this probability. Consider a customer Jack whos record states that the Service  Low service level is low, Sex  M male, and Rate  L mortgage rate is low. The customer is classified by the decision tree. It can be seen that Jack falls into the leaf node B, which predicts that Jack will have only a 20 percent chance of being loyal or Jack will have an 80 percent chance to churn in the future. The algorithm will now search through all other leaves A, C, D, and E in the decision tree to see if Jack can be replaced into a best leaf with the highest net profit.  1. Consider leaf A. It does have a higher probability of being loyal 90 percent, but the cost of action would be very high Jack should be changed to female, so the net profit is a negative infinity.  2. Consider leaf C. It has a lower probability of being loyal, so the net profit must be negative, and we can safely skip it. 3. Consider leaf D. There is a probability gain of 60 percent 80 percent  20 percent if Jack falls into D. The action needed is to change Service from L low to H high. Assume that the cost of such a change is 200 given by the bank. If the bank can make a total profit of 1,000 from Jack when he is 100 percent loyal, then this probability gain 60 percent is converted into 600 1000 X 0.6 of the expected gross profit. Therefore, the net profit would be 400 600 200. 4. Consider leaf E. The probability gain is 30 percent 50 percent  20 percent, which transfers to 300 of the expected gross profit. Assume that the cost of the actions change Service from L to H and change Rate from L to H is 250, then the net profit of moving Jack from B to E is 50 300  250. Clearly, the node with the maximal net profit for Jack is D, with suggested action of changing Service from L to H.   2.1  Cost Matrix In the discussion so far, we assume that attributevalue changes will incur costs. These costs can only be determined by domain knowledge and domain experts. For each attribute used in the decision tree, a cost matrix is used to represent such costs. In many applications, the values of  many attributes, such as sex, address, number of children, cannot be changed with any reasonable amount of money. Those attributes are called hard attributes. For hard attributes, users must assign a very large number to every entry in the cost matrix. If, on the other hand, some value changes are possible with reasonable costs, then those attributes such as the Service level, interest rate, and promotion packages are called soft attributes. Note that the cost matrix needs not to be symmetric. One can assign 200 as the cost of changing service level from low to high, but infinity a very large number as the cost from high to low, if the bank does not want to degrade service levels of customers as an action. For continuous attributes, such as interest rates that can be varied within a certain range, the numerical ranges can be discretized first using a number of techniques for feature transformation.   3  POSTPROCESSING DECISION TREES           THE LIMITED RESOURCE CASE 3.1   A Formal Definition of BSP    Previous case considered each leaf node of the decision tree to be a separate customer group. For each such customer group, we were free to design actions to act on it in order to increase the net profit. However, in practice, a company may be limited in its resources. The limitedresource problem can be formulated as a precise computational problem. Consider a decision tree DT with a number of source leaf nodes that correspond to customer segments to be converted and a number of candidate destination leaf nodes, which correspond to the segments we wish customers to fall in. Formally, the bounded segmentation problem BSP is defined as follows  Given 1. a decision tree DT built from the training examples, with a collection S of m source leaf nodes and a collection D of n destination leaf nodes   2. a prespecified constant k k  m, where m is the 133International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org  total number of source leaf nodes, 3. a cost matrix  Cattri,u,v,i1,2,.. which specifies the cost of converting an attribute attris value from u to v, where u and v are indices for attris values,   4. a unit benefit vector BcLi denoting the benefit obtained from a single customer x when the x belongs to the positive class in a leaf node Li, i1,2,..N ,Where N is the number of leaf nodes in the tree DT, and   5. a set Ctest of test cases.  A solution is a set of  k goals Gi , i1, 2, . . .k, where each goal consists of a set of source leaf nodes Sij and one designation leaf node Di, denoted as  Sij,j1,2,.,GiDi where Sij and Di are leaf nodes from the decision tree DT. The meaning of a goal is to transform customers that belong to the source nodes S to the destination node D via a number of attributevalue changing actions 6.  Here the main aim is to find a solution with the maximal net profit defined below.   Goals. Given a source leaf node S and a destination leaf node D, we denote the objective of converting a customer x from S to D as a goal, and denote it as S  D. The concept of a goal can be extended to a set of source nodes To transform a set of leaf nodes Si to a designation leaf node D, the goal is Si,i1,2,.D.  Actions.  n order to change the classification of a customer x from S to D, one may need to apply more than one attributevalue changing action. An action A is defined as a change to an attribute value for an attribute attr. Suppose that for a customer x, the attribute attr has an original value u. To change its value to v, an action is needed. This action A is denoted as Aattr ,uv.   Action Sets.  A goal is achieved by a set of actions. To achieve a goal of changing a customer x from a leaf node S to a destination node D, a set of actions that contains more than one action may be needed. Specifically, consider the path between the root node and D in the tree DT. Let attrivi,i1,2,.ND be set of attributevalues along this path. For x, let the corresponding attributevalues be attri  ui ,i 1,2,.ND. Then, the actions of the form can be generated Asetattriuivi,i1,2,.ND where we remove all null actions where ui is identical to vi thus, no change in value is needed for an attri. This action set ASet can be used for achieving the goal S  D.   Net Profits.  The net profit of converting one customer x from a leaf node S to a destination node D is defined as follows Consider a set of actions Aset for achieving the goal S D. For each action attri, u  v in Aset, there is a cost as defined in the cost matrix Cattri, u, v. Let the sum of the cost for all of Aset be Ctotal, S  Dx. Let the probability of x to belong to the positive class in S be px,S. Likewise, let the probability of a customer in D be in the positive class be px,D. Recall that from the input, we have a benefit vector Bc Lfor the leaf nodes L. Thus, we have BcS as the benefit of belonging to node S and Bc D as the benefit of belonging to node D. Then, the unit net profit of converting one customer x from S to D is  Punit x,S  D  BcD px,D  BcS px,S  Ctotal,SDx Then, for a collection Ctest of all test cases that fall in node S, the total net profit of applying an action set for achieving the goal S D is  PCtest,SDSigma xeCtestPunit x,S  D When the index of S is i, and the index of D is j, we denote Punit x,S  D  as Pij for simplicity.  Thus, the BSP problem is to find the best k groups of source leaf nodes Groupi, i  1,2, . . .  k and their corresponding goals and associated action sets to maximize the total net profit for a given test data set Ctest. An Example to illustrate, consider an example in Fig. 2.Assume that for leaf nodes L1 to L4, the probability values of being in the desired class are 0.9, 0.2, 0.8, and 0.5, respectively. Now consider the task of transforming L2 and L4 to a higher probability node, such that the net profit of making all To illustrate the process, consider a test data set such that there is exactly one member that falls in each leaf node of this decision tree. In order to calculate the net profit, we assume all leaf nodes to have an initial benefit of one unit. For simplicity, we also assume that the cost of transferring a customer is equal to the number of attribute value changes multiplied by 0.1. Thus, to change from L2 to L1, we need to modify the value of the attribute Status with a profit gain of transformations is maximized.  0.910.21  0.1 0.6.                                            Fig. 2 An example of decision tree  134International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org  The BSP problem has an equivalent matrix representation. From 2 and 3, we obtain a profit matrix  MPij,i1,2,m,j1,2,.nformed by listing all source leaf nodes Si as the row index and all the action sets ASetj, for achieving the goal Si  Dj, as the column index here, we omit Si in the column headings. In this matrix M, Pij  0, 1  i  m where m is the number of source leaf nodes, and 1 j  n n is the number of destination leaf nodes. Pij denotes the profit gain computed by applying ASetj to Si for all testcase customers that falls in Si. If Pij  0, that is, applying ASetj to transfer Si to the corresponding destination leaf node can bring about a net profit, we say that the source leaf node Si can be covered by the action set ASetj. From 2 and 3, the computation of the profit matrix M..,.. can be done in Om n.  As an example of the profit matrix computation, a part of the profit matrix corresponding to the source leaf node.  TABLE 1 An Example of the Profit Matrix Computation      4  CASE STUDY An Application is developed in Java, SQL Server for incrementing the profit of Mobile handset sellingshop  Industry by applying different discounts to the customers. Before giving discount to the different types of customers, we can check how much profit can be gained by the shop  Industry as well as by the customer. Also we can find the profit gained by a particular customer for a particular scheme and accordingly we can offer most profitable scheme to the customer to gain high profit.  This system relies on not only a prediction, but also a probability estimation of the classification, such as the probability of being loyal. Such information is available from decision trees 7.          Input               Feedback, Phones Viewed,                  Refernces, Purchase amt, etc                                   Decision tree     Output       Loyal and Unloyal Customers Probability      Input        Post processing actions is the action performed on the customer, in an attempt to make him loyal..  Post processing Decision Tree    Actions  In order to change the classification of a customer x from Loyal and UnLoyal, one may need to apply more than one attributevalue changing action. An action A is defined as a change to an attribute value for an attribute Attr. Suppose that for a customer x, the attribute Attr has an original value u. To change its value to v, an action is needed.   U is probability that we got  V is what we expecting..  Therefore Action a is to be taken on customer so that he is loyal, and Profit is also not affected..  This action A is denoted as A  Attr, uv  5 CONCLUSIONS AND FUTURE WORK  Most data mining algorithms and tools produce only the segments and ranked lists of customers or products in their outputs. In this paper, we present a novel technique to take these results as input and produce a set of actions that can be applied to transform customers from undesirable classes to desirable ones. For decision trees considered, aim is to maximize the expected net profit of all the customers. We have found a greedy heuristic algorithm to solve both problems efficiently and presented an ensemblebased decisiontree algorithm that use a collection of decision trees, rather than a single tree, to generate the actions. Also it is shown  that the resultant action set is indeed more robust with respect to training data changes.  In my future work, we will research other forms of limited resources problem as a result of post processing data mining models and evaluate the effectiveness of our algorithms in the realworld deployment of the action oriented data mining.  6 REFERENCES 1  C. X. Ling and C. Li. Data mining for direct marketing specific problems and solutions. In Proceedings of Fourth International Conference on Knowledge Discovery and Data Mining KDD98, pages 73  79. 1998.  2  J. Dyche, The CRM Handbook A Business Guide to Customer Relationship Management. AddisonWesley, 2001.  135International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org  3  C.X. Ling, T. Chen, Q. Yang, and J. Cheng, Mining Optimal Actions for Intelligent CRM, Proc. IEEE Intl Conf. Data Mining ICDM, 2002.   4 X. Zhang and C.E. Brodley, Boosting Lazy Decision Trees, Proc. Intl Conf. Machine Learning ICML, pp. 178185, 2003.  5  B. Liu, W. Hsu, L.F. Mun and H.Y. Lee. Finding interesting patterns using user expectations. Knowledge and Data Engineering, 116817832, 1999.  6 P. S. Usama Fayyad, Gregory PiatetskyShapiro. From data mining to knowledge discovery in databases. AI Magazine, 1711, Fall 1996.  7  Q. Yang, J. Yin, C.X. Ling, and T. Chen,  Post processing  Decision  Trees to Extract Actionable Knowledge, Proc. IEEE Conf. Data Mining ICDM 03, pp. 685688, 2003.                 136International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                               ISSN 22295518   IJSER  2011 httpwww.ijser.org   New Approach to Weighted Pattern Sequential MiningDataset                                            D.Saravana Kumar, N.Ananthi, D.Yadavaram  Abstract  In real world data the knowledge used for mining rule is almost time varying. The item have the dynamic characteristic in terms of transaction , which have seasonal selling rate and it hold timebased association ship with another item. Traditional model of association rule mining is adapted to handle weighted association rule mining problems where each item is allowed to have a weight. Association rule mining is to find out association rules that satisfy the predefined minimum support and confidence from a given database. End users of association rule mining tools encounter several in practice when data bases come with binary attributes. In this paper, we introduce a new measure, which does not require initially allotted weights .The quality of transactions is considered by link based models and a fast mining algorithm is adopted.  Keywords Association Rule, Fast algorithm, weighted support.         1 INTRODUCTION  HE classical model of association rule mining employs the support measure, which treats every transaction equally. In contrast, different transactions have different weights in reallife data sets. During recent years, one of active research topic is Association rule discovery. The association rule discovery is used to identify relationships between items in very large databases, to extract interesting correlations, associations among sets of items in the transaction databases or other data repositories.  For example, given a market basket database, it would be interesting for decision support to know the fact that 30 of customers who bought coca powder and sugar also bought butter. This analysis may be used to provide some basis if is required to increase the sales and introduce from free schemes like, if 3 kg of sugar is bought then 100g butter free. In a census database, we should discover that 20 of persons who worked last year earned more than the average income, or in a medical database, that 35 of patients who have cold also have sinus. Association Rule Mining aims to explore large transaction databases for association rules, which may reveal the implicit relationships among the data attributes. It has number of practical applications, including classification, text mining, and Web log analysis, and Share Market and recommendation systems. The classical model of association rule mining employs the support measure, which treats every transaction equally. In contrast, different transactions have different weights in reallife data sets. For example, in the market basket data, each transaction is recorded with some profit. Much effort has been dedicated to association rule mining with preassigned weights. However, most data types do not come with such preassigned weights, such as Web site clickstream data. There should be some notion of importance in those data. Data mining technology has emerged as a means for identifying patterns and trends from large quantities of data. Data mining, also known as Knowledge Discovery in Databases, has been defined as The nontrivial extraction of implicit, previously unknown, and potentially useful information from data. Data mining is used to extract structured knowledge automatically from large data sets. The information that is mined is expressed as a model of the semantic structure of the dataset, where in the prediction or classification of the obtained data is facilitated with the aid of the model.              2 WEIGHTED ASSOCIATION RULE MINING The concept of association rule was first introduced. It proposed the supportconfidence measurement framework and reduced association rule mining to the discovery of frequent item sets. The following year a fast mining algorithm, A priori, was proposed. Much effort has been dedicated to the classical binary association rule mining problem since then. Numerous algorithms T137International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                               ISSN 22295518   IJSER  2011 httpwww.ijser.org   have been proposed to extract the rules more efficiently. These algorithms strictly follow the classical measurement framework and produce the same results once the minimum support and minimum confidence are given.   WARM generalizes the traditional model to the case where items have weights. Ram Kumar et al. introduced weighted support of association rules based on the costs assigned to both items as well as transactions. An algorithm called WIS was proposed to derive the rules that have a weighted support larger than a given threshold. Cai et al defined weighted support in a similar way except that they only took item weights into account. The goal is to steer the mining focus to those significant relationships involving items with significant weights rather than being flooded in the combinatorial explosion of insignificant relationships Discovery of association rules has been found useful in many applications.  3. RELATED WORKS Quantitative association rule mining problem has been introduced in  and some algorithms for quantitative values also have been proposed, where the algorithm finds association rules by partitioning the attribute domain, combining adjacent partitions and then transforming the problem into a binary state.                 Mining QARs by a generic BAR mining algorithm, however, is infeasible in most cases for the following reasons. First, QAR mining suffers from the same problem of a combinatorial explosion of attribute sets as does BAR mining that is, given a set of N distinct attributes, the number of its nonempty subsets is 2N1. However, as shown by  it is necessary to combine the consecutive intervals of a quantitative attribute to gain sufficient support and more meaningful intervals. This leads to another combinatorial explosion problem if the domain of a quantitative attribute is partitioned into n intervals, the total number of intervals of the attribute grows to On2 after combining the consecutive intervals. When we join the attributes in the mining process, the number of itemsets i.e., a set of attribute, interval pairs can become prohibitively large if the number of intervals associated with an attribute is large.          The second one is caused by the sharp boundary between intervals. To dominant this problem, Mining fuzzy association rules for quantitative values has been considered by a number of researches 813, most of which have based their methods on the important APriori algorithm. Chan and Au introduced FAPACS for mining fuzzy association rules .Instead of using intervals, FAPACS employs linguistic terms to represent the revealed regularities and exceptions . Kuoks algorithm expects the user or an expert to provide the required fuzzy sets of the quantitative attributes and their corresponding membership functions. Fu argues that experts may not give the right fuzzy sets and their corresponding membership functions. Hence, he proposed a method to find the fuzzy sets based on clustering techniques. Each of these researchers treated all attributes or all the linguistic terms as uniform. However, in realworld applications, the users perhaps have more interest in the rules that contain fashionable items. Gyenesei introduces the problem of mining weighted quantitative association rules based on fuzzy approach. He assigns weights to the fuzzy sets to reflect their importance to the user and proposes two different definitions of weighted support with and without normalization similar to his previous method.  Ishibuchi et al. extended the genetic algorithmbased rule selection method in Ref. 16 to the case where various fuzzy partitions with different granularities are used for each input. This extension increases the number of candidate rules. Hence, they proposed a prescreening procedure which is based on two rule evaluation criteria of association rules, to decrease the number of candidate rules. Kaya et al proposed an automated clustering method based on multiobjective genetic algorithms. This method automatically clusters the values of a given quantitative attribute in order to obtain large number of large itemsets in low duration.              The support counting procedure of the Apriori algorithm has attracted voluminous research owing to the fact that the performance of the algorithm mostly relies on this aspect. Park et al. proposed an optimization, called DHP Direct Hashing and Pruning intended towards restricting the number of candidate itemstes, shortly following the Apriori algorithms mentioned above. Brin et al put forth the DIC algorithm that 138International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                               ISSN 22295518   IJSER  2011 httpwww.ijser.org   partitions the database into intervals of a fixed size so as to reduce the number of traversals through the database . Another algorithm called the CARMA algorithm Continuous Association Rule Mining Algorithm employs an identical technique in order to restrict the interval size to 1.  4 WSUPPORT    FAST MINING ALGORITHM                 Item set evaluation by support in classical association rule mining is based on counting. In this section, we will introduce a linkbased measure called wsupport and formulate association rule mining in terms of this new concept.The problem of mining association rules that satisfy some minimum wsupport and wconfidence can be decomposed into two subproblems 1. Find all significant item sets with wsupport above the given threshold. 2. Derive rules from the item sets found in Step 1. The first step is more important and expensive. The key to achieving this step is that if an item set satisfies some minimum wsupport, then all its subsets satisfy the minimum wsupport as well. It is called the downward closure property of wsupport. Besides, the hub weights of all transactions are nonnegative.                    hubT                       hubT TXTTD    TXTTD   Hence, based on this property, we can extract significant item sets in a levelwise manner, as the Apriorilike algorithm.     FAST MINING ALGORITHM 1 Initialize authi to 1 for each item i 2 Forl0lnumitl do begin 3   Auth i0 for each item i 4 For all transactions t  D do begin 5 Hubt  iitauthi 6 Auth.ihubt for each item i t 7 End 8 Authiauthi for each item I, normalize auth 9 End 10 L1iwsuppiminwsupp 11 Fork2Lk10kdo begin 12 CkapriorigenLk1 13 For all transactions t  D do begin 14 CtsubsetCk,t 15 For all candidates c  Ct do 16 C.wsupp hubt 17 Hhubt 18 End 19 LkcCkc.wsuppHminwsupp 20 End 21 Answer  Uk Lk  WSupport Algorithm Input  1 A transaction database  TDB              2 A Minimum Support   Threshold               3 An approximate factor,                4 Normalized weights of the Item Output The result set of approximate                  Weighted frequent patterns.  Begin   Initialize WAFP      Let WAFP      Be the result set of approximate Weighted frequent patterns.   Scan TDB once and find the global      Approximate weighted frequent items    Whose maximum approximate                  Weighted supports MWS X       are no less than a minimum    threshold,    Remove the approximate weighted infrequent             Items in each transaction by the weight   Ascending order. Scan the TDB again and build a global   FPTree based on the weight ascending order.  Call WAF FPTree, WAFP  End  5. PERFORMANCE EVALUATION             To evaluate the linkbased association rule mining framework, we have modified the Apriori implementation by Bodon so that it uses wsupport and wconfidence as the rule selection thresholds. Several tests have been carried out on some classical data sets. The Experiments were conducted on a 1.8GHz Sempron 3000 machine with 1 Gbyte of RAM.            The proposed mining algorithm requires an additional iterative procedure to compute the hub weights of all transactions. The database is scanned 139International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                               ISSN 22295518   IJSER  2011 httpwww.ijser.org   exactly once in each iteration. Therefore, the convergence rate of the hub weights is critical to the performance. Let Hi denote the vector of hub weights after the ith iteration.. It is clear that HITS converges fast on transaction databases. Generally, three or four Iterations are enough to achieve a good estimation, which means that our linkbased method works at the cost of three or four additional database scans over the traditional techniques.           Since wsupport and wconfidence are normally larger than support and confidence, respectively, a comparison of the two measurement techniques with the same thresholds does not make sense. Instead, we select the thresholds so that the two models produce about the same amount of item sets and association rules.             Consider the data set retail as an example. With minwsupp  46 percent and minwconf  66 percent in the linkbased model, 81 item sets and 19 rules are generated with minsupp  34 percent and minconf  88 percent in the traditional model, 84 item sets and 19 rules are discovered discovered. Observe that the two models agree well on most of the rules, though they both advocate some rules that are not discovered by the other. Basically, two types of association rules are likely missing in the traditional model but not in the linkbased model and Compared with the traditional countingbased measurement, the proposed model emphasizes large transactions, because they are generally valuable. In the retail business, maintainers are more interested in customers who buy lots of stuff in Web clickstream data, longer sessions may correspond to regular visitors in recommendation systems, a user who has rated many movies or anything else is likely to have better taste. The assumption that large transactions are more important may not be a ground truth, but when it is very likely, the model works   1. Good Transactions 2. Small confidence 3. Good hubs support. It is hard to tell whether a ruleitem set is valuable by any objective measure, because traditional association rule mining does lack effective measures. A similar case happens in Web search. It is hard to tell which Web page should be ranked high. All we can argue academically is the model itself. The success of Google proves the efficiency of the linkbased models.  6. CONCLUSION As there are wide applications of association rules in data mining, it is important to provide good performance. In view of this, we proposed a new algorithm which considers the significance of the item also but not only support of the item. Experimental results show that the computational cost of the linkbased model is reasonable. At the expense of three or four additional database scans, we can acquire results different from those obtained by traditional countingbased models. Particularly for sparse data sets, some significant item sets that are not so frequent can be found in the linkbased model. Through comparison, we found that our model and method address emphasis on highquality transactions. In this paper, a brief discussion of a number of algorithms was presented along with a comparative study of a few significant ones based on their performance and memory usage.  ACKNOWLEDGEMENTS          I could not accomplish my research without support of my guide at Dr.GRD College of science.  First of all, I would like to express my deep gratitude to my guide D.Saravana Kumar. His invaluable advice and patience allowed me to undergo and finish the challenging process of my research work. I also would like to thank all the of my friends who advise and friendship gave me a lot of encouragement for this research work.   REFERENCES  1. R. Agrawal, T. Imielinski, and A. Swami, Mining AssociationRules between Sets of Items in Large Datasets, Proc. ACM SIGMOD 93, pp. 207216, 1993 2. R. Agrawal and R. Srikant, Fast Algorithms for Mining Association Rules, Proc. 20th Intl Conf. Very Large Data Bases VLDB 94, pp. 487499, 1994. 3. J.M. Kleinberg, Authoritative Sources in a Hyperlinked Environment, J. ACM, vol. 46, no. 5, pp. 604632, 1999. 4. O. Kurland and L. Lee, Respect My Authority HITS without Hyperlinks, Utilizing ClusterBased Language Models, Proc. ACM SIGIR, 2006. 5. K. Wang and M.Y. Su, Item Selection by 140International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                               ISSN 22295518   IJSER  2011 httpwww.ijser.org   HubAuthority Profit Ranking, Proc. ACM SIGKDD, 2002. 6. W. Wang, J. Yang, and P.S. Yu, Efficient Mining of Weighted Association Rules WAR, Proc. ACM SIGKDD 00, pp. 270274, 2000 7. B. Liu, W. Hsu, and Y. Ma, Integrating Classification and Association Rule Mining, Proc. ACM SIGKDD 98, pp. 8086, 1998. 8 M. S. Chen, J. Han, and P. S. Yu, 1996 Data mining An overview from a database Perspective, IEEE Transactions on Knowledge Data Engineering, Vol.8, pp. 866883. 9 R. Agrawal, H. Mannila, R. Srikant, H. Toivonen, and A.I. Verkamo, 1996. Fast discovery of association rules In U.M. Fayyad, G. Piatetsky  Shapiro, P. Smyth, and R. Uthurusamy, editors, Advances in Knowledge Discovery and Data Mining, pp. 307328. MIT Press. 10 S. Brin, R. Motwani, J.D. Ullman, and S. Tsur, 1997. Dynamic itemset counting and implication rules for market basket data. In Proceedings of the 1997 ACM SIGMOD International Conference on Management of Data, volume 262 of SIGMOD Record, pp. 255264. ACM Press. 11 M. J. Zaki and C.J. Hsiao, October 1999. CHARM An efficient algorithm for closed association rule mining. Technical Report 9910, Computer Science Dept., Rensselaer Polytechnic Institute.  12 C. Hidber, 1999. Online association rule mining. In A. Delis, C. Faloutsos, and S. Ghandeharizadeh, editors, Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data, volume 282 of SIGMOD Record, pp. 145156. ACM Press. 13 Bing Liu, Wynne Hsu, LaiFun Mun, and HingYan Lee, 1999. Finding Interesting Patterns Using User Expectations, IEEE Transactions on Knowledge and Data Engineering, Vol 11, No. 6, pp. 817832. 14 Roberto J. Bayardo Jr., and Rakesh Agrawal, 1999 Mining the Most Interesting Rules, Proceedings of the fifth ACM SIGKDD international conference on Knowledge discovery and data mining, ACM Press, pp. 145154. 15 Roberto J. Bayardo Jr., Rakesh Agrawal and Dimitrios Gunopulos, 1999. ConstraintBased Rule Mining in Large, Dense Databases, Research Report  IBM, In Proceedings of the 15th International Conference on Data Engineering, pp.188197. 16 R. Agrawal, C. Aggarwal, and V. Prasad, 2000. A Tree Projection Algorithm for Generation of Frequent Item Sets, Parallel and Distributed Computing, pp. 350371. 17 Wei Wang, jiong yang and philip S. Yu, 2000. Efficient Mining of Weighted Association Rules WAR, Proceedings of the sixth ACM SIGKDD international conference on Knowledge discovery and data mining,pp. 270  274. 18 Erwin, A., Gopalan, R. P., and Achuthan, N. R., 2007. CTUMine An Efficient High Utility Itemset Mining Algorithm Using the Pattern Growth Approach, IEEE 7th International Conferences on Computer and Information Technology, pp. 7176. 19 Yeh J. S., Li, Y. C., Chang C. C., 2007.  A TwoPhase Algorithm for UtilityFrequent Mining, To appear in Lecture Notes in Computer Science, International Workshop on High Performance Data Mining and Applications. 20 Vid Podpecan, Nada Lavrac, and Igor Kononenko, 2007. A Fast Algorithm for Mining Utility Frequent Itemsets, International Workshop on Constraintbased Mining and Learning at ECMLPKDD. 21 ChunJung Chu, Vincent S. Tseng, Tyne Liang, 2008. An efficient algorithm for mining temporal high utility itemsets from data streams, Journal of System Software, Vol. 81, No. 7, pp. 11051117. 22 Guangzhu Yu, Keqing Li, Shihuang Shao, 2008. Mining High Utility Itemsets in Large High Dimensional Data, International Workshop on Knowledge Discovery and Data Mining WKDD, pp. 1720.  141International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518     IJSER  2011 httpwww.ijser.org  Decreasing Inventory Levels Fluctuations by Moving Horizon Control Method and Move Suppression in the Demand Network  Mohammad Miranbeigi, Aliakbar Jalali  Abstract The significance of the basic idea implicit in the moving horizon control MHC has been recognized a long time ago in the operations management literature as a tractable scheme for solving stochastic multi period optimization problems, such as production planning and supply chain management, under the term moving horizon. In this paper, a moving horizon controller with move suppression term used for inventory management of the demand network supply chain. Index Terms moving horizon control, production planning, supply chain management, move suppression term, inventory management ,demand network.        1 INTRODUCTION                                                                     ey elements to an efficient supply chain are accurate pinpointing of process flows and timing of supply needs at each entity, both of which enable entities to request items as they are needed, thereby reducing safety stock levels to free space and capital. The operational planning and direct control of the network can in principle be addressed by a variety of methods, including deterministic analytical models and stochastic analytical models, and simulation models, coupled with the desired optimization objectives and network performance measures 1.  The significance of the basic idea implicit in the moving horizon control MHC or MHC has been recognized a long time ago in the operations management literature as a tractable scheme for solving stochastic multi period optimization problems, such as production planning and supply chain management, under the term moving horizon 2. In a recent paper 3, a MHC strategy was employed for the optimization of productiondistribution systems, including a simplified scheduling model for the manufacturing function. The suggested control strategy considers only deterministic type of demand, which reduces the need for an inventory control mechanism 4,5. For the purposes of our study and the time scales of interest, a discrete time difference model is developed 6. The model is applicable to multi echelon supply chain networks of arbitrary structure. To treat process uncertainty within the deterministic supply chain network model, a MHC approach is suggested 7,8.  Typically, MHC is implemented in a centralized fashion 9. The algorithm uses a moving horizon, to allow the incorporation of past and present control actions to future predictions 10,11,12,13.  In this paper, a moving horizon controller with move suppression term used for inventory management of the demand network. 2 MODELLING AND CONTROL In this work, a discrete time difference model is developed4. The model is applicable to multi echelon supply chain networks of arbitrary structure, that  DP denote the set of desired products in the supply Chain and these can be manufactured at plants, P, by utilizing various resources, RS. The manufacturing function considers independent production lines for the distributed products. The products are subsequently transported to and stored at warehouses, W. Products from warehouses are transported upon customer demand, either to distribution centers, D, or directly to retailers, R.  Retailers receive time varying orders from different customers for different products. Satisfaction of customer demand is the primary target in the supply chain management mechanism. Unsatisfied demand is recorded as backorders for the next time period. A discrete time difference  model  is used for description  of  the supply  chain network dynamics. It is assumed that decisions are taken within equally spaced time periods e.g. hours, days, or weeks. The duration of the base time period depends on the dynamic characteristics of the network. As a result, dynamics of higher frequency than that of the selected time scale are considered negligible and completely attenuated by the network 4,14. Plants P, warehouses W, distribution centers D, and retailers R constitute the nodes of the system. For each node, k, there is a set of upstream  nodes and a set of K  Mohammad Miranbeigi  is currently pursuing Phd degree program in control engineering in University of Tehran, Iran, Email m.miranut.ac.ir  A. A. Jalali, is associate professor in  the Department of   Electrical Engineering, Iran University of Science and Technology, Narmak ,Tehran, Iran,                           Email  drjalaliiust.ac.ir. 142International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518    IJSER  2011 httpwww.ijser.org  downstream nodes, indexed by , kk  . Upstream  nodes can supply node k and  downstream nodes can be supplied by k. All valid , kk   andor , kk  pairs constitute permissible routes within the network. All variables in the supply chain network e.g. inventory, transportation loads valid for bulk commodities and products. For unit products, continuous variables can still be utilized, with the addition of a post processing rounding step to identify neighbouring integer solutions. This approach, though clearly not formally optimal, may be necessary to retain computational tractability in systems of industrial relevance.  A product balance around any network node involves the inventory level in the node at time instances t and t  1, as well as the total inflow of products from upstream nodes and total outflow to downstream nodes. The following balance equation is valid for nodes that are either warehouses or distribution centers    DPiTtDWkktkkixkkkLtkkixtkiytkiy ,,,,,,,,,1,,            1       where kiy , is the inventory of product i stored in node k kkix ,,  denotes the amount of the ith product transported through route , kk   kkL , denotes the transportation lag delay time for route , kk  , i.e. the required time periods for the transfer of material from the supplying node to the current node. The transportation lag is assumed to be an integer multiple of the base time period. For  retailer nodes, the inventory balance is slightly modified to account for the actual delivery of the ith product attained, denoted by , td ki .    .,,,,,,,1,, DPiTtRktkidkkkLtkkixtkiytkiy               2  The amount of unsatisfied demand is recorded as backorders for each product and time period. Hence, the balance equation for back orders takes the following form    .,,,,,,1,,DPiTtRktkiLOtkidtkiRtkiBOtkiBO                     3  where kiR , denotes the demand for the ith product at the kth retailer node and time period t. kiLO , denotes the amount of cancelled back orders lost orders because the network failed to satisfy them within a reasonable time limit. Lost orders are usually expressed as a percentage of unsatisfied demand at time t. Note that the model does not require a separate balance for customer orders at nodes other than the final retailer nodes 4,15. MHC originated in the late seventies and has developed considerably since then. The term MHC does not designate a specific control strategy but rather an ample range of control methods which make explicit use of a model of the process to obtain the control signal by minimizing an objective function. The ideas, appearing in greater or lesser degree in the predictive control family, are basically the explicit use of a model to predict the process output at future time instants horizon, the calculation of a control sequence minimizing an objective function and the use of a moving strategy, so that at each instant the horizon is displaced towards the future, which involves the application of the first control signal of the sequence calculated at each step. The success of  MHC is due to the fact that it is perhaps the most general way of posing the control problem in the time domain. The use a finite horizon strategy allows the explicit handling of process and operational constraints by the MHC. The control system aims at operating the supply chain at the optimal point despite the influence of demand changes 12,13. The control system is required to possess built in capabilities to recognize the optimal operating policy through meaningful and descriptive cost performance indicators and mechanisms to successfully alleviate the detrimental effects of demand uncertainty and variability. The main objectives of the control strategy for the supply chain network can be summarized as follows i maximize customer satisfaction, and ii minimize supply chain operating costs. The first target can be attained by the minimization of back orders i.e. unsatisfied demand over a time period because unsatisfied demand would have a strong impact on company reputation and subsequently on future demand and total revenues. The second goal can be achieved by the minimization of the operating costs that include transportation and inventory costs that can be further divided into storage costs and inventory assets in the supply chain network. Based on the fact that past and present control actions affect the future response of the system, a moving time horizon is selected. Over the specified time horizon the future behavior of the supply chain is predicted using the described difference model Eqs. 13. In this model, the state variables are the product inventory levels at the storage nodes, y, and the back orders, BO, at the order receiving nodes. The manipulated control or decision variables are the product quantities transferred through the networks permissible routes, x, and the delivered amounts to customers, d. Finally, the product back orders, BO, are also matched to the output variables. The inventory target levels e.g. inventory setpoints are time invariant parameters. The control actions that minimize a performance index associated with the outlined control objectives are then calculated over the moving time horizon. At each time period the first control action in the calculated sequence is implemented. The effect of unmeasured demand disturbances and model mismatch is computed through comparison of the actual 143International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518    IJSER  2011 httpwww.ijser.org  current demand value and the prediction from a stochastic disturbance model for the demand variability. The difference that describes the overall demand uncertainty and system variability is fed back into the MHC scheme at each time period facilitating the corrective action that is required. The centralized mathematical formulation of the performance index considering simultaneously back orders, transportation and inventory costs takes the following form4          .1,,2,,,,,,,2,,,,,2,,,,,,,2,,,,,          Mtt RDWk DPikkikkikkixPtt Rk DPikikiBOMtt RDWk DPikkikkixPtt RDWk DPikiskikiytotaltxtxwtBOwtxwtytywJ              4  The performance index, J, in compliance with the outlined control objectives consists of four quadratic terms. Two terms account for inventory and transportation costs throughout the supply chain over the specified prediction and control horizons P , M. A term penalizes back orders for all products at all order receiving nodes e.g. retailers over the moving horizon P. Also a term penalizes deviations for the decision variables i.e. transported product quantities from the corresponding value in the previous time period over the control horizon M.  The term is equivalent to a penalty on the rate of change in the manipulated variables and can be viewed as a move suppression term for the control system. Such a policy tends to eliminate abrupt and aggressive control actions and subsequently, safeguard the network from saturation and undesired excessive variability induced by sudden demand changes. In addition, transportation activities are usually preferred to resume a somewhat constant level rather than fluctuate from one time period to another. However, the move suppression term would definitely affect control performance leading to a more sluggish dynamic response. The weighting factors, kiyw ,, , reflect the inventory storage costs and inventory assets per unit product, kkixw ,,,  , account for the transportation cost per unit product for route , kk  .Weights kiBOw ,,  correspond to the penalty imposed on unsatisfied demand and are estimated based on the impact service level has on the company reputation and future demand. Weights kkixw ,,,  , are associated with the penalty on the rate of change for the transferred amount of the ith product through route , kk  . Even though, factors kiyw ,, , kkixw ,,,  and kiBOw ,,  are cost related that can be estimated with a relatively good accuracy, factors kkixw ,,,   are judged and selected mainly on grounds of desirable achieved performance. The weighting factors in cost function also reflect the relative importance between the controlled back orders and inventories and manipulated transported products variables. Note that the performance index of cost function reflects the implicit assumption of a constant profit margin for each product or product family. As a result, production costs and revenues are not included in the index. In this centralized implementation, MHC will optimized for whole policy and then will sent downstream optimal inputs to upstream joint nodes to those nodes which it is coupled, as measurable disturbances.  Each node completely by a centralized MHC optimizes for whole policy. At each time period, the first control action in the calculated sequence is implemented until MHC process complete. 3 SIMULATIONS A four echelon supply chain system is used in the simulated examples. The supply chain network consists of one product, two production nodes, two warehouses, four distribution centers, and four retailer nodes.  All possible connections between immediately successive echelons are permitted. One product is being distributed through the network. Inventory setpoints, maximum storage capacities at every node, and transportation cost data for each supplying route are reported in Table 1. A prediction horizon of 20 time periods and a control horizon of 10 time periods were selected and was considered 3LO  for every times.  So each  delay was replaced by its 4th order Pade approximation.   Table 1. Supply chain data  R D W Echelon 300 700 1000 Max inventory  level  180  300  500 Inventory  setpoint D  to R 5.02.05.05.02.05.05.05.05.05.05.02.05.05.02.05.0 W  to D T5.02.05.02.02.05.02.05.0 P to W 5.02.02.05.0  Transportation cost 1 1 1 Inventory  weights 1    Backorder  weights 0.5   Delivery weights 3 3 3 Move suppression  weights term 180 300 500 Initial condition 2 3 5 Delays 144International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518    IJSER  2011 httpwww.ijser.org  The simulated scenarios lasted for 50 time periods. Variant demand is presented in Fig. 1. The move suppression term would definitely affect control performance leading to a more sluggish dynamic response. In this part, centralized MHC  method is applied to the supply chain network with a variant customer demand that is seeing in figures 2 and 3.  Fig. 1  Demand  Fig. 2  MHC outputs of supply chain management systemdeltaU curves is with move suppression effect   CONCLUSION The large majority of successful MHC applications address the case of multivariable control in the presence of constraints, motivating its extensive distribution for applications where traditional control usually comes close to its limits. The success of MHC is due to the fact that it is perhaps the most general way of posing the control problem in the time domain. The use a finite horizon strategy allows the explicit handling of process and operational constraints by the MHC. Typically, MHC is implemented in a centralized fashion.  In this paper, a centralized moving horizon controller applying to a supply chain management system consist of  one plant supplier, two distribution centers and three retailers. Also a move suppression term add to cost function, that increase system robustness toward changes on demands.  Through illustrative simulations with variation of demand, it is demonstrated that move suppression effect decreases maximum changes of customer demands.  Fig. 3  MHC inputs of supply chain management systemdeltaU curves is with move suppression effect  REFERENCES 1 M. Beamon, Supply chain design and analysis models and  Methods. International Journal of Production Economics, 55, pp. 281, 1998. 2 S. Igor , Model predictive functional control for processes with unstable poles. Asian journal of control,vol 10, pp. 507513, 2008. 3 E . PereaLopez, , B. E . Ydstie,  A model predictive control strategy for supply chain optimization. Computers and Chemical Engineering, 27, pp. 1201, 2003. 4 P. Seferlis, N. F. Giannelos, A two layered optimization based control sterategy for multi echelon supply chain network, Computers and Chemical Engineering, vol.  28 , pp. 799809, 2004. 5 G. Kapsiotis , S. Tzafestas,  Decision making for inventoryproduction planning using model based predictive control,  Parallel and distributed computing in engineering systems. Amsterdam Elsevier, pp.  551556, 1992. 6 S. Tzafestas ,  G. Kapsiotis , Modelbased predictive control for generalized production planning problems,  Computers in Industry, vol. 34, pp. 201210, 1997. 7 W. Wang, R. Rivera ,  A novel model predictive control algorithm for supply chain management in semiconductor manufacturing,  Proceedings of the American control conference, vol. 1, pp. 208213, 2005. 8 S. Chopra, P. Meindl, Supply Chain Management Strategy, Planning and Operations, Pearson Prentice Hall Press, New Jersey, pp. 5879, 2004. 9 H. Sarimveis, P. Patrinos,  D. Tarantilis, T. Kiranoudis, Dynamic modeling and control of supply chain systems A review, Computers  Operations Researc, vol.  35, pp.  3530  3561, 2008. 10 E. F. Camacho, C. Bordons, Model Predictive Control. Springer, 2004. 11 R. Findeisen,  F. Allgwer, L. T. Biegler, Assessment and future directions of  nonlinear model predictive control, Springer, 2007. 12 P. S.  Agachi, Z. K. Nagy, M. V. Cristea, A. ImreLucaci, Model Based Control,  WILEYVCH Verlag GmbH  Co, 2009. 0 5 10 15 20 25 30 35 40 45 50050100150200Time daysdemand0 5 10 15 20 25 30 35 40 45 501000100200300400500600Time daysInvntory levels  WdeltaUDdeltaURdeltaUBOdeltaUWDRBO0 5 10 15 20 25 30 35 40 45 500200400x P,W  0 5 10 15 20 25 30 35 40 45 500100200x W,D0 5 10 15 20 25 30 35 40 45 500100200x D,R0 5 10 15 20 25 30 35 40 45 500100200Time daysdeliveredxdaltaUx145International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518    IJSER  2011 httpwww.ijser.org  13 R. Towill, Dynamic Analysis of An Inventory and Order Based Production Control System, Int. J. Prod. Res, vol. 20, pp. 671687, 2008. 14 E. Perea, Dynamic Modeling and Classical Control Theory for Supply Chain Management,  Computers and Chemical Engineering, vol. 24, pp. 11431149, 2007. 15 J. D. Sterman , Business Dynamics Systems Thinking and Modelling in A Complex World, Mcgraw Hill Press, pp. 113128, 2002.  146International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518   IJSER  2011 httpwww.ijser.org  Receding Horizon Control on Large Scale Supply Chain  Mohammad Miranbeigi, Aliakbar Jalali  Abstract Supply chain management system is a network of facilities and distribution entities suppliers, manufacturers, distributors, retailers. The control system aims at operating the supply chain at the optimal point despite the influence of demand changes. In this paper, a centralized constrained receding horizon controller applying to a supply chain management system consist of two product, one plant, two distribution centers and three retailers. Index Terms supply chain, supply chain management system, suppliers, manufacturers, distributors, retailers , control system, demand, receding horizon controller.         1 INTRODUCTION                                                                     HE network of suppliers, manufacturers, distributors and retailers constitutes a supply chain management system. Between interconnected entities, there are two types of process flows information flows, e.g., an order requesting goods, and material flows, i.e., the actual shipment of goods. Key elements to an efficient supply chain are accurate pinpointing of process flows and timing of supply needs at each entity, both of which enable entities to request items as they are needed, thereby reducing safety stock levels to free space and capital. The operational planning and direct control of the network can in principle be addressed by a variety of methods, including deterministic analytical models and stochastic analytical models, and simulation models, coupled with the desired optimization objectives and network performance measures 1.  The significance of the basic idea implicit in the receding horizon control RHC or RHC has been recognized a long time ago in the operations management literature as a tractable scheme for solving stochastic multi period optimization problems, such as production planning and supply chain management, under the term receding horizon 2. In a recent paper 3, a RHC strategy was employed for the optimization of productiondistribution systems, including a simplified scheduling model for the manufacturing function. The suggested control strategy considers only deterministic type of demand, which reduces the need for an inventory control mechanism 4,5. For the purposes of our study and the time scales of interest, a discrete time difference model is developed 6. The model is applicable to multi echelon supply chain networks of arbitrary structure. To treat process uncertainty within the deterministic supply chain network model, a RHC approach is suggested 7,8.  Typically, RHC is implemented in a centralized fashion  9. The algorithm uses a receding horizon, to allow the incorporation of past and present control actions to future predictions 10,11,12,13.  In this paper, a centralized receding horizon controller applying to a supply chain management system consist of  one plant supplier, two distribution centers and three retailers.  2 DISCRETE TIME DIFFERENCE MODEL In this work, a discrete time difference model is developed4. The model is applicable to multi echelon supply chain networks of arbitrary structure, that  DP denote the set of desired products in the supply Chain and these can be manufactured at plants, P, by utilizing various resources, RS. The manufacturing function considers independent production lines for the distributed products. The products are subsequently transported to and stored at warehouses, W. Products from warehouses are transported upon customer demand, either to distribution centers, D, or directly to retailers, R.  Retailers receive time varying orders from different customers for different products. Satisfaction of customer demand is the primary target in the supply chain management mechanism. Unsatisfied demand is recorded as backorders for the next time period. A discrete time difference  model  is used for description  of  the supply  chain network dynamics. It is assumed that decisions are taken within equally spaced time periods e.g. hours, days, or weeks. The duration of the base time period depends on the dynamic characteristics of the network. As a result, dynamics of higher frequency than that of the selected time scale are considered T  Mohammad Miranbeigi  is currently pursuing Phd degree program in control engineering in University of Tehran, Iran, Email m.miranut.ac.ir  A. A. Jalali, is associate professor in  the Department of   Electrical Engineering, Iran University of Science and Technology, Narmak ,Tehran, Iran,                           Email  drjalaliiust.ac.ir. 147International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org  negligible and completely attenuated by the network 4,14. Plants P, warehouses W, distribution centers D, and retailers R constitute the nodes of the system. For each node, k, there is a set of upstream  nodes and a set of downstream nodes, indexed by , kk  . Upstream  nodes can supply node k and  downstream nodes can be supplied by k. All valid , kk   andor , kk  pairs constitute permissible routes within the network. All variables in the supply chain network e.g. inventory, transportation loads valid for bulk commodities and products. For unit products, continuous variables can still be utilized, with the addition of a post processing rounding step to identify neighbouring integer solutions. This approach, though clearly not formally optimal, may be necessary to retain computational tractability in systems of industrial relevance.  A product balance around any network node involves the inventory level in the node at time instances t and t  1, as well as the total inflow of products from upstream nodes and total outflow to downstream nodes. The following balance equation is valid for nodes that are either warehouses or distribution centers     DPiTtDWkktkkixkkkLtkkixtkiytkiy ,,,,,,,,,1,,            1        where kiy , is the inventory of product i stored in node k kkix ,,  denotes the amount of the ith product transported through route , kk   kkL , denotes the transportation lag delay time for route , kk  , i.e. the required time periods for the transfer of material from the supplying node to the current node. The transportation lag is assumed to be an integer multiple of the base time period. For  retailer nodes, the inventory balance is slightly modified to account for the actual delivery of the ith product attained, denoted by , td ki .     .,,,,,,,1,, DPiTtRktkidkkkLtkkixtkiytkiy               2   The amount of unsatisfied demand is recorded as backorders for each product and time period. Hence, the balance equation for back orders takes the following form     .,,,,,,1,,DPiTtRktkiLOtkidtkiRtkiBOtkiBO                     3  where kiR , denotes the demand for the ith product at the kth retailer node and time period t. kiLO , denotes the amount of cancelled back orders lost orders because the network failed to satisfy them within a reasonable time limit.      Lost orders are usually expressed as a percentage of unsatisfied demand at time t. Note that the model does not require a separate balance for customer orders at nodes other than the final retailer nodes 4,15.  3 RHC DESIGNATION RHC originated in the late seventies and has developed considerably since then. The term RHC does not designate a specific control strategy but rather an ample range of control methods which make explicit use of a model of the process to obtain the control signal by minimizing an objective function. The ideas, appearing in greater or lesser degree in the predictive control family, are basically the explicit use of a model to predict the process output at future time instants horizon, the calculation of a control sequence minimizing an objective function and the use of a receding strategy, so that at each instant the horizon is displaced towards the future, which involves the application of the first control signal of the sequence calculated at each step. The success of  RHC is due to the fact that it is perhaps the most general way of posing the control problem in the time domain. The use a finite horizon strategy allows the explicit handling of process and operational constraints by the RHC. The control system aims at operating the supply chain at the optimal point despite the influence of demand changes 12,13.  The control system is required to possess built in capabilities to recognize the optimal operating policy through meaningful and descriptive cost performance indicators and mechanisms to successfully alleviate the detrimental effects of demand uncertainty and variability. The main objectives of the control strategy for the supply chain network can be summarized as follows i maximize customer satisfaction, and ii minimize supply chain operating costs. The first target can be attained by the minimization of back orders i.e. unsatisfied demand over a time period because unsatisfied demand would have a strong impact on company reputation and subsequently on future demand and total revenues. The second goal can be achieved by the minimization of the operating costs that include transportation and inventory costs that can be further divided into storage costs and inventory assets in the supply chain network. Based on the fact that past and present control actions affect the future response of the system, a receding time horizon is selected. Over the spe148International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org  cified time horizon the future behavior of the supply chain is predicted using the described difference model Eqs. 13.  In this model, the state variables are the product inventory levels at the storage nodes, y, and the back orders, BO, at the order receiving nodes. The manipulated control or decision variables are the product quantities transferred through the networks permissible routes, x, and the delivered amounts to customers, d. Finally, the product back orders, BO, are also matched to the output variables. The inventory target levels e.g. inventory setpoints are time invariant parameters. The control actions that minimize a performance index associated with the outlined control objectives are then calculated over the receding time horizon. At each time period the first control action in the calculated sequence is implemented.  The effect of unmeasured demand disturbances and model mismatch is computed through comparison of the actual current demand value and the prediction from a stochastic disturbance model for the demand variability. The difference that describes the overall demand uncertainty and system variability is fed back into the RHC scheme at each time period facilitating the corrective action that is required. The centralized mathematical formulation of the performance index considering simultaneously back orders, transportation and inventory costs takes the following form4         .2,,,,,2,,,,,,,2,,,,,       Ptt Rk DPikikiBOMtt RDWk DPikkikkixPtt RDWk DPikiskikiytotaltBOwtxwtytywJ                                        4  The performance index, J, in compliance with the outlined control objectives consists of four quadratic terms. Two terms account for inventory and transportation costs throughout the supply chain over the specified prediction and control horizons P , M. A term penalizes back orders for all products at all order receiving nodes e.g. retailers over the receding horizon P.  The weighting factors, kiyw ,, , reflect the inventory storage costs and inventory assets per unit product, kkixw ,,,  , account for the transportation cost per unit product for route , kk  .Weights kiBOw ,,  correspond to the penalty imposed on unsatisfied demand and are estimated based on the impact service level has on the company reputation and future demand. Factors kiyw ,, , kkixw ,,,  and kiBOw ,,  are cost related that can be estimated with a relatively good accuracy. The weighting factors in cost function also reflect the relative importance between the controlled back orders and inventories and manipulated transported products variables. Note that the performance index of cost function reflects the implicit assumption of a constant profit margin for each product or product family. As a result, production costs and revenues are not included in the index. In this centralized implementation, RHC will optimized for whole policy and then will sent downstream optimal inputs to upstream joint nodes to those nodes which it is coupled, as measurable disturbances.  Each node completely by a centralized RHC optimizes for whole policy. At each time period, the first control action in the calculated sequence is implemented until RHC process complete Fig. 1.      Fig. 1  Centralized RHC on supply chain management system   4 SIMULATIONS A three echelon supply chain system is used in the simulated examples. The supply chain network consists of two product, one production nodes, two distribution centers, and three retailer nodes.  All possible connections between immediately successive echelons are permitted. Two product is being distributed through the network. Inventory setpoints, maximum storage capacities at every node, and transportation cost data for each supplying route are reported in Table 1. A prediction horizon of 20 time periods and a control horizon of 10 time periods were selected and was considered 0LO  for every times.  So each  delay was replaced by its 4th order Pade approximation after system model transformed to continuous time model and then returned to discrete time model.    Table 1. Supply chain data  R D Echelon 300 700 Max inventory level Order Measurable R P W D Measurable RHC 149International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org  Node1180 Node2120 Node380 Node1300 Node2250 Inventory  setpoint D  to R  1.02.01.01.01.02.0 W  to D   1.02.0  Transportation  cost  1 1 Inventory  weights 1   Backorder  weights 0 0 Initial condition 1 2 Delays Node150 Node240 Node330   Demand     The simulated scenarios lasted for 50 time periods with Eq. 5. Response to constant demand is presented in Fig. 2. In this part, centralized RHC  method is applied to the supply chain network with a constant customer demand that is seeing in figures 2 and 3.    1241243,23,12,22,11,21,13,23,12,22,11,21,13,2,21,2,22,,23,2,11,2,12,,12,1,21,1,21,,22,1,11,1,11,,11161163,23,12,22,11,21,13,23,12,22,11,21,12,22,11,21,1654321654321121110987654321,6543216543214321rrrrrrddddddIIIIIIIIIIIIRRRRRRddddddxxxxxxxxxxxxUBBBBBBRRRRRRDDDDBOBOBOBOBOBOyyyyyyyyyyYRRRRRRRRRRRRRDRDDPRDRDDPRDRDDPRDRDDPRRRRRRRRRRRRDDDD                       5  0 5 10 15 20 25 30 35 40 45 500100200I1,I40 5 10 15 20 25 30 35 40 45 5002040I2,I50 5 10 15 20 25 30 35 40 45 50050100I3,I60 5 10 15 20 25 30 35 40 45 500100200I7,I100 5 10 15 20 25 30 35 40 45 50050100I8,I110 5 10 15 20 25 30 35 40 45 50050100I9,I120 5 10 15 20 25 30 35 40 45 50050100d1,d20 5 10 15 20 25 30 35 40 45 50050100d3,d40 5 10 15 20 25 30 35 40 45 50050Time daysd5,d6150International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org  0200400D10200400D20200400D30200400D40100200R10100200R22000200R30100200R4050100R5050100R61000100B11000100B21000100B31000100B41000100B50 5 10 15 20 25 30 35 40 45 501000100B6Plant OutputsTime days Fig. 2  RHC outputs of supply chain management system  Fig. 3  RHC inputs of supply chain management system  CONCLUSION The large majority of successful RHC applications address the case of multivariable control in the presence of constraints, motivating its extensive distribution for applications where traditional control usually comes close to its limits. The success of RHC is due to the fact that it is perhaps the most general way of posing the control problem in the time domain. The use a finite horizon strategy allows the explicit handling of process and operational constraints by the RHC. Typically, RHC is implemented in a centralized fashion.  In this paper, a centralized receding horizon controller applying to a supply chain management system consist of  one plant supplier, two distribution centers and three retailers.   REFERENCES 1 M. Beamon, Supply chain design and analysis models and  Methods. International Journal of Production Economics, 55, pp. 281, 1998. 2 S. Igor , Model predictive functional control for processes with unstable poles. Asian journal of control,vol 10, pp. 507513, 2008. 3 E . PereaLopez, , B. E . Ydstie,  A model predictive control strategy for supply chain optimization. Computers and Chemical Engineering, 27, pp. 1201, 2003. 4 P. Seferlis, N. F. Giannelos, A two layered optimization based control sterategy for multi echelon supply chain network, Computers and Chemical Engineering, vol.  28 , pp. 799809, 2004. 5 G. Kapsiotis , S. Tzafestas,  Decision making for inventoryproduction planning using model based predictive control,  Parallel and distributed computing in engineering systems. Amsterdam Elsevier, pp.  551556, 1992. 6 S. Tzafestas ,  G. Kapsiotis , Modelbased predictive control for generalized production planning problems,  Computers in Industry, vol. 34, pp. 201210, 1997. 7 W. Wang, R. Rivera ,  A novel model predictive control algorithm for supply chain management in semiconductor manufacturing,  Proceedings of the American control conference, vol. 1, pp. 208213, 2005. 8 S. Chopra, P. Meindl, Supply Chain Management Strategy, Planning and Operations, Pearson Prentice Hall Press, New Jersey, pp. 5879, 2004. 9 H. Sarimveis, P. Patrinos,  D. Tarantilis, T. Kiranoudis, Dynamic modeling and control of supply chain systems A review, Computers  Operations Researc, vol.  35, pp.  3530  3561, 2008. 10 E. F. Camacho, C. Bordons, Model Predictive Control. Springer, 2004. 11 R. Findeisen,  F. Allgwer, L. T. Biegler, Assessment and future directions of  nonlinear model predictive control, Springer, 2007. 12 P. S.  Agachi, Z. K. Nagy, M. V. Cristea, A. ImreLucaci, Model Based Control,  WILEYVCH Verlag GmbH  Co, 2009. 13 R. Towill, Dynamic Analysis of An Inventory and Order Based Production Control System, Int. J. Prod. Res, vol. 20, pp. 671687, 2008. 14 E. Perea, Dynamic Modeling and Classical Control Theory for Supply Chain Management,  Computers and Chemical Engineering, vol. 24, pp. 11431149, 2007. 15 J. D. Sterman , Business Dynamics Systems Thinking and Modelling in A Complex World, Mcgraw Hill Press, pp. 113128, 2002.  151International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518   IJSER  2011 httpwww.ijser.org  Load Forecasting Using New Error Measures In Neural Networks Devesh Pratap Singh, Mohd. Shiblee, Deepak Kumar Singh  Abstract Load forecasting plays a key role in helping an electric utility to make important decisions on power, load switching, voltage control, network reconfiguration, and infrastructure development. It enhances the energyefficient and reliable operation of a power system. This paper presents a study of shortterm load forecasting using new error metrices for Artificial Neural Networks ANNs and applied it on the England ISO.  Index Terms Load forecasting, Neural networks, New neuron Models, New error metrics for neural networks, England ISO         1 INTRODUCTION                                                                     here is a growing tendency towards unbundling the electricity system. This is continually confronting the different sectors of the industry generation, transmission, and distribution with increasing demand on planning management and operations of the network. The operation and planning of a power utility company requires an adequate model for electric power load forecasting. Load forecasting plays a key role in helping an electric utility to make important decisions on power, load switching, voltage control, network reconfiguration, and infrastructure development.      Methodologies of load forecasts can be divided into various categories that include shortterm forecasts, mediumterm forecasts, and longterm forecasts. Shortterm forecasting which forms the focus of this paper, gives a forecast of electric load one hour ahead of time. Such forecast can help to make decisions aimed at preventing imbalance in the power generation and load demand, thus leading to greater network reliability and power quality.      Many methods have been used for load forecasting in the past. These include statistical methods such as regression and similarday approach, fuzzy logic, expert systems, support vector machines, econometric models, enduse models, etc. 2.      A supervised artificial neural network has been used in this work. Here, the neural network is trained on input data as well as the associated target values. The trained network can then make predictions based on the relationships learned during training. A real life case study of the power industry in Nigeria was used in this work.  In this paper a supervised artificial neural network has been used for load forecasting.    Devesh Pratap Singh is a graduate in engineering from UP Technical University, India. Email deveshpratap.85gmail.com  Mohd. Shiblee is currently pursuing PhD from IIT Kanpur, India, Email shibleeiitk.ac.in  Deepak Kumar Singh is currently pursuing PhD from IIT Kanpur, India, Email deepaksiitk.ac.in   Here, the neural network is trained on input data as well as the associated target values. The error measures used in the presented model is different from the conventional models. England ISO data has been used for the simulation.  In further sections of this paper, we discuss neural network in greater detail, introduce the conventional model of artificial neural network and the mathematical error models used in our model.   2 NEURAL NETWORK  2.1 INTRODUCTION      Artificial neuron model is inspired by biological neuron. Biological neurons are the basic unit of brain for information processing system.  Artificial Neural Network is a massive parallel distributed processing system that has a natural propensity for storing experimental knowledge and making it available for use 1. The basic processing units in ANN are neurons. Artificial Neural Network can be divided in two categories viz. Feedforward and Recurrent networks. In Feedforward neural networks, data flows from input to output units in a feedforward direction and no feedback connections are present in the network. Widely known feedforward neural networks are Multilayer Perceptron MLP 3, Probabilistic Neural Network PNN 4, General Regression Neural Network GRNN 5 and Radial Basis Function RBF Neural Networks 6. Multilayer Perceptron MLP is composed of a hierarchy of processing units Perceptron, organized in series of two or more mutually exclusive sets layers. It consists of an input layer, which serves as the holding site for the input applied to the network. One or more hidden layers with desired number of neurons and an output layer at which the overall mapping of the network input is available 7. There are three types of learning in Artificial Neural Networks, Supervised, unsupervised and Reinforcement learning. In supervised learning, the network is trained by providing it with input and target output patterns. In Unsupervised T152International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org  learning or Selforganization, an output unit is trained to respond to clusters of pattern within the input. In this paradigm the system is supposed to discover statistically salient features of the input population.Reinforcement learning may be considered as an intermediate form of supervised and unsupervised learning. Here the learning machine does some action on the environment and gets a feedback response from the environment. Most popular learning algorithm for feedforward networks is Backpropagation.  The first order optimization techniques in Backpropagation which uses steepest gradient decent algorithm show poor convergence. ANN is a very popular tool in the field of engineering. It has been applied for various problem like times series prediction, classification, optimization, function approximation, control systems and vector quantization. Many real life application fall into one of these categories. In the existing literature, various neural network learning algorithms have been used for these applications. In this paper, MLP neural network has been used successfully with different error measures for load forecasting.  2.2 New Error Measures  In Backpropagation algorithm, training of the neuron model is done by minimizing the error between target value and the observed value. In order to determine error between target and observed value, distance metric is used. It has been observed that Euclidean distance metric is the most commonly used for error measures in Neural Network applications. But it has been suggested that this distance metric is not appropriate for many problems 12. In this work the aim is to find best error metric to use in Back propagation learning algorithm.      The likelihood and loglikelihood functions are the basis for deriving estimators for parameters, for given set of data.  In maximum likelihood method we estimate the value of y for the given value of x in presence of error. See equation 1 i iy x e                                                                               1 Let xi denote the data points in the distribution and let N denotes the number of data points. Then an estimator   of  can be estimated by minimizing the error metric with respect to   1 , Nif x                                                                              2 where  , f x   is the distance metric. The error function  is differentiated with respect to   and equated to zero.  1 ,  0Nif x                                                                                                            3 Jie, et. al., 1011 has proposed some new distance metrics based on different means. These distance metrics can be used to improve the performance of the neuron model for learning the bestfit weights of the neuron models.  Distance metrics associated with the distribution models that imply the arithmetic mean, harmonic mean and geometric mean in See Table 1 are inferred using equation 3.   TABLE 1 ERROR METRICS AND MEAN ESTIMATION   Error metric Mean Arithmetic 12 , Niix     11 NiixN   Harmonic 211 0tNiixx      11Ni iNx Geometric 210lo gNiix         111N Nix                      Figure 1 illustrates the difference between the distance error metrics for the arithmetic mean, harmonic mean and geometric mean. For the sake of comparison, the value of  is set to 15. It is found that in the distribution associated with the harmonic and geometric estimations, the observations xi which are far away from   will contribute less towards , in contrast to arithmetic mean and thus the estimated values will be less sensitive to the bad observations i.e., observation with large variance, and therefore they are more robust in nature 9.    .Due to the robust property of harmonic and geometric distance metrics, generalizations of these distance metrics have also been done which may fit the distribution of data better 89  2.2.1 Generalized Harmonic Type I Error Metric      Generalized Harmonic Type I Error metric derived from the Generalized Harmonic Type I mean estimation using  Fig. 1.the error criterion of the arithmetic mean, harmonic and geometric mean 153International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org  equation 3 is given below.   Generalized Harmonic Type I Mean  1121pNx iipNx ii                                                                                                  4 The corresponding error metric is given below Generalized Harmonic Type I Error Metric  21  1Npii ixx                                                                             5  The parameter p is used to define the specific distance metrics. If p1, it becomes ordinary harmonic distance metric and for p2, it will become Euclidean distance metric. Figure 2 illustrates the generalized harmonic Type I distance metric for the Generalized Harmonic Mean Type I.                  2.2.2 Generalized HarmonicType II Error metric Harmonic Type II Error metric derived from the Generalized Harmonic Type II mean estimation using equation 3 is given below. GeneralizedHarmonicType II Mean  11NiiqqNx                                                  6  Generalized Harmonic Type II Error Metric   21   Nq qiix                                               7  The parameter q is used to define the specific distance metrics. If q 1, it becomes ordinary harmonic distance metric and for q1, it will become Euclidean distance metric. Figure 3 illustrates the generalized harmonic Type II distance metric for the generalized harmonic mean Type II               2.2.3 Generalized Geometric Error Metric Generalized Geometric Error Metric derived from the Generalized Geometric Mean estimation using equation 3 is given below . Generalized Geometric Mean     2 2111Nr ri iiNix xix                                            8 The corresponding error metric is given below Generalized Geometric Error Metrics  21  logNr iiixx                                                                                  9 The parameter r is used to define the specific distance metrics.  For the generalized geometric mean estimation, if r  0, it will become an ordinary geometric mean. Figure 4 illustrates the generalized geometric distance metric for the generalized geometric mean.                It is obvious that the generalized metrics correspond to a wide range of mean estimations and distribution models.  These error metrics have been used in the Back propagation algorithm of MLP which enhances the prediction efficiency. Section 2.3 describes in brief the MLP model ,  Fig. 2 The error criterion of the Generalized Harmonic Mean Type 1 .  Fig. 4 The error criterion of the Generalized Geometric Mean .   Fig. 3 The error criterion of the Generalized Harmonic Mean Type II .  154International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org  the learning algorithm of MLP with new error metrics has been derived.  2.3 Multilayer Perceptron  Multilayer Perceptron MLP is composed of a hierarchy of processing units Perceptron, organized in series of two or more mutually exclusive sets layers. It consists of an input layer, which serves as the holding site for the input applied to the network. One or more hidden layers with desired number of neurons and an output layer at which the overall mapping of the network input is available 1314. The input signal propagates through the network layerbylayer 2.     The architecture of the MLP network is shown in Figure 5. It has been proved that a single hidden layer is sufficient to approximate any continuous function 15. A three layer MLP is thus taken into account in this chapter. It has   nh  neurons in hidden layer and  no  neurons in output layer. It has   ni  inputs. The input and output vectors of the network are  1 2, , ......,TniX x x x  and  1 2, , .....,TnoY y y y , respectively.                  If the weight that connects the thi  neuron of the input layer with the thj  neuron of the hidden layer is ijw  and bias of the thj  neuron of the hidden layer is jbh  the net value of the thj neuron can be given as  110  1, 2, .......,n ij ji i jineth w h x bh j nh   The output of the thj neutron hj of the hidden layer after applying activation can be given as 1 1 jj j nethh nethe                                                     11 Similarly the net value netyk and the final output yk of kth neuron of the output layer can be given as 1  1,2,.......,nhk kj j kinety wo h bh k no      12 where is the activation function.   jh is the output of thj neuron of hidden layer.                kjwo is the connection weight of thj hidden layer    neuron with thk output layer neuron.    bok  is the weight of the bias at thk  neuron of output          layer.   no is the number of output neuron.  Output of thk  neuron of output layer is 1 1 kk k netyy netye                                        13  2.3.1 Training algorithm of MLP with new error metrics  In this section the error backpropagation learning of MLP with different error metrics have been derived. Let E denote the cumulative error at the output layer. In BP algorithm aim is to minimize the error at the output layer.         The weight update equations using gradient descent rule are given bellow     . 14kji jik jiyEwh new wh oldy wh       15kj jk jyEbh new bh oldy bh        16kkj kjk kjyEwo new wo oldy wo    17    kk kk kyEbo new bo oldy bk      where, is the learning  rate  and  1118192021 1   1  1   1 1 1 nokk k kj j j ikjinokk k kj j jkjkk kkkk k jjkyy y w h h xwhyy y w h hbhyy yboyy y hwo                   Fig. 5 Architecture of the MLP network  .  155International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org   For different error criterion only E y  will change and is shown as follows This is dependent on the distance metric used in computation of the total error E. Case 1 Least Mean Square Error LMSE 21 11 2N nok kn kt yE                                           23                                                                         Then,  k kkEt yy                                                     24 Case 2   Geometric error metric 21 11log 2N nokn k kyEt                                                            25             Then 1log  log k kk kEt yy y                                  26                                    Case 3 Harmonic error metric 21 11 12N nokkn k kyE tt                                                   27                                       Then   1k k kkEt y ty                                           28                                     Case 4 Generalized geometric error metric 21 11 log 2N nor kkn k ktE ty                                                 29              Then1log  log r rk k k kk kEt t y ty y                    30                                 Case 5 Generalized harmonic error metric Type I 21 11 12N nop kkn k kyE tt                   31                                  Then     1pk k kkEt y ty                                      32                   Case 6 Generalized harmonic error metric Type II 21 11 2N noq qk kn kE t y                                                       33                        Then  kq q qk k kEyt y q y                                         34 Where, yt denotes the desired vlue of neuron.and ti notes the target value for the ith pattern.  3 Results  The data is taken from England ISO, and the simulation is done using MATLAB.the network designed is of two lagers with 20 neuron .For converentional model mean absolutes error MAE is used. To display the result clearly only forcasting of one week is given. The results are shown in following figures. The x axis in all the figures below represents the days data here shown is only for 1 week 1st January 2008 to 8th January 2008 and the y axis depict the forecasted load demand in MW.           Fig. 6 Forecasting using MAE conventional model. Fig. 7 Forecasting using Generalized Harmonic Type I Error Metric 156International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org   4 CONCLUSION As it is clearly analyazed from the result. The new models are giving better result from the convestional model.this result are justified from the theory that the generalized mean models will capture the variation well.the reason is clear that the variation are largely nonlinear  stocasting in nature so conventional linear models are unable to catch it.  References.   1 S. Haykin.  Neural Networks A Comprehensive Foundation, Macmillan College Publishing Company, New York, 1994 2 Engle,R.F.,Mustafa,C.,andRice,J. 1992.Modelling peak electricity Demand.Journal of forecasting.11241251.  Eugene, A.F and Dora,G.2004.Load Forecasting.269285.State University of New York New York 3 D.E. Rumelhart, Parallel Distributed Processing, Plenary Lecture presented at Proc. IEEE International Conference on Neural Networks, San Diego, California, 1988. 4 Donald F. Specht, Probabilistic neural networks, in Neural Networks, 3, pp. 109118, 1990 5 Donald F. Specht, A General Regression Neural Network, IEEE Trans on Neural Networks,Vol 2, 1991. 6 S. Chen, C.F.N. Cowan, P.M. Grant, Orthogonal least squares learningalgorithmfor radial basis function networks, IEEE Transactions on Neural Networks, vol. 2, pp.302309, 1991. 7  Satish Kumar.  Neural Networks A Classroom Approach, Tata McGraw Hill Publishing Company, New Delhi, 3rd Ed. 2007. 8 W. Schiffmann, M. Joost, and R. Werner, , Optimization of the backpropagation algorithm for multilayer perceptrons, Univ. Koblenz, Inst. Physics, Rheinau 34, Germany. 9 R. Battiti, First and secondorder methods for learning between steepest descent and Newtons method, Neural Computation, Vol. 2, 141166, 1992. 10  J. Yu, J. Amores, N. Sebe, Q. Tian, Toward an improve Error metric, International Conference on Image Processing ICIP, 2004. 11  J. Yu, J. Amores, N. Sebe, Q. Tian, Toward Robust Distance   Metric Analysis for Similarity Estimation, IEEE Computer  Sosciety Conference on Computer Vision and Pattern Recognition,    2006. 12  W. J. J. Rey, Introduction to Robust and QuasiRobust Statistical Methods, SpringerVerlag, Berlin, 110116, 1983. 13  K. Hornik, M. Stinchombe and H. White, Multilayer feedforward networks are universal approximators, Neural networks, vol. 2, pp. 359366, 1989. 14 J. M. Zurada, Introduction to Artificial Neural Systems, Jaicob  publishing house, India, 2002 15 G. Cybenko, Approximation by superpositions of a sigmoidal function, Mathematics of Control, Signals, and Systemsvol. 2 pp.303314, 1989.      Fig. 8 Forecasting using Generalized Harmonic Type II Error Metric Fig. 9 Forecasting using Generalized Geometric Error Metric 157International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org    Effect of Temperature on Deformation Characteristics of Gold Ball Bond in AuAl Thermosonic Wire Bonding Gurbinder Singh, Othman Mamat Abstract Wire bonding  is a process that is used to form solidstate bonds to interconnect metals such as gold wires to metalized pads deposited on silicon integrated circuits. Typically, there are 3 main wire bonding techniques Thermocompression, Ultrasonic and Thermosonic. This experiment utilizes thermosonic bonding which applies heat, ultrasonic energy and force on an AuAl system. Sixteen groups of bonding conditions at various temperature settings were compared to establish the relationship between ball deformation and temperature. The results of this study will clearly indicate the effects of applied bonding temperature towards bond strength and deformation characteristics of gold ball bonding. Index Terms  Gold ball bond, Intermetallic phase , Shear strength, Thermosonic wire bonding         1. INTRODUCTION                                                                        N recent years, thermosonic wire  bonding has been prevalent in the application of solid state interconnect technology. In the process of making an interconnection, two wire bonds are formed. The first bond involves the formation of a ball with Electric Flame Off EFO process. The ball is placed in direct contact within the bond pad opening on the die. With application of load bond force and ultrasonic energy within a few milliseconds bond time under the influence of heat, a ball bond is formed at the aluminum bond pad. Factors such as ultrasonic energy, temperature, and pressure may influence the quality of bonding quality 1, 2.  Upon application of the primary factors to form an intermetallic layer that makes the connection on the bond pad of a die, the wire is the lifted to form a loop and then placed in contact with the desired bond area of a leadframe to form a wedge bond. In this process, the bonding temperature is one of the main bonding parameters which play an important role in the bonding 3. Essentially, different temperature will lead to different bonding output response as different temperature conditions mean different bonding environment. In previous studies, a parabolic relationship between temperature and strength has been determined. Too low or too high temperature for bonding can lead to unsuccessful bonding or low bonding strength 4, 5.  Although many studies about temperature effect in wire bonding has been carried out, it is worth investigating the criticality of temperature application on thermosonic wire bonding while keeping other factors at constant.  This study will be able to depict the actual deformation characteristics of Gold Au  ball bond with respect to temperature, while keeping other bonding factors as constant.  Sixteen groups of bonding data at various temperature settings were compared to establish the relationship between ball deformation and temperature.     I  Gurbinder Singh is currently pursuing a masters degree program in Mechanical Engineering at Universiti Teknologi Petronas, Tronoh, Perak. Malaysia,  Email Gurbinsinghyahoo.com   Othman Mamat, PhD is currently an Assoc. Professor in the Mechanical Engineering Department at Universiti Teknologi Petronas, Tronoh, Perak. Malaysia,  Email drothmanmamatpetronas.com.my   158International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org    2. Experiment Method  Gold wire bonding was carried out using a 138 KHz Thermosonic wire bonder. The bonding temperature was tuned to produce different thermosonic bonding process while all other bonding parameters such as bonding ultrasonic power, bonding force and time were kept unchanged.  The bonding parameters are listed in Table 1.  Table1 Bonding Parameters US Power DAC Bond Force gm Bond Time ms Temperature C 48 28 15 40C  340C   The diameter of gold wire 99.99 Au used is 1 mil. The gold wire bonding was performed on a 16mil x 16 mil die size with metal composition Al99.5, Cu0.5. The bond pad opening is 4mil x 4 mils and bond pad pitch is 5 mils.   The following is an outline of the gold wire bonding procedure which was carried out. Refer to Figure 1.                       Fig.1. Thermosonic wire bonding setup i. The Au wire at the capillary tip was melted to produce the Free air ball FAB using discharge from electric flame off EFO ii. The capillary is lowered and the free air ball is compressed onto the Al pad for bonding. iii. The capillary is lifted and the Au wire is connected to the lead to produce a wedge bond. iv. The Au wire is cut. Steps 1  4 are repeated.                                                                                          The bonding temperature was measured by a Ktype thermocouple sensor with a measurement range of 0  500C. A total of 16 runs were performed with bonding temperature range of 40C to 340C with a step of 20C.The bonding experiments were consistently repeated for 50 times under each testing condition for statistical analysis.  To assess the output response of the thermosonic bonding, the wire pull test and ball shear test are performed using Westbond wire pull tester and Royce 550 wire bond tester respectively. The destructive shear strength between the gold ball and substrate is a common judgment for bondability 6, 7.   To further analyze the bondability, intermetallic coverage assessment is also carried out using fuming potassium hydroxide, KOH.  Cross sectioning using grinding and polishing machines were used to enable analysis on gold ball bond. Samples were observed using an optical microscope and scanning electron microscope SEM. Thickness and diameter of the ball bond profile and Inter Metallic Coverage IMC growth was measured using micrometer scale attached to the optical microscope and the scope was calibrated using the standards provided with the scope.   159International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org    3. Result and Discussion I. Effect of Temperature on Bond Strength of              AuAl Wire Bonding  During the bonding process of Au and Al system in thermosonic system, the temperature plays a very important role. The relationship between bonding strength and temperature was successfully obtained from the experiment carried out and is in coherence with previous studies carried out 4, 5. Results of relationship of shear strength and temperature are shown in Figure 2.  304050 Ball Shear g50 100 150 200 250 300 350Temperature C Fig 2 Shear strength against temperature   Statistical mean, standard deviations, process capability index of bonding strength and intermetallic coverage for each experimental temperature consisting of 50 tests each is listed in Table 2. IMC is distinguishable in its dark colored formation in Table 2. It is observed that too low or too high temperature may result in unfavorable bonding or low bonding strength. This experiment suggests that, the most suitable bonding temperature for Au and Al system is between 200C and 240C.  This observable fact can be further explained by the following related theories    8, 9, and 10 and experiment       Table 2 Bond strength response against temperature   a. The presence of oxide and other forms of contamination may not be able to be removed and may impede diffusion between the bonding metals. This may lead to unsuccessful bonding or lower bonding strength with poor IMC. 160International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org    b. The application of appropriate and adequate temperature will soften the metal surface and accelerate the diffusion between the bonding systems. c. Higher application of temperature may cause the atom to diffuse to each other more quickly and may cause cracks and voids in the interface Results of this experiment show voids at higher temperature range in Table 2, leading to lower bonding strength. II. Effect of Temperature on Ball Bond Deformation   Results of this experiment shows significant difference in ball bond profiles with respect to different temperature levels. Research carried out previously with the application of shorttime Fourier transform STFT to input output power of an ultrasonic transducer is able to deduce this phenomena 11, 12.   The ball bond image in figure 3 shows a thicker ball deformation as compared to at higher temperatures. At 60C the gold ball bond thickness obtained is 19.9um. The available theory 4 depicts, at this stage of lower temperature, the impedance of piezotransducer is low and the consumed power of piezotransducer is high. This may result in poor exertion of bonding power to the ball bond. This result corelates to the lower ball shear strength and lower IMC that was obtained in this study                         Fig 3 Ball Bond profile at 60C  Results of experiment at temperature range 200C to 220 C shows this is a good range which results in the desired output response. The impedance of piezotransducer is high and the consumed power of piezotransducer is low. This effect may exert adequate physical bonding power to the bonded aluminum interface and result in a thinner ball bond of 17.9um Figure 4. Intermetallic coverage response from the experiment carried out in this temperature range is observed to be much uniformed and homogeneous as compared to the lower range temperature.                   Fig 4 Ball Bond profile at 200C  Results of ball bond obtained at higher temperature shows a thinner ball bond of 12.5um Figure 5. At this state, the available theory from previous study depicts, impedance of piezotransducer is higher and the consumed power of the piezotransducer is lower. This effect may result in higher physical exertion of bonding power to the ball bond and produces a thinner ball bond of 12.5um. This result corelates to the low shear strength obtained from this study, and may attribute to the nonuniform and nonhomogenous intermetallic coverage which shows voids in the center of ball bond.  161International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org                      Fig 5 Ball bond profile at 320C  4. Conclusion   From the experimental measurements, the effect of applied temperature on deformation of gold ball bond in AuAl thermosonic wire bonding was investigated. It is revealed and understood that poor gold bonding happens with low temperature which produces thicker ball bonds which results in lower bonding strength and poor intermetallic coverage. The application of high temperatures lead to ultra thin ball bonds and also results in lower bonding strengths and poor intermetallic coverage. Only when moderate temperature is applied, good shear strengths are attained with homogeneous intermetallic coverage from the medium ball bond thicknesses obtained  It is concluded that, applied bonding temperature, unaccompanied by other bonding factors, has an effect on ball bond deformation, bonding strength and intermetallic coverage. Further studies and experimentation of preheating on bonding surfaces should be explored for enhanced wire bondability  References 1   Antony, J., Improving the wire bonding process   quality using statistically designed experiments, J.  Microelectronics. 30, 161168, 1999.  2 Liang, Z.N., Kuper, F.G., A  concept to relate wire  bonding  parameters to bondability and ball bond  reliability. J. Microelectronics. 38, 12781291, 1998. 3 Hu, S.J., Lim,G.E., Lim,T.L.,  Study of temperature  parameter on the thermosonic gold wire bonding of  highspeed CMOS.  IEEE. J.  Transactions on  components, hybrids and manufacturing.4, 855858,  1991. 4 Wu,Y.X., Long, Z.L., Han Lei., Zhong Jue.,   Temperature effect in thermosonic wire  bonding. J. Transactions on nonferrous materials  society, 618622, 2006. 5 Long, Z.L., Lei Han, Wu, Y.X., Jue Zhong., Study of  Temperature Parameter in AuAg wire Bonding.  IEEE. J. Transactions on Electronics Packaging  Manufacturing. 3, 31, 2008.  6 Lei Han, Wang, F.L., Xu, W.H., Bondability window  and power input for wire bonding.J. Microelectronics  Reliability. 46, 610615., 2006. 7 Srikanth, N., Murali, S., Wong, Y.M., Critical study of  thermosonic copper ball bonding.  J. Thin Solid Flims.  462, 339345, 2004. 8  Li, M.Y., Ji, H.J., Wang, C., Interdiffusion of AlNi  system enhance by ultrasonic vibration at ambient  temperature.  J. Ultrasonics. 45, 6165, 2006. 9 Liu, D.S.,Ni, C.Y.,  A thermo  mechanical study on the  electrical  resistance of aluminum wire  conductors.    J. Microelectron.Rel. 42, 367374, 2002. 10 Murali, S., Formation and growth of  intermetallics  in thermosonic wire bonds Significance of vacancy solute  binding  energy.  J. Alloys compounds.  426, 200207, 2006. 11 Cohen, L., TimeFrequency  Analysis.  Prentice Hall, Eaglewood Cliffs, NJ, 1995. 12 Zhang, D., Ling, S., Monitoring Wire Bonding via  Timefrequency Analysis of horn vibration.  IEEE.J. Trans.electron.Packag.Manuf. 26, 216220, 2006.   162International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                              ISSN 22295518  IJSER  2011 httpwww.ijser.org  Sorption of Cr VI  As V on HDTMA Modified Zeolites Vandana Swarnakar, Nishi Agrawal, Radha Tomar  AbstractSorption of Cr VI  As V on HDTMA modified zeolites were investigated by batch technique, Xray diffraction, Fourier transform infrared analysis, Energy dispersive spectroscopy and Scanning electron microscopy. HDTMA was exchanged with extra structural cations of zeolite up to the external cation exhange capacity. The HDTMA modified surface was stable when exposed to extremes in pH, ionic strength and to oxoanions. The HDTMA modified zeolites showed significant sorption for chromate and arsenate ions in aqueous solution. Sorption data for each anion was well described by Freundlich isotherm equation. Increase in Cr VI  As V sorption on to  modified surface occurred in neutral solution pH7 and the amount of sorbed Cr VI  As V described rapidly with increasing pH since OH concentration competes against Cr VI  As V for the sorption sites, thus, inhibiting formation of Cr VISMZ  As VSMZ complex. FTIR analysis showed that sorbed SMZ forms an admicelle surfactant surface coverage, which is responsible for Cr VI and As V sorption.  Keyword Sorption Cr VI As V HexaDecylTriMethylAmmoniumBromide HDTMAB         1. Introduction hromium is one of the most abundant inorganic groundwater contaminant at hazardous waste sites. Compared to its trivalent counterpart, hexavalent Cr forms chromate CrO42 or hydrogen chromate HCrO4 that is more toxic and more soluble at various pH. Because of the negative charges, chromate sorption on aquifer minerals is limited, making it more mobile in subsurface soils and aquifers. Conventional treatment of chromaterich effluent is to reduce Cr VI to Cr III and precipitate Cr III as chromium hydroxide, or chromium iron hydroxide at high pH, followed by disposal of resulting dewatered sludge. Wastewater containing relatively low concentrations of Cr VI is usually treated with ion exchange resins to remove Cr VI 1. These conventional methods for Cr VI removal are expensive and thus research to find inexpensive sorbent materials have been conducted recently 28.  These metals are found well above the tolerance limit many a times in the aquatic environment 9. Chromium is widely used in electroplating, leather tanning, dye, cement and photography industries producing large quantities of effluent containing the toxic metal 10. Cr VI is of particular concern because of its toxicity 11. The recommended limit of Cr VI in potable water is only 0.05 mgl1 12. However, industrial and mining effluents contain much higher concentrations compared to the permissible limit. Arsenic in groundwater is largely due to minerals dissolving naturally from weathered rocks and soils. Also, it has many industrial applications and is also used extensively in the production of agricultural pesticides 13, 14. Runoff from these uses and the leaching of arsenic from generated wastes has resulted in increased levels of various forms of soluble arsenic in water. Use of arsenic contaminated water may cause numerous diseases of the skin and internal organs 1317. Consequently, extensive research to develop costeffective methods for arsenic removal has been carried out recently using various sorbents 1721. Sorption technique is generally considered to be promising method amongst the existing technologies due to easy separation of sorbent from aqueous media after treatment 22. Naturally occurring zeolites are hydrated aluminosilicate materials with high cation exchange capacities 2327. Sorption of arsenic on natural zeolites has been studied extensively in recent years due to their low cost and availability in nature 2842. See Fig. 1.  Thus, treatment of effluent to reduce  remove the pollutant before discharging into the environment becomes inevitable. Different methods such as reduction and precipitation 43, ion exchange 44, electrolysis, reverse osmosis, solvent extraction 45, C  Vandana Swarnkar is currently pursuing Ph.D. freomSOS in Chemistry Jiwaji University, Gwalior, India. Mobile  919425700609. Email sonivandana20gmail.com  Nishi Agrawal is currently pursuingPh.D. . freomSOS in Chemistry Jiwaji University, Gwalior, India.  Radha Tomar, Professor, SOS in chemistry, Jiwaji University, Gwalior is guide to help in pursuing Ph.D. 163International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                              ISSN 22295518  IJSER  2011 httpwww.ijser.org  sorption 46, and electrochemical precipitation 47 have been suggested for the removal of Cr VI. Among  Fig.1  A model of modification of zeolite surface by surfactant and sorption of As V species.  all these, sorption is the most promising technique and a feasible alternative 48. A variety of materials have been tried as sorbents for Cr VI and a number of studies have been reported using sorbents like granular activated carbon 49, soya cake 50, rubber tyres and sawdust 51, activated sludge 52, lingocellular substrate 53, fly ash 54, rice husk based activated carbon 55 etc.   The present work deals with sorption of Cr VI  As V on Surfactant Modified Zeolites and to determine their sorption capacity. The well known thermodynamic functions and isotherm studies have been reported to elucidate the equilibrium adsorption behavior at different temperatures. In addition to the effect of temperature, the effect of pH, adsorbent dose, time and concentration of sorbate on percentage sorption have been investigated. Thus, Zeolites Erionite, Cowlesite and Willhendersonite modified by cationic surfactant hexdecyltrimethylammonium bromide HDTMAB has been potentially used as sorbents to remove anionic contaminants such as Cr VI and As V from waste water.   2.1. Sample  Zeolites were synthesized by hydrothermal method and modified by hexadecyltrimethylammonium bromide HDTMAB followed by centrifugation. Scanning electron microscope SEM fig.2 and XRay diffraction analysis XRD fig.3 confirmed that the sample was of high purity. The zeolites were modified by HDTMAB from HIMEDIA. In centrifugal tube, 5gm of raw zeolites were mixed with 2.5 gm of HDTMAB with 180ml of distilled water. The solutions were mixed on a water bath shaker for 24 hrs. Zeolites were then separated from solution by centrifugation. After the supernatant solutions were decantated the zeolites were washed twice with deionized DI watered and air dried.  The surface morphology change after surfactant modification was similar to that of Camontmorillonite after hexadecyletrimethyl ammonium HDTMA modification 56. Energy dispersion spectrum showed increase in carbon after surfactant modification.  2.2. XRay Diffraction Analysis Powder Xray diffraction analysis was performed on an Xray diffractometer with CuK radiation at 45 kV and 40mA. XRD patterns of randomly oriented samples were collected from 2 equal to 5o to 70o at the scanning rate of 2omin using 1o divergent slit and scatter slit and 0.3 mm receiving slit, while those of oriented samples were collected from 2 equal to 5o to 50o.   2.3. Sorption of Cr VI and As V Cr VI solution of desired concentration was made using K2CrO4 and As V solution was made using Na2HAsO4. The pH of Cr VI and As V solution was adjusted to 1, 3, 5, 7, 9 using HNO3  or NaOH. To each centrifugal tube 0.1 gm of surfactant modified Erionite 164International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                              ISSN 22295518  IJSER  2011 httpwww.ijser.org  ESMZ, Cowlesite CSMZ and Willhendersonite WSMZ were added to 50ml Cr VI and As V solution at Cr VI and As V concentration 108.96 mgL and 104.0 mgL respectively. The highest Cr VI and As V sorption on ESMZ, CSMZ  WSMZ were achieved at pH 7. Thus, all the solutions were adjusted to pH 7 in the subsequent sorption experiments. For Cr VI and As V sorption isotherm determination, different weights of  ESMZ, CSMZ and WSMZ were added to 50 ml metal oxoanions solution of initial concentration 108.96 645.80 mgL for Cr VI  and 104.12502.01mgL for As V with 0.01N increment. To study the thermodynamic parameters 0.1gm of ESMZ, CSMZ and WSMZ were added to 50ml of 0.01N Cr VI and As V solutions at pH 7. Sorption was carried out at temperature 288, 298 and 308K in a constant temperature water bath shaker. The amount of Cr VI and As V sorbed on to ESMZ, CSMZ and WSMZ were calculated from the difference between the initial and equilibrium concentrations of Cr VI and As V.   2.4. FTIR Analysis FTIR analysis was carried out on a NICOLET 410 Fouriertransform infrared spectrometer. The spectra were recorded in the region 4004000cm1 with a spectral resolution of 2cm1, using a pressed KBr pellet technique.  2.5. Quality Assurance In order to ascertain reliability, accuracy and reproducibility of  the assembled data, the batch equilibrium tests carried out for Cr VI and As V sorption were replicated twice and experimental blanks were run in parallel. All the glass wares were pre soaked before use in 5 HNO3 for about 24 h followed by washing with deionized water and drying in an oven. Sample blanks were analyzed for correction of background effect on instrument response.  1. Results  Discussion 3.1 XRD Analysis Xray diffractogram of synthetic gel has been recorded using Cu K radiation in a range 2  5o to 70o at a scanning speed of 1stepsecond. Powder Xray diffraction pattern of hydrothermally synthesized material ESMZ, CSMZ and WSMZ are represented in Fig 2a, Fig. 2b and Fig. 2c. In all cases medium was NaOH, the degree of crystallinity is very high as shown by the peak intensity for the major diffraction peak. These pattern shows maxima at 2  26.85o, 14.75o  25.94o, therefore d spacing between the plane is found to be d  0.20, 0.20  0.10 . Powder Xray diffraction analysis shows that NaOH medium for ESMZ, CSMZ  WSMZ type material synthesis resulted in crystalline products.   Fig. 2a  XRD diffractogram of hydrothermally synthesized ESMZ   Fig. 2b  XRD diffractogram of hydrothermally synthesized CSMZ   Fig. 2c  XRD diffractogram of hydrothermally synthesized WSMZ   3.2. FTIR Spectroscopy FTIR NICOLET  410 Spectrometer spectra of ESMZ, CSMZ and WSMZ are presented in Fig. 3. It is found in the range 9501250 cm1  420500cm1 strongest vibration at 9501250 cm1 is assigned to TO stretching and the next strongest band at 420500 cm1 is assigned to TO bending mode T Si or Al. The Hydroxyl bond OH stretch near 3550 cm1 in Spectra indicates the bimodal absorbance. The water 165International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                              ISSN 22295518  IJSER  2011 httpwww.ijser.org  molecules attached to zeolite frame work shows strong characteristic structure sensitive bands due to water H2O bending vibration at 1630 cm1. The peaks below 550 cm1 indicates OTO bending of rotation mode. The peaks between 700850cm1 and 10001150 cm1 are assigned to symmetric and antisymmetric TOT stretching vibration. Two bands around 30003500 cm1  28002900 cm1 appeared in the ESMZ, CSMZ and WSMZ indicate asymmetric  symmetric stretching vibration of CH2 of alkyl chain and band  at about 1200  1600 cm1 was assigned to vibration of trimethyl ammonium quaternary group CN CH33.  Fig. 3  FTIR spectra of hydrothermaly synthesize ESMZ  3.3. Scanning Electron Microscopy SEM The SEM images Fig. 4 of synthetic modified Erionite, Cowlesite and Willhendersonite gel suggest the crystal size of the material to be in the range 1m 10m show various morphology of the meso structured material depending on the crystallization conditions. Photomicrographs of the synthetic modified ESMZ, CSMZ  WSMZ exhibited well defined narrow shape with excellent crystal edges. The aggregation of zeolites as HDTMAB is sorbed provides information about surfactant aggregation on the surface. These scanning electron microscopy images show the underlying crystalline structure of ESMZ, CSMZ and W SMZ.      Fig.4  SEM images showing high purity of Surfactant Modified ESMZ, CSMZ  W SMZ. 3.4. Energy dispersive spectrometry EDS The chemical composition of the synthesized modified material was checked for metal ions Na, Ca, Al, and Si using energy dispersive spectrometry. A revealing feature in the synthesis of zeolite is the strong correlation between SiAl ratio of the resulting crystals, the nature of the cation used and medium of synthesis. The flexibility in the synthesis to produce desired composition of zeolite represent a critical step in the improvement of the material for many environmental process in which SiAl ratio is the key for maximizing performance. The chemical composition of the prepared material is given in Fig. 5 for ESMZ, CSMZ and WSMZ. It is observed that composition of the prepared material is quite close to modified zeolites ESMZ, CSMZ and WSMZ.166International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                              ISSN 22295518  IJSER  2011 httpwww.ijser.org         Fig.5  EDAX spectra showing high purity of Surfactant Modified ESMZ, CSMZ  W SMZ167International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                              ISSN 22295518  IJSER  2011 httpwww.ijser.org  3.5. Thermodynamics of Sorption The effect of temperature on the sorption of oxoanions on ESMZ, CSMZ  WSMZ was also checked using the optimized conditions. The temperature was varied from 288 to 308K. The amounts of metal oxoanions sorbed at various temperature which reveals that the uptake of Cr VI and As V  increase with increase in temperature, indicating better sorption at higher temperature. The enhancement amount of oxoanions sorbed at equilibrium with the rise in temperature may be either due to creation of some new active sites on the sorbent surface.  The amounts of oxoanions sorbed at equilibrium at different temperatures have been utilized to evaluate the thermodynamically parameters for the sorption system. The vant Hoff plot of lnkc Vs 1T was a straight line Fig. 6a  Fig. 6b.  Fig. 6a  Vants Hoff Plot of lnkc Vs 1T for Cr VI sorption on Zeolites.  Fig. 6b  Vants Hoff Plot of lnkc Vs 1T for As V sorption on modified zeolites.  3.6. Sorption Isotherm Analysis of the isotherm data is necessary in order to develop an equation that can accurately represent the results and could be used for design purposes. The data obtained from the sorption isotherm studies were fitted to the Freundlich Isotherms.  The Freundlich isotherm shown in above equation assumes that the uptake of metal ions occurs on a heterogeneous surface by multilayer sorption and the amount of sorbate adsorbed increases with increasing concentration. The K and 1n are the constants of the Freundlich isotherm that corresponds to the sorption capacity and intensity respectively. The parameter Ceq correspondence to the remaining concentration of the sorbate in the solution and Cads is the amount sorbed at equilibrium. The constants of the isotherm equations can be computed from the intercept and slope of the linearized plot of the experimental  Cads  K Ceq 1n           1  The isotherm constants were calculated from the slope and intercept, which is shown in Fig. 7a  Fig. 7b. The values of correlation coefficient R2 for CrO42  AsO43 are higher in the Freundlich isotherm which indicates that the sorption process is well represented by the Freundlich equation.  168International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                              ISSN 22295518  IJSER  2011 httpwww.ijser.org   Fig .7a  b  Freundlich sorption isotherm of CrO42  AsO43 on ESMZ at 25 0C  3.7. Effect of pH on Cr VI And As V Sorption The results of experiments carried out in order to evaluate the efficiency of E SMZ, CSMZ  WSMZ sorbent in removing As V over a range of pH 19. The sorbent removed arsenic effectively over the initial pH range 610. The predominant forms of arsenate in this pH range are H2AsO4 and HAsO42 14. The surface anion exchange between these two arsenate forms and counter ion bromide of surfactantmodified zeolites SMZ can be presented conceptually by Eq. 2 and 3 which were well verified and discussed by previous workers 21. It is evident from the figure that both of these forms can be effectively sorbed by ESMZ, CSMZ  WSMZ. Compared to the previous works related to As V sorption by surfactantmodified zeolites, where optimum pH range were reported to be 7.27.5 21 and 7.4 37, the sorption for As V by investigated surfactantmodified zeolites is of a wide optimum pH range, which should be of significant importance for practical operation. At initial pH 1, As V in solution exist in neutral form H3AsO4 14, no ion exchange took place with bromide and observed As V sorption is only for physical sorption. Therefore, As V sorption efficiency is remarkably low at pH1.  SMZBr  H2AsO4  SMZH2AsO4 Br       2  2SMZBr  HAsO42  SMZ2HAsO4 2Br      3 The change of final pH as a function of initial pH range 19. The sorbent follow the same trend of pH change indicating homogeneous nature of the surfaces in respect of existing ions. In the pH range 610, the pH of solution shifted towards acidic region. This may be due to the fact that zeolite surface could still generate protonated AlOH2 groups in solution even after modification with HDTMAB. The drop of final pH is due to OH consumption via deprotonation of surface AlOH2 groups giving back terminal aluminol group, AlOH Eq. 4  AlOH2  AlOH  H         4 In case of Cr VI ion maximum sorption occure at pH 7 chromium is present in the form of CrO42 which is more stable form of Cr VI. However, at lower pH dichromate ions are formed which has larger particle size as compared to chromate ions. The pore size of zeolite is suitable for the sorption of CrO42 ions rather than the Cr2O72 ions. HCrO4    CrO42  H         5 H2CrO4 HCrO4  H         6 Cr2O7  H2O  HCrO4         7 In case of basic solution when pH is greater than 7 the chromate ions are converted into CrOH3 species, it has less oxidizing ability. In the case of Cr, 6 oxidation states are more stable as compared to 3 oxidation state therefore chromium shows maximum sorption at pH 7.   CrO4  4H2O  3e                CrOH3  5OH     8 The sorption behavior of CrO42  AsO43 on ESMZ, CSMZ and WSMZ were checked at different pH of the solution viz. pH 1, 3, 5, 7, 9. Fig. 8 a  Fig. 8 b show the effect of pH on sorption of CrO42  AsO43 on E SMZ, CSMZ and WSMZ. The data shows maximum sorption at pH 7 for ESMZ. The results show that the sorption percentage increases up to pH 7 with increase in pH of the solution and thereafter sorption percentage decreases.  Fig. 8a  Sorption Cr VI on surfactant modified zeolites as a function of solution pH 169International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                              ISSN 22295518  IJSER  2011 httpwww.ijser.org   Fig. 8b  Sorption As V on surfactant modified zeolites as a function of solution pH  4. Conclusion Surfactant modified Erionite, Cowlesite and Willhendersonite sorbs Cr VI and As V effectively in neutral solutions and sorption of Cr VI  and As V is strongly pH dependent. Cr VI sorption capacity reached 21 kg, and As V sorption capacity reached 19gkg the sorption process is well described by freundlich equation. The enthalpy change H of the sorption is 0.1258 kjmol and 0.9188 kjmol indicating sorption of Cr VI and As V on to surfactant modified Erionite . FTIR analysis shows that HDTMA cations in the interlayer region caused polarization of adsorbed H2O molecules. The uptake of negatively charged Cr VI and As V ions in  the interlayer regions of surfactant modified  Erionite, Cowlesite and Willhendersonite are probably due to electrostatic force derived from the polarized water molecules and positively charged groups of the HDTMA cations.  Acknowledgment  The authors acknowledge to Head IIT, IIT Roorkee providing necessary instrumental facilities for XRD, EDS and SEM analysis.  References  1 D. Kratochvil, P. Pimentel and B.Volesky, Removal of trivalent and hexavalent chromium by seaweed biosorbent., Environ. Sci. Technol. 32, 26932698, 1998. 2 J.M. Zachara, C.E. Cowman, R.L. Schmidt and C.C. Ainsworth, Chromate adsorption by kaolin, Clays Clay Miner. 36, 317326, 1988. 3 Z. Li and R.S. Bowman, Counterion effects on the sorption of cationic surfactant and chromate on natural clinoptilolite, Environ. Sci. Technol. 31, 24072412, 1997. 4 Z. Li, and R.S. Bowman, Sorption of chromate and PCE by surfactant modified clay minerals Environ. Eng. Sci. 15, 237245, 1998. 5 Z. Li, Sorption of oxyanions and surface anion exchange by surfactant modified clay minerals, J. Environ. Qual. 28, 14571463, 1999. 6 Z. Li, D. Alessi, P. Zhang, R.S. Bowman, Organoillite as a low permeability sorbent to retard migration of anionic contaminants, J. Environ. Eng. 128, 583587, 2002. 7 B.S. Krishna, D.S.R. Murty, B.S. Jai Prakash, Thermodynamics of chromium VI anionic species sorption onto surfactantmodified montmorillonite, clay. J. Colloid Interface Sci. 229, 230236, 2000. 8 B. S. Krishna, D. S. R. Murty and B. S. Jai Prakash, Surfactantmodified clay as adsorbent for chromate, Appl. Clay Sci. 20, 6571, 2001. 9 N. Tewaria, P. Vasudevana and B.K. Guhab, Study on biosorption of Cr VI by Mucor hiemalis, Biochem,  Eng. J. 23, 185192, 2005. 10 C.  Raji and T.S. Anirudhan, Water Re, 32, 3772, 1998. 11 E. Oguz, Adsorption characteristics and the kinetics of the Cr VI on the Thuja oriantalis, Colloids Surf. A Physicochem. Eng. Aspects 252, 121128, 2005. 12 K. Slvaraj, S. Manonmani and S. Pattabhi, Bioresour. Technol, 89, 207211, 2003. 13 R. MenhageBena, H. Kazemian, M. GhaziKhansari, M. Hosseini and S.J. Shahtaheri, Evaluation of some natural zeolites and their relevant synthetic types as sorbents for removal of arsenic from drinking water, Iran. J. Public Health 33, 3644, 2004. 14 S. Shevade and R.G. Ford, Use of synthetic zeolites for arsenate removal from pollutant water, Water Res. 38, 31973204, 2004. 15 C.J. Wyatt, V.L. Quiroga, R.T.O. Acosta and R.O. Mendez, Excretion of arsenic As in urine of children, 711 years, exposed to elevated levels of As in the city water supply in Hermosillo, Sonora, Mexico, Environ. Res. 78, 1924, 1998. 16 M.P. ElizaldeGonzalez, J. Mattusch and R. Wennrich, Application of natural zeolites for preconcentration of arsenic species in water samples, J. Environ. Monit. 3, 2226, 2001. 17 S.W. Al Rmalli, C.F. Harrington, M. Ayub and P.I. Haris, A biomaterial based approach for arsenic removal from water, J. Environ. Monit. 7, 279282, 2005. 18 D. Mohan, C.U. Jr. Pittman, M. Bricka, F. Smith, B. Yancey, J. Mohammad, P.H. Steele, M.F. AlexandreFranco, V. GomezSerrano and H. Gong, Sorption of arsenic, cadmium, and lead by chars produced from fast pyrolysis of wood and bark during biooil production, J. Colloid Interface Sci. 310, 5773, 2007. 19 D. Mohan and  Jr,C.U. Pittman, Arsenic removal from waterwastewater using adsorbentsa critical review, J. Hazard. Mater. 142, 153, 2007. 20 D. Borah, S. Satokawa, S. Kato, T. Kojima, Surfacemodified carbon black for As V sorption, J. Colloid Interface Sci. 319, 5362, 2008. 170International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                              ISSN 22295518  IJSER  2011 httpwww.ijser.org  21 Z. Li, R. Beachner, Z. McManama and H. Hanlie, Sorption of arsenic by surfactantmodified zeolite and kaolinite,  Micropor. Mesopor. Mater. 105, 291297, 2007. 22 Y. H. Xu, T. Nakajima and A. Ohki, Adsorption and removal of arsenicV from drinking water by aluminumloaded shirasuzeolite, J. Hazard. Mater. B 92, 275287, 2002. 23 A. Kuleyin, Removal of phenol and 4chlorophenol by surfactantmodi.ed natural zeolite, J. Hazard. Mater. 144, 307315, 2007. 24 E. Erdem, N. Karapinar and R. Donat, The removal of heavy metal cations by natural zeolites, J. Colloid Interface Sci. 280, 309314, 2004. 25 E. Chmielewska, Adsorption of arsenate and chromate from waters on hydrophobized  zeolite media, Turk. J. Chem. 27, 639648, 2003. 26 M. Ghiaci, R. Kia, A. Abbaspur and F. Seyedeyn  Azad, Adsorption of chromate by surfactantmodi.ed zeolites and MCM41 molecular sieve, Sep. Purif. Technol. 40,  285295, 2000. 27 G.M. Haggerty and R.S. Bowman, Sorption of chromate and other inorganic anions by organozeolite, Environ. Sci. Technol. 28, 452458, 1994. 28 B. Dousova, T. Grygar, A. Martaus, L. Fuitova, D. Kolousek, V. Machovic, Sorption of As V on aluminosilicates treated with FeII nanoparticles, J. Colloid Interface Sci. 302, 424431, 2006. 29 K.B. Payne and T.M. Abdel  Fattah Adsorption of arsenate and arsenite by iron treated activated carbon and zeolites effects of pH, temperature, and ionic strength, J. Environ. Sci. 40, 723749, 2005. 30 Y. Xu, A. Ohki and S. Maeda, Adsorption of arsenic V by use of aluminiumloaded shirasuzeolites, Chem. Lett. 27, 10151016, 1998. 31 M. P. Elizalde  Gonzalez, J. Mattusch, W. Einicke and D. R. Wennrich, Sorption on natural solids for arsenic removal, Chem. Eng. J. 81, 187195, 2001. 32 H. GencFuhrman, P.S. Mikkelsen and A. Ledin, Simultaneous removal of As, Cd, Cr, Cu, Ni and Zn from stormwater experimental comparison of 11 different sorbents, Water Res. 41, 591602, 2007. 33 R. Menhaje  Bena, H. Kazemian, S. Shahtaheri, M. Ghazi  Khansari and M. Hosseini, Evaluation of iron modi.ed zeolites for removal of arsenic from drinkingwater, Stud. Surf. Sci. Catal. 154, 18921899, 2004. 34 M. HabudaStanic, M. Kules, B. Kalajdzic and Z. Romic, Quality of groundwater in eastern Croatia. The problem of arsenic pollution, Desalination, 210, 157162, 2007 35 V. Campos and P. M. Buchler, Anionic sorption onto modified natural zeolites using chemical activation, Environ. Geol. 52, 11871192, 2007. 36 E.J. Sullivan, R.S. Bowman and I.A. Legiee Sorption of arsenic from soil washing leachate by surfactantmodified zeolite, J. Environ. Qual.  32, 23872391, 2003. 37 P. Misaelides, V.A. Nikashina, A. Godelitsas, P.A. Gembitskii, E.M. Kats,  Sorption of As V from aqueous solutions by organomodified natural zeolitic materials, J. Radioanal. Nucl. Chem. 227  183186, 1998. 38 Z. Li and R.S. Bowman, Regeneration of surfactantmodified zeolite after saturation with chromate and perchloroethylene, Water Res. 35, 322326, 2001. 39 Z. Li, Sorption kinetics of hexadecyltrimethylammonium on natural clinoptilolite, Langmuir, 15, 64386445, 1999. 40 A. I. P. Cordoves, M. G. Valdes, J. C. T. Fernandez, G. P. Luis, J. A., GarciaGalzon, M. E. D. Garcia, Characterization of the binding site affinity distribution of a surfactantmodified clinoptilolite, Micropor. Mesopor. Mater. 109, 3848, 2008. 41 H. Faghihian and R.S. Bowman, Adsorption of chromate by clinoptilolite exchanged with various metal cations, Water Res. 39, 10991104, 2005. 42 Md. J., Haron, S.A., Masdan, M.Z., Hussein, Z., Zainal, Kassim, Kinetics and thermodynamic for sorption of arsenate by lanthanumexchanged zeolite, Malays, J. Anal. Sci. 11, 219228, 2007. 43 J. M. Philipot, F. Chaffange and J. Sibony, Water Sci. Technol. 17,11211132, 1984. 44 S. E. Jorgensen, Ind. Wastewater Manage. 7, 8192, 1979. 45 J. W. Patterson, Water Treatment Technology, 3rd ed., Ann Arbor Science, Ann Arbor Michigan, MI, 1978. 46 N. Kongsricharoern and C. Polprasert, Water Sci. Technol. 31, 109117,1995. 47 C. P. Huang and M. M. Wu, J. Water Pollut. Control Fed. 47, 24372446, 1975. 48 D. D. Das, R. Mahapatra, J. Pradhan S. N. Das and R. S. Thakur, Removal of Cr VI from aqueous solution using activated cow dung carbon, J. Colloid Interf. Sci. 232, 235240, 2000. 49 D. Agrawal, M. Goyal, and R. C. Bansal, Adsorption of chromium by activated carbon from aqueous solution, Carbon, 37, 19891997, 1999. 50 N. Daneshvar, D. Salari and S. Aber, Chromium adsorption and Cr VI reduction to trivalent chromium in aqueous solutions by soya cake, J. Hazard. Mater. 94, 4961, 2002. 51 N. K. Hamadi, X.D. Chen,  M. M. Farid, M.G.Q. Lu, Adsorption kinetics for the removal of chromiumVI from aqueous solution by adsorbents derived from used tyres and sawdust, Chem. Eng. J. 84, 95105, 2001. 52 S. E. Lee, H. S. Shin and B. C. Paik, Treatment of Cr VI  containing waste water by addition of powdered activated carbon to the activated sludge process, Water Res. 23, 6772, 1989. 53 T. Aoki and M. Munemori, Recovery of chromium VI from wastewaters with ironIII hydroxideI. Adsorption mechanism of chromium VI on iron III hydroxide, Water Res. 16,793796, 1982. 54 D. D., X. Meng, Utilization of fly ash for stabilization solidification of heavy metal contaminated soils, Eng. Geol. 70, 377394, 2003. 55 Y. Guo, J. Qi, S. Yang, K. Yu, Z. Wang and H. Xu, Adsorption of Cr VI on micro and mesoporous rice huskbased active carbon, Mater. Chem. Phys. 78. 132137, 2003. 171International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518   Analysis of a Population of Diabetic Patients Databases in Weka Tool P.Yasodha,   M. Kannan   Abstract  Data mining is an important tool in many areas of research and industry. Companies and organizations are increasingly interested in applying data mining tools to increase the value added by their data collections systems. Nowhere is this potential more important than in the healthcare industry. As medical records systems become more standardized and commonplace, data quantity increases with much of it going unanalyzed. Taking into account the prevalence of diabetes among men and women the study is aimed at finding out the characteristics that determine the presence of diabetes and to track the maximum number of men and women suffering from diabetes with 249 population using weka tool. In this paper the data classification is diabetic patients data set is developed by collecting data from hospital repository consists of 249 instances with 7 different attributes. The instances in the Dataset are pertaining to the two categories of blood tests, urine tests. WEKA tool is used to classify the data and the data is evaluated using 10fold cross validation and the results are compared.  Keywords Data Mining, Diabetics data, Classification algorithm, Association algorithm Weka tool         1. Introduction  HE main focus of this paper is the classification of different types of datasets that can be performed to determine if a person is diabetic. The solution for this problem will also include the cost of the different types of datasets. For this reason, the goal of this paper is classifier in order to correctly classify the datasets, so that a doctor can safely and costeffectively select the best datasets for the diagnosis of the disease. The major motivation for this work is that diabetes affects a large number of the world population and its a hard disease to diagnose. A diagnosis is a continuous process in which a doctor gathers information from a patient and other sources, like family and friends, and from physical datasets of the patient. The process of making a diagnosis begins with the identification of the patients symptoms. The symptoms will be the basis of the hypothesis from which the doctor will start analyzing the patient.  This is our main concern, to optimize the task of correctly selecting the set of medical tests that a patient must perform to have the best, the less expensive and time consuming diagnosis possible. A solution like this one, will not only assist doctors in making decisions, and make all this process more agile, it will also reduce health care costs and waiting times for the patients. This paper will focus on the analysis of data from a data set called Diabetes data set.  2. Related Work  The few medical data mining applications as compared to other domains. 4 reported their  experience in trying to automatically acquire medical knowledge from clinical databases. They did some experiments on three medical databases and the rules induced are used to compare against a set of predefined clinical rules. Past research in dealing with this problem can be described with the following approaches  a Discover all rules first and then allow the user to query and retrieve those heshe is interested in. The representative approach is that of templates 3. This approach lets the user to specify what rules heshe is interested as templates. The system then uses the templates to retrieve the rules that match the templates from the set of discovered rules. b Use constraints to constrain the mining process to generate only relevant rules. 12 proposes an algorithm that can take item constraints specified by the user in the association rule mining process so that only those rules that satisfy the user specified item constraints are generated. This also does not work well because doctors often do not have any specific rules to mine. c Find unexpected rules. This approach first asks the user to specify hisher existing knowledge about the domain. The system finds those unexpected rules 5, 6, 13.   3.  Data Mining In The Expert System Data mining sometimes called data or knowledge discovery is the process of analyzing data from different perspectives and summarizing it into useful information  information that can be used to increase revenue, cuts costs, or both. Data mining software is one of a number of analytical tools for analyzing data. It allows users to analyze data from many different T172International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518   dimensions or angles, categorize it, and summarize the relationships identified. Technically, data mining is the process of finding correlations or patterns among dozens of fields in large relational databases. In our thesis, some data mining methods on socialdemographic data of the users were applied. Correction and actuality of the data is very important for data mining for diabetes.  3.1 Data Mining by WEKA Engine  Weka is a collection of machine learning algorithms for data mining tasks. The algorithms can either be applied directly to a dataset or called from your own Java code. Weka contains tools for data preprocessing, classification, regression, clustering, association rules, and visualization. It is also wellsuited for developing new machine learning schemes. Weka is developed by the University of Waikato. In our paper we used the Weka as data mining engine, and made a bridge between the Diabetes Expert System interface and Weka. No user needs to install Weka in his workstation, but it do already enough to be installed on server machine.  3.1.1 Classification  Classification is a data mining machine learning technique used to predict group membership for data instances. For example, you may wish to use classification to predict whether the weather on a particular day will be sunny, rainy or cloudy. Popular classification techniques include decision trees and neural networks.  Bayes Network Classifier  Bayes Network learning uses various searching algorithms and quality measures. This is the base module for a Bayes Network Classifier, and also provides data structures network structure, conditional probability distributions, etc. and facilities common to Bayes Network learning algorithms like K2 and B   Figure1 BayesNet  J48 Pruned Tree  J48 is a module for generating a pruned or unpruned C4.5 decision tree. When we applied J48 onto refreshed data, we got the results shown as below on Figure2 .   Figure2 J48 Tree  173International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518     REP Tree  Fast decision tree learner. Builds a decisionregression tree using information gainvariance and prunes it using reducederror pruning with back fitting. Only sorts values for numeric attributes once. Missing values are dealt with by splitting the corresponding instances into pieces i.e. as in C4.5.   Figure3 REP tree  Random Tree  Class for constructing a tree that considers K randomly chosen attributes at each node. Performs no pruning. Also has an option to allow estimation of class probabilities based on a holdout set backfitting.   Figure4 Random Tree 3.1.2 Association Rules  One of the reasons behind maintaining any database is to enable the user to find interesting patterns and trends in the data. For example, in a supermarket, the user can figure out which items are being sold most frequently. But this is not the only type of trend which one can possibly think of. The goal of database mining is to automate this process of finding interesting patterns and trends. Once this information is available, we can perhaps get rid of the original database. The output of the datamining process should be a summary of the database. This goal is difficult to achieve due to the vagueness associated with the term interesting. The solution is to define various types of trends and to look for only those trends in the database.   Apriori   Apriori is a module implementing an Aprioritype algorithm. Iteratively reduces the minimum support until it finds the required number of rules with the given minimum confidence. The algorithm has an option to mine class association rules. It is adapted as explained in the second reference. Apriori is a classic algorithm for learning association rules. Apriori is designed to operate on databases containing transactions for example, collections of items bought by customers, or details of a website frequentation. Other algorithms are designed for finding association 174International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518   rules in data having no transactions. As is common in association rule mining, given a set of itemsets for instance, sets of retail transactions, each listing individual items purchased, the algorithm attempts to find subsets which are common to at least a minimum number C the cutoff, or confidence threshold of the itemsets. Apriori uses a bottom up approach, where frequent subsets are extended one item at a time a step known as candidate generation, and groups of candidates are tested against the data. The algorithm terminates when no further successful extensions are found Agrawal et al. 1994.   Figure5 Apriori   4. RESULTS  DISCUSSIONS   The main purpose of the system is to guide diabetic patients during the disease. Diabetic patients could benefit from the diabetes expert system by entering their daily glucoses rate and insulin dosages producing a graph from insulin history consulting their insulin dosage for next day. The diabetes expert system is not only for diabetic patient, but also for the people who suspect if they are diabetic.  Its also tried to determine an estimation method to predict glucose rate in blood which indicates diabetes risk. In our Paper, the target class consists of 249 men and women who are actually affected with diabetes. However, it is unknown that in what stage the disease is. Hence this study will not only help in estimating the maximum number of people suffering from diabetes with specific characteristics but also helps in identifying the state of the disease.   REFERENCES  1 Mats Jontell, Oral medicine, Sahlgrenska Academy, Gteborg University 1998 A Computerised Teaching Aid in Oral Medicine and Oral Pathology.  Olof Torgersson, department of Computing Science, Chalmers University of Technology, Gteborg.  2 T. Mitchell, Decision Tree Learning, in T. Mitchell, Machine Learning 1997 the McGrawHill Companies, Inc.,  pp. 5278.  3 Klemetinen, M., Mannila, H., Ronkainen, P., Toivonen, H., and Verkamo, A. I 1994 Finding interesting rules from large sets of discovered association rules, CIKM.   4 Tsumoto S., 1997Automated Discovery of Plausible Rules Based on Rough Sets and Rough Inclusion, Proceedings of  the Third PacificAsia Conference PAKDD, Beijing, China, pp 210219.  5 Liu B., Hsu W., 1996 Postanalysis of learned rules, AAAI,  pp. 828834.  6 Liu B., Hsu W., and Chen S., 1997 Using general impressions to analyze discovered classification rules, Proceedings of the Third ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.  7 Stutz J., P. Cheeseman. 1996 Bayesian classification autoclass Theory and results. In Advances in Knowledge Discovery and Data Mining. AAAIMIT Press  8 Witten Ian H., E. Frank, Data Mining Practical Machine Learning Tools and Techniques with Java Implementations, Ch. 8,  2000 Morgan Kaufmann Publishers   9 httpwww.cs.waikato.ac.nzmlweka, accessed 060521.  175International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518    10 httpgrb.mnsu.edugrbtsdocmanual J48DecisionT rees.html, accessed  11 Wikipedia, ID3algorithm accessed 20071209 URL httpen.wikipedia.orgwikiID3algorithm  12 Srikant,R.,Vu,Q.andAgrawal,R.,1997, Mining association rules with item constraints, Proceedings of the Third International Conference on Knowledge Discovery and Data Mining, Newport Beach, USA, pp 6773.  AUTHOR PROFILE      P.Yasodha Mphil Research Scholar in the Department of Computer Science  Applications in Sri Chandrasekharendra Saraswathi Viswa Mahavidyalaya University, Enathur, Kanchipuram 631 561. She received the degree in Master of Computer Applications from SCSVMV University in 2010. Her research interest lies in the area of Data Mining and Artificial Intelligence.                M.Kannan has been working as a Assistant Professor and Ph.D Research Scholar in the Department of Computer Science  Applications in Sri Chandrasekharendra Saraswathi Viswa Mahavidyalaya University, Enathur, Kanchipuram 631 561. He received the degree in Master of Computer Applications from Bharathidasan University in 2001 and M.PhilComputer Science from Madurai Kamaraj University in 2005.  His research interest includes Software Engineering  Data Mining. 176International Journal of Scientific  Engineering Research Volume 2, Issue 6, June2011                                                                                                    ISSN 22295518  IJSER  2011 httpwww.ijser.org A Study and Application on CrossDisciplinary Proficiency Learning of Artificial Intelligence Pavan Gujjar Panduranga Rao , Dr.G Lavanya Devi,  Dr.P Srinivasa Rao  AbstractThis paper broadly elaborated the concept,  significance and main strategy of cross disciplinary proficiency  learning as  well as the basic structure of cross disciplinary proficiency learning  system. By combining several basic  ideas  of  main   strategies,  great  effort  are  laid  on introducing  several cross disciplinary proficiency learning methods, such as Memorizing a type of learning, Reasonbased learning, Learning  from instruction, Learning by deduction, Learning by analogy and Inductive learning, Learning by Experts etc. Meanwhile,  comparison and analysis are made upon their respective advantages and limitations. At the end of  the  article, it proposes the research objective of crossdisciplinary proficiency  learning  and  points  out  its  development   trend. Crossdisciplinary proficiency learning  is  a  fundamental  way  that   enable  the computer to have the intelligence  Its application which had been used mainly the method of induction and the synthesis rather than the deduction has already reached many fields of Artificial Intelligence area.  Index Terms  Cross Disciplinary  ProficiencyCDP learning , AI , System    structure , learning strategy , algorithm .            1 INTRODUCTION  Along with the latest development of modern Internet technology and multimedia  technical,  Artificial Intelligence  AI  research has  emerged  a  number of  new challenging issues.AI  has captured  increasing attention in many area and disciplines, which is an edge of line disciplines that is used to simulate the process of human thought. Scientists who are in many different professional backgrounds get some new thoughts process and new methodologies in the fields of AI . As a branch of computer science, these systems are showing a human intelligence and behavior characteristics. AI expert system for sub as a branch of AI has entered a stage of practical application in various departments on the national economy and government  as well as many aspects of social life in sociology to play a role and continue moving in the direction of inbreadth and  indepth development.     2 THE CONCEPT AND SIGNIFICANCE        OF CDP LEARNING 2.1   The Concept of CDP Learning CDP  learning CDPL is studying how the computer   to simulate or to realize the study and behavior of human  being of there thoughts. The  intention  is  to  obtain  the new derived knowledge or the skill , organize   the   knowledge  representation structure ,which   can   make drastic and progressive improvement of its own potential and performance. It is the core part of AI It is a elementary and fundamental way that enable the computer to have the intelligence  The application of it is reach in many  application areas  of  AI  and  is  mainly  emphasize uses  the  method  of induction and the synthesis but not the deduction or other knowledge representation  . The CDPL research establishes the computation model or the Comprehensive understanding model, according to the study mechanism of humanity through the sociology physiology, the cognitive science and philosophy develops each kind of study aspect and the study method, studies the general algorithm and carries on the theoretically analysis, and establishes study approach that has the specific application facing the duty.  2.2. The Significance of CDPL Learning        Research Whether CDPL  ability can surpass the humans or not, the main argument that many human beings  who are holding denial opinion is    Environment CDPL Knowledge Base Execution  Figure 1.   C D P  Learning system basic structure  1Pavan Gujjar Panduranga Rao  is a professor in computer science and engineering Department  in Park college of Engineering and Technology , Anna       University, Coimbatore , India  Email drgppavanyahoo.com     2,3  Dr.G Lavanya Devi , Dr.P.Srinivasa Rao,  Professor in Computer Science and System Engineering Department ,College of Engineering ,Andhra          University, India  177International Journal of Scientific  Engineering Research Volume 2, Issue 6, June2011                                                                                                    ISSN 22295518  IJSER  2011 httpwww.ijser.org The CDPL is manmade, its performance and the  movement  are  completely stipulated by the architect or designer , therefore its ability cannot surpass designer in any case. This opinion is right for the CDPL which do not have study ability , but is worth considering for the learning capability CDP. Because the ability of this kind of CDP  can be increased constantly in the application, after period of time, even the designer would not know the level of its ability. The CDPL has the extremely important status in the AI research work.  The intelligent machine system which does not have the learning capability is difficulty to be called a true one, but the former intelligent machine system generally lacks of the study ability . Its application has been throughout the various branches of AI area , such as gaming system ,expert or domain systems, automated reasoning system ,NLP, understanding Comprehension,PR,  computer  vision, intelligent  electromechanical  robots  and  other fields. Specific applications such as any search engines, medical diagnosis and medical search engines, detection of credit card fraud, stock market analysis applied in ecommerce, DNA sequencing sequences in bioinformatics, voice and handwriting detection and recognition  , strategy games and the use of machinery robots .  3.THE BASIC STRUCTURE OF THE CDPL    LEARNING SYSTEM Fig. 1 is the basic model and structure of CDPL. The environment provides certain energy informations to the learning part of system, and the learning part revisions knowledge base or library by using these information. To enhance the unique performance of system implementation part. In the environment of complete application, the knowledge base and   library and the execution part have decided the precise work content the learning part which needed to solve the task to be determined completely by the above three parts. We are going to narrate the impact of these three parts to design the CDP learning system separately as follows The most important factor which affects to CDP learning machine system design is the information which is provided to the system by the environment, specifically, or the information quality. Knowledge stored in guiding the development and implementation of part of the general principles of action. However, the overview and environment and to the CDP learning system is provided by a variety of information. If the information quality is higher and the difference of the general principle of equality is smaller, then the learning part is quite little easy to deal with. If CDP learning system to provide guidance and disorderly implementation of specific action specific information, the learning  system  deletes  of the  unnecessary details  after gaining sufficient dataDeduction, sums up the promotion, to form the general principles of guiding the action, and puts it into the knowledge base. Then the task of learning some of is heavier, the design is more tedious ,difficult and complex . Because the information obtained from the CDP learning system is often incomplete, reasoning is not entirely reliable which is carried out by reasoning . The rules summed up by reason  is  fact  possibly,  or  not.  This  must  be  tested through the implementation of the effect. The factcorrect rules make the system efficiency improve, it should be retained The incorrect rules should be modified or be deducted or deleted from the knowledge database.   The knowledge base is the second factor which affects to the  CDP learning  system design. The  knowledge expressed  in different  forms, for instance, characteristic vector, step comparing  sentence, reproduction pattern rule, semantic network ,association and frame and so on. These expressions have their feature characters respectively, when you choose any one of these expressions, you  must  take  into  four or many  aspects  account For example the four aspects for our work  1  Ability  to express strong  2 Easy reasoning  3   It is easy to modify Knowledge Base 4  Knowledge Representation is easy to expand and interpret. A problem  finally needs to explain which regarding the knowledge base library is studies the system not to be able not to completely have in any knowledge situation the baseless knowledge base in acquisition, each CDP learning system all requests to have certain knowledge to understand the environment provides the information,  the analysis comparison, makes the  supposition, examines and revises these  structured suppositions.     Therefore, to be precisely , the learning system is to  the existing knowledge expansion and the dynamic improvement. The execution fourth part is core of the entire CDP learning system, because operative part of the action is targeted and aimed at improving learning action. With the development and implementation of some related three issues are complexity, feedbackF and transparencyT.  4.SEVERAL COMMONLY LEARNING      METHOD  BASED ON LEARNING      STRATEGY  The learning strategy is a reasoning strategy which is used in the process of CDP learning system. The learning system is always composed by the learning and the environment had two parts. Learning Strategies is on the basis of the division and classification criteria for converting an electronic message students to achieve the necessary degree of difficulty reasoning and how to classify and follow simple to complex messages , from small to many multiorder divided into the following five basic types  4.1   Memorizing OR  Rote Learning The rote learning is the most simple machine learning method. The rote learning is the memory. That is, the new fact of knowledge is stored, the supply and demand wants when retrieves transfers, but does not need to compute  and the inference. 178International Journal of Scientific  Engineering Research Volume 2, Issue 6, June2011                                                                                                    ISSN 22295518  IJSER  2011 httpwww.ijser.org When the rote learning system operative to solve specific problems, the learning system remembers this query and the solutions. We can regard the learning system execution part as some function abstractly, before computing  and outputting the function value y1, y2,, yp, this function obtains the independent variable input value x1, x2,, xn. The rote learning makes a simple memory storage in the memory x1, x2,, xn,y1, y2,,yp. When it needs fx1, x2,, xn, but the execution part on y1, y2,, yp retrieves from the memory stored rather than recomptation. This kind of simple learning  pattern is as follows x1,x2,x n  f y1,y2,  yp  store x1,x2,x n, y1,y2,yp  4.2  Expert or Learner  ExplanationBased Learning The target  goal concept, a  specific example, the domain expert theory which provides according to the teacher and learners operational guidelines. First, a struct explanation showed satisfies the target goal idea for the assorted this qualified example. Then, explained the promotion explains  for target concept may operate the criterion the sufficient condition. EBL has been widely applied in the knowledge base refinement and the improvement of the learning  system performance by explanation.  4.3   Learning from Instruction The student teachers or other information sources such as journals, articles, textbooks and so on gains the  information from the environment, transforms the  knowledge into the expression form which the interior to   exterior may use, and combines the new knowledge with the original knowledge nonmechanically and organically. Therefore,   the student is required to have certain degree inference caliber and ability. However, now the environment still have to do a   lot   of   work.  The   teacher   proposes   and   organizes knowledge base by some form, to increase the knowledge base which the student has continuously. This learning method is similar with human being  societys university  teaching way. The learning duty is to establish a learning system that enables it to accept the opinion , guidance and   the   suggestion,   stores   and   implies   the   learning knowledge method effectively. Now, At present, many expert domain systems use this method to realize the knowledge base gain when they established the knowledge domain. 4.4   Learning by deduction The  deductive  reasoning  is used  by  the  learner .  The reasoning embarks from the lemma , axiom ,implications infers the conclusion after the logical transformation. This kind of reasoning is a process that is from fealty or allegiance  transform to specialize specialization   ,   the   learner    can   obtain   the   useful knowledge in the reasoning process. This learning method contains  macrooperation learning,  the  knowledge edition and the chunking technique . The inverse process of deductive reasoning is inductive reasoning.  4.5  Learning by Cognitive Process or        Analogy                                       The module maintains analogy prodigy is one kind of useful and effective inference method, it can succinctly describe similarity clearly between the entities  At the same time, it also either departer, d shifts certain test similar nature duty from the orator to the listener or learner ,l .Through the analogy, using the similarity between two different domains of the knowledge,   including   similar   characteristic   and   other nature  infers the  goal territory from the  source  territory knowledge the corresponding knowledge, we can learn from it. The analogy learning needs more case reasoning than the three kinds of learning ways above. It requests to retrieve the available knowledge generally from the knowledge source, then transforms it into the new form, apply it to the new condition . The analogy learning is playing the vital role in the human science ,engineering and technology history, many scientific discoveries are obtained by the analogy. 4.6  Inductive  Learning The inductive learning is the most widely as a symbol learning logics and methods. It expressed conceives the supposition from the example the process. The teacher or the environment provides some examples or the counter example in some concept, lets the student obtain the general description in this concept through the inductive reasoning.  This kind of learning reasoning work load is more heavier than the demonstration learning and the deduct learning , because the environment does not provide the general concept description for example axiom.To some extent, the number of induction learning reasoning is heavier than he analogy learning , because there is no one similar concept can be used as  source concept . The inductive learning is the most basic method, its development is a mature learning method  as  well,  it  has been  used  to  research  and  apply widely in the artificial intelligence, knowledge management domain.  5.   CROSSDISCIPLINARY PROFICIENCY         LEARNING  RESEARCH  AIM                  There are three aims in the Machine learning General  learning algorithm theoretical analysis and development Develops   the   humanity   to   learning   the   process   the computation model The structure specialpurpose learning system face the duty research.  5.1   General Learning Algorithm This direction research is the theoretical analysis duty and the development uses in the nonusable learning duty the algorithm. There is no limit to the algorithm type. The algorithm not necessarily is similar the method which uses in the humanity. Some person thought studies the knowledge structure which produces to be supposed to be similar humanitys knowledge structure at least, even if the learning process is different. 179International Journal of Scientific  Engineering Research Volume 2, Issue 6, June2011                                                                                                    ISSN 22295518  IJSER  2011 httpwww.ijser.org  At present, some scientists are researching the possible learning algorithm the theory space.  5.2   Cognitive  Model This direction is a studying humans learning computation theory and an experimental model for prediction. Not only had this kind of research vital significance of humanity education, but also of developing the machine learning system in diversity model.  5.3   The Goal of the Project This direction is aimed at solving the special actual problem, and developing to accomplish these tasks the project system. Not only do these questions often concern on the learning but also on other questions, for example, input signal by reasonable explanation or development question specialpurpose data conversion and transformation . 6. Conclusion   AI science is the only way that raises the CDP learning  intelligence level . Only improve the CDPlearning function and model continuously, can we make the CDP machine close to or surpasses the humanitys intelligent leveinterface. To the CDPL discussion  and  the  CDPL  research  progress,  will  certainly make the artificial intelligence and the entire science and technology further development in current era.  REFERENCES  1    Wu springhead, Liu Jiang.  Artificial intelligence and expert system.National   University   of   Defense   Technology   Publishing   house.1995160180, 277280 2 Schorr,H. Artificia intelligence Second Tide. Computer Information Packet, 1987 1Vol.431. 3    Wang Yongqing. Expert system WMES Research  and  Realization. The microcomputer Develops. 1994,61415. 4 HayesRoth,F.,Overview  of  Knowledge  Engineering.   Computer. 1984,Vol.17,No.91128. 5 Zhang  Yangsen  Artificial  Intelligence  Principle  and  Application  Higher Education Publishing House  2009 .          180International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                           ISSN 22295518  IJSER  2011 httpwww.ijser.org A Navel Approach for Evaluating Completeness of Business Rules using First Order Logic   M.Thirumaran, E. IIavarasan, R. Manoharan, Thanigaivel.K                                                                                   Abstract  Organizations are under increasing scrutiny to develop the rules to regulate their business requirements and it is important to standardize the business rules to automate the complex process into simple logic. So several process modeling languages and rule modeling languages are evolved to modulate the organizational policies and procedures. Business rules are the constraints which influence the behaviors and also specify the derivation of conditions that affect the execution flow. These rules are form of conditional operations attached to the process to give data result. These systems implement business rules that are restructured when organizations change the data to the varying business needs. When these rules are changed, it must be efficient to provide a decision based on the given constraints or based on business requirements. The correct decision logic are verified by evaluating or validating the completeness of the business rule. More often the rules can associate with another rule to derive business logic, but these rules are not complete enough to determine the computability of business logic. So it is necessary to check whether the rule is complete, this can be proved when the rules are interpreted with first order logic. This paper provides standard approach to evaluate the completeness of business rules by formulating the rules using first order logic. Applying these strategies introduces a structured approach and management aspects within rules by focusing on rule sources which are important for the process goal and providing a meaningful rule structure. It points out the necessity and also the real possibilities to establish new facilities for manipulation of business rules into the software development process. This can give rise increase in the performance of the legacy system.             Index Terms  Completeness, Rule Execution, Business process, Business rules, Business Rules Approaches, First Order Logic,                    Business rules, First Order logic, Completeness algorithm.         1. INTRODUCTION   any efforts are already made to promote and integrate business rules modeling and management tools into business processes automation. Being ready for execution as a part of business process management applications business rules and the business rules approach still seem to lack a common methodological ground and support. Especially the fact that, before business rules can be modeled and managed they first need to be captured and extracted from different sources is often left to analysts  intuition. Well known possible sources to look at to identify business rules are, among others process documentation, source code or implicit sources like internal problemsolving knowledge of the employees involved in this process. But since not many enterprises capture their business rules in a structured, explicit form like documents or implicit software codes, they need to be identified first, before being captured and managed. Here the question about how to identify the potential sources of business rules emerges. Though some of them may seem obvious, like legal restrictions, standards or mandatory best practices, some are implicitly included within the elements involved in the process. On the other hand, many enterprises capture their business processes in business process models, providing a structured view for further analysis and management. Business is performed according to rules. They make an important and integral part of each information system IS by expressing business logic, constraints on concepts, their interpretation and relationships. Therefore is relevant to pay special attention to business rules in development of information systems. Defined rules of structure and behavior are captured in the models of structures, states, processes and other IS models. In general, models can be expressed using graphic symbols, text and formulas. Modeling constructs are grouped into a variety of diagram types by various aspects, for example, process, static structure, states. These rules are need to check for M 181International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                           ISSN 22295518  IJSER  2011 httpwww.ijser.org consistency and completeness. In order to facilitate the completeness of the rules we are using First Order Logic.   2. LITERATURE SURVEY  Business Process Management BPM is an established discipline for building, maintaining, and evolving large enterprise systems on the basis of business process models. A business process model is a floworiented representation of a set of work practices aimed at achieving a goal, such as processing a customer request or complaint, satisfying a regulatory requirement, etc. The Business Process Modeling Notation BPMN is gaining adoption as a standard notation for capturing business processes. The main purpose of business process models generally, and BPMN models in particular, is to facilitate communication between domain analysts and to support decisionmaking based on techniques such as cost analysis, scenario analysis, and simulation Chun Ouyang 1 proposed an acyclic BPMN model, or an acyclic fragment of a BPMN model, falls under the class if it satisfies a number of semantic conditions such as absence of deadlock. He applied Petri net analysis techniques to statically check these semantic conditions on the source BPMN model. According to Michael zur Muehlen 2, Business Processes are sets of activities that create value for a customer. While research in Business Process Management initially focused on the documentation and organizational governance of processes, organizations are increasingly automating processes using workflow systems, and are building elaborate management systems around their processes. Such management infrastructures integrate modeling, automation, and business intelligence applications. The inclusion of compliance management activities is a logical next step in governing the business process life cycle. Josef Schiefer 3 says that Business Process Management BPM systems are software solutions that support the management of the life cycle of a business process. For the execution of business processes, many organizations are increasingly using process engines supporting standardbased process models such as WSBPEL to improve the efficiency of their processes and keep the testing independent from specific middleware. A major challenge of current BPM solutions is to continuously monitor ongoing activities in a business environment and to respond to business events with minimal latency. One of the most promising concepts that approaches the problems of closedloop decision making and the lack of gaining realtime business knowledge is the concept of Complex Event Processing CEP, The system automatically discovers and analyzes business situations or exceptions and can create reactive and proactive responses, such as generating early warnings, preventing damage, loss or excessive cost, exploiting timecritical business opportunities, or adapting business systems with minimal latency. Claire Costello 4 says A business process as a complete set of endtoend activities that together create value for the customer. Business process management is the ability to orchestrate and control the execution of a business process across heterogeneous systems and allow users to view the components of an infrastructure from a process view rather than set of applications and databases. Anca Andreescu 5 says every organization operates according to a set o business rules. These may be external rules, coming from legal regulations that must be observed by all organizations acting in a certain field, or internal rules which define the organizations business politics and aim to ensure competitive advantages in the market. Starting from the previous observations, it is obvious the important role that business rules play within the development process of a software system. Milan Milanovi1 6 proposed that BPM languages have limited support for representing logical expressions, business vocabularies, and business rules, which severely limits their flexibility and expressivity. To address these challenges, he integrated business rule modeling constructs of the REWERSE Rule Markup Language R2ML with the Business Process Modeling Notation BPMN, resulting in a rBPMN proposal. Olegas Vasilecas 7 says Business rules make an important 182International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                           ISSN 22295518  IJSER  2011 httpwww.ijser.org and integral part of each information system IS by expressing business logic, constraints of concepts, and their interpretation and relationships. Therefore it is relevant to pay special attention to business rules in development of information systems. Rules related to domain structure and behavior are presented in data, states, processes and other IS models. Taking into account that rules are expressed in several models, there is a risk that overall specification is inconsistent. Unambiguous models are crucial for the successful implementation of IS models transformation and finally code generation tasks. Therefore it is necessary to check consistency among related rules models. The problem of models inconsistency can be solved by using of formal or partially formal models with constraints. However formal models are often too complex to be used in practice. Semiformal models are widely used, but constraints used in such a models often are suitable only for one model and relationships among models are not defined. He suggests extending of IS approach based on semiformal models and constraints, by adding the consistency rules for IS models. Anis Charfi 8 applied the divide and conquer principle to web service composition by explicitly separating business rules from the process specification. The combination of the business rules approach with the processoriented composition solves a twofold problem. First, he provided a solution to the problem of dynamic adaptation of the composition. In fact, current standards for processbased web service composition are not capable to deal with the flexibility requirements of composite web services. Second, business rules are important assets of a business organization that embody valuable domain knowledge. So, it is no longer acceptable to bury them in the rest of the composition. Bruno de Moura Araujo 9 proposed Business rules BR are declarations which constrain, derive and give conditions for existence, representing the knowledge of the business. BRs are not descriptions of a process or processing. Rather, they define the conditions under which a process is carried out or the new conditions that will exist after a process has been completed. BR can be represented using Semantics of Business Vocabulary and Rules SBVR.  SBVR is appropriate to be used by business experts, since it allows the representation of business vocabulary and rules using controlled natural language. Business vocabulary concepts can be automatically transformed into conceptual models, like the UML class model. Nicholas Zsifkov 10 says Enterprise business rules are usually defined as constraints or as metadata about business operations on the business side, business rules are special policies that define constraintsmetadata about the business operation on the information system implementation side, business rules are constraints about the data, about data manipulation and about system processes. Antonio Oliveira Filho 11 proposed a new traceability technique that defines dependency links with the same semantics that can be observed in the relationships among business rules. The goal of the technique is to provide rates of recall and precision of 100 for changes in software requirements that correspond to business rules. Jose F. Mejia Bernal 12 says decomposing the initial business process structure in a set of rules is a procedure based on pattern identification. This approach consists of two phases mapping of business process to rules, and reliable adaptation of the business process according to the context data. The first phase is executed to provide a representation of the initial business process definition in terms of rules. The second phase is applied to provide a reliable workflow process modification. Timon C. Du and HsingLing Chen propose an active collaboration and negotiation framework ACNF, which is a negotiation support system that uses active documents with embedded business logics or business rules that can adapt to different collaborative strategies in a businesstobusiness B2B environment 13. Sam Weber and Isabelle Rouvellou describe a fully functional prototype middleware system which provides the users to control software components without the need of programming knowledge. Thus the core applications need not be altered for anticipated changes from external factors. In this system, application behavior modification is fast and easy, making this middleware suitable for 183International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                           ISSN 22295518  IJSER  2011 httpwww.ijser.org frequently changing programs 14. H. M. Sneed15, in the context of reverse engineering source code into UML diagrams many tools and approaches have been developed.CPP2XMI is a reverse engineering tool which lows extracting UML class, sequence and activity diagrams in XMI format from C source code. Sangseung Kang16, Business rules are business statements that define some aspect of a business. They describe, constrain and control the structure, operations and strategy of the business. In his paper, he analyzes business rules and business rule systems, and present requirements and considerations for the business rule expression and system. Olga Levina17, suggests an extraction process for business rules identification from business process models. Applying this process introduces a structured approach and management aspects within rules discovery by focusing on rule sources that are important for the process goal and providing a rule structure. Mohammed Alawairdhi18, suggests a businesslogicbased framework for evolving software systems is proposed. The goal of the framework is evolving software in a higher abstract layer. Olegas Vasilecas19, suggests The  rules are expressed in several models, there is a risk that overall specification is inconsistent. Unambiguous models are crucial for the successful implementation of IS models transformation and finally code generation tasks. Therefore it is necessary to check consistency among related rules models. The problem of models inconsistency can be solved by using of formal or partially formal models with constraints. According to Anis Charfi20, the Business Rules Group , a business rule is a statement that defines or constrains some aspect of the business. It is intended to assert business structure or to control the behavior of the business. Business rules are usually expressed either as constraints or in the form if conditions then action. The conditions are also called rule premises. The business rule approach encompasses a collection of terms definitions, facts connection between terms and rules computation, constraints and conditional logic. Terms and Facts are statements that contain sensible business relevant observations, whereas rules are statements used to discover new information or guide decision making. Jose F. Mejia Bernal21, proposed a way for representing Dynamic Business Process in terms of Rules based on patterns identification. With this approach it is easy to apply on a business process instance both userbased personalization rules and automatic rules inferred by an underlying contextaware system. Gulnoza Ziyaeva proposed framework to enable the contentbased intelligent routing path construction and message routing in ESB which defines the routing tables and mechanisms of message routings and facilitate the service selection based on message content 22.         3. Rules for Car license  SNO Rule Syntax JESS Syntax Jrule  First Order Logic 1 Car must not be rented to if Customer.ValidLicenceNumber  FALSE  def rule no car rented without lenience no ifCustomer.ValidLicenceNumber  true def rule no car rented without lenience no when Customer.ValidLicenceycustomer idy  zlicence noz              application 184International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                           ISSN 22295518  IJSER  2011 httpwww.ijser.org customers without a valid license number.            application.Status  Reject      Customer.Eligibile  false                          addapplication.statusreject      addcustomer.eligiblefalse   Number  true  then application.statusreject      customer.eligiblefalse  statusrejecty,z.  2 Car must not be rented to customers of Age less than 18. if Customer.age  18                                                         application.Status  Reject                                Customer.Eligibile  false                           def rule no car rented to customer of age less than 18 ifcustomer.age  18ageage addapplication.statusreject      addcustomer.eligiblefalse  def rule no car rented to customer of age less than 18 when  customer.age   18age  then application.statusreject      customer.eligiblefalse  ycustomer.agey     application statusvalidy   c customer eligiblec,y  3 Car must not be rented to customers with bad history level 3   if Customer.BadHistoryLevel  3                                                         application.Status  Reject                                Customer.Eligibile  false                          def rule no car to customer of bad history level 3  ifCustome.badhistorylevel  3 addapplication.statusreject      addcustomer.eligiblefalse   def rule no car to customer of bad history level 3 when Custome.badhistorylevel  3  then application.statusreject      customer.eligiblefalse   ycustomer.bad historylevely    application statusrejecty     customer eligiblefalse c,y          4  Rent for small cars is 80 aud per day    if Customer.Eligible  true     if Car.type  Small      ent.RentPerDay  80   def rule rent for small car is 80day  ifCustomer.Eligible true  ifcartypesmall  addrent.perday  def rule rent for small car is 80day when Customer.Eligible true  ifcartypesmall  then   ycustomer.eligibletruey                       rrent per day80,y,c      185International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                           ISSN 22295518  IJSER  2011 httpwww.ijser.org               price 80   rent.perdayprice 80  5 Rent for awd cars is 100 aud per day.    if Car.type  AWD      rent.RentPerDay  100     def rule rent for awd cars 100day   ifcar.typeAWD  addrent.perdayprice 100   def rule rent for awd cars 100day when  car.typeAWD   then rent.perdayprice 100    ycar.typetruey,AWDc rrent per day100,y,c   6 Rent for luxury cars is 150 aud per day    if Car.type  Luxury        rent.RentPerDay  150       def rule rent for luxury cars 150day   ifcar.typeluxury  addrent.perdayprice 150   def rule rent for luxury cars 150day when  car.typeluxury   then rent.perdayprice 150   ycar.typetruey,luxury carc rrent per day150,y,c   7 Rent payable is calculated as the product of rentperday and rentalperiod in days.   rent.RentPayable  rent.RentPerDay  rent.No of rent days   if CustomerBadHistoryLevel  0    def rule rent calculated as the product of rentperday and rentalperiod in days   rent.rentpayablerentperdayrentalperiod ifCustome.badhistorylevel  0 addrent.rentpayablepriceprice  def rule rent calculated as the product of rentperday and rentalperiod in days  when rent.rentpayablerentperdayrentalperiod ifCustome.badhistorylevel  0  then rent.rentpayablepriceprice  yrent.payablerent.per dayzno of rent per daya ccustomer .bad history level0  z,a,c  8 Penalty of 20  of rent must be applied for customers with bad history level 2.  if Customer.BadHistoryLevel  2       rent.PenaltyFee  rent.RentPayable  0.2       def rule Penalty of 20  of rent  for customers with bad history level 2  ifCustomer.BadHistoryLevel2   addrent.penaltyfeerent.rentpayable0.def rule Penalty of 20  of rent  for customers with bad history level 2 when Customer.BadHistoryLevel2    then rent.penaltyfeerent.rentpayable0.2  ycustomer.badhistorylevelypPenality.feep   rrent.payabler0.2 186International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                           ISSN 22295518  IJSER  2011 httpwww.ijser.org  2   9 Penalty of 10  of rent must be applied for customers with bad history level 1. if Customer.BadHistoryLevel  1      rent.PenaltyFee  rent.RentPayable  0.1          def rule Penalty of 10  of rent  for customers with bad history level 1  ifCustomer.BadHistoryLevel1   addrent.penaltyfeerent.rentpayable0.1 def rule Penalty of 10  of rent  for customers with bad history level 1  ifCustomer.BadHistoryLevel1    then rent.penaltyfeerent.rentpayable0.1   ycustomer.badhistorylevelypPenality.feep   rrent.payabler0.1    4. COMPLETENESS  The problem is to prove the business rules are complete and also to show the rules are semantically valid. When the correspondence between the syntax and semantics tighter , we would say the logic  is complete. Generally the business  logic consists of four major tuples such as Rules, Functions, Parameters and dependency relation which are related as                                                                     Therefore any semantically valid argument can be captured by formal proof. The choice of rules are to be made for its completeness. It can be easily done when the system is in static phase. When it is done in runtime the conditional variable and iterative variable are also changes. When these variables are modified it brings out bugs in the logic. So these variables must be declared with  certain range. The extra complications come about because of these restrictions, needed to guarantee soundness of the rules , on the status of the variables. An algorithm has been proposed by considering the above standards.                        LR,F,P,D                            Let L be the Business Logic, which consists of 4tuples such as R  Set of Rules, F Set of Functions,  P Set of Parameters D Set of dependency relation                     187International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                           ISSN 22295518  IJSER  2011 httpwww.ijser.org                                                                                                                                                          Fi                                                              When i value is between 0 to n  Then add  i value to function fx,fy,fz Modify the Logic and store in FSM and if it does not tend to give logic make  i    else  repeat.  Each state of the simulated FSM stores the parameters and its  current value  Find all the parameters are  Px1, px2........pK  q1 ,q2,....qn,t   t,r1,r2,...,rm s1,s2,s3............sn  P1, p2....pk, r1,r2,...rm q1,q2,...q1,  s1,s2,s3,...sn Theorem Set of rules is contradictory if and only if another rule can be derived from it.  proves completeness Proof   Consider the conclusion is not empty.  The value of F, q1,q2,q3....ql, s1,s2,s3...sn all have value T and p1,p2,pk , q1,q2,......ql  r1,r2,...,rm all have value F. If t has value f then t, r1,r2,...,rm  s1,s2,s3............sn has value f.   The value of F, q1,q2,q3....ql, s1,s2,s3...sn all have value T  Fq q1,q2,q3....ql, s1,s2,s3...sn    p1,p2,pk , q1,q2,......ql  r1,r2,...,rm all have value F.         p1,p2,pk , q1,q2,......ql   t   If t has value f then t, r1,r2,...,rm  s1,s2,s3............sn has value f. If T fx Then r1,r2,...,rm s1,s2,s3............sn f else   If conclusion is empty then klmn0.       Output Parameters O1,O2 are bound with discrete values then return Boolean value True It proves the Completeness logic.                                                                        Completeness Theorem  LR,F,P,D Let L be the Business Logic, which consists of 4tuples such as  R  Set of Rules    F Set of Functions P Set of Parameters D Set of dependency relation  Theorem The Business Logic L is complete iff  the associated Rules, Functions, Parameters and the dependency relations are complete. Proof  Parameters are complete   Input Parameters I1,I2,I3 are bound with discrete values then return Boolean value True   Get all the parameters including the temporary variables and add it into the parameter list. Let T be the temporary variable. T is valid only if fx,fy,fz is in bound Ft fx,fy,fz     Ft                       Since ft   Let x,y        X                                                        fx,fy      f z,y     for some y,z                             fx       f x ,y     for some y              x     f x                 x        Hence every temporary variable which has parameter is associated with function.  Create FSM for the parameter list by following the sequential relation from the logic source to terminating point.  If the parameter and the function gives the logic at the runtime the iterative and conditional value are get changed. Let conditional variable be ii1,i2,i3,......in  Let the iterative variable be j and k  j1,j2,j3...jn  k1,k2,k3.....kn  .  188International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                           ISSN 22295518  IJSER  2011 httpwww.ijser.org                         5.PERFORMANCE OF SPEEDUP ALGORITHM  Fig1. Performance of speed up algorithm  Defines the time taken to response for each rule change request second.This graph shows the responsiveness of the rule editor based on the given request. The proposed work responsiveness for the developers request  sec is less when compared to the previous work.    6. CONCLUSION This paper provides platform for the business rules to be complete with the help of well defined language. It not only provides completeness for the business rules, but also they could generate definitions of executable rules, processes, and services for the developers. Previous attempts mainly provide rules on the level of concrete syntax without precise language definitions. This system will allow users to model and manage comprehensive rule sets which generate responses in the form of new events. A key focus of this future research work will be the visualization of events which have been processed with sense and respond rules.    7. REFERENCES  1 Chun Ouyang , From Business Process Models to ProcessOriented Software Systems  2 Michael zur Muehlen, Business Process and Business Rule Modeling Languages for Compliance Management A Representational Analysis, TwentySixth International Conference on Conceptual Modeling  ER 2007  Tutorials, Posters, Panels and Industrial Contributions, Auckland, New Zealand. International Journal on Web Service Computing IJWSC, Vol.1, No.2, December 2010 .2831  3 Josef Schiefer, EventDriven Rules for Sensing and Responding to Business Situations.  4 Claire Costello,  Orchestrating Supply chain Interactions using Emerging Process Description Languages and Business Rules.  Algorithm for speed up  begin procedure Define business rule Jrule def rule string get the rule  rule string defining business policy pass array str function input str  rule do read the rule from rule base ainput.next whileaddingrule get the set of conditional rules  for each rule in the memory  locating memory size set . memory sizesize of memory  add facts rule to memory Ifoldsizenull then  update rulememory ifrule.activetrue search new facts rules ifconditional rulepositive then addnew rule else no action end if  for deleting rule from memory if size of memorynull set rule activefalse else if if rule activetrue validate the rule end if end if end for end procedure  189International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                           ISSN 22295518  IJSER  2011 httpwww.ijser.org 5 Anca Andreescu,  A General Software Development Process Suitable for Explicit Manipulation of Business Rules  6 Milan Milanovic1,  Modeling Service Orchestrations with a Ruleenhanced Business Process Language  7 Olegas Vasilecas, Ensuring Consistency of Information Systems Rules Models.  8 Anis Charfi,  Hybrid Web Service Composition Business Processes Meet Business Rules.  9 Bruno de Moura Araujo,  A method for Validating the Compliance of Business Processes to Business Rules.  10 Nicholas Zsifkov,  Business Rules Domains and Business Rules Modeling.  11 Antonio Oliveira,  Filho Change Impact Analysis   from Business Rules.  12 Jose F. Mejia Bernal,  Dynamic ContextAware Business Process A RuleBased Approach Supported by Pattern Identification.  13 Timon C. Du and HsingLing Chen ,Building a MultipleCriteria Negotiation Support System, IEEE transactions on knowledge and data engineering, vol. 19, no. 6, June 2007.  14 Sam Weber, Hoi Chan, Lou Degenaro, Judah Diament, Achille FokoueNkoutche, and Isabelle Rouvellou, Fusion A System For Business Users To Manage Program Variability, IEEE Transaction on software engineering , Nov 2008.  15 H. M. Sneed, Extracting Business Logic from Existing COBOL Programs as a Basis for Redevelopment, 9th International Workshop on Program Comprehension, Toronto, Canada, 2001, pp. 167175.  16 Sangseung Kang, Design of Rule Object Model for Business Rule Systems, 1996,pp. 818822.  17 Olga Levina, Extracting Business Logic from Business Process Models, 2010 IEEE.  18 Mohammed Alawairdhi, A BusinessLogic Based Framework for Evolving Software Systems, 2009 33rd Annual IEEE International Computer Software and Applications Conference.  19 Olegas Vasilecas, Ensuring Consistency of Information Systems Rules Models, the International Multiconference on Computer Science and Information Technology, 2008.  20 Anis Charfi,  Hybrid Web Service Composition Business Processes Meet Business Rules, ICSOC04, November 1519, 2004, New York, New York, USA.  21 Jose F. Mejia Bernal,  Dynamic ContextAware Business Process A RuleBased Approach Supported by Pattern Identification, SAC10, March 2226, 2010, Sierre, Switzerland.  22 Gulnoza Ziyaeva, Eunmi Choi, and Dugki Min, ContentBased Intelligent Routing and Message Processing in Enterprise Service Bus, International Conference on Convergence and Hybrid Information Technology, 2008  23 en.wikipedia.orgwikiBusinessrule   Authors  Thirumaran. M, working as Asst.Professor in Pondicherry Engineering College, Pondicherry, India, one of Indias premier institutions providing high quality education and a great platform for research. He pursued his B.Tech and M.Tech in Computer Science and Engineering from the Pondicherry University. The author is specialized in Web Services and Business Object Model and possesses a very profound knowledge in the same. He has worked on establishing the Business Object Model and Business Logic System with respect to Web Service Computation, Web Service Composition and Web Service Customization. His flair for research has made him explore deep in this domain and he has published more than 20 papers in various International Conferences, Journals and Magazines. Currently 190International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                           ISSN 22295518  IJSER  2011 httpwww.ijser.org he is working on developing a model for Business Logic Systems for various ECommerce systems. Ilavarasan. E, working as Associate Professor in Pondicherry Engineering College, Pondicherry, India. He received his Bachelors degree in Mathematics from the University of Madras in the year 1987 and Masters degree in Computer Applications in the year 1990 from Pondicherry University. Later he completed his M.Tech., degree in Computer Science and Engineeering at Pondicherry University in the year 1997. He has published more than Twenty five research papers in the International Journals and Conferences. His area of specialization includes Parallel and Distributed Systems, Design of Operating System and Web Technology. Thanigaivel. K, studying in Pondicherry Engineering College, Pondicherry, India. He received his B.Tech degree in Computer Science and Engineering from the Pondicherry University in the year 2009. He currently pursuing M.Tech., degree in Computer Science and Engineering at Pondicherry University and he is currently working in the area of Webservices.   191International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                        ISSN 22295518                                          IJSER  2011  httpwww.ijser.org   Implementation of RSA Cryptosystem Using Verilog  Chiranth E, Chakravarthy H.V.A, Nagamohanareddy P, Umesh T.H, Chethan Kumar M.   AbstractThe RSA system is widely employed and achieves good performance and high security. In this paper, we use Verilog to implement a 16bit RSA block cipher system. The whole implementation includes three parts key generation, encryption and decryption process. The key generation stage aims to generate a pair of public key and private key, and then the private key will be distributed to receiver according to certain key distribution schemes. Data security is achieved after the 64bit input data are block encrypted by RSA public key. The cipher text can be decrypted at receiver side by RSA secret key. These are simulated in NC LAUNCH and hardware is synthesized using RTL Compiler of CADENCE. Netlist generated from RTL Compiler will be used to generate IC. Index Terms  Cadence, Cryptosystem, Decryption, Encryption, Implementation, Key Generation, Modular Exponentiation, Modular Multiplication, RSA, Verilog.         1   INTRODUCTION  HE first public key scheme was developed in 1977 by Ron Rivest, Adi Shamir, and Len Adleman at MIT. Now RivestShamirAdleman RSA is the most widely accepted and implemented public key cryptosystem. The public key system is based on using different keys, one key for encryption and a different but related key for decryption. The whole process involves computing the remainder after exponential and modular operation of large number. Encryption and decryption have the following form, for some plaintext block M and cipher text block C  C  M e mod n. M  C d mod n.  Generally, it includes a third party to generate a pair of public key and to distribute keys to transmitter and receiver. Transmitter and receiver should both know the value of n. The transmitter has the knowledge of public key e, and only the receiver knows the private key d. Thus, a public key of e, n and secret key d, n generated by third party is distributed to transmitter and receiver separately. For this algorithm to be satisfactory for publickey encryption, the following requirements must be met      1. It is possible to find values of e, d, n that Mde mod n  M1 for all M  n.     2. It is relatively easy to calculate Me mod n and Cd for all values of M  n.     3. It is infeasible to determine d given e and n.  Steps involved in Implementation of RSA   The following step is taken to implement the RSA public key scheme  1. Choose two large prime numbers, p and q. Let npq, Let n  p1q1. 2. Randomly choose a value e 1 e  n, which is relative prime to n that gcd e, n. 3. Calculate de1 mod n, send public key e, n to transmitter and secret key d, n to receiver. 4. Transmitter encrypt the original message, C  M e mod n, then send cipher text to receiver.  5. Receiver decrypt cipher text by M  C d mod n and retrieve the original message.  The rest of the paper is organized as follows Section 2 gives an overview of RSA Implementation. Section 3 gives the simulation results. Section 4 gives the conclusion. The final section gives the references used.  2   RSA Implementation  The RSA algorithm was inverted by Rivest, Shamir, and Adleman in 1977 and published in 1978. It is one of the simplest and most widely used publickey cryptosystems. Fig.1 summarizes the RSA algorithm.  T192International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                        ISSN 22295518                                          IJSER  2011  httpwww.ijser.org     Fig.1 The RSA algorithm   Fig.2 The system architecture for RSA key generation  The system architecture for key generation is shown in Fig.2. A random number generator generates 16bit pseudo random numbers and stores them in the rand FIFO. Once the FIFO is full, the random number generator stops working until a random number is pulled out by the primality tester. The primality tester takes a random number as input and tests if it is a prime. Confirmed primes are put in the prime FIFO. Similarly to the random number generator, primality tester starts new test only when prime FIFO is not full. A lot of power is saved by using the two FIFOs because computation is performed only when needed. When new key pair is required, the down stream component pulls out two primes from the prime FIFO, and calculates n and n. N is stored in a register. n is sent to the Greatest Common Divider GCD, where public exponent e is selected such that gcdn, e  1, and private exponent d is obtained by inverting e modulo n. E and d are also stored in registers.  Once n, d, and e are generated, RSA encryptiondecryption is simply a modular exponentiation operation. Fig.3 shows the RSA encryptiondecryption structure in hardware implementation.                 Fig.3 The RSA encryptiondecryption structure  The core of the RSA implementation is how efficient the modular arithmetic operations are, which include modular addition, modular subtraction, modular multiplication and modular exponentiation. The RSA also involves some regular arithmetic operations, such as regular addition, subtraction and multiplication used to calculate n and n, and regular division used in GCD operation  2.1   Random Number Generator  Linear Feedback Shift Register LFSR is used to generate random numbers. In theory, an nbit linear feedback shift register can generate a 2n  1bit long  random sequence before repeating. However, an LFSR with a maximal period must satisfy the following property the polynomial formed from a tap sequence plus the constant 1 must be a primitive polynomial modulo 2. We are unable to find the primitive polynomial for a 16bit LFSR, so we implemented a 16bit LFSR and used its least significant 16 bits to generate 16bit  random numbers. Fig.4 shows the structure of the 16bit LFSR. 193International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                        ISSN 22295518                                          IJSER  2011  httpwww.ijser.org    Fig.4 Structure of the 16bit LFSR  2.2   GCD  After we get n, we need to find a small number e with gcdn,e  1, which indicates that e is relative prime to n.    Extended Euclidean algorithm was implemented to find gcdn,e, if the gcd is 1, e and its multiplicative inverse d are returned. The following pseudo code shows  Euclidean algorithm and extended Euclidean algorithm.   Euclida,b Aa Bb loop if B0    Return Agcda,b end if RAmodB AB BR goto loop  Extended Euclid m,b A1, A2, A3  1, 0, m B1, B2, B3  0, 1, b loop if B3  0     return A3  gcdm,b end if if B3  1    return B3  gcdm,bB2  b1 mod m end if Q  A3B3 T1, T2, T3  A1   QB1, A2   QB2, A3  QB3 A1, A2, A3  B1, B2, B3 B1, B2, B3  T1, T2, T3 goto loop 2.3   Encryption and Decryption  Modular Multiplication We constructed modular multiplication using shiftadd multiplication algorithm. Let A and B are two kbit positive integers, respectively. Let Ai and Bi are the ith bit of A and B, respectively. The algorithm is stated as follows  Modular Multiplication Input A, B, n Output M  AB mod n  P  A M  0 for i  0 to k1    if Bi  1        M  M  P mod n    end if    P  2P mod n end for return M  For a 16bit modular multiplier, inputs A and B are both 16 bits. However, B might be a small number with a lot of leading 0s. In the implementation, before getting into the shiftadd iterations, we search for the position of the first leading 1 in B, and set k1 to be this position. By doing this, we avoid unnecessary shift and modular operations, making the multiplication faster when B is small. We used Omuras method to correct the partial product M and temporary value P when any of them becomes greater than 216. The corrected M and P may still greater than n, so before returning M, we will do final correction on M to make sure M is less than n.   Modular Exponentiation The modular exponentiation operation is simply an exponentiation operation where modular multiplication is intensively performed. We implemented the 16bit modular exponentiation components using LR binary method, where LR stands for the lefttoright scanning direction of the exponent. The following pseudo code describes the LR binary algorithm.   Modular Exponentiation Input A, B, n Output E  AB mod n  E  1 for i  k1 to 0    if Bi  1       E  AE mod n    end if    if i  0       E  EE mod n    end if 194International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                        ISSN 22295518                                          IJSER  2011  httpwww.ijser.org   end for return E  Similar to modular multiplication, we search for the position of the first leading 1 in exponent B and set k  1 to be the position. This avoids unnecessary modular squaring operations. For small exponent such as the public exponent e, the modular exponentiation is much faster than big exponent such as the private exponent d.   3   Simulation results              The Random number generator, Primality tester, GCD, Encryption and decryption are written in Verilog Code and simulated in NC Launch and synthesized in RTL Compiler and Results are mentioned below. Sections below gives the respective simulation results.   3.1   Random number generator          The 16bit random number generator implemented in Cadence using the 16bit LFSR is shown in Fig.5.       Fig.5 Simulated Waveform for Random number generator in nclaunch  Fig.5 shows the waveform of the random odd number generator which has generated few odd numbers that is 1091, 5455 and 20863.  After using the path of setup.g and slownormal.lib and elaborating, the RTL view is generated for random number generator.   3.2   Primality tester  The 16bit Primality tester implemented in Cadence is shown in Fig.6.    Fig.6 Simulated Waveform for Primality tester in nclaunch  Fig.6 shows the waveform for primality tester in which 37 is given as input and it checks whether the number is prime or not, and resulted in giving 37 as prime number.  After using the path of setup.g and slownormal.lib and elaborating, the RTL view is generated for Primality tester.  3.3   GCD  The 16bit GCD extended Euclidean algorithm implemented in Cadence is shown in Fig.7.    Fig.7 Simulated Waveform for GCD in nclaunch  Fig.7 shows the waveform for extended Euclidean algorithm in which two inputs are given A372 and B35, the resulted output is the public key e5 and the private key d29.  After using the path of setup.g and slownormal.lib and elaborating, the RTL view is generated for GCD.  3.4   Encryption and Decryption  The 16bit Modular Multiplication implemented in Xilinx ISE design suit is shown in Fig.8. 195International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                        ISSN 22295518                                          IJSER  2011  httpwww.ijser.org      Fig.8 Simulated Waveform for Modular multiplication in Xilinx  Fig.8 shows the waveform for modular multiplication module in which the input value is A79, B83 and N58, the resulted output is M3.  The 16bit Encryption module implemented in Cadence is shown in Fig.9,10.    Fig.9 Simulated Waveform for encryption in nclaunch  Fig.9 shows the waveform for encryption module in which the input value is e5, n91 and plain text p8, the resulted output is the encrypted cipher text m8.    Fig.10 RTL view of  encryption  After using the path of setup.g and slownormal.lib and elaborating, the RTL view generated for Encryption is shown in Fig.10.   The 16bit decryption module implemented in Cadence is shown in Fig.11,12.    Fig.11 Simulated Waveform for decryption in nclaunch  Fig.11 shows the waveform for decryption module in which the input value is d29, n91 and cipher text p8, the resulted output is the encrypted plain text m8.  196International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                        ISSN 22295518                                          IJSER  2011  httpwww.ijser.org     Fig.12 RTL view of  decryption  After using the path of setup.g and slownormal.lib and elaborating, the RTL view generated for decryption is shown in Fig.12  3.5   Top Module RSA  The 16bit Entire RSA Module  implemented in Xilinx ISE design suit is shown in Fig.13.    Fig.13 Simulated Waveform for Top module RSA in Xilinx  Fig.13 shows the waveform of the Top module of entire RSA. Here the random prime number generated are 101 and 43 which are P and Q ie P101 and Q43. After getting P and Q, n and n are calculated ie n4340 and n4200. And public key e is selected by random prime number as e11. GCD checks whether e and n are relatively prime or not by getting the GCD as 1, and using Extended Euclidean algorithm d is calculated as d2291. Now the plain text M88 is given, and by using e and n, after encryption we get the cipher text C586. Now by using d, n and the cipher text, after decryption we got back the plain text P88.   4   Conclusion  In this, we implemented a 16bit RSA circuit in Verilog. It is a fullfeatured RSA circuit including key generation, data encryption and data decryption. In our Verilog implementation of RSA, we have implemented GCD algorithm using Euclidean algorithm, random number generator using LFSR and Encryption and Decryption using Modular multiplication and modular exponentiation algorithms LR binary algorithms. Each subcomponent and top module of RSA was simulated in CadenceXilinx and proved functionally correct. Netlist were generated which will be used to generate IC.  The total area required for encryption and decryption is 6060nm.         We can gain more security than the other strategy because we use the random numbers. And our implementation can easily extend to large bits such as 256 or 1024 or even longer. Future work has to be carried out on Cadence and xilinx to implement in FPGA and to generate IC.  References  1 R.L.Rivest, A.Shamir, and L. Adleman, A Method for     Obtaining Digital Signatures and PublicKey Cryptosystems, Communications of the ACM 21 1978 2 Behrouz A.Forouzan, Cryptography and Network Security,   Tata McGraw Hill, Special Indian Edition 2007. 3 William Stallings,  Cryptography and Network Security, PrenticeHall of India private limited, Third Edition  2004. 4 Neal Koblitz, A Course in Number   Theory and Cryptography,   Springer, Second Edition 2000. 5  Implementing the Rivest, Shamir, Adleman cryptographic   algorithm on the Motorola 56300 family of digital signal processors. httpieeexplore.ieee.orgstampstamp.jsparnumber00535035 6 Modular Arithmetic for RSA Cryptography. Courtesyhttpgtk.hopto.org8089MODULARRSA.pdf 7 RSA Encryption Courtesyhttpwww.geometer.orgmathcirclesRSA.pdf 8 Implementing a 1024bit RSA on FPGA. Courtesyhttpwww.arl.wustl.edujl1educationcs502courseproject.htm. 9 Ridha Ghayoula, ElAmjed Hajlaoui, Talel Korkobi, Mbarek Traii, Hichem Trabelsi, FPGA Implementation of RSA Cryptosystem, International Journal of Engineering and Applied Sciences 23 2006.  10 TzongSun Wu, HanYu Lin, Secure Convertible Authenticated Encryption Scheme Based on RSA, Informatica 33 2009 481486. 11 Guilherme Perin, Daniel GomesMesquita, and Joao BaptistaMartins, MontgomeryModularMultiplication on Reconfigurable Hardware Systolic versus Multiplexed  Implementation, Hindawi Publishing Corporation International Journal of Reconfigurable Computing Volume 2011. 197International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                        ISSN 22295518                                          IJSER  2011  httpwww.ijser.org   12 Muhammad I. Ibrahimy, Mamun B.I. Reaz, Khandaker Asaduzzaman and Sazzad Hussain, FPGA Implementation of RSA Encryption Engine with Flexible Key Size, International Journal of Communications. 13 ChungHsien Wu, JinHua Hong and ChengWen Wu, VLSI Design of RSA Cryptosystem Based on the Chinese Remainder Theorem, Journal of Information Science and Engineering 17, 967980 2001. 14 Md. AliAlMamun, Mohammad Motaharul Islam, S.M. Mashihure Romman and A.H. Salah Uddin Ahmad,  Performance Evaluation of Several Efficient RSA Variants,  IJCSNS International Journal of Computer Science and Network Security, VOL.8 No.7, July 2008 15 Ramzi A. Haraty, N. ElKassar and Bilal Shibaro, A Comparative Study of RSA Based Digital Signature Algorithms, Journal of Mathematics and Statistics 2 1 354359, 2006.  16 YiShiung Yeh, TingYu Huang, HanYu Lin and YuHao Chang, A Study on Parallel RSA Factorization, Journal of Computers, vol. 4, no. 2, February 2009. 17 D. Boneh and H. Shacham, Fast Variants of RSA, CryptoBytes, Vol. 5, No. 1, pp. 19, 2002.  Authors                       Chiranth E is currently pursuing B.E degree in Electronics and Communication Engineering in Bahubali College of Engineering, Shravanabelagola, India. Under Visvesvaraya Technological University, Belgaum, India. Email chirubornfreeyahoo.co.in Chakravarthy H.V.A is currently pursuing B.E degree in Electronics and Communication Engineering in Bahubali College of Engineering, Shravanabelagola, India. Under Visvesvaraya Technological University, Belgaum, India. Email abhishekchakravarthygmail.com  Nagamohanareddy P is currently pursuing B.E degree in Electronics and Communication Engineering in Bahubali College of Engineering, Shravanabelagola, India. Under Visvesvaraya Technological University, Belgaum, India. Email mohan.reddy79yahoo.com  Umesh T.H is currently pursuing B.E degree in Electronics and Communication Engineering in Bahubali College of Engineering, Shravanabelagola, India. Under Visvesvaraya Technological University, Belgaum, India. Email umeshdeepugmail.com  Chethan Kumar M received his  B.E degree in Electronics and Communication Engineering in Bahubali College of Engineering, Shravanabelagola, India. Under VTU Belgaum. Currently he is working as a lecturer in Electronics and Communication Department, Bahubali College of Engineering. Email chethankumarmyahoo.co.in  198International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518   IJSER  2011 httpwww.ijser.org  Metrics for Component Based Measurement Tools  P. Edith Linda, V. Manju Bashini, S. Gomathi  Abstract The main aim of this paper is to integrate the different object oriented metric tools and make them available as a single addon. The first part of this paper analyzed five different tools and they are migrated into one to make use of those tools in efficient manner.  Index Terms Metrics, component, decompiler, SLOC, LOC, NOS, NOP, Cyclomatic complexity        1 INTRODUCTION                                                                     HE emerging Object oriented programming is most widely used by the programmers to develop the software. The object oriented programs are used in the IT world which is used for secured transactions and mainly convenient. Even though they are widely used, the complex and confusing structure may arise if the program is not properly measured and not properly structured. To overcome this problem many proposed varieties of metric tools to measure the criteria of Object oriented programming are used. Again the problem arises that some metric tools measure the particular criteria of the object oriented programs. In order to overcome the individuality of those tools, they should be migrated.   Component based measurement tool is the concept of migrating many tools available widely. The main aim is to search some important tools and making them as a single addon to provide the user friendly environment.  Each individual tool will measure some parameter to measure the java program. But  a single tool will not measure all the parameters. Thus there exist a need of collection of tools to measure the java programs which will satisfy all the parameters to be measured.  2     METRICS Software metrics are used to measure the software quality to check whether it satisfies the        requirements. Metrics are defined as Quantifiable measures that could be used to measure      characteristics of a software system or the software development process.  Software metrics are essential to plan, predict, monitor, control, evaluate, products and processes. The main goal of the software metrics is to reduce costs, Improve quality, ControlMonitor schedule, small testing effort, many reusable fragments, to better understand the quality of the product and the program. 3 METRICS CATEGORIZATION Metrics can be categorized into three Kinds, measures and size.  Two kinds of software metrics they are Product metrics quantify characteristics of the product being developed to measure the size, reliability and Process metrics quantify characteristics of the process being used to develop the software to measure the efficiency of fault detection.  Types of Measures are Direct Measures internal attributes to measure the Cost, effort, LOC, speed, memory and Indirect Measures external attributes to measure the Functionality, quality, complexity, efficiency, reliability, maintainability.   Sizeoriented metrics are LOC  Lines Of Code, KLOC  1000 Lines Of Code, LOC  Statement Lines of Code ignore whitespace.    4 COMPONENT Components are the collections of many pre programmed tools which is used as the addon page which is to make use of those tools. There are various tools available to measure the Java source code. Those tools are developed T  P. Edith Linda, Assistant Professor, School of IT  Science,  Dr. G. R. Damodaran college of science, Coimbatore641014  p.lindavinodgmail.com  V. Manju Bashini,, Assistant Professor, School of IT and Science, Dr. G. R. Damodaran college of science, Coimbatore  641014  manjubashinigmail.com   S. Gomathi, Research scholor, Dr. G. R. Damodaran college of science, Coimbatore 641014. Gomathisrinivasn88gmail.com 199International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org  and modeled based on the developers point of view. Some tools will measure some parameters to be measured in java program. Initially the tools which are used to measure the java object oriented programs are searched and analyzed individually to make it as a component. The component based tools will be executed separately but available in a same location and some tools will provide a chart for the results when the program is executed. 5 EVOLUTION OF COMPONENTS In 1990s, software systems for ebusiness transactions were built of components across multiple platforms, programming language and network protocols. No tools were available for detecting bugs or to measure the efficiencies in the programs object oriented programs. Initially the errors are printed using the output statement and the number of lines 1 , number of packages and most important concepts of object oriented programs with the critical programs  Those programs which are used to measure the OOPS programs seem to be critical and they are implemented with the large number of coding.  In 1997, Intermetrics, a software engineering company , proposed to develop a debugging tool for Component based software systems. It only identifies the errors and yields the error free program.  But finding the error wont give the remedy. Thus the problem to find the efficiency of the program is solved with the help of the component based tools which efficiently provides the charts for the results and measures the program. The main advantage of component based tool is that the user can select their own tool to measure their program according to their needs. In this, the user can also see the links from where the tools are available, that is the user can also download or know more about the tool from the direct link. The collection of more tools which will provide more flexibility and choice to the user or the programmer and he can know more about tools. Those tools will display an error if the program is not correct.  Some tools will also provide the charts and warning message if the program is not structured and programmed in a proper format based on the measurement criteria.  6 COMPONENT INTEGRATION All the tools wont meet the user required criteria. Some may be useful for generating report and some may be used just for measuring the program. Some tool may measure the oldest method of measuring the program with some limited parameters which are not in current use and which may be efficient to measure. All the listed tools will measure the common parameter like SLOC Source Line Of Coding and LOC Lines Of Coding. The need depends on the programmer or the user who uses the component metrics. Integrating the various tools have many advantages and also some disadvantages. Those are listed. Advantages on integrating components 1. Flexible 2. User friendly 3. Compatibility 4. Comparability Disadvantages on integrating components 1. Confusion may arise about which tool to be used 2. Some may not meet the user requirement 3. Some tools are only trial versions 4. All the tools which are listed may have some common criteria which is not useful. 7  RELATED WORKS The paper by Amandeep Kaur et. al. 7 to formulate a framework of metrics representing the attributes of object oriented system. And the suggestion is that the impact of OO design on software quality characteristics made an experimental validation and a rework in defect programs. Encapsulation, inheritance, polymorphism, reusability, Data hiding and messagepassing are the major attribute of an Object Oriented system and are the indicators of the object oriented programs. The well known quantifiable approach to express any attribute is a metrics. Empirical Data is collected from three different projects based on object oriented paradigms to calculate the metrics.   Lakshmi Narasimhan and Bayu Hendradjaya8 defined a two suites which cover the static and dynamic aspects of the component assembly. The triangular metric which is used to classify the type and nature of the application, is formed with the complexity and criticality metrics. During the runtime of the complete application the dyamic metrics are collected. This paper is mainly deals with the collection of some metrics and evaluated the results.  200International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org  Shyam R. Chidamber and Chris F. Kemerer 11 stated that the metrics is must and needed for a new organization which is adopting a new technology for whom the proper practice is needed to evaluate the program size and other paradigms. To practice and to evaluate the programmer need to implement  the a new metric suite for Object Oriented programs for OO design. The empirical validation of a theoretical metrics of OO design and the development is the main idea of the paper.   Dr. Linda H. Rosenberg 12 stated the development of object oriented programs is very much needed in todays life cycle. A different approach of design and implementation, and also a different approach to software metrics is required by Object oriented development. Object oriented technology uses the objects as its fundamental blocks and not algorithms. The object oriented programs needs a special metrics to evaluate. Cyclomatic complexity is standard for traditional functional and procedural programs. There are many proposed object oriented metrics for evaluating the object oriented program.  Dr K. Anbumani, K.P. Srinivasan 13 stated use of  software metrics in evaluating the quality of software products and the software development process. There are currently three sets of design metrics for object oriented software which are  suggested by different groups of authors, Chidamber and kemerer metrics.  Lakshmi Narasimhan et. al.14 of   Department of Computer Science from East Carolina University, Greenville, NC, USA suggested that during the execution phase, there exist lack of metrics which will reduce the maintenance costs and defined metrics whose values are collected. Metrics are useful for increase the reusability and  assessing the maintenance cost of individual components and that of the application in which the component is integrated.   Radu Marinescu 15 given that the object oriented paradigm has influenced the software engineers for ease of program writing and implementing. The impact of misunderstanding the object oriented designs made the design complex and more tedious to program and poor OO design. Object oriented metrics have the benefits with the high degree of code reuse, higher maintainability and flexibility, etc., Now they can improve the quality of the design using the object oriented metrics.  Lakshmi Narasimhan and Bayu Hendradjaya16 defined a two suites which cover the static and dynamic aspects of the component assembly. The triangular metric which is used to classify the type and nature of the application, is formed with the complexity and criticality metrics. During the runtime of the complete application the dyamic metrics are collected. This paper is mainly deals with the collection of some metrics and evaluated the results.  Magnus Andersson Patrik Vestergren17 targeted to evaluate whether the software metrics can be used to determine the objectoriented design quality of a software system. There are several metrics to evaluate the object oriented design and programs. To validate each metric an experimental study was conducted. The final conclusion is software metrics can improve the system design quality such as complexity of methodsclasses, package structure design and the level of abstraction in a system. Therefore metrics can be assured that the rules are followed and the program is properly written with the rules.   According to Saida Benlarbi et. al.18 the basic premise behind the object oriented metrics which is an earlier predictors of classes that contains error which may be found during the testing. In this paper they conclude that the empirical validation methods provide misleading conclusion to the validity of object oriented metrics. The result of the empirical validation is studied with the C which is an object oriented program.   The paper by Tieng Wei Koh et. al.19 review the 12 objected oriented software metrics proposed in 90s by Chidamber, Kemerer and Li. The authors stated that the software cost estimation is mainly based on the size or predicting the volume of various kinds of software deliverable items. Size and complexity is very critical and it influences the integral software development effort. Software cost estimation is criticsl because it is mainly based on the software size estimation.   Marco Scotto et. al.20 metrics tools is to be updated for the standardization of software measures but it is not still standardize. Intermediate abstraction layer is the possible solution suggested here to decouple the extraction process from the use of the information. This paper presents the web metrics which is an automated tool for software metrics collection. This open source project is computed with the CK metric suite.  8 EXISTING SYSTEM The tools which are analyzed in this paper are LOCC, JHAWK, CODE COUNTER. These tools are used to 201International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org  measure the object oriented paradigms in the program. The main advantages of these tools are, it will generate the charts and will produce the reports about the programs. The programmer should remodel or restructure the program according to the report or the warning chart generated by those tools9,10. Another software called cavaj Decompiler which is used to convert the class files into java files for user convenience is also integrated into this component.  8.1 LOCC This is the tool to measure the Object oriented program to yield the LOC, SLOC, NOP, NOC, NOM, NOI in that particular program which is given as input. The main drawback of this metric tool is that it wont generate the charts for the report or result generated by the tool 4. LOCC metrics, which provides a grammarbased architecture and interface to the production and use of different size metrics. Developers can use the size metrics distributed with LOCC or can design their own metrics, which can be easily incorporated into LOCC. LOCC pays specific attention to the problem of supporting incremental development, where a work product is not created all at once but rather through a sequence of small changes applied to previously developed programs.   8.2 JHAWK This tool is used to measure the various parameters of CYCLOMATIC COMPLEXITY, COMMENT LINES, NOP in the given input program. The main advantage is that it will generate a chart for the program and also it will show the warning chart. It will mainly measures the looping and iterations which are present in the input program6. JHawk is a Stand alone, Eclipse plugin and command line versions. JHawk Metric interchange format that allows you to record a snapshot of a code base and keep it for future use either in JHawk itself or in our new DataViewer product. JHawk Data Viewer, a standalone product that allows graphical and textual comparison of metrics over time.   The disadvantage is that the Halstead metrics are not used now for measuring the program. Some may need to measure those metrics.  8.3 CODE COUNTER This metrics is mainly used to measure the NOS, COMMENT LINES, TOTAL of the two metrics. The main advantage of this tool is, it will measure and generate the report for many  languages like ASP, HTML, C, C etc. The main advantage is it will measure many languages and will generate the three forms of reports according to the user needs. The main drawback is it wont cover the important parameters to be measured for the object oriented programs 4. The main features of code counter are Easy to use with just 3 steps, Quickly counts several types of source code  including Java, JSP, C or C, VB, PHP, HTML, Delphi or Pascal, ASM, XML, COBOL etc., Smart Comment feature  knows which comment types are used by each language and counts accordingly, Save and load count profiles  no need to enter the same information over and over for the same source files to count, Supports unicode files and contents. Can also count unix type or windows type of text files CRLFCR only  8.4 LOC METRICS This tool is mainly used to measure the total lines of code LOC, blank lines of code BLOC, comment lines of code CLOC, lines with both code and comments CSLOC, logical source lines of code SLOCL, McCabe VG complexity MVG, and number of comment words CWORDS. Physical executable source lines of code SLOCP is calculated as the total lines of source code minus blank lines and comment lines. Counts are calculated on a per file basis and accumulated for the entire project. Loc Metrics also generates a comment word histogram4.  8.5  SLOC METRICS SLOC Metrics measures the size of your source code based on the Physical Source Lines of Code metric recommended by the Software Engineering Institute at Carnegie Mellon University CMUSEI92TR019. Specifically, the source lines that are included in the count are the lines that contain executable statements, declarations, andor compiler directives. Comments, and blank lines are excluded from the count. When a line or statement contains more than one type, it is classified as the type with the highest precedence5. The order of precedence for the types is executable, declaration, compiler directive, comment and lastly, white space.  Main features of SLOC are  Fast and efficient processing of source files, Process any size of file, Process any number of files. Flexible support for CC, C, Java, HTML, Perl, Visual Basic and more. Ability to add definitions for other file types, Formatted and hyperlinked results in HTML file format. Project folders and files are sorted by their Source Lines of Code so that you can easily identify code intensive modules and files, Shows differences in counts of source lines of codes between two measurements by SLOCMetrics as DIFF.html in the output folder , Formatted results in CSV Comma Delimited file format, 202International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org  Prepares results as a wellformed XML document , Multiple SLOC Configurations can be saved as files and reused, can be run as a Console application using sloccmd.exe present in the SLOCMetrics 3.0 installation directory. Configuration file path can be passed as a parameter to sloccmd.exe, An easy to use, Windowsbased graphical user interface.  9    PROPOSED SYSTEM Main drawback of the existing system is they are available individually. So the users may meet some difficulty in order to measure the program.In this proposed system, those tools are integrated and made as a single addon for the users flexibility and user friendliness. All the tools are available in the single web page so that the user can access those tools efficiently. The java decompiler, an open source, also combined in this page which will convert the class files into java files.  The tools will measyre the different parameters which need to be measured. The tools itself will generate the charts and reports according to the result. The tools also shows the warning message if the program have to restructure and also the error message if the given program is contain any error.  10 COMPARISION OF EXISTING METRIC TOOLS  Metric tools  Parameters measured  Chart Generation LOCC   loc, sloc, nop ,noc ,nom, noi  No JHAWK5  cyclomatic complexity,  comment lines, nop  Yes CODE COUNTER  nos, comment lines, total  Yes LOC METRICS loc, bloc, cloc, csloc, slocl, mvg, cwords, slocp  Yes SLOC METRICS comments, blank lines, executable declarations, compiler directive, white space yes xml, xsl, html, notepad 11    CONCLUSION As there are wide applications of association rules in data mining, it is important to provide good performance. In view of this, we proposed a new algorithm which considers the significance of the item also but not only support of the item. The results through experiment shows that the computational cost of the linkbased model is reasonable. At the expense of three or four additional database scans, we can acquire results different from those obtained by traditional countingbased models. Particularly for sparse data sets, some significant item sets that are not so frequent can be found in the linkbased model. Through comparison, we found that our model and method address emphasis on highquality transactions. In this paper, a brief discussion of a number of algorithms was presented along with a comparative study of a few significant ones based on their performance and memory usage.  REFERENCES 1  httpwww.atp.nist.govgems97060038gem.pdf  2  An Analysis of Quality of Service Metrics and       Frameworks in Grid Computing Environment, RussWakefield, Colorado State University, Ft. Collins, Colorado  3   Russ Wakefield, Colorado State University, Ft.Collins,Colorado, An Analysis of Quality of Service Metrics and  Frameworks in a Grid Computing Environment.   4  httpwww.locmetrics.com 5  httpmicroguru.comsloc 6httpwww.downloadpipe.comfreetrialJHawk5PersonalLicensedownload1326333.html 7 Amandeep Kaur, Satwinder Singh, Dr. K. S. Kahlon, A Metric Framework for Analysis of Quality of Object Oriented Design.  8  V. Lakshmi Narasimhan, and Bayu Hendradjaya Theoretical Considerations for Software Component Metrics   9  Boehm, Barry Dr. Software Engineering Economics Prentice Hall, Englewood Cliffs, NJ 1981 900 pages. 10  Galorath, Daniel D.  Evans, Michael W. Software Sizing, Estimation, and Risk Management When Performance is Measured Performance Improves Auerbach, Philadelphia, AP ISBN 100849335930 2006 576 pages. 11  Shyam R. Chidamber and Chris F. Kemerer, A METRICS SUITE FOR OBJECT ORIENTED DESIGN  12  Dr. Linda H. Rosenberg  Applying and Interpreting Object Oriented Metrics.  13  Dr. K. Anbumani, K. P. Srinivasan, A Set of Objected Oriented design Metrics.             14  V. Lakshmi Narasimhan, P. T. Parthasarathy, and M. Das Evaluation of a Suite of Metrics for Component Based Software Engineering CBSE. 15  Radu Marinescu, Using ObjectOriented Metrics for Automatic Design Flaws Detection in Large Scale Systems, How to Use ObjectOriented Metrics in the Early Stages of a Redesign Operation 16   V. Lakshmi Narasimhan, and Bayu Hendradjaya Theoretical Considerations for Software Component Metrics   17  Magnus Andersson Patrik Vestergren ObjectOriented Design Quality Metrics  18  Saida, Benlarbi, Khaled El Emam, Nishith Goel  Issues in Validating ObjectOriented Metrics for Early Risk Prediction 203International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org  httpbooks.google.com 19  Tieng Wei Koh, Mohd Hasan Selamat, Abdul Azim Abdul Ghani, Rusli Abdullah Review of Complexity Metrics for Object Oriented Software Products 20  Marco Scotto, Alberto Sillitti, Giancarlo Succi, Tullio Vernazza A relational approach to software metrics   204International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org  Reactive Power Management for Wind Electric Generator  Er. V. Karunakaran.A.M.I.E.,  Er. R. Karthikeyan, M. E., M.B.A  Abstract  Energy in the Wind is converted into Rotary Mechanical Energy by the Wind turbine. Most of the Wind Electric Generators are having Induction Motor, Asynchronous Motor with constant speed and drawing more Reactive Power from the Grid, during starting  low wind period.  This thesis emphasized the need of replacing the existing conventional Asynchronous Induction motor of Constant Speed by Wound Rotor Synchronous Induction motor of variable speed, namely, Doubly Fed Induction Generator DFIG.  The control principle is either the Direct Torque Control DTC method,   Or, Two  axis Current Vector Control Method. The Direct Torque Control Method is more effective than the two axis current vector control method, for Reactive Power Management for Wind Electric Generator. The minimum usage of only about 30 of Power Electronics results in considerable cost savings and reduction of harmonics when compared with Fully Converter Wind Electric Generators. Index Terms Asynchronous Generator, DFIG Doubly Fed Induction Generator, Direct Torque Control Method, Generator Side Converter, Grid Side Converter, Two Axis Current Vector Control Method, Reactive Power Control, Synchronous G en er at or .          I. INTRODUCTION HE mankind has witnessed the   tremendous advancement in Science and Technology, modernization and industrialization in the last 100 years, which was not seen for millions of years. Energy had been a key issue to industrialization after discovery of Fossil fuels like Petroleum and Coal. This came together with Pollution of Environment like Air, Water, Earth, including vegetation, animals and even new born Babies. Burning of Fossil Fuel over the last 200 years h a d  a d d e d  400Giga tones of Carbon  di  Oxide into the atmosphere. 1GT1000 Million tones   The Plants have been able to absorb only 200 GT and the balance is still there in the atmosphere. This is   the primary cause of Global Warming and climate change.       It is now felt by mankind through Climate changes, melting of Ice caps in Artic Zone, rise in the sea level reducing the Land area, threat to marine life etc.        The solution to the Global Warming lies in the development of the Green cover or Forest cover to absorb the excess CO2, to gether with harnessing Renewable Sources of Energy in lieu of Fossil fuel so as to arrest the Pollution  level from further increasing. Carbon Emission, a major cause of Global Warming is primarily due to use of fossil fuels such as Coal and Petroleum in Thermal power Plants and Automobiles. 2 It is proved that Fossil Fuels continue to diminish. 5          Renewable  Energy is energy which comes from Natural Resources  such  as Sun  Light, Wind,  Rain,  Movement  of  Water Hydro  Power,  including  ocean Surface  Waves used  for Wave Power  Tides, and  Geothermal heat, which are Renewable Naturally Replenished.1 Among the said alternative Power Sources, Wind  Power  is  one  of  the  most promising New  Energy Sources. Wind is appealing for  several reasons.  It is abundant, cheap, inexhaustible, widely distributed, clean and climatebenign, a set of attributes that no other Energy Source can match.5 Wind has considerable potential as a Global Clean Energy Source and producing no Pollution during Power Generation GREEN ENERGY DEVLOPMENT. 7       Harnessing of Wind Energy could play a significant role in the Energy Mix of each Nation. Wind is caused due to the uneven heating of the earth surface by the Sun.  The Air in contact with the ground gets heated up and becomes light in weight by losing its density and starts rising up. Thus heated air is displaced by the cold air which is higher in density. This is how the air is displaced and set in motion to generate winds and it is called Wind Breeze.      The most prominent feature of the Wind Climatology in India is the Monsoon Circulations.      T205International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org      Winds in India are influenced by the strong SouthWest Summer Monsoon, which starts in MayJune, when cool, humid air moves towards the land and weaker North East Winter Monsoon, which starts in the month of October, when cool dry air moves towards the ocean.        During  the period March to A u g u s t , the kinds are uniformly strong over the whole Indian Peninsula, except the Eastern Peninsular coast. Wind speeds during the period November to March are relatively weaker, though higher winds are available during a part of this period on the Tamil Nadu Coastline. 2       Also, the State of Tamil Nadu is blessed with conductive Natural Metrological and to geographical settings for wind Energy Generation.  Three Passes namely, Palghat Pass, Shengottah Pass and Aralvoimozhi Pass are endowed with heavy Wind Flows due to the Tunneling Effect during South West Monsoon.          Tamil Nadu is endowed with 3 lengthy Mountain Ranges on the Western side with Potential of 1650 MW in Palghat Pass in Coimbatore District, 1300 MW in Shengottai Pass in the Tirunelveli District and 2100 MW in Aralvoimozhi Pass in Kanyakumari District and 450 MW in other areas totaling to 5500 MW. 7           As on  31.03.2010,  the installed capacity of the Wind Electric  Generators in Tamil Nadu is 4907 MW, stand  first among all the  States  in  India  whereas,  the capacity  in  Maharashtra  is  2078 MW in Gujarath, it is 1864 MW and in  Karnataka, it is 1473 MW. 8        India stood 5th place all over the world as the installed capacity of the Wind Electric Generators is 13,065 MW as on 31.12.2010. China stood 1st place, as the installed capacity is 44,733 MW. United States of America stood 2nd place,  as the installed capacity is 40,180 MW  Germany stood 3rd place as the installed capacity is 27,214 MW Spain is having 20,676 MW and stood 4th place. 9    II. Power Conversion from Kinetic Wind Energy to Rotational Energy       The Kinetic Energy of a Mass of Air m having the speed vw is given by           The Power associated to this moving air Mass is the derivative of the Kinetic Energy with respect to time.   Where q represents the mass flow given by the expression        is the air density and A is the  cross  section  of  the  air  mass flow and q represents  the mass flow. Only a fraction of the total Kinetic Power can be extracted by a Wind Turbine and converted into Rotational Power at the shaft. This fraction of Power P wind depends on the Wind Speed, Rotor Speed and Blade Position for Pitch and active stall control turbines and on the turbine design. CP 3      The best utilization of the Wind Energy in Wind Electric Generators WEGs establishing a theoretical limit for the Power Extraction,  independently of the turbine  design, at most   theoretical maximum of the Wind Kinetic  Energy  can  be  converted into Mechanical Energy. 4 Every particle in motion has an associated Kinetic Energy proportional to its mass and the square of its speed. Turbines convert this Kinetic Energy to a mechanical motion and during the conversion process the speed of the flow is reduced. Betz limit      Finally, the Mechanical Power extracted from the Wind is calculated using 206International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org   P Mech    2  R2   C ,  v 3   3 or, say  PWind 12AwtCp ,v3windW     where  is the Air density Kgm3, A wt   is the Wind Turbine Swept  area  in m2  , vwind is the Wind  Speed  in  ms    Cp  is  the Power Coefficient and dimensionless  depends on both the Tip Speed Ratio  lower than Betz limit and  Pitch  angle   degrees 4    III. Types of the Wind Electric Generators       Wind turbines can r o t a t e  about either a Horizontal for a Vertical axis, the former being both older and common.       The first electricity generating Wind Turbine was a battery charging machine installed in July 1887 by Scottish academic, James Blyth to light his holiday home in Marykirk, Scotland. The first   automatically oper a ted  wind turbine, built in Cleveland in 1888 by Charles F. Brush. It was 60 feet 18 m tall, weighed 4 tons 3.6 metric Tones and powered a 12KW Generator. 1        In the 1990s, wind power turbines   were characterized by a fixedspeed operation. Basically, they consisted of the coupling of a Wind Turbine, a Gearbox and an Induction Machine directly connected to the Grid.         Additionally, a soft starter is used to energize the machine and a bank of Capacitors to compensate the machine Power Reactive absorption. Although being simple, reliable and robust, the fixedspeed wind turbines were inefficient and power fluctuations were transmitted to the network due to wind speed fluctuations. 4          The Induction Generators are gaining the popularity due to its simplicity and no Synchronization problem. However, the major drawback of the Induction Generator is its additional Reactive Burden on the system, where it is connected.          The two most common modern Wind Power Plant configurations are a Wind power plants with a Synchronous generator and two fullsized converters or b doublyfed induction generators with two converters in the Rotor Circuit. Schematic representations are shown below.         Fig 2 Synchronous Machine  Fully Converter WEG Using Power Electronics.      In order to enable variable speed operation   Synchronous Generators are decoupled from the Power Grid with its fixed frequency 50 or 60 Hz through two backto back frequency converters, which have a common DClink. The main disadvantage of this topology is that the frequency converters are designed to handle the full Generator power output. This inevitably means higher costs. 207International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org    Fig 3 Doubly Fed Induction Generator  Using Power Electronics only on Rotor Side               Doubly  fed  electric machines are Electric in or Electric Generators that have windings on both   stationary and rotating parts, where both windings transfer significant Power between Shaft and Electrical System. Doublyfed machines are useful in applications that require varying speed of the machines shaft for a fixed Power System Frequency. 6    208International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org  III. Analysis and Comparison at par with the P.F KWhr, rKVAh, and iKVAh components of the existing Conventional and Fully Converter types WEGother than DFIG  Case Study1      A  conventional WEG Smake having the Induction Squirrel Cage  synchronous Motor,  which  is  directly  coupled with the utility Grid, kept installed at Tirunelveli district in Tamil Nadu of India and in working condition  till date, is taken for the study purpose.       The Reactive  Power compensation  is  fed  by  using  the Capacitors,  in  to  the circuit,  on switched  relay mode, whenever required to carryout the compensation and not by adopting the method of full  Controlling mechanism, as available in the Modern Wind Electric  Generators. All   the parameters have been studied and the details of the month wise Power Factor for the year of 2009 and 2010 are displayed below                  YEAR 2009                   YEAR 2010   Import   Mon KWHr iKVAh P.F  KWHr iKVAh P.F Jan 234 882 0.27  558 2250 0.25 Feb 630 2574 0.25  486 1746 0.28 Mar 1602 6120 0.26  1674 6390 0.26 Apr 2592 9900 0.26  2070 7668 0.27 May 648 2250 0.29  2106 7344 0.29 June 450 1710 0.26  882 2304 0.38 July 216 990 0.22  738 2682 0.28 Aug 378 1908 0.20  630 2052 0.31 Sep 936 4068 0.23  918 3348 0.27 Oct 756 3438 0.22  1332 5274 0.25 Nov 2232 9810 0.23  1260 5148 0.24 Dec 1098 4428 0.25  2016 8676 0.23       It is understood that the S type WEG is drawing Reactive Power from the Grid and the Power Factor   ranging from 0.2 to 0.38 only.    209International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org    Case Study 2       A  Modern WEG E make having Synchronous Motor, with ACDCAC Converter  and Inverter Fully equipped with Power Electronics and  kept connected  with  the Grid, installed at Dindukul  district in Tamil Nadu of India and in working condition till date, is taken for the study purpose.      The details of the month wise A c t i v e  a n d  Reactive Power generated and drawn from the Grid are studied and the Power Factor at the Grid side is charted below.        YEAR 2010  Import    Mon KWHr iKVAh P.F Jan 528 1176 0.45 Feb 312 576 0.54 Mar 840 1488 0.56 Apr 768 1272 0.60 May 744 1224 0.61 June 4 144 0.67 July 216 360 0.60 Aug 4 48 1.00 Sep 7 144 0.50 Oct 816 1632 0.50 Nov 768 1272 0.60 Dec 744 1224 0.61        It is understood that the E type WEG is drawing Reactive Power from the Grid with the P.F ranging from 0.45 to 1.00.  The Unity P.F has been achieved during August 2010.  The reason for the U.P.F is due to the application of Power Electronics  the availability of the continuous Wind and the good condition of the Machine WEG during the study period. IV. The Doubly Fed Induction Machine Concept      DFIG is an abbreviation for Double Fed Induction Generator, a   generating principle widely used in Wind Turbines. It is based on an Induction Generator with a multiphase wound rotor and a multiphase slip ring assembly with brushes for access to the rotor windings. It is possible to avoid the multiphase slip ring assembly Brushless Doubly Fed Electric Machines, but there are problems with ef f ic iency, cost and size.  A better alternative i s  a Br us h  l e s s  Wound Rotor DFIG.      The principle is that Rotor Windings are connected to the Grid via slip rings and back  to  back Voltage source converter that controls both the rotor and the grid currents. Thus rotor frequency can freely differ from the grid frequency 50 or 60 Hz.                 Fig 4 WEG with DFIG Concept      By using the converter to control the rotor currents, it is possible to adjust the Active and Reactive   power fed to the Grid from the Stator independently of the Generators turning speed.  The control principle used is either the twoaxis  current Vector control  or Direct Torque Control  DTC has turned out to have better  stability than current vector control especially when high Reactive currents are required from the Generator  1      The Prime Mover, consisting of a pitchangle controlled wind turbine, the shaft and the gearbox drives a slipring Induction Generator.  The stator of the DFIG is directly connected to the Grid the sliprings of the rotor are fed by selfcommutated converters. These converters allow controlling the 210International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org  rotor voltage in magnitude and phase angle and can therefore be used for Active and Reactive Power control. 3   V. Function of the D. F. I. G Reactive Power Control      The Wound Rotor Doubly  Fed Electric Machine is the only Electric Machine that operates with rated torque to twice Synchronous  speed  for  a  given frequency of excitation i.e., 6000 rpm   50 Hz  and  one  polepair versus  3000  rpm for singlyfed electric  machines.  Higher   speed with a given frequency of excitation gives lower cost, higher efficiency, and higher power density.       In concept, any   electric machine  can be converted to a Wound  Rotor  Doubly Fed Electric  Motor or  Generator by changing  the  rotor  assembly  to  a multiphase  wound  rotor  assembly of equal stator winding set rating. If the rotor winding set can transfer power to the electrical system, the conversion result is a   Wound  Rotor Doubly  Fed Electric Motor or Generator with twice the speed and power as the original singlyfed electric machine. The resulting dualported transformer   circuit topology allows very high torque current without core saturation, all by electronically controlling half or less of the Total Motor Power for Full Variable Speed Control. 1      Due to the main advantage of allowing Variable  Speed Operation, the Power Extraction from the Wind can be optimized. The converters feed the Low Frequency Rotor circuits from the Grid.  The converters are partially scaled requiring a rated power of about 30 of the generator rating. Usually, the slip varies between 40 at SubSynchronous speed and 30 at SuperSynchronous speed. The GridSide Converter is controlled to have a Unity Power Factor and a constant voltage at the DClink.       The RotorSide Converter is usually controlled to have a Optimal Power Extraction from the Wind and b a specified Reactive Power at the Generator ter minal . Sinusoidal 3 Phase Voltages at the Slip Frequency.       Therefore, assuming that the converters are lossless, the Net Power injected by the Generator to the grid is given by  Pgen  Ps  Pr  Qgen    Q s    4    VI. Conclusion        It is concluded that a DFIG is a WoundRotor Doublyfed electric machine basically operates similar to a Synchronous Generator, and as its rotor circuit is controlled by a Power Electronics Converter, the Induction   Generator is able to Manage   Control   Import   and Export Reactive Power.       Apart from the Reactive Management, the control of the rotor voltages and currents enables the Induction Machine to remain synchronized with the grid while the wind turbine speed varies. A variable speed wind turbine utilizes the available wind resource more efficiently than a fixed speed wind turbine, especially during light wind conditions.       The cost of the converter is low when compared with other variable speed solutions because only a fraction of the Mechanical Power, typically 2530 , is fed to the Grid through the Converter, the rest being fed to Grid directly from the Stator. 1       The Mechanical Efficiency in a Wind Turbine is dependent of the Power Coefficient. The Power Coefficient of a rotating Wind Turbine is given by the Pitch Angle and the Tip Speed Ratio. Adjustable speed will improved the system efficiency since the turbine speed can be adjusted as a function of Wind Speed to maximize output Power.       Also, the Cost of inverter filters is reduced. Filters are rated for 0.25 to 0.30 p.u of the total System Power and Inverter Harmonics represent smaller fraction of Total System Harmonics.     VII. References 211International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org    1. Wikipedia Collections  httpen.wikipedia.orgwikiWindpower  and   http  en.wikipedia.orgwikiDoublyfedelectricmachine   2. Karnataka Renewable Energy Dev. Limited., http  kredl. kar.nic.inWind.htm  3. Technical Documentation on Dynamic Modelling of DFIG  httpwww.digsilent.d  4. DFIG Machine in Wind Power Generation by Hector A. Pulgar Painemal, Peter W. Sauer. http www. magelab. Comuploads4c51ddb 1e2c90.pdf  5. DFIG Control for an Urban Wind Turbine by A. Naamane and N.K.Msirdi. http www.irec.cmerp. net papers WOEPaper 20ID20160  6.Transients in Doubly Fed Induction Machine due to supply Voltage Sags   by Y.  Plotkin, C Saniter, D.Schulz and R.Hanitsch. www.ansoft.comnew articles Pape rPCIM05Plotkin.pdf  7. Tamil Nadu Energy Regulatory Commission TNERC Order No 3 of 2010 Dated 31.07.10.  8. Indian Wind Energy Association Web. httpwww.inwea.Orgabout windenergy.htm 9.cleantecnica  httpcleantechnica.com 20110407 worldwindpowercapacityanidea    First Author  Er. V.Karunakaran.    Student, kanchikaruna8860gmail.com                    Guide  Er. R. Karthikeyan,M.E.,M.B.A., Assistant Professor, Department of EEE. SVSVMV University. kascs2007gmail.com        . Sri Chandrasekharendra Saraswathi Viswa Mahavidayalaya University, SVSVMV Enathur, Kanchipuram, Tamil Nadu, India           212International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518   IJSER  2011 httpwww.ijser.org  Security Challenges  Preventions in Wireless Communications Kashif Laeeq  Abstract Without the need of an infrastructure, lowcost, automanaged, flexible and low power consumer, wireless communication is becoming emerging technology. It shows great binder for present as well as future hitech applications. Increasing reliance on wireless communication also brings great challenges to the security measures and other correlated issues. Although the newly introduced corrected security standard, IEEE 802.11i, offers extensive security for the wireless environment but it is still premature and does not provide effective measures to protect the wireless networks from confidentiality and integrity threats. The main issues for deployment of wireless networks are security attacks, vulnerabilities, battery power and improper security models. This paper provides a study on these problems especially in adhoc wireless networks. The study is based on numerous proposed schemes in the endeavor to secure such networks. The goal of this paper is to probe the principal security issues, challenges and fundamental security requirements of wireless communications on the bases of their proposed solutions.  Index Terms Attack, Denial of service, Mobile Adhoc Networks, Security issues, Vulnerabilities, Wireless Communication, WSN.        1 INTRODUCTION                                                                     UE to low cost, low power consumption, flexible, no physical infrastructure and easy to deploy, wireless communications have been an admired research area over the past few years with tremendous growth in the population of wireless users. Many systems are still mapping wire to wireless media. Nowadays , there are number of wireless technologies on hand for long range applications like cellular mobile, satellite communications, Radio Frequency RF, and short range applications such as Bluetooth, Infrared IR,  Near Field Communication NFC, ZigBee, Ultra Wide Band UWB. These short range wireless technologies are being used in many wireless technologies like wireless local area networks WLAN, wireless body area networks WBANs, wireless personal area networks WPANs, and, adhoc network etc. Although wireless communications have numerous compensations over the usual wired networks in contrast it is exposed to a range of intrusion attacks. Unlike the wired networks, the wireless networks face unique challenges due to their inherent vulnerabilities. Any wireless signal is subject to interception, jamming and false command disruption.   The coverage area of wireless communication is dependable on the devices used. The more powerful device may cover large area but it will be expensive, consume more power and also produce more electromagnetic radiation that might be danger for human health 11. One of the solution provides in that enlarge the coverage area hop by hope, by applying the ZigBee protocol, but the energy issue remain constant. Majority of times the limited energy restrict the security schemes 15. Nearly all the proposed solutions concentrate on a specific security problem but pay no attention to others, those which pull off low energy and memory burning up negotiates on security level. Therefore the demand for a model which fulfills all these issues with low cost, high security and low power, is increasing with raising the wireless technology.  Attack, is also one of the major issues in wireless communications. The technology suffers with two major types of attacks, i.e. inside and outside attacks. Outside attacker cant get access to the network but inside attacker can do it and may disrupt the network resources such that encryption keys or further codes used by the network.  It is observed many times that only cryptographic security schemes may be failed to combat with numerous types of attacks in contrast all the proposed intrusion detection systems are not surely detect and remove the intruders. In reactive routing protocols like AODV, chances of attack are higher 18. 2 ISSUES AND CHALLENGESS Wireless communication has emerged as a major breakthrough in traditional wired communications. It has changed messy wired world into a clean and flexible atmosphere.  According to a well known adage, there is no unmixed good in this world implementation of wireless network carries   numerous performance and security issues. These issues include 2.1 Vulnerabilities in Current Wireless Communications The wireless communications survivability relates to wireless communication protection mechanism and robustness against attacks and failure of wireless network elements or communication itself. Some of these issues are as follows D   Kashif Laeeq is lecturer at Computer Dept. in Federal Urdu University of Arts, Science  Technology, Karachi, Pakistan, PH03002511667. Email kashiflaeeqyahoo.com 213International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org   The Wireless Sensor Network gateway forms a single point of breakdown for the backtoback sensor network infrastructure 1.  After deployment of network, sensor nodes remain unattended which is a root cause of security lapses 2.  The existing location tracking methods have their own boundaries in tracing wireless intruders 3.  Major threat and challenges of wireless communications  are still not considered in IEEE 802.11i revised specification 4  Adhoc Wireless Sensor Networks deployment for monitoring physical environments are still in vulnerable zone 5. 2.2 Current Security Models and Prevailing Security Threats Different performance issues of wireless networks operation, administration and management are encountered due to improper security model. Many security schemes dont guard against some prevailing threats. Some of these issues are as follows  Present security schemes for Wireless Personal Area Networks WPANs are  immature 6   There is no proper visualization technique present for wireless communications 7  Security model for wired network not necessarily effective for wireless networks 8.  Compressed Real time Transport Protocol CRTP is not appropriate for wireless links, that have a very high and erratic bit error rate BER9  Inadequate security integration scheme for heterogeneous networks 10.  The current wireless smart home system has range limitation issue 11. 2.3 Major Attacks on Wireless Sensor Net              works WSN Wireless Sensor Network WSN is a prevailing technology that shows great promise for diverse ultramodern applications both for mass public and intelligence. Security in wireless sensor networks is still in its childhood, as little consideration has been made to this area by the research community, due to this ignorance, WSN still facing numerous issues and challenges. Some of these issues are as follows  There is no common model to guaranteed security for each layer in a Wireless Sensor Networks WSNs 12.  Current security Solutions for wireless Sensor networks are not feasible against all Prevailing security threats 13  Protection mechanism in wireless sensor network is still adolescent age 14.  The current protocols for data link layer  network layers are not adequate for handling various security threats in WSN 15.  The existing security measures for wireless sensor networks WSN are insufficient 16. 2.4  Security Attacks on Adhoc Wireless Networks Truly speaking the most demanding area of wireless networking is adhoc wireless networks, but unfortunately it is the most at risk. Any intruder can easily get the access on the network resources and disrupts the communications. Some of these issues are as follows    In many cases cryptographicbased solution for detection the intruder in adhoc networks are ineffective 17.    Attacks on wireless ad hoc network specially on routing protocols upsets network performance and reliability 18  No response method and limitations to handle wormhole attacks in Wireless Adhoc Networks WANs. 19.  Many existing adhoc routing protocols concern only the length of the routers 20. 3 APPROACHES TO MITIGATE ABOVE CHALLENGES The main issues for deployment of wireless networks are security attacks, vulnerabilities, battery power and improper security models. The research on security issues and challenges in wireless communication comprises performance implications due to different factors. The effects of these factors or problem areas have been addressed by using different tools, algorithms, models, simulations and design modifications. These sub domains and the approaches or methodologies are discussed in subsequent paragraphs 3.1 Protecting Wireless Network against Vulnerabilities  The Wireless Sensor Network gateway forms a single point of collapse for the backtoback sensor network infrastructure. Even if a strong security model is used, but whenever intruder attacks on WSNgateway, the whole network operations hampered.  The fault endurance of WSNgateway should be increased to avoid single point failure. In 1 a commercial grade WSN is considered and threw a pingbased DDos attack on the gateway of WSN. Various computers send ping attack traffic simultaneously to the WSNgateway. For the entire testing of 4 hours of DDoS attack, processor fatigue and sensed data were collected. It is clearly observed that the computing resources of network gateway exhaustion under pingbased DDoS attack traffic. At a load exceeds by 20 of the ping attack, the WSN gateway processor became 100 busy, which caused the WSNgateway to discontinue collection, reporting and recording the log sensor data. Result of this experiment stress to increase the fault endurance to avoid 214International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org  single point failure. 1.    Due to the vulnerable attributes of sensor networks, there is always a risk of threats, along with the objective to ensure the privacy, integrity and reliability of transmission over these sensor networks. Model present in 2 associated with Communal Reputation and Individual Trust CRIT within sensor nodes overcomes this problem.  In this model, a node judge the reliability or adequacy of a neighbor node throughout a set of values associated to the neighbor nodes reliability and reputation. A node observes their neighboring nodes and positions the neighbors in conditions of a trust vote. Trust table kept by neighboring nodes conclude the communal and individual trustworthiness of nodes. A node keeps two tables, a trust table and other is reputation table. Trust table keeps the trust and untrust observations for all other neighbor nodes. In the same way reputation table keeps reputation observations for all the neighbor nodes. If a node throws a unique message, and that is not confirmed by all other nodes, then the reliability of the node is under question, and also the untrust value for the node increases. This message comes to cluster leader, by all other nodes about a specific node, it transmit the information so that all other nodes disregard the untrustworthy node and it is discarded from the network 2.  The current location tracking schemes have their own boundaries in tracing wireless intruders. The modification for Triangulation method present in 3 seeks to overcome this limitation. The technique based on two separate databases, the values of these databases consider by two locations, one for inside and other for outside the building. For location tracking using databases, execute the Triangulation method followed by matching up to the value with entries of data bases. It will be at a glance that attacker is inside the building or outside, then looking for closest values in databases the matching observations in column without having triangulation would be the more precise location of device 3. The IEEE 802.11i modification has concluded to deal with security issues in wireless local area networks but major threats like DoS attacks, insider attacks and offline guessing attacks are still lookedfor consideration. An improved authentication mechanism present in 4 can overcome this ignorance. This scheme adopts an asymmetric cryptography technique to achieve effective defense in six categories that are discovery phase, authentication and association phase, RADIUS authentication, 4way handshake, group key handshake, and secure data communication. This integrated protection for, null data frames, EAPOL frames, management frames as well as protection from some fundamental DoS attacks, Offline guessing attacks and insider attacks. This authentication mechanism is also capable to allow stations to organize themselves automatically 4.   Adhoc wireless sensor networks deployment for monitoring physical environments, where targets have unforeseen motions, are still in vulnerable zone.  For such type of Self Organizing Wireless Sensor Networks SOWSNs, statmesh architecture and mobility scheme provide an efficient monitoring system 5.  The architecture has a base station BS node and sensor nodes SN, which unites a mesh of routers to expand radio coverage with star endpoints. StarMesh architecture utilizes multihopping to offer multipath routing, by means of an adhoc network based approach. The SN collects environmental in formations and rearranging events generated by BS.  The BS performs actions on receiving events also managing the routes. Initially when a node gets a message it stores the flooded one and transmit the message to all its neighbors. A new received message that has the same flood id is erased. Each SN has an initial amount of power hence it can drive signals to all nodes surrounded by its transmission range. Each SN connected at least one B.S. the BS, with great amount of energy, works as a gateway that connects SN to the analysis center. The BSs are fixed and each BS knows exactly its position information. The BSs are assumed to be arbitrarily located. 5                Fig.1  The Adhoc SOWSN architecture 5 3.2 Schemes to Reduce Current Security Threats and Range Limitation Issues At hand security schemes for Wireless Personal Area Networks WPANs needed more research. In 6 T.Kennedy and R. Hunt review WPAN security according to them protection mechanism for Bluetooth is that PIN should not entered into the Bluetooth devices for paring in public and only known devices should pair. Encryption technology is essential for unique session key. For ZigBee the source decides whether a protected or nonprotected acknowledge frame is needed, also performing authentication of the source address. Use symmetric key key exchange SKKE acknowledgement of a link key between trust centre and connected devices. For Near Field Communication, a standard key handshake protocol such as diffieHellman associated on elliptic 215International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org  curve cryptography, or RSA could be used to set up a shared secret between two devices for securing the channel 6. Wireless network visualization techniques in presentation mode visualize the information of access points, mobile devices and relationship using the icons, colored lines and symbol. in this way more information about network status and performance can be achieve, and will help the network performance and security attacks 7. A novel method using MAC Spoofing proposed in 8 used to avoid any intruder into the wireless communications. MAC address of authenticated user can be used to save any unauthorized access a data base of all authorized client MAC address maintain by organization. If the intrusion detection system finds more than one request of MAC address in the same network, it can be sure that the MAC address has been fraud and can block access to that MAC address temporarily 8. A modified Enhance Compressed Realtime Transport Protocol ECRTP is suitable for wireless pointtopoint links, which have a very high and erratic Bit Error Rate BER. In modified ECRTP, the size of header is reduced. In compressed RTP packets occupy only 2 bytes. These bytes may be the UDP checksum or the compressor interleaved header checksum. By sending these checksums only in some packets, the average header size can be reduced 9. Heterogeneous Network Integration Model proposed in 10, yields a security scheme for wireless mesh networks. Each of the mixed wireless networks has produced connection with mesh backbone throughout the mesh gateway interface. Whenever these networks correspond with the mesh cloud, they cross the gateway routers of mesh backbone. Security issues of the border between the heterogeneous wireless networks and the mesh communications should be contracted intensively. In order with this, when passing during the mesh cloud, every of these heterogeneous networks require the mesh infrastructure to perform their own individual security requirements.  The current wireless home system has range limitation issue. By using IEEE 802.15.4 standard a system is proposed for smart home environment. The most vital part of the structure is main controller, which will provide interfacing between users and the system. PICI8f452 microcontroller is used as a brain of the main controller for low power consumption CMOS technologys ICs are used. A GSM modem is attached with the controller for SMS. Approximately nine phone numbers can be stored and only these numbers can communicate with main controller for sending  receiving SMS  system resource controller. User can enter into system by entering password. The software consists of programming PICI6LF452 microcontroller using Mikroc compiler from Mikroelectronika. Using CLanguage, all of this programming is completed 11. 3.3 Preventive Measures against Security Issues in Wireless Sensor Networks  Majority of proposed security schemes are supported by particular network model. It is needed to have a model that accomplishes the need of security for each layer in a network. The proposed holistic approach 12 with respect to security for wireless sensor networks mitigates this issue. The holistic scheme has some fundamental principles like in a certain network security is to be guaranteed for all the layers of protocol stack, the value for ensuring security should not exceed the assessed security risk at a particular time, if there is no substantial security ensured for the sensors. In a particular network safety is to be ensured for every layers of the protocol stack then the cost for guarantee security should not exceed the assessed security risk at a particular time, if there is no physical security guaranteed for the sensors. The security evaluate must be capable to exhibit a refined degradation if some of sensors in the system are compromised. The security considered should be developed to function in a decentralized mode. If security is not measured for all the security layers there are a few efficient security methods working in other layers. By formation security layers as in the holistic scheme, protection could be recognized for the overall network 12.   Fig.2 Holistic View of Security in wireless sensor networks 12  Classification and association of security VTA Vulnerabilities Threats  Attack, is proposed to remodel applicationspecific WSNs. The proposed scheme has redefined the concepts of vulnerability, threats and attacks with respect to wireless sensor network. On the basis of this differentiation we can check the list of security VTAs, which can reduce the ambiguity of security information on VTAs. Then by probing each of VTAs we relate it with a security appraisal framework for analysis 13.  Table 1. Classification and Association of Security VTAs with Discrete Security Assessment Framework 13 N E Vulnerability average energy exhaustion network, low  computational capacity, limited network storage time,  216International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org  T W O R K self organization, faulttolerance level, distributed  storage, task details, simple ciphering, and node  deployment Threat Topology change, change of frequency, large messaging overhead, nonscalability, recursive routing, system failures Attack complete DoS or DDoS L I N K Vulnerability Radio link, Signal transmission range 916MHz,2.4GHz, Broadcasting, Topologyless infrastructure, Ad hoc Topology information Threat NonReachable, Linkfailure, Highdensity of nodes, Indefinite jamming of signals, Data tampering, High noise, unmanaged mobility, Higher delays linksetup Attack Collision or checksum mismatch, Unfairness, Spoofing, Sybil, Wormholes, Hello flood, ACKspoofing S I N K Vulnerability Energy exhaustion at Sink, Task details Threat Unauthorized access Attack Sinkhole, desynchronization N O D E Vulnerability Energy exhaustion at node, Resilience to physical security, Limited memory, shortstorage time Threat Node failure, Recursive localization, Indefinite flooding Attacks Selective Forwarding O T H E R Vulnerability Threat Natural hazards, Environmental interference, Human Interaction to damage network, catastrophicmanmade Attack Nil  Multitier security architecture is required where each mechanism has different resource requirements. Identification of the data type present in sensor network identifies the security threats to the communication for each data type. Every employing multitier security scheme is tailored to make the most out of the available resources. Use localized algorithm in which only one node heap all others sensors data and then sends this mix data to a sensor node which can communicate network and users 14. The current protocols for data link layer and network layers are not adequate for handling various security threats in WSN. The proposed scheme in 15 is turn to account in the form of two layers, link and network layers. This strategy seep through the jumble attacks layer by layer to reduce the price of energy for processing. Type of attacks that proposed intrusion detection scheme in link layer can easily notice integrity, collision and exhausting attacks, and in network layer it behold sewage pool, wormhole, selectiveforwarding and hello flood news attacks. This security architecture needed no extra component or hardware 15.  The Adhoc personal area network  Wireless Sensor Secure NETwork AWISSENET distributed detection system DIDS proposed in 16 for secure WSNs. The model has plugin based design in order to enable a flexible management of the algorithms that running on each node. The local IDS agent is composed of four components. The plugin manager, data manager, decision model and communication model. Intrusion algorithm runs just a subset of the AWISSENET nodes. The AWISSENET cluster can be multihop. The size of a cluster is nearly same scale as the number of bunches in network. The AWISSENET DIDS employs timestamps and absorbs, secrete keys are shared within each cluster and among the cluster heads. Timestamps are used to decide the freshness of the messages and stop replay attacks 16. 3.4 Schemes to Secure Adhoc Networks   The proposed system in 17 acts on control messages by checking the truthfulness of their content. Nodes operating the OLSR protocol keep neighborhood informations. All nodes of the network participate in IDS. This solution represents the first line of protection for the OLSR protocol. It alleviates threats exploiting flaws in the OLSR specifications to reroute the common routing operation 17.  A Grouped Black Hole Attack Security Model GBHASM mitigates the grouped malicious nodes to broadcast the shortest pathway through them to source and destination. Scheme is consisting of two modules first module has the explanation about new node connectivity  communications. Server receives request acket from new node. It answers with membership acknowledgement to the node and stay for the acceptance. If node doesnt replay within a time limit, the server discards the joining request. Otherwise it throws its information. The information received by new joining node is placed in the database and also assign Node Code Pkk1 or pkk2.The second module handles all communication activities within the network. Once becoming a part of the network, the node drives call for shortest path through pkk2 packet. Each node will check pkk1 with pkk2, if key matches with in a given time limit, information will be released otherwise the time of the packets to live, force it to become meaningless 18. An effective wormhole attack defense method is proposed to limits the wormhole attacks on wireless adhoc network. In this method each new node of adhoc network collects information about one hope and twohope neighbors, in this way the nodes construct a neighbor list and allocate a session key with all neighbor. The identity and MAC address is also present with a packet comes from every node. The next node then verifies whether the forwarder is a neighbor. This technique drops the replayed packet, and it broadcasts the exit of the wormhole 19. A secure routing mechanism called securityaware adhoc routing SAR not only concern the length of the routers. The security metric is integral part of routing request or RREQ packet and change the forwarding behavior of the nodes receive on RREQ packet with a particular security matrix or trust level. SAR conforms that the node can only forward it, if the node itself can recommend the required security, otherwise the RREQ is discarded. If backtoback path with the mandatory security elements can be found, an appropriate modified RREQ is launched from 217International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org  an intermediate node or the ultimate destination. SAR can be employed, based on adhoc ondemand routing protocol such as AODV with suitable modification 20.                                              Table 2. Major issues and Suggestions        Domain Challenges  Issues Suggestions   Mobile Ad Hoc Networks Cryptographicbased scheme may be failed to find out the intruders.  Use any proper intrusion detection system IDS. Ad hoc network routing protocols are prone to security attacks. Use any attack prevention scheme particularly at the time of root construction. Majority of routing protocols concern only the length of the routers. The security metric should be integral part of routing protocols. S E N S O R  N E T WO R K Deployment Issues WSN gateway form single point failure. Fault endurance of WSNgateway should be increase. After deployment sensor nodes are gone unattended. Periodically checking all the attached nodes of a WSN.  Protocol Issues Current protocols for link layer and network layer can be failed to handle security threads in WSN.  Need for security architecture to handle this issue. No common model to guarantee security for each layer in WSN.  Holistic approach with respect to security can mitigate this issue.  Inherited Issues Limited computation power, short memory and low power supply.    Extensive research is required to resolve these inherited issues.  Increment in the range of devices will increase the energy consumption. Increment in the range will create high electromagnetic radiation. Energy issue restricts the implementation of security schemes. 218International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org  4 CONCLUSION In this survey paper, we look into the security issues and challenges in wireless communications, particularly in ad hoc communications. We have divided our studies into four subdomains that are Security attacks, Vulnerabilities, Security models and Range limitation issues. Main issues addressed in this paper comprise the continuity of environment monitoring, limitation and vulnerabilities of sensors networks, the adhoc communication scheme, and the security scheme that protects against large number of attacks including DoS, Wormhole attacks, HELLO flood news attacks etc. we also discussed some security model for protection against attacks, these mechanisms still have limitations, which are discussed in this paper. We also provide a tabular form of major issues and challenges in WSN and MANETs and also provide some suggestion towards solutions. The contribution of this paper is to spell out the current security threats and other correlated issues in wireless communications and discussed     the proposed solutions which may offer a new way of thinking towards the solution space.   REFERENCES 1 Kumar, R. Valdez, O.Gomez ,S.Bose Survivability evaluation of wireless sensor network under DDoS attacksIEEE  International Conference on Systems and Mobile Communications and Learning Technologies,2006 pp.8282 2 Tanveer A Zia and Md Zahidul Islam Communal Reputation and Individual Trust CRIT in Wireless Sensor NetworksIEEE, International Conference on Availability, Reliability and Security,  ARES 10 International Conference on, 2010 pp. 347352 3 H.R. Zeilandoo, M.A. Ngadi Intruder Location Tracking second international conference on computer and electirical engineering, DO110.1109ICCEE.2009.53, 2009 pp.507511 4 Xinyu Xing Shakshuki, E. Benoit, D. Sheltami, T Security Analysis and Authentication Improvement for IEEE 802.11i Specification IEEE lobal Telecommunications Conference, 2008. IEEE GLOBECOM 2008. pp. 15 5 Boudriga, N. Baghdadi, M. Obaidat, M.S A New Scheme for Mobility, Sensing, and Security Management in Wireless Ad Hoc Sensor NetworksIEEE 39th Annual Simulation Symposium, November Digital Object Identifier 10.1109ANSS.2006.8, 2006. 6 Todd Kennedy, Ray Hunt, Christchurch A Review of WPAN Security Attacks and Prevention the International Conference on Mobile , 2008  portal.acm.org 7 Chi Yoon Jeong, Beom Hwan Chang and Jung Chan Na, A Survey on Visualization for Wireless Security Networked Computing and Advanced Information Management, 2008. NCM 08. Fourth International Conference Volume 1 , 2008.pp. 129132 8 Neel Diksha, Agarwal Shubham Backdoor Intrusion in Wireless NetworksProblems and Solutions Communication Technology, 2006. ICCT 06. International Conference 2006 pp. 14  9 Binod Vaidya, SangDuck Lee, Jongan Park Evaluation of Secure Multimedia Services over Wireless Access Network, Ubiquitous Multimedia Computing, 2008. UMC 08. International Symposium   , May 2008 pp. 181184 10 Hassen Redwan and KiHyung Kim Survey of Security Requirements, Attacks and Network Integration in Wireless Mesh Networks Frontier of Computer Science and Technology, 2008. FCST 08. JapanChina Joint Workshop , 2008 pp. 39  11 Sarijari, M.A.B. Rashid, R.A. Rahim, M.R.A. Mahalin, N.H Wireless Home Security and Automation System Utilizing ZigBee based Multihop CommunicationProceedings of IEEE 2008 6th National Conference on Telecommunication Technologies, August 2008  pp. 242245. 12 Pathan, A.S.K. HyungWoo Lee Choong Seon Hong Security in Wireless Sensor Networks Issues and Challenges Advanced Communication Technology, 2006. ICACT 2006. The 8th International ConferenceVolume 2 , Feb 2006 pp. 1048 13 Ashraf, A. Rauf, A. Mussadiq, M. Chowdhry, B.S. Hashmani, M A Model for Classifying Threats and Framework Association in Wireless Sensor Networks Anticounterfeiting, Security, and Identification in Communication, 2009. ASID 2009. 3rd International Conference pp. 79 14 Slijepcevic, S. Potkonjak, M. Tsiatsis, V. Zimbeck, S. Srivastava, M.B On Communication Security in Wireless AdHoc Sensor Networks Enabling Technologies Infrastructure for Collaborative Enterprises, 2002. WET ICE 2002. Proceedings. Eleventh IEEE International Workshops,2002 PP.139144 15 Xi Peng Zheng Wu Debao Xiao Yang YuStudy on Security Management Architecture for Sensor Network based on Intrusion Detection Networks Security, Wireless Communications and Trusted Computing, 2009. NSWCTC 09. International Conference on Networks Security, Wireless Communications  Trusted Computing,Vol. 2, 2009 pp. 503507 16 Lionel Besson, Philippe Leleu, Colombes Cedex France A Distributed Intrusion Detection System for AdHoc Wireless Sensor Networks, Systems, Signals and Image Processing, IWSSIP 2009. 16th International Conference, 2009 pp.13 17 Alia Fourati, Khaldoun Al Agha An IDS First Line of Defense for AdHoc Networks, Wireless Communications and Networking Conference, 2007.WCNC 2007. IEEE pp. 26192624 18 Shahid Shehzad Bajwa, M. Khalid KhanGrouped Black hole Attacks Security ModelGBHASM for Wireless AdHoc Networks Computer and Automation Engineering ICCAE, 2010 The 2nd International Conference Vol.1 , 2010 pp. 756760 19 Gunhee Lee, DongKyoo Kim, Jungtack Seo     An Approach to Mitigate Wormhole Attack in Wireless AdHoc Networks 219International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org  Information Security and Assurance, 2008. ISA 2008. International Conference  , 2008 pp. 220225 20 S Yi, P Naldurg, R Kravets  Urbana  Citeseer A SecurityAware Routing Protocol for Wireless AdHoc Networks                       220International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518   IJSER  2011 httpwww.ijser.org  Compost Adsorption Desdorption of Picloram in the Presence of Surfactant on Six Agricultural Soils  Rounak M. Shariff Dept. of Chemistry, College of Scince, University of Salahaddin  HawlerIraq Abstract To investigate the effect of different types of surfactants on adsorption behavior of pesticide, picloram 4amino3, 5, 6trichloropicolinic acid which is an ionic herbicide on six agricultural soil samples, picloram soil water systemsurfactant. The Freundlich adsorption coefficients Ks values for picloram adsorption in the presences of surfactant in three concentration of critical micelles concentration cmc, batch equilibrium experiments performed of cationic surfactant Hexadecyltrimethylammonium bromide HDTMA. The Ks values of picloram range between 0.9401.344, 0.9431.407, and 0.9521.434 mlg, for cmc10, cmc  and cmc20 respectively. Freundlich adsorption coefficients of picloram in the presence of anionic surfactant sodium dodecyl sulphate SDS was determined, the values of Ks obtained were in the range 0.7611.151, 0.6541.141, and 0.6311.099mlg, for cmc10, cmc, and cmc20 respectively. The Ks values for polyoxyethylene sorbitanmonooleate  tween80 were in the range 0.971 1.229, 1.1041.303, and 1.1891.404  mlg,  for cmc10, cmc, and cmc20 respectively. The values of Freundlich desorption coefficients Ksdes, linearity factor for desorption nsdes and regression factor for desorption R2 ranged from 0.8391.286, 0.2670.619, and 0.8890.993 respectively with HDTMA. The values of Ksdes, nsdes and R2 values ranged from 0.8951.289, 0.4940.818, and 0.8890.999 for desorption process respectively by anionic surfactant SDS, the values of Ksdes, nsdes and R2 ranged from 0.9531.270, 0.4290.650, and 0.8880.995 respectively for nonionic surfactant Tween80. Index Terms  Adsorption  desorption kinetics, Adsorption isotherms, HPLC, Picloram, Surfactant.          1 INTRODUCTION                                                                     He use of kinetic models in the study of adsorption and desorption processes in heterogenous system is important. Three reasons for the use of kinetic or timedependent models in soils have been suggested. First, many reactions in soil are slow yet, they proceed at measurable rates. Second, nonequilibrium conditions can exist as a result of the physical transport of gases and solutes. Third, information about reaction mechanisms and processes occurring may be obtained from such data 1, 2.  The sorption pattern indicates an initial fast sorption that occurs within the first 24h tail it attainted the equilibrium within 48h. This was followed by slow reactions that appear to be the dominant processes i.e desorption of picloram if compared to the amount of picloram still sorbed on the soil. Picloram is anionic herbicide is used to control unwanted woody plants and to prepare sites for planting trees and used to control broadleaf plants and trees 3, 4. Its adsorption involved ionic interaction with positive charges in soil and also the less energetic Van der Waals forces and charge transfer 5, 6. A two step adsorptiondesorption mechanism was used to model the observed behavior that can be described in terms of external and internal sorption sites. Desorption from external sites is relatively fast, taking place in about 5h and is characterized by a firstorder rate constant. Many studies have indicated that the sorption and desorption of the organic chemicals in soils are not rapid, reversible process, despite past assumption to the contrary 7, 8.   A study of picloram desorption isotherms show positive hysteresis coefficients H in the six selected soil samples 9, 10. Hysteresis coefficients H1, where Na Ndes ratio for Ferundlich adsorption and desorption constants, respectively, indicating the greater or lesser irreversibility of adsorption in all samples, the highest values corresponding for which the highest adsorption constant was obtained. The coefficient H1 is a simple one and easy to use, indicating an increase in the irreversibility of the adsorption of herbicid as the clay content increases 8.   2 METHODLOGY 2.1 Soils Fresh soil samples were taken from plough layer 015 cm depth, after removal of stones and debris, air dried under shade, ground then sieved through 2mm sieve and stored in black plastic container in dark11, 12. The six soil samples were collected from six main agricultural, representing a range of physicochemical properties. Subsamples of homogenized soils were analyzed for moisture content, organic matter content, particle size distribution, texture, pH, loss on ignition and exchangeable basic cations were listed in Table 1 a  b. T  Rounak M.Shariff, Departmen of Chemistry, College of Science, University of Salahaddin HwalerIraq. Email rounakm2000yahoo.com 221International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org  2.2 Pesticide and Surfactant  Analytical grad substituted picloram herbicide was purchased from Riedalde Haen, SigmaAldrich company ltd. With following purities expressed in weight percent picloram 97.4 CASNo.1918021 respectively. The three different surfactants employed in this study comprised the cationic Hexadecyltrimethylammonium bromide HDTMA, the anionic sodium dodecyl sulphate SDS and nonionic polyoxyethylene sorbitanmonooleate Tween80. These compounds were purchased at reagent grade purity from BDH, and were used without further treatments. The critical micelles concentration cmc of the three surfactants are shown in Table 2. The three surfactants were studied at three concentrations the critical micelles concentration cmc, twentyfold higher than cmc 20cmc, and 10fold lower than cmc cmc10. All chemicals used were of analytical grade reagents and used without pretreatments. Standard stock solutions of the pesticides were prepared in deionised water.  2.3 Adsorption Experiments Adsorption of picloram from aqueous solution was determined at laboratory temperature 251 C employing a standard batch equilibrium method 13, 14. Duplicate airdried soil samples were equilibrated with different pesticide concentrations 2, 5, 10, and 15 g ml1 were for the pesticide at the soil solution ratios 48, in 16 ml glass tube fitted with Teflonlined screw caps. The samples plus blanks no pesticide and control no soil were thermostated and placed in shaker for 0.5, 1, 3, 6, 9, 12, 24, 48h. The tubes were centrifuged for 20 min. at 3500 rpm. One ml of the clear supernatant was removed and analyzed for the pesticide concentration 15. Pesticide identification was done by PerkinElmer series 200 USA family high performance liquid chromatography HPLC equipped with a changed loop 20l, C18 reversed phase column, flow rate 1.0  ml min1, and a variable wave length UV detector at wavelength 220 nm . Separation of picloram in aqueous phase was achieved with a mobile phase of 40 acetonitrile and 60 water acidified with 0.1 phosphoric acid. Each sample was injected twice to determine the pesticide content by integrating the obtained peak with the respective standard pesticides. The pesticide content was average of two measurements, with no more than 5 deviation between the measurements. The same procesure is repeted in precence of surfactants.  2.4 Desorption Experiments Desorption processes were done as each test tube was placed in a thermostated shaker at 25C after equilibration for 48 h with different pesticide concentrations 2, 5, 10 and 15 g ml1 the samples were centrifuged, 5ml of supernatant was removed from the adsorption equilibrium solution and immediately replaced by 5ml of water and was this repeated for four times. The resuspended samples were shaken for 0.5, 1, 3, 6, 9, 12, 24, and 48h for the kinetic study. Desorption of picloram was studied in the six selected soil samples, initially treated with different concentrations alone 2, 5, 10 and 15 gml1 in presence of surfactant, after equilibrium had been reached for 24h, 5ml were removed from the solution and immediately replaced by 5ml of the surfactant suspension used in the study. The resuspended samples were shaken for 24h, after sufficient time, were centrifuged and the desorbed picloram was measured as reported previously, this desorption procedure was repeated two times for each soil.  The amount of picloram at each desorption stage was calculated as the difference between the initial amount adsorbed and the amount desorbed, all determinations were carried out in duplicate. Competitive picloram adsorptiondesorption between soil and surfactant in the soilpicloramwatersurfactant system, in the presence HDTMA, SDS, and Tween80 at concentrations of cmc10, cmc, and cmc20 were conducted adsorptiondesorption isotherms 16. 3 DATA ANALYSIS 3.1 Adsorption Isotherms During adsorption studies, equilibrium concentration of pesticide in solution Ce was determined by direct analysis of the solution and amount of pesticide adsorbed on soil Cs was computed by the difference between the initial and the equilibrium concentration in the aqueous phase. Analysis of control samples showed that, in the absent of soil, pesticide concentration remained constant during the course of the batch experiments 17.   3.2 Freundlich AdsorptionDesorption Isotherms Adsorption isotherm parameters were calculated using the linearized form of Freundlich equation 18. Cs and Ce were defined previously, KF is Freundlich adsorption coefficients, and n is a linearity factor, it is also known as adsorption intensity, 1n is the slope and logKF is the intercept of the straight line resulting from the plot of logCs versus logCe shown in Fig1. The values of KF and 1n calculated, from the regression equation showed that Freundlich adsorption model effectively describes isotherms for the pesticides in all cases were listed in Table 3.  1   Desorption isotherms of picloram were fitted to the linearzed form of the Freundlich equation 19, 20.                                                     2   The values of KFdes is Freundlich desorption coefficients, and 1 ndes is a linearity factor, it is also known as desorpeFs CnKC log1loglog edesFdess CnKC log1loglog 222International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org  tion intensity and from the regression equation showed that Freundlich desorption model effectively describes isotherms for the pesticides in all cases, 21, 22 were listed in Table 4.  To investigate the effect of different types of surfactants on adsorption behavior of pesticides 23, 24.The Freundlich adsorption equation in the presence of surfactant used to determined follows as 8, 25.   3   Where Cs is the amount of adsorbed herbicide equation in the presence of surfactant g ml1, Ce is the equilibrium concentration of herbicide in solution of herbicides in solution equation in the presence of surfactant g ml1, and Ks and ns are the Freundlich affinity and nonlinearity coefficients respectively in the presence of surfactant. The Freundlich equation for desorption in the presence of surfactant, values of Ksdes for all experiments were calculated using the following equations  4     Where Cs is the amount of picloram still adsorbed g g1, Ce is the equilibrium concentration of picloram in solution after desorption equation in the presence of surfactant g mL1, and KFdes g g1nfdes mlnfdes g1 and nfd are the Freundlich desorption affinity and nonlinearity coefficients, respectively equation, for all the three surfactant at the three different concentrations.                  3.3 Hystersis ceofficient A study of picloram desorption isotherms show positive hysteresis coefficients H in the six selected soil samples 8, 10. Hysteresis coefficients H1, can be determined by using the following equation.  5  Where Na Ndes ratio for Ferundlich adsorption and desorption constants, respectively, indicating the greater or lesser irreversibility of adsorption in all samples, the highest values corresponding for which the highest adsorption constant was obtained. The coefficient H1 is a simple one and easy to use, Data in Table 5 demonstrated H1 values for picloram from the selected soil samples in the range from 0.2230.553 for desorption process, indicating an increase in the irreversibility of the adsorption of herbicide as the clay content increases 8. 4 RESULT AND DISCUSSION 2.1 Adsorption  Desorption Isotherms Previously work indicated that the linear model not fitted properly most experimental data with the pesticides 12. The nonlinear adsorption isotherms might be expected for the compounds for which competition for a limited number of cation exchange sites contributes significantly to adsorption process. Data from batch equilibrium method revealed that the adsorption of herbicides on the selected soil samples followed the first order rate law. To investigate the effect of different types of surfactants on adsorption behavior of pesticides 26, 27. The Ks values for picloram adsorption in the presences of cationic surfactant HDTMA The Ks values of picloram range between 0.9401.344, 0.9431.407, and 0.9521.434 mlg, for cmc10, cmc, and cmc20 respectively. Freundlich adsorption coefficients of picloram in the presence of anionic surfactant SDS was determined and summarized in table 3, the Ks values obtained were in the range 0.7611.151, 0.6541.141, and 0.6311.099 mlg, for cmc10, cmc, and cmc20 respectively. Batch equilibrium experiments performed for tween80, picloram soil water system and Ks values were determined and summarized in table 3. The Ks values were in the range 0.971 1.229, 1.1041.303, and 1.1891.404  mlg,  for cmc10, cmc, and cmc20 respectively. The values of KF, n and R2 ranged from 1.0781.211, 0.3440.966, and 0.8820.993 respectively for adsorption picloram without surfactant, while the values of KFdes, n and R2 ranged from 1.0451.586, 0.7181.947, and 0.9870.999 respectively for desorption of picloram without surfactant. The desorption isotherms of picloram with HDTMA treatment was lower. Data demonstrated in table 4 represents the values of Ksdes, nsdes and R2 for desorption of picloram from the selected soil samples. The values of Ksdes, nsdes and R2 ranged from 0.8391.286, 0.2670.619, and 0.8890.993 respectively. However sorption of anionic herbicides picloram was almost unaffected by anionic surfactant SDS, slightly decrease in adsorption, with slightly increased in desorption of picloram was detected. Data demonstrated in table 4. Ksdes, nsdes and R2 for desorption of picloram from the selected soil samples. esss CnKC log1loglog esdessdess CnKC log1loglog desaNNH 1 Fig.1. Fitted adsorption isotherm models Ferundlich model for picloram selected soils  S1,  S2,  S3,   x S4,  S5, S6.  223International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org  Ksdes, nsdes and R2 values ranged from 0.8951.289, 0.4940.818, and 0.8890.999 for the desorption process respectively. The nonionic surfactant Tween80 at different concentrations enhanced desorption of herbicides from soils. All desorption isotherms showed that hysteresis coefficients were decreased. These variations were dependent on surfactant concentration and soil OM and the clay contents. The effect of tween80 on desorption of the picloram was very low in soil with a high clay content. The results indicate the potential use of tween80 to facilitate desorption of these herbicides from soil to the watersurfactant system. Values of Ksdes, nsdes and R2 for desorption of picloram from the selected soil samples. Data in Table 4 listed the values of Ksdes, nsdes and R2 ranged from 0.9531.270, 0.4290.650, and 0.8880.995 respectively. Important aspect to be considered is the interaction of surfactant with soil, since it may, on one hand, alter the surfactant concentration in solution, thereby decreasing its efficiency for desorption, and on the other, alter the soil surface, where surfactant molecules may be adsorbed in the form of monomer or forming hemicelles or admicelles. Thus surfactant adsorption increases the organic content of the soil and increased hydrophobic surfaces, which may contribute to decrease in the organic compound desorption. Of all the above processes, the study of surfactantenhanced desorption for organic pollutants adsorbed on soil has been addressed by many investigators in recent years although such information can only be considered a beneficial effect in the context of major engineered remediation processes. In the study of surfactant enhanced desorption it is necessary to take into account the characteristics of the surfactant e.g., chemical structure, hydrophiliclipophilic balance HLB, or cmc, its concentration in the soilwater system, the solubility and hydrophobicity of the characteristics of soil e.g., OM, clay content. Enhanced solubility of pollutants has been clearly indicated by several authors at surfactant concentrations higher than the cmc. However, at surfactant concentrations below the cmc competitive adsorption of organic compound by soil andor by a surfactant in solution may occur, and hence an increase or decrease in desorption of compound from soil, depending on the characteristics of soil and organic compound 28, 29. 4.2 Hystersis Coefficients Desorption isotherms of picloram show a positive hysteresis coefficients H1. Values of hysteresis coefficient for adsorption desorption of picloram on the selected soil samples in the presence of HDTMA, SDS and tween80 were summarized in Table 5. The results of present study, which show the decrease in H1  values indicated higher hysteresis at lower pesticide concentration in presence of HDTMA, exhibited a higher sorption affinity and higher resistance for desorption30. Data in Table5 demonstrated that H1 for desorption of picloram from the selected soil samples in the presence of HDTMA were in the range S3 S6 S5S4 S1 S2, values of H1 ranged from 0.6932.594. The increase or decrease in hysteresis of the adsorptiondesorption isotherms in the presence of SDS solutions depend on the SDS concentration and on OM content of the soils .Below the cmc, SDS only increase the desorption of picloram in the highest OM content3.196. However, above the cmc20 desorption of picloram increases in all soils while the efficiency of desorption increasing with OM content of the Soil 31, 32. Data in Table 4 demonstrated the value of H1 for desorption of picloram from the selected soil samples in the presence of SDS were in the range S5 S4S6  S1 S2 S3, and values of H1 ranged from 0.4931.667. Consistent with the previous study 33, 34.  The H1 values for tween80 decrease indicated higher hysteresis at lower picloram concentration. OM considered as the primary soil component responsible for the adsorption of nonionic pesticides. Desorption of the neutral form was completely reversible, however, the charged species exhibited desorptionresistance fraction. The difference in sorption and desorption between the neutral and charged species is attributed to the fact that the neutral form partition by the hydrophobic binding to the soil, while anionic sorbs by a more specific exothermic adsorption reaction 35, 36.  Data in Table 4 demonstrated that H1 for adsorptiondesorption of picloram from the selected soil samples in the presence of tween80 were in the range S2 S1S5 S4 S6 S3, and values of H1 ranged from 0.3312.741. Desorption of soilassociated pesticides, hysteresis, and possible mechanisms have received considerable attention in literature 37, 38. Desorption rates of pesticides can be characterized by three types of processes, rapid desorption, ratelimited desorption, and a fraction that does not desorbed over experimental time scale. Many factors affect the adsorptiondesorption of pesticides such as pesticide type soil properties, organic matter, clay content, soil pH and environmental conditions 39, 40. Equation 5 shows the main effect of surfactant at concentrations close to cmc is to increase the affinity of picloram for the soil with, except for soils high in clay content where the surfactant effect is to enhance the affinity of picloram for aqueous phase 41. 5 CONCLUTION Desorption rates of pesticides can be characterized by three types of processes, rapid desorption, ratelimited desorption, and a fraction that does not desorbed over experimental time scale. Many factors affect the adsorptiondesorption of pesticides such as pesticide type soil properties, organic matter, clay content, soil pH and environmental conditions.  The batch kinetics experiments were used to differentiate the behavior of the pesticide in six agricultural soil samples. The desorption studies demonstrated that picloram has stronger affinity to all the selected soil samples than adsorption, and the soils varied widely in their adsorption capacities for picloram. We have further found that soil OC and clay content and the 224International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org  chemical nature of the constituents determined the adsorption affinity of the soil. Soil characteristics as solubility and hydrophobicity e.g., OM, clay content. Enhanced solubility of pollutants has been clearly indicated by several authors at surfactant concentrations higher than the cmc. ACKNOWLEDGMENT The authors wish to thank all the chemistry staff in Salahaddin University. I express my gratitude to Assit proff Dr. Kasim and Dr. Kaffia. REFERENCES 1 S.A. Wasay, W. Parker, P. J. Van Geel, S. Barrington and S. Tokunaga., Arsenic Pollution of a Loam soil Retention from and Decontamination. Journal of Soil Contamination. 9 2000 5164.  2 Kahan, S. Equilibrium and Kinetic Studies of the Adsorption Studies of the Adsorption of 2, 4D and Picloram on Humic Acid. Canadian Journal of soil Science. 53 1973429434. 3  Nearpass, D.C. Adsorption of Picloram by Humic Acids and Humin. Soil Sci. 121 1976272277. 4 Grover, R. Influence of Properties on Phytotoxicity of 4Amino3, 5, 6Trichloropicolinic acid Picloram. Weed Res. 8 1968226232. 5 Massaroppi, M.R.C. Machado, S.A.S. Avaca, .A. J. Braz. chem. Soc. 14 2003113. 6 Tan L.K Humphries D Yeung PYp Florence LZ. Determinations of Clopyralid, Picloram, and Silvex at Low Concentration in Soils by Calcium HydroxideWater Extraction and Gas Chromatography Measurement. J. Agricult. Food Chem. 44 199611351143. 7 James W. Bigger, Uri Mingelgrin, and Max W. Cheung. Equilibrium and Kinetics of Adsorption of Picloram and Parathion with Soils J, Agric. Food Chem. 26 197813061312 8 M.S. RodriguzCruz, M. J. SanchezMartin, and M. SanchezCamazano,. Enhanced Desorption of Herbicides Sorbed on Soils by Addition of TritonX100. J. Environ. Qual. 33 2004920929. 9 DiCesare, D., and J. A. Simth. Surfactant Effects on Desorption of Nonionic Compounds. Rev. Environ. Contam. Toxicol.134 1994129. 10 R.Celies, M. Real, M. C, Hermosin, and J. Crnejo, Desorption, persistence, and leaching of dibenzofuran in European soils, Soil Sci. Soc.Am. J. 70 200613101317.  11 R. Ahmad, R. S. Kookana, and A.M. Alston, Bulletin of Environ Contaminat and toxicol, 66 2001313318.  12 Rounak M. Shariff, The Study of adsorption and desorption of Picloram on Six Agricultural Soils Journal of Alanbar un.iversity for Pure Scince, 320091324. 13 Aekseeva T., Besse P., Binet F. Delort AM., Forano C., Sancelme M. Effect of Earthworms by Soil Ingestion on Atrazine Adsorption and Biodegradation Applied and Environmental Microbiology, 49 2004582587. 14 Piccolo, A.  Celano, G. HydrogenBonding Interactions between the Herbicide Glyphosate and Water Soluble Humic Substances. Envirinmental Toxicology and Chemistry, 13 199417371741. 15 White, J.L., ClayParticle Interactions In Kaufman, D.D., Still, G.G., Paulson, G.D., Bandal, S.K. Eds., Bound and Conjugated pesticide Residues. ACS Symposium Series, Washington, 1976208218. 16  Thanh H. Dao Competitive Anion Sorption Effects on Dairy West Water Dissolved phosphorus Extraction with Zeolitebased Sorbents. Food, Agriculture  Environment., 4 2003263269. 17  Murphy, E. M. and Zachara, J.M. The Role of Sorbed Humic Substances on the Distribution of Organic and Inorganic Contaminants in Groundwater, Geoderma. 671995 103124. 18  Stearman, G.K., R.J. Ewis, L.J. Tortorelli, and D.D. Tyler. Herbicide Reactivity of Soil Organic Matter Fraction in NoTilled Cotton. Soil Sci. Am. J. 53 1989 16901694.  19 Suman, G., Gajbhiye, v. T. Adsorption Desorption, Presestance, and Leaching Beahavior of Dithiopyr in an Auuvial soil of India. J. Environ. Sci. and Health. B 372002573586. 20 Mohammed A Ali and Peter J. Baugh. Sorption, Desorption Studies of Six Pyerethroids and Mirex on Soils using GC MSNICI Internet. J. Environ. Anal. Chem., 83 2003923933. 21 Soledad M. Andades R. Sonia Rodri GuezCruz M. Jesus SanchezMartin M.,  Maria SanchezCamazano, JanuaryMarch, Effect of the Modification of Natural Clay Minerals with Hexadecylpyridinium Cation on the AdsorptionDesorption of Fungicides Intern.J. Environ. Anal. Chem. 84 2004133141. 22 R. Ahmad., R. S. Kookona, A. M. Alston, and R. H. Bromilow,   Difference in Sorption Behavior of Carbaryl and Phosalone in Soils from Australia, Pakistan, and the United Kingdom. Aust. J. Soil Res., 39 2001 893908. 23 Baoshan Xing, Joseph J. Pingnatello, and Barbara Giglitti. Competitive Sorption between Atrazine and other Organic Compounds in Soils and Model Sorbents. Environ. Sci. Technol.30 1996 24322440 24 Werkheiser, W.O. and Anderson, S.J. Effect of Soil Properties and Surfactant on Primisulfuron Sorption. Journal of Environmental Quality, 25 1996 809814. 25 J. Park, S.D. Comfort, P. J. Shea, and T. A. Machacek. Remediating MunitionsContaminated Soil with Zerovalent Iron and Cationic Surfactants. J. Environ. Qual. 33 2004 13051313. 26 Maria Jose Carrizosa, Pamela J. Rice, William C. Koskinen. Ignacio Carrizosa and Maria del Carmen Hermosin. Camazano. Sorption of Isoxaflutole and DKN on Organoclays. Clay Minerals Socity. 52 2004341349. 27 Spalding, R.F., M.E. Exner, D.D. Snow, D.A. Cassada, M.E. Burbach, and S.J. Monson. Herbicides in ground water beneath Nebraskas management systems evaluation area. J. Environ. Qual. 32 20039299. 28 Rosen, M.J. 1989. Sufactants and Interfacial Phenomena. 2 nd Edition. John Wily and Sons. New York 29 J. W. Bowden, A. M. Posner. And J. P. Quirk. Ionic Adsorption on Variable Charge Mineral Surfaces. Theoretical  Charge Development and Titration Curves. Aust. J. Soi Res. l5 1977 121124. 30 M. Sanchezcamazano, M., M.J. SanchejMartin, M.S. RodriguezCurz. Sodium Dodycyl SulphateEnhanced Desorption of 225International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org  Atrazine Effect of Surfactant Concentration and Organic Matter Content of Soil. Chemosphere, 41 200013101305. 31 Rafael Celis and William C. Koskinen Aamaned. Characterization of Pesticide Desorption from Soil by the Isotopic Exchange Technique Soil Sci. Soc. Am. J. 63 199916591666. 32 Uan Boh cheah, Ralph C. Kirkwood, KengYeang Lum.  Adsorption, Desorption and Mobility of four Commonly used Pesticides in Malaysian Agricultural soils Pesticide Science. 50 19995363. 33 Locke, Martin A. Reddy, Krishan N., Gaston, lewis A., Zablotowicz, Robert. Adjuvant Modification of Herbicide Interactions in Aqueouse Soil Suspensions Soil Science.  2002. 1677444452. 34 Nelson, P. N., Baldock, A., Clarke, P., J. M., and Churchman, G. J. Dispersed Clay and Organic Matter in Soil their Nature and Associations. Australian Journal of Soil Research. 1999. 37 289316.  35 Mah Yew keong, K. Hidajat and M. S Uddin. Surfactant enhaned electroKinetic remedation of contaminated soil. Journal of the Institution of Engineers, 45 20056170. 36 Luning Pang and Murry Close. Afenuation and Transport of Atrazine and Picloram in an Alluvial gravel aquifer a Reacter Test and Batch Study New Zealandd Journal of Marine and Fresh Water Research.  3331999279291. 37 Ahmed F. ElAswad. Effect of Organicamendments on Aldicarb SorptionDesorption and SoilBound Residue Journal of Applied Sciences Research. 2007. 311 14371448 38 Zareen Khan, and Y.Anjaneyulu. Influence of Soil Components on AdsorptionDesorption of Hazardous OrganicsDevelopment of Low Cost Technology for Reclamation of Hazardous Waste Duumpsites. Journal of Hazardous Materials. . 9 2004305315. 39 Liang k. Tan. David Humphries, Pauky, Y. P. Yeung and L.Zeak Fibo. Determinations of Clopyralid, Picloram, and Silvex at Low Concentration in Soils by Calcium Hydroxid Water Extraction and Gas Chromatography Measurment. J. Agric. Food Chem. 44199611351143.  40 I. K. Konstantinon and T. A. Albanis. Adsorption Desorption Studies of Selected Herbicides in Soil Fly Ash Mixture. J. Agrie. Food Chem. 48 2000 47804790. 41 Davids. Kossen and Stephen V. Byrme. Interaction of Aniline with Soil and Ground Water at an Industrial Spill Site. Environmental Health Preespective. . 1031995932942.                                                       226International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org                          TABLE 1A SOME PHYSICOCHEMICAL PROPERTIES OF THE SELECTED SOIL SAMPLES Soil OM  Moisture Loss On ignition                C EC                                 meq100g1   E.C102  sm inD.w pH  In   D.W  in CaCl2 S1 2.799 2.544 7.864 47.760 0.414 7.516 7.501 S2 1.039 1.864 3.445 54.276 0.431 6.825 6.805 S3 3.196 2.633 6.160 26.780 0.572 6.981  6.906 S4 2.356 2.604 4.058 41.133 0.388 7.651 7.621 S5 1.914 2.012 5.442 32.488 0.492 6.395  6.305 S6 1.509 3.417 2.926 55.121 0.545 6.940 6.900  TABLE 1B PARTICALE SIZE DISTIBUTION A AND THE TEXTURE OF THE SELECTED SOIL SAMPLES No. Soil X m Y m Sand Silt  Clay  Texture         S1 Halabjh 4558.905  35101.166  4.4 39.4 56.2 Clay S2 Darb nidikhan 4542.31  3506.39  37.4 46.1 20..2 Loam S3 Jamjamal 4450.026  3531.355  11.7 52.2 36.1 Silty Clay Loam S4 Halabjh 4558.786  4510.338  20.2 48.0 31.8 Clay loam S5 Kirkuk 4421.825  3534.940  17.6 61.8 20.6 Silt Loam S6 Duhok 4253.944  3651.185  11.6 45.7 42.7 Silty Clay 227International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org             TABLE 3 FREUNDLICH ADSORPTION COEFFICENT FOR THE ADSORPTION OF PICLORAM IN THE PRESENCE OF HDTMA, SDS AND TWEEN80 OF THREE DIFFERENT CONCENTRATIONS ON THE SELECTED SOIL SAMPLES  TABLE 2  SELECTED PROPERTIES OF THE SURFACTANTS M6 TO 13, N7 TO13  Surfactant Type Formula cmc gL1     HDTMA Cationic C16H33 NCH3 3Br 0.3 SDS anionic C18H29SO4Na 2.38 Tween80 nonionic CH3CH2mCH2OCH2CH2OnH   0.04   Soil KFmlg Without surfac. n R2        Ks mlg HDTMA ns R2        Ks mlg SDS ns R2 Ks mlg Tween80 ns R2     cmc10 cmc cmc 20    cmc10 cmc  cmc 20  cmc10 cmc  cmc 20  S1 1.171 0.397 0.933 1.006 0.558 0.970 1.092 0.519 0.957 1.177 0.472 0.994    1.061 0.431 0.973 1.035 0.516 0.951 0.865 0.547 0.903 1.054 0.945 0.973 1.277 0.881 0.881 1.327 1.211 0.901 S2 1.085 0.435 0.973 0.940 0.423 0.991 0.943 0.398 0.990 0.952 0.605 0.980    0.984 0.456 0.977 0.974 0.655 0.989 0.814 0.824 0.939 1.199 0.671 0.960 1.218 0.884 0.939 1.221 0.797 0.937 S3 1.211 0.497 0.997 1.344 0.785 0.985 1.374 0.943 0.975 1.397 1.063 0.974    0.761 0.337 0.811 0.654 0.872 0.852 0.631 0.557 0.704 1.021 0.301 0.971 1.104 0.645 0.974 1.211 0.839 0.974 S4 1.078 0.344 0.999 1.142 0.652 0.965 1.153 0.647 0.976 1.195 0.885 0.938    0.963 0.707 0.929 0.936 0.782 0.914 0.852 0.763 0.901 0.971 0.290 0.997 1.117 0.926 0.923 1.189 1.395 0.893 S5 1.168 0.559 0.978 1.198 0.751 0.950 1.199 0.796 0.948 1.211 0.919 0.949    1.151 0.657 0.920 1.141 0.548 0.928 1.099 0.853 0.971 1.229 0.508 0.989 1.303 0.808 0.963 1.404 1.124 0.936 S6 1.189 0.966 0.882 1.328 1.046 0.871 1.407 1.269 0.865 1.434 1.398 0.884    0.816 0.706 0.788 0.794 0.886 0.762 0.675 0.970 0.752 1.003 0.204 0.926 1.198 0.802 0.987 1.211 0.692 0.990 228International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org   TABLE 4 FREUNDLICH DESORPTION COEFFICENT FOR THE DESORPTION OF PICLORAM IN THE PRESENCE OF HDTMA, SDS AND TWEEN80 OF THREE DIFFERENT CONCENTRATIONS ON THE SELECTED SOIL SAMPLES Soil    KFdesmlg Without surfac. n R2   Ksdes mlg HDTMA nsdes R2  Ksdes mlg SDSnsdes R2  Ksdes mlg Tween80 nsdes R2          cmc10 cmc  cmc 20  cmc10 cmc  cmc 20  cmc10 cmc  cmc 20  S1  1.045 0.718 0.999 0.839 0.408 0.985 1.211 0.617 0.991 1.237 0.619 0.993 0.895 0.558 0.901 0.964 0.658 0.889 1.219 0.494 0.949 1.174 0.650 0.995 1.199 0.561 0.919 1.256 0.542 0.992 S2  1.586 1.947 0.987   1.286 0.553 0.979 1.270 0.574 0.988 1.273 0.555 0.989 1.014 0.599 0.939 1.161 0.671 0.981 1.184 0.537 0.996 0.953 0.429 0.888 1.152 0.520 0.938 1.175 0.523 0.94 S3  1.066 1.189 0.999 1.136 0.267 0.961 1.217 0.543 0.948 1.223 0.519 0.979 1.156 0.683 0.999 1.198 0.614 0.961 1.202 0.641 0.963 1.018 0.909 0.999 1.167 0.599 0.974 1.270 0.499 0.970 S4  1.277 1.425 0.992 1.004 0.429 0.932 1.199 0.545 0.957 1.208 0.535 0.959 1.058 0.738 0.912 1.164 0.617 0.966 1.189 0.659 0.999 1.189 0.564 0.977 1.197 0.518 0.971 1.212 0.509 0.974 S5  1.398 1.733 0.998    1.038    0.479    0.896 1.196 0.563 0.926 1.212 0.566 0.938 1.009 0.621 0.981 1.082 0.631 0.975 1.241 0.577 0.995 1.097 0.495 0.952 1.146 0.512 0.961 1.158 0.501 0.965 S6  1.493 1.867 0.995 1.162 0.539 0.889 1.177 0.536 0.911 1.197 0.539 0.934 1.105 0.818 0.998 1.211 0.714 0.923 1.289 0.582 0.974 1.183 0.561 0.982 1.196 0.540 0.968 1.206 0.545 0.975  TABLE 5 HYSTERSIS COEFFICENT FOR ADSORPTION DESORPTION OF PICLORAM ON THE SELECTED D SOIL SAMPLES   H1 HDTAM H1SDS H1 Tween80                   Soil cmc10 cmc  cmc 20  cmc10 cmc  cmc20  cmc10 cmc  cmc20            S1 1.368 0.841 0.763 0.772 0.784 1.107 1.454 1.570 2.234 S2 0.765 0.693 1.090 0.761 0.976 1.534 1.564 1.700 1.524 S3 2.940 1.737 2.048 0.493 1.420 0.869 0.331 1.077 1.681 S4 1.519 1.187 1.654 0.958 1.267 1.158 0.514 1.788 2.741 S5 1.568 1.413 1.624 1.058 0.868 1.478 1.026 1.578 2.243 S6 1.941 2.367 2.594 0.863 1.241 1.667 0.364 1.485 1.269    229International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                                             ISSN 22295518   IJSER  2011 httpwww.ijser.org      A Few Aspects of Power Quality Improvement Using Shunt Active Power Filter C.Nalini Kiran, Subhransu Sekhar Dash, S.Prema Latha  Abstract Power quality standards IEEE519 compel to limit the total harmonic distortion within the acceptable range .This paper mainly deals with shunt active power filter which has been widely used for harmonic elimination. Active power filter which has been used here monitors the load current constantly and continuously adapt to the changes in load harmonics. The performance of three phase shunt active power filter using instantaneous power theory with PI and Hysteresis current controller is explained in this paper.  Index Terms Active power filters APF, composite load, harmonic compensation, linear and non linear load, reactive power.          1   INTRODUCTION harmonic is a component of a periodic wave having a frequency that is an integral multiple of the fundamental power line frequency. Harmonics are the multiple of the fundamental frequency, and whereas total harmonic distortion is the contribution of all the harmonic frequency currents to the fundamental. Harmonics are the byproducts of modern electronics. They occur frequently when there are large numbers of personal computers single phase loads, uninterruptible power supplies UPSs, variable frequency drives AC and DC or any electronic device using solid state power switching supplies 1 to convert incoming AC to DC. Nonlinear loads create harmonics by drawing current in abrupt short pulses, rather than in a smooth sinusoidal manner.  Fig 1 Difference between Linear and NonLinear Loads The terms linear and nonlinear define the relationship of current to the voltage waveform. A linear relationship exists between the voltage and current, which is typical of an acrosstheline load. A nonlinear load has a discontinuous current relationship that does not correspond to the applied voltage waveform. All variable frequency drives cause harmonics because of the nature of the frontend rectifier. 1.1 Need For Harmonic Compensation The implementation of Active Filters in this modern electronic age has become an increasingly essential element to the power network. With advancements in technology since the early eighties and significant trends of power electronic devices among consumers and industry, utilities are continually pressured in providing a quality and reliable supply. Power electronic devices 2 such as computers, printers, faxes, fluorescent lighting and most other office equipment all create harmonics. These types of devices are commonly classified collectively as nonlinear loads. Nonlinear loads create harmonics by drawing current in abrupt short pulses rather than in a smooth sinusoidal manner.  The major issues associated with the supply of harmonics to nonlinear loads are severe overheating and insulation damage. Increased operating temperatures of generators and transformers degrade the insulation material of its windings. If this heating were continued to the point at which the insulation fails, a flashover may occur should it be combined with leakage current from its conductors. This would permanently damage the device and result in loss of generation causing widespread blackouts.  One solution to this foreseeable problem is to install active filters for each nonlinear load in the power system network. Although presently very uneconomical, the installation of active filters proves indispensable for solving power quality 12 problems in distribution networks such as harmonic current compensation, reactive current compensation, voltage sag compensation, voltage flicker compensation and negative phase sequence current compensation. Ultimately, this would ensure a polluted free system with increased reliability and quality.  The objective of this project is to understand the modeling and analysis of a shunt active power filter. In A230International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                                             ISSN 22295518   IJSER  2011 httpwww.ijser.org  doing so, the accuracy of current compensation for current harmonics found at a nonlinear load, for the PQ theory control technique is supported and also substantiates the reliability and effectiveness of this model for integration into a power system network. The model is implemented across a two bus network including generation to the application of the nonlinear load.  The aim of the system simulation is to verify the active filters effectiveness for a nonlinear load. In simulation, total harmonic distortion measurements are undertaken along with a variety of waveforms and the results are justified accordingly.    One of the most important features of the shunt active filter system proposed is its versatility over a variety of different conditions. The application of the positive sequence voltage detector from within the active filter controller is the key component of the system. The positive sequence voltage detector gives incredible versatility to the application of the active filter, because it can be installed and compensate for load current harmonics even when the input voltage is highly distorted. When filters alike do not contain this feature and is installed with a distorted voltage input, the outcome is a low efficient current harmonic compensator with poor accuracy of compensation current determination.  1.2   Harmonic filters  Harmonic filters are used to eliminate the harmonic distortion caused by nonlinear loads. Specifically, harmonic filters are designed to attenuate or in some filters eliminate the potentially dangerous effects of harmonic currents active within the power distribution system. Filters can be designed to trap these currents and, through the use of a series of capacitors, coils, and resistors, shunt them to ground. A filter may contain several of these elements, each designed to compensate a particular frequency or an array of frequencies.  1.3 Types of harmonic filters involved in harmonic compensation  Filters are often the most common solution that is used to mitigate harmonics from a power system. Unlike other solutions, filters offer a simpler inexpensive alternative with high benefits. There are three different types of filters each offering their own unique solution to reduce and eliminate harmonics. These harmonic filters are broadly classified into   passive, active and hybrid structures. The choice of filter used is dependent upon the nature of the problem and the economic cost associated with implementation.   A passive filter is composed of only passive elements such as inductors, capacitors and resistors thus not requiring any operational amplifiers. Passive filters are inexpensive compared with most other mitigating devices. Its structure may be either of the series or parallel type. The structure chosen for implementation depends on the type of harmonic source present. Internally, they cause the harmonic current to resonate at its frequency. Through this approach, the harmonic currents are attenuated in the LC circuits tuned to the harmonic orders requiring filtering. This prevents the severe harmonic currents traveling upstream to the power source causing increased widespread problems.  An active filter is implemented when orders of harmonic currents are varying. One case evident of demanding varying harmonics from the power system are variable speed drives. Its structure may be either of the series of parallel type. The structure chosen for implementation depends on the type of harmonic sources present in the power system and the effects that different filter solutions would cause to the overall system performance.   Active filters use active components such as IGBTtransistors to inject negative harmonics into the network effectively replacing a portion of the distorted current wave coming from the load.   This is achieved by producing harmonic components of equal amplitude but opposite phase shift, which cancel the harmonic components of the nonlinear loads. Hybrid filters combine an active filter and a passive filter. Its structure may be either of the series or parallel type. The passive filter carries out basic filtering 5th order, for example and the active filter, through precise control, covers higher harmonics. 1.4 Passive Filters     Passive filters are generally constructed from passive elements such as resistances, inductances, and capacitances. The values of the elements of the filter circuit are designed to produce the required impedance pattern. There are many types of passive filters, the most common ones are singletuned filters and highpass filters. This type of filter removes the harmonics by providing a very low impedance path to the ground for harmonic signals.  1.4.1 Advantages and Disadvantages of the Passive Filters   The advantages of the Passive filters are  Shunt filters have the extra advantage of providing reactive compensation needed by the harmonic producing devices.  The disadvantages of the Passive filters are 231International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                                             ISSN 22295518   IJSER  2011 httpwww.ijser.org   The source impedance influences the compensation characteristics of the LC filters.  Frequency variation of the power system and tolerances in filter components affect the compensation characteristics of the LC filters.  1.5 Representation of harmonics  Fig 2 Harmonics from a Non Linear Load Fig 2   illustrates components of shunt connected active power filter with wave forms showing cancellation of harmonics from a nonlinear load.   The current waveform for cancelling harmonics is achieved with the voltage source inverter and reactor. The reactor converts the voltage signal created by the inverter to a current signal. The desired waveform is obtained by accurately controlling the switches in the inverter. Control of the current wave shape is limited by the switching frequency of the inverter and by the available driving voltage across the interface reactor.  The driving voltage across the reactor determines the maximum didt that can be achieved by the filter. This is important because relatively high values of didt may be needed to cancel higher order harmonic components.  2   ACTIVE FILTERS 2.1 Introduction to Active Filters                The increasing use of power electronicsbased loads adjustable speed drives, switch mode power supplies,etc. to improve system efficiency and controllability is increasing the concern for harmonic distortion levels in end use facilities and on the overall power system. The application of passive tuned filters creates new system resonances which dependent on specific system conditions. In addition, passive filters often need to be significantly overrated to account for possible harmonic absorption from the power system. Passive filter ratings must be cocoordinated with reactive power requirements of the loads and it is often difficult to design the filters to avoid leading power factor operation for some load conditions. Active filters have the advantage of being able to compensate for harmonic without fundamental frequency reactive power concerns. This means that the rating of the active power can be less than a comparable passive filter for the same nonlinear load and the active filter will not introduce system resonances that can move a harmonic problem from one frequency to another. 2.2 Types of Active Filters Active filter can be classified based on the connection scheme as  Shunt active filter  Series active filter and  Hybrid active filter.  2.2.1 Shunt Active Filter The active filter concept uses power electronic equipment to produce harmonic current components that cancel the harmonic current components that cancel the harmonic current components from the nonlinear loads.. In this configuration, the filter is connected in parallel with the load being compensated .Therefore the configuration is often referred to as an active parallel or shunt filter.    Fig 3 illustrates the concept of the harmonic current cancellation so that the current being supplied from the source is sinusoidal. The voltage source inverter used in the active filter makes the harmonic control possible. This inverter uses dc capacitors as the supply and can switch at a high frequency to generate a signal that will cancel the harmonics from the nonlinear load. The active filter does not need to provide any real power to cancel harmonic currents from the load. The harmonic currents to be cancelled show up as 232International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                                             ISSN 22295518   IJSER  2011 httpwww.ijser.org  reactive power. Reduction in the harmonic voltage distortion occurs because the harmonic currents flowing through the source impedance are reduced. Therefore, the dc capacitors and the filter components must be rated based on the reactive power associated with the harmonics to be cancelled and on the actual current waveform rms and peak current magnitude that must be generated to achieve the cancellation.  Fig 3 Shunt Active Power Filter The current wave form for canceling harmonics is achieved with the voltage source inverter in the current controlled mode and an interfacing filter. The filter provides smoothing and isolation for high frequency components. The desired current waveform is obtained by accurately controlling the switching of the insulated gate bipolar transistors IGBTs in the inverter. Control of the current wave shape is limited by the switching frequency of the inverter and by the available driving voltage across the interfacing inductance. The driving voltage across the interfacing inductance determines the maximum didt that can be achieved by the filter. This is important because relatively high values of didt may be needed to cancel higher order harmonic components. Therefore, there is trade off involved in sizing the interface inductor. A large inductor is better for isolation from the power system and protection from transient disturbances. However, the larger inductor limits the ability of the active filter to cancel higher order harmonics. The inverter in the Shunt Active Power filter is a bilateral converter and it is controlled in the current Regulated mode i.e. the switching of the inverter is done in such a way that it delivers a current which is equal to the set value of current in the current control loop. Thus the basic principle of Shunt Active Filter is that it generates a current equal and opposite to the harmonic current drawn by the load and injects it to the point of coupling there by forcing the source current to be pure sinusoidal. This type of Shunt Active Power Filter is called the Current Injection Type APF. .                Fig 4 shows need of shunt active filter  2.3 Harmonic compensation 2.3.1 Current Harmonic Compensation  Current harmonic compensation strategies are exceptionally important .Current harmonics are greatly reduced by the compensation of voltage harmonics at the consumers point of common coupling. The reduction in current harmonics is not only important for reasons such as device heating and reduction in life of devices but also in design of power system equipment. One of the major design criteria covers the magnitude of the current and its waveform.  This is to reduce cable and feeder losses. Since the root mean square RMS of the load current incorporates the sum of squares of individual harmonics, true current harmonic compensation will aid system designers for better approached power rating equipment.       2.3.2 Harmonic detection and extraction  A shunt active filter acts as a controllable harmonic current source. In principle, harmonic compensation is 233International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                                             ISSN 22295518   IJSER  2011 httpwww.ijser.org  achieved when the current source is commanded to inject harmonic currents of the same magnitude but opposite phase to the load harmonic currents. Before the inverter can subtly inject opposing harmonic currents into the power system, appropriate harmonic detection strategies must be implemented to efficiently sense and determine the harmonic current from the nonlinear load.   2.4 Types of harmonic detection strategies  There are 3 different types of harmonic detection strategies used to determine the current reference for the active filter.  These are 1. Measuring the load harmonic current to be compensated and using this as a reference command. 2. Measuring source harmonic current and controlling the filter to minimize it.  3. Measuring harmonic voltage at the active filter point of common coupling PCC and controlling the filter to minimize the voltage distortion.  So out of these harmonic detection strategies here we are using first method i.e., measuring the load current. 2.5 SPECIFICATION OF THE DESIGN  2.5.1 PI Controller Gain value                                KP0.32                                KI0.12 Filter inductance0.01H Filter capacitance1400f  2.5.2 Active power filter  Table 1 active power filter specifications  3 SHUNT ACTIVE POWER FILTER  3.1 Introduction to Open Loop System              In typical distribution systems the proliferation of diode rectifiers has resulted in serious utility interface issues as well as power quality degradation such as supply current and voltage harmonics, reactive power, flicker and resonance problems in industrial applications. Voltage distortion due to current harmonics is becoming a major problem for the utilities at distribution levels.  Utilities more frequently encounter harmonic related problems, such as higher transformer and line losses, reactive power and resonance problems, required derating of distribution equipment, harmonic interactions between customers or between the utility and load, reduced system stability and reduced safe operating margins. This has led to the proposal of more stringent requirements regarding power quality standards such as IEEE519 5 reflect these preoccupations. Passive filters are being used widely for harmonic elimination. However, they may create system resonances, need to be significantly overrated to account for possible harmonic absorption from the power system, must be coordinated with reactive power requirements of the loads and need a separate filter for each harmonic frequency to be cancelled. The concept of using active power filters to mitigate harmonic problems and to compensate reactive power was proposed more than two decades ago. Since then the theories and applications of active power filters have become more popular and have attracted great attention. The concept of using active power filters to mitigate harmonic problems and to compensate reactive power was proposed more than two decades ago.   Fig5  Main block diagram of an open loop system . Without the drawbacks of passive harmonic filters, the active power filter appears to be a viable solution for reactive power compensation as well as for eliminating harmonic currents. Dc link voltage  600V DC side capacitance C 1000F AC side inductance Lc 30mH AC side resistance Rc 10 234International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                                             ISSN 22295518   IJSER  2011 httpwww.ijser.org  The schematic diagram of a power conditioner system employing shunt APF is as shown above. The composite load includes a threephase diode rectifier, and a three phase R load. The ac side inductance is often sufficiently provided by the connected transformer. The unbalance in the present study is created by connecting resistive load between two phases. 3.3 Instantaneous Power Theory Proposed  theory6 based on instantaneous values in threephase power systems with or without neutral wire, and is valid for steadystate or transitory operations, as well as for generic voltage and current waveforms called as Instantaneous Power Theory7 or Active Reactive pq theory which consists of an algebraic transformation Clarke transformation of the threephase voltages in the abc coordinates to the 0 coordinates, followed by the calculation of the pq theory instantaneous power components.                                                                           Here  va,vb,vc arevoltages at load converted into vo,v,v by using above matrix8                   Here  ia,ib,ic load currentsconverted into io,i,i by using above matrix                                                   Real and reactive power So from the above equations active and reactive powers are obtained  Above equations pandq can be written in a matrix form  p and q are converted in to by using i,i.     so finaly we can obtain ica,icb,icc. 3.4 Block Diagram  Active filters produce a nearly sinusoidal supply current by measuring the harmonic currents and then injecting them back into the power system with a 180 phase shift.  The output waveform is thus the harmonic power which is recognized as containing only current harmonics.  This is justified as once can assume a perfectly sinusoidal voltage source by virtue of the integrated positive sequence voltage detector.  235International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                                             ISSN 22295518   IJSER  2011 httpwww.ijser.org   Fig6  Reprsenting instantaneous power theory 3.5 Hysterisis current controller A controlled current inverter is required to generate this compensating current. Hysteresis current control is a method of controlling a voltage source inverter so that an output current is generated which follows a reference current waveform. This method controls the switches in an inverter asynchronously to ramp the current through an inductor up and down so that it tracks a reference current signal. Hysteresis current control is the easiest control method to implement. A hysteresis current controller is implemented with a closed loop control system and is shown in diagrammatic form in Fig 7. An error signal, et, is used to control the switches in an inverter. This error is the difference between the desired current, ireft, and the current being injected by the inverter, iactualt. When the error reaches an upper limit, the transistors are switched to force the current down. When the error reaches a lower limit the current is forced to increase. The minimum and maximum values of the error signal are emin and emax respectively.     Fig 7 block diagram The range of the error signal, emax  emin, directly controls the amount of ripple in the output current from the inverter and this is called the Hysteresis Band.  The hysteresis limits, e min and e max, relate directly to an offset from the reference signal and are referred to as the Lower Hysteresis Limit and the Upper Hysteresis Limit. The current is forced to stay within these limits even while the reference current is changing. So these switching pulses s1,s2,s3,s4,s5,s6  are given to the voltage source inverter,that will produce harmonic current to compensate the harmonic current produced by non linear load.  4 SIMULATION RESULTS WITH FFT ANALYSIS 4.1 Open Loop with Voltage source inverter as a Filter with one load 236International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                                             ISSN 22295518   IJSER  2011 httpwww.ijser.org   Fig 8 Open Loop with Voltage source inverter as a Filter with one load The voltage source inverter used in the active power filter makes the harmonic control possible. This inverter uses a D.C capacitor. as the supply and can switch at a high frequency to generate a signal which will cancel the harmonics from the nonlinear load. Here a VSI with 1800 mode is used which will reduce the T.H.D. 4.1.1 Voltage source inverter output Here filter is taken with voltage source inverter to reduce higher order harmonics.          Fig9  Voltage source inverter out put  4.1.2 FFT Analysys Of  Load Current with one load  Fig 10  FFT Analysys Of  Load Current with one load Here open loop with one load power electronic     load is used,line voltage of vsi and fft analysis for load current is taken.  4.2 Open loop circuit with composite load  Fig11 open loop system with both linear and nonlinear load  Openloop systems sense the load current and the harmonics it contains. They inject a fixed amount of power in the form of current mainly reactive into the system, which may compensate for most of the harmonics andor reactive power available. Since there is no feedback loop on this system, there is no reference to check the performance and accuracy of the filter.   4.2.1 Fft analysis of load current  Here we can observe in open loop system higher order hormonics are reduced.Here as another load is added to the system at system total hormonic current distortion will be increased. 237International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                                             ISSN 22295518   IJSER  2011 httpwww.ijser.org      Fig12  fft analysis of load current   4.3 Closed loop system   Closed loop control systems incorporate a feedback loop providing greater accuracy of current injection   for harmonic compensation as well as reactive power reduction well over the open loop design. Here in the closed loop system a reference current is generated. This reference current is generated by using instantaneous power theory based on active power filter .This current is given input to the hysteresis current controller and compared with the one the phase current. So these two currents are compared and an error will be produced this error given to the hysteresis loop band. In the hysteresis control technique the error function is centered in a preset hysteresis band. When the error exceeds the upper or lower hysteresis limit the hysteretic controller makes an appropriate switching decision to control the error within the preset band.  4.3.1 Closed Loop Simulation Circuit  Closed loop control systems incorporate a feedback loop providing greater accuracy of current injection for harmonic compensation as well as reactive power reduction well over the open loop design.    Fig13 closed loop   4.3.2 Triggering pulses      Fig14  Triggering pulses obtained by hysteresis controller  4.3.3 Compensating voltage   238International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                                             ISSN 22295518   IJSER  2011 httpwww.ijser.org    Fig 15 Compensating current in open loop  4.3.4 Compensating current in closed loop   Fig16 compensating current in closed loop  4.3.8 Fft analysis of load current      Fig 17 FFT analysis for load current   4.4 Comparison   Table 2Total Harmonic Distortion Comparison Table   CONCLUSION A current decomposition technique based on instantaneous power theory for shunt active power filters is studied, a simu link model is designed and total harmonic distortion is calculated using FFT analysis.Active power filter which has been used here monitors the load current constantly and continuously adapt to the changes in load harmonics. The performance of three phase shunt active power filter using instantaneous power theory with PI and hysteresis current controller is explained in this paper.  ACKNOWLEDGMENT This work is affiliated to the Power Quality Improvement which has been supported by SRM University, Department of Electrical and Electronics Engineering.    TOTAL HARMONIC DISTORTION    Method Open loop only with  Nonlinear load Open loop with Composite load Closed loop with composite load Instantaneous Power Theory used for active power filters with PI and hysteresis current controller 14.12 18.26 2.75 239International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                                             ISSN 22295518   IJSER  2011 httpwww.ijser.org                REFERENCES  1 Schlabbach, D. Blume, and T. Stephanblome, Voltage Quality in Electrical Power Systems, ser. PEE Series. New York IEE Press,2001. 2 L. Gyugyi and E. C. Strycula, Active AC power filter, in Proc. IEEE IAS Annu. Meeting, 1976, pp. 529529. 3  H. Akagi, Y. Kanazawa, and A. Nabae, Generalized theory of the instantaneous reactive power in threephase circuits, in Proc. IEEE and JIEE IPEC, 1983, pp. 821827. 4 Y. Komatsu and T. Kawabata, Experimental comparison of pq and extended pq method for active power filter, in Proc. EPE, 1997, pp.2.7292.734 5 V. Soares, P. Verdelho, and P. D. Marques, Active power filter control circuit based on instantaneous active and reactive current id  iq method, in Proc. IEEE PESC, 1997, pp. 106101. 6 B. Singh, K. AlHaddad, and A. Chandra, A new control approach to threephase active filter for harmonics and reactive power compensation, 7 IEEE Trans. Power Syst., vol. 13, no. 1, pp. 133138, Feb. 1998.  Active power filters for nonlinear loads.Instantaneous Power Theory Based Active Power Filter A Mat lab Simulink Approach.  8 Generalized theory of the instantaneous reactive power in threephase circuits, in Proc. IEEE and JIEE IPEC, 1983, pp. 821827.           9Harashima F., Inaba H. and Tsuboi K., A closeloop control                        system for the reduction of reactive power required by                  electronic converters,Toshiba Electric Co., Tokyo, Japan,                  Oct. 1975.         10Knoell H., 3 kWswitchmode supply providing           Sinusoidal  mains current and large range of dcoutput,           PCI, Munich, Germany, 1980.          11Nastran J.. Active Power Filter, dissertation presented at                    the Faculty of Electrical Engineering, Ljubljana,                          Yugoslavia,1985.. 240International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518   IJSER  2011 httpwww.ijser.org  Tidal Power An Effective Method of Generating Power Shaikh Md. Rubayiat Tousif, Shaiyek Md. Buland Taslim AbstractThis article is about tidal power. It describes tidal power and the various methods of utilizing tidal power to generate electricity. It briefly discusses each method and provides details of calculating tidal power generation and energy most effectively. The paper also focuses on the potential this method of generating electricity has and why this could be a common way of producing electricity in the near future.            Index Terms  dynamic tidal power, tidal power, tidal barrage, tidal steam generator.        1 INTRODUCTIONIDAL power, also called tidal energy, is a form of hydropower that converts the energy of tides into electricity or other useful forms of power. The first largescale tidal power plant the Rance Tidal Power Station started operation in 1966.  Although not yet widely used, tidal power has potential for future electricity generation. Tides are more predictable than wind energy and solar power. Among sources of renewable energy, tidal power has traditionally suffered from relatively high cost and limited availability of sites with sufficiently high tidal ranges or flow velocities, thus constricting its total availability. However, many recent technological developments and improvements, both in design e.g. dynamic tidal power, tidal lagoons and turbine technology e.g. new axial turbines, cross flow turbines, indicate that the total availability of tidal power may be much higher than previously assumed, and that economic and environmental costs may be brought down to competitive levels. Tidal power traditionally involves erecting a dam across the opening to a tidal basin. The dam includes a sluice that is opened to allow the tide to flow into the basin the sluice is then closed, and as the sea level drops, traditional hydropower technologies can be used to generate electricity from the elevated water in the basin.  2 GENERATION OF TIDAL ENERGY Tidal power is the only form of energy which derives directly from the relative motions of the EarthMoon system, and to a lesser extent from the EarthSun system. Tidal forces produced by the Moon and Sun, in combination with Earths rotation, are responsible for the generation of the tides. Other sources of energy originate directly or indirectly from the Sun, including fossil fuels, conventional hydroelectric, wind, biofuels, wave power and solar. Nuclear energy makes use of Earths mineral deposits of fissile elements, while geothermal power uses the Earths internal heat which comes from a combination of residual heat from planetary accretion about 20 and heat produced through radioactive decay 80. Tidal energy is extracted from the relative motion of large bodies of water. Periodic changes of water levels, and associated tidal currents, are due to the gravitational attraction of the Sun and Moon. Magnitude of the tide at a location is the result of the changing positions of the Moon and Sun relative to the Earth, the effects of Earth rotation, and the local geography of the sea floor and coastlines. Because the Earths tides are ultimately due to gravitational interaction with the Moon and Sun and the Earths rotation, tidal power is practically inexhaustible and classified as a renewable energy resource. A tidal generator uses this phenomenon to generate electricity. Greater tidal variation or tidal current velocities can dramatically increase the potential for tidal electricity generation. The movement of the tides causes a continual loss of mechanical energy in the EarthMoon system due to pumping of water through the natural restrictions around coastlines, and consequent viscous dissipation at the seabed and in turbulence. This loss of energy has caused the rotation of the Earth to slow in the 4.5 billion years since formation. During the last 620 million years the period of rotation has increased from 21.9 hours to the 24 hours we see now in this period the Earth has lost 17 of its rotational energy. While tidal power may take additional energy from the system, increasing the rate of slowdown, the effect would be noticeable over millions of years only, thus being negligible.  2.1 Generating methods Tidal power can be classified into three generating methods Tidal stream generator, Tidal barrage, Dynamic   Shaiyek Md. Buland Taslim is currently pursuing masters degree program in electric power engineering in Royal Institute of Techolgy KTH Sweden, PH01913531992. Email bulandtaslimyahoo.com  Shaikh Md. Rubayiat Tousif is working as a Lecturer in American International UniversityBangladesh AIUB, Bangladesh, PH01913531993.        E mail tousifaiub.edu  T241International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518   IJSER  2011 httpwww.ijser.org  tidal power.  3 TIDAL STREAMGENERATOR A tidal stream generator is a machine that extracts energy from moving masses of water, or tides. These machines function very much like underwater wind turbines, and are sometimes referred to as tidal turbines. Tidal stream generators are the cheapest and the least ecologically damaging among the three main forms of tidal power generation.   Fig. 1. The worlds first commercialscale and gridconnected tidal stream generator. 3.1 Types of tidal stream generators Since tidal stream generators are an immature technology, no standard technology has yet emerged as the clear winner, but large varieties of designs are being experimented with, some very close to large scale deployment. Several prototypes have shown promise with many companies making bold claims, some of which are yet to be independently verified, but they have not operated commercially for extended periods to establish performances and rates of return on investments. 3.2 Energy calculations Various turbine designs have varying efficiencies and therefore varying power output. If the efficiency of the turbine  is known the equation below can be used to determine the power output of a turbine. The energy available from these kinetic systems can be expressed as    Where   the turbine efficiency P  the power generated in watts   the density of the water seawater is 1025 kgm A  the sweep area of the turbine in m V  the velocity of the flow  Relative to an open turbine in free stream, depending on the geometry of the shroud shrouded turbines are capable of as much as 3 to 4 times the power of the same turbine rotor in open flow.  3.3 Resource assessment While initial assessments of the available energy in a channel have focus on calculations using the kinetic energy flux model, the limitations of tidal power generation are significantly more complicated. For example, the maximum physical possible energy extraction from a strait connecting two large basins is given to within 10 by  Where   the density of the water seawater is 1025 kgm, g  gravitational acceleration 9.81 ms2, Hmax  maximum differential water surface elevation across the channel, Qmax maximum volumetric flow rate though the channel.  4 TIDAL BARRAGE A Tidal barrage is a damlike structure used to capture the energy from masses of water moving in and out of a bay or river due to tidal forces. Instead of damming water on one side like a conventional dam, a tidal barrage first allows water to flow into the bay or river during high tide, and releasing the water back during low tide. This is done by measuring the tidal flow and controlling the sluice gates at key times of the tidal cycle. Turbines are then placed at these sluices to capture the energy as the water flows in and out.  4.1 Generating methods The barrage method of extracting tidal energy involves building a barrage across a bay or river that is subject to tidal flow. Turbines installed in the barrage wall generate power as water flows in and out of the estuary basin, bay, or river. These systems are similar to a hydro dam that produces Static Head or pressure head a height of water pressure. When the water level outside of the basin or lagoon changes relative to the water level inside, the turbines are able to produce power. The basic elements of a barrage are caissons, embank242International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518   IJSER  2011 httpwww.ijser.org  ments, sluices, turbines, and ship locks.  Fig. 2. An artistic impression of a tidal barrage, including embankments, a ship lock and caissons housing a sluice and two turbines. 4.2 Ebb generation The basin is filled through the sluices until high tide. Then the sluice gates are closed. At this stage there may be Pumping to raise the level further. The turbine gates are kept closed until the sea level falls to create sufficient head across the barrage, and then are opened so that the turbines generate until the head is again low. Then the sluices are opened, turbines disconnected and the basin is filled again. The cycle repeats itself. Ebb generation also known as outflow generation takes its name because generation occurs as the tide changes tidal direction.  4.3 Flood generation The basin is filled through the turbines, which generate at tide flood. This is generally much less efficient than ebb generation, because the volume contained in the upper half of the basin which is where ebb generation operates is greater than the volume of the lower half filled first during flood generation. Therefore the available level difference  important for the turbine power produced  between the basin side and the sea side of the barrage, reduces more quickly than it would in ebb generation. Rivers flowing into the basin may further reduce the energy potential, instead of enhancing it as in ebb generation. Of course this is not a problem with the lagoon model, without river inflow.  4.4 Pumping Turbines are able to be powered in reverse by excess energy in the grid to increase the water level in the basin at high tide for ebb generation. This energy is more than returned during generation, because power output is strongly related to the head. If water is raised 2 ft 61 cm by pumping on a high tide of 10 ft 3 m, this will have been raised by 12 ft 3.7 m at low tide. The cost of a 2 ft rise is returned by the benefits of a 12 ft rise. This is since the correlation between the potential energy is not a linear relationship, rather, is related by the square of the tidal height variation.  4.5 Twobasin schemes Another form of energy barrage configuration is that of the dual basin type. With two basins, one is filled at high tide and the other is emptied at low tide. Turbines are placed between the basins. Twobasin schemes offer advantages over normal schemes in that generation time can be adjusted with high flexibility and it is also possible to generate almost continuously. In normal estuarine situations, however, twobasin schemes are very expensive to construct due to the cost of the extra length of barrage. There are some favorable geography, however, which are well suited to this type of scheme. 4.6 Tidal lagoon power Tidal pools are independent enclosing barrages built on high level tidal estuary land that trap the high water and release it to generate power, single pool, around 3.3Wm2. Two lagoons operating at different time intervals can guarantee continuous power output, around 4.5Wm2. Enhanced pumped storage tidal series of lagoons raises the water level higher than the high tide, and uses intermittent renewable for pumping, around 7.5Wm2 i.e. 10 x 10 km delivers 750MW constant output 247. These independent barrages do not block the flow of the river and are a viable alternative to the Severn Barrage. 4.7 Energy calculations The energy available from a barrage is dependent on the volume of water. The potential energy contained in a volume of water is  Where h is the vertical tidal range, A is the horizontal area of the barrage basin,  is the density of water  1025 kg per cubic meter    seawater varies between 1021 and 1030 kg per cubic meter and g is the acceleration due to the Earths gravity  9.81 meters per second squared. The factor is half due to the fact that the basin flows empty through the turbines the hydraulic head over the dam reduces. The maximum head is only available at the moment of low water, assuming the high water level is still present in the basin. 4.8 Example calculation of tidal power generation  Assumptions  Let us assume that the tidal range of tide at a particular place is 32 feet  10 m approx The surface of the tidal energy harnessing plant is 9 km 3 km  3 km  3000 m  3000 m  9  106 m2 243International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518   IJSER  2011 httpwww.ijser.org  Density of sea water  1025.18 kgm3 Mass of the sea water  volume of sea water  density of sea water  area  tidal range of water  mass density  9  106 m2  10 m  1025.18 kgm3  92  109 kg approx Potential energy content of the water in the basin at high tide    area  density  gravitational acceleration  tidal range squared    9  106 m2  1025 kgm3  9.81 ms2  10 m2 4.5  1012 J approx Now we have 2 high tides and 2 low tides every day. At low tide the potential energy is zero.  Therefore the total energy potential per day  Energy for a single high tide  2  4.5  1012 J  2  9  1012 J Therefore, the mean power generation potential  Energy generation potential  time in 1 day  9  1012 J  86400 s  104 MW Assuming the power conversion efficiency to be 30 The dailyaverage power generated  104 MW  30  100  31 MW approx  Because the available power varies with the square of the tidal range, a barrage is best placed in a location with very highamplitude tides. Suitable locations are found in Russia, USA, Canada, Australia, Korea, and the UK. Amplitudes of up to 17 m 56 ft occur for example in the Bay of Fundy, where tidal resonance amplifies the tidal range.  5 DYNAMIC TIDAL POWER Dynamic tidal power or DTP is a new and untested method of tidal power generation. It would involve creating large damlike structure extending from the coast straight to the ocean, with a perpendicular barrier at the far end, forming a large T shape. This long Tdam would interfere with coastparallel oscillating tidal waves which run along the coasts of continental shelves, containing powerful hydraulic currents  5.1 Description  Fig. 3. Topdown view of a DTP dam. Blue and dark red colors indicate low and high tides, respectively. A DTP dam is a long dam of 30 to 60 km which is built perpendicular to the coast, running straight out into the ocean, without enclosing an area. The horizontal acceleration of the tides is blocked by the dam. In many coastal areas the main tidal movement runs parallel to the coast the entire mass of the ocean water accelerates in one direction, and later in the day back the other way. A DTP dam is long enough to exert an influence on the horizontal tidal movement, which generates a water level differential head over both sides of the dam. The head can be converted into power using a long series of conventional lowhead turbines installed in the dam. 5.2Benefits A single dam can accommodate over 8 GW 8000 MW of installed capacity, with a capacity factor of about 30, for an estimated annual power production of each dam of about 23 billion kWh 83 PJyr. To put this number in perspective, an average European person consumes about 6800 kWh per year, so one DTP dam could supply energy for about 3.4 million Europeans. If two dams are installed at the right distance from one another about 200 km apart, they can complement one another to level the output one dam is at full output when the other is not generating power. Dynamic tidal power doesnt require a very high natural tidal range, so more sites are available and the total availability of power is very high in countries with suitable conditions, such as Korea, China, and the UK the total amount of available power in China is estimated at 80  150 GW. 5.3Challenges A major challenge is that a demonstration project would yield almost no power, even at a dam length of 1 km or so, because the power generation capacity increases as the square of the dam length both head and volume increase in a more or less linear manner for increased dam length, resulting in a quadratic increase in power generation. Economic viability is estimated to be reached for dam lengths of about 30 km. Other concerns include 244International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518   IJSER  2011 httpwww.ijser.org  shipping routes, marine ecology, sediments, and storm surges. Amidst the great number of challenges and few environmental impacts the method of utilizing tidal power to generate electricity has great potential and is certainly a technology most of the countries will try to harness in near future.  ACKNOWLEDGMENT We would like to thank Prof. Sekh Abdur Rob for helping us with his valuable ideas and supporting us throughout. We would like to thank him for motivating us to work on tidal power.    REFERENCES 1 Ocean Energy Tide and Tidal Power by Roger H. Charlier 2 Ocean Wave Energy Current Status and Future Prespectives Green Energy and Technology by Joao Cruz 3 Ocean Wave Energy Conversion by Michael E. McCormick 4 The Analysis of Tidal Stream Power by Jack Hardisty 5 Developments in Tidal Energy Proceedings of the Third Conference on Tidal Power, Institution Of Civil Engineers Contributor 6 Ocean, Tidal, and Wave Energy Power from the Sea Energy Revolution by Lynne Peppas 245International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518   IJSER  2011 httpwww.ijser.org  Design and Development of Fault Tolerent Control system for an Infant Incubator  Suswetha Parisineti, Eswaran.P  Abstract This paper proposes the design and implementation of real time monitoring of an infant incubator, based on sensor fault tolerant control system, using a PIC microcontroller. Temperature and humidity are two parameters considered for the design infant incubator. The purpose of a Fault tolerant control systems FTCS scheme is to ensure that faults do not result in malfunctioning and system failure and to achieve the of best performance even with minimum number of sensors working. Fault tolerant control systems FTCS have ability to detect sensor fault automatically and to isolate faulty sensor which leads to system failure. The fault detection and the isolation FDI problem is an inherently complex one and for this reason the immediate goals are to preserve the stability of the process and, if is possible, to control and continue the process in a slightly degraded manner. The role of the FDI algorithms is that the control equipment must automatically isolate the faulted area, to adopt the correct attitude, to generate, to choose and to validate the correct decision. Prototype of infant incubator using FTCS was implemented by using redundant sensor with Build in self test BIST facility.  Index Terms Sensor fault tolerant control, real time based, microcontroller, FTCS.        1 INTRODUCTION                                                                     HE objective of a fault tolerant control system FTCS is to maintain system availability when fault occurs, to improve the reliability of the control system and to minimize the effects on the system performance and safety 1.  Fault is a kind of malfunction in the system, which may lead to system degradation or any unacceptable performance of the system. The output of the Sensor should be constrained between the lower and upper limits, if it crosses these bounds then it is said that the senor is failed. Fault tolerant control FTC has been increasing in the last few years because FTC system has the ability to increase complex systems reliability and performance requirement in the events of faults. The design of a FTC system requires knowledge of advanced control mechanism 3. Systems mostly are very complicated. Designing a FTC system could also be very challenging. Different types of faults such as actuators, sensors, and system faults can occur. Each type of fault requires different approach to work with. A fault tolerant control system must be able to perform, fault detection, fault isolation, and fault diagnosis 3. FTC should also have the ability to detect faults and provide correction. Fault tolerant control system results on two approaches active and passive. The active approach relies on fault detection and isolation FDI scheme to detect the occurrence of faults in the system and to identify the source and severity of the faults. Secondly, in passive FTC, potential component faults are known a prior and are all taken into consideration in the control system design stage 5. Infant incubator provides a controlled environment for newborns needing special care, such as those born prematurely. By placing an infant in an incubator, doctors and nurses can set and monitor different aspects of the childs environment in order to create ideal conditions for survival and moreover it protect infants from pollutants and infection2.  This paper proposes the design and development of microcontroller based temperature and humidity controller for an infant incubator monitors and controls these two parameters constantly which are very critical for the normal growth of the new born premature babies. Infant incubator is used mainly to keep a babys care temperature stable at 37 Celsius and the relative humidity is maintained at  45 to 55RH.This system can automatically control the infants temperature at optimum level and to maintain high relative humidity so as to minimize the thermal loss. The developed system must be user friendly, cost effective and accurate. 2   SENSOR FAULT TOLERANT CONTROL SYSTEM Infant incubators and other advances in medical technology have made it possible for small or premature babies to survive in higher numbers than they did in the middle of the 20th century. An incubator is an infantstimulating system used for intensive care of the new born, premature or sick baby. It provides a safe and clean environment, which has fresh air, clean and sterile amT  Suswetha Parisineti is currently pursuing masters degree program in Embedded system Technologyry, in SRM University, Kattankulathur, Chennai, India. PH04427452270. Email suswetha.pgmail.com  Eswaran.P is currently working as Assistant professor in SRM University, Kattankulathur, and Chennai, India. PH04427452270.   Email eswaranpktr.srmuniv.ac.in 246International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518   IJSER  2011 httpwww.ijser.org  bient conditions for the babies. In addition to these, the incubator environment provides a homogeneous and stable temperature, a relative humidity RH level that are needed especially for intensive care of the premature baby.  The sensor fault tolerant control was implemented for an infant incubator system. The system mainly consists of the microcontroller along with two temperature sensors, two humidity sensors, switches connected on either side of the sensors, a relay board with fan and heater, an LCD and LEDs for display purpose. The block diagram of an infant incubator is shown in the figure 1. The input of the sensor ambient condition of infant incubator and the output of the sensor is connected to the analog to digital converter ADC of the microcontroller. The microcontroller gets the value from the sensors and then displays in LCD display. The control action is taken by the micro controller to detect which sensor is failed or working properly according to the values taken by the sensors. The LEDs, LCD display and the relay board are connected to the IO ports of the microcontroller. The heater and fan are connected to the relay board.                   3 IMPLEMENTATION OF FTCS  Temperature and humidity are two very important parameters that need to be monitored continuously in the infant incubator chamber. Similar environment can be replicated for the preterm infant or new born baby. Temperature can be displayed in terms of degree Celsius 0C and humidity in terms of relative humidity which is expressed as  Relative Humidity RH.  3.1 Hardware Implementation of  FTCS   The PIC16F877A microcontroller chip selected for the purpose of realizing the plant model. The model mainly consists of two LM35 temperature sensors, two CHR01 humidity sensors, power supply circuitry, Switching board with 8 LEDS  4 red and 4 green, 8 switches connected, each on either side of the 4 sensors, and a relay board for connection of a fan and a heater. The implementation blockdiagram was shown in the figure 2.                   LM35 is an integrated circuit temperature Sensor and CHR01 is an Impedance type humidity sensor are shown in the figure 3 a, b. The three pins of LM35 sensors are input pin is connected to VCC power supply, the output is connected to the ADC of the microcontroller and the ground pin is connected to the ground. ULN2002 used as driver to relay. Through relay load terminals the heater and fan are connected as shown in the figure 4 a.                          Fig. 1. Block Diagram of an infant incubator.   Fig.2 Implementation block diagram of an infant incubator   Fig.3 a LM35 Temperature Sensors  Fig.3 b CHR01 Humidity Sensor  247International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518   IJSER  2011 httpwww.ijser.org  .                                                       The switching board consists of 8 ONOFF switches and 8 LEDs used to do self test of FTCS system. Of these 8 LEDs 4 are red LEDs and 4 are green LEDs. Red LED glows, indicate the failure operation and the green LEDglows, indicate the correct and the safe operation and their component layout are shown in the figure 4 b, c.  Figure 4 d shows the complete hardware implementation of the FTCS Infant incubator system.It has microcontroller module, sensors module two temperature and two humidity sensors, BIST module sensor switching board, Power supply module, and Load driver module with fan and heater. 3.2 Software implementation of FTCS Consider,  T1 is the value from the temperature sensor1  T2 is the value from the temperature sensor2  H1 is the value from the humidity sensor1  H2 is the value from the humidity sensor2                    Fig.4 a Circuit diagram of the hardware components in the infant incubator.   Fig.4 b Switching Board practice   Fig.4 c Relay Board.   Fig 4 d    Hardware Implementation of the FTCS for Infant Incubator   Fig.4e Flow chart of the operation of the Infant Incubator system.  248International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518   IJSER  2011 httpwww.ijser.org   Table 1 shows the sensor conditions for FTCS. The flowchart shown in the figure 4 e explains the operation of the sensor fault tolerant control of an infant incubator system. The T1, T2, H1, H2 values are taken from the sensors, the microcontroller reads the values and then compares them with the upper and lower limits. If it is not within the limit then the heater or fan operations are handled. If the values read by the sensors are less then 10 C or 10RH then the sensors are said to be failed. If all the sensors show the error value then the system is said to be a failure one and the operation should be stopped. 4  RESULTS AND DISCUSSION OF FTCS Figure 5a shows the Simulation results of complete circuit carried out using Proteus software.  The results obtained from the PIC16F877A microcontroller interfaced with two temperature sensors, two potentiometers as humidity sensors, an LCD and LEDs. The LCD is used to monitor the sensor readings. The LEDs are also connected for identification of the sensor working properly.                                        The figure 5 b shows that the two LM35 temperature sensors and the two CHR01 humidity sensors are working correctly and the system is said to be 100 reliable. The LEDs glowing indicates the sensors are working properly and there is no malfunction in the system. The LCD display shows both the temperature and the humidity sensors values are within the specified range.                            TABLE 1 Analysis of the Sensor Fault Tolerant Control System   Fig.5a  Simulation result showing  Temperature sensors and Humidity sensors are working effectively.   Fig.5c FTCS Showing One humidity sensor and one temperature senor are isolated.  Fig.5d  One humidity sensor and one temperature senor are failed.  Fig.5b  Hardware of FTCS showing  Temperature sensors and Humidity sensors are working effectively. 249International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518   IJSER  2011 httpwww.ijser.org  Figure 5 c shows that the only one LM35 temperature sensor and the only one CHR01 humidity sensor are working correctly. In that case the system is said to be 50 reliable. If one of the LED was not glowing indicates the system operation continues with some warnings. The LCD display shows one temperature sensor value and the one humidity sensor value are within the specified range and one temperature sensor value and the one humidity sensor value are not within the range since they are isolated.                   The figure 5 d, e shows that the only one LM35 temperature sensor and the only one CHR01 humidity sensor are working correctly and one temperature sensor and one humidity sensor are failed. The system is said to be 50 reliable. The LEDs are not glowing indicates the system operation is not disturbed but some warnings are arised. The LCD display shows one temperature sensor value and the one humidity sensor value are within the specified range and one temperature sensor value and the one humidity sensor value are not within the range since they are failed. .                  The figure 5 f, g shows the implementation of FTCS infant incubator in LABVIEW environment,  that the all the sensors are working effectively and the value displayed also is within the desired range. So the system works correctly without any error or warnings. The LED glowing indicates the error or failure operation.                  The results are analyzed for different conditions of sensor. If they are not within the desired upper and lower limits then the LED indicating the specific sensor glows as the alarm to inform that the sensor is failed or an error is occurred to the sensor. 6 CONCLUSION A suitable and realistic sensor fault tolerant control system for realtime implementation on an infant incubator for parameters like temperature and humidity is implemented according to the specifications given in Table1 using the PIC16F877A microcontroller. The system uses the redundancy technique, i.e., if one sensor is failed then the microcontroller considers the value from the other sensor. The FTCS oeration represented by the flowcharts are then translated into equivalent C language and compiled using MPLAB IDE, the PIC16F877A software development tool. MPLAB IDE then translates the C files into corresponding hex files which are uploaded onto the PIC16F877A microcontroller. The microcontroller embedded with the proposed FTCS for real time implementation. Software Simulation using proteus IDE and circuit was implemented with hardware and tested. Test result shows PIC microcontroller is capable of realizing the FTCS operation.By simulation results were obtained for various input conditions manually, by ON and OFF of the switches connected in series andparallel to the sensors. Using LABVIEW, the operation BIST implemented with the switches is explained for how the LED glows and indicates the safe and error and failure operation. The different sensor condition are considered and tested. For each and every condition and the output is determined. By various tests infant incubator with FTCS was working effectively and it is observed that the overall system has a  Fig.5 e FTCS showing one humidity sensor and one temperature senor are failed.    Fig.5f Block diagram of the switching operation using LABVIEW  Fig.5 g Front Panel view of switching operation using LABVIEW.  250International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518   IJSER  2011 httpwww.ijser.org  reliabity of 50.  REFERENCES 1 Chen, J., Patton R.J. 1999. Robust model based fault diagnosis for dynamic systems. Kluwer Academic Publishers.  2 Simon BN, Reddy NP, Anand K.A theoretical model of an infant incubator dynamics J. Biomech Eng 1162636, 1994. 3 Patton, R.J. 1997 Faulttolerant control the 1997 situation. Proceedings of the IFAC   SAFE PROCESS, 2, 10331055.    4 S. S. Yang,   Haider A. F. Mohamed, M. Moghavvemi, P. H. Ng. Realtime Model Based Sensor Fault Tolerant Control System on a Chip. 5 Blanke, M., Staroswiecki, M., Wu, N.E. 2001.Concepts and Methods in Fault   Tolerant Control. Proceedings of the American Control Conferences, 4, 26062620.    251International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518   IJSER  2011 httpwww.ijser.org  Behavior of Eight Bus System with TCIPC V.V.Satyanarayana Rao.R, S.Rama Reddy Abstract Environmental, regulatory and economic constraints have restricted the growth of electric power transmission facilities, and the topologies to enlarge the levels of power transmission and enhance stability through existing transmission lines have become greatly needed. Many approaches have been proposed for solving the stability problems found in power system operations. Considering the diversity of both, the solutions and the problems, it is often difficult to identify the most suitable solution. The main purpose of this paper is to demonstrate the capability of Inter Phase Power Flow IPC Controller as a mean for stability improvement in power systems. In this paper 8 bus system is used as a test bed, the results are shown with and without TCIPC. The results indicate the robustness of this Flexible AC Transmission System FACTS controller to the variation of system operating conditions. Index Terms Controlled Series Compensator CSC, Eight Bus System, Flexiable ac transmission system FACTS, Interphase power controller, Static phase shifting transformer, TCIPC, UPFC        1 INTRODUCTION                                                                     he  basic  operating requirements  of an AC  power system  are  that  the  synchronous  generators  must remain  in  synchronism  and  the  voltages  must  be kept close to their rated values. The capability of a power  system  to  meet  these  requirements  in  the face  of possible disturbance is characterized  by its transient  or  first  swing,  dynamic  or  power oscillation,  and  voltage  stability.  Transient stability may be defined as the ability of an electric power system to remain in synchronism after being subjected to a major system  disturbance such as  a short  circuit.  According  to  equalarea  criteria transient stability  of a  power  system  is maintained if the accelerating area equals the decelerating area during  the  first  rotor  swing  following  the  fault clearance.  To  avoid  stability  problems  a  fast  power  flow control  within  the  first  swing  of  the  generator  is required.  This  can be achieved by different means, such  as  high  performance  excitation  systems  and high  ceiling  voltage,  breaking  resistors  usage,  superconducting  magnetic  energy  storage systems and etc. The  recent  availability  of  solidstate  power switching  devices  with  controlled  turn  off  capability  has  made  possible  further  advances  in power  conversion  and  control,  leading  to  the  development  of  a  new  generation  of  FACTS devices.  FACTS  Flexible  AC  Transmission Systems  devices,  as  discussed in references  ,  are  first  of  all,  effective  tools  for dynamic  power  flow  control.  On the other  hand  power flow is clearly related to a systems transient stability  problems.  As  a  result  FACTS  devices,  such as UPFC Unified Power Flow Controller, SPS Static Phase Shifting Transformers , CSC Controlled Series Compensator , are presented as  an  effective  tool  to  mitigate  transient  stability problems in electric power systems. These  devices are  power  electronic  based  controllers,  which  can influence  transmission  system  voltages,  currents, influence  transmission system  voltages,  currents, impedances  andor  phase  angle  rapidly.  Thus FACTS devices  or  controllers  can  improve  both the security and flexibility of a power system. This paper presents the capability of IPC Interphase Power Controller    as  a  mean  for power  stability  improvement.  Concerning  this  matter,  it  is  necessary  to  replace  the  conventional PST  Phase  Shifting  Transformer  with  the  static PST  SPS.  The  result  of  these  changes  is  a  new FACTS  device,  which  is  referenced  to  it  as Thyristor Controlled IPC or TCIPC.    T  V V Satyanarayana Rao.R received Masters Degree in Power Engineering from JNT University, Hyderabad, India in 2007. He is pursing the Ph.D in Power Systems at SCSVMV University, Kanchipuram, Chennai, India. Presently he is working as Senior Assistant Professor in Sri Sarathi Institute of Engineering  technology, Nuzvid, India. His areas of interest are FACTS, Power Electronics and Deregulated Power Systems. PH09396677506. Email rvvsssietyahoo.com   Dr.S.Rama Reddy received his ME degree from college of Engineering, Anna University Chennai, India in 1987.He received his Ph.D degree in the area of Resonant Converters from College of Engineering, Anna University Chennai in 1995.Presently he is working as Dean in Electrical  Electronics Department , Jerusalem College of Engineering , Chennai. He has publish 30 research papers in reputed Journals. His research areas are power electronic converters, Drives and FACTS. 252International Journal of Scientific  Engineering Research Volume 2, Issue 4, April2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org  2  INTRODUCTION TO IPC One of the problems for interconnected power systems is overrating of circuit breakers and associated substation equipment due to short circuit level. Conventional options to decrease the short circuit levels are splitting existing bus into two or more sections, addition of series reactors in transmission lines and using transformers with high impedance or replacing overduty substation circuit breakers and associated equipments. However, none of the above methods provide additional transmission capability or ability to control and redirect the power flow.  Splitting an existing bus into more than one section decreases the substation fault problem in a relatively costeffective manner, but operating flexibility and reliability will be decreased. In practice, it may be difficult to obtain permission to change the existing bus configuration. Series reactors can neither completely eliminate the fault current contributions nor efficiently reduce the transmission constraints. At normal conditions, series reactors absorb reactive power. Under heavy loading conditions, this solution can make more problems for voltage regulation. Replacing the underrated circuit breakers and associated substation equipments with higher interrupting devices, is another method to overcome the fault duty problem . Depending on voltage levels, the number of circuit breakers involved and desired new rating for the breakers, the replacement of breakers can be expensive. In addition, scheduling large number of circuit breaker replacements imposes planning and engineering challenges.  Some new techniques for fault limitation such as series compensation, flexible alternative current transmission systems FACTS, phase shifting transformer PST or Inter phase Power Controller IPC in an existing substation can be very attractive options. In the present thesis, the role of IPC is discussed .   3 IPC DESCRIPTION The basic design goal in IPC technology is to find passive solutions to fundamental frequency problems. Power electronics modules can be added in situations where rapid control action is required to damp oscillations or prevent excessive voltage variations. Hence, basic IPC solutions utilize only conventional equipment, such as capacitors, inductors and phaseshifting transformers. They generate no harmonics and have no commutation losses. Robustly built, they require much less maintenance than power electronicsbased devices.  The IPC does not have a fixed configuration, being more a technology for creating different and innovative power flow controllers with diverse characteristics and configurations. Generically, it is a seriesconnected device consisting of two parallel branches, each with an impedance in series with a phaseshifting element Figure 1. The four design parameters two impedances and two phase shifts allow enormous design flexibility and make a wide variety of applications possible. Because of the different characteristics these IPC applications can have, they have their own specific names.   Fig 1 The Single diagram of Generic IPC   IPCs can be adapted to specific operating conditions. In general, the adaptation also results in an optimization. For example, the removal of the phase shift in one of the two branches of the IPC reduces the amount of equipment and relocates the control characteristic to a more favorable position in the powerangle plane. The adaptability of the IPC technology is also demonstrated by the various ways in which the internal phase shifts can be implemented. Conventional phaseshifting transformers PST are the first obvious choice, but the IPC characteristics can also be obtained using conventional transformers which have auxiliary windings added to create the desired internal phase shift by injecting series voltages from other phases.  4 THYRISTOR CONTROLLED INTERPHASE POWER CONTROLLER TCIPC In this paper a 8 bus system is implemented with thyristor controlled interphase controller. Based on this model it is demonstrated that the TCIPC can be very effective to damp power systems oscillations. Simulation results indicate the robustness of this Flexi253International Journal of Scientific  Engineering Research Volume 2, Issue 4, April2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org  ble AC transmission system FACTS controller to the variation of system operating too, IPC system is shown in the fifure 2a. Real and reactive powers with 1440 and 1530 are shown in figure 2b  2 c respectively.    Fig2 a Interphase controller with Thyristors     Fig 2b  Real and Reactive Power at Alpha 144 Degree      Fig 2c  Real and Reactive Power at Alpha 153 Degree   It have been shown that real and reactive power could be controlled by Varying alpha in Thyristor controlledInter phase power controller. Eight bus system without TCIPC is shown in figure 3a. Real and reactive powers are shown in figure 3b, current and voltage at bus 3 is shown in figure 3c.    Fig 3a 8 Bus System without TCIPC    Fig 3b Real and reactive power across bus3    Fig 3c Current and volatge across bus3 Eight bus system with TCIPC is shown in figure 4a. Real 254International Journal of Scientific  Engineering Research Volume 2, Issue 4, April2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org  and reactive power at busses  7,1,3  4 are shown in figure 4b to 4e, voltage and current at bus 3 are shown in figure 4f. Reactive power with and without TCIPC are shown in Table1.  Fig 4a Bus System with TCIPC    Fig 4b Real and reactive power across bus7    Fig 4c Real and reactive power across bus1    Fig 4d Real and reactive power across bus3     Fig 4e Real and reactive power across bus4     Fig 4f Current and volatge across bus3    255International Journal of Scientific  Engineering Research Volume 2, Issue 4, April2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org   Table1 Reactive Power with and without TCIPC  BUS NO REACTIVE POWER MVA WITHOUT TCSC REACTIVE POWER MVA WITH  TCSC BUS7 0.174 0.198 BUS1 0.133 0.148 BUS3 0.836 1.090 BUS11 0.663 0.456  6 CONCLUSION This paper presents the simuation resuts of 8 bus system with and without TCIPC, simuation resuts shows that TCIPC can improve the system stabiity and mean while preserved the merits of conventiona IPC, simuation resuts indicate that  TCIPC as a FACTS controller can improve the dynamic performance of the system. Simuation results indicate that TCIPC can increase the real and reactive powers in the line. ACKNOWLEDGEMENT We are thankful to Department of Electrical and Electonics Engineering of SCSVMV University and Jerusalem College of Engineering, Chennai, India with whom we had useful discussions regarding FACTS Performance of transmissions lines. Any suggestions for further improvement of this topic are most welcome. REFERENCES 1 N. G. Hingorani, FACTSflexible A.C. transmission system, in Proc. Inst. Elect. Eng. 5th. Int. Conf. A.C. D.C. Power Transmission, 2 S.Chitra Selvi and R.P.Kumudini Devi  Stability Analysis of Thyristor Controlled Interphase power controller in deregulated power System  Power System Technology and IEEE Power India Conference, 2008. POWERCON 2008.  3 Mohammadi, M. Gharehpetian,  Thyristor controlled inter phase power controller modeling for power system dynamic studied TENCON 2004. 2004 IEEE Region 10 Conference. 4 FACTS OVERVIEW Joint IEEEKIGRE Special Publication No.95 TP 108 Sybille G, Haj Maharsi Y , Morin G , Beeauregard. 5 E.V. Larsen, J.J SanchezGasca and J.H.Chow, Concept for Design of FACTS Controllers to damp overswings. IEEE Transactions on power systems Vol.10 1995 6 R. Mohan Mathur, Rajiv K. Varma Thyristorbased facts controllers for electrical transmission systems. 7 A.J.Power An Overview of Transmission Fault Current Limiters IEEE 1995. 8 H.F.Wang, F.J.Swith, The Capability of Static VAR Compensator in Damping Power System Oscillations IEEE ProcGener. Transm. Distrib.No4,1996. 9 J.Brochu Interphase Power Controller book Polytechnics International Press 1999  256International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518                                                                                IJSER  2011 httpwww.ijser.org  Implementing Anti Collision Algorithm for Multiple Tag Identification Sindhura kodali, Eswaran.P  Abstract RFID is core technology in the area of ubiquitous computing.To identify the objects begins with the reader query to the tag attached to the subject. When multiple tags exist in the readers interrogation zone, these tags simultaneously respond to the readers query, resulting in collision. This kind collision makes the reader to take long time to identify tags within the readers identification range and impossible to identify even one tag.In RFID system, the reader needa the anticollision algorithm which can quickly identify all the tags in interrogation zone. This paper proposes a design methodology with two different RF modules, 433.92MHz and 315 MHz are interfaced into microcontroller, encoder, decoder ICs are used.According to query technique, the reader sends the query signal to all tags with one frequency and the tag corresponding to the query will respond to reader with another frequency, likewise anticollision algorithm is implemented to avoid collision and number of query reponses can be reduced.  Index Terms Anti collision Algorithom, Active tag, multiple tags, query technique RFID, RF modules, Tag collision.        1 INTRODUCTION                                                               FID Radio Frequency Identification is a technology that deciphers or identifies the tag information through a reader or interrogator without contact. RFID have become very popular in many service industries, purchasing and distribution logistics, industry, manufacturing companies and material flow systems. Automatic Identification procedures exist to provide information about people, animals, goods and products in transit 1,2. The reader receives required information from the tags by sending and receiving wireless signals with the tag. Since the communication between the readers and the tags shares wireless channels, there exist collisions. The collisions can be divided into the reader collision and the tag collision. The reader collision occurs when multiple readers send request signals to one tag, and the tag receives the wrong request signal due to signal interference between readers. The tag collision occurs when more than two tags simultaneously respond to one reader and the reader cannot identify any tags. This kind of collision makes the reader take long time to identify tags within the readers identification range and impossible to identify even one tag 3 6. Therefore, the collision is a crucial problem that must be resolved in RFID systems, so many studies to resolve this problem have been carried out as well as are ongoing. This paper focuses on the tag collision problem which occurs in the case where one reader identifies multiple tags.  A design methodology is implemented in reader  and tag module to avoid collision problem and an anticollision algorithm is implemented in RFID reader . 2 RFID SYSTEM FOR MULTIPLE TAG IDENTIFICATION  RFID systems typically use small, lowcost, battery free devices called TAGs, which use the radio signal from a specialised RFID reader for power and communication. When queried, each TAG responds with a unique identification number by reflecting energy back to the reader with a technique called backscatter modulation. Usually TAGs are application specific, fixed function devices that have an operating range of 1050 cm for inductively coupled devices and 310 m for UHF TAGs. Traditionally, RFID TAGs have been used as a replacement for barcodes in applications such as supplychain monitoring, asset management, and building security 8. When multiple tags are present in readers interrogation area,all tags try to access the reader at a time as a result the reader cannot  notify a single tag or take long time for identification as shown in fig1 R  Sindhura kodali is currently pursuing masters degree program in Embedded system Technologyry, in SRM University, Kattankulathur, Chennai, India. PH04427452270. Email sindhurakodali25gmail.com  Eswaran.P is currently working as Assistant professor in SRM University, Kattankulathur Chennai, India. PH04427452270.                  Email eswaranpktr.srmuniv.ac.in 257International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518                                                                                IJSER  2011 httpwww.ijser.org      The tag collision is avoided by implementing an anticollision algorithm in reader89.In RFID an anticollision algorithm is implemented suchthat The Reader continuously queries the tags one by one for the identifications. Whenever the tag is identified then the data will be processed through RF signals from tag to reader. In the reader module the data is decoded and stored in EEPROM of the controller. The controller sends the data to PCfor display purpose as shown in fig 2. In this way multiple tags can be identified by reader query and data can be retrieved.                  The hardware modules are divided into these categories  RFID tag module  RFID Reader module    2.1 Active RFID Tag Module  Active tags are powered by an internal battery and are typically readwrite. The life of an active tag is limited by the life of the battery. The data within an active tag can be rewritten or modified. An active tags memory can vary from a few bytes to 1MB. The batterysupplied power of an active tag generally gives it a longer read range. The trade off is, greater the size the greater cost, and a limited operational life. Depending upon the battery type and temperatures, the life of such tags could be 10 years. Some active tags can also be smart and do not send their information all the time. In a typical readwrite RFID system, a tag might give a machine a set of instructions, and the machine would then report its performance to the tag. This encoded data would then become part of the tagged parts history. This data can be detailed about the port of transit with dates. The RFID transponder is sometimes called the RFID tag or an inlay. The transponder   is usually made of an antenna that is bonded to an integrated circuit IC chip. The IC chip contains the RF circuit, encoders, decoders, and memory as shown in fig 3.     Active transponders are woken up when they receive a signal from a reader. Active tags have a read range of up to 300 feet 100 meters and can be read reliably because they broadcast a signal to the reader . This active RFID tag module consists of a transmitter with a encoder and receiver with decoder to interpret the data, and microcontroller programmed with information. The RF provides a means of communicating with the interrogator. Active tag consists of transmitter, receiver and for data security encoder and decoder modules are used. Whenever the query from the reader comes to tag then the query will stored in the receiver buffer and then it is processed to microcontroller and the relative data operations will be done. Then the ancknowledgement signal is transmitted to reader via transmitter                 2.2 RFID Reader Module  RFID technology explaining how an RFID chip interacts with a reader through different radio wave frequencies. A Reader is also called interrogator, is comprised of a transmitter, receiver, control module and PC connection for data display 6. The query response is sent through the RFtransmitter to tag RFreceiver for tag identification .once the tag is identified, the tag sends encoded data through RFtransmitter. The reader receives data through RFreceiver and data is decoded and checks whether queried tag is identified or not. Data is displayed on PC dis Fig. 2 Block diagram of RFID system for multiple tag identification.   Fig.2. Block diagram of Active RFID tag.     Fig 1 Tag collision problem in RFID system  258International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518                                                                                IJSER  2011 httpwww.ijser.org  play through serial communication interface.The RFID reader modulu shown in the fig 4              3  SOFTWARE IMPLEMENTATION  Tag collision in RFID systems happens when multiple tags reflect the signal back to the reader. This problem is often seen whenever a large volume of tags must be read together in the same RF field. The reader is unable to differentiate these signals from all tag collision confuses the reader.                   To avoid the collision two different frequencies of RF module are   interfaced with microcontroller. With one frequency 433.92MHz RF TX module the reader sends query to all tags. All tags receive signal with 315MHz RFRx module and check for match of data and address of the received signal .Then Corresponding tag transimitter module is enable and sends the acknowledgement signal with another frequency,suchthat the remaining tags dont  receive the acknowledgement signal of  corresponding query tag. In this way the reader sends simultaneous queries to tags and acknowledgement is retrieved from each tag separately as per query as shown in fig 5.  3.1 Algorithm for Multiple Tag Identification Step1 Reader initialization Step2 Sends the query to all the active tags Step3 Decoder of each tag checks for corresponding query  Step4 If match occurs then valid transmission VT1 Step5 If VT1 then transmission enable TE0 for transmitter to send acknowledgement Step6 Then VT bit of reader will be active high and data will be sent  Step7 Goto step2  3.2 FLOW CHART OF RFID SYSTEM The reader sends query signal to all the tags. For example if 0001 is the data related to the tag1 ,the data is encoded and send to all the tags present in the readers interrogation area. All tags will decode the data and checks whether the data and address are correct or not. If match occurs then VT valid transmission of the corresponding tag will be high which makes the TE transmission enable low of the encoder for sending acknowledgement signal to reader. once the acknowledgement signal is received then it send data serially to PC for display purpose and again start querying for next tag and continue the process until all tags are identified as flowchart shown in fig6.                      4 HARDWARE IMPLEMENTATION RFID SYSTEM  4.1 RFID Tag Module  The tag module as shown in fig 7 consist of RF transmitter 433.92MHz and receiver module 315MHz, data security encoder and decoder modules. Tag receives  Fig.4. Block diagram for RFID reader module.   Fig.4. Showing the functionality of RFID system   Fig.6. Flow chart of RFID system.  259International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518                                                                                IJSER  2011 httpwww.ijser.org  query signal from the reader through RF receiver and data is decoded by HT12D decoder. Then the corresponding tag VT transmission will be high in decoder, which makes the RF transmitter enable TE0 . Tag sends the acknowledgement signal to the reader module through RF transmitter.                 4.2 RFID READER MODULE The reader module as shown in fig8 consist of microcontroller,RF transmitter 315MHz,RF receiver 433.92MHz , HT12E encoder and HT12D decoder ICs and serial port for PC serial data transfer. Once the reader is power up, then microcontroller starts querying each tag one by one. The encoded data is sent to all tags through RF transmitter 10 and corresponding tag will sent acknowledgement signal to the reader and data is sent through serial port from controller to the PC for display purpose.                  4.2 PROPOSED RFID SYSTEM The RFID reader is connected to laptop through USB to serialport converter as shown in fig 9.Hyperterminal window is launched in PC by configuring with serial port and setting the properties of port. All the tags are made active by powerup in interrogation area .once the reader is powerup then the microcontroller start sending query signals to all the tags and identified tag data is displayed on the screen.             5 RESULTS AND DISCUSSIONS When the reader is active, then microcontroller runs and in hyper terminal it asks to press any key to scan as show in fig 10. If any key on keyboard is pressed then it start scaning all tags which are present in readers interrogation area and identified tags are displayed on screen. If tags are not present in the readers interrogation area then it again ask to press any key to start scanning for tags.It is as shown in fig 11. The tags which are active in readers interrogation area are identified and displayed on the screen as shown in the following figs 121314.                          Fig. 7. RFID Tag module.   Fig.8. RFID Reader module.  Fig.9. proposed RFID system.   Fig.10. Reader initialization  Fig.11. No tag is identified  260International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518                                                                                IJSER  2011 httpwww.ijser.org                                            7 CONCLUSION In RFID system, the tag collision problem occurs in multitag environment where multiple tags have to be  identified. The anticollision algorithm is necessary to arbitrate the collision and to identify all the tags faster.        When multiple tags exist in the readers interrogation zone, these tags simultaneously respond to the readers query, resulting in collision. To avoid the collision two different frequencies of RF module are interfaced with microcontroller. With RF transmitter module the reader sends query to all tags in interrogation area. Once the corresponding tag is identified, the tag transfers data with another frequency, so that only required tag can be identified. In this way the reader sends simultaneous queries to tags and data is retrieved from each tag. By using this design methodology multiple tags can be identified in efficient mannerand number of query responses from reader are reduced. REFERENCES 1 K. Finkenzeller, RFID Hand Book Fundamentals and Applications in Contactless       smart card and identification, Second Edition, John Wiley  Sons Ltd, 2003.                                                       2 P. H. Cole, Fundamentals in RFID part1, Korean RFID course,2006,Available at  httpautoidlab.eleceng.adelaide.edu.aueducationFundamentalsInRfidPart1.pdf. 3 D. W. Engels and S. E. Sarma, The Reader Collision Problem, In Proceedings of     IEEE International Conference on SMC 02, 2002. 4    S. Sarma, D. Brock, and D. Engels, Radio Frequency Identifica                  tion  and the Electronic Product Code, IEEE Micro, vol. 21, no.         6, pp. 5054, November 2001. 5J   Waldrop, D. W. Engels, and S. E. Sarma, Colorwave An Anticollision Algorithm for the Reader Collision, In proceed ings of IEEE ICC 03, 2003.  6   J. Myung, and W. Lee, Adaptive Binary Splitting A RFID Tag  Collision Arbitration Protocol for Tag Identification, ACM MoNET 06, vol. 11, no.5, pp.711722, 2006.   7  Y. H. Kim, S. S. Kim, S. J. Lee, and K. S. Ahn,  An AntiCollision          Algorithm without Idle Cycle using 4ary Tree in RFID System,            ICUIMC 09, ACM, pp.642646, 2009.  8   Y. T. Kim, S. J. Lee, and K. S. Ahn,  An Efficient AntiCollision            Protocol Using Bit Change Sensing Unit in RFID System, The            14th IEEE International Conference on RTCSA 08, pp. 8188,            2008.295  9    P. H. Cole, Fundamentals in RFID part1, Korean RFID course,2006,           Available at   httpautoidlab.eleceng.adelaide.edu.aueducationFundamentalsInRfidPart1.pdf. 10  Data monitoring system,  httpwww.embed4u.comcat9paged2   Fig.12. Two tags are present in readers interrogation area  Fig.13. Three tags are present in readers interrogation area,   Fig.14. Four tags are present in readers interrogation area.  261International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                   ISSN 22295518 IJSER  2011 httpwww.ijser.org  Face Recognition using Neural Network and Eigenvalues with Distinct Block Processing  Prashant Sharma, Amil Aneja, Amit Kumar, Dr.Shishir Kumar  Abstract Human face recognition has been employed in different commercial and law enforcement applications. It has also been employed for mug shots matching, bankstore security, crowd surveillance, expert identification, witness face reconstruction, electronics mug shots book, and electronic lineup. A face recognition system based on principal component analysis and neural networks has been developed. The system consists of three stages preprocessing, principal component analysis, and recognition. In preprocessing stage, normalization illumination, and head orientation were performed. Principal component analysis is applied to obtain the aspects of face which are important for identification. Eigenvectors and eigenfaces are calculated from the initial face image set with the help of distinct block processing. New faces are projected onto the space expanded by eigenfaces and represented by weighted sum of the eigenfaces. These weights are used to identify the faces. Neural network is used to create the face database and recognize and authenticate the face by using these weights. In this paper, a separate network was developed for each person. The input face has been projected onto the eigenface space first and new descriptor is obtained. The new descriptor is used as input to each persons network, trained earlier. The one with maximum output is selected and reported as the equivalent image.  Index Terms  Eigenface, eigenvector, eigenvalue, Neural network, distinct block processing, face recognition, face space.  I. INTRODUCTION HE face is our primary focus of attention in social intercourse, playing a major role in conveying identity and emotion. Although the ability to infer intelligence or character from facial appearance is suspect, the human ability to recognize faces is remarkable. We can recognize thousands of human faces1,2,3 learned throughout our lifetime and identify familiar faces at a glance even after years of separation. This skill is quite robust, despite large changes in the visual stimulus due to viewing conditions, expression, aging, and distractions such as glasses or change in hairstyle or facial hair. As a consequence, the visual processing of human faces has fascinated philosophers and scientists for centuries.   Computational models of face recognition 5,7,8,9,10,11,     in     particular, are interesting because they can contribute not only to theoretical   insights but also to practical applications. Computers that recognize faces could be applied to a wide variety of problems, including criminal identifications, security systems, image and film processing and human   computer interaction. For example, the ability to model a particular face and distinguish it from a large number of stored face models would make it vastly improve criminal identification. Even the ability to merely detect faces, as opposed to recognizing them can be important. Detecting faces in photographs20, for instance, is an important problem in automating color film development, since the effect of many enhancement and noise reduction techniques depends on the picture content.  Unfortunately, developing a computational model of face recognition is quite difficult, because faces are complex, multidimensional and meaningful visual stimuli. Thus unlike most early visual functions, for which we may construct detailed models28 of retinal or striate activity, face recognition is a very high level task for which computational approaches can currently only suggest broad constraints on the corresponding neural activity.  We, therefore, focused my paper towards implementing a sort of early, preattentive pattern recognition11,42 capability that does not depend on T  Prashant Sharma  is currently pursuing B.Tech degree program in Computer Science Engineering in Jaypee University of Engineering and Technology,Guna, India.  Email Sharmaprashant10yahoo.co.uk  Amil Aneja is currently working with Accenture, former student of Jaypee University of Engineering and Technology, Guna, India.  Amit Kumar is Senior Lecturer Department of Computer Science Engineering, Jaypee University of Engineering and Technology, Guna, India.  Dr.Shishir Kumar HOD of Computer Science Engineering Jaypee University of Engineering and Technology, Guna, India. 262International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                   ISSN 22295518 IJSER  2011 httpwww.ijser.org  having threedimensional information or detailed geometry. Our goal is to develop a computational model of face recognition which would be fast, reasonably simple, and accurate in constrained environments such as an office or a household. In addition, the approach is biologically implementable and in concern with preliminary findings in the physiology and psychology of face recognition. The scheme is based on an information theory approach that decomposes face images into a small set of characteristics feature images called eigenfaces12,23,24,34,36, which may be through of as the principal components of the initial training set of face images. Recognition is performed by projecting a new image into the subspace spanned by the eigenfaces face space and then classifying the face by comparing its position in face space with the positions of known individuals.  II. BACKGROUND AND RELATED WORK  Much of the work in computer recognition9,44 of faces has focused on detecting individual features5,9,43 such as the eyes, nose, mouth, and head outline, and defining a face model by the position, size, and relationships among these features. Such approaches have proven difficult to extend to multiple views and have often been quite fragile, requiring a good initial guess to guide them. Research in human strategies of face recognition, moreover, has shown that individual features and their immediate relationships comprise an insufficient representation to account for the performance of adult human face identification. Nonetheless, this approach to face recognition remains the most popular one in the computer vision literature.   Bledsoe16,17 was the first to attempt semiautomated face recognition with a hybrid humancomputer system that classified faces on the basis of fiducially marks entered on photographs by hand. Parameters for the classification were normalized distances and ratios among points such as eye corners, mouth corners, nose tip, and chin point. Later work at Bell Labs developed a vector of up to 21 features, and recognized faces using standard pattern classification techniques.   Fischler and Elschlager40, attempted to measure similar features automatically. They described a linear embedding algorithm that used local feature template matching and a global measure of fit to find and measure facial features. This template matching approach has been continued and improved by the recent work of Yuille and Cohen. Their strategy is based on deformable templates, which are parameterized models of the face and its features in which the parameter values are determined by interactions with the face image.   Connectionist approaches to face identification seek to capture the configurationally nature of the task. Kohonen and Lehtio30,31 describe an associative network with a simple learning algorithm that can recognize face images and recall a face image from an incomplete or noisy version input to the network. Fleming and Cottrell19    extend these ideas using nonlinear units, training the system by back propagation.    Others have approached automated face recognition by characterizing a face by a set of geometric parameters and performing pattern recognition based on the parameters. Kanades face identification system20 was the first system in which all steps of the recognition process were automated, using a topdown control strategy directed by a generic model of expected feature characteristics. His system calculated a set of facial parameters from a single face image and used a pattern classification technique to match the face from a known set, a purely statistical approach depending primarily on local histogram analysis and absolute grayscale values.   Recent work by Burt26 uses a smart sensing approach based on multiresolution template matching. This approch to fine strategy uses a special purpose computer built to calculate multiresolution pyramid images quickly, and has been demonstrated identifying people in near real time.  III. METHODLOGY EMPLOYED  A. Eigen Face Approach  Much of the previous work on automated face recognition has ignored the issue of just what aspects of the face stimulus are important for face recognition. 263International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                   ISSN 22295518 IJSER  2011 httpwww.ijser.org  This suggests the use of an information theory approach of coding and decoding of face images, emphasizing the significant local and global features. Such features may or may not be directly related to our intuitive notion of face features such as the eyes, nose, lips, and hair.   In the language of information theory, the relevant information in a face image is extracted, encoded as efficiently as possible, and then compared with a database of models encoded similarly. A simple approach for extracting the information contained in an image of a face is to somehow capture the variation in a collection of face images, independent of any judgment of features, and use this information to encode and compare individual face images.   In mathematical terms, the principal components of the distribution of faces, or the eigenvectors of the covariance matrix of the set of face images, treating an image as point or vector in a very high dimensional space is sought. The eigenvectors are ordered, each one accounting for a different amount of the variation among the face images. These eigenvectors can be thought of as a set of features that together characterize the variation between face images. Each image location contributes more or less to each eigenvector, so that it is possible to display these eigenvectors as a sort of ghostly face image which is called an eigenface.   Sample face images, the average face of them, eigenfaces of the face images and the corresponding eigenvalues are shown in Figure 1, 2, 3,  4 respectively. Each eigenface deviates from uniform gray where some facial feature differs among the set of training faces. Eigenfaces can be viewed as a sort of map of the variations between faces.               Fig  1 Sample Faces                Fig. 2 Average face of the Sample face  264International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                   ISSN 22295518 IJSER  2011 httpwww.ijser.org  Each individual face can be represented exactly in terms of a linear combination of the eigenfaces. Each face can also be approximated using only the best eigenfaces, those having the largest eigenvalues, and which therefore account for the most variance within the set of face images. As seen from the Figure 3.4, the eigenvalues drops very quickly, that means one can represent the faces with relatively small number of eigenfaces. The best M eigenfaces span an Mdimensional subspace which we call the face space of all possible images.   Fig. 3  Eigen face of the Sample face   Fig. 4  Eigen values corresponding to Eigenfaces  Kirby and Sirovich3,25 developed a technique for efficiently representing pictures of faces using principal component analysis. Starting with an ensemble of original face images, they calculated a best coordinate system for image compression, where each coordinate is actually an image that they termed as eigen picture. They argued that, at least in principle, any collection of face images can be approximately reconstructed by storing a small collection of weights for each face, and a small set of standard pictures the eigen pictures.   265International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                   ISSN 22295518 IJSER  2011 httpwww.ijser.org  Fig 5  Reconstruction of first image with the number of Eigen Faces   The weights describing each face are found by projecting the face image onto each eigen picture.   Turk and A. Pentland12 argued that, if a multitude of face images can be reconstructed by weighted sum of a small collection of characteristic features or eigen pictures, perhaps an efficient way to learn and recognize faces would be to build up the characteristic features by    experience over time and recognize particular faces by comparing  the  feature  weights  needed  to  approximately  reconstruct them with the weights associated with known individuals. Therefore, each individual is characterized by a small set of feature or eigenpicture weights needed to describe and reconstruct them. This is an extremely compact representation when compared with the images themselves. The projected image of the Face1 with a 266International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                   ISSN 22295518 IJSER  2011 httpwww.ijser.org  number of eigenvalues is shown in Figure 5. As seen from the figure among the 25 faces database 25 eigenfaces, 15 eigenfaces are enough for reconstruct the faces accurately. These feature or eigenpicture weights are called feature vectors. If a multitude of face images can be reconstructed by weighted sums of a small collection of characteristic features or eigenspaces, an efficient way to learn and recognize faces would be to build up the characteristic features by experience over time and recognize particular faces by comparing the feature weights needed to reconstruct them with the weights associated with known individuals. Each individual, therefore, would be characterized by the small set of feature or eigenspace weights needed to describe and reconstruct them  an extremely compact representation when compared with the images themselves.  This approach to face recognition involves the following initialization operations  i Acquire an initial set of face images       trainingset. ii Calculate the eigenfaces from the training set, keeping only the M images that correspond to the highest eigenvalues. These M images define the face space. As new faces are experienced, the eigenvalues can be updated or recalculated.  These operations can also be performed from time to time whenever there is free excess computational capacity. Having initialized the system, the following steps are then used to recognize new face images.  i Calculate a set of weights based on the input image and the M eigenfaces by projecting the input image onto each of the eigenfaces. ii Determine if the image is a face at all by checking to see if the image is sufficiently close to face image. iii If it is a face, classify the weight pattern as either a known person or unknown.  B. Face Space Visited  Calculate the set of all possible images, those representing a face, make up only a small fraction of it. It has been decided to represent images as very long vectors, instead of the usual matrix representation. This makes up the entire image space where each image is a point.  Since faces however possess similar structure eye, nose, mouth, position the vectors representing them will be correlated. We will see that faces will group at a certain location in the image space. We might say that faces lie in a small and separate region from other images. The idea behind eigen images is to find a lower dimensional space in which shorter vectors well describe face images, and only those.   In order to efficiently describe this cluster of images, we have chosen the set of directions in the image space along with the variance of the cluster is maximum. This is achieved through the standard procedure of Principal Component Analysis. A direction defined in terms of the coordinates of its extremity is in the image space actually an image. Transforming coordinates amounts to projection onto new coordinates and expressing an image as a linear combination of base images. The identified directions, thus, are images or more precisely eigenimages, and in this case it will be called them eigenfaces because faces are being decribed.   Let a face image lx,y be a twodimensional N x N array of 8bit intensity values. An image may also be considered as a vector of dimension N2, so that a typical image of size 256X256 becomes a vector of dimension 65,536, or equivalently a point in 65,536dimensional space. An ensemble of images, then, maps to a collection of points in this huge space.   Images of faces, being similar in overall configuration, will not be randomly distributed in this huge image space and thus can be described by a relatively low dimensional subspace. The main idea of the principal component analysis is to find the vectors that best account for the distribution of face images within the entire image space.   These vectors define the subspace of face images, which will be called as face space. Each vector is of length N2 describes an NXN image, and is a linear combination of the original face images. Because these vectors are the eigenvectors of the covariance matrix corresponding to the original face images, and they are facelike in appearance, it will be refer to them as eigenfaces.   Recognizing similar faces, is equivalent to identifying the closest point to the query in the newly defined facespace. If a person is represented in the database more than once, the problem is to decide to which group of images the query is most similar to. Finally, if the input image is not a face at 267International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                   ISSN 22295518 IJSER  2011 httpwww.ijser.org  all, its projection into the face space will give inconsistent results, so  this case will also be identified.   IV. ROLE OF NEURAL NETWORKS  Neural networks10,32,42 are composed of simple elements operating in parallel. The neuron model shown in figure 6  is the one which is widely used in artificial neural networks with some minor modifications on it.  The artificial neuron given in this figure has N input, denoted as u1,u2,uN. Each line connecting these inputs to the neuron is assigned a weight, which is denoted as w1,w2,.,wN respectively. The threshold in artificial neuron is usually represented by  and the activation is given by the formula                                     The inputs and weight are real values. A negative value for a weight indicates an inhibitory connection while a positive value indicating excitatory one. If  is positive, it is usually referred as bias. For its mathematical convenience  sign is used in the activation formula. Sometimes, the threshold is combined for simplicity into the summation part by assuming an imaginary input u0  1 and a connection weight w0  . Hence the activation formula becomes                                    The vector notation                                                    A  wT u     is useful for expressing the activation for a neuron. The neuron output function fa can be represented as   Linear                                      Threshold           Fig. 6 Artificial Neuron                                   Ramp                                        Sigmoid                                      Functional implementations can be perfprmed by adjusting the weights and the threshold of the neuron. Furthermore, by connecting the outputs of some neurons as inputs to the others, neural network will be established, and any function can be implemented by these networks. The last layer of neurons is called the output layer and the layers between the input and output layer are called the hidden layers. The input layer is made up of special input neurons, transmitting only the applied external input to their outputs. In a network, if there is only the layer of input nodes and a single layer of neurons constituting the output layer then they are called single layer network. If there are one or more hidden layers, such networks are called multilayer networks. There are two types of network architecture recurrent and feed forward neural network.   The eigenfaces approach to face recognition can be summarized in the following steps    Form a face library that consists of the face images of known individuals.   268International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                   ISSN 22295518 IJSER  2011 httpwww.ijser.org   Choose a training set that includes a number of images M for each person with some variation in expression and in the lighting.    Calculate the M x M matrix L, find its eigenvectors and eigenvalues, and choose the M eigenvectors with the highest associated eigenvalues.    Combine the normalized training set of images according to produce M eigenfaces. Store these eigenfaces for later use.    For each member in the face library, compute and store a feature vector.    Create Neural Network for each person in the database   Train these networks as the faces will be used as positive examples for their own networks and negative examples for all other networks    For each new face image to be identified, calculate its feature vector    Use these feature vectors as network inputs and simulate all networks with these inputs    Select the network with the maximum output. If the output of the selected network passes a predefined threshold, it will be reported as the host of the input face. Otherwise it will be reported as unknown and adds this member to the face library with its feature vector and network.  V. METHODOLOGY PROPOSED  A. Distinct Block Processing  In this section, a basic yet novel method of face recognition is presented. The technique integrates the concepts of distinct block processing and relative standard deviations. Faces can be encoded in a block wise fashion wherein the standard deviation for each block is encoded.   In the proposed method, each image is segmented into distinct blocks and the standard deviation of each of these is calculated. The difference in standard deviations of two images gives a measure for comparison. The system has been tested using varying facial conditions controlled condition, varying facial expression, varying pose and varying illumination.   Certain image processing operations involve processing an image in sections called blocks, rather than processing the entire image at once. In a distinct block operation, the input image is processed a block at a time. That is, the image is divided into rectangular blocks, and some operation is performed on each block individually to determine the values of the pixels in the corresponding block of the output image.   Distinct Blocks are rectangular partitions that divide a matrix into mbyn sections. Distinct blocks overlay the image matrix starting in the upper left corner, with no overlap. If the blocks dont fit exactly over the image, zero padding is added so that they do as needed.   In order to determine the optimal block size for block processing, a simple algorithm is used. Suppose the syntax of the function that runs the algorithm is given as below  Size  bestblk m.n,k  This function returns for an mbyn image, the optimal block size for block processing, k is a scalar specifying the maximum row and column dimensions for the block if the argument is omitted, it defaults to 100. The return value size is a 1by2 vector containing the row and column dimensions for the block.  The algorithm for determining size is  If m is less than or equal to k, return m.  If m is greater than k, consider all values between minm10,k2 and k  Return that value that minimizes the padding required  The same algorithm is then repeated for n, for example   Size  bestblk640,800,72  B. Proposed Technique  Proposed Face Recognition system consists of a face database that may store multiple face images of the same person. First of all, these images are not actual face images.   The face image before being stored into the database is first resized into a standard size. It has been taken this as 112 X 269International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                   ISSN 22295518 IJSER  2011 httpwww.ijser.org  92. Then the optimal block size is found for this image size it comes out as 8 X 5. Then for every block, its standard deviation is calculated as follows  S     Xi  X2n1 Where Xi is the individual pixel value, X is the mean of all pixel values in that block and n is the number of pixels in that block.  After the standard deviation for the block is calculated, all the pixels in that block are assigned that value. This is done for all the blocks.   Then these values are stored as 112 X 92 matrix in the database. In case an additional image of the same person has to be stored, it is first converted into the above matrix form and its mean will be taken with the matrix stored in the database corresponding to that person. Then this new matrix is overwritten in the database.   Now to compare the input image, it is also processed in the above form. Then this image matrix is compared with every image matrix in the face database. The comparison is carried out as follows  For a block in the test image, choose any pixel value. Take a pixel value from the corresponding block of the image matrix in the database, find their absolute differences. Repeat this for all the blocks.   Sum all the absolute differences obtained. This gives a measure to compare the difference between the two images. Perform this calculation for each image stored in the database. The image for which this difference is the least is the one that matches best to the input test image.   C. Training and Test Image Constraints  The only constraint that this technique has is that for all images of the same person, the size of the face   in the frame and its relative position in the frame must be more or less same.   VI. EXPERIMENTAL RESULT AND ANALYSIS  This method has been tested on 10 pictures from an ORL Database. This database has one face images with different conditions expression, illumination, etc., of each individual. In the following section, detailed information for this database and their corresponding performance results for the proposed face recognition method are given.   The number of networks used for the ORL database are equal to the number of people in the database. The initial parameters of the neural networks used in these tests are given below    Type Feed forward bacpropagation network    Number of layers 3 input, one hidden, output layer      Number of neurons in input layer  Number of      eigen faces to describe the faces      Number of neurons in hidden layer  10     Number of neurons in output layer  1   A. Test Results for the Olivetti and Oracle Research Laboratory ORL Face Database   The Olivetti and Oracle Research Laboratory ORL face database is used in order to test this method in the presence of headpose variations. There are 10 different images of each of 40 distinct subjects.   For some subjects, the images were taken at different times, varying lighting, facial expressions open  closed eyes, smiling  not smiling, facial details glasses  no glasses and head pose tilting and rotation up to 20 degrees. All the images were taken against a dark homogeneous background. Figure 7 shows the whole set of 40 individuals, 10 images per person from the ORL database.   Since the number of networks is equal to the number of people in the database, forty networks, one for each person were created. Within the ten images, first 4 of them are used for tanning the neural networks, then these networks are tested and their properties are updated with the next 3 pictures for getting the minimum squared error function, and these networks will be used for later use for recognition purposes.  For testing the whole database, the faces used in training, testing and recognition are changed and the recognition performance is given for whole database. For this database, 270International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                   ISSN 22295518 IJSER  2011 httpwww.ijser.org  the mean face of the whole database, the calculated top 30 with the highest eigenvalues eigenfaces, and their corresponding eigenvalues are shown in Figure 8, Figure 9 and Figure 10 respectively. The recognition performance increases with the number of faces used to train the neural networks, so the recognition rates, with different number of faces used in training, are calculated and given in Table 1.   The number of eigenfaces that is used to describe the faces, and the number of neurons in hidden layer also affects the recognition rate, so the tests are done with different number of eigenfaces and neurons, and the results are given in Table 2.   Histogram equalization enhances the contrast of images by transforming the values in an intensity image so that the histogram of the output image approximately matches a specified histogram. Table 2 also shows the recognition rates with and without histogram equalization. Up to now, the recognition rates are calculated with neural networks only for first choice largest output, The recognition rates with neural networks other choices are also given in Table 3   B. Computational Time   The images in the database were tested for 10 pictures that were trained with the help of Neural Network and a very small amount of time3 seconds  is required to compute the result. VII. CONCLUSION AND FUTURE WORK  The Eigenfaces algorithm has been successfully implemented which is fast popular and practical, besides familiarizing the user with the basics of image processing and the various other approaches to the face recognition.   The Eigenface approach to face recognition was motivated by information theory leading to the idea of basing face recognition on a small set of image features that best approximate the set of known face images without requiring that they correspond to our intuitive notions of facial parts and features. Although it is not an elegant solution to the general recognition problem, the Eigenface approach does provide a practical solution that is well fitted to the problem of face recognition. It is fast, relatively simple and has been shown to work well in a constrained environment.   It important to note that many applications of face recognition do not require perfect identification although most require a low falsepositive rate. For instance, in searching a large database of faces, it may be preferable to find a small set of likely matches to present to the user.   Subsequently a novel algorithm have been developed which integrates the concepts of Distinct Block Processing and Relative Standard Deviation. The work is still in progress and may be enhanced further.   271International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                   ISSN 22295518 IJSER  2011 httpwww.ijser.org   Figure 7 ORL Face Database    Figure 8 Mean face for ORL Face Database              Figure 9 The eigen values for ORL Face Database  272International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                   ISSN 22295518 IJSER  2011 httpwww.ijser.org     Figure 5.4 The top 30 eigen faces for the ORL Face Database  273International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                   ISSN 22295518 IJSER  2011 httpwww.ijser.org     Number of Images  used in Training  per individual  Number of Images  used in Testing  per individual No of Eigenfaces  Hist.  Equ.  Recognition  Rate   1  9  10    8.8  1  9  10  X  9.7  2  8  20    25  2  8  20  X  26  3  7  25    51.7  3  7  25  X  51.7  4  6  30    75  4  6  30  X  76.2  5  5  50    87.5  5  5  50  X  90  6  4  60    90  6  4  60  X  92.5  7  3  100    89.1  7  3  100  X  91.6  8  2  100    88.8  8  2  100  X  91.2  9  1  100    87.5  9  1  100  X  90                             Table 1 Recognition Rate using different Number of Training                             and Test Images, and wwo Histogram Equalization. Number of                             Neurons in Hidden Layer 15    Number of EigenFaces  Number of Neurons in Hidden Layer  RECOGNITION  Rate   40  5  40  10  58.3  15  72  20  74.5  45  5  41  10  73.5  15  80.8  20  88.5  274International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                   ISSN 22295518 IJSER  2011 httpwww.ijser.org  50  5  54.8  10  73  15  87.5  20  90.8  55  5  50  10  81.8  15  88.5  20  91.5  60  5  48.8  10  81.3  15  89.5  20  92.8  65  5  52  10  81.8  15  91.5  20  94.3  70  5  50  10  86.3  15  92.3  20  93.3                  Table 2 Recognition Rate using different Number of Eigen Faces and Neuron in hidden layer.                               Number of Images used in Training  5                    Number of Images used in Testing  5                    Histogram Equalization  Done for all images   Table 3 Recognition Rate with Neural Networks different choices.                Number of Testing Images 5               Number of Eigenfaces 50               Number of H. L. Neurons 15   REFERENCES 1  Goldstein, A. J., Harmon, L. D., and Lesk, A. B., Identification of human       faces, Proc. IEEE 59, pp. 748760, 1971.  2 Haig, N. K., How faces differ  a new comparative technique, Perception       14, pp. 601615, 1985.  3 Kirby, M., and Sirovich, L., Application of the KarhunenLoeve        procedure for   the characterization of human faces, IEEE PAMI, Vol.        12, pp. 103108,   1990. 4 Terzopoulos, D., and Waters, K., Analysis of facial images using physical        and anatomical models,  Proc. 3rd Int. Conf. on Computer Vision, pp.        727732, 1 F.Galton, Personal identification and description 1,1 Nature,  275International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                   ISSN 22295518 IJSER  2011 httpwww.ijser.org        pp.173177,21  5 Manjunath, B. S., Chellappa, R., and Malsburg, C., A feature based       approach to face recognition, Trans. of IEEE, pp. 373378, 1992.  6 Harmon, L. D., and Hunt, W. F., Automatic recognition of human face        profiles, Computer Graphics and Image Processing, Vol. 6, pp. 135156,        1977.  7 Harmon, L. D., Khan, M. K., Lasch, R., and Ramig, P. F., Machine        identification of human faces, Pattern Recognition, Vol. 132, pp. 97       110,1981.  8 Kaufman, G. J., and Breeding, K. J, The automatic recognition of human       faces from profile  silhouettes, IEEE Trans. Syst. Man Cybern., Vol. 6,       pp. 113120, 1976.  9 Wu, C. J., and Huang, J. S., Human face profile recognition by        computer, Pattern Recognition, Vol.  2334, pp. 255259, 1990.  10 Kerin, M. A., and Stonham, T. J., Face recognition using a digital neural          network with selforganizing capabilities, Proc. 10th Int. Conf. On          Pattern Recognition, pp.738741, 1990.  11 Nakamura, O., Mathur, S., and Minami, T., Identification of human          faces based on isodensity maps, Pattern Recognition, Vol. 243, pp.          263272, 1991.  12 Turk, M., and Pentland, A., Eigenfaces for recognition, Journal of          Cognitive Neuroscience, Vol. 3, pp. 7186, 1991.  13 Yuille, A. L., Cohen, D. S., and Hallinan, P. W., Feature extraction from faces using deformable  templates, Proc. of CVPR, 1989.  14 P.Philips, The FERET database and evaluation procedure for face recognition algorithms,  Image  and Vision Computing, vol.16, no.5, pp.295306, 1998  15  Carey, S., and Diamond, R., From piecemeal to configurational           representation of faces, Science   195, pp. 312313, 1977.  16   Bledsoe, W. W., The model method in facial recognition, Panoramic          Research Inc. Palo Alto, CA,  Rep. PRI15, August 1966.  17 Bledsoe, W. W., Manmachine facial recognition, Panoramic Research           Inc. Palo Alto, CA, Rep. PRI22, August 1966.   18 Yuille, A. L., Cohen, D. S., and Hallinan, P. W., Feature extraction          from faces using deformable templates, Proc. of CVPR, 1989.   19 Fleming, M., and Cottrell, G., Categorization of faces using           unsupervised feature extraction, Proc.  of IJCNN, Vol. 902, 1990.  20 Kanade, T., Picture processing system by computer complex and          recognition of human faces, Dept. of Information Science, Kyoto           University, 1973.   21 Ching, C. W., and Huang, L. C., Human face recognition from a single front view, Int. J. of Pattern Recognition and Artificial Intelligence, Vol. 64, pp. 570593, 1992.   22  E. DEDE, Face Recognition Using Geometric Features and Template Matching By Dimension Reduction, MSc Thesis, METU, September 2003  23  D. Pissarenko 2003. Eigenfacebased facial recognition.  24  P. Belhumeur, J. Hespanha, and D. Kriegman july 1997. Eigenfaces          vs.  Fisherfaces Recognition Using Class Specific Linear Projection. IEEE Transactions on pattern analysis and machine intelligence 19 7. 25  L. Sirovich and M. Kirby 1987. Lowdimensional procedure for the            characterization of human faces. Journal of the Optical Society of           America A 4 519524. 26 Burt, P., Smart sensing within a Pyramid Vision Machine, Proc. of IEEE, Vol. 768, pp. 139153,     1988. 27 Characterization of human faces. IEEE Transactions on Pattern analysis           and Machine Intelligence 12 1 103108.   28  ReinLien Hsu, Face Detection and Modeling for Recognition, PhD  276International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                   ISSN 22295518 IJSER  2011 httpwww.ijser.org           thesis,  Department of Computer Science  Engineering, Michigan State           University, USA, 2002.  29 Henry A.Rowley ,Neural Networkbased face detection PhD thesis, Carnegie Mellon University,  Pittsburgh, USA, May 1999.  30 Kohonen, T., Selforganization and associative memory, Berlin          Springer Verlag, 1989.  31 Kohonen, T., and Lehtio, P., Storage and processing of information in distributed associative  memory systems, 1981. 32 Howard Demuth, Mark Beale, Neural Network Toolbox Users Guide For Use with MATLAB, by The MathWorks, Inc.1998.  33  John Daugman, Face and Gesture Recognition Overview IEEE           PAMI, vol.19, no.7, July 1997.   34  M.H. Yang. N. Ahuja, and D. Kriegman, Face recognition using           Kernel Eigenfaces. Advances in  NIPS,Vol. 14, 2002. 35 Q. Liu and S. Ma. Face Recognition Using Kernel       Based Fisher Discriminant Analysis. IEEE Conf. on Automatic Face and Gesture Recognition, 2002. 36  M.H. Yang. Kernel Eigenfaces vs. Kernel Fisherfaces Face           Recognition Using Kernel Methods IEEE Conf. on Automatic Face and          Gesture Recognition, 2002. 37 S. Gutta, V. Philomin and M. Trajkovic. An Investigation into the use of          PartialFaces for Face Recognition IEEE Conf. on Automatic Face and          Gesture Recognition, 2002. 38 B. Scholkopf, Statistical Learning Kernel Methods. NIPS00, 2000. 39 R. Zhang and A. I. Rudnicky. A large Scale Clustering Scheme for          Kernel KMeans Proc. of ICPR, 40  Fischler, M. A., and Elschlager, R. A., The representation and matching of pictorial structures, IEEE Trans. on Computers, c22.1, 1973.   41 P. J. Phillips, H. Wechsler, J. Huang, and P. Rauss, The FERET database and evaluation procedure  for face recognition algorithms, Image and Vision Computing J, Vol. 16, No. 5. pp 295306, 1998.  42 C..M.Bishop, Neural Networks for Pattern Recognition, Clarendon          Press, 1995.  43 W. Zhao, R. Chellappa, and A. Rosenfeld, Face recognition a literature           survey. ACM Computing Surveys, Vol. 35pp. 399458, December           2003.  44 V. Bruce, P.J.B. Hancock and A.M. Burton, Comparisons between           human and computer recognition of faces, Proceedings Third          IEEE International Conference on Automatic Face and Gesture          Recognition, Vol., Iss., 1416 Pages408413, Apr 1998   277International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                     ISSN 22295518  IJSER  2011 httpwww.ijser.org  Molecular Biocoding of Insulin  Amino Acid Gly Lutvo Kuri  Abstract  The modern science mainly treats the biochemical basis of sequencing in biomacromolecules and processes in medicine and biochemistry. One can ask weather the language of biochemistry is the adequate scientific language to explain the phenomenon in that science. Is there maybe some other language, out of biochemistry, that determines how the biochemical processes will function and what the structure and organization of life systems will be The research results provide some answers to these questions. They reveal to us that the process of sequencing in biomacromolecules is conditioned and determined not only through biochemical, but also through cybernetic and information principles. Many studies have indicated that analysis of protein sequence codes and various sequencebased prediction approaches, such as predicting drugtarget interaction networks He et al., 2010, predicting functions of proteins Hu et al., 2011 Kannan et al., 2008, analysis and prediction of the metabolic stability of proteins Huang et al., 2010, predicting the network of substrateenzymeproduct triads Chen et al., 2010, membrane protein type prediction Cai and Chou, 2006 Cai et al., 2003 Cai et al., 2004, protein structural class prediction Cai et al., 2006 Ding et al., 2007, protein secondary structure prediction Chen et al., 2009 Ding et al., 2009b, enzyme family class prediction Cai et al., 2005 Ding et al., 2009a Wang et al., 2010, identifying cyclin proteins Mohabatkar, 2010, protein subcellular location prediction Chou and Shen, 2010a Chou and Shen, 2010b Kandaswamy et al., 2010 Liu et al., 2010, among many others as summarized in a recent review Chou, 2011, can timely provide very useful information and insights for both basic research and drug design and hence are widely welcome by science community. The present study is attempted to develop a novel sequencebased method for studying insulin in hopes that it may become a useful tool in the relevant areas. Index TermsAmino Acid Gly, Human Insulin, Insulin Model, Insulin Code.         1 INTRODUCTION The biologic role of any given protein in essential life processes, eg, insulin, depends on the positioning of its component amino acids, and is understood by the positioning of letters forming words. Each of these words has its biochemical base. If this base is expressed by corresponding discrete numbers, it can be seen that any given base has its own program, along with its own unique cybernetics and information characteristics.   Indeed, the sequencing of the molecule is determined not only by distin biochemical features, but also by cybernetic and information principles. For this reason, research in this field deals more with the quantitative rather than qualitative characteristcs of genetic information and its biochemical basis. For the purposes of this paper, specific physical and chemical factors have been selected in order to express the genetic information for insulin.Numerical values are them assigned to these factors, enabling them to be measured. In this way it is possible to determine oif a connection really exists between the quantitative ratios in the process of transfer of genetic information and the qualitative appearance of the insulin molecule. To  select these factors, preference is given to classical physical and chemical parameters, including the number of atoms in the relevant amino acids, their analog values, the position in these amino acids in the peptide chain, and their frenquencies.There is a arge numbers of these parameters, and each of their gives important genetic information. Going through this process, it becomes clear that there is a mathematical relationship between quantitative ratios and the qualitative appearance of the biochemical genetic processes and that there is a measurement method that can be used to describe the biochemistry of insulin.  2 METHODS Insulin can be represented by two different forms, ie, a discrete form and a sequential form. In the discrete form, a molecule of insulin is represented by a set of discrete codes or a multiple dimension vector. In the sequential form, an insulin molecule is represent by a series of amino acids according to the order of their position in the chains 1AI0.  Therefore, the sequential form can naturally reflect all the information about the sequence order and lenght of an insulin molecule. The key issue is whether we can develop a different discrete method of representing an insulin molecule that will allow accomodation of partial, 278International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                     ISSN 22295518  IJSER  2011 httpwww.ijser.org  if not all sequence order information Because a protein sequence is usually represented by a series of amino acids should be assigned to these codes in order to optimally convert the sequence order information into a series of numbers for the discrete form representation 3 Expression of Insulin Code Matrix 1AI0  The matrix mechanism of Insulin, the evolution of biomacromolecules and, especially, the biochemical evolution of Insulin language, have been analyzed by the application of cybernetic methods, information theory and system theory, respectively. The primary structure of a molecule of Insulin  is the exact specification of its atomic composition and the chemical bonds connecting those atoms.  3.1 Model  The structure 1AI0 has in total 12 chains A,B,C,D,E,F,G,H,I,J,K,L.   1AI0A G I V E Q C C T S I C S L Y Q L E N Y C N 10 22 19 19 20 14 14 17 14 22 14 14 22 24 20 22 19 17 24 14 17 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  1AI0B F V N Q H I C G S H L V E A L 23 19 17 20 20 22 14 10 14 20 22 19 19 13 22 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36  Y L V C G E R G F I Y T P K T 24 22 19 14 10 19 26 10 23 22 24 17 17 24 17 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 etc.  Fig. 1. Group of chains A,B,C,D,E,F,G,H,I,J,K,L.  Notes Aforementioned aminoacids are positioned from number 1 to 306. Numbers 1, 2, 3, n... present the position of a certain aminoacid. This positioning is of the key importance for understanding of programmatic, cybernetic and information principles in this protein. The scientific key for interpretation of bio chemical processes is the same for insulin and as well as for the other proteins and other sequences in biochemistry.   The first aminoacid in this example has 10 atoms, the second one 22, the third one 19, etc. They have exactly these numbers of atoms because there are many codes in the insulin molecule, analog codes, and other voded features. In fact, there is a cybernetic algorithm which it is recorded that the firs amino acid has to have 10 atoms, the second one 22, the third one 19, etc.  The first amino acid has its own biochemistry, as does the second and the third, etc. The obvious conclusion is that there is a concrete relationship between quantitative ratios in the process of transfer of genetic information and qualitative appearance, ie, the characteristcs of the organism.  3.2 Algorithm  We shall now give some mathematical evidences that will prove that in the biochemistry of hemoglob in there really is programmatic and cybernetic algorithm in which it is recorded, 279International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                     ISSN 22295518  IJSER  2011 httpwww.ijser.org  in the language of mathematics, how the molecule will be built and what will be the quantitative characteristics of the given genetic information.   3.2.1 Atomic progression  Step 1 Amino acids from 1 to 306  AC1  10 atoms AC2  22 atoms AC3  19 atoms... AC306  17 atoms  AC1  AC1 AC2  AC1 AC2 AC3...,  AC1 AC2 AC3...,  AC147  S1  AC1  APa1  10 AC1 AC2  1022  APa2  32  AC1 AC2 AC3  102219  APa3  51 AC1 AC2 AC3...,  AC306  APa306  5640 atoms APa1,2,3,n  Atomic progression of amino acids 1,2,3,n  APa1APa2APa3...,  APa306  103251,  5640  S1  S1  863 208  Example   Atomic progression 1 APa G I V E Q C . . P K T Sum 10 22 19 19 20 14 . . 17 24 17 5640 1 2 3 4 5 6 . . 304 305 306 46971  G I V E Q C . . P K T Sum 10 32 51 70 90 104 . . 5599 5623 5640 863 208 1 2 3 4 5 6 . . 304 305 306 46971  010  10 102232 101119  51 etc.  Fig. 2. Atomic progression 1 APa of amino acids from 1 to 306.  Notes By using chemicalinformation procedures, we calculated the arithmetic progression for the information content of aforementioned aminoacids.   Step 2 Amino acids from 306 to 1  AC306  17 atoms AC305  24 atoms AC304  17 atoms... AC1  10 atoms  AC306  AC306 AC305  AC306 AC305 AC304...,  AC306AC305AC304..., AC1  S2   AC306  APb306  17 AC306 AC305  1724  APb306  41 280International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                     ISSN 22295518  IJSER  2011 httpwww.ijser.org  AC306 AC305 AC304  172417  APb304  58 AC306 AC305 AC304...,  AC1  APb1  5640 atoms APb306,305,304, ,1  Atomic progression of amino acids 306,305,304,1  APb306APb305APb1304...,  APb1  174158,  5640  868 272  S2  868 272   Example  Atomic progression 2 APb  G I V . . I Y T P K T Sum 10 22 19 . . 22 24 17 17 24 17 5640 1 2 3 . . 301 302 303 304 305 306 46971  Sum G I V . . I Y T P K T 868 272 5640 5630 5608 . . 121 99 75 58 41 17 46971 1 2 3 . . 301 302 303 304 305 306 017  17 172441 17241758 etc.  Fig. 3. Schematic representation of the atomic progression 2 from 306 to 1.  Within the digital pictures in biochemistry, the physical and chemical parameters are in a strict compliance with programmatic, cybernetic and information principles. Each bar in the protein chain attracts only the corresponding aminoacid, and only the relevant aminoacid can be positioned at certain place in the chain. Each peptide chain can have the exact number of aminoacids necessary to meet the strictly determined mathematical conditioning. It can have as many atoms as necessary to meet the mathematical balance of the biochemical phenomenon at certain mathematical level, etc. The digital language of biochemistry has a countless number of codes and analogue codes, as well as other information content. These pictures enable us to realize the very essence of functioning of biochemical processes. There are some examples                  Table 1.     Atomic progression APa and APb Amino acid Gly  position from                                             1 to 306 AA  The structure 1AI0  Amino acid Gly  G G G G G G G G Number of atoms 10 10 10 10 10 10 10 10 Rank 1 29 41 44 52 80 92 95 APa 10 523 741 796 950 1463 1681 1736 APb 5640 5127 4909 4854 4700 4187 3969 3914 APa,b 5650 5650 5650 5650 5650 5650 5650 5650   G G G G G G G G Number of atoms 10 10 10 10 10 10 10 10 Rank 103 131 143 146 154 182 194 197 APa 1890 2403 2621 2676 2830 3343 3561 3616 APb 3760 3247 3029 2974 2820 2307 2089 2034 APa,b 5650 5650 5650 5650 5650 5650 5650 5650 281International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                     ISSN 22295518  IJSER  2011 httpwww.ijser.org    G G G G G G G G Number of atoms 10 10 10 10 10 10 10 10 Rank 205 233 245 248 256 284 296 299 APa 3770 4283 4501 4556 4710 5223 5441 5496 APb 1880 1367 1149 1094 940 427 209 154 APa,b 5650 5650 5650 5650 5650 5650 5650 5650  Table 1. Schematic representation of the atomic progression APa and APb Amino acid Gly                   position from 1 to 306 AA.   Notes Namely, having mathematically analyzed the atomic preogression model of Insulin Model Table 1 we have found out that the protein code is based on a periodic law. This being the only to read the picture, the solution of the main problem concering an arrangement where each amino acid takes only one, precisely determined position in the code, is quite manifest  Atomic progression model of insulin should, in fact, be remodelled into a periodic system. Examples   Atomic progression APa and APb  G G G G G G G G              Rank 299 1 29 296 41 284 44 256  1250            APa 5496 10 523 5441 741 5223 796 4710  22940            R 144 144 314 314 314 314 144 144  680            APb 5640 154 209 5127 427 4909 940 4854  22260             G G G G G G G G   Rank 1 299 296 29 284 41 256 44  1250 R  54965640  144 10154  144 523209  314 etc.   G G G G G G G G              Rank 52 248 80 245 92 233 95 205  1250            APa 950 4556 1463 4501 1681 4283 1736 3770  22940            R 144 144 314 314 314 314 144 144  680            APb 1094 4700 1149 4187 1367 3969 1880 3914  22260             G G G G G G G G   Rank 248 52 245 80 233 92 205 95  1250    G G G G G G G G              Rank 103 197 131 194 143 182 146 154  1250 282International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                     ISSN 22295518  IJSER  2011 httpwww.ijser.org             APa 1890 3616 2403 3561 2621 3343 2676 2830  22940            R 144 144 314 314 314 314 144 144  680            APb 2034 3760 2089 3247 2307 3029 2820 2974  22260             G G G G G G G G   Rank 197 103 194 131 182 143 154 146  1250    G G G G G G G G G G G G          10 10 10 10 Rank 299 1 44 256 52 248 95 205 103 197 146 154             APa 5496 10 796 4710 950 4556 1736 3770 1890 3616 2676 2830              144 144 144 144 144 144 144 144 144 144 144 144             APb 5640 154 940 4854 1094 4700 1880 3914 2034 3760 2820 2974              G G G G G G G G G G G G Rank 10 10 10 10 10 10 10 10 10 10 10 10  1 299 256 44 248 52 205 95 197 103 154 146   G G G G G G G G G G G G          10 10 10 10 Rank 29 296 41 284 80 245 92 233 131 194 143 182             APa 523 5441 741 5223 1463 4501 1681 4283 2403 3561 2621 3343              314 314 314 314 314 314 314 314 314 314 314 314             APb 209 5127 427 4909 1149 4187 1367 3969 2089 3247 2307 3029              G G G G G G G G G G G G  10 10 10 10 10 10 10 10 10 10 10 10 Rank 296 29 284 41 245 80 233 92 194 131 182 143  Fig. 4. Atomic progression APa and APb Amino acid Gly  position from 1 to 306 AA.   In this example, the amino acids Gly atomic progression APa and APb as a result was given the codes 144 and 314th.  As we see, the insulin code is itself a unique structure of program, cybernetic and informational system and law.  The research we carried out have shown that atomic  progression are one of quantitative characteristics in biochemistry. Atomic progression is, actually, a discrete code that protects and guards genetic information 283International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                     ISSN 22295518  IJSER  2011 httpwww.ijser.org  coded in biochemical processes. This a recently discovered code, and more detailed knowledge on it is yet to be discovered.  In a similar way we shall calculate bio codes of other unions of amino acids. Once we do this, we will find out that all these unions of amino acids are connected by various bio codes, analogue codes as well as other quantitative features. Examples   Atomic progression APa and APb  G G G G G G G G          APa 5496 10 523 5441 741 5223 796 4710 APa 950 4556 1463 4501 1681 4283 1736 3770 APa 1890 3616 2403 3561 2621 3343 2676 2830 APb 5640 154 209 5127 427 4909 940 4854 APb 1094 4700 1149 4187 1367 3969 1880 3914 APb 2034 3760 2089 3247 2307 3029 2820 2974            67800     67800    G G G G  Sum        APa 5496 10 523 5441  11470 APa 950 4556 1463 4501  11470 APa 1890 3616 2403 3561  11470   G G G G  Sum        APa 741 5223 796 4710  11470 APa 1681 4283 1736 3770  11470 APa 2621 3343 2676 2830  11470   G G G G  Sum        APb 5640 154 209 5127  11130 APb 1094 4700 1149 4187  11130 APb 2034 3760 2089 3247  11130   G G G G  Sum        APb 427 4909 940 4854  11130 APb 1367 3969 1880 3914  11130 APb 2307 3029 2820 2974  11130 Fig. 5. Atomic progression APa and APb Amino acid Gly  position from 1 to 306 AA.   Atomic progression presented in figure 2 are calculated using the relationship between corresponding groups of those rogressions. These are groups with different progression. There are different ways and methods of selecting these groups of progressions, which method is most efficient some We hope that science will determine which method is most efficient for this selection.  284International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                     ISSN 22295518  IJSER  2011 httpwww.ijser.org  3.2.2 Rank of atomic progression   G G G G G G G G Sum           Rank 299 1 29 296 41 284 44 256 1250 Rank 1 299 296 29 284 41 256 44 1250 Rank 52 248 80 245 92 233 95 205 1250 Rank 248 52 245 80 233 92 205 95 1250 Rank 103 197 131 194 143 182 146 154 1250 Rank 197 103 194 131 182 143 154 146 1250 Sum 900 900 975 975 975 975 900 900    G G G G          Rank 299 1 29 296  625 Rank 1 299 296 29  625 Rank 52 248 80 245  625 Rank 248 52 245 80  625 Rank 103 197 131 194  625 Rank 197 103 194 131  625   G G G G  Sum        Rank 41 284 44 256  625 Rank 284 41 256 44  625 Rank 92 233 95 205  625 Rank 233 92 205 95  625 Rank 143 182 146 154  625 Rank 182 143 154 146  625   G G G G  Sum        Rank 299 1 44 256  600 Rank 1 299 256 44  600 Rank 52 248 95 205  600 Rank 248 52 205 95  600 Rank 103 197 146 154  600 Rank 197 103 154 146  600 Sum 900 900 900 900     G G G G  Sum        Rank 29 296 41 284  650 Rank 296 29 284 41  650 Rank 80 245 92 233  650 Rank 245 80 233 92  650 Rank 131 194 143 182  650 Rank 194 131 182 143  650 Sum 975 975 975 975   Fig. 6. Rank atomic progression APa and APb Amino acid Gly  position from 1 to 306               AA.  In those examples, we have the mathematical balance in rows and columns in this figure. 285International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                     ISSN 22295518  IJSER  2011 httpwww.ijser.org  In fact, we have discovered  the mathematical balance in distribution of sequences in figure 6 is achieved.  3.2.3 Correlation of atomic progression  Atomic progression of this amino acid to us as a result of its correlation give a variety of codes. Here are some examples   G G    G G    G G    10 10    10 10    10 10    41 284  325  80 245  325  143 182  325 APa 741 5223  5964  1463 4501  5964  2621 3343  5964 APb 4909 427  5336  4187 1149  5336  3029 2307  5336                R 314 314  11300  314 314  11300  314 314  11300   G G    G G    G G    10 10    10 10    10 10    44 256  300  52 248  300  146 154  300 APa 796 4710  5506  950 4556  5506  2676 2830  5506 APb 4854 940  5794  4700 1094  5794  2974 2820  5794                R 314 314  11300  314 314  11300  314 314  11300 Fig. 7. Correlation of atomic progression and rank of APa and APb Amino acid Gly  position from 1 to 306 AA.  In those examples we have the correlation of atomic progression and rank of APa and APb.    3.2.4 Odd and even progression  Progression of the APa and APb, in fact, odd and even numbers. These numbers are one of the keys to decoding and decoding molecules insulina. This decoding we can make this  Steam progression  G G G G G G G G G G G G  10 10 10 10 10 10 10 10 10 10 10 10  1 44 52 95 103 146 154 197 205 248 256 299 APa 10 796 950 1736 1890 2676 2830 3616 3770 4556 4710 5496 APb 5640 4854 4700 3914 3760 2974 2820 2034 1880 1094 940 154  5650 5650 5650 5650 5650 5650 5650 5650 5650 5650 5650 5650  etc.  Steam progression are  APa  10, 5640 1736, 3914 1890, 3760 etc.  Progression are the odd and even rank.  Odd rank  G G G G G G    10 10 10 10 10 10   Rank 1 95 103 197 205 299  900 286International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                     ISSN 22295518  IJSER  2011 httpwww.ijser.org  APa 10 1736 1890 3616 3770 5496  16518 APb 5640 3914 3760 2034 1880 154  17382  5650 5650 5650 5650 5650 5650    Even rank  G G G G G G    10 10 10 10 10 10   Rank 44 52 146 154 248 256  900 APa 796 950 2676 2830 4556 4710  16518 APb 4854 4700 2974 2820 1094 940  17382  5650 5650 5650 5650 5650 5650    Odd rank  Even rank  900 Odd APa  Even APa  16518 Odd APb  Even APb  17382  Fig. 8. Odd and even progression of aminoacid Gly.  Notes Within the digital pictures in biochemistry, the physical and chemical parameters are in a strict compliance with programmatic, cybernetic and information principles. Each bar in the protein chain attracts only the corresponding aminoacid, and only the relevant aminoacid can be positioned at certain place in the chain. Each peptide chain can have the exact number of aminoacids necessary to meet the strictly determined mathematical conditioning. It can have as many atoms as necessary to meet the mathematical balance of the biochemical phenomenon at certain mathematical level, etc. The digital language of biochemistry has a countless number of codes and analogue codes, as well as other information content. These pictures enable us to realize the very essence of functioning of biochemical processes.   Odd rank progression  G G G G G G            Rank 1 299 95 205 103 197  900          APa 10 5496 1736 3770 1890 3616  16518           144 144 144 144 144 144            APb 154 5640 1880 3914 2034 3760  17382           G G G G G G   Rank 299 1 205 95 197 103  900                                 34836     110144154299 103  34836   Even rank progression  G G G G G G            Rank 44 256 52 248 146 154  900 287International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                     ISSN 22295518  IJSER  2011 httpwww.ijser.org           APa 796 4710 950 4556 2676 2830  16518           144 144 144 144 144 144            APb 940 4854 1094 4700 2820 2974  17382           G G G G G G   Rank 256 44 248 52 154 146  900                                 34836     44796 144940256 146  34836                            Fig. 9. Odd and even rank progression of aminoacid Gly.     Odd APa progression   G G G G G G G G G G G G  10 10 10 10 10 10 10 10 10 10 10 10  29 41 80 92 131 143 182 194 233 245 284 296 APa 523 741 1463 1681 2403 2621 3343 3561 4283 4501 5223 5441 APb 5127 4909 4187 3969 3247 3029 2307 2089 1367 1149 427 209  5650 5650 5650 5650 5650 5650 5650 5650 5650 5650 5650 5650   Odd APa progression  Even rank  G G G G G G        Rank 80 296 92 284 182 194        APa 1463 5441 1681 5223 3343 3561         1254 1254 1254 1254 1254 1254        APb 209 4187 427 3969 2089 2307         G G G G G G Rank 296 80 284 92 194 182  Odd APa progression  Odd rank   G G G G G G        Rank 29 245 41 233 131 143        288International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                     ISSN 22295518  IJSER  2011 httpwww.ijser.org  APa 523 4501 741 4283 2403 2621         626 626 626 626 626 626        APb 1149 5127 1367 4909 3029 3247         G G G G G G Rank 245 29 233 41 143 131   Even rank and odd rank  G G G G G G        Rank par 80 296 92 284 182 194 Rank nepar 29 245 41 233 131 143  51 51 51 51 51 51        Rank par 296 80 284 92 194 182 Rank nepar 245 29 233 41 143 131  51 51 51 51 51 51 8029  51 296245  51 9241  51   Even APa and odd rank  G G G G G G        APa rank even 1463 5441 1681 5223 3343 3561 APa rank odd 523 4501 741 4283 2403 2621  940 940 940 940 940 940        APb odd 1149 5127 1367 4909 3029 3247 APb even 209 4187 427 3969 2089 2307  940 940 940 940 940 940 8029  51 296245  51 9241  51  Figure 10. Odd APa progression, odd APa progressioneven rank, odd APa progression odd rank, even rank and odd rank and even APa and odd rank   3.3 Bio frequency  289International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                     ISSN 22295518  IJSER  2011 httpwww.ijser.org  Insulin is composed of aminoacids with various numerical values. This numerical values are in an irregular order. For example, the first one has 10 atoms, the second one 22. Their frequency is X. Second amino acid has 22 atoms, and the third one 19. Their frequency is Y etc... Frequency is the measurement for establishment of intervals of numerical values of amino acids in proteins. This value can be positive, negative or a zero value. These frequencies are showing us one completely new dimension of protein sequencing. Through these frequencies we can establish which of aminoacids are of primary, and which are of secondary significance in biochemical processes of insulin. Here is a concrete example    G I V E Q C C T  10 22 19 19 20 14 14 17  1 2 3 4 5 6 7 8 10 12 3 0 1 6 0 3 3 From 0 to 10  10 From 10 to 22  12 From 22 to 19   3 From 19 to 19  0 etc  Schematic representation of the amino acid and frequency we will show in the fig.11.  G G G G G G G G G G G G   10 10 10 10 10 10 10 10 10 10 10 10   1 29 41 44 52 80 92 95 103 131 143 146   12 4 9 13 12 4 9 13 12 4 9 13  114  G G G G G G G G G G G G   10 10 10 10 10 10 10 10 10 10 10 10   154 182 194 197 205 233 245 248 256 284 296 299   12 4 9 13 12 4 9 13 12 4 9 13  114    G G G G G G G G              Rank 1 299 29 296 41 284 44 256  1250            Fr 12 13 4 9 9 4 13 12  76             1 1 5 5 5 5 1 1  0            Fr 13 12 9 4 4 9 12 13  76             G G G G G G G G   Rank 299 1 296 29 284 41 256 44  1250    G G G G G G G G              Rank 52 248 80 245 92 233 95 205  1250            APa 12 13 4 9 9 4 13 12  76             1 1 5 5 5 5 1 1  0            APb 13 12 9 4 4 9 12 13  76 290International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                     ISSN 22295518  IJSER  2011 httpwww.ijser.org              G G G G G G G G   Rank 248 52 245 80 233 92 205 95  1250    G G G G G G G G              Rank 103 197 131 194 143 182 146 154  1250            APa 12 13 4 9 9 4 13 12  76             1 1 5 5 5 5 1 1  0            APb 13 12 9 4 4 9 12 13  76             G G G G G G G G   Rank 197 103 194 131 182 143 154 146  1250 Figure 11. Schematic representation of the amino acid Gly and their frequency      Odd rank and frequency  G G G G G G G G G G G G   10 10 10 10 10 10 10 10 10 10 10 10   1 29 41 95 103 131 143 197 205 233 245 299   12 4 9 13 12 4 9 13 12 4 9 13  114  Even rank and frequency G G G G G G G G G G G G   10 10 10 10 10 10 10 10 10 10 10 10   44 52 80 92 146 154 182 194 248 256 284 296   13 12 4 9 13 12 4 9 13 12 4 9  114 Figure 12. Schematic representation of the amino acid Gly and their  oddeven rank and                      frequency    Therefore, there is a mathematical balance between the group of aminoacids with positive frequency and those of negative frequency. Aminoacids with a positive frequency have a primary role in the mathematical picture of that protein, and the negative frequencies have a secondary role in it. We assume that aminoacids with a positive frequency have a primary role in the biochemical picture of that protein, and the negative frequencies have a secondary role in it. If this really is the case and research on an experimental level proves it, a radically new way of learning about biochemical processes will be opened.  3.4 Analog bio code  Each numerical value has its analogue expression. For example The analogue expression for number 19 is 91.  91  19  291International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                     ISSN 22295518  IJSER  2011 httpwww.ijser.org  In a similar way we can calculate the analogue expression for any numerical value. Our research has shown that analog codes are quantitative characteristics in biochemistry. Analogue biocode is a discrete code that protects and guards genetic information coded in biochemical processes.   This a recently discovered code, and more detailed knowledge about it is necessary.  Odd rank and analog frequency    G G G G G G G G G G G G     10 10 10 10 10 10 10 10 10 10 10 10     1 29 41 95 103 131 143 197 205 233 245 299   Frequencu  12 04 09 13 12 04 09 13 12 04 09 13  114                 Analog frequency  21 40 90 31 21 40 90 31 21 40 90 31  546  Even rank and analog frequency   G G G G G G G G G G G G     10 10 10 10 10 10 10 10 10 10 10 10     44 52 80 92 146 154 182 194 248 256 284 296   Frequency  13 12 04 09 13 12 04 09 13 12 04 09  114                 Analog Frequency  31 21 40 90 31 21 40 90 31 21 40 90  546 Figure 13. Schematic representation of the amino acid Gly and their  oddeven rank and                     analog frequency   Analogue code is , actually, a discrete code that protects and guards genetic information coded in biochemical processes.   In the previous examples we translated the physical and chemical parameters from the language of biochemistry into the digital language of programmatic, cybernetic and information principles. This we did by using the adequate mathematical algorithms. By using chemicalinformation procedures, we calculated the numerical value for the information content of molecules. What we got this way is the digital picture of the phenomenon of biochemistry. These digital pictures reveal to us a whole new dimension of this science. They reveal to us that the biochemical process is strictly conditioned and determined by programmatic, cybernetic and information principles.   From the previous examples we can see that this protein really has its quantitative characteristics. It can be concluded that there is a connection between quantitative characteristics in the process of transfer of genetic information and the qualitative appearance of given genetic processes.  4 DISCUSSION  The results of our research show that the processes of sequencing the molecules are conditioned and arranged not only with chemical and biochemical lawfulness, but also with program, cybernetic and informational lawfulness too. At the first stage of our research we replaced nucleotides from the Amino Acid Code Matrix with numbers of the atoms and atomic numbers in those nucleotides. Translation of the biochemical language of these amino acids into a digital language may be very useful for developing new methods of predicting protein subcellular localization, membrane protein type, protein structure secondary prediction or any other protein attributes.  292International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                     ISSN 22295518  IJSER  2011 httpwww.ijser.org  The success of human genome project has generated deluge of sequence information. The explosion of biological data has challenged scientists to accelerate the speed for their analysis. Nowadays, protein sequences are generally stored in the computer database system in the form of long character strings. It would act like a snails pace for human beings to read these sequences with the naked eyes Xiao and Chou, 2007. Also, it is very hard to extract any key features by directly reading these long character strings. However, if they can be converted to some signal process, many important features can be automatically manifested and easily studied by means of the existing tools of information theory Xiao and Chou, 2007. The novel approach as presented here may help improve this kind of situation.  5 CONCLUSIONS AND PERSPECTIVES The process of sequencing in biomacromolecules is conditioned and determined not only through biochemical, but also through cybernetic and information principles. The digital pictures of biochemistry provide us with cybernetic and information interpretation of the scientific facts. Now we have the exact scientific proofs that there is a genetic language that can be described by the theory of systems and cybernetics, and which functions in accordance with certain principles.   BIBLIOGRAPHY   1  Cai, Y.D., and Chou, K.C., 2006. Predicting membrane protein type by functional          Domain composition and pseudo amino acid composition. J Theor Biol 238, 395400.  2 Cai, Y.D., Zhou, G.P., and Chou, K.C., 2003. Support vector machines for predicting               membrane protein types by using functional domain composition. Biophys J 84, 3257          3263.  3 Cai, Y.D., Zhou, G.P., and Chou, K.C., 2005. Predicting enzyme family classes by          hybridizing gene product composition and pseudoamino acid composition. J Theor Biol          234, 145149.  4 Cai, Y.D., Feng, K.Y., Lu, W.C., and Chou, K.C., 2006. Using LogitBoost classifier to         predict protein structural classes. J Theor Biol 238, 172176.   5  Cai, Y.D., PongWong, R., Feng, K., Jen, J.C.H., and Chou, K.C., 2004. Application of           SVM to predict membrane protein types. J Theor Biol 226, 373376.  6  Chen, C., Chen, L., Zou, X., and Cai, P., 2009. Prediction of protein secondary structure           content by using the concept of Chous pseudo amino acid composition and support           vector machine. Protein  Peptide Letters 16, 2731.  7  Chen, L., Feng, K.Y., Cai, Y.D., Chou, K.C., and Li, H.P., 2010. Predicting the network           of substrateenzymeproduct triads by combining compound similarity and functional           domain composition. BMC Bioinformatics 11, 293.  8  Chou, K.C., 2011. Some remarks on protein attribute prediction and pseudo amino acid           composition 50th Anniversary Year Review. J Theor Biol 273, 236247.    9  Chou, K.C., and Shen, H.B., 2010a. CellPLoc 2.0 An improved package of webservers           for predicting subcellular localization of proteins in various organisms. Natural Science           2, 10901103 openly accessible at httpwww.scirp.orgjournalNS. 10 Chou, K.C., and Shen, H.B., 2010b. PlantmPLoc A TopDown Strategy to Augment the           Power for Predicting Plant Protein Subcellular Localization. PLoS ONE 5, e11335. 11 Ding, H., Luo, L., and Lin, H., 2009a. Prediction of cell wall lytic enzymes using Chous           amphiphilic pseudo amino acid composition. Protein  Peptide Letters 16, 351355. 12 Ding, Y.S., Zhang, T.L., and Chou, K.C., 2007. Prediction of protein structure classes           with pseudo amino acid composition and fuzzy support vector machine network. Protein             Peptide Letters 14, 811815. 13 Ding, Y.S., Zhang, T.L., Gu, Q., Zhao, P.Y., and Chou, K.C., 2009b. Using maximum           entropy model to predict protein secondary structure with single sequence. Protein    293International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                     ISSN 22295518  IJSER  2011 httpwww.ijser.org          Peptide Letters 16, 552560. 14 He, Z.S., Zhang, J., Shi, X.H., Hu, L.L., Kong, X.G., Cai, Y.D., and Chou, K.C., 2010.          Predicting drugtarget interaction networks based on functional groups and biological          features. PLoS ONE 5, e9603. 15 Hu, L., Huang, T., Shi, X., Lu, W.C., Cai, Y.D., and Chou, K.C., 2011. Predicting         functions of proteins in mouse based on weighted proteinprotein interaction network and          protein hybrid properties PLoS ONE 6, e14556. 16 Huang, T., Shi, X.H., Wang, P., He, Z., Feng, K.Y., Hu, L., Kong, X., Li, Y.X., Cai,          Y.D., and Chou, K.C., 2010. Analysis and prediction of the metabolic stability of proteins           based on their sequential features, subcellular locations and interaction networks PLoS          ONE 5, e10972. 17 Kandaswamy, K.K., Pugalenthi, G., Moller, S., Hartmann, E., Kalies, K.U., Suganthan,           P.N., and Martinetz, T., 2010. Prediction of Apoptosis Protein Locations with Genetic          Algorithms and Support Vector Machines Through a New Mode of Pseudo Amino Acid          Composition. Protein and Peptide Letters 17, 14731479. 18 Kannan, S., Hauth, A.M., and Burger, G., 2008. Function prediction of hypothetical           proteins without sequence similarity to proteins of known function. Protein  Peptide          Letters 15, 11071116. 19 Liu, T., Zheng, X., Wang, C., and Wang, J., 2010. Prediction of Subcellular Location of          Apoptosis Proteins using Pseudo Amino Acid Composition An Approach from Auto          Covariance Transformation. Protein  Peptide Letters 17, 12639. 20 Mohabatkar, H., 2010. Prediction of cyclin proteins using Chous pseudo amino acid           composition. Protein  Peptide Letters 17, 12071214. 21 Wang, Y.C., Wang, X.B., Yang, Z.X., and Deng, N.Y., 2010. Prediction of enzyme         subfamily class via pseudo amino acid composition by incorporating the conjoint triad         feature. Protein  Peptide Letters 17, 14411449.  22 Xiao, X., and Chou, K.C., 2007. Digital coding of amino acids based on hydrophobic            index. Protein  Peptide Letters 14, 871875.           294International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518   IJSER  2011 httpwww.ijser.org  Electrical Power Generation Using Piezoelectric Crystal Anil Kumar   Abstract The usefulness of most high technology devices such as cell phones, computers, and sensors is limited by the storage capacity of batteries. In the future, these limitations will become more pronounced as the demand for wireless power outpaces battery development which is already nearly optimized. Thus, new power generation techniques are required for the next generation of wearable computers, wireless sensors, and autonomous systems to be feasible. Piezoelectric materials are excellent power generation devices because of their ability to couple mechanical and electrical properties. For example, when an electric field is applied to piezoelectric a strain is generated and the material is deformed. Consequently, when a piezoelectric is strained it produces an electric field therefore, piezoelectric materials can convert ambient vibration into electrical power. Piezoelectric materials have long been used as sensors and actuators however their use as electrical generators is less established. A piezoelectric power generator has great potential for some remote applications such as in vivo sensors, embedded MEMS devices, and distributed networking. Developing piezoelectric generators is challenging because of their poor source characteristics high voltage, low current, high impedance and relatively low power output. This paper presents a theoretical analysis to increase the piezoelectric power generation that is verified with experimental results.     Index TermsPiezoelectric materials, piezoelectricity, power generation, PZT ceramics.        1 INTRODUCTION                                                                     echanical stresses applied to piezoelectric materials distort internal dipole moments and generate electrical potentials voltages in direct proportion to the applied forces. These same crystalline materials also lengthen or shorten in direct proportion to the magnitude and polarity of applied electric fields. Because of these properties, these materials have long been used as sensors and actuators. One of the earliest practical applications of piezoelectric materials was the development of the first SONAR system in 1917 by Langevin who used quartz to transmit and receive ultrasonic waves 1. In 1921, Cady first proposed the use of quartz to control the resonant frequency of oscillators. Today, piezoelectric sensors e.g., force, pressure, acceleration and actuators e.g., ultrasonic, micro positioning are widely available. The same properties that make these materials useful for sensors can also be utilized to generate electricity. Such materials are capable of converting the mechanical energy of compression into electrical energy, but developing piezoelectric generators is challenging because of their poor source characteristics high voltage, low current, high impedance. This is especially true at low frequencies and relatively low power output.  These challenges have limited the use of such generators primarily because the relatively small amount of available regulated electrical power has not been useful. The recent advent of extremely low power electrical and mechanical devices e.g., micro electromechanical systems or MEMS makes such generators attractive in several applications where remote power is required. Such applications are sometimes referred to as power scavenging and include in vivo sensors, embedded MEMS devices, and distributed networking. Several recent studies have investigated piezoelectric power generation. One study used lead zirconate titanate PZT wafers and flexible, multilayer polyvinylidene fluoride PVDF films inside shoes to convert mechanical walking energy into usable electrical energy 2, 3. This system has been proposed for mobile computing and was ultimately able to provide continuously 1.3 mW at 3 V when walking at a rate of 0.8 Hz. Other projects have used piezoelectric films to extract electrical energy from mechanical vibration in machines to power MEMS devices 4. This work extracted a very small amount of power 5uW from the vibration and no attempt was made to condition or store the energy. Similar work has extracted slightly more energy 70uW from machine and building vibrations 5. Piezoelectric materials have also been studied to generate electricity from pressure variations in micro hydraulic systems 6. The power would presumably be used for MEMS but this work is  still in the conceptual phase. Other work has used piezoelectric materials to convert kinetic energy into a spark to detonate an explosive projectile on impact 7. Still other work has proposed using flexible piezoelectric polymers for energy conversion in windmills 8, and to convert flowin oceans and rivers into electric power 9.A recent medical application has proposed the use of piezoelectric materials to generate electricity to promote bone growth 10. This work uses an implanted bone prosthesis containing a piezoelectric generator configured to deliver electric current to specific locations around the implant. This device uses unregulated high voltage energy and it is not clear if the technique has advanced beyond the conceptual phase. The M295International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org  above studies have all had some success in extracting electrical power from piezoelectric elements. However, many issues such as efficiency, conditioning and storage have not been fully addressed. This paper presents the idea to increase the power generation by the piezoelectric. A few researchers have used single off theshelf piezoelectric devices to harvest electrical power, yet little has been done to overcome the main weaknesses associated with piezoelectric power harvesting.  This research seeks to systematically overcome the weaknesses associated with cantilevermounted piezoelectric used for mobile power harvesting to maximize the power from a piezoelectric device the load impedance must match the impedance of device. This is problematic for frequencies between 10100 Hz because a single piezoelectric may have impedance in the range of several hundred thousand ohms to ten million ohms. Thus, little current can be produced, and battery charging is diminished due to low current production. To reduce the impedance and increase electrical current, two offtheshelf actuators 8 piezoelectric totals are connected electrically in parallel and tuned to resonate in the frequency range of an ambient vibration similar to that produced by a person walking. A picture of the experimental setup may be seen in Figure 1.  Figure 1 The mobile power harvester attached to a shaker for experimental testing                To demonstrate the power harvesting advantage, 40 and 80 mAhr Nickel Metal Hydride batteries are recharged with each individual actuator then charged with both actuators connected in parallel. For a 1.4 Hz frequency a brisk walking pace, the parallel combination charges two 40 mAhr batteries in 3.09 hours, and two 80 mAhr batteries in 5.64 hours. The individual actuators require 16.1 hours to charge a 40 mAhr battery, and 22.7 hours to charge an 80 mAhr battery. Clearly, the parallel combination of multiple offtheshelf piezoelectric actuators increases battery charge times, and adding more parallel devices could increase power production so long as the total voltage exceeds the charged voltage of the battery. Since most production piezoelectric devices are designed as actuators, research is being conducted to optimize piezoelectric for power harvesting. Specifically, the locations of each piezoelectric on the cantilevered structure is being studied and designs that reduce electrical cancellation due to out of phase motions are ongoing.  On october 6, 2009 the Hefer intersection along the old coastal road of Route 4 in Israel was the place where a piezoelectric generator was put to the test and generated some 2,000 watthours of electricity. The setup consists of a tenmeter strip of asphalt, with generators lying underneath, and batteries in the roads proximity. Being the first practical test of the system, the researchers still expect energetic and feasibility results. Technion was helped by Innowattech, a company from Israel to finish the pilot project The project manager, Dr. Lucy EderyAzulay, explained that the generators developed by Innowattech are embedded about five centimeters beneath the upper layer of asphalt. The technology is based on piezoelectric materials that enable the conversion of mechanical energy exerted by the weight of passing vehicles into electrical energy. As far as the drivers are concerned, the road is the same, she says EderyAzulay added that expanding the project to a length of one kilometer along a single lane would produce 200 KWh, while a fourlane highway could produce about a MWh sufficient electricity to provide for the average consumption in 2,500 households.  CONCLUSION As the results shows that by using double actuators in parallel we can reduce the charging time of the battery and increase the power generated by the piezoelectric device.In second research where a piezoelectric generator was put to the test and generated some 2,000 watthours of electricity. The setup consists of a tenmeter strip of asphalt, with generators lying underneath, and batteries in the roads proximity. So that it is clear by using parallel combination we can overcome the problems like of impedance matching and low power generation. The results clearly show that piezoelectric materials are the future of electric power generation.           296International Journal of Scientific  Engineering Research Volume 2, Issue 5, May2011                                                                                   ISSN 22295518  IJSER  2011 httpwww.ijser.org  7.3 References 1 Garnett E. Simmers Jr., Henry A. Sodano Center for Intelligent Materials Systems and Structures, Mechanical Engineering Department, Virginia.  2 Starner, T., 1996, HumanPowered Wearable Computing, IBM Systems Journal, Vol. 35, pp. 618. 3 Stephen R. Platt, Shane Farritor, and Hani Haider On LowFrequency Electric Power Generation With PZT Ceramics        4. V. Hugo Schmidt, Piezoelectric energy                    conversion in windmills, in Proc. Ultrasonic Symp., 1992, pp. 897904.    297
