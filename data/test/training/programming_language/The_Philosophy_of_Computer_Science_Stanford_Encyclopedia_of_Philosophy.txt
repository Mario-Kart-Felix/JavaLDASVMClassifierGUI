The Philosophy of Computer Science (Stanford Encyclopedia of Philosophy) Stanford Encyclopedia of Philosophy Menu Browse Table of Contents What's New Random Entry Chronological Archives About Editorial Information About the SEP Editorial Board How to Cite the SEP Special Characters Advanced Tools Contact Support SEP Support the SEP PDFs for SEP Friends Make a Donation SEPIA for Libraries Entry Navigation Entry Contents Bibliography Academic Tools Friends PDF Preview Author and Citation Info Back to Top The Philosophy of Computer Science First published Tue Aug 20, 2013; substantive revision Thu Jan 19, 2017 The philosophy of computer science is concerned with those ontological, methodological, and ethical issues that arise from within the academic discipline of computer science as well as from the practice of software development. Thus, the philosophy of computer science shares the same philosophical goals as the philosophy of mathematics and the many subfields of the philosophy of science, such as the philosophy of biology or the philosophy of the social sciences. The philosophy of computer science also considers the analysis of computational artifacts , that is, human-made computing systems, and it focuses on methods involved in the design, specification, programming, verification, implementation, and testing of those systems. The abstract nature of computer programs and the resulting complexity of implemented artifacts, coupled with the technological ambitions of computer science, ensures that many of the conceptual questions of the philosophy of computer science have analogues in the philosophy of mathematics , the philosophy of empirical sciences, and the philosophy of technology . Other issues characterize the philosophy of computer science only. We shall concentrate on three tightly related groups of topics that form the spine of the subject. First we discuss topics related to the ontological analysis of computational artifacts, in Sections 1–5 below. Second, we discuss topics involved in the methodology and epistemology of software development, in Sections 6–9 below. Third, we discuss ethical issues arising from computer science practice, in Section 10 below. Applications of computer science are briefly considered in section 11. 1. Computational Artifacts 1.1 Duality 1.2 Technical Artifacts 2. Specification and Function 2.1 Definition 2.2 Definitions as Specifications 2.3 Abstract Artifacts 2.4 Theories of Function 3. Implementation 3.1 What Is Implementation? 3.2 Implementation as Semantic Interpretation 3.3 Specification and Implementation 4. Semantics 4.1 Two Kinds of Semantic Theory 4.2 Programming Languages as Axiomatic Theories 4.3 The Implementation of Programming Languages 5. The Ontology of Programs 5.1 Programs as Mathematical Objects 5.2 Programs as Technical Artifacts 5.3 Abstract and Concrete 5.4 Programs and Specifications 6. Verification 6.1 Models and Theories 6.2 Testing and Experiments 6.3 Explanation 7. Correctness 7.1 Mathematical Correctness 7.2 The Complexity Challenge 7.3 The Empirical Challenge 7.4 Physical Correctness 7.5 Miscomputations 8. Abstraction 8.1 Abstraction in Computer Science 8.2 Information Hiding 9. The Epistemological Status of Computer Science 9.1 Computer Science as a Mathematical Discipline 9.2 Computer Science as an Engineering Discipline 9.3 Computer Science as a Scientific Discipline 10. Computer Ethics 10.1 Intellectual Property Rights on Computational Artifacts 10.2 Moral Responsibility of Computing Practitioners 11. Applications of Computer Science Bibliography Academic Tools Other Internet Resources Related Entries 1. Computational Artifacts Computational artifacts underpin our Facebook pages, control air traffic around the world, and ensure that we will not be too surprised when it snows. They have been applied in algebra, car manufacturing, laser surgery, banking, gastronomy, astronomy, and astrology. Indeed, it is hard to find an area of life that has not been fundamentally changed and enhanced by their application. But what is it that is applied? What are the things that give substance to such applications? The trite answer is the entities that computer scientists construct, the artifacts of computer science, computational artifacts, if you will. Much of the philosophy of computer science is concerned with their nature, specification, design, and construction. 1.1 Duality Folklore has it that computational artifacts fall into two camps: hardware and software. Presumably, software includes compilers and natural language understanding systems, whereas laptops and tablets are hardware. But how is this distinction drawn: How do we delineate what we take to be software and what we take to be hardware? A standard way identifies the distinction with the abstract-physical one (see the entry on abstract objects ), where hardware is taken to be physical and software to be abstract. Unfortunately, this does not seem quite right. As Moor (1978) points out, programs, which are normally seen as software, and therefore under this characterization abstract, may also be physical devices. In particular, programs were once identified with sequences of physical lever pulls and pushes. There are different reactions to this observation. Some have suggested there is no distinction. In particular, Suber (1988) argues that hardware is a special case of software, and Moor (1978) argues that the distinction is ontologically insignificant. On the other hand, Duncan (2011) insists that there is an important difference but it is one that can only be made within an ontological framework that supports finer distinctions than the simple abstract-physical one (e.g., B. Smith 2012). Irmak (2012) also thinks that software and hardware are different: software is an abstract artifact, but apparently not a standard one, because it has temporal properties. Whether or not the software-hardware distinction can be made substantial, most writers agree that, although a program can be taken as an abstract thing, it may also be cashed out as a sequence of physical operations. Consequently, they (e.g., Colburn 2000; Moor 1978) insist that programs have a dual nature: they have both an abstract guise and a physical one. Indeed, once this is conceded, it would seem to apply to the majority of computational artifacts. On the one hand, they seem to have an abstract guise that enables us to reflect and reason about them independently of any physical manifestation. This certainly applies to abstract data types (Cardelli & Wegner 1985). For example, the list abstract data type consists of the carrier type together with operations that support the formation and manipulation of lists. Even if not made explicit, these are determined by several axioms that fix their properties: e.g., if one adds an element to the head of a list to form a new list, and then removes the head, the old list is returned. Similarly, an abstract stack is determined by axioms that govern push and pop operations. Using such properties, one may reason about lists and stacks in a mathematical way, independently of any concrete implementation. And one needs to. One cannot design nor program without such reasoning; one cannot construct correct programs without reasoning about what the programs are intended to do. If this is right, computational artifacts have an abstract guise that is separable from their physical realization or implementation. Indeed, this requirement to entertain abstract devices to support reasoning about physical ones is not unique to computer science. On the other hand, they must have a physical implementation that enables them to be used as things in the physical world. This is obviously true of machines, but it is equally so for programs: Programmers write programs to control physical devices. A program or abstract machine that has no physical realization is of little use as a practical device for performing humanly intractable computations. For instance, a program that monitors heart rate must be underpinned by a physical device that actually performs the task. The computer scientist Dijkstra puts it as follows. A programmer designs algorithms, intended for mechanical execution, intended to control existing or conceivable computer equipment. (Dijkstra 1974: 1) On the duality view, computer science is not an abstract mathematical discipline that is independent of the physical world. To be used, these things must have physical substance. And once this observation is made, there is a clear link with a central notion in the philosophy of technology (Kroes 2010; Franssen et al. 2010), to which we now turn. 1.2 Technical Artifacts Technical artifacts include all the common objects of everyday life such as toilets, paper clips, tablets, and dog collars. They are intentionally produced things. This is an essential part of being a technical artifact . For example, a physical object that accidentally carries out arithmetic is not by itself a calculator. This teleological aspect distinguishes them from other physical objects, and has led philosophers to argue that technical artifacts have a dual nature fixed by two sets of properties (e.g., Kroes 2010; Meijers 2001; Thomasson 2007; Vermaas & Houkes 2003): functional properties and structural properties. Functional properties say what the artifact does. For example, a kettle is for boiling water, and a car is for transportation. On the other hand, structural properties pertain to its physical makeup. They include its weight, color, size, shape, chemical constitution, etc. For example, we might say that our car is red and has white seats. The notion of a technical artifact will help to conceptualize and organize some of the central questions and issues in the philosophy of computer science. We begin with a concept that underpins much of the activity of the subject. Indeed, it is the initial expression of functional properties. 2. Specification and Function In computer science, the function of an artifact is initially laid out in a (functional) specification (Sommerville 2016 [1982]; Vliet 2008). Indeed, on the way to a final device, a whole series of specification-artifact pairs of varying degrees of abstractness come into existence. The activities of specification, implementation and correctness raise a collection of overlapping conceptual questions and problems (B.C. Smith 1985; Turner 2011; Franssen et al. 2010). 2.1 Definition Specifications are expressed in a variety of ways, including ordinary vernacular. But the trend in computer science has been towards more formal and precise forms of expression. Indeed, specialized languages have been developed that range from those designed primarily for program specification (e.g., VDM, Jones 1990 [1986]; Z, Woodcock & Davies 1996; B, Abrial 1996) and wide spectrum languages such UML (Fowler 2003), to specialized ones that are aimed at architectural description (e.g., Rapide, Luckham 1998; Darwin, Distributed Software Engineering 1997; Wright, Allen 1997). They differ with respect to the their underlying ontologies and their means of articulating requirements. Z is based upon predicate logic and set theory. It is largely employed for the specification of suites of individual program modules or simple devices. UML (Fowler 2003) has a very rich ontology and a wide variety of expression mechanisms. For example, its class language allows the specification of software patterns (Gamma et al. 1994). In general, an architectural description language is used to precisely specify the architecture of a software system (Bass et al. 2003 [1997]). Typically, these languages employ an ontology that includes notions such as components , connectors , interfaces and configurations . In particular, architectural descriptions written in Rapide, Darwin, or Wright are precise expressions in formalisms that are defined using an underlying mathematical semantics. But what is the logical function of the expressions of these languages? On the face of it, they are just expressions in a formal language. However, when the underlying ontology is made explicit, each of these languages reveals itself to be a formal ontology that may be naturally cast as a type theory (Turner 2009a). Under this interpretation, these expressions are stipulative definitions (Gupta 2012). As such, each defines a new abstract object within the formal ontology of its system. 2.2 Definitions as Specifications However, taken by itself a definition need not be a specification of anything; it may just form part of a mathematical exploration. So when does a definition act as a specification? Presumably, just in case the definition is taken to point beyond itself to the construction of an artifact. It is the intentional act of giving governance of the definition over the properties of a device or system that turns a mere definition into a specification. The definition then determines whether or not the device or system has been built correctly. It provides the criteria of correctness and malfunction. From this perspective, the role of specification is a normative one. If one asks whether the device work, it is the definition functioning as a specification that tells us whether it does. Indeed, without it, the question would be moot. At any level of abstraction (see §8.1 ), the logical role of specification is always the same: It provides a criterion for correctness and malfunction. This is the perspective argued for by Turner (2011). Indeed, this normative role is taken to be part of any general theory of function (Kroes 2012). It should go without saying that this is an idealization. A specification is not fixed throughout the design and construction process. It may have to be changed because a client changes her mind about the requirements. Furthermore, it may turn out for a variety of reasons that the artifact is impossible to build. The underlying physical laws may prohibit matters. There may also be cost limitations that prevent construction. Indeed, the underlying definition may be logically absurd. In these cases, the current specification will have to be given up. But the central normative role of specification remains intact. Unlike functional descriptions, specifications are taken to be prescribed in advance of the artifact construction; they guide the implementer. This might be taken to suggest a more substantive role for specification i.e., to provide a method for the construction of the artifact. However, the method by which we arrive at the artifact is a separate issue from its specification. The latter dictates no such method. There is no logical difference between a functional specification and functional description; logically they both provide a criterion of correctness. 2.3 Abstract Artifacts Software is produced in a series of layers of decreasing levels of abstraction, where in the early layers both specification and artifact are abstract (Brooks 1995; Sommerville 2016 [1982]; Irmak 2012). For example, a specification written in logical notation might be taken to be a specification of a linguistic program. In turn, the linguistic program, with its associated semantics, might be taken as the specification of a physical device. In other words, we admit abstract entities as artifacts. This is a characteristic feature of software development (Vliet 2008). It distinguishes it from technology in general. The introduction of abstract intermediate artifacts is essential (Brooks 1995; Sommerville 2016 [1982]). Without them logically complex computational artifacts would be impossible to construct. So what happens to the duality thesis? It still holds good, but now the structural description does not necessarily provide physical properties but another abstract device. For example, an abstract stack can act as the specification of a more concrete one that is now given a structural description in a programming language as an array. But the array is itself not a physical thing, it is an abstract one. Its structural description does not use physical properties but abstract ones, i.e., axioms. Of course, eventually, the array will get implemented in a physical store. However, from the perspective of the implementer who is attempting to implement stacks in a programming language with arrays as a data type, the artifact is the abstract array of the programming language. Consequently, the duality thesis must be generalized to allow for abstract artifacts. 2.4 Theories of Function Exactly how the physical and intentional conceptualizations of our world are related remains a vexing problem to which the long history of the mind-body problem in philosophy testifies. This situation also affects our understanding of technical artifacts: a conceptual framework that combines the physical and intentional (functional) aspects of technical artifacts is still lacking. (Kroes & Meijers 2006: 2) The literature on technical artifacts (e.g., Kroes 2010; Meijers 2001; Thomasson 2007; Vermaas & Houkes 2003) contains two main theories about how the two conceptualizations are related: causal-role theories and intentional ones. Causal-role theories insist that actual physical capacities determine function. Cummins’s theory of functional analysis (Cummins 1975) is an influential example of such a theory. The underlying intuition is that, without the physical thing and its actual properties, there can be no artifact. The main criticism of these theories concerns the location of any correctness criteria. If all we have is the physical device, we have no independent measure of correctness (Kroes 2010): The function is fixed by what the device actually does. Causal role theories… have the tendency to let functions coincide with actual physical capacities: structure and function become almost identical. The main drawback of this approach is that it cannot account for the malfunctioning of technical artifacts: an artifact that lacks the actual capacity for performing its intended function by definition does not have that function. The intentions associated with the artifact have become irrelevant for attributing a function. (Kroes 2010: 3) This criticism has the same flavor as that made by Kripke (1982) in his discussion of rule following. Intentional theories insist that it is agents who ascribe functions to artifacts. Objects and their components possess functions only insofar as they contribute to the realization of a goal. Good examples of this approach are McLaughlin (2001) and Searle (1995). But how exactly does the function get fixed by the desires of an agent? One interpretation has it that the function is determined by the mental states of the agents, i.e., the designers and users of technical artifacts. In their crude form such theories have difficulty accounting for how they impose any constraints upon the actual thing that is the artifact. If functions are seen primarily as patterns of mental states, on the other hand, and exist, so to speak, in the heads of the designers and users of artifacts only, then it becomes somewhat mysterious how a function relates to the physical substrate in a particular artifact. (Kroes 2010: 2) For example, how can the mental states of an agent fix the function of a device that is intended to perform addition? This question is posed in a rather different context by Kripke. Given … that everything in my mental history is compatible both with the conclusion that I meant plus and with the conclusion that I meant quus, it is clear that the skeptical challenge is not really an epistemological one. It purports to show that nothing in my mental history of past behavior—not even what an omniscient God would know—could establish whether I meant plus or quus. But then it appears to follow that there was no fact about me that constituted my having meant plus rather than quus. (Kripke 1982: 21) Of course, one might also insist that the artifact is actually in accord with the specification, but this does not help if the expression of the function is only located in the mental states of an agent. This version of the intentional theory is really a special case of a causal theory where the agent’s head is the physical device in which the function is located. However, there is an alternative interpretation of the intentional approach. On his commentary on Wittgenstein’s notion of acting intentionally (Wittgenstein 1953), David Pears suggests that anyone who acts intentionally must know two things. Firstly, she must know what activity she is engaged in. Secondly, she must know when she has succeeded (Pears 2006). According to this perspective, establishing correctness is an externally observable, rule-based activity. The relation between the definition and the artifact is manifest in using the definition as a canon of correctness for the device. I must be able to justify my reasons for thinking that it works: If I am asked if it works I must be able to justify that it does with reference to the abstract definition. The content of the function is laid out in the abstract definition, but the intention to take it as a specification is manifest in using it as one ( §2.2 ). 3. Implementation Broadly speaking an implementation is a realization of a specification. Examples includes the implementation of a UML specification in Java, the implementation of an abstract algorithm as a program in C, the implementation of an abstract data type in Miranda, or the implementation of a whole programming language. Moreover, implementation is often an indirect process that involves many stages before physical bedrock, it involves a specification-artifact pairing and a notion of implementation. But what is an implementation? Is there just one notion or many? 3.1 What Is Implementation? The most detailed philosophical study of implementation is given by Rapaport (1999, 2005). He argues that implementation involves two domains: a syntactic one (the abstraction) and a semantic one (the implementation). Indeed, he suggests that a full explication of the notion requires a third hidden term, a medium of implementation: \(I\) is an implementation of \(A\) in medium \(M\). Here \(I\) is the semantic component, \(A\) is the abstraction, and \(M\) is the medium of implementation. He allows for the target medium to be abstract or physical. This is in line with the claim that artifacts may be abstract or concrete. Superficially, this seems right. In all the examples cited, there is a medium of implementation in which the actual thing that is the implementation is carved out. Perhaps the clearest example is the implementation of a programming language. Here, the syntactic domain is the actual language and the semantic one its interpretation on an abstract machine: the medium of interpretation. He suggests that we implement an algorithm when we express it in a computer programming language, and we implement an abstract data type when we express it as a concrete one. Examples that he does not mention might include the UML definition of design patterns implemented in Java (Gamma et al. 1994). He further argues that there is no intrinsic difference between which of the domains is semantic and which is syntactic. This is determined by the asymmetry of the implementation mapping. For example, a physical computer process that implements a program plays the role of the semantics to the linguistic program, while the same linguistic program can play the role of semantic domain to an algorithm. This asymmetry is parallel to that of the specification-artifact connection. On the face of it, there is little to cause any dissension. It is a straightforward description of the actual use of the term implementation. However, there is an additional conceptual claim that is less clear. 3.2 Implementation as Semantic Interpretation Apparently, the semantic domain, as its name suggests, is always taken to be a semantic representation of the syntactic one; it closes a semantic gap between the abstraction and the implementation in that the implementation fills in details. This is a referential view of semantics in that the syntactic domain refers to another domain that provides its meaning. Indeed, there is a strong tradition in computer science that takes referential or denotational semantics as fundamental (Stoy 1977; Milne & Strachey 1976; Gordon 1979). We shall examine this claim later when we consider the semantics of programming languages in more detail ( §4 ). For the moment, we are only concerned with the central role of any kind of semantics. One view of semantics insists that it must be normative. Although the exact form of the normative constraint (Glüer & Wikforss 2015; Miller & Wright 2002) is debated, there is a good deal of agreement on a minimal requirement: a semantic account must fix what it is to use an expression correctly. The fact that the expression means something implies that there is a whole set of normative truths about my behavior with that expression; namely, that my use of it is correct in application to certain objects and not in application to others…. The normativity of meaning turns out to be, in other words, simply a new name for the familiar fact that, regardless of whether one thinks of meaning in truth-theoretic or assertion-theoretic terms, meaningful expressions possess conditions of correct use. Kripke’s insight was to realize that this observation may be converted into a condition of adequacy on theories of the determination of meaning: any proposed candidate for the property in virtue of which an expression has meaning, must be such as to ground the “normativity” of meaning—it ought to be possible to read off from any alleged meaning constituting property of a word, what is the correct use of that word. (Boghossian 1989: 513) On the assumption that this minimal requirement has to be satisfied by any adequate semantic theory, is implementation always, or even ever, semantic interpretation? Are these two notions at odds with each other? One standard instance of implementation concerns the interpretation of one language in another. Here the abstraction and the semantic domain are both languages. Unfortunately, this does not provide a criterion of correctness unless we have already fixed the semantics of the target language. While translating between languages is taken to be implementation, indeed a paradigm case, it is not, on the present criterion, semantic interpretation. It only satisfies the correctness criterion when the target language has an independently given notion of correctness. This may be achieved in an informal or in a mathematical way. But it must not end in another uninterpreted language. So this paradigm case of implementation does not appear to satisfy the normative constraints required for semantic interpretation. On the other hand, Rapaport (1995) argues that providing a recursive definition of implementation requires a base case, that is, the process must end in an uninterpreted language. However, such a language can be interpreted on itself, mapping each symbol either on itself or on different symbols of that language. Next consider the case where the abstraction is a language and the semantic medium is set theory. This would be the case with denotational semantics (Stoy 1977). This does provide a notion of correctness. Our shared and agreed understanding of set theory provides this. Unfortunately, it would not normally be taken as an implementation. Certainly, it would not, if an implementation is something that is eventually physically realizable. Now consider the case where the syntactic component is an abstract stack and the semantic one is an array. Here we must ask what it means to say that the implementation is correct. Does the medium of the array fix the correct use of stacks? It would seem not: The array does not provide the criteria for deciding whether we have the correct axioms for stacks or whether we have used them correctly in a particular application. Rather, the stack is providing the correctness criteria for the implementation that is the array. Instead, the axioms provide the fundamental meanings of the constructs. While the array is an implementation of the stack, it does not provide it with a notion of correctness: The cart and the horse have been interchanged. Finally, suppose the semantic domain is a physical machine and the syntactic one is an abstract one. The suggestion is that the physical machine provides a semantic interpretation of the abstract one. But again, a semantic interpretation must provide us with a notion of correctness and malfunction, and there are compelling arguments against this that are closely related to the causal theories of function ( §2.4 ). This issue will be more carefully examined in section ( §4 ) where we consider programming language semantics. Given that a semantic account of a language must supply correctness criteria, and that the term semantics is to have some bite, these are serious obstacles for the view that implementation is semantic interpretation. There are several phenomena all rolled into one. If these objections are along the right lines, then the relationship between the source and target is not semantic interpretation. Of course, one may counter all this by arguing against the correctness requirement for semantic theory. 3.3 Specification and Implementation An alternative analysis of implementation is implicit in Turner (2014, 2012). Consider the case where the data type of finite sets is implemented in the data types of lists. Each of these structures is governed by a few simple axioms. The implementation represents finite sets as lists, the union operation on sets as list concatenation, and equality between sets as extensional equality on lists etc. This is a mathematical relationship where the axioms for sets act as a specification of the artifact, which in this case is implemented in the medium of lists. It would appear that the logical connection between the two is that of specification and artifact. The mapping does not have to be direct i.e., there does not have to be a simple operation-to-operation correspondence, but the list properties of the implemented operations must satisfy the given set axioms. In standard mathematical terms, the list medium must provide a mathematical model (in the sense of model theory, W. Hodges 2013) of the set axioms. The case where one language is implemented in another is similar, and fleshed out by the semantic definitions of the two languages. Finally, consider the case where the medium of implementation is a physical device e.g., an abstract stack is implemented as a physical one. Once again the abstract stack must provide the correctness criteria for the physical device. This is what happens in practice. We check that the physical operations satisfy the abstract demands given by the axioms for stacks. There are issues here that have to do with the adequacy of this notion of correctness. We shall discuss these when we more carefully consider the computer science notion of correctness ( §7.4 ). If this analysis is along the right lines, implementation is best described as a relation between specification and artifact. Implementation is not semantic interpretation; indeed, it requires an independent semantic account in order to formulate a notion of implementation correctness. So, what is taken to be semantic interpretation in computer science? 4. Semantics How is a semantic account of a programming language to be given? What are the main conceptual issues that surround the semantic enterprise? There are many different semantic candidates in the literature (Gordon 1979; Gunter 1992; Fernández 2004; Milne & Strachey 1976). One of the most important distinctions centers upon the difference between operational and denotational semantics (Turner 2007; White 2003). 4.1 Two Kinds of Semantic Theory Operational semantics began life with Landin (1964). In its logical guise (Fernández 2004) it provides a mechanism of evaluation where, in its simplest form, the evaluation relation is represented as follows. \[P \Downarrow c\] This expresses the idea that the program \(P\) converges to the canonical form given by \(c\). The classical case of such a reduction process occurs in the lambda calculus where reduction is given by the reduction rules of the calculus, and canonical forms are its normal forms i.e., where none of the reduction rules apply. The following is a simple example: \[z(\lambda x.y)\] This is usually called big step semantics. It is normally given in terms of rules that provide the evaluation of a complex program in terms of the evaluation of its parts. For example, a simple rule for sequencing (\(\degr\)) would take the form \[ \frac{P\Downarrow c \quad Q\Downarrow d} {P\degr Q \Downarrow c\degr d} \] These canonical or normal forms are other terms in the programming language which cannot be further reduced by the given rules. But they are terms of the language. For this reason, this operational approach is often said to be unsatisfactory. According to this criticism, at some point in the interpretation process, the semantics for a formal language must be mathematical. We can apparently get quite a long way expounding the properties of a language with purely syntactic rules and transformations… One such language is the Lambda Calculus and, as we shall see, it can be presented solely as a formal system with syntactic conversion rules … But we must remember that when working like this all we are doing is manipulating symbols-we have no idea at all of what we are talking about. To solve any real problem, we must give some semantic interpretation. We must say, for example, “these symbols represent the integers”. (Stoy 1977: 9) In contrast, operational semantics is taken to be syntactic . In particular, even if one of them is in canonical form, the relation \(P\Downarrow c\) relates syntactic objects. This does not get at what we are talking about. Unless the constants of the language have themselves an independently given mathematical meaning, at no point in this process do we reach semantic bedrock: we are just reducing one syntactic object to another, and this does not yield a normative semantics. This leads to the demand for a more mathematical approach. Apparently, programming languages refer to (or are notations for) abstract mathematical objects, not syntactic ones (Strachey 2000; McGettrick 1980; Stoy 1977). In particular, denotational semantics provides, for each syntactic object \(P\), a mathematical one. Moreover, it generally does this in a compositional way: Complex programs have their denotations fixed in terms of the denotations of their syntactic parts. These mathematical objects might be set theoretic, category theoretic, or type theoretic. But whichever method is chosen, programs are taken to refer to abstract mathematical things. However, this position relies on a clear distinction between syntactic and mathematical objects. 4.2 Programming Languages as Axiomatic Theories Mathematical theories such as set theory and category theory are axiomatic theories. And it is this that makes them mathematical. This is implicit in the modern axiomatic treatment of mathematics encouraged by (Bourbaki 1968) and championed by Hilbert (1931). It is worth pointing out that the axiomatic account, as long as it is precise and supports mathematical reasoning, does not need to be formal. If one accepts this as a necessary condition for mathematical status, does it rule out operational accounts? Prima facie it would seem so. Apparently, programs are reduced to canonical constants with no axiomatic definitions. But Turner (2009b, 2010) argues this is to look in the wrong place for the axiomatization: the latter resides not in the interpreting constants but in the rules of evaluation, i.e., in the theory of reduction given by the axiomatic relation \(\Downarrow\). Given that both denotational and operational semantics define matters axiomatically, it should not matter which we take to define the language as a formal mathematical theory. Unfortunately, they don’t always agree: The notion of equality provided by the operational account, although preserved by the denotational one, is often more fine grained. This has led to very special forms of denotational semantics based upon games (Abramsky & McCusker 1995; Abramsky et al. 1994). However, it is clear that practitioners take the operational account as fundamental, and this is witnessed by the fact that they seek to devise denotational accounts that are in agreement with the operational ones. Not only is there no metaphysical difference between the set theoretic account and the operational one, but the latter is taken to be the definitive one. This view of programming languages is the perspective of theoretical computer science: Programming languages, via their operational definitions, are mathematical theories of computation. However, programming languages are very combinatorial in nature. They are working tools, not elegant mathematical theories; it is very hard to explore them mathematically. Does this prevent them from being mathematical theories? There has been very little discussion of this issue in the literature; Turner (2010) and Strachey (2000) are exceptions. On the face of it, Strachey sees them as mathematical objects pure and simple. Turner is a little more cautious and argues that actual programming languages, while often too complex to be explored as mathematical theories, contain a core theory of computation that may be conservatively extended to the full language. 4.3 The Implementation of Programming Languages However, Turner (2014) further argues that programming languages, even at their core, are not just mathematical objects. He argues that they are best conceptualized as technical artifacts. While their axiomatic definition provides their function, they also require an implementation. In the language of technical artifacts, a structural description of the language must say how this is to be achieved: It must spell out how the constructs of the language are to be implemented. To illustrate the simplest case, consider the assignment instruction. \[x := E\] A physical implementation might take the following form. Physically compute the value of \(E\). Place the (physical token for) the value of \(E\) in the physical location named \(x\) any existing token of value to be replaced. This is a description of how assignment is to be physically realized. It is a physical description of the process of evaluation. Of course, a complete description will spell out more, but presumably not what the actual machine is made of; one assumes that this would be part of the structural description of the underlying computer, the medium of implementation. The task of the structural description is only to describe the process of implementation on a family of similarly structured physical machines. Building on this, we stipulate how the complex constructs of the language are to be implemented. For example, to execute commands in sequence we could add a physical stack that arranges them for processing in sequence. Of course, matters are seldom this straightforward. Constructs such as iteration and recursion require more sophisticated treatment. Indeed, interpretation and compilation may involve many layers and processes. However, in the end there must be some interpretation into the medium of a physical machine. Turner (2014) concludes that a programming language is a complex package of syntax and semantics (function) together with the implementation as structure. Some have suggested that a physical implementation actually defines the semantics of the language. Indeed, this is a common perspective in the philosophy of computer science literature. We have already seen that Rapaport (1999) sees implementation as a semantic interpretation. Fetzer (1988) observes that programs have a different semantic significance from theorems. In particular, he asserts: …programs are supposed to possess a semantic significance that theorems seem to lack. For the sequences of lines that compose a program are intended to stand for operations and procedures that can be performed by a machine, whereas the sequences of lines that constitute a proof do not. (Fetzer 1988: 1059) This seems to say that the physical properties of the implementation contribute to the meaning of programs written in the language. Colburn (2000) is more explicit when he writes that the simple assignment statement \(A := 13\times 74\) is semantically ambiguous between something like the abstract account we have given, and the physical one given as: physical memory location \(A\) receives the value of physically computing 13 times 74. (Colburn 2000: 134) The phrase “physically computing” seems to imply that what the physical machine actually does is semantically significant i.e.; what it actually does determines or contributes to the meaning of assignment. Is this to be taken to imply that to fix what assignment means we have to carry out a physical computation? However, if an actual physical machine is taken to contribute in any way to the meaning of the constructs of the language, then their meaning is dependent upon the contingencies of the physical device. In particular, the meaning of the simple assignment statement may well vary with the physical state of the device and with contingencies that have nothing to with the semantics of the language, e.g., power cuts. Under this interpretation, multiplication does not mean multiplication but rather what the physical machine actually does when it simulates multiplication. This criticism parallels that for causal theories of function ( §2.4 ). 5. The Ontology of Programs The nature of programs has been the subject of a good amount of philosophical and legal reflection. What kinds of things are they? Are they abstract (perhaps mathematical or symbolic) objects or concrete physical things? Indeed, the legal literature even contains a suggestion that programs constitute a new kind of (legal) entity (§ 10.1 ). The exact nature of computer programs is difficult to determine. On the one hand, they are related to technological matters. On the other hand, they can hardly be compared to the usual type of inventions. They involve neither processes of a physical nature, nor physical products, but rather methods of organization and administration. They are thus reminiscent of literary works even though they are addressed to machines. Neither industrial property law nor copyright law in their traditional roles seems to be the appropriate instrument for the protection of programs, because both protections were designed for and used to protect very different types of creations. The unique nature of the computer program has led to broad support for the creation of sui generis legislation. (Loewenheim 1989: 1) This highlights the curious legal status of programs. Indeed, it raises tricky ontological questions about the nature of programs and software: they appear to be abstract, even mathematical objects with a complex structure, and yet they are aimed at physical devices. In this section, we examine some of the philosophical issues that have arisen regarding the nature of programs and software. 5.1 Programs as Mathematical Objects What is the content of the claim that programs are mathematical objects? In the legal literature, the debate seems to center on the notion that programs are symbolic objects that can be formally manipulated (Groklaw 2011, 2012—see Other Internet Resources ). Indeed, there is a branch of theoretical computer science called formal language theory that treats grammars as objects of mathematical study (Hopcroft & Ullman 1969). While this does give some substance to the claim, this is not the most important sense in which programs are mathematical. This pertains to their semantics, where programming languages are taken to be axiomatic theories ( §4.2 ). This perspective locates programs as elements in a theory of computation (Turner 2007, 2010). 5.2 Programs as Technical Artifacts While agreeing that programs have an abstract guise, much of the philosophical literature (e.g., Colburn 2000; Moor 1978) has it that they also possess a concrete physical manifestation that facilitates their use as the cause of computations in physical machines. For example, Moor observes: It is important to remember that computer programs can be understood on the physical level as well as the symbolic level. The programming of early digital computers was commonly done by plugging in wires and throwing switches. Some analogue computers are still programmed in this way. The resulting programs are clearly as physical and as much a part of the computer system as any other part. Today digital machines usually store a program internally to speed up the execution of the program. A program in such a form is certainly physical and part of the computer system. (Moor 1978: 215) The following is of more recent origin, and more explicitly articulates the duality thesis in its claim that software has both abstract and physical guises. Many philosophers and computer scientists share the intuition that software has a dual nature (Moor 1978; Colburn 2000). It appears that software is both an algorithm, a set of instructions, and a concrete object or a physical causal process. (Irmak 2012: 3) 5.3 Abstract and Concrete Anyone persuaded by the abstract-physical duality for programs is under an obligation to say something about the relationship between these two forms of existence. This is the major philosophical concern and parallels the question for technical artifacts in general. One immediate suggestion is that programs, as textual objects, cause mechanical processes. The idea seems to be that somehow the textual object physically causes the mechanical process. Colburn (2000, 1999) denies that the symbolic text itself has any causal effect; it is its physical manifestation, the thing on the disk, which has such an effect. For him, software is a concrete abstraction that has a medium of description (the text, the abstraction) and a medium of execution (e.g., a concrete implementation in semiconductors). The duality is unpacked in a way that is parallel to that found in the philosophy of mind (see the entry on dualism ), where the physical device is taken as a semantic interpretation of the abstract one. This is close to the perspective of Rapaport (1999). However, we have already alluded to problems with this approach ( §3.3 ). A slightly different account can be found in Fetzer (1988). He suggests that abstract programs are something like scientific theories: A program is to be seen as a theory of its physical implementation— programs as causal models. In particular, the simple assignment statement and its semantics is a theory about a physical store and how it behaves. If this is right, and a program turns out not to be an accurate description of the physical device that is its implementation, the program must be changed: If the theory that is enshrined in the program does not fit the physical device, it should be changed. But this does not seem to be what happens in practice. While the program may have to be changed, this is not instigated by any lack of accord with its physical realization, but by an independent abstract semantics for assignment. If this is correct, the abstract semantics appears not to be a theory of its concrete implementation. The alternative picture has it that the abstract program (determined by its semantics) provides the function of the artifact, and the physical artifact, or rather its description, provides its structure. It is the function of the program, expressed in its semantics, that fixes the physical implementation and provides the criteria of correctness and malfunction. Programs as computational artifacts have both an abstract aspect that somehow fixes what they do and a physical aspect that enables them to cause physical things to happen. 5.4 Programs and Specifications What is the difference between programming and specification? One suggestion is that a specification tells us what it is to do without actually saying how to do it. For instance, the following is a specification written in VDM (Jones 1990 [1986]). SQRTP \((x\):real, \(y\):real) Pre : \(x \ge 0\) Post : \(y* y = x\) and \(y \ge 0\) This is a specification of a square root function with the precondition that the input is positive. It is a functional description in that it says what it must do without saying how it is to be achieved. One way to unpack this what - how difference is in terms of the descriptive-imperative distinction. Programs are imperative and say how to achieve the goal, whereas specifications are declarative and only describe the input/output behavior of the intended program. Certainly, in the imperative programming paradigm, this seems to capture a substantive difference. But it is not appropriate for all. For example, logic and functional programming languages (Thompson 2011) are not obviously governed by it. The problem is that programming languages have evolved to a point where this way of describing the distinction is not marked by the style or paradigm of the programming language. Indeed, in practice, a program written in Haskell (Thompson 2011) could act as a specification for a program written in C (Huss 1997, Other Internet Resources ). A more fundamental difference concerns the direction of governance, i.e., which is the normative partner in the relationship and which is the submissive one. In the case of the specification of the square root function, the artifact is the linguistic program. When the program is taken as the specification, the artifact is the next level of code, and so on down to a concrete implementation. This is in accord with Rapaport (2005) and his notion of the asymmetry of implementation. 6. Verification One of the crucial parts of the software development process is verification: After computational artifacts have been specified, instantiated into some high-level programming language, and implemented in hardware, developers are involved in the activities of evaluating whether those artifacts are correct with respect to the provided program specifications. Correctness evaluation methods can be roughly sorted into two main groups: formal verification and testing. Formal verification (Monin 2003) involves some mathematical proof of correctness, software testing (Ammann & Offutt 2008) rather implies running the implemented program and observing whether performed executions comply or do not comply with the advanced specifications on the behaviors of such program. In many practical cases, formal methods and testing are used together for verification purposes (see for instance Callahan et al. 1996). 6.1 Models and Theories Formal verification methods include the construction of representations of the piece of software to be verified against some set of program specifications. In theorem proving (see Van Leeuwen 1990), programs are represented in terms of axiomatic systems and a set of rules of inference for programs’ transition conditions; a proof of correctness is provided by deriving opportunely formalized specifications from those set of axioms. In model checking (Baier & Katoen 2008), a program is represented in terms of some state transition system, the program’s property specifications are represented in terms of temporal logic formulas (Kröger & Merz 2008), and a proof of correctness is achieved by a depth-first search algorithm that checks whether those temporal logic formulas hold of the state transition system. Axiomatic systems and state transition systems used to evaluate whether the executions of the represented computational artifacts conform or do not conform with the behaviors prescribed by their specifications can be understood as theories of the represented systems in that they are used to predict and explain the future behaviors of those systems. In particular, state transition systems in model checking can be compared, on a methodological basis, with scientific models in empirical sciences (Angius & Tamburrini 2011). For instance, Kripke Structures are in compliance with Suppes’ (1960) definition of scientific models as set-theoretic structures establishing proper mapping relations with models of data collected by means of experiments on the target empirical system (see also the entry on models in science ). A Kripke Structure \(M = (S\), \(S_0\), \(R, L)\) is a set-theoretic model composed of a non-empty set of states \(S\), together with a non-empty set of initial states \(S_0\), a total state transition relation \(R \subseteq S \times S\), and a function \(L: S \rightarrow 2^{\textit{AP}}\) labeling each state in \(S\) with subsets of a set of atomic propositions AP . Kripke Structures and other state transition systems utilized in formal verification methods are often called system specifications. They are distinguished from common specifications, also called property specifications. The latter specify some required behavioral properties the artifact to be encoded must instantiate, while the former specify (in principle) all potential executions
