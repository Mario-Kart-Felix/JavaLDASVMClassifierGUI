1
May 23, 2001 Data Mining: Concepts and Techniques 1
Association Rules
May 23, 2001 Data Mining: Concepts and Techniques 2
Mining Association Rules in 
Large Databases
! Introduction to association rule mining
! Mining single-dimensional Boolean association rules 
from transactional databases
! Mining multilevel association rules from transactional 
databases
! Mining multidimensional association rules from 
transactional databases and data warehouse
! From association mining to correlation analysis
! Constraint-based association mining
! Summary
May 23, 2001 Data Mining: Concepts and Techniques 3
What Is Association Rule 
Mining?
! Association rule mining:
! Finding frequent patterns, associations, correlations, or 
causal structures among sets of items or objects in 
transaction databases, relational databases, and other 
information repositories.
! Applications:
! Basket data analysis, cross-marketing, catalog design, 
loss-leader analysis, clustering, classification, etc.
! Examples:
! Rule form:  Body → Ηead [support, confidence].
! buys(x, diapers) → buys(x, beers) [0.5%, 60%]
! major(x, CS) ^ takes(x, DB) →  grade(x, A) [1%, 
75%]
May 23, 2001 Data Mining: Concepts and Techniques 4
Association Rules: Basic Concepts
! Given: (1) database of transactions, (2) each transaction is 
a list of items (purchased by a customer in a visit)
! Find: all rules that correlate the presence of one set of 
items with that of another set of items
! E.g., 98% of people who purchase tires and auto 
accessories also get automotive services done
! Applications
! ? ⇒ Maintenance Agreement (What the store should 
do to boost Maintenance Agreement sales)
! Home Electronics ⇒ ? (What other products should 
the store stocks up?)
! Attached mailing in direct marketing
May 23, 2001 Data Mining: Concepts and Techniques 5
Association Rules: Definitions
! Set of items: I = {i1, i2, , im}
! Set of transactions: D = {d1, d2, , dn}
Each di ⊆ I
! An association rule: A ⇒ B
where A ⊂ I, B ⊂ I, A ∩ B = ∅
A
BI
 Means that to some extent A
implies B.
 Need to measure how strong the
implication is.
May 23, 2001 Data Mining: Concepts and Techniques 6
Association Rules: Definitions II
! The probability of a set A:
! k-itemset: tuple of items, or sets of items: 
 Example: {A,B} is a 2-itemset
 The probability of {A,B} is the probability of the set
A∪B, that is the fraction of transactions that contain
both A and B. Not the same as P(A∩B).
||
),(
)(
D
dAC
AP i
i∑= Where:


 ⊆
=
else0
if1
),(
YX
YXC
2
May 23, 2001 Data Mining: Concepts and Techniques 7
Association Rules: Definitions III
! Support of a rule A ⇒ B is the probability of the 
itemset {A,B}. This gives an idea of how often 
the rule is relevant.
! support(A ⇒ B ) = P({A,B})
! Confidence of a rule A ⇒ B is the conditional 
probability of B given A. This gives a measure 
of how accurate the rule is.
! confidence(A ⇒ B) = P(B|A) 
=  support({A,B}) / support(A)
May 23, 2001 Data Mining: Concepts and Techniques 8
Rule Measures: Support and 
Confidence
! Find all the rules X  ⇒ Y given 
thresholds for minimum confidence 
and minimum support.
! support, s, probability that a 
transaction contains {X, Y}
! confidence, c, conditional 
probability that a transaction 
having X also contains Y
Transaction ID Items Bought
2000 A,B,C
1000 A,C
4000 A,D
5000 B,E,F
With minimum support 50%, 
and minimum confidence 
50%, we have
! A ⇒ C  (50%, 66.6%)
! C ⇒ A  (50%, 100%)
Y:
Customer
buys diaper
Customer
buys both
X: Customer
buys beer
May 23, 2001 Data Mining: Concepts and Techniques 9
Association Rule Mining: A Road Map
! Boolean vs. quantitative associations (Based on the types of values 
handled)
! buys(x, SQLServer) ^ buys(x, DMBook) →  buys(x, DBMiner) 
[0.2%, 60%]
! age(x, 30..39) ^ income(x, 42..48K) →  buys(x, PC) [1%, 75%]
! Single dimension vs. multiple dimensional associations (see ex. Above)
! Single level vs. multiple-level analysis
! What brands of beers are associated with what brands of diapers?
! Various extensions and analysis
! Correlation, causality analysis
! Association does not necessarily imply correlation or causality
! Maxpatterns and closed itemsets
! Constraints enforced
! E.g., small sales (sum < 100) trigger big buys (sum > 1,000)?
May 23, 2001 Data Mining: Concepts and Techniques 10
Chapter 6: Mining Association 
Rules in Large Databases
! Association rule mining
! Mining single-dimensional Boolean association rules 
from transactional databases
! Mining multilevel association rules from transactional 
databases
! Mining multidimensional association rules from 
transactional databases and data warehouse
! From association mining to correlation analysis
! Constraint-based association mining
! Summary
May 23, 2001 Data Mining: Concepts and Techniques 11
Mining Association RulesAn Example
For rule A ⇒ C:
support = support({A, C }) = 50%
confidence = support({A, C })/support({A}) = 66.6%
The Apriori principle:
Any subset of a frequent itemset must be frequent
Transaction ID Items Bought
2000 A,B,C
1000 A,C
4000 A,D
5000 B,E,F
Frequent Itemset Support
{A} 75%
{B} 50%
{C} 50%
{A,C} 50%
Min. support 50%
Min. confidence 50%
May 23, 2001 Data Mining: Concepts and Techniques 12
Mining Frequent Itemsets: the 
Key Step
! Find the frequent itemsets: the sets of items that have 
at least a given minimum support
! A subset of a frequent itemset must also be a 
frequent itemset
! i.e., if {A, B} is a frequent itemset, both {A} and {B} 
should be a frequent itemset
! Iteratively find frequent itemsets with cardinality 
from 1 to k (k-itemset)
! Use the frequent itemsets to generate association 
rules.
3
May 23, 2001 Data Mining: Concepts and Techniques 13
The Apriori Algorithm
! Join Step: Ck is generated by joining Lk-1with itself
! Prune Step:  Any (k-1)-itemset that is not frequent cannot be 
a subset of a frequent k-itemset
! Pseudo-code:
Ck : Candidate itemset of size k
Lk : frequent itemset of size k
L1 = {frequent items}
for (k = 1; Lk !=∅; k++) do begin
Ck+1 = candidates generated from Lk
for each transaction t in database do
increment the count of all candidates in Ck+1
that are contained in t
Lk+1 = candidates in Ck+1 with min_support
end
return ∪k Lk;
May 23, 2001 Data Mining: Concepts and Techniques 14
The Apriori Algorithm  Example
TID Items
100 1 3 4
200 2 3 5
300 1 2 3 5
400 2 5
Database D itemset sup.
{1} 2
{2} 3
{3} 3
{4} 1
{5} 3
itemset sup.
{1} 2
{2} 3
{3} 3
{5} 3
Scan D
C1
L1
itemset
{1 2}
{1 3}
{1 5}
{2 3}
{2 5}
{3 5}
itemset sup
{1 2} 1
{1 3} 2
{1 5} 1
{2 3} 2
{2 5} 3
{3 5} 2
itemset sup
{1 3} 2
{2 3} 2
{2 5} 3
{3 5} 2
L2
C2 C2
Scan D
C3 L3itemset
{2 3 5}
Scan D itemset sup
{2 3 5} 2
May 23, 2001 Data Mining: Concepts and Techniques 15
How to do Generate Candidates?
! Suppose the items in Lk-1 are listed in an order
! Step 1: self-joining Lk-1
insert into Ck
select p.item1, p.item2, , p.itemk-1, q.itemk-1
from Lk-1 p, Lk-1 q
where p.item1=q.item1, , p.itemk-2=q.itemk-2, p.itemk-1 < 
q.itemk-1
! Step 2: pruning
forall itemsets c in Ck do
forall (k-1)-subsets s of c do
if (s is not in Lk-1) then delete c from Ck
May 23, 2001 Data Mining: Concepts and Techniques 16
How to Count Supports of 
Candidates?
! Why counting supports of candidates a problem?
! The total number of candidates can be very huge
! One transaction may contain many candidates
! Method:
! Candidate itemsets are stored in a hash-tree
! Leaf node of hash-tree contains a list of itemsets 
and counts
! Interior node contains a hash table
! Subset function: finds all the candidates contained in 
a transaction
May 23, 2001 Data Mining: Concepts and Techniques 17
Example of Generating Candidates
! L3={abc, abd, acd, ace, bcd}
! Self-joining: L3*L3
! abcd from abc and abd
! acde from acd and ace
! Pruning:
! acde is removed because ade is not in L3
! C4={abcd}
May 23, 2001 Data Mining: Concepts and Techniques 18
Methods to Improve Aprioris Efficiency
! Hash-based itemset counting: A k-itemset whose corresponding 
hashing bucket count is below the threshold cannot be frequent
! Transaction reduction: A transaction that does not contain any 
frequent k-itemset is useless in subsequent scans
! Partitioning: Any itemset that is potentially frequent in DB must be 
frequent in at least one of the partitions of DB
! Sampling: mining on a subset of given data, lower support 
threshold + a method to determine the completeness
! Dynamic itemset counting: add new candidate itemsets only when 
all of their subsets are estimated to be frequent
4
May 23, 2001 Data Mining: Concepts and Techniques 19
Is Apriori Fast Enough?  Performance 
Bottlenecks
! The core of the Apriori algorithm:
! Use frequent (k  1)-itemsets to generate candidate frequent k-
itemsets
! Use database scan and pattern matching to collect counts for the
candidate itemsets
! The bottleneck of Apriori: candidate generation
! Huge candidate sets:
! 104 frequent 1-itemset will generate 107 candidate 2-itemsets
! To discover a frequent pattern of size 100, e.g., {a1, a2, , 
a100}, one needs to generate 2100 ≈ 1030 candidates.
! Multiple scans of database: 
! Needs (n +1 ) scans, n is the length of the longest pattern
May 23, 2001 Data Mining: Concepts and Techniques 20
Mining Frequent Patterns Without 
Candidate Generation
! Compress a large database into a compact,  Frequent-
Pattern tree (FP-tree) structure
! highly condensed, but complete for frequent pattern 
mining
! avoid costly database scans
! Develop an efficient, FP-tree-based frequent pattern 
mining method
! A divide-and-conquer methodology: decompose mining 
tasks into smaller ones
! Avoid candidate generation: sub-database test only!
May 23, 2001 Data Mining: Concepts and Techniques 21
Presentation of Association Rules 
(Table Form )
May 23, 2001 Data Mining: Concepts and Techniques 22
Visualization of Association Rule Using Plane Graph
May 23, 2001 Data Mining: Concepts and Techniques 23
Visualization of Association Rule Using Rule Graph
May 23, 2001 Data Mining: Concepts and Techniques 24
Chapter 6: Mining Association 
Rules in Large Databases
! Association rule mining
! Mining single-dimensional Boolean association rules 
from transactional databases
! Mining multilevel association rules from transactional 
databases
! Mining multidimensional association rules from 
transactional databases and data warehouse
! From association mining to correlation analysis
! Constraint-based association mining
! Summary
5
May 23, 2001 Data Mining: Concepts and Techniques 25
Multiple-Level Association Rules
! Items often form hierarchy.
! Items at the lower level are 
expected to have lower 
support.
! Rules regarding itemsets at
appropriate levels could be 
quite useful.
! Transaction database can be 
encoded based on 
dimensions and levels
! We can explore shared multi-
level mining
Food
breadmilk
skim
SunsetFraser
2% whitewheat
TID Items
T1 {111, 121, 211, 221}
T2 {111, 211, 222, 323}
T3 {112, 122, 221, 411}
T4 {111, 121}
T5 {111, 122, 211, 221, 413}
May 23, 2001 Data Mining: Concepts and Techniques 26
Mining Multi-Level Associations
! A top_down, progressive deepening approach:
! First find high-level strong rules:
milk → bread  [20%, 60%].
! Then find their lower-level weaker rules:
2% milk → wheat bread [6%, 50%].
! Variations at mining multiple-level association rules.
! Level-crossed association rules:
2% milk → Wonder wheat bread
! Association rules with multiple, alternative 
hierarchies:
2% milk → Wonder bread
May 23, 2001 Data Mining: Concepts and Techniques 27
Multi-level Association: Uniform 
Support vs. Reduced Support
! Uniform Support: the same minimum support for all levels
! + One minimum support threshold.   No need to examine itemsets
containing any item whose ancestors do not have minimum 
support.
!  Lower level items do not occur as frequently. If support 
threshold 
! too high ⇒ miss low level associations
! too low ⇒ generate too many high level associations
! Reduced Support: reduced minimum support at lower levels
! There are 4 search strategies:
! Level-by-level independent
! Level-cross filtering by k-itemset
! Level-cross filtering by single item
! Controlled level-cross filtering by single item
May 23, 2001 Data Mining: Concepts and Techniques 28
Uniform Support
Multi-level mining with uniform support
Milk
[support = 10%]
2% Milk
[support = 6%]
Skim Milk
[support = 4%]
Level 1
min_sup = 5%
Level 2
min_sup = 5%
Back
May 23, 2001 Data Mining: Concepts and Techniques 29
Reduced Support
Multi-level mining with reduced support
2% Milk
[support = 6%]
Skim Milk
[support = 4%]
Level 1
min_sup = 5%
Level 2
min_sup = 3%
Back
Milk
[support = 10%]
May 23, 2001 Data Mining: Concepts and Techniques 30
Multi-level Association: Redundancy 
Filtering
! Some rules may be redundant due to ancestor 
relationships between items.
! Example
! milk ⇒ wheat bread    [support = 8%, confidence = 70%]
! 2% milk ⇒ wheat bread [support = 2%, confidence = 72%]
! We say the first rule is an ancestor of the second rule.
! A rule is redundant if its support is close to the expected 
value, based on the rules ancestor.
6
May 23, 2001 Data Mining: Concepts and Techniques 31
Multi-Level Mining: Progressive 
Deepening
! A top-down, progressive deepening approach:
! First mine high-level frequent items:
milk (15%), bread (10%)
! Then mine their lower-level weaker frequent 
itemsets:
2% milk (5%), wheat bread (4%)
! Different min_support threshold across multi-levels 
lead to different algorithms:
! If adopting the same min_support across multi-
levels
then toss t if any of ts ancestors is infrequent.
! If adopting reduced min_support at lower levels
then examine only those descendents whose ancestors 
support is frequent/non-negligible.
May 23, 2001 Data Mining: Concepts and Techniques 32
Progressive Refinement of Data 
Mining Quality
! Why progressive refinement?
! Mining operator can be expensive or cheap, fine or 
rough
! Trade speed with quality: step-by-step refinement.
! Superset coverage property: 
! Preserve all the positive answersallow a positive false 
test but not a false negative test.
! Two- or multi-step mining:
! First apply rough/cheap operator (superset coverage)
! Then apply expensive algorithm on a substantially 
reduced candidate set (Koperski & Han, SSD95).
May 23, 2001 Data Mining: Concepts and Techniques 33
Chapter 6: Mining Association 
Rules in Large Databases
! Association rule mining
! Mining single-dimensional Boolean association rules 
from transactional databases
! Mining multilevel association rules from transactional 
databases
! Mining multidimensional association rules from 
transactional databases and data warehouse
! From association mining to correlation analysis
! Constraint-based association mining
! Summary
May 23, 2001 Data Mining: Concepts and Techniques 34
Multi-Dimensional Association: 
Concepts
! Single-dimensional rules:
buys(X, milk)  ⇒ buys(X, bread)
! Multi-dimensional rules: " 2 dimensions or predicates
! Inter-dimension association rules (no repeated 
predicates)
age(X,19-25)  ∧ occupation(X,student) ⇒ buys(X,coke)
! hybrid-dimension association rules (repeated predicates)
age(X,19-25) ∧ buys(X, popcorn) ⇒ buys(X, coke)
! Categorical Attributes
! finite number of possible values, no ordering among 
values
! Quantitative Attributes
! numeric, implicit ordering among values
May 23, 2001 Data Mining: Concepts and Techniques 35
Techniques for Mining MD 
Associations
! Search for frequent k-predicate set:
! Example: {age, occupation, buys} is a 3-predicate 
set.
! Techniques can be categorized by how age are 
treated.
1. Using static discretization of quantitative attributes
! Quantitative attributes are statically discretized by 
using predefined concept hierarchies.
2. Quantitative association rules
! Quantitative attributes are dynamically discretized 
into bins based on the distribution of the data.
3. Distance-based association rules
! This is a dynamic discretization process that 
considers the distance between data points.
May 23, 2001 Data Mining: Concepts and Techniques 36
Static Discretization of Quantitative 
Attributes
! Discretized prior to mining using concept hierarchy.
! Numeric values are replaced by ranges.
! In relational database, finding all frequent k-predicate sets 
will require k or k+1 table scans.
! Data cube is well suited for mining.
! The cells of an n-dimensional 
cuboid correspond to the 
predicate sets.
! Mining from data cubes
can be much faster.
(income)(age)
()
(buys)
(age, income) (age,buys) (income,buys)
(age,income,buys)
7
May 23, 2001 Data Mining: Concepts and Techniques 37
Quantitative Association Rules
age(X,”30-34”) ∧ income(X,”24K -
48K”)
⇒ buys(X,”high resolution TV”)
! Numeric attributes are dynamically discretized
! Such that the confidence or compactness of the rules 
mined is maximized.
! 2-D quantitative association rules: Aquan1 ∧ Aquan2 ⇒ Acat
! Cluster adjacent 
association rules
to form general 
rules using a 2-D 
grid.
! Example:
May 23, 2001 Data Mining: Concepts and Techniques 38
ARCS (Association Rule Clustering 
System)
How does ARCS work?
1. Binning
2. Find frequent
predicateset
3. Clustering
4. Optimize
May 23, 2001 Data Mining: Concepts and Techniques 39
Limitations of ARCS
! Only quantitative attributes on LHS of rules.
! Only 2 attributes on LHS.  (2D limitation)
! An alternative to ARCS
! Non-grid-based
! equi-depth binning
! clustering based on a measure of partial 
completeness.
! Mining Quantitative Association Rules in 
Large Relational Tables by R. Srikant and R. 
Agrawal.
May 23, 2001 Data Mining: Concepts and Techniques 40
Chapter 6: Mining Association 
Rules in Large Databases
! Association rule mining
! Mining single-dimensional Boolean association rules 
from transactional databases
! Mining multilevel association rules from transactional 
databases
! Mining multidimensional association rules from 
transactional databases and data warehouse
! From association mining to correlation analysis
! Constraint-based association mining
! Summary
May 23, 2001 Data Mining: Concepts and Techniques 41
Interestingness Measurements
! Objective measures
Two popular measurements: 
# support; and 
$ confidence
! Subjective  measures (Silberschatz & Tuzhilin, 
KDD95)
A rule (pattern) is interesting if
# it is unexpected (surprising to the user); 
and/or
$ actionable (the user can do something with it)
May 23, 2001 Data Mining: Concepts and Techniques 42
Criticism to Support and Confidence
! Example 1: (Aggarwal & Yu, PODS98)
! Among 5000 students
! 3000 play basketball
! 3750 eat cereal
! 2000 both play basket ball and eat cereal
! play basketball ⇒ eat cereal [40%, 66.7%]  is misleading 
because the overall percentage of students eating cereal is 75% 
which is higher than 66.7%.
! play basketball ⇒ not eat cereal [20%, 33.3%] is far more 
accurate, although with lower support and confidence
basketball not basketball sum(row)
cereal 2000 1750 3750
not cereal 1000 250 1250
sum(col.) 3000 2000 5000
8
May 23, 2001 Data Mining: Concepts and Techniques 43
Criticism to Support and Confidence 
(Cont.)
! Example 2:
! X and Y: positively correlated,
! X and Z, negatively related
! support and confidence of 
X=>Z dominates 
! We need a measure of dependent 
or correlated events
! P(B|A)/P(B) is also called the lift
of rule A => B
X 1 1 1 1 0 0 0 0
Y 1 1 0 0 0 0 0 0
Z 0 1 1 1 1 1 1 1
Rule Support Confidence
X=>Y 25% 50%
X=>Z 37.50% 75%)()(
)(
, BPAP
BAP
corr BA
U=
May 23, 2001 Data Mining: Concepts and Techniques 44
Other Interestingness Measures
! Interest (correlation, lift)
! taking both P(A) and P(B) in consideration
! P(A^B)=P(B)*P(A), if A and B are independent events
! A and B negatively correlated, if the value is less than 1; 
otherwise A and B positively correlated
)()(
)(
BPAP
BAP ∧
X 1 1 1 1 0 0 0 0
Y 1 1 0 0 0 0 0 0
Z 0 1 1 1 1 1 1 1
Itemset Support Interest
X,Y 25% 2
X,Z 37.50% 0.9
Y,Z 12.50% 0.57
May 23, 2001 Data Mining: Concepts and Techniques 45
Chapter 6: Mining Association 
Rules in Large Databases
! Association rule mining
! Mining single-dimensional Boolean association rules 
from transactional databases
! Mining multilevel association rules from transactional 
databases
! Mining multidimensional association rules from 
transactional databases and data warehouse
! From association mining to correlation analysis
! Constraint-based association mining
! Summary
May 23, 2001 Data Mining: Concepts and Techniques 46
Constraint-Based Mining
! Interactive, exploratory mining giga-bytes of data?  
! Could it be real?  Making good use of constraints!
! What kinds of constraints can be used in mining?
! Knowledge type constraint: classification, association, 
etc.
! Data constraint: SQL-like queries 
! Find product pairs sold together in Vancouver in Dec.98.
! Dimension/level constraints:
! in relevance to region, price, brand, customer category.
! Rule constraints
! small sales (price  < $10) triggers big sales (sum > $200).
! Interestingness constraints:
! strong rules (min_support ≥ 3%, min_confidence ≥ 60%).
May 23, 2001 Data Mining: Concepts and Techniques 47
Rule Constraints in Association Mining
! Two kind of rule constraints:
! Rule form constraints: meta-rule guided mining.
! P(x, y) ^ Q(x, w) →   takes(x, database systems).
! Rule (content) constraint: constraint-based query 
optimization (Ng, et al., SIGMOD98).
! sum(LHS) < 100 ^ min(LHS) > 20 ^ count(LHS) > 3 ^ sum(RHS) > 
1000
! 1-variable vs. 2-variable constraints (Lakshmanan, et al. 
SIGMOD99): 
! 1-var: A constraint confining only one side (L/R) of the 
rule, e.g., as shown above. 
! 2-var: A constraint confining both sides (L and R).
! sum(LHS) < min(RHS) ^ max(RHS) < 5* sum(LHS)
May 23, 2001 Data Mining: Concepts and Techniques 48
Constrained Association Query 
Optimization Problem
! Given a CAQ = { (S1, S2) | C }, the algorithm should be :
! sound: It only finds frequent sets that satisfy the 
given constraints C
! complete: All frequent sets satisfy the given 
constraints C are found
! A naïve solution:
! Apply Apriori for finding all frequent sets, and then
to test them for constraint satisfaction one by one.
! More advanced approach:
! Comprehensive analysis of the properties of 
constraints and try to push them as deeply as 
possible inside the frequent set computation.
9
May 23, 2001 Data Mining: Concepts and Techniques 49
Summary
! Association rules offer an efficient way to mine 
interesting probabilities about data in very large 
databases.
! Can be dangerous when mis-interpreted as signs 
of statistically significant causality.
! The basic Apriori algorithm and its extensions 
allow the user to gather a good deal of 
information without too many passes through 
data.
May 23, 2001 Data Mining: Concepts and Techniques 50
Appendix A: FP-growth
! FP-growth offers significant speed up over 
Apriori.
May 23, 2001 Data Mining: Concepts and Techniques 51
Benefits of the FP-tree Structure
! Completeness: 
! never breaks a long pattern of any transaction
! preserves complete information for frequent pattern 
mining
! Compactness
! reduce irrelevant informationinfrequent items are gone
! frequency descending ordering: more frequent items are 
more likely to be shared
! never be larger than the original database (if not count 
node-links and counts)
! Example: For Connect-4 DB, compression ratio could be 
over 100
May 23, 2001 Data Mining: Concepts and Techniques 52
Construct FP-tree from a 
Transaction DB
{}
f:4 c:1
b:1
p:1
b:1c:3
a:3
b:1m:2
p:2 m:1
Header Table
Item frequency head
f 4
c 4
a 3
b 3
m 3
p 3
min_support = 0.5
TID Items bought (ordered) frequent items
100 {f, a, c, d, g, i, m, p} {f, c, a, m, p}
200 {a, b, c, f, l, m, o} {f, c, a, b, m}
300 {b, f, h, j, o} {f, b}
400 {b, c, k, s, p} {c, b, p}
500 {a, f, c, e, l, p, m, n} {f, c, a, m, p}
Steps:
1. Scan DB once, find frequent 
1-itemset (single item 
pattern)
2. Order frequent items in 
frequency descending order
3. Scan DB again, construct 
FP-tree
May 23, 2001 Data Mining: Concepts and Techniques 53
Mining Frequent Patterns Using FP-tree
! General idea (divide-and-conquer)
! Recursively grow frequent pattern path using the FP-
tree
! Method 
! For each item, construct its conditional pattern-base, 
and then its conditional FP-tree
! Repeat the process on each newly created conditional 
FP-tree 
! Until the resulting FP-tree is empty, or it contains only 
one path (single path will generate all the combinations of its 
sub-paths, each of which is a frequent pattern)
May 23, 2001 Data Mining: Concepts and Techniques 54
Major Steps to Mine FP-tree
1) Construct conditional pattern base for each node in the 
FP-tree
2) Construct conditional FP-tree from each conditional 
pattern-base
3) Recursively mine conditional FP-trees and grow 
frequent patterns obtained so far
% If the conditional FP-tree contains a single path, 
simply enumerate all the patterns
10
May 23, 2001 Data Mining: Concepts and Techniques 55
Step 1: From FP-tree to Conditional 
Pattern Base
! Starting at the frequent header table in the FP-tree
! Traverse the FP-tree by following the link of each frequent item
! Accumulate all of transformed prefix paths of that item to form a 
conditional pattern base
Conditional pattern bases
item cond. pattern base
c f:3
a fc:3
b fca:1, f:1, c:1
m fca:2, fcab:1
p fcam:2, cb:1
{}
f:4 c:1
b:1
p:1
b:1c:3
a:3
b:1m:2
p:2 m:1
Header Table
Item frequency head
f 4
c 4
a 3
b 3
m 3
p 3
May 23, 2001 Data Mining: Concepts and Techniques 56
Properties of FP-tree for Conditional 
Pattern Base Construction
! Node-link property
! For any frequent item ai, all the possible frequent 
patterns that contain ai can be obtained by following 
ai's node-links, starting from ai's head in the FP-tree 
header
! Prefix path property
! To calculate the frequent patterns for a node ai in a 
path P, only the prefix sub-path of ai in P need to be 
accumulated, and its frequency count should carry the 
same count as node ai.
May 23, 2001 Data Mining: Concepts and Techniques 57
Step 2: Construct Conditional FP-tree
! For each pattern-base
! Accumulate the count for each item in the base
! Construct the FP-tree for the frequent items of the 
pattern base
m-conditional pattern
base:
fca:2, fcab:1
{}
f:3
c:3
a:3
m-conditional FP-tree
All frequent patterns
concerning m
m,
fm, cm, am,
fcm, fam, cam,
fcam
! !
{}
f:4 c:1
b:1
p:1
b:1c:3
a:3
b:1m:2
p:2 m:1
Header Table
Item frequency head
f 4
c 4
a 3
b 3
m 3
p 3
May 23, 2001 Data Mining: Concepts and Techniques 58
Mining Frequent Patterns by Creating 
Conditional Pattern-Bases
EmptyEmptyf
{(f:3)}|c{(f:3)}c
{(f:3, c:3)}|a{(fc:3)}a
Empty{(fca:1), (f:1), (c:1)}b
{(f:3, c:3, a:3)}|m{(fca:2), (fcab:1)}m
{(c:3)}|p{(fcam:2), (cb:1)}p
Conditional FP-treeConditional pattern-baseItem
May 23, 2001 Data Mining: Concepts and Techniques 59
Step 3: Recursively mine the 
conditional FP-tree
{}
f:3
c:3
a:3
m-conditional FP-tree
Cond. pattern base of am: (fc:3)
{}
f:3
c:3
am-conditional FP-tree
Cond. pattern base of cm: (f:3)
{}
f:3
cm-conditional FP-tree
Cond. pattern base of cam: (f:3)
{}
f:3
cam-conditional FP-tree
May 23, 2001 Data Mining: Concepts and Techniques 60
Single FP-tree Path Generation
! Suppose an FP-tree T has a single path P
! The complete set of frequent pattern of T can be 
generated by enumeration of all the combinations of the 
sub-paths of P
{}
f:3
c:3
a:3
m-conditional FP-tree
All frequent patterns
concerning m
m,
fm, cm, am,
fcm, fam, cam,
fcam
!
11
May 23, 2001 Data Mining: Concepts and Techniques 61
Principles of Frequent Pattern 
Growth
! Pattern growth property
! Let α be a frequent itemset in DB, B be α's 
conditional pattern base, and β be an itemset in B.  
Then α ∪ β is a frequent itemset in DB iff β is 
frequent in B.  
! abcdef  is a frequent pattern, if and only if
! abcde  is a frequent pattern, and
! f  is frequent in the set of transactions containing 
abcde 
May 23, 2001 Data Mining: Concepts and Techniques 62
Why Is Frequent Pattern Growth
Fast?
! Our performance study shows
! FP-growth is an order of magnitude faster than 
Apriori, and is also faster than tree-projection
! Reasoning
! No candidate generation, no candidate test
! Use compact data structure
! Eliminate repeated database scan
! Basic operation is counting and FP-tree building
May 23, 2001 Data Mining: Concepts and Techniques 63
FP-growth vs. Apriori: Scalability With 
the Support Threshold
0
10
20
30
40
50
60
70
80
90
100
0 0.5 1 1.5 2 2.5 3
Support threshold(%)
R
un
tim
e(
se
c.
)
D1 FP-grow th runtime
D1 Apriori runtime
Data set T25I20D10K
May 23, 2001 Data Mining: Concepts and Techniques 64
FP-growth vs. Tree-Projection: Scalability 
with Support Threshold
0
20
40
60
80
100
120
140
0 0.5 1 1.5 2
Support threshold (%)
R
u
n
ti
m
e
(s
ec
.)
D2 FP-growth
D2 TreeProjection
Data set T25I20D100K
May 23, 2001 Data Mining: Concepts and Techniques 65
References
! R. Agarwal, C. Aggarwal, and V. V. V. Prasad. A tree projection algorithm for generation of frequent
itemsets. In Journal of Parallel and Distributed Computing (Special Issue on High Performance Data 
Mining), 2000.
! R. Agrawal, T. Imielinski, and A. Swami.  Mining association rules between sets of items in large 
databases.  SIGMOD'93, 207-216, Washington, D.C.
! R. Agrawal and R. Srikant. Fast algorithms for mining association rules. VLDB'94 487-499, Santiago, 
Chile.
! R. Agrawal and R. Srikant. Mining sequential patterns. ICDE'95, 3-14, Taipei, Taiwan. 
! R. J. Bayardo. Efficiently mining long patterns from databases. SIGMOD'98, 85-93, Seattle, 
Washington.
! S. Brin, R. Motwani, and C. Silverstein.  Beyond market basket: Generalizing association rules to 
correlations. SIGMOD'97, 265-276, Tucson, Arizona.
! S. Brin, R. Motwani, J. D. Ullman, and S. Tsur. Dynamic itemset counting and implication rules for 
market basket analysis. SIGMOD'97, 255-264, Tucson, Arizona, May 1997.
! K. Beyer and R. Ramakrishnan. Bottom-up computation of sparse and iceberg cubes. SIGMOD'99, 359-
370, Philadelphia, PA, June 1999.
! D.W. Cheung, J. Han, V. Ng, and C.Y. Wong. Maintenance of discovered association rules in large 
databases: An incremental updating technique. ICDE'96, 106-114, New Orleans,  LA.
! M. Fang, N. Shivakumar, H. Garcia-Molina, R. Motwani, and J. D. Ullman. Computing iceberg queries 
efficiently. VLDB'98, 299-310, New York, NY, Aug. 1998.
May 23, 2001 Data Mining: Concepts and Techniques 66
References (2)
! G. Grahne, L. Lakshmanan, and X. Wang. Efficient mining of constrained correlated sets. ICDE'00, 512-
521, San Diego, CA, Feb. 2000.
! Y. Fu and J. Han.  Meta-rule-guided mining of association rules in relational databases.  KDOOD'95, 
39-46, Singapore, Dec. 1995.
! T. Fukuda, Y. Morimoto, S. Morishita, and T. Tokuyama. Data mining using two-dimensional optimized 
association rules: Scheme, algorithms, and visualization. SIGMOD'96, 13-23, Montreal, Canada.
! E.-H. Han, G. Karypis, and V. Kumar. Scalable parallel data mining for association rules. SIGMOD'97, 
277-288, Tucson, Arizona.
! J. Han, G. Dong, and Y. Yin. Efficient mining of partial periodic patterns in time series database. 
ICDE'99, Sydney, Australia.
! J. Han and Y. Fu. Discovery of multiple-level association rules from large databases. VLDB'95, 420-431, 
Zurich, Switzerland.
! J. Han, J. Pei, and Y. Yin. Mining frequent patterns without candidate generation. SIGMOD'00, 1-12, 
Dallas, TX, May 2000.
! T. Imielinski and H. Mannila. A database perspective on knowledge discovery. Communications of ACM, 
39:58-64, 1996.
! M. Kamber, J. Han, and J. Y. Chiang. Metarule-guided mining of multi-dimensional association rules 
using data cubes. KDD'97, 207-210, Newport Beach, California.
! M. Klemettinen, H. Mannila, P. Ronkainen, H. Toivonen, and A.I. Verkamo. Finding interesting rules 
from large sets of discovered association rules. CIKM'94, 401-408, Gaithersburg, Maryland.
12
May 23, 2001 Data Mining: Concepts and Techniques 67
References (3)
! F. Korn, A. Labrinidis, Y. Kotidis, and C. Faloutsos. Ratio rules: A new paradigm for fast, quantifiable 
data mining. VLDB'98, 582-593, New York, NY.
! B. Lent, A. Swami, and J. Widom.  Clustering association rules. ICDE'97, 220-231, Birmingham, 
England.
! H. Lu, J. Han, and L. Feng. Stock movement and n-dimensional inter-transaction association rules.  
SIGMOD Workshop on Research Issues on Data Mining and Knowledge Discovery (DMKD'98), 12:1-
12:7, Seattle, Washington.
! H. Mannila, H. Toivonen, and A. I. Verkamo. Efficient algorithms for discovering association rules. 
KDD'94, 181-192, Seattle, WA, July 1994.
! H. Mannila, H Toivonen, and A. I. Verkamo. Discovery of frequent episodes in event sequences. Data 
Mining and Knowledge Discovery, 1:259-289, 1997.
! R. Meo, G. Psaila, and S. Ceri. A new SQL-like operator for mining association rules. VLDB'96, 122-
133, Bombay, India.
! R.J. Miller and Y. Yang.  Association rules over interval data. SIGMOD'97, 452-461, Tucson, Arizona.
! R. Ng, L. V. S. Lakshmanan, J. Han, and A. Pang. Exploratory mining and pruning optimizations of 
constrained associations rules. SIGMOD'98, 13-24, Seattle, Washington.
! N. Pasquier, Y. Bastide, R. Taouil, and L. Lakhal. Discovering frequent closed itemsets for association 
rules. ICDT'99, 398-416, Jerusalem, Israel, Jan. 1999.
May 23, 2001 Data Mining: Concepts and Techniques 68
References (4)
! J.S. Park, M.S. Chen, and P.S. Yu. An effective hash-based algorithm for mining association rules. 
SIGMOD'95, 175-186, San Jose, CA, May 1995.
! J. Pei, J. Han, and R. Mao.  CLOSET: An Efficient Algorithm for Mining Frequent Closed Itemsets. 
DMKD'00, Dallas, TX, 11-20, May 2000.
! J. Pei and J. Han.  Can We Push More Constraints into Frequent Pattern Mining? KDD'00.  Boston, 
MA.  Aug. 2000.
! G. Piatetsky-Shapiro. Discovery, analysis, and presentation of strong rules. In G. Piatetsky-Shapiro 
and W. J. Frawley, editors, Knowledge Discovery in Databases, 229-238. AAAI/MIT Press, 1991.
! B. Ozden, S. Ramaswamy, and A. Silberschatz. Cyclic association rules. ICDE'98, 412-421, Orlando, 
FL.
! J.S. Park, M.S. Chen, and P.S. Yu. An effective hash-based algorithm for mining association rules. 
SIGMOD'95, 175-186, San Jose, CA.
! S. Ramaswamy, S. Mahajan, and A. Silberschatz. On the discovery of interesting patterns in 
association rules. VLDB'98, 368-379, New York, NY..
! S. Sarawagi, S. Thomas, and R. Agrawal. Integrating association rule mining with relational database 
systems: Alternatives and implications.  SIGMOD'98, 343-354, Seattle, WA.
! A. Savasere, E. Omiecinski, and S. Navathe. An efficient algorithm for mining association rules in 
large databases. VLDB'95, 432-443, Zurich, Switzerland.
! A. Savasere, E. Omiecinski, and S. Navathe. Mining for strong negative associations in a large 
database of customer transactions. ICDE'98, 494-502, Orlando, FL, Feb. 1998.
May 23, 2001 Data Mining: Concepts and Techniques 69
References (5)
! C. Silverstein, S. Brin, R. Motwani, and J. Ullman. Scalable techniques for mining causal 
structures.  VLDB'98, 594-605, New York, NY.
! R. Srikant and R. Agrawal. Mining generalized association rules. VLDB'95, 407-419, Zurich, 
Switzerland, Sept. 1995.
! R. Srikant and R. Agrawal. Mining quantitative association rules in large relational tables. 
SIGMOD'96, 1-12, Montreal, Canada.
! R. Srikant, Q. Vu, and R. Agrawal. Mining association rules with item constraints. KDD'97, 67-73, 
Newport Beach, California.
! H. Toivonen.  Sampling large databases for association rules.  VLDB'96, 134-145, Bombay, India, 
Sept. 1996.
! D. Tsur, J. D. Ullman, S. Abitboul, C. Clifton, R. Motwani, and S. Nestorov. Query flocks:  A 
generalization of association-rule mining. SIGMOD'98, 1-12, Seattle, Washington.
! K. Yoda, T. Fukuda, Y. Morimoto, S. Morishita, and T. Tokuyama. Computing optimized 
rectilinear regions for association rules. KDD'97, 96-103, Newport Beach, CA, Aug. 1997.
! M. J. Zaki, S. Parthasarathy, M. Ogihara, and W. Li. Parallel algorithm for discovery of 
association rules. Data Mining and Knowledge Discovery, 1:343-374, 1997.
! M. Zaki.  Generating Non-Redundant Association Rules.  KDD'00.  Boston, MA.  Aug. 2000.
! O. R. Zaiane, J. Han, and H. Zhu.  Mining Recurrent Items in Multimedia with Progressive 
Resolution Refinement.  ICDE'00, 461-470, San Diego, CA, Feb. 2000.

