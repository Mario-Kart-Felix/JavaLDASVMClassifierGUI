Spectrally-normalized margin bounds for neural networks
Peter Bartlett∗ Dylan J. Foster† Matus Telgarsky‡
Abstract
This paper presents a margin-based multiclass generalization bound for neural networks which
scales with their margin-normalized spectral complexity : their Lipschitz constant, meaning the
product of the spectral norms of the weight matrices, times a certain correction factor. This bound is
empirically investigated for a standard AlexNet network on the mnist and cifar10 datasets, with
both original and random labels, where it tightly correlates with the observed excess risks.
1 Overview
Neural networks owe their astonishing success not only to their ability to fit any data set: they also
generalize well, meaning they provide a close fit on unseen data. A classical statistical adage is that
models capable of fitting too much will generalize poorly; what’s going on here?
Let’s navigate the many possible explanations provided by statistical theory. A first observation is
that any analysis based solely on the number of possible labellings on a finite training set — as is the
case with VC dimension — is doomed: if the function class can fit all possible labels (as is the case with
neural networks in standard configurations (Zhang et al., 2017)), then this analysis can not distinguish it
from the collection of all possible functions!
epoch 10 epoch 100
excess risk 0.3
excess risk 0.9
cifar excess risk
cifar Lipschitz
cifar Lipschitz/margin
cifar [random] excess risk
cifar [random] Lipschitz
Figure 1: An analysis of AlexNet (Krizhevsky et al., 2012) trained on cifar10, both with original and
with random labels. Triangle-marked curves track excess risk across training epochs (on a log scale),
with an ‘x’ marking the earliest epoch with zero training error. Circle-marked curves track Lipschitz
constants, normalized so that the two curves for random labels meet. The Lipschitz constants tightly
correlate with excess risk, and moreover normalizing them by margins (resulting in the square-marked
curve) neutralizes growth across epochs.
∗<peter@berkeley.edu>; University of California, Berkeley; work performed while visiting the Simons Institute.
†<djf244@cornell.edu>; Cornell University; work performed while visiting the Simons Institute.
‡<mjt@illinois.edu>; University of Illinois, Urbana-Champaign; work performed while visiting the Simons Institute.
1
ar
X
iv
:1
70
6.
08
49
8v
1 
 [
cs
.L
G
] 
 2
6 
Ju
n 
20
17
Next let’s consider scale-sensitive measures of complexity, such as Rademacher complexity and
metric entropy, which work directly with real-valued function classes, and moreover are sensitive to
their magnitudes. Figure 1 plots the excess risk (the test error minus the training error) across training
epochs against one candidate scale-sensitive complexity measure, the Lipschitz constant of the network
(the product of the spectral norms of their weight matrices), and demonstrates that they are tightly
correlated (which is not the case for, say, the l2 norm of the weights). The data considered in Figure 1
is the standard cifar10 dataset, both with original and with random labels, which has been used as a
sanity check when analyzing neural network generalization (Zhang et al., 2017).
There is still an issue with basing a complexity measure purely on the Lipschitz constant (although it
has already been successfully employed to regularize neural networks (Cisse et al., 2017)): as depicted in
Figure 1, the measure grows over time, despite the excess risk plateauing. Fortunately, there is a standard
resolution to this issue: investigating the margins (a precise measure of confidence) of the outputs of the
network. This tool has been used to study the behavior of 2-layer networks, boosting methods, SVMs,
and many others (Bartlett, 1996; Schapire et al., 1997; Boucheron et al., 2005); in boosting, for instance,
there is a similar growth in complexity over time (each training iteration adds a weak learner), whereas
margin bounds correctly stay flat or even decrease. This behavior is recovered here: as depicted in
Figure 1, even though standard networks exhibit growing Lipschitz constants, normalizing these Lipschitz
constants by the margin instead gives a decaying curve.
1.1 Contributions
This work investigates a complexity measure for neural networks which is based on the Lipschitz constant,
but normalized by the margin of the predictor. The two central contributions are as follows.
• Theorem 1.1 below will give the rigorous statement of the generalization bound which is the basis of
this work. In contrast to prior work, this bound: (a) scales with the Lipschitz constant (product of
spectral norms of weight matrices) divided by the margin; (b) has no dependence on combinatorial
parameters (e.g., number of layers or nodes) outside of log factors; (c) is multiclass (with no explicit
dependence on the number of classes); (d) measures complexity against a reference network (e.g.,
for the ResNet (He et al., 2016), the reference network has identity mappings at each layer). The
bound is stated below, with a general form and analysis summary appearing in Section 3, the full
details relegated to the appendix.
• An empirical investigation, in Section 2, of neural network generalization on the standard datasets
cifar10, cifar100, and mnist using the preceding bound. Rather than using the bound to provide
a single number, it can be used to form a margin distribution as in Figure 2. These margin
distributions will illuminate the following intuitive observations: (a) cifar10 is harder than mnist;
(b) random labels make cifar10 and mnist much more difficult; (c) the margin distributions
(and bounds) converge during training, even though the weight matrices continue to grow; (d) l2
regularization (“weight decay”) does not significantly impact margins or generalization.
A more detailed description of the margin distributions is as follows. Suppose a neural network computes
a function f : Rd → Rk, where k is the number of classes; the most natural way to convert this to a
classifier is to select the output coordinate with the largest magnitude, meaning x 7→ arg maxj f(x)j . The
margin, then, is to measure the gap between the output for the correct label and other labels, meaning
f(x)y −maxj 6=y f(x)j .
Unfortunately, margins alone do not seem to say much; see for instance Figure 2a, where the collections
of all margins for all data points — the unnormalized margin distribution — are similar for cifar10
with and without random labels. What is missing is an appropriate normalization, as in Figure 2b. This
normalization is provided by Theorem 1.1, which can now be explained in detail.
To state the bound, a little bit of notation is necessary. The networks will use L fixed nonlinearities
(σ1, . . . , σL), where σi is ρi-Lipschitz (e.g., as with coordinate-wise ReLU, and max-pooling, as discussed
in Appendix A.1); occasionally, it will also hold that σi(0) = 0. Given L weight matrices A = (A1, . . . , AL)
2
0
cifar
random
(a) Margins.
0
cifar
cifar random
(b) Normalized margins.
Figure 2: Margin distributions at the end of training AlexNet on cifar10, with and without random
labels. With proper normalization, random labels demonstrably correspond to a harder problem.
let FA denote the function computed by the corresponding network:
FA(x) := σL(ALσL−1(AL−1 · · ·σ1(A1x) · · · )).
Whenever data (x1, . . . , xn) are given, collect them as rows of a matrix X ∈ Rn×d. Occasionally, notation
will be overloaded to discuss FA(XT ), a matrix whose ith column is FA(xi). The l2 norm ‖ · ‖2 is always
computed entry-wise; thus, for a matrix, it corresponds to the Frobenius norm.
Next, define a collection of reference matrices (M1, . . . ,ML) with each dimension at most W ; for
instance, to obtain a good bound for ResNet (He et al., 2016), it is sensible to set Mi := I, the identity
map, and the bound below will worsen as the network moves farther from the identity map; for AlexNet
(Krizhevsky et al., 2012), the simple choice Mi = 0 suffices. Finally, letting ‖ · ‖σ and ‖ · ‖1 respectively
denote spectral norm and the unrolled l1 vector norm, the spectral complexity RFA = RA of a network
FA with weights A is
RA :=
 L∏
i=1
ρi‖Ai‖σ
 L∑
i=1
‖Ai −Mi‖2/31
‖Ai‖2/3σ
3/2 . (1.1)
The following theorem provides a generalization bound for neural networks whose nonlinearities are
fixed but whose weight matrices A have bounded spectral complexity RA.
Theorem 1.1. Let nonlinearities (σ1, . . . , σL) and reference matrices (M1, . . . ,ML) be given as above
(i.e., σi is ρi-Lipschitz and σi(0) = 0). Then with probability at least 1− δ over an iid draw of n examples
((xi, yi))
n
i=1, every margin γ > 0 and network FA : Rd → Rk with weight matrices A = (A1, . . . , AL)
satisfy
Pr
[
arg max
j
FA(x)j 6= y
]
≤ R̂γ(FA) + Õ
(
‖X‖2RA
γn
ln(n) ln(W ) +
√
ln(1/δ)
n
)
,
where R̂γ(f) ≤ n−1
∑
i 1
[
f(xi)yi ≤ γ + maxj 6=yi f(xi)j
]
and ‖X‖2 =
√∑
i ‖xi‖22.
The full proof (based on metric entropy) is relegated to the appendix, but a sketch is provided in
Section 3, along with a more general form (not limited to spectral norms), along with a (non-matching!)
lower bound. Section 3 also gives a discussion of related work, but briefly it’s essential to note that
margin and Lipschitz-sensitive bounds have a long history in the neural networks literature (Bartlett,
1996; Anthony and Bartlett, 1999; Neyshabur et al., 2015); the distinction here is the sensitivity to
specifically the spectral norm, as well as no explicit appearance of combinatorial quantities such as
numbers of parameters or layers (outside of log terms, and indices to summations and products).
To close, miscellaneous observations and open problems are collected in Section 4.
3
0
cifar
cifar random
mnist
(a) Mnist is easier than cifar10.
0
cifar
cifar random
mnist random
(b) Random mnist is as hard as random cifar10!
0
cifar
cifar random
cifar100
(c) cifar100 is as hard as cifar10 with random labels!
0
cifar
random label
random input
(d) Random inputs are harder than random labels.
Figure 3: A variety of margin distributions. Axes are re-scaled in Figure 3a, but identical in the other
subplots; the cifar10 (blue) and random cifar10 (green) distributions are the same each time.
2 Generalization case studies via margin distributions
This section will now use the core generalization bound, stated in Theorem 1.1, to empirically study
generalization behavior of neural networks via margin distributions.
Before proceeding with the plots, it’s a good time to give a more refined description of the margin
distribution, one that is suitable for comparisons across datasets. Given n data points ((xi, yi))ni=1 as
rows of matrix X ∈ Rn×d and predictor FA : Rd → Rk with spectral complexity RA (cf. eq. (1.1)),
the (normalized) margin distribution is the univariate empirical distribution of the data points each
transformed into a single scalar according to
(x, y) 7→ FA(x)y −maxi 6=y FA(x)i
RA‖X‖2/n
,
where spectral complexity RA is from eq. (1.1). The normalization is thus derived from the bound in
Theorem 1.1, but ignoring log terms.
Taken this way, the two margin distributions for two datasets can be interpreted as follows. Considering
any fixed point on the horizontal axis, if the cumulative distribution of one density is lower than the
other, then it corresponds to a lower right hand side in Theorem 1.1. For no reason other than visual
interpretability, the plots here will instead depict a density estimate of the margin distribution. The
vertical and horizontal axes are rescaled in different plots, but the random and true cifar10 margin
distributions are always the same.
A little more detail about the experimental setup is as follows. All experiments were implemented in
Keras (Chollet et al., 2015). In order to minimize conflating effects of optimization and regularization,
the optimization method was vanilla sgd with step size 0.01, and all regularization (weight decay, batch
normalization, etc.) were disabled. “cifar” in general refers to cifar10, however cifar100 will also be
4
10 epochs
20 epochs
40 epochs
80 epochs
160 epochs
(a) Margins across epochs.
0
10 6
10 5
10 4
(b) Various levels of l2 regularization.
Figure 4
explicitly mentioned. The network architecture is essentially AlexNet (Krizhevsky et al., 2012) with all
normalization/regularization removed, and with no adjustments of any kind (even to the learning rate)
across the different experiments.
Comparing datasets. A first comparison is of cifar10 and the standard mnist digit data. mnist
is considered “easy”, by which it is typically meant that any of a variety of methods can achieve roughly
1% test error. The “easiness” is corroborated by Figure 3a, where the margin distribution for mnist
places all its mass far to the right of the mass for cifar10. Interestingly, randomizing the labels of
mnist, as in Figure 3b, results in a margin distribution to the left of not only cifar10, but also slightly
to the left of (but close to) cifar10 with randomized labels.
Next, Figure 3c compares cifar10 and cifar100, where cifar100 uses the same input images as
cifar10; indeed, cifar10 is obtained from cifar100 by collapsing the original 100 categories into 10
groups. Interestingly, cifar100, from the perspective of margin bounds, is just as difficult as cifar10
with random labels. This is consistent with the large test observed error on cifar100 (which has not
been “optimized” in any way via regularization).
Lastly, Figure 3d replaces the cifar10 input images with random images sampled from Gaussians
matching the first- and second-order image statistics (see (Zhang et al., 2017) for similar experiments).
Convergence of margins. As was pointed out in Section 1, the weights of the neural networks
do not seem to converge in the usual sense during training (the norms grow continually). However, as
depicted in Figure 4a, the sequence of (normalized!) margin distributions is itself converging.
Regularization. As remarked in (Zhang et al., 2017), regularization only seems to bring minor
benefits to test error (though adequate to be employed in all cutting edge results). This observation is
certainly consistent with the margin distributions in Figure 4b, which do not improve (e.g., by shifting to
the right) in any visible way under regularization. An open question, discussed further in Section 4, is to
design regularization that improves margins.
3 Analysis of margin bound
This section will sketch the proof of Theorem 1.1, state a few generalizations, give a lower bound, and
discuss related work.
5
3.1 Multiclass margin bound
The starting point of this analysis is a margin-based bound for multiclass prediction. To state the bound,
first recall the margin operator M(v, y) := vy −max
i 6=y
vi, and define the ramp loss as
`γ(r) :=

0 r < −γ,
1 + r/γ r ∈ [−γ, 0],
1 r > 0,
and ramp risk as Rγ(f) := E(`γ(−M(f(x), y))). Given a sample S := ((x1, y1), . . . , (xn, yn)), define
an empirical counterpart R̂γ of Rγ as R̂γ(f) := n−1
∑
i `γ(−M(f(xi), yi)); note that Rγ and R̂γ
respectively upper bound the probability and fraction of error on the source distribution and training
set. Lastly, given a set of real-valued functions H, define the Rademacher complexity as R(H|S) :=
E suph∈H
∑n
i=1 ih(xi, yi) where the expectation is over the Rademacher random variables (1, . . . , n)
(meaning Pr[i = +1] = Pr[i = −1] = 1/2).
With this notation in place, the basic bound is as follows.
Lemma 3.1. Given functions F with F 3 f : Rd → Rk and any γ > 0, define
Fγ :=
{
(x, y) 7→ `γ(−M(f(x), y)) : f ∈ F
}
.
Then, with probability at least 1− δ over a sample S of size n, every f ∈ F satisfies
Pr[arg max
i
f(x)i 6= y] ≤ R̂γ(f) + 2R((Fγ)|S) + 3
√
ln(1/δ)
2n
.
This bound is a direct consequence of standard tools in Rademacher complexity. In order to instantiate
this bound, covering numbers will be used to directly upper bound the Rademacher complexity term
R((Fγ)|S). Interestingly, the choice of directly working in terms of covering numbers seems essential to
providing a bound with no explicit dependence on k; by contrast, prior work primarily (solely?) handles
multiclass via a Rademacher complexity analysis on each coordinate of a k-tuple of functions, and pays a
factor
√
k (Zhang, 2004).
3.2 Covering number complexity upper bounds
This subsection proves Theorem 1.1 via Lemma 3.1 by controlling the Rademacher complexity R((Fγ)|S)
for networks with bounded spectral complexity via covering numbers. This first step is to prove a more
general covering number bound (in terms of general operator norms) which will eventually be specialized
with l2 data norms and spectral operator norms to prove Theorem 1.1. A tantalizing direction for future
work is to specialize the general bound in other ways, namely ones that are better adapted to the geometry
of neural networks as encountered in practice.
The structure of the networks is the same as before; namely, given matrices A = (A1, . . . , AL), define
a mapping FA as
FA(Z) := σL(ALσL−1(AL1 · · ·σ1(A1Z) · · · )),
with the convention F∅(Z) = Z.
• The input Z ∈ V1 is associated with some norm |Z|1 ≤ B. The subscript merely indicates an index,
and does not refer to any l1 norm. The vector space V1, and moreover the eventual collection of
vector spaces (V1, . . . ,VL), have no fixed meaning and are simply abstract vector spaces. However,
when using these tools to prove Theorem 1.1, V1 = Rd×n and Z ∈ V1 is formed by collecting the n
data points into its columns; that is, Z = X>.
6
• The linear operators Ai : Vi →Wi+1 are associated with some operator norm |Ai|i→i+1 ≤ ci:
|Ai|i→i+1 := sup
|Z|i≤1
|||AiZ|||i+1 = ci,
where once again |||·|||i+1 is an abstract norm introduced purely to lend this proof more flexibility.
As stated before, these linear operators A = (A1, . . . , AL) vary across functions FA. When used to
prove Theorem 1.1, Z is a matrix (the forward image of data matrix X> across layers), and these
norms are all matrix norms.
• The ρi-Lipschitz mappings σi :Wi+1 → Vi+1 have ρi measured with respect to norms | · |i+1 and
|||·|||i+1: for any z, z′ ∈ Wi+1, ∣∣σi(z)− σi(z′)∣∣i+1 ≤ ρi|||z − z′|||i+1.
These Lipschitz mappings are considered fixed within FA. Note again that these operations, when
applied to prove Theorem 1.1, operate on matrices which represent the forward images of all
data points together. Lipschitz properties of the standard coordinate-wise ReLU and max-pooling
operators can be found in Appendix A.1.
Lastly, just before giving the bound, the notation for (proper) covering numbers is as follows. The
notation N (U, , ‖ · ‖) means the least cardinality of any subset V ⊆ U which covers U at scale  with
norm ‖ · ‖, meaning
sup
A∈U
min
B∈V
‖A−B‖ ≤ .
Choices of U that will be used in the present work include both the image F|S of data S under some
function class F , as well as the conceptually simpler choice of a family of matrix products.
Lemma 3.2. Let (1, . . . , L) be given, along with fixed Lipschitz mappings (σ1, . . . , σL) (where σi is
ρi-Lipschitz), and operator norm bounds constants (c1, . . . , cL). Suppose the matrices A = (A1, . . . , AL)
lie within B1 × · · · × BL where Ai ∈ Bi has |Ai|i→i+1 ≤ ci. Lastly, let data Z be given with |Z|1 ≤ B.
Then the neural net images HZ := {FA(Z) : A ∈ B1 × · · · × BL} have covering number bound
N
(
HZ , τ, | · |L+1
)
≤
L∏
i=1
sup
(A1,...,Ai−1)
∀j<iAj∈Bj
N
({
AiF(A1,...,Ai−1)(Z) : Ai ∈ Bi
}
, i, |||·|||i
)
where τ :=
∑
j≤L
jρj
L∏
l=j+1
ρlcl.
The method of proof is to recursively build a cover with each layer: first a cover F1 of {A1Z : A1 ∈ B1}
is produced, then for each F ∈ F1 a cover of {A2σ1(F ) : A2 ∈ B2} is produced and these are unioned
together across all F ∈ F1; continuing this procedure arrives at a final cover at layer L.
It remains to show that the resulting set is indeed a cover. To this end, fix a particular A =
(A1, . . . , AL) and Z, and for convenience define mapped elements Fi+1 and Gi via Fi+1 = Ai+1Gi =
Ai+1σi(Ai · · ·σ1(A1Z) · · · ) which represent states of the network at layer i+1 while evaluating Z. Suppose
inductively that there is a cover element Ĝi close to Gi, in the sense that |Gi − Ĝi|i+1 is small.
Now choose F̂i+1 close to Ai+1Ĝi, meaning |||Ai+1Ĝi − F̂i+1|||i+2 ≤ i+1. The essential chain of
inequalities is
|Gi+1 − Ĝi+1|i+2 ≤ ρi+1|||Fi+1 − F̂i+1|||i+2
≤ ρi+1|||Fi+1 −Ai+1Ĝi|||i+2 + ρi+1|||Ai+1Ĝi − F̂i+1|||i+2 ∵ triangle inequality
≤ ρi+1|Ai+1|i+1→i+2
∣∣∣Gi − Ĝi∣∣∣
i+1
+ ρi+1i+1, ∵ choice of F̂i+1
7
where the term |Gi − Ĝi|i+1 is handled by induction. These inequalities are similar to those in an
existing covering number proof (Anthony and Bartlett, 1999, Chapter 12) (itself rooted in the earlier
work of Bartlett (1996)); however (a) that proof operates node by node, and can not take advantage
of special norms on A, and (b) that proof does not maintain an empirical cover across layers, instead
explicitly covering the parameters of all weight matrices, which incurs the number of parameters as a
multiplicative factor. The idea here to push an empirical cover through layers, meanwhile, is reminiscent
of VC dimension proofs for neural networks (Anthony and Bartlett, 1999, Chapter 8).
Returning to the task of proving Theorem 1.1, what remains is to instantiate the general covering
bound, Lemma 3.2, in a way that gives rise to spectral norms.
Theorem 3.3. Let fixed nonlinearities (σ1, . . . , σL) and reference matrices (M1, . . . ,ML) be given, where
σi is ρi-Lipschitz and σi(0) = 0. Let spectral norm bounds (s1, . . . , sL), and matrix l1 norm bounds
(b1, . . . , bL) be given. Let data matrix X ∈ Rn×d be given, where the n rows correspond to data points.
Let HX denote the family of matrices obtained by evaluating X with all choices of network FA:
HX :=
{
FA(X
T ) : A = (A1, . . . , AL), ‖Ai‖σ ≤ si, ‖Ai −Mi‖1 ≤ bi
}
,
where each matrix has dimension at most W along each axis. Then for any  > 0,
lnN (HX , , ‖ · ‖2) ≤
‖X‖22 ln(2W 2)
2
 L∏
j=1
s2jρ
2
j
 L∑
i=1
(
bi
si
)2/33 .
The key to the proof of Theorem 3.3 via Lemma 3.2 is the following matrix covering lemma.
Lemma 3.4. Let positive reals a, b,  and positive integer p be given, along with matrix X ∈ Rn×d with
maxi ‖Xei‖2 ≤ b. Then
lnN
({
XA : A ∈ Rd×p, ‖A‖1 ≤ a
}
, , ‖ · ‖2
)
≤
⌈
a2b2
2
⌉
ln(2dp).
The proof of Lemma 3.4 relies upon the Maurey sparsification lemma (Pisier, 1980) which is stated in
terms of sparsifying convex hulls, and in its use here is inspired by covering number bounds for linear
predictors (Zhang, 2002). Following those techniques, it is possible to produce a bound that scales with
‖A‖2 and ‖X‖2, but even for the case of the identity matrix X = I, this incurs an extra dimension factor,
thus the use of ‖A‖1 here as a simplifying choice which helps Theorem 1.1 avoid any appearance of W
and L outside of log terms.
The path to prove Theorem 1.1 is now clear. Thanks to standard conversions between Rademacher
complexity and covering numbers, the Rademacher complexity term in the generic margin bound
Lemma 3.1 can be controlled via the covering number estimate from Theorem 3.3.
Lemma 3.5. Let fixed nonlinearities (σ1, . . . , σL) and reference matrices (M1, . . . ,ML) be given where
σi is ρi-Lipschitz and σi(0) = 0. Further let margin γ > 0, data bound B, spectral norm bounds (si)Li=1,
and l1 norm bounds (bi)Li=1 be given. Then with probability at least 1− δ over an iid draw of n examples
((xi, yi))
n
i=1 with
√∑
i ‖xi‖22 ≤ B, every network FA : Rd → Rk whose weight matrices A = (A1, . . . , AL)
obey ‖Ai‖σ ≤ si and ‖Ai −Mi‖1 ≤ bi satisfies
Pr
[
arg max
j
FA(x)j 6= y
]
≤ R̂γ(f) +
8
n
+
72B ln(2W ) ln(n)
γn
 L∏
i=1
siρi
 L∑
i=1
b
2/3
i
s
2/3
i
3/2 + 3√ ln(1/δ)
2n
.
Theorem 1.1 follows from Lemma 3.5 by union bounding over various choices of γ, B, (s1, . . . , sL),
(b1, . . . , bL).
8
3.3 Rademacher complexity lower bounds
By reduction to the linear case (i.e., removing all nonlinearities), it is easy to provide a lower bound on
the Rademacher complexity of the networks studied here. Unfortunately, this bound only scales with the
product of spectral norms, and not the other terms in RA (cf. eq. (1.1)).
Theorem 3.6. Consider the setting of Theorem 3.3, but all nonlinearities are the ReLU z 7→ max{0, z},
and all non-output dimensions are at least 2 (meaning W ≥ 2). Let data S := (x1, . . . , xn) be collected
into data matrix X ∈ Rn×d. Then, for any scalar r > 0,
R
({
FA : A = (A1, . . . , AL),
∏
i
‖Ai‖σ ≤ r
}
|S
)
≥ Ω(r · ‖X‖2). (3.1)
Note that, due to the nonlinearity, the lower bound should indeed depend on
∏
i ‖Ai‖σ and not
‖
∏
iAi‖σ; as a simple sanity check, there exist networks for which the latter quantity is 0, but the
network does not compute the zero function.
3.4 Related work
To close this section on proofs, it is a good time to summarize connections to existing literature.
Margin theory originally arose to analyze 2-layer networks (Bartlett, 1996), indeed with a proof
technique that inspired the layer-wise induction used to prove Theorem 1.1 in the present work. Margin
theory was quickly extended to many other settings (see for instance the survey by Boucheron et al.
(2005)), one major success being an explanation of the generalization ability of boosting methods, which
exhibit an explicit growth in the size of the function class over time, but a stable excess risk (Schapire
et al., 1997). The contribution of the present work is to provide a margin bound (and corresponding
Rademacher analysis) which can be adapted to various operator norms at each layer. Additionally, the
present work operates in the multiclass setting, and avoids an explicit dependence on the number of
classes k, which seems to appear in prior work (Zhang, 2004; Tewari and Bartlett, 2007).
There are numerous generalization bounds for neural networks, including VC-dimension and fat-
shattering bounds (many of these can be found in (Anthony and Bartlett, 1999)). Scale-sensitive analysis
of neural networks started with (Bartlett, 1996), which can be interpreted in the present setting as
utilizing data norm ‖ · ‖∞ and operator norm ‖ · ‖∞→∞ (equivalently, the ‖ · ‖1,∞ matrix norm). This
analysis can be adapted to give a Rademacher complexity analysis (Bartlett and Mendelson, 2002), and
has been adapted to other norms (Neyshabur et al., 2015), although the preceding ‖ · ‖∞ setting is still
needed to avoid extra combinatorial factors. More work is still needed to develop complexity analyses
that have matching upper and lower bounds, and also to determine which norms are well-adapted to
neural networks as used in practice.
The present analysis utilizes covering numbers, and is most closely connected to earlier covering
number bounds (Anthony and Bartlett, 1999, Chapter 12), themselves based on the earlier fat-shattering
analysis (Bartlett, 1996), however the technique here of pushing an empirical cover through layers is
akin to VC dimension proofs for neural networks (Anthony and Bartlett, 1999). The use of Maurey’s
sparsification lemma was inspired by linear predictor covering number bounds (Zhang, 2002).
4 Further observations and open problems
Adversarial examples. Adversarial examples are a phenomenon where the neural network predictions
can be altered by adding seemingly imperceptible noise to an input (Goodfellow et al., 2014). This
phenomenon can be connected to margins as follows. The margin is nothing more than the distance an
input must traverse before its label is flipped; consequently, low margin points are more susceptible to
adversarial noise than high margin points. Concretely, taking the 100 lowest margin inputs from cifar10
and adding uniform noise at scale 0.15 yielded flipped labels on 5.86% of the images, whereas the same
level of noise on high margin points yielded 0.04% flipped labels. Can the bounds here suggest a way to
defend against adversarial examples?
9
Regularization. It was observed in (Zhang et al., 2017) that explicit regularization contributes little
to the generalization performance of neural networks. In the margin framework, standard weight decay
(l2) regularization seemed to have little impact on margin distributions in Section 2. On the other
hand, in the boosting literature, special types of regularization were developed to maximize margins
(Shalev-Shwartz and Singer, 2008); perhaps a similar development can be performed here?
SGD. The present analysis applies to predictors that have large margins; what is missing is an analysis
verifying that sgd applied to standard neural networks returns large margin predictors! Indeed, perhaps
sgd returns not simply large margin predictors, but predictors that are well-behaved in a variety of other
ways which can be directly translated into refined generalization bounds.
Improvements to Theorem 1.1. There are many way to improve Theorem 1.1. One is simply
to determine better lower bounds. Another question is whether there can be a better choice of layer
geometries (norms) which yields better bounds on practical networks. Lastly, is there a way to replace
the nonlinearities’ worst-case Lipschitz constant with an (empirically) averaged quantity?
Rademacher vs. covering. Is it possible to prove Theorem 1.1 solely via Rademacher complexity,
with no invocation of covering numbers?
Acknowledgements
The authors thank Srinadh Bhojanapalli, Ryan Jian, Behnam Neyshabur, Maxim Raginsky, Andrew J.
Risteski, and Belinda Tzen for useful conversations and feedback. The authors thank Ben Recht for giving
a provocative lecture at the Simons Institute, stressing the need for understanding of both generalization
and optimization of neural networks. M.T. and D.F. acknowledge the use of a GPU machine provided
by Karthik Sridharan and made possible by a NVIDIA GPU grant. D.F. acknowledges the support of
the NDSEG fellowship. Lastly, the authors are grateful to La Burrita (both the north and the south
Berkeley campus locations) for upholding the glorious tradition of the California Burrito.
References
Martin Anthony and Peter L. Bartlett. Neural Network Learning: Theoretical Foundations. Cambridge
University Press, 1999.
Peter L. Bartlett. For valid generalization the size of the weights is more important than the size of the
network. In NIPS, 1996.
Peter L. Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and
structural results. JMLR, 3:463–482, Nov 2002.
Stéphane Boucheron, Olivier Bousquet, and Gabor Lugosi. Theory of classification: A survey of some
recent advances. ESAIM: Probability and Statistics, 9:323–375, 2005.
François Chollet et al. Keras. https://github.com/fchollet/keras, 2015.
Moustapha Cisse, Piotr Bojanowski, Edouard Grave, Yann Dauphin, and Nicolas Usunier. Parseval
networks: Improving robustness to adversarial examples. In ICML, 2017.
Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. 2014. arXiv:1412.6572 [stat.ML].
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks.
In ECCV, 2016.
10
Alex Krizhevsky, Ilya Sutskever, and Geoffery Hinton. Imagenet classification with deep convolutional
neural networks. In NIPS, 2012.
Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of Machine Learning. MIT
Press, 2012.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural networks.
In COLT, 2015.
Gilles Pisier. Remarques sur un résultat non publié de b. maurey. Séminaire Analyse fonctionnelle (dit),
pages 1–12, 1980.
Robert E. Schapire, Yoav Freund, Peter Bartlett, and Wee Sun Lee. Boosting the margin: A new
explanation for the effectiveness of voting methods. In ICML, pages 322–330, 1997.
Shai Shalev-Shwartz and Yoram Singer. On the equivalence of weak learnability and linear separability:
New relaxations and efficient boosting algorithms. In COLT, 2008.
Ambuj Tewari and Peter L. Bartlett. On the consistency of multiclass classification methods. Journal of
Machine Learning Research, 8:1007–1025, 2007.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep
learning requires rethinking generalization. ICLR, 2017.
Tong Zhang. Covering number bounds of certain regularized linear function classes. Journal of Machine
Learning Research, 2:527–550, 2002.
Tong Zhang. Statistical analysis of some multi-category large margin classification methods. Journal of
Machine Learning Research, 5:1225–1251, 2004.
11
A Proofs
This appendix collects various proofs omitted from the main text.
A.1 Lipschitz properties of ReLU and max-pooling nonlinearities
The standard ReLU (“Rectified Linear Unit”) is the univariate mapping
σr(r) := max{0, r}.
When applied to a vector or a matrix, it operates coordinate-wise. While the ReLU is currently the most
popular choice of univariate nonlinearity, another common choice is the sigmoid r 7→ 1/(1 + exp(−r)).
More generally, these univariate nonlinearities are Lipschitz, and this carries over to their vector and
matrix forms as follows.
Lemma A.1. If σ : Rd → Rd is ρ-Lipschitz along every coordinate, then it is ρ-Lipschitz according to
‖ · ‖p for any p ≥ 1.
Proof. for any z, z′ ∈ Rd,
‖σ(z)− σ(z′)‖p =
∑
i
|σ(z)i − σ(z′)i|p
1/p ≤
∑
i
ρp|zi − z′i|p
1/p = ρ‖z − z′‖p.
Define a max-pooling operator P as follows. Given an input and output pair of finite-dimensional
vector spaces T and T ′ (possibly arranged as matrices or tensors), the max-pooling operator iterates
over a collection of sets of indices Z (whose cardinality is equal to the dimension of T ’), and for each
element of Zi ∈ Z sets the corresponding coordinate i in the output to the maximum entry of the input
over Zi: given T ∈ T ,
P(T )i := max
j∈Zi
Tj .
The following Lipschitz constant of pooling operators will depend on the number of times each coordinate
is accessed across elements of Z; when this operator is used in computer vision, the number of times is
typically a small constant, for instance 5 or 9 (Krizhevsky et al., 2012).
Lemma A.2. Suppose that each coordinate j of the input appears in at most m elements of the collection
Z. Then the max-pooling operator P is m1/p-Lipschitz wrt ‖ · ‖p for any p ≥ 1. In particular, the
max-pooling operator is 1-Lipschitz whenever Z forms a partition.
Proof. Let T, T ′ ∈ T be given. First consider any fixed set of indices Z ∈ Z, and suppose without loss of
generality that P(T )Z = maxj∈Z Tj ≥ maxj∈Z T ′j . Then
|P(T )Z − P(T ′)Z |p =
(
min
j′∈Z
max
j∈Z
Tj − T ′j′
)p
= max
j∈Z
(
Tj − T ′j
)p
≤
∑
j∈Z
∣∣∣Tj − T ′j∣∣∣p .
Consequently,
‖P(T )− P(T ′)‖p =
∑
i
|P(T )i − P(T ′)i|p
1/p =
∑
Z∈Z
|P(T )Z − P(T ′)Z |p
1/p
≤
∑
Z∈Z
∑
j∈Z
|Tj − T ′j |p
1/p =
∑
j
∑
Z∈Z:j∈Z
|Tj − T ′j |p
1/p
≤
m∑
j
|Tj − T ′j |p
1/p = m1/p‖T − T ′‖p.
12
A.2 Margin properties in Section 3.1
The goal of this subsection is to prove the general margin bound in Lemma 3.1. To this end, it is first
necessary to establish a few properties of the margin operatorM(v, j) := vj −maxi 6=j vi and of the ramp
loss `λ.
Lemma A.3. For every j and every p ≥ 1,M(·, j) is 2-Lipschitz wrt ‖ · ‖p.
Proof. Let v, v′, j be given, and suppose (without loss of generality) M(v, j) ≥ M(v′, j). Choose
coordinate i 6= j so thatM(v′, j) = v′j − v′i. Then
M(v, j)−M(v′, j) =
(
vj −max
l 6=j
vj
)
−
(
v′j − v′i
)
= vj − v′j + v′i + min
l 6=j
(−vl)
≤
(
vj − v′j
)
+
(
v′i − vi
)
≤ 2‖v − v′‖∞ ≤ 2‖v − v′‖p.
Next, recall the definition of the ramp loss
`γ(r) :=

0 r < −γ,
1 + r/γ r ∈ [−γ, 0],
1 r > 0,
and of the ramp risk
Rγ(f) := E(`γ(−M(f(x), y))).
(These quantities are standard; see for instance (Boucheron et al., 2005; Zhang, 2004; Tewari and Bartlett,
2007).)
Lemma A.4. For any f : Rd → Rk and every γ > 0,
Pr[arg max
i
f(x)i 6= y] ≤ Pr[M(f(x), y) ≥ 0] ≤ Rγ(f),
where the arg max follows any deterministic tie-breaking strategy.
Proof.
Pr[arg max
i
f(x)i 6= y] ≤ Pr[max
i 6=y
f(x)i ≥ f(x)y]
= Pr[−M(f(x), y) ≥ 0]
= E1[−M(f(x), y) ≥ 0]
≤ E`γ(−M(f(x), y))
With these tools in place, the proof of Lemma 3.1 is straightforward.
Proof of Lemma 3.1. Since `γ has range [0, 1], it follows by standard properties of Rademacher complexity
(Mohri et al., 2012, Theorem 3.1) that with probability at least 1− δ, every f ∈ F satisfies
Rγ(f) ≤ R̂γ(f) + 2R((Fγ)|S) + 3
√
ln(2/δ)
2n
.
The bound now follows by applying Lemma A.4 to the left hand side.
13
A.3 Proof of general covering bound (Lemma 3.2)
Proof of Lemma 3.2. Inductively construct covers F1, . . . ,FL of W2, . . . ,WL+1 as follows.
• Choose an 1-cover F1 of {A1Z : A1 ∈ B1}, thus
|F1| ≤ N ({A1Z : A1 ∈ B1} , 1, |||·|||2) =: N1.
• For every element F ∈ Fi, construct an i+1-cover Gi+1(F ) of{
Ai+1σi(F ) : Ai+1 ∈ Bi+1
}
.
Since the covers are proper, meaning F = AiF(A1,...,Ai−1)(Z) for some matrices (A1, . . . , Ai) ∈
B1 × · · · × Bi, it follows that∣∣Gi+1(F )∣∣ ≤ sup
(A1,...,Ai)
∀j≤iAj∈Bj
N
({
Ai+1FA1,...,Ai(Z) : Ai+1 ∈ Bi+1
}
, i+1, |||·|||i+2
)
=: Ni+1.
Lastly form the cover
Fi+1 :=
⋃
F∈Fi
Gi+1(F ),
whose cardinality satisfies
|Fi+1| ≤ |Fi| ·Ni+1 ≤
i+1∏
l=1
Nl.
Define F :=
{
σL(F ) : F ∈ FL
}
; by construction, F satisfies the desired cardinality constraint. to
show that it is indeed a cover, fix any (A1, . . . , AL) satisfying the above constraints, and for convenience
define recursively the mapped elements
F1 = A1X ∈ W2, Gi = σi(Fi) ∈ Vi+1 Fi+1 = Ai+1Gi ∈ Wi+2.
The goal is to exhibit ĜL ∈ F satisfying |GL − ĜL|L+1 ≤ τ . To this end, inductively construct
approximating elements (F̂i, Ĝi) as follows.
• Base case: set Ĝ0 = X.
• Choose F̂i ∈ Fi with |||AiĜi−1 − F̂i|||i+1 ≤ i, and set Ĝi := σi(F̂i).
To complete the proof, it will be shown inductively that
|Gi − Ĝi|i+1 ≤
∑
1≤j≤i
jρj
i∏
l=j+1
ρlcl.
For the base case,
|G0 − Ĝ0|1 = 0.
For the inductive step,
|Gi+1 − Ĝi+1|i+2 ≤ ρi+1|||Fi+1 − F̂i+1|||i+2
≤ ρi+1|||Fi+1 −Ai+1Ĝi|||i+2 + ρi+1|||Ai+1Ĝi − F̂i+1|||i+2
≤ ρi+1|Ai+1|i+1→i+2
∣∣∣Gi − Ĝi∣∣∣
i+1
+ ρi+1i+1
≤ ρi+1ci+1
∑
j≤i
jρj
i∏
l=j+1
ρlcl
+ ρi+1i+1
=
∑
j≤i+1
jρj
i+1∏
l=j+1
ρlcl.
14
A.4 Proof of spectral covering bound (Theorem 3.3)
The first step is to establish the matrix covering bound in Lemma 3.4. This proof is inspired by covering
number bounds for linear predictors due to Zhang (2002), which centrally rely upon the following
sparsification lemma due to Maurey.
Lemma A.5 (Maurey; cf. (Pisier, 1980), (Zhang, 2002, Lemma 1)). Fix Hilbert space H with norm ‖ · ‖.
Let U ∈ H be given with representation U =
∑d
i=1 αiVi where Vi ∈ H and α ∈ Rd≥0 \ {0}. Then for any
positive integer k, there exists a choice of nonnegative integers (k1, . . . , kd),
∑
i ki = k, such that∥∥∥∥∥∥U − ‖α‖1k
d∑
i=1
kiVi
∥∥∥∥∥∥
2
≤ ‖α‖1
k
d∑
i=1
αi‖Vi‖2 ≤
‖α‖21
k
max
i
‖Vi‖2.
Proof. Set β := ‖α‖1 for convenience, and let (W1, . . . ,Wk) denote k iid random variables where
Pr[W1 = βVi] := αi/β. Define W := k−1
∑k
i=1Wi, whereby
EW = EW1 =
d∑
i=1
βVi
(
αi
β
)
= U.
Consequently
E‖U −W‖2 = 1
k2
E
∥∥∥∥∥∥
∑
i
(U −Wi)
∥∥∥∥∥∥
2
=
1
k2
E
∑
i
‖U −Wi‖2 +
∑
i 6=j
〈
U −Wi, U −Wj
〉
=
1
k
E‖U −W1‖2 =
1
k
(
E‖W1‖2 − ‖U‖2
)
≤ 1
k
E‖W1‖2
=
1
k
d∑
i=1
αi
β
‖βVi‖2 =
β
k
d∑
i=1
αi‖Vi‖2
≤ β
2
k
max
i
‖Vi‖2.
To finish, by the probabilistic method, there exists integers (j1, . . . , jk) ∈ {1, . . . , d}k and an assignment
Ŵi := βVji and Ŵ := k−1
∑k
i=1 Ŵi such that∥∥∥U − Ŵ∥∥∥2 ≤ E‖U −W‖2 .
The result now follows by defining integers (k1, . . . , kd) according to ki :=
∑k
l=1 1[jl = i].
The Maurey sparsification lemma easily gives the matrix covering bound in Lemma 3.4.
Proof of Lemma 3.4. Let matrix X ∈ Rn×d be given, set N := 2dp and k := da2b2/2e, and define
{V1, . . . , VN} :=
{
sXeie
>
j : s ∈ {−1,+1} , i ∈ {1, . . . , d} , j ∈ {1, . . . , p}
}
,
C :=
ak
N∑
i=1
kiVi : ki ≥ 0,
N∑
i=1
ki = k
 =
ak
k∑
j=1
Vij : (i1, . . . , ik) ∈ [N ]
k
 ,
where the ki’s are integers, and ‖Vi‖2 ≤ maxi ‖Xei‖2 ≤ b by construction. It will now be shown that C
is the desired cover. Firstly, |C| ≤ Nk by construction, namely the final equality above. Secondly, let A
with ‖A‖1 ≤ a be given, and note that
XA = X
d∑
i=1
p∑
j=1
Aijeie
>
j = ‖A‖1
d∑
i=1
p∑
j=1
Aij
‖A‖1
(
Xeie
>
j
)
∈ a · conv({V1, . . . , VN}),
15
where conv({V1, . . . , Vn}) is the convex hull of {V1, . . . , VN}. Consequently, by Lemma A.5, there exist
nonnegative integers (k1, . . . , kN ) with
∑
i ki = k with∥∥∥∥∥∥XA− ak
N∑
i=1
kiVi
∥∥∥∥∥∥
2
2
≤ a
2b2
k
≤ 2.
Together, the preceding pieces give the proof of Theorem 3.3.
Proof of Theorem 3.3. First dispense with the parenthetical statement regarding coordinate-wise ReLU
and max-pooling operaters, which are Lipschitz by Lemmas A.1 and A.2. The rest of the proof is
now a consequence of Lemma 3.2 with all data norms set to the l2 norm (| · |i = |||·|||i = ‖ · ‖2),
all operator norms set to the spectral norm (| · |i→i+1 = ‖ · ‖σ), the matrix constraint sets set to
Bi =
{
Ai : ‖Ai‖σ ≤ si, ‖Ai −Mi‖1 ≤ bi
}
, and lastly the per-layer cover resolutions (1, . . . , L) set
according to
i :=
αi
ρi
∏
j>i ρjsj
where αi :=
1
ᾱ
(
bi
si
)2/3
, ᾱ :=
L∑
j=1
(
bj
sj
)2/3
.
By this choice, it follows that the final cover resolution τ provided by Lemma 3.2 satisfies
τ ≤
∑
j≤L
jρj
L∏
l=j+1
ρlsl =
∑
j≤L
αj = .
The key technique in the remainder of the proof is to apply Lemma 3.2 with the covering number estimate
from Lemma 3.4, but centering the covers at Mi (meaning the cover at layer i is of matrices Bi where
Ai ∈ Bi satisfies ‖Ai −Mi‖1 ≤ bi), and collecting (x1, . . . , xn) as rows of matrix X ∈ Rn×d.
To start, the covering number estimate from Lemma 3.2 can be combined with Lemma 3.4 to give
lnN (H|S , , ‖ · ‖2) ≤
L∑
i=1
sup
(A1,...,Ai−1)
∀j<iAj∈Bj
lnN
({
AiF(A1,...,Ai−1)(X
>) : Ai ∈ Bi
}
, i, ‖ · ‖2
)
(∗)
=
L∑
i=1
sup
(A1,...,Ai−1)
∀j<iAj∈Bj
lnN
({
F(A1,...,Ai−1)(X
>)>(Ai −Mi)> : ‖Ai −Mi‖1 ≤ bi, ‖Ai‖σ ≤ si
}
, i, ‖ · ‖2
)
≤
L∑
i=1
sup
(A1,...,Ai−1)
∀j<iAj∈Bj
lnN
({
F(A1,...,Ai−1)(X
>)>(Ai −Mi)> : ‖Ai −Mi‖1 ≤ bi
}
, i, ‖ · ‖2
)
≤
L∑
i=1
sup
(A1,...,Ai−1)
∀j<iAj∈Bj
b2i maxj ‖F(A1,...,Ai−1)(X>)>ej‖22
2i
ln(2W 2), (A.1)
where (∗) follows first since l2 covering a matrix and its transpose is the same, and secondly since the
cover can be translated by F(A1,...,Ai−1)(X
>)>M>i without changing its cardinality. In order to simplify
this expression, note for any (A1, . . . , Ai−1) that
max
j
‖F(A1,...,Ai−1)(X
>)>ej‖2 ≤ ‖F(A1,...,Ai−1)(X
>)>‖2
= ‖F(A1,...,Ai−1)(X
>)‖2
= ‖σi−1(Ai−1F(A1,...,Ai−2)(X
>)− σi−1(0)‖2
≤ ρi−1‖Ai−1F(A1,...,Ai−2)(X
>)− 0‖2
≤ ρi−1‖Ai−1‖σ‖F(A1,...,Ai−2)(X
>)‖2,
16
which by induction gives
max
j
‖F(A1,...,Ai−1)(X
>)>ej‖2 ≤ ‖X‖2
i−1∏
j=1
ρj‖Aj‖σ. (A.2)
Combining eqs. (A.1) and (A.2), then expanding the choice of i and collecting terms,
lnN (H|S , , ‖ · ‖2) ≤
L∑
i=1
sup
(A1,...,Ai−1)
∀j<iAj∈Bj
b2i ‖X‖22
∏
j<i ρ
2
j‖Aj‖2σ
2i
ln(2W 2)
≤
L∑
i=1
b2iB
2
∏
j<i ρ
2
js
2
j
2i
ln(2W 2)
=
B2 ln(2W 2)
∏L
j=i ρ
2
js
2
j
2
L∑
i=1
b2i
α2i s
2
i
=
B2 ln(2W 2)
∏L
j=i ρ
2
js
2
j
2
(
ᾱ3
)
.
A.5 Proof of Theorem 1.1
The first step is to prove Lemma 3.5, which, in contrast to Theorem 1.1, has matrix and data constraints
given before the data is seen.
Proof of Lemma 3.5. Consider the class of networks Fλ obtained by affixing the ramp loss `γ and the
negated margin operator −M to the output of the provided network class:
Fγ :=
{
(x, y) 7→ `γ(−M(f(x), y)) : f ∈ F
}
;
Since (z, y) 7→ `γ(−M(z, y)) is 2/γ-Lipschitz wrt ‖ · ‖2 by Lemma A.3 and definition of `γ , the function
class Fγ still falls under the setting of Theorem 3.3, and gives
lnN
(
(Fγ)|S , , ‖ · ‖2
)
≤ 4B
2 ln(2W 2)
γ22
 L∏
j=1
s2jρ
2
j
 L∑
i=1
(
bi
si
)2/33 =: R
2
.
What remains is to relate covering numbers and Rademacher complexity via a Dudley entropy integral;
note that most presentations of this technique place 1/n inside the covering number norm, and thus the
application here is the result of a tiny amount of massaging. Continuing with this in mind, the Dudley
entropy integral bound on Rademacher complexity grants
R((Fγ)|S) ≤ inf
α>0
(
4α+
12
n
∫ √n
α
√
R
2
d
)
= inf
α>0
(
4α+ ln(
√
n/α)
12
√
R
n
)
.
The inf is uniquely minimized at α := 3
√
R/n, but the desired bound may be obtained by the simple
choice α := 1/n, and plugging the resulting Rademacher complexity estimate into Lemma 3.1.
The proof of Theorem 1.1 now follows by instantiating Lemma 3.5 for many choices of its various
parameters, and applying a union bound. There are many ways to cut up this parameter space and
organize the union bound; the following lemma makes one such choice, whereby Theorem 1.1 is easily
proved. A slightly better bound is possible by invoking positive homogeneity of (σ1, . . . , σL) to balance
the spectral norms of the matrices (A1, . . . , AL), however these rebalanced matrices are then used in the
comparison to (M1, . . . ,ML), which is harder to interpret when Mi 6= 0.
17
Lemma A.6. Suppose the setting and notation of Theorem 1.1. With probability at least 1− δ, every
network FA : Rd → Rk with weight matrices A = (A1, . . . , AL) and every γ > 0 satisfy
Pr
[
arg max
j
FA(x)j 6= y
]
≤ R̂γ(FA) +
8
n
+
144 ln(n) ln(2W )
γn
∏
i
ρi
(1 + ‖X‖2)
 L∑
i=1
( 1
L
+ ‖Ai −Mi‖1
)∏
j 6=i
(
1
L
+ ‖Aj‖σ
)2/3

3/2
+
√
9
2n
√√√√ln(1/δ) + ln(2n/γ) + 2 ln(2 + ‖X‖2) + 2 L∑
i=1
ln(2 + L‖Ai −Mi‖1) + 2
L∑
i=1
ln(2 + L‖Ai‖σ).
(A.3)
Proof. Given positive integers (~j,~k,~l) = (j1, j2, j3, k1, . . . , kL, l1, . . . , lL), define a set of instances (a set
of triples (γ,X,A))
B(~j,~k,~l) = B(j1, j2, j3, k1, . . . , kL, l1, . . . , lL)
:=
{
(γ,X,A) : 0 < 1
γ
<
2j1
n
, ‖X‖2 < j2, ‖Ai −Mi‖1 <
ki
L
, ‖Ai‖σ <
li
L
}
.
Correspondingly subdivide δ as
δ(~j,~k,~l) = δ(j1, j2, j3, k1, . . . , kL, l1, . . . , lL)
:=
δ
2j1 · j2(j2 + 1) · k1(k1 + 1) · · · kL(kL + 1) · l1(l1 + 1) · · · lL(lL + 1)
.
Fix any (~j,~k,~l). By Lemma 3.5, with probability at least 1− δ(~j,~k,~l), every (γ,X,A) ∈ B(~j,~k,~l) satisfies
Pr
[
arg max
j
FA(x)i 6= y
]
≤ R̂γ(f) +
8
n
+
72 · 2j1 · j2 ln(2W ) ln(n)
n2
 L∏
i=1
ρi

 L∑
i=1
ki
L
∏
j 6=i
lj
L
2/3

3/2
︸ ︷︷ ︸
=:♥
+ 3
√
ln(1/δ) + ln(2j1) + 2 ln(1 + j2) + 2
∑L
i=1 ln(1 + ki) + 2
∑L
i=1 ln(1 + li)
2n︸ ︷︷ ︸
=:♣
.
(A.4)
Since
∑
~j,~k,~l δ(
~j,~k,~l) = δ, by a union bound, the preceding bound holds simultaneously over all B(~j,~k,~l)
with probability at least 1− δ.
Thus, to finish the proof, discard the preceding failure event, and let an arbitrary (γ,X,A) be given.
Choose the smallest (~j,~k,~l) so that (γ,X,A) ∈ B(~j,~k,~l); by the preceding union bound, eq. (A.4) holds
for this (~j,~k,~l). The remainder of the proof will massage eq. (A.4) into the form in the statement of
Theorem 1.1.
As such, first consider the case j1 = 1, meaning γ < 2/n; then
Pr
[
arg max
j
FA(x)j 6= y
]
≤ 1 < 1
γn
,
18
where the last expression lower bounds the right hand side of eq. (A.3), thus completing the proof in the
case j1 = 1. Suppose henceforth that j1 ≥ 2 (and γ ≥ 2/n).
Combining the preceding bound j2 ≥ 2 with the definition of B(~j,~k,~l), the elements of (~j,~k,~l) satisfy
2j1 ≤ 2n
γ
,
j2 ≤ 1 + ‖X‖2,
∀i  ki ≤ 1 + L‖Ai −Mi‖1,
∀i  li ≤ 1 + L‖Ai‖σ.
For the term ♥, the factors with (~j,~k,~l) are bounded as
2j1 · j2
 L∑
i=1
ki∏
j 6=i
lj
2/3

3/2
≤ 2n
γ
(
1 + ‖X‖2
) L∑
i=1
(L−1 + ‖Ai −Mi‖1)∏
j 6=i
(L−1 + ‖Ai‖σ)
2/3

3/2
.
For the term ♣, the factors with (~j,~k,~l) are bounded as
ln(2j1) + 2 ln(1 + j2) + 2
L∑
i=1
ln(1 + ki) + 2
L∑
i=1
ln(1 + li)
≤ ln(2n/γ) + 2 ln(2 + ‖X‖2) + 2
L∑
i=1
ln(2 + L‖Ai −Mi‖1) + 2
L∑
i=1
ln(2 + L‖Ai‖σ).
Plugging these bounds on ♥ and ♣ into eq. (A.4) gives eq. (A.3).
The proof of Theorem 1.1 is now a consequence of Lemma A.6, simplifying the bound with a Õ(·).
Before proceeding, it is useful to pin down the asymptotic notation Õ(·), as it is not completely standard
in the multivariate case. The notation can be understood via the lim sup view of O(·); namely, f = Õ(g)
if there exists a constant C so that any sequence ((n(j), γ(j), X(j), A(j)1 , . . . , A
(j)
L ))
∞
j=1 with n(j) → ∞,
γ(j) →∞, ‖X(j)‖2 →∞, ‖A(j)i ‖1 →∞ satisfies
lim sup
j→∞
f(n(j), γ(j), X(j), A
(j)
1 , . . . , A
(j)
L )
g(n(j), γ(j), X(j), A
(j)
1 , . . . , A
(j)
L ) poly log(g(n(j), γ(j), X(j), A
(j)
1 , . . . , A
(j)
L ))
≤ C.
Proof of Theorem 1.1. Let f = f0 + f1 + f2 denote the three excess risk terms of the upper bound from
Lemma A.6, and g = g1 + g2 denote the two excess risk terms of the upper bound from Theorem 1.1; as
discussed above, the goal is to show that there exists a universal constant C so that for any sequence of
tuples ((n(j), γ(j), X(j), A(j)1 , . . . , A
(j)
L ))
∞
j=1 increasing as above, lim supj→∞ f/(g poly log(g)) ≤ C.
It is immediate that lim supj→∞ f0/g = 0 and lim supj→∞ f1/(g1 ln(g)) ≤ 144. The only trickiness
arises when studying f2/(g2 ln(g)), namely the term
∑
i ln(2 + L‖Ai −Mi‖1), since g2 instead has the
term ln(
∑
i ‖Ai −Mi‖
2/3
1 ), and the ratio of these two can scale with L. A solution however is to compare
to ln(
∏
i ‖Ai‖σ), noting that ‖Ai‖1 ≤W‖Ai‖2 ≤W 3/2‖Ai‖σ:
lim sup
j→∞
∑
i ln(2 + L‖A
(j)
i −Mi‖1)
ln(
∏
i ‖A
(j)
i ‖σ)
≤ lim sup
j→∞
∑
i ln(2 + L‖A
(j)
i ‖1 + L‖Mi‖1)∑
i ln(‖A
(j)
i ‖1/W 3/2)
= 1.
19
A.6 Proof of lower bound (Theorem 3.6)
Proof of Theorem 3.6. Define
F(r) :=
ALσL−1(AL−1 · · ·σ2(A2σ1(A1x)) :
L∏
i=1
‖Ai‖σ ≤ r
 ,
where each σi = σ is the ReLU and each Ak ∈ Rdk×dk−1 , with d0 = d and dL = 1, and let S := (x1, . . . , xn)
denote the sample.
Define a new class G(r) =
{
x 7→ 〈a, x〉 | ‖w‖2 ≤ r
}
. It will be shown that G(r) ⊆ F(C · r) for some
C > 0, whereby the result easily follows from a standard lower bound on R(G(r)|S).
Given any linear function x 7→ 〈a, x〉 with ‖a‖2 ≤ r, construct a network f = ALσL−1(AL−1 · · ·σ2(A2σ1(A1x)))
as follows:
• A1 = (e1 − e2)a>.
• Ak = e1e>1 + e2e>2 for each k ∈ {2, . . . , L− 1}.
• AL = e1 − e2.
It is now shown that f(x) = 〈a, x〉 pointwise. First, observe σ(A1x) = (σ(〈a, x〉), σ(−〈a, x〉), 0, . . . , 0).
Since σ is positive homogeneous, σL−1(AL1 · · ·σ2(A2y) = AL−1AL−2 · · ·A2y = (y1, y2, 0, . . . , 0) for
any y in the non-negative orthant. Because σ(A1x) lies in the non-negative orthant, this means
σL−1(AL−1 · · ·σ2(A2σ1(A1x))) = (σ(〈a, x〉), σ(−〈a, x〉), 0, . . . , 0). Finally, the choice of AL = e1 − e2
gives f(x) = σ(〈a, x〉)− σ(−〈a, x〉) = 〈a, x〉.
Observe that for all k ∈ {2, . . . , L− 1}, ‖Ak‖σ = 1. For the other layers, ‖AL‖σ = ‖AL‖2 =
√
2 and
‖A1‖σ =
√
2 · r, which implies f ∈ F(2r).
Combining the pieces,
R(F(2r)|S) ≥ R(G(r)|S) = E sup
a:‖a‖2≤r
n∑
t=1
t〈a, xt〉 = r · E
∥∥∥∥∥∥
n∑
t=1
txt
∥∥∥∥∥∥
2
.
Finally, by the Khintchine-Kahane inequality there exists c > 0 such that
E
∥∥∥∥∥∥
n∑
t=1
txt
∥∥∥∥∥∥
2
≥ c ·
√√√√ n∑
t=1
‖xt‖22 = c‖X‖2.
20

