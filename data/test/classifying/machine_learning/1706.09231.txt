Asymptotic Confidence Regions for
Highdimensional Structured Sparsity.
Benjamin Stucky∗, Sara van de Geer∗
June 29, 2017
Abstract
In the setting of high-dimensional linear regression models, we propose
two frameworks for constructing pointwise and group confidence sets for
penalized estimators which incorporate prior knowledge about the orga-
nization of the non-zero coefficients. This is done by desparsifying the
estimator as in van de Geer et al. [18] and van de Geer and Stucky [17],
then using an appropriate estimator for the precision matrix Θ. In order
to estimate the precision matrix a corresponding structured matrix norm
penalty has to be introduced. After normalization the result is an asymp-
totic pivot. The asymptotic behavior is studied and simulations are added
to study the differences between the two schemes.
Keywords— Asymptotic confidence regions, structured sparsity,
high-dimensional linear regression, penalization.
1 Introduction
We focus on the basic high dimensional linear regression model, which is at the
core of understanding more complex models:
Y = Xβ0 + . (1.1)
Here Y ∈ Rn is an observable response variable, X is a given n×p design matrix
with p >> n, β0 ∈ Rp is a parameter vector of unknown coefficients and  ∈ Rn
is unobservable noise. Due to the high-dimensionality of the design the question
arises as to find the solution to an underdetermined system. The idea to restrict
ourselves to sparse solutions has become the new paradigm to solve this problem
for high-dimensional data. In such a setting, the LASSO estimator (introduced
by Tibshirani [15]) is the most widely used method in pursuance of estimating
the unknown parameter vector β0, while avoiding the high-dimensional problem
of overfitting:
β̂`1 := arg min
β∈Rp
{
‖Y −Xβ‖2n + 2λL‖β‖1
}
.
∗Seminar for Statistics, ETH Zürich, Switzerland
1
ar
X
iv
:1
70
6.
09
23
1v
1 
 [
m
at
h.
ST
] 
 2
8 
Ju
n 
20
17
The loss function is defined as ‖Y −Xβ‖2n :=
∑n
j=1(Y −Xβ)2j/n, and ‖β‖1 de-
notes the `1-norm. The main purpose of the added `1-norm penalty is to achieve
an entry-wise sparse β̂`1 solution, while at the same time the least squares loss
ensures good prediction properties. Furthermore the constant λL > 0 is the
penalty level, regulating the amount of sparsity introduced to the solution.
The `1-norm penalty is a simple convex relaxation of the non-convex `0 penalty
(‖β‖`0 := #{i : βi 6= 0}). Let us recall that the `1-norm penalty does not
promote any specific sparsity structure. In other words the LASSO estimator
does not assume anything about the organization of the non-zero coefficients.
In this sense the LASSO estimator does not incorporate any prior knowledge of
the structure of the true unknown active set S0 := {i : β0,i 6= 0}. In practice
however, prior knowledge is often available. Prior knowledge may emerge from
physical systems or known biological processes. For the purpose of integrating
the available prior information, the `1-norm penalty needs to be replaced in such
a way, that the new penalty reflects this knowledge. One can find many examples
of such penalties and their properties in the recently emerging literature on the
sparsity structure of the unknown parameter vector, see for example Bach [2],
Bach et al. [1], Micchelli et al. [10], Micchelli et al. [9], Maurer and Pontil [6].
A more comprehensive overview can be found in Obozinski and Bach [12].
We will focus on norm penalties and therefore generalize the LASSO estimator
to a large family of penalized estimators (see van de Geer [16] and Stucky and
van de Geer [14]), each with distinct properties to promote sparsity structures
in the parameter vector:
β̂Ω := arg min
β∈Rp
{
‖Y −Xβ‖2n + λΩ(β)
}
. (1.2)
Here Ω is any norm on Rp that reflects some aspects of the pattern of spar-
sity for the parameter vector β0. Again for readability we let β̂ = β̂Ω. We
characterize the Ω-norm in terms of its weakly decomposable subsets of Rp. A
weakly decomposable norm is in some sense able to split up into two norms, one
norm measuring the size of the vector on the active set and the other norm the
size on its complement. The weakly decomposable norm itself reflects the prior
information of the underlying sparsity.
Notation: Depending on the context, for a set J ⊂ {1, . . . , p} and a vector
β ∈ Rp the vector βJ is either the |J |-dimensional vector {βj : j ∈ J} or the
p-dimensional vector {βj l{j ∈ J} : j = 1, . . . , p}. More generally, for a vector
wJ := {wj : j ∈ J}, we use the same notation for its extended version wJ ∈ Rp
where wj,J = 0 for all j /∈ J . For a set B we let BJ = {βJ : β ∈ B}.
The definition of a weakly decomposable norm is crucial to the following sec-
tions, so we introduce it as in van de Geer [16] or Stucky and van de Geer [14].
This idea goes back to Bach et al. [1].
Definition 1 (Weak decomposability). A norm Ω in Rp is called weakly de-
composable for an index set S ⊂ {1, ..., p}, if there exists another norm ΩSc on
R|Sc| such that
∀β ∈ Rp : Ω(βS) + ΩS
c
(βSc) ≤ Ω(β). (1.3)
2
A set S is called allowed if Ω is a weakly decomposable norm for this set. From
now on we use the notation ΥS(β) := Ω(βS)+Ω
Sc(βSc) the lower bounding norm
from the weak decomposability definition and ΛS(β) := Ω(βS) + Ω(βSc) the
upper bounding norm from the triangle inequality. The weak decomposability
now reads
ΥS(β) ≤ Ω(β) ≤ ΛS(β).
Therefore the ΥS-norm mimics the decomposability property of the `1-norm
(‖β‖1 = ‖βS‖1 + ‖βSc‖1) for the set S.
For the LASSO estimator, most work up until recently has been focusing on
point estimation among other topics, with not much focus on establishing un-
certainty in high dimensional models. Interest has been growing rapidly on the
very important topic of constructing confidence regions for the LASSO estima-
tor, see for example van de Geer et al. [18], van de Geer and Stucky [17], Zhang
and Zhang [21], Javanmard and Montanari [5] and Meinshausen [7]. When it
comes to confidence regions for structured sparsity estimators there has not yet
been done much work to our knowledge. The paper van de Geer and Stucky
[17] mentions one approach for group confidence regions for structured sparsity
briefly, which we will develop further.
The main goal of this paper is therefore to construct asymptotic group confi-
dence regions for structured sparsity estimators in two possible ways. In order to
do this, we introduce a de-sparsified version of the estimators in (1.2), following
the idea of van de Geer et al. [18]. An appropriate estimation of the precision
matrix will be needed for the definition of a de-sparsified estimator. The estima-
tion of the precision matrix can be done in two ways which are beneficial for the
construction of asymptotic confidence regions. These two frameworks differ in
the structure of the penalty function. The theoretical behavior and the assump-
tions on the sparsity is studied. Furthermore, a simulation compares these two
frameworks in the high dimensional case and outlines potential applications.
2 De-sparsified Ω structured estimator
For a given norm Ω(·) on Rp we can determine its sparsity structure by listing all
the subsets S := {S1, ..., Sk} for which the norm is weakly decomposable. The
estimator β̂Ω (1.2) prefers to set the complement of any of the sets S
c
1, ..., S
c
k
to zero. Unfortunately the joint distribution of estimator (1.2) is not easy
to access. But it is possible to de-sparsify (1.2) and asymptotically describe
the distribution of this new estimator. The essential idea for the de-sparsified
estimator comes from the following lemma, which establishes a variation to the
KKT conditions of β̂Ω, following directly from Stucky and van de Geer [14].
Lemma 1. For the estimator defined in (1.2) with ̂ := Y − Xβ̂ the KKT
conditions are XT ̂/n = λẐ, where Ω∗(Ẑ) ≤ 1 and ẐT β̂ = Ω(β̂).
3
Here Ω∗(·) is another norm on Rp called the dual norm.
Ω∗(α) := sup
β∈Rp,Ω(β)=1
βTα.
Since Y = Xβ0 +  and using the notation Σ̂ := XTX/n we can write the KKT
conditions as
Σ̂(β̂ − β0) + λẐ = XT /n.
Suppose we have an appropriate surrogate for the precision matrix Θ̂, we get
Θ̂Σ̂(β̂ − β0) + λΘ̂Ẑ = Θ̂XT /n, and
β̂ + λΘ̂Ẑ − β0 = Θ̂XT /n+ ∆/
√
n
Here ∆ :=
√
n(Θ̂Σ̂ − I)(β̂ − β0) is the error term. We define the de-sparsified
Ω structured estimator as follows.
Definition 2. The de-sparsified Ω structured estimator is
b̂Ω := β̂ + λΘ̂Ẑ.
When Ω(·) = ‖·‖1 is the `1-norm, and if we have a β`1 sparsity assumption of
order o(
√
n/ log(p)), a reasonable sparsity assumption on the precision matrix
and if we assume the errors to follow i.i.d. Gaussian distributions, then van
de Geer et al. [18] have shown that the de-sparsified `1 structured estimator
follows an asymptotic Gaussian distribution with an asymptotically negligible
error term.
In order to get similar results for the Ω penalization, we need to discuss how to
estimate the precision matrix Θ̂. The main problem that arises is, that good
estimation error bounds are only available expressed in the ΥS? -norm, where
S? is the unknown oracle set from the main theorem in Stucky and van de Geer
[14]. The next two sections give two different ways to estimate Θ̂ in such a way
that ∆ is asymptotically negligible.
3 First framework: gauge confidence regions
A way to construct an estimate for the precision matrix is to do |J |-wise re-
gression with any fixed set J ⊂ {1, ..., p}. |J |-wise regression is a very similar
method as node-wise regression (introduced by Meinshausen and Bühlmann [8]),
but instead of one node, we have simultaneously |J | nodes. With this |J |-wise
regression we try to capture the group interdependencies stored in the precision
matrix. This is why we require a multivariate model of the form
B̂J := arg min
BJ∈R|Jc|×|J|
(‖XJ −XJcBJ‖nuc + λJΨ(BJ)) . (3.1)
The nuclear norm is defined as
‖A‖nuc := tr
(√
ATA
)
=
min(n,|J|)∑
i=1
σi(A),
4
where σi(A) are the singular values of a n×|J | matrix A and for a square m×m
matrix B the trace function is defined as tr (B) :=
∑m
i=1Bi,i. Furthermore the
penalty is defined as
Ψ(A) :=
|J|∑
j=1
g(Aj). (3.2)
It is a matrix norm on R|Jc|×|J| (it is the dual matrix norm of an operator norm),
that uses the computational cost effective `1-norm on the columns together with
another norm g on Rp. Here Aj ∈ Rp is equal to the j-th column of the matrix
A on the set Jc, and 0 on the set J . The norm g is defined so that it lower
bounds all ΥS-norms where S ∈ S is any non trivial allowed set of the Ω-norm.
Furthermore the norm g should satisfy the following reflection property
g(βf(J)) = g(β) , where βf(J) := βJ − βJc .
This is a natural condition on g, because for each allowed set S we have
ΥS(βf(S)) = ΥS(β).
Where ΥS is defined as ΥS(β) := Ω(βS) + Ω
Sc(βSc). In order to construct
the norm g we construct a convex set where we will take the gauge function.
Remark that minS∈S ΥS(·) is in general not a norm, therefore we need to take
the convex hull. The convex set is defined through
B :=
⋃
S allowed
BΥS , with BΥS the unit ball of ΥS − norm,
Bg := Conv
(
B ∪ flipJ(B)
)
.
The function flipJ(·) reflects a set along the hyperplane defined by the subset
J . To be more precise for a subset B ⊂ Rp we have
flipJ(B) := {γ : γJ = βJ and γJc = −βJc ,∀β ∈ B}.
Then we can define its gauge function (also known as Minkowski functional) as
follows:
g(x) := inf (λ > 0; x ∈ λBg) .
See Figure 1 for a graphical representation of the gauge function. From this
definition of the g we can see that the following Lemma holds.
Lemma 2. For the gauge function g the following properties hold
(1): g defines a norm on Rp.
(2): g(β) ≤ ΥS(β) ≤ Ω(β) and g(β) ≤ ΥS(βf(J)), ∀S allowed sets.
(3): g(β) =
(
max
S allowed
max
(
Υ∗S(β),Υ
∗
S(βf(J))
))∗
.
Furthermore Υ∗S(z) = max
(
Ω∗(βS),Ω
Sc,∗(βSc)
)
∀β ∈ Rp.
5
Figure 1: Intuition about the gauge function. Left: lower bounding nature,
right: additive nature.
(4): g(βJc) ≤ g(β) for all β ∈ Rp.
Lemma 2 covers the main properties of the gauge function. Result (1) shows
that Ψ is in fact a matrix norm. Result (3) gives a characterization of what the
gauge function is in our case. Results (2) and (4) are the main properties of the
function g, which will be needed to let the error term for the de-sparsified esti-
mator go to 0. Why we chose to construct the Ψ-norm for the |J |-wise regression
as a column sum of this gauge function g will become more evident later in this
paper. Regarding the construction of confidence sets by means of estimating
the precision matrix through the |J |-wise multivariate regression, we will need
to specify the Karush-Kuhn-Tucker (KKT) conditions for the multivariate re-
gression estimator B̂J . The first thing we will need is the subdifferential of a
matrix norm. In the paper of Watson [19] one can find the formulation of the
subdifferential for a norm of a m× n matrix A
∂||A|| =
{
G ∈ Rm×n : ||B|| ≥ ||A||+ tr
(
(B −A)TG
)
, ∀ B ∈ Rm×n
}
.
Furthermore we have the following characterization of the subdifferential
G ∈ ∂||A|| ⇐⇒
{
i) ||A|| = tr
(
GTA
)
ii) ||G||∗ ≤ 1
. (3.3)
Here the dual matrix norm is defined as ||A||∗ = sup
B: ||B||≤1
tr
(
BTA
)
. Let us
briefly note that, by definition of the dual matrix norm, a generalized version
of the Cauchy Schwartz Inequality holds true for matrices
tr
(
ABT
)
≤ ||A|| · ||B||∗.
Therefore the dual of the Ψ-matrix norm is defined as
Ψ∗(A) := sup
B: Ψ(B)≤1
tr
(
BTA
)
.
6
Applying equation (3.3) to the optimal solution of equation (3.1) leads to the
KKT conditions (in the case of ‖XJ −XJcB̂J‖nuc 6= 0):
B̂J is optimal ⇐⇒
1
λJ
XTJc(XJ −XJcB̂J)Σ̂
−1/2
J /n ∈ ∂Ψ(B̂J)
⇐⇒
i) λJΨ(B̂J) = tr
({
XTJc(XJ −XJcB̂J)Σ̂
−1/2
J /n
}T
B̂J
)
ii) λJ ≥ Ψ∗
(
XTJc(XJ −XJcB̂J)Σ̂
−1/2
J /n
)
.
(3.4)
Here we denote Σ̂J := (XJ − XJcB̂J)T (XJ − XJcB̂J)/n (assumed to be non-
singular). Let us additionally define the |J | de-sparsified Ω structured estimator
with the help of the following notations.
TJ := (XJ −XJcB̂J)XJ/n.
The normalizing matrix can then be written as
M :=
√
nΣ̂
−1/2
J TJ
This leads to the definition of the |J | de-sparsified Ω structured estimator. Defin-
ing a de-sparsified estimator in this way lets us deal with group-wise confidence
sets.
Definition 3. The |J | de-sparsified Ω structured estimator is
b̂J := β̂J + T
−1
J (XJ −XJcB̂J)
T (Y −Xβ̂)/n. (3.5)
With these definitions we are now ready to describe the asymptotic behavior of
the estimator (3.5) in the following Theorem.
Theorem 1. Assume that the error in the model (1.1) is i.i.d. Gaussian dis-
tributed  ∼ Nn(0, σ20I). Then with the definition b̂J from (3.5) together with
B̂J as an estimator of the precision matrix and its normalized version Mb̂J we
have
M(b̂J − β0J)/σ0 = N|J|(0, I) + rem,
where the `∞ norm of the reminder term rem can be upper bounded by
‖rem‖∞ ≤
√
nλg(β̂Jc − β0Jc)/σ0
≤
√
nλΥS?(β̂ − β0)/σ0. (3.6)
As we can see from Lemma 2 (4) we can upper bound part of the reminder term
from Theorem 1 as g(β̂Jc − β0Jc) ≤ g(β̂ − β0). By the definition of the gauge
function g, from Lemma 2 (2) we get that
g(β̂Jc − β0Jc) ≤ ΥS(β̂ − β0), ∀ S allowed sets of Ω.
7
But how can we bound ΥS?(β̂−β0)? From van de Geer [16] and Stucky and van
de Geer [14] we can get sharp oracle results for an estimation error expressed in a
measure very close to the ΥS? -norm, where S? is the active set of the oracle, but
the used measure is not quite the ΥS? -norm. A refined version of the theorem
in van de Geer [16] leads to sharp oracle result, which we will use to upper
bound ‖rem‖∞. The Lemma 16 can be found in the Appendix. In conclusion
Theorem 1 together with Lemma 16 leads to the asymptotic normality of the
normalized de-sparsified Ω estimator on the set J . A studentized version leads
to an asymptotic pivot. To get the studentized version one could for example
use Stucky and van de Geer [14] or generalize the more optimal bounds from the
paper van de Geer and Stucky [17]. The results are summarized in the following
corollary.
Corollary 1. Assume that the error in the model (1.1) is i.i.d. Gaussian
distributed  ∼ Nn(0, σ20I). The de-sparsified estimator b̂J is as in (3.5) and
the normalized version Mb̂Jc , together with the multivariate estimator B̂J from
(3.2), as an estimator of the precision matrix. Assume that 0 ≤ δ < 1, and also
that λm < cλJ , with λ
m as in Lemma 16. Furthermore assume
√
nλJζ −→ 0
as n → ∞. We invoke weak decomposability for S and Ω. Assume we have a
consistent estimator of σ0. Then we have
‖M(b̂J − β0J)‖2`2/σ̂ = χ
2
|J|(1 + oIP(1)).
With Corollary 1 asymptotic confidence sets can be constructed. But the size
of the set J is not controlled. One can find an approach with the group LASSO
and the nuclear norm as a penalty in Mitra and Zhang [11], but they need more
assumptions. We only need to assume the usual sparsity assumptions on β0, we
do not assume sparsity on X. It just happens, due to the KKT conditions, that
a sparse surrogate of the precision matrix bounds the remainder term.
4 Second framework: Ω confidence sets
The first framework made use of the gauge function g, which is able to lower
bound all the ΥS-norms associated with the Ω-norm, therefore the remainder
term was asymptotically negligible. But here we will discuss a more direct
approach in order to estimate the precision matrix with the Ω-norm itself. But
there might be a price to pay. This approach was discussed briefly in van
de Geer and Stucky [17] but without mentioning the full consequences of this
approach. In contrast to the first framework, J needs to be a non trivial allowed
set of Ω (complements of allowed sets would also work). It is quite natural to be
interested in allowed sets (or complements of it). We define another multivariate
optimization procedure to get an approximation of the precision matrix as
ĈJ := arg min
CJ∈R|Jc|×|J|
(‖XJ −XJcCJ‖nuc + λJΞ(CJ)) . (4.1)
8
Figure 2: Intuition about the constant CS? .
Here we again use the nuclear norm for its nice KKT properties together with
the following norm
Ξ(A) :=
|J|∑
j=1
Ω(Aj). (4.2)
In fact we use the Ω-norm as a measure of the columns of a |Jc|× |J | matrix A,
where Aj denotes again the j-th column of the matrix A on the set J
c, and 0 on
the set J . One new problem arises in this setting, namely that for all allowed
sets S
ΥS(β) ≤ Ω(β), ∀ β ∈ Rp.
Therefore some work has to be done in order to get good bounds for the estima-
tion error expressed in the Ω-norm. And this is why we will need to modify the
sparsity assumption in order for the reminder term of a de-sparsified version of
β̂J to be asymptotically negligible.
Lemma 3. For any weakly decomposable norm Ω there exists a constant CS?
which may depend on the support S? of the true underlying parameter β
0 such
that
Ω(βSc?) ≤ CS?Ω
Sc?(βSc?),∀β ∈ R
p.
Here S? denotes again the optimal allowed oracle set from Lemma 16.
This means that we need to quantify how far off the ΥS? -norm on S
c
? is compared
to the Ω norm, see Figure 2.
9
For the estimation error expressed in the Ω-norm one can already find oracle
results in the literature. One can see for example the consistency result Propo-
sition 6 in Obozinski and Bach [12]. But the result from Lemma 3 together
with Lemma 16 provides more optimal results for our case. This is due to the
fact, that the sub optimal constant 1/ρ from Obozinski and Bach [12] appears
squared in the bound. Therefore our constant is better suited for our problem.
In the Section 5 we will further discuss this for some widely used examples and
show how to choose the constant CS? for those examples.
Again, as in Section 3 we need to define a de-sparsified version of the estimator
β̂. This will be a different de-sparsified estimator due to a different estimation
of the precision matrix. In a similar fashion to Section 3 we have the following
definitions
TJ := (XJ −XJcĈJ)TXJ/n
Σ̂J := (XJ −XJcĈJ)T (XJ −XJcĈJ)/n.
For the sake of simplicity and readability we keep the same notations as in
Section 3 for all these definitions, even though they are defined through ĈJ and
not B̂J .
Definition 4. The |J | de-sparsified Ω estimator is again defined as
b̂J := β̂J + T
−1
J (XJ −XJcĈJ)
T (Y −Xβ̂)/n. (4.3)
Here M :=
√
nΣ̂−1/2TJ and the normalized version of b̂J is Mb̂J .
Now with the help of Lemma 3 we can formulate the following theorem.
Theorem 2. Assume that the error in the model (1.1) is i.i.d. Gaussian dis-
tributed  ∼ Nn(0, σ20I). Then with the definition b̂J from (4.3) together with
B̂J as an estimator of the precision matrix and its normalized version Mb̂J we
have
M(b̂J − β0J) = N|J|(0, I) + rem.
Where the `∞ norm of the reminder term rem can be upper bounded by
‖rem‖∞ ≤ 2
√
nλCS?ΥS?(β̂ − β0).
Again a similar corollary to Corollary 1 holds for this construction of confidence
regions, but with an additional sparsity assumption. This sparsity assumption
needs to be specified case by case. It depends on the Ω-norm.
5 Examples of penalties and their behavior in
the two frameworks
In this section we try to give the gauge functions g and the constant C∗ for some
of the common norm penalties used in the literature and for some interesting
10
` 1
-n
or
m
L
o
re
n
tz
N
o
rm
A
ll
su
b
se
ts
S
⊂
{1
,.
..
,p
}
a
re
a
ll
o
w
ed
.
Ω
S
c
(β
S
c
)
=
‖β
S
c
‖ 1
.
g
(β
)
:=
‖β
‖ 1
a
n
d
C
S
?
:=
1
.
S
=
{p
,S
−
p
},
w
it
h
S
−
p
⊂
{1
,.
..
,p
−
1
}.
Ω
S
c
(β
S
c
)
=
m
in
a
S
c
∈
A
S
c
1 2
( ∑ j∈
S
c
β
2 j
a
j
+
a
j
) .
g
(·
)
=
‖·
‖ 1
a
n
d
C
S
?
:=
3
/
2
.
G
ro
u
p
L
A
S
S
O
n
or
m
W
ed
g
e
N
o
rm
A
ll
su
b
se
ts
co
n
si
st
in
g
o
f
g
ro
u
p
s
S
=
∪
j
∈
J
G
j
,
J
⊂
{1
,.
..
,g
}
a
re
a
ll
o
w
ed
.
Ω
S
c
(β
S
c
)
=
‖β
S
c
‖ g
r
L
.
g
(β
)
:=
‖β
‖ g
r
L
a
n
d
C
S
?
:=
1
.
A
ll
se
ts
o
f
th
e
fo
rm
S
=
{1
,.
..
,s
},
w
it
h
so
m
e
1
≤
s
≤
p
.
Ω
S
c
(β
S
c
)
=
Ω
(β
S
c
,A
S
c
).
g
(β
)
:=
‖β
‖ 1
a
n
d
C
S
?
:=
√ |S
?
|+
1
.
W
ei
gh
te
d
` 1
-n
or
m
G
ro
u
p
W
ed
g
e
N
o
rm
A
ll
su
b
se
ts
co
n
si
st
in
g
o
f
g
ro
u
p
s
S
=
∪
j
∈
J
G
j
,
J
⊂
{1
,.
..
,g
}
a
re
a
ll
o
w
ed
.
Ω
S
c
(β
S
c
)
=
∑ |S
c
|
i=
1
l |
S
|+
i
|β
| (i
,S
c
)
.
g
(β
)
:=
l p
‖β
‖ 1
a
n
d
C
S
?
:=
l 1 l p
=
o
(l
o
g
(p
))
.
A
ll
se
ts
o
f
th
e
fo
rm
S
=
{G
1
,.
..
,G
s
},
w
it
h
so
m
e
1
≤
s
≤
g
.
Ω
S
c
(
β
S
c
)
=
∥ ∥ ∥ ∥(‖
β
G
|S
|+
1
‖
2
,
..
.,
‖
β
G
g
‖
2
)
T
∥ ∥ ∥ ∥ W
.
g
(β
)
:=
∥ ∥ βG
∥ ∥ grL
a
n
d
C
S
?
:=
√ |S
?
|+
1
.
Table 1: Summary of norm properties.
11
new norm penalties. Furthermore Table 1 gives an overview of the properties
of each example.
5.1 LASSO: the `1 Penalty
As already mentioned the weak decomposable norms all collapse into the `1-
norm due its decomposability. Therefore the gauge function is g(β) = ‖β‖1.
This means that both of the frameworks for constructing asymptotic confidence
sets are in fact the same. Indeed `1 has a constant of CS? = 1.
5.2 Group LASSO
The Group LASSO norm is defined by ‖β‖grL :=
∑g
i=1‖βGi‖`2 , where {G1, ..., Gg}
is a partition of {1, ..., p}. We know that the active sets for this norm are the
groups themselves S = ∪i∈SgGi where Sg is any subset of {1, ..., g}. The gauge
function is the group LASSO itself g(β) =
∑g
i=1‖βGi‖`2 .
Due to the nested `1-nature of the group LASSO penalty, we have similar de-
composable properties as the `1-norm and get CS? = 1.
5.3 SLOPE
The sorted `1 norm together with some decreasing sequence 1 ≥ l1 ≥ l2 ≥ ... ≥
lp > 0 is defined as
Jl(β) := l1|β|(1) + ...+ lp|β|(p).
This was shown to be a norm by Zeng and Figueiredo [20]. The SLOPE was
introduced by Bogdan et al. [3] in order to control the false discovery rate:
β̂SLOPE := arg min
β∈Rp
{
‖Y −Xβ‖2n + λJl(β)
}
.
For the SLOPE we have the following two lemmas.
Lemma 4. For the SLOPE g(β) = lp‖β‖1.
Lemma 5. The SLOPE has CS? = l1/lp.
5.4 Wedge
The wedge norm was introduced in Micchelli et al. [9], and fits in a more broader
structured sparsity concept. This concept is nicely compatible from the view-
point of weakly decomposable norms, as discussed at length in van de Geer [16].
Let us define the convex cone A := {a : a ∈ Rp++, aj ≥ aj+1, j ∈ Nn−1}, where
Rp++ denotes the positive orthant. Then the wedge norm is defined as
‖β‖W = Ω(β;A) := min
a∈A
1
2
p∑
j=1
(
β2j
aj
+ aj
)
,
12
with the notation 0/0 = 0. Define
AS := {aS : a ∈ A}.
Moreover, van de Geer [16] showed that any S satisfying AS ⊂ A is an allowed
set for the wedge norm with ΩS
c
(βSc) := Ω(βSc ,ASc). This leads to S :=
{1, .., s} for any s ∈ {1, ..., p−1} being an allowed set. Hence the wedge estimator
can be defined as
β̂Wedge = arg min
β∈Rp
{
‖Y −Xβ‖2n + λ‖β‖W
}
.
Lemma 6. For the wedge norm the gauge function is the `1-norm g(β) = ‖β‖1,
for all β ∈ Rp.
The next lemma shows that the wedge estimator has an influence on the amount
of sparsity needed for confidence sets. But as the simulations will suggest this
might be improvable.
Lemma 7. For the wedge penalty we have CS? =
√
|S?|+ 1.
5.5 Group Wedge
This is a new idea for a more general wedge norm. It is based on the con-
cept of grouping variables together. Assume that we have g disjoint groups
{G1, ..., Gg} = G with ∪gi=1Gi = {1, ..., p}. Let us denote for a vector β ∈ Rp
the `2-norm on a given group Gj as ‖βGj‖`2 :=
√
|Gj |
√∑
i∈Gj β
2
i . Then for a
vector β we define the following g-dimensional vector
βG := (‖βG1‖`2 , ‖βG2‖`2 , ..., ‖βGg‖`2)T .
Now we are able to define the group wedge in terms of the previously defined
g-dimensional wedge norm on Rg as
‖β‖GWedge := ‖βG‖W .
We recover the wedge penalty again if we set the groups to beGi := {i} for any i ∈
{1, ..., p}. The first lemma shows that we have a norm again, the proof can be
found in Section 8.
Lemma 8. The group Wedge is in fact a norm.
Lemma 9. The active sets are of the form S = ∪i∈SgGi for some subset of
group indices Sg ⊂ {1, ..., g}, and we have
ΩS
c
(βSc) = ‖(‖βGs+1‖`2 , ..., ‖βGg‖`2)T ‖W .
Moreover, the lower bounding gauge norm is the the Group LASSO norm with
wedge groups g(β) =
∑g
i=1‖βGg‖`2 .
Lemma 10. For the group wedge penalty we have CS? =
√
|S?|+ 1, where S?
denotes the oracle set.
13
5.6 Lorentz norm
Let us first define the Lorentz Cone (also known as the Ice Cream Cone):
A :=


a1...
ap−1
ap
 ∈ Rp++
∣∣∣∣∣∣∣∣ap ≥ ‖
 a1...
ap−1
‖`2

In a similar fashion to the definition of the wedge norm, the Lorentz norm is
‖β‖Lo := 12 mina∈A
p∑
i=1
(
β2i
ai
+ ai
)
. This next lemma shows, that the Lorentz norm
lets the index p always be part of the preferred active sets.
Lemma 11. For the Lorentz norm it holds true that all the allowed sets contain
p and are of the form
S = {p, ... any combination of other variables} .
And we get the next lemma.
Lemma 12. For the Lorentz norm g(·) = ‖·‖1.
Lemma 13. For the Lorentz norm CS? = 3/2.
The Lorentz norm can be generalized to include any set P ⊂ {1, .., p} in the
allowed sets. The generalized convex cone is
B :=
{
b ∈ Rp++
∣∣bj ≥ ‖bP c‖`2 ∀j ∈ P} ,
and the generalized Lorentz norm can be defined as
‖β‖genLo :=
1
2
min
b∈B
p∑
i=1
(
β2i
bi
+ bi
)
.
Now by an analogous proof to the proof of Lemma 11, we can see that the allowed
sets of the generalized Lorentz norm always contain the set P . In particular an
allowed set S is of the form S = P ∪ B, with B ⊂ P c being any subset of the
complement of P . The gauge function does not change, it is the `1-norm and
we still get a constant of CS? = (|P |+ 2)/2.
6 Simulations
We look at the following linear model: Y = Xβ0 + , where we have n = 100
observations and p = 150 variables with  ∼ N (0, I). The design X is randomly
chosen, such that the covariance matrix has the following Toeplitz structure
Σi,j = 0.9
|i−j|. The underlying parameter vector β0 is chosen to be the regularly
decreasing sequence
β{1,...,s0} :=
(
4, 4− 2
s0 − 1
, 4− 2 · 2
s0 − 1
, ..., 4− (s0 − 2) ·
2
s0 − 1
, 2
)T
,
14
where s0 = |S0| will be different values. This structure of active set fits nicely
in the wedge framework. Therefore to find a solution for the unknown β0 we
use the wedge
β̂Wedge = arg min
β∈Rp
‖Y −Xβ‖2n + λmina∈A 12
p∑
j=1
(
β2j
aj
+ aj
) .
Now we will construct confidence sets based on the two frameworks for the
point-wise sets {1},{2},...,{p}. For each of these p sets we compute r = 100
repetitions. To find the solution of the LASSO (`1 is the gauge function in this
case), the glmnet R package Simon et al. [13] has been used. To solve the wedge
the same code as in Micchelli et al. [9] has been used. The following two cases
have been considered: s0 = 5 and s0 = 18. Let us remark that n/ log(p) ≈ 20
and (n/ log(p))2/3 ≈ 7. For each case the average coverage out of these 100
replications has been computed together with the average confidence set length.
The penalty level for the node-wise LASSO and node-wise Wedge have been
chosen such that the average coverage are about the same, in order to compare
their average set lengths. Of course a reasonable penalty level for practical
applications is up for debate.
Sparsity s0 = 5: For a very sparse setting the simulations, as seen in Figure 3,
show that there is no essential difference between using the node-wise LASSO or
the node-wise Wedge in order to construct the estimate of the precision matrix.
0 50 100 150
0.4
0.5
0.6
0.7
0.8
0.9
1.0
LASSO
β indices
av
. C
ov
er
ag
e
0.92
0 50 100 150
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Wedge
β indices
av
. C
ov
er
ag
e
0.93
0 50 100 150
0
1
2
3
4
5
LASSO
β indices
av
. L
en
gt
h
1.24
0 50 100 150
0
1
2
3
4
5
Wedge
β indices
av
. L
en
gt
h
1.27
Figure 3: Left: Average coverage, Right: Average length, s0 = 5, λLASSO =
15.5, λWedge = 15 and in red are the mean values over all point-wise sets of the
active set S0.
Sparsity s0 = 18: Surprisingly for a less sparse setting the simulations, see Fig-
ure 4, still show no noticeable difference between the node-wise LASSO or the
node-wise Wedge. This might indicate that there could be a more direct way
bound the estimation error expressed in the Ω norm, and that the bound of the
remainder term
√
nλCS?ΥS?(β̂J − β0J) might not be optimal for the wedge.
15
0 50 100 150
0.4
0.5
0.6
0.7
0.8
0.9
1.0
LASSO
β indices
av
. C
ov
er
ag
e
0.94
0 50 100 150
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Wedge
β indices
av
. C
ov
er
ag
e
0.95
0 50 100 150
0
1
2
3
4
5
LASSO
β indices
av
. L
en
gt
h
1.46
0 50 100 150
0
1
2
3
4
5
Wedge
β indices
av
. L
en
gt
h
1.46
Figure 4: Left: Average coverage, Right: Average Length, s0 = 18, λLASSO =
12, λWedge = 10, and in red are the mean values over all point-wise sets of the
active set S0.
7 Conclusion
Two frameworks for penalized estimators which incorporate structured spar-
sity patterns have been proposed. The first framework makes use of the gauge
function, which is in most cases an `1 type norm due to the additivity of the
lower bounding weak decomposable norms. The second framework is penal-
ized by the structured sparse norm itself. They are both quite general in the
sense that they can be used in case of any weakly decomposable norm penalty,
but they have their own properties regarding sparsity assumptions. Interest-
ingly the simulations suggest that at least for the presented Toeplitz case both
frameworks seem to perform nearly indistinguishable, even for less strict spar-
sity assumptions. Therefore it would be very interesting for future research to
further understand if oracle results for the estimation error expressed in the
weakly decomposable norms can be achieved.
8 Proofs
In the dual world inequalities for norms change the direction.
Lemma 14. Let Ω(·) and Υ(·) be any two norms on Rp satisfying Υ(β) ≤
Ω(β),∀β ∈ Rp. Then for the corresponding dual norms we have the following
inequality:
Υ∗(ω) ≥ Ω∗(ω),∀ω ∈ Rp.
Proof. First, let us remark that the unit balls BΥ := {β : Υ(β) ≤ 1} and
BΩ := {β : Ω(β) ≤ 1} fulfill the following
BΥ ⊃ BΩ.
This is due to the fact that for all β ∈ BΩ we have
Υ(β) ≤ Ω(β) ≤ 1,
16
which means that such β are also element of the BΥ-Ball. Now if we look at
the definition of the dual norm together with the fact that the supremum over
the set BΥ can only be bigger than over the set BΩ, we get
Υ∗(ω) = sup
β∈BΥ
ωTβ ≥ sup
β∈BΩ
ωTβ = Ω∗(ω),∀ω ∈ Rp.
Due to the disjoint nature of the definition of ΥS as a sum of two norms on the
set S and set Sc we can get an explicit formula for the dual norm.
Lemma 15. For the weakly decomposable norm it holds true that
Υ∗S(β) = max (Ω
∗(βS),Ω
∗
Sc(βSc)) ,∀β ∈ Rp.
Proof. Let us show how to lower and upper bound it.
Inequality 1: To show ”≥”:
Υ∗S(ω) := sup
ΥS(β)=1
βTω ≥ sup
ΥS(β)=1,
β=βS
βTω
= sup
Ω(βS)=1
βTω ≥ Ω∗(ωS)
A similar result holds true if we restrict β to βSc . Therefore the maximum lower
bounds the dual of the ΥS-norm.
Inequality 2: To show ”≤”:
Υ∗S(ω) = sup
ΥS(β)=1
βTω = sup
ΥS(β)=1
{
βTS ωS + β
T
ScωSc
}
= sup
ΥS(β)=1
{
βTS ωSΩ(βS)
Ω(βS)
+
βTScωScΩ
Sc(βSc)
ΩSc(βSc)
}
≤ sup
ΥS(β)=1
{
Ω∗(ωS)Ω(βS) + Ω
Sc
∗ (ωSc)Ω
Sc(βSc)
}
= sup
a+b=1
{
Ω∗(ωS)a+ Ω
Sc
∗ (ωSc)b
}
= max
(
Ω∗(ωS),Ω
Sc
∗ (ωSc)
)
Proof of Lemma 2.
(1): The gauge function is again a norm on Rp, because 0 is in the convex set,
see for example Clarke [4] Theorem 2.36.
(2): First of all, the unit ball Bg of norm g contains all the unit balls BΥS and
flipJ(BΥS ), therefore
g(β) ≤ ΥS(β) ∀ S allowed and
g(β) ≤ ΥS(βf(J)) ∀ S allowed.
17
(3): To prove that the dual norm of g is the maximum we need to make the
following observations. First, from Lemma 14 together with Lemma 2 (2)
we have
Υ∗S(β) ≤ g∗(β) ∀ S allowed.
Due to the fact that this holds for all allowed sets S we get
max
(
max
S all.
Υ∗S(β), max
S all.
Υ∗S(βf(J))
)
≤ g∗(β).
To prove the other inequality we need to look at the definition of the dual
norm g∗:
g∗(z) = max
g(x)≤1
zTx.
Because the convex hull in the definition of g is the set of all convex
combinations of points in B ∪ flipJ(B) we have that
x ∈{y : g(y) ≤ 1} ⇔ x ∈ Bg
⇔There exists some n ∈ Rp and
n∑
i
αi = 1, αi ≥ 0,
with a sequence of bi ∈ B ∪ flipJ(B),
such that we can write x =
n∑
i=1
αibi.
That is why we can write
g∗(z) = max
n∈N,
∑n
i αi=1,
bi∈B∪flipJ (B)
n∑
i=1
(
αiz
T bi
)
≤ max
n∈N,
∑n
i αi=1
n∑
i=1
(
αi · max
b∈B∪flipJ (B)
zT b
)
≤ 1 ·max
(
max
S all.
Υ∗S(z), max
S all.
Υ∗S(zf(J))
)
.
Thus equality holds, and one can easily see that
max
(
max
S all.
Υ∗S(z), max
S all.
Υ∗S(zf(J))
)
is a norm again. Now the condition
Ω(βf(J)) = Ω(β) for all β ∈ Rp forces all ΥS-norms to have the same
symmetrical property, and thus BΥS = flipJ(BΥS ). Therefore the one
maximum can be omitted and the claim is proven. For the characterization
of the dual of the ΥS-norm we can just apply Lemma 15.
18
(4): The function flipJ leads to g(βJ − βJc) = g(β). Hence
g(βJc) = g(
1
2
(β − βJ) +
1
2
βJc)
≤ 1
2
g(β) +
1
2
g(βJ − βJc)
=
1
2
g(β) +
1
2
g(β)
= g(β)
Proof of Theorem 1. Then
M(b̂J − β̂J) =
√
nΣ̂
−1/2
J (XJ −XJcB̂J)
TXJ/n · ...
·
(
T−1J (XJ −XJcB̂J)
T (Xβ0 −Xβ̂ + )
)
=
√
nΣ̂
−1/2
J (XJ −XJcB̂J)
T
(
+X(β0 − β̂)
)
= Σ̂
−1/2
J (XJ −XJcB̂J)
T · ...
·
(
XJc(β
0 − β̂)J +XJc(β0 − β̂)Jc + 
)
/
√
n
= Σ̂
−1/2
J (XJ −XJcB̂J)
TXJc(β
0 − β̂)Jc/
√
n+ ...
+ Σ̂
−1/2
J (XJ −XJcB̂J)
T /
√
n+ ...
− Σ̂−1/2J (XJ −XJcB̂J)
TXJ(β̂ − β0)J/
√
n
We can simplify the term
Σ̂
−1/2
J (XJ −XJcB̂J)
TXJ(β̂ − β0)J/
√
n = M(β̂J − β0J)
That is why we can conclude that
M(b̂J − β0J) = M(b̂J − β̂J) +M(β̂J − β0J)
= Σ̂
−1/2
J (XJ −XJcB̂J)
T /
√
n+ ...
+
√
nΣ̂
−1/2
J (XJ −XJcB̂J)
TXJc(β
0 − β̂)Jc/n
= Σ̂
−1/2
J (XJ −XJcB̂J)
T /
√
n︸ ︷︷ ︸
Gaussian Random Variable
+λZ(β0 − β̂)Jc/
√
n︸ ︷︷ ︸
Remainder Term
where Z comes from the KKT conditions which fulfills:
Ψ∗(Z) ≤ 1
tr(ZT B̂J) = Ψ(B̂J).
The remainder term can be bounded with the generalized Cauchy Schwartz
inequality in the `∞-norm by
19
λ‖Z(β0 − β̂)Jc‖∞/
√
n = λ max
1≤j≤|J|
Zj(β
0 − β̂)Jc/
√
n
≤ λ max
1≤j≤|J|
g∗(Zj)g(β
0
Jc − β̂Jc)/
√
n
≤ λΨ∗(Z)g(β0Jc − β̂Jc)/
√
n
≤ λg(β0Jc − β̂Jc)/
√
n (KKT conditions.)
Dividing everything by σ0 leads to the result.
Proof of Lemma 3. Let us first observe:
Ω(β0 − β̂) ≤ Ω(β0S? − β̂S?) + Ω(β
0
S∗c − β̂S∗c )
≤ Ω(β0S? − β̂S?) + Ω
S∗
c
(β0S∗c − β̂S∗c )
− ΩS
∗c
(β0S∗c − β̂S∗c ) + Ω(β
0
S∗c − β̂S∗c )
≤ ΥS?(β0 − β̂) + ∆S∗c (β0 − β̂).
Here we define ∆S∗c (β) := Ω(βS∗c )−ΩS
∗c
(βS∗c ). But we are left with another
problem, how to bound ∆S∗c . Understanding this distance will give us a bound
on how far apart the weakly decomposable norm and the norm from the triangle
inequality are. Now let us take the optimal constant CS? , which may depend on
the active set of the oracle |S?|, such that Ω(βS∗c ) ≤ CS? ·ΩS
∗c
(βS∗c ), ∀β ∈ Rp.
Then with this we can write
∆S∗c (β
0 − β̂) ≤ (CS? − 1)ΩS
∗c
(β0S∗c − β̂S∗c ) ≤ (CS? − 1)ΥS?(β
0 − β̂).
In the last inequality we have used the weak decomposability condition. There-
fore Ω(β0 − β̂) ≤ CS?ΥS?(β0 − β̂).
Proof of Theorem 2. The first part follows directly the proof of Theorem 1,
with Ξ-norm instead of Ψ-norm. The remainder term can be bounded with the
generalized Cauchy Schwartz inequality in the `∞-norm by
λ‖Z(β0 − β̂)Jc‖∞/
√
n = λ max
1≤j≤|J|
Zj(β
0 − β̂)Jc/
√
n
≤ λ max
1≤j≤|J|
Ω∗(Zj)Ω(β
0
Jc − β̂Jc)/
√
n
≤ λΞ∗(Z)Ω(β0Jc − β̂Jc)/
√
n
≤ λΩ(β0Jc − β̂Jc)/
√
n (KKT conditions.)
≤ λ2Ω(β0 − β̂)
The last inequality comes directly from the weak decomposability of the allowed
20
set J :
Ω(β0Jc − β̂Jc) ≤ Ω
(
(β0Jc − β̂Jc) + (β0J − β̂J)− (β0J − β̂J)
)
≤ Ω(β0 − β̂) + Ω(β0J − β̂J)
≤ 2Ω(β0 − β̂).
Now with the calculation in the proof of Lemma 3 and by dividing everything
by σ0 the proof is finished.
Proof of Lemma 4. First of all, let us see that lp‖β‖1 indeed is a lower bound
for all weakly decomposable norms of Ω = Jl. From Stucky and van de Geer
[14] we know that for any subset S ⊂ {1, ..., p} we have
ΥS(β) =
|S|∑
j=1
lj |β|(j,S) +
|Sc|∑
i=1
l|S|+i|β|(i,Sc)
with 1 ≥ l1 ≥ l2 ≥ ... ≥ lp > 0 and |β|(1,Sc) ≥ · · · ≥ |β|(r,Sc) being the ordered
sequence in {βi : i ∈ Sc}. We can now lower bound each li and lj by the
minimum of the decreasing sequence, namely lp. That is why we get the sought
lower bound
lp‖β‖1 ≤ ΥS(β) ≤ Ω(β) ∀S ⊂ {1, ..., p} and all β ∈ Rp.
Therefore λp‖β‖1 is a candidate for the gauge function, but we need to show
that this norm is the best lower bounding norm. Assume by contradiction that
there is another norm g(·) on Rp such that
lp‖β‖1 ≤ g(β) ≤ ΥS(β) ∀S ⊂ {1, ..., p} and all β ∈ Rp,
and that there exists γ ∈ Rp such that lp‖γ‖1 < g(γ).
Denote the k-th standard basis vectors in Rp as ek. Where ek is the vector
having a one at the k-th entry and zeroes otherwise. Then γ can be written in
the standard basis as a combination of the standard basis vectors
γ = v1e1 + v2e2 + ...+ vpep.
From the above assumption and the fact that the set without the k-th index
{1, ..., p}r{k}, denoted briefly as r{k}, is an allowed set, we have that for each
standard basis vector ek the following needs to hold true
lp‖ek‖1 ≤ g(ek) ≤ Υr{k}(ek) ∀k ∈ {1, ..., p}.
Inserting the values ‖ek‖1 = 1 and Υr{k}(ek) = lp ∀k ∈ {1, .., p} leads to
lp ≤ g(ek) ≤ lp.
21
Therefore we can conclude that g(ek) = lp for all k ∈ {1, ..., p}. Now applying
the triangle inequality tho g we have
g(γ) ≤ |v1|g(e1) + ...+ |vp|g(ep) = (|v1|+ ...+ |vp|)lp.
On the other hand we get lp‖γ‖1 = lp(|v1|+ ...+ |vp|). This now clearly contra-
dicts our assumption because lp‖γ‖1 ≮ g(γ) ≤ lp‖γ‖1.
Proof of Lemma 5. By Ω(βS) =
∑|S|
j=1 lj |β|(j,S) and upper bounding all lj , j =
{1, ..., |S|} we have
Ω(βS∗c )/l1 ≤ ‖βS∗c ‖1.
In a similar fashion by ΩS
c
(βSc) =
∑|Sc|
i=1 l|S|+i|β|(i,Sc) and lower bounding all
li, i = {|S|+ 1, ..., p}, we get
ΩS
∗c
(βS∗c )/lp ≥ ‖βS∗c‖1.
Combining these two inequalities leads to
ΩS
∗c
(βS∗c )l1/lp ≥ Ω(βS∗c ).
for the SLOPE penalty CS? = l1/lp = o(log(p)). The last equality comes from
the Bonferroni l-sequence choice in Bogdan et al. [3].
Proof of Lemma 6. First we know by Micchelli et al. [9] that ‖β‖1 ≤ Υ(β) for
all allowed sets S and all β ∈ Rp. Now in order to show that this is the best
lower bounding norm, let us assume by contradiction that there exists another
norm g(·) which is strictly better than ‖·‖1:
‖β‖1 ≤ g(β) ≤ Υ(β) ∀S allowed ∀β ∈ Rp,
∃γ ∈ Rp such that ‖γ‖1 < g(γ) ≤ Υ(γ) ∀S allowed.
Define the standard basis as ek, k ∈ {1, ..., p} being the vector having a one at
the k-th entry and zero entries otherwise. Let us fix any allowed set S. It is
straight forward to check that
Υ(e1) = 1, and Υ(es+1) = 1.
By the assumption we get that
1 = ‖es+1‖1 ≤ g(es+1) ≤ Υ(es+1) = 1 ∀S allowed.
And similarly for the first standard basis vector e1 we have
1 = ‖e1‖1 ≤ g(e1) ≤ Υ(e1) = 1 ∀S allowed.
Now because s ∈ {1, ..., p− 1} we get that:
g(ek) = 1, for any k ∈ {1, ..., p}.
22
So we know the values that g attains for the standard basis. With this we can
conduct the following contradiction. The vector γ has a unique representation
in the standard basis γ = v1e1 + v2e2 + ... + vpep, and therefore we can apply
the triangle inequality p times to get:
g(γ) ≤ |v1|g(e1) + |v2|g(e2) + ...+ |vp|g(ep)
= |v1|+ |v2|+ ...+ |vp|
= ‖γ‖1
This contradicts our assumption that ‖γ‖1 < g(γ), and the claim is proven.
Proof of Lemma 7. For any allowed set S, the weakly decomposable Υ-norm
consists of the following two parts
Ω(βSc) = min
aSc∈ASc
1
2
( ∑
j∈Sc
(β2j
aj
+ aj
)
+ s · as+1
)
,
ΩS
c
(βSc) = min
aSc∈ASc
1
2
( ∑
j∈Sc
β2j
aj
+ aj
)
.
Here we have used that aj ≥ aj+1 for all 1 ≥ j ≤ p−1. Because of the structure
of the cone A we have
Ω(βSc) = min
aSc∈ASc
1
2
( p∑
j=s+2
(β2j
aj
+ aj
)
+
β2s+1
as+1
+ (s+ 1)as+1
)
≤ min
aSc∈ASc
1
2
( p∑
j=s+2
(β2j
aj
+ (s+ 1)aj
)
+
β2s+1
as+1
+ (s+ 1)as+1
)
=
√
s+ 1 min
aSc∈ASc
1
2
( p∑
j=s+1
β2j√
s+ 1aj
+
√
s+ 1aj
)
.
In the second inequality we added
∑p
j=s+2 saj ≥ 0, and in the last inequal-
ity we take
√
s+ 1 outside the minimum. Now in this setting we know that
for aSc ∈ ASc we have as+1 ≥ as+2 ≥ ... ≥ ap ≥ 0. Furthermore a
′
Sc :=
(
√
s+ 1as+1,
√
s+ 1as+2, ...,
√
s+ 1ap)
T ∈ ASc , in fact any sequence aSc ∈ ASc
can be displayed by a sequence which is multiplied by
√
s+ 1. Therefore
Ω(βSc) ≤
√
s+ 1 min
a
′
Sc
∈ASc
1
2
( p∑
j=s+1
β2j
a
′
j
+ a
′
j
)
≤
√
s+ 1 · ΩS
c
(βSc)
Proof of Lemma 9. The `2-norm does not have any non trivial active sets, and
the g-dimensional wedge norm has active sets S = {1, ..., s} for any s ∈ {1, ..., g}.
23
Combining theses facts leads to the conclusion that only for active sets of the
form S = ∪i∈SgGi we have weak decomposability:
‖βS‖grW + ‖βSc‖grW ≤ ‖β‖grW .
Because of the definition of the group wedge as a composition of the wedge and
`2-norm this is the best lower bound. For the gauge function g it is easy to see
that by applying Lemma 6, we get the Group LASSO.
Proof of Lemma 8.
(1): ‖β‖grW = 0 ⇐⇒ ‖βGi‖`2 = 0 ∀i ∈ {1, .., g} ⇐⇒ β ≡ 0.
(2): The following calculations hold true:
‖aβ‖grW = ‖(‖aβGs+1‖`2 , ..., ‖aβGg‖`2)T ‖W
= ‖a(‖βGs+1‖`2 , ..., ‖βGg‖`2)T ‖W
= a‖β‖grW .
(3): The triangle inequality holds due to the properties of the wedge and `2-
norms.
‖β + γ‖grW = ‖(‖βGs+1 + γGs+1‖`2 , ..., ‖βGg + γGg‖`2)T ‖W
≤ ‖(‖βGs+1‖`2 + ‖γGs+1‖`2 , ..., ‖βGg‖`2 + ‖γGg‖`2)T ‖W
≤ ‖βG + γG‖W
≤ ‖βG‖W + ‖γG‖W = ‖β‖grW + ‖γ‖grW
Proof of Lemma 10. By applying Lemma 7 in this context, together with S?
being the optimal active groups, we immediately get the desired result.
Proof of Lemma 11. By van de Geer [16] we know that for the structured spar-
sity norms, as introduced in Micchelli et al. [9], it holds that
S is an allowed set ⇐⇒ AS := {aS : a ∈ A} ⊂ A.
Let us distinguish two cases, in order to proof the lemma.
Case 1: Assume p /∈ S.
Therefore AS consists of vectors with the p-th variable set to zero. This means
that there exists at least one vector a such that aS is not in A
ap,S = 0  ‖
 a1...
ap−1

S
‖`2 .
In other words AS * A. Therefore sets S which do not contain p cannot be
allowed sets.
24
Case 2: Assume that the set S satisfies S 3 p.
For each vector aS in AS we have
ap,S = ap ≥ ‖
 a1...
ap−1
‖`2 ≥ ‖
 a1...
ap−1

S
‖`2 .
The first inequality is due to a being in A. For the second inequality it suffices
to see that the `2 norm can only decrease by setting certain values to zero.
therefore we know that any set S which contains p fulfills AS ⊂ A.
Proof of Lemma 12. Again by Micchelli et al. [9] we know that ‖β‖1 ≤ Υ(β) for
all allowed sets S and all β ∈ Rp. Define the standard basis as ek, k ∈ {1, ..., p}.
Let us fix any allowed set S from Lemma 11. We can calculate that
Υ(ek) =
√
2 if k ∈ S r {p},Υ(ek) = 1 if k /∈ S r {p},
Υ(ep) = 1.
Taking the special allowed set S = {g} we have
1 = ‖ek‖1 ≤ g(ek) ≤ Υ{g}(ek) = 1,∀k ∈ {1, .., p}.
This leads to g(ek) = 1,∀k ∈ {1, .., p}. Therefore we can use the same idea of
the proof from Lemma 6, and we get that g(·) = ‖·‖1.
Proof of Lemma 13. We have that
ΩS
c
(βSc) = min
aSc∈ASc
1
2
∑
j∈Sc
(
β2j
aj
+ aj
)
= ‖βSc‖1.
This is due to p /∈ Sc and therefore the {aj : j ∈ Sc} can be chosen independently
of each other, leading to the minimum aj = βj . Furthermore we have the
following upper bound:
Ω(βSc) = min
a∈A
1
2
∑
j∈Sc
(
β2j
aj
+ aj
)
+ ap
≤ min
a∈A
1
2
∑
j∈Sc
(2βj) + ‖βSc‖2 with aj = βj
= ‖βSc‖1 +
1
2
‖βSc‖2
≤ 3
2
‖βSc‖1 =
3
2
ΩS
c
(βSc).
Which leads to the desired constant.
25
Appendix: A refined Sharp Oracle Inequality
Let us first remind us of the definition of the theoretical lambda
λm := max
(
Ω∗(TXS?),Ω
Sc?∗(TXSc?)
)
/n = Υ∗S?
(
TX
)
/n. Lemma 16 refines
the sharp oracle result from van de Geer [16]. In particular, the sharp oracle
inequality from van de Geer [16] measures a variation of the estimation error in
the following way Ω(β̂S? −β∗) + ΩS
c
?∗(β̂Sc?), with S? = supp(β
∗). Let us remark
here that the optimal oracle parameter β∗ may not be equal to β0. Therefore
we have no guarantee to get an upper bound on the estimation error expressed
as ΥS?(β̂ − β0). But this is needed for both confidence frameworks to work.
Therefore we will rework Theorem 4.1 from van de Geer [16] to make S and β
independent of each other. Let us furthermore define the Ω-effective sparsity as
in van de Geer [16] and denote it Γ2Ω(L, S).
Lemma 16 (Refined Sharp Oracle Inequality). Assume that 0 ≤ δ < 1, and
also that λ
m
λ = c with 0 < c < 1. We invoke weak decomposability for S and
Ω. Here the active set S and parameter vector β can be chosen independently.
Then it holds true that
ΥS?(β̂ − β0) ≤ min
β,S,δ
(
ΥS(β
0 − β) + C1λΓ2Ω(LS , S)+
+
C2
λ
‖X(β − β0)‖2n + C3Ω(βSc)
)
, (8.1)
with LS :=
λ+λm
λ−λm
1+δ
1−δ , constants C1 =
[(1+δ)(1+c)]2
2δ(1−c) , C2 =
1
2δ(1−c) , C3 =
2
δ(1−c)
and
S? := arg min
S
min
β,δ
[
ΥS(β
0 − β) + C1λΓ2Ω(LS , S) + ...
+
C2
λ
‖X(β − β0)‖2n + C3Ω(βSc)
]
.
Proof. Follows directly from the proof of the main theorem in van de Geer [2014]
and the triangle inequality.
26
References
[1] F. Bach, R. Jenatton, J. Mairal, and G. Obozinski. Optimization with
sparsity-inducing penalties. In Foundations and Trends in Machine Learn-
ing, volume 4, pages 1–106, 2012.
[2] F.R. Bach. Structured sparsity-inducing norms through submodular func-
tions. In Advances in Neural Information Processing Systems (NIPS), vol-
ume 23, pages 118–126, 2010.
[3] M. Bogdan, E. van den Berg, C. Sabatti, W. Su, and E. J. Candès.
SLOPE—adaptive variable selection via convex optimization. Annals of
Applied Statistics, 9(3):1103–1140, 2015.
[4] F. Clarke. Functional analysis, calculus of variations and optimal control.
Graduate texts in mathematics. Springer, London, 2013. ISBN 978-1-4471-
4819-7.
[5] A. Javanmard and A. Montanari. Confidence intervals and hypothesis test-
ing for high-dimensional regression. Journal of Machine Learning Research,
15:2869–2909, 2014.
[6] A. Maurer and M. Pontil. Structured sparsity and generalization. Journal
of Machine Learning Research, 13(1):671–690, 2012.
[7] N. Meinshausen. Group bound: confidence intervals for groups of variables
in sparse high dimensional regression without assumptions on the design.
Journal of the Royal Statistical Society. Series B. Statistical Methodology,
77(5):923–945, 2015.
[8] N. Meinshausen and P. Bühlmann. High-dimensional graphs and variable
selection with the lasso. Annals of Statistics, 34(3):1436–1462, 06 2006.
[9] C.A. Micchelli, J. Morales, and M. Pontil. A family of penalty functions for
structured sparsity. In J.D. Lafferty, C.K.I. Williams, J. Shawe-Taylor, R.S.
Zemel, and A. Culotta, editors, Advances in Neural Information Processing
Systems 23, pages 1612–1623. Curran Associates, Inc., 2010.
[10] C.A. Micchelli, J. Morales, and M. Pontil. Regularizers for structured
sparsity. Advances in Computational Mathematics, 38(3):455–489, 2013.
[11] Ritwik Mitra and Cun-Hui Zhang. The benefit of group sparsity in group
inference with de-biased scaled group Lasso. Electronic Journal of Statis-
tics, 10(2):1829–1873, 2016.
[12] Guillaume Obozinski and Francis Bach. Convex Relaxation for Com-
binatorial Penalties. Technical report, May 2012. URL https://hal.
archives-ouvertes.fr/hal-00694765. 35 page.
27
[13] Noah Simon, Jerome Friedman, Trevor Hastie, and Rob Tibshirani. Reg-
ularization paths for cox’s proportional hazards model via coordinate de-
scent. Journal of Statistical Software, 39(5):1–13, 2011.
[14] B. Stucky and S. van de Geer. Sharp oracle inequalities for square root
regularization. ArXiv e-prints, 2015. URL http://arxiv.org/abs/1509.
04093.
[15] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of
the Royal Statistical Society. Series B., 58(1):267–288, 1996.
[16] S. van de Geer. Weakly decomposable regularization penalties and struc-
tured sparsity. Scandinavian Journal of Statistics. Theory and Applications,
41(1):72–86, 2014.
[17] S. van de Geer and B. Stucky. χ2-confidence sets in high-dimensional re-
gression. In Statistical analysis for high-dimensional data: The Abel Sym-
posium 2014, volume 11 of Abel Symposia, pages 279–306, Cham, 2016.
Springer.
[18] S. van de Geer, P. Bühlmann, Y. Ritov, and R. Dezeure. On asymptotically
optimal confidence regions and tests for high-dimensional models. Annals
of Statistics, 42(3):1166–1202, 06 2014.
[19] G. A. Watson. Characterization of the subdifferential of some matrix norms.
Linear Algebra and its Applications, 170:33–45, 1992.
[20] X. Zeng and M. A. T. Figueiredo. Decreasing weighted sorted l1 regular-
ization. IEEE Signal Processing Letters, 21(10):1240–1244, June 2014.
[21] C.H. Zhang and S. Zhang. Confidence intervals for low dimensional param-
eters in high dimensional linear models. Journal of the Royal Statistical
Society. Series B., 76(1):217–242, 2014.
28

