Asymptotic Confidence Regions for
Highdimensional Structured Sparsity.
Benjamin Stuckyâˆ—, Sara van de Geerâˆ—
June 29, 2017
Abstract
In the setting of high-dimensional linear regression models, we propose
two frameworks for constructing pointwise and group confidence sets for
penalized estimators which incorporate prior knowledge about the orga-
nization of the non-zero coefficients. This is done by desparsifying the
estimator as in van de Geer et al. [18] and van de Geer and Stucky [17],
then using an appropriate estimator for the precision matrix Î˜. In order
to estimate the precision matrix a corresponding structured matrix norm
penalty has to be introduced. After normalization the result is an asymp-
totic pivot. The asymptotic behavior is studied and simulations are added
to study the differences between the two schemes.
Keywordsâ€” Asymptotic confidence regions, structured sparsity,
high-dimensional linear regression, penalization.
1 Introduction
We focus on the basic high dimensional linear regression model, which is at the
core of understanding more complex models:
Y = XÎ²0 + . (1.1)
Here Y âˆˆ Rn is an observable response variable, X is a given nÃ—p design matrix
with p >> n, Î²0 âˆˆ Rp is a parameter vector of unknown coefficients and  âˆˆ Rn
is unobservable noise. Due to the high-dimensionality of the design the question
arises as to find the solution to an underdetermined system. The idea to restrict
ourselves to sparse solutions has become the new paradigm to solve this problem
for high-dimensional data. In such a setting, the LASSO estimator (introduced
by Tibshirani [15]) is the most widely used method in pursuance of estimating
the unknown parameter vector Î²0, while avoiding the high-dimensional problem
of overfitting:
Î²Ì‚`1 := arg min
Î²âˆˆRp
{
â€–Y âˆ’XÎ²â€–2n + 2Î»Lâ€–Î²â€–1
}
.
âˆ—Seminar for Statistics, ETH ZuÌˆrich, Switzerland
1
ar
X
iv
:1
70
6.
09
23
1v
1 
 [
m
at
h.
ST
] 
 2
8 
Ju
n 
20
17
The loss function is defined as â€–Y âˆ’XÎ²â€–2n :=
âˆ‘n
j=1(Y âˆ’XÎ²)2j/n, and â€–Î²â€–1 de-
notes the `1-norm. The main purpose of the added `1-norm penalty is to achieve
an entry-wise sparse Î²Ì‚`1 solution, while at the same time the least squares loss
ensures good prediction properties. Furthermore the constant Î»L > 0 is the
penalty level, regulating the amount of sparsity introduced to the solution.
The `1-norm penalty is a simple convex relaxation of the non-convex `0 penalty
(â€–Î²â€–`0 := #{i : Î²i 6= 0}). Let us recall that the `1-norm penalty does not
promote any specific sparsity structure. In other words the LASSO estimator
does not assume anything about the organization of the non-zero coefficients.
In this sense the LASSO estimator does not incorporate any prior knowledge of
the structure of the true unknown active set S0 := {i : Î²0,i 6= 0}. In practice
however, prior knowledge is often available. Prior knowledge may emerge from
physical systems or known biological processes. For the purpose of integrating
the available prior information, the `1-norm penalty needs to be replaced in such
a way, that the new penalty reflects this knowledge. One can find many examples
of such penalties and their properties in the recently emerging literature on the
sparsity structure of the unknown parameter vector, see for example Bach [2],
Bach et al. [1], Micchelli et al. [10], Micchelli et al. [9], Maurer and Pontil [6].
A more comprehensive overview can be found in Obozinski and Bach [12].
We will focus on norm penalties and therefore generalize the LASSO estimator
to a large family of penalized estimators (see van de Geer [16] and Stucky and
van de Geer [14]), each with distinct properties to promote sparsity structures
in the parameter vector:
Î²Ì‚â„¦ := arg min
Î²âˆˆRp
{
â€–Y âˆ’XÎ²â€–2n + Î»â„¦(Î²)
}
. (1.2)
Here â„¦ is any norm on Rp that reflects some aspects of the pattern of spar-
sity for the parameter vector Î²0. Again for readability we let Î²Ì‚ = Î²Ì‚â„¦. We
characterize the â„¦-norm in terms of its weakly decomposable subsets of Rp. A
weakly decomposable norm is in some sense able to split up into two norms, one
norm measuring the size of the vector on the active set and the other norm the
size on its complement. The weakly decomposable norm itself reflects the prior
information of the underlying sparsity.
Notation: Depending on the context, for a set J âŠ‚ {1, . . . , p} and a vector
Î² âˆˆ Rp the vector Î²J is either the |J |-dimensional vector {Î²j : j âˆˆ J} or the
p-dimensional vector {Î²j l{j âˆˆ J} : j = 1, . . . , p}. More generally, for a vector
wJ := {wj : j âˆˆ J}, we use the same notation for its extended version wJ âˆˆ Rp
where wj,J = 0 for all j /âˆˆ J . For a set B we let BJ = {Î²J : Î² âˆˆ B}.
The definition of a weakly decomposable norm is crucial to the following sec-
tions, so we introduce it as in van de Geer [16] or Stucky and van de Geer [14].
This idea goes back to Bach et al. [1].
Definition 1 (Weak decomposability). A norm â„¦ in Rp is called weakly de-
composable for an index set S âŠ‚ {1, ..., p}, if there exists another norm â„¦Sc on
R|Sc| such that
âˆ€Î² âˆˆ Rp : â„¦(Î²S) + â„¦S
c
(Î²Sc) â‰¤ â„¦(Î²). (1.3)
2
A set S is called allowed if â„¦ is a weakly decomposable norm for this set. From
now on we use the notation Î¥S(Î²) := â„¦(Î²S)+â„¦
Sc(Î²Sc) the lower bounding norm
from the weak decomposability definition and Î›S(Î²) := â„¦(Î²S) + â„¦(Î²Sc) the
upper bounding norm from the triangle inequality. The weak decomposability
now reads
Î¥S(Î²) â‰¤ â„¦(Î²) â‰¤ Î›S(Î²).
Therefore the Î¥S-norm mimics the decomposability property of the `1-norm
(â€–Î²â€–1 = â€–Î²Sâ€–1 + â€–Î²Scâ€–1) for the set S.
For the LASSO estimator, most work up until recently has been focusing on
point estimation among other topics, with not much focus on establishing un-
certainty in high dimensional models. Interest has been growing rapidly on the
very important topic of constructing confidence regions for the LASSO estima-
tor, see for example van de Geer et al. [18], van de Geer and Stucky [17], Zhang
and Zhang [21], Javanmard and Montanari [5] and Meinshausen [7]. When it
comes to confidence regions for structured sparsity estimators there has not yet
been done much work to our knowledge. The paper van de Geer and Stucky
[17] mentions one approach for group confidence regions for structured sparsity
briefly, which we will develop further.
The main goal of this paper is therefore to construct asymptotic group confi-
dence regions for structured sparsity estimators in two possible ways. In order to
do this, we introduce a de-sparsified version of the estimators in (1.2), following
the idea of van de Geer et al. [18]. An appropriate estimation of the precision
matrix will be needed for the definition of a de-sparsified estimator. The estima-
tion of the precision matrix can be done in two ways which are beneficial for the
construction of asymptotic confidence regions. These two frameworks differ in
the structure of the penalty function. The theoretical behavior and the assump-
tions on the sparsity is studied. Furthermore, a simulation compares these two
frameworks in the high dimensional case and outlines potential applications.
2 De-sparsified â„¦ structured estimator
For a given norm â„¦(Â·) on Rp we can determine its sparsity structure by listing all
the subsets S := {S1, ..., Sk} for which the norm is weakly decomposable. The
estimator Î²Ì‚â„¦ (1.2) prefers to set the complement of any of the sets S
c
1, ..., S
c
k
to zero. Unfortunately the joint distribution of estimator (1.2) is not easy
to access. But it is possible to de-sparsify (1.2) and asymptotically describe
the distribution of this new estimator. The essential idea for the de-sparsified
estimator comes from the following lemma, which establishes a variation to the
KKT conditions of Î²Ì‚â„¦, following directly from Stucky and van de Geer [14].
Lemma 1. For the estimator defined in (1.2) with Ì‚ := Y âˆ’ XÎ²Ì‚ the KKT
conditions are XT Ì‚/n = Î»ZÌ‚, where â„¦âˆ—(ZÌ‚) â‰¤ 1 and ZÌ‚T Î²Ì‚ = â„¦(Î²Ì‚).
3
Here â„¦âˆ—(Â·) is another norm on Rp called the dual norm.
â„¦âˆ—(Î±) := sup
Î²âˆˆRp,â„¦(Î²)=1
Î²TÎ±.
Since Y = XÎ²0 +  and using the notation Î£Ì‚ := XTX/n we can write the KKT
conditions as
Î£Ì‚(Î²Ì‚ âˆ’ Î²0) + Î»ZÌ‚ = XT /n.
Suppose we have an appropriate surrogate for the precision matrix Î˜Ì‚, we get
Î˜Ì‚Î£Ì‚(Î²Ì‚ âˆ’ Î²0) + Î»Î˜Ì‚ZÌ‚ = Î˜Ì‚XT /n, and
Î²Ì‚ + Î»Î˜Ì‚ZÌ‚ âˆ’ Î²0 = Î˜Ì‚XT /n+ âˆ†/
âˆš
n
Here âˆ† :=
âˆš
n(Î˜Ì‚Î£Ì‚ âˆ’ I)(Î²Ì‚ âˆ’ Î²0) is the error term. We define the de-sparsified
â„¦ structured estimator as follows.
Definition 2. The de-sparsified â„¦ structured estimator is
bÌ‚â„¦ := Î²Ì‚ + Î»Î˜Ì‚ZÌ‚.
When â„¦(Â·) = â€–Â·â€–1 is the `1-norm, and if we have a Î²`1 sparsity assumption of
order o(
âˆš
n/ log(p)), a reasonable sparsity assumption on the precision matrix
and if we assume the errors to follow i.i.d. Gaussian distributions, then van
de Geer et al. [18] have shown that the de-sparsified `1 structured estimator
follows an asymptotic Gaussian distribution with an asymptotically negligible
error term.
In order to get similar results for the â„¦ penalization, we need to discuss how to
estimate the precision matrix Î˜Ì‚. The main problem that arises is, that good
estimation error bounds are only available expressed in the Î¥S? -norm, where
S? is the unknown oracle set from the main theorem in Stucky and van de Geer
[14]. The next two sections give two different ways to estimate Î˜Ì‚ in such a way
that âˆ† is asymptotically negligible.
3 First framework: gauge confidence regions
A way to construct an estimate for the precision matrix is to do |J |-wise re-
gression with any fixed set J âŠ‚ {1, ..., p}. |J |-wise regression is a very similar
method as node-wise regression (introduced by Meinshausen and BuÌˆhlmann [8]),
but instead of one node, we have simultaneously |J | nodes. With this |J |-wise
regression we try to capture the group interdependencies stored in the precision
matrix. This is why we require a multivariate model of the form
BÌ‚J := arg min
BJâˆˆR|Jc|Ã—|J|
(â€–XJ âˆ’XJcBJâ€–nuc + Î»JÎ¨(BJ)) . (3.1)
The nuclear norm is defined as
â€–Aâ€–nuc := tr
(âˆš
ATA
)
=
min(n,|J|)âˆ‘
i=1
Ïƒi(A),
4
where Ïƒi(A) are the singular values of a nÃ—|J | matrix A and for a square mÃ—m
matrix B the trace function is defined as tr (B) :=
âˆ‘m
i=1Bi,i. Furthermore the
penalty is defined as
Î¨(A) :=
|J|âˆ‘
j=1
g(Aj). (3.2)
It is a matrix norm on R|Jc|Ã—|J| (it is the dual matrix norm of an operator norm),
that uses the computational cost effective `1-norm on the columns together with
another norm g on Rp. Here Aj âˆˆ Rp is equal to the j-th column of the matrix
A on the set Jc, and 0 on the set J . The norm g is defined so that it lower
bounds all Î¥S-norms where S âˆˆ S is any non trivial allowed set of the â„¦-norm.
Furthermore the norm g should satisfy the following reflection property
g(Î²f(J)) = g(Î²) , where Î²f(J) := Î²J âˆ’ Î²Jc .
This is a natural condition on g, because for each allowed set S we have
Î¥S(Î²f(S)) = Î¥S(Î²).
Where Î¥S is defined as Î¥S(Î²) := â„¦(Î²S) + â„¦
Sc(Î²Sc). In order to construct
the norm g we construct a convex set where we will take the gauge function.
Remark that minSâˆˆS Î¥S(Â·) is in general not a norm, therefore we need to take
the convex hull. The convex set is defined through
B :=
â‹ƒ
S allowed
BÎ¥S , with BÎ¥S the unit ball of Î¥S âˆ’ norm,
Bg := Conv
(
B âˆª flipJ(B)
)
.
The function flipJ(Â·) reflects a set along the hyperplane defined by the subset
J . To be more precise for a subset B âŠ‚ Rp we have
flipJ(B) := {Î³ : Î³J = Î²J and Î³Jc = âˆ’Î²Jc ,âˆ€Î² âˆˆ B}.
Then we can define its gauge function (also known as Minkowski functional) as
follows:
g(x) := inf (Î» > 0; x âˆˆ Î»Bg) .
See Figure 1 for a graphical representation of the gauge function. From this
definition of the g we can see that the following Lemma holds.
Lemma 2. For the gauge function g the following properties hold
(1): g defines a norm on Rp.
(2): g(Î²) â‰¤ Î¥S(Î²) â‰¤ â„¦(Î²) and g(Î²) â‰¤ Î¥S(Î²f(J)), âˆ€S allowed sets.
(3): g(Î²) =
(
max
S allowed
max
(
Î¥âˆ—S(Î²),Î¥
âˆ—
S(Î²f(J))
))âˆ—
.
Furthermore Î¥âˆ—S(z) = max
(
â„¦âˆ—(Î²S),â„¦
Sc,âˆ—(Î²Sc)
)
âˆ€Î² âˆˆ Rp.
5
Figure 1: Intuition about the gauge function. Left: lower bounding nature,
right: additive nature.
(4): g(Î²Jc) â‰¤ g(Î²) for all Î² âˆˆ Rp.
Lemma 2 covers the main properties of the gauge function. Result (1) shows
that Î¨ is in fact a matrix norm. Result (3) gives a characterization of what the
gauge function is in our case. Results (2) and (4) are the main properties of the
function g, which will be needed to let the error term for the de-sparsified esti-
mator go to 0. Why we chose to construct the Î¨-norm for the |J |-wise regression
as a column sum of this gauge function g will become more evident later in this
paper. Regarding the construction of confidence sets by means of estimating
the precision matrix through the |J |-wise multivariate regression, we will need
to specify the Karush-Kuhn-Tucker (KKT) conditions for the multivariate re-
gression estimator BÌ‚J . The first thing we will need is the subdifferential of a
matrix norm. In the paper of Watson [19] one can find the formulation of the
subdifferential for a norm of a mÃ— n matrix A
âˆ‚||A|| =
{
G âˆˆ RmÃ—n : ||B|| â‰¥ ||A||+ tr
(
(B âˆ’A)TG
)
, âˆ€ B âˆˆ RmÃ—n
}
.
Furthermore we have the following characterization of the subdifferential
G âˆˆ âˆ‚||A|| â‡â‡’
{
i) ||A|| = tr
(
GTA
)
ii) ||G||âˆ— â‰¤ 1
. (3.3)
Here the dual matrix norm is defined as ||A||âˆ— = sup
B: ||B||â‰¤1
tr
(
BTA
)
. Let us
briefly note that, by definition of the dual matrix norm, a generalized version
of the Cauchy Schwartz Inequality holds true for matrices
tr
(
ABT
)
â‰¤ ||A|| Â· ||B||âˆ—.
Therefore the dual of the Î¨-matrix norm is defined as
Î¨âˆ—(A) := sup
B: Î¨(B)â‰¤1
tr
(
BTA
)
.
6
Applying equation (3.3) to the optimal solution of equation (3.1) leads to the
KKT conditions (in the case of â€–XJ âˆ’XJcBÌ‚Jâ€–nuc 6= 0):
BÌ‚J is optimal â‡â‡’
1
Î»J
XTJc(XJ âˆ’XJcBÌ‚J)Î£Ì‚
âˆ’1/2
J /n âˆˆ âˆ‚Î¨(BÌ‚J)
â‡â‡’
ï£±ï£´ï£²ï£´ï£³i) Î»JÎ¨(BÌ‚J) = tr
({
XTJc(XJ âˆ’XJcBÌ‚J)Î£Ì‚
âˆ’1/2
J /n
}T
BÌ‚J
)
ii) Î»J â‰¥ Î¨âˆ—
(
XTJc(XJ âˆ’XJcBÌ‚J)Î£Ì‚
âˆ’1/2
J /n
)
.
(3.4)
Here we denote Î£Ì‚J := (XJ âˆ’ XJcBÌ‚J)T (XJ âˆ’ XJcBÌ‚J)/n (assumed to be non-
singular). Let us additionally define the |J | de-sparsified â„¦ structured estimator
with the help of the following notations.
TJ := (XJ âˆ’XJcBÌ‚J)XJ/n.
The normalizing matrix can then be written as
M :=
âˆš
nÎ£Ì‚
âˆ’1/2
J TJ
This leads to the definition of the |J | de-sparsified â„¦ structured estimator. Defin-
ing a de-sparsified estimator in this way lets us deal with group-wise confidence
sets.
Definition 3. The |J | de-sparsified â„¦ structured estimator is
bÌ‚J := Î²Ì‚J + T
âˆ’1
J (XJ âˆ’XJcBÌ‚J)
T (Y âˆ’XÎ²Ì‚)/n. (3.5)
With these definitions we are now ready to describe the asymptotic behavior of
the estimator (3.5) in the following Theorem.
Theorem 1. Assume that the error in the model (1.1) is i.i.d. Gaussian dis-
tributed  âˆ¼ Nn(0, Ïƒ20I). Then with the definition bÌ‚J from (3.5) together with
BÌ‚J as an estimator of the precision matrix and its normalized version MbÌ‚J we
have
M(bÌ‚J âˆ’ Î²0J)/Ïƒ0 = N|J|(0, I) + rem,
where the `âˆ norm of the reminder term rem can be upper bounded by
â€–remâ€–âˆ â‰¤
âˆš
nÎ»g(Î²Ì‚Jc âˆ’ Î²0Jc)/Ïƒ0
â‰¤
âˆš
nÎ»Î¥S?(Î²Ì‚ âˆ’ Î²0)/Ïƒ0. (3.6)
As we can see from Lemma 2 (4) we can upper bound part of the reminder term
from Theorem 1 as g(Î²Ì‚Jc âˆ’ Î²0Jc) â‰¤ g(Î²Ì‚ âˆ’ Î²0). By the definition of the gauge
function g, from Lemma 2 (2) we get that
g(Î²Ì‚Jc âˆ’ Î²0Jc) â‰¤ Î¥S(Î²Ì‚ âˆ’ Î²0), âˆ€ S allowed sets of â„¦.
7
But how can we bound Î¥S?(Î²Ì‚âˆ’Î²0)? From van de Geer [16] and Stucky and van
de Geer [14] we can get sharp oracle results for an estimation error expressed in a
measure very close to the Î¥S? -norm, where S? is the active set of the oracle, but
the used measure is not quite the Î¥S? -norm. A refined version of the theorem
in van de Geer [16] leads to sharp oracle result, which we will use to upper
bound â€–remâ€–âˆ. The Lemma 16 can be found in the Appendix. In conclusion
Theorem 1 together with Lemma 16 leads to the asymptotic normality of the
normalized de-sparsified â„¦ estimator on the set J . A studentized version leads
to an asymptotic pivot. To get the studentized version one could for example
use Stucky and van de Geer [14] or generalize the more optimal bounds from the
paper van de Geer and Stucky [17]. The results are summarized in the following
corollary.
Corollary 1. Assume that the error in the model (1.1) is i.i.d. Gaussian
distributed  âˆ¼ Nn(0, Ïƒ20I). The de-sparsified estimator bÌ‚J is as in (3.5) and
the normalized version MbÌ‚Jc , together with the multivariate estimator BÌ‚J from
(3.2), as an estimator of the precision matrix. Assume that 0 â‰¤ Î´ < 1, and also
that Î»m < cÎ»J , with Î»
m as in Lemma 16. Furthermore assume
âˆš
nÎ»JÎ¶ âˆ’â†’ 0
as n â†’ âˆ. We invoke weak decomposability for S and â„¦. Assume we have a
consistent estimator of Ïƒ0. Then we have
â€–M(bÌ‚J âˆ’ Î²0J)â€–2`2/ÏƒÌ‚ = Ï‡
2
|J|(1 + oIP(1)).
With Corollary 1 asymptotic confidence sets can be constructed. But the size
of the set J is not controlled. One can find an approach with the group LASSO
and the nuclear norm as a penalty in Mitra and Zhang [11], but they need more
assumptions. We only need to assume the usual sparsity assumptions on Î²0, we
do not assume sparsity on X. It just happens, due to the KKT conditions, that
a sparse surrogate of the precision matrix bounds the remainder term.
4 Second framework: â„¦ confidence sets
The first framework made use of the gauge function g, which is able to lower
bound all the Î¥S-norms associated with the â„¦-norm, therefore the remainder
term was asymptotically negligible. But here we will discuss a more direct
approach in order to estimate the precision matrix with the â„¦-norm itself. But
there might be a price to pay. This approach was discussed briefly in van
de Geer and Stucky [17] but without mentioning the full consequences of this
approach. In contrast to the first framework, J needs to be a non trivial allowed
set of â„¦ (complements of allowed sets would also work). It is quite natural to be
interested in allowed sets (or complements of it). We define another multivariate
optimization procedure to get an approximation of the precision matrix as
CÌ‚J := arg min
CJâˆˆR|Jc|Ã—|J|
(â€–XJ âˆ’XJcCJâ€–nuc + Î»JÎ(CJ)) . (4.1)
8
Figure 2: Intuition about the constant CS? .
Here we again use the nuclear norm for its nice KKT properties together with
the following norm
Î(A) :=
|J|âˆ‘
j=1
â„¦(Aj). (4.2)
In fact we use the â„¦-norm as a measure of the columns of a |Jc|Ã— |J | matrix A,
where Aj denotes again the j-th column of the matrix A on the set J
c, and 0 on
the set J . One new problem arises in this setting, namely that for all allowed
sets S
Î¥S(Î²) â‰¤ â„¦(Î²), âˆ€ Î² âˆˆ Rp.
Therefore some work has to be done in order to get good bounds for the estima-
tion error expressed in the â„¦-norm. And this is why we will need to modify the
sparsity assumption in order for the reminder term of a de-sparsified version of
Î²Ì‚J to be asymptotically negligible.
Lemma 3. For any weakly decomposable norm â„¦ there exists a constant CS?
which may depend on the support S? of the true underlying parameter Î²
0 such
that
â„¦(Î²Sc?) â‰¤ CS?â„¦
Sc?(Î²Sc?),âˆ€Î² âˆˆ R
p.
Here S? denotes again the optimal allowed oracle set from Lemma 16.
This means that we need to quantify how far off the Î¥S? -norm on S
c
? is compared
to the â„¦ norm, see Figure 2.
9
For the estimation error expressed in the â„¦-norm one can already find oracle
results in the literature. One can see for example the consistency result Propo-
sition 6 in Obozinski and Bach [12]. But the result from Lemma 3 together
with Lemma 16 provides more optimal results for our case. This is due to the
fact, that the sub optimal constant 1/Ï from Obozinski and Bach [12] appears
squared in the bound. Therefore our constant is better suited for our problem.
In the Section 5 we will further discuss this for some widely used examples and
show how to choose the constant CS? for those examples.
Again, as in Section 3 we need to define a de-sparsified version of the estimator
Î²Ì‚. This will be a different de-sparsified estimator due to a different estimation
of the precision matrix. In a similar fashion to Section 3 we have the following
definitions
TJ := (XJ âˆ’XJcCÌ‚J)TXJ/n
Î£Ì‚J := (XJ âˆ’XJcCÌ‚J)T (XJ âˆ’XJcCÌ‚J)/n.
For the sake of simplicity and readability we keep the same notations as in
Section 3 for all these definitions, even though they are defined through CÌ‚J and
not BÌ‚J .
Definition 4. The |J | de-sparsified â„¦ estimator is again defined as
bÌ‚J := Î²Ì‚J + T
âˆ’1
J (XJ âˆ’XJcCÌ‚J)
T (Y âˆ’XÎ²Ì‚)/n. (4.3)
Here M :=
âˆš
nÎ£Ì‚âˆ’1/2TJ and the normalized version of bÌ‚J is MbÌ‚J .
Now with the help of Lemma 3 we can formulate the following theorem.
Theorem 2. Assume that the error in the model (1.1) is i.i.d. Gaussian dis-
tributed  âˆ¼ Nn(0, Ïƒ20I). Then with the definition bÌ‚J from (4.3) together with
BÌ‚J as an estimator of the precision matrix and its normalized version MbÌ‚J we
have
M(bÌ‚J âˆ’ Î²0J) = N|J|(0, I) + rem.
Where the `âˆ norm of the reminder term rem can be upper bounded by
â€–remâ€–âˆ â‰¤ 2
âˆš
nÎ»CS?Î¥S?(Î²Ì‚ âˆ’ Î²0).
Again a similar corollary to Corollary 1 holds for this construction of confidence
regions, but with an additional sparsity assumption. This sparsity assumption
needs to be specified case by case. It depends on the â„¦-norm.
5 Examples of penalties and their behavior in
the two frameworks
In this section we try to give the gauge functions g and the constant Câˆ— for some
of the common norm penalties used in the literature and for some interesting
10
` 1
-n
or
m
L
o
re
n
tz
N
o
rm
A
ll
su
b
se
ts
S
âŠ‚
{1
,.
..
,p
}
a
re
a
ll
o
w
ed
.
â„¦
S
c
(Î²
S
c
)
=
â€–Î²
S
c
â€– 1
.
g
(Î²
)
:=
â€–Î²
â€– 1
a
n
d
C
S
?
:=
1
.
S
=
{p
,S
âˆ’
p
},
w
it
h
S
âˆ’
p
âŠ‚
{1
,.
..
,p
âˆ’
1
}.
â„¦
S
c
(Î²
S
c
)
=
m
in
a
S
c
âˆˆ
A
S
c
1 2
( âˆ‘ jâˆˆ
S
c
Î²
2 j
a
j
+
a
j
) .
g
(Â·
)
=
â€–Â·
â€– 1
a
n
d
C
S
?
:=
3
/
2
.
G
ro
u
p
L
A
S
S
O
n
or
m
W
ed
g
e
N
o
rm
A
ll
su
b
se
ts
co
n
si
st
in
g
o
f
g
ro
u
p
s
S
=
âˆª
j
âˆˆ
J
G
j
,
J
âŠ‚
{1
,.
..
,g
}
a
re
a
ll
o
w
ed
.
â„¦
S
c
(Î²
S
c
)
=
â€–Î²
S
c
â€– g
r
L
.
g
(Î²
)
:=
â€–Î²
â€– g
r
L
a
n
d
C
S
?
:=
1
.
A
ll
se
ts
o
f
th
e
fo
rm
S
=
{1
,.
..
,s
},
w
it
h
so
m
e
1
â‰¤
s
â‰¤
p
.
â„¦
S
c
(Î²
S
c
)
=
â„¦
(Î²
S
c
,A
S
c
).
g
(Î²
)
:=
â€–Î²
â€– 1
a
n
d
C
S
?
:=
âˆš |S
?
|+
1
.
W
ei
gh
te
d
` 1
-n
or
m
G
ro
u
p
W
ed
g
e
N
o
rm
A
ll
su
b
se
ts
co
n
si
st
in
g
o
f
g
ro
u
p
s
S
=
âˆª
j
âˆˆ
J
G
j
,
J
âŠ‚
{1
,.
..
,g
}
a
re
a
ll
o
w
ed
.
â„¦
S
c
(Î²
S
c
)
=
âˆ‘ |S
c
|
i=
1
l |
S
|+
i
|Î²
| (i
,S
c
)
.
g
(Î²
)
:=
l p
â€–Î²
â€– 1
a
n
d
C
S
?
:=
l 1 l p
=
o
(l
o
g
(p
))
.
A
ll
se
ts
o
f
th
e
fo
rm
S
=
{G
1
,.
..
,G
s
},
w
it
h
so
m
e
1
â‰¤
s
â‰¤
g
.
â„¦
S
c
(
Î²
S
c
)
=
âˆ¥ âˆ¥ âˆ¥ âˆ¥(â€–
Î²
G
|S
|+
1
â€–
2
,
..
.,
â€–
Î²
G
g
â€–
2
)
T
âˆ¥ âˆ¥ âˆ¥ âˆ¥ W
.
g
(Î²
)
:=
âˆ¥ âˆ¥ Î²G
âˆ¥ âˆ¥ grL
a
n
d
C
S
?
:=
âˆš |S
?
|+
1
.
Table 1: Summary of norm properties.
11
new norm penalties. Furthermore Table 1 gives an overview of the properties
of each example.
5.1 LASSO: the `1 Penalty
As already mentioned the weak decomposable norms all collapse into the `1-
norm due its decomposability. Therefore the gauge function is g(Î²) = â€–Î²â€–1.
This means that both of the frameworks for constructing asymptotic confidence
sets are in fact the same. Indeed `1 has a constant of CS? = 1.
5.2 Group LASSO
The Group LASSO norm is defined by â€–Î²â€–grL :=
âˆ‘g
i=1â€–Î²Giâ€–`2 , where {G1, ..., Gg}
is a partition of {1, ..., p}. We know that the active sets for this norm are the
groups themselves S = âˆªiâˆˆSgGi where Sg is any subset of {1, ..., g}. The gauge
function is the group LASSO itself g(Î²) =
âˆ‘g
i=1â€–Î²Giâ€–`2 .
Due to the nested `1-nature of the group LASSO penalty, we have similar de-
composable properties as the `1-norm and get CS? = 1.
5.3 SLOPE
The sorted `1 norm together with some decreasing sequence 1 â‰¥ l1 â‰¥ l2 â‰¥ ... â‰¥
lp > 0 is defined as
Jl(Î²) := l1|Î²|(1) + ...+ lp|Î²|(p).
This was shown to be a norm by Zeng and Figueiredo [20]. The SLOPE was
introduced by Bogdan et al. [3] in order to control the false discovery rate:
Î²Ì‚SLOPE := arg min
Î²âˆˆRp
{
â€–Y âˆ’XÎ²â€–2n + Î»Jl(Î²)
}
.
For the SLOPE we have the following two lemmas.
Lemma 4. For the SLOPE g(Î²) = lpâ€–Î²â€–1.
Lemma 5. The SLOPE has CS? = l1/lp.
5.4 Wedge
The wedge norm was introduced in Micchelli et al. [9], and fits in a more broader
structured sparsity concept. This concept is nicely compatible from the view-
point of weakly decomposable norms, as discussed at length in van de Geer [16].
Let us define the convex cone A := {a : a âˆˆ Rp++, aj â‰¥ aj+1, j âˆˆ Nnâˆ’1}, where
Rp++ denotes the positive orthant. Then the wedge norm is defined as
â€–Î²â€–W = â„¦(Î²;A) := min
aâˆˆA
1
2
pâˆ‘
j=1
(
Î²2j
aj
+ aj
)
,
12
with the notation 0/0 = 0. Define
AS := {aS : a âˆˆ A}.
Moreover, van de Geer [16] showed that any S satisfying AS âŠ‚ A is an allowed
set for the wedge norm with â„¦S
c
(Î²Sc) := â„¦(Î²Sc ,ASc). This leads to S :=
{1, .., s} for any s âˆˆ {1, ..., pâˆ’1} being an allowed set. Hence the wedge estimator
can be defined as
Î²Ì‚Wedge = arg min
Î²âˆˆRp
{
â€–Y âˆ’XÎ²â€–2n + Î»â€–Î²â€–W
}
.
Lemma 6. For the wedge norm the gauge function is the `1-norm g(Î²) = â€–Î²â€–1,
for all Î² âˆˆ Rp.
The next lemma shows that the wedge estimator has an influence on the amount
of sparsity needed for confidence sets. But as the simulations will suggest this
might be improvable.
Lemma 7. For the wedge penalty we have CS? =
âˆš
|S?|+ 1.
5.5 Group Wedge
This is a new idea for a more general wedge norm. It is based on the con-
cept of grouping variables together. Assume that we have g disjoint groups
{G1, ..., Gg} = G with âˆªgi=1Gi = {1, ..., p}. Let us denote for a vector Î² âˆˆ Rp
the `2-norm on a given group Gj as â€–Î²Gjâ€–`2 :=
âˆš
|Gj |
âˆšâˆ‘
iâˆˆGj Î²
2
i . Then for a
vector Î² we define the following g-dimensional vector
Î²G := (â€–Î²G1â€–`2 , â€–Î²G2â€–`2 , ..., â€–Î²Ggâ€–`2)T .
Now we are able to define the group wedge in terms of the previously defined
g-dimensional wedge norm on Rg as
â€–Î²â€–GWedge := â€–Î²Gâ€–W .
We recover the wedge penalty again if we set the groups to beGi := {i} for any i âˆˆ
{1, ..., p}. The first lemma shows that we have a norm again, the proof can be
found in Section 8.
Lemma 8. The group Wedge is in fact a norm.
Lemma 9. The active sets are of the form S = âˆªiâˆˆSgGi for some subset of
group indices Sg âŠ‚ {1, ..., g}, and we have
â„¦S
c
(Î²Sc) = â€–(â€–Î²Gs+1â€–`2 , ..., â€–Î²Ggâ€–`2)T â€–W .
Moreover, the lower bounding gauge norm is the the Group LASSO norm with
wedge groups g(Î²) =
âˆ‘g
i=1â€–Î²Ggâ€–`2 .
Lemma 10. For the group wedge penalty we have CS? =
âˆš
|S?|+ 1, where S?
denotes the oracle set.
13
5.6 Lorentz norm
Let us first define the Lorentz Cone (also known as the Ice Cream Cone):
A :=
ï£±ï£´ï£´ï£²ï£´ï£´ï£³
ï£«ï£¬ï£¬ï£­
a1...
apâˆ’1
ap
ï£¶ï£·ï£·ï£¸ âˆˆ Rp++
âˆ£âˆ£âˆ£âˆ£âˆ£âˆ£âˆ£âˆ£ap â‰¥ â€–
ï£«ï£­ a1...
apâˆ’1
ï£¶ï£¸â€–`2
ï£¼ï£´ï£´ï£½ï£´ï£´ï£¾
In a similar fashion to the definition of the wedge norm, the Lorentz norm is
â€–Î²â€–Lo := 12 minaâˆˆA
pâˆ‘
i=1
(
Î²2i
ai
+ ai
)
. This next lemma shows, that the Lorentz norm
lets the index p always be part of the preferred active sets.
Lemma 11. For the Lorentz norm it holds true that all the allowed sets contain
p and are of the form
S = {p, ... any combination of other variables} .
And we get the next lemma.
Lemma 12. For the Lorentz norm g(Â·) = â€–Â·â€–1.
Lemma 13. For the Lorentz norm CS? = 3/2.
The Lorentz norm can be generalized to include any set P âŠ‚ {1, .., p} in the
allowed sets. The generalized convex cone is
B :=
{
b âˆˆ Rp++
âˆ£âˆ£bj â‰¥ â€–bP câ€–`2 âˆ€j âˆˆ P} ,
and the generalized Lorentz norm can be defined as
â€–Î²â€–genLo :=
1
2
min
bâˆˆB
pâˆ‘
i=1
(
Î²2i
bi
+ bi
)
.
Now by an analogous proof to the proof of Lemma 11, we can see that the allowed
sets of the generalized Lorentz norm always contain the set P . In particular an
allowed set S is of the form S = P âˆª B, with B âŠ‚ P c being any subset of the
complement of P . The gauge function does not change, it is the `1-norm and
we still get a constant of CS? = (|P |+ 2)/2.
6 Simulations
We look at the following linear model: Y = XÎ²0 + , where we have n = 100
observations and p = 150 variables with  âˆ¼ N (0, I). The design X is randomly
chosen, such that the covariance matrix has the following Toeplitz structure
Î£i,j = 0.9
|iâˆ’j|. The underlying parameter vector Î²0 is chosen to be the regularly
decreasing sequence
Î²{1,...,s0} :=
(
4, 4âˆ’ 2
s0 âˆ’ 1
, 4âˆ’ 2 Â· 2
s0 âˆ’ 1
, ..., 4âˆ’ (s0 âˆ’ 2) Â·
2
s0 âˆ’ 1
, 2
)T
,
14
where s0 = |S0| will be different values. This structure of active set fits nicely
in the wedge framework. Therefore to find a solution for the unknown Î²0 we
use the wedge
Î²Ì‚Wedge = arg min
Î²âˆˆRp
ï£±ï£²ï£³â€–Y âˆ’XÎ²â€–2n + Î»minaâˆˆA 12
pâˆ‘
j=1
(
Î²2j
aj
+ aj
)ï£¼ï£½ï£¾ .
Now we will construct confidence sets based on the two frameworks for the
point-wise sets {1},{2},...,{p}. For each of these p sets we compute r = 100
repetitions. To find the solution of the LASSO (`1 is the gauge function in this
case), the glmnet R package Simon et al. [13] has been used. To solve the wedge
the same code as in Micchelli et al. [9] has been used. The following two cases
have been considered: s0 = 5 and s0 = 18. Let us remark that n/ log(p) â‰ˆ 20
and (n/ log(p))2/3 â‰ˆ 7. For each case the average coverage out of these 100
replications has been computed together with the average confidence set length.
The penalty level for the node-wise LASSO and node-wise Wedge have been
chosen such that the average coverage are about the same, in order to compare
their average set lengths. Of course a reasonable penalty level for practical
applications is up for debate.
Sparsity s0 = 5: For a very sparse setting the simulations, as seen in Figure 3,
show that there is no essential difference between using the node-wise LASSO or
the node-wise Wedge in order to construct the estimate of the precision matrix.
0 50 100 150
0.4
0.5
0.6
0.7
0.8
0.9
1.0
LASSO
Î² indices
av
. C
ov
er
ag
e
0.92
0 50 100 150
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Wedge
Î² indices
av
. C
ov
er
ag
e
0.93
0 50 100 150
0
1
2
3
4
5
LASSO
Î² indices
av
. L
en
gt
h
1.24
0 50 100 150
0
1
2
3
4
5
Wedge
Î² indices
av
. L
en
gt
h
1.27
Figure 3: Left: Average coverage, Right: Average length, s0 = 5, Î»LASSO =
15.5, Î»Wedge = 15 and in red are the mean values over all point-wise sets of the
active set S0.
Sparsity s0 = 18: Surprisingly for a less sparse setting the simulations, see Fig-
ure 4, still show no noticeable difference between the node-wise LASSO or the
node-wise Wedge. This might indicate that there could be a more direct way
bound the estimation error expressed in the â„¦ norm, and that the bound of the
remainder term
âˆš
nÎ»CS?Î¥S?(Î²Ì‚J âˆ’ Î²0J) might not be optimal for the wedge.
15
0 50 100 150
0.4
0.5
0.6
0.7
0.8
0.9
1.0
LASSO
Î² indices
av
. C
ov
er
ag
e
0.94
0 50 100 150
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Wedge
Î² indices
av
. C
ov
er
ag
e
0.95
0 50 100 150
0
1
2
3
4
5
LASSO
Î² indices
av
. L
en
gt
h
1.46
0 50 100 150
0
1
2
3
4
5
Wedge
Î² indices
av
. L
en
gt
h
1.46
Figure 4: Left: Average coverage, Right: Average Length, s0 = 18, Î»LASSO =
12, Î»Wedge = 10, and in red are the mean values over all point-wise sets of the
active set S0.
7 Conclusion
Two frameworks for penalized estimators which incorporate structured spar-
sity patterns have been proposed. The first framework makes use of the gauge
function, which is in most cases an `1 type norm due to the additivity of the
lower bounding weak decomposable norms. The second framework is penal-
ized by the structured sparse norm itself. They are both quite general in the
sense that they can be used in case of any weakly decomposable norm penalty,
but they have their own properties regarding sparsity assumptions. Interest-
ingly the simulations suggest that at least for the presented Toeplitz case both
frameworks seem to perform nearly indistinguishable, even for less strict spar-
sity assumptions. Therefore it would be very interesting for future research to
further understand if oracle results for the estimation error expressed in the
weakly decomposable norms can be achieved.
8 Proofs
In the dual world inequalities for norms change the direction.
Lemma 14. Let â„¦(Â·) and Î¥(Â·) be any two norms on Rp satisfying Î¥(Î²) â‰¤
â„¦(Î²),âˆ€Î² âˆˆ Rp. Then for the corresponding dual norms we have the following
inequality:
Î¥âˆ—(Ï‰) â‰¥ â„¦âˆ—(Ï‰),âˆ€Ï‰ âˆˆ Rp.
Proof. First, let us remark that the unit balls BÎ¥ := {Î² : Î¥(Î²) â‰¤ 1} and
Bâ„¦ := {Î² : â„¦(Î²) â‰¤ 1} fulfill the following
BÎ¥ âŠƒ Bâ„¦.
This is due to the fact that for all Î² âˆˆ Bâ„¦ we have
Î¥(Î²) â‰¤ â„¦(Î²) â‰¤ 1,
16
which means that such Î² are also element of the BÎ¥-Ball. Now if we look at
the definition of the dual norm together with the fact that the supremum over
the set BÎ¥ can only be bigger than over the set Bâ„¦, we get
Î¥âˆ—(Ï‰) = sup
Î²âˆˆBÎ¥
Ï‰TÎ² â‰¥ sup
Î²âˆˆBâ„¦
Ï‰TÎ² = â„¦âˆ—(Ï‰),âˆ€Ï‰ âˆˆ Rp.
Due to the disjoint nature of the definition of Î¥S as a sum of two norms on the
set S and set Sc we can get an explicit formula for the dual norm.
Lemma 15. For the weakly decomposable norm it holds true that
Î¥âˆ—S(Î²) = max (â„¦
âˆ—(Î²S),â„¦
âˆ—
Sc(Î²Sc)) ,âˆ€Î² âˆˆ Rp.
Proof. Let us show how to lower and upper bound it.
Inequality 1: To show â€â‰¥â€:
Î¥âˆ—S(Ï‰) := sup
Î¥S(Î²)=1
Î²TÏ‰ â‰¥ sup
Î¥S(Î²)=1,
Î²=Î²S
Î²TÏ‰
= sup
â„¦(Î²S)=1
Î²TÏ‰ â‰¥ â„¦âˆ—(Ï‰S)
A similar result holds true if we restrict Î² to Î²Sc . Therefore the maximum lower
bounds the dual of the Î¥S-norm.
Inequality 2: To show â€â‰¤â€:
Î¥âˆ—S(Ï‰) = sup
Î¥S(Î²)=1
Î²TÏ‰ = sup
Î¥S(Î²)=1
{
Î²TS Ï‰S + Î²
T
ScÏ‰Sc
}
= sup
Î¥S(Î²)=1
{
Î²TS Ï‰Sâ„¦(Î²S)
â„¦(Î²S)
+
Î²TScÏ‰Scâ„¦
Sc(Î²Sc)
â„¦Sc(Î²Sc)
}
â‰¤ sup
Î¥S(Î²)=1
{
â„¦âˆ—(Ï‰S)â„¦(Î²S) + â„¦
Sc
âˆ— (Ï‰Sc)â„¦
Sc(Î²Sc)
}
= sup
a+b=1
{
â„¦âˆ—(Ï‰S)a+ â„¦
Sc
âˆ— (Ï‰Sc)b
}
= max
(
â„¦âˆ—(Ï‰S),â„¦
Sc
âˆ— (Ï‰Sc)
)
Proof of Lemma 2.
(1): The gauge function is again a norm on Rp, because 0 is in the convex set,
see for example Clarke [4] Theorem 2.36.
(2): First of all, the unit ball Bg of norm g contains all the unit balls BÎ¥S and
flipJ(BÎ¥S ), therefore
g(Î²) â‰¤ Î¥S(Î²) âˆ€ S allowed and
g(Î²) â‰¤ Î¥S(Î²f(J)) âˆ€ S allowed.
17
(3): To prove that the dual norm of g is the maximum we need to make the
following observations. First, from Lemma 14 together with Lemma 2 (2)
we have
Î¥âˆ—S(Î²) â‰¤ gâˆ—(Î²) âˆ€ S allowed.
Due to the fact that this holds for all allowed sets S we get
max
(
max
S all.
Î¥âˆ—S(Î²), max
S all.
Î¥âˆ—S(Î²f(J))
)
â‰¤ gâˆ—(Î²).
To prove the other inequality we need to look at the definition of the dual
norm gâˆ—:
gâˆ—(z) = max
g(x)â‰¤1
zTx.
Because the convex hull in the definition of g is the set of all convex
combinations of points in B âˆª flipJ(B) we have that
x âˆˆ{y : g(y) â‰¤ 1} â‡” x âˆˆ Bg
â‡”There exists some n âˆˆ Rp and
nâˆ‘
i
Î±i = 1, Î±i â‰¥ 0,
with a sequence of bi âˆˆ B âˆª flipJ(B),
such that we can write x =
nâˆ‘
i=1
Î±ibi.
That is why we can write
gâˆ—(z) = max
nâˆˆN,
âˆ‘n
i Î±i=1,
biâˆˆBâˆªflipJ (B)
nâˆ‘
i=1
(
Î±iz
T bi
)
â‰¤ max
nâˆˆN,
âˆ‘n
i Î±i=1
nâˆ‘
i=1
(
Î±i Â· max
bâˆˆBâˆªflipJ (B)
zT b
)
â‰¤ 1 Â·max
(
max
S all.
Î¥âˆ—S(z), max
S all.
Î¥âˆ—S(zf(J))
)
.
Thus equality holds, and one can easily see that
max
(
max
S all.
Î¥âˆ—S(z), max
S all.
Î¥âˆ—S(zf(J))
)
is a norm again. Now the condition
â„¦(Î²f(J)) = â„¦(Î²) for all Î² âˆˆ Rp forces all Î¥S-norms to have the same
symmetrical property, and thus BÎ¥S = flipJ(BÎ¥S ). Therefore the one
maximum can be omitted and the claim is proven. For the characterization
of the dual of the Î¥S-norm we can just apply Lemma 15.
18
(4): The function flipJ leads to g(Î²J âˆ’ Î²Jc) = g(Î²). Hence
g(Î²Jc) = g(
1
2
(Î² âˆ’ Î²J) +
1
2
Î²Jc)
â‰¤ 1
2
g(Î²) +
1
2
g(Î²J âˆ’ Î²Jc)
=
1
2
g(Î²) +
1
2
g(Î²)
= g(Î²)
Proof of Theorem 1. Then
M(bÌ‚J âˆ’ Î²Ì‚J) =
âˆš
nÎ£Ì‚
âˆ’1/2
J (XJ âˆ’XJcBÌ‚J)
TXJ/n Â· ...
Â·
(
Tâˆ’1J (XJ âˆ’XJcBÌ‚J)
T (XÎ²0 âˆ’XÎ²Ì‚ + )
)
=
âˆš
nÎ£Ì‚
âˆ’1/2
J (XJ âˆ’XJcBÌ‚J)
T
(
+X(Î²0 âˆ’ Î²Ì‚)
)
= Î£Ì‚
âˆ’1/2
J (XJ âˆ’XJcBÌ‚J)
T Â· ...
Â·
(
XJc(Î²
0 âˆ’ Î²Ì‚)J +XJc(Î²0 âˆ’ Î²Ì‚)Jc + 
)
/
âˆš
n
= Î£Ì‚
âˆ’1/2
J (XJ âˆ’XJcBÌ‚J)
TXJc(Î²
0 âˆ’ Î²Ì‚)Jc/
âˆš
n+ ...
+ Î£Ì‚
âˆ’1/2
J (XJ âˆ’XJcBÌ‚J)
T /
âˆš
n+ ...
âˆ’ Î£Ì‚âˆ’1/2J (XJ âˆ’XJcBÌ‚J)
TXJ(Î²Ì‚ âˆ’ Î²0)J/
âˆš
n
We can simplify the term
Î£Ì‚
âˆ’1/2
J (XJ âˆ’XJcBÌ‚J)
TXJ(Î²Ì‚ âˆ’ Î²0)J/
âˆš
n = M(Î²Ì‚J âˆ’ Î²0J)
That is why we can conclude that
M(bÌ‚J âˆ’ Î²0J) = M(bÌ‚J âˆ’ Î²Ì‚J) +M(Î²Ì‚J âˆ’ Î²0J)
= Î£Ì‚
âˆ’1/2
J (XJ âˆ’XJcBÌ‚J)
T /
âˆš
n+ ...
+
âˆš
nÎ£Ì‚
âˆ’1/2
J (XJ âˆ’XJcBÌ‚J)
TXJc(Î²
0 âˆ’ Î²Ì‚)Jc/n
= Î£Ì‚
âˆ’1/2
J (XJ âˆ’XJcBÌ‚J)
T /
âˆš
nï¸¸ ï¸·ï¸· ï¸¸
Gaussian Random Variable
+Î»Z(Î²0 âˆ’ Î²Ì‚)Jc/
âˆš
nï¸¸ ï¸·ï¸· ï¸¸
Remainder Term
where Z comes from the KKT conditions which fulfills:
Î¨âˆ—(Z) â‰¤ 1
tr(ZT BÌ‚J) = Î¨(BÌ‚J).
The remainder term can be bounded with the generalized Cauchy Schwartz
inequality in the `âˆ-norm by
19
Î»â€–Z(Î²0 âˆ’ Î²Ì‚)Jcâ€–âˆ/
âˆš
n = Î» max
1â‰¤jâ‰¤|J|
Zj(Î²
0 âˆ’ Î²Ì‚)Jc/
âˆš
n
â‰¤ Î» max
1â‰¤jâ‰¤|J|
gâˆ—(Zj)g(Î²
0
Jc âˆ’ Î²Ì‚Jc)/
âˆš
n
â‰¤ Î»Î¨âˆ—(Z)g(Î²0Jc âˆ’ Î²Ì‚Jc)/
âˆš
n
â‰¤ Î»g(Î²0Jc âˆ’ Î²Ì‚Jc)/
âˆš
n (KKT conditions.)
Dividing everything by Ïƒ0 leads to the result.
Proof of Lemma 3. Let us first observe:
â„¦(Î²0 âˆ’ Î²Ì‚) â‰¤ â„¦(Î²0S? âˆ’ Î²Ì‚S?) + â„¦(Î²
0
Sâˆ—c âˆ’ Î²Ì‚Sâˆ—c )
â‰¤ â„¦(Î²0S? âˆ’ Î²Ì‚S?) + â„¦
Sâˆ—
c
(Î²0Sâˆ—c âˆ’ Î²Ì‚Sâˆ—c )
âˆ’ â„¦S
âˆ—c
(Î²0Sâˆ—c âˆ’ Î²Ì‚Sâˆ—c ) + â„¦(Î²
0
Sâˆ—c âˆ’ Î²Ì‚Sâˆ—c )
â‰¤ Î¥S?(Î²0 âˆ’ Î²Ì‚) + âˆ†Sâˆ—c (Î²0 âˆ’ Î²Ì‚).
Here we define âˆ†Sâˆ—c (Î²) := â„¦(Î²Sâˆ—c )âˆ’â„¦S
âˆ—c
(Î²Sâˆ—c ). But we are left with another
problem, how to bound âˆ†Sâˆ—c . Understanding this distance will give us a bound
on how far apart the weakly decomposable norm and the norm from the triangle
inequality are. Now let us take the optimal constant CS? , which may depend on
the active set of the oracle |S?|, such that â„¦(Î²Sâˆ—c ) â‰¤ CS? Â·â„¦S
âˆ—c
(Î²Sâˆ—c ), âˆ€Î² âˆˆ Rp.
Then with this we can write
âˆ†Sâˆ—c (Î²
0 âˆ’ Î²Ì‚) â‰¤ (CS? âˆ’ 1)â„¦S
âˆ—c
(Î²0Sâˆ—c âˆ’ Î²Ì‚Sâˆ—c ) â‰¤ (CS? âˆ’ 1)Î¥S?(Î²
0 âˆ’ Î²Ì‚).
In the last inequality we have used the weak decomposability condition. There-
fore â„¦(Î²0 âˆ’ Î²Ì‚) â‰¤ CS?Î¥S?(Î²0 âˆ’ Î²Ì‚).
Proof of Theorem 2. The first part follows directly the proof of Theorem 1,
with Î-norm instead of Î¨-norm. The remainder term can be bounded with the
generalized Cauchy Schwartz inequality in the `âˆ-norm by
Î»â€–Z(Î²0 âˆ’ Î²Ì‚)Jcâ€–âˆ/
âˆš
n = Î» max
1â‰¤jâ‰¤|J|
Zj(Î²
0 âˆ’ Î²Ì‚)Jc/
âˆš
n
â‰¤ Î» max
1â‰¤jâ‰¤|J|
â„¦âˆ—(Zj)â„¦(Î²
0
Jc âˆ’ Î²Ì‚Jc)/
âˆš
n
â‰¤ Î»Îâˆ—(Z)â„¦(Î²0Jc âˆ’ Î²Ì‚Jc)/
âˆš
n
â‰¤ Î»â„¦(Î²0Jc âˆ’ Î²Ì‚Jc)/
âˆš
n (KKT conditions.)
â‰¤ Î»2â„¦(Î²0 âˆ’ Î²Ì‚)
The last inequality comes directly from the weak decomposability of the allowed
20
set J :
â„¦(Î²0Jc âˆ’ Î²Ì‚Jc) â‰¤ â„¦
(
(Î²0Jc âˆ’ Î²Ì‚Jc) + (Î²0J âˆ’ Î²Ì‚J)âˆ’ (Î²0J âˆ’ Î²Ì‚J)
)
â‰¤ â„¦(Î²0 âˆ’ Î²Ì‚) + â„¦(Î²0J âˆ’ Î²Ì‚J)
â‰¤ 2â„¦(Î²0 âˆ’ Î²Ì‚).
Now with the calculation in the proof of Lemma 3 and by dividing everything
by Ïƒ0 the proof is finished.
Proof of Lemma 4. First of all, let us see that lpâ€–Î²â€–1 indeed is a lower bound
for all weakly decomposable norms of â„¦ = Jl. From Stucky and van de Geer
[14] we know that for any subset S âŠ‚ {1, ..., p} we have
Î¥S(Î²) =
|S|âˆ‘
j=1
lj |Î²|(j,S) +
|Sc|âˆ‘
i=1
l|S|+i|Î²|(i,Sc)
with 1 â‰¥ l1 â‰¥ l2 â‰¥ ... â‰¥ lp > 0 and |Î²|(1,Sc) â‰¥ Â· Â· Â· â‰¥ |Î²|(r,Sc) being the ordered
sequence in {Î²i : i âˆˆ Sc}. We can now lower bound each li and lj by the
minimum of the decreasing sequence, namely lp. That is why we get the sought
lower bound
lpâ€–Î²â€–1 â‰¤ Î¥S(Î²) â‰¤ â„¦(Î²) âˆ€S âŠ‚ {1, ..., p} and all Î² âˆˆ Rp.
Therefore Î»pâ€–Î²â€–1 is a candidate for the gauge function, but we need to show
that this norm is the best lower bounding norm. Assume by contradiction that
there is another norm g(Â·) on Rp such that
lpâ€–Î²â€–1 â‰¤ g(Î²) â‰¤ Î¥S(Î²) âˆ€S âŠ‚ {1, ..., p} and all Î² âˆˆ Rp,
and that there exists Î³ âˆˆ Rp such that lpâ€–Î³â€–1 < g(Î³).
Denote the k-th standard basis vectors in Rp as ek. Where ek is the vector
having a one at the k-th entry and zeroes otherwise. Then Î³ can be written in
the standard basis as a combination of the standard basis vectors
Î³ = v1e1 + v2e2 + ...+ vpep.
From the above assumption and the fact that the set without the k-th index
{1, ..., p}r{k}, denoted briefly as r{k}, is an allowed set, we have that for each
standard basis vector ek the following needs to hold true
lpâ€–ekâ€–1 â‰¤ g(ek) â‰¤ Î¥r{k}(ek) âˆ€k âˆˆ {1, ..., p}.
Inserting the values â€–ekâ€–1 = 1 and Î¥r{k}(ek) = lp âˆ€k âˆˆ {1, .., p} leads to
lp â‰¤ g(ek) â‰¤ lp.
21
Therefore we can conclude that g(ek) = lp for all k âˆˆ {1, ..., p}. Now applying
the triangle inequality tho g we have
g(Î³) â‰¤ |v1|g(e1) + ...+ |vp|g(ep) = (|v1|+ ...+ |vp|)lp.
On the other hand we get lpâ€–Î³â€–1 = lp(|v1|+ ...+ |vp|). This now clearly contra-
dicts our assumption because lpâ€–Î³â€–1 â‰® g(Î³) â‰¤ lpâ€–Î³â€–1.
Proof of Lemma 5. By â„¦(Î²S) =
âˆ‘|S|
j=1 lj |Î²|(j,S) and upper bounding all lj , j =
{1, ..., |S|} we have
â„¦(Î²Sâˆ—c )/l1 â‰¤ â€–Î²Sâˆ—c â€–1.
In a similar fashion by â„¦S
c
(Î²Sc) =
âˆ‘|Sc|
i=1 l|S|+i|Î²|(i,Sc) and lower bounding all
li, i = {|S|+ 1, ..., p}, we get
â„¦S
âˆ—c
(Î²Sâˆ—c )/lp â‰¥ â€–Î²Sâˆ—câ€–1.
Combining these two inequalities leads to
â„¦S
âˆ—c
(Î²Sâˆ—c )l1/lp â‰¥ â„¦(Î²Sâˆ—c ).
for the SLOPE penalty CS? = l1/lp = o(log(p)). The last equality comes from
the Bonferroni l-sequence choice in Bogdan et al. [3].
Proof of Lemma 6. First we know by Micchelli et al. [9] that â€–Î²â€–1 â‰¤ Î¥(Î²) for
all allowed sets S and all Î² âˆˆ Rp. Now in order to show that this is the best
lower bounding norm, let us assume by contradiction that there exists another
norm g(Â·) which is strictly better than â€–Â·â€–1:
â€–Î²â€–1 â‰¤ g(Î²) â‰¤ Î¥(Î²) âˆ€S allowed âˆ€Î² âˆˆ Rp,
âˆƒÎ³ âˆˆ Rp such that â€–Î³â€–1 < g(Î³) â‰¤ Î¥(Î³) âˆ€S allowed.
Define the standard basis as ek, k âˆˆ {1, ..., p} being the vector having a one at
the k-th entry and zero entries otherwise. Let us fix any allowed set S. It is
straight forward to check that
Î¥(e1) = 1, and Î¥(es+1) = 1.
By the assumption we get that
1 = â€–es+1â€–1 â‰¤ g(es+1) â‰¤ Î¥(es+1) = 1 âˆ€S allowed.
And similarly for the first standard basis vector e1 we have
1 = â€–e1â€–1 â‰¤ g(e1) â‰¤ Î¥(e1) = 1 âˆ€S allowed.
Now because s âˆˆ {1, ..., pâˆ’ 1} we get that:
g(ek) = 1, for any k âˆˆ {1, ..., p}.
22
So we know the values that g attains for the standard basis. With this we can
conduct the following contradiction. The vector Î³ has a unique representation
in the standard basis Î³ = v1e1 + v2e2 + ... + vpep, and therefore we can apply
the triangle inequality p times to get:
g(Î³) â‰¤ |v1|g(e1) + |v2|g(e2) + ...+ |vp|g(ep)
= |v1|+ |v2|+ ...+ |vp|
= â€–Î³â€–1
This contradicts our assumption that â€–Î³â€–1 < g(Î³), and the claim is proven.
Proof of Lemma 7. For any allowed set S, the weakly decomposable Î¥-norm
consists of the following two parts
â„¦(Î²Sc) = min
aScâˆˆASc
1
2
( âˆ‘
jâˆˆSc
(Î²2j
aj
+ aj
)
+ s Â· as+1
)
,
â„¦S
c
(Î²Sc) = min
aScâˆˆASc
1
2
( âˆ‘
jâˆˆSc
Î²2j
aj
+ aj
)
.
Here we have used that aj â‰¥ aj+1 for all 1 â‰¥ j â‰¤ pâˆ’1. Because of the structure
of the cone A we have
â„¦(Î²Sc) = min
aScâˆˆASc
1
2
( pâˆ‘
j=s+2
(Î²2j
aj
+ aj
)
+
Î²2s+1
as+1
+ (s+ 1)as+1
)
â‰¤ min
aScâˆˆASc
1
2
( pâˆ‘
j=s+2
(Î²2j
aj
+ (s+ 1)aj
)
+
Î²2s+1
as+1
+ (s+ 1)as+1
)
=
âˆš
s+ 1 min
aScâˆˆASc
1
2
( pâˆ‘
j=s+1
Î²2jâˆš
s+ 1aj
+
âˆš
s+ 1aj
)
.
In the second inequality we added
âˆ‘p
j=s+2 saj â‰¥ 0, and in the last inequal-
ity we take
âˆš
s+ 1 outside the minimum. Now in this setting we know that
for aSc âˆˆ ASc we have as+1 â‰¥ as+2 â‰¥ ... â‰¥ ap â‰¥ 0. Furthermore a
â€²
Sc :=
(
âˆš
s+ 1as+1,
âˆš
s+ 1as+2, ...,
âˆš
s+ 1ap)
T âˆˆ ASc , in fact any sequence aSc âˆˆ ASc
can be displayed by a sequence which is multiplied by
âˆš
s+ 1. Therefore
â„¦(Î²Sc) â‰¤
âˆš
s+ 1 min
a
â€²
Sc
âˆˆASc
1
2
( pâˆ‘
j=s+1
Î²2j
a
â€²
j
+ a
â€²
j
)
â‰¤
âˆš
s+ 1 Â· â„¦S
c
(Î²Sc)
Proof of Lemma 9. The `2-norm does not have any non trivial active sets, and
the g-dimensional wedge norm has active sets S = {1, ..., s} for any s âˆˆ {1, ..., g}.
23
Combining theses facts leads to the conclusion that only for active sets of the
form S = âˆªiâˆˆSgGi we have weak decomposability:
â€–Î²Sâ€–grW + â€–Î²Scâ€–grW â‰¤ â€–Î²â€–grW .
Because of the definition of the group wedge as a composition of the wedge and
`2-norm this is the best lower bound. For the gauge function g it is easy to see
that by applying Lemma 6, we get the Group LASSO.
Proof of Lemma 8.
(1): â€–Î²â€–grW = 0 â‡â‡’ â€–Î²Giâ€–`2 = 0 âˆ€i âˆˆ {1, .., g} â‡â‡’ Î² â‰¡ 0.
(2): The following calculations hold true:
â€–aÎ²â€–grW = â€–(â€–aÎ²Gs+1â€–`2 , ..., â€–aÎ²Ggâ€–`2)T â€–W
= â€–a(â€–Î²Gs+1â€–`2 , ..., â€–Î²Ggâ€–`2)T â€–W
= aâ€–Î²â€–grW .
(3): The triangle inequality holds due to the properties of the wedge and `2-
norms.
â€–Î² + Î³â€–grW = â€–(â€–Î²Gs+1 + Î³Gs+1â€–`2 , ..., â€–Î²Gg + Î³Ggâ€–`2)T â€–W
â‰¤ â€–(â€–Î²Gs+1â€–`2 + â€–Î³Gs+1â€–`2 , ..., â€–Î²Ggâ€–`2 + â€–Î³Ggâ€–`2)T â€–W
â‰¤ â€–Î²G + Î³Gâ€–W
â‰¤ â€–Î²Gâ€–W + â€–Î³Gâ€–W = â€–Î²â€–grW + â€–Î³â€–grW
Proof of Lemma 10. By applying Lemma 7 in this context, together with S?
being the optimal active groups, we immediately get the desired result.
Proof of Lemma 11. By van de Geer [16] we know that for the structured spar-
sity norms, as introduced in Micchelli et al. [9], it holds that
S is an allowed set â‡â‡’ AS := {aS : a âˆˆ A} âŠ‚ A.
Let us distinguish two cases, in order to proof the lemma.
Case 1: Assume p /âˆˆ S.
Therefore AS consists of vectors with the p-th variable set to zero. This means
that there exists at least one vector a such that aS is not in A
ap,S = 0  â€–
ï£«ï£­ a1...
apâˆ’1
ï£¶ï£¸
S
â€–`2 .
In other words AS * A. Therefore sets S which do not contain p cannot be
allowed sets.
24
Case 2: Assume that the set S satisfies S 3 p.
For each vector aS in AS we have
ap,S = ap â‰¥ â€–
ï£«ï£­ a1...
apâˆ’1
ï£¶ï£¸â€–`2 â‰¥ â€–
ï£«ï£­ a1...
apâˆ’1
ï£¶ï£¸
S
â€–`2 .
The first inequality is due to a being in A. For the second inequality it suffices
to see that the `2 norm can only decrease by setting certain values to zero.
therefore we know that any set S which contains p fulfills AS âŠ‚ A.
Proof of Lemma 12. Again by Micchelli et al. [9] we know that â€–Î²â€–1 â‰¤ Î¥(Î²) for
all allowed sets S and all Î² âˆˆ Rp. Define the standard basis as ek, k âˆˆ {1, ..., p}.
Let us fix any allowed set S from Lemma 11. We can calculate that
Î¥(ek) =
âˆš
2 if k âˆˆ S r {p},Î¥(ek) = 1 if k /âˆˆ S r {p},
Î¥(ep) = 1.
Taking the special allowed set S = {g} we have
1 = â€–ekâ€–1 â‰¤ g(ek) â‰¤ Î¥{g}(ek) = 1,âˆ€k âˆˆ {1, .., p}.
This leads to g(ek) = 1,âˆ€k âˆˆ {1, .., p}. Therefore we can use the same idea of
the proof from Lemma 6, and we get that g(Â·) = â€–Â·â€–1.
Proof of Lemma 13. We have that
â„¦S
c
(Î²Sc) = min
aScâˆˆASc
1
2
âˆ‘
jâˆˆSc
(
Î²2j
aj
+ aj
)
= â€–Î²Scâ€–1.
This is due to p /âˆˆ Sc and therefore the {aj : j âˆˆ Sc} can be chosen independently
of each other, leading to the minimum aj = Î²j . Furthermore we have the
following upper bound:
â„¦(Î²Sc) = min
aâˆˆA
1
2
âˆ‘
jâˆˆSc
(
Î²2j
aj
+ aj
)
+ ap
â‰¤ min
aâˆˆA
1
2
âˆ‘
jâˆˆSc
(2Î²j) + â€–Î²Scâ€–2 with aj = Î²j
= â€–Î²Scâ€–1 +
1
2
â€–Î²Scâ€–2
â‰¤ 3
2
â€–Î²Scâ€–1 =
3
2
â„¦S
c
(Î²Sc).
Which leads to the desired constant.
25
Appendix: A refined Sharp Oracle Inequality
Let us first remind us of the definition of the theoretical lambda
Î»m := max
(
â„¦âˆ—(TXS?),â„¦
Sc?âˆ—(TXSc?)
)
/n = Î¥âˆ—S?
(
TX
)
/n. Lemma 16 refines
the sharp oracle result from van de Geer [16]. In particular, the sharp oracle
inequality from van de Geer [16] measures a variation of the estimation error in
the following way â„¦(Î²Ì‚S? âˆ’Î²âˆ—) + â„¦S
c
?âˆ—(Î²Ì‚Sc?), with S? = supp(Î²
âˆ—). Let us remark
here that the optimal oracle parameter Î²âˆ— may not be equal to Î²0. Therefore
we have no guarantee to get an upper bound on the estimation error expressed
as Î¥S?(Î²Ì‚ âˆ’ Î²0). But this is needed for both confidence frameworks to work.
Therefore we will rework Theorem 4.1 from van de Geer [16] to make S and Î²
independent of each other. Let us furthermore define the â„¦-effective sparsity as
in van de Geer [16] and denote it Î“2â„¦(L, S).
Lemma 16 (Refined Sharp Oracle Inequality). Assume that 0 â‰¤ Î´ < 1, and
also that Î»
m
Î» = c with 0 < c < 1. We invoke weak decomposability for S and
â„¦. Here the active set S and parameter vector Î² can be chosen independently.
Then it holds true that
Î¥S?(Î²Ì‚ âˆ’ Î²0) â‰¤ min
Î²,S,Î´
(
Î¥S(Î²
0 âˆ’ Î²) + C1Î»Î“2â„¦(LS , S)+
+
C2
Î»
â€–X(Î² âˆ’ Î²0)â€–2n + C3â„¦(Î²Sc)
)
, (8.1)
with LS :=
Î»+Î»m
Î»âˆ’Î»m
1+Î´
1âˆ’Î´ , constants C1 =
[(1+Î´)(1+c)]2
2Î´(1âˆ’c) , C2 =
1
2Î´(1âˆ’c) , C3 =
2
Î´(1âˆ’c)
and
S? := arg min
S
min
Î²,Î´
[
Î¥S(Î²
0 âˆ’ Î²) + C1Î»Î“2â„¦(LS , S) + ...
+
C2
Î»
â€–X(Î² âˆ’ Î²0)â€–2n + C3â„¦(Î²Sc)
]
.
Proof. Follows directly from the proof of the main theorem in van de Geer [2014]
and the triangle inequality.
26
References
[1] F. Bach, R. Jenatton, J. Mairal, and G. Obozinski. Optimization with
sparsity-inducing penalties. In Foundations and Trends in Machine Learn-
ing, volume 4, pages 1â€“106, 2012.
[2] F.R. Bach. Structured sparsity-inducing norms through submodular func-
tions. In Advances in Neural Information Processing Systems (NIPS), vol-
ume 23, pages 118â€“126, 2010.
[3] M. Bogdan, E. van den Berg, C. Sabatti, W. Su, and E. J. CandeÌ€s.
SLOPEâ€”adaptive variable selection via convex optimization. Annals of
Applied Statistics, 9(3):1103â€“1140, 2015.
[4] F. Clarke. Functional analysis, calculus of variations and optimal control.
Graduate texts in mathematics. Springer, London, 2013. ISBN 978-1-4471-
4819-7.
[5] A. Javanmard and A. Montanari. Confidence intervals and hypothesis test-
ing for high-dimensional regression. Journal of Machine Learning Research,
15:2869â€“2909, 2014.
[6] A. Maurer and M. Pontil. Structured sparsity and generalization. Journal
of Machine Learning Research, 13(1):671â€“690, 2012.
[7] N. Meinshausen. Group bound: confidence intervals for groups of variables
in sparse high dimensional regression without assumptions on the design.
Journal of the Royal Statistical Society. Series B. Statistical Methodology,
77(5):923â€“945, 2015.
[8] N. Meinshausen and P. BuÌˆhlmann. High-dimensional graphs and variable
selection with the lasso. Annals of Statistics, 34(3):1436â€“1462, 06 2006.
[9] C.A. Micchelli, J. Morales, and M. Pontil. A family of penalty functions for
structured sparsity. In J.D. Lafferty, C.K.I. Williams, J. Shawe-Taylor, R.S.
Zemel, and A. Culotta, editors, Advances in Neural Information Processing
Systems 23, pages 1612â€“1623. Curran Associates, Inc., 2010.
[10] C.A. Micchelli, J. Morales, and M. Pontil. Regularizers for structured
sparsity. Advances in Computational Mathematics, 38(3):455â€“489, 2013.
[11] Ritwik Mitra and Cun-Hui Zhang. The benefit of group sparsity in group
inference with de-biased scaled group Lasso. Electronic Journal of Statis-
tics, 10(2):1829â€“1873, 2016.
[12] Guillaume Obozinski and Francis Bach. Convex Relaxation for Com-
binatorial Penalties. Technical report, May 2012. URL https://hal.
archives-ouvertes.fr/hal-00694765. 35 page.
27
[13] Noah Simon, Jerome Friedman, Trevor Hastie, and Rob Tibshirani. Reg-
ularization paths for coxâ€™s proportional hazards model via coordinate de-
scent. Journal of Statistical Software, 39(5):1â€“13, 2011.
[14] B. Stucky and S. van de Geer. Sharp oracle inequalities for square root
regularization. ArXiv e-prints, 2015. URL http://arxiv.org/abs/1509.
04093.
[15] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of
the Royal Statistical Society. Series B., 58(1):267â€“288, 1996.
[16] S. van de Geer. Weakly decomposable regularization penalties and struc-
tured sparsity. Scandinavian Journal of Statistics. Theory and Applications,
41(1):72â€“86, 2014.
[17] S. van de Geer and B. Stucky. Ï‡2-confidence sets in high-dimensional re-
gression. In Statistical analysis for high-dimensional data: The Abel Sym-
posium 2014, volume 11 of Abel Symposia, pages 279â€“306, Cham, 2016.
Springer.
[18] S. van de Geer, P. BuÌˆhlmann, Y. Ritov, and R. Dezeure. On asymptotically
optimal confidence regions and tests for high-dimensional models. Annals
of Statistics, 42(3):1166â€“1202, 06 2014.
[19] G. A. Watson. Characterization of the subdifferential of some matrix norms.
Linear Algebra and its Applications, 170:33â€“45, 1992.
[20] X. Zeng and M. A. T. Figueiredo. Decreasing weighted sorted l1 regular-
ization. IEEE Signal Processing Letters, 21(10):1240â€“1244, June 2014.
[21] C.H. Zhang and S. Zhang. Confidence intervals for low dimensional param-
eters in high dimensional linear models. Journal of the Royal Statistical
Society. Series B., 76(1):217â€“242, 2014.
28

