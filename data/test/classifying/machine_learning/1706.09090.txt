An Actor-Critic Contextual Bandit
Algorithm for Personalized Mobile Health
Interventions
Huitian Lei
Department of Statistics, University of Michigan
Ambuj Tewari
Department of Statistics, University of Michigan
Susan A. Murphy âˆ—
Department of Statistics,
Department of Psychiatry,
Institute of Social Research, University of Michigan
June 29, 2017
âˆ—The authors gratefully acknowledge acknowledge funding from the National Institutes of Health grants,
R01HL125440, R01AA023187, P50DA039838, U54EB020404, NSF CAREER grant IIS-1452099 and Sloan
Research Fellowship
1
ar
X
iv
:1
70
6.
09
09
0v
1 
 [
st
at
.M
L
] 
 2
8 
Ju
n 
20
17
Abstract
Increasing technological sophistication and widespread use of smartphones and
wearable devices provide opportunities for innovative and highly personalized health
interventions. A Just-In-Time Adaptive Intervention (JITAI) uses real-time data col-
lection and communication capabilities of modern mobile devices to deliver interven-
tions in real-time that are adapted to the in-the-moment needs of the user. The lack
of methodological guidance in constructing data-based JITAIs remains a hurdle in
advancing JITAI research despite the increasing popularity of JITAIs among clinical
scientists. In this article, we make a first attempt to bridge this methodological gap by
formulating the task of tailoring interventions in real-time as a contextual bandit prob-
lem. Interpretability requirements in the domain of mobile health lead us to formulate
the problem differently from existing formulations intended for web applications such
as ad or news article placement. Under the assumption of linear reward function, we
choose the reward function (the â€œcriticâ€) parameterization separately from a lower di-
mensional parameterization of stochastic policies (the â€œactorâ€). We provide an online
actor-critic algorithm that guides the construction and refinement of a JITAI. Asymp-
totic properties of the actor-critic algorithm are developed and backed up by numerical
experiments. Additional numerical experiments are conducted to test the robustness
of the algorithm when idealized assumptions used in the analysis of contextual bandit
algorithm are breached.
Keywords: mobile health, just-in-time adaptive interventions, contextual bandit problems,
bandit problems with covariates, actor-critic learning algorithms
2
1 Introduction
Equipped with sophisticated sensing, communication and computation capabilities, smart-
phones and mobile devices are being increasingly used to deliver Just-In-Time Adaptive
Interventions (JITAIs). JITAIs are mobile health interventions where treatment is delivered
in real time to individuals as they go about their daily lives. A key ingredient of a JITAI
is a policy, that is, a decision rule that inputs sensor and self-report information at any
given decision point and output a decision. The decision can be whether or not to provide
treatment or the type of treatment to be provided. The use of decision rules to adapt the
type and timing of treatment delivery to the individual makes JITAIs particularly promising
in facilitating long-term health behavior change, a pressing but notoriously hard problem
(Nahum-Shani et al. (2014)). Indeed JITAIs have received increasing popularity and have
been used to support health behavior change in a variety of domains including physical ac-
tivity (King et al. (2013); Consolvo et al. (2008)), eating disorders (Bauer et al. (2010)),
drug abuse (Scott and Dennis (2009)), alcohol use (Witkiewitz et al. (2014); Suffoletto et al.
(2012); Gustafson et al. (2011)), smoking cessation (Riley et al. (2011)), obesity and weight
management (Patrick et al. (2009)), and other chronic disorders.
Despite the growing popularity of JITAIs, there is a lack of guidance concerning how to
best learn a high-quality evidence-based JITAIs in an â€œonlineâ€ setting. That is, learning
occurs in a sequential manner as a given user experiences the treatments and sensor/self-
report data, including health outcomes of interest, are collected. Ideally, the policy we
learn for a given user should take into account the specific way he or she responds to the
delivered treatments and is thus personalized to the user. However, most of the JITAIs
used in existing clinical trials are specified a priori and are based primarily on domain
expertise. The main contribution of this article to take a first step towards bridging the
gap between the enthusiasm for JITAIs in the mobile health field and the current lack of
statistical methodology to guide the online construction of a personalized policy for a user.
We model the learning of a user-specific optimal policy as a contextual bandit problem
(Woodroofe (1979); Langford and Zhang (2008); Li et al. (2010)). A contextual bandit
problem, also called a bandit problem with side-information, is a sequential decision making
3
problem where a learning algorithm, (i) chooses an action (e.g., treatment) at each time
point based on the context or side information, and (ii) receives an reward that reflects
the quality of the action under the current context. In mobile health settings, the context
can include summaries of the sensor and self-report data available at each time point. The
goal of the algorithm is to learn the optimal policy, that is, the policy that maximizes a
regularized average reward for a user. We propose an online â€œactor-criticâ€ algorithm for
learning the optimal policy. Compared to offline learning, in online learning the contexts
and rewards arrive in a sequential fashion and the estimate of the optimal policy is updated
as data accumulates. The updated policy is used to choose the treatment action at the
subsequent time point. In our actor-critic algorithm, the critic estimates parameters in a
model for the conditional mean of the reward given context and action. The actor then
updates the estimated optimal policy based on the estimated reward model. Under idealized
assumptions, we derive asymptotic theory for the consistency and asymptotic normality of
the estimated optimal policy.
Our work is motivated by our collaboration on HeartSteps (Klasnja et al. (2015); Dempsey
et al. (2015)). In the HeartSteps project, the second and third of three studies will involve
the use of a online learning algorithm for constructing personalized policies; the algorithm
presented here represents our first step in developing the learning algorithm. The goal
of the HeartStepsproject is to reduce sedentary behavior and increase physical activity in
individuals who have experienced a cardiac event and been in cardiac rehab. The current
version of HeartSteps involves data collection both via a smartphone as well as wristband
sensor. A variety of sensor and self-report data is available at each time point, including
step count, GPS location, weather, time and user calendar busyness. The current version of
HeartSteps can deliver a treatment (an activity suggestion) at any of 5 time points per day
via an audible ping and a notification on the smartphone lock screen.
This article is organized as follows. In Section 2, we formulate online learning of a policy
for a given user as a contextual bandit problem and define what we mean by an optimal policy.
Due to the concern that deterministic policies may habituate users to treatments, thereby
causing them to ignore treatment, our definition of optimality is different from the ones found
4
in most existing contextual bandit papers. In Section 3, we present an actor-critic contextual
bandit algorithm for learning the optimal policy. In Section 4, we derive asymptotic theory
on the consistency and asymptotic normality of the estimated optimal policy. In Section 5,
we present a comprehensive simulation study to investigate the performance of the actor-
critic algorithm under various simulation settings including settings which violate the usual
assumptions underpinning contextual bandit algorithms.
2 Learning JITAIs via a Contextual Bandit Algorithm
We formulate the online learning of optimal policy for a given user as a stochastic contextual
bandit problem. A contextual bandit problem is specified by a quadruple (S, d,A, r), where
S is the context space, d is a probability distribution on the context space, A is the action
space and r is the reward space. At a decision point t, the online learning algorithm collects
the context St âˆˆ S, take an action At âˆˆ A after which a reward Rt âˆˆ r is revealed before
the next decision point. The online learning algorithm does not have access to the rewards
that would occur had other actions been taken. Prior to decision point t+ 1, the algorithm
has access to the sequence of tuples {(SÏ„ , AÏ„ , RÏ„ )}tÏ„=1. We make the following assumption.
Assumption 1. (i.i.d. contexts) Action At has a in-the-moment effect on the reward Rt
with expected reward function:
E (Rt|St = s, At = a) = r(s, a),
but At does not affect the distribution of SÏ„ for Ï„ â‰¥ t+ 1. We further assume that contexts
St are i.i.d. with probability density function d(s).
This assumption matches the conceptual design of many JITAIs well. In fact, interven-
tion options in a JITAI are sometimes referred to as â€œEcological Momentary Interventionsâ€
(EMIs) or â€œmicro-interventionsâ€. Such a terminology emphasizes that the effects of many of
the treatments in this domain are expected to be short-lived in nature.
A (stochastic) policy is a mapping from the context space to (a probability distribution
over) the action space. In JITAIs, policies are used to specify (the probability of) an ac-
tion given a context. In this article, we focus on a binary action space A = {0, 1} and
5
a class of parametrized stochastic policies in which P (A = 1|S = s) is parameterized as
Ï€Î¸(s, 1) =
eg(s)
T Î¸
1+eg(s)
T Î¸
. Here g(s) is a p-dimensional vector that contains candidate variables
that may be useful for decision making. Using the parametrized policy, the way each variable
in g(s) influences the choice of action is reflected by the sign and magnitude of the corre-
sponding component in Î¸. Confidence intervals for and hypothesis testing on the optimal Î¸
can answer scientific questions about the usefulness of a particular contextual variable for
decision making. For example, suppose the scientist includes a GPS location based variable
as a candidate variable in the policy, yet the confidence interval for the Î¸ coefficient of this
variable turns out to contain 0. Then we might omit the sensing of this variable in future
because continuously sensing GPS location on smartphones drains the battery. Similarly,
self-reported measures on userâ€™s emotional states induce user burden. Therefore, if the con-
fidence interval for the Î¸ coefficients of these variables contains 0 we may reduce user burden
by omitting their collection.
2.1 The Regularized Average Reward
Empirical evidence and some behavioral science theories indicate that deterministic poli-
cies can lead to habituation and that treatment variety can increase user engagement and
retard habituation (Raynor and Epstein (2001); Epstein et al. (2009, 2011); Wilson et al.
(2005)).To maintain treatment variety, we consider stochastic policies. However it turns out
that standard definitions of optimality often lead to deterministic policies. For example, a
natural and intuitive definition of an optimal policy is a policy that maximizes the average
reward:
V âˆ—(Î¸) =
âˆ«
sâˆˆS
d(s)
âˆ‘
aâˆˆA
r(s, a)Ï€Î¸(s, a)ds,
where d(s) is the probability density function of context. The following lemma shows that,
in a simple setting where the context space is one-dimensional and finite, there always exists
a deterministic optimal policy. The proof of this lemma is provided in the supplementary
material section A.
Lemma 1. Suppose that the context space is discrete and finite, S = {s1, s2, ..., sK}. Among
6
the policies parameterized as Ï€Î¸(s, 1) =
eÎ¸0+Î¸1s
1+eÎ¸0+Î¸1s
, there exists a policy that maximizes V âˆ—(Î¸)
for which P (Ï€Î¸(S, 1) = 0 or 1) = 1.
One way to ensure treatment variety is to introduce a chance constraint (also called a
â€œprobabilistic constraintâ€; see, e.g., PreÌkopa (1995)) that ensures, with high probability over
the context distribution, that the probability of taking each treatment action under any
policy we consider is bounded sufficiently away from 0. For binary actions the constraint
has the form:
P (p0 â‰¤ Ï€Î¸(S, 1) â‰¤ 1âˆ’ p0) â‰¥ 1âˆ’ Î± (1)
where 0 < p0 < 0.5, 0 < Î± < 1 are constants controlling the amount of stochasticity. The
stochasticity constraint requires that, for at least (1 âˆ’ Î±)100% of the contexts, there is at
least p0 probability to take either of the two available actions.
Maximizing the average reward V âˆ—(Î¸) subject to the stochasticity constraint (1) is a
chance constrained optimization problem, an active research area in recent years (Nemirovski
and Shapiro (2006); Campi and Garatti (2011)). Solving this chance constraint problem,
however, involves a major difficulty: constraint (1) is, in general, a non-convex constraint on
Î¸. Moreover, the left hand side of the chance constraint is an expectation of a non-smooth
indicator function. Both the non-convexity and the non-smoothness make the optimization
problem computationally intractable. We circumvent this difficulty by relaxing constraint
(1) to a convex alternative:
Î¸TE[g(S)g(S)T ]Î¸ = Î¸T [
âˆ«
sâˆˆS
g(s)g(s)Td(s)ds]Î¸ â‰¤
(
log(
p0
1âˆ’ p0
)
)2
Î±, (2)
which is obtained by bounding the probability in (1) using Markovâ€™s inequality and some
algebra. Since the quadratic constraint is derived using an upper bound on the original
probability, it is more stringent than the chance constraint and always guarantees at least
the desired amount of treatment variety.
Instead of solving the quadratic optimization problem that maximizes the average reward
V âˆ—(Î¸) subject to the quadratic constraint (2), we choose to maximize the corresponding La-
grangian function. Incorporating inequality constraints by using Lagrangian multipliers has
7
been widely used in reinforcement learning literature to solve constrained Markov decision
problem (Borkar (2005); Bhatnagar and Lakshmanan (2012)). Given a Lagrangian multiplier
Î», the following Lagrangian function:
Jâˆ—Î»(Î¸) =
âˆ«
sâˆˆS
d(s)
âˆ‘
aâˆˆA
r(s, a)Ï€Î¸(s, a)dsâˆ’ Î» Î¸TE[g(S)g(S)T ]Î¸ (3)
is referred to as the regularized average reward in this article. For a fixed value of Î», we define
the optimal policy to be the policy that maximizes the regularized average reward, namely
Î¸âˆ—Î» = argmax J
âˆ—
Î»(Î¸). Under mild regularity conditions, we show that there is a one-to-one
correspondence between quadratic constrained optimization of the average reward (using the
constraint (2)) and the unconstrained optimization of the regularized average reward (3).
Details can be found in the supplementary material section B. There are two computational
advantages of maximizing the regularized average reward as opposed to solving a constrained
optimization. First, optimizing the regularized average reward function results in a unique
solution even when there is no treatment effect. When the expected reward does not depend
on the treatment action, i.e., E(R|S = s, A = a) = E(R|S = s), all policies in the feasible
set given by the constraint have the same average reward. The regularized average reward
function, in contrast, has a unique maximizer at Î¸ = 0pÃ—1, a purely random policy that
assigns 50% probability to both actions. Therefore, maximizing the regularized average
reward gives rise to a 0 estimand when there is no treatment effect. Second, even when
the uniqueness of optimal policy is not an issue, maximization of Jâˆ—Î»(Î¸) has computational
advantages over maximization of V âˆ—(Î¸) under the constraint (2) because the subtraction
of the quadratic term Î»Î¸TE[g(S)g(S)T ]Î¸ introduces a degree of concavity to the surface of
Jâˆ—Î»(Î¸), thus stabilizing the optimization.
3 The Online Actor-Critic Algorithm
In this section, we propose an online actor-critic algorithm for learning the policy parameter
Î¸âˆ—Î». We consider a fixed Î» in this section and will drop the subscript in Î¸
âˆ—
Î» from now on.
Recall that right before decision point t + 1, the observed â€œtraining dataâ€ consists of a
stream of triples {(SÏ„ , AÏ„ , RÏ„ )}tÏ„=1. The Î¸ coefficients in the optimal policy can be estimated
8
by maximizing an empirical version of the aforementioned regularized average reward:
JÎ»(Î¸) =
1
t
tâˆ‘
Ï„=1
âˆ‘
a
r(SÏ„ , a)Ï€Î¸(SÏ„ , a)âˆ’ Î»Î¸T
(
1
t
tâˆ‘
Ï„=1
g(SÏ„ )g(SÏ„ )
T
)
Î¸. (4)
The proposed actor-critic algorithm has two parts: the critic estimates the expected reward,
r; the estimated expected reward is then plugged into (4), which is then maximized to
estimate the parameter, Î¸, in the optimal policy. The estimated optimal policy is used to
select an action at the next decision point.
We develop a critic algorithm based on a linear assumption for the expected reward
function.
Assumption 2. (Linear model assumption) Given context St = s and action At = a,
the reward is generated according to the model Rt = f(s, a)
TÂµâˆ— + t, where f(s, a) is a k-
dimensional reward feature. The error terms t are i.i.d. with mean 0 and variance Ïƒ
2. In
particular, we have r(s, a) = f(s, a)TÂµâˆ—.
See Section 5 for simulation results concerning the robustness of the developed method
to breakdown of this linearity assumption as well as Section 6 for discussion. We further
assume that values of the reward, the reward parameter and the reward feature are bounded
as follows.
Assumption 3. (Bounded rewards and features) There exist constants which provide an a.s.
upper bound on the absolute value |R| of the reward as well as the norms |f(S,A)|2, |Âµâˆ—|2 of
the reward feature and reward parameter vector. Furthermore, we assume that we know the
constant K for which P [|R| â‰¤ K] = 1. Without further loss of generality we assume all of
these constants are 1 (including K = 1).
Bounded rewards along with bounded features are standard assumption in the bandit
literature (see for instance Agrawal and Goyal (2013)). In practice, a known bound on the
reward is usually available. For example, consider HeartSteps in which the reward is the
number of steps over 30 minutes following time t; there is a generally accepted upper limit
on the number of steps a human can take in 30 minutes.
9
A natural estimator of the reward parameter Âµâˆ— is the L2 penalized least squares estima-
tor:
ÂµÌ‚t =
(
Î¶I +
tâˆ‘
Ï„=1
f(SÏ„ , AÏ„ )f(SÏ„ , AÏ„ )
T
)âˆ’1 tâˆ‘
Ï„=1
f(SÏ„ , AÏ„ )RÏ„ (5)
where the Î¶ is the weight on the L2 penalty. This penalty ensures invertibility of the first
term on the right hand side when t is small (note that k, the dimension of the reward feature
f does not vary with time t so the penalty term is solely to ensure invertibility). Note that
even though we have assumed that the contexts SÏ„ are i.i.d., this does not meant that the
feature vectors, f(SÏ„ , AÏ„ ) are i.i.d. Indeed recall that the actions AÏ„ are drawn according
to the estimated optimal policy at decision point Ï„ âˆ’ 1, which depends on the entire history
at or before decision point Ï„ âˆ’ 1. This dependency presents challenges in analyzing the
actor-critic algorithm; see Section 4.
An additional challenge in analyzing the actor-critic algorithm is getting around an inher-
ent circular dependence: the boundedness of the actor estimates depends on the boundedness
of the estimated reward function, or equivalently the critic. The criticâ€™s estimates, in turn,
depends on the actions selected by the actor. To deal with this challenge we use the known
bound from assumption 3, to construct a bounded estimator of r(s, a). The penalized least
squares estimator results in the estimator rÌ‚t(S,A) = f(S,A)
T ÂµÌ‚t; this estimator may not
be bounded a.s. even though by assumption 3, |r(S,A)| is bounded by 1 a.s. We enforce
boundedness on the estimator of r(s, a); in particular we replace rÌ‚t(s, a) by
rÌ‚t(s, a) =
ï£±ï£´ï£´ï£´ï£²ï£´ï£´ï£´ï£³
âˆ’2 if f(s, a)T ÂµÌ‚t < âˆ’2
f(s, a)T ÂµÌ‚t if |f(s, a)T ÂµÌ‚t| â‰¤ 2
2 if f(s, a)T ÂµÌ‚t > 2
(6)
The estimated reward function, rÌ‚t(s, a), is the output of the critic step. In Theorem 1 we
show that the above procedure will result in a consistent ÂµÌ‚t. Thus for any  > 0, for large t,
with high probability, |rÌ‚t(S,A)|2 will a.s. be less than 1+ . We point out that, when reward
is bounded by a positive constant K other than 1, one shall modify the above projection by
replacing 2 by K + 1.
10
Next the actor step maximizes the estimated JÎ»(Î¸):
JÌ‚t(Î¸, ÂµÌ‚t) =
1
t
tâˆ‘
Ï„=1
âˆ‘
a
rÌ‚t(SÏ„ , a)Ï€Î¸(SÏ„ , a)âˆ’ Î» Î¸T
(
1
t
tâˆ‘
Ï„=1
g(SÏ„ )g(SÏ„ )
T
)
Î¸ (7)
at decision point t to obtain Î¸Ì‚t. The action at decision point t+1 is selected according to the
stochastic policy Ï€Î¸Ì‚t(St+1, a). The actor critic algorithm, which alternates between a critic
step and an actor step is depicted in Algorithm 1.
Algorithm 1: An online actor-critic algorithm with linear expected reward and
stochastic policies
Inputs: T , the total number of decision points; a k dimensional reward feature
f(s, a); a p dimensional policy feature g(s).
Critic initialization: B(0) = Î¶IkÃ—k; A(0) = 0kÃ—1.
Actor initialization: Î¸0 is initial policy parameter based on domain theory or
historical data.
Start from t = 0.
while t â‰¤ T do
At decision point t, observe context St.
Draw an action At according to probability distribution Ï€Î¸Ì‚tâˆ’1(St, A).
Observe an immediate reward Rt.
Critic update:
B(t) = B(tâˆ’ 1) + f(St, At)f(St, At)T , A(t) = A(tâˆ’ 1) + f(St, At)Rt,
ÂµÌ‚t = B(t)
âˆ’1A(t). The estimated reward function is rÌ‚t(s, a) from (6).
Actor update:
Î¸Ì‚t = argmax
Î¸
1
t
tâˆ‘
Ï„=1
âˆ‘
a
rÌ‚t(SÏ„ , a)Ï€Î¸(SÏ„ , a)âˆ’ Î» Î¸T
(
1
t
tâˆ‘
Ï„=1
g(SÏ„ )g(SÏ„ )
T
)
Î¸.
Go to decision point t+ 1.
end
11
4 Asymptotic Theory for the Actor-Critic Algorithm
In this section, we present consistency and the asymptotic normality results for the proposed
actor-critic algorithm. Proofs are provided in the supplementary material section C. In
addition to assumptions 1, 2 and 3, we make the following assumption that ensures the
identifiability of each component of the policy parameter.
Assumption 4. (Positive definiteness of policy features) The pÃ— p matrix E(g(S)g(S)T ) =âˆ«
sâˆˆS d(s)g(s)g(s)
Tds is positive definite.
As the very first step towards establishing the asymptotic properties of the actor-critic
algorithm, we show that, for a fixed Lagrangian multiplier Î», the optimal policy parameter
that maximizes the regularized average reward (3) lies in a bounded set. Moreover, the
estimated optimal policy parameter is bounded with probability going to 1. Lemma 2 sets
the foundation for us building on which we can use existing asymptotic statistical theory.
Lemma 2. Assume that Assumption 3 and 4 hold. Given a fixed Î», the population optimal
policy parameter, Î¸âˆ—, lies in a compact set. In addition, the sequence of estimated optimal
policy parameters, Î¸Ì‚t, lies in a compact set almost surely as tâ†’âˆ. Denote this compact set
by CÎ¸âˆ—.
Following this lemma, we make additional assumptions to establish the asymptotic theory
of the actor-critic algorithm.
Assumption 5. (Positive definiteness of reward features) The kÃ—k matrix EÎ¸(f(S,A)f(S,A)T ) =âˆ«
sâˆˆS d(s)
âˆ‘
a f(s, a)f(s, a)
TÏ€Î¸(s, a)ds is positive definite for all Î¸ in the compact set CÎ¸âˆ— in
Lemma 2.
Assumption 6. (Uniform separateness of the global maximum) There exists a neighborhood
of Âµâˆ—, say B(Âµâˆ—) such that the following holds. J(Î¸, Âµ) as a function of Î¸ has unique global
maximum denoted by Î¸Âµ, for each Âµ âˆˆ B(Âµâˆ—). Moreover, for any Î´ > 0, there exists  > 0
and neighborhood of Î¸Âµ, denoted by B(Î¸Âµ, ), such that
J(Î¸Âµ, Âµ)âˆ’ max
Î¸/âˆˆB(Î¸Âµ,)
J(Î¸, Âµ) â‰¥ Î´ (8)
for all Âµ âˆˆ B(Âµâˆ—).
12
Under assumptions 1 through 6, the following theorems establish the consistency and
asymptotic normality of the critic and the actor.
Theorem 1. (Asymptotic properties of the critic) The kÃ— 1 vector ÂµÌ‚t converges to the true
reward parameter Âµâˆ— in probability. In addition,
âˆš
t(ÂµÌ‚t âˆ’ Âµâˆ—) converges in distribution to
a multivariate normal with mean 0kÃ—1 and covariance matrix [EÎ¸âˆ—(f(S,A)f(S,A)T )]âˆ’1Ïƒ2,
where EÎ¸(f(S,A)f(S,A)T ) =
âˆ«
s
d(s)
âˆ‘
a f(s, a)f(s, a)
TÏ€Î¸(s, a)ds is the expected value of
f(S,A)f(S,A)T under the policy with parameter Î¸, and Ïƒ is the standard deviation of the er-
ror term in Assumption 2. The plug-in estimator of the asymptotic covariance is consistent.
Theorem 2. (Asymptotic properties of the actor) The p Ã— 1 vector Î¸Ì‚t converges to Î¸âˆ— in
probability. In addition,
âˆš
t(Î¸Ì‚t âˆ’ Î¸âˆ—) converges in distribution to multivariate normal with
mean 0pÃ—1 and covariance matrix [JÎ¸Î¸(Âµ
âˆ—, Î¸âˆ—]âˆ’1V âˆ—[JÎ¸Î¸(Âµ
âˆ—, Î¸âˆ—)]âˆ’1, where
V âˆ— = Ïƒ2JÎ¸Âµ(Âµ
âˆ—, Î¸âˆ—)EÎ¸[f(S,A)f(S,A)T ]JÂµÎ¸(Âµâˆ—, Î¸âˆ—) + E[jÎ¸(Âµâˆ—, Î¸âˆ—, S)jÎ¸(Âµâˆ—, Î¸âˆ—, S)T ]
. In the expression of asymptotic covariance matrix,
jÎ¸(Âµ, Î¸, S) =
âˆ‚
âˆ‚Î¸
(âˆ‘
a
f(S, a)TÂµ Ï€Î¸(S, a)âˆ’ Î»Î¸T [g(S)g(S)T ]Î¸
)
,
and both JÎ¸Î¸ and JÎ¸Âµ are the second order partial derivatives with respect to Î¸ twice and with
respect Î¸ and Âµ, respectively of J :
J(Âµ, Î¸) =
âˆ«
sâˆˆS
d(s)
âˆ‘
aâˆˆA
f(s, a)TÂµ Ï€Î¸(s, a)dsâˆ’ Î»Î¸TE[g(S)g(S)T ]Î¸. (9)
A bound on the expected regret can be derived as a by-product of the square-root con-
vergence rate of Î¸Ì‚t. The expected regret of an online algorithm up to time T is the difference
between the expected cumulative reward under the algorithm and that under the optimal
policy Î¸âˆ—:
U(T ) = T
âˆ«
sâˆˆS
d(s)
âˆ‘
a
r(s, a)Ï€Î¸âˆ—(s, a)dsâˆ’ E
[
Tâˆ‘
t=1
Rt
]
(10)
13
where {Rt}Tt=1 is the sequence of rewards generated in the algorithm and the expectation
E is with respect to the distribution of {St, At âˆ¼ Ï€Î¸Ì‚tâˆ’1 , Rt}
T
t=1. Straightforward calculation
shows that,
U(T ) =
Tâˆ‘
t=1
âˆ«
sâˆˆS
d(s)
âˆ‘
a
r(s, a)[Ï€Î¸âˆ—(s, a)âˆ’ E(Ï€Î¸Ì‚tâˆ’1(s, a))]ds
=
Tâˆ‘
t=1
âˆ«
sâˆˆS
d(s)
âˆ‘
a
r(s, a)E[Ï€â€²
Î¸Ì‚t,s,a
(s, a)(Î¸âˆ— âˆ’ Î¸Ì‚tâˆ’1)]ds
where Î¸Ì‚t,s,a is a random variable that lies on the line segment joining Î¸
âˆ— and Î¸Ì‚tâˆ’1. The
boundedness of r(s, a) together with Theorem 2 imply the following corollary.
Corollary 1. The expected regret U(T ) of the actor-critic algorithm 1 is O(
âˆš
T ).
Readers familiar with contextual bandit literature may wish to compare the above regret
bound with the regret bounds for LinUCB (Chu et al. (2011)) and for Thompson sampling
(Agrawal and Goyal (2013)). There are at least three differences between our framework
and those considered in LinUCB and Thompson sampling papers. First, we parameterize
explicitly both the policy class as well as the expected reward. These two papers parameter-
ize the expected reward which then implicitly implies a parameterized deterministic policy
class. As a result, our optimal policy is the policy that maximizes the regularized average
reward over our explicitly defined policy class; however their optimal policy is the policy
that maximizes the unregularized average reward. Second, we restrict our policy class to be
stochastic whereas their implicitly defined policy class is composed of deterministic policies.
Lastly, the setting considered here is more restrictive in that we assume contexts are i.i.d.
whereas these papers allows for arbitrary contexts as long as the conditional mean of the
reward in any context is linear in the context features.
5 Numerical Experiments
To assess the performance of the proposed actor-critic algorithm we conducted extensive
simulations across a variety of realistic scenarios. Firstly, we conduct simulations to eval-
uate the relevance of our asymptotic theory in finite T settings in which the contexts are
14
indeed i.i.d. As will be seen, the bias and mean squared error (MSE) in estimating opti-
mal policy decreases to 0 as sample size increases, and the bootstrap confidence interval
for the optimal policy parameter achieves nominal confidence level. Secondly, we note that
the i.i.d. assumption on the contexts is likely violated in real world applications in at least
two ways: the current context may be influenced by the context at previous decision points
and the current context may be influenced by past actions. Simulations in which the con-
texts follow an auto-regressive process show that the actor-critic algorithm is quite robust
to auto-correlation among contexts. We also create simulation settings where context is
influenced by previous actions through a burden effect of the treatments on the users. We
observe reasonable robustness of the bandit actor-critic algorithm when the burden effects
are small or moderate. Last but not least, we investigate how performance of the algorithm
may deteriorate when assumption 2 is violated, that is, the conditional mean of the reward
is non-linear.
Throughout we base the simulations on a generative model that is motivated by the
Heartsteps application for improving daily physical activity (Klasnja et al. (2015); Dempsey
et al. (2015)). A simplified description of HeartSteps follows. HeartSteps is a mobile health
smartphone application seeking to reduce usersâ€™ sedentary behavior and increase physical
activity such as walking. A commercial wristband sensor is usted to collect minute level steps
counts. Each evening self-report on the usefulness of the application as well as problems in
daily life are collected. At each of 3 decision points per day, sensor data is collected including
the userâ€™s current location (home/work/other) and weather. At each decision point, the
algorithm on the smartphone application must decide whether to â€œpushâ€ a tailored physical
activity suggestion, i.e., At = 1, or remain silent, i.e., At = 0. Our generative model uses a
three dimensional context at decision point t: St = [St,1, St,2, St,3]. St,1 represents weather,
with St,1 = âˆ’âˆ being extremely severe and unfriendly weather for any outdoor activities
and St,1 =âˆ being the opposite. St,2 reflects the userâ€™s recent habits in engaging in physical
activity. St,2 =âˆ represents that the user has been maintaining positive daily physical habits
while St,2 = âˆ’âˆ represents the opposite. St,3 is a composite measure of disengagement with
HeartSteps. St,3 = âˆ’âˆ reflects an extreme state that the user is fully engaged, is adherent
15
and is reporting that the application is useful. On the other hand, St,3 = âˆ denotes the
opposite state of disengagement. Note that although we assumed bounded features in proving
theoretical properties of the algorithm, these feature vectors have unbounded range. Results
shown in later sections demonstrate robustness to the boundness assumption.
The goal of HeartSteps is to reduce usersâ€™ sedentary behavior. Here we reverse code the
reward and define the cost to be the sedentary time per hour between two decision points.
So the goal of the actor-critic algorithm is to minimize an average penalized cost as opposed
to maximizing an average penalized reward. The generative model for the cost is a linear
model: Ct = 10 âˆ’ .4St,1 âˆ’ .4St,2 âˆ’ At Ã— (0.2 + 0.2St,1 + 0.2St,2) + 0.4St,3 + Î¾t,0, where Î¾t,0
are i.i.d. N(0,1) errors. In this linear model, higher values of S1 and S2, good weather and
positive physical activity habits, are associated with less sedentary time while a higher value
of S3, disengagement, leads to increased sedentary time. The negative main effect of At
indicates that physical activity suggestion (At = 1) reduces sedentary behavior compared to
no suggestion At = 0. The negative interaction between At and St,1 and between At and St,2
reflects that physical activity suggestions are more effective when the weather condition is
activity friendly or when the user has acquired good physical activity habits.
The class of parametrized policies is Ï€Î¸(S, 1) =
eÎ¸0+
âˆ‘3
i=1 Î¸iSi
1+eÎ¸0+
âˆ‘3
i=1
Î¸iSi
. The average cost under
policy Ï€Î¸ is:
C(Î¸) =
âˆ«
sâˆˆS
dÎ¸(s)
âˆ‘
a
E(C|S = a,A = a)Ï€Î¸(s, a)ds
where dÎ¸(s) is the stationary distribution of context under policy Ï€Î¸. When actions have
no impact on context distributions, the stationary distribution d(s) does not depend on the
policy parameter Î¸. In this case, the average cost reduces to: C(Î¸) =
âˆ«
sâˆˆS d(s)
âˆ‘
a E(C|S =
a,A = a)Ï€Î¸(s, a)ds. This is true for the generative models we investigate in Section 5.1 and
Section 5.2. The generative model we investigate in Section 5.3 allows actions to impact the
context distribution at future decision points. In such a case, the stationary distribution of
context depends on the policy parameter Î¸. A quadratic constraint is enforced so that the
optimal policy is stochastic. In the quadratic inequality (2), we use Î± = 0.1 and p0 = 0.1
throughout the numerical experiment unless otherwise specified. We then minimize the
corresponding Lagrangian function.
16
The optimal policy Î¸âˆ— and the oracle Î»âˆ—. According to results in Section 2.1, we have
that for every pair of (p0, Î±) there exists a Lagrangian multiplier Î»
âˆ— such that the optimal
solution to the regularized average cost function:
Î¸âˆ— = argmin
Î¸
C(Î¸) + Î»Î¸T
âˆ‘
s
dÎ¸([1, s1, s2, s3][1, s1, s2, s3]
T )Î¸ (11)
satisfies the quadratic constraint with equality. Furthermore, as Î» increases the stringency
of the quadratic constraint increases: an increased value of Î» penalizes the quadratic term
Î¸âˆ—T
âˆ‘
s dÎ¸âˆ—([1, s1, s2, s3][1, s1, s2, s3]
T )Î¸âˆ— more heavily. For a fixed pair of (p0, Î±), we perform
a line search to find the smallest Î», denoted as Î»âˆ—, such that the minimizer to the regularized
average cost, denoted as Î¸âˆ— satisfies the quadratic constraint. We recognize the difficulty in
solving the optimization problem due to the non-convexity of the regularized average cost
function. In our search for a global minimizer, we therefore use grid search, for a given Î»,
to find a crude solution to the optimization problem. We then improve the accuracy of the
optimal solution using a more refined grid search provided by the pattern search function in
Matlab. The regularized average cost function is approximated by Monte Carlo samples. We
used 5000 Monte Carlo samples to approximate the regularized average cost for simulation
in Section 5.1 and Section 5.2 where the stationary distribution of contexts does not depend
on the policy. For the simulations in Section 5.3, where context distribution does depend on
the policy, we generate a trajectory of 100000 Monte Carlo samples and ignore the first 10%
of the samples to approximate the stationary distribution.
Estimating Î» online. In practice, the decision maker has no access to the oracle La-
grangian multiplier Î»âˆ—. A natural remedy is to integrate the estimation of Î»âˆ— with the online
actor-critic algorithm that estimates the policy parameters. An actor-critic algorithm with a
fixed Lagrangian multiplier solves the â€œprimalâ€ problem while the â€œdualâ€ problem is solved
by searching for Î»âˆ—. Our integrated algorithm performs a line search to find the smallest
Î» such that the estimated optimal policy satisfies the quadratic constraint. The stationary
distribution of the contexts is approximated by the empirical distribution. Estimating Î»
can be very time consuming, therefore in our simulations, the algorithm performs the line
search over Î» only every 10 decision points. Similar ideas with gradient based updates on Î»
have appeared in reinforcement literature to find the optimal policies in constrained MDP
17
problems, see Borkar (2005); Bhatnagar and Lakshmanan (2012) for examples.
Bootstrap confidence intervals. In a number of trial simulations, we found that
the plug-in variance estimator derived from Theorem 2 tends to underestimate in small to
moderate sample size, a direct consequence of which is the anti-conservatism of the Wald
confidence interval. Details of the anti-conservatism are discussed in the supplementary
material section D. Our solution to the anti-conservative Wald confidence interval is the
percentile-t bootstrap confidence interval. Algorithm 2 shows how to generate a bootstrap
sample. Algorithm 2 is repeated for a total of B times to obtain a bootstrap sample of the
estimated optimal policy parameters, {Î¸Ì‚bT}Bb=1 and plug-in variance estimates, {VÌ‚ bT}Bb=1. We
create bootstrap percentile-t confidence intervals for Î¸âˆ—i , the i-th component of the optimal
policy parameter. For each Î¸âˆ—i , we use the empirical percentile of
{âˆš
t(Î¸Ì‚bT,iâˆ’Î¸Ì‚T,i)âˆš
VÌ‚ bT
}B
b=1
, denoted
by pÎ± to replace the normal distribution percentile in Wald confidence intervals. A (1âˆ’2Î±)%
confidence interval is [
Î¸Ì‚T,i âˆ’ pÎ±
VÌ‚iâˆš
T
, Î¸Ì‚T,i + pÎ±
VÌ‚iâˆš
T
]
(12)
where Î¸Ì‚T,i is the i-th component of Î¸Ì‚T and VÌ‚i is the plug-in variance estimate based on the
original sample.
Simulation details. The simulation results presented in the following sections are based
on 1000 independent simulated users. For each simulated user, we allow a burn-in period of
20 decision points. During the burn-in period, actions are chosen by fair coin flips. After
the burn-in period, the online actor-critic algorithm is implemented to learn the optimal
policy and obtain an end-of-study estimated optimal policy at the last decision point. In
these simulations we did not force rÌ‚t(s, a) to be in the interval [âˆ’2, 2] as in Algorithm 1 or
Algorithm 2. We do not encounter any issues in convergence of the algorithm.
Both bias and MSE shown in all of the following tables are averaged over 1000 end-of-
study estimated optimal policies. For each simulated user the 95% bootstrapped confidence
intervals for Î¸âˆ— is based on 500 bootstrapped samples generated by Algorithm 2. With 95%
confidence, we expect that the empirical coverage rate of a confidence interval should be
within 0.936 and 0.964, if the true confidence level is 0.95.
18
Algorithm 2: Generating a bootstrap sample estimate Î¸Ì‚bT , VÌ‚
b
T
Inputs: The observed context history {St}Tt=1. A bootstrap sample of residuals
{bt}Tt=1. The estimated reward parameter ÂµÌ‚T
Critic initialization: B(0) = Î¶IkÃ—k, a k Ã— k identity matrix. A(0) = 0k is a k Ã— 1
column vector.
Actor initialization: Î¸Ì‚b0 = Î¸Ì‚0 is the best treatment policy based on domain theory or
historical data.
while t < T do
Context is St ;
Draw an action Abt according to policy Ï€Î¸Ì‚btâˆ’1
;
Generate a bootstrap reward Rbt = f(St, A
b
t)
T ÂµÌ‚T + 
b
t ;
Critic update:
B(t) = B(tâˆ’ 1) + f(St, At)f(St, At)T , A(t) = A(tâˆ’ 1) + f(St, At)Rbt ;
ÂµÌ‚bt = A(t)
âˆ’1B(t). The bounded estimate to reward function is rÌ‚bt (s, a). ;
Actor update:
Î¸Ì‚bt = argmax
Î¸
1
t
tâˆ‘
Ï„=1
âˆ‘
a
rÌ‚bt (SÏ„ , a)Ï€Î¸(a|St)âˆ’ Î»Î¸T [
1
t
tâˆ‘
Ï„=1
g(SÏ„ , 1)
Tg(SÏ„ , 1)]Î¸
Go to decision point t+ 1 ;
end
Plugin ÂµÌ‚bT and Î¸Ì‚
b
T to the asymptotic variance formula to get a bootstrapped variance
estimate VÌ‚ bT .
19
T (sample size)
Bias MSE
Î¸0 Î¸1 Î¸2 Î¸3 Î¸0 Î¸1 Î¸2 Î¸3
200 âˆ’0.081 âˆ’0.090 âˆ’0.089 0.010 0.054 0.052 0.052 0.055
500 âˆ’0.053 âˆ’0.037 âˆ’0.034 âˆ’0.002 0.027 0.024 0.021 0.029
Table 1: I.I.D. contexts: bias and MSE in estimating the optimal policy parameter.
Bias=E(Î¸Ì‚T )âˆ’ Î¸âˆ—
.
T(sample size) Î¸0 Î¸1 Î¸2 Î¸3
200 0.962 0.942 0.938 0.945
500 0.96 0.948 0.968 0.941
Table 2: I.I.D. contexts: coverage rates of percentile-t bootstrap confidence intervals for the
optimal policy parameter.
5.1 I.I.D. Contexts
In this generative model, we choose the simplest setting where contexts at different deci-
sion points are i.i.d. We generate contexts {[St,1, St,2, St,3]}Tt=1 from a multivariate normal
distribution with mean 0 and identity covariance matrix. The population optimal policy
is Î¸âˆ— = [0.417778, 0.394811, 0.389474, 0.001068] at Î»âˆ— = 0.046875. Table 1 lists the bias
and mean squared error (MSE) of the estimated optimal policy parameters. Both measures
shrink towards 0 as T , sample size per simulated user, increases from 200 to 500, which
is consistent with the convergence in estimated optimal policy parameter as established in
Theorem 2. Table 2 shows the empirical coverage rates of percentile-t bootstrap confidence
interval at sample sizes 200 and 500. At sample size 200, the empirical coverage rates are
between 0.936 and 0.964 for all Î¸iâ€™s. At sample size 500, however, the bootstrap confidence
interval for Î¸2 is a little conservative with an empirical coverage rate of 0.968.
20
5.2 AR(1) Context
In this section, we study the performance of the actor-critic algorithm when the dynamics
of the context is an auto-regressive stochastic process. We envision that in many health
applications, contexts at adjacent decision points are likely to be correlated. Using Heart-
Steps as an example, weather (S1) at two adjacent decisions points are likely to be similar.
So are usersâ€™ learning ability (S2) and disengagement level S3. One way to incorporate the
correlation among contexts at near-by decision points is through a first order auto-regression
process. We simulate the context according to
St,1 = 0.4Stâˆ’1,1 + Î¾t,1,
St,2 = 0.4Stâˆ’1,2 + Î¾t,2,
St,3 = Î¾t,3
Here we choose Î¾t,1 âˆ¼ N(0, 1 âˆ’ 0.42), Î¾t,2 âˆ¼ N(0, 1 âˆ’ 0.42) and Î¾t,3 âˆ¼ N(0, 1) so that the
stationary distribution of St is multivariate normal with zero mean and identity covariance
matrix, same as the distribution of St in the previous section. The initial distribution of
St, t = 1 is a multivariate standard normal.
The oracle Lagrangian multiplier is Î»âˆ— = 0.05 and the population optimal policy is
Î¸âˆ— = [0.417, 0.395, 0.394, 0], same as in the i.i.d. simulation. Bias and MSE of the estimated
policy parameters are shown in Table 3. Empirical coverage rate of the percentile t bootstrap
confidence interval is reported in Table 4. Both the bias and MSE diminish towards 0 as the
sample size increases from 200 to 500, a clear indication that convergence of the algorithm
is not affected by the auto-correlation in context. The bootstrap confidence interval for Î¸3
is anti-conservative at sample size 200, but recovers decent coverage at sample size 500.
5.3 Actions Cause Increased Burden
In this section, we study behavior of the actor-critic algorithm in the presence of an interven-
tion burden effect. Our generative model with a burden effect represents a scenario where
users disengage with the Heartsteps application, and hence the recommended intervention,
if the application provides physical activity suggestions at too high a frequency. When users
21
T (sample size)
Bias MSE
Î¸0 Î¸1 Î¸2 Î¸3 Î¸0 Î¸1 Î¸2 Î¸3
200 âˆ’0.093 âˆ’0.089 âˆ’0.076 0.006 0.058 0.053 0.047 0.057
500 âˆ’0.046 âˆ’0.032 âˆ’0.040 âˆ’0.005 0.025 0.022 0.024 0.028
Table 3: AR(1) contexts: bias and MSE in estimating the optimal policy parameter.
Bias=E(Î¸Ì‚T )âˆ’ Î¸âˆ—.
T(sample size) Î¸0 Î¸1 Î¸2 Î¸3
200 0.963 0.952 0.957 0.927*
500 0.969 0.962 0.96 0.949
Table 4: AR(1) contexts: coverage rates of percentile-t bootstrap confidence intervals. Cov-
erage rates significantly lower than 0.95 are marked with asterisks (*).
experience intervention burden effects, they become frustrated and have a tendency of falling
back to their sedentary behavior. In our burden effect generative model, St,3 represents the
disengagement level whose value increases if there is a physical activity suggestion at the
previous decision point Atâˆ’1 = 1. The positive main effect of St,3 in the cost model (13) be-
low reflects that higher disengagement level is associated with higher cost (higher sedentary
time). The initial distribution of St is the standard multivariate normal distribution. After
the first decision point, contexts are generated according to the following stochastic process:
St,1 = 0.4Stâˆ’1,1 + Î¾t,1,
St,2 = 0.4Stâˆ’1,2 + Î¾t,2,
St,3 = 0.4Stâˆ’1,3 + 0.2Stâˆ’1,3Atâˆ’1 + 0.4Atâˆ’1 + Î¾t,3
We simulate the cost, sedentary time per hour between two decision points, according to the
following linear model:
Ct = 10âˆ’ .4St,1 âˆ’ .4St,2 âˆ’ At Ã— (0.2 + 0.2St,1 + 0.2St,2) + Ï„St,3 + Î¾t,0. (13)
22
where parameter Ï„ controls the â€œsizeâ€ of the burden effect: the larger Ï„ is, the more severe
the burden effect is. We study the performance of our algorithm in five different cases
corresponding to Ï„ = 0, 0.2, 0.4, 0.6, 0.8. Different values of Ï„ represent users who experience
different levels of burden effect. Ï„ = 0 represents the type of users who experience no burden
effect while Ï„ = 0.8 represents the type of users who experience a large burden effect.
Table 15 in the supplementary material section E lists the oracle Î»âˆ— and the corresponding
optimal policy Î¸âˆ— at different levels of burden effect. Higher level of burden effects calls for
increased value of oracle Î»âˆ— to keep the desired intervention variety. The negative sign of
Î¸âˆ—3 at Ï„ â‰¥ 0.2 indicates that the application should lower the probability of pushing an
activity suggestion when the disengagement level is high. The magnitude of Î¸âˆ—3 rises with
the size of the burden effect, implying that as burden effect increases the application should
further lower the probability of pushing activity suggestions at high disengagement level. Î¸âˆ—0
decreases to be negative when Ï„ increases, which indicates that as the size of burden effect
grows, the application should lower the frequency of activity suggestions in general.
Table 5 and 6 list the bias, MSE and the empirical coverage rate of the percentile-t
bootstrap confidence interval at sample size 200. Table 7 and 8 list these three measures at
sample size 500. When there is no burden effect (Ï„ = 0), St,3 has no influence on the cost and
is therefore considered as a â€œnoiseâ€ variable. The optimal policy parameters are estimated
with low bias and MSE under the generative model with Ï„ = 0 and the bootstrap confi-
dence intervals have decent coverage, both of which are clear indications that the algorithm
is robust to presence of noise variables that are affected by previous actions. As burden
effects levels go up, we observe an increased bias and MSE in the estimated optimal policy
parameters, Î¸0 and Î¸3 in particular. The empirical coverage rates of bootstrap confidence
intervals for Î¸0 and Î¸3 are below the nominal 95% level. There are two reasons to explain the
increased bias and MSE. The most important one is the near-sightedness of bandit actor-
critic algorithm. The bandit algorithm chooses the policy that maximizes the (immediate)
average cost while ignoring the negative consequence of a physical activity suggestion At = 1
on the disengagement level at the next decision point. The bandit algorithm therefore tends
to â€œover-treatâ€ in general and in particular at high disengagement level, which is reflected
23
in an over-estimated Î¸0 and Î¸3. The second reason comes from the bias in estimating Î»,
the Lagrangian multiplier. The oracle Lagrangian multiplier Î»âˆ— is chosen so that the opti-
mal policy parameter satisfies the quadratic constraint while the online bandit actor-critic
algorithm estimates the Lagrangian multiplier so that the bandit-estimated optimal policy
satisfies the quadratic constraint. To separate the consequence of underestimated Î» from the
consequence of the myopia of the bandit algorithm, we implement the bandit algorithm with
oracle Î»âˆ—. Results of these experiments are shown in the supplementary material section E.
We observe that, even with the use of oracle Î»âˆ—, the overestimation of Î¸0 and Î¸3 as well as
the anti-conservatism of the confidence intervals are still present.
Overall, the estimation of Î¸1 and Î¸2 shows robustness to the presence of burden effects. Î¸1
and Î¸2 are estimated with low bias and MSE under the presence of small to moderate burden
effects (Ï„ = 0.2, 0.4). While we observe biases in estimating Î¸1 and Î¸2 under moderate to
large burden effects (Ï„ = 0.6, 0.8), the magnitude of such bias increases slowly with the size
of the burden effect. Empirical coverage rates of the bootstrap confidence intervals for Î¸1
and Î¸2 are decent for Ï„ = 0.2, 0.4 and only degrade slowly under 95% when Ï„ = 0.6, 0.8.
Ï„
Bias MSE
Î¸0 Î¸1 Î¸2 Î¸3 Î¸0 Î¸1 Î¸2 Î¸3
0 âˆ’0.027 âˆ’0.036 âˆ’0.030 0.003 0.058 0.037 0.036 0.036
0.2 0.229 âˆ’0.093 âˆ’0.104 0.164 0.110 0.044 0.046 0.063
0.4 0.506 âˆ’0.063 âˆ’0.035 0.235 0.313 0.040 0.037 0.091
0.6 0.645 0.043 0.073 0.272 0.473 0.038 0.041 0.110
0.8 0.702 0.084 0.096 0.272 0.550 0.043 0.045 0.110
Table 5: Burden effect: bias and MSE in estimating the optimal policy parameter at sample
size 200. Bias=E(Î¸Ì‚T )âˆ’ Î¸âˆ—.
24
Ï„ Î¸0 Î¸1 Î¸2 Î¸3
0 0.963 0.963 0.955 0.942
0.2 0.853* 0.946 0.937 0.862*
0.4 0.565* 0.96 0.954 0.776*
0.6 0.39* 0.937 0.916* 0.739*
0.8 0.329* 0.908* 0.899* 0.739*
Table 6: Burden effect: coverage rates of percentile-t bootstrap confidence intervals for
the optimal policy parameter at sample size 200. Î» is estimated online. Coverage rates
significantly lower than 0.95 are marked with asterisks (*).
Ï„
Bias MSE
Î¸0 Î¸1 Î¸2 Î¸3 Î¸0 Î¸1 Î¸2 Î¸3
0 0.006 0.010 0.017 âˆ’0.008 0.027 0.018 0.016 0.019
0.2 0.263 âˆ’0.048 âˆ’0.057 0.153 0.096 0.020 0.019 0.042
0.4 0.539 âˆ’0.018 0.012 0.224 0.318 0.018 0.016 0.069
0.6 0.678 0.088 0.120 0.261 0.487 0.026 0.030 0.087
0.8 0.735 0.129 0.143 0.261 0.568 0.035 0.037 0.087
Table 7: Burden effect: bias and MSE in estimating the optimal policy parameter at sample
size 500. Bias=E(Î¸Ì‚t)âˆ’ Î¸âˆ—.
25
Ï„ Î¸0 Î¸1 Î¸2 Î¸3
0 0.973 0.949 0.955 0.942
0.2 0.714* 0.95 0.962 0.788*
0.4 0.217* 0.951 0.961 0.635*
0.6 0.101* 0.886* 0.835* 0.545*
0.8 0.07* 0.806* 0.788* 0.546*
Table 8: Burden effect: coverage rates of percentile-t bootstrap confidence intervals for
the optimal policy parameter at sample size 200. Î» is estimated online. Coverage rates
significantly lower than 0.95 are marked with asterisks (*).
Figure 1 and 2 assess the quality of the estimated optimal policies by comparing the
regularized average cost with the optimal regularized average cost. Figure 1 does the com-
parison at five levels of burden effect: Ï„ = 0, 0.2, 0.4, 0.6, 0.8, at sample size 200. As the
burden effects level up, the overall long-run average cost goes up, which is simply an artifact
of the increasing main effect size of the disengagement level. Having a higher long-term
average cost, the estimated optimal policy by the contextual bandit algorithm is always in-
ferior then the optimal policy. The inferiority gap, as measure by the difference between the
median long-run average cost and the long-run average cost of the optimal policy increases
as Ï„ increases. When sample size increases from 200 to 500, we observe less variation in
the long-run average cost of the estimated optimal policies. Nevertheless, the gap remains
stable. We also observe that the variance in the regularized average cost increases as the
burden effect level goes up.
26
0 0.2 0.4 0.6 0.8
Burden e,ect =
9.9
10
10.1
10.2
10.3
10.4
A
ve
ra
ge
 c
os
t
Figure 1: Burden effect: box plots of
regularized average cost at different levels
of the burden effect at sample size 200.
0 0.2 0.4 0.6 0.8
Burden e,ect =
9.9
10
10.1
10.2
10.3
10.4
A
ve
ra
ge
 c
os
t
Figure 2: Burden effect: box plots of regu-
larized average cost at different levels of the
burden effect at sample size 500.
Because our theoretical results assume i.i.d. contexts, we have no proof that the optimal
policy estimated by the bandit actor-critic algorithm will converge to the optimal policy.
Nevertheless, we observe convergence in the estimated policy as sample size T grows. We
conjecture that, when actions affect contexts distributions, the bandit algorithm converges
to the policy Ï€Î¸âˆ—âˆ— that satisfies the following equilibrium equation:
Î¸âˆ—âˆ— = argmin
Î¸
âˆ‘
s
dÎ¸âˆ—âˆ—(s)
âˆ‘
a
Ï€Î¸(a|s)E(C|A = a, S = s)âˆ’ Î»âˆ—âˆ—Î¸TEÎ¸âˆ—âˆ— [g(S)g(S)T ]Î¸ (14)
where Î»âˆ—âˆ— is the smallest Î» such that Î¸âˆ—âˆ—
âˆ‘
s
dÎ¸âˆ—âˆ—(s)g(s)
Tg(s)Î¸âˆ—âˆ— â‰¤ (log( p0
1âˆ’ p0
))2Î± (15)
When actions do not influence contexts distributions, the equilibrium equation is the same
system of equations satisfied by the optimal policy. When previous actions have an impact
on context distribution at later decision points, the stationary distribution of context is a
function of policy. We call solution to equation 15, the myopic equilibrium policy. The myopic
equilibrium policy minimizes the regularized average cost under the stationary distribution
generated by itself. Such policy achieves an â€œequilibrium stateâ€ and there is no reason
for the actor-critic to change the current policy if a myopic equilibrium has been reached.
27
The conjecture is supported by our numerical results. Since myopic equilibrium policy
only depends on the context dynamics and the treatment effect E(C|A = 1, S = s) âˆ’
E(C|A = 0, S = s), it remains the same at different levels of the burden effect. The myopic
equilibrium policy is Î¸âˆ—âˆ— = [0.392, 0.372, 0.371, 0]. The bias and MSE in estimating the
myopic equilibrium policy for Ï„ = 0.4 is shown in table 9. The bias and MSE at other
levels of the burden effect are the same. These results support with our conjecture that
the estimated optimal policy by the bandit algorithm converges to the myopic equilibrium
policy.
Sample size (T)
Bias MSE
Î¸0 Î¸1 Î¸2 Î¸3 Î¸0 Î¸1 Î¸2 Î¸3
200 âˆ’0.078 âˆ’0.081 âˆ’0.075 0.004 0.063 0.042 0.041 0.036
500 âˆ’0.045 âˆ’0.035 âˆ’0.028 âˆ’0.007 0.029 0.019 0.017 0.019
Table 9: Burden effect: bias and MSE in estimating the myopic equilibrium policy for
Ï„ = 0.4. Bias=E(Î¸Ì‚t)âˆ’ Î¸âˆ—âˆ—.
5.4 Expected Cost is a Nonlinear function of the Cost Feature
In this section, we investigate the performance of the online actor critic algorithm when
the expected cost is a nonlinear function of the cost feature used in the critic step. In
such scenarios, the linear actor critic algorithm finds the â€œbestâ€ policy in two steps: first it
projects the true cost function into the linear space spanned by the cost feature, then it finds
the policy that minimizes the regularized cost function under the projection. In contrast,
the true optimal policy is the policy that minimizes the regularized cost function without
the projection. In this simulation, we are interested to see how the extra step of projection
affects the estimation and inference of the optimal policy parameter.
Recall that the cost feature is f(St, At) = [1, St,1, St,2, St,3, At, AtSt,1, AtSt,2, AtSt,3]. In
particular consider the case where the interaction term between At and St,1 is a linear
28
combination of a linear cost function and a nonlinear one:
Ct = (1âˆ’ Î±)[10âˆ’ .4St,1 âˆ’ .4St,2 âˆ’ At Ã— (0.2 + 0.2St,1 + 0.2St,2) + 0.4St,3 + Î¾t,0]
+ Î±[10âˆ’ .4S2t,1 âˆ’ .4St,2 âˆ’ At Ã— (0.2 + 0.2S2t,1 + 0.2St,2) + 0.4St,3 + Î¾t,0]
= 10âˆ’ .4[(1âˆ’ Î±)St,1 + Î±St,1]âˆ’ .4St,2 âˆ’ At Ã— (0.2 + 0.2[(1âˆ’ Î±)St,1 + Î±St,1] + 0.2St,2) + 0.4St,3 + Î¾t,0
The tuning parameter Î± âˆˆ [0, 1] controls the amount of nonlinearity: when Î± = 0, the ex-
pected cost is the linear cost function used in the previous sections. Nonlinearity increasingly
dominates the interaction between St,1 and At as Î± increases. The online actor critic algo-
rithm, unaware of the possible nonlinearity in the cost function, uses the same cost feature
and the same policy feature as in the previous sections. Recall the policy is parameterized
as Ï€Î¸(S, 1) =
eÎ¸0+
âˆ‘3
i=1 Î¸iSi
1+eÎ¸0+
âˆ‘3
i=1
Î¸iSi
. Table 23 in the supplementary material section F provides the
optimal Î¸ values. Table 10 and Table 11 show the bias and MSE of the linear actor critic
algorithm at different levels of nonlinearity at sample size 200 and 500. The bias for es-
timating Î¸âˆ—i , i = 1, 2, 3 remains stable whereas the MSE inflates as the Î± increases. Both
the bias and MSE for estimating Î¸âˆ—0 increase as the cost function moves away from a linear
structure. Table 12 and table 13 show the coverage rates of the confidence interval for Î¸âˆ—.
The confidence interval coverages for Î¸âˆ—i , i = 0.1, 2 deteriorate as the level of nonlinearity
increases. However, the confidence level for Î¸âˆ—3 = 0, the coefficient for St,3 which is not a
useful tailoring variable, remains decent as the level of nonlinearity increases.
Î±
Bias MSE
Î¸0 Î¸1 Î¸2 Î¸3 Î¸0 Î¸1 Î¸2 Î¸3
0 âˆ’0.100 âˆ’0.074 âˆ’0.103 âˆ’0.007 0.063 0.052 0.051 0.057
0.2 âˆ’0.137 âˆ’0.012 âˆ’0.109 âˆ’0.013 0.064 0.065 0.050 0.053
0.4 âˆ’0.179 0.012 âˆ’0.109 âˆ’0.014 0.076 0.099 0.048 0.048
0.6 âˆ’0.211 0.020 âˆ’0.099 âˆ’0.017 0.083 0.142 0.043 0.042
Table 10: Nonlinear Cost: bias and MSE in estimating the optimal policy parameter at
sample size 200. Bias=E(Î¸Ì‚T )âˆ’ Î¸âˆ—.
29
Î±
Bias MSE
Î¸0 Î¸1 Î¸2 Î¸3 Î¸0 Î¸1 Î¸2 Î¸3
0 âˆ’0.038 âˆ’0.036 âˆ’0.045 âˆ’0.002 0.025 0.022 0.022 0.027
0.2 âˆ’0.080 0.030 âˆ’0.067 âˆ’0.008 0.026 0.032 0.023 0.023
0.4 âˆ’0.109 0.065 âˆ’0.069 âˆ’0.010 0.031 0.064 0.023 0.020
0.6 âˆ’0.136 0.058 âˆ’0.068 âˆ’0.009 0.038 0.112 0.021 0.018
Table 11: Nonlinear Cost: bias and MSE in estimating the optimal policy parameter at
sample size 500. Bias=E(Î¸Ì‚T )âˆ’ Î¸âˆ—.
Î± Î¸0 Î¸1 Î¸2 Î¸3
0 0.944 0.947 0.954 0.939
0.2 0.926* 0.879* 0.942 0.935*
0.4 0.892* 0.738* 0.922* 0.942
0.6 0.835* 0.588* 0.914* 0.942
Table 12: Nonlinear Cost: coverage rates of percentile-t bootstrap confidence intervals for
the optimal policy parameter at sample size 200. Î» is estimated online. Coverage rates
significantly lower than 0.95 are marked with asterisks (*).
Î± Î¸0 Î¸1 Î¸2 Î¸3
0 0.971 0.961 0.966 0.958
0.2 0.931* 0.875* 0.936 0.956
0.4 0.885* 0.655* 0.924* 0.958
0.6 0.837* 0.471* 0.915* 0.961
Table 13: Nonlinear Cost: coverage rates of percentile-t bootstrap confidence intervals for
the optimal policy parameter at sample size 500. Î» is estimated online. Coverage rates
significantly lower than 0.95 are marked with asterisks (*).
30
6 Conclusion and Discussion
In this article, we present a general framework to define optimal policies for use in JITAIs that
encourages intervention variety. We also gave an online actor-critic algorithm to learn the
optimal policy. Although the theoretical properties of the algorithm assume i.i.d. contexts,
the numerical experiments show robustness of the algorithm to violations of this assumption.
In particular, experiments show that performance of the algorithm, in term of bias, MSE and
confidence interval coverage, is not affected by auto-correlation among contexts. Experiments
also demonstrate some robustness of the algorithm when distribution of the context depends
on previous actions. Furthermore, we conjecture that, when actions influence the distribution
of context at later decision points, the contextual bandit algorithm converges to the myopic
equilibrium policy. Our numerical experiments back up this conjecture. Theoretical proof
of the conjecture, however, is an open question and requires future work.
There are a few areas for which the actor-critic algorithm could be improved and ex-
tended. First, the linear expected reward assumption might be a bit strong in some sce-
narios, especially when a low dimension reward feature is used. When the assumption is
deemed untenable, more sophisticated components should be added to the reward (cost)
feature. To this end, both the actor-critic algorithm and the asymptotic theory should be
extended to encompass the scenario where the dimension of the reward (cost) feature grows
with the sample size. If one intends to use linear reward (cost) model with a fixed dimen-
sion of reward feature, we highly recommend frequent validation of the linear model using
model diagnostic tools. Linear regression diagnostic tools can be used as the first line of
defense. However, more sophisticated model checking methods for online learning need to
be developed to make sure the reward (cost) model is adequate. Second, there is room
for improvement in optimization in the actor step. Optimizing the estimated regularized
average reward function is in general a non-convex optimization problem and could be time-
consuming. In the proposed algorithm, optimization at decision point t + 1 does not use
the estimated policy parameters at previous decision points. In other words, the optimiza-
tion is not incremental and may waste computing resources when the sample size gets large.
Careful design of online optimization methods that leverages previous estimates will likely
31
significantly improve the computational efficiency of the actor-critic algorithm and help in
its practical adoption in mobile health applications. Third, the algorithm presented in this
article learns a userâ€™s the optimal policy based solely on his/her history. However, in order to
speed up the learning it is attractive idea, especially in the beginning of the learning period,
to pool data across multiple users. Methods and theories for learning based on multiple users
need to be developed.
References
Agrawal, S. and N. Goyal (2013). Thompson sampling for contextual bandits with linear
payoffs. In ICML (3), pp. 127â€“135.
Bauer, S., J. de Niet, R. Timman, and H. Kordy (2010). Enhancement of care through
self-monitoring and tailored feedback via text messaging and their use in the treatment of
childhood overweight. Patient education and counseling 79 (3), 315â€“319.
Bertsekas, D. P. (1999). Nonlinear programming.
Bhatnagar, S. and K. Lakshmanan (2012). An online actorâ€“critic algorithm with function
approximation for constrained markov decision processes. Journal of Optimization Theory
and Applications 153 (3), 688â€“708.
Billingsley, P. (1961). The lindeberg-levy theorem for martingales. Proceedings of the Amer-
ican Mathematical Society 12 (5), 788â€“792.
Borkar, V. S. (2005). An actor-critic algorithm for constrained markov decision processes.
Systems & control letters 54 (3), 207â€“213.
Campi, M. C. and S. Garatti (2011). A sampling-and-discarding approach to chance-
constrained optimization: feasibility and optimality. Journal of Optimization Theory and
Applications 148 (2), 257â€“280.
32
Chu, W., L. Li, L. Reyzin, and R. E. Schapire (2011). Contextual bandits with linear
payoff functions. In International Conference on Artificial Intelligence and Statistics, pp.
208â€“214.
Consolvo, S., D. W. McDonald, T. Toscos, M. Y. Chen, J. Froehlich, B. Harrison, P. Klasnja,
A. LaMarca, L. LeGrand, R. Libby, et al. (2008). Activity sensing in the wild: a field
trial of ubifit garden. In Proceedings of the SIGCHI Conference on Human Factors in
Computing Systems, pp. 1797â€“1806. ACM.
Dempsey, W., P. Liao, P. Klasnja, I. Nahum-Shani, and S. A. Murphy (2015). Randomised
trials for the fitbit generation. Significance 12 (6), 20â€“23.
Epstein, L. H., K. A. Carr, M. D. Cavanaugh, R. A. Paluch, and M. E. Bouton (2011).
Long-term habituation to food in obese and nonobese women. The American journal of
clinical nutrition 94 (2), 371â€“376.
Epstein, L. H., J. L. Temple, J. N. Roemmich, and M. E. Bouton (2009). Habituation as a
determinant of human food intake. Psychological review 116 (2), 384.
Fiacco, A. V. and Y. Ishizuka (1990). Sensitivity and stability analysis for nonlinear pro-
gramming. Annals of Operations Research 27 (1), 215â€“235.
Gustafson, D. H., B. R. Shaw, A. Isham, T. Baker, M. G. Boyle, and M. Levy (2011).
Explicating an evidence-based, theoretically informed, mobile technology-based system
to improve outcomes for people in recovery for alcohol dependence. Substance use &
misuse 46 (1), 96â€“111.
Keener, R. (2010). Theoretical Statistics: Topics for a Core Course. Springer Texts in
Statistics. Springer New York.
King, A. C., E. B. Hekler, L. A. Grieco, S. J. Winter, J. L. Sheats, M. P. Buman, B. Banerjee,
T. N. Robinson, and J. Cirimele (2013). Harnessing different motivational frames via
mobile phones to promote daily physical activity and reduce sedentary behavior in aging
adults. PloS one 8 (4), e62613.
33
Klasnja, P., E. B. Hekler, S. Shiffman, A. Boruvka, D. Almirall, A. Tewari, and S. A.
Murphy (2015). Microrandomized trials: An experimental design for developing just-in-
time adaptive interventions. Health Psychology 34 (Suppl), 1220â€“1228.
Langford, J. and T. Zhang (2008). The epoch-greedy algorithm for multi-armed bandits with
side information. In Advances in neural information processing systems, pp. 817â€“824.
Li, L., W. Chu, J. Langford, and R. E. Schapire (2010). A contextual-bandit approach
to personalized news article recommendation. In Proceedings of the 19th international
conference on World wide web, pp. 661â€“670. ACM.
Nahum-Shani, I., S. N. Smith, A. Tewari, K. Witkiewitz, L. M. Collins, B. Spring, and
S. Murphy (2014). Just in time adaptive interventions (jitais): An organizing framework
for ongoing health behavior support. Methodology Center technical report (14-126).
Nemirovski, A. and A. Shapiro (2006). Convex approximations of chance constrained pro-
grams. SIAM Journal on Optimization 17 (4), 969â€“996.
Patrick, K., F. Raab, M. Adams, L. Dillon, M. Zabinski, C. Rock, W. Griswold, and G. Nor-
man (2009). A text message-based intervention for weight loss: randomized controlled
trial. Journal of medical Internet research 11 (1), e1.
PreÌkopa, A. (1995). Stochastic programming. Springer Science & Business Media.
Raynor, H. A. and L. H. Epstein (2001). Dietary variety, energy regulation, and obesity.
Psychological bulletin 127 (3), 325.
Riley, W. T., D. E. Rivera, A. A. Atienza, W. Nilsen, S. M. Allison, and R. Mermelstein
(2011). Health behavior models in the age of mobile interventions: are our theories up to
the task? Translational behavioral medicine 1 (1), 53â€“71.
Scott, C. K. and M. L. Dennis (2009). Results from two randomized clinical trials evaluating
the impact of quarterly recovery management checkups with adult chronic substance users.
Addiction 104 (6), 959â€“971.
34
Suffoletto, B., C. Callaway, J. Kristan, K. Kraemer, and D. B. Clark (2012). Text-message-
based drinking assessments and brief interventions for young adults discharged from the
emergency department. Alcoholism: Clinical and Experimental Research 36 (3), 552â€“560.
Tropp, J. A. (2012). User-friendly tail bounds for sums of random matrices. Foundations of
computational mathematics 12 (4), 389â€“434.
Van der Vaart, A. W. (2000). Asymptotic statistics, Volume 3. Cambridge university press.
Wilson, T. D., D. B. Centerbar, D. A. Kermer, and D. T. Gilbert (2005). The pleasures
of uncertainty: prolonging positive moods in ways people do not anticipate. Journal of
personality and social psychology 88 (1), 5.
Witkiewitz, K., S. A. Desai, S. Bowen, B. C. Leigh, M. Kirouac, and M. E. Larimer (2014).
Development and evaluation of a mobile intervention for heavy drinking and smoking
among college students. Psychology of Addictive Behaviors 28 (3), 639.
Woodroofe, M. (1979). A one-armed bandit problem with a concomitant variable. Journal
of the American Statistical Association 74 (368), 799â€“806.
35
Supplementary Material
A Proof of Lemma 1
Proof. Without the loss of generality, we assume that 0 < s1 < s2 < ... < sK . Otherwise, if
some siâ€™s are negative, we can transform all the contexts to be positive by adding to siâ€™s a
constant greater than min1â‰¤iâ‰¤K si. Denote this constant by M and the corresponding policy
parameter by Î¸Ìƒ. There is a one-to-one correspondence between the two policy classes:
Î¸Ìƒ0 = Î¸0 âˆ’MÎ¸1
Î¸Ìƒ1 = Î¸1
Therefore if the lemma holds when all contexts are positive the same conclusion hold in the
general setting. We use p(Î¸) to denote the probability the probability of choosing action
A = 1 for policy Ï€Î¸ at the K different values of context:
(
eÎ¸0+Î¸1s1
1 + eÎ¸0+Î¸1s1
,
eÎ¸0+Î¸1s2
1 + eÎ¸0+Î¸1s2
, ...,
eÎ¸0+Î¸1sK
1 + eÎ¸0+Î¸1sK
)
Notice that each entry in p(Î¸) is number between 0 and 1 with equality if the policy is
deterministic at certain context. A key step towards proving deterministic optimal policy is
to show the following closed convex hull equivalency:
conv({p(Î¸) : Î¸ âˆˆ R2}) = conv({(Î½1, ..., Î½K), Î½i âˆˆ {0, 1}, Î½1 â‰¤ ... â‰¤ Î½K or Î½1 â‰¥ ... â‰¥ Î½K})
We examine the limiting points of p(Î¸) when Î¸0 and Î¸1 tends to infinity. We consider the
case where Î¸0 6= 0 and let Î¸1 = pÎ¸0 where p is a fixed value. It holds that
eÎ¸0+Î¸1s
1 + eÎ¸0+Î¸1s
=
eÎ¸0(1+ps)
1 + eÎ¸0(1+ps)
â†’
ï£±ï£´ï£´ï£´ï£´ï£´ï£²ï£´ï£´ï£´ï£´ï£´ï£³
0 : ifÎ¸0 â†’ âˆ’âˆ, p > âˆ’1/s
0 : ifÎ¸0 â†’âˆ, p < âˆ’1/s
1 : ifÎ¸0 â†’ âˆ’âˆ, p < âˆ’1/s
1 : ifÎ¸0 â†’âˆ, p > âˆ’1/s
It follows that when Î¸0 â†’ âˆ’âˆ and p scans through the K + 1 intervals on R: (âˆ’âˆ,âˆ’1/s1],
(âˆ’1/s1,âˆ’1/s2], . ... (âˆ’1/sK ,âˆ), p(Î¸) approaches the following K + 1 limiting points:
A1
(1, 1, ..., 1)
(0, 1, ..., 1)
...
(0, 0, ..., 1)
(0, 0, ..., 0)
when Î¸0 â†’âˆ and p scans through the K + 1 intervals, p(Î¸) approaches the following K + 1
limiting points
(0, 0, ..., 0)
(1, 0, ..., 0)
...
(1, 1, ..., 0)
(1, 1, ..., 1)
There are in total 2K limiting points: {(Î½1, ..., Î½K), Î½i âˆˆ {0, 1}, Î½1 â‰¤ ... â‰¤ Î½K or Î½1 â‰¥ ... â‰¥
Î½K}. Each limiting point is a K dimensional vector with 0-1 entries in an either increasing
or decreasing order. Now we show that any p(Î¸), Î¸ âˆˆ R2 is a convex combination of the
limiting points. Let p(Î¸) = [p1(Î¸), p2(Î¸), ..., pK(Î¸)]. In fact,
â€¢ If Î¸1 = 0, p(Î¸) = (1âˆ’ p1(Î¸))(0, 0, ..., 0) + p1(Î¸)(1, 1, ..., 1)
â€¢ If Î¸1 > 0, we have 0 < p1(Î¸) < p2(Î¸) < ... < pK(Î¸) < 1 and
p(Î¸) = p1(Î¸)(1, 1, ..., 1) + (p2(Î¸)âˆ’ p1(Î¸))(0, 1, ..., 1) + ...
+(pK(Î¸)âˆ’ pKâˆ’1(Î¸))(0, 0, ..., 1) + (1âˆ’ pK(Î¸)) âˆ— (0, 0, ..., 0)
â€¢ If Î¸1 < 0, we have 1 > p1(Î¸) > p2(Î¸) > ... > pK(Î¸) > 0 and
p(Î¸) = (1âˆ’ p1(Î¸)) âˆ— (0, 0, ..., 0) + (p1(Î¸)âˆ’ p2(Î¸))(1, 0, ..., 0) + ...
+(pK(Î¸)âˆ’ pKâˆ’1(Î¸))(1, 1, ..., 0) + pK(Î¸)(1, 1, ..., 1)
A2
Returning to optimizing the average reward, we denote Î±i = P (S = si)(E(R|S = si, A =
1)âˆ’ E(R|S = si, A = 0)).
max
Î¸
V âˆ—(Î¸) = max
Î¸
Kâˆ‘
i=1
Î±ipi(Î¸) (16)
= max
(p1,...,pK)âˆˆ{p(Î¸):Î¸âˆˆR2}
Kâˆ‘
i=1
Î±ipi (17)
= max
(p1,...,pK)âˆˆconv({p(Î¸):Î¸âˆˆR2})
Kâˆ‘
i=1
Î±ipi (18)
= max
(p1,...,pK)âˆˆconv({(Î½1,...,Î½K),Î½iâˆˆ{0,1},Î½1â‰¤...â‰¤Î½K or Î½1â‰¥...â‰¥Î½K})
Kâˆ‘
i=1
Î±ipi (19)
. Equation from (17) to (18) is followed by the fact that the objective function is linear (and
thus convex) in piâ€™s. Equivalency from (18) to (19) is a direct product of the closed convex
hull equivalency. Theories in linear programming theory suggests that one of the maximal
points is attained at the vertices of the convex hull of the feasible set. Therefore we have
proved that one of the policy that maximizes V âˆ—(Î¸) is deterministic.
B One-to-one Correspondence between Constrained and
Unconstrained Optimization
The constrained optimization finds the policy that maximizes the average reward subject to
the quadratic constraint, i.e.,
max
Î¸
V âˆ—(Î¸), s. t. Î¸TE[g(S)Tg(S)]Î¸ â‰¤ (log( p0
1âˆ’ p0
))2Î± (20)
The unconstrained optimization finds the policy that maximizes the regularized average
reward:
Î¸âˆ— = argmax
Î¸
Jâˆ—Î»(Î¸) (21)
A natural question to ask, when transforming the constrained optimization problem (20)
to an unconstrained one (21), is whether a Lagrangian multiplier exists for each level of
A3
stringency of the quadratic constraint. While the correspondence between the constrained
optimization and the unconstrained one may not seem so obvious due to the lack of convex-
ity in V âˆ—(Î¸), we established the following lemma 3 given assumption 7 and assumption 4.
Assumption 7 assumes the uniqueness of the global maximum for all positive Î».
Assumption 7. For every 0 < Î» <âˆ, the global maximum of the regularized average reward
is a singleton.
Jâˆ—Î»(Î¸) =
âˆ‘
sâˆˆS
d(s)
âˆ‘
aâˆˆA
E(R|S = s, A = a)Ï€Î¸(s, a)âˆ’ Î»Î¸TE[g(S)Tg(S)]Î¸
Lemma 3. If the maximizer of the average reward function V âˆ—(Î¸) is deterministic, i.e.
P (Ï€Î¸(A = 1|S) = 1) > 0 or P (Ï€Î¸(A = 0|S) = 1) > 0, under assumption 7 and 4, for
every K = (log( p0
1âˆ’p0 ))
2Î± > 0 there exist a Î» > 0 such that the solution of the constrained
optimization problem 20 is the solution of the unconstrained optimization problem 21.
Proof. Let Î¸âˆ—Î» be one of the global maxima of the Lagrangian function: Î¸
âˆ—
Î» = argmaxÎ¸ J
âˆ—
Î»(Î¸).
Let Î²Î» = Î¸
âˆ—T
Î» E[g(S)Tg(S)]Î¸âˆ—Î». By proposition 3.3.4 in Bertsekas (1999), Î¸âˆ—Î» is a global maxi-
mum of constrained problem:
max
Î¸
V âˆ—(Î¸)
s.t. Î¸TE[g(S)Tg(S)]Î¸ â‰¤ Î²Î»
In addition, the stringency of the quadratic constraint increases monotonically with the value
of the Lagrangian coefficient Î». Let 0 < Î»1 < Î»2 and with some abuse of notation, let Î¸1 and
Î¸2 be (one of) the global maximals of Lagrangian function J
âˆ—
Î»1
(Î¸) and Jâˆ—Î»2(Î¸). It follows that
âˆ’ V âˆ—(Î¸2) + Î»2Î¸T2 E[g(S)Tg(S)]Î¸2
â‰¤âˆ’ V âˆ—(Î¸1) + Î»2Î¸T1 E[g(S)Tg(S)]Î¸1
=âˆ’ V âˆ—(Î¸1) + Î»1Î¸T1 E[g(S)Tg(S)]Î¸1 + (Î»2 âˆ’ Î»1)Î¸T1 E[g(S)Tg(S)]Î¸1
â‰¤âˆ’ V âˆ—(Î¸2) + Î»1Î¸T2 E[g(S)Tg(S)]Î¸2 + (Î»2 âˆ’ Î»1)Î¸T1 E[g(S)Tg(S)]Î¸1
A4
It follows that
Î¸T1 E[g(S)Tg(S)]Î¸1 â‰¥ Î¸T2 E[g(S)Tg(S)]Î¸2
. As Î» approaches 0, the maximal of the regularized average reward approaches the maximal
of the average reward function, for which E(Î¸Tg(S))2 â†’ âˆ. As Î» increases towards âˆ,
maximal of the regularized average reward approaches the random policy with Î¸ = 0. Itâ€™s
only left to show that Î¸âˆ—TÎ» E[g(S)Tg(S)]Î¸âˆ—Î» is a continuous function of Î». Under assumption
7, we can verify that conditions in Theorem 2.2 in Fiacco and Ishizuka (1990) holds. This
theorem implies that the solution set of the unconstrained optimization 21 is continuous in
Î», sufficient to conclude the continuity of Î¸âˆ—TÎ» E[g(S)Tg(S)]Î¸âˆ—Î».
C Proof of Asymptotic Theory of the Actor Critic Al-
gorithm
C.1 Proof of Lemma 2
Proof. This lemma is proved by comparing the regularized average reward function Jâˆ—Î»(Î¸) at
Î¸âˆ— and at 0p. The optimal regularized average reward is:
Jâˆ—Î»(Î¸
âˆ—) =
âˆ‘
sâˆˆS
d(s)
âˆ‘
aâˆˆA
f(s, a)TÂµâˆ—Ï€Î¸âˆ—(A = a|S = s)âˆ’ Î»Î¸âˆ—TE[g(S)Tg(S)]Î¸âˆ—
â‰¤
âˆ‘
s,a
d(s)
|f(s, a)|22 + |Âµâˆ—|22
2
Ï€Î¸âˆ—(A = a|S = s)âˆ’ Î»Î¸âˆ—TE[g(S)Tg(S)]Î¸âˆ—
â‰¤ 1âˆ’ Î»Î¸âˆ—TE[g(S)Tg(S)]Î¸âˆ—
While on the other hand the regularized average reward for the random policy Î¸ = 0p is
Jâˆ—Î»(0p) =
âˆ‘
s,a
d(s)f(s, a)TÂµâˆ—/2 â‰¥ 0
By the optimality of policy Î¸âˆ—, 1 âˆ’ Î»Î¸TE[g(S)Tg(S)]Î¸ â‰¥ 0, which leads to the necessary
condition for the optimal policy parameter:
A5
Î¸âˆ—TE[g(S)Tg(S)]Î¸âˆ— â‰¤ 1
Î»
(22)
According to assumption 4, the above inequality defines a bounded ellipsoid for Î¸âˆ—, which
concludes the first part of the lemma. To prove the second part of this lemma, we notice
that the estimated reward function rÌ‚t(s, a) is by definition bounded. By comparing JÌ‚t(Î¸, ÂµÌ‚t)
at Î¸ = Î¸Ì‚t and Î¸ = 0p we have
Î¸Ì‚Tt [
1
t
tâˆ‘
Ï„=1
g(SÏ„ )g(SÏ„ )
T ]Î¸Ì‚t â‰¤
2
Î»
(23)
It remains to show that the smallest eigenvalue of 1
t
âˆ‘t
Ï„=1 g(SÏ„ )g(SÏ„ )
T is bounded away
from 0 with probability going to 1. Using the matrix Chernoff inequality, theorem 1 in
Tropp (2012), for any 0 < Î´ < 1,
P{Î»min(
1
t
tâˆ‘
Ï„=1
g(SÏ„ )g(SÏ„ )
T ) â‰¤ (1âˆ’ Î´)Î»min(Eg(S)g(S)T )} â‰¤ p[
eâˆ’Î´
(1âˆ’ Î´)1âˆ’Î´
]
tÎ»min(Eg(S)g(S)
T )
Q
(24)
where Q is the bound on the maximal eigenvalue of g(S)g(S)T and p is the length of g(S).
Taking Î´ = 0.5, the right-hand side of the Chernoff inequality goes to 0 as t goes to âˆ.
Therefore with probability going to 1, inequality 23 defines a compact set on Rp. We have
proved the second part of the lemma.
C.2 Proof of the consistency of the critic
Proof. Based on the expression of ÂµÌ‚t, its L2 distance from Âµâˆ— is
|ÂµÌ‚t âˆ’ Âµâˆ—|2 = C(t)B(t)âˆ’1B(t)âˆ’1C(t) + op(1) (25)
=
C(t)
t
(
B(t)
t
)âˆ’1(
B(t)
t
)âˆ’1
C(t)
t
+ op(1) (26)
A6
where
C(t) =
tâˆ‘
Ï„=1
f(SÏ„ , AÏ„ )Ï„
B(t) = Î¶IkÃ—k +
tâˆ‘
Ï„=1
f(SÏ„ , AÏ„ )f(SÏ„ , AÏ„ )
T
The two steps in proving |ÂµÌ‚t âˆ’ Âµâˆ—|22 â†’ 0 in probability are
1. The matrix B(t)
t
has eigenvalue bounded away from 0 with probability going to 1, and
2. C(t)
t
converges to 0d in probability.
To prove the first step, we construct a matrix-valued martingale difference sequence. Define
K(Î¸) = EÎ¸[f(S,A)f(S,A)T ] =
âˆ‘
s d(s)
âˆ‘
a f(s, a)f(s, a)
TÏ€Î¸(A = a|S = s)
Xi = f(Si, Ai)f(Si, Ai)
T âˆ’ E(f(Si, Ai)f(Si, Ai)T |Fi)
= f(Si, Ai)f(Si, Ai)
T âˆ’
âˆ«
s
d(s)
âˆ‘
a
f(s, a)f(s, a)TÏ€Î¸iâˆ’1(s, a)ds
= f(si, ai)f(si, ai)
T âˆ’K(Î¸Ì‚iâˆ’1)
where the filtration Fi = Ïƒ{Î¸Ì‚j, j â‰¤ i âˆ’ 1} is the sigma algebra expand by the estimated
optimal policy before decision point i. By assumption 3, the sequence of random matrices
{Xi} are uniformly bounded. Applying the matrix Azuma inequality in Tropp (2012), it
follows that
Î»max(
B(t)
t
âˆ’
âˆ‘t
i=1K(Î¸Ì‚iâˆ’1)
t
)â†’ 0 in probability
Î»min(
B(t)
t
âˆ’
âˆ‘t
i=1K(Î¸Ì‚iâˆ’1)
t
)â†’ 0 in probability
Let the operators Î»min and Î»max represent the smallest and the largest eigenvalue of a matrix.
Î»min(
B(t)
t
) = Î»min(
B(t)
t
âˆ’
âˆ‘t
i=1K(Î¸Ì‚iâˆ’1)
t
+
âˆ‘t
i=1K(Î¸Ì‚iâˆ’1)
t
)
â‰¥ Î»min(
B(t)
t
âˆ’
âˆ‘t
i=1K(Î¸Ì‚iâˆ’1)
t
) + Î»min(
âˆ‘t
i=1K(Î¸Ì‚iâˆ’1)
t
)
A7
By assumption 5, the second term Î»min(
âˆ‘t
i=1K(Î¸Ì‚iâˆ’1)
t
) is bounded with probability going to
1. Hence we have shown that the minimal eigenvalue of B(t)
t
is bounded with probability
going to 1. Using the same proving techniques we can show that the maximal eigenvalue of
(B(t)
t
)âˆ’1 is bounded with probability going to 1.
The second step in proving consistency of the critic is standard. Using the same filtration
Fi, we construct vector-valued martingale difference sequence Yi = f(Si, Ai)i. The sequence
has bounded variance under assumption 3. The in-probability convergence of C(t)
t
to 0 follows
immediately by applying the vector-valued Azuma inequality.
C.3 Proof of the consistency of the actor
First we prove the following lemma, which will be utilized in proving both the consistency
and the asymptotic normality of the actor.
Lemma 4. Define Î¸Ìƒt to be the estimated optimal policy parameter by plugging in the possible
unbounded estimated reward function:
Î¸Ìƒt = argmax
Î¸
1
t
tâˆ‘
Ï„=1
âˆ‘
a
f(SÏ„ , a)
T ÂµÌ‚tÏ€Î¸(SÏ„ , a)âˆ’ Î» Î¸T
(
1
t
tâˆ‘
Ï„=1
g(SÏ„ )g(SÏ„ )
T
)
Î¸.
The probability that Î¸Ìƒt 6= Î¸Ì‚t goes to 0 as tâ†’âˆ.
Proof. The proof starts by noticing that Î¸Ì‚t and Î¸Ìƒt are equal if |f(s, a)T ÂµÌ‚t| are bounded by 2
for all combinations of (SÏ„ , a), 1 â‰¤ Ï„ â‰¤ t, a âˆˆ A.
P (Î¸Ìƒt 6= Î¸Ì‚t) â‰¤ P (âˆƒ1 â‰¤ Ï„ â‰¤ t, a âˆˆ A, s.t.|f(SÏ„ , a)T ÂµÌ‚t| > 2)
â‰¤ P (âˆƒ1 â‰¤ Ï„ â‰¤ t, a âˆˆ A, s.t. |f(SÏ„ , a)T (ÂµÌ‚t âˆ’ Âµâˆ—)|+ |f(SÏ„ , a)TÂµâˆ—| > 2)
â‰¤ P (âˆƒ1 â‰¤ Ï„ â‰¤ t, a âˆˆ A, s.t. |f(SÏ„ , a)T (ÂµÌ‚t âˆ’ Âµâˆ—)| > 1)
â‰¤ P (âˆƒ1 â‰¤ Ï„ â‰¤ t, a âˆˆ A, s.t. |f(SÏ„ , a)|2|ÂµÌ‚t âˆ’ Âµâˆ—|2 > 1)
â‰¤ P (|ÂµÌ‚t âˆ’ Âµâˆ—|2 > 1)
â†’ 0
A8
where the third inequality is based on |f(SÏ„ , a)TÂµâˆ—| â‰¤ 1 a.s. from assumption 3. The fourth
inequality is based on Holderâ€™s inequality. In the end we use the consistency of ÂµÌ‚t.
Now we can prove the consistency of the actor by proving the consistency of Î¸Ìƒt.
Proof. Proof of the theorem consists of two steps. As the first step, we claim that if a sequence
Âµt converges to Âµ
âˆ—, Î¸Ìƒt = argmaxÎ¸ J(Î¸, Âµt) converges to Î¸
âˆ—. By Lemma 9.1 in Keener (2010),
J(Î¸, Âµ) is an absolute continuous function. We proof the claim by contradiction. Suppose the
claim does not hold, i.e. there exist  such that â€–Î¸Ìƒtâˆ’Î¸âˆ—â€–2 â‰¥  for all t by taking a subsequence
if necessary. The optimality of Î¸Ìƒt implies that the inequality J(Î¸Ìƒt, Âµt) â‰¥ J(Î¸âˆ—, Âµt) holds for
all t. Since Î¸Ìƒt is bounded, it converges to an accumulation point Î¸Ìƒ by taking a subsequence if
necessary. Let tâ†’âˆ we have J(Î¸Ìƒ, Âµâˆ—) â‰¥ J(Î¸âˆ—, Âµâˆ—). On the other hand â€–Î¸âˆ—âˆ—âˆ’Î¸âˆ—â€–2 â‰¥ , which
contradicts with assumption 7. As the second step, we prove that the following M-estimator
converges uniformly in a neighborhood of Âµâˆ—, namely
Î¸Âµt = argmax
Î¸
1
t
tâˆ‘
Ï„=1
âˆ‘
aâˆˆA
f(SÏ„ , a)
TÂµÏ€Î¸(SÏ„ , a)âˆ’ Î¸T [
1
t
tâˆ‘
Ï„=1
g(SÏ„ )g(SÏ„ )
T ]Î¸
â†’ Î¸Âµ = argmax
Î¸
J(Î¸, Âµ)
in probability, and uniformly over all Âµ in a neighborhood of Âµâˆ—. Arguments in the proof
are parallel to those in Theorem 9.4 in Keener (2010). The key is to observe that the class
of random functions {j(Î¸, Âµ, s) =
âˆ‘
aâˆˆA f(s, a)
TÂµÏ€Î¸(s, a) âˆ’ Î¸Tg(s)g(s)T Î¸ : Î¸ âˆˆ Rp, |Âµ|2 â‰¤ 1}
are Glivenko-Cantelli. We have now proved the consistency of Î¸Ìƒt and thus the consistency
of Î¸Ì‚t.
C.4 Proof of the asymptotic normality of the critic
Proof. Based on the formula of ÂµÌ‚t,
ÂµÌ‚t âˆ’ Âµâˆ— = (Î¶Id +
tâˆ‘
i=1
f(Si, Ai)f(Si, Ai)
T )âˆ’1(
tâˆ‘
i=1
f(Si, Ai)i âˆ’ Âµâˆ—)
= (
Î¶Id +
âˆ‘t
i=1 f(Si, Ai)f(Si, Ai)
T
t
)âˆ’1
âˆš
t
âˆ‘t
i=1 f(Si, Ai)i
t
+ op(1)
A9
Based on the consistency of Î¸t, we have that
Î¶Id+
âˆ‘t
i=1 f(Si,Ai)f(Si,Ai)
T
t
converges in proba-
bility to EÎ¸âˆ—(f(S,A)f(S,A)T . Now it is the key to analyze the asymptotic distribution
of the martingale difference sequence {f(Si, Ai)i}ti=1. With respect to filtration Ft,j =
Ïƒ({Si, Ai, i}ji=1). Define Mâˆ— = [EÎ¸âˆ—(f(S,A)f(S,A)T )]âˆ’1/2 and a martingale difference se-
quence {Î¾t,i = M
âˆ—f(si,ai)iâˆš
t
}ti=1 which is adapted to the filtration Ft,j and satisfies E(Î¾t,i|Ft,iâˆ’1) =
0, To apply vector Lindberg-Levy central limit theorem for martingale difference sequences
(Billingsley (1961)), we check the two conditions in this theorem:
1. The conditional variance assumption.
Vt =
tâˆ‘
i=1
E(Î¾2t,i|Ft,iâˆ’1)
=
1
t
tâˆ‘
i=1
Mâˆ—EÎ¸iâˆ’1(f(s, a)f(s, a)T )Mâˆ—
converges in probability to IdÏƒ
2 by consistency of Î¸t.
2. The Lindeberg condition. For any given Î´ > 0,
tâˆ‘
i=1
E(Î¾2t,iI(â€–Î¾t,iâ€–2 > Î´)|Ft,iâˆ’1)
=
1
t
tâˆ‘
i=1
E(Mâˆ—f(Si, Ai)f(Si, Ai)T 2iMâˆ—I(â€–Mâˆ—f(Si, Ai)iâ€–1 >
âˆš
tÎ´)|Ft,iâˆ’1)
â‰¤1
t
tâˆ‘
i=1
E(Mâˆ—f(Si, Ai)f(Si, Ai)T 2iMâˆ—I(â€–Mâˆ—f(Si, Ai)â€–22i >
âˆš
tÎ´)|Ft,iâˆ’1)
By assumption 3, f(S,A) are bounded almost surely, therefore the above expression
goes to 0 as tâ†’ 0.
The Lindberg-Levy martingale central limit theorem concludes that
tâˆ‘
i=1
Î¾t,i â†’ N(0d, IdÏƒ2) in distribution
Therefore
âˆš
t(ÂµÌ‚t âˆ’ Âµâˆ—)â†’ N(0d, [EÎ¸âˆ—(f(S,A)f(S,A)T )]âˆ’1Ïƒ2) (27)
A10
C.5 Proof of the asymptotic normality of the actor
Again, our strategy is to derive the asymptotic normality of Î¸Ìƒt and then use the fact that Î¸Ì‚t
must have the same asymptotic distribution.
Proof. We first prove that
GtjÎ¸(ÂµÌ‚t, Î¸Ì‚t, S)âˆ’GtjÎ¸(Âµâˆ—, Î¸âˆ—, S) = op(1) (28)
, where Gt =
âˆš
t(Ptâˆ’P ), the empirical process induced by the â€œmarginalâ€ stochastic process
{Si}ti=1 formed by the history of contexts. The â€œfullâ€ stochastic process involves the sequence
of triples {Si, Ai, i}ti=1, the complete history of contexts, actions and reward errors. We
consider the class of functions F = {jÎ¸(Âµ, Î¸, s) : â€–Î¸ âˆ’ Î¸âˆ—â€–2 â‰¤ Î´, â€–Âµ âˆ’ Âµâˆ—â€–2 â‰¤ Î´}, where
jÎ¸(Âµ, Î¸, s) is the partial derivative with respect to Î¸ of function:
j(Âµ, Î¸, s) =
âˆ‘
a
f(s, a)TÂµÏ€Î¸(s, a)âˆ’ Î»Î¸Tg(s)g(s)T Î¸
The boundedness assumption on reward feature, policy feature and reward ensures that the
parametrized class of functions jÎ¸(Âµ, Î¸, s) is P-Donsker in a neighborhood of (Âµ
âˆ—, Î¸âˆ—). In other
words F is P-Donsker, where P is the distribution of the marginal stochastic process formed
by contexts. We complete the first part of the proof by modiftying Lemma 19.24 in Van der
Vaart (2000). It may seem that the dependence of ÂµÌ‚t and Î¸Ìƒt on the full stochastic process
could introduce complexity but a closer inspection shows that the proof goes through. The
random function jÎ¸(ÂµÌ‚t, Î¸Ìƒt, S) belongs to the P-Donsker class defined above and satisfies that
âˆ‘
s
d(s)(jÎ¸(ÂµÌ‚t, Î¸Ìƒt, s)âˆ’ jÎ¸(Âµâˆ—, Î¸âˆ—, s))2 â†’ 0
in probability. This is a result of the consistency of both ÂµÌ‚t and Î¸Ìƒt, as well as applying the con-
tinuous mapping theorem. By Theorem 18.10(v) in Van der Vaart (2000), (Gt, jÎ¸(ÂµÌ‚t, Î¸Ìƒt, s))â†’
(Gp, jÎ¸(Âµâˆ—, Î¸âˆ—, s)) in distribution, where Gp is the P-Brownian bridge. The key here is that
Theorem 18.10 only relies on the convergence of two stochastic processes, regardlessly of
A11
whether the stochastic processes consist of i.i.d. observations and whether or not the two pro-
cesses are dependent. By Lemma 18.15 in Van der Vaart (2000), almost all sample paths of Gp
are continuous on F . Define a mapping h : l(F)âˆÃ—F â†’ R by h(z, f) = z(f)âˆ’z(jÎ¸(Âµâˆ—, Î¸âˆ—, s)),
which is continuous at almost every point of (Gp, jÎ¸(Âµâˆ—, Î¸âˆ—, s)). By the continuous mapping
theorem, we have
Gt(jÎ¸(ÂµÌ‚t, Î¸Ìƒt, s)âˆ’ jÎ¸(Âµâˆ—, Î¸âˆ—, s)) = h(Gt, jÎ¸(ÂµÌ‚t, Î¸Ìƒt, s))â†’ h(Gp, jÎ¸(Âµâˆ—, Î¸âˆ—, s)) = 0
in distribution and thus in probability, therefore (28) holds. The second part of the proof
begins by noticing that Î¸Ìƒt satisfies the estimating equation PtjÎ¸(ÂµÌ‚t, Î¸Ìƒt, s) = 0, so we have
GtjÎ¸(ÂµÌ‚t, Î¸Ìƒt, s) =
âˆš
t(PjÎ¸(Âµ
âˆ—, Î¸âˆ—, s)âˆ’ PjÎ¸(ÂµÌ‚t, Î¸Ìƒt, s))
=
âˆš
t(JÎ¸(Âµ
âˆ—, Î¸âˆ—)âˆ’ JÎ¸(ÂµÌ‚t, Î¸Ìƒt))
=
âˆš
tJâˆ—Î¸Î¸(Î¸
âˆ— âˆ’ Î¸Ìƒt) +
âˆš
tJâˆ—Î¸Âµ(Âµ
âˆ— âˆ’ ÂµÌ‚t) +
âˆš
top(â€–Î¸Ìƒt âˆ’ Î¸âˆ—â€–) + op(1)
Together with (28) the above implies
âˆš
t(Î¸âˆ— âˆ’ Î¸Ìƒt) = (Jâˆ—Î¸Î¸)âˆ’1Jâˆ—Î¸Âµ
âˆš
t(ÂµÌ‚t âˆ’ Âµâˆ—) +
âˆš
top(â€–Î¸Ìƒt âˆ’ Î¸âˆ—â€–) + (Jâˆ—Î¸Î¸)âˆ’1GtjÎ¸(Âµâˆ—, Î¸âˆ—, s) + op(1)
= Op(1) +
âˆš
top(â€–Î¸Ìƒt âˆ’ Î¸âˆ—â€–)
where Jâˆ—Î¸Î¸ and J
âˆ—
Î¸Âµ are JÎ¸Î¸ and JÎ¸Âµ evaluated at (Î¸
âˆ—, Âµâˆ—). The
âˆš
t consistency of Î¸Ìƒt follows
through. Now (29) has become
âˆš
t(Î¸âˆ— âˆ’ Î¸Ìƒt) = (Jâˆ—Î¸Î¸)âˆ’1Jâˆ—Î¸Âµ
âˆš
t(ÂµÌ‚t âˆ’ Âµâˆ—) + (Jâˆ—Î¸Î¸)âˆ’1GtjÎ¸(Âµâˆ—, Î¸âˆ—, S) + op(1) (29)
Since both the two non-vanishing terms on the righthand side are asymptotically normal
with zero mean,
âˆš
t(Î¸âˆ— âˆ’ Î¸Ìƒt) is asymptotically normal. The only task left is to derive the
asymptotic variance. Plugging in the formula for ÂµÌ‚t, we have
âˆš
t(Î¸âˆ— âˆ’ Î¸Ìƒt) = (Jâˆ—Î¸Î¸)âˆ’1
âˆ‘t
i=1 J
âˆ—
Î¸ÂµB
âˆ—f(Si, Ai)i + jÎ¸(Âµ
âˆ—, Î¸âˆ—, Si)
t
+ op(1)
= (Jâˆ—Î¸Î¸)
âˆ’1
tâˆ‘
i=1
Î¶t,i + op(1)
A12
where Bâˆ— = (Mâˆ—)2 = [EÎ¸âˆ—f(S,A)f(S,A)T ]âˆ’1. {Î¶i =
Jâˆ—Î¸ÂµB
âˆ—f(Si,Ai)i+jÎ¸(Âµ
âˆ—,Î¸âˆ—,Si)
t
}ti=1 is a martin-
gale difference sequence with asymptotic variance
tâˆ‘
i=1
E(Î¶2t,i|Ft,i)
=
1
t
tâˆ‘
i=1
E(2i gâˆ—Î¸ÂµBâˆ—f(Si, Ai)f(Si, Ai)TBâˆ—gâˆ—ÂµÎ¸
+ jÎ¸(Âµ
âˆ—, Î¸âˆ—, Si)jÎ¸(Âµ
âˆ—, Î¸âˆ—, Si)
T âˆ’ 2JÎ¸ÂµBâˆ—f(Si, Ai)jÎ¸(Âµâˆ—, Î¸âˆ—, Si)T i|Ft,i)
=
1
t
tâˆ‘
i=1
Ïƒ2Jâˆ—Î¸ÂµB
âˆ—EÎ¸iâˆ’1(f(S,A)f(S,A)T )Bâˆ—Jâˆ—ÂµÎ¸ +
âˆ‘
s
d(s)jÎ¸(Âµ
âˆ—, Î¸âˆ—, s)jÎ¸(Âµ
âˆ—, Î¸âˆ—, s)T
which converges in probability to V âˆ— = Ïƒ2Jâˆ—Î¸ÂµB
âˆ—Jâˆ—ÂµÎ¸+
âˆ‘
s d(s)jÎ¸(Âµ
âˆ—, Î¸âˆ—, s)jÎ¸(Âµ
âˆ—, Î¸âˆ—, s)T . There-
fore the asymptotic variance of
âˆš
t(Î¸âˆ— âˆ’ Î¸Ìƒt) is (Jâˆ—Î¸Î¸)âˆ’1V âˆ—(Jâˆ—Î¸Î¸)âˆ’1.
D Small Sample Variance estimation and Bootstrap
Confidence intervals
In this section, we discuss issues, challenges and solutions in creating confidence intervals
for the optimal policy parameter Î¸âˆ— when the sample size, the total number of decision
points, is small. We use a simple example to illustrate that the traditional plug-in variance
estimator is plagued with underestimation issue, the direct consequence of which is the
deflated confidence levels of the Wald-type confidence intervals for Î¸âˆ—. We propose to use
bootstrap confidence intervals when the sample size is finite. We use simulation to evaluate
the bootstrap confidence intervals.
D.1 Plug-in Variance Estimation and Wald Confidence intervals
One of the most straightforward ways to estimate the asymptotic variance of Î¸t is through
the plug-in variance estimation, the formulae of which is provided in Theorem 2. Once an
A13
estimated variance VÌ‚i is obtained for
âˆš
t(Î¸Ì‚iâˆ’ Î¸âˆ—i ), a (1âˆ’ 2Î±)% Wald type confidence interval
for Î¸âˆ—i has the form: [Î¸Ì‚i âˆ’ zÎ± VÌ‚iâˆšt , Î¸Ì‚i + zÎ±
VÌ‚iâˆš
t
]. Here Î¸i is the i-th component in Î¸ and zÎ± is the
upper 100Î± percentile of a standard normal distribution. The plug-in variance estimator and
the associated Wald confidence intervals work well in many statistics problems. We shall
see that, however, the plug-in variance estimator of the estimated optimal policy parameters
suffers from underestimation issue in small to moderate sample sizes. In particular this
estimator is very sensitive to the plugged-in value of the estimated reward parameter and
policy parameter: a small deviation from the true parameters can result in an inflated
or deflated variance estimation. Deflated variance estimation produces anti-conservative
confidence intervals, a grossly undesirable property for confidence intervals. The following
simple example illustrates the problem.
Example 1. The context is binary with probability distribution P(S = 1) = P(S = âˆ’1) =
0.5. The reward is generated according to the following linear model: given context S âˆˆ
{âˆ’1, 1} and action A âˆˆ {0, 1},
R = Âµâˆ—0 + Âµ
âˆ—
1S + Âµ
âˆ—
2A+ Âµ
âˆ—
3SA+ 
where  follows a normal distribution with mean zero and standard deviation 9. The true
reward parameter is Âµâˆ— = [1, 1, 1, 1]. Both Âµâˆ— and the standard deviation of  are chosen to
approximate the realistic signal noise ratio in mobile health applications. We consider the
policy class Ï€Î¸(A = 1|S = s) = e
Î¸0+Î¸1s
1+eÎ¸0+Î¸1s
.
The differences between the plug-in estimated variance and its population counterpart are
that (1) the former uses the empirical distribution of context to replace the unknown popu-
lation distribution and (2) the unknown reward parameter and optimal policy parameter are
replaced by their estimates. We emphasize that it is the second difference that leads to the
underestimated variance in small sample size. To see this, we ignore the difference between
the empirical distribution and the population distribution of contexts, which is very small for
sample size T â‰¥ 50 under a Bernoulli context distribution with equal probability. Now the
plug-in variance estimator is a function of the estimated reward parameter ÂµÌ‚t and the esti-
mated policy parameter Î¸Ì‚t. Notice that Î¸Ì‚t = [Î¸Ì‚t,0, Î¸Ì‚t,1] is a function of ÂµÌ‚t = [ÂµÌ‚t,0, ÂµÌ‚t,1, ÂµÌ‚t,2, ÂµÌ‚t,3]
A14
and the empirical distribution of context. If we replace the empirical distribution in calcu-
lating Î¸Ì‚t by its population counterpart, Î¸Ì‚t is simply a function of ÂµÌ‚t. In the rest part of the
example, we drop the subscript t in the estimated reward parameter and denote the estimate
of Âµ2 and Âµ3 by ÂµÌ‚2 and ÂµÌ‚3, respectively. Likewise, Î¸Ì‚t,i is replaced by Î¸Ì‚i for i = 0, 1.
Figure 3 is the surface plot showing how the plug-in variance estimation changes as func-
tion of the estimated reward parameter. The surface plot of the plug-in variance estima-
tion has a mountain-like pattern with two ridges along the two diagonals ÂµÌ‚2 + ÂµÌ‚3 = 0 and
ÂµÌ‚2 âˆ’ ÂµÌ‚3 = 0. The height of the ridge increases as both ÂµÌ‚2 and ÂµÌ‚3 approaches the origin.
The peak of mountain is at the origin where ÂµÌ‚2 = ÂµÌ‚3 = 0. The true reward parameter
(Âµâˆ—2, Âµ
âˆ—
3) = (1, 1) is close to the origin and lies right on the one of the ridges. There are four
â€œvalleysâ€ where the combinations of ÂµÌ‚2 and ÂµÌ‚3 gives a small plug-in variance.
Figure 3: Plug in variance estimation as a function of
ÂµÌ‚2 and ÂµÌ‚3, x axis represents ÂµÌ‚t,2, y axis represents ÂµÌ‚t,3 and z axis represents the plug-in
asymptotic variance of Î¸Ì‚0 with Î» = 0.1
Due to large areas of valley the plug-in variance estimation is biased down, a direct
consequence of which is the anti-conservatism of the Wald confidence intervals. We perform
a simulation study using the toy generative model described above. The simulation consists
of 1000 repetitions of running the online actor critic algorithm and recording the end-of-
A15
study statistics, including the plugin variance estimate, the Wald confidence intervals and the
theoretical Wald confidence intervals based on the true asymptotic variance. The first two
columns in table 14 show the bias of plug-in variance at different sample sizes. At all three
different sample sizes, the plug-in variance estimator underestimates the true asymptotic
variance, which is 293.03 for both policy parameters. Column 3 and column 4 show the
coverage rate of the Wald-type confidence interval (CI) using the plug-in estimated variance.
It is not surprising that the confidence intervals suffer from severe anti-conservatism, a
consequence of the heavily biased variance estimation. Column 5 and 6 show the coverage
rate of the Wald-type confidence interval based on the true asymptotic variance. Comparing
the coverage rates, it is clear that the anti-conservatism is due to the underestimated variance.
A16
sa
m
p
le
si
ze
b
ia
s
in
va
ri
an
ce
es
ti
m
at
io
n
co
ve
ra
ge
of
W
al
d
C
I
(%
)
co
ve
ra
ge
of
th
eo
re
ti
ca
l
W
al
d
C
I
(%
)
Î¸ 0
Î¸ 1
Î¸ 0
Î¸ 1
Î¸ 0
Î¸ 1
10
0
-1
81
.5
6
-1
81
.5
6
75
.5
74
.9
10
0.
0
10
0.
0
25
0
-1
31
.7
1
-1
31
.7
1
77
.9
77
.3
98
.5
98
.1
50
0
-1
08
.6
4
-1
08
.6
4
78
.8
79
.2
98
.9
98
.7
T
ab
le
14
:
U
n
d
er
es
ti
m
at
io
n
of
th
e
p
lu
g-
in
va
ri
an
ce
es
ti
m
at
or
an
d
th
e
W
al
d
co
n
fi
d
en
ce
in
te
rv
al
s.
T
h
eo
re
ti
ca
l
W
al
d
C
I
is
cr
ea
te
d
b
as
ed
on
th
e
tr
u
e
as
y
m
p
to
ti
c
va
ri
an
ce
.
A17
To detail how the confidence interval coverage is connected with the estimated reward
parameter (ÂµÌ‚2, ÂµÌ‚3), figure 4 and figure 5 present two scatter plots of ÂµÌ‚2, ÂµÌ‚3 for the 1000
simulated datasets at sample size 100 and 500. Different colors are used to mark the datasets
where the confidence intervals of both Î¸0 and Î¸1 cover the true parameter (blue), only one
of them cover the truth (green), neither of them covers the truth (fading yellow). The true
parameter are marked with a red asterisk. Indeed the yellow points and green points are in
the â€œvalleysâ€. Some of the blue points are away from truth, but nevertheless they remain on
the ridge, which produces a high variance estimate. Comparing the two scatter plots, as the
sample size increases, the estimated reward parameter is less spread out. Nevertheless there
are still significantly many pair of ÂµÌ‚2, ÂµÌ‚3 that fall in the â€œvalleysâ€, leading to a underestimated
variance and anti-conservative confidence intervals.
A18
7Ì‚
2
-8
-6
-4
-2
0
2
4
6
8
7Ì‚3
-8-6-4-2
02468
F
ig
u
re
4:
W
al
d
co
n
fi
d
en
ce
in
te
rv
al
co
ve
ra
ge
fo
r
10
00
si
m
u
-
la
te
d
d
at
as
et
s
as
a
fu
n
ct
io
n
of
ÂµÌ‚
3
an
d
ÂµÌ‚
2
at
sa
m
p
le
si
ze
10
0.
7Ì‚
2
-8
-6
-4
-2
0
2
4
6
8
7Ì‚3
-8-6-4-2
02468
F
ig
u
re
5:
W
al
d
co
n
fi
d
en
ce
in
te
rv
al
co
ve
ra
ge
in
10
00
si
m
u
la
te
d
d
at
as
et
s
as
a
fu
n
ct
io
n
of
ÂµÌ‚
3
an
d
ÂµÌ‚
2
at
sa
m
p
le
si
ze
50
0.
A19
Figure 6 shows the histogram for the normalized distance
Ë†âˆš
T (Î¸iâˆ’Î¸âˆ—i )
VÌ‚i
for i = 0, 1 where
T = 100. This is the distance between the estimated and the true optimal policy parameter
normalized by the estimated asymptotic variance. For the Wald confidence intervals to have
descent coverage, histogram of the normalized distances need to approximate a standard nor-
mal distribution. However, as figure 6 suggests, the histograms have heavier tails compared
to a standard normal due to the underestimated variance. The figure also suggests that the
percentile-t bootstrap confidence intervals can be a good remedy.
standardized distance for 3Ì‚0
-10 -5 0 5 10
de
ns
ity
0
0.1
0.2
0.3
0.4
0.5
0.6
standardized distance for 3Ì‚1
-10 -5 0 5 10
de
ns
ity
0
0.1
0.2
0.3
0.4
0.5
0.6
Figure 6: Histograms of the normalized distance
Ë†âˆš
T (Î¸iâˆ’Î¸âˆ—i )
VÌ‚i
for i = 0, 1 at sample size 100
A20
E Burden Effect: Actor Critic Algorithm Uses Î»âˆ—
Î½ Î»âˆ— Î¸âˆ—0 Î¸
âˆ—
1 Î¸
âˆ—
2 Î¸
âˆ—
3
0.0 0.06 0.3410 0.3269 0.3264 0
0.2 0.05 0.0844 0.3844 0.4 -0.1609
0.4 0.06 -0.1922 0.3547 0.3312 -0.2313
0.6 0.08 -0.3312 0.2488 0.2234 -0.2687
0.8 0.1 -0.3883 0.2078 0.2 -0.2687
Table 15: Burden effect: the optimal policy and the oracle lambda.
Ï„ Î¸âˆ—0 Î¸
âˆ—
1 Î¸
âˆ—
2 Î¸
âˆ—
3
0 âˆ’0.027 352 âˆ’0.035 565 âˆ’0.030 344 0.003 449
0.2 0.229 47 âˆ’0.092 877 âˆ’0.104 06 0.164 21
0.4 0.505 86 âˆ’0.063 199 âˆ’0.035 223 0.234 73
0.6 0.645 07 0.042 695 0.072 542 0.271 98
0.8 0.702 29 0.083 867 0.096 08 0.2718
Table 16: Burden effect: bias in estimating the optimal policy parameter at sample size 200.
The algorithm uses Î»âˆ— instead of learning Î» online. Bias=E(Î¸Ì‚t)âˆ’ Î¸âˆ—.
A21
Ï„ Î¸âˆ—0 Î¸
âˆ—
1 Î¸
âˆ—
2 Î¸
âˆ—
3
0 0.057 811 0.037 16 0.036 343 0.035 898
0.2 0.109 61 0.044 463 0.046 192 0.062 836
0.4 0.312 95 0.039 819 0.036 65 0.090 984
0.6 0.473 09 0.037 714 0.040 625 0.109 84
0.8 0.550 24 0.042 799 0.044 54 0.1097
Table 17: Burden effect: MSE in estimating the optimal policy parameter at sample size
200. The algorithm uses Î»âˆ— instead of learning Î» online.
Î½ Î¸0 Î¸1 Î¸2 Î¸3
0 0.963 0.963 0.955 0.942
0.2 0.853* 0.946 0.937 0.862*
0.4 0.565* 0.96 0.954 0.776*
0.6 0.39* 0.937 0.916* 0.739*
0.8 0.329* 0.908* 0.899* 0.739*
Table 18: Burden effect: coverage rates of percentile-t bootstrap confidence intervals for the
optimal policy parameter at sample size 200. The algorithm uses Î»âˆ— instead of learning Î»
online. Coverage rates significantly lower than 0.95 are marked with asterisks (*).
A22
Ï„ Î¸âˆ—0 Î¸
âˆ—
1 Î¸
âˆ—
2 Î¸
âˆ—
3
0.0 âˆ’0.017 692 âˆ’0.013 808 âˆ’0.006 068 âˆ’0.008 696
0.2 0.288 29 âˆ’0.031 35 âˆ’0.039 795 0.148 92
0.4 0.515 53 âˆ’0.041 593 âˆ’0.010 872 0.222 59
0.6 0.591 24 0.005 305 0.037 288 0.261 54
0.8 0.606 07 0.006 205 0.020 356 0.262 63
Table 19: Burden effect: bias in estimating the optimal policy parameter at sample size 500.
The algorithm uses Î»âˆ— instead of learning Î» online. Bias=E(Î¸Ì‚t)âˆ’ Î¸âˆ—.
Ï„ Î¸âˆ—0 Î¸
âˆ—
1 Î¸
âˆ—
2 Î¸
âˆ—
3
0.0 0.029 022 0.016 576 0.015 445 0.016 196
0.2 0.120 73 0.022 334 0.021 348 0.042 485
0.4 0.294 46 0.018 117 0.015 525 0.065 667
0.6 0.366 97 0.011 343 0.011 526 0.078 681
0.8 0.378 72 0.008 136 0.007 766 0.076 209
Table 20: Burden effect: MSE in estimating the optimal policy parameter at sample size
500. The algorithm uses Î»âˆ— instead of learning Î» online.
A23
Ï„ Î¸0 Î¸1 Î¸2 Î¸3
0.0 0.944 0.950 0.952 0.933*
0.2 0.689* 0.943 0.959 0.815*
0.4 0.159* 0.944 0.954 0.6*
0.6 0.006* 0.941 0.928* 0.295*
0.8 0* 0.94 0.944 0.144*
Table 21: Burden effect: coverage rates of percentile-t bootstrap confidence intervals for the
optimal policy parameter at sample size 500. The algorithm uses Î»âˆ— instead of learning Î»
online. Coverage rates significantly lower than 0.95 are marked with asterisks (*).
Ï„ Î¸âˆ—0 Î¸
âˆ—
1 Î¸
âˆ—
2 Î¸
âˆ—
3
0 0.392 0.3723 0.3713 âˆ’0.0006
0.2 0.3921 0.3722 0.3713 âˆ’0.0006
0.4 0.392 0.3723 0.3713 âˆ’0.0006
0.6 0.392 0.3723 0.3713 âˆ’0.0006
0.8 0.392 0.3723 0.3713 âˆ’0.0006
Table 22: Burden effect: the myopic equilibrium policy.
F Nonlinear Reward: The Optimal Policy
Î± Î¸âˆ—0 Î¸
âˆ—
1 Î¸
âˆ—
2 Î¸
âˆ—
3
0 0.418 035 0.395 067 0.397 071 âˆ’0.001 615
0.2 0.496 240 0.296 973 0.385 421 0.000 480
0.4 0.564 503 0.202 857 0.365 239 0.001 684
0.6 0.811 000 0.542 000 0.888 000 0.925 000
Table 23: Nonlinear reward: the optimal policy.
A24

