498 IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 47, NO. 2, FEBRUARY 2001
Factor Graphs and the Sum-Product Algorithm
Frank R. Kschischang, Senior Member, IEEE, Brendan J. Frey, Member, IEEE, and
Hans-Andrea Loeliger, Member, IEEE
Abstract—Algorithms that must deal with complicated global
functions of many variables often exploit the manner in which the
given functions factor as a product of “local” functions, each of
which depends on a subset of the variables. Such a factorization
can be visualized with a bipartite graph that we call a factor graph.
In this tutorial paper, we present a generic message-passing algo-
rithm, the sum-product algorithm, that operates in a factor graph.
Following a single, simple computational rule, the sum-product
algorithm computes—either exactly or approximately—var-
ious marginal functions derived from the global function. A
wide variety of algorithms developed in artificial intelligence,
signal processing, and digital communications can be derived as
specific instances of the sum-product algorithm, including the
forward/backward algorithm, the Viterbi algorithm, the iterative
“turbo” decoding algorithm, Pearl’s belief propagation algorithm
for Bayesian networks, the Kalman filter, and certain fast Fourier
transform (FFT) algorithms.
Index Terms—Belief propagation, factor graphs, fast Fourier
transform, forward/backward algorithm, graphical models, iter-
ative decoding, Kalman filtering, marginalization, sum-product
algorithm, Tanner graphs, Viterbi algorithm.
I. INTRODUCTION
THIS paper provides a tutorial introduction to factor graphsand the sum-product algorithm, a simple way to under-
stand a large number of seemingly different algorithms that have
been developed in computer science and engineering. We con-
sider algorithms that deal with complicated “global” functions
of many variables and that derive their computational efficiency
by exploiting the way in which the global function factors into
a product of simpler “local” functions, each of which depends
on a subset of the variables. Such a factorization can be visual-
ized using a factor graph, a bipartite graph that expresses which
variables are arguments of which local functions.
Manuscript received August 3, 1998; revised October 17, 2000. The work
of F. R. Kschischang was supported in part, while on leave at the Massachu-
setts Institute of Technology, by the Office of Naval Research under Grant
N00014-96-1-0930, and by the Army Research Laboratory under Cooperative
Agreement DAAL01-96-2-0002. The work of B. J. Frey was supported,
while a Beckman Fellow at the Beckman Institute of Advanced Science and
Technology, University of Illinois at Urbana-Champaign, by a grant from
the Arnold and Mabel Beckman Foundation. The material in this paper was
presented in part at the 35th Annual Allerton Conference on Communication,
Control, and Computing, Monticello, IL, September 1997.
F. R. Kschischang is with the Department of Electrical and Computer
Engineering, University of Toronto, Toronto, ON M5S 3G4, Canada (e-mail:
frank@comm.utoronto.ca).
B. J. Frey is with the Faculty of Computer Science, University of Waterloo,
Waterloo, ON N2L 3G1, Canada, and the Faculty of Electrical and Com-
puter Engineering, University of Illinois at Urbana-Champaign, Urbana, IL
61801-2307 USA (e-mail: frey@.uwaterloo.ca).
H.-A. Loeliger is with the Signal Processing Lab (ISI), ETH Zentrum,
CH-8092 Zürich, Switzerland (e-mail: loeliger@isi.ee.ethz.ch).
Communicated by T. E. Fuja, Associate Editor At Large.
Publisher Item Identifier S 0018-9448(01)00721-0.
The aim of this tutorial paper is to introduce factor graphs
and to describe a generic message-passing algorithm, called the
sum-product algorithm, which operates in a factor graph and at-
tempts to compute various marginal functions associated with
the global function. The basic ideas are very simple; yet, as
we will show, a surprisingly wide variety of algorithms devel-
oped in the artificial intelligence, signal processing, and dig-
ital communications communities may be derived as specific
instances of the sum-product algorithm, operating in an appro-
priately chosen factor graph.
Genealogically, factor graphs are a straightforward gen-
eralization of the “Tanner graphs” of Wiberg et al. [31],
[32]. Tanner [29] introduced bipartite graphs to describe
families of codes which are generalizations of the low-density
parity-check (LDPC) codes of Gallager [11], and also described
the sum-product algorithm in this setting. In Tanner’s original
formulation, all variables are codeword symbols and hence
“visible”; Wiberg et al., introduced “hidden” (latent) state vari-
ables and also suggested applications beyond coding. Factor
graphs take these graph-theoretic models one step further, by
applying them to functions. From the factor-graph perspective
(as we will describe in Section III-A), a Tanner graph for a
code represents a particular factorization of the characteristic
(indicator) function of the code.
While it may seem intuitively reasonable that some algo-
rithms should exploit the manner in which a global function
factors into a product of local functions, the fundamental insight
that many well-known algorithms essentially solve the “MPF”
(marginalize product-of-functions) problem, each in their own
particular setting, was first made explicit in the work of Aji
and McEliece [1]. In a landmark paper [2], Aji and McEliece
develop a “generalized distributive law” (GDL) that in some
cases solves the MPF problem using a “junction tree” represen-
tation of the global function. Factor graphs may be viewed as
an alternative approach with closer ties to Tanner graphs and
previously developed graphical representations for codes. Es-
sentially, every result developed in the junction tree/GDL set-
ting may be translated into an equivalent result in the factor
graph/sum-product algorithm setting, and vice versa. We prefer
the latter setting not only because it is better connected with pre-
vious approaches, but also because we feel that factor graphs are
in some ways easier to describe, giving them a modest pedagog-
ical advantage. Moreover, the sum-product algorithm can often
be applied successfully in situations where exact solutions to the
MPF problem (as provided by junction trees) become computa-
tionally intractable, the most prominent example being the iter-
ative decoding of turbo codes and LDPC codes. In Section VI
we do, however, discuss strategies for achieving exact solutions
to the MPF problem in factor graphs.
0018–9448/01$10.00 © 2001 IEEE
KSCHISCHANG et al.: FACTOR GRAPHS AND THE SUM-PRODUCT ALGORITHM 499
There are also close connections between factor graphs and
graphical representations (graphical models) for multidimen-
sional probability distributions such as Markov random fields
[16], [18], [26] and Bayesian (belief) networks [25], [17]. Like
factor graphs, these graphical models encode in their structure a
particular factorization of the joint probability mass function of
several random variables. Pearl’s powerful “belief propagation”
algorithm [25], which operates by “message-passing” in a
Bayesian network, translates immediately into an instance of
the sum-product algorithm operating in a factor graph that
expresses the same factorization. Bayesian networks and belief
propagation have been used previously to explain the iterative
decoding of turbo codes and LDPC codes [9], [10], [19], [21],
[22], [24], the most powerful practically decodable codes
known. Note, however, that Wiberg [31] had earlier described
these decoding algorithms as instances of the sum-product
algorithm; see also [7].
We begin the paper in Section II with a small worked example
that illustrates the operation of the sum-product algorithm in a
simple factor graph. We will see that when a factor graph is
cycle-free, then the structure of the factor graph not only en-
codes the way in which a given function factors, but also en-
codes expressions for computing the various marginal functions
associated with the given function. These expressions lead di-
rectly to the sum-product algorithm.
In Section III, we show how factor graphs may be used as a
system and signal-modeling tool. We see that factor graphs are
compatible both with “behavioral” and “probabilistic” modeling
styles. Connections between factor graphs and other graphical
models are described briefly in Appendix B, where we recover
Pearl’s belief propagation algorithm as an instance of the sum-
product algorithm.
In Section IV, we apply the sum-product algorithm to
trellis-structured (hidden Markov) models, and obtain the
forward/backward algorithm, the Viterbi algorithm, and the
Kalman filter as instances of the sum-product algorithm. In
Section V, we consider factor graphs with cycles, and obtain
the iterative algorithms used to decode turbo-like codes as
instances of the sum-product algorithm.
In Section VI, we describe several generic transformations
by which a factor graph with cycles may sometimes be con-
verted—often at great expense in complexity—to an equivalent
cycle-free form. We apply these ideas to the factor graph repre-
senting the discrete Fourier transform (DFT) kernel, and derive
a fast Fourier transform (FFT) algorithm as an instance of the
sum-product algorithm.
Some concluding remarks are given in Section VII.
II. MARGINAL FUNCTIONS, FACTOR GRAPHS, AND THE
SUM-PRODUCT ALGORITHM
Throughout this paper we deal with functions of many vari-
ables. Let , be a collection of variables, in which,
for each , takes on values in some (usually finite) domain
(or alphabet) . Let be an -valued function
of these variables, i.e., a function with domain
and codomain . The domain of is called the configuration
space for the given collection of variables, and each element of
is a particular configuration of the variables, i.e., an assign-
ment of a value to each variable. The codomain of may in
general be any semiring [2], [31, Sec. 3.6]; however, at least ini-
tially, we will lose nothing essential by assuming that is the
set of real numbers.
Assuming that summation in is well defined, then associ-
ated with every function are marginal func-
tions . For each , the value of is obtained by
summing the value of over all configurations of
the variables that have .
This type of sum is so central to this paper that we introduce
a nonstandard notation to handle it: the “not-sum” or summary.
Instead of indicating the variables being summed over, we indi-
cate those variables not being summed over. For example, if is
a function of three variables , , and , then the “summary
for ” is denoted by
In this notation we have
i.e., the th marginal function associated with is
the summary for of .
We are interested in developing efficient procedures for com-
puting marginal functions that a) exploit the way in which the
global function factors, using the distributive law to simplify the
summations, and b) reuses intermediate values (partial sums).
As we will see, such procedures can be expressed very natu-
rally by use of a factor graph.
Suppose that factors into a product of several
local functions, each having some subset of as
arguments; i.e., suppose that
(1)
where is a discrete index set, is a subset of ,
and is a function having the elements of as argu-
ments.
Definition: A factor graph is a bipartite graph that expresses
the structure of the factorization (1). A factor graph has a vari-
able node for each variable , a factor node for each local func-
tion , and an edge-connecting variable node to factor node
if and only if is an argument of .
A factor graph is thus a standard bipartite graphical represen-
tation of a mathematical relation—in this case, the “is an argu-
ment of” relation between variables and local functions.
Example 1 (A Simple Factor Graph): Let
be a function of five variables, and suppose that can
be expressed as a product
(2)
500 IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 47, NO. 2, FEBRUARY 2001
Fig. 1. A factor graph for the product f (x )f (x )f (x ; x ; x )
 f (x ; x )f (x ; x ).
of five factors, so that , ,
, , , and
. The factor graph that corresponds to (2) is shown in
Fig. 1.
A. Expression Trees
In many situations (for example, when rep-
resents a joint probability mass function), we are interested in
computing the marginal functions . We can obtain an ex-
pression for each marginal function by using (2) and exploiting
the distributive law.
To illustrate, we write from Example 1 as
or, in summary notation
(3)
Similarly, we find that
(4)
In computer science, arithmetic expressions like the
right-hand sides of (3) and (4) are often represented by or-
dered rooted trees [28, Sec. 8.3], here called expression trees,
in which internal vertices (i.e., vertices with descendants)
represent arithmetic operators (e.g., addition, multiplication,
negation, etc.) and leaf vertices (i.e., vertices without descen-
dants) represent variables or constants. For example, the tree of
Fig. 2 represents the expression . When the operators
in an expression tree are restricted to those that are completely
symmetric in their operands (e.g., multiplication and addition),
Fig. 2. An expression tree representing x(y + z).
it is unnecessary to order the vertices to avoid ambiguity in
interpreting the expression represented by the tree.
In this paper, we extend expression trees so that the leaf ver-
tices represent functions, not just variables or constants. Sums
and products in such expression trees combine their operands in
the usual (pointwise) manner in which functions are added and
multiplied. For example, Fig. 3(a) unambiguously represents the
expression on the right-hand side of (3), and Fig. 4(a) unambigu-
ously represents the expression on the right-hand side of (4). The
operators shown in these figures are the function product and the
summary, having various local functions as their arguments.
Also shown in Figs. 3(b) and 4(b), are redrawings of the factor
graph of Fig. 1 as a rooted tree with and as root vertex,
respectively. This is possible because the global function de-
fined in (2) was deliberately chosen so that the corresponding
factor graph is a tree. Comparing the factor graphs with the cor-
responding trees representing the expression for the marginal
function, it is easy to note their correspondence. This observa-
tion is simple, but key: when a factor graph is cycle-free, the
factor graph not only encodes in its structure the factorization
of the global function, but also encodes arithmetic expressions
by which the marginal functions associated with the global func-
tion may be computed.
Formally, as we show in Appendix A, to convert a cycle-free
factor graph representing a function to the cor-
responding expression tree for , draw the factor graph as
a rooted tree with as root. Every node in the factor graph
then has a clearly defined parent node, namely, the neighboring
node through which the unique path from to must pass. Re-
place each variable node in the factor graph with a product op-
erator. Replace each factor node in the factor graph with a “form
product and multiply by ” operator, and between a factor node
and its parent , insert a summary operator. These
local transformations are illustrated in Fig. 5(a) for a variable
node, and in Fig. 5(b) for a factor node with parent . Trivial
products (those with one or no operand) act as identity opera-
tors, or may be omitted if they are leaf nodes in the expression
tree. A summary operator applied to a function with a
single argument is also a trivial operation, and may be omitted.
Applying this transformation to the tree of Fig. 3(b) yields the
expression tree of Fig. 3(a), and similarly for Fig. 4. Trivial op-
erations are indicated with dashed lines in these figures.
B. Computing a Single Marginal Function
Every expression tree represents an algorithm for computing
the corresponding expression. One might describe the algorithm
as a recursive “top-down” procedure that starts at the root vertex
and evaluates each subtree descending from the root, combining
the results as dictated by the operator at the root. Equivalently,
we prefer to describe the algorithm as a “bottom-up” procedure
that begins at the leaves of the tree, with each operator vertex
KSCHISCHANG et al.: FACTOR GRAPHS AND THE SUM-PRODUCT ALGORITHM 501
Fig. 3. (a) A tree representation for the right-hand side of (3). (b) The factor graph of Fig. 1, redrawn as a rooted tree with x as root.
Fig. 4. (a) A tree representation for the right-hand side of (4). (b) The factor graph of Fig. 1, redrawn as a rooted tree with x as root.
combining its operands and passing on the result as an operand
for its parent. For example, , represented by the ex-
pression tree of Fig. 2, might be evaluated by starting at the leaf
nodes and , evaluating , and passing on the result as an
operand for the operator, which multiplies the result with .
Rather than working with the expression tree, it is simpler
and more direct to describe such marginalization algorithms in
terms of the corresponding factor graph. To best understand
such algorithms, it helps to imagine that there is a processor
associated with each vertex of the factor graph, and that the
factor-graph edges represent channels by which these proces-
sors may communicate. For us, “messages” sent between pro-
cessors are always simply some appropriate description of some
marginal function. (We describe some useful representations in
Section V-E.)
We now describe a message-passing algorithm that we will
temporarily call the “single- sum-product algorithm,” since it
computes, for a single value of , the marginal function
in a rooted cycle-free factor graph, with taken as root vertex.
The computation begins at the leaves of the factor graph. Each
leaf variable node sends a trivial “identity function” message to
its parent, and each leaf factor node sends a description of
to its parent. Each vertex waits for messages from all of its
children before computing the message to be sent to its parent.
This computation is performed according to the transformation
shown in Fig. 5; i.e., a variable node simply sends the product
of messages received from its children, while a factor node
with parent forms the product of with the messages received
from its children, and then operates on the result with a
summary operator. By a “product of messages” we mean an
appropriate description of the (pointwise) product of the cor-
responding functions. If the messages are parametrizations of
the functions, then the resulting message is the parametrization
of the product function, not (necessarily) literally the numerical
502 IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 47, NO. 2, FEBRUARY 2001
Fig. 5. Local substitutions that transform a rooted cycle-free factor graph to
an expression tree for a marginal function at (a) a variable node and (b) a factor
node.
product of the messages. Similarly, the summary operator is ap-
plied to the functions, not necessarily literally to the messages
themselves.
The computation terminates at the root node , where the
marginal function is obtained as the product of all mes-
sages received at .
It is important to note that a message passed on the edge
, either from variable to factor , or vice versa, is a
single-argument function of , the variable associated with the
given edge. This follows since, at every factor node, summary
operations are always performed for the variable associated with
the edge on which the message is passed. Likewise, at a variable
node, all messages are functions of that variable, and so is any
product of these messages.
The message passed on an edge during the operation of the
single- sum-product algorithm can be interpreted as follows. If
is an edge in the tree, where is a variable node
and is a factor node, then the analysis of Appendix A shows
that the message passed on during the operation of the sum-
product algorithm is simply a summary for of the product of
the local functions descending from the vertex that originates
the message.
C. Computing All Marginal Functions
In many circumstances, we may be interested in computing
for more than one value of . Such a computation might
be accomplished by applying the single- algorithm separately
for each desired value of , but this approach is unlikely to
be efficient, since many of the subcomputations performed for
different values of will be the same. Computation of
for all simultaneously can be efficiently accomplished by es-
sentially “overlaying” on a single factor graph all possible in-
stances of the single- algorithm. No particular vertex is taken
as a root vertex, so there is no fixed parent/child relationship
among neighboring vertices. Instead, each neighbor of any
given vertex is at some point regarded as a parent of . The
message passed from to is computed just as in the single-
algorithm, i.e., as if were indeed the parent of and all other
neighbors of were children.
As in the single- algorithm, message passing is initiated at
the leaves. Each vertex remains idle until messages have ar-
rived on all but one of the edges incident on . Just as in the
Fig. 6. A factor-graph fragment, showing the update rules of the sum-product
algorithm.
single- algorithm, once these messages have arrived, is able
to compute a message to be sent on the one remaining edge
to its neighbor (temporarily regarded as the parent), just as in
the single- algorithm, i.e., according to Fig. 5. Let us denote
this temporary parent as vertex . After sending a message to
, vertex returns to the idle state, waiting for a “return mes-
sage” to arrive from . Once this message has arrived, the vertex
is able to compute and send messages to each of its neigh-
bors (other than ), each being regarded, in turn, as a parent.
The algorithm terminates once two messages have been passed
over every edge, one in each direction. At variable node ,
the product of all incoming messages is the marginal function
, just as in the single- algorithm. Since this algorithm op-
erates by computing various sums and products, we refer to it
as the sum-product algorithm.
The sum-product algorithm operates according to the fol-
lowing simple rule:
The message sent from a node on an edge is the
product of the local function at (or the unit function
if is a variable node) with all messages received at
on edges other than , summarized for the variable
associated with .
Let denote the message sent from node to node
in the operation of the sum-product algorithm, let
denote the message sent from node to node . Also, let
denote the set of neighbors of a given node in a factor graph.
Then, as illustrated in Fig. 6, the message computations per-
formed by the sum-product algorithm may be expressed as fol-
lows:
(5)
(6)
where is the set of arguments of the function .
The update rule at a variable node takes on the particularly
simple form given by (5) because there is no local function to
include, and the summary for of a product of functions of is
KSCHISCHANG et al.: FACTOR GRAPHS AND THE SUM-PRODUCT ALGORITHM 503
Fig. 7. Messages generated in each (circled) step of the sum-product
algorithm.
simply that product. On the other hand, the update rule at a local
function node given by (6) in general involves nontrivial func-
tion multiplications, followed by an application of the summary
operator.
We also observe that variable nodes of degree two perform
no computation: a message arriving on one (incoming) edge is
simply transferred to the other (outgoing) edge.
D. A Detailed Example
Fig. 7 shows the flow of messages that would be generated by
the sum-product algorithm applied to the factor graph of Fig. 1.
The messages may be generated in five steps, as indicated with
circles in Fig. 7. In detail, the messages are generated as follows.
Step 1:
Step 2:
Step 3:
Step 4:
Step 5:
Termination:
In the termination step, we compute as the product of
all messages directed toward . Equivalently, since the message
passed on any given edge is equal to the product of all but one
of these messages, we may compute as the product of the
two messages that were passed (in opposite directions) over any
single edge incident on . Thus, for example, we may compute
in three other ways as follows:
III. MODELING SYSTEMS WITH FACTOR GRAPHS
We describe now various ways in which factor graphs may be
used to model systems, i.e., collections of interacting variables.
In probabilistic modeling of systems, a factor graph can be
used to represent the joint probability mass function of the vari-
ables that comprise the system. Factorizations of this function
can give important information about statistical dependencies
among these variables.
Likewise, in “behavioral” modeling of systems—as in the
work of Willems [33]—system behavior is specified in set-the-
oretic terms by specifying which particular configurations of
variables are valid. This approach can be accommodated by a
factor graph that represents the characteristic (i.e., indicator)
function for the given behavior. Factorizations of this charac-
teristic function can give important structural information about
the model.
In some applications, we may even wish to combine these
two modeling styles. For example, in channel coding, we model
both the valid behavior (i.e., the set of codewords) and the a
posteriori joint probability mass function over the variables that
define the codewords given the received output of a channel.
(While it may even be feasible to model complicated channels
with memory [31], in this paper we will model only memoryless
channels.)
In behavioral modeling, “Iverson’s convention” [14, p. 24]
can be useful. If is a predicate (Boolean proposition) in-
volving some set of variables, then is the -valued
function that indicates the truth of , i.e.
if is true
otherwise.
(7)
For example, is the function that
takes a value of if the condition is satisfied, and
otherwise.
504 IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 47, NO. 2, FEBRUARY 2001
If we let denote the logical conjunction or “AND” operator,
then an important property of Iverson’s convention is that
(8)
(assuming and ). Thus, if can be written
as a logical conjunction of predicates, then can be factored
according to (8), and hence represented by a factor graph.
A. Behavioral Modeling
Let be a collection of variables with config-
uration space . By a behavior in ,
we mean any subset of . The elements of are the valid
configurations. Since a system is specified via its behavior ,
this approach is known as behavioral modeling [33].
Behavioral modeling is natural for codes. If the domain of
each variable is some finite alphabet , so that the configuration
space is the -fold Cartesian product , then a behavior
is called a block code of length over , and the valid
configurations are called codewords.
The characteristic (or set membership indicator) function for
a behavior is defined as
Obviously, specifying is equivalent to specifying . (We
might also give a probabilistic interpretation by noting that
is proportional to a probability mass function that is uniform
over the valid configurations.)
In many important cases, membership of a particular config-
uration in a behavior can be determined by applying a series
of tests (checks), each involving some subset of the variables.
A configuration is deemed valid if and only if it passes all tests;
i.e., the predicate may be written as a logical
conjunction of a series of “simpler” predicates. Then factors
according to (8) into a product of characteristic functions, each
indicating whether a particular subset of variables is an element
of some “local behavior.”
Example 2 (Tanner Graphs for Linear Codes): The char-
acteristic function for any linear code defined by an parity-
check matrix can be represented by a factor graph having
variable nodes and factor nodes. For example, if is the bi-
nary linear code with parity-check matrix
(9)
then is the set of all binary -tuples
that satisfy three simultaneous equations expressed in matrix
form as . (This is a so-called kernel representation,
since the linear code is defined as the kernel of a particular linear
transformation.) Membership in is completely determined by
checking whether each of the three equations is satisfied. There-
fore, using (8) and (9) we have
Fig. 8. A Tanner graph for the binary linear code of Example 2.
where denotes the sum in GF . The corresponding factor
graph is shown in Fig. 8, where we have used a special symbol
for the parity checks (a square with a “ ” sign). Although
strictly speaking the factor graph represents the factorization
of the code’s characteristic function, we will often refer to the
factor graph as representing the code itself. A factor graph
obtained in this way is often called a Tanner graph, after [29].
It should be obvious that a Tanner graph for any linear
block code may be obtained from a parity-check matrix
for the code. Such a parity-check matrix has columns and
at least rows. Variable nodes correspond to the columns
of and factor nodes (or checks) to the rows of , with an
edge-connecting factor node to variable node if and only if
. Of course, since there are, in general, many parity-
check matrices that represent a given code, there are, in general,
many Tanner graph representations for the code.
Given a collection of general nonlinear local checks, it may be
a computationally intractable problem to determine whether the
corresponding behavior is nonempty. For example, the canon-
ical NP-complete problem SAT (Boolean satisfiability) [13] is
simply the problem of determining whether or not a collection
of Boolean variables satisfies all clauses in a given set. In effect,
each clause is a local check.
Often, a description of a system is simplified by introducing
hidden (sometimes called auxiliary, latent, or state) variables.
Nonhidden variables are called visible. A particular behavior
with both auxiliary and visible variables is said to represent a
given (visible) behavior if the projection of the elements of
on the visible variables is equal to . Any factor graph for
is then considered to be also a factor graph for . Such graphs
were introduced by Wiberg et al. [31], [32] and may be called
Wiberg-type graphs. In our factor graph diagrams, as in Wiberg,
hidden variable nodes are indicated by a double circle.
An important class of models with hidden variables are the
trellis representations (see [30] for an excellent survey). A trellis
for a block code is an edge-labeled directed graph with distin-
guished root and goal vertices, essentially defined by the prop-
erty that each sequence of edge labels encountered in any di-
rected path from the root vertex to the goal vertex is a codeword
in , and that each codeword in is represented by at least one
such path. Trellises also have the property that all paths from
the root to any given vertex should have the same fixed length
, called the depth of the given vertex. The root vertex has depth
, and the goal vertex has depth . The set of depth vertices
can be viewed as the domain of a state variable . For example,
KSCHISCHANG et al.: FACTOR GRAPHS AND THE SUM-PRODUCT ALGORITHM 505
Fig. 9. (a) A trellis and (b) the corresponding Wiberg-type graph for the code of Fig. 8.
Fig. 9(a) is a trellis for the code of Example 2. Vertices at the
same depth are grouped vertically. The root vertex is leftmost,
the goal vertex is rightmost, and edges are implicitly directed
from left to right.
A trellis divides naturally into sections, where the th trellis
section is the subgraph of the trellis induced by the vertices
at depth and depth . The set of edge labels in may be
viewed as the domain of a (visible) variable . In effect, each
trellis section defines a “local behavior” that constrains the
possible combinations of , , and .
Globally, a trellis defines a behavior in the configuration
space of the variables . A configu-
ration of these variables is valid if and only if it satisfies the
local constraints imposed by each of the trellis sections. The
characteristic function for this behavior thus factors naturally
into factors, where the th factor corresponds to the th trellis
section and has , , and as its arguments.
The following example illustrates these concepts in detail for
the code of Example 2.
Example 3 (A Trellis Description): Fig. 9(a) shows a
trellis for the code of Example 2, and Fig. 9(b) shows the
corresponding Wiberg-type graph. In addition to the visible
variable nodes , there are also hidden (state)
variable nodes . Each local check, shown as a
generic factor node (black square), corresponds to one section
of the trellis.
In this example, the local behavior corresponding to the
second trellis section from the left in Fig. 9 consists of the fol-
lowing triples :
(10)
where the domains of the state variables and are taken to
be and , respectively, numbered from bottom
to top in Fig. 9(a). Each element of the local behavior corre-
sponds to one trellis edge. The corresponding factor node in
the Wiberg-type graph is the indicator function
.
It is important to note that a factor graph corresponding to
a trellis is cycle-free. Since every code has a trellis representa-
Fig. 10. Generic factor graph for a state-space model of a time-invariant or
time-varying system.
tion, it follows that every code can be represented by a cycle-free
factor graph. Unfortunately, it often turns out that the state-space
sizes (the sizes of domains of the state variables) can easily be-
come too large to be practical. For example, trellis representa-
tions of turbo codes have enormous state spaces [12]. However,
such codes may well have factor graph representations with rea-
sonable complexities, but necessarily with cycles. Indeed, the
“cut-set bound” of [31] (see also [8]) strongly motivates the
study of graph representations with cycles.
Trellises are basically conventional state-space system
models, and the generic factor graph of Fig. 10 can represent
any state-space model of a time-invariant or time-varying
system. As in Fig. 9, each local check represents a trellis
section; i.e., each check is an indicator function for the set of
allowed combinations of left (previous) state, input symbol,
output symbol, and right (next) state. (Here, we allow a trellis
edge to have both an input label and an output label.)
Example 4 (State-Space Models): For example, the
classical linear time-invariant state-space model is given by the
equations
(11)
where is the discrete time index,
are the time- input variables,
are the time- output variables, are
the time- state variables, , , , and are matrices of ap-
propriate dimension, and the equations are over some field .
506 IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 47, NO. 2, FEBRUARY 2001
Any such system gives rise to the factor graph of Fig. 10. The
time- check function
is
In other words, the check function enforces the local behavior
defined by (11).
B. Probabilistic Modeling
We turn now to another important class of functions that we
will represent by factor graphs: probability distributions. Since
conditional and unconditional independence of random vari-
ables is expressed in terms of a factorization of their joint prob-
ability mass or density function, factor graphs for probability
distributions arise in many situations. We begin again with an
example from coding theory.
Example 5 (APP Distributions): Consider the standard
coding model in which a codeword is
selected from a code of length and transmitted over a
memoryless channel with corresponding output sequence
. For each fixed observation , the joint
a posteriori probability (APP) distribution for the com-
ponents of (i.e., ) is proportional to the function
, where is the a priori distribution
for the transmitted vectors, and is the conditional
probability density function for when is transmitted.
Since the observed sequence is fixed for any instance of
“decoding” a graph we may consider to be a function of
only, with the components of regarded as parameters. In other
words, we may write or as , meaning that
the expression to be “decoded” always has the same parametric
form, but that the parameter will in general be different in
different decoding instances.
Assuming that the a priori distribution for the transmitted
vectors is uniform over codewords, we have ,
where is the characteristic function for and is the
number of codewords in . If the channel is memoryless, then
factors as
Under these assumptions, we have
(12)
Now the characteristic function itself may factor into a
product of local characteristic functions, as described in the pre-
vious subsection. Given a factor graph for , we obtain a
factor graph for (a scaled version of) the APP distribution over
simply by augmenting with factor nodes corresponding to the
different factors in (12). The th such factor has only
one argument, namely , since is regarded as a parameter.
Thus, the corresponding factor nodes appear as pendant vertices
(“dongles”) in the factor graph.
Fig. 11. Factor graph for the joint APP distribution of codeword symbols.
For example, if is the binary linear code of Example 2, then
we have
whose factor graph is shown in Fig. 11.
Various types of Markov models are widely used in signal
processing and communications. The key feature of such
models is that they imply a nontrivial factorization of the joint
probability mass function of the random variables in question.
This factorization may be represented by a factor graph.
Example 6 (Markov Chains, Hidden Markov Models): In
general, let denote the joint probability mass
function of a collection of random variables. By the chain rule
of conditional probability, we may always express this function
as
For example, if , then
which has the factor graph representation shown in Fig. 12(b).
In general, since all variables appear as arguments of
, the factor graph of Fig. 12(b) has no ad-
vantage over the trivial factor graph shown in Fig. 12(a). On the
other hand, suppose that random variables (in
that order) form a Markov chain. We then obtain the nontrivial
factorization
whose factor graph is shown in Fig. 12(c) for .
Continuing this Markov chain example, if we cannot observe
each directly, but instead can observe only , the output of a
memoryless channel with as input, then we obtain a so-called
“hidden Markov model.” The joint probability mass or density
function for these random variables then factors as
whose factor graph is shown in Fig. 12(d) for . Hidden
Markov models are widely used in a variety of applications;
e.g., see [27] for a tutorial emphasizing applications in signal
processing.
Of course, since trellises may be regarded as Markov models
for codes, the strong resemblance between the factor graphs of
KSCHISCHANG et al.: FACTOR GRAPHS AND THE SUM-PRODUCT ALGORITHM 507
Fig. 12. Factor graphs for probability distributions. (a) The trivial factor graph. (b) The chain-rule factorization. (c) A Markov chain. (d) A hidden Markov model.
Fig. 12(c) and (d) and the factor graphs representing trellises
(Figs. 9(b) and 10) is not accidental.
In Appendix B we describe very briefly the close relationship
between factor graphs and other graphical models for proba-
bility distributions: models based on undirected graphs (Markov
random fields) and models based on directed acyclic graphs
(Bayesian networks).
IV. TRELLIS PROCESSING
As described in the previous section, an important family of
factor graphs contains the chain graphs that represent trellises
or Markov models. We now apply the sum-product algorithm
to such graphs, and show that a variety of well-known algo-
rithms—the forward/backward algorithm, the Viterbi algorithm,
and the Kalman filter—may be viewed as special cases of the
sum-product algorithm.
A. The Forward/Backward Algorithm
We start with the forward/backward algorithm, sometimes re-
ferred to in coding theory as the BCJR [4], APP, or “MAP” al-
gorithm. This algorithm is an application of the sum-product
algorithm to the hidden Markov model of Example 6, shown
in Fig. 12(d), or to the trellises of examples Examples 3 and 4
(Figs. 9 and 10) in which certain variables are observed at the
output of a memoryless channel.
The factor graph of Fig. 13 models the most general situ-
ation, which involves a combination of behavioral and prob-
abilistic modeling. We have vectors ,
, and that represent, re-
spectively, input variables, output variables, and state variables
in a Markov model, where each variable is assumed to take on
values in a finite domain. The behavior is defined by local check
functions , as described in Examples 3 and
4. To handle situations such as terminated convolutional codes,
we also allow for the input variable to be suppressed in certain
trellis sections, as in the rightmost trellis section of Fig. 13.
This model is a “hidden” Markov model in which we cannot
observe the output symbols directly. As discussed in Example 5,
Fig. 13. The factor graph in which the forward/backward algorithm operates:
the s are state variables, the u are input variables, the x are output variables,
and each y is the output of a memoryless channel with input x .
the a posteriori joint probability mass function for , , and
given the observation is proportional to
where is again regarded as a parameter of (not an argument).
The factor graph of Fig. 13 represents this factorization of .
Given , we would like to find the APPs for each .
These marginal probabilities are proportional to the following
marginal functions associated with :
Since the factor graph of Fig. 13 is cycle-free, these marginal
functions may be computed by applying the sum-product algo-
rithm to the factor graph of Fig. 13.
Initialization: As usual in a cycle-free factor graph, the sum-
product algorithm begins at the leaf nodes. Trivial messages are
sent by the input variable nodes and the endmost state variable
nodes. Each pendant factor node sends a message to the corre-
sponding output variable node. As discussed in Section II, since
the output variable nodes have degree two, no computation is
performed; instead, incoming messages received on one edge
are simply transferred to the other edge and sent to the corre-
sponding trellis check node.
Once the initialization has been performed, the two endmost
trellis check nodes and will have received messages on
three of their four edges, and so will be in a position to create
an output message to send to a neighboring state variable node.
508 IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 47, NO. 2, FEBRUARY 2001
Fig. 14. A detailed view of the messages passed during the operation of the
forward/backward algorithm.
Again, since the state variables have degree two, no computation
is performed; at state nodes messages received on one edge are
simply transferred to the other edge.
In the literature on the forward/backward algorithm (e.g.,
[4]), the message is denoted as , the message
is denoted as , and the message
is denoted as . Additionally, the message will
be denoted as .
The operation of the sum-product algorithm creates two nat-
ural recursions: one to compute as a function of
and and the other to compute as a function of
and . These two recursions are called the forward
and backward recursions, respectively, according to the direc-
tion of message flow in the trellis. The forward and backward
recursions do not interact, so they could be computed in parallel.
Fig. 14 gives a detailed view of the message flow for a single
trellis section. The local function in this figure represents the
trellis check .
The Forward/Backward Recursions: Specializing the gen-
eral update equation (6) to this case, we find
Termination: The algorithm terminates with the computa-
tion of the messages.
These sums can be viewed as being defined over valid trellis
edges such that . For each edge
, we let , , and .
Denoting by the set of edges incident on a state in the
th trellis section, the and update equations may be rewritten
as
(13)
The basic operations in the forward and backward recursions
are therefore “sums of products.”
The and messages have a well-defined probabilistic inter-
pretation: is proportional to the conditional probability
mass function for given the “past” ; i.e., for
each state , is proportional to the condi-
tional probability that the transmitted sequence passed through
state given the past. Similarly, is proportional to
the conditional probability mass function for given the “fu-
ture” , i.e., the conditional probability that the
transmitted sequence passed through state . The probability
that the transmitted sequence passed through a particular edge
is thus given by
Note that if we were interested in the APPs for the state vari-
ables or the symbol variables , these could also be com-
puted by the forward/backward algorithm.
B. The Min-Sum and Max-Product Semirings and the Viterbi
Algorithm
We might in many cases be interested in determining which
valid configuration has largest APP, rather than determining
the APPs for the individual symbols. When all codeword are
a priori equally likely, this amounts to maximum-likelihood
sequence detection (MLSD).
As mentioned in Section II (see also [31], [2]), the codomain
of the global function represented by a factor graph may in
general be any semiring with two operations “ ” and “ ” that
satisfy the distributive law
(14)
In any such semiring, a product of local functions is well de-
fined, as is the notion of summation of values of . It follows
that the “not-sum” or summary operation is also well-defined.
In fact, our observation that the structure of a cycle-free factor
graph encodes expressions (i.e., algorithms) for the computation
of marginal functions essentially follows from the distributive
law (14), and so applies equally well to the general semiring
case. This observation is key to the “generalized distributive
law” of [2].
A semiring of particular interest for the MLSD problem is the
“max-product” semiring, in which real summation is replaced
with the “max” operator. For nonnegative real-valued quantities
, , and , “ ” distributes over “max”
Furthermore, with maximization as a summary operator,
the maximum value of a nonnegative real-valued function
is viewed as the “complete summary” of ; i.e.
For the MLSD problem, we are interested not so much in deter-
mining this maximum value, as in finding a valid configuration
that achieves this maximum.
In practice, MLSD is most often carried out in the negative
log-likelihood domain. Here, the “product” operation becomes
a “sum” and the “ ” operation becomes a “ ” operation,
KSCHISCHANG et al.: FACTOR GRAPHS AND THE SUM-PRODUCT ALGORITHM 509
so that we deal with the “min-sum” semiring. For real-valued
quantities , , , “ ” distributes over “min”
We extend Iverson’s convention to the general semiring case
by assuming that contains a multiplicative identity and a
null element such that and for all .
When is a predicate, then by we mean the -valued
function that takes value whenever is true and otherwise.
In the “min-sum” semiring, where the “product” is real addition,
we take and . Under this extension of Iverson’s
convention, factor graphs representing codes are not affected by
the choice of semiring.
Consider again the chain graph that represents a trellis, and
suppose that we apply the min-sum algorithm; i.e., the sum-
product algorithm in the min-sum semiring. Products of posi-
tive functions (in the regular factor graph) are converted to sums
of functions (appropriate for the min-sum semiring) by taking
their negative logarithm. Indeed, such functions can be scaled
and shifted (e.g., setting where
and are constants with ) in any manner that is con-
venient. In this way, for example, we may obtain squared Eu-
clidean distance as a “branch metric” in Gaussian channels, and
Hamming distance as a “branch metric” in discrete symmetric
channels.
Applying the min-sum algorithm in this context yields the
same message flow as in the forward/backward algorithm. As in
the forward/backward algorithm, we may write an update equa-
tion for the various messages. For example, the basic update
equation corresponding to (13) is
(15)
so that the basic operation is a “minimum of sums” instead of
a “sum of products.” A similar recursion may be used in the
backward direction, and from the results of the two recursions
the most likely sequence may be determined. The result is a
“bidirectional” Viterbi algorithm.
The conventional Viterbi algorithm operates in the forward
direction only; however, since memory of the best path is main-
tained and some sort of “traceback” is performed in making
a decision, even the conventional Viterbi algorithm might be
viewed as being bidirectional.
C. Kalman Filtering
In this section, we derive the Kalman filter (see, e.g., [3], [23])
as an instance of the sum-product algorithm operating in the
factor graph corresponding to a discrete-time linear dynamical
system similar to that given by (11). For simplicity, we focus on
the case in which all variables are scalars satisfying
where , , , and are the time- state, output, input,
and noise variables, respectively, and , , , and are
assumed to be known time-varying scalars. Generalization to the
case of vector variables is standard, but will not be pursued here.
We assume that the input and noise are independent white
Gaussian noise sequences with zero mean and unit variance, and
that the state sequence is initialized by setting . Since
linear combinations of jointly Gaussian random variables are
Gaussian, it follows that the and sequences are jointly
Gaussian.
We use the notation
to represent Gaussian density functions, where and rep-
resent the mean and variance. By completing the square in the
exponent, we find that
(16)
where
and
Similarly, we find that
(17)
As in Example 5, the Markov structure of this system permits
us to write the conditional joint probability density function of
the state variables given as
(18)
where is a Gaussian density with mean
and variance , and is a Gaussian density with
mean and variance . Again, the observed values of the
output variables are regarded as parameters, not as function ar-
guments.
The conditional density function for given observations up
to time is the marginal function
where we have introduced an obvious generalization of the
“not-sum” notation to integrals. The mean of this conditional
density, is the minimum mean-
squared-error (MMSE) estimate of given the observed
outputs. This conditional density function can be computed
via the sum-product algorithm, using integration (rather than
summation) as the summary operation.
A portion of the factor graph that describes (18) is shown
in Fig. 15. Also shown in Fig. 15 are messages that are
passed in the operation of the sum-product algorithm.
We denote by the message passed to from
. Up to scale, this message is always of the form
, and so may be represented by the
pair . We interpret as the MMSE
prediction of given the set of observations up to time .
According to the product rule, applying (16), we have
510 IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 47, NO. 2, FEBRUARY 2001
Fig. 15. A portion of the factor graph corresponding to (18).
where
and
Likewise, applying (17), we have
where
(19)
and
In (19), the value
is called the filter gain.
These updates are those used by a Kalman filter [3]. As men-
tioned, generalization to the vector case is standard. We note
that similar updates would apply to any cycle-free factor graph
in which all distributions (factors) are Gaussian. The operation
of the sum-product algorithm in such a graph can, therefore, be
regarded as a generalized Kalman filter, and in a graph with cy-
cles as an iterative approximation to the Kalman filter.
V. ITERATIVE PROCESSING: THE SUM-PRODUCT ALGORITHM
IN FACTOR GRAPHS WITH CYCLES
In addition to its application to cycle-free factor graphs, the
sum-product algorithm may also be applied to factor graphs
with cycles simply by following the same message propaga-
tion rules, since all updates are local. Because of the cycles
in the graph, an “iterative” algorithm with no natural termi-
nation will result, with messages passed multiple times on a
given edge. In contrast with the cycle-free case, the results of the
sum-product algorithm operating in a factor graph with cycles
cannot in general be interpreted as exact function summaries.
However, some of the most exciting applications of the sum-
product algorithm—for example, the decoding of turbo codes
or LDPC codes—arise precisely in situations in which the un-
derlying factor graph does have cycles. Extensive simulation re-
sults (see, e.g., [5], [21], [22]) show that with very long codes
such decoding algorithms can achieve astonishing performance
(within a small fraction of a decibel of the Shannon limit on a
Gaussian channel) even though the underlying factor graph has
cycles.
Descriptions of the way in which the sum-product algorithm
may be applied to a variety of “compound codes” are given in
[19]. In this section, we restrict ourselves to three examples:
turbo codes [5], LDPC codes [11], and repeat–accumulate (RA)
codes [6].
A. Message-Passing Schedules
Although a clock may not be necessary in practice, we
assume that messages are synchronized with a global dis-
crete-time clock, with at most one message passed on any
edge in any given direction at one time. Any such message
effectively replaces previous messages that might have been
sent on that edge in the same direction. A message sent from
node at time will be a function only of the local function at
(if any) and the (most recent) messages received at prior to
time .
Since the message sent by a node on an edge in general de-
pends on the messages that have been received on other edges at
, and a factor graph with cycles may have no nodes of degree
one, how is message passing initiated? We circumvent this diffi-
culty by initially supposing that a unit message (i.e., a message
representing the unit function) has arrived on every edge inci-
dent on any given vertex. With this convention, every node is in
a position to send a message at every time along every edge.
A message-passing schedule in a factor graph is a specifica-
tion of messages to be passed during each clock tick. Obviously
a wide variety of message-passing schedules are possible. For
example, the so-called flooding schedule [19] calls for a mes-
sage to pass in each direction over each edge at each clock tick.
A schedule in which at most one message is passed anywhere
in the graph at each clock tick is called a serial schedule.
We will say that a vertex has a message pending at an edge
if it has received any messages on edges other than after the
transmission of the most previous message on . Such a mes-
sage is pending since the messages more recently received can
affect the message to be sent on . The receipt of a message at
from an edge will create pending messages at all other edges
incident on . Only pending messages need to be transmitted,
since only pending messages can be different from the previous
message sent on a given edge.
In a cycle-free factor graph, assuming a schedule in which
only pending messages are transmitted, the sum-product algo-
rithm will eventually halt in a state with no messages pending.
In a factor graph with cycles, however, it is impossible to reach
a state with no messages pending, since the transmission of a
message on any edge of a cycle from a node will trigger a
chain of pending messages that must return to , triggering to
send another message on the same edge, and so on indefinitely.
In practice, all schedules are finite. For a finite schedule, the
sum-product algorithm terminates by computing, for each ,
the product of the most recent messages received at variable
KSCHISCHANG et al.: FACTOR GRAPHS AND THE SUM-PRODUCT ALGORITHM 511
Fig. 16. Turbo code. (a) Encoder block diagram. (b) Factor graph.
node . If has no messages pending, then this computation
is equivalent to the product of the messages sent and received
on any single edge incident on .
B. Iterative Decoding of Turbo Codes
A “turbo code” (“parallel concatenated convolutional code”)
has the encoder structure shown in Fig. 16(a). A block of data
to be transmitted enters a systematic encoder which produces ,
and two parity-check sequences and at its output. The first
parity-check sequence is generated via a standard recursive
convolutional encoder; viewed together, and would form the
output of a standard rate convolutional code. The second
parity-check sequence is generated by applying a permutation
to the input stream, and applying the permuted stream to a
second convolutional encoder. All output streams , , and
are transmitted over the channel. Both constituent convolutional
encoders are typically terminated in a known ending state.
A factor graph representation for a (very) short turbo code is
shown in Fig. 16(b). Included in the figure are the state variables
for the two constituent encoders, as well as a terminating trellis
section in which no data is absorbed, but outputs are generated.
Except for the interleaver (and the short block length), this graph
is generic, i.e., all standard turbo codes may be represented in
this way.
Iterative decoding of turbo codes is usually accomplished via
a message-passing schedule that involves a forward/backward
computation over the portion of the graph representing one con-
stituent code, followed by propagation of messages between en-
coders (resulting in the so-called extrinsic information in the
turbo-coding literature). This is then followed by another for-
ward/backward computation over the other constituent code,
and propagation of messages back to the first encoder. This
schedule of messages is illustrated in [19, Fig. 10]; see also [31].
C. LDPC Codes
LDPC codes were introduced by Gallager [11] in the early
1960s. LDPC codes are defined in terms of a regular bipartite
graph. In a LDPC code, left nodes, representing code-
word symbols, all have degree , while right nodes, representing
checks, all have degree . For example, Fig. 17 illustrates the
factor graph for a short LDPC code. The check enforces
the condition that the adjacent symbols should have even overall
parity, much as in Example 2. As in Example 2, this factor graph
is just the original unadorned Tanner graph for the code.
Fig. 17. A factor graph for a LDPC code.
LDPC codes, like turbo codes, are very effectively decoded
using the sum-product algorithm; for example MacKay and
Neal report excellent performance results approaching that of
turbo codes using what amounts to a flooding schedule [21],
[22].
D. RA Codes
RA codes are a special, low-complexity class of turbo codes
introduced by Divsalar, McEliece, and others, who initially de-
vised these codes because their ensemble weight distributions
are relatively easy to derive. An encoder for an RA code op-
erates on input bits repeating each bit times,
and permuting the result to arrive at a sequence .
An output sequence is formed via an accumulator
that satisfies and for .
Two equivalent factor graphs for an RA code are shown in
Fig. 18. The factor graph of Fig. 18(a) is a straightforward repre-
sentation of the encoder as described in the previous paragraph.
The checks all enforce the condition that incident variables sum
to zero modulo . (Thus a degree-two check enforces equality
of the two incident variables.) The equivalent but slightly less
complicated graph of Fig. 18(b) uses equality constraints to
represent the same code. Thus, e.g., , corre-
sponding to input variable and state variables , , and
of Fig. 18(a).
E. Simplifications for Binary Variables and Parity Checks
For particular decoding applications, the generic updating
rules (5) and (6) can often be simplified substantially . We treat
here only the important case where all variables are binary
(Bernoulli) and all functions except single-variable functions
are parity checks or repetition (equality) constraints, as in
Figs. 11, 17, and 18. This includes, in particular, LDPC codes
and RA codes. These simplifications are well known, some
dating back to the work of Gallager [11].
512 IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 47, NO. 2, FEBRUARY 2001
Fig. 18. Equivalent factor graphs for an RA code.
The probability mass function for a binary random variable
may be represented by the vector , where .
According to the generic updating rules, when messages
and arrive at a variable node of degree three,
the resulting (normalized) output message should be
(20)
Similarly, at a check node representing the function
(where “ ” represents modulo- addition), we have
(21)
We note that at check node representing the dual (repetition con-
straint) , we would have
i.e., the update rules for repetition constraints are the same as
those for variable nodes, and these may be viewed as duals to
those for a simple parity-check constraint.
We view (20) and (21) as specifying the behavior of ideal
“probability gates” that operate much like logic gates, but with
soft (“fuzzy”) values.
Since , binary probability mass functions can be
parametrized by a single value. Depending on the parametriza-
tion, various probability gate implementations arise. We give
four different parametrizations, and derive the and
functions for each.
Likelihood Ratio (LR):
Definition: .
Log-Likelihood Ratio (LLR):
Definition: .
(22)
Likelihood Difference (LD):
Definition: .
Signed Log-Likelihood Difference (SLLD):
Definition: .
if
if
In the LLR domain, we observe that for
Thus, an approximation to the function (22) is
which turns out to be precisely the min-sum update rule.
By applying the equivalence between factor graphs illustrated
in Fig. 19, it is easy to extend these formulas to cases where
variable nodes or check nodes have degree larger than three. In
particular, we may extend the and functions to more
than two arguments via the relations
(23)
Of course, there are other alternatives, corresponding to the var-
ious binary trees with leaf vertices. For example, when
we may compute as
which would have better time complexity in a parallel imple-
mentation than a computation based on (23).
VI. FACTOR-GRAPH TRANSFORMATIONS
In this section we describe a number of straightforward trans-
formations that may be applied to a factor graph in order to
KSCHISCHANG et al.: FACTOR GRAPHS AND THE SUM-PRODUCT ALGORITHM 513
Fig. 19. Transforming variable and check nodes of high degree to multiple
nodes of degree three.
modify a factor graph with an inconvenient structure into a more
convenient form. For example, it is always possible to transform
a factor graph with cycles into a cycle-free factor graph, but at
the expense of increasing the complexity of the local functions
and/or the domains of the variables. Nevertheless, such trans-
formations can be useful in some cases; for example, at the end
of this section we apply them to derive an FFT algorithm from
the factor graph representing the DFT kernel. Similar general
procedures are described in [17], [20], and in the construction
of junction trees in [2].
A. Clustering
It is always possible to cluster nodes of like type—i.e.,
all variable nodes or all function nodes—without changing
the global function being represented by a factor graph. We
consider the case of clustering two nodes, but this is easily
generalized to larger clusters. If and are two nodes being
clustered, simply delete and and any incident edges from
the factor graph, introduce a new node representing the pair
, and connect this new node to nodes that were neighbors
of or in the original graph.
When and are variables with domains and , re-
spectively, the new variable has domain . Note that
the size of this domain is the product of the original domain
sizes, which can imply a substantial cost increase in computa-
tional complexity of the sum-product algorithm. Any function
that had or as an argument in the original graph must be
converted into an equivalent function that has as an
argument, but this can be accomplished without increasing the
complexity of the local functions.
When and are local functions, by the pair we mean
the product of the local functions. If and denote the sets
of arguments of and , respectively, then is the set
of arguments of the product. Pairing functions in this way can
imply a substantial cost increase in computational complexity of
the sum-product algorithm; however, clustering functions does
not increase the complexity of the variables.
Clustering nodes may eliminate cycles in the graph so that
the sum-product algorithm in the new graph computes marginal
functions exactly. For example, clustering the nodes associated
with and in the factor graph fragment of Fig. 20(a) and con-
necting the neighbors of both nodes to the new clustered node,
we obtain the factor graph fragment shown in Fig. 20(b). No-
tice that the local function node connecting and in the
original factor graph appears with just a single edge in the new
factor graph. Also notice that there are two local functions con-
necting to .
The local functions in the new factor graph retain their de-
pendences from the old factor graph. For example, although
is connected to and the pair of variables , it does not ac-
tually depend on . So, the global function represented by the
new factor graph is
which is identical to the global function represented by the old
factor graph.
In Fig. 20(b), there is still one cycle; however, it can be re-
moved by clustering function nodes. In Fig. 20(c), we have clus-
tered the local functions corresponding to , , and
(24)
The new global function is
which is identical to the original global function.
In this case, by clustering variable vertices and function ver-
tices, we have removed the cycles from the factor graph frag-
ment. If the remainder of the graph is cycle-free, then the sum-
product algorithm may be used to compute exact marginals. No-
tice that the sizes of the messages in this region of the graph have
increased. For example, and have alphabets of size and
, respectively, and if functions are represented by a list of
their values, the length of the message passed from to
is equal to the product .
B. Stretching Variable Nodes
In the operation of the sum-product algorithm, in the mes-
sage passed on an edge , local function products are sum-
marized for the variable associated with the edge. Outside of
those edges incident on a particular variable node , any func-
tion dependency on is represented in summary form; i.e., is
marginalized out.
Here we will introduce a factor graph transformation that
will extend the region in the graph over which is represented
without being summarized. Let denote the set of nodes
that can be reached from by a path of length two in . Then
is a set of variable nodes, and for any , we
can pair and , i.e., replace with the pair , much as
in a clustering transformation. The function nodes incident on
would have to be modified as in a clustering transformation, but,
as before, this modification does not increase their complexity.
We call this a “stretching” transformation, since we imagine
node being “stretched” along the path from to .
More generally, we will allow further arbitrary stretching of
. If is a set of nodes to which has been stretched, we will
514 IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 47, NO. 2, FEBRUARY 2001
Fig. 20. Clustering transformations. (a) Original factor graph fragment. (b) Variable nodes y and z clustered. (c) Function nodes f , f , and f clustered.
Fig. 21. Stretching transformation. (a) Original factor graph. (b) Node x is stretched to x and x . (c) The node representing x alone is now redundant and
can be removed.
allow to be stretched to any element of , the set of vari-
able nodes reachable from any node of by a path of length
two. In stretching in this way, we retain the following basic
property: the set of nodes to which has been paired (together
with the connecting function nodes) induces a connected sub-
graph of the factor graph. This connected subgraph generates a
well-defined set of edges over which is represented without
being summarized in the operation of the sum-product algo-
rithm. This stretching leads to precisely the same condition that
define junction trees [2]: the subgraph consisting of those ver-
tices whose label includes a particular variable, together with
the edges connecting these vertices, is connected.
Fig. 21(a) shows a factor graph, and Fig. 21(b) shows an
equivalent factor graph in which has been stretched to all
variable nodes.
When a single variable is stretched in a factor graph, since
all variable nodes represent distinct variables, the modified vari-
ables that result from a stretching transformation are all distinct.
However, if we permit more than one variable to be stretched,
this may no longer hold true. For example, in the Markov chain
factor graph of Fig. 12(c), if both and are stretched to all
variables, the result will be a factor graph having two vertices
representing the pair . The meaning of such a peculiar
“factor graph” remains clear, however, since the local functions
and hence also the global function are essentially unaffected by
the stretching transformations. All that changes is the behavior
of the sum-product algorithm, since, in this example, neither
nor will ever be marginalized out. Hence we will permit
the appearance of multiple variable nodes for a single variable
whenever they arise as the result of a series of stretching trans-
formations.
Fig. 12(b) illustrates an important motivation for introducing
the stretching transformation; it may be possible for an edge, or
indeed a variable node, to become redundant. Let be a local
function, let be an edge incident on , and let be the set
of variables (from the original factor graph) associated with .
If is contained in the union of the variable sets associated
with the edges incident on other than , then is redundant. A
redundant edge may be deleted from a factor graph. (Redundant
edges must be removed one at a time, because it is possible for
an edge to be redundant in the presence of another redundant
edge, and become relevant once the latter edge is removed.) If
all edges incident on a variable node can be removed, then the
variable node itself is redundant and may be deleted.
For example, the node containing alone is redundant
in Fig. 21(b) since each local function neighboring has a
neighbor (other than ) to which has been stretched. Hence
this node and the edges incident on it can be removed, as shown
in Fig. 21(c). Note that we are not removing the variable
from the graph, but rather just a node representing . Here,
unlike elsewhere in this paper, the distinction between nodes
and variables becomes important.
Let be a variable node involved in a cycle, i.e., for which
there is a nontrivial path from to itself. Let
be the last two edges in , for some variable node and some
function node . Let us stretch along all of the variable nodes
involved in . Then the edge is redundant and hence can
be deleted since both and are incident on . (Actually,
KSCHISCHANG et al.: FACTOR GRAPHS AND THE SUM-PRODUCT ALGORITHM 515
Fig. 22. The DFT. (a) Factor graph. (b) A particular spanning tree. (c) Spanning tree after clustering and stretching transformation.
there is also another redundant edge, corresponding to traveling
in the opposite direction.) In this way, the cycle from to
itself is broken.
By systematically stretching variables around cycles and then
deleting a resulting redundant edge to break the cycle, it is pos-
sible to use the stretching transformation to break all cycles in
the graph, transforming an arbitrary factor graph into an equiva-
lent cycle-free factor graph for which the sum-product algorithm
produces exact marginals. This can be done without increasing
the complexity of the local functions, but comes at the expense
of an (often quite substantial) increase in the complexity of the
variable alphabets.
C. Spanning Trees
A spanning tree for a connected graph is a connected,
cycle-free subgraph of having the same vertex set as . Let
be a connected factor graph with a spanning tree and for every
variable node of , let denote the set of function nodes
having as an argument. Since is a tree, there is a unique
path between any two nodes of , and in particular between
and every element of . Now suppose is stretched to all
variable nodes involved in each path from to every element of
, and let be the resulting transformed factor graph.
It turns out that every edge of not in is redundant and all
such edges can be deleted from . Indeed, if is an edge of
not in , let be the set of variables associated with , and let
be the local function on which is incident. For every variable
, there is a path in from to , and is stretched to all
variable nodes along this path, and in particular is stretched to a
neighbor (in ) of . Since each element of appears in some
neighboring variable node not involving , is redundant. The
removal of does not affect the redundant status of any other
edge of not in , hence all such edges may be deleted from
.
This observation implies that the sum-product algorithm can
be used to compute marginal functions exactly in any spanning
tree of , provided that each variable is stretched along all
variable nodes appearing in each path from to a local function
having as an argument. Intuitively, is not marginalized out
in the region of in which is “involved.”
D. An FFT
An important observation due to Aji and McEliece [1], [2] is
that various fast transform algorithms may be developed using
a graph-based approach. We now show how we may use the
factor-graph transformations of this section to derive an FFT.
The DFT is a widely used tool for the analysis of dis-
crete-time signals. Let be a complex-
valued -tuple, and let , with , be a prim-
itive th root of unity. The DFT of is the complex-valued
-tuple where
(25)
Consider now the case where is a power of two, e.g.,
for concreteness. We express variables and in (25) in
binary; more precisely, we let and let
, where and take values from .
We write the DFT kernel, which we take as our global function,
in terms of these variables as
where and we have used the
relations , , and . We see
that the DFT kernel factors into a product of local functions as
expressed by the factor graph of Fig. 22(a).
We observe that
(26)
so that the DFT can be viewed as a marginal function, much
like a probability mass function. When is composite, sim-
516 IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 47, NO. 2, FEBRUARY 2001
ilar prime-factor-based decompositions of and will result in
similar factor graph representations for the DFT kernel.
The factor graph in Fig. 22(a) has cycles. We wish to carry
out exact marginalization, so we form a spanning tree. There
are many possible spanning trees, of which one is shown in
Fig. 22(b). (Different choices for the spanning tree will lead to
possibly different DFT algorithms when the sum-product algo-
rithm is applied.) If we cluster the local functions as shown in
Fig. 22(b), essentially by defining
then we arrive at the spanning tree shown in Fig. 22(c). The vari-
ables that result from the required stretching transformation are
shown. Although they are redundant, we have included variable
nodes and . Observe that each message sent from left to
right is a function of three binary variables, which can be repre-
sented as a list of eight complex quantities. Along the path from
to first , then , and then are marginalized
out as , , and are added to the argument list of the func-
tions. In three steps, the function is converted to the func-
tion . Clearly, we have obtained an FFT as an instance of the
sum-product algorithm.
VII. CONCLUSION
Factor graphs provide a natural graphical description of the
factorization of a global function into a product of local func-
tions. Factor graphs can be applied in a wide range of application
areas, as we have illustrated with a large number of examples.
A major aim of this paper was to demonstrate that a single
algorithm—the sum-product algorithm—based on only a single
conceptually simple computational rule, can encompass an
enormous variety of practical algorithms. As we have seen,
these include the forward/backward algorithm, the Viterbi
algorithm, Pearl’s belief propagation algorithm, the iterative
turbo decoding algorithm, the Kalman filter, and even certain
FFT algorithms! Various extensions of these algorithms—for
example, a Kalman filter operating on a tree-structured
system—although not treated here, can be derived in a straight-
forward manner by applying the principles enunciated in this
paper.
We have emphasized that the sum-product algorithm may
be applied to arbitrary factor graphs, cycle-free or not. In the
cycle-free finite case, we have shown that the sum-product al-
gorithm may be used to compute function summaries exactly.
In some applications, e.g., in processing Markov chains and
hidden Markov models, the underlying factor graph is natu-
rally cycle-free, while in other applications, e.g., in decoding
of LDPC codes and turbo codes, it is not. In the latter case, a
successful strategy has been simply to apply the sum-product
algorithm without regard to the cycles. Nevertheless, in some
cases it might be important to obtain an equivalent cycle-free
representation, and we have given a number of graph transfor-
mations that can be used to achieve such representations.
Factor graphs afford great flexibility in modeling systems.
Both Willems’ behavioral approach to systems and the tradi-
tional input/output or state-space approaches fit naturally in the
factor graph framework. The generality of allowing arbitrary
functions (not just probability distributions or characteristic
functions) to be represented further enhances the flexibility of
factor graphs.
Factor graphs also have the potential to unify modeling and
signal processing tasks that are often treated separately in cur-
rent systems. In communication systems, for example, channel
modeling and estimation, separation of multiple users, and de-
coding can be treated in a unified way using a single graphical
model that represents the interactions of these various elements,
as suggested by Wiberg [31]. We believe that the full potential of
this approach has not yet been realized, and we suggest that fur-
ther exploration of the modeling power of factor graphs and ap-
plications of the sum-product algorithm will prove to be fruitful.
APPENDIX A
FROM FACTOR TREES TO EXPRESSION TREES
Let be a function that can be repre-
sented by a cycle-free connected factor graph, i.e., a factor tree
. We are interested in developing an expression for
i.e., the summary for of . We consider to be the root of ,
so that all other vertices of are descendants of .
Assuming that has neighbors in , then without loss of
generality may be written in the form
where is the product of all local functions in the sub-
tree of that have the th neighbor of as root, and is the
set of variables in that subtree. Since is a tree, for ,
and , i.e.,
is a partition of . This decom-
position is represented by the generic factor tree of Fig. 23, in
which is shown in expanded form.
Now, by the distributive law, and using the fact that
are pairwise disjoint, we obtain
i.e., the summary for of is the product of the summaries for
of the functions.
KSCHISCHANG et al.: FACTOR GRAPHS AND THE SUM-PRODUCT ALGORITHM 517
Fig. 23. A generic factor tree.
Consider the case . To compute the summary for of
, observe that, without loss of generality, can be
written as
where, for convenience, we have numbered the arguments
of so that is the first neighbor of .
This decomposition is illustrated in Fig. 23. We note that
is a partition of . Again,
using the fact that these sets are pairwise-disjoint and applying
the distributive law, we obtain
In words, we see that if is a neighbor of ,
to compute the summary for of the product of the local func-
tions in the subtree of descending from , we should do the
following:
1) for each neighbor of (other than ), compute the
summary for of the product of the functions in the
subtree descending from ;
2) form the product of these summaries with , summa-
rizing the result for .
The problem of computing the summary for of the
product of the local subtree descending from is a problem
of the same general form with which we began, and so the
same general approach can be applied recursively. The result
of this recursion justifies the transformation of the factor
tree for with root vertex into an expression tree for
, as illustrated in Fig. 5.
APPENDIX B
OTHER GRAPHICAL MODELS FOR PROBABILITY DISTRIBUTIONS
Factor graphs are by no means the first graph-based language
for describing probability distributions. In the next two exam-
ples, we describe very briefly the close relationship between
factor graphs and models based on undirected graphs (Markov
random fields) and models based on directed acyclic graphs
(Bayesian networks).
A. Markov Random Fields
A Markov random field (see, e.g., [18]) is a graphical model
based on an undirected graph in which each node
corresponds to a random variable. The graph is a Markov
random field (MRF) if the distribution satisfies
the local Markov property
(27)
where denotes the set of neighbors of . In words, is an
MRF if every variable is independent of nonneighboring vari-
ables in the graph, given the values of its immediate neighbors.
MRFs are well developed in statistics, and have been used in a
variety of applications (see, e.g., [18], [26], [16], [15]).
A clique in a graph is a collection of vertices which are all
pairwise neighbors. Under fairly general conditions (e.g., pos-
itivity of the joint probability density is sufficient), the joint
probability mass function of an MRF may be expressed as the
product of a collection of Gibbs potential functions, defined on
the set of cliques in the MRF, i.e.
(28)
where is a normalizing constant, and each is a
clique. For example (cf. Fig. 1), the MRF in Fig. 24(a) may be
used to express the factorization
Clearly, (28) has precisely the structure needed for a factor
graph representation. Indeed, a factor graph representation may
be preferable to an MRF in expressing such a factorization,
since distinct factorizations, i.e., factorizations with different
’s in (28), may yield precisely the same underlying MRF
graph, whereas they will always yield distinct factor graphs.
(An example in a coding context of this MRF ambiguity is
given in [19].)
B. Bayesian Networks
Bayesian networks (see, e.g., [25], [17], [10]) are graphical
models for a collection of random variables that are based on
518 IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 47, NO. 2, FEBRUARY 2001
Fig. 24. Graphical probability models. (a) A Markov random field. (b) A Bayesian network. (c) A factor graph.
directed acyclic graphs (DAGs). Bayesian networks, combined
with Pearl’s “belief propagation algorithm” [25], have become
an important tool in expert systems. The first to connect
Bayesian networks and belief propagation with applications
in coding theory were MacKay and Neal [21]; more recently,
[19], [24] develop a view of the “turbo decoding” algorithm [5]
as an instance of probability propagation in a Bayesian network
model of a code.
Each node in a Bayesian network is associated with a
random variable. Denoting by the set of parents of (i.e.,
the set of vertices from which an edge is incident on ), by
definition, the distribution represented by the Bayesian network
may be written as
(29)
If (i.e., has no parents), we take .
For example (cf. (2)) Fig. 24(b) shows a Bayesian network that
expresses the factorization
(30)
Again, as with Markov random fields, Bayesian networks ex-
press a factorization of a joint probability distribution that is
suitable for representation by a factor graph. The factor graph
corresponding to (30) is shown in Fig. 24(c); cf. Fig. 1.
It is a straightforward exercise to translate the update
rules that govern the operation of the sum-product algorithm
to Pearl’s belief propagation rules [25], [17]. To convert a
Bayesian network into a factor graph: simply introduce a
function node for each factor in (29) and draw
edges from this node to and its parents . An example
conversion from a Bayesian network to a factor graph is shown
in Fig. 24(c).
Equations similar to Pearl’s belief updating and bottom-
up/top-down propagation rules [25, pp. 182–183] may be de-
rived from the general sum-product algorithm update equations
(5) and (6) as follows.
In belief propagation, messages are sent between “variable
nodes,” corresponding to the dashed ellipses for the particular
Bayesian network shown in Fig. 25. In a Bayesian network, if an
edge is directed from vertex to vertex , then is a parent of
Fig. 25. Messages sent in belief propagation.
and is a child of . Messages sent between variables are always
functions of the parent . In [25], a message sent from to is
denoted , while a message sent from to is denoted as
, as shown in Fig. 25 for the specific Bayesian network of
Fig. 24(c).
Consider the central variable in Fig. 25. Clearly, the mes-
sage sent upwards by the sum-product algorithm to the local
function contained in the ellipse is, from (5), given by the
product of the incoming messages, i.e.
The message sent from to is, according to (6), the product
of with the other messages received at summarized for .
Note that this local function is the conditional probability mass
function ; hence
Similarly, the message sent from to the ellipse con-
taining is given by
KSCHISCHANG et al.: FACTOR GRAPHS AND THE SUM-PRODUCT ALGORITHM 519
In general, let us denote the set of parents of a variable by
, and the set of children of by . We will have, for
every
(31)
and, for every
(32)
The termination condition for cycle-free graphs, called the “be-
lief update” equation in [25], is given by the product of the mes-
sages received by in the factor graph
BEL
(33)
Pearl also introduces a scale factor in (32) and (33) so that the
resulting messages properly represent probability mass func-
tions. The relative complexity of (31)–(33) compared with the
simplicity of the sum-product update rule given in Section II
provides a strong pedagogical incentive for the introduction of
factor graphs.
ACKNOWLEDGMENT
The concept of factor graphs as a generalization of Tanner
graphs was devised by a group at ISIT ’97 in Ulm, Germany,
that included the authors, G. D. Forney, Jr., R. Kötter, D. J. C.
MacKay, R. J. McEliece, R. M. Tanner, and N. Wiberg. The au-
thors benefitted greatly from the many discussions on this topic
that took place in Ulm. They wish to thank G. D. Forney, Jr.,
and the referees for many helpful comments on earlier versions
of this paper.
The work of F. R. Kschischang took place in part while on
sabbatical leave at the Massachusetts Institute of Technology
(MIT). He gratefully acknowledges the support and hospitality
of Prof. G. W. Wornell of MIT. H.-A. Loeliger performed his
work while with Endora Tech AG, Basel, Switzerland. He
wishes to acknowledge the support of F. Tarköy.
REFERENCES
[1] S. M. Aji and R. J. McEliece, “A general algorithm for distributing infor-
mation on a graph,” in Proc. 1997 IEEE Int. Symp. Information Theory,
Ulm, Germany, July 1997, p. 6.
[2] , “The generalized distributive law,” IEEE Trans. Inform. Theory,
vol. 46, pp. 325–343, Mar. 2000.
[3] B. D. O. Anderson and J. B. Moore, Optimal Filtering. Englewood
Cliffs, NJ: Prentice-Hall, 1979.
[4] L. R. Bahl, J. Cocke, F. Jelinek, and J. Raviv, “Optimal decoding of linear
codes for minimizing symbol error rate,” IEEE Trans. Inform. Theory,
vol. IT-20, pp. 284–287, Mar. 1974.
[5] C. Berrou, A. Glavieux, and P. Thitimajshima, “Near Shannonlimit
error-correcting coding and decoding: Turbo codes,” in Proc. 1993
IEEE Int. Conf. Communications, Geneva, Switzerland, May 1993, pp.
1064–1070.
[6] D. Divsalar, H. Jin, and R. J. McEliece, “Coding theorems for ‘turbo-
like’ codes,” in Proc. 36th Allerton Conf. Communications, Control, and
Computing, Urbana, IL, Sept. 23–25, 1998, pp. 201–210.
[7] G. D. Forney Jr., “On iterative decoding and the two-way algorithm,” in
Proc. Int. Symp. Turbo Codes and Related Topics, Brest, France, Sept.
1997.
[8] , “Codes on graphs: Normal realizations,” IEEE Trans. Inform.
Theory, vol. 47, pp. 520–548, Feb. 2001.
[9] B. J. Frey and F. R. Kschischang, “Probability propagation and iterative
decoding,” in Proc. 34th Annu. Allerton Conf. Communication, Control,
and Computing, Monticello, IL, Oct. 1–4, 1996.
[10] B. J. Frey, Graphical Models for Machine Learning and Digital Com-
munication. Cambridge, MA: MIT Press, 1998.
[11] R. G. Gallager, Low-Density Parity-Check Codes. Cambridge, MA:
MIT Press, 1963.
[12] R. Garello, G. Montorsi, S. Benedetto, and G. Cancellieri, “Interleaver
properties and their applications to the trellis complexity analysis of
turbo codes,” IEEE Trans. Commun., to be published.
[13] M. R. Garey and D. S. Johnson, Computers and Intractability: A Guide
to the Theory of NP-Completeness. New York: Freeman, 1979.
[14] R. L. Graham, D. E. Knuth, and O. Patashnik, Concrete Mathe-
matics. Reading, MA: Addison-Wesley, 1989.
[15] G. E. Hinton and T. J. Sejnowski, “Learning and relearning in Boltz-
mann machines,” in Parallel Distributed Processing: Explorations in
the Microstructure of Cognition, D. E. Rumelhart and J. L. McClelland,
Eds. Cambridge, MA: MIT Press, 1986, pp. 282–317.
[16] V. Isham, “An introduction to spatial point processes and Markov
random fields,” Int. Stat. Rev., vol. 49, pp. 21–43, 1981.
[17] F. V. Jensen, An Introduction to Bayesian Networks. New York:
Springer-Verlag, 1996.
[18] R. Kindermann and J. L. Snell, Markov Random Fields and Their Ap-
plications. Providence, RI: Amer. Math. Soc., 1980.
[19] F. R. Kschischang and B. J. Frey, “Iterative decoding of compound codes
by probability propagation in graphical models,” IEEE J. Select. Areas
Commun., vol. 16, pp. 219–230, Feb. 1998.
[20] S. L. Lauritzen and D. J. Spiegelhalter, “Local computations with prob-
abilities on graphical structures and their application to expert systems,”
J. Roy. Statist. Soc., ser. B, vol. 50, pp. 157–224, 1988.
[21] D. J. C. MacKay and R. M. Neal, “Good codes based on very sparse
matrices,” in Cryptography and Coding. 5th IMA Conference (Lecture
Notes in Computer Science), C. Boyd, Ed. Berlin, Germany: Springer,
1995, vol. 1025, pp. 100–111.
[22] D. J. C. MacKay, “Good error-correcting codes based on very sparse
matrices,” IEEE Trans. Inform. Theory, vol. 45, pp. 399–431, Mar. 1999.
[23] P. S. Maybeck, Stochastic Models, Estimation, and Control. New
York: Academic, 1979.
[24] R. J. McEliece, D. J. C. MacKay, and J.-F. Cheng, “Turbo decoding as
an instance of Pearl’s ‘belief propagation’ algorithm,” IEEE J. Select.
Areas Commun., vol. 16, pp. 140–152, Feb. 1998.
[25] J. Pearl, Probabilistic Reasoning in Intelligent Systems, 2nd ed. San
Francisco, CA: Kaufmann, 1988.
[26] C. J. Preston, Gibbs States on Countable Sets. Cambridge, U.K.: Cam-
bridge Univ. Press, 1974.
[27] L. Rabiner, “A tutorial on hidden Markov models and selected appli-
cations in speech recognition,” Proc. EEE, vol. 77, pp. 257–286, Feb.
1989.
[28] K. H. Rosen, Discrete Mathematics and its Applications, 4th ed. New
York: WCB/McGraw-Hill, 1999.
[29] R. M. Tanner, “A recursive approach to low complexity codes,” IEEE
Trans. Inform. Theory, vol. IT-27, pp. 533–547, Sept. 1981.
[30] A. Vardy, “Trellis structure of codes,” in Handbook of Coding Theory,
V. S. Pless and W. C. Huffman, Eds. Amsterdam, The Netherlands:
Elsevier, 1998, vol. 2.
[31] N. Wiberg, “Codes and decoding on general graphs,” Ph.D. dissertation,
Linköping Univ., Linköping, Sweden, 1996.
[32] N. Wiberg, H.-A. Loeliger, and R. Kötter, “Codes and iterative de-
coding on general graphs,” Eur. Trans. Telecomm., vol. 6, pp. 513–525,
Sept./Oct. 1995.
[33] J. C. Willems, “Models for Dynamics,” in Dynamics Reported, Volume
2, U. Kirchgraber and H. O. Walther, Eds. New York: Wiley, 1989, pp.
171–269.

